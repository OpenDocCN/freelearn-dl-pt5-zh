- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Text with Natural Language Processing
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Natural Language Processing** (**NLP**) is the field of machine learning
    that deals with the understanding of language, in the form of text data. It is
    one of the fields that has seen a strong evolution in the last few years, achieving
    great results in the areas of sentiment analysis, chatbots, text summarization,
    and machine translation. NLP is at the core of the assistants developed by Amazon
    (Alexa), Google, and Apple (Siri), as well as modern assistants such as ChatGPT
    and Llama 2.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will learn how to use GluonNLP, an MXNet Gluon library specific
    to NLP, how to build our own networks, and how to use its Model Zoo API for several
    applications of pre-trained models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing NLP networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying news highlights with topic modeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing sentiment in movie reviews
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Translating text from Vietnamese to English
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    technical requirements apply to this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you have completed *Recipe 1*, *Installing MXNet, Gluon, GluonCV
    and GluonNLP*, from [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016), *Up and Running*
    *with MXNet*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ensure that you have completed *Recipe 4*, *Toy dataset for text classification:
    Load, manage, and visualize a spam email dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch06](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch06)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, you can access each recipe directly from Google Colab; for example,
    use the following link for the first recipe of this chapter: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/6_1_Introducing_NLP_Networks.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/6_1_Introducing_NLP_Networks.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing NLP networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we saw how different architectures, such as **Multi-Layer
    Perceptrons** (**MLPs**) and **Convolutional Neural Networks** (**CNNs**), deal
    with numerical data and images, respectively. In this recipe, we will analyze
    the most important architectures to process natural language expressed as text
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The most important characteristic of natural language is that it is a list of
    words of variable length, and the order of those words matters; it is a sequence.
    The previous architectures that we have analyzed are not suited for variable-length
    data inputs and also do not exploit the relationships among words effectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will introduce neural networks that have been developed
    to process sequences of words:'
  prefs: []
  type: TYPE_NORMAL
- en: We will start by applying the network introduced in the previous chapter, that
    is, CNNs for text processing, called **TextCNNs**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Afterward, we will introduce **Recurrent Neural Networks** (**RNNs**) and their
    vanilla implementation. Then, we will continue with an improved version known
    as **Long Short-Term** **Memory** (**LSTM**).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, as we did with computer vision, we will introduce **GluonNLP Model Zoo**,
    one of the most value-adding features of MXNet. We will leverage these libraries
    MXNet and GluonNLP to understand and implement **transformers** and their self-attention
    mechanisms, and how these networks deal with sequences of variable length.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As in previous chapters, in this recipe, we will be using a little bit of matrix
    operations and linear algebra, but it will not be hard at all.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will be doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying CNNs for text processing (TextCNNs)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introducing RNNs
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improving RNNs with LSTM
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introducing GluonNLP Model Zoo
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Paying attention to Transformers
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s go through each of these network architectures in detail next.
  prefs: []
  type: TYPE_NORMAL
- en: Applying CNNs for text processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: CNNs were introduced in the previous chapter and are typically used for working
    with images. However, with some slight changes, CNNs can work very efficiently
    with text data.
  prefs: []
  type: TYPE_NORMAL
- en: 'Images are 2D data and, as shown in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with Computer Vision*, we worked with two layers on this data:'
  prefs: []
  type: TYPE_NORMAL
- en: 2D convolutions layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Max pooling layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These operations are slightly modified to work with text data, which can be
    seen as a 1D sequence. Therefore, for 1D convolution layers, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – 1D convolution](img/B16591_06_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – 1D convolution
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in *Figure 6**.1*, the sequence combined with the kernel varies
    over time, yielding a new sequence as output. Please note how the number of words
    that are analyzed at the same time is the kernel size (3 in the preceding figure).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the max pooling layers, as we only have one dimension, which corresponds
    to time, these layers are known as **max-over-time** **pooling** layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Max-over-time pooling](img/B16591_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Max-over-time pooling
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen from *Figure 6**.2*, the maximum value from the sequence is selected.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using MXNet Gluon, we can define the 1D convolution and max-over-time layers
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: An example of how to work with these layers can be found in the GitHub code.
  prefs: []
  type: TYPE_NORMAL
- en: As with the CNN architecture shown in the previous chapter (see *Figure 5**.6*),
    typically, after a feature learning phase, we have a classifier. As an example
    application, this kind of architecture will help us later with sentiment analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Recurrent Neural Networks (RNNs)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As discussed in the recipe introduction, RNNs are architectures that deal with
    sequences of data of variable length. For NLP, these data points are sentences,
    composed of words, but they can also be utilized for sequences of images (video),
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: RNN’s history is a series of step-by-step attempts to improve the recurrent
    processing of different inputs of a sequence. There have been several important
    contributions, the most notable being Hopfield (1982), Jordan (1986), and Elman
    (1990).
  prefs: []
  type: TYPE_NORMAL
- en: The idea behind RNNs is that once the output is processed from the input, that
    output is fed again to the model (in a recurrent manner), in combination with
    the new incoming input. This mechanism allows the model to have memory (can access
    past data) and process the new input, taking into account that past information
    as well. This basic architecture is typically referred to as a **vanilla RNN**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that, as introduced in *Recipe 4*, *Understanding text datasets
    – loading, managing, and visualizing the Enron Email dataset*, in [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon and DataLoader*, the inputs
    to NLP networks, including RNNs, are not the words obtained from the dataset,
    but numerical representations of those words, such as one-hot encoding or word
    embeddings.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If a sequence of data is modeled as successive inputs, x(1), x(2), .... x(t),
    the architecture of an RNN can be visualized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – RNN architecture](img/B16591_06_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – RNN architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6**.3*, we can see how every input *x(t)* is processed over time,
    and how the processed input (hidden state) is looped back for the next iterations.
    Let’s take a deeper look at what is happening at each step, as in the preceding
    figure, activation functions and biases are not shown for simplicity. The actual
    equations of a vanilla RNN cell are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Equations for a vanilla RNN cell](img/B16591_06_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Equations for a vanilla RNN cell
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in *Figure 6**.4*, for each step, the input, *x(t)*, is multiplied
    by a weight matrix, *U*, and a bias vector, *b*, is added, which yields the value
    *a(t)*, assuming there was no previous input, *h(t – 1) = 0*. The state value,
    *h(t)*, is computed as the output of the activation function, *tanh*, of that
    value. When there is a previous input, the previous state value, *h(t – 1)*, is
    multiplied by a weight matrix, *W*, and added to the computations of values *a(t)*
    and *h(t)*.
  prefs: []
  type: TYPE_NORMAL
- en: The state value, *h(t)*, is then multiplied by a weight matrix, *V*, and a bias
    vector, *c*, is added which yields the value *o(t)*. The output of the cell is
    computed as the output of the activation function, *softmax*, of that value, yielding
    the final output value *y(t)*.
  prefs: []
  type: TYPE_NORMAL
- en: These cells can be stacked together to produce a complete RNN.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using MXNet and Gluon, we can easily create our own custom RNN networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use this code to define a custom RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, we use the following to process a sequential input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The previous code shown runs a custom RNN. Feel free to play with the notebook
    available in the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, one advantage of RNNs is that the information present in previous
    inputs is kept stored in the states transferred along time steps. However, this
    information is constantly multiplied by different weight matrices and passed through
    non-linear functions (*tanh*), and the outcome is that, after several time steps,
    the state information is modified and does not work anymore as memory; the information
    stored has changed too much. Long-term memory storage is a problem for RNNs.
  prefs: []
  type: TYPE_NORMAL
- en: Training RNNs
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous chapters, we saw how to train networks using supervised learning,
    by computing the loss function between the expected outputs, the **ground truth**,
    and the actual outputs of the network. This error could then be back-propagated
    from the outer layers of the network to the inner layers of the network and update
    the weights of these layers.
  prefs: []
  type: TYPE_NORMAL
- en: With RNNs, at each time step, the network is shown a sequence of inputs and
    the expected sequence in the outputs. Errors are computed for each time step and
    back-propagated from the outer layers of the network to the inner layers of the
    network. This variation, suitable for RNNs, is called **Back-Propagation Through**
    **Time** (**BPTT**).
  prefs: []
  type: TYPE_NORMAL
- en: 'As with other networks, computing the different gradients involves the iterative
    multiplication of matrices. This operation is exponential, which means that, after
    several occurrences, the values will either shrink or blow up. This leads to a
    problem we have already discussed: **vanishing gradients** and **exploding gradients**.
    These issues make the training of RNNs very unstable. Another important drawback
    of BPTT is that as it is a sequential computation, it cannot be parallelized.'
  prefs: []
  type: TYPE_NORMAL
- en: Improving RNNs with Long Short-Term Memory (LSTM)
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'LSTMs were introduced by Hochreiter and Schmidhuber in 1997, as a mechanism
    to solve the problems described previously (lack of long-term memory and vanishing/exploding
    gradients). In LSTMs, instead of having the previous state multiplied and passed
    through the non-linear function, the connection is much more straightforward.
    To provide this mechanism, each LSTM cell receives two inputs from the previous
    cell: the **hidden state** (**ht**) and the **cell** **state** (**ct**):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – LSTM network](img/B16591_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – LSTM network
  prefs: []
  type: TYPE_NORMAL
- en: 'The key components of LSTMs are called gates, which define how a certain input
    is modified to become a part of the outputs. These vectors have values between
    0 and 1 and help activate/deactivate the information from the input. They are,
    therefore, a sigmoid operation (depicted in *Figure 6**.5* as *σ*) of a weighted
    sum of the inputs, followed by a multiplication operation. To emphasize, each
    of the three gates present in an LSTM cell allows how much of the input or the
    state passes through each of the outputs. Taking this into account, the following
    equations define the LSTM behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 6.6 – Equations for \uFEFFan LSTM cell](img/B16591_06_6.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Equations for an LSTM cell
  prefs: []
  type: TYPE_NORMAL
- en: 'The equations in *Figure 6**.6* can be explained as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input gate (it)**: This is the gate that decides how the information from
    the previous state and the current input will be updated.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forget gate (ft)**: This is the gate that decides how the information from
    the previous state and the current input will become part of the long-term memory
    (cell state). This is how much of the current step we want to forget.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory cell candidate (gt)**: This is the computation that decides how the
    information from the previous state and the current input will become part of
    the memory cell. It must allow for positive and negative values; therefore, *tanh*
    is the activation function selected.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output gate (ot)**: This is the gate that decides how the information from
    the previous state and the current input will become part of the output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory cell (ct)**: This is the computation that combines the previous state
    (*c*t-1) and the current memory cell candidate (*g*t) into the new cell state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output state (ht)**: This is the computation that combines the memory cell
    with the output gate value.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using MXNet and Gluon, we can easily create our own custom LSTM networks:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can use the following code to define a custom RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Furthermore, we can use the following to process a sequential input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The code shown here runs a custom LSTM network. Feel free to play with the notebook
    available in the GitHub repository accompanying this book.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude, LSTMs allow RNNs to be trained more optimally and have been implemented
    to solve a large number of tasks in NLP, such as sentiment analysis and language
    modeling.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing GluonNLP Model Zoo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the best features that MXNet GluonCV provides is its large pool of pre-trained
    models, readily available for its users to use and deploy in their own applications.
    This model library is called **Model Zoo**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Model Zoo, models have been pre-trained for the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Language models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment analysis
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Machine translation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentence classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Question answering
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Named entity recognition
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Joint intent classification and slot labeling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will examine in detail the pre-trained models included for
    sentiment analysis and machine translation.
  prefs: []
  type: TYPE_NORMAL
- en: Paying attention with Transformers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although LSTMs have been proven to work well for a lot of applications, they
    also have significant drawbacks, including taking longer and requiring more memory
    to train, as well as being sensitive to random initialization. New architectures
    have been developed that overcome these limitations. One of the most important
    examples is **Transformers**.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers were introduced by Google Brain in 2017\. It is a novel approach
    of an encoder-decoder architecture (as seen in *Recipe 4*, *Segmenting objects
    in images with PSPNet and DeepLab-v3*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with Computer Vision*) with a repurposed mechanism to process
    sequences of data, called **attention**. The largest improvement of this architecture
    is that it does not depend on processing the data sequentially. All the data can
    be processed in parallel, allowing for faster training and inference. This improvement
    allowed a very large amount of text, the **corpus**, to be processed, yielding
    **Large Language Models** (**LLMs**) such as **Bidirectional Encoder Representations
    from** **Transformers** (**BERT**).
  prefs: []
  type: TYPE_NORMAL
- en: 'The architecture of Transformers can be represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Transformer architecture](img/B16591_06_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – Transformer architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 6**.7*, we can distinguish several components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Input and output preprocessing**: Embeddings and positional encodings are
    computed before inputting the data into the network.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Encoder-decoder architecture**: The left part corresponds to the encoder
    and the right part corresponds to the decoder. Feed-forward processing, residual
    connections, and normalization are parts of this component.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Attention heads**: The sequence inputs the path from the encoder to the decoder
    and the sequence outputs are all processed via this mechanism.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see each of these components in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Input and output preprocessing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the original paper, the inputs and outputs, before being fed into the network,
    are processed through **learned embeddings**. These embedding vectors typically
    have a size of 512 elements.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, as we will see afterward, these embeddings are passed through a
    *softmax* function, combining several pieces of information into one number, but
    also losing the positional information in the process. To maintain the positional
    information through the whole encoding and decoding, for both inputs and outputs,
    the input embeddings are added as a vector with information on position. These
    are called **positional encodings**.
  prefs: []
  type: TYPE_NORMAL
- en: Encoder-decoder architecture
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As noted in the original *Google Brain’s original Transformer* paper: [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf
    ) the encoder and decoder are composed of six identical layers (in *Figure 6**.5*,
    *N = 6* in the left diagram and right diagram, respectively). Each encoder layer
    has two components, a multi-head self-attention mechanism followed by a fully
    connected feed-forward network. For each decoded layer, another attention component
    is added, a masked multi-head self-attention mechanism. Attention heads will be
    explained in the following section.'
  prefs: []
  type: TYPE_NORMAL
- en: Residual connections are also added, similar to what we saw for ResNets in *Recipe
    2*, *Classifying images with MXNet – GluonCV Model Zoo, AlexNet, and ResNet*,
    in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing Images with* *Computer
    Vision*.
  prefs: []
  type: TYPE_NORMAL
- en: This information together is normalized using layer normalization, which is
    similar to batch normalization, introduced in *Recipe 3*, *Training for regression
    models*, in [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving Regression
    Problems*. The most important difference is that, as described in the paper introducing
    layer normalization ([https://arxiv.org/abs/1607.06450](https://arxiv.org/abs/1607.06450)),
    with layer normalization, all the hidden units in a layer share the same normalization
    terms, but different input data can have different normalization terms. Layer
    normalization has been proven to work better than batch normalization for processing
    sequences.
  prefs: []
  type: TYPE_NORMAL
- en: By combining all these layers across the different encoder and decoder steps
    (including the embeddings), the vectors will all have a dimension of 512.
  prefs: []
  type: TYPE_NORMAL
- en: Attention heads
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In Transformers, how each word in a sequence is connected to each of the words
    in another sequence is done via the attention mechanism. For example, if we have
    an input sequence with *N* words in English (*I love you very much*) and its translation
    to French has *M* words (*Je t’aime beaucoup*), the attention matrix of weights
    between these two sequences will have *NxM* dimensions. Connecting sequences using
    this mechanism has a strong advantage over a recurrent mechanism (such as the
    one used in RNNs), which is parallelization. Attention is a matrix operation and
    therefore can be optimally parallelized.
  prefs: []
  type: TYPE_NORMAL
- en: '|  | I | love | you | very | much |'
  prefs: []
  type: TYPE_TB
- en: '| je | 0.90 | 0.02 | 0.06 | 0.01 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| t’ | 0.11 | 0.01 | 0.80 | 0.03 | 0.05 |'
  prefs: []
  type: TYPE_TB
- en: '| aime | 0.03 | 0.92 | 0.03 | 0.01 | 0.01 |'
  prefs: []
  type: TYPE_TB
- en: '| beaucoup | 0.02 | 0.02 | 0.02 | 0.41 | 0.53 |'
  prefs: []
  type: TYPE_TB
- en: Figure 6.8 – Example of attention matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'In the Transformer paper, three of the matrices shown in *Figure 6**.9* were
    introduced, *Query (Q), Key (K), and Value (V).* The following is an explanation
    of each of these matrices:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Query**: This is the input representation of each input word'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Key and value**: Similar to a **hash map** that maps keys into values, these
    matrices are used for indexing (key) and providing a representation (value) of
    the word (different from the query)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The combination of the operations carried out by these three matrices is called
    the dot-product attention function and can be described as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Attention function](img/B16591_06_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.9 – Attention function
  prefs: []
  type: TYPE_NORMAL
- en: When the output is a representation of the input sequence, this mechanism is
    called a **self-attention head**. In the architecture diagram shown earlier in
    *Figure 6**.7*, the two attention mechanisms closer to the input sequence and
    the output sequence (lower part of the diagram) are self-attention mechanisms,
    as the sequence outputted by the function is the same as the sequence that is
    inputted. When this is not the case, such as in the attention mechanism that connects
    the encoder to the decoder, this is known as a **cross-attention head**. The self-attention
    head for the output vectors is masked, meaning that only past information is available
    in the training of the network. This allows Transformer models to generate text
    from a limited input (auto-regressive models such as GPT-3 or BLOOM).
  prefs: []
  type: TYPE_NORMAL
- en: 'Instead of processing all the input data in parallel, in Google Brain’s Attention
    is All You Need original paper ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)),
    eight attention heads are used in parallel. As the output is expected to have
    the same dimensionality (512), each head works with a reduced vector (with a dimensionality
    of 64). This is described in the paper as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Multi-head attention allows the model to jointly attend to information from
    different representation subspaces at different positions."'
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the dimensionality allows for the total computation cost to be similar
    to using a complete (full-dimensionality) attention head.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation in GluonNLP
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'GluonNLP has its own implementation of a Transformer model and therefore, getting
    our encoder and decoder is straightforward, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can use the encoder to process the inputs; however, with Transformers,
    we can process the whole input at the same time:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Large-scale Transformers are the current state of the art for most NLP tasks,
    such as topic modeling, sentiment analysis, or question answering, and the encoder
    and decoder architectures are also used separately for different tasks.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we have introduced several networks to work with NLP using
    MXNet, Gluon, and GluonNLP:'
  prefs: []
  type: TYPE_NORMAL
- en: RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have reviewed how each of these architectures works and analyzed its advantages
    and disadvantages, as well as how each one has improved on the previous one, exploring
    concepts such as sequences, BPTT, memory cells, and attention.
  prefs: []
  type: TYPE_NORMAL
- en: In the following recipes, we will explore how to use these architectures to
    solve practical problems such as topic modeling, sentiment analysis, and machine
    translation.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Some of the concepts explored in this recipe are too advanced to cover in detail
    in this book. I strongly suggest taking a look at the following references if
    you would like to get a deeper understanding:'
  prefs: []
  type: TYPE_NORMAL
- en: '**TextCNNs (****Paper):** [https://aclanthology.org/D14-1181.pdf](https://aclanthology.org/D14-1181.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RNNs (Intuitive explanation):** [https://towardsdatascience.com/a-battle-against-amnesia-a-brief-history-and-introduction-of-recurrent-neural-networks-50496aae6740](https://towardsdatascience.com/a-battle-against-amnesia-a-brief-history-and-introduction-of-recurrent-neural-networks-50496aae6740)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RNNs (Backpropagation Through** **Time):** [https://d2l.ai/chapter_recurrentneural-networks/bptt.html](https://d2l.ai/chapter_recurrentneural-networks/bptt.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vanishing/exploding gradients research papers (Learning Long-Term Dependencies
    with Gradient Descent is** **Difficult):** [http://www.comp.hkbu.edu.hk/~markus/teaching/comp7650/tnn-94-gradient.pdf](http://www.comp.hkbu.edu.hk/~markus/teaching/comp7650/tnn-94-gradient.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vanishing/exploding gradients research papers: (On the difficulty of training
    Recurrent Neural** **Networks):** [https://arxiv.org/pdf/1211.5063.pdf](https://arxiv.org/pdf/1211.5063.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vanishing/exploding gradients research papers (The exploding gradient problem
    demystified - definition, prevalence, impact, origin, tradeoffs,and** **solutions):**
    [https://arxiv.org/pdf/1712.05577.pdf](https://arxiv.org/pdf/1712.05577.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LSTMs (****Paper):** [http://www.bioinf.jku.at/publications/older/2604.pdf](http://www.bioinf.jku.at/publications/older/2604.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LSTMs (Intuitive** **explanation):** [http://colah.github.io/posts/2015-08-Understanding-LSTMs/](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformers (Original paper – Attention Is All You** **Need):** [https://arxiv.org/pdf/1706.03762.pdf](https://arxiv.org/pdf/1706.03762.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformers (Intuitive** **explanation):** [https://towardsdatascience.com/transformers-89034557de14](https://towardsdatascience.com/transformers-89034557de14)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformers (Layer** **Normalization):** [https://arxiv.org/pdf/1607.06450.pdf](https://arxiv.org/pdf/1607.06450.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State-of-the-art models in NLP (GPT-3** **paper):** [https://arxiv.org/pdf/2005.14165.pdf](https://arxiv.org/pdf/2005.14165.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**State-of-the-art models in NLP (BLOOM (open source** **alternative):** [https://huggingface.co/blog/bloom](https://huggingface.co/blog/bloom)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we are going to analyze in detail the following tasks: topic
    modeling, sentiment analysis, and text translation. However, MXNet GluonNLP Model
    Zoo contains lots of models pre-trained for a large number of tasks. You are encouraged
    to explore the different examples included at [https://nlp.gluon.ai/model_zoo/index.html](https://nlp.gluon.ai/model_zoo/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Classifying news highlights with topic modeling
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we are going to study one of the most interesting tasks in NLP,
    topic modeling. In this task, the user must find the number of topics given a
    set of documents. Sometimes, the topics (and the number of topics) are known beforehand
    and the supervised learning techniques that we have seen in previous chapters
    can be applied. However, in a typical scenario, topic modeling datasets do not
    provide ground truth and are therefore unsupervised learning problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we will use a pre-trained model from GluonNLP Model Zoo and
    apply its word embeddings to feed a clustering algorithm, which will yield the
    clustered topics. We will apply this process to a new dataset: *1 Million* *News
    Headlines*.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As in previous chapters, in this recipe, we will be using a little bit of matrix
    operations and linear algebra, but it will not be hard at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we will be working with text datasets. Therefore, we will revisit
    some concepts already seen in *Recipe 4*, *Understanding text datasets – loading,
    managing, and visualizing the Enron Email dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will be carrying out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the *1 Million News* *Headlines* dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Applying word embeddings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clustering the topics using K-means
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Putting everything together
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s go through each of these steps in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the 1 Million News Headlines dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This dataset is one of the most well-known datasets used for topic modeling.
    It contains 19 years of noteworthy news headlines published by the **Australian
    Broadcasting Corporation** (**ABC**) website, from 2003 to 2021 (inclusive). The
    topic of each news headline is not included.
  prefs: []
  type: TYPE_NORMAL
- en: 'As expected from a real-world dataset, the corpus contains a large number of
    words. Therefore, before going further, we will proceed to clean the data and
    follow the process described in *Recipe 4*, *Understanding text datasets – loading,
    managing, and visualizing the Enron Email dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*:'
  prefs: []
  type: TYPE_NORMAL
- en: Tokenizing
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lemmatization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Furthermore, this dataset contains more than 1 million headlines (1.2 million,
    actually). In order to be able to process them in a time-efficient manner, we
    will work with a reduced subset of 5%:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If we analyze this subset to compute the number of words each headline has,
    we can plot the following histogram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Distribution of headlines by number of words](img/B16591_06_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.10 – Distribution of headlines by number of words
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in *Figure 6**.10*, most of the headlines have between 4 and
    8 words.
  prefs: []
  type: TYPE_NORMAL
- en: Applying word embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In *Recipe 4*, *Understanding text datasets – loading, managing, and visualizing
    the Enron Email dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon and DataLoader*, we introduced
    two embedding models from GluonNLP Model Zoo: Google’s **word2vec** and **GloVe**
    from Stanford University. For this use case, we are going to work with word2vec,
    because it was trained on a dataset called Google News, composed of 3 million
    words and phrases from a corpus of 100 billion words. As the corpus is composed
    of news information, this embedding model is very well suited for our use case.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.11 – 2D representation of word2vec embeddings](img/B16591_06_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.11 – 2D representation of word2vec embeddings
  prefs: []
  type: TYPE_NORMAL
- en: By using this model, each word is transformed into a vector with 300 components.
    However, for our application with headlines (full sentences), we are more interested
    in a representation of the complete headline and not just its independent words.
    A simple but effective method to accomplish this for our application is to compute
    the average vector of each preprocessed word.
  prefs: []
  type: TYPE_NORMAL
- en: Clustering the topics using K-means
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With our headline embeddings ready, the last step to classify our headlines
    is to cluster them. There are many clustering algorithms, such as **expectation
    maximization clustering** and **mean shift clustering**. However, for our application,
    we will use my favorite one, **K-means**, which is implemented in a Python package
    we have already talked about, **scikit-learn**.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – K-means visualization](img/B16591_06_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.12 – K-means visualization
  prefs: []
  type: TYPE_NORMAL
- en: The intuitive idea behind K-means is that given a number of clusters, *K*, it
    will assign the clusters’ centroids randomly, and add each newly seen vector to
    the closest centroid (assignment step). Then, it will compute the new centroid
    as the mean of the vectors that belong to the cluster (update step) and iterate.
    This process is repeated until there is no significant change in the centroids’
    positions. Therefore, the full dataset can be iterated several times. For large
    datasets, other criteria can be added for convergence and stopping criteria.
  prefs: []
  type: TYPE_NORMAL
- en: One important drawback of K-means is that the number of clusters is an input
    parameter, and therefore, it requires some intuition or knowledge about the dataset.
    In practice, knowing this information beforehand is difficult. Therefore, I strongly
    recommend running the algorithm for several different values of *K*. For our use
    case, the number of clusters chosen is 4.
  prefs: []
  type: TYPE_NORMAL
- en: Putting everything together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: After this three-step process (cleaning the data, embedding, and clustering),
    we are ready to analyze some results.
  prefs: []
  type: TYPE_NORMAL
- en: 'The most interesting output of topic modeling is identifying which headline
    topics are associated with each cluster. A helpful approach to doing this is to
    visualize the most important words for each cluster and come up with the connecting
    topic. Therefore, we can plot the following for the first identified cluster (cluster
    0):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Top 15 words in order of importance for the first cluster](img/B16591_06_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.13 – Top 15 words in order of importance for the first cluster
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6**.13*, we can see that the most important words are `win`, `world`,
    `cup`, and `final`. All these words are sports-related, and therefore the topic
    for cluster 0 is *sports*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can put together the most important words per cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'From the preceding information, we can conclude the topics for each cluster:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Cluster` `0`: Sports'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Cluster 1`: Global affairs'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Cluster` `2`: Economy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Cluster 3`: Crime/current happenings'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With this information, now we can take any of the headlines that were not used
    during the clustering process and predict their topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This piece of code will yield a similar statement to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The preceding sentence is a clear reference to sports; therefore, we would expect
    the predicted cluster to be `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to predict the cluster group, we need to follow following steps: Define
    the function, run it and verify the result.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s code a prediction function that will perform the necessary cleaning and
    preprocessing, embedding, and clustering steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s run this function with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is the actual predicted cluster:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Great work!
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we have explored the NLP task known as topic modeling. This
    task tries to come up with the topics associated with a given set of documents.
    Typically, no answer is given (no ground truth), and so this task is better solved
    via unsupervised learning. We attempted to solve this task with ABC’s *1 Million
    News* *Headlines* dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'We followed a three-step approach:'
  prefs: []
  type: TYPE_NORMAL
- en: Data processing and cleaning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Word embeddings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Clustering
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For the first step, we followed a typical pipeline for any NLP problem:'
  prefs: []
  type: TYPE_NORMAL
- en: Data cleaning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenizing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Removing stop words
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Stemming
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lemmatization
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For the second step, we applied Google’s word2vec to compute embeddings for
    each word, and each headline embedding was computed as the average of the embeddings
    of each one of its words.
  prefs: []
  type: TYPE_NORMAL
- en: In the third step, we explored the unsupervised learning algorithm K-means,
    selected four clusters, and computed its centroids. We generated the following
    topic clusters sports, global affairs, economy and crime, and current happenings.
  prefs: []
  type: TYPE_NORMAL
- en: With this information, we selected a random headline and accurately predicted
    the topic it was related to.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Unsupervised learning is a very wide topic and an active field of research.
    To learn more, a good starting point is its Wikipedia article: [https://en.wikipedia.org/wiki/Unsupervised_learning](https://en.wikipedia.org/wiki/Unsupervised_learning).'
  prefs: []
  type: TYPE_NORMAL
- en: Apart from the *1 Million News Headlines* dataset, another well-known reference
    dataset for topic modeling is the 20 Newsgroups dataset. I recommend working with
    the larger 6 Newsgroups choice as many Newsgroups had a lot of themes in common.
    More information can be found at [http://qwone.com/~jason/20Newsgroups/](http://qwone.com/~jason/20Newsgroups/).
  prefs: []
  type: TYPE_NORMAL
- en: One simplification we followed during the processing of embeddings is that the
    computation of our headline embedding was done by averaging each of the corresponding
    word embeddings. However, there are other approaches, known as document embeddings
    or sentence embeddings, with models such as **Doc2Vec** or **Sentence-BERT**,
    which can be more useful for other applications. An analysis comparing some of
    these approaches can be found at [https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/](https://www.analyticsvidhya.com/blog/2020/08/top-4-sentence-embedding-techniques-using-python/).
  prefs: []
  type: TYPE_NORMAL
- en: For a detailed explanation of how the K-means algorithm works, it is suggested
    you review [https://towardsdatascience.com/k-means-clustering-explained-4528df86a120](https://towardsdatascience.com/k-means-clustering-explained-4528df86a120).
  prefs: []
  type: TYPE_NORMAL
- en: When predicting the topic of a given headline, K-means is equivalent to another
    algorithm, called 1-nearest neighbor, which is the specific case of K-nearest
    neighbor with K = 1\. More information regarding this supervised learning algorithm
    can be found at [https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm](https://en.wikipedia.org/wiki/K-nearest_neighbors_algorithm).
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing sentiment in movie reviews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Sentiment analysis** is the use of several different techniques, including
    NLP, to identify the emotional state associated with human-generated information,
    text in our case. In this recipe, we are going to perform sentiment analysis on
    real-world movie reviews. We will classify the reviews into two sentiments: positive
    or negative.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, we will use several pre-trained models from GluonNLP Model
    Zoo, and apply its word embeddings to feed a classifier, which will output the
    predicted sentiment. We will apply this process to a new dataset: **IMDb** **Movie
    Reviews**.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As in previous chapters, in this recipe, we will be using a little bit of matrix
    operations and linear algebra, but it will not be hard at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we will be classifying text datasets. Therefore, we will revisit
    some concepts already seen in *Recipe 4*, *Understanding text datasets – loading,
    managing, and visualizing the Enron Email dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will be carrying out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the *IMDb Movie* *Reviews* dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combining TextCNNs with word embeddings
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introducing BERT
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Putting everything together
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exploring the IMDb Movie Reviews dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This dataset was collected in 2011 by researchers from Stanford University.
    It is split into a training set and a test set, with each of the sets having 25,000
    reviews. They included at most 30 reviews per movie. The sentiments are quite
    polar, with negative reviews with values between [1, 4] and positive reviews with
    values between [7, 10].
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – Histogram of movie reviews (imbalanced dataset)](img/B16591_06_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.14 – Histogram of movie reviews (imbalanced dataset)
  prefs: []
  type: TYPE_NORMAL
- en: For our analysis, we will simplify the sentiment values to a binary sentiment
    classification task. Therefore, negative reviews are assigned a 0 value and positive
    reviews a 1 value. As a by-product of this simplification, the dataset becomes
    balanced.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – Histogram of binarized movie reviews (balanced dataset)](img/B16591_06_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.15 – Histogram of binarized movie reviews (balanced dataset)
  prefs: []
  type: TYPE_NORMAL
- en: 'Another point to take into account, noted by the paper’s authors, is that as
    nuances in the language can contain information regarding the sentiment, the preprocessing
    of the reviews must not include usual stop words and stemming. We took this remark
    into account in our preprocessing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: The file can be accessed from [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/utils.py](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch06/utils.py).
  prefs: []
  type: TYPE_NORMAL
- en: This function is applied to all samples in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Combining TextCNNs with word embeddings
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'After processing the dataset, we are now ready to use it with any architecture
    of our choice. In the first recipe of this chapter, we showed how we can use CNNs
    with sequences. In order to provide language information, TextCNNs can use pre-trained
    token representations as input. For this recipe, we will use two word embeddings
    that will generate inputs for our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**word2vec**: These embeddings were introduced in *Recipe 4*, *Understanding
    text datasets – loading, managing, and visualizing the Enron Email dataset*, from
    [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029), *Working with MXNet and Visualizing
    Datasets: Gluon* *and DataLoader*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BERT**: A language model introduced by Google in 2018'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing BERT
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: RNNs and Transformers can work with large sequences of text. However, one of
    the largest disadvantages is that the data is processed in a single direction,
    from left to right. BERT provides a mechanism so that every word representation
    (token) can jointly use information from both directions, to the left and to the
    right of that specific word.
  prefs: []
  type: TYPE_NORMAL
- en: Another distinctive characteristic of BERT is that its attention mechanisms
    are solely based on self-attention layers; no cross-attention layers are used.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – BERT architecture: Comparison of BERT bidirectional approach
    with Transformers, such as GPT-1; left-to-right-only approach](img/B16591_06_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.16 – BERT architecture: Comparison of BERT bidirectional approach
    with Transformers, such as GPT-1; left-to-right-only approach'
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT was trained in an unsupervised manner, using two task objectives:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Masked language model**: The word under analysis is not shown, and therefore
    the model needs to understand its meaning from context alone'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Next-sentence prediction**: Given two sentences, this task predicts whether
    they have a connection (one could happen after the other in a longer text) or
    they are not related'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This training methodology, combined with the BERT architecture, proved to be
    very successful, beating state-of-the-art on 11 NLP tasks at the time the paper
    was published.
  prefs: []
  type: TYPE_NORMAL
- en: 'GluonNLP provides two pre-trained BERT models with the following features:'
  prefs: []
  type: TYPE_NORMAL
- en: '`BERT_12_768_12`: 12 layers, 768-dimensional embedding vectors, 12 self-attention
    heads. This model is known as **BERT base**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`BERT_24_1024_16`: 24 layers, 1,024-dimensional embedding vectors, 16 self-attention
    heads. This model is known as **BERT large**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our experiments, we will use the BERT base model, which can be easily loaded
    with the following code statement:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: With the preceding function, we can easily obtain a BERT model (`bert_model`)
    and its vocabulary (`vocab`), based on the architecture of 12 layers, 768-dimensional
    embedding vectors, 12 self-attention heads, and a dataset from English Wikipedia
    (`book_corpus_wiki_en_uncased`).
  prefs: []
  type: TYPE_NORMAL
- en: Putting everything together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s summarize all the steps we have seen so far.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our *IMDb Movie Reviews* dataset is composed of 25,000 training samples and
    25,000 test samples. For cost and compute optimization purposes, we work with
    the following datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training set**: 5,000 samples (from the original training set)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Validation set**: 1,250 samples (from the original training set; no overlap
    with our 5,000-sample training set)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test set**: 25,000 samples'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We use two embedding models as inputs to TextCNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '**word2vec**: Vectors with 300 components'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BERT base**: Vectors with 768 components'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TextCNN architecture is very similar for both approaches. The kernel sizes
    for TextCNN are 3, 4, and 5, that is, analyzing 3, 4, and 5 words at the same
    time, and the number of channels is equivalent to the embedding components. Furthermore,
    as we have a binary output (*negative* or *positive*), the classifier is a fully
    connected layer with one sigmoid output (the sigmoid activation function is included
    in the loss function due to computational optimizations).
  prefs: []
  type: TYPE_NORMAL
- en: 'For the training, equivalent parameters are used for both embedding models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizer**: Adam'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning** **rate**: 10-3'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function**: Sigmoid cross-entropy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epochs**: 5'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch** **size**: 4'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these parameters, we have the following results using a word2vec embedding
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Training loss/validation loss and accuracy using word2vec](img/B16591_06_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.17 – Training loss/validation loss and accuracy using word2vec
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6**.17*, we can see how the training improved with the epochs, yielding
    the best validation accuracy of 0.89 at the end of the training process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check the results qualitatively, selecting a movie review from our test
    set (unseen samples) and seeing the output of our sentiment analysis algorithm.
    The example movie review is from [https://ieee-dataport.org/open-access/imdb-movie-reviews-dataset](https://ieee-dataport.org/open-access/imdb-movie-reviews-dataset)
    with license CC BY 4.0):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We can format this input as embeddings expected by our TextCNN network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'We can pass it through our best model from training:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'These commands yield the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen in the preceding output, our algorithm has classified the review
    correctly as *positive*.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, for a more thorough and formal analysis, we can quantitatively process
    the full test set and compute the final accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'However, that number alone does not provide the full information about *Type
    I* and *Type II* errors. Therefore, we can also display the results as a confusion
    matrix (introduced in *Recipe 4*, *Evaluating classification models*, in [*Chapter
    4*](B16591_04.xhtml#_idTextAnchor075), *Solving* *Classification Problems*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Confusion matrix using word2vec](img/B16591_06_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.18 – Confusion matrix using word2vec
  prefs: []
  type: TYPE_NORMAL
- en: 'When following the same approach, but this time using our BERT model for embeddings,
    we have the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Training loss/validation loss and accuracy using BERT](img/B16591_06_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.19 – Training loss/validation loss and accuracy using BERT
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 6**.19*, we can see how the training improved with the epochs, yielding
    the best validation accuracy of 0.91 at the end of the training process. This
    figure is higher than with word2vec, as expected, as BERT is able to build more
    contextual relationships between the words in the reviews.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also pass the same review from the test set through our best model from
    training, using the same code statements as previously, obtaining the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This experiment produces the same result (positive review) as the previous experiment
    with *word2vec*.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the test set accuracy, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Compared to the word2vec result, BERT provides a 3% higher accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'The confusion matrix is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Confusion matrix using BERT](img/B16591_06_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.20 – Confusion matrix using BERT
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from these results, BERT clearly outperforms word2vec. Another
    important advantage to mention is that, as Transformers allow for better parallelization,
    the training process is also faster.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we tackled the sentiment analysis problem. We analyzed an architecture,
    TextCNN, to solve this task and explored how it can be applied to different word
    embedding models.
  prefs: []
  type: TYPE_NORMAL
- en: We explored a new dataset, *IMDb Movie Reviews*, and made adequate transformations
    so that we could work with the dataset in a constrained computation environment
    and simplify it to a binary classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'We introduced BERT, a new word embedding model introduced by Google in 2018,
    and compared it to a previously explored model, word2vec, understanding its differences,
    advantages, and constraints. We understood the two most important advantages of
    BERT: using bidirectional information for each word and masking each word in training
    so that the information about each word is purely based on its context.'
  prefs: []
  type: TYPE_NORMAL
- en: We ran experiments to compare these two word embedding approaches and saw that
    despite both approaches solving the problem rather well (the test accuracy for
    word2vec and BERT being 88% and 91%, respectively), BERT performed better.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Sentiment analysis is a well-researched task in the literature. To learn more,
    it is recommended to read this: [https://www.datarobot.com/blog/introduction-to-sentiment-analysis-what-is-sentiment-analysis/](https://www.datarobot.com/blog/introduction-to-sentiment-analysis-what-is-sentiment-analysis/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The paper introducing the *IMDb Movie Reviews* dataset, which also proposed
    a model for sentiment analysis, can be found here: *Learning Word Vectors for
    Sentiment* *Analysis*, [https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf](https://ai.stanford.edu/~ang/papers/acl11-WordVectorsSentimentAnalysis.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT was introduced in the paper [https://arxiv.org/pdf/1810.04805.pdf](https://arxiv.org/pdf/1810.04805.pdf).
    However, a more intuitive explanation can be found here: [https://huggingface.co/blog/bert-101](https://huggingface.co/blog/bert-101).
    Reading the preceding article is strongly encouraged due to its analysis of how
    data can embed biases in our models.'
  prefs: []
  type: TYPE_NORMAL
- en: BERT is very powerful and can be complemented with even better language models,
    such as RoBERTa (improved version) or DistilBERT (smaller model with similar performance),
    and lots of models fine-tuned for specific tasks. A list of the pre-trained models
    available in MXNet GluonNLP can be found at [https://nlp.gluon.ai/model_zoo/bert/index.html](https://nlp.gluon.ai/model_zoo/bert/index.html).
  prefs: []
  type: TYPE_NORMAL
- en: Translating text from Vietnamese to English
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Translating text automatically (machine translation) has been a very interesting
    and useful use case for NLP since its inception, as breaking language barriers
    has lots of applications, including chatbots and automated subtitles in multiple
    languages.
  prefs: []
  type: TYPE_NORMAL
- en: Before deep learning, machine translation was typically approached as a statistical
    problem. Even after deep learning, it was not until Google, in 2016, applied deep
    learning to machine translation that the area of **Neural Machine Translation**
    (**NMT**) was born. This model set the foundation for translating tasks now available
    in LLMs, such as **OpenAI GPT** and **Google Bard**.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will apply these techniques to translate sentences from Vietnamese
    to English, using pre-trained models from GluonNLP Model Zoo.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As in previous chapters, in this recipe, we will be using a little bit of matrix
    operations and linear algebra, but it will not be hard at all.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we will be classifying text datasets. Therefore, we will revisit
    some concepts already seen in *Recipe 4*, *Understanding text datasets – loading,
    managing, and visualizing the Enron Email dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will be carrying out the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the *IWSLT2015* dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluating machine translators (BLEU)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Introducing the GNMT model and exploring Transformers for this task
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Putting everything together
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s look at these steps in detail next.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the IWSLT2015 dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **International Workshop on Spoken Language Translation** (**IWSLT**) is
    a yearly scientific workshop focused on all forms of translation (not necessarily
    machine translation). They have generated several very important datasets and
    benchmarks that have helped the field of machine translation evolve. In 2015,
    an English-Vietnamese dataset was published, composed of a training set of 130,000+
    sentence pairs and validation/test sets with 1,000+ sentence pairs. This dataset
    is publicly available with MXNet GluonNLP and can be easily retrieved, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'This version of the dataset provides the following data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The preprocessing is similar to previous pipelines we have already seen, and
    includes the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Sentence clipping (to define maximum values)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Tokenizing
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adding **End-of-Sentence** (**EOS**) tokens to the source sentence (Vietnamese)
    and **Beginning-of-Sentence** (**BOS**) and EOS tokens to the target sentence
    (English)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Furthermore, to optimize training, a bucketing process is applied, where sentences
    are grouped by similar length:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Fixed bucket sampler](img/B16591_06_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.21 – Fixed bucket sampler
  prefs: []
  type: TYPE_NORMAL
- en: 'The example in *Figure 6**.21* shows this strategy with 10 buckets, yielding
    a minimal amount of padding (required so that all sentences in 1 batch can be
    processed in parallel). The size of the buckets is also exponentially increased.
    Use MXNet GluonNLP, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: In the preceding example, the size (width) of each bucket is augmented by 20%
    (1.2 increments).
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating machine translators (BLEU)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Evaluating how successful machine translation systems are is very difficult.
    For example, using a single number to measure the quality of a translation is
    inherently subjective. For our use case, we will work with a widely used metric
    called **BiLingual Evaluation** **Understudy** (**BLEU**).
  prefs: []
  type: TYPE_NORMAL
- en: With BLEU, several reference translations are provided, and it tries to measure
    how close the automated translation is to its reference translations. To do so,
    it compares the different N-grams (size 1 to 4) of the automated translation to
    the N-grams of the reference translations.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.22 – BLEU metric](img/B16591_06_22.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.22 – BLEU metric
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen in *Figure 6**.22*, BLEU tries to minimize the subjectivity associated
    with translations.
  prefs: []
  type: TYPE_NORMAL
- en: Another metric is **Perplexity**, which defines approximately how “surprised”
    the model is to see a translated word. When the model is not surprised, it means
    it is performing well; therefore, for Perplexity, a lower value is better. Computing
    Perplexity is much faster than BLEU, and so it is used more as a checking metric
    during per-batch computations in training, leaving BLEU for per-epoch computations.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the GNMT model and exploring Transformers for this task
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As mentioned, the largest improvement in the field of machine translation was
    introduced by Google in 2016 with their **Google Neural Machine Translator** (**GNMT**)
    model ([https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html)).
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.23 – GNMT architecture](img/B16591_06_23.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.23 – GNMT architecture
  prefs: []
  type: TYPE_NORMAL
- en: GNMT is a pioneer of transformers and makes use of attention with an encoder-decoder
    architecture as well. The encoder and the decoder are LSTM RNNs, with eight layers
    in the encoder and another eight layers in the decoder. The attention mechanism
    implemented in the model is a cross-attention layer.
  prefs: []
  type: TYPE_NORMAL
- en: At the end of the model, a beam-search sampler is chained to generate new translations
    to maximize the trained conditional probability of the translations. As in the
    paper, in our implementation, the scoring function includes a length penalty so
    that all words in the translation are covered.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll compare GNMT with Transformers for our use case of Vietnamese-to-English
    machine translation. For our application, these are the most important parameters
    for each model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**GNMT**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of layers for the encoder: 2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of layers for the decoder: 2'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of units: 512'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of layers for the encoder: 4'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of layers for the decoder: 4'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of units: 512'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next sections, we will compare both architectures for the same translation
    task.
  prefs: []
  type: TYPE_NORMAL
- en: Putting everything together
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s summarize all the steps we have seen so far.
  prefs: []
  type: TYPE_NORMAL
- en: Our *IWSLT2015* Vietnamese-to-English dataset is composed of 133,000+ training
    samples and 1,000+ validation/test samples. We’ll work with the complete datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll use two models for our machine translation use case:'
  prefs: []
  type: TYPE_NORMAL
- en: GNMT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For the training, equivalent parameters are used for both architectures:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizer**: Adam'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate**: 10-3, with a learning rate schedule of a step decay, halving
    the learning rate every epoch after half training'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss function**: Masked softmax cross-entropy, similar to the cross-entropy
    loss functions we have already explored with the added feature that when predictions
    are longer than their valid length, the redundant words are masked out'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epochs**: 12'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch** **size**: 128'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these parameters, we have the following evolution in the training using
    the GNMT model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.24 – GNMT training evolution (training loss and validation loss,
    Perplexity, and BLEU)](img/B16591_06_24.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.24 – GNMT training evolution (training loss and validation loss, Perplexity,
    and BLEU)
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied
    by 100) obtained in the test set are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Current state-of-the-art models can yield above 30 points in their BLEU score,
    but this score is certainly very high.
  prefs: []
  type: TYPE_NORMAL
- en: 'Qualitatively, we can also check how well our model is performing with a sentence
    example. In our case, we chose `I like to read books`, and this can be verified
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'These code statements will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen from the results, the text has been correctly translated from
    Vietnamese to English.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are going to repeat the same exercises with our Transformer model.
    With the parameters defined previously for its training, we have the following
    evolution in the training:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Transformer training evolution (training loss and validation
    loss, Perplexity, and BLEU)](img/B16591_06_25.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.25 – Transformer training evolution (training loss and validation loss,
    Perplexity, and BLEU)
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied
    by 100) obtained in the test set are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, the Transformer architecture yields around ~0.015 higher BLEU
    score points.
  prefs: []
  type: TYPE_NORMAL
- en: 'As done for GNMT, we can also check how well our model is performing qualitatively
    with the same sentence example and code. The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: As can be seen from the results, the text has been correctly translated from
    Vietnamese to English.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we solved one of the most useful tasks in NLP, machine translation.
    We introduced a new architecture, GNMT, the precursor of the Transformer, and
    compared both models.
  prefs: []
  type: TYPE_NORMAL
- en: We explored a new dataset, *IWSLT2015*, which, among other language pairs, supports
    translations between Vietnamese and English. We introduced the Perplexity and
    BLEU metrics, which are widely used to evaluate translation models.
  prefs: []
  type: TYPE_NORMAL
- en: We ran experiments to compare these two models and saw that, despite both approaches
    solving the problem rather well (the BLEU scores in the test set for GNMT and
    the Transformer were 23.15 and 24.34, respectively), the Transformer performed
    better.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Machine translation is a difficult problem to tackle. Two very good official
    guides from MXNet GluonNLP where this problem is solved are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Official machine translation tutorials of MXNet** **GluonNLP:** [https://nlp.gluon.ai/examples/machine_translation/index.html](https://nlp.gluon.ai/examples/machine_translation/index.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**AMLC19-GluonNLP:** [https://github.com/eric-haibin-lin/AMLC19-GluonNLP](https://github.com/eric-haibin-lin/AMLC19-GluonNLP)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This recipe used code from the previous references. I would like to kindly thank
    the contributors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The IWSLT conference takes place every year. For more info, please visit their
    official site: [https://iwslt.org/](https://iwslt.org/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We introduced two new metrics for translation problems, Perplexity and BLEU.
    Work is actively being carried out to improve these metrics, with new metrics
    being developed recently, such as **SacreBLEU**. Some references that tackle this
    very important topic are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Perplexity**: [http://blog.echen.me/2021/12/23/a-laymans-introduction-to-perplexity-in-nlp/](http://blog.echen.me/2021/12/23/a-laymans-introduction-to-perplexity-in-nlp/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BLEU**: [https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1](https://towardsdatascience.com/bleu-bilingual-evaluation-understudy-2b4eab9bcfd1)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Improving BLEU with** **SacreBLEU**: [https://aclanthology.org/W18-6319.pdf](https://aclanthology.org/W18-6319.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We also discussed GNMT for the first time, which was one of the first real-world
    systems that used deep learning for translation (NMT), developed by Google in
    2016\. The blog post where this was announced is worth reading: [https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html](https://ai.googleblog.com/2016/09/a-neural-network-for-machine.html).'
  prefs: []
  type: TYPE_NORMAL
- en: There are many translation models that have used the *IWSLT2015* dataset. The
    results can be found at [https://paperswithcode.com/sota/machine-translation-on-iwslt2015-english-1](https://paperswithcode.com/sota/machine-translation-on-iwslt2015-english-1).
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, in this recipe, we analyzed language-to-language translation, which
    has been the de facto approach of the industry for a long time, using English
    as a bridge language for multilingual translation. This is an active area of research,
    and recently, Meta, formerly known as Facebook, has developed the **No Language
    Left Behind** (**NLLB-200**) model. More information about this breakthrough can
    be found at [https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/](https://ai.facebook.com/blog/nllb-200-high-quality-machine-translation/).
  prefs: []
  type: TYPE_NORMAL
