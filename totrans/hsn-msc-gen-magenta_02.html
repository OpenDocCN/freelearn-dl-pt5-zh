<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Introduction to Magenta and Generative Art</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, you'll learn the basics of generative music and what already exists. You'll learn about the new techniques of artwork generation, such as machine learning, and how those techniques can be applied to produce music and art. Google's Magenta open source research platform will be introduced, along with Google's open source machine learning platform TensorFlow, along with an overview of its different parts and the installation of the required software for this book. We'll finish the installation by generating a simple MIDI file on the command line.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>Overview of generative artwork</li>
<li>New techniques with machine learning</li>
<li>Magenta and TensorFlow in music generation</li>
<li>Installing Magenta</li>
<li>Installing the music software and synthesizers</li>
<li>Installing the code editing software</li>
<li>Generating a basic MIDI file</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll use the following tools:</p>
<ul>
<li><strong>Python</strong>, <strong>Conda</strong>, and <strong>pip</strong>, to install and execute the Magenta environment</li>
<li><strong>Magenta</strong>, to test our setup by performing music generation<strong><br/></strong></li>
<li><strong>Magenta GPU</strong> (<strong>optional</strong>), CUDA drivers, and cuDNN drivers, to make Magenta run on the GPU</li>
<li><strong>FluidSynth</strong>, to listen to the generated music sample using a software synthesizer</li>
<li>Other optional software we might use throughout this book, such as <strong>Audacity</strong> for audio editing, <strong>MuseScore</strong> for sheet music editing, and <strong>Jupyter Notebook</strong> for code editing.</li>
</ul>
<p>It is recommended that you follow this book's source code when you read the chapters in this book. The source code also provides useful scripts and tips. Follow these steps to check out the code in your user directory (you can use another location if you want):</p>
<ol>
<li>First, you need to install Git, which can be installed on any platform by downloading and executing the installer at <a href="https://git-scm.com/downloads">git-scm.com/downloads</a>. Then, follow the prompts and make sure you add the program to your PATH so that it is available on the command line.</li>
<li>Then, clone the source code repository by opening a new Terminal and executing the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>&gt; git clone https://github.com/PacktPublishing/hands-on-music-generation-with-magenta</strong><br/><strong>&gt; cd hands-on-music-generation-with-magenta</strong></pre>
<p>Each chapter has its own folder; <kbd>Chapter01</kbd>, <kbd>Chapter02</kbd>, and so on. For example, the code for this chapter is located at <a href="https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter01">https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter01</a>. The examples and code snippets will be located in this chapter's folder. For this chapter, you should open <kbd>cd Chapter01</kbd> before you start.</p>
<div class="packt_tip">We won't be using a lot of Git commands except <kbd>git clone</kbd>, which duplicates a code repository to your machine, but if you are unfamiliar with Git and want to learn more, a good place to start is the excellent Git Book (<a href="https://git-scm.com/book/en/v2">git-scm.com/book/en/v2</a>), which is available in multiple languages.</div>
<div>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2O847tW">http://bit.ly/2O847tW</a></p>
</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Overview of generative art</h1>
                </header>
            
            <article>
                
<p>The term <strong>generative art</strong> has been coined with the advent of the computer, and since the very beginning of computer science, artists and scientists used technology as a tool to produce art. Interestingly, generative art predates computers, because generative systems can be derived by hand.</p>
<p>In this section, we'll provide an overview of generative music by showing you interesting examples from art history going back to the 18th century. This will help you understand the different types of generative music by looking at specific examples and prepare the groundwork for later chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Pen and paper generative music</h1>
                </header>
            
            <article>
                
<p class="mce-root">There's a lot of examples of generative art in the history of mankind. A popular example dates back to the 18th century, where a game called Musikalisches Würfelspiel (German for <em>musical dice game</em>) grew popular in Europe. The concept of the game was attributed to Mozart by Nikolaus Simrock in 1792, though it was never confirmed to be his creation.</p>
<p class="mce-root">The players of the game throw a dice and from the result, select one of the predefined 272 musical measures from it. Throwing the dice over and over again allows the players to compose a full minute (the musical genre that is generated by the game) that respects the rules of the genre because it was composed in such a way that the possible arrangements sound pretty.</p>
<p>In the following table and the image that follows, a small part of a musical dice game can be seen. In the table, the y-axis represents the dice throw outcome while the x-axis represents the measure of the score you are currently generating. The players will throw two dices 16 times:</p>
<ol>
<li>On the first throw of two dices, we read the first column. A total of two will output the measure 96 (first row), a total of two will output the measure 32 (second row), and so on.</li>
</ol>
<ol start="2">
<li>On the second throw of two dices, we read the second column. A total of two will output the measure 22 (first row), a total of three will output the measure 6 (second row), and so on.</li>
</ol>
<p class="mce-root">After 16 throws, the game will have output 16 measures for the index:</p>
<table border="1" style="border-collapse: collapse;width: 100%">
<tbody>
<tr>
<th class="CDPAlignCenter CDPAlign"/>
<th class="CDPAlignCenter CDPAlign">1</th>
<th class="CDPAlignCenter CDPAlign">2</th>
<th class="CDPAlignCenter CDPAlign">3</th>
<th class="CDPAlignCenter CDPAlign">4</th>
<th class="CDPAlignCenter CDPAlign">5</th>
<th class="CDPAlignCenter CDPAlign">6</th>
<th class="CDPAlignCenter CDPAlign">7</th>
<th class="CDPAlignCenter CDPAlign">8</th>
<th class="CDPAlignCenter CDPAlign">9</th>
<th class="CDPAlignCenter CDPAlign">10</th>
<th class="CDPAlignCenter CDPAlign">11</th>
<th class="CDPAlignCenter CDPAlign">12</th>
<th class="CDPAlignCenter CDPAlign">13</th>
<th class="CDPAlignCenter CDPAlign">14</th>
<th class="CDPAlignCenter CDPAlign">15</th>
<th class="CDPAlignCenter CDPAlign">16</th>
</tr>
<tr>
<th class="CDPAlignCenter CDPAlign">2</th>
<td class="CDPAlignCenter CDPAlign">96</td>
<td class="CDPAlignCenter CDPAlign">22</td>
<td class="CDPAlignCenter CDPAlign">141</td>
<td class="CDPAlignCenter CDPAlign">41</td>
<td class="CDPAlignCenter CDPAlign">105</td>
<td class="CDPAlignCenter CDPAlign">122</td>
<td class="CDPAlignCenter CDPAlign">11</td>
<td class="CDPAlignCenter CDPAlign">30</td>
<td class="CDPAlignCenter CDPAlign">70</td>
<td class="CDPAlignCenter CDPAlign">121</td>
<td class="CDPAlignCenter CDPAlign">26</td>
<td class="CDPAlignCenter CDPAlign">9</td>
<td class="CDPAlignCenter CDPAlign">112</td>
<td class="CDPAlignCenter CDPAlign">49</td>
<td class="CDPAlignCenter CDPAlign">109</td>
<td class="CDPAlignCenter CDPAlign">14</td>
</tr>
<tr>
<th class="CDPAlignCenter CDPAlign">3</th>
<td class="CDPAlignCenter CDPAlign">32</td>
<td class="CDPAlignCenter CDPAlign">6</td>
<td class="CDPAlignCenter CDPAlign">128</td>
<td class="CDPAlignCenter CDPAlign">63</td>
<td class="CDPAlignCenter CDPAlign">146</td>
<td class="CDPAlignCenter CDPAlign">46</td>
<td class="CDPAlignCenter CDPAlign">134</td>
<td class="CDPAlignCenter CDPAlign">81</td>
<td class="CDPAlignCenter CDPAlign">117</td>
<td class="CDPAlignCenter CDPAlign">39</td>
<td class="CDPAlignCenter CDPAlign">126</td>
<td class="CDPAlignCenter CDPAlign">56</td>
<td class="CDPAlignCenter CDPAlign">174</td>
<td class="CDPAlignCenter CDPAlign">18</td>
<td class="CDPAlignCenter CDPAlign">116</td>
<td class="CDPAlignCenter CDPAlign">83</td>
</tr>
<tr>
<th class="CDPAlignCenter CDPAlign">4</th>
<td class="CDPAlignCenter CDPAlign">69</td>
<td class="CDPAlignCenter CDPAlign">95</td>
<td class="CDPAlignCenter CDPAlign">158</td>
<td class="CDPAlignCenter CDPAlign">13</td>
<td class="CDPAlignCenter CDPAlign">153</td>
<td class="CDPAlignCenter CDPAlign">55</td>
<td class="CDPAlignCenter CDPAlign">110</td>
<td class="CDPAlignCenter CDPAlign">24</td>
<td class="CDPAlignCenter CDPAlign">66</td>
<td class="CDPAlignCenter CDPAlign">139</td>
<td class="CDPAlignCenter CDPAlign">15</td>
<td class="CDPAlignCenter CDPAlign">132</td>
<td class="CDPAlignCenter CDPAlign">73</td>
<td class="CDPAlignCenter CDPAlign">58</td>
<td class="CDPAlignCenter CDPAlign">145</td>
<td class="CDPAlignCenter CDPAlign">79</td>
</tr>
<tr>
<th class="CDPAlignCenter CDPAlign">5</th>
<td class="CDPAlignCenter CDPAlign">40</td>
<td class="CDPAlignCenter CDPAlign">17</td>
<td class="CDPAlignCenter CDPAlign">113</td>
<td class="CDPAlignCenter CDPAlign">85</td>
<td class="CDPAlignCenter CDPAlign">161</td>
<td class="CDPAlignCenter CDPAlign">2</td>
<td class="CDPAlignCenter CDPAlign">159</td>
<td class="CDPAlignCenter CDPAlign">100</td>
<td class="CDPAlignCenter CDPAlign">90</td>
<td class="CDPAlignCenter CDPAlign">176</td>
<td class="CDPAlignCenter CDPAlign">7</td>
<td class="CDPAlignCenter CDPAlign">34</td>
<td class="CDPAlignCenter CDPAlign">67</td>
<td class="CDPAlignCenter CDPAlign">160</td>
<td class="CDPAlignCenter CDPAlign">52</td>
<td class="CDPAlignCenter CDPAlign">170</td>
</tr>
<tr>
<th class="CDPAlignCenter CDPAlign">6</th>
<td class="CDPAlignCenter CDPAlign">148</td>
<td class="CDPAlignCenter CDPAlign">74</td>
<td class="CDPAlignCenter CDPAlign">163</td>
<td class="CDPAlignCenter CDPAlign">45</td>
<td class="CDPAlignCenter CDPAlign">80</td>
<td class="CDPAlignCenter CDPAlign">97</td>
<td class="CDPAlignCenter CDPAlign">36</td>
<td class="CDPAlignCenter CDPAlign">107</td>
<td class="CDPAlignCenter CDPAlign">25</td>
<td class="CDPAlignCenter CDPAlign">143</td>
<td class="CDPAlignCenter CDPAlign">64</td>
<td class="CDPAlignCenter CDPAlign">125</td>
<td class="CDPAlignCenter CDPAlign">76</td>
<td class="CDPAlignCenter CDPAlign">136</td>
<td class="CDPAlignCenter CDPAlign">1</td>
<td class="CDPAlignCenter CDPAlign">93</td>
</tr>
<tr>
<th class="CDPAlignCenter CDPAlign">7</th>
<td class="CDPAlignCenter CDPAlign">104</td>
<td class="CDPAlignCenter CDPAlign">157</td>
<td class="CDPAlignCenter CDPAlign">27</td>
<td class="CDPAlignCenter CDPAlign">167</td>
<td class="CDPAlignCenter CDPAlign">154</td>
<td class="CDPAlignCenter CDPAlign">68</td>
<td class="CDPAlignCenter CDPAlign">118</td>
<td class="CDPAlignCenter CDPAlign">91</td>
<td class="CDPAlignCenter CDPAlign">138</td>
<td class="CDPAlignCenter CDPAlign">71</td>
<td class="CDPAlignCenter CDPAlign">150</td>
<td class="CDPAlignCenter CDPAlign">29</td>
<td class="CDPAlignCenter CDPAlign">101</td>
<td class="CDPAlignCenter CDPAlign">162</td>
<td class="CDPAlignCenter CDPAlign">23</td>
<td class="CDPAlignCenter CDPAlign">151</td>
</tr>
<tr>
<th class="CDPAlignCenter CDPAlign">8</th>
<td class="CDPAlignCenter CDPAlign">152</td>
<td class="CDPAlignCenter CDPAlign">60</td>
<td class="CDPAlignCenter CDPAlign">171</td>
<td class="CDPAlignCenter CDPAlign">53</td>
<td class="CDPAlignCenter CDPAlign">99</td>
<td class="CDPAlignCenter CDPAlign">133</td>
<td class="CDPAlignCenter CDPAlign">21</td>
<td class="CDPAlignCenter CDPAlign">127</td>
<td class="CDPAlignCenter CDPAlign">16</td>
<td class="CDPAlignCenter CDPAlign">155</td>
<td class="CDPAlignCenter CDPAlign">57</td>
<td class="CDPAlignCenter CDPAlign">175</td>
<td class="CDPAlignCenter CDPAlign">43</td>
<td class="CDPAlignCenter CDPAlign">168</td>
<td class="CDPAlignCenter CDPAlign">89</td>
<td class="CDPAlignCenter CDPAlign">172</td>
</tr>
<tr>
<th class="CDPAlignCenter CDPAlign">9</th>
<td class="CDPAlignCenter CDPAlign">119</td>
<td class="CDPAlignCenter CDPAlign">84</td>
<td class="CDPAlignCenter CDPAlign">114</td>
<td class="CDPAlignCenter CDPAlign">50</td>
<td class="CDPAlignCenter CDPAlign">140</td>
<td class="CDPAlignCenter CDPAlign">86</td>
<td class="CDPAlignCenter CDPAlign">169</td>
<td class="CDPAlignCenter CDPAlign">94</td>
<td class="CDPAlignCenter CDPAlign">120</td>
<td class="CDPAlignCenter CDPAlign">88</td>
<td class="CDPAlignCenter CDPAlign">48</td>
<td class="CDPAlignCenter CDPAlign">166</td>
<td class="CDPAlignCenter CDPAlign">51</td>
<td class="CDPAlignCenter CDPAlign">115</td>
<td class="CDPAlignCenter CDPAlign">72</td>
<td class="CDPAlignCenter CDPAlign">111</td>
</tr>
<tr>
<th class="CDPAlignCenter CDPAlign">10</th>
<td class="CDPAlignCenter CDPAlign">98</td>
<td class="CDPAlignCenter CDPAlign">142</td>
<td class="CDPAlignCenter CDPAlign">42</td>
<td class="CDPAlignCenter CDPAlign">156</td>
<td class="CDPAlignCenter CDPAlign">75</td>
<td class="CDPAlignCenter CDPAlign">129</td>
<td class="CDPAlignCenter CDPAlign">62</td>
<td class="CDPAlignCenter CDPAlign">123</td>
<td class="CDPAlignCenter CDPAlign">65</td>
<td class="CDPAlignCenter CDPAlign">77</td>
<td class="CDPAlignCenter CDPAlign">19</td>
<td class="CDPAlignCenter CDPAlign">82</td>
<td class="CDPAlignCenter CDPAlign">137</td>
<td class="CDPAlignCenter CDPAlign">38</td>
<td class="CDPAlignCenter CDPAlign">149</td>
<td class="CDPAlignCenter CDPAlign">8</td>
</tr>
<tr>
<th class="CDPAlignCenter CDPAlign">11</th>
<td class="CDPAlignCenter CDPAlign">3</td>
<td class="CDPAlignCenter CDPAlign">87</td>
<td class="CDPAlignCenter CDPAlign">165</td>
<td class="CDPAlignCenter CDPAlign">61</td>
<td class="CDPAlignCenter CDPAlign">135</td>
<td class="CDPAlignCenter CDPAlign">47</td>
<td class="CDPAlignCenter CDPAlign">147</td>
<td class="CDPAlignCenter CDPAlign">33</td>
<td class="CDPAlignCenter CDPAlign">102</td>
<td class="CDPAlignCenter CDPAlign">4</td>
<td class="CDPAlignCenter CDPAlign">31</td>
<td class="CDPAlignCenter CDPAlign">164</td>
<td class="CDPAlignCenter CDPAlign">144</td>
<td class="CDPAlignCenter CDPAlign">59</td>
<td class="CDPAlignCenter CDPAlign">173</td>
<td class="CDPAlignCenter CDPAlign">78</td>
</tr>
<tr>
<th class="CDPAlignCenter CDPAlign">12</th>
<td class="CDPAlignCenter CDPAlign">54</td>
<td class="CDPAlignCenter CDPAlign">130</td>
<td class="CDPAlignCenter CDPAlign">10</td>
<td class="CDPAlignCenter CDPAlign">103</td>
<td class="CDPAlignCenter CDPAlign">28</td>
<td class="CDPAlignCenter CDPAlign">37</td>
<td class="CDPAlignCenter CDPAlign">106</td>
<td class="CDPAlignCenter CDPAlign">5</td>
<td class="CDPAlignCenter CDPAlign">35</td>
<td class="CDPAlignCenter CDPAlign">20</td>
<td class="CDPAlignCenter CDPAlign">108</td>
<td class="CDPAlignCenter CDPAlign">92</td>
<td class="CDPAlignCenter CDPAlign">12</td>
<td class="CDPAlignCenter CDPAlign">124</td>
<td class="CDPAlignCenter CDPAlign">44</td>
<td class="CDPAlignCenter CDPAlign">131</td>
</tr>
</tbody>
</table>
<p> </p>
<p class="mce-root">The preceding table shows a small part of the whole score, with each measure annotated with an index. For each of the generated 16 indexes, we take the corresponding measure in order, which constitutes our minuet (the minuet is the style that's generated by this game <span>– </span>basically, it's a music score with specific rules).</p>
<div class="packt_figref CDPAlignCenter CDPAlign"/>
<p>There are different types of generative properties:</p>
<ul>
<li><strong>Chance or randomness</strong>, which the dice game is a good example of, where the outcome of the generated art is partially or totally defined by chance. Interestingly, adding randomness to a process in art is often seen as <em>humanizing</em> the process, since an underlying rigid algorithm might generate something that sounds <em>artificial</em>.</li>
<li class="mce-root"><strong>Algorithmic generation</strong> (or rule-based generation), where the rules of the generation will define its outcome. Good examples of such generation include a cellular automaton, such as the popular Conway's Game of Life, a game where a grid of cells changes each iteration according to predefined rules: each cell might be on or off, and the neighboring cells are updated as a function of the grid's state and rules. The result of such generation is purely deterministic; it has no randomness or probability involved.</li>
<li><strong>Stochastic-based generation</strong>, where sequences are derived from the probability of elements. Examples of this include Markov chains, a stochastic model in which for each element of a sequence, the resulting probability of the said event is defined only on the present state of the system. Another good example of stochastic-based generation is machine learning generation, which we'll be looking at throughout this book.</li>
</ul>
<p>We will use a simple definition of generative art for this book:</p>
<div class="packt_quote"><q>"Generative art is an artwork partially or completely created by an autonomous system"</q>.</div>
<p>By now, you should understand that we don't actually need a computer to generate art since the rules of a system can be derived by hand. But using a computer makes it possible to define complex rules and handle tons of data, as we'll see in the following chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computerized generative music</h1>
                </header>
            
            <article>
                
<p class="mce-root">The first instance of generative art by computer dates back to 1957, where <span class="js-about-item-abstr">Markov chains were used to generate a</span> score on an electronic computer, the ILLIAC I, by composers Lejaren Hiller and Leonard Issacson. Their paper, <em>Musical Composition with a High-Speed Digital Computer</em>, describes the techniques that were used in composing the music. The composition, titled <em>Illac Suite</em>, consists of four movements, each exploring a particular technique of music generation, from a rule-based generation of <em>cantus firmi</em> to stochastic generation with <span class="js-about-item-abstr">Markov chain</span>.</p>
<p class="mce-root">Many famous examples of generative composition have followed since, such as Xenakis's <em>Atrées</em> in 1962, which explored the idea of stochastic composition; Ebcioglo's composition software named CHORAL, which contained handcrafted rules; and David Cope's software called EMI, which extended the concept to be able to learn from a corpus of scores.</p>
<p class="mce-root"/>
<p class="mce-root">As of today, generative music is everywhere. A lot of tools allow musicians to compose original music based on the generative techniques we described previously. A whole genre and musical community, called <strong>algorave</strong>, originated from those techniques. Stemming from the underground electronic music scene, musicians use generative algorithms and software to produce live dance music on stage, hence the name of the genre. Software such as <em>TidalCycles</em> and <em>Orca</em> allow the musician to define rules on the fly and let the system generate the music autonomously.</p>
<p>Looking back on those techniques, stochastic models such as Markov chains have been widely used in generative music. It stems from the fact that they are conceptually simple and easy to represent since the model is a transition probability table and can learn from a few examples. The problem with Markov models is that representing a long-term temporal structure is hard since most models will only consider <em>n</em> previous states, where <em>n</em> is small, to define the resulting probability. Let's take a look at what other types of models can be used to generate music.</p>
<div class="mce-root packt_infobox">In a 2012 paper titled <em>Ten Questions Concerning Generative Computer Art</em>, the author talks about the possibility of machine creation, the formalization of human aesthetics, and randomness. More importantly, it defines the limitations of such systems. What can a generative system produce? Can machines only do what they are instructed to?</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">New techniques with machine learning</h1>
                </header>
            
            <article>
                
<p>Machine learning is important for computer science because it allows complex functions to be modeled without them being explicitly written. Those models are automatically learned from examples, instead of being manually defined. This has a huge implication for arts in general since explicitly writing the rules of a painting or a musical score is inherently difficult.</p>
<p>In recent years, the advent of deep learning has propelled machine learning to new heights in terms of efficiency. Deep learning is especially important for our use case of music generation since using deep learning techniques doesn't require a preprocessing step of <em>feature extraction</em>, which is necessary for classical machine learning and hard to do on raw data such as image, text, and <span>– </span>you guessed it <span>–</span> audio. In other words, traditional machine learning algorithms do not work well for music generation. Therefore, all the networks in this book will be deep neural networks.</p>
<p>In this section, we'll learn what advances in deep learning allow for music generation and introduce the concepts we'll be using throughout this book. We'll also look at the different types of musical representations for those algorithms, which is important as it will serve as the groundwork for this book for data in general.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Advances in deep learning</h1>
                </header>
            
            <article>
                
<p>We all know that deep learning has recently become a fast-growing domain in computer science. Not so long ago, no deep learning algorithms could outperform standard techniques. That was before 2012 when, for the first time, a deep learning algorithm, AlexNet, did better in an image classification competition by using a deep neural network trained on GPUs (see the <em>Further reading</em> section for the AlexNet paper, one of the most influential papers that was published in computer vision). Neural network techniques are more than 30 years old, but the recent reemergence can be explained by the availability of massive data, efficient computing power, and technical advances.</p>
<p>Most importantly, a deep learning technique is <em>general</em>, in the sense that, as opposed to the music generation techniques we've specified previously, a machine learning system is agnostic and can learn from an arbitrary corpus of music. The same system can be used in multiple musical genres, as we'll see during this book when we train an existing model on jazz music in <a href="1ca56e24-b4d2-40de-b4cf-ae6bbb3c0eef.xhtml">Chapter 6</a>, <em>Data Preparation for Training</em>.</p>
<p>Many techniques in deep learning were discovered a long time ago but only find meaningful usage today. Of the technical advances in the field that concern music generation, those are present in Magenta and will be explained later in this book:</p>
<ul>
<li><strong>Recurrent Neural Networks</strong> (<strong>RNNs</strong>) are interesting for music generation because they allow us to operate over sequences of vectors for the input and output. When using classic neural networks or convolutional networks (which are used in image classification), you are limited to a fixed size input vector to produce a fixed size output vector, which would be very limiting for music processing, but works well for certain types of image processing. The other advantage of RNN is the possibility of producing a new state vector at each pass by combining a function with the previous state vector, which a powerful mean of describing complex behavior and long-term state. We'll be talking about RNNs in <a href="b60deee5-c58f-45eb-88a2-23718802e580.xhtml">Chapter 2</a>, <em>Generating Drum Sequences with Drums RNN</em>.</li>
<li><strong>Long Sho</strong><strong>rt-Term Memory</strong> (<strong>LSTM</strong>) is an RNN with slightly different properties. It solves the problem of vanishing gradients that is present in RNNs and makes it impossible for the network to learn long-term dependencies, even if it theoretically could. The approach of using LSTM in music generation has been presented by Douglas Eck and Jurgen Schmidhuber in 2002 in a paper called <em>Finding temporal structure in music: Blues improvisation with LSTM recurrent networks</em>. We'll be talking about LSTM in <a href="48023567-4100-492a-a28e-53b18a63e01e.xhtml">Chapter 3</a>, <em>Generating Polyphonic Melodies</em>.</li>
<li><strong>Variational autoencoders</strong> (<strong>VAEs</strong>) are analogous to classical autoencoders, in the sense that their architecture is similar, consisting of an encoder (for the input to a hidden layer), a decoder (for a hidden layer to the output), and a loss function, with the model learning to reconstruct the original input with specific constraints. The usage of VAE in generative models is recent but has shown interesting results. We'll be talking about VAE in <a href="838da33e-26a9-4701-bfd3-5014dfff4146.xhtml"/><a href="838da33e-26a9-4701-bfd3-5014dfff4146.xhtml">Chapter 4</a>, <em>Latent Space Interpolation with Music VAE</em>. </li>
<li><strong>Generative adversarial networks</strong> (<strong>GANs</strong>) are a class of machine learning systems where two neural networks compete with each other in a game: a generative network generates candidates while a discriminating network evaluates them. We'll be talking about GANs in <a href="feb070b7-92ac-4762-a4ac-7c1a797a47ef.xhtml">Chapter 5</a>, <em>Audio Generation with NSynth and GANSynth</em>.</li>
</ul>
<p>Recent deep learning advances have profoundly changed not only music generation but also genre classification, audio transcription, note detection, composition, and more. We won't be talking about these subjects here, but they all share common ground: musical representation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Representation in music processes</h1>
                </header>
            
            <article>
                
<p>These systems can work with different representations:</p>
<ul>
<li><strong>Symbolic representation</strong>, such as the <strong>MIDI</strong> (<strong>Musical Instrument Digital Interface</strong> (<strong>MIDI</strong>), describes the music using a notation containing the musical notes and timing, but not the sound or timbre of the actual sound. In general, sheet music is a good example of this. A symbolic representation of music has no sound by itself; it has to be played by instruments.</li>
<li><strong>Sub-symbolic representation</strong>, such as a raw audio waveform or a spectrogram, describes the actual sound of the music.</li>
</ul>
<p class="mce-root"/>
<p>Different processes will require a different representation. For example, most speech recognition and synthesis models work with spectrograms, while most of the examples we will see in this book uses MIDI to generate music scores. Processes that integrate both representations are rare, but an example of this could be a score transcription that takes an audio file and translate it into MIDI or other symbolic representations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Representing music with MIDI</h1>
                </header>
            
            <article>
                
<p>There are other symbolic representations than MIDI, such as MusicXML and AbcNotation, but MIDI is by far the most common representation. The MIDI specification also doubles down as a protocol since it is used to carry note messages that can be used in real-time performance as well as control messages.</p>
<p>Let's consider some parts of a MIDI message that will be useful for this book:</p>
<ul>
<li><strong>Channel [0-15]</strong>: This indicates the track that the message is sent on</li>
<li><strong>Note number [0-127]</strong>: This indicates the pitch of the note</li>
<li><strong>Velocity [0-127]</strong>: This indicates the volume of the note</li>
</ul>
<p>To represent a musical note in MIDI, you have to send two different message types with proper timing: a <kbd>Note On</kbd> event, followed by a <kbd>Note Off</kbd> event. This implicitly defines the <strong>length</strong> of the note, which is not present in the MIDI message. This is important because MIDI was defined with live performance in mind, so using two messages <span>–</span> one for a keypress and another for a key release <span>–</span> makes sense.</p>
<p>From a data perspective, we'll need either need to convert MIDI notes into a format that has the note length encoded in it or keep a note on and note off approach, depending on what we're trying to do. For each model in Magenta, we'll see how the MIDI notes are encoded.</p>
<p>The following image shows a MIDI representation of a generated drum file, shown as a plot of time and pitch. Each MIDI note is represented by a rectangle. Because of the nature of percussion data, all the notes have the same length ("note on" followed by "note off" messages), but in general, that could vary. A drum file, by essence, is polyphonic, meaning that multiple notes can be played at the same time. We'll be talking about monophony and polyphony in the upcoming chapters.</p>
<p class="mce-root"/>
<p>Note that the abscissa is expressed in seconds, but it is also common to note it with bars or measures. The MIDI channel is absent from this diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1cd47be5-1cd9-4af3-9f28-a1c24ddbef99.png"/></p>
<div class="packt_infobox packt_tip">The script for plotting a generated MIDI file can be found in the GitHub code for this chapter in the <kbd>Chapter01/provided</kbd> folder. The script is called <kbd>midi2plot.py</kbd>.</div>
<p>In the case of music generation, the majority of current deep learning systems use symbolic notation. This is also the case with Magenta. There are a couple of reasons for this:</p>
<ul>
<li>It is easier to represent the essence of music in terms of composition and harmony with symbolic data.</li>
<li>Processing those two types of representations by using a deep learning network is similar, so choosing between both boils down to whichever is faster and more convenient. A good example of this is that the WaveNet audio generation network also has a MIDI implementation, known as the MidiNet symbolic generation network.</li>
</ul>
<p>We'll see that the MIDI format is not directly used by Magenta, but converted into and from <kbd>NoteSequence</kbd>, a <strong>Protocol Buffers</strong> (<strong>Protobuf</strong>) implementation of the musical structure that is then used by TensorFlow. This is hidden from the end user since the input and output data is always MIDI. The <kbd>NoteSequence</kbd> implementation is useful because it implements a data format that can be used by the models for training. For example, instead of using two messages to define a note's length, a <kbd>Note</kbd> in a <kbd>NoteSequence</kbd> has a length attribute. We'll be explaining the <kbd>NoteSequence</kbd> implementation as we go along.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Representing music as a waveform</h1>
                </header>
            
            <article>
                
<p>An audio waveform is a graph displaying amplitude changes over time. Zoomed out, a waveform looks rather simple and smooth, but zoomed in, we can see tiny variations <span>– </span>it is those variations that represent the sound.</p>
<p>To illustrate how a waveform works, imagine a speaker cone that's is at rest when the amplitude is at 0. If the amplitude moves to a negative value of 1, for example, then the speaker moves backward a little bit, or forward in the case of a positive value. For each amplitude variation, the speaker will move, making the air move, thus making your eardrums move.</p>
<p>The bigger the amplitude is in the waveform, the more the speaker cone moves in terms of distance, and the louder the sound. This is expressed in <strong>decibel</strong> (<strong>dB</strong>), a measure of sound pressure.</p>
<p>The faster the movement, the higher the pitch. This is expressed in <strong>hertz</strong> (<strong>Hz</strong>).</p>
<p>In the following image, we can see the MIDI file from the previous section played by instruments to make a WAV recording. The instrument that's being used is a 1982 Roland TR-808 drum sample pack. You can visually match some instruments, such as double the Conga Mid (MIDI note 48) at around 4.5 seconds. In the upper right corner, you can see a zoom of the waveform at 100th of a second to show the actual amplitude change:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/fb612e63-ef48-4c37-aaae-935c764fc653.png"/></p>
<p class="mce-root"/>
<div class="packt_tip">The script for plotting a WAV file can be found in the GitHub code for this chapter in the <kbd>Chapter01/provided</kbd> folder. The script is called <kbd>wav2plot.py</kbd>.</div>
<p>In machine learning, using a raw audio waveform used to be uncommon as a data source since the computational load is bigger than other transformed representations, both in terms of memory and processing. But recent advances in the field, such as WaveNet models, makes it on par with other methods of representing audio, such as spectrograms, which were historically more popular for machine learning algorithms, especially for speech recognition and synthesis.</p>
<p>Bear in mind that training on audio is really cost-intensive because raw audio is a dense medium. Basically, a waveform is a digital recreation of a dynamic voltage over time. Simply put, a process called <strong>Pulse Code Modulation</strong> (<strong>PCM</strong>) assigns a bit value to each sample at the sampling rate you are running. The sampling rate for recording purposes is pretty standard: 44,100 Hz, which is called the Nyquist Frequency. But you don't always need a 44,100 Hz sample rate; for example, 16,000 Hz is more than enough to cover human speech frequencies. At that frequency, the first second of audio is represented by 16,000 samples.</p>
<p>If you want to know more about PCM, the sampling theory for audio, and the Nyquist Frequency, check out the <em>Further reading</em> section at the end of this chapter.</p>
<div class="packt_infobox">This frequency was chosen for a very specific purpose. Thanks to the Nyquist theorem, it allows us to recreate the original audio without a loss of sounds that humans can hear.<br/>
<br/>
The human ear can hear sounds up to 20,000 Hz, so you need 40,000 Hz to represent it in a waveform since you need a negative value and a positive value to make a sound (see the explanation at the beginning of this subsection). Then, you can add 4,100 Hz for rounding errors on very low and very high frequencies to make 44,100 Hz.<br/>
<br/>
This is a good example of a sampled (discrete) representation that can be reversed to its original continuous representation because the pitch spectrum the ear can hear is limited.</div>
<p>We'll look at audio representation in more detail in <a href="feb070b7-92ac-4762-a4ac-7c1a797a47ef.xhtml">Chapter 5</a>, <em>Audio Generation with NSynth and GANSynth</em>, since we are going to be using NSynth, a Wavenet model, to generate audio samples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Representing music with a spectrogram</h1>
                </header>
            
            <article>
                
<p>Historically, spectrograms have been a popular form of handling audio for machine learning, for two reasons <span>–</span> it is compact and extracting features from it is easier. To explain this, imagine the raw audio stream of the example from the previous section and cut it into chunks of 1/50th of a second (20 milliseconds) for processing. Now, you have chunks of 882 samples that are hard to represent; it is a mixed bag of amplitudes that don't really represent anything.</p>
<p>A spectrogram is the result of doing a Fourier transform on the audio stream. A Fourier transform will decompose a signal (a function of time) into its constituent frequencies. For an audio signal, this gives us the intensity of a frequency band, with a band being a small split of the whole spectrum, for example, 50 Hz. After applying a Fourier transform on our previous example and taking sample 1 of the 882 samples, we'll end up with the intensity for each frequency band:</p>
<ul>
<li><em>[0 Hz - 50 Hz]: a<sub>1</sub></em></li>
<li><em>[50 Hz - 100 Hz]: a<sub>2</sub></em></li>
<li><em>...</em></li>
<li><em>[22000 Hz - 22050 Hz:]: a<sub>n</sub></em></li>
</ul>
<p>You'll end up with intensity <em>[a<sub>1</sub>, a<sub>2</sub>, ..., a<sub>n</sub>]</em> for each band of 50 Hz up to 22,050, which is the y-axis, with an assigned color spectrum for smaller to bigger intensities. Repeating that for each 20 ms on the x-axis until the whole audio is covered gives you a spectrogram. What is interesting in a spectrogram is that you can actually see the content of the music. If a C major chord is played, you'll see C, E, and G emerge in the spectrogram at their corresponding frequency.</p>
<p class="mce-root"/>
<p>The following spectrogram has been generated from the waveform of the previous section. From this, you can clearly see the frequencies that are being played by the TR 808 from the given MIDI file. You should be able to visually match the waveform from the previous section with the spectrogram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f226d763-cc79-4e90-b897-f514da3c7ec8.png"/></p>
<div class="packt_tip">The script for plotting the spectrogram of a WAV file can be in the GitHub code for this chapter in the <kbd>Chapter01/provided</kbd> folder. The script is called <kbd>wav2spectrogram.py</kbd>.</div>
<p>Spectrograms are mainly used in speech recognition. They are also used in speech synthesis: first, a model is trained on spectrograms aligned with text, and from there, the model will be able to produce a spectrogram that corresponds to a given text. The Griffin-Lim algorithm is used to recover an audio signal from a spectrogram.</p>
<p>We won't be using spectrograms in this book, but knowing how they work and what they are used for is important since they are used in many applications.</p>
<div class="packt_infobox">Fun fact: musicians have been known to hide images in music that are visible when looking at the audio's spectrogram. A famous example is the Aphex Twin's <em>Windowlicker</em> album, where he embedded his grinning face on the second track.</div>
<p class="mce-root"/>
<p>So far, we have learned which deep learning technical advances are important in music generation and learned about music representation in those algorithms. These two topics are important because we'll be looking at them throughout this book.</p>
<p>In the next section, we'll introduce Magenta, where you'll see much of this section's content come into play.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Google's Magenta and TensorFlow in music generation</h1>
                </header>
            
            <article>
                
<p>Since its launch, TensorFlow has been important for the data scientist community for being <em>An Open Source Machine Learning Framework for Everyone</em>. Magenta, which is based on TensorFlow, can be seen the same way: even if it's using state of the art machine learning techniques, it can still be used by anyone. Musicians and computer scientists alike can install it and generate new music in no time.</p>
<p>In this section, we'll look at the content of Magenta by introducing what it can and cannot do and refer to the chapters that explain the content in more depth.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a music generation system</h1>
                </header>
            
            <article>
                
<p>Magenta is a framework for art generation, but also for attention, storytelling, and the evaluation of generative music. As the book advances, we'll come to see and understand how those elements are crucial for pleasing music generation.</p>
<div class="packt_infobox">Evaluating and interpreting generative models is inherently hard, especially for audio. A common criterion in machine learning is the average log-likelihood, which calculates how much the generated samples deviate from the training data, which might give you the proximity of two elements, but not the musicality of the generated one.<br/>
<br/>
Even if the progress in GANs is promising in such evaluations, we are often left with only our ears to evaluate. We can also imagine a Turing test for a music piece: a composition is played to an audience that has to decide whether the piece was generated by a computer.</div>
<p>We'll be using Magenta for two different purposes, assisting and autonomous music creation:</p>
<ul>
<li><strong>Assisting music systems</strong> helps with the process of composing music. Examples of this would be the Magenta interface, <kbd>magenta_midi.py</kbd>, where the musician can enter a MIDI sequence and Magenta will answer with a generated sequence that's inspired by the provided one. These types of systems can be used alongside traditional systems to compose music and get new inspirations. We'll be talking about this in <a href="8018122a-b28e-44ff-8533-5061a0ad356b.xhtml">Chapter 9</a>, <em>Making Magenta Interact with Music Applications</em>, where Magenta Studio can be integrated into a traditional music production tool.</li>
<li><strong>Autonomous music systems</strong> continuously produce music without the input of an operator. At the end of this book, you'll have all the tools you'll need to build an autonomous music generation system consisting of the various building blocks of Magenta.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking at Magenta's content</h1>
                </header>
            
            <article>
                
<p>Remembering what we saw in the previous section, there are many ways of representing music: symbolic data, spectrogram data, and raw audio data. Magenta works mainly with symbolic data, meaning we'll mainly work on the underlying score in music instead of working directly with audio. Let's look into Magenta's content, model by model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Differentiating models, configurations, and pre-trained models</h1>
                </header>
            
            <article>
                
<p>In Magenta and in this book, the term <strong>model</strong> refers to a specific deep neural network that is specific for one task. For example, the Drums RNN model is an LSTM network with attention configuration, while the MusicVAE model is a variational autoencoder network. The Melody RNN model is also an LSTM network but is geared toward generating melodies instead of percussion patterns.</p>
<p>Each model has different <strong>configurations</strong> that will change how the data is encoded for the network, as well as how the network is configured. For example, the Drums RNN model has a <kbd>one_drum</kbd> configuration, which encodes the sequence to a single class, as well as a <kbd>drum_kit</kbd> configuration, which maps the sequence to nine drum instruments and also configures the attention length to 32.</p>
<p class="mce-root"/>
<p>Finally, each configuration comes with one or more <strong>pre-trained models</strong>. For example, Magenta provides a pre-trained Drums RNN <kbd>drum_kit</kbd> model, as well as multiple pre-trained MusicVAE <kbd>cat-drums_2bar_small</kbd> models.</p>
<p>We'll be using this terminology throughout this book. For the first few chapters, we'll be using the Magenta pre-trained models, since they are already quite powerful. After, we'll create our own configurations and train our own models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating and stylizing images</h1>
                </header>
            
            <article>
                
<p>Image generation and stylization can be achieved in Magenta with the <em>Sketch RNN</em> and <em>Style Transfer</em> models, respectively. Sketch-RNN is a <strong>Sequence-to-Sequence</strong> (<strong>Seq2Seq</strong>) variational autoencoder.</p>
<p>Seq2Seq models are used to convert sequences from one domain into another domain (for example, to translate a sentence in English to a sentence in French) that do not necessarily have the same length, which is not possible for a traditional model structure. The network will encode the input sequence into a vector, called a latent vector, from which a decoder will try to reproduce the input sequence as closely as possible.</p>
<p>Image processing is not part of this book, but we'll see the usage of latent space in <a href="838da33e-26a9-4701-bfd3-5014dfff4146.xhtml">Chapter 4</a>, <em>Latent Space Interpolation with MusicVAE</em>, when we use the MusicVAE model. If you are interested in the SketchRNN model, see the <em>Further reading</em> section for more information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating audio</h1>
                </header>
            
            <article>
                
<p>Audio generation in Magenta is done with the <em>NSynth</em>, a WaveNet-based autoencoder, and <em>GANSynth</em> models. What's interesting about WaveNet is that it is a convolutional architecture, prevalent in image applications, but seldom used in music applications, in favor of recurrent networks. <strong>Convolutional neural networks</strong> (<strong>CNNs</strong>) are mainly defined by a convolution stage, in which a filter is slid through the image, computing a feature map of the image. Different filter matrices can be used to detect different features, such as edges or curves, which are useful for image classification.</p>
<p>We'll see the usage of these models in <a href="feb070b7-92ac-4762-a4ac-7c1a797a47ef.xhtml">Chapter 5</a>, <em>Audio Generation with NSynth and GANSynth</em>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating, interpolating, and transforming score</h1>
                </header>
            
            <article>
                
<p>Score generation is the main part of Magenta and can be split into different categories representing the different parts of a musical score:</p>
<ul>
<li><strong>Rhythms generation</strong>: This can be done with the "Drums RNN" model, an RNN network that applies language modeling using an LSTM. Drum tracks are polyphonic by definition because multiple drums can be hit simultaneously. This model will be presented in <a href="b60deee5-c58f-45eb-88a2-23718802e580.xhtml">Chapter 2</a>, <em>Generating Drum Sequences with Drums RNN</em>.</li>
<li><strong>Melody generation</strong>: Also known as monophonic generation, this can be done with the "Melody RNN" and "Improv RNN" models, which also implement the use of attention, allowing the models to learn longer dependencies. These models will be presented in <a href="48023567-4100-492a-a28e-53b18a63e01e.xhtml"/><a href="48023567-4100-492a-a28e-53b18a63e01e.xhtml">Chapter 3</a>, <em>Generating Polyphonic Melodies</em>.</li>
<li><strong>Polyphonic generation</strong>: This can be done with the <em>Polyphony RNN</em> and <em>Performance RNN</em> models, where the latter also implements expressive timing (sometimes called groove, where the notes don't start and stop exactly in the grid, giving it a human fell) and dynamics (or velocity). These models will be presented in <a href="48023567-4100-492a-a28e-53b18a63e01e.xhtml">Chapter 3</a>, <em>Generating Polyphonic Melodies</em>.</li>
<li><strong>Interpolation</strong>: This can be done with the MusicVAE model, a variational autoencoder that learns the latent space of a musical sequence and can interpolate between existing sequences. This model will be presented in <a href="838da33e-26a9-4701-bfd3-5014dfff4146.xhtml">Chapter 4</a>, <em>Latent Space Interpolation with Music VAE</em>.</li>
<li><strong>Transformation</strong>: This can be done with the <em>GrooVAE</em> model, a variant of the MusicVAE model that will add groove to an existing drum performance. This model will be presented in <a href="838da33e-26a9-4701-bfd3-5014dfff4146.xhtml"/><a href="838da33e-26a9-4701-bfd3-5014dfff4146.xhtml">Chapter 4</a>, <em>Latent Space Interpolation with Music VAE</em>.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Magenta and Magenta for GPU</h1>
                </header>
            
            <article>
                
<p>Installing a machine learning framework is not an easy task and often a pretty big entry barrier, mainly because Python is an infamous language concerning dependency management. We'll try to make this easy by providing clear instructions and versions. We'll be covering installation instructions for Linux, Windows, and macOS since the commands and versions are mostly the same.</p>
<p class="mce-root"/>
<p>In this section, we'll be installing Magenta and Magenta for GPU, if you have the proper hardware. Installing Magenta for a GPU takes a bit more work but is necessary if you want to train a model, which we will do in <a href="6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml">Chapter 7</a>, <em>Training Magenta Models</em>. If you are unsure about doing this, you can skip this section and come back to it later. We'll also provide a solution if you don't have a GPU but still want to do the chapter by using cloud-based solutions.</p>
<p>TensorFlow will be installed through Magenta's dependencies. We'll also look at optional but useful programs that can help you visualize and play audio content.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing the right versions</h1>
                </header>
            
            <article>
                
<p>At the time of writing, newer versions of Python and CUDA are available, but we are using the following versions because of incompatibilities with TensorFlow and TensorFlow GPU. We'll be using Magenta 1.1.7 since it is the stable version of Magenta at the time of writing. You can try using a newer version for the examples and roll back if it doesn't work:</p>
<ul>
<li>Magenta: 1.1.7</li>
<li>TensorFlow: 1.15.0 (this version is installed automatically when installing Magenta)<strong><br/></strong></li>
</ul>
<p>This means that we need to use exactly the following versions for TensorFlow to work:</p>
<ul>
<li>Python: 3.6.x</li>
<li>CUDA libraries: 10.0</li>
<li>CudRNN: 7.6.x (the latest version is OK)</li>
</ul>
<p>Let's look at how to install those versions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a Python environment with Conda</h1>
                </header>
            
            <article>
                
<p class="mce-root">Throughout this book, we'll be using a Python environment, a standalone and separate installation of Python you can switch to when you are working with a specific piece of software, such as when you're working on this book or another piece of software. This also ensures that the system-wide installation remains safe.</p>
<p class="mce-root"/>
<p class="mce-root">There are many Python environment managers available, but we'll use Conda here, which we'll come installed with a standalone Python installation called <strong>Miniconda</strong>. You can think of Miniconda as a program with a packaged Python, some dependencies, and Conda.</p>
<p>To install Miniconda, go to <a href="https://docs.conda.io/en/latest/miniconda.html">docs.conda.io/en/latest/miniconda.html</a>, download the installer for your platform, choose Python 3.7 as the Python version (this is NOT the Python version Magenta will run in), and either 32-bit or 64-bit (you probably have the latter).</p>
<p>For Windows, follow these steps:</p>
<ol>
<li>Double-click the installer. Then, follow the prompts and leave the defaults as they are.</li>
<li>Add <kbd>conda</kbd> to PATH by going to <kbd>Control Panel &gt; System &gt; Advanced system settings &gt; Environment Variables... &gt; Path &gt; Edit... &gt; New</kbd> and add the <kbd>condabin</kbd> folder to the Miniconda installation folder (which should be <kbd>C:\Users\Packt\Miniconda3\condabin</kbd>).</li>
</ol>
<div class="packt_tip">If you are facing issues installing Miniconda on Windows, you can also use <strong>Anaconda</strong>, which is the same software but packaged with more tools.<br/>
<br/>
First, download Anaconda from <a href="https://www.anaconda.com/distribution/">www.anaconda.com/distribution</a>, double-click the installer, follow the prompts, and leave the defaults as they are.<br/>
<br/>
Then, launch <strong>Anaconda Prompt</strong> from the Start menu instead of the <strong>Command Prompt</strong>, which will launch a new command-line window with Conda initialized.</div>
<p>For macOS and Linux, open a Terminal where you downloaded the file as follow these steps:</p>
<ol>
<li class="mce-root">Make the script executable for your user by replacing <kbd>&lt;platform&gt;</kbd> with the platform you downloaded: </li>
</ol>
<pre style="padding-left: 60px"><strong>chmod u+x Miniconda3-latest-&lt;platform&gt;</strong></pre>
<ul>
<li class="mce-root">Now, execute the script, which will install the software:</li>
</ul>
<pre style="padding-left: 60px"><strong>./Miniconda3-latest-&lt;platform&gt;</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now that Conda has been installed, let's check if it works properly:</p>
<ol>
<li>Open a new Terminal and type in the following:</li>
</ol>
<pre style="padding-left: 60px"><strong>&gt; conda info</strong><br/><br/><strong>    active environment : None</strong><br/><strong>            shell level : 0</strong><br/><strong>       user config file : C:\Users\Packt\.condarc</strong><br/><strong> populated config files :</strong><br/><strong>          conda version : 4.7.5</strong><br/><strong>    conda-build version : not installed</strong><br/><strong>         python version : 3.7.3.final.0</strong><br/><strong>       virtual packages : __cuda=10.1</strong><br/><strong>       base environment : C:\Users\Packt\Miniconda3  (writable)</strong><br/><strong>[...]</strong><br/><strong>       envs directories : C:\Users\Packt\Miniconda3\envs</strong><br/><strong>                          C:\Users\Packt\.conda\envs</strong><br/><strong>                          C:\Users\Packt\AppData\Local\conda\conda\envs</strong><br/><strong>               platform : win-64</strong><br/><strong>             user-agent : conda/4.7.5 requests/2.21.0 CPython/3.7.3 Windows/10 Windows/10.0.18362</strong><br/><strong>          administrator : False</strong><br/><strong>             netrc file : None</strong><br/><strong>           offline mode : False</strong></pre>
<p style="padding-left: 60px">Your output will look different, but the idea is the same. </p>
<ol start="2">
<li>Now, we need to create a new environment for this book. Let's call it "magenta":</li>
</ol>
<pre style="padding-left: 60px"><strong>&gt; conda create --name magenta python=3.6</strong></pre>
<p style="padding-left: 60px">Notice that the Python version is 3.6, as we mentioned at the beginning of this section.</p>
<p class="mce-root" style="padding-left: 60px">Your new environment with the correct version of Python and some dependencies has been created. You now have three different Python environments, each with a version of Python with its own dependencies :</p>
<ul>
<li style="padding-left: 30px"><strong>None</strong>: This is the system-wide installation of Python from the system (this might be absent on Windows), and you can switch to it with <kbd>conda deactivate</kbd>.</li>
<li style="padding-left: 30px"><strong>base</strong>: This is the Miniconda Python installation of Python 3.7 we downloaded, and you can switch to it with <kbd>conda activate base</kbd>.</li>
<li style="padding-left: 30px"><strong>magenta</strong>: This is our new Python 3.6 installation for this project, and you can switch to it with <kbd>conda activate magenta</kbd>.</li>
</ul>
<p class="mce-root" style="padding-left: 60px">Since we are still in the base environment, we need to activate the "magenta" environment.</p>
<ol start="3">
<li class="mce-root">Use the <kbd>activate</kbd> flag to change environments:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>&gt; conda activate magenta</strong></pre>
<p class="mce-root" style="padding-left: 60px">From there, your Terminal should prefix the line with "(magenta)", meaning the commands you are executing are being executed in this specific environment.</p>
<ol start="4">
<li class="mce-root">Let's check our Python version:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>&gt; python --version</strong><br/><strong>Python 3.6.9 :: Anaconda, Inc.</strong></pre>
<p class="mce-root">If you have something else here (you should have Python version 3.6.x and Anaconda packaging), stop and make sure you followed the installation instructions properly.</p>
<div class="mce-root packt_infobox">This is just a reminder that you <strong>need Python 3.6.x</strong>. An older version of Python won't be able to run the code in this book because we are using language features from 3.6, and a newer version won't run TensorFlow because it doesn't support 3.7 yet.</div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing prerequisite software</h1>
                </header>
            
            <article>
                
<p>Now that we have a Python environment up and running, you'll need some prerequisite software that will be useful throughout this book. Let's get started:</p>
<ol>
<li>First, we'll need to install <kbd>curl</kbd>, which is preinstalled on Windows and macOS, but not Linux (at least not in all distributions). On a Debian distribution, use the following command. On other distributions, use your package manager:</li>
</ol>
<pre style="padding-left: 60px"><strong>&gt; sudo apt install curl</strong></pre>
<ol start="2">
<li>Now, we need to install Visual MIDI, the MIDI visualization library we'll use to make the diagrams of our generated scores. While in the Magenta environment, run the following command:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>&gt; pip install visual_midi<br/></strong></pre>
<ol start="3">
<li><span>Finally, we'll install the tables modules, which will be useful later to read external datasets stored in H5 databases:</span></li>
</ol>
<pre style="padding-left: 60px"><strong><span>&gt; pip install tables</span></strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Magenta</h1>
                </header>
            
            <article>
                
<p>With our environment and the prerequisite software installed, we can now install Magenta version 1.1.7. You can use a more recent version of Magenta, but do this at your own risk: this book's code was written with version 1.1.7, and the Magenta source code has a tendency to change. Let's get started:</p>
<ol>
<li>While in the Magenta environment, run the following command:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>&gt; pip install magenta==1.1.7</strong></pre>
<div class="packt_tip">If you want to try a more recent version of Magenta, just remove the version information contained in the <kbd>pip</kbd> command. Then, it will install the latest version. If you have problems using a newer version, you can reinstall version 1.1.7 using the <kbd>pip install 'magenta==1.1.7' --force-reinstall</kbd> command.</div>
<ol start="2">
<li class="mce-root">Then, test the installation by importing Magenta into a Python shell and printing the version:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>&gt; python -c "import magenta; print(magenta.__version__)"</strong></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Magenta for GPU (optional)</h1>
                </header>
            
            <article>
                
<p class="mce-root">Now that we have installed Magenta, we'll install Magenta for GPU, which is required for Magenta to execute on the GPU. This is optional if you don't have a GPU, are not planning to train any models, or want to use a cloud-based solution for training. Before continuing, we need to make sure our GPU is CUDA enabled with a compute capability greater than 3.0 by checking out NVIDIA's website: <a href="https://developer.nvidia.com/cuda-gpus">developer.nvidia.com/cuda-gpus</a>.</p>
<p>On Windows, you need to download and install the Visual Studio Community IDE from <a href="https://visualstudio.microsoft.com/">visualstudio.microsoft.com</a>, which should install all the required dependencies for us.</p>
<p>Then, for all platforms, follow these steps:</p>
<ol>
<li>Download the <strong>CUDA Toolkit</strong> from <a href="https://developer.nvidia.com/cuda-10.0-download-archive">developer.nvidia.com/cuda-10.0-download-archive</a> and launch the installation wizard using any of the provided installation methods.</li>
</ol>
<p style="padding-left: 60px">During the CUDA driver's installation, you might get a message saying that "Your display drivers are more recent than the ones provided with this installation". This is normal since this CUDA version is not the latest. You can keep your current display drivers by selecting <strong>CUDA drivers only</strong>.</p>
<ol start="2">
<li>Now that CUDA has been installed, you might have to restart your computer to load the NVIDIA driver. You can test your installation by using the following command:</li>
</ol>
<pre style="padding-left: 60px"><strong>&gt; nvcc --version</strong><br/><strong>nvcc: NVIDIA (R) Cuda compiler driver</strong><br/><strong>Copyright (c) 2005-2018 NVIDIA Corporation</strong><br/><strong>Built on Sat_Aug_25_21:08:04_Central_Daylight_Time_2018</strong><br/><strong>Cuda compilation tools, release 10.0, V10.0.130</strong></pre>
<ol start="3">
<li>Now, we need to install the <strong>cuDNN library</strong>, which is a toolkit for executing deep learning commands on the GPU with the CUDA driver. You should be able to use the most recent cuDNN version from <a href="https://developer.nvidia.com/rdp/cudnn-download">developer.nvidia.com/rdp/cudnn-download</a> for CUDA 10.0. Choose <kbd>Download cuDNN v7.6.x (...), for CUDA 10.0</kbd>.<br/>
Make sure you use the <kbd>cuDNN Library for Platform</kbd> link so that we have the full library archive to work with (do not download the <kbd>.deb</kbd> file, for example).</li>
</ol>
<p> </p>
<ol start="4">
<li>Once downloaded, we'll have to copy the files from the proper location; see the following commands for each platform:
<ul>
<li>Linux: <a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux-tar">docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installlinux-tar</a></li>
<li>macOS: <a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#install-mac">docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#install-mac</a></li>
<li>Windows: <a href="https://docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installwindows">docs.nvidia.com/deeplearning/sdk/cudnn-install/index.html#installwindows</a></li>
</ul>
</li>
<li>Now, we are ready to install Magenta for GPU:</li>
</ol>
<pre style="padding-left: 60px"><strong>&gt; pip install magenta-gpu===1.1.7</strong></pre>
<p>Check out the tip in the <em>Generating a basic MIDI file</em> section to verify TensorFlow is working properly with your GPU.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing the music software and synthesizers</h1>
                </header>
            
            <article>
                
<p>During the course of this book, we'll be handling MIDI and audio files. Handling the MIDI files requires specific software that you should install now since you'll need it for the entirety of this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing the FluidSynth software synthesizer</h1>
                </header>
            
            <article>
                
<p>A software synthesizer is a piece of software that will play incoming MIDI notes or MIDI files with virtual instruments from sound banks (called SoundFont) or by synthesizing audio using waveforms. We will need a software synthesizer to play the notes that are generated by our models.</p>
<p>For this book, we'll be using FluidSynth, a powerful and cross-platform software synth available on the command line. We'll go through the installation procedure for each platform in this section.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing SoundFont</h1>
                </header>
            
            <article>
                
<p>The SoundFont installation is the same for all platforms. We'll download and keep the SoundFont file in an easy-access location since we'll need it throughout this book. Follow these steps:</p>
<ol>
<li>Download SoundFont at <a href="http://ftp.debian.org/debian/pool/main/f/fluid-soundfont/fluid-soundfont_3.1.orig.tar.gz">ftp.debian.org/debian/pool/main/f/fluid-soundfont/fluid-soundfont_3.1.orig.tar.gz</a>.</li>
<li>Extract the <kbd>.tar.gz</kbd> file.</li>
<li>Copy the <kbd>FluidR3_GM.sf2</kbd> file to an easy-access location.</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing FluidSynth</h1>
                </header>
            
            <article>
                
<p>Unfortunately, for <strong>Windows</strong>, binaries are not maintained by the FluidSynth core team. Instead of building from the source, we'll need to fetch the binaries from a GitHub project (the versions might be a bit behind the release schedule, though). Follow these steps:</p>
<ol>
<li>Download the zip at <a href="https://github.com/JoshuaPrzyborowski/FluidSynth-Windows-Builds/archive/master.zip">github.com/JoshuaPrzyborowski/FluidSynth-Windows-Builds/archive/master.zip</a>.</li>
<li>Unzip the file and navigate to the <kbd>bin64</kbd> folder.</li>
<li>Copy the <kbd>fluidsynth-2.0.x</kbd> folder (containing the latest version of FluidSynth) to an easy-access location.</li>
<li>Copy the content of the <kbd>fluidsynth-required-dlls</kbd> file to <kbd>C:\Windows\System32</kbd>.</li>
<li>Add FluidSynth to <kbd>PATH</kbd> by going to <kbd>Control Panel &gt; System &gt; Advanced system settings &gt; Environment Variables... &gt; Path &gt; Edit... &gt; New</kbd> and add the <kbd>bin</kbd> folder from the copied folder from <em>step 3</em>.</li>
</ol>
<p>For <strong>Linux</strong>, most distributions maintain a FluidSynth package. Here, we're providing the installation instruction for Debian-based distributions. Refer to your package manager for other distributions. In a Terminal, use the <kbd>sudo apt install fluidsynth</kbd> command to download FluidSynth.</p>
<p>For <strong>MacOS X</strong>, we'll be using Homebrew to install FluidSynth. Before starting, make sure you have the latest Homebrew version. In a Terminal, use the <kbd>brew install fluidsynth</kbd> command to download FluidSynth.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Testing your installation</h1>
                </header>
            
            <article>
                
<p>Now, you can test your FluidSynth installation (do this by replacing <kbd>PATH_SF2</kbd> with the path to the SoundFont we installed previously): </p>
<ul>
<li class="mce-root">Linux: <kbd>fluidsynth -a pulseaudio -g 1 -n -i PATH_TO_SF2<br/></kbd></li>
<li class="mce-root">macOS: <kbd>fluidsynth -a coreaudio -g 1 -n -i PATH_TO_SF2<br/></kbd></li>
<li class="mce-root">Windows: <kbd>fluidsynth -g 1 -n -i PATH_TO_SF2<br/></kbd></li>
</ul>
<p>You should see an output similar to the following, without any errors:</p>
<pre>FluidSynth runtime version 2.0.3<br/>Copyright (C) 2000-2019 Peter Hanappe and others.<br/>Distributed under the LGPL license.<br/>SoundFont(R) is a registered trademark of E-mu Systems, Inc</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a hardware synthesizer (optional)</h1>
                </header>
            
            <article>
                
<p>Instead of using a software synthesizer, you could use a hardware synthesizer to listen to your generated MIDI files. We'll look at this in more detail in <a href="8018122a-b28e-44ff-8533-5061a0ad356b.xhtml">Chapter 9</a>, <em>Making Magenta Interact with Music Applications</em>, but you can already plug the synthesizer into your computer via USB; the device should register as a new input MIDI port. This port can be used by Magenta to send incoming MIDI notes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Audacity as a digital audio editor (optional)</h1>
                </header>
            
            <article>
                
<p>We won't be handling audio until <a href="feb070b7-92ac-4762-a4ac-7c1a797a47ef.xhtml">Chapter 5</a>, <em>Audio Generation with NSynth and GANSynth</em>, so you can wait until that chapter to install Audacity. Audacity is an amazing open source cross-platform (Windows, Linux, macOS) software for editor audio clips. It doesn't have the functionality of a Digital Audio Workstation (see the <em>Installing a Digital Audio Workstation</em> section for more on this), but it is easy to use and powerful.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Audacity can be used to easily record audio, cut and split audio clips, add simple effects, do simple equalization, and export various formats:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5ba1f748-93eb-462c-ac14-8bc2a69726ea.png"/></p>
<p>We'll be explaining how to use Audacity in <a href="feb070b7-92ac-4762-a4ac-7c1a797a47ef.xhtml">Chapter 5</a>, <em>Audio Generation with NSynth and GANSynth</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing MuseScore for sheet music (optional)</h1>
                </header>
            
            <article>
                
<p>Throughout this book, we'll be working with sheet music a lot, especially MIDI. We'll have command-line utilities to generate still images representing the score, but it is useful to see and edit the sheet music in a GUI, as well as listen to them with digital instruments. Take note that MuseScore cannot play live MIDI, so it is different from a software synthe<span>sizer. It also doesn't work well with expressive timing (where the notes do not fall on the beginning and end steps). We'll make note of</span><span> </span><span>when not to use MuseScore in the next chapter.</span></p>
<p>MuseScore is a good and free notation software available at <a href="https://musescore.org">musescore.org</a> and works on all platforms. You can install it now if you want, or wait until later when you need it.</p>
<p class="mce-root"/>
<p>MuseScore also doubles down as a collaborative sheet music database at <a href="https://musescore.com">musescore.com</a>, which we'll use throughout this book:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b61b9e6b-80b5-4f0d-afac-38d7cacdb56f.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing a Digital Audio Workstation (optional)</h1>
                </header>
            
            <article>
                
<p>Installing a DAW is not necessary for this book, except for <a href="8018122a-b28e-44ff-8533-5061a0ad356b.xhtml">Chapter 9</a>, <em>Making Magenta Interact with Music Applications</em>. Such software comes in various forms and complexity and is important in music production in general since it can handle all the necessities of music production, such as audio and MIDI handling, composition, effects, mastering, VSTs, and so on.</p>
<p>Ardour (<a href="https://ardour.org">ardour.org</a>) is the only open source and cross-platform DAW available and requires you to pay a small fee for a pre-built version of the software. Depending on your platform, you might want to try different DAWs. On Linux, you can go with Ardour. On macOS and Windows, you can use Ableton Live, a well-established DAW. We won't be recommending any specific software for this part, so you can go with whatever you are used to. In <a href="8018122a-b28e-44ff-8533-5061a0ad356b.xhtml">Chapter 9</a>, <em>Making Magenta Interact with Music Applications</em>, we'll go into more detail by giving specific examples for specific DAWs, so you can wait until then to install a new one.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing the code editing software</h1>
                </header>
            
            <article>
                
<p>In this section, we'll recommend optional software regarding code editing. While not mandatory, it might help considerably to use them, especially for newcomers, for whom plain code editing software can be daunting.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Jupyter Notebook (optional)</h1>
                </header>
            
            <article>
                
<p>Notebooks are a great way of sharing code that contains text, explanations, figures, and other rich content. It is used extensively in the data science community because it can store and display the result of long-running operations, while also providing a dynamic runtime to edit and execute the content in.</p>
<p>The code for this book is available on GitHub as plain Python code, but also in the form of Jupyter Notebooks. Each chapter will have its own notebook that serves as an example.</p>
<p>To install Jupyter and launch your first notebook, follow these steps:</p>
<ol>
<li>While in the Magenta environment, execute the following command:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>&gt; pip install jupyter</strong></pre>
<ol start="2">
<li>Now, we can start the Jupyter server by executing the following command (also while in the Magenta environment):</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"><strong>&gt; jupyter notebook</strong></pre>
<p style="padding-left: 60px">The Jupyter interface will be shown in a web browser. The previous command should have launched your default browser. If not, use the URL in the output of the command to open it.</p>
<ol start="3">
<li>Once in the notebook UI, you should see your disk content. Navigate to the code for this book and load the notebook from <kbd>Chapter01/notebook.ipynb</kbd>.</li>
<li>Make sure the selected kernel is <strong>Python 3</strong>. This kernel corresponds to the Python interpreter that's been installed in your Magenta environment.</li>
<li>Run the code blocks using the <strong>Run</strong> button for each cell. This will make sure that Jupyter executes in a proper environment by printing the TensorFlow and Magenta versions.</li>
</ol>
<p>This is what the notebook should look like:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b0e9fce6-c40a-4eb1-aadc-acb4e4e9510d.png"/></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing and configuring an IDE (optional)</h1>
                </header>
            
            <article>
                
<p>The usage of an <strong>Integrated Development Environment</strong> (<strong>IDE</strong>) is not necessary for this book since all the examples run from the command line. However, an IDE is a good tool to use since it provides autocompletion, integrated development tools, refactoring options, and more. It is also really useful for debugging since you can step into the code directly.</p>
<p>A good IDE for this book is JetBrains's PyCharm (<a href="https://www.jetbrains.com/pycharm">www.jetbrains.com/pycharm</a>), a Python IDE with a community (open source) edition that provides everything you need.</p>
<p>Whether you use PyCharm or another IDE, you'll need to change Python interpreter to the one we installed previously. This is the equivalent of activating our Magenta environment using Conda. In the project settings in the IDE, find the Python interpreter settings and change it to the installation path of our environment.</p>
<p>If you don't remember its location, use the following commands:</p>
<pre><strong>&gt; conda activate magenta</strong><br/><strong>&gt; conda info</strong><br/><strong>...</strong><br/><strong>    active env location : C:\Users\Packt\Miniconda3\envs\magenta</strong><br/><strong>...</strong></pre>
<p class="mce-root"/>
<p>On Windows, the Python interpreter is in the root folder, while on Linux or macOS, it is in the <kbd>bin</kbd> directory under it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating a basic MIDI file</h1>
                </header>
            
            <article>
                
<p>Magenta comes with multiple command-line scripts (installed in the <kbd>bin</kbd> folder of your Magenta environment). Basically, each model has its own console script for dataset preparation, model training, and generation. Let's take a look:</p>
<ol start="1">
<li>While in the Magenta environment, download the Drums RNN pre-trained model, <kbd>drum_kit_rnn</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>&gt; curl --output "drum_kit_rnn.mag" "http://download.magenta.tensorflow.org/models/drum_kit_rnn.mag"</span></strong></pre>
<ol start="2">
<li>Then, use the following command to generate your first few MIDI files:</li>
</ol>
<pre style="padding-left: 60px"><span><strong>&gt; drums_rnn_generate --bundle_file="drum_kit_rnn.mag"</strong><br/></span></pre>
<p style="padding-left: 60px">By default, the preceding command generates the files in <kbd>/tmp/drums_rnn/generated</kbd> (on Windows <kbd>C:\tmp\drums_rnn\generated</kbd>). You should see 10 new MIDI files, along with timestamps and a generation index.</p>
<div class="packt_tip">If you are using a GPU, you can verify if TensorFlow is using it properly by searching for "Created TensorFlow device ... -&gt; <strong>physical GPU</strong> (name: ..., compute capability: ...)" in the output of the script. If it's not there, this means it is executing on your CPU.<br/>
<br/>
You can also check your GPU usage while Magenta is executing, which should go up if Magenta is using the GPU properly.</div>
<ol start="3">
<li>Finally, to listen to the generated MIDI, use your software synthesizer or MuseScore. For the software synth, refer to the following command, depending on your platform, and replace <kbd>PATH_TO_SF2</kbd> and <kbd>PATH_TO_MIDI</kbd> with the proper values:
<ul>
<li>Linux: <kbd>fluidsynth -a pulseaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>macOS: <kbd>fluidsynth -a coreaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>Windows: <kbd>fluidsynth -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
</ul>
</li>
</ol>
<p>Congratulations! You have generated your first musical score using a machine learning model! You'll learn how to generate much more throughout this book.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>This chapter is important because it introduces the basic concepts of music generation with machine learning, all of which we'll build upon throughout this book.</p>
<p>In this chapter, we learned what generative music is and that its origins predate even the advent of computers. By looking at specific examples, we saw different types of generative music: random, algorithmic, and stochastic.</p>
<p>We also learned how machine learning is rapidly transforming how we generate music. By introducing music representation and various processes, we learned about MIDI, waveforms, and spectrograms, as well as various neural network architectures we'll get to look at throughout this book.</p>
<p>Finally, we saw an overview of what we can do with Magenta in terms of generating and processing image, audio, and score. By doing that, we introduced the primary models we'll be using throughout this book; that is, Drums RNN, Melody RNN, MusicVAE, NSynth, and others.</p>
<p>You also installed your development environment for this book and generated your first musical score. Now, we're ready to go!</p>
<p>The next chapter will delve deeper into some of the concepts we introduced in this chapter. We'll explain what an RNN is and why it is important for music generation. Then, we'll use the Drums RNN model on the command line and in Python while explaining its inputs and outputs. We'll finish by creating the first building block of our autonomous music generating system.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li class="mce-root">On what generative principle does the <em>musical dice game</em> rely upon?</li>
<li class="mce-root">What stochastic-based generation technique was used in the first computerized generative piece of music, <em>Illiac Suite</em>?</li>
<li>What is the name of the music genre where a live coder implements generative music on the scene?</li>
</ol>
<p> </p>
<ol start="4">
<li>What model structure is important for tracking temporally distant events in a musical score?</li>
<li>What is the difference between autonomous and assisting music systems?</li>
<li>What are examples of symbolic and sub-symbolic representations?</li>
<li>How is a note represented in MIDI?</li>
<li>What frequency range can be represented without loss at a sample rate of 96 kHz? Is it better for listening to audio?</li>
<li>In a spectrogram, a block of 1 second of intense color at 440 Hz is shown. What is being played?</li>
<li>What different parts of a musical score can be generated with Magenta?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>Ten Questions Concerning Generative Computer Art:</strong> An interesting paper (2012) on generative computer art (<a href="http://users.monash.edu/~jonmc/research/Papers/TenQuestionsLJ-Preprint.pdf">users.monash.edu/~jonmc/research/Papers/TenQuestionsLJ-Preprint.pdf</a>).</li>
<li><strong>Pulse Code Modulation (PCM):</strong> A short introduction to PCM (<a href="https://www.technologyuk.net/telecommunications/telecom-principles/pulse-code-modulation.shtml">www.technologyuk.net/telecommunications/telecom-principles/pulse-code-modulation.shtml</a>).</li>
<li><strong>Making Music with Computers:</strong> A good introduction to the sampling theory and the Nyquist frequency (<a href="http://legacy.earlham.edu/~tobeyfo/musictechnology/4_SamplingTheory.html">legacy.earlham.edu/~tobeyfo/musictechnology/4_SamplingTheory.html</a>).</li>
<li><strong>SketchRNN model released in Magenta:</strong> A blog post from the Magenta team on SketchRNN, with a link to the corresponding paper (<a href="https://magenta.tensorflow.org/sketch_rnn">magenta.tensorflow.org/sketch_rnn</a>).</li>
<li><strong>Creation by refinement: a creativity paradigm for gradient descent learning networks:</strong> An early paper (1988) on generating content using a gradient-descent search (<a href="https://ieeexplore.ieee.org/document/23933">ieeexplore.ieee.org/document/23933</a>).</li>
<li><strong>A First Look at Music Composition using LSTM Recurrent Neural Networks:</strong> An important paper (2002) on generating music using LSTM (<a href="https://www.semanticscholar.org/paper/A-First-Look-at-Music-Composition-using-LSTM-Neural-Eck-Schmidhuber/3b70fbcd6c0fdc7697c93d0c3fb845066cf34487">www.semanticscholar.org/paper/A-First-Look-at-Music-Composition-using-LSTM-Neural-Eck-Schmidhuber/3b70fbcd6c0fdc7697c93d0c3fb845066cf34487</a>).</li>
<li><strong>ImageNet Classification with Deep Convolutional Neural Networks:</strong> The AlexNet paper, one of the most influential papers that was published in computer vision (<a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf">papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf</a>).</li>
<li><strong>WaveNet: A Generative Model for Raw Audio:</strong> A paper (2016) on WaveNet (<a href="https://arxiv.org/abs/1609.03499">arxiv.org/abs/1609.03499</a>).</li>
<li><strong>DeepBach: a Steerable Model for Bach Chorales Generation:</strong> A paper (2016) on Bach-like polyphonic music generation (<a href="https://arxiv.org/abs/1612.01010">arxiv.org/abs/1612.01010</a>).</li>
<li><strong>SampleRNN: An Unconditional End-to-End Neural Audio Generation Model:</strong> A paper (2017) on generating audio (<a href="https://arxiv.org/abs/1612.07837">arxiv.org/abs/1612.07837</a>).</li>
</ul>


            </article>

            
        </section>
    </body></html>