["```py\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import SimpleRNN\n\nn_units = 10\nt_steps = 5\ninpt_ftrs=2\nmodel = Sequential()\nmodel.add(SimpleRNN(n_units, input_shape=(t_steps, inpt_ftrs)))\nmodel.summary()\n```", "```py\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)            Output Shape  Param # \n=================================================================\nsimple_rnn (SimpleRNN)  (None, 10)    130 \n=================================================================\nTotal params: 130\nTrainable params: 130\nNon-trainable params: 0\n```", "```py\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Embedding\n\nvocab_size = 30\nembddng_dim = 10\nseqnc_lngth = 5\n\nmodel = Sequential()\nmodel.add(Embedding(vocab_size, embddng_dim, input_length=seqnc_lngth))\nmodel.summary()\n```", "```py\nModel: \"sequential\"\n_________________________________________________________________\nLayer (type)             Output Shape     Param # \n=================================================================\nembedding (Embedding)    (None, 5, 10)    300 \n=================================================================\nTotal params: 300\nTrainable params: 300\nNon-trainable params: 0\n```", "```py\nfrom keras.datasets import imdb\nfrom keras.preprocessing import sequence\n\ninpt_dim = 128\nindex_from = 3\n\n(x_train, y_train),(x_test, y_test)=imdb.load_data(num_words=10000,\n                                                   start_char=1,\n                                                   oov_char=2,\n                                                   index_from=index_from,\n                                                   skip_top=20)\nx_train = sequence.pad_sequences(x_train, \n                                 maxlen=inpt_dim).astype('float32')\nx_test = sequence.pad_sequences(x_test, maxlen=inpt_dim).astype('float32')\n\n# let's print the shapes\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n```", "```py\n# let's print the indices of sample #7\nprint(' '.join(str(int(id)) for id in x_train[7]))\n\n# let's print the actual words of sample #7\nwrd2id = imdb.get_word_index()\nwrd2id = {k:(v+index_from) for k,v in wrd2id.items()}\nwrd2id[\"<PAD>\"] = 0\nwrd2id[\"<START>\"] = 1\nwrd2id[\"<UNK>\"] = 2\nwrd2id[\"<UNUSED>\"] = 3\n\nid2wrd = {value:key for key,value in wrd2id.items()}\nprint(' '.join(id2wrd[id] for id in x_train[7] ))\n```", "```py\nx_train shape: (25000, 128)\nx_test shape: (25000, 128)\n\n 55   655    707    6371     956    225    1456    841     42 1310   225     2 ...\nvery middle class suburban setting there's zero atmosphere or mood there's <UNK> ...\n```", "```py\nfrom keras.datasets import imdb\nfrom keras.preprocessing import sequence\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import SimpleRNN, Embedding, BatchNormalization\nfrom tensorflow.keras.layers import Dense, Activation, Input, Dropout\n\nseqnc_lngth = 128\nembddng_dim = 64\nvocab_size = 10000\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size, \n                                                      skip_top=20)\nx_train = sequence.pad_sequences(x_train, \n                                 maxlen=seqnc_lngth).astype('float32')\nx_test = sequence.pad_sequences(x_test, \n                                maxlen=seqnc_lngth).astype('float32')\n```", "```py\ninpt_vec = Input(shape=(seqnc_lngth,))\nl1 = Embedding(vocab_size, embddng_dim, input_length=seqnc_lngth)(inpt_vec)\nl2 = Dropout(0.3)(l1)\nl3 = SimpleRNN(32)(l2)\nl4 = BatchNormalization()(l3)\nl5 = Dropout(0.2)(l4)\noutput = Dense(1, activation='sigmoid')(l5)\n\nrnn = Model(inpt_vec, output)\n\nrnn.compile(loss='binary_crossentropy', optimizer='adam', \n            metrics=['accuracy'])\nrnn.summary()\n```", "```py\nModel: \"functional\"\n_________________________________________________________________\nLayer (type)                 Output Shape     Param # \n=================================================================\ninput_1 (InputLayer)         [(None, 128)]    0 \n_________________________________________________________________\nembedding (Embedding)        (None, 128, 64)  640000 \n_________________________________________________________________\ndropout_1 (Dropout)          (None, 128, 64)  0 \n_________________________________________________________________\nsimple_rnn (SimpleRNN)       (None, 32)       3104 \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 32)       128 \n_________________________________________________________________\ndropout_2 (Dropout)          (None, 32)       0 \n_________________________________________________________________\ndense (Dense)                (None, 1)        33 \n=================================================================\nTotal params: 643,265\nTrainable params: 643,201\nNon-trainable params: 64\n```", "```py\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport matplotlib.pyplot as plt\n\n#callbacks\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, \n                              min_delta=1e-4, mode='min', verbose=1)\n\nstop_alg = EarlyStopping(monitor='val_loss', patience=7, \n                         restore_best_weights=True, verbose=1)\n\n#training\nhist = rnn.fit(x_train, y_train, batch_size=100, epochs=1000, \n               callbacks=[stop_alg, reduce_lr], shuffle=True, \n               validation_data=(x_test, y_test))\n\n```", "```py\n# save and plot training process\nrnn.save_weights(\"rnn.hdf5\")\n\nfig = plt.figure(figsize=(10,6))\nplt.plot(hist.history['loss'], color='#785ef0')\nplt.plot(hist.history['val_loss'], color='#dc267f')\nplt.title('Model Loss Progress')\nplt.ylabel('Brinary Cross-Entropy Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training Set', 'Test Set'], loc='upper right')\nplt.show()\n```", "```py\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ny_hat = rnn.predict(x_test)\n\n# gets the ROC\nfpr, tpr, thresholds = roc_curve(y_test, y_hat)\nroc_auc = auc(fpr, tpr)\n\n# plots ROC\nfig = plt.figure(figsize=(10,6))\nplt.plot(fpr, tpr, color='#785ef0', \n         label='ROC curve (AUC = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='#dc267f', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# finds optimal threshold and gets ACC and CM\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold = thresholds[optimal_idx]\nprint(\"Threshold value is:\", optimal_threshold)\ny_pred = np.where(y_hat>=optimal_threshold, 1, 0)\nprint(balanced_accuracy_score(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n```", "```py\nThreshold value is: 0.81700134\n\n0.8382000000000001\n\n[[10273 2227]\n [ 1818 10682]]\n```", "```py\nfrom keras.datasets import imdb\nfrom keras.preprocessing import sequence\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import LSTM, Embedding, BatchNormalization\nfrom tensorflow.keras.layers import Dense, Activation, Input, Dropout\n\nseqnc_lngth = 128\nembddng_dim = 64\nvocab_size = 10000\n\n(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size, \n                                                      skip_top=20)\nx_train = sequence.pad_sequences(x_train, maxlen=seqnc_lngth).astype('float32')\nx_test = sequence.pad_sequences(x_test, maxlen=seqnc_lngth).astype('float32')\n```", "```py\ninpt_vec = Input(shape=(seqnc_lngth,))\nl1 = Embedding(vocab_size, embddng_dim, input_length=seqnc_lngth)(inpt_vec)\nl2 = Dropout(0.3)(l1)\nl3 = LSTM(32)(l2)\nl4 = BatchNormalization()(l3)\nl5 = Dropout(0.2)(l4)\noutput = Dense(1, activation='sigmoid')(l5)\n\nlstm = Model(inpt_vec, output)\n\nlstm.compile(loss='binary_crossentropy', optimizer='adam', \n             metrics=['accuracy'])\nlstm.summary()\n```", "```py\nModel: \"functional\"\n_________________________________________________________________\nLayer (type) Output Shape Param # \n=================================================================\ninput (InputLayer)          [(None, 128)] 0 \n_________________________________________________________________\nembedding (Embedding)       (None, 128, 64) 640000 \n_________________________________________________________________\ndropout_1 (Dropout)         (None, 128, 64) 0 \n_________________________________________________________________\nlstm (LSTM)                 (None, 32) 12416 \n_________________________________________________________________\nbatch_normalization (Batch  (None, 32) 128 \n_________________________________________________________________\ndropout_2 (Dropout)         (None, 32) 0 \n_________________________________________________________________\ndense (Dense)               (None, 1) 33 \n=================================================================\nTotal params: 652,577\nTrainable params: 652,513\nNon-trainable params: 64\n```", "```py\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport matplotlib.pyplot as plt\n\n#callbacks\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, \n                              min_delta=1e-4, mode='min', verbose=1)\n\nstop_alg = EarlyStopping(monitor='val_loss', patience=7, \n                         restore_best_weights=True, verbose=1)\n\n#training\nhist = lstm.fit(x_train, y_train, batch_size=100, epochs=1000, \n                callbacks=[stop_alg, reduce_lr], shuffle=True, \n                validation_data=(x_test, y_test))\n```", "```py\n# save and plot training process\nlstm.save_weights(\"lstm.hdf5\")\n\nfig = plt.figure(figsize=(10,6))\nplt.plot(hist.history['loss'], color='#785ef0')\nplt.plot(hist.history['val_loss'], color='#dc267f')\nplt.title('Model Loss Progress')\nplt.ylabel('Brinary Cross-Entropy Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training Set', 'Test Set'], loc='upper right')\nplt.show()\n```", "```py\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import balanced_accuracy_score\nfrom sklearn.metrics import roc_curve, auc\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ny_hat = lstm.predict(x_test)\n\n# gets the ROC\nfpr, tpr, thresholds = roc_curve(y_test, y_hat)\nroc_auc = auc(fpr, tpr)\n\n# plots ROC\nfig = plt.figure(figsize=(10,6))\nplt.plot(fpr, tpr, color='#785ef0', \n         label='ROC curve (AUC = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='#dc267f', linestyle='--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver Operating Characteristic Curve')\nplt.legend(loc=\"lower right\")\nplt.show()\n\n# finds optimal threshold and gets ACC and CM\noptimal_idx = np.argmax(tpr - fpr)\noptimal_threshold = thresholds[optimal_idx]\nprint(\"Threshold value is:\", optimal_threshold)\ny_pred = np.where(y_hat>=optimal_threshold, 1, 0)\nprint(balanced_accuracy_score(y_test, y_pred))\nprint(confusion_matrix(y_test, y_pred))\n```", "```py\nThreshold value is: 0.44251397\n0.8544400000000001\n[[10459 2041]\n [ 1598 10902]]\n```", "```py\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Activation, Input\nfrom tensorflow.keras.layers import BatchNormalization, Dropout\nfrom tensorflow.keras.layers import Embedding, LSTM\nfrom tensorflow.keras.layers import RepeatVector, TimeDistributed\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport numpy as np\n\nseqnc_lngth = 28    # length of the sequence; must be 28 for MNIST\nltnt_dim = 2        # latent space dimension; it can be anything reasonable\n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\n\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n```", "```py\ninpt_vec = Input(shape=(seqnc_lngth, seqnc_lngth,))\nl1 = Dropout(0.1)(inpt_vec)\nl2 = LSTM(seqnc_lngth, activation='tanh', \n          recurrent_activation='sigmoid')(l1)\nl3 = BatchNormalization()(l2)\nl4 = Dropout(0.1)(l3)\nl5 = Dense(ltnt_dim, activation='sigmoid')(l4)\n\n# model that takes input and encodes it into the latent space\nencoder = Model(inpt_vec, l5)\n```", "```py\nl6 = RepeatVector(seqnc_lngth)(l5)\nl7 = LSTM(seqnc_lngth, activation='tanh', recurrent_activation='sigmoid', \n          return_sequences=True)(l6)\nl8 = BatchNormalization()(l7)\nl9 = TimeDistributed(Dense(seqnc_lngth, activation='sigmoid'))(l8)\n\nautoencoder = Model(inpt_vec, l9)\n```", "```py\nautoencoder.compile(loss='binary_crossentropy', optimizer='adam')\nautoencoder.summary()\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, \n                              min_delta=1e-4, mode='min', verbose=1)\n\nstop_alg = EarlyStopping(monitor='val_loss', patience=15, \n                         restore_best_weights=True, verbose=1)\n\nhist = autoencoder.fit(x_train, x_train, batch_size=100, epochs=1000, \n                       callbacks=[stop_alg, reduce_lr], shuffle=True, \n                       validation_data=(x_test, x_test))\n```", "```py\nx_train shape: (60000, 28, 28)\nx_test shape: (10000, 28, 28)\n\nModel: \"functional\"\n_________________________________________________________________\nLayer (type)               Output Shape      Param # \n=================================================================\ninput (InputLayer)         [(None, 28, 28)]  0 \n_________________________________________________________________\ndropout_1 (Dropout)        (None, 28, 28)    0 \n_________________________________________________________________\nlstm_1 (LSTM)              (None, 28)        6384 \n_________________________________________________________________\nbatch_normalization_1 (Bat (None, 28)        112 \n_________________________________________________________________\n.\n.\n.\ntime_distributed (TimeDist (None, 28, 28)    812 \n=================================================================\nTotal params: 10,950\nTrainable params: 10,838\nNon-trainable params: 112\n_________________________________________________________________\n\nEpoch 1/1000\n600/600 [==============================] - 5s 8ms/step - loss: 0.3542 - val_loss: 0.2461\n.\n.\n.\n\n```", "```py\nencoder.predict(x_test[0:1])\n```", "```py\narray([[3.8787320e-01, 4.8048562e-01]], dtype=float32)\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nx_hat = autoencoder.predict(x_test)\n\nsmp_idx = [3,2,1,18,4,8,11,0,61,9]      # samples for 0,...,9 digits\nplt.figure(figsize=(12,6))\nfor i, (img, y) in enumerate(zip(x_hat[smp_idx].reshape(10, 28, 28), y_test[smp_idx])):\n  plt.subplot(2,5,i+1)\n  plt.imshow(img, cmap='gray')\n  plt.xticks([])\n  plt.yticks([])\n  plt.title(y)\nplt.show()\n```", "```py\nplt.figure(figsize=(12,6))\nfor i, (img, y) in enumerate(zip(x_test[smp_idx].reshape(10, 28, 28), y_test[smp_idx])):\n  plt.subplot(2,5,i+1)\n  plt.imshow(img, cmap='gray')\n  plt.xticks([])\n  plt.yticks([])\n  plt.title(y)\nplt.show()\n```", "```py\ny_ = list(map(int, y_test))\nX_ = encoder.predict(x_test)\n\nplt.figure(figsize=(10,8))\nplt.title('LSTM-based Encoder')\nplt.scatter(X_[:,0], X_[:,1], s=5.0, c=y_, alpha=0.75, cmap='tab10')\nplt.xlabel('First encoder dimension')\nplt.ylabel('Second encoder dimension')\nplt.colorbar()\n```", "```py\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Dense, Activation, Input\nfrom tensorflow.keras.layers import BatchNormalization, Dropout\nfrom tensorflow.keras.layers import Bidirectional, LSTM\nfrom tensorflow.keras.layers import RepeatVector, TimeDistributed\nfrom tensorflow.keras.datasets import mnist\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\nimport numpy as np\n\nseqnc_lngth = 28\nltnt_dim = 100 \n\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\n```", "```py\ninpt_vec = Input(shape=(seqnc_lngth, seqnc_lngth,))\nl1 = Dropout(0.5)(inpt_vec)\nl2 = Bidirectional(LSTM(seqnc_lngth, activation='tanh', \n                        recurrent_activation='sigmoid'))(l1)\nl3 = BatchNormalization()(l2)\nl4 = Dropout(0.5)(l3)\nl5 = Dense(ltnt_dim, activation='sigmoid')(l4)\n\n# sequence to vector model\nencoder = Model(inpt_vec, l5, name='encoder')\n```", "```py\nltnt_vec = Input(shape=(ltnt_dim,))\nl6 = Dropout(0.1)(ltnt_vec)\nl7 = RepeatVector(seqnc_lngth)(l6)\nl8 = Bidirectional(LSTM(seqnc_lngth, activation='tanh', \n                   recurrent_activation='sigmoid', \n                   return_sequences=True))(l7)\nl9 = BatchNormalization()(l8)\nl10 = TimeDistributed(Dense(seqnc_lngth, activation='sigmoid'))(l9)\n\n# vector to sequence model\ndecoder = Model(ltnt_vec, l10, name='decoder')\n```", "```py\nrecon = decoder(encoder(inpt_vec))\nautoencoder = Model(inpt_vec, recon, name='ae')\n\nautoencoder.compile(loss='binary_crossentropy', optimizer='adam')\nautoencoder.summary()\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, \n                              min_delta=1e-4, mode='min', verbose=1)\n\nstop_alg = EarlyStopping(monitor='val_loss', patience=15, \n                         restore_best_weights=True, verbose=1)\n\nhist = autoencoder.fit(x_train, x_train, batch_size=100, epochs=1000, \n                       callbacks=[stop_alg, reduce_lr], shuffle=True, \n                       validation_data=(x_test, x_test))\n```", "```py\nModel: \"ae\"\n_________________________________________________________________\nLayer (type)          Output Shape     Param # \n=================================================================\ninput (InputLayer)    [(None, 28, 28)] 0 \n_________________________________________________________________\nencoder (Functional)  (None, 100)      18692 \n_________________________________________________________________\ndecoder (Functional)  (None, 28, 28)   30716 \n=================================================================\nTotal params: 49,408\nTrainable params: 49,184\nNon-trainable params: 224\n_________________________________________________________________\nEpoch 1/1000\n600/600 [==============================] - 9s 14ms/step - loss: 0.3150 - val_loss: 0.1927\n.\n.\n.\n```", "```py\nz = np.random.rand(1,100)\nx_ = decoder.predict(z)\nprint(x_.shape)\nplt.imshow(x_[0], cmap='gray')\n```", "```py\n(1, 28, 28)\n```", "```py\nplt.figure(figsize=(12,6))\nfor i in range(10):\n  plt.subplot(2,5,i+1)\n  rnd_vec = np.round(np.mean(x_test[y_test==i],axis=0))   #(a)\n  rnd_vec = np.reshape(rnd_vec, (1,28,28))                #(b)\n  z = encoder.predict(rnd_vec)                            #(c)\n  decdd = decoder.predict(z)                              #(d)\n  plt.imshow(decdd[0], cmap='gray')\n  plt.xticks([])\n  plt.yticks([])\n  plt.title(i)\nplt.show()\n```"]