<html><head></head><body>
		<div id="_idContainer126">
			<h1 id="_idParaDest-105"><em class="italic"><a id="_idTextAnchor109"/>Chapter 4</em>: Training Models with Text Data</h1>
			<p>In <a href="B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083"><em class="italic">Chapter 3</em></a>, <em class="italic">Training Models with Tabular Data</em>, you went through a series of recipes that demonstrated how to use the facilities of fastai to train deep learning models on tabular data. In this chapter, we will examine how to take advantage of the fastai framework to train deep learning models on text datasets. </p>
			<p>To explore deep learning with text data in fastai, we will start by taking a pre-trained <strong class="bold">language model</strong> (that is, a model that, when given a phrase, predicts what words come next) and fine-tuning it with the IMDb curated dataset. We will then use the resulting fine-tuned language model to create a <strong class="bold">text classifier model</strong> for the movie review use case represented by the IMDb dataset. The text classifier predicts the class of a phrase; in the movie review use case, it predicts whether a given phrase is <strong class="bold">positive</strong> or <strong class="bold">negative</strong>. </p>
			<p>Finally, we apply the same approach to a standalone (that is, non-curated) text dataset of Covid-related tweets. First, we will fine-tune the existing language model on the Covid tweets dataset. Then, we will use the fine-tuned language model to train a text classifier that predicts the class of a phrase according to the categories defined in the Covid tweets dataset: <strong class="bold">extremely negative</strong>, <strong class="bold">negative</strong>, <strong class="bold">neutral</strong>, <strong class="bold">positive</strong>, and <strong class="bold">extremely positive</strong>.</p>
			<p>The approach to training deep learning models on text datasets that is used in this chapter, also known as <strong class="bold">ULMFiT</strong>, was initially described by the creators of fastai in their paper entitled <em class="italic">Universal Language Model Fine-Tuning for Text Classification</em> <a href="https://arxiv.org/abs/1801.06146">https://arxiv.org/abs/1801.06146</a>. This approach introduced the concept of <strong class="bold">transfer learning</strong> to <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>). </p>
			<p>Transfer learning is taking a model that has been trained on a large, general-purpose dataset and making it applicable to a specific use case by fine-tuning the large model with a smaller dataset that is specific to the use case. </p>
			<p>The ULMFiT approach to transfer learning for NLP can be summarized as follows:</p>
			<ol>
				<li>Start with a large language model trained on a large text dataset.</li>
				<li>Fine-tune this language model on a text dataset that is related to a specific use case.</li>
				<li>Use the fine-tuned language model to create a text classifier for the specific use case.</li>
			</ol>
			<p>In this chapter, the large model is referred to as <strong class="source-inline">AWD_LSTM</strong>, and it has been trained on a big corpus taken from Wikipedia articles. We will fine-tune this large language model on datasets for two specific use cases: IMDb for movie reviews, and Covid tweets for social media posts regarding the Covid 19 pandemic. We then use each of the resulting fine-tuned language models to train text classifiers for each use case.</p>
			<p>Here are the recipes that will be covered in this chapter:</p>
			<ul>
				<li>Training a deep learning language model with a curated IMDb text dataset</li>
				<li>Training a deep learning classification model with a curated text dataset</li>
				<li>Training a deep learning language model with a standalone text dataset </li>
				<li>Training a deep learning text classifier with a standalone text dataset</li>
				<li>Test your knowledge</li>
			</ul>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor110"/>Technical requirements</h1>
			<p>Ensure that you have completed the setup sections from <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with fastai</em>, and have a working <strong class="bold">Gradient</strong> instance or <strong class="bold">Colab</strong> setup. The recipes described in this chapter assume that you are using Gradient. Ensure that you have cloned the repository for the book from <a href="https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook">https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook</a> and have access to the <strong class="source-inline">ch4</strong> folder. This folder contains the code samples described in this chapter.</p>
			<p>Some of the examples in this chapter will take over an hour to run.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Do not use Colab for these examples. With Colab you cannot control the GPU that you get for a session and these examples may run for many hours. For the examples in this chapter, use a for-pay GPU-enabled Gradient environment to ensure they complete in a reasonable time.</p>
			<h1 id="_idParaDest-107"><a id="_idTextAnchor111"/>Training a deep learning language model with a curated IMDb text dataset</h1>
			<p>In this section, you <a id="_idIndexMarker272"/>will go through<a id="_idIndexMarker273"/> the process of training a language model on a curated text dataset using fastai. We take a pre-existing language model that is packaged with fastai and fine-tune it with one of the curated text datasets, IMDb, that contains text samples for the movie review use case. The result will be a language model with the broad language capability of the pre-existing language model, along with the use case-specific details of the IMDb dataset. This recipe illustrates one of the breakthroughs made by the team that created fastai, that is, transfer learning applied to NLP.</p>
			<h2 id="_idParaDest-108"><a id="_idTextAnchor112"/>Getting ready</h2>
			<p>For the recipes so far in this book, we have recommended using the Gradient environment. You can use Gradient for this recipe and the instructions below include several workarounds to make the recipe work on Gradient. In particular, the pre-trained <strong class="source-inline">AWD_LSTM</strong> model will not be available in Gradient if its initial setup gets interrupted and the directory for the <strong class="source-inline">IMDB</strong> model is not writeable. If the setup of the pre-trained <strong class="source-inline">AWD_LSTM</strong> model gets interrupted, follow these steps:</p>
			<ol>
				<li value="1">In Colab, run the cells of the <strong class="source-inline">text_model_training.ipynb</strong> notebook up to and including the learner definition and training cell. Once you have done so, copy the contents of the <strong class="source-inline">/root/.fastai/models/wt103-fwd</strong> directory to a folder in your Drive environment.</li>
				<li>Upload the files you copied in the previous step to the <strong class="source-inline">/storage/models/wt103-fwd</strong> directory in your Gradient environment. </li>
			</ol>
			<p>With these steps, you should now be able to run the notebook for this recipe (and other recipes that make use of <strong class="source-inline">AWD_LSTM</strong>) in Gradient.</p>
			<p>I am grateful for the opportunity to include the IMDB dataset featured in this chapter.</p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, Christopher Potts. (2011) <em class="italic">Learning Word Vectors for Sentiment Analysis</em> (<a href="https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf">https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf</a>) </p>
			<h2 id="_idParaDest-109"><a id="_idTextAnchor113"/>How to do it…</h2>
			<p>In this section, you will be running through the <strong class="source-inline">text_model_training.ipynb</strong> notebook to train a language model using the IMDb curated dataset. Once you have the notebook <a id="_idIndexMarker274"/>open in Colab, complete <a id="_idIndexMarker275"/>the following steps:</p>
			<ol>
				<li value="1">Run the cells in the notebook up to the <strong class="source-inline">Training a language model</strong> cell.</li>
				<li>Run the following cell to define a <strong class="source-inline">path</strong> object associated with the IMDb curated dataset:<p class="source-code">path = untar_data(URLs.IMDB)</p></li>
				<li>You can see the directory structure for this dataset in the output of the <strong class="source-inline">tree -d</strong> command run in the <strong class="source-inline">imdb</strong> directory (<strong class="source-inline">/storage/data/imdb</strong> in Gradient). Note that the labels for the dataset (whether a review is positive or negative) are encoded by the directory in which the text sample is located. For example, the negative review text samples in the training dataset are contained in the <strong class="source-inline">train/neg</strong> directory.<p>The following is the output of the <strong class="source-inline">tree -d</strong> command run in the <strong class="source-inline">imdb</strong> directory:</p><p class="source-code">├── test</p><p class="source-code">│   ├── neg</p><p class="source-code">│   └── pos</p><p class="source-code">├── tmp_clas</p><p class="source-code">├── tmp_lm</p><p class="source-code">├── train</p><p class="source-code">│   ├── neg</p><p class="source-code">│   └── pos</p><p class="source-code">└── unsup</p></li>
				<li>Run the following cell to define a <strong class="source-inline">TextDataLoaders</strong> object:<p class="source-code">dls =TextDataLoaders.from_folder(\</p><p class="source-code">path, valid = 'test', is_lm=True, bs=16)</p><p>Here are the arguments for the definition of the <strong class="source-inline">TextDataLoaders</strong> object:</p><p>a) <strong class="source-inline">path</strong>: The <strong class="source-inline">path</strong> object (associated with the IMDb curated dataset) that you defined earlier in the notebook.</p><p>b) <strong class="source-inline">valid</strong>: Identifies the folder in the dataset's directory structure that will be used to assess the performance of the model: <strong class="source-inline">imdb/test</strong>.</p><p>c) <strong class="source-inline">is_lm</strong>: Set to <strong class="source-inline">True</strong> to indicate that this object will be used for a language model (as opposed <a id="_idIndexMarker276"/>to a text <a id="_idIndexMarker277"/>classifier).</p><p>d) <strong class="source-inline">bs</strong>: Specifies the batch size.</p><p class="callout-heading">Note </p><p class="callout">When you are training a language model with a large dataset such as IMDb, adjusting the <strong class="source-inline">bs</strong> value to be lower than the default batch size of <strong class="source-inline">64</strong> will be essential for avoiding memory errors, and that is why it is set to <strong class="source-inline">16</strong> in this <strong class="source-inline">TextDataLoaders</strong> definition.</p></li>
				<li>Run the following cell to show a couple of items from a sample batch:<p class="source-code">dls.show_batch(max_n=2)</p><p>The <strong class="source-inline">max_n</strong> argument specifies the number of sample batch items to show.</p><p>Note the output of this cell. The <strong class="source-inline">text</strong> column shows the original text. The <strong class="source-inline">text_</strong> column shows the same text shifted one token ahead, that is, it starts one word after the original text and ends one word past the original text. Given a sample such as one of the entries in the <strong class="source-inline">text</strong> column, the language model will predict the next word, as shown in the <strong class="source-inline">text_</strong> column. We can see the output of <strong class="source-inline">show_batch()</strong> in the following screenshot:</p><div id="_idContainer110" class="IMG---Figure"><img src="image/B16216_4_1.jpg" alt="Figure 4.1 – Output of show_batch()&#13;&#10;"/></div><p class="figure-caption">Figure 4.1 – Output of show_batch()</p></li>
				<li>Run<a id="_idIndexMarker278"/> the following <a id="_idIndexMarker279"/>cell to define and train the deep learning model: <p class="source-code">learn = language_model_learner(\</p><p class="source-code">dls,AWD_LSTM, metrics=accuracy).to_fp16()</p><p class="source-code">learn.fine_tune(1, 1e-2)</p><p>Here are the arguments for the definition of the <strong class="source-inline">language_model_learner</strong> object:</p><p>a) <strong class="source-inline">dls</strong>: The <strong class="source-inline">TextDataLoaders</strong> object that is defined previously in this notebook.</p><p>b) <strong class="source-inline">AWD_LSTM</strong>: The pre-trained model to use as a basis for this model. This is the pre-trained language model incorporated with fastai that is trained with Wikipedia. If you are running this notebook on Colab, you can find the files that make up this model in the <strong class="source-inline">/root/.fastai/models/wt103-fwd</strong> directory after you have run this cell.</p><p>c) <strong class="source-inline">metrics</strong>: The performance metric to be optimized for the model, in this case, accuracy.</p><p>Here are the arguments for the <strong class="source-inline">fine_tune</strong> statement:</p><p>a) The epoch number (first argument) specifies the number of epochs, that is, the number of times the algorithm goes through the full training data during the training process.</p><p>b) The learning rate (second argument) specifies the learning rate for the training process. The <a id="_idIndexMarker280"/>learning <a id="_idIndexMarker281"/>rate is the rate at which the algorithm moves toward learning optimal parameters.</p><p class="callout-heading">Note</p><p class="callout">a) Depending on your environment, it may take more than an hour for this cell to run to completion. I strongly recommend that you use a for-pay GPU-enabled Gradient environment for this example and specify at least 3 hours for the instance to ensure that it completes in a reasonable time and that the instance doesn't shut down while this cell is running.</p><p class="callout">b) The <strong class="source-inline">language_model_learner</strong> definition includes a call to <strong class="source-inline">to_fp16()</strong> to specify mixed-precision training (summarized here: <a href="https://docs.fast.ai/callback.fp16.html#Learner.to_fp16">https://docs.fast.ai/callback.fp16.html#Learner.to_fp16</a>) to reduce the memory consumption of the training process and to prevent memory errors. Refer to the <em class="italic">There's more…</em> section for more details.</p><p>The output of the <strong class="source-inline">fine_tune</strong> statement shows the accuracy of the model and the time taken to complete the fine-tuning, as shown in the following screenshot:</p><div id="_idContainer111" class="IMG---Figure"><img src="image/B16216_4_2.jpg" alt="Figure 4.2 – Output of the fine_tune statement&#13;&#10;"/></div><p class="figure-caption">Figure 4.2 – Output of the fine_tune statement</p></li>
				<li>Run the following cell to exercise the language model you just trained:<p class="source-code">learn.predict("what comes next", n_words=20)</p><p>Here are the arguments for this invocation of the language model:</p><p>a) The input text sample <strong class="source-inline">"what comes next"</strong> (first argument) is the phrase that the model will complete. The language model will predict what words should follow this phrase.</p><p>b) <strong class="source-inline">n_words</strong>: This is the number of words that the language model is supposed to predict to complete the input phrase.</p><p>The following screenshot shows an example of what the model's prediction could look like:</p><div id="_idContainer112" class="IMG---Figure"><img src="image/B16216_4_3.jpg" alt="Figure 4.3 – The language model completes a phrase &#13;&#10;"/></div><p class="figure-caption">Figure 4.3 – The language model completes a phrase </p></li>
				<li>Run the<a id="_idIndexMarker282"/> following cell <a id="_idIndexMarker283"/>to save the model. You can update the cell to specify the directory and filename to which to save the model:<p class="source-code">learn.export('/notebooks/temp/models/lm_model_'+modifier)</p></li>
				<li>Run the following cell to save the current path value:<p class="source-code">keep_path = learn.path</p></li>
				<li>Run the following cell to assign a new value to the learner object path. The reason for doing this is that the default location for the model is not writeable on Gradient so you need to change the path value to a directory where you have write access:<p class="source-code">learn.path = Path('/notebooks/temp')</p></li>
				<li>Run the following cell to save the encoder subset of the model. This is the model minus the final layer. You will use this in the <em class="italic">Training a deep learning classification model with a curated text dataset</em> section when you train a text classifier:<p class="source-code">learn.save_encoder('ft_'+modifier)</p></li>
			</ol>
			<p>Congratulations! You have successfully applied transfer learning to train a language model on the curated IMDb dataset. Note that the idea of applying transfer learning to NLP like this was only described for the first time in 2018. Now, thanks to the fastai framework, with just a few lines of code, you can take advantage of a technique that didn't exist scarcely 4 years ago!</p>
			<h2 id="_idParaDest-110"><a id="_idTextAnchor114"/>How it works…</h2>
			<p>In this section, you have seen a simple example of how to train a language model with fastai deep learning using a curated text dataset. The language model is created by taking a model (<strong class="source-inline">AWD_LSTM</strong>) that has been pre-trained with the massive wiki dataset and then fine-tuning it using the IMDb dataset. </p>
			<p>By taking advantage of transfer learning in this way, we end up with a language model that combines a good degree of capability on general-purpose English (thanks to the model pre-trained on the wiki dataset) as well as the capability to produce text that is specific to the use case of <a id="_idIndexMarker284"/>movie<a id="_idIndexMarker285"/> reviews (thanks to the IMDb dataset).</p>
			<p>It's worthwhile to look a bit closer at the model in this recipe. A deeply detailed description of the model is beyond the scope of this book, so we will just focus on some highlights here. </p>
			<p>As shown in the recipe, the model is defined as a  <strong class="source-inline">language_model_learner</strong> object (documentation here: <a href="https://docs.fast.ai/text.learner.html#language_model_learner">https://docs.fast.ai/text.learner.html#language_model_learner</a>). This object is a specialization of the fastai <strong class="source-inline">learner</strong> object which you first saw in the <em class="italic">Understanding the world in four applications: tables, text, recommender systems</em> section, and in the  images of <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a><em class="italic">, Getting Started with fastai</em>. </p>
			<p>The model in the recipe is based on the predefined <strong class="source-inline">AWD_LSTM</strong> model (documentation here: <a href="https://docs.fast.ai/text.models.awdlstm.html#AWD_LSTM">https://docs.fast.ai/text.models.awdlstm.html#AWD_LSTM</a>). For this model, the output of <strong class="source-inline">learn.summary()</strong> shows only the high-level structure, including LSTM layers (fundamental to traditional NLP deep learning models) and dropout layers (used to reduce overfitting). Similarly, the output of <strong class="source-inline">learn.model</strong> for this model starts with layers for encoding (that is, transforming the input data to an intermediate representation used within the model) and ends with layers for decoding (that is, transforming the internal representation back to words).</p>
			<h2 id="_idParaDest-111">The<a id="_idTextAnchor115"/>re's more…</h2>
			<p>In this chapter, you will be working with some very large datasets, which means that you may need to take some extra steps to ensure that you don't run out of memory while you are preparing the datasets and training the model. Here, we'll describe some steps you can take to ensure that you train your fastai deep learning models on text datasets without running out of memory. We'll also go into more detail about how to save encoders in Gradient.</p>
			<h3>What happens if you run out of memory?</h3>
			<p>If you are trying<a id="_idIndexMarker286"/> to train a large model, you may get an out of memory message such as the following:</p>
			<p class="source-code">RuntimeError: CUDA out of memory. Tried to allocate 102.00 MiB (GPU 0; 7.93 GiB total capacity; 7.14 GiB already allocated; 6.50 MiB free; 7.32 GiB reserved in total by PyTorch)</p>
			<p>This message is saying that you have run out of memory on the GPU for your environment. What can you do if you encounter such a memory error? There are three steps you can take to get around this kind of memory error:</p>
			<ul>
				<li>Explicitly set the batch size.</li>
				<li>Use mixed-precision training.</li>
				<li>Ensure that you have only one notebook active at a time.</li>
			</ul>
			<h3>Memory error mitigation #1: Explicitly set the batch size</h3>
			<p>To explicitly <a id="_idIndexMarker287"/>set the batch size, you can restart the kernel for your notebook and then update the definition of the <strong class="source-inline">TextDataLoaders</strong> object to set the <strong class="source-inline">bs</strong> parameter, as shown here:</p>
			<p class="source-code">dls = TextDataLoaders.from_folder(untar_data(URLs.IMDB), valid='test', bs=16)</p>
			<p>Setting the <strong class="source-inline">bs</strong> parameter explicitly specifies a batch size (the number of items on which the average loss is calculated) other than the default of 64. By explicitly setting the batch size to a smaller value than the default, you limit the amount of memory consumed by each training epoch (that is, a complete iteration through the training data). When you set the value of <strong class="source-inline">bs</strong> explicitly like this, ensure that the value you set for the <strong class="source-inline">bs</strong> parameter is a multiple of 8.</p>
			<h3>Memory error mitigation #2: Mixed-precision training</h3>
			<p>Another technique <a id="_idIndexMarker288"/>that you can use to control memory <a id="_idIndexMarker289"/>consumption is using <strong class="bold">mixed-precision training</strong>. You can specify mixed-precision training by applying the <strong class="source-inline">to_fp16()</strong> function to the definition of the learner object, as shown here:</p>
			<p class="source-code">learn = language_model_learner(dls,AWD_LSTM,</p>
			<p class="source-code">drop_mult=0.5,metrics=accuracy).to_fp16()</p>
			<p>By specifying this call to <strong class="source-inline">to_fp16()</strong>, you allow the model to be trained using floating-point numbers that are less precise and are therefore expressed with less memory. The result is that the model training process consumes less memory. Refer to the fastai documentation for more details: <a href="https://docs.fast.ai/callback.fp16.html#Learner.to_fp16">https://docs.fast.ai/callback.fp16.html#Learner.to_fp16</a>.</p>
			<h3>Memory error mitigation #3: Stick to a single active notebook</h3>
			<p>Finally, another<a id="_idIndexMarker290"/> approach that you can take to prevent running out of memory is to run a single notebook at a time. On Gradient, for example, if you have multiple notebooks active at the same time, then you can exhaust your available memory. If you take the steps of setting a smaller batch size in the <strong class="source-inline">TextDataLoaders</strong> object and specifying <strong class="source-inline">to_fp16()</strong> in the learner object and still get memory errors, shut down the kernel of all the notebooks except the one you are currently working on. </p>
			<p>In JuptyerLab in Gradient, you can shut down the kernel for a notebook by right-clicking on the notebook in the navigation pane and selecting <strong class="bold">Shut Down Kernel</strong> from the menu, as shown in <a id="_idIndexMarker291"/>the following screenshot: </p>
			<div>
				<div id="_idContainer113" class="IMG---Figure">
					<img src="image/B16216_4_4.jpg" alt="Figure 4.4 – Shutting down a kernel in Gradient JupyterLab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – Shutting down a kernel in Gradient JupyterLab</p>
			<h3>Workaround to allow you to save encoders</h3>
			<p>In addition to<a id="_idIndexMarker292"/> the memory tips we've just reviewed, there is one more tip you need to know for text models if you are using Gradient. On Gradient, you may run into a situation where you are not able to save and retrieve interim objects in the directory where fastai wants to save them. </p>
			<p>For example, you need to save the encoder from the language model and then load that encoder when you train the text classifier. However, fastai forces you to save the encoder in the path for the dataset. The <strong class="source-inline">save_encoder</strong> function only lets you specify the unqualified file name, not the directory in which to save the encoder, as you can see in the following call to <strong class="source-inline">save_encoder</strong>: </p>
			<p class="source-code">learn.save_encoder('ft_'+modifier)</p>
			<p>At the same time, in Gradient, the directory for the IMDb dataset, <strong class="source-inline">/storage/data/imdb</strong>, is read-only. So, how can you save an encoder if the directory where it must be saved is not writeable? You can work around this problem by temporarily updating the learner's <strong class="source-inline">path</strong> object, saving the encoder in the directory indicated by this temporary <strong class="source-inline">path</strong> value, and then setting the <strong class="source-inline">path</strong> object back to its original value, as shown here:</p>
			<ol>
				<li value="1">Save the <a id="_idIndexMarker293"/>path value for your model:<p class="source-code">keep_path = learner.path</p></li>
				<li>Change the path value for your model to a directory that you have write access to, for example:<p class="source-code">learner.path = Path('/temp/models')</p></li>
				<li>Save the model.</li>
				<li>Change the path back to the original value:<p class="source-code">learner.path = keep_path</p></li>
			</ol>
			<h1 id="_idParaDest-112"><a id="_idTextAnchor116"/>Training a deep learning classification model with a curated text dataset</h1>
			<p>In the previous <a id="_idIndexMarker294"/>section, we<a id="_idIndexMarker295"/> trained a language model using the curated text IMDb dataset. The model in the previous section predicted the next set of words that would follow a given set of words. In this section, we will take the language model that was fine-tuned on the IMDb dataset and use it to train a text classification model that classifies text samples that are specific to the movie review use case.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor117"/>Getting ready</h2>
			<p>This recipe makes use of the encoder that you trained in the previous section, so ensure that you have followed the steps in the recipe in that section, in particular, that you have saved the encoder from the trained language model. </p>
			<p>As mentioned in the previous section, you need to take some additional steps before you can run recipes in Gradient that use the language model pre-trained on the Wikipedia corpus. To ensure that you have access to the pre-trained language model that you need to use in this recipe, complete the following steps if the setup of AWD_LSTM was interrupted:</p>
			<ol>
				<li value="1">In Colab, run the cells of the <strong class="source-inline">text_model_training.ipynb</strong> notebook up to and including the learner definition and training cell. Once you have done so, copy the contents of the <strong class="source-inline">/root/.fastai/models/wt103-fwd</strong> directory to a folder in your Drive environment.</li>
				<li>Upload the files you copied in the previous step to the <strong class="source-inline">/storage/models/wt103-fwd</strong> directory in your Gradient environment. </li>
			</ol>
			<p>With these steps, you will be able to run the notebook for this recipe (and other recipes that make use of <strong class="source-inline">AWD_LSTM</strong>) in Gradient.</p>
			<h2 id="_idParaDest-114"><a id="_idTextAnchor118"/>How to do it…</h2>
			<p>In this <a id="_idIndexMarker296"/>section, you will <a id="_idIndexMarker297"/>be running through the <strong class="source-inline">text_classifier_model.ipynb</strong> notebook to train a text classifier deep learning model using the <strong class="source-inline">IMDb</strong> curated dataset. Once you have the notebook open in Gradient, complete these steps:</p>
			<ol>
				<li value="1">Run the cells in the notebook up to the <strong class="source-inline">Define the text classifier</strong> cell.</li>
				<li>Run the following cell to define a <strong class="source-inline">TextDataLoaders</strong> object:<p class="source-code">dls_clas = TextDataLoaders.from_folder(\</p><p class="source-code">path, valid='test')</p><p>Here are the arguments for the definition of the <strong class="source-inline">TextDataLoaders</strong> object:</p><p>a) <strong class="source-inline">path</strong>: Defines the path of the dataset used to define the <strong class="source-inline">TextDataLoaders</strong> object</p><p>b) <strong class="source-inline">valid</strong>: Identifies the folder in the dataset's directory structure that will be used to assess the performance of the model: <strong class="source-inline">imdb/test</strong></p></li>
				<li>Run the following cell to see a sample of entries from a batch:<p class="source-code">dls_clas.show_batch(max_n=3)</p></li>
				<li>The output of <strong class="source-inline">show_batch()</strong> shows text samples along with the class (indicated in the <strong class="source-inline">category</strong> column). fastai knows that the class is encoded in the directory where the text sample is and correctly renders it in <strong class="source-inline">show_batch()</strong>, as seen in the following screenshot: <div id="_idContainer114" class="IMG---Figure"><img src="image/B16216_4_5.jpg" alt="Figure 4.5 – Output of show_batch()&#13;&#10;"/></div><p class="figure-caption">Figure 4.5 – Output of show_batch()</p></li>
				<li>Run<a id="_idIndexMarker298"/> the following<a id="_idIndexMarker299"/> cell to define the text classifier model:<p class="source-code">learn_clas = text_classifier_learner(dls_clas, AWD_LSTM, </p><p class="source-code">                                metrics=accuracy).to_fp16()</p><p>Here are the arguments for the definition of the <strong class="source-inline">text_classifier_learner</strong> object:</p><p>a) <strong class="source-inline">dls_clas</strong>: This is the <strong class="source-inline">TextDataLoaders</strong> object defined in the previous cell.</p><p>b) <strong class="source-inline">AWD_LSTM</strong>: This is the pre-trained model to use as a basis for this model. If you run this notebook in Colab, you can find the files that make up this model in the <strong class="source-inline">/root/.fastai/models/wt103-fwd</strong> directory after you have run this cell.</p><p>c) <strong class="source-inline">metrics</strong>: This is the performance metric to be optimized for the model, in this case, accuracy.</p></li>
				<li>You need to load the encoder that you saved as part of the recipe in the previous section. The first step is to set the path for the <strong class="source-inline">learn_clas</strong> object so that it is the path in which the encoder is saved by running the following cell. Ensure that the directory specified is the directory where you saved the encoder in the previous recipe:<p class="source-code">learn_clas.path = Path('/notebooks/temp')</p></li>
				<li>Run the following cell to load the encoder that you saved in the recipe in the <em class="italic">Training a deep learning language model with a curated text dataset</em> section to the <strong class="source-inline">learn_clas</strong> object:<p class="source-code">learn_clas = learn_clas.load_encoder('ft_'+modifier)</p></li>
				<li>Run<a id="_idIndexMarker300"/> the following<a id="_idIndexMarker301"/> cell to train the model:<p class="source-code">learn_clas.fit_one_cycle(5, 2e-2)</p><p>Here are the arguments for <strong class="source-inline">fit_one_cycle</strong>:</p><p>a) The argument epoch count (first argument) specifies that the training is run for <strong class="source-inline">5</strong> epochs. </p><p>b) The argument learning rate (second argument) specifies that the learning rate is equal to <strong class="source-inline">0.02</strong>.</p><p>The output of this cell shows the results of the training, including the accuracy and the time taken for each epoch, as shown in the following screenshot:</p><div id="_idContainer115" class="IMG---Figure"><img src="image/B16216_4_6.jpg" alt="Figure 4.6 – Results of training the text classification model&#13;&#10;"/></div><p class="figure-caption">Figure 4.6 – Results of training the text classification model</p></li>
				<li>Run the cells to get predictions on text strings that you expect to be negative and positive and observe whether the trained model makes the expected predictions, as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer116" class="IMG---Figure">
					<img src="image/B16216_4_7.jpg" alt="Figure 4.7 – Using the text classifier to get predictions on text strings&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – Using the text classifier to get predictions on text strings</p>
			<p>Congratulations! You <a id="_idIndexMarker302"/>have taken the <a id="_idIndexMarker303"/>language model that was fine-tuned on the IMDb dataset and used it to train a text classification model that classifies text samples that are specific to the movie review use case.</p>
			<h2 id="_idParaDest-115"><a id="_idTextAnchor119"/>How it works…</h2>
			<p>You can see the power of fastai by contrasting the code in this section, which defines a text classifier, with the code in the previous section, which defines a language model. There are only three differences in total:</p>
			<ol>
				<li value="1">In the definition of the <strong class="source-inline">TextDataLoaders</strong> object, the following applies:<p>a) The language model has the <strong class="source-inline">is_lm</strong> argument to indicate that the model is a language model.</p><p>b) The text classifier has the <strong class="source-inline">label_col</strong> argument to indicate which column in the dataset contains the category that is being predicted by the model. In the case of the text classifier defined in this section, the label for the dataset is encoded in the directory structure of the dataset rather than as a column in the dataset, so this parameter is not needed in the definition of the <strong class="source-inline">TextDataLoaders</strong> object.</p></li>
				<li>In the definition of the model, the following applies:<p>a) The language model defines a <strong class="source-inline">language_model_learner</strong> object.</p><p>b) The text classifier defines a <strong class="source-inline">text_classifier_learner</strong> object.</p></li>
				<li>In getting a prediction from the model, the following applies:<p>a) The language model takes two arguments for its call to <strong class="source-inline">learn.predict()</strong>, the string on which to make the prediction, and the number of words to predict.</p><p>b) The text classifier takes one argument for its call to <strong class="source-inline">learn.predict()</strong>, the string whose class the model will predict.</p></li>
			</ol>
			<p>With just these three differences, fastai takes care of all the underlying differences between a language model and a text classifier. </p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor120"/>There's more…</h2>
			<p>If you are using<a id="_idIndexMarker304"/> the Gradient <a id="_idIndexMarker305"/>environment and you are using a notebook with a cost, you will want to control how long the notebook is active to avoid paying for more time than you need. You can select the duration of your session when you start it up by selecting an hour value from the <strong class="bold">Auto-Shutdown</strong> menu, as shown in the following screenshot:</p>
			<div>
				<div id="_idContainer117" class="IMG---Figure">
					<img src="image/B16216_4_8.jpg" alt="Figure 4.8 – Selecting a duration for a Gradient notebook session&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 – Selecting a duration for a Gradient notebook session</p>
			<p>Suppose that you end up selecting more time than you need and you are done with your session before the auto-shutdown time limit is reached. Should you explicitly shut down the session? </p>
			<p>My experience has been that if you try to stop the instance in the Gradient notebook interface by selecting the <strong class="bold">Stop Instance</strong> button in the <strong class="bold">Instance</strong> view (as shown in <em class="italic">Figure 4.9</em>), you<a id="_idIndexMarker306"/> will risk putting<a id="_idIndexMarker307"/> your instance into a state where you cannot start it again easily:</p>
			<div>
				<div id="_idContainer118" class="IMG---Figure">
					<img src="image/B16216_4_9.jpg" alt="Figure 4.9 – Stop Instance button in the Instance view in Gradient&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – Stop Instance button in the Instance view in Gradient</p>
			<p>If you select <strong class="bold">Stop Instance</strong> and your instance gets into a state where you cannot start it again, then you will have to open a ticket with <em class="italic">Paperspace</em> support to fix your instance. After this happened to me a couple of times, I stopped using the <strong class="bold">Stop Instance</strong> button and just let the instance time out when I was finished working with it. You will save yourself time by never explicitly stopping your Gradient instance and instead just letting it time out when you are done with a session.</p>
			<h1 id="_idParaDest-117"><a id="_idTextAnchor121"/>Training a deep learning language model with a standalone text dataset</h1>
			<p>In the previous<a id="_idIndexMarker308"/> sections, we<a id="_idIndexMarker309"/> trained a language model and a text classifier using the curated text dataset IMDb. In this section and the next section, we will train a language model and a text classifier using a standalone text dataset, the <em class="italic">Kaggle Coronavirus tweets NLP – Text Classification</em> dataset described here: <a href="https://www.kaggle.com/datatattle/covid-19-nlp-text-classification">https://www.kaggle.com/datatattle/covid-19-nlp-text-classification</a>. This dataset includes a selection of tweets related to the Covid-19 pandemic, along with categorization for the tweets according to the following five categories:</p>
			<ul>
				<li>Extremely negative</li>
				<li>Negative</li>
				<li>Neutral</li>
				<li>Positive</li>
				<li>Extremely positive</li>
			</ul>
			<p>The goal of the language model trained on this dataset is to predict the subsequent words in a Covid-related tweet given a starting phrase. The goal of the text classification model trained on this dataset, as described in the <em class="italic">Training a deep learning text classifier with a standalone text dataset</em> section, is to predict which of the five categories a phrase belongs in.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor122"/>Getting ready</h2>
			<p>As mentioned in previous sections in this chapter, you need to take some additional steps before you can run this recipe in Gradient to ensure that you have access to the pre-trained language model that you will use in this recipe. If you have not done so already, follow these steps to prepare your Gradient environment if the setup of AWD_LSTM was interrupted:</p>
			<ol>
				<li value="1">In Colab, run the cells of the <strong class="source-inline">text_model_training.ipynb</strong> notebook up to and including the learner definition and training cell. Once you have done so, copy the contents of the <strong class="source-inline">/root/.fastai/models/wt103-fwd</strong> directory to a folder in your Drive environment.</li>
				<li>Upload the files you copied in the previous step to the <strong class="source-inline">/storage/models/wt103-fwd</strong> directory in your Gradient environment. </li>
			</ol>
			<p>With these steps, you should now be able to run the notebook for this recipe (and other recipes that make use of <strong class="source-inline">AWD_LSTM</strong>) in Gradient.</p>
			<p>Ensure that you have uploaded the files that make up the standalone Covid-related tweets dataset to your Gradient environment by following these steps:</p>
			<ol>
				<li value="1">Download the <strong class="source-inline">archive.zip</strong> file from <a href="https://www.kaggle.com/datatattle/covid-19-nlp-text-classification">https://www.kaggle.com/datatattle/covid-19-nlp-text-classification</a>.</li>
				<li>Unzip the downloaded <strong class="source-inline">archive.zip</strong> file to extract the <strong class="source-inline">Corona_NLP_test.csv</strong> and <strong class="source-inline">Corona_NLP_train.csv</strong> files.</li>
				<li>From<a id="_idIndexMarker310"/> the terminal in your <a id="_idIndexMarker311"/>Gradient environment, make <strong class="source-inline">/storage/archive</strong> your current directory:<p class="source-code"><strong class="bold">cd /storage/archive</strong></p></li>
				<li>Create the <strong class="source-inline">/storage/archive/covid_tweets</strong> directory:<p class="source-code"><strong class="bold">mkdir covid_tweets</strong></p></li>
				<li>Make <strong class="source-inline">/storage/archive/covid_tweets</strong> your current directory:<p class="source-code"><strong class="bold">cd /storage/archive/covid_tweets</strong></p></li>
				<li>Create <strong class="source-inline">test</strong> and <strong class="source-inline">train</strong> directories in <strong class="source-inline">/storage/archive/covid_tweets</strong>:<p class="source-code"><strong class="bold">mkdir test</strong></p><p class="source-code"><strong class="bold">mkdir train</strong></p></li>
				<li>Upload the files you extracted in <em class="italic">step 2</em> (<strong class="source-inline">Corona_NLP_test.csv</strong> and <strong class="source-inline">Corona_NLP_train.csv</strong>) to <strong class="source-inline">/storage/archive/covid_tweets</strong>. You can use the upload button in JupyterLab in Gradient to do the upload, but you need to do it in several steps:<p>a) From the terminal in your Gradient environment, make <strong class="source-inline">/notebooks</strong> your current directory:</p><p class="source-code"><strong class="bold">cd /notebooks</strong></p><p>b) If you have not already created a <strong class="source-inline">notebooks/temp</strong> directory, make a new <strong class="source-inline">/notebooks/temp</strong> directory:</p><p class="source-code"><strong class="bold">mkdir temp</strong></p><p>c) In the JupyterLab file browser, make <strong class="source-inline">temp</strong> your current folder, select the upload button (see <em class="italic">Figure 4.10</em>), and select the <strong class="source-inline">Corona_NLP_test.csv</strong> and <strong class="source-inline">Corona_NLP_train.csv</strong> files from your local system folder where you extracted them in <em class="italic">step 2</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer119" class="IMG---Figure">
					<img src="image/B16216_4_10.jpg" alt="Figure 4.10 – Upload button in JupyterLab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – Upload button in JupyterLab</p>
			<p>d) From the<a id="_idIndexMarker312"/> terminal in your<a id="_idIndexMarker313"/> Gradient environment, copy the <strong class="source-inline">Corona_NLP_test.csv</strong> file into the <strong class="source-inline">/storage/archive/covid_tweets/test</strong> directory:</p>
			<p class="source-code"><strong class="bold">cp /notebooks/temp/Corona_NLP_test.csv /storage/archive/covid_tweets/test/Corona_NLP_test.csv</strong></p>
			<p>e) Copy the <strong class="source-inline">Corona_NLP<a id="_idTextAnchor123"/>_train.csv</strong> file into the <strong class="source-inline">/storage/archive/covid_tweets/train</strong> directory:</p>
			<p class="source-code"><strong class="bold">cp /notebooks/temp/Corona_NLP_train.csv /storage/archive/covid_tweets/train/Corona_NLP_train.csv</strong></p>
			<p>Once you have completed the steps to upload the files that make up the Covid-related tweets dataset, you should have the following directory structure in the <strong class="source-inline">/storage/archive/covid_tweets</strong> directory in your Gradient environment:</p>
			<p class="source-code">├── test</p>
			<p class="source-code">│   └── Corona_NLP_test.csv</p>
			<p class="source-code">└── train</p>
			<p class="source-code">    └── Corona_NLP_train.csv</p>
			<p>With these preparation steps, you have brought the files that make up the dataset into the correct location in your Gradient environment to be used by a fastai model.</p>
			<p>I am grateful for the opportunity to include the Covid tweets dataset in this book and I would like to thank the curators of this dataset and Kaggle for making the dataset available.</p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">Aman Miglani (2020). <em class="italic">Coronavirus tweets NLP - Text Classification</em> (<a href="https://www.kaggle.com/datatattle/covid-19-nlp-text-classification">https://www.kaggle.com/datatattle/covid-19-nlp-text-classification</a>)</p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor124"/>How to do it…</h2>
			<p>In this section, you will be running through the <strong class="source-inline">text_standalone_dataset_lm.ipynb</strong> <a id="_idIndexMarker314"/>notebook to train a <a id="_idIndexMarker315"/>language model using the Covid-related tweets standalone dataset. Once you have the notebook open in Gradient, complete these steps:</p>
			<ol>
				<li value="1">Run the cells in the notebook up to the <strong class="source-inline">Ingest the dataset</strong> cell.</li>
				<li>Run the following cell to define a path object for the dataset: <p class="source-code">path = URLs.path('covid_tweets')</p><p class="callout-heading">Note</p><p class="callout">The argument for this path definition is the name of the root of the directory hierarchy in your Gradient environment into which you copied the CSV files for the dataset.</p></li>
				<li>Run the following cell to define a <strong class="source-inline">df_train</strong> dataframe to contain the contents of the <strong class="source-inline">Corona_NLP_train.csv</strong> file:<p class="source-code">df_train = pd.read_csv(path/'train/Corona_NLP_train.csv',</p><p class="source-code">                       encoding = "ISO-8859-1")</p><p>Here are the arguments for the definition of the dataframe:</p><p>a) The <strong class="source-inline">path/'train/Corona_NLP_train.csv'</strong> argument specifies the partially qualified filename for the training portion of the dataset.</p><p>b) The <strong class="source-inline">encoding = "ISO-8859-1"</strong> argument specifies the encoding to use for the file. This encoding is selected to ensure that the content of the CSV file can be ingested into a dataframe without any errors.</p></li>
				<li>Run the following cell to define a <strong class="source-inline">TextDataLoaders</strong> object:<p class="source-code">dls = TextDataLoaders.from_df(df_train, path=path, </p><p class="source-code">                             text_col='OriginalTweet',</p><p class="source-code">                             is_lm=True)</p><p>Here are the arguments for the definition of the <strong class="source-inline">TextDataLoaders</strong> object:</p><p>a) <strong class="source-inline">df_train</strong>: The dataframe that you created in the previous step.</p><p>b) <strong class="source-inline">path</strong>: The path object for the dataset.</p><p>c) <strong class="source-inline">text_col</strong>: The column in the dataframe containing the text that will be used to train the model. For this dataset, the <strong class="source-inline">OriginalTweet</strong> column contains the text used to train the model.</p><p>d) <strong class="source-inline">is_lm</strong>: An indicator that this model is a language model.</p></li>
				<li>Run the <a id="_idIndexMarker316"/>following <a id="_idIndexMarker317"/>cell to define and train the deep learning model with a <strong class="source-inline">language_model_learner</strong> object: <p class="source-code">learn = language_model_learner(dls,AWD_LSTM,</p><p class="source-code">                           metrics=accuracy).to_fp16()</p><p class="source-code">learn.fine_tune(1, 1e-2)</p><p class="callout-heading">Note</p><p class="callout">The definition of the <strong class="source-inline">language_model_learner</strong> object includes the call to <strong class="source-inline">to_fp16()</strong> to specify mixed-precision training (summarized here: <a href="https://docs.fast.ai/callback.fp16.html#Learner.to_fp16">https://docs.fast.ai/callback.fp16.html#Learner.to_fp16</a>) to reduce the memory consumption of the training process.</p><p>Here are the arguments for the definition of the <strong class="source-inline">language_model_learner</strong> object:</p><p>a) <strong class="source-inline">dls</strong>: The <strong class="source-inline">TextDataLoaders</strong> object that you defined in the previous step.</p><p>b) <strong class="source-inline">AWD_LSTM</strong>: The pre-trained model to use as a basis for this model. This is the pre-trained language model incorporated with fastai that is trained with Wikipedia.</p><p>c) <strong class="source-inline">metrics</strong>: The performance metric to be optimized for the model, in this case, accuracy.</p><p>Here are the arguments for the <strong class="source-inline">fine_tune</strong> statement:</p><p>a) The epoch count argument (first argument) specifies the number of epochs for the training process.</p><p>b) The learning rate argument (second argument) specifies the learning rate for the training process.</p><p>The results of the training process, as shown in <em class="italic">Figure 4.11</em>, are displayed once the <strong class="source-inline">fine_tune</strong> statement has been run:</p><div id="_idContainer120" class="IMG---Figure"><img src="image/B16216_4_11.jpg" alt="Figure 4.11 – Results of the training process&#13;&#10;"/></div><p class="figure-caption">Figure 4.11 – Results of the training process</p></li>
				<li>Run the <a id="_idIndexMarker318"/>following cell to <a id="_idIndexMarker319"/>exercise the trained language model:<p class="source-code">learn.predict("what comes next", n_words=20)</p><p>The results are displayed as follows:</p><div id="_idContainer121" class="IMG---Figure"><img src="image/B16216_4_12.jpg" alt="Figure 4.12 – Prediction of a language model trained on a standalone text dataset&#13;&#10;"/></div><p class="figure-caption">Figure 4.12 – Prediction of a language model trained on a standalone text dataset</p></li>
				<li>Run the following cell to save the model. You can update the cell to specify the directory and filename to which to save the model:<p class="source-code">learn.export('/notebooks/temp/models/lm_model_standalone'+modifier)</p></li>
				<li>Run the following cell to save the current path value:<p class="source-code">keep_path = learn.path</p></li>
				<li>Run the following cell to assign a new value to the learner object path. The reason for doing this is that the default location for the model is not writeable on Gradient, so you need to change the path value to a directory where you have write access:<p class="source-code">learn.path = Path('/notebooks/temp')</p></li>
				<li>Run the following cell to save the encoder subset of the language model. This is the model minus the final layer. You will use this encoder in the next recipe when you train a text classifier on the Covid-related tweets standalone dataset:<p class="source-code">learn.save_encoder('ft_standalone'+modifier)</p></li>
			</ol>
			<p>Congratulations! You used fastai to do transfer learning on top of an existing model with the Covid-related tweets standalone dataset to create a language model fine-tuned on that dataset. In the next section, you will use the encoder that you saved in the last step to fine-tune a text classifier trained on the standalone dataset.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor125"/>How it works…</h2>
			<p>In this section, you have seen a simple example of how to train a language model with fastai using <a id="_idIndexMarker320"/>a standalone text <a id="_idIndexMarker321"/>dataset. The language model is created by taking an existing model (<strong class="source-inline">AWD_LSTM</strong>) that has been trained with the massive Wikipedia dataset and then fine-tuning it using the standalone Covid-related tweets dataset. </p>
			<p>By taking advantage of transfer learning in this way, we end up with a language model that combines a good degree of capability in terms of general-purpose English (thanks to the model pre-trained on the wiki dataset) as well as the capability to produce text that is specific to the use case of social media related to the Covid-19 pandemic (thanks to the Covid tweets dataset). By following the recipe in this section, you can take advantage of fastai to apply this approach (transfer learning for NLP) on other text datasets for other use cases.</p>
			<h1 id="_idParaDest-121"><a id="_idTextAnchor126"/>Training a deep learning text classifier with a standalone text dataset</h1>
			<p>In the <em class="italic">Training a deep learning language model with a standalone text dataset</em> section, we trained a<a id="_idIndexMarker322"/> language <a id="_idIndexMarker323"/>model using the standalone text dataset: the Kaggle Coronavirus tweets NLP – Text Classification dataset described here: <a href="https://www.kaggle.com/datatattle/covid-19-nlp-text-classification">https://www.kaggle.com/datatattle/covid-19-nlp-text-classification</a>. In this section, we will use this language model to create a text classifier trained with the Covid-related tweets dataset.</p>
			<h2 id="_idParaDest-122"><a id="_idTextAnchor127"/>Getting ready</h2>
			<p>This recipe makes use of the encoder that you trained in the <em class="italic">Training a deep learning language model with a standalone text dataset</em> section, so ensure that you have followed the steps in the recipe in that section. In particular, ensure that you have saved the encoder from the language model you trained in the previous section. </p>
			<p>Also, make sure you have followed all the steps from the <em class="italic">Getting ready</em> sub-section of the previous section to ensure the following:</p>
			<ul>
				<li>That you have access to the <strong class="source-inline">AWD_LSTM</strong> model in your Gradient environment</li>
				<li>That you have uploaded the files (<strong class="source-inline">Corona_NLP_test.csv</strong> and <strong class="source-inline">Corona_NLP_train.csv</strong>) that <a id="_idIndexMarker324"/>make up the<a id="_idIndexMarker325"/> standalone Covid-related tweets dataset to your Gradient environment</li>
			</ul>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor128"/>How to do it…</h2>
			<p>In this section, you will be running through the <strong class="source-inline">text_standalone_dataset_classifier.ipynb</strong> notebook to train a text classifier deep learning model using the Covid-related tweets dataset. Once you have the notebook open in Gradient, perform the following steps:</p>
			<ol>
				<li value="1">Run the cells in the notebook up to the <strong class="source-inline">Ingest the dataset</strong> cell.</li>
				<li>Run the following cell to define a path object for the dataset. Note that the argument is the name of the root directory in your Gradient environment into which you copied the CSV files for the dataset:<p class="source-code">path = URLs.path('covid_tweets')</p></li>
				<li>Run the following cell to define a dataframe to contain the contents of the <strong class="source-inline">Corona_NLP_train.csv</strong> file (the training portion of the Covid-related tweets dataset):<p class="source-code">df_train = pd.read_csv(path/'train/Corona_NLP_t<a id="_idTextAnchor129"/>rain.csv',</p><p class="source-code">                       encoding = "ISO-8859-1")</p><p>Here are the arguments for the definition of the dataframe:</p><p>a) The <strong class="source-inline">path/'train/Corona_NLP_train.csv'</strong> argument specifies the partially qualified filename for the training portion of the dataset.</p><p>b) The <strong class="source-inline">encoding = "ISO-8859-1"</strong> argument specifies the encoding to use for the file. This encoding is selected to ensure that the content of the CSV file can be ingested into a dataframe without any errors.</p></li>
				<li>Run the following cell to define a <strong class="source-inline">TextDataLoaders</strong> object:<p class="source-code">dls = TextDataLoaders.from_df(df_train, path=path, text_col='OriginalTweet',label_col='Sentiment')</p><p>Here are the arguments for the definition of the <strong class="source-inline">TextDataLoaders</strong> object:</p><p>a) <strong class="source-inline">df_train</strong>: The dataframe that you created in the previous step.</p><p>b) <strong class="source-inline">path</strong>: The path object for the dataset.</p><p>c) <strong class="source-inline">text_col</strong>: The column in the dataframe containing the text that will be used to train the model. For this dataset, the <strong class="source-inline">OriginalTweet</strong> column contains the text used to train the model.</p><p>d) <strong class="source-inline">label_col</strong>: The <a id="_idIndexMarker326"/>column in <a id="_idIndexMarker327"/>the dataframe containing the labels that the text classifier will predict.</p></li>
				<li>Run the following cell to see a batch from the <strong class="source-inline">TextDataLoaders</strong> object that you defined in the previous step:<p class="source-code">dls.show_batch(max_n=3)</p><p>The output of this statement, the <strong class="source-inline">text</strong> and <strong class="source-inline">category</strong> columns, will be as follows:</p><div id="_idContainer122" class="IMG---Figure"><img src="image/B16216_4_13.jpg" alt="Figure 4.13 – Batch from the Covid tweets dataset&#13;&#10;"/></div><p class="figure-caption">Figure 4.13 – Batch from the Covid tweets dataset</p></li>
				<li>Run the following cell to define the <strong class="source-inline">text_classifier_learner</strong> object for the text classifier model: <p class="source-code">learn_clas = text_classifier_learner(dls, AWD_LSTM, </p><p class="source-code">                           metrics=accuracy).to_fp16()</p><p class="callout-heading">Note</p><p class="callout">The definition of the <strong class="source-inline">text_classifier_learner</strong> object includes the call to <strong class="source-inline">to_fp16()</strong> to specify mixed-precision training (summarized here: <a href="https://docs.fast.ai/callback.fp16.html#Learner.to_fp16">https://docs.fast.ai/callback.fp16.html#Learner.to_fp16</a>) to reduce the memory consumption of the training process.</p><p>Here are the arguments for the definition of the <strong class="source-inline">text_classifier_learner</strong> object:</p><p>a) <strong class="source-inline">dls</strong>: The <strong class="source-inline">TextDataLoaders</strong> object that you defined in the previous step.</p><p>b) <strong class="source-inline">AWD_LSTM</strong>: The pre-trained model to use as a basis for this model. This is the pre-trained language model incorporated with fastai that is trained with Wikipedia.</p><p>c) <strong class="source-inline">metrics</strong>: The performance metric to be optimized for the model, in this case, accuracy.</p></li>
				<li>Run the <a id="_idIndexMarker328"/>following cell to <a id="_idIndexMarker329"/>assign a new value to the learner object path. The reason for doing this is to set the path to match the directory where you saved the encoder in the recipe in the previous section:<p class="source-code">learn_clas.path = Path('/notebooks/temp')</p></li>
				<li>Run the following cell to load the encoder that you saved in the recipe in the <em class="italic">Training a deep learning language model with a standalone text dataset</em> section to the <strong class="source-inline">learn_clas</strong> object:<p class="source-code">learn_clas =\</p><p class="source-code">learn_clas.load_encoder('ft_standalone'+modifier)</p></li>
				<li>Run the following cell to reset the value of the learner object path:<p class="source-code">learn_clas.path = keep_path</p></li>
				<li>Run the following cell to train the model:<p class="source-code">learn_clas.fit_one_cycle(1, 2e-2)</p><p>Here are the arguments for <strong class="source-inline">fit_one_cycle</strong>:</p><p>a) The epoch count argument (first argument) specifies that the training is run for 1 epoch. </p><p>b) The learning rate argument (second argument) specifies that the learning rate is equal to 0.02.</p><p>The output of this cell (as shown in <em class="italic">Figure 4.14</em>) shows the results of the training:</p><div id="_idContainer123" class="IMG---Figure"><img src="image/B16216_4_14.jpg" alt="Figure 4.14 – Output of the text classifier training&#13;&#10;"/></div><p class="figure-caption">Figure 4.14 – Output of the text classifier training</p></li>
				<li>Run the cells <a id="_idIndexMarker330"/>to get predictions on <a id="_idIndexMarker331"/>text strings that you expect to be negative and positive and observe whether the trained model makes the expected predictions (as shown in <em class="italic">Figure 4.15</em>):</li>
			</ol>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B16216_4_15.jpg" alt="Figure 4.15 – Using the text classifier to get predictions on text strings&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.15 – Using the text classifier to get predictions on text strings</p>
			<p>Congratulations! You have taken advantage of the facilities of fastai to train a text classifier on a standalone dataset using transfer learning. </p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor130"/>How it works…</h2>
			<p>The code for the text classifier model for the standalone Covid-related tweets dataset has some differences from the code for the text classifier model for the curated IMDb text dataset. Let's examine some of these differences.</p>
			<p>For the IMDb dataset, the <strong class="source-inline">TextDataLoaders</strong> definition does not include a <strong class="source-inline">label_col</strong> parameter:</p>
			<p class="source-code">dls_clas = TextDataLoaders.from_folder(path, valid='test')</p>
			<p>By contrast, the <strong class="source-inline">TextDataLoaders</strong> definition for the standalone dataset includes both <strong class="source-inline">text_col</strong> and <strong class="source-inline">label_col</strong> parameters:</p>
			<p class="source-code">dls = TextDataLoaders.from_df(df_train, path=path, text_col='OriginalTweet',label_col='Sentiment')</p>
			<p>What's the reason for these differences? First, for the <strong class="source-inline">IMDb</strong> dataset, we use the <strong class="source-inline">from_folder</strong> variation of <strong class="source-inline">TextDataLoaders</strong> because the dataset is organized as a collection of individual <a id="_idIndexMarker332"/>text files <a id="_idIndexMarker333"/>whose class is encoded by the directory that the file is in. Here is the directory structure of the IMDb dataset:</p>
			<p class="source-code">├── test</p>
			<p class="source-code">│   ├── neg</p>
			<p class="source-code">│   └── pos</p>
			<p class="source-code">├── tmp_clas</p>
			<p class="source-code">├── tmp_lm</p>
			<p class="source-code">├── train</p>
			<p class="source-code">│   ├── neg</p>
			<p class="source-code">│   └── pos</p>
			<p class="source-code">└── unsup</p>
			<p>Consider one file from the IMDB dataset, <strong class="source-inline">train/pos/9971_10.txt</strong>:</p>
			<p><em class="italic">This film was Excellent, I thought that the original one was quiet mediocre. This one however got all the ingredients, a factory 1970 Hemi Challenger with 4 speed transmission that really shows that Mother Mopar knew how to build the best muscle cars! I was in Chrysler heaven every time Kowalski floored that big block Hemi, and he sure did that a lot :)</em></p>
			<p>How does fastai know the class of this review when we train the text classifier model? It knows because this file is in the <strong class="source-inline">/pos</strong> directory. Thanks to the flexibility of fastai, we simply have to pass the <strong class="source-inline">path</strong> value to the definition of the <strong class="source-inline">TextDataLoaders</strong> object and the fastai framework figures out the category of each text sample in the dataset.</p>
			<p>Now, let's consider the standalone Covid-related tweets dataset. This dataset is packaged as CSV files that look like this:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B16216_4_16.jpg" alt="Figure 4.16 – Sample from the Covid-related tweets dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.16 – Sample from the Covid-related tweets dataset</p>
			<p>Unlike the IMDb dataset, the Covid-related tweets dataset is encapsulated in just two files (one for the training dataset and one for the test dataset). The columns of these files have the information that fastai needs to train the model:</p>
			<ul>
				<li>The text of the sample – in the <strong class="source-inline">OriginalTweet</strong> column</li>
				<li>The class (also known as the label) of the sample – in the <strong class="source-inline">Sentiment</strong> column</li>
			</ul>
			<p>To tell fastai how to interpret this dataset, we need to explicitly tell it which column of the dataset <a id="_idIndexMarker334"/>the text is in <a id="_idIndexMarker335"/>and which column the class (or label) is in the definition of the <strong class="source-inline">TextDataLoaders</strong> object, as follows:</p>
			<p class="source-code">dls = TextDataLoaders.from_df(df_train, path=path, text_col='OriginalTweet',label_col='Sentiment')</p>
			<p>The <strong class="source-inline">IMDb</strong> dataset is made up of thousands of individual text files spread across a complex set of directories that encode the class of each text file. By contrast, the Covid-related tweets dataset is made up of two CSV files that have the text samples and their classes as columns. Despite the differences in the organization of these two datasets, fastai can ingest them and prepare them to train a deep learning model with just a few tweaks to the definition of the <strong class="source-inline">TextDataLoaders</strong> object. fastai's ability to easily ingest datasets in a variety of different formats isn't just useful for text datasets; it is useful for all kinds of datasets. As you will see in <a href="B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 6</em></a>, <em class="italic">Training Models with Visual Data</em>, we really benefit from this ability when we deal with image datasets, which have many different kinds of organization.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor131"/>Test your knowledge</h1>
			<p>Now that you have worked through a number of extended examples of training fastai deep learning models with text datasets, you can try some variations to practice what you've learned. </p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor132"/>Getting ready</h2>
			<p>Ensure that you have followed the <em class="italic">Getting ready</em> steps from the <em class="italic">Training a deep learning text classifier with a standalone text dataset</em> section to prepare your Gradient environment and upload the Covid-related tweets dataset.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor133"/>How to do it…</h2>
			<p>You can follow the steps in this section to try some variations on the models that you trained with the Covid-related tweets dataset:</p>
			<ol>
				<li value="1">Make a copy of the <strong class="source-inline">text_standalone_dataset_lm.ipynb</strong> notebook that you worked through in the <em class="italic">Training a deep learning language model with a standalone text dataset</em> recipe. Give your new copy of the notebook the following name: <strong class="source-inline">text_standalone_dataset_lm_combo.ipynb</strong>.</li>
				<li>In your new notebook, in addition to creating a dataframe for the train CSV <strong class="source-inline">Corona_NLP_train.csv</strong> file, create a dataframe for the test CSV <strong class="source-inline">Corona_NLP_test.csv</strong> file by adding a cell to the notebook that looks like this:<p class="source-code">df_test = pd.read_csv(path/'test/Corona_NLP_test.csv ',encoding = "ISO-8859-1")</p></li>
				<li>Use the pandas <strong class="source-inline">concat</strong> function to combine the two dataframes into a new dataframe called <strong class="source-inline">df_combo</strong>:<p class="source-code">df_combo = pd.concat([df_train, df_test], axis=0)</p></li>
				<li>Now update the remainder of your new notebook to use <strong class="source-inline">df_combo</strong> instead of <strong class="source-inline">df_train</strong> and run the whole notebook to train a new language model. Do you notice any difference in the performance of the model?</li>
				<li>In most model training situations, you need to ensure that you don't use the test dataset to train the model. Can you think of why you could get away with using the test set to train a language model like this?</li>
			</ol>
			<p>Congratulations! You have completed your review of training fastai deep learning models on text datasets using fastai.</p>
		</div>
	</body></html>