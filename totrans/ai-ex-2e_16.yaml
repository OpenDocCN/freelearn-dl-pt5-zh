- en: '16'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Improving the Emotional Intelligence Deficiencies of Chatbots
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Emotions remain irrational and subjective. AI algorithms never forsake rationality
    and objectivity. Cognitive dissonance ensues, which complicates the task of producing
    an efficient chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 14*, *Preparing the Input of Chatbots with Restricted Boltzmann
    Machines (RBMs) and Principal Component Analysis (PCA)*, we built a rational chained
    algorithm process with an RBM and a PCA approach. From there, we extracted critical
    objective data on a market segment. From that market segment and its features,
    we then designed a dialog in *Chapter 15*, *Setting Up a Cognitive NLP UI/CUI
    Chatbot*. The dialog was rational, and we produced a probable choice of services
    for the user. We did this out of good faith, to make the path from a request to
    its outcome as short as possible. It was a *yes* path in which everything went
    smoothly.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will confront human nature with unexpected reactions. A
    *no* path will challenge our dialog. One of the problems we face resides in emotional
    polysemy, confusing emotional signals from a user.
  prefs: []
  type: TYPE_NORMAL
- en: We will address the *no* unexpected path with information drawn from *Chapter
    14*, *Preparing the Input of Chatbots with Restricted Boltzmann Machines (RBM)
    and Principal Component Analysis (PCA)* and *Chapter 15*, *Setting up a Cognitive
    NLP UI/CUI Chatbot*, and go into the world of data logging.
  prefs: []
  type: TYPE_NORMAL
- en: Data logging will provide critical contextual data to satisfy the user. The
    goal will be to create emotions, not just react randomly to a user's emotional
    state.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will open the door to researching ways to generate text automatically
    through RNN-LSTM approaches. The idea will be to create automatic dialogs in the future
    based on data logging.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Emotional polysemy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Small talk
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data logging
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating emotions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring RNN-LSTM approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will first explore the difference between simply reacting to emotions and
    creating emotions.
  prefs: []
  type: TYPE_NORMAL
- en: From reacting to emotions, to creating emotions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Designing a chatbot that reacts to what a user expresses is one thing. But creating
    emotions during a dialog like a human does requires deeper understanding of how
    a chatbot manages emotions. Let's start with emotional polysemy.
  prefs: []
  type: TYPE_NORMAL
- en: Solving the problems of emotional polysemy
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be enhancing the emotional intelligence of a chatbot starting by addressing
    the issue of emotional polysemy. We are used to defining polysemy with words,
    not emotions, in the sense that polysemy is the capacity of a word to have multiple
    meanings. In *Chapter 6*, *How to Use Decision Trees to Enhance K-Means Clustering*,
    we explored the confusion that arose with the word "coach." "Coach" can mean a
    bus or a sports trainer, which leads to English to French translation issues.
  prefs: []
  type: TYPE_NORMAL
- en: 'Polysemy also applies to the interpretation of emotions by artificial intelligence.
    We will explore this domain with two examples: greetings and affirmations.'
  prefs: []
  type: TYPE_NORMAL
- en: Then we will go through the speech recognition and facial analysis as silver
    bullet solutions fallacies that mislead us into thinking it's easy to read emotions
    on faces.
  prefs: []
  type: TYPE_NORMAL
- en: The greetings problem example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To implement this chapter, open Dialogflow and go to the agent named `cogfilm+<your
    unique ID>` created in *Chapter 15*, *Setting Up a Cognitive NLP UI/CUI Chatbot*.
  prefs: []
  type: TYPE_NORMAL
- en: Suppose somebody types "Hi" to a chatbot agent. Almost everybody will think
    that this is a good beginning. But is it really?
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s explore some of the many possible interpretations:'
  prefs: []
  type: TYPE_NORMAL
- en: '**"Hi" meaning the person is very tense and irritated**: This could be the
    case of a top manager who never uses "Hi" to say hello, does not like chatbots,
    or doubts that the one that is being tested is worth anything at all.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This manager usually says, "Good morning," "Good afternoon," and "Good evening."
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**"Hi" meaning "So what?"**: It is more like, "Yeah, hi." This could be a person,
    P, who dislikes person Q who just said good morning to P.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**"Hi," meaning "I''m in trouble."**: This could be a usually chirpy, happy
    person who says, "Hello, everyone. How are things going today?" But today, it''s
    just a brief "Hi." This will trigger alert reactions from others, such as "Are you
    okay?," "Is something wrong?"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**"Hi," meaning "I''m trying to be nice."**: This could be a person that is
    usually grumpy in the morning and just sits down and stares down a laptop until
    the caffeine in their coffee kicks in. But today, this person comes in totally
    in shape, wide awake, and says, "Hi." This might trigger alert reactions from others
    such as, "Somebody had a great evening or night! Am I wrong?", with some laughter
    from the others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I could go on with literally hundreds of other situations and uses of "Hi."
    Why? Because humans have an indefinite number of behaviors that can be reflected
    in that first "Hi" in an encounter.
  prefs: []
  type: TYPE_NORMAL
- en: This could apply to ending a conversation without saying "bye" or saying it
    in many ways. The way a person says goodbye to another person in the morning can have
    an incredible number of significations.
  prefs: []
  type: TYPE_NORMAL
- en: This is therefore one of our challenges. Before we go further with this, let's
    look at one more challenge by considering the affirmation example.
  prefs: []
  type: TYPE_NORMAL
- en: The affirmation example
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Suppose somebody types or says "Yes" in a chatbot. Does that really mean "Yes"?
  prefs: []
  type: TYPE_NORMAL
- en: '**"Yes", as in "Yeah, whatever."**: The user hates the chatbot. The dialog
    is boring. The user is thinking that if they do not say "Yes" and get it over
    with, this dialog will go on forever.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**"Yes", as in "I''m afraid to say no."**: The user does not want to say "Yes."
    The question could be, "Are you satisfied with your job?" The user could fear the
    answers are logged and monitored. The user fears sanctions. Although this person
    hates their job, the answer will be "Yes" or might even be "I sure do!"'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**"Yes" as a good faith "yes" that a person regrets right after**: A person
    says "Yes" to a purchase, stimulated by the ad pressure at that moment in the
    chatbot. But minutes later, the same person thinks, "Why did I say yes and buy
    that?" Therefore, some platforms allow refunds even before the product or service
    is delivered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just as for "Hi," I could list hundreds of situations of emotional polysemy
    with "Yes."
  prefs: []
  type: TYPE_NORMAL
- en: Now, that we have understood the challenge at hand, let's explore the silver
    bullet fallacies mentioned previously.
  prefs: []
  type: TYPE_NORMAL
- en: The speech recognition fallacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Many editors and developers believe that speech recognition will solve the problem
    of emotional intelligence by detecting the tone of a voice.
  prefs: []
  type: TYPE_NORMAL
- en: However, emotional polysemy applies to the tone of voice, as well. Human beings
    tend to hide their emotions when they feel threatened, and open up when they trust
    their environment.
  prefs: []
  type: TYPE_NORMAL
- en: Let's go back to the "Hi" and "Yes" examples.
  prefs: []
  type: TYPE_NORMAL
- en: '**"Hi" in a chirpy tone**: A person, X, comes into an office, for example.
    Somebody says, "Oh, hi there! Great to see you!" Person Y answers "Hi" in a very
    happy tone. Google Home or Amazon Alexa, in their research lab, produces 0.9 probability
    that the conversation is going well.'
  prefs: []
  type: TYPE_NORMAL
- en: This could be true. Or it could be false.
  prefs: []
  type: TYPE_NORMAL
- en: For example, person Y hates person X. Person X knows it and says, "Great to
    see you!" on purpose. Person Y knows that person X knows that they hate each other
    but won't give in to bursting out first. So person "Y" answers "Hi" in a super-happy
    tone.
  prefs: []
  type: TYPE_NORMAL
- en: At that point, many turn to facial analysis.
  prefs: []
  type: TYPE_NORMAL
- en: The facial analysis fallacy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Emotional polysemy also applies to facial analysis. Many think that deep learning
    facial analysis will solve the polysemy problem.
  prefs: []
  type: TYPE_NORMAL
- en: I saw a post recently by a developer with a picture of an obviously forced smile
    with the text stating that happiness could be detected with DL facial analysis!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take two basic facial expressions and explore them: a smile and a frown.
    By now, you know that emotional polysemy will apply to both cases.'
  prefs: []
  type: TYPE_NORMAL
- en: A smile
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If somebody smiles and a DL facial analysis algorithm detects that smile, it
    means the person is happy. Is this true? Maybe. Maybe not.
  prefs: []
  type: TYPE_NORMAL
- en: Maybe the person is happy. Maybe the smile is ironic, meaning "Yeah, sure, dream
    if you want, but I don't agree!" It could mean "Get out of my way," or, "I'm happy
    because I'm going to hurt you," or, "I'm happy to see you." Who knows?
  prefs: []
  type: TYPE_NORMAL
- en: The truth is that nobody knows, and sometimes even the person that smiles doesn't
    know. Sometimes a person will think, "Why did I smile at her/him? I hate her/him!"
  prefs: []
  type: TYPE_NORMAL
- en: A frown
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: If somebody frowns and a DL facial analysis algorithm detects that frown, it
    means the person is sad or unhappy. Is that true? Maybe. Maybe not.
  prefs: []
  type: TYPE_NORMAL
- en: Maybe the person is happy that day. Things are going smoothly, and the person
    just forgot a book, for example, at home before coming to this location. Maybe
    the second after the person will smile, thinking, "So what? It's a great day and
    I don't care!"
  prefs: []
  type: TYPE_NORMAL
- en: Maybe the person is unhappy. Maybe the person is having a great time watching
    some kind of ball game, and their favorite player missed something. The second
    after, the person thinks "Oh, so what? My team is winning anyway," and smiles.
    Some people just frown if they're thinking hard, but it doesn't mean they're unhappy.
  prefs: []
  type: TYPE_NORMAL
- en: We can now see that there are thousands of cases of emotional polysemy that
    occur with words, tone of voice, and facial expressions, and therefore there is
    no magical solution that is going to suddenly overcome the inherent difficulty
    that AI have when it comes to interpreting people's emotions.
  prefs: []
  type: TYPE_NORMAL
- en: We will now explore some realistic solutions to this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Small talk
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Small talk is not a silver bullet to solve the emotional intelligence problem
    of chatbots at all. In fact, even without speaking about chatbots, we all suffer
    from emotional distress in one situation or another. Small talk adds little unimportant
    phrases to a dialog such as "wow," "cool," "oops," "great," and more.
  prefs: []
  type: TYPE_NORMAL
- en: '*We do not need to seek perfection, but show goodwill*. Every human knows the
    difficulty of emotional intelligence and polysemy. A human can accept an error
    in a dialog if goodwill is shown by the other party to make up for that error.'
  prefs: []
  type: TYPE_NORMAL
- en: Small talk is a little step in making amends to show goodwill.
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve the "making customers happy" purpose, scroll down the main menu
    to **Small Talk**, click on that option, and enable it, as shown in the following
    screenshot. We will be focusing on **Courtesy** and **Emotions**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.1: Small Talk in menu'
  prefs: []
  type: TYPE_NORMAL
- en: Courtesy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Courtesy will help make a conversation smoother when things go wrong. Emotional
    intelligence is not answering 100% correctly every time.
  prefs: []
  type: TYPE_NORMAL
- en: '*Emotional intelligence (EI) is adjusting to an environment, correcting a mistake
    made, and trying to ease the tension at all times.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, click on **Enable**, which will trigger small talk responses during
    a dialog:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.2: Enabling Small Talk'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will notice that the **Courtesy** progress bar is at 0%. We need to increase
    EI:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.3: Small talk themes'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will carefully answer the first possible "question" a user can ask or a
    phrase they might express: **That''s bad.**'
  prefs: []
  type: TYPE_NORMAL
- en: We are in trouble here! This is the worst-case scenario. We are going to have
    to work hard to make up for this.
  prefs: []
  type: TYPE_NORMAL
- en: Emotional polysemy makes the situation extremely difficult to deal with. The
    one thing we do not want to do is to pretend our bot is intelligent.
  prefs: []
  type: TYPE_NORMAL
- en: 'I would recommend two courses of action:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, answer carefully, saying that we need to investigate this with something
    like:'
  prefs: []
  type: TYPE_NORMAL
- en: '*I am very sorry about this. Could you please describe why it''s bad? We will
    regularly check our history log and try to improve all the time. You can also
    send us an email at <your customer service email address>. We will answer as soon
    as possible.*'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can enter this answer as follows and click on the **SAVE** button:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.4: Courtesy'
  prefs: []
  type: TYPE_NORMAL
- en: You will notice the **Courtesy** progress bar has jumped up to 17%. We have
    covered a critical area of a dialog. Default answers are provided when we don't
    fill everything in, but they are random, which makes it better to enter your own
    phrases if you activate this function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now test the dialog by entering "That''s bad" in the test console at the top
    right. You will see the response appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.5: Default response'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you type "bad" instead of "That''s bad," it will work too, thanks to the
    ML functionality of Dialogflow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.6: Default response'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data logging** will tremendously help to boost the quality of a chatbot.'
  prefs: []
  type: TYPE_NORMAL
- en: We will explore data logging in the next section. But let's check our emotions
    first.
  prefs: []
  type: TYPE_NORMAL
- en: Emotions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will deal with the first reaction: **Ha ha ha!** If we go back to emotional
    polysemy issues, knowing the user can say this at any time, we are in trouble
    again!'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.7: Managing Emotions'
  prefs: []
  type: TYPE_NORMAL
- en: Is the user happy, or are they making fun of the chatbot? Who knows? Even with
    facial analysis and tone analysis, a quick "Ha ha ha!" is very difficult to interpret.
  prefs: []
  type: TYPE_NORMAL
- en: I would suggest a careful low-profile answer such as "Well, that's cheerful!",
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: This will get the user to think that the chatbot has a sense of humor. When
    you click on **SAVE**, the **Emotions** progress bar will jump up.
  prefs: []
  type: TYPE_NORMAL
- en: You will notice that beyond the variants Dialogflow detects, you can also enter
    variants directly in your responses. Also, if the user enters a phrase that is
    not in the dialog, there is a fallback intent in the intents list.
  prefs: []
  type: TYPE_NORMAL
- en: Small talk might make a dialog smoother, but it is only one of the components
    of emotional intelligence, in a chatbot or in everyday life.
  prefs: []
  type: TYPE_NORMAL
- en: Data logging will take us a step further.
  prefs: []
  type: TYPE_NORMAL
- en: Data logging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapter 15*, *Setting Up a Cognitive NLP UI/CUI Chatbot*, we took the context
    of a dialog into account using follow-up intents. However, even follow-up intents
    will not provide solutions to unexpected answers on the part of a user.
  prefs: []
  type: TYPE_NORMAL
- en: To enhance a dialog, data logging will create a long-term memory for the chatbot
    by remembering the key aspects of a dialog.
  prefs: []
  type: TYPE_NORMAL
- en: 'A user and a Dialogflow designer have to agree to the terms of the Google Dialogflow data
    logging features, as described on this page: [https://cloud.google.com/dialogflow/docs/data-logging](https://cloud.google.com/dialogflow/docs/data-logging).'
  prefs: []
  type: TYPE_NORMAL
- en: Privacy is a serious matter. However, you will notice that when you use a search
    engine for a given product, you end up viewing or receiving ads related to the
    search. This is data logging.
  prefs: []
  type: TYPE_NORMAL
- en: Making this decision depends on your goal and target audience. Suppose the user
    accepts the terms of the agreement. Now, data logging is activated. Then, data
    logging will provide the chatbot with long-term memory.
  prefs: []
  type: TYPE_NORMAL
- en: The rest of this chapter explores data logging, with the assumption of it having
    been clearly accepted by the user.
  prefs: []
  type: TYPE_NORMAL
- en: Google Cloud, like all chatbot platforms (Amazon, Microsoft, and others), offers
    logs to improve chatbots. Many functions, interfaces, and services provide great
    support to boost the quality of dialogs.
  prefs: []
  type: TYPE_NORMAL
- en: Data logging can drive cognitive-adaptive dialogs beyond speech recognition
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will explore one way of doing this through the history of a dialog. Go to
    **History**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.8: Dialog history option in the menu'
  prefs: []
  type: TYPE_NORMAL
- en: 'You will see a list of past conversations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.9: Dialog history'
  prefs: []
  type: TYPE_NORMAL
- en: Notice the **All platforms** list, which contains information for Google Assistant
    and other platforms. You can deploy your chatbot by clicking on **See how it works
    on Google Assistant** on the right-hand side of the screen. From there, you can
    follow the instructions and have it running on smartphones, Google Home, and elsewhere.
    Also, you will have advanced log data to improve the chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you tested "That''s bad" in the *Courtesy* section, the history of the interactions
    will be present:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.10: Chatbot interactions'
  prefs: []
  type: TYPE_NORMAL
- en: One way to know the username is to ask the user their name when an issue comes
    up. This can come in handy to customize a dialog. We can thus have a special dialog
    for this person or this category of persons. We can thus ask the person to state
    their name in their response with an email address, for example. When we analyze
    the data logs manually or with scripts in the **Fulfillment** section, we can
    track the problem down and improve the chatbot on a personal level.
  prefs: []
  type: TYPE_NORMAL
- en: Having completed the **Small Talk** sections and then activated the data log
    authorization for your use of data logging, we can proceed to create emotions.
    Google will continue to improve our chatbot with our data logging features.
  prefs: []
  type: TYPE_NORMAL
- en: If we know which user said what, we can improve the dialog, as we will see in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Creating emotions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When the user enters ambiguous responses involving emotional polysemy, it is
    difficult for a chatbot to consider the hundreds of possibilities described in
    the previous sections.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will focus on a user trying to obtain a service such as
    access to a movie on a streaming platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'An efficient chatbot should *create emotions in the user*. The most effective
    method is to:'
  prefs: []
  type: TYPE_NORMAL
- en: Generate *customer satisfaction*. Customer satisfaction is the ultimate emotion
    a chatbot should try to produce in a frictionless and expected dialog. If the
    customer is not satisfied with an answer, tensions and frustration will build
    up.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use functions such as the RBM-PCA approach of *Chapter 14*, *Preparing the Input
    of Chatbots with Restricted Boltzmann Machines (RBMs) and Principal Component
    Analysis (PCA)*, to suggest options that shorten the dialog path, thus its duration
    making the user "happy."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will now explore the *no* path of the dialog encountered in *Chapter 15,
    Setting Up a Cognitive NLP UI/CUI Chatbot*.
  prefs: []
  type: TYPE_NORMAL
- en: 'To access the *no* path of the dialog, go to **Intents**, click on the **choose_movie**
    intent and click on **Add follow-up intent**, and click on **no** in the drop-down
    menu:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.11: Adding a follow-up intent'
  prefs: []
  type: TYPE_NORMAL
- en: 'A **choose_movie - no** option should now appear:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.12: Follow-up options'
  prefs: []
  type: TYPE_NORMAL
- en: Click on **choose_movie - no**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google has entered several default "no" variants, as shown in the following
    screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.13: Dialogflow training phrases'
  prefs: []
  type: TYPE_NORMAL
- en: This "no" response comes as a surprise to the chatbot. In *Chapter 14*, this
    market segment was explored. Something has gone wrong!
  prefs: []
  type: TYPE_NORMAL
- en: The chatbot was working on a specific market segment, the "action" superhero
    fan type viewer. The answer being "no" means that we need to examine the other
    features available.
  prefs: []
  type: TYPE_NORMAL
- en: 'The features in *Chapter 14*, *Preparing the Input of Chatbots with Restricted
    Boltzmann Machines (RBMs) and Principal Component Analysis (PCA)*, in the `RBM.py`
    program were:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The "action" feature predicted so far groups several features:'
  prefs: []
  type: TYPE_NORMAL
- en: Action = {happiness, action, violence}
  prefs: []
  type: TYPE_NORMAL
- en: 'The following features were not taken into account:'
  prefs: []
  type: TYPE_NORMAL
- en: '{love, family, horizons}'
  prefs: []
  type: TYPE_NORMAL
- en: 'Since we want to keep the path short, we must find a way to ask a question
    that:'
  prefs: []
  type: TYPE_NORMAL
- en: Covers these three features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Can use an existing feature matrix for another marketing segment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The viewer also may have:'
  prefs: []
  type: TYPE_NORMAL
- en: Recently seen enough action movies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Progressively grown out of the superhero period of their life and be looking
    for other types of movies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In both cases, the viewer's market segment might overlap with another segment
    that contains family-love values.
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in the *Adding fulfillment functionality to an agent* section in
    *Chapter 15*, *Setting up a Cognitive NLP UI/CUI Chatbot*, we can use a script
    to:'
  prefs: []
  type: TYPE_NORMAL
- en: Cover these three features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use an existing feature matrix for another marketing segment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Classical marketing segments take age into account. Let''s continue in this
    direction and prepare for the possibility that the viewer, a young superhero fan,
    is growing a bit older and entering another age-movie-type segment that overlaps
    with the one used in `RBM.py` in *Chapter 14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We should add some love-family features in the matrix with the corresponding
    movies. We will then obtain another marketing segment. In the end, the chatbot
    will manage many marketing segments, which is the standard practice on many streaming
    platforms.
  prefs: []
  type: TYPE_NORMAL
- en: 'A variant of the chart in *Chapter 15*, *Setting Up a Cognitive NLP UI/CUI
    Chatbot*, could be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **MOVIE/FEATURE** | LOVE | HAPPINESS | FAMILY | HORIZONS | ACTION | VIOLENCE
    |'
  prefs: []
  type: TYPE_TB
- en: '| 24H in Kamba | 1 | 1 | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Lost | 1 | 1 | 0 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Cube Adventures | 1 | 0 | 0 | 0 | 0 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| A Holiday | 1 | 1 | 0 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Jonathan Brooks | 1 | 0 | 0 | 0 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| The Melbourne File | 1 | 1 | 0 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| WNC Detectives | 1 | 0 | 0 | 0 | 0 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Stars | 1 | 1 | 0 | 1 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Space II | 1 | 1 | 1 | 0 | 1 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Zone 77 | 1 | 0 | 0 | 1 | 1 | 1 |'
  prefs: []
  type: TYPE_TB
- en: 'This feature matrix contains a movie with the missing features from the previous
    matrix: Space II.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A streaming platform contains many marketing segments:'
  prefs: []
  type: TYPE_NORMAL
- en: '*M* = {*s*[1], *s*[2], … *s*[n]}'
  prefs: []
  type: TYPE_NORMAL
- en: Many of these marketing segments contain variants, merged features, combinations,
    and more.
  prefs: []
  type: TYPE_NORMAL
- en: 'Since data logging has been activated, from this point on we now have the following
    information:'
  prefs: []
  type: TYPE_NORMAL
- en: Whether this viewer has seen one of the several movies available in this marketing
    segment. This constitutes another tricky issue since some viewers may want to
    watch a movie again.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The viewer's new marketing segment.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Building a chatbot for a streaming platform will take months of designing with
    many build possibilities. For this example, we will focus on the age progression
    scenario, keep the dialog path as short as possible, and provide the following
    response:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Would you like to watch SPACE II? It''s a blockbuster with a family that has
    an adventure in space. There is some action but it''s mostly the story of a family
    that tries to survive in space."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Scroll down to the **Text Response** section and enter the response as follows,
    then click on **SAVE** to trigger the training process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.14: A look at the training process'
  prefs: []
  type: TYPE_NORMAL
- en: If the viewer answers "yes," then the dialog will lead to the movie's page.
    To continue in this direction, go back to *Chapter 15*, *Setting Up a Cognitive
    NLP UI/CUI Chatbot*, and a "yes" follow-up exchange to this part of the dialog
    as you wish.
  prefs: []
  type: TYPE_NORMAL
- en: We have added some emotional intelligence to the agent. We will now explore
    the future of chatbot architecture through text augmentation with **recurrent
    neural networks** (**RNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: An RNN can process sequential data such as sequences of words, events, and more.
  prefs: []
  type: TYPE_NORMAL
- en: RNN research for future automatic dialog generation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The future of chatbots lies in producing dialogs automatically, based on data
    logging dialogs, their cognitive meanings, the personal profile of a user, and
    more. As RNNs progress, we will get closer to this approach. There are many generative
    approaches that can produce automatic sequences of sounds and texts. Understanding
    an RNN is a good place to start.
  prefs: []
  type: TYPE_NORMAL
- en: An RNN model is based on sequences, in this case, words. It analyzes anything
    in a sequence, including images. To speed the mind-dataset process up, data augmentation
    can be applied here, exactly as it is to images in other models.
  prefs: []
  type: TYPE_NORMAL
- en: 'A first look at its graph data flow structure shows that an RNN is a neural
    network like the others previously explored. The following diagram shows a conceptual
    view of an RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_15.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.15: Data flow structure'
  prefs: []
  type: TYPE_NORMAL
- en: The *y* inputs (test data) go to the loss function (**Loss_Train**). The *x*
    inputs (training data) will be transformed through weights and biases into logits
    with a softmax function.
  prefs: []
  type: TYPE_NORMAL
- en: 'Looking at the RNN area of the graph shows the following **basic_lstm_cell**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_16.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.16: The basic_lstm_cell—the RNN area of the graph'
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM cell of an RNN contains "forget" gates that will prevent vanishing
    gradients when the sequences become too long for an RNN unit.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs at work
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'An RNN contains functions that take the output of a layer and feed it back
    to the input in sequences simulating time. This feedback process takes information
    in a sequence, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '*The* -> movie -> was -> **interesting** -> but -> I -> didn''t -> like ->
    *it*'
  prefs: []
  type: TYPE_NORMAL
- en: An RNN will unroll a stack of words into a sequence and parse a window of words to
    the right and the left. For example, in this sentence, an RNN can start with interesting
    (bold) and then read the words on the right and left (in italic). These are some
    of the hyperparameters of the RNN.
  prefs: []
  type: TYPE_NORMAL
- en: This sequence aspect opens the door to sequence prediction. Instead of recognizing
    a whole pattern of data at the same time, it is recognizing the sequence of data,
    as in this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'A network with no RNN will recognize the following vector as a week, a pattern
    just like any other:'
  prefs: []
  type: TYPE_NORMAL
- en: Monday
  prefs: []
  type: TYPE_NORMAL
- en: Tuesday
  prefs: []
  type: TYPE_NORMAL
- en: Wednesday
  prefs: []
  type: TYPE_NORMAL
- en: Thursday
  prefs: []
  type: TYPE_NORMAL
- en: Friday
  prefs: []
  type: TYPE_NORMAL
- en: Saturday
  prefs: []
  type: TYPE_NORMAL
- en: Sunday
  prefs: []
  type: TYPE_NORMAL
- en: 'An RNN will explore the same data in a sequence by unrolling streams of data:'
  prefs: []
  type: TYPE_NORMAL
- en: Monday -> Tuesday -> Wednesday -> Thursday -> Friday -> Saturday -> Sunday
  prefs: []
  type: TYPE_NORMAL
- en: The main difference lies in the fact that once trained, the network will predict
    the word that follows; if Wednesday is the input, Thursday could be one of the
    outputs. This is shown in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: RNN, LSTM, and vanishing gradients
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To simulate sequences and memory, an RNN and an LSTM will use backpropagation
    algorithms. An LSTM is an improved version of RNN in some cases.
  prefs: []
  type: TYPE_NORMAL
- en: An RNN often has problems with gradients when calculating them over deeper and
    deeper layers in the network. Sometimes, it vanishes (too close to 0) due to the sequence
    property, just like us when a memory sequence becomes too long.
  prefs: []
  type: TYPE_NORMAL
- en: The backpropagation (just like us with a sequence) becomes less efficient. There
    are many backpropagation algorithms, such as vanilla backpropagation, which is
    commonly used. This algorithm performs efficient backpropagation because it updates
    the weights after every training pattern.
  prefs: []
  type: TYPE_NORMAL
- en: One way to force the gradient not to vanish is to use a ReLU activation function,
    *f*(*x*) = max(0, *x*), forcing values on the model so that it will not get stuck.
  prefs: []
  type: TYPE_NORMAL
- en: Another way is to use an LSTM cell containing a forget gate between the input
    and the output cells, a bit like us when we get stuck in a memory sequence, and
    we say "whatever" and move on.
  prefs: []
  type: TYPE_NORMAL
- en: The LSTM cell will act as a memory gate with 0 and 1 values, for example. This
    cell will forget some information to have a fresh view of the information it has
    unrolled into a sequence. In recent TensorFlow versions (2.0 and above), you can
    choose to use RNN or LSTM units in a layer. Your choice will depend on several
    factors. The key factor is the behavior of the gradient. If it vanishes in the
    RNN units, you might want to improve your model or move to LSTM units.
  prefs: []
  type: TYPE_NORMAL
- en: The key idea of an RNN to bear in mind is that it unrolls information into sequences, remembering
    the past to predict the future. The main idea of an LSTM relies upon its "forget"
    gate, avoiding the vanishing gradient. In TensorFlow 2.x, the choice of RNN or
    LSTM units can be made in a few lines.
  prefs: []
  type: TYPE_NORMAL
- en: Let's run an example on Google Colaboratory.
  prefs: []
  type: TYPE_NORMAL
- en: Text generation with an RNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: To view the program, log into your Dialogflow account, upload `text_generation_tf2.ipynb`
    (located in the `CH16` directory in the GitHub repository of this book) to your
    Google Colaboratory environment, and save it in your drive, as explained in the
    *Getting started with Google Colaboratory* section in *Chapter 13*, *Visualizing
    Networks with TensorFlow 2.x and TensorBoard*.
  prefs: []
  type: TYPE_NORMAL
- en: This TensorFlow authors' program has been well designed for educational purposes.
    The program starts by setting up TensorFlow 2.x and the necessary libraries.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will thus focus on the main points of the program that you
    can then explore, run, and modify.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorizing the text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The main entry step to an RNN consists of taking the sequence of words, the
    strings, and converting them into a **numerical representation**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We obtain a numerical value for each character:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice that this "dictionary" can be interpreted in two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: character2number
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: integer2character
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The RNN will run its calculations but the predictions will come out in characters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, the program can take the first sequence of the loaded text and
    produce the mapped integers of the text as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this example, the result is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The RNN will run through numerical sequences, integer segments, or windows of the
    text to train and then make predictions. To do this, the program creates examples
    and targets as for all neural networks that have training batches.
  prefs: []
  type: TYPE_NORMAL
- en: Building the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Building neural networks with TensorFlow 2 has become so simple to write in
    a few lines that you can even miss seeing them in the example programs!
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s clarify some basic concepts before getting to those few lines:'
  prefs: []
  type: TYPE_NORMAL
- en: A **sequential** model contains a pile or stack of layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Embedding** takes the number of each character and stores it in a vector.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GRU** stands for gated recurrent unit. A GRU contains gates that manage hidden
    units, keeping some information and forgetting other information. An RNN GRU can
    sometimes get confused when the sequences become long and thus mismanage the gradient,
    which then disappears. The more efficient LSTM units are part of a recurrent network
    unit as well with feedback connections with a cell, an input gate, an output gate,
    and a forget gate. But in the end the choice of the types units will always be
    yours depending on the context of your project. In any case, the key concept to
    keep in mind is that recurrent networks manage sequences of data, keeping the
    past in mind while forgetting some information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **dense** layer, in this case, is the output layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A **timestep** is a predefined sequence length. In another model, it could be
    actual time if we are working on time-dependent data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A sequential model is built in three layers only:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'And that''s it! You can replace the basic `rnn_units` with an LSTM layer if
    the model requires it during the training phase. Once the model is built, the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: Looks an embedding up, as in a "dictionary."
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Runs the GRU for a timestep.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The dense layer will then generate **logits** (see *Chapter 2*, *Building a
    Reward Matrix – Designing Your Datasets*) to produce a prediction using a likelihood
    function, a probability distribution.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure of the TensorFlow author''s program sums the process up:'
  prefs: []
  type: TYPE_NORMAL
- en: '![A drawing of the data passing through the model](img/B15438_16_17.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.17: TensorFlow model'
  prefs: []
  type: TYPE_NORMAL
- en: Generating text
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After trying and training the model, the program will generate text automatically,
    for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'As you''ll notice, `ROMEO:` has been set up as the starting string. It then
    shows that the following predictions come from the initial text written by Shakespeare
    and are loaded at the beginning of the program:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You can go back to the beginning of the program and change the URL. Instead
    of loading Shakespeare, change it to your own text:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Before running the program, go to **Runtime** -> **Change runtime type**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.18: Runtime type'
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on **Change runtime type**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_16_19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 16.19: Notebook settings'
  prefs: []
  type: TYPE_NORMAL
- en: I recommend using the GPU. Also, verify that **Omit code cell output when saving
    this notebook** is not checked if you want to save your notebook with the results
    produced when you run the program.
  prefs: []
  type: TYPE_NORMAL
- en: You are now ready to explore and do your own research to contribute to the future
    of automatic text generation!
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Emotional polysemy makes human relationships rich and excitingly unpredictable.
    However, chatbots remain machines and do not have the ability to manage wide ranges
    of possible interpretations of a user's phrases.
  prefs: []
  type: TYPE_NORMAL
- en: Present-day technology requires hard work to get a cognitive NPL CUI chatbot
    up and running. Small talk will make the conversation smoother. It goes beyond
    being a minor feature; courtesy and pleasant emotional reactions are what make
    a conversation go well.
  prefs: []
  type: TYPE_NORMAL
- en: We can reduce the limits of present-day technology by creating emotions in the
    users through a meaningful dialog that creates a warmer experience. Customer satisfaction
    constitutes the core of an efficient chatbot. One way to achieve this goal is
    to implement cognitive functions based on data logging. We saw that when a user
    answers "no" when we expect "yes," the chatbot needs to adapt, exactly the way
    we humans do.
  prefs: []
  type: TYPE_NORMAL
- en: Cognitive data logging can be achieved through the preparation we explored in
    *Chapter 14*, *Preparing the Input of Chatbots with Restricted Boltzmann Machines
    (RBMs) and Principal Component Analysis (PCA)*, the cognitive dialog of *Chapter
    15*, *Setting Up a Cognitive NLP UI/CUI Chatbot*, and the adaptive dialog built
    in this chapter. In our example, the viewer changed market segments, and the chatbot
    logged the new profile. Dialogflow-fulfillment scripts can manage the whole adaptive
    process, though that is beyond the scope of this book.
  prefs: []
  type: TYPE_NORMAL
- en: We looked at the study of sequences of data through RNNs eventually leading
    to automatic dialogs. Chatbots, using cognitive approaches such as the RBM-PCA
    and the adaptive data logging inferences of this chapter, will one day build their
    own dialogs.
  prefs: []
  type: TYPE_NORMAL
- en: The following chapters will explore ways to achieve higher levels of artificial
    intelligence through genes, biological neurons, and qubits. The next chapter explores genetic
    algorithms and then implements them into a hybrid neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When a chatbot fails to provide a correct response, a hotline with actual humans
    needs to take over the conversation. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Small talk serves no purpose in everyday life or with chatbots. It is best to
    just get to the point. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data logging can be used to improve speech recognition. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The history of a chatbot agent's conversations will contain valuable information.
    (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Present-day technology cannot make use of the data logging of a user's dialogs.
    (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An RNN uses sequences of data to make predictions. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An RNN can generate the dialog flow of a chatbot automatically for all applications.
    (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Information on RNNs: [https://www.tensorflow.org/tutorials/recurrent](https://www.tensorflow.org/tutorials/recurrent)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'More on text generation: [https://www.tensorflow.org/tutorials/text/text_generation](https://www.tensorflow.org/tutorials/text/text_generation)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
