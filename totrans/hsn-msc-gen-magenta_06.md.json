["```py\nimport os\nimport tensorflow as tf\nfrom six.moves import urllib\n\ndef download_checkpoint(model_name: str,\n                        checkpoint_name: str,\n                        target_dir: str):\n  tf.gfile.MakeDirs(target_dir)\n  checkpoint_target = os.path.join(target_dir, checkpoint_name)\n  if not os.path.exists(checkpoint_target):\n    response = urllib.request.urlopen(\n      f\"https://storage.googleapis.com/magentadata/models/\"\n      f\"{model_name}/checkpoints/{checkpoint_name}\")\n    data = response.read()\n    local_file = open(checkpoint_target, 'wb')\n    local_file.write(data)\n    local_file.close()\n```", "```py\nfrom magenta.models.music_vae import TrainedModel, configs\n\ndef get_model(name: str):\n  checkpoint = name + \".tar\"\n  download_checkpoint(\"music_vae\", checkpoint, \"bundles\")\n  return TrainedModel(\n    # Removes the .lohl in some training checkpoints\n    # which shares the same config\n    configs.CONFIG_MAP[name.split(\".\")[0] if \".\" in name else name]\n    # The batch size changes the number of sequences \n    # to be run together\n    batch_size=8,\n    checkpoint_dir_or_path=os.path.join(\"bundles\", checkpoint))\n```", "```py\nfrom typing import List\nfrom magenta.protobuf.music_pb2 import NoteSequence\n\nfrom utils import save_midi, save_plot\n\ndef sample(model_name: str,\n           num_steps_per_sample: int) -> List[NoteSequence]:\n  model = get_model(model_name)\n\n  # Uses the model to sample 2 sequences\n  sample_sequences = model.sample(n=2, length=num_steps_per_sample)\n\n  # Saves the midi and the plot in the sample folder\n  save_midi(sample_sequences, \"sample\", model_name)\n  save_plot(sample_sequences, \"sample\", model_name)\n\n  return sample_sequences\n```", "```py\nnum_bar_per_sample = 2\nnum_steps_per_sample = num_bar_per_sample * DEFAULT_STEPS_PER_BAR\ngenerated_sample_sequences = sample(\"cat-drums_2bar_small.lokl\",\n                                    num_steps_per_sample)\n```", "```py\nsample_sequences = model.sample(n=2, length=64, same_z=True)\n```", "```py\n> curl --output \"checkpoints/cat-drums_2bar_small.lokl.tar\" \"https://storage.googleapis.com/magentadata/models/music_vae/checkpoints/cat-drums_2bar_small.lokl.tar\"\n> music_vae_generate --config=\"cat-drums_2bar_small\" --checkpoint_file=\"checkpoints/cat-drums_2bar_small.lokl.tar\" --mode=\"sample\" --num_outputs=\"2\" --output_dir=\"output/sample\"\n```", "```py\nTraceback (most recent call last):\n...\n  File \"/home/Packt/miniconda3/envs/magenta/lib/python3.5/site-packages/magenta/models/music_vae/trained_model.py\", line 224, in encode\n    (len(extracted_tensors.inputs), note_sequence))\nmagenta.models.music_vae.trained_model.MultipleExtractedExamplesError: Multiple (2) examples extracted from NoteSequence: ticks_per_quarter: 220\n```", "```py\nimport magenta.music as mm\n\ndef interpolate(model_name: str,\n                sample_sequences: List[NoteSequence],\n                num_steps_per_sample: int,\n                num_output: int,\n                total_bars: int) -> NoteSequence:\n  model = get_model(model_name)\n\n  # Use the model to interpolate between the 2 input sequences\n  interpolate_sequences = model.interpolate(\n      start_sequence=sample_sequences[0],\n      end_sequence=sample_sequences[1],\n      num_steps=num_output,\n      length=num_steps_per_sample)\n\n  save_midi(interpolate_sequences, \"interpolate\", model_name)\n  save_plot(interpolate_sequences, \"interpolate\", model_name)\n\n  # Concatenates the resulting sequences into one single sequence\n  interpolate_sequence = mm.sequences_lib.concatenate_sequences(\n      interpolate_sequences, [4] * num_output)\n\n  save_midi(interpolate_sequence, \"merge\", model_name)\n  save_plot(interpolate_sequence, \"merge\", model_name,               \n            plot_max_length_bar=total_bars,\n            bar_fill_alphas=[0.50, 0.50, 0.05, 0.05])\n\n  return interpolate_sequence\n```", "```py\nnum_output = 6\ntotal_bars = num_output * num_bar_per_sample\ngenerated_interpolate_sequence = \\\ninterpolate(\"cat-drums_2bar_small.hikl\",\n             generated_sample_sequences,\n             num_steps_per_sample,\n             num_output,\n             total_bars)\n```", "```py\n> curl --output \"checkpoints/cat-drums_2bar_small.hikl.tar\" \"https://storage.googleapis.com/magentadata/models/music_vae/checkpoints/cat-drums_2bar_small.hikl.tar\" > music_vae_generate --config=\"cat-drums_2bar_small\" --checkpoint_file=\"checkpoints/cat-drums_2bar_small.hikl.tar\" --mode=\"interpolate\" --num_outputs=\"6\" --output_dir=\"output/interpolate\" --input_midi_1=\"output/sample/SAMPLE_1.mid\" --input_midi_2=\"output/sample/SAMPLE_2.mid\"\n```", "```py\ndef groove(model_name: str,\n           interpolate_sequence: NoteSequence,\n           num_steps_per_sample: int,\n           num_output: int,\n           total_bars: int) -> NoteSequence:\n  model = get_model(model_name)\n\n  # Split the sequences in chunks of 4 seconds\n  split_interpolate_sequences = mm.sequences_lib.split_note_sequence(\n      interpolate_sequence, 4)\n\n  # Uses the model to encode the list of sequences\n  encoding, mu, sigma = model.encode(\n      note_sequences=split_interpolate_sequences)\n\n  # Uses the model to decode the encoding\n  groove_sequences = model.decode(\n      z=encoding, length=num_steps_per_sample)\n\n  groove_sequence = mm.sequences_lib.concatenate_sequences(\n      groove_sequences, [4] * num_output)\n\n  save_midi(groove_sequence, \"groove\", model_name)\n  save_plot(groove_sequence, \"groove\", model_name,\n            plot_max_length_bar=total_bars, show_velocity=True,\n            bar_fill_alphas=[0.50, 0.50, 0.05, 0.05])\n\n  return groove_sequence\n```", "```py\ngenerated_groove_sequence = groove(\"groovae_2bar_humanize\",\n                                   generated_interpolate_sequence,\n                                   num_steps_per_sample,\n                                   num_output,\n                                   total_bars)\n```", "```py\nnum_output = 10\nnum_steps_per_sample = num_bar_per_sample * DEFAULT_STEPS_PER_BAR\ntotal_bars = num_output * num_bar_per_sample\n\ngenerated_sample_sequences = sample(\"cat-mel_2bar_big\",\n                                    num_steps_per_sample)\ninterpolate(\"cat-mel_2bar_big\",\n            generated_sample_sequences,\n            num_steps_per_sample,\n            num_output,\n            total_bars)\n```", "```py\nsample(\"hierdec-trio_16bar\", num_steps_per_sample)\n```", "```py\nlstm_utils.rnn_cell(\n    [layer_size],\n    hparams.dropout_keep_prob,\n    hparams.residual_encoder,\n    is_training)\n```", "```py\ncell = rnn.LSTMBlockCell(rnn_cell_size[i])\ncell = rnn.DropoutWrapper(cell, input_keep_prob=dropout_keep_prob)\n```", "```py\nself._oh_encoder_decoder = mm.MultiDrumOneHotEncoding(\n    drum_type_pitches=[(i,) for i in range(num_classes)])\n```", "```py\nself._output_layer = layers_core.Dense(\n    output_depth, name='output_projection')\nself._dec_cell = lstm_utils.rnn_cell(\n    hparams.dec_rnn_size, hparams.dropout_keep_prob,\n    hparams.residual_decoder, is_training)\n```", "```py\nINFO:tensorflow:Building MusicVAE model with BidirectionalLstmEncoder, CategoricalLstmDecoder:\nINFO:tensorflow:Encoder Cells (bidirectional): units: [512]\nINFO:tensorflow:Decoder Cells: units: [256, 256]\n```", "```py\nmu = tf.layers.dense(\n    encoder_output,\n    z_size,\n    name='encoder/mu',\n    kernel_initializer=tf.random_normal_initializer(stddev=0.001))\nsigma = tf.layers.dense(\n    encoder_output,\n    z_size,\n    activation=tf.nn.softplus,\n    name='encoder/sigma',\n    kernel_initializer=tf.random_normal_initializer(stddev=0.001))\n\nreturn ds.MultivariateNormalDiag(loc=mu, scale_diag=sigma)\n```", "```py\nfor _ in range(int(np.ceil(n / batch_size))):\n  if self._z_input is not None and not same_z:\n    feed_dict[self._z_input] = (\n        np.random.randn(batch_size, z_size).astype(np.float32))\n  outputs.append(self._sess.run(self._outputs, feed_dict))\nsamples = np.vstack(outputs)[:n]\n```", "```py\nself._config.data_converter.to_items(samples)\n```", "```py\n_, mu, _ = self.encode([start_sequence, end_sequence], assert_same_length)\nz = np.array([_slerp(mu[0], mu[1], t)\n              for t in np.linspace(0, 1, num_steps)])\nreturn self.decode(\n    length=length,\n    z=z,\n    temperature=temperature)\n```", "```py\nencoding, _, _ = model.encode(split_interpolate_sequences)\ngroove_sequences = model.decode(encoding, num_steps_per_sample)\n```"]