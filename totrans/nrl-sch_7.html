<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer085">
<h1 class="chapter-number" id="_idParaDest-91"><a id="_idTextAnchor101"/>7</h1>
<h1 id="_idParaDest-92"><a id="_idTextAnchor102"/>Exploring Advanced Use Cases of Jina</h1>
<p>In this chapter, we discuss more advanced applications of the Jina neural search framework. Building on the concepts we have learned in the previous chapters, we will now look at what else we can achieve with Jina. We will examine multi-level granularity matches, querying while indexing, and a cross-modal example. These are challenging concepts in neural search and are required to achieve complex real-life applications. In particular, we will be covering these topics in this chapter:</p>
<ul>
<li>Introducing multi-level granularity </li>
<li>Cross-modal search with images with text</li>
<li>Concurrent querying and indexing data </li>
</ul>
<p>These cover a wide variety of real-life requirements of neural search applications. Using these examples, together with the basic examples in <a href="B17488_06.xhtml#_idTextAnchor085"><em class="italic">Chapter 6</em></a>, <em class="italic">Basic Practical Examples with Jina</em>, you can expand and improve your Jina applications to cover even more advanced usage patterns.</p>
<h1 id="_idParaDest-93"><a id="_idTextAnchor103"/>Technical requirements</h1>
<p>In this chapter, we will build and execute the advanced examples provided in the GitHub repository. The code is available at <a href="https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07">https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07</a>. Make sure to download this and navigate to each of the examples’ respective folders when following the instructions for how to reproduce the use cases. </p>
<p>To run this code, you will need the following:</p>
<ul>
<li>macOS, Linux, or Windows with WSL2 installed. Jina does not run on native Windows.</li>
<li>Python 3.7 or 3.8</li>
<li>Optionally, a clean new virtual environment for each of the examples</li>
<li>Docker</li>
</ul>
<h1 id="_idParaDest-94"><a id="_idTextAnchor104"/>Introducing multi-level granularity</h1>
<p>In<a id="_idIndexMarker481"/> this section, we will discuss how Jina can capture and leverage the hierarchical structure of real-life data. In order to follow along with the existing code, check the chapter’s code for a folder named <strong class="source-inline">multires-lyrics-search</strong>. This is the example we will be referring to in this section.</p>
<p>This example relies on the <strong class="source-inline">Document</strong> type’s capacity to hold chunks (child documents) and refer to a specific parent. Using this structure, you can compose advanced arbitrary level hierarchies of documents within documents. This mimics various real-life data-related problems. Examples could be patches of images, sentences of a paragraph, video clips of a longer movie, and so on.</p>
<p>See the following code for how to perform this with Jina’s <strong class="source-inline">Document</strong> API:</p>
<pre class="source-code">from jina import Document
 document = Document() 
chunk1 = Document(text='this is the first chunk') 
chunk2 = Document(text='this is the second chunk') 
document.chunks = [chunk1, chunk2]</pre>
<p>This can then be chained, with multiple levels of granularity, with each chunk having its own chunks. This becomes helpful when dealing with hierarchical data structures. For more information on the <strong class="source-inline">Document</strong> data type, you can check the <em class="italic">Understanding Jina components</em> section in <a href="B17488_04.xhtml#_idTextAnchor054"><em class="italic">Chapter 4</em></a>, <em class="italic">Learning Jina’s Basics</em>.</p>
<p>In this example, the dataset will be composed of lyrics from various popular songs. In this case, the granularity is based on linguistic concepts. The top level will be the entire contents of the body of a song’s lyrics. The level under that will be individual sentences extracted from the top-level body. This splitting is done using the <strong class="source-inline">Sentencizer</strong> Executor, which splits the long piece of text by looking for specific separator text tokens, such as <strong class="source-inline">.</strong> or <strong class="source-inline">,</strong>.</p>
<p>This application helps showcase the<a id="_idIndexMarker482"/> concept of <strong class="bold">chunking</strong> and its importance in search systems. This is important because, in order to get the best results in a neural search system, it is best to search with text inputs of the same length. Otherwise, the context-to-content ratio will be different between the data you are searching with and the data you have trained your model on. Once we have built the example, we can visualize how the system<a id="_idIndexMarker483"/> is matching input to output via a custom frontend.</p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor105"/>Navigating through the code</h2>
<p>Let’s now go through<a id="_idIndexMarker484"/> the logic of the app and the functions of each component. You can follow along with the code in the repository, at <a href="https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/multires-lyrics-search">https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/multires-lyrics-search</a>. I will explain the purpose and design of the main files in the folder.</p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor106"/>app.py</h2>
<p>This <a id="_idIndexMarker485"/>is the main entry point of the example. The user can use this script to either index (add) new data or search with their desired queries. For indexing data, this is done from the command line as follows:</p>
<p class="source-code">python -t app.py index</p>
<p>Instead of providing the <strong class="source-inline">index</strong> argument, you can also provide <strong class="source-inline">query</strong> or <strong class="source-inline">query_text</strong> as arguments. <strong class="source-inline">query</strong> starts the Flow to be used by an external REST API. You can then use the custom frontend provided in the repository to connect to this. <strong class="source-inline">query_text</strong> allows the user to search directly from the command line.</p>
<p>When indexing, the data is sequentially read from a <strong class="source-inline">CSV</strong> file. We also attach relevant tag information, such as author, song name, album name, and language, for displaying metadata in the interface. Tags can also be used by the user in whatever way they need. They were discussed in the <em class="italic">Accessing nested attributes from tags</em> subsection in the <em class="italic">Understanding Jina components</em> section in <a href="B17488_04.xhtml#_idTextAnchor054"><em class="italic">Chapter 4</em></a>, <em class="italic">Learning Jina’s Basics</em>.</p>
<h2 id="_idParaDest-97"><a id="_idTextAnchor107"/>index.yml</h2>
<p>This file<a id="_idIndexMarker486"/> defines the structure of the Flow used when indexing data (adding data). Following are the different configuration options provided in the file:</p>
<ul>
<li><strong class="source-inline">jtype</strong> informs the YAML parser about the class type of this object. In this case, it’s the <strong class="source-inline">Flow</strong> class. The YAML parser will then instantiate the class with the respective configuration parameters.</li>
<li><strong class="source-inline">workspace</strong> defines the default location where each Executor might want to store its data. Not all Executors require a workspace. This can be overridden by each Executor’s <strong class="source-inline">workspace</strong> parameter. </li>
<li><strong class="source-inline">executors</strong> is a list that defines the processing steps in this Flow. These steps are defined by specific classes, all of which are subclasses of the <strong class="source-inline">Executor</strong> class. </li>
</ul>
<p>The indexing Flow is represented by the following diagram:</p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<img alt=" Figure 7.1 – Index Flow showing document chunking " height="264" src="image/Figure_7.1_B17488.jpg" width="1624"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> Figure 7.1 – Index Flow showing document chunking</p>
<p>Notice how the data Flow is split at the gateway. The original document is stored as is in <strong class="source-inline">root_indexer</strong>, for future retrieval. On the other path, the document gets processed in order to extract its chunks, encode them, and then store them in the indexer.</p>
<p>Following are the different Executors used in this example:</p>
<ol>
<li>The first one is <strong class="source-inline">segmenter</strong>, which uses the <strong class="source-inline">Sentencizer</strong> class, from Jina Hub. We use the default configuration. This splits the body of the lyrics into sentences using a set of punctuation markers that usually delimit sentences, such as <strong class="source-inline">.</strong>, <strong class="source-inline">,</strong>, <strong class="source-inline">;</strong>, <strong class="source-inline">!</strong>. This is where the chunks are being created and assigned to their parent document, based on where these tokens are found in the text.</li>
<li>The next is <strong class="source-inline">encoder</strong>. This is the component in the Flow that transforms the sentence from text into a numeric format. The component uses the <strong class="source-inline">TransformerTorchEncoder</strong> class. It downloads the <strong class="source-inline">distilbert-base-cased</strong> model from the <strong class="source-inline">Huggingface</strong> API and uses it to encode the text itself into vectors, which can then be used for vector similarity computation. We will also define some configuration options here:<ul><li><strong class="source-inline">pooling_strategy: 'cls'</strong>: This is the pooling strategy that is used by the encoder.</li>
<li><strong class="source-inline">pretrained_model_name_or_path: distilbert-base-cased</strong>: This is the deep learning model that is used. It is pre-trained and downloaded at the start time by the Executor.</li>
<li><strong class="source-inline">max_length: 96</strong>: This indicates the maximum number of characters to encode from the sentence. Sentences longer than this limit get trimmed (the extra characters are removed).</li>
<li><strong class="source-inline">device: 'cpu'</strong>: This configuration instructs the Executor to run on the CPU. The Executor can also be run on the GPU (with <strong class="source-inline">'gpu'</strong>).</li>
<li><strong class="source-inline">default_traversal_paths: ['c']</strong>: This computes the embeddings on the chunk level. This represents the hierarchy level of the sentences extracted by <strong class="source-inline">segmenter</strong>. We only encode these, as we will perform the search matching at this level only. Matching the entire body of a song’s <a id="_idIndexMarker487"/>lyrics will not perform well, due to the amount of data a model needs to encode. </li>
</ul></li>
<li>We will now deep-dive into the actual storage engine, <strong class="source-inline">indexer</strong>. For this, we use the Executor called <strong class="source-inline">SimpleIndexer</strong>, again from Jina Hub. This uses the <strong class="source-inline">DocumentArrayMemmap</strong> class from Jina, to store the data on disk, but at the same time, load it into memory for reading and writing as needed, without consuming too much memory. We define the following configuration options for it:<ul><li><strong class="source-inline">default_traversal_paths: ['c']</strong>: These options configure the component to store the chunks of the documents. This has the same purpose as the previous usage of <strong class="source-inline">default_traversal_paths</strong>.</li>
</ul></li>
<li>Next is another indexer, <strong class="source-inline">root_indexer</strong>. This is part of the specific requirements of this example. Before, at <strong class="source-inline">indexer</strong>, we stored only the chunks of the document. But, at search time, we need to also retrieve the parent document itself, in order to obtain the tags associated with it (artist name, song name, and much more). As such, we need to store these documents somewhere. That is why we need this additional Executor. Usually, this will not be required, depending on your use case in your application. We define the following configuration options:<ul><li><strong class="source-inline">default_traversal_paths: ['r']</strong>: We define that we will index the root level of the document (i.e., not chunk-level)</li>
<li><strong class="source-inline">needs: [gateway]</strong>: This tells the Flow to send requests in parallel, to two separate paths: one is sent to the <strong class="source-inline">segmenter</strong> and <strong class="source-inline">encoder</strong> path, and the other is sent directly to <strong class="source-inline">root_indexer</strong>, since this one does not depend on any<a id="_idIndexMarker488"/> Executor in the other path</li>
</ul></li>
</ol>
<p>You will have noticed an additional argument that is repeated across some of the Executors, <strong class="source-inline">volumes</strong>. This conforms to the Docker syntax for mounting a local directory in the Docker container, in order to mount the workspace in the running Docker container.</p>
<h2 id="_idParaDest-98"><a id="_idTextAnchor108"/>query.yml</h2>
<p>This file<a id="_idIndexMarker489"/> defines the structure of the Flow used when querying data (searching data). This is different from the Flow configuration used at index time because the order of operations is different. Looking at the following diagram, we notice the main change is that the operations at query time are strictly sequential:</p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<img alt="Figure 7.2 – Query Flow showing document chunking " height="261" src="image/Figure_7.2_B17488.jpg" width="1604"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Query Flow showing document chunking</p>
<p>The<a id="_idIndexMarker490"/> matches are retrieved from <strong class="source-inline">indexer</strong>, which operates at the chunk level, as we previously defined. <strong class="source-inline">ranker</strong> then creates one single match for each parent ID present in the chunks. Finally, the original metadata of this parent match document is retrieved from <strong class="source-inline">root_indexer</strong> based on its ID. This is required in order to get the full context of the chunk (the parent’s full text contents and the name of the artist and song).</p>
<p>Just like the <strong class="source-inline">index.yml</strong> file, the <strong class="source-inline">query.yml</strong> file also defines a Flow with Executors. We will discuss their configuration choices, but we will only cover the differences from their equivalent in the <strong class="source-inline">index.yml</strong> file. If a parameter is not covered in this section, check the previous section. The following are the Executors defined in the query Flow:</p>
<ul>
<li><strong class="source-inline">segmenter</strong> is the same.</li>
<li><strong class="source-inline">encoder</strong> is also the same.</li>
<li><strong class="source-inline">indexer</strong> is also the same.</li>
</ul>
<ol>
<li value="5">The first new Executor is <strong class="source-inline">ranker</strong>. This performs a custom ranking and sorting of the results from the search. We use <strong class="source-inline">SimpleRanker</strong>, from Jina Hub. The only parameter here is <strong class="source-inline">metric: 'cosine'</strong>. This configures the class to use the <strong class="source-inline">cosine</strong> metric to base its ranking on. It works by aggregating the scores of a parent document’s chunks (children documents) into an overall score for the parent document. This is required to ensure that the matches are sorted in a meaningful way for the client (the frontend, REST API client, or command-line interface).</li>
<li>The last hop is <strong class="source-inline">root_indexer</strong>. Here, we change <strong class="source-inline">default_traversal_paths</strong> to <strong class="source-inline">['m']</strong>. This means that we want to retrieve the metadata of the matches of the document, not of the request document itself. This takes the document’s ID and performs a lookup for the metadata. As <a id="_idIndexMarker491"/>mentioned previously, <strong class="source-inline">indexer</strong> only stores the chunks of the document. We need to retrieve the full metadata of the chunks’ parent Document.</li>
</ol>
<h2 id="_idParaDest-99"><a id="_idTextAnchor109"/>Installing and running the example</h2>
<p>I will now guide <a id="_idIndexMarker492"/>you through installing and running this example application:</p>
<ol>
<li value="1">Make sure the requirements defined at the beginning of this chapter are fulfilled.</li>
<li>Clone the Git repository from <a href="https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina">https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina</a> and open a terminal in the example’s folder, at <strong class="source-inline">src/Chapter07/multires-lyrics-search</strong>.</li>
<li>Install the requirements: <p class="source-code"><strong class="bold">pip install -r requirements.txt</strong></p></li>
<li>Download the full dataset. This step is optional; you can skip this step and use the sample data provided:<ol><li>Begin by installing the Kaggle library if you haven’t already done so. You will also need to set up your API keys as explained here: <a href="https://github.com/Kaggle/kaggle-api#api-credentials%0D">https://github.com/Kaggle/kaggle-api#api-credentials:</a></li>
</ol><p class="source-code"><strong class="bold">pip install kaggle</strong></p><ol><li value="2">Running the following <strong class="source-inline">bash</strong> script should perform all the steps needed to download the full dataset:</li>
</ol><p class="source-code"><strong class="bold">bash get_data.sh</strong></p></li>
<li>The next step is to index the data. This step processes your data and stores it in the workspace of the Flow’s Executors:<p class="source-code"><strong class="bold">python app.py -t index</strong></p></li>
<li>Search your<a id="_idIndexMarker493"/> data. Here you have two options:<ul><li><strong class="source-inline">python app.py -t query_text</strong>: This option starts a command-line application. At some point, it will ask for a phrase as input. The phrase will be processed and then used as a search query. The results will be displayed in the terminal.</li>
<li><strong class="source-inline">python app.py -t query</strong>: This starts the application in server mode. It listens for incoming requests on the REST API and responds to the client with the best matches. </li>
</ul></li>
</ol>
<p>In the second mode, you can use the custom frontend we have built to explore the results. You can start the frontend by running the following commands in a terminal:</p>
<p class="source-code">cd static</p>
<p class="source-code">python -m http.server --bind localhost</p>
<p>Now you can open <a href="http://127.0.0.1:8000/">http://127.0.0.1:8000/</a> in your browser and you should see a web interface. In this interface, you can type your text in the left-side box. You will then get results on the right side. The <a id="_idIndexMarker494"/>matching chunks will be highlighted in the body of the lyrics.</p>
<p>Following is a screenshot of the interface:</p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<img alt="Figure 7.3 – Lyrics search engine example showing matching songs " height="744" src="image/Figure_7.3_B17488.jpg" width="1039"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Lyrics search engine example showing matching songs</p>
<p>For example, if you<a id="_idIndexMarker495"/> add the sentence <strong class="source-inline">I am very happy today</strong>, you should see a similar result. Each of these boxes you see on the right-hand side is a song in your dataset. Each highlighted sentence is a <em class="italic">match</em>. A match is a similar sentence, determined by how close two vectors are in embedding space.</p>
<p>Similarity can be adjusted using the breakdown slider on the left-hand side. As you move the slider to the right, you will see more matches appear. This is because we are increasing our radius in the vector space to find similar matches. </p>
<p>The relevance score you see at the bottom of the song box summarizes all the matches in a song. Each match has a numeric value between 0 and 1, determining how close it is to the original input in the vector space. The average of these match values is the relevance score. This means that a song with only good matches will be ranked as highly relevant. </p>
<p>The example also allows for more complex, multi-sentence queries. If you input two or three sentences when querying, the query Flow will break down the total input into individual “chunks.” These chunks in this example are sentences, but you can determine what a chunk is for your own data when building Jina.</p>
<p>In this section, we have covered how you can model the hierarchical structure of real-life data in the Jina framework. We use the <strong class="source-inline">Document</strong> class and its ability to hold chunks as our representation of this data. We have then built an example application that we can use to <a id="_idIndexMarker496"/>search through song lyrics, on the sentence level. This approach can be generalized to any text (or other modality) data application. In the next section, we will see how we can leverage a document’s modality in order to search for images with text.</p>
<h1 id="_idParaDest-100"><a id="_idTextAnchor110"/>Cross-modal search with images with text</h1>
<p>In this section, we will cover an advanced example showcasing <strong class="bold">cross-modal search</strong>. Cross-modal search<a id="_idIndexMarker497"/> is a subtype of neural search, where the data we index and the data we search with belong to different modalities. This is something that is unique to neural search, as none of the traditional search technologies could easily achieve this. This is possible due to the central neural search technology: all deep learning models fundamentally transform all data types to the same shared numeric representation of a vector (the embedding extracted from a specific layer of the network).</p>
<p>These <a id="_idIndexMarker498"/>modalities can be represented by different data types: audio, text, video, and images. At the same time, they can also be of the same type, but of different distributions. An example of this could be searching with a paper summary and wanting to get the paper title. They are both texts, but the underlying data distribution is different. The distribution is thus a modality as well in this case.</p>
<p>The purpose of the example in this section is to show how the Jina framework helps us to easily perform this sort of search. We highlight how the Flow can be used to split the data processing, depending on modalities, into two pipelines of Executors. This is done with the <strong class="source-inline">needs</strong> field, which defines the previously required step of an Executor. Chaining these <strong class="source-inline">needs</strong>, we can obtain separate paths.</p>
<p>Let’s now go through the logic of the app and what each file’s purpose is. The code can be found at <a href="https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina">https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina</a> in the folder <strong class="source-inline">src/Chapter07/cross-modal-search</strong>.</p>
<h2 id="_idParaDest-101"><a id="_idTextAnchor111"/>app.py</h2>
<p>This is the<a id="_idIndexMarker499"/> main entry point of the example. The user can call it to either <strong class="bold">index</strong> or <strong class="bold">search</strong>. It then creates the Flows and either indexes data or searches with the query from the user. </p>
<h2 id="_idParaDest-102"><a id="_idTextAnchor112"/>flow-index.yml</h2>
<p>This file defines <a id="_idIndexMarker500"/>the structure of the Flow used when indexing data (adding data). I will explain the different steps.</p>
<p>The Flow itself has the following arguments:</p>
<ul>
<li><strong class="source-inline">prefetch</strong> defines the number of documents to prefetch from the client’s request.</li>
<li><strong class="source-inline">workspace</strong> defines the default location where data will be stored. This can be overridden by each Executor’s <strong class="source-inline">workspace</strong> parameter.</li>
</ul>
<p>Then, the <strong class="source-inline">executors</strong> list defines the Executors used in this Flow. Each item in this list is an Executor and its configuration. </p>
<p>Following is a diagram representing the indexing Flow. Notice how the path bifurcates from the gateway, depending on whether the data is image or text:</p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<img alt="Figure 7.4 – Index Flow showing cross-modal features " height="278" src="image/Figure_7.4_B17488.jpg" width="1518"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Index Flow showing cross-modal features</p>
<p>We will describe the purpose of each of the Executors, grouped by paths. The first path is the path for image data:</p>
<ol>
<li value="1">The first <a id="_idIndexMarker501"/>Executor is <strong class="source-inline">image_loader</strong>. This uses the <strong class="source-inline">ImageReader</strong> class, defined locally in the <strong class="source-inline">flows/executors.py</strong> file. This will load the image files from a specific folder and pass them down further into the Flow for processing. When a document is created, we can assign it a <strong class="source-inline">mime</strong> type. This can then be used in specific Executors to perform custom logic. Here, we are using it to restrict which documents go to which Executors. </li>
</ol>
<p>The parameters are as follows:</p>
<ul>
<li> <strong class="source-inline">py_modules</strong>: This tells the Python process where to find extra classes that can then be used in the <strong class="source-inline">uses</strong> parameter.</li>
<li> <strong class="source-inline">needs</strong>: This creates a direct connection from the gateway (which is always the first and last hop of the Flow) to this Executor. It makes this component wait for requests from the gateway. This is required here because we want two separate paths for text and images.</li>
</ul>
<ol>
<li value="2">The next one is <strong class="source-inline">image_encoder</strong>. This is where the brunt of the work is done. Encoders are the Executors that transform data into a numeric representation. It uses <strong class="source-inline">CLIPImageEncoder</strong>, version 0.1. The parameters are as follows:<ul><li><strong class="source-inline">needs</strong>: This defines the path of the data on the image path</li>
</ul></li>
<li><strong class="source-inline">image_indexer</strong> is the storage for the embeddings and metadata of the documents that contain images. It uses <strong class="source-inline">SimpleIndexer</strong>. The parameters are as follows:<ul><li><strong class="source-inline">index_file_name</strong>: This defines the folder where the data is stored</li>
<li><strong class="source-inline">needs</strong>: This makes the Executor part of the image processing path, by explicitly making it depend on <strong class="source-inline">image_encoder</strong></li>
</ul></li>
<li>The next elements will be part of the text path. <strong class="source-inline">text_filter</strong> is similar to <strong class="source-inline">image_filter</strong>. It reads data, but only text-based documents. The parameters used here are as follows:<ul><li><strong class="source-inline">py_modules</strong>: This parameter again defines the files where the <strong class="source-inline">TextFilterExecutor</strong> is defined.</li>
<li><strong class="source-inline">needs: gateway</strong> defines the path of dependencies between the Executors. In this case, this Executor is at the beginning of the path and thus depends on <strong class="source-inline">gateway</strong>.</li>
</ul></li>
<li>Next, similar<a id="_idIndexMarker502"/> to the image path, we have the encoder <strong class="source-inline">text_encoder</strong>. This processes the text and encodes it using <strong class="source-inline">CLIPTextEncoder</strong>. The parameters used here are as follows:<ul><li><strong class="source-inline">needs: text_filter</strong>: This parameter specifies that this Executor is part of the text pat.</li>
</ul></li>
<li><strong class="source-inline">text_indexer</strong> stores the embeddings of the Executor.</li>
<li>Finally, we join the two paths. <strong class="source-inline">join_all</strong> joins the results from the two paths into one. The <strong class="source-inline">needs</strong> parameter here <a id="_idIndexMarker503"/>is given a list of Executor names.</li>
</ol>
<p>You will have noticed an argument that is repeated across some of the Executors:</p>
<p><strong class="source-inline">volumes</strong>: This is the Docker syntax for mounting a local directory into the Docker container.</p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor113"/>query.yml</h2>
<p>In this section, we <a id="_idIndexMarker504"/>will cover the query (search) Flow. This designates the process for searching the data you have indexed (stored) with the aforementioned index Flow. The configuration of the Executors is the same, at an individual level.</p>
<p>As can be seen from the following diagram, the Flow path is also similar. It also bifurcates at the start, depending on the data type:</p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<img alt="Figure 7.5 – Query Flow showing cross-modal features " height="272" src="image/Figure_7.5_B17488.jpg" width="1510"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – Query Flow showing cross-modal features</p>
<p>The difference is <a id="_idIndexMarker505"/>that now we are searching with documents across the two modalities. Thus, <strong class="source-inline">text_loader</strong> sends the documents with text to be encoded by <strong class="source-inline">text_encoder</strong>, but the actual similarity matching is done with image documents that have been stored in <strong class="source-inline">image_indexer</strong>, from the index Flow. This is the central aspect that allows us to achieve cross-modality searching in this example.</p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor114"/>Installing and running the example</h2>
<p>To run the example, do the following:</p>
<ol>
<li value="1">Make sure the<a id="_idIndexMarker506"/> requirements defined at the beginning of this chapter are fulfilled.</li>
<li>Clone the code from the repository at <a href="https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina">https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina</a> and open a terminal in the <strong class="source-inline">src/Chapter07/cross-modal-search</strong> folder.</li>
<li>Note that this example only includes two images as a sample dataset. In order to download the entire dataset and explore the results, you will need to download it from Kaggle. You can do so by registering for a free Kaggle account. Then, set up your API token. Finally, to download the <strong class="source-inline">flickr 8k</strong> dataset, run the following command in a terminal:<p class="source-code"><strong class="bold">bash get_data.sh </strong></p></li>
<li>To index the full dataset, run the following:<p class="source-code"><strong class="bold">python app.py -t index -d f8k -n 8000</strong></p></li>
<li>Starting the index Flow and indexing the sample data is done from the command line, like so:<p class="source-code"><strong class="bold">python app.py -t index</strong></p></li>
</ol>
<p>This creates the index Flow, processes the data in the specific folder, and stores it in a local folder, <strong class="source-inline">workspace</strong>.</p>
<ol>
<li value="6">Then, in <a id="_idIndexMarker507"/>order to start the searching Flow and allow the user to perform a search query, you can run this command:<p class="source-code"><strong class="bold">python app.py -t query</strong></p></li>
</ol>
<p>Let’s begin by<a id="_idIndexMarker508"/> running a small test query. This test query actually contains both an image and a text document. The text is the sentence <strong class="source-inline">a black dog and a spotted dog are fighting.</strong> The image is <strong class="source-inline">toy-data/images/1000268201_693b08cb0e.jpg</strong>. The system then searches with both the image and the text, in a cross-modal manner. This means the image is used to search across the text data and the text is used to search across the image data.</p>
<p>The text results from searching with the image will be printed in your terminal as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer082">
<img alt="Figure 7.6 – Cross-modal search terminal output " height="263" src="image/Figure_7.6_B17488.jpg" width="1319"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Cross-modal search terminal output</p>
<p>The image results will be shown in a <strong class="source-inline">matplotlib</strong> figure as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<img alt="Figure 7.7 – Cross-modal search plot output " height="394" src="image/Figure_7.7_B17488.jpg" width="521"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – Cross-modal search plot output</p>
<p>In this case, a<a id="_idIndexMarker509"/> lower score is better, as it measures the distance between the vectors.</p>
<p>You can pass your own image queries with the following:</p>
<p class="source-code">python app.py -t query --query-image path_to_your_image</p>
<p>The <strong class="source-inline">path_to_your_image</strong> variable can be provided as either an absolute or relative path, from the terminal’s current working directory path.</p>
<p>Or, for text, you can do it like so:</p>
<p class="source-code">python app.py -t query --query-text "your text"</p>
<p>In this section, we have covered how the Jina framework allows us to easily build a cross-modal search application. This is possible due to Jina’s universal and generalizable data types, mainly the document, and flexible pipeline construction process. We see that the <strong class="source-inline">needs</strong> parameter allows us to split the processing pipeline into two paths, depending on the <em class="italic">mime</em> type. In the following section, we will see how we can serve data while modifying it. </p>
<h1 id="_idParaDest-105"><a id="_idTextAnchor115"/>Concurrent querying and indexing data</h1>
<p>In this<a id="_idIndexMarker510"/> section, we will present the methodology for how to continuously serve your client’s requests while still being able to update, delete, or add new data to your database. This is a common requirement in the industry, but it is not trivial to achieve. The challenges here are around maintaining the vector index actualized with the most recent data, while also being able to update that data in an atomic manner, but also doing all these operations in a scalable, containerized environment. With the Jina framework, all of these challenges can be easily met and overcome. </p>
<p>By default, in a Jina Flow, you cannot both index data and search at the same time. This is due to the nature of the network protocol. In essence, each Executor is a single-threaded application. You can use sharding to extend the number of copies of an Executor that form an Executor group. However, this is only safe for purely parallel operations, such as encoding data. These sorts of operations do not affect the state of the Executor. On the other hand, <strong class="bold">CRUD</strong> (<strong class="bold">Create/Read/Update/Delete</strong>) are operations that affect the<a id="_idIndexMarker511"/> state. Generally, these are harder to parallelize in scalable systems. Thus, if you send a lot of data to index (to add) to your application, this will block all searching requests from your clients. This is, of course, highly limiting. In this solution, I will show how this can be tackled within Jina.</p>
<p>The key component of the solution is the <strong class="bold">HNSWPostgresIndexer</strong> Executor. This is an Executor for the Jina framework. It combines an in-memory HNSW vector database with a connection to a PostgreSQL database. The metadata of your documents is stored in the SQL database, while the embeddings are stored in RAM. Unlike the applications in the previous examples, it does not require two distinct Flows. All the CRUD operations are performed within one Flow life cycle. This is possible due to the Executor’s capacity to synchronize the states between the SQL database and its in-memory vector database. This can be configured to be done automatically or can be triggered manually at the desired time.</p>
<p>Let’s now delve into what each component of this example is doing. The code can be found at <a href="https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina">https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina</a> in the folder <strong class="source-inline">/src/Chapter07/wikipedia-sentences-query-while-indexing</strong>.</p>
<h2 id="_idParaDest-106"><a id="_idTextAnchor116"/>app.py</h2>
<p>This is the main entry<a id="_idIndexMarker512"/> point of the example. The user can call it to start the index and search Flows or to search documents. In order to start the Flows, you run <strong class="source-inline">app.py</strong> as follows:</p>
<p class="source-code">python app.py -t flow</p>
<p>This will initialize the Flow of the Jina application, with its Executors. It will then add new data to the <strong class="bold">HNSWPostgreSQL</strong> Executor, in batches of five documents at a time. This data is at first only inserted into the SQL database. This is because the SQL database is considered the primary source of data. The <strong class="bold">HNSW</strong> vector index will be gradually updated based on the data in the SQL database. Once there is data present, the Executor will automatically synchronize it into the HNSW vector index. This process continues until the data is fully inserted. Once one round has been completed, there will be data available for searching for the user. The user can then query the data with the following command:</p>
<p class="source-code">python app.py -t client</p>
<p>Then the user will be prompted for text input for a query. This text will then be encoded and compared with the existing dataset to get the best matches. These will be printed back to the terminal.</p>
<h2 id="_idParaDest-107"><a id="_idTextAnchor117"/>flow.yml</h2>
<p>This file defines the structure of the <a id="_idIndexMarker513"/>Flow used both when indexing data (adding data) and searching. I will explain the different options.</p>
<p>Following is the diagram of the index Flow. Notice that it is quite simple: we are just encoding and storing the encoded data. The complexity of this example application arises from the internal behavior of the <strong class="bold">HNSWPostgreSQL</strong> Executor.</p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<img alt="Figure 7.8 – Query Flow showing concurrency " height="173" src="image/Figure_7.8_B17488.jpg" width="1596"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Query Flow showing concurrency</p>
<p>The Flow itself has the <a id="_idIndexMarker514"/>following arguments:</p>
<ul>
<li><strong class="source-inline">protocol</strong>: Defines that the Flow should open its HTTP protocol to the exterior</li>
<li><strong class="source-inline">port_expose</strong>: Defines the port for listening on</li>
</ul>
<p>Then, the Executors define the steps in the Flow:</p>
<ul>
<li>The first one is <strong class="source-inline">storage_encoder</strong>. This uses <strong class="source-inline">FlairTextEncoder</strong> from Jina Hub. This encodes the text into a vector, for the linear algebra operations required in machine learning.</li>
<li>The second one is <strong class="source-inline">indexer</strong>. This uses <strong class="source-inline">HNSWPostgresIndexer</strong>, also from Jina Hub. The parameters used here are the following:<ul><li><strong class="source-inline">install_requirements</strong>: Setting this to <strong class="source-inline">True</strong> will install the libraries required for this Executor</li>
<li><strong class="source-inline">sync_interval</strong>: How many seconds to wait between automatically synchronizing the data from the SQL database into the vector database</li>
<li><strong class="source-inline">dim</strong>: The dimensionality of the embeddings</li>
</ul></li>
</ul>
<p>You will have noticed an additional argument that is repeated across some of the Executors:</p>
<ul>
<li><strong class="source-inline">timeout_ready</strong>: This defines the number of seconds to wait for an Executor to become available before it’s canceled. We set it to <strong class="source-inline">–1</strong> so we wait as long as it’s required. Depending on your scenario, this should be adjusted. For example, if you want to safely terminate a long-running downloading request, you can set it to whatever amount of seconds <a id="_idIndexMarker515"/>you want to wait for the Executor to start.</li>
</ul>
<h2 id="_idParaDest-108"><a id="_idTextAnchor118"/>Installing and running the example</h2>
<p>Before running this <a id="_idIndexMarker516"/>example, make sure you understand the basic text search from the previous chapter, the chatbot example. Also, you will need to install Docker on your computer: </p>
<ol>
<li value="1">Clone the Git repository from <a href="https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/wikipedia-sentences-query-while-indexing">https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter07/wikipedia-sentences-query-while-indexing</a> and open a terminal in the example’s folder.</li>
<li>Create a new Python 3.7 environment. Although it is not required, it is strongly recommended.</li>
<li>Install the requirements:<p class="source-code"><strong class="bold">pip install -r requirements.txt</strong></p></li>
</ol>
<p>The repository includes a small subset of the Wikipedia dataset, for quick testing. You can just use that. If you want to use the entire dataset, run <strong class="source-inline">bash</strong> <strong class="source-inline">get_data.sh</strong> and then modify the <strong class="source-inline">DATA_FILE</strong> constant (in <strong class="source-inline">app.py</strong>) to point to that file.</p>
<ol>
<li value="4">Then start the Flow with the following command:<p class="source-code"><strong class="bold">python app.py -t flow</strong></p></li>
</ol>
<p>This creates the<a id="_idIndexMarker517"/> Flow and establishes the data synchronizing loop, as described in <strong class="source-inline">app.py</strong> previously. </p>
<ol>
<li value="5">In order to<a id="_idIndexMarker518"/> query the data, run the following:<p class="source-code"><strong class="bold">python app.py -t client</strong></p></li>
</ol>
<p>You will then be prompted for some text input. Enter whatever query you wish. You will then get back the best matches for your query.</p>
<p>Since the Flows expose an HTTP protocol, you can query the REST API with the Jina Client, cURL, Postman, or the custom Swagger UI built within Jina. The Swagger UI can be reached at the URL informed by the Flow, in the terminal. Usually, it’s <strong class="source-inline">http://localhost:45678/docs</strong>, but it depends on your configured system.</p>
<p>In this section, we have learned how we can use the <strong class="source-inline">HNSWPostgreSQLIndexer</strong> Executor to concurrently index and search data in our live system. In the previous examples, the Flow needed to be redefined and restarted in order to switch between the two modes. Since this Executor combines both the metadata store (via a connection to a SQL database) and the embeddings index (via an in-memory HNSW index), it is possible to perform <a id="_idIndexMarker519"/>all CRUD operations within one Flow life cycle. Using these techniques, we can have a real client-facing application that is not blocked by the need to update the underlying database in the index.</p>
<h1 id="_idParaDest-109"><a id="_idTextAnchor119"/>Summary</h1>
<p>In this chapter, we have analyzed and practiced how you can use Jina’s advanced features, such as chunking, modality, and the advanced <strong class="source-inline">HNSWPostgreSQL</strong> Executor, in order to tackle the most difficult goals of neural search. We implemented solutions for arbitrary hierarchical depth data representation, cross-modality searching, and non-blocking data updates. Chunking allowed us to reflect on some data’s properties of having multiple levels of semantic meaning, such as sentences in a paragraph or video clips in longer films. Cross-modal searching opens up one of the main advantages of neural search – its data universality. This means that you can search with any data for any type of data, as long as you use the correct model for the data type. Finally, the <strong class="source-inline">HNSWPostgreSQL</strong> Executor allows us to build a live system where users can both search and index at the same time, with the data being kept in sync.  </p>
</div>
<div>
<div id="_idContainer086">
</div>
</div>
</div>
</body></html>