<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Machine Learning Basics</h1>
                </header>
            
            <article>
                
<p>Welcome to <em>Hands-On Deep Learning with R</em>! This book will take you through all of the steps that are necessary to code deep learning models using the R statistical programming language. It begins with simple examples as the first step for those just getting started, along with a review of the foundational elements of deep learning for those with more experience. As you progress through this book, you will learn how to code increasingly complex deep learning solutions for a wide variety of tasks. However, regardless of the complexity, each chapter will carefully detail each step. This is so that all topics and concepts can be fully comprehended and the reason for every line of code is completely explained.</p>
<p>In this chapter, we will go through a quick overview of the machine learning process as it will form a base for the subsequent chapters of this book. We will look at p<span>rocessing a dataset to review techniques such as handling outliers and missing values.</span> We will learn how to m<span>odel data to brush up on the process of predicting an outcome</span> <span>and </span><span>evaluating the results</span><span>, and we will also review the most suitable metrics for various problems.</span> <span>We will look at </span><span>improving a model using parameter tuning, feature engineering, and ensembling, and we will learn</span><span> when to use different machine learning algorithms based on the task to solve</span><span>.</span></p>
<p>This chapter will cover the following topics:</p>
<ul>
<li>An overview of machine learning</li>
<li>Preparing data for modeling</li>
<li>Training a model on prepared data</li>
<li>Evaluating model results</li>
<li>Improving model results</li>
<li>Reviewing different algorithms</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An overview of machine learning</h1>
                </header>
            
            <article>
                
<p>All deep learning is machine learning, but not all machine learning is deep learning. Throughout this book, we will focus on processes and techniques that are specific to deep learning in R. However, all the core principles of machine learning are essential to understand before we can move on to explore deep learning.</p>
<p>Deep learning is marked as a special subset of machine learning based on the use of neural networks that mimic brain activity behavior. The learning is referred to as being deep because, during the modeling process, the data is manipulated by a number of hidden layers. In this type of modeling, specific information is gathered from each layer. For example, one layer may find the edges of images while another finds particular hues.</p>
<p>Notable applications for this type of machine learning include the following:</p>
<ul>
<li>Image recognition (including facial recognition)</li>
<li>Signal detection</li>
<li>Recommendation systems</li>
<li>Document summarization</li>
<li>Topic modeling</li>
<li>Forecasting</li>
<li>Solving games</li>
<li>Moving an object through space, for example, self-driving cars</li>
</ul>
<p>All of these topics will be covered throughout the course of this book. All of these topics implement deep learning and neural networks, which are primarily used for classification and regression.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing data for modeling</h1>
                </header>
            
            <article>
                
<p>One of the benefits of deep learning is that it largely removes the need for feature engineering, which you may be used to with machine learning. That being said, the data still needs to be prepared before we begin modeling. Let's review the following goals to prepare data for modeling:</p>
<ul>
<li>Remove no-information and extremely low-information variables</li>
<li>Identify dates and extract date parts</li>
<li>Handle missing values</li>
<li>Handle outliers</li>
</ul>
<p>In this chapter, we will be investigating air quality data using data provided by the London Air Quality Network. Specifically, we will look at readings for nitrogen dioxide in the area of Tower Hamlets (Mile End Road) during 2018. This is a very small dataset with only a few features and approximately 35,000 observations. We are using a limited dataset here so that all of our code, even our modeling, runs quickly. That said, the dataset fits well for the process that we will explore. It requires some, but not an inordinate amount of, initial cleaning and preparation. In addition to this, it is suitable to use for decision tree-based modeling, which will be a useful form of machine learning to review as we start to apply deep learning models in future chapters.</p>
<p>Our first step will be to do some cursory data exploration to see what data cleaning and preparation steps will be necessary. R has some really helpful convenience packages for this type of exploratory data analysis. Let's do a quick review by looking at some important areas of exploratory data analysis using the following series of code blocks:</p>
<ol>
<li>We will start by loading our data and libraries. To do this, we will use the base R <kbd>library()</kbd> function to load all of the libraries that we will need. If there are any libraries listed that you do not have installed, use the <kbd>install.packages()</kbd> function to install these libraries. We will also use the <kbd>read_csv()</kbd> function from the <kbd>readr</kbd> package to load in the data. We load libraries and data using the following code:</li>
</ol>
<pre style="padding-left: 60px">library(tidyverse)<br/>library(lubridate)<br/>library(xgboost)<br/>library(Metrics) <br/>library(DataExplorer)<br/>library(caret)<br/>la_no2 &lt;- readr::read_csv("data/LondonAir_TH_MER_NO2.csv")</pre>
<p style="padding-left: 60px">Packages are included before the functions are called; therefore, this gives us an understanding of where and why each package is being used. As shown in the preceding code block, the following packages will be used in this chapter:</p>
<ul>
<li style="padding-left: 60px"><kbd>tidyverse</kbd>: This suite of packages will be used extensively. In this case, the <kbd>dplyr</kbd> package is used in this chapter for data wrangling, for instance, looking at aggregate values or adding and removing columns and rows.</li>
<li style="padding-left: 60px"><kbd>lubridate</kbd>: This will be used to easily extract details from a column holding values with a date data type.</li>
<li style="padding-left: 60px"><kbd>xgboost</kbd>: This will be the model that we will use for our data.</li>
<li style="padding-left: 60px"><kbd>Metrics</kbd>: This will be used to evaluate our model.</li>
<li style="padding-left: 60px"><kbd>DataExplorer</kbd>: This will be used for generating exploratory data analysis plots.</li>
<li style="padding-left: 60px"><kbd>caret</kbd>: This will be used when tuning our model as it provides a convenient method for performing a grid search of hyperparameters.</li>
</ul>
<ol start="2">
<li>Next, we will view the structure of the data using the <kbd>str</kbd> function, which provides details on the data object class and dimensions and column-specific details on the data type, along with some sample values, as shown in the following code:</li>
</ol>
<pre style="padding-left: 60px">utils::str(la_no2)</pre>
<p style="padding-left: 60px">After running the code, we will see the following printed to our console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2e73256f-5d6a-4543-a8ae-7865d1cd07be.png" style="width:41.67em;height:11.92em;"/></p>
<ol start="3">
<li>
<p>All of this data is from the same site, with readings for the same species of pollutants, and we can conclude that the unit of measure will likely be consistent throughout as well. If this is the case, we can remove these columns as they provide no informational value. Even if we did not know the first variable would always be the same, we can start to see this pattern from the results of the structure (<kbd>str</kbd>) function. We can confirm that this is the case, though, by running the following code, using the <kbd>group_by</kbd> and <kbd>summarise</kbd> functions from the <kbd>dplyr</kbd> package:</p>
</li>
</ol>
<pre style="padding-left: 60px">la_no2 %&gt;% dplyr::group_by(Units, Site, Species) %&gt;% dplyr::summarise(count = n())</pre>
<p style="padding-left: 60px">After running the preceding code, we will see the following printed to our console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7e62e5f7-96fc-4110-adbf-c4835663da13.png" style="width:44.50em;height:8.00em;"/></p>
<p style="padding-left: 60px"><span>We have confirmed that the</span> <kbd>Site</kbd>, <kbd>Species</kbd>, <span>and</span> <kbd>Units</kbd><span class="packt_screen"> </span><span>values are always the same, so we can remove them from the data as they will provide no information. We can also see that the actual reading values are stored as character strings and we have dates stored that way as well. In its current form, the <kbd>date</kbd> field exhibits a characteristic known as high cardinality, which is to say that there are a large number of unique values. When we see this, we will usually want to act on these types of columns so that they have fewer distinct values. </span><span>I</span><span>n this case, the technique is clear because we know that this should be a date value. </span></p>
<ol start="4">
<li><span>In the code that follows, we will use the <kbd>dplyr</kbd></span> <kbd>select</kbd> <span>function to remove the columns that we don't want to keep. We will use the <kbd>dplyr</kbd></span> <span><kbd>mutate</kbd> </span><span>function along with functions from the</span> <kbd>lubridate</kbd> <span>package to transform the variables we have identified. After transforming the data, we can remove the old</span> character <span>string date column and the full date column as we will use the atomized date values going forward. We remove the columns we don't need and break the converted date field into its component parts using the following code:</span></li>
</ol>
<pre style="padding-left: 60px">la_no2 &lt;- la_no2 %&gt;%<br/>dplyr::select(c(-Site,-Species,-Units)) %&gt;%<br/>  dplyr::mutate(<br/>    Value = as.numeric(Value),<br/>    reading_date = lubridate::dmy_hm(ReadingDateTime),<br/>    reading_year = lubridate::year(reading_date),<br/>    reading_month = lubridate::month(reading_date),<br/>    reading_day = lubridate::day(reading_date),<br/>    reading_hour = lubridate::hour(reading_date),<br/>    reading_minute = lubridate::minute(reading_date)<br/>  ) %&gt;%<br/>  dplyr::select(c(-ReadingDateTime, -reading_date)) </pre>
<p class="mce-root" style="padding-left: 60px">Before running the preceding code, we should note that the dataframe in our <span class="packt_screen">Environment</span> pane <span>looks like the following screenshot</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-916 image-border" src="assets/90893f34-2a4b-47f5-ac2c-b07650f18196.png" style="width:30.92em;height:14.75em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign" style="padding-left: 60px">After running the code, we can note the differences to the dataframe, which should now look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-918 image-border" src="assets/37ede6ab-693c-42d1-827a-39365609dfb9.png" style="width:29.75em;height:15.50em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign" style="padding-left: 60px">As you can see, the columns that contained only one value have been removed, and the data column now occupies five columns with one for each of the date and time parts.</p>
<ol start="5">
<li>Next, we will use the <kbd>DataExplorer</kbd> package to explore any missing values. There are numerous ways in which we summarize the number and proportion of missing values in a given data object. Of these, the <kbd>plot_missing()</kbd> function offers a count and percentage of missing values along with a visualization<span>—</span>all from one function call. We plot missing values using the following line of code:</li>
</ol>
<pre style="padding-left: 60px">DataExplorer::plot_missing(la_no2)</pre>
<p class="mce-root" style="padding-left: 60px">After running this code, a plot is produced. In your <span class="packt_screen"><strong>Viewer</strong></span> pane, you should see the following plot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-919 image-border" src="assets/bbf2f7c5-bd39-40b0-862a-bd86a8823db1.png" style="width:35.17em;height:28.50em;"/></p>
<p class="mce-root CDPAlignLeft CDPAlign" style="padding-left: 60px">As you can see, there are no missing values among the independent variables. However, among the target variable class, there are around 1,500 missing values, accounting for 3.89% of the column.</p>
<ol start="6">
<li>Since we have missing values, we should consider whether any action should be taken. In this case, we will simply remove these rows as there are not many of them, and the portion of the data with no missing values will still be representative of the entire dataset. While, in this case, the values are simply removed, there are a number of options available to handle missing values. A summary of possible actions is presented later on in this chapter. To remove the rows with missing values, we run the following line of code:</li>
</ol>
<pre style="padding-left: 60px">la_no2 &lt;- la_no2 %&gt;% filter(!is.na(Value))</pre>
<ol start="7">
<li>We will also run a check on our discrete variable as well just to see the distribution of values among the categories present in this column. Again, the <kbd>DataExplorer</kbd> package offers a convenient function for this, which will provide a plot for the discrete values present noting the frequency of each. We generate this plot with the <kbd>plot_bar</kbd> function using the following line of code:</li>
</ol>
<pre style="padding-left: 60px">DataExplorer::plot_bar(la_no2)</pre>
<p style="padding-left: 60px">After running the preceding function, we are able to view the following visualization, which clearly shows that there are more <strong><span class="packt_screen">Ratified</span></strong> results than <span class="packt_screen"><strong>Provisional</strong></span> results. From reading the documentation for this dataset, the <span class="packt_screen"><strong>Ratified</strong></span> results are verified and can be trusted to be accurate, while the <span class="packt_screen"><strong>Provisional</strong></span> results may not be as accurate. Let's take a look at the output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-920 image-border" src="assets/19b0d38a-2dac-4413-bb72-2d78982c2ec2.png" style="width:31.67em;height:26.08em;"/></p>
<p style="padding-left: 60px">We created a plot in order to quickly see the distribution of values among the discrete terms in the <span class="packt_screen"><strong>Provisional</strong> or <strong>Ratified</strong></span> column. We can also create a table using the following code to get more specific details:</p>
<pre style="padding-left: 60px">la_no2 %&gt;% dplyr::group_by(`Provisional or Ratified`) %&gt;% dplyr::summarise(count = n())</pre>
<p class="mce-root" style="padding-left: 60px">The preceding code uses the <kbd>group_by</kbd> function to <span>group rows based on the values present in the</span> <kbd>Provisional or Ratified</kbd> <span>column. It then uses the <kbd>summarise</kbd> function, along with</span> <span>setting</span> <span>the <kbd>count</kbd> argument equal to <kbd>n()</kbd>, to calculate the number of rows containing each of the discrete values from the</span> <kbd>Provisional or Ratified</kbd> <span>column. Running the preceding code will print the output to your console, as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dc87a317-c018-4d13-a281-87aa35e51c16.png" style="width:22.42em;height:9.42em;"/></p>
<ol start="8">
<li>Since there are very few values marked as <kbd>Provisional</kbd>, we will just remove these rows. To do so, we will first use the <kbd>filter</kbd> function, which is used to remove rows based on a particular condition. In this case, we will filter the data so that only rows with an <kbd>'R'</kbd> value in the <kbd>Provisional or Ratified</kbd> column will remain, as shown here:</li>
</ol>
<pre style="padding-left: 60px">la_no2 &lt;- la_no2 %&gt;%<br/>  dplyr::filter(<br/>    `Provisional or Ratified` == 'R'<br/>  )</pre>
<ol start="9">
<li>Next, we will remove the <kbd>Provisional or Ratified</kbd> column since it only holds one unique value. To do this, we will use the <kbd>select()</kbd> function, which is used to remove columns in a similar way to how a filter is used to remove rows. Here, the <kbd>select()</kbd> function is called and the argument passed to the function is <kbd>-`Provisional or Ratified`</kbd>, which will remove this column. Alternatively, all of the other column names, aside from <kbd>Provisional or Ratified</kbd>, could be passed in as an argument. The <kbd>select()</kbd> function works by either using the columns to include or the columns to exclude. In this case, it is faster to note the column to exclude, which is why this choice was made. Please refer to the following code:</li>
</ol>
<pre style="padding-left: 60px">la_no2 &lt;- la_no2 %&gt;%<br/>  dplyr::select(-`Provisional or Ratified`)</pre>
<ol start="10">
<li>Earlier, we noted that all of our data is from the year 2018. Now that our date data is broken up into component parts, we should only see one value in the <kbd>reading_year</kbd> column. One way to test whether this is the case is to use the <kbd>range()</kbd> function. We check for the minimum and maximum values in the <kbd>reading_year</kbd> column by running the following code:</li>
</ol>
<pre style="padding-left: 60px">range(la_no2$reading_year)</pre>
<p class="mce-root" style="padding-left: 60px">Running the preceding code will result in values being printed to our console. Your console should look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/066e67dd-a241-4afe-9579-76d2c8b912aa.png" style="width:16.50em;height:2.83em;"/></p>
<p class="mce-root" style="padding-left: 60px">We can see from the results of our call to the <kbd>range()</kbd> function that, in fact, the <kbd>reading_year</kbd> column only includes one value. With this being the case, we can use the <kbd>select()</kbd> function to remove the <kbd>reading_year</kbd> column with the help of the following code:</p>
<pre style="padding-left: 60px">la_no2 &lt;- la_no2 %&gt;%<br/>  dplyr::select(-reading_year)</pre>
<ol start="11">
<li>Next, we can do a histogram check of continuous variables to look for outliers. To accomplish this, we will once again use the <kbd>DataExplorer</kbd> package. This time, we will use the <kbd>plot_histogram</kbd> function to visualize the continuous values for all of the columns with these types of values, as shown in the following code block:</li>
</ol>
<pre style="padding-left: 60px">DataExplorer::plot_histogram(la_no2)</pre>
<p style="padding-left: 60px">After running the preceding code, five plots are generated displaying the frequency for continuous values among the five columns that have these types of values. Your <span class="packt_screen"><strong>Viewer</strong></span> pane should look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-925 image-border" src="assets/918b3b3b-8119-4bc4-8bd0-7e88e503861b.png" style="width:36.83em;height:30.25em;"/></p>
<p style="padding-left: 60px">In the preceding output, we do see that our independent variable is slightly right-skewed, and, as a result, we could take some action on the outlier values. If there was a more dramatic skew, then we could apply a log transformation. As there are not many, we could also remove these values if we thought they were noisy. However, for now, we will just leave them in the data. If, in the end, our model is performing poorly, then it would be worthwhile to perform some outlier treatment to see whether this improves performance.</p>
<ol start="12">
<li>Lastly, let's do a correlation check. We will again use the <kbd>DataExplorer</kbd> package, and this time we will use the <kbd>plot_correlation()</kbd> function. To generate a correlation plot, we run the following line of code:</li>
</ol>
<pre style="padding-left: 60px">DataExplorer::plot_correlation(la_no2)</pre>
<p>Running the preceding line of code will generate a plot. The plot uses blue and red colors to denote negative and positive correlations, respectively. These colors will not be present in the diagram in this book; however, you can still see the correlation values. After running the preceding code, you will see a plot in your <span class="packt_screen">Viewer</span> pane, which looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-926 image-border" src="assets/f41a9729-df2e-493a-961b-f943ad256b53.png" style="width:38.25em;height:31.42em;"/></p>
<p>From this plot, we can see that the pollutant value does have some correlation with the <kbd>reading_hour</kbd> feature, and less correlation with the <kbd>reading_day</kbd> and <kbd>reading_month</kbd> features, suggesting that there is a higher trend throughout the day for when this pollutant is being produced than in the week, month, or year. The main objective of this plot is to look for independent variables that are highly correlated as it might suggest that these variables are conveying the same information, and, in that case, we may want to remove one or combine them in some way.</p>
<p>We now have a dataset that is properly preprocessed and ready for modeling. The correlation plot shows that the variables are not significantly correlated. Of course, this is not surprising as the remaining variables simply describe discrete moments in time.</p>
<p>The date value was converted from a string that provided no informational value to a date value further split into date parts represented as numeric data. All columns that contained only one value were removed as they provided no information. The few rows marked as provisional values were removed since there were not many of these, and, in the description of the data, there were warnings about the validity of pollutant measures marked this way. Lastly, rows containing null values for the predictor variable were removed. This was done because there were not many of these; however, there are other tactics we could have taken in this situation. We have noted them in the following section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Handling missing values</h1>
                </header>
            
            <article>
                
<p>In the preprocessing work that we just completed, we decided to remove missing values. This is an option when there are very few cases that contain missing values and, in this example, this was true. However, other situations may require different approaches to handling missing values. Here are some common options in addition to deleting rows and columns:</p>
<ul>
<li><strong>Imputation with a measure of centrality</strong> (<strong>mean</strong>/<strong>median</strong>/<strong>mode</strong>): Use one of the measures of centrality to fill in the missing values. This can work well if you have normally distributed numeric data. Modal imputation can also be used on non-numeric data by selecting the most frequent value to replace the missing values.</li>
<li><strong>Tweak for the missing values</strong>: You can use the known values to impute the missing values. Examples of this approach include using regression with linear data or the <strong>k-nearest neighbor</strong> (<strong>KNN</strong>) algorithm to assign a value based on similarity to known values in the feature space.</li>
<li><strong>Replace it with a constant value</strong>: The missing value can also be replaced with a constant value outside the range of values present or not already present in the categorical data. The advantage here is that it will become clear later on whether these missing values have any informational value, as they will be clearly set to the side. This is in contrast to imputing with a measure of centrality where the final result will be some missing values now containing the imputed value, while some equal values will have actually already been present in the data. In this case, it becomes difficult to know which values were missing values and which were the values already present in the data.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training a model on prepared data</h1>
                </header>
            
            <article>
                
<p>Now that the data is ready, we will split it into train and test sets and run a simple model. The objective at this point is not to try to achieve the best performance, but rather to get some type of a benchmark result to use in the future as we try to improve our model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Train and test data</h1>
                </header>
            
            <article>
                
<p class="mce-root">When we build predictive models, we need to create two separate sets of data with the help of the following segments. One is used by the model to learn the task and the other is used to test how well the model learned the task. Here are the types of data that we will look at:</p>
<ul>
<li class="mce-root"><strong>Train data</strong>: The segment of the data used to fit the model. The model has access to the explainer variables or independent variables, which are the <span>selected </span>columns, to describe a record in your data, as well as the target variable or dependent variable. That is the value we are trying to predict during the training process using this dataset. This segment should usually be between 50% and 80% of your total data.</li>
<li class="mce-root"><strong>Test data</strong><span>: The segment of the data used to evaluate the model results. The model should never have access to this data during the learning process and should never see the target variable. This dataset is used to test what the model has learned about the dependent variables. After fitting our model during the training phase, we now use this model to predict values on the test set. During this phase, only we have the correct answers; the independent variable, that is, the model, never has access to these values. After the model makes its predictions, we can evaluate how well the model performed by comparing the predicted values to the actual correct values.</span></li>
<li class="mce-root"><strong>Validation data</strong><span><span>: Validation data is a portion of the training dataset that the model uses to refine hyperparameters. As the varying values are selected for the hyperparameters, the model makes checks against the validation set and uses the results gathered during this process to select the values for the hyperparameters that produce the best performing models.</span></span></li>
<li class="mce-root"><strong>C</strong><strong>ross-validation</strong><span>: One potential issue that can arise when we use only one train and test set is that the model will learn about specific descriptive features that are particular to this segment of the data. What the model learns may not generalize well when applied to other data in the future. This is known as overfitting. To mitigate this problem, we can use a process known as cross-validation. In a simple example, we can do an 80/20 split of the data where 20% is held for test data and we can model and test on this split. We can then create a separate 80/20 split and do the same modeling and testing. We can repeat this process 5 times with 5 different test sets—each composed of a different fifth of the data. This exact type of cross-validation is known as 5-fold cross-validation. After all of the iterations are completed, we can check whether the results are consistent for each. And, if so, we can feel more confident that our model is not overfitting and use this to generalize on more data.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing an algorithm</h1>
                </header>
            
            <article>
                
<p>For this task, we will use <kbd>xgboost</kbd>, which is a very popular implementation of the gradient tree boosting algorithm. The reason this works so well is that each model iteration learns from the results of the previous model. This model uses boosting for iterative learning in contrast to bagging. Both of these ensembling techniques can be used to compensate for a known weakness in tree-based learners, which has to do with overfitting to the training data.</p>
<p>One simple difference between bagging and boosting is that, with bagging, full trees are grown and then the results are averaged, while, with boosting, each iteration of the tree model learns from the model before it. This is an important concept, as this idea of an algorithm that incorporates an additive function with information gained after modeling on the residuals of the previous model will be used in deep learning as we move forward.</p>
<p>Here, we will explore the power of this type of machine learning algorithm on this simple example. Additionally, we will pay particular attention to how what we learn here is relevant for more complex examples in the subsequent chapters. We will use the following code to train an <kbd>xgboost</kbd> model:</p>
<ol>
<li>We start the process of fitting a model to our data by partitioning our data into train and test sets, using the following code:</li>
</ol>
<pre style="padding-left: 60px">set.seed(1)<br/>partition &lt;- sample(nrow(la_no2), 0.75*nrow(la_no2), replace=FALSE)<br/>train &lt;- la_no2[partition,]<br/>test &lt;- la_no2[-partition,]<br/><br/>target &lt;- train$Value<br/><br/>dtrain &lt;- xgboost::xgb.DMatrix(data = as.matrix(train), label= target)<br/>dtest &lt;- xgboost::xgb.DMatrix(data = as.matrix(test))</pre>
<p style="padding-left: 60px">In the preceding code, we started with the <kbd>set.seed()</kbd> function. This is because aspects of this modeling process involve pseudorandomness, and setting the seed ensures that the same values are used for these elements every time, so we can consistently produce the same results.</p>
<p style="padding-left: 60px">We split the data into training data, which we will model on, and a test set to check whether our predictions are accurate. We do this by getting a random sample of row index values, which we store in the vector labeled <kbd>partition</kbd>, and use them to subset our data. In addition, after partitioning, we split out the target variable and store this in a vector.</p>
<p style="padding-left: 60px">Then, we convert our data into a dense matrix so that it is in the proper format to be passed to the <kbd>xgboost</kbd> algorithm.</p>
<ol start="2">
<li>Following this, we will create a list of parameters. Some of these values are required for the analysis that we will be doing and others are just starting values chosen arbitrarily. For those values, later on, we will look at ways to more scientifically choose them. We prepare our initial parameter list using the following code:</li>
</ol>
<pre style="padding-left: 60px">params &lt;-list(<br/>  objective = "reg:linear",<br/>  booster = "gbtree",<br/>  eval_metric = "rmse",<br/>  eta=0.1, <br/>  subsample=0.8,<br/>  colsample_bytree=0.75,<br/>  print_every_n = 10,<br/>  verbose = TRUE<br/>)</pre>
<p style="padding-left: 60px">In the preceding code, the required values in this list include the following:</p>
<ul>
<li style="padding-left: 60px"><kbd>objective = "reg:linear"</kbd>: This is used to define the task objective as linear regression. Here, we are conducting a regression task seeking the value of nitrogen dioxide.</li>
<li style="padding-left: 60px"><kbd>booster = "gbtree"</kbd><span>:</span><span> This</span> tells us that we will use gradient tree boosting to choose the best model for predicting results.</li>
<li style="padding-left: 60px"><kbd>eval_metric = "rmse"</kbd><span>:</span><span> This tells us </span>that we will use the <strong>Root Mean Squared Error</strong> (<strong>RMSE</strong>) to evaluate the success of our model. Later on, we will look at why this is the most appropriate choice, along with some of the other options we can use here for other tasks.</li>
</ul>
<p style="padding-left: 60px">Next, these are the variables where we are arbitrarily choosing starting values:</p>
<ul>
<li style="padding-left: 60px"><kbd>eta<span>=</span>0.1</kbd><span>:</span><span> This is used </span>to define the learning rate. To begin, it makes sense to use a larger number; however, as we move forward, we will want to use a smaller learning rate and additional rounds to improve performance.</li>
<li style="padding-left: 60px"><kbd>subsample<span>=</span>0.8</kbd><span>:</span><span> This tells us </span>that, for each tree, we will use 80% of the rows from the data.</li>
<li style="padding-left: 60px"><kbd>col_subsample=0.75</kbd><span>:</span><span> This tells us </span>that, for each tree, we will use 75% of the columns from the data.</li>
</ul>
<p style="padding-left: 60px">These are the parameters that do not impact the model and only impact how we review the results of the model:</p>
<ul>
<li style="padding-left: 60px"><kbd>print_every_n = 10</kbd><span>:</span><span> This is used </span>to state that the evaluation scores should be printed after every 10 rounds.</li>
<li style="padding-left: 60px"><kbd>verbose<span> </span>= TRUE</kbd><span>:</span><span> This is used </span>to denote that the evaluation scores should be printed to the console so that they can be seen by the end user during the model run.</li>
</ul>
<ol start="3">
<li>Now that we have the parameters defined, we will run the model using the following code:</li>
</ol>
<pre style="padding-left: 60px">xgb &lt;- xgboost::xgb.train( <br/>  params = params, <br/>  data = dtrain,<br/>  nrounds = 100<br/>)</pre>
<p style="padding-left: 60px">When we run the model, we bring in the list of parameters that we previously defined, that is, the model should be run against the training dataset and we choose to run the model for 100 rounds. This means that we will grow 100 trees. This is, again, an arbitrary value. Later, we will look at ways to discover the optimal number of rounds. Then, we predict the test dataset using the model that we just defined. For our train dataset, the algorithm knows the correct value and uses this to adjust the model.</p>
<ol start="4">
<li>In the next section, we will apply the model to the test data where the model no longer has access to the correct values, and, without this knowledge, the model uses the independent variables to make predictions for the target variable. Please refer to the following code block:</li>
</ol>
<pre style="padding-left: 60px">pred &lt;- predict(xgb, dtest)</pre>
<p style="padding-left: 60px">When we run the preceding code, we take the data from the dense matrix, labeled <kbd>dtest</kbd>, and run it through our model labeled <kbd>xgb</kbd>. The model takes the tree splits that were calculated during training and applies them to the new data to make predictions. The predictions are stored in a vector called <kbd>pred</kbd>.</p>
<ol start="5">
<li>Lastly, we will use the RMSE function to check model performance. For this evaluation metric, the closer it is to zero, the better, as this is a measure of the difference between true values and predicted values. To evaluate the performance of our model, we run the following line of code:</li>
</ol>
<pre style="padding-left: 60px">Metrics::rmse(test$Value,XGBpred) </pre>
<p>After running the preceding code, we will see a value printed to our console. Your console should look like the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-927 image-border" src="assets/18404f69-a22b-4d99-b795-01f4b55398fb.png" style="width:25.83em;height:4.67em;"/></p>
<p>From our quick and simple model, we have achieved an RMSE score of 0.054. Soon, we will make some changes to the model parameters to try to improve our score. Before that, let's take a quick look at all of the different evaluation metrics that we can use in addition to RMSE while also taking a deep dive into explaining how RMSE works.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating model results</h1>
                </header>
            
            <article>
                
<p>We only know whether a model is successful if we can measure it, and it is worthwhile taking a moment to remember which metrics to use in which scenarios. Take, for example, a credit card fraud dataset where there is a large imbalance in the target variable because there will only be a, relatively, few cases of fraud among many non-fraudulent cases.</p>
<p>If we use a metric that just measures the percentage of the target variable that we predict successfully, then we will not be evaluating our model in a very helpful way. In this case, to keep the math simple, let's imagine we have 10,000 cases and only 10 of them are fraudulent accounts. If we predict that all cases are not fraudulent, then we will have 99.9% accuracy. This is very accurate, but it is not very helpful. Here is a review of the different metrics and when to use them.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Machine learning metrics</h1>
                </header>
            
            <article>
                
<p><span>Choosing the wrong metric will make it very difficult to evaluate performance and, as a result, improve our model. Therefore, it is very important to choose the right metric. </span>Let's take a look at the following machine learning metrics:</p>
<ul>
<li><strong>Accuracy</strong>: The simplest evaluation metric is accuracy. Accuracy measures the difference between the predicted value and the actual value. This metric is easy to interpret and communicate; however, as we mentioned earlier, it doesn't measure performance well when used to evaluate a highly unbalanced target variable, for example.</li>
<li><strong>Confusion Matrix</strong>: The confusion matrix provides a convenient way to display classification accuracy along with Type I and Type II errors. The combined view of these four related metrics can be especially informative in deciding where to focus our efforts during the tuning process. It can also help <span>mark </span>cases where other metrics may be more helpful. When there are too many values in the majority class, then a metric that is designed for use with class imbalances, such as log-loss, should be employed.</li>
<li><strong>Mean Absolute Error</strong> (<strong>MAE</strong>):<span> This metric takes the difference between the predicted value and the actual value and calculates the mean value of these errors. This metric is simple to interpret and is useful when there is no need to apply an additional penalty to large errors. If an error that is three times larger than another error is three times as bad, then this is a good metric to use. However, there are many cases where an error that is three times larger than another error is much more than three times as bad, and this results in adding an additional penalty, which can be accomplished with the next metric.</span></li>
<li><strong>RMSE</strong><span><span>: This metric takes the square of the error for every prediction, the difference between the predicted value and the actual value, sums these squared errors, and then takes the square root of the sums. In this case, if the squaring has even a few highly inaccurate predictions, it will result in a sizable penalty and a higher value on this error metric. We can see how this would help in our preceding example and why we have chosen to use RMSE. This metric is used for regression.</span></span></li>
<li><strong>Area Under the Curve</strong> (<strong>AUC</strong>)<span>: The AUC refers to the <em>Area under the Receiver-Operator Curve</em>. In this model, your target variable needs to be a value expressing the confidence or probability that a row belongs to the positive or negative target condition. To make this more concrete, AUC can be used when your task is to predict how likely someone is to make a given purchase. Clearly, from this explanation, we can see that AUC is a metric for classification.</span></li>
<li><strong>Logarithmic Loss</strong> (<strong>Log-Loss</strong>)<span>: The log-loss evaluation metric rewards confident predictions more than AUC and penalizes neutral predictions. This is important when we have an imbalanced target dataset and finding the minority class is critical. Having an extra penalty on incorrect guesses helps us get to the model that better predicts these minority class members correctly. Log-loss is also better for multiclass models where the target variable is not binary.</span></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Improving model results</h1>
                </header>
            
            <article>
                
<p>Since we have a regression problem, we now know why we chose RMSE, and we have a baseline metric of performance, we can begin to work on improving our model. Every model will have its own different way of improving results; however, we can generalize slightly. Feature engineering helps to improve model performance; however, since this type of work is less important with deep learning, we will not focus on that here. Also, we have already used feature engineering to generate our date and time parts. In addition, we can run our model for longer at a slower learning rate and we can tune hyperparameters. In order to find the best values using this type of model improvement method, we will use a technique called <strong>grid search</strong> to look at a range of values for a number of different fields.</p>
<p>Let's search for the optimal number of rounds. Using the cross-validation version of <kbd>xgboost</kbd> through the R interface, we can train our model again on our default hyperparameter settings. This time, instead of choosing 100 rounds, we will use the functionality within <kbd>xgboost</kbd> to determine the optimal number of trees to grow. Using cross-validation, the model can evaluate the error rate at the end of every round, and we will use the <kbd>early_stopping_rounds</kbd> feature so that the model stops growing additional trees after a given number of attempts, when it no longer continues to decrease the error rate.</p>
<p>Let's take a look at the following code, which determines the number of rounds or number of trees that produces the lowest error rate given the default setting:</p>
<pre>xgb_cv &lt;- xgboost::xgb.cv( <br/>  params = params, <br/>  data = dtrain, <br/>  nrounds = 10000, <br/>  nfold = 5, <br/>  showsd = T, <br/>  stratified = T,<br/>  print_every_n = 100,<br/>  early_stopping_rounds = 25, <br/>  maximize = F)</pre>
<p>After we run the preceding code, we will see model performance metrics printing to the console as the model runs. The beginning of this report on the console should look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-928 image-border" src="assets/a3f1eb80-0aef-4aca-ab6c-b7fe03e2b5dc.png" style="width:39.92em;height:15.42em;"/></p>
<p>Here, we can see that our model performance is improving rapidly, and we have confirmation that our model will stop training when the model hasn't improved for 25 rounds.</p>
<p>When your model reaches the optimal number of runs, your console should look like the <span>following screenshot</span>:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-930 image-border" src="assets/405145eb-3455-465a-90b3-7f44a11164e9.png" style="width:43.50em;height:12.50em;"/></p>
<p>Here, we can see that the model performance is improving more slowly and that the model has stopped because it was no longer improving. The report that is printed out to the console also identifies the best performing round.</p>
<p>Breaking down everything that is happening in the preceding example, we again train an <kbd>xgboost</kbd> model on the same data with the same parameters as we did previously. However, we set the number of rounds to be much higher. In order to find the best iteration, we need enough rounds so that the model doesn't stop growing trees before finding the optimal number of trees. In this case, the best iteration occurs at around 3,205; so, if we had set the number of rounds at 1,000, for example, the modeling process would have completed after growing 1,000 trees. However, we still would not know the number of rounds that produces the lowest error rate, which is why we set the number of rounds to be so high.</p>
<p><span>The following is the list of settings being used in the preceding code. Let's take a look at the purpose of using each of these settings</span>:</p>
<ul>
<li><kbd>nfold</kbd>: This is the number of folds or how many partitions to make in the data for cross-validation. Here, we use <kbd>5</kbd>, which utilizes the alternating 80/20 split referenced earlier.</li>
<li><kbd>showsd</kbd>: This shows the standard deviation in order to note the variation among the results from the different combinations of folds. This is important to ensure the model works well on all sets of data and will generalize well when used on future data.</li>
<li><kbd>stratified</kbd>: This ensures that each fold of data contains the same proportion of the target class.</li>
<li><kbd>print_every_n</kbd>: This tells us how often to print the results of the cross-validation to the console.</li>
<li><kbd>early_stopping_rounds</kbd>: This is a value that decides when the model should stop growing trees. You can use this value to check whether performance has improved during the given number of rounds. <span>The process will stop when the model no longer improves while growing trees to the limit of rounds set</span>.</li>
<li><kbd>maximize</kbd>: This notes whether the evaluation metric is one where improvement involves maximizing the score or minimizing the score.</li>
</ul>
<p>Now, let's do a grid search on select hyperparameters. As the name implies, a grid search will model for all defined hyperparameter value combinations. Using this technique, we can adjust a few settings that will control how the model grows trees and then evaluate which settings provide the best performance. A complete list of all of the tunable hyperparameters for <kbd>xgboost</kbd> is included with the package documentation. For this example, we will focus on the following three hyperparameters:</p>
<ul>
<li><kbd>max_depth</kbd>: This is the maximum depth of the tree. Since we only have 4 features, we will try depths of <kbd>2</kbd>, <kbd>3</kbd>, and <kbd>4</kbd>.</li>
<li><kbd>gamma</kbd>: This is the minimum loss reduction needed to continue creating splits. Setting this level higher will create shallower trees as nodes that contribute less to reducing the error rate are not further divided. We will try values of <kbd>0</kbd>, <kbd>0.5</kbd>, and <kbd>1</kbd>.</li>
<li><kbd>min_child_weight</kbd>: This is the number of instances needed to grow a node. If there are fewer instances than the threshold, then the tree will discontinue partitioning from this node. The higher this number, the more shallow trees that will be grown. For this example, we will try values of <kbd>1</kbd>, <kbd>3</kbd>, and <kbd>5</kbd>.</li>
</ul>
<p>We will now go through all the code required to perform a grid search to tune our parameters to the optimal values in order to improve model performance:</p>
<ol>
<li>Our first step will be to define our search grid by assigning the vector of values, mentioned previously, to their respective hyperparameter within the parameter grid. We define the values we will try for our hyperparameters by running the following code:</li>
</ol>
<pre style="padding-left: 60px">xgb_grid &lt;- expand.grid(<br/>  nrounds = 500,<br/>  eta = 0.01,<br/>  max_depth = c(2,3,4),<br/>  gamma = c(0,0.5,1),<br/>  colsample_bytree = 0.75,<br/>  min_child_weight = c(1,3,5),<br/>  subsample = 0.8<br/>)</pre>
<p style="padding-left: 60px"><span>As shown in the preceding code, for expediency, we will set the number of rounds to 500. Though, you could use the rounds found by searching for the best iteration previously in a real-world situation.</span></p>
<div class="packt_tip"><span>When including <kbd>eta</kbd> in your grid search, also remember to include <kbd>nrounds</kbd> as you will need more rounds as the learning rate decreases.<br/></span></div>
<ol start="2">
<li>After this, we will use the <kbd>trainControl</kbd> function within <kbd>caret</kbd> to define how we want to handle this parameter search. For this, we will list the code and then walk through the settings selected. There are many additional settings for <kbd>trainControl</kbd>; however, we are focusing on a select few for this chapter. We set how we will train our model using the following code:</li>
</ol>
<pre style="padding-left: 60px">xgb_tc = caret::trainControl(<br/>  method = "cv",<br/>  number = 5,<br/>  search = "grid",<br/>  returnResamp = "final",<br/>  savePredictions = "final",<br/>  verboseIter = TRUE,<br/>  allowParallel = TRUE<br/>)</pre>
<p style="padding-left: 60px">The <kbd>method</kbd> and <kbd>number</kbd> parameters for this function simply define our cross-validation strategy, which will be 5-fold again, as used previously. We will use a grid search to go through all possible combinations among the parameter settings defined in the last section of code. The next two parameters are used to save the resample and prediction details for the best iteration after modeling on all combinations. Lastly, we set <kbd>verboseIter</kbd> to <kbd>TRUE</kbd> to print iteration details to the console, and <kbd>allowParallel</kbd> is set to <kbd>TRUE</kbd> to use parallel processing to increase computational speed.</p>
<ol start="3">
<li>With the grid search values in place and the search strategy defined, we now run the model again using every possible hyperparameter combination. We then train our model while employing a grid search of our hyperparameters using the settings from the first two steps:</li>
</ol>
<pre style="padding-left: 60px">xgb_param_tune = caret::train(<br/>  x = dtrain,<br/>  y = target,<br/>  trControl = xgb_tc,<br/>  tuneGrid = xgb_grid,<br/>  method = "xgbTree",<br/>  verbose = TRUE<br/>)</pre>
<p style="padding-left: 60px">After running this code, we will see a report printing to our console that is similar to when we ran our model the first time. Your console should look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-931 image-border" src="assets/0fd3e9e5-7cb7-4d1d-86a1-906cc08b8020.png" style="width:49.92em;height:18.33em;"/></p>
<p style="padding-left: 60px">The report shows the current fold and current parameter setting as the model goes through all combinations on all five different splits of the data. In the preceding screenshot, we see a test of all <kbd>min_child_weight</kbd> options holding everything else constant on the fifth split of the data. In the end, we see the best tuning parameters being selected.</p>
<p style="padding-left: 60px">In this situation, the best depth is using all of the features, which is not surprising given the lack of features. The best minimum child weight is <kbd>1</kbd>, which means that even sparsely populated nodes still hold important information for our model. The best gamma value is <kbd>0</kbd>, which is the default value. As this value rises, it places a slight constraint on the error rate improvement needed by each node before splitting. In this case, after our grid search, we are largely left with the default values; yet, we can see the process by which we would choose alternatives to these defaults if they helped to improve model performance.</p>
<ol start="4">
<li>Now that we know the best hyperparameter settings, we can plug them back into the model that we ran before and see whether there is any improvement. We train our model using the parameters and iteration count that we found optimizes performance by running the following code:</li>
</ol>
<pre style="padding-left: 60px">params &lt;-list(<br/>  objective = "reg:linear",<br/>  booster = "gbtree",<br/>  eval_metric = "rmse",<br/>  eta=0.01, <br/>  subsample=0.8,<br/>  colsample_bytree=0.75,<br/>  max_depth = 4,<br/>  min_child_weight = 1,<br/>  gamma = 1<br/>)<br/>xgb &lt;- xgboost::xgb.train( <br/>  params = params, <br/>  data = dtrain,<br/>  nrounds = 3162,<br/>  print_every_n = 10,<br/>  verbose = TRUE,<br/>  maximize = FALSE<br/>)<br/><br/>pred &lt;- stats::predict(xgb, dtest)<br/>Metrics::rmse(test$Value,pred) </pre>
<p class="mce-root" style="padding-left: 60px">When we run the preceding code, we train our model and make predictions, like we did earlier, and we also run the line of code to calculate the RMSE value. When we run this line, we will see a value printed to our console. Your console should look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-932 image-border" src="assets/5d8f1590-72d6-450f-8c12-05069c79dbc6.png" style="width:24.33em;height:4.00em;"/></p>
<p class="mce-root">From the results of calculating the error rate, we can see that the score has changed from 0.054 to 0.022, which is an improvement from our first attempt. Using this data, with limited features, we may think that a time series model would have been a better choice; however, with only 1 year of data, a time series approach wouldn't catch any late seasonality effects that are not already present. This type of modeling creates a map for future years and shows that missing data can be predicted by simply using the date and time values for known data. This means that we can estimate NO2 values for future years. After collecting several years of data, a time series approach could then be used to make predictions that take into account year over year trends, in addition to the seasonality information captured here.</p>
<p>We used <kbd>xgboost</kbd>, which is a popular tree boosting algorithm, to predict pollution levels in an area of London. Earlier, we walked through creating a simple model to establish a benchmark. We then looked at how we should measure performance. Then, we took steps that improved performance. While we selected <kbd>xgboost</kbd> for this task, there are other machine learning algorithms that we could choose from, which we will review next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reviewing different algorithms</h1>
                </header>
            
            <article>
                
<p>We have raced through machine learning relatively quickly, as we wanted to focus on the underlying concepts that will follow along with us as we head into deep learning. As such, we cannot offer a comprehensive explanation of all machine learning techniques; however, we will quickly review the different algorithm types here, as this will be helpful to remember going forward.</p>
<p>We'll do a quick review of the following machine learning algorithms:</p>
<ul>
<li><strong>Decision Trees</strong>:<span> A decision tree is a simple model that makes up the base learners of many more complex algorithms. A decision tree simply splits a dataset at a given variable and notes the proportion of the target class that exists in the splits. For example, if we were to predict who is more likely to enjoy playing with baby toys, then a split on age would likely show that the split of the data containing just those under the age of 3 has a high percentage of true results in the <kbd>target</kbd> variable, that is, a high proportion that does enjoy this type of activity, while those who are older would likely not enjoy this.</span></li>
<li><strong>Random Forests</strong><span>: Random forests are similar to <kbd>xgboost</kbd>, which was used in this brief machine learning overview. A notable distinction between random forests and <kbd>xgboost</kbd> is that random forests build full decision trees. This makes up the set of base learners. The results from these simple base learner models are then averaged together to arrive at predictions that are better than any base learner. This technique is known as bagging. In contrast, <kbd>xgboost</kbd> uses boosting, which includes what it has learned from building previous base learners as it applies additional decision trees to the data. While both are useful and powerful ways to ensemble results and improve performance, we have chosen to focus on <kbd>xgboost</kbd> in this example because this idea of carrying forward information learned from the previous iteration is also present in deep learning.</span></li>
<li><strong>Logistic Regression and Support Vector Machines</strong> (<strong>SVM</strong>):<strong> </strong><span><span>SVMs separate features in <em>n</em>-dimensional space with a line that is the farthest from the two closest points in that space. This boundary is then applied to the test data and points on one side are classified one way, while points on the other are classified as a member of the other class. This is similar to logistic regression, with the main difference being that logistic regression evaluates all data points, while SVM just includes the points nearest to the line used to split the data. In addition, logistic regression works better when there are fewer explainer variables, and SVM works better when the dataset contains a larger number of dimensions. SVM will seek to find a line that divides all features, while logistic regression will use a combination of best-fitting, but not perfect, lines to estimate the probability that a data point belongs to a particular member of the target variable.</span></span></li>
<li><strong>KNN</strong> <strong>and k-means</strong>:<span> These are two ways to create clusters within our data. KNN is a supervised learning technique. Using this method, the model plots the points on a <em>k</em>-dimensional feature space. When new points are introduced during the training process, the model identifies the class for the nearest neighbors to the new point and assigns this class to this record. By contrast, <em>k</em>-means is an unsupervised learning technique that finds centroids in the feature space such that <em>k</em> clusters can be created where each point is classified based on the minimum distance to a given centroid.</span></li>
<li><strong>GBM and LightGBM</strong>:<span> Aside from <kbd>xgboost</kbd>, GBM and LightGBM also provide a means to generate predictions using a boosting mechanism for improving model performance between iterations. <strong>Gradient Boosting Machines</strong> (<strong>GBM</strong>) is the precursor to <kbd>xgboost</kbd> and LightGBM. It largely operates the same way by using a boosting ensemble technique on decision tree base learners; however, it is more primitive in its approach. GBM grows full trees using all features, while <kbd>xgboost</kbd> and LightGBM have different ways to reduce the number of splits that take place, which speed up how fast trees can be grown.</span></li>
</ul>
<p class="mce-root">The biggest difference between <kbd>xgboost</kbd> and LightGBM is that, where <kbd>xgboost</kbd> grows a new level for every tree after computing the feature splits, LightGBM will just grow the level below the most predictive leaf. This leaf-wise splitting offers superior speed advantages over the level-wise splitting used by <kbd>xgboost</kbd>. Also, with the focus on single leaf splits, the model can better find the values that minimize error compared with splitting into entire levels, which can lead to better performance. LightGBM may overtake <kbd>xgboost</kbd> as the go-to model for practitioners; however, for now, <kbd>xgboost</kbd> is still more widely used, which is why it was selected for this brief overview.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we referred to a raw dataset, explored the data, and took the necessary preprocessing steps to get the data ready for modeling. We performed data type transformations to convert numbers and dates being stored as character strings into numeric and date value columns, respectively. In addition, we performed some feature engineering by breaking up the date value into its component parts. After completing preprocessing, we modeled our data. We followed an approach that included creating a baseline model and then tuning hyperparameters to improve our initial score. We used early stopping rounds and grid searches to identify hyperparameter values that produced the best results. After modifying our model-based results from our tuning procedures, we noticed much better performance.</p>
<p><span>All of the aspects of machine learning that were discussed in this chapter will be used in the subsequent chapters too. </span>We will need to get our data ready for modeling, and we will need to know how we can improve model performance by adjusting its settings. In addition, we have been focusing on a decision tree ensembling model in <kbd>xgboost</kbd> because our work with neural networks in upcoming chapters will be similar. We will need to consider efficiency and performance just as we did with <kbd>xgboost</kbd>, by adjusting how trees are grown.</p>
<p>This review of machine learning provides the foundation for stepping into deep learning. We begin, in the next chapter, with installing and exploring the packages used. </p>


            </article>

            
        </section>
    </body></html>