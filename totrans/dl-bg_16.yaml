- en: Recurrent Neural Networks
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces recurrent neural networks, starting with the basic model
    and moving on to *newer* recurrent layers that are able to handle internal memory
    learning to remember, or forget, certain patterns found in datasets. We will begin
    by showing that recurrent networks are powerful in the case of inferring patterns
    that are temporal or sequential, and then we will introduce an improvement on
    the traditional paradigm for a model that has internal memory, which can be applied
    in both directions in the temporal space.
  prefs: []
  type: TYPE_NORMAL
- en: We will approach the learning task by looking at a sentiment analysis problem
    as a sequence-to-vector application, and then we will focus on an autoencoder
    as a vector-to-sequence and sequence-to-sequence model at the same time. By the
    end of this chapter, you will be able to explain why a long short-term memory
    model is better than the traditional dense approach. You will be able to describe
    how a bi-directional long short-term memory model might represent an advantage
    over the single directional approach. You will be able to implement your own recurrent
    networks and apply them to NLP problems or to image-related applications, including
    sequence-to-vector, vector-to-sequence, and sequence-to-sequence modeling.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is organized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to recurrent neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Long short-term memory models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-vector models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Vector-to-sequence models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-sequence models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical implications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to recurrent neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Recurrent neural networks** (**RNNs**) are based on the early work of Rumelhart
    (Rumelhart, D. E., et al. (1986)), who was a psychologist who worked closely with
    Hinton, whom we have already mentioned here several times. The concept is simple,
    but revolutionary in the area of pattern recognition that uses sequences of data.'
  prefs: []
  type: TYPE_NORMAL
- en: A **sequence of data** is any piece of data that has high correlation in either
    time or space. Examples include audio sequences and images.
  prefs: []
  type: TYPE_NORMAL
- en: 'The concept of recurrence in RNNs can be illustrated as shown in the following
    diagram. If you think of a dense layer of neural units, these can be stimulated
    using some input at different time steps, ![](img/8e859b39-e2bf-4310-b7df-7d1aa72d46ca.png).
    *Figures 13.1 (b)* and *(c)* show an RNN with five time steps, ![](img/9c53c6b3-e449-4a6f-9c95-12ff4cb30010.png).
    We can see in *Figures 13.1 (b)* and *(c)* how the input is accessible to the
    different time steps, but more importantly, the output of the neural units is
    also available to the next layer of neurons:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1f337f3e-cee6-404b-a74b-db34d983cdfa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1\. Different representations of recurrent layers: (a) will be the
    preferred use in this book; (b) depicts the neural units and the feedback loop;
    and (c) is the expanded version of (b), showing what really happens during training'
  prefs: []
  type: TYPE_NORMAL
- en: 'The ability of an RNN to see how the previous layer of neurons is stimulated
    helps the network to interpret sequences much better than without that additional
    piece of information. However, this comes at a cost: there will be more parameters
    to be calculated in comparison to a traditional dense layer due to the fact that
    there are weights associated with the input ![](img/37624e33-af8b-4b40-8359-c74bd9dec3cd.png) and
    the previous output ![](img/5869c1cc-4186-404b-802e-caadae5f3bbe.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Simple RNNs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In Keras, we can create a simple RNN with **five time steps** and **10 neural
    units** (see *Figure 13.1*) as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The preceding sample code assumes that the number of **features in the input**
    would be just **two**; for example, we can have sequential data in two dimensions.
    These types of RNNs are called *simple* because they resemble the simplicity of
    dense networks with `tanh` activations and a recurrence aspect to it.
  prefs: []
  type: TYPE_NORMAL
- en: RNNs are usually tied to embedding layers, which we discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Embedding layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An embedding layer is usually paired with RNNs when there are sequences that
    require additional processing in order to make RNNs more robust. Consider the
    case when you have the sentence *"This is a small vector"*, and you want to train
    an RNN to detect when sentences are correctly written or poorly written. You can
    train an RNN with all the sentences of length five that you can think of, including *"This
    is a small vector".* For this, you will have to figure out a way to transform
    a sentence into something that the RNN can understand. Embedding layers come to
    the rescue.
  prefs: []
  type: TYPE_NORMAL
- en: 'There is a technique called **word embedding**, which is tasked with converting
    a word into a vector. There are several successful approaches out there, such
    as Word2Vec (Mikolov, T., et al. (2013)) or GloVe (Pennington, J., et al. (2014)).
    However, we will focus on a simple technique that is readily available. We will
    do this in steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Determine the length of the sentences you want to work on. This will become
    the dimensionality of the input for the RNN layer. This step is not necessary
    for the design of the embedding layer, but you will need it for the RNN layer
    very soon, and it is important that you decide this early on.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Determine the number of different words in your dataset and assign a number
    to them, creating a dictionary: word-to-index. This is known as a vocabulary.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most people will determine the vocabulary and then calculate the frequency of
    each word to rank the words in the vocabulary so as to have the index 0 corresponding
    to the most common word in the dataset, and the last index corresponding to the
    most uncommon word. This can be helpful if you want to ignore the most common
    words or the most uncommon words, for example.
  prefs: []
  type: TYPE_NORMAL
- en: Substitute the words in all the sentences of the dataset with their corresponding
    index.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Determine the dimensionality of the word embedding and train an embedding layer
    to map from the numerical index into a real-valued vector with the desired dimensions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Look at the example in *Figure 13.2*. If we take the word *This*, whose given
    index is 7, some trained embedding layer can map that number into a vector of
    size 10, as you can see in *Figure 13.2 (b)*. That is the word embedding process.
  prefs: []
  type: TYPE_NORMAL
- en: You can repeat this process for the complete sentence *"This is a small vector"*,
    which can be mapped to a **sequence** of indices [7, 0, 6, 1, 28], and it will
    produce for you a **sequence** of vectors; see *Figure 13.2 (c)*. In other words,
    it will produce a **sequence of word embeddings**. The RNN can easily process
    these sequences and determine whether the sentence that these sequences represent
    is a correct sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, we must say that determining whether a sentence is correct is a challenging
    and interesting problem (Rivas, P. et al. (2019)):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d91edafe-3594-4b85-890f-9cb37ef14142.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2\. Embedding layer: (a) will be the preferred use in this book;
    (b) shows an example of a word embedding; and (c) shows a sequence of words and
    its corresponding matrix of word embeddings'
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the model shown in *Figure 13.2*, an embedding layer in Keras can
    be created as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following summary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Note, however, that the vocabulary size is usually in the order of thousands
    for typical NLP tasks in most common languages. Just think of your good old-fashioned
    dictionary ... How many entries does it have? Several thousand, usually.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, sentences are usually longer than five words, so you should expect
    to have longer sequences than in the preceding example.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the embedding dimension depends on how rich you want your model to
    be in the embedding space, or on your model space constraints. If you want a smaller
    model, consider having embeddings of 50 dimensions for example. But if space is
    not a problem and you have an excellent dataset with millions of entries, and
    you have unlimited GPU power, you should try embedding dimensions of 500, 700,
    or even 1000+ dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's try to put the pieces together with a real-life example.
  prefs: []
  type: TYPE_NORMAL
- en: Word embedding and RNNs on IMDb
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The IMDb dataset was explained in previous chapters, but to keep things brief,
    we will say that it has movie reviews based on text and a positive (1) or negative
    (0) review associated with every entry.
  prefs: []
  type: TYPE_NORMAL
- en: Keras lets you have access to this dataset and gives a couple of nice features
    to optimize time when designing a model. For example, the dataset is already processed
    according to the frequency of each word such that the smallest index is associated
    with frequent words and vice versa. With this in mind, you can also exclude the
    most common words in the English language, say 10 or 20\. And you can even limit
    the size of the vocabulary to, say, 5,000 or 10,000 words.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go further, we will have to justify some things you are about see:'
  prefs: []
  type: TYPE_NORMAL
- en: A vocabulary size of 10,000\. We can make an argument in favor of keeping a
    vocabulary size of 10,000 since the task here is to determine whether a review
    is positive or negative. That is, we do not need an overly complex vocabulary
    to determine this.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eliminating the top 20 words. The most common words in English include words
    such as "a" or "the"; words like these are probably not very important in determining
    whether a movie review is positive or negative. So, eliminating the 20 most common
    should be OK.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentence length of 128 words. Having smaller sentences, such as 5-word sentences,
    might be lacking enough content, and it would not make a lot of sense having longer
    sentences, such as 300-word sentences, since we can probably sense the tone of
    a review in fewer words than that. The choice of 128 words is completely arbitrary,
    but justified in the sense explained.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With such considerations, we can easily load the dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also print some data for verification purposes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The first part of the preceding code shows how to load the dataset split into
    training and test sets, `x_train` and `y_train`, `x_test` and `y_test`, respectively.
    The remaining part is simply to display the shape of the dataset (dimensionality)
    for purposes of verification, and also for verification, we can print out sample
    #7 in its original form (the indices) and also its corresponding word. Such a
    portion of the code is a little bit strange if you have not used IMDb before.
    But the major points are that we need to reserve certain indices for special tokens:
    beginning of the sentence `<START>`, unused index `<UNUSED>`, unknown word index
    `<UNK>`, and zero padding index `<PAD>`. One we have made a special allocation
    for these tokens, we can easily map from the indices back to words. These indices
    will be learned by the RNN, and it will learn how to handle them, either by ignoring
    those, or by giving specific weights to them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement the architecture shown in the following diagram, which
    uses all the layers explained previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/50bfc3af-30bc-4ba2-8e63-699a4b32fbec.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3\. An RNN architecture for the IMDb dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram shows the same example (#7 from the training set) that is associated
    with a negative review. The architecture depicted in the diagram along with the
    code that loads the data is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The layers of the model are defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This model uses the standard loss and optimizer that we have used before, and
    the summary produced is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can train the network using the callbacks that we have used before:
    a) early stopping, and b) automatic learning rate reduction. The learning can
    be executed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we save the model and display the loss like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code produces the plot shown in the following diagram, which
    indicates that the network starts to overfit after epoch #3:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/42c839cf-deb9-4108-879b-f8df79fe75cf.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.4\. RNN loss during training
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is quite common in recurrent networks and you should not be surprised
    by this behavior. As of today, with the current algorithms, this happens a lot.
    However, one interesting fact about RNNs is that they also converge really fast
    compared to other traditional models. As you can see, convergence after three
    epochs is not too bad.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we must examine the actual classification performance by looking at the
    balanced accuracy, the confusion matrix, and the **area under the ROC curve**
    (**AUC**). We will do this only in the test set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'First, let''s analyze the plot produced here, which is shown in *Figure 13.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a2deb947-03bf-45c9-a981-eb58bf955efb.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.5\. ROC and AUC of the RNN model calculated in the test set
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram shows a good combination of **True Positive Rates** (**TPR**) and
    **False Positive Rates** (**FPR**), although it is not ideal: we would like to
    see a sharper step-like curve. The AUC is 0.92, which again is good, but the ideal
    would be an AUC of 1.0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the code produces the balanced accuracy and confusion matrix, which
    would look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: First of all, we calculate here the optimal threshold value as a function of
    the TPR and FPR. We want to choose the threshold that will give us the maximum
    TPR and minimum FPR. The threshold and results shown here **will vary** depending
    on the initial state of the network; however, the accuracy should typically be
    around a very similar value.
  prefs: []
  type: TYPE_NORMAL
- en: Once the optimal threshold is calculated, we can use NumPy's `np.where()` method
    to threshold the entire predictions, mapping them to {0, 1}. After this, the balanced
    accuracy is calculated to be 83.82%, which again is not too bad, but also not
    ideal.
  prefs: []
  type: TYPE_NORMAL
- en: One of the possible ways to improve on the RNN model shown in *Figure 13.3*
    would be to somehow give the recurrent layer the ability to *remember* or *forget *specific
    words across layers and have them continue to stimulate neural units across the
    sequence. The next section will introduce a type of RNN with such capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Long short-term memory models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Initially proposed by Hochreiter, **Long Short-Term Memory Models** (**LSTMs**)
    gained traction as an improved version of recurrent models [Hochreiter, S., *et
    al.* (1997)]. LSTMs promised to alleviate the following problems associated with
    traditional RNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: Vanishing gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploding gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inability to remember or forget certain aspects of the input sequences
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following diagram shows a very simplified version of an LSTM. In *(b)*,
    we can see the additional self-loop that is attached to some memory, and in *(c)*,
    we can observe what the network looks like when unfolded or expanded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4f0eb33-ca40-4135-b0b9-9a854216321a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.6\. Simplified representation of an LSTM
  prefs: []
  type: TYPE_NORMAL
- en: There is much more to the model, but the most essential elements are shown in
    *Figure 13.6*. Observe how an LSTM layer receives from the previous time step
    not only the previous output, but also something called **state**, which acts
    as a type of memory. In the diagram, you can see that while the current output
    and state are available to the next layer, these are also available to use at
    any point if they are needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of the things that we are not showing in *Figure 13.6* include the mechanisms
    by which the LSTM remembers or forgets. These can be complex to explain in this
    book for beginners. However, all you need to know at this point is that there
    are three major mechanisms:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output control**: How much an output neuron is stimulated by the previous
    output and the current state'
  prefs:
  - PREF_UL
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Memory control**: How much of the previous state will be forgotten in the
    current state'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input control**: How much of the previous output and new state (memory) will
    be considered to determine the new current state'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: These mechanisms are trainable and optimized for each and every single dataset
    of sequences. But to show the advantages of using an LSTM as our recurrent layer,
    we will repeat the exact same code as before, only changing the RNN by an LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code to load the dataset and build the model is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'The model can be specified as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This essentially replicates the model shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/33464f77-9425-4bb9-8172-8e17ad37ce68.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.7\. LSTM-based neural architecture for the IMDb dataset
  prefs: []
  type: TYPE_NORMAL
- en: Notice that this model has nearly 10,000 more parameters than the simple RNN
    approach. However, the premise is that this increase in parameters should also
    result in an increase in performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We then train our model the same as before, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we save the model and display its performance as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This code will produce the plot shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e37d404e-9f38-4e4b-8609-84255ffe6f5d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.8\. Loss across epochs of training an LSTM
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice from the diagram that the model begins to overfit after **one epoch**.
    Using the trained model at the best point, we can calculate the actual performance
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the ROC shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2baf0a7-1cf9-4b01-b57e-38169a66ec94.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.9\. ROC curve of an LSTM-based architecture
  prefs: []
  type: TYPE_NORMAL
- en: From the plot, we can see that there is a slight gain in the model, producing
    an AUC of 0.93 when the simple RNN model had an AUC of 0.92.
  prefs: []
  type: TYPE_NORMAL
- en: 'When looking at the balanced accuracy and the confusion matrix, which was produced
    by the preceding code, this shows numbers like these:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, we can appreciate that the accuracy was of 85.44%, which is a gain of
    about 2% over the simple RNN. We undertook this experiment simply to show that
    by switching the RNN models, we can easily see improvements. Of course there are
    other ways to improve the models, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Increase/reduce the vocabulary size
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase/reduce the sequence length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase/reduce the embedding dimension
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increase/reduce the neural units in recurrent layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And there may be others besides.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, you have seen how to take text representations (movie reviews), which
    is a common NLP task, and find a way to represent those in a space where you can
    classify them into negative or positive reviews. We did this through embedding
    and LSTM layers, but at the end of this, there is a dense layer with one neuron
    that gives the final output. We can think of this as mapping from the text space
    into a one-dimensional space where we can perform classification. We say this
    because there are three main ways in which to consider these mappings:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequence-to-vector**: Just like the example covered here, mapping sequences
    to an *n-*dimensional space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vector-to-sequence**: This goes the opposite way, from an *n*-dimensional
    space to a sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sequence-to-sequence**: This maps from a sequence to a sequence, usually
    going through an *n*-dimensional mapping in the middle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To exemplify these things, we will use an autoencoder architecture and MNIST
    in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-vector models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, you *technically *saw a sequence-to-vector model, which
    took a sequence (of numbers representing words) and mapped to a vector (of one
    dimension corresponding to a movie review). However, to appreciate these models
    further, we will move back to MNIST as the source of input to build a model that
    will take one MNIST numeral and map it to a latent vector.
  prefs: []
  type: TYPE_NORMAL
- en: Unsupervised model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s work in the autoencoder architecture shown in the following diagram.
    We have studied autoencoders before and now we will use them again since we learned
    that they are powerful in finding vectorial representations (latent spaces) that
    are robust and driven by unsupervised learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6af9a8ff-db2e-4cdc-b0c7-2ba2350e2ca2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.10\. LSTM-based autoencoder architecture for MNIST
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal here is to take an image and find its latent representation, which,
    in the example of *Figure 13.10*, would be two dimensions. However, you might
    be wondering: how can an image be a sequence?'
  prefs: []
  type: TYPE_NORMAL
- en: We can interpret an image as a sequence of rows or as a sequence of columns.
    Let's say that we interpret a two-dimensional image, 28x28 pixels, as a sequence
    of rows; we can look at every row from top to bottom as a sequence of 28 vectors
    whose dimensions are each 1x28\. In this way, we can use an LSTM to process those
    sequences, taking advantage of the LSTM's ability to understand temporal relationships
    in sequences. By this, we mean that, for example in the case of MNIST, the chances
    that a particular row in an image will look like the previous or next row are
    very high.
  prefs: []
  type: TYPE_NORMAL
- en: Notice further that the model proposed in *Figure 13.10* does not require an
    embedding layer as we did before when processing text. Recall that when processing
    text, we need to embed (vectorize) every single word into a sequence of vectors.
    However, with images, they already are sequences of vectors, which eliminates
    the need for an embedding layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code that we will show here has nothing new to show except for two useful
    data manipulation tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RepeatVector()`: This will allow us to arbitrarily repeat a vector. It helps
    in the decoder (see *Figure 13.10*) to go from a vector to a sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TimeDistributed()`: This will allow us to assign a specific type of layer
    to every element of a sequence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These two are part of the `tensorflow.keras.layers` collection. These are implemented
    in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'After loading the data we can define the encoder part of the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we can define the decoder part of the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally we compile and train the model like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The code should print the following output, corresponding to the dimensions
    of the dataset, a summary of the model parameters, followed by the training steps,
    which we omitted in order to save space:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: The model will eventually converge to a valley where it is stopped automatically
    by the callback. After this, we can simply invoke the `encoder` model to literally
    convert any valid sequence (for example, MNIST images) into a vector, which we
    will do next.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can invoke the `encoder` model to convert any valid sequence into a vector
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce a two-dimensional vector with values corresponding to a vectorial
    representation of the sequence `x_test[0]`, which is the first image of the test
    set of MNIST. It might look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: However, remember that this model was trained without supervision, hence, the
    numbers shown here will be different for sure! The encoder model is literally
    our sequence-to-vector model. The rest of the autoencoder model is meant to do
    the reconstruction.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are curious about how the autoencoder model is able to reconstruct a
    28x28 image from a vector of just two values, or if you are curious about how
    the entire test set of MNIST would look when projected in the learned two-dimensional
    space, you can run the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Which displays samples of the original digits, as shown in Figure 11.
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2ee1636e-8781-4b4a-bbfd-b288336795e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11\. MNIST original digits 0-9
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code produces samples of the reconstructed digits:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: The reconstructed digits appear as shown in *Figure 12:*
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4cce3195-d9fc-46ef-83bf-5cee6a876b8d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12\. MNIST reconstructed digits 0-9 using an LSTM-based autoencoder
  prefs: []
  type: TYPE_NORMAL
- en: 'The next piece of code will display a scatter plot of the original data projected
    into the latent space, which is shown in *Figure 13*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that these results may vary due to the unsupervised nature of the autoencoder.
    Similarly, the learned space can be visually conceived to look like the one shown
    in *Figure 13*, where every dot corresponds to a sequence (MNIST digit) that was
    made a vector of two dimensions:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0424aed3-9b57-4bf1-89ec-b2578136e808.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13\. Learned vector space based on the MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 13*, we can see that the sequence-to-vector model is working decently
    even when the reconstruction was based only in two-dimensional vectors. We will
    see larger representations in the next section. However, you need to know that
    sequence-to-vector models have been very useful in the last few years [Zhang,
    Z., *et al.* (2017)].
  prefs: []
  type: TYPE_NORMAL
- en: Another useful strategy is to create vector-to-sequence models, which is going
    from a vectorial representation to a sequential representation. In an autoencoder,
    this would correspond to the decoder part. Let's go ahead and discuss this next.
  prefs: []
  type: TYPE_NORMAL
- en: Vector-to-sequence models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you look back at *Figure 10*, the vector-to-sequence model would correspond
    to the decoder funnel shape. The major philosophy is that most models usually
    can go from large inputs down to rich representations with no problems. However,
    it is only recently that the machine learning community regained traction in producing
    sequences from vectors very successfully (Goodfellow, I., et al. (2016)).
  prefs: []
  type: TYPE_NORMAL
- en: You can think of *Figure 10* again and the model represented there, which will
    produce a sequence back from an original sequence. In this section, we will focus
    on that second part, the decoder, and use it as a vector-to-sequence model. However,
    before we go there, we will introduce another version of an RNN, a bi-directional
    LSTM.
  prefs: []
  type: TYPE_NORMAL
- en: Bi-directional LSTM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A **Bi-directional LSTM** (**BiLSTM**), simply put, is an LSTM that analyzes
    a sequence going forward and backward, as shown in *Figure 14*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/65dedd87-5a7e-4f19-8f0b-00c6aa7dede3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14\. A bi-directional LSTM representation
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following examples of sequences analyzed going forward and backward:'
  prefs: []
  type: TYPE_NORMAL
- en: An audio sequence that is analyzed in natural sound, and then going backward
    (some people do this to look for *subliminal *messages).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A text sequence, like a sentence, that is analyzed for good style going forward,
    and also going backward since some patterns (at least in the English and Spanish
    languages) make reference backward; for example, a verb that makes reference to
    a subject that appears at the beginning of the sentence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An image that has peculiar shapes going from top to bottom, or bottom to top,
    or from side to side and backwards; if you think of the number 9, going from top
    to bottom, a traditional LSTM might forget the round part at the top and remember
    the slim part at the bottom, but a BiLSTM might be able to recall both important
    aspects of the number by going top to bottom and bottom to top.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From *Figure 14 (b)*, we can also observe that the state and output of both
    the forward and backward pass are available at any point in the sequence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can implement the bi-directional LSTM by simply invoking the `Bidirectional()`
    wrapper around a simple LSTM layer. We will then take the architecture in *Figure
    10* and modify it to have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 100 dimensions in the latent space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A BiLSTM replacing LSTM layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An additional dropout layer going from the latent space into the decoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The new architecture will look like *Figure 15*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/66ead033-612e-4d8c-be82-593830cb24ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 15\. Implementing BiLSTMs with a view to building a vector-to-sequence
    model
  prefs: []
  type: TYPE_NORMAL
- en: Recall that the most important point here is to make the latent space (the input
    to the vector-to-sequence model) as rich as possible in order to generate better
    sequences. We are trying to achieve this by increasing the latent space dimensionality
    and adding BiLSTMS. Let's go ahead and implement this and look a the results.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation and results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The code to implement the architecture in *Figure 15* is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'We define the encoder portion of the model as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The decoder portion of the model can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Next we compile the autoencoder and train it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'There is nothing new here, except for the `Bidirectional()` wrapper used that
    has been explained previously. The output should produce a summary of the full
    autoencoder model and the full training operation and will look something like
    this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, after a number of epochs of unsupervised learning, the training will stop
    automatically and we can use the `decoder` model as our vector-to-sequence model.
    But before we do that, we might want to quickly check the quality of the reconstructions
    by running the same code as before to produce the images shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43b0a05b-9283-44a1-84cd-12274c6959e5.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 16\. MNIST digits reconstructed with a BiLSTM autoencoder
  prefs: []
  type: TYPE_NORMAL
- en: If you compare *Figure 11* with *Figure 16*, you will notice that the reconstructions
    are much better and the level of detail is better when compared to the previous
    model reconstructions in *Figure 12*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now we can call our vector-to-sequence model directly with any compatible vector
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output and the plot in *Figure 17*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '![](img/bda7e5f5-ec39-4f70-8ac7-4e566aae3894.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 17\. Sequence produced by a model from a random vector
  prefs: []
  type: TYPE_NORMAL
- en: You can generate as many random vectors as you wish and test your vector-to-sequence
    model. And another interesting thing to observe is a sequence-to-sequence model,
    which we will cover next.
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A Google Brain scientist (Vinyals, O., et al. (2015)) wrote the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '"Sequences have become first-class citizens in supervised learning thanks to
    the resurgence of recurrent neural networks. Many complex tasks that require mapping
    from or to a sequence of observations can now be formulated with the **sequence-to-sequence**
    (**seq2seq**) framework, which employs the chain rule to efficiently represent
    the joint probability of sequences."'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is astoundingly correct because now the applications have grown. Just
    think about the following sequence-to-sequence project ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Document summarization. Input sequence: a document. Output sequence: an abstract.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Image super resolution. Input sequence: a low-resolution image. Output sequence:
    a high-resolution image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Video subtitles. Input sequence: video. Output sequence: text captions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Machine translation. Input sequence: text in source language. Output sequence:
    text in a target language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These are exciting and extremely challenging applications. If you have used
    online translators, chances are you have used some type of sequence-to-sequence
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, to keep it simple, we will continue using the autoencoder
    in *Figure 15* as our main focus, but just to make sure we are all on the same
    page with respect to the generality of sequence-to-sequence models, we will point
    out the following notes:'
  prefs: []
  type: TYPE_NORMAL
- en: Sequence-to-sequence models can map across domains; for example, video to text
    or text to audio.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-sequence models can map in different dimensions; for example, a
    low-res image to high-res or vice versa for compression.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sequence-to-sequence models can use many different tools, such as dense layers,
    convolutional layers, and recurrent layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With this in mind, you can pretty much build a sequence-to-sequence model depending
    of your application. For now, we will come back to the model in *Figure 15* and
    show that the autoencoder is a sequence-to-sequence model in the sense that it
    takes a sequence of rows of an image and produces a sequence of rows of another
    image. Since this is an autoencoder, the input and output dimensions must match.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will limit our showcase of the previously trained sequence-to-sequence model
    (autoencoder) to the following short code snippet, which builds up from the code
    in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain some of these steps. In *(a)*, we calculate the average sequence
    for every single number; this is in response to the question: what can we use
    as our input sequence since doing random is so easy? Well, using the average sequences
    to form the test set sounds interesting enough.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, *(b)* is simply to make the input compatible with the encoder input dimensions.
    Then, *(c)* takes the average sequence and makes a vector out of it. Finally,
    *(d)* uses that vector to recreate the sequence, producing the plot shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c00afefc-9376-43b2-845f-c1b8b8883fa3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 18\. Sequence-to-sequence example outputs
  prefs: []
  type: TYPE_NORMAL
- en: From the diagram, you can easily observe well-defined patterns consistent with
    handwritten numbers, which are generated as sequences of rows by bi-directional
    LSTMs.
  prefs: []
  type: TYPE_NORMAL
- en: Before we finish this, let's have a word on the ethical implications of some
    of these models.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical implications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the resurgence of recurrent models and their applicability in capturing
    temporal information in sequences, there is a risk of finding latent spaces that
    are not properly being fairly distributed. This can be of higher risk in unsupervised
    models that operate in data that is not properly curated. If you think about it,
    the model does not care about the relationships that it finds; it only cares about
    minimizing a loss function, and therefore if it is trained with magazines or newspapers
    from the 1950s, it may find spaces where the word "women" may be close (in terms
    of Euclidean distance) to home labor words such as "broom", "dishes", and "cooking",
    while the word "man" may be close to all other labor such as "driving", "teaching",
    "doctor", and "scientist". This is an example of a bias that has been introduced
    into the latent space (Shin, S.,et al. (2020)).
  prefs: []
  type: TYPE_NORMAL
- en: The risk here is that the vector-to-sequence or sequence-to-sequence models
    will find it much easier to associate a doctor with a man than with a woman, and
    cooking with a woman than with a man, just to name a couple of examples. You can
    take this to images of faces as well and find that certain people with certain
    features might be associated incorrectly. This is why it is so important to undertake
    the type of analysis we are doing here, trying to visualize the latent space whenever
    possible, trying to look at what the model outputs, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The key takeaway here is that, while the models discussed here are extremely
    interesting and powerful, they also carry the risk of learning things about our
    societies that are particularly perceived as unwanted. If the risk exists and
    goes undetected, it might cause bias (Amini, A., et al. (2019)). And if bias goes
    undetected, it might lead to several forms of discrimination. Please always be
    careful about these things as well as things beyond your own societal context.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This advanced chapter showed you how to create RNNs. You learned about LSTMs
    and its bi-directional implementation, which is one of the most powerful approaches
    for sequences that can have distant temporal correlations. You also learned to
    create an LSTM-based sentiment analysis model for the classification of movie
    reviews. You designed an autoencoder to learn a latent space for MNIST using simple
    and bi-directional LSTMs and used it both as a vector-to-sequence model and as
    a sequence-to-sequence model.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should feel confident explaining the motivation behind memory
    in RNNs founded in the need for more robust models. You should feel comfortable
    coding your own recurrent network using Keras/TensorFlow. Furthermore, you should
    feel confident implementing both supervised and unsupervised recurrent networks.
  prefs: []
  type: TYPE_NORMAL
- en: 'LSTMs are great in encoding highly correlated spatial information, such as
    images, or audio, or text, just like CNNs. However, both CNNs and LSTMs learn
    very specific latent spaces that may lack diversity. This can cause a problem
    if there is a malicious hacker that is trying to break your system; if your model
    is very specific to your data, it may create certain sensitivity to variations,
    leading to disastrous consequences in your outputs. Autoencoders solve this by
    using a generative approach called the variational autoencoder, which learns the
    distribution of the data rather than the data itself. However, the question remains:
    How can we implement this idea of generative approaches in other types of networks
    that are not necessarily autoencoders? To find out the answer, you cannot miss
    the next chapter, [Chapter 14](7b09fe4b-078e-4c57-8a81-dc0863eba43d.xhtml), *Generative
    Neural Networks*. The next chapter will present a way of overcoming the fragility
    of neural networks by attacking them and teaching them to be more robust. But
    before you go, quiz yourself with the following questions.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**If both CNNs and LSTMs can model spatially correlated data, what makes LSTMs
    particularly better?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Nothing in general, other than the fact that LSTMs have memory. But in certain
    applications, such as NLP, where a sentence is discovered sequentially as you
    go forward and backward, there are references to certain words at the beginning,
    middle, and end, and multiples at a time. It is easier for BiLSTMs to model that
    behavior faster than a CNN. A CNN may learn to do that, but it may take longer
    to do so in comparison.
  prefs: []
  type: TYPE_NORMAL
- en: '**Does adding more recurrent layers make the network better? **'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No. It can make things worse. It is recommended to keep it simple to no more
    than three layers, unless you are a scientist and are experimenting with something
    new. Otherwise, there should be no more than three recurrent layers in a row in
    an encoder model.
  prefs: []
  type: TYPE_NORMAL
- en: '**What other applications are there for LSTMs?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Audio processing and classification; image denoising; image super-resolution;
    text summarization and other text-processing and classification tasks; word completion;
    chatbots; text completion; text generation; audio generation; image generation.
  prefs: []
  type: TYPE_NORMAL
- en: '**It seems like LSTMs and CNNs haver similar applications. What makes you choose
    one over the other?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: LSTMs are faster to converge; thus, if time is a factor, LSTMs are better. CNNs
    are more stable than LSTMs; thus, if your input is very unpredictable, chances
    are an LSTM might carry the problem across recurrent layers, making it worse every
    time, in which case CNNs could alleviate that with pooling. On a personal level,
    I usually try CNNs first for image-related applications, and LSTMs first for NLP
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). *Learning representations
    by backpropagating errors*. *Nature*, 323(6088), 533-536.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). *Distributed
    representations of words and phrases and their compositionality*. In *Advances
    in neural information processing systems* (pp. 3111-3119).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Pennington, J., Socher, R., and Manning, C. D. (October 2014). *Glove: Global
    vectors for word representation*. In *Proceedings of the 2014 conference on empirical
    methods in natural language processing* (EMNLP) (pp. 1532-1543).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rivas, P., and Zimmermann, M. (December 2019). *Empirical Study of Sentence
    Embeddings for English Sentences Quality Assessment*. In *2019 International Conference
    on Computational Science and Computational Intelligence* (CSCI) (pp. 331-336).
    IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hochreiter, S., and Schmidhuber, J. (1997). *Long short-term memory*. *Neural
    computation*, 9(8), 1735-1780.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zhang, Z., Liu, D., Han, J., and Schuller, B. (2017). *Learning audio sequence
    representations for acoustic event classification*. *arXiv preprint* arXiv:1707.08729.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Goodfellow, I., Bengio, Y., and Courville, A. (2016). *Sequence modeling: Recurrent
    and recursive nets*. *Deep learning*, 367-415.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Vinyals, O., Bengio, S., and Kudlur, M. (2015). *Order matters: Sequence to
    sequence for sets*. *arXiv preprint* arXiv:1511.06391.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shin, S., Song, K., Jang, J., Kim, H., Joo, W., and Moon, I. C. (2020). *Neutralizing
    Gender Bias in Word Embedding with Latent Disentanglement and Counterfactual Generation*.
    *arXiv preprint* arXiv:2004.03133.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Amini, A., Soleimany, A. P., Schwarting, W., Bhatia, S. N., and Rus, D. (January
    2019). *Uncovering and mitigating algorithmic bias through learned latent structure*.
    In *Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society* (pp.
    289-295).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
