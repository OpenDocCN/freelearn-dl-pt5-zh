- en: 6\. Training Techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6\. 训练技巧
- en: This chapter describes important ideas in neural network training, including
    the optimization techniques that are used to search for optimal weight parameters,
    the initial values of weight parameters, and the method for setting hyperparameters—all
    of which are important topics when it comes to neural network training. We will
    look at regularization methods such as weight decay and dropout to prevent overfitting
    and implement them. Lastly, we will look at batch normalization, which has been
    used in a lot of research in recent years. By using the methods described in this
    chapter, you will be able to promote neural network training efficiently to improve
    recognition accuracy.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章描述了神经网络训练中的重要思想，包括用于寻找最优权重参数的优化技术、权重参数的初始值以及设置超参数的方法——这些都是神经网络训练中的重要话题。我们将讨论正则化方法，如权重衰减和丢弃法，以防止过拟合并加以实现。最后，我们将探讨批量归一化，这在近年来的研究中被广泛使用。通过使用本章介绍的方法，你将能够有效地推进神经网络训练，从而提高识别精度。
- en: Updating Parameters
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更新参数
- en: The purpose of neural network training is to find the parameters that minimize
    the value of the loss function. The problem is finding the optimal parameters—a
    process called **optimization**. Unfortunately, the optimization is difficult
    because the parameter space is very complicated, and the optimal solution is difficult
    to find. You cannot do this by solving an equation to obtain the minimum value
    immediately. In a deep network, it is more difficult because the number of parameters
    is huge.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络训练的目的是找到使损失函数值最小化的参数。问题在于找到最优参数——这一过程被称为**优化**。不幸的是，优化非常困难，因为参数空间非常复杂，且最优解很难找到。你不能通过解方程来立即得到最小值。在深度网络中，情况更加困难，因为参数的数量非常庞大。
- en: So far, we have depended on the gradients (derivatives) of the parameters to
    find the optimal parameters. By repeatedly using the gradients of the parameters
    to update the parameters in the gradient direction, we approach the optimal parameters
    gradually. This is a simple method called **stochastic gradient descent** (**SGD**),
    but it is a "smarter" method than searching the parameter space randomly. However,
    SGD is a simple method, and (for some problems) there are some methods that work
    better. So, let's first consider the disadvantage of SGD and introduce other optimization
    techniques.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们依赖参数的梯度（导数）来寻找最优参数。通过反复使用参数的梯度在梯度方向上更新参数，我们逐步接近最优参数。这是一种简单的方法，称为**随机梯度下降法**（**SGD**），但它比随机搜索参数空间是一种“更聪明”的方法。然而，SGD是一种简单的方法，（对于某些问题）也有一些方法能够更好地工作。那么，我们首先来考虑SGD的缺点，并介绍其他优化技术。
- en: Story of an Adventurer
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 冒险者的故事
- en: Before moving on to the main topic, we can consider an allegory to describe
    the situation we are in regarding optimization.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入主题之前，我们可以通过一个寓言来描述我们在优化方面所面临的情况。
- en: Note
  id: totrans-7
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: There is a strange adventurer. He travels through a vast dry region to find
    a deep valley floor every day. His goal is to reach the deepest valley bottom,
    which he calls the "deep place." It is the reason why he travels. In addition,
    he has put two strict "restrictions" on himself. One of them is to not use a map,
    while the other is to cover his eyes. Therefore, he does not know where the deepest
    valley bottom exists in the vast land, and he cannot see anything. Under these
    strict conditions, how can this adventurer look for the "deep place"? How can
    he move to find the "deep place" efficiently?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个奇怪的冒险者。他每天穿越广阔的干旱地区，寻找一片深谷的底部。他的目标是到达最深的谷底，他称之为“深处”。这也是他旅行的原因。此外，他对自己设定了两个严格的“限制”。其中一个是不能使用地图，另一个是要蒙住双眼。因此，他不知道最深的谷底在哪个地方，也看不见任何东西。在这种严格的条件下，这个冒险者如何寻找“深处”？他如何高效地移动以寻找“深处”？
- en: The situation we are in when searching for the optimal parameters is a world
    of darkness just like that of this adventurer. We must look for the "deep place"
    blindfolded and without a map in a vast and complicated landscape
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在寻找最优参数时所处的情境，就像这个冒险者的世界一样是一片黑暗。我们必须在广阔且复杂的地形中，蒙着眼睛、没有地图地寻找“深处”。
- en: What is important in this difficult situation is the "inclination" of the ground.
    The adventurer cannot see around him, but he knows the inclination of the ground
    due to where he stands (his feet can feel it). So, moving in the direction where
    the inclination is the steepest is the strategy of SGD. "By repeating this, I
    may be able to reach the "deep place" someday," the brave adventurer thinks.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种困难的情况下，重要的是“地面的倾斜度”。冒险者看不到周围的环境，但他知道地面的倾斜度，因为他能从站立的位置（他的双脚可以感觉到）感知到它。因此，SGD的策略是沿着地面倾斜度最陡的方向前进。勇敢的冒险者想：“通过不断重复，我可能某天能够到达‘深处’。”
- en: SGD
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SGD
- en: 'Now that we understand the difficulty of this optimization problem, let''s
    start by reviewing SGD. Equation 6.1 represents SGD as follows:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经理解了这个优化问题的困难，接下来让我们回顾一下SGD。方程6.1表示SGD如下：
- en: '| ![65](img/Figure_6.1a.png) | (6.1) |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| ![65](img/Figure_6.1a.png) | (6.1) |'
- en: 'Here, the weight parameters to update are W and the gradients of the loss function
    for W are ![66](img/Figure_6.1b.png). η is the learning rate. We need to predefine
    it as a value, such as 0.01 or 0.001\. `<-` in the equation indicates that the
    value on the right-hand side is used to update the value on the left-hand side.
    As equation 6.1 shows, SGD is a simple method that moves a certain distance in
    the gradient direction. Now, we will implement `SGD` as a class in Python:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，要更新的权重参数是W，W的损失函数梯度是![66](img/Figure_6.1b.png)。η是学习率，我们需要预定义它的值，如0.01或0.001。方程中的`<-`表示右侧的值将用于更新左侧的值。如方程6.1所示，SGD是一个简单的方法，它在梯度方向上移动一定的距离。现在，我们将以Python类的形式实现`SGD`：
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Here, the argument at initialization, `lr`, is the learning rate. The learning
    rate is retained as an instance variable. We will also define the `update(params,
    grads)` method, which is called repeatedly in SGD. The arguments, `params` and
    `grads`, are dictionary variables (as in the implementation of neural networks
    so far). Like `params[''W1'']` and `grads[''W1'']`, each element stores a weight
    parameter or a gradient. By using the `SGD` class, you can update the parameters
    in a neural network as follows (the following code is pseudocode that doesn''t
    run):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，初始化时的参数`lr`是学习率。学习率被保留为一个实例变量。我们还将定义`update(params, grads)`方法，这个方法会在SGD中被反复调用。`params`和`grads`是字典类型的变量（如同我们至今在神经网络实现中所做的那样）。例如，`params['W1']`和`grads['W1']`，每个元素存储着权重参数或梯度。通过使用`SGD`类，你可以像下面这样更新神经网络中的参数（以下代码是伪代码，不可直接运行）：
- en: '[PRE1]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The name of the variable that appears here, `optimizer`, means a "person who
    optimizes." Here, SGD plays this role. The `optimizer` variable takes responsibility
    for updating the parameters. All we need to do here is pass information regarding
    the parameters and gradients to the optimizer.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这里出现的变量名`optimizer`表示“优化者”。在此，SGD扮演着这一角色。`optimizer`变量负责更新参数。我们所需要做的就是将关于参数和梯度的信息传递给优化器。
- en: Thus, separately implementing the class that optimizes facilitates the modularization
    of the features. For example, we will soon implement another optimization technique
    called `update(params, grads)`. Then, we can switch from SGD to Momentum by changing
    the `optimizer = SGD()` statement to `optimizer = Momentum()`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，单独实现一个优化类有助于特性的模块化。例如，我们很快就会实现另一个优化技术，叫做`update(params, grads)`。然后，我们只需将`optimizer
    = SGD()`语句更改为`optimizer = Momentum()`，就能从SGD切换到Momentum优化技术。
- en: Note
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: In many deep learning frameworks, various optimization techniques are implemented,
    and a mechanism is provided so that we can switch between them easily. For example,
    in a deep learning framework called Lasagne, optimization techniques are implemented
    as functions in the `updates.py` file ([http://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py](http://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py)).
    The user can select the desired optimization technique from them.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多深度学习框架中，已经实现了各种优化技术，并提供了机制，让我们可以轻松地在它们之间切换。例如，在一个叫Lasagne的深度学习框架中，优化技术作为函数实现，并存放在`updates.py`文件中（[http://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py](http://github.com/Lasagne/Lasagne/blob/master/lasagne/updates.py)）。用户可以从中选择所需的优化技术。
- en: Disadvantage of SGD
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: SGD的缺点
- en: 'Although SGD is simple and easy to implement, it may be inefficient for some
    problems. To discuss the disadvantage of SGD, let''s consider a problem that calculates
    the minimum value of the following function:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SGD（随机梯度下降）简单且易于实现，但在某些问题上可能效率较低。为了讨论SGD的缺点，我们考虑一个计算以下函数最小值的问题：
- en: '| ![67](img/Figure_6.1c.png) | (6.2) |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| ![67](img/Figure_6.1c.png) | (6.2) |'
- en: The shape of the function represented by equation 6.2 looks like a "bowl" stretched
    in the x-axis direction, as shown in the following plots. Actually, the contour
    lines of equation 6.2 look like ellipses extended in the x-axis direction.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at the gradients of the function that are represented by equation
    6.2\. *Figure 6.2* shows the gradients. These gradients are large in the y-axis
    direction and small in the x-axis direction. In other words, the inclination in
    the y-axis direction is steep, while in the x-axis direction, it's gradual. Note
    that the position of the minimum value of equation 6.2 is `(x, y) = (0, 0)` but
    that the gradients in *Figure 6.2* do not point to the (0, 0) direction in many
    places.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s apply SGD to the function that has the shape shown in the following
    plots. It starts searching at (x, y) = (−7.0, 2.0) (initial values). *Figure 6.3*
    shows the result:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1: Graph of  (left) and its contour lines (right)'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_1.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.1: Graph of ![69](img/Figure_6.1d.png) (left) and its contour lines
    (right)'
  id: totrans-30
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '![Figure 6.2: Gradients of'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_2.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.2: Gradients of ![68](img/Figure_6.1e.png)'
  id: totrans-33
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'SGD moves in a zigzag, as shown in the following plot. The disadvantage of
    SGD is that its search path becomes inefficient if the shape of a function is
    not isotropic—that is, if it is elongated. So, we need a method that is smarter
    than SGD that moves only in the gradient direction. The root cause of SGD''s search
    path being inefficient is that the gradients do not point to the correct minimum
    values:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3: Update path of optimization by SGD – inefficient because it moves'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: in a zigzag to the minimum value (0, 0)
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_3.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.3: Update path of optimization by SGD – inefficient because it moves
    in a zigzag to the minimum value (0, 0)'
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'To improve the disadvantage of SGD, we will introduce three alternative methods:
    Momentum, AdaGrad, and Adam. We will describe each of them briefly and show their
    equations and implementations in Python.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Momentum
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Momentum is related to physics; it means the "quantity of motion." The Momentum
    technique is represented by the following equations:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: '| ![70](img/Figure_6.3a.png) | (6.3) |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
- en: '| ![71](img/Figure_6.3b.png) | (6.4) |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
- en: 'Just like SGD, W is the weight parameter to update, ![72](img/Figure_6.3c.png)
    is the gradients of the loss function for W, and η is the learning rate. A new
    variable that appears here, v, is the "velocity" in physics. Equation 6.3 represents
    a physical law stating that an object receives a force in the gradient direction
    and is accelerated by this force. In Momentum, update functions are used as if
    a ball had been rolled on the ground, as shown in the following diagram:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4: Image of Momentum – a ball rolls on the slope of the ground'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_4.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.4: Image of Momentum – a ball rolls on the slope of the ground'
  id: totrans-47
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The term αv in equation 6.3 slows the object down gradually when it receives
    no force (a value such as 0.9 is set for α). This is the friction created by the
    ground or air resistance. The following code shows the implementation of Momentum
    (the source code is located at `common/optimizer.py`):'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 方程6.3中的项αv会在物体不受力时逐渐减速（为α设置一个值，如0.9）。这就是地面或空气阻力造成的摩擦力。以下代码展示了动量的实现（源代码位于`common/optimizer.py`）：
- en: '[PRE2]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The instance variable, `v`, retains the velocity of the object. At initialization,
    `v` retains nothing. When `update()` is called, it retains the data of the same
    structure as a dictionary variable. The remaining implementation is simple: it
    just implements equations 6.3 and 6.4.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 实例变量`v`保留了物体的速度。在初始化时，`v`不保留任何值。当调用`update()`时，它保留了与字典变量结构相同的数据。剩下的实现很简单：它只需实现方程
    6.3 和 6.4。
- en: Now, let's use Momentum to solve the optimization problem of equation 6.2\.
    The following image shows the result.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用动量来解决方程6.2中的优化问题。下图展示了结果。
- en: 'As shown in the following plot, the update path moves like a ball being rolled
    around in a bowl. You can see that "the degree of zigzag" is reduced compared
    to SGD. The force in the x-axis direction is very small, but the object always
    receives the force in the same direction and is accelerated constantly in the
    same direction. On the other hand, the force in the y-axis direction is large,
    but the object receives the forces in the positive and negative directions alternately.
    They cancel each other out, so the velocity in the y-axis direction is unstable.
    This can accelerate the motion in the x-axis direction and reduce the zigzag motion
    compared to SGD:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如下图所示，更新路径像一个球在碗中滚动。可以看到，与SGD相比，“之字形程度”有所减少。x轴方向上的力非常小，但物体始终在相同的方向上受到力，并且在相同的方向上不断加速。另一方面，y轴方向上的力很大，但物体在正负方向上交替受力，彼此抵消，因此y轴方向上的速度不稳定。这可以加速x轴方向上的运动，并减少与SGD相比的之字形运动：
- en: '![Figure 6.5: Update path for optimization by Momentum'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.5：通过动量优化的更新路径'
- en: '](img/fig06_5.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_5.jpg)'
- en: 'Figure 6.5: Update path for optimization by Momentum'
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.5：通过动量优化的更新路径
- en: AdaGrad
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: AdaGrad
- en: In neural network training, the value of the learning `rate--η` in the `equation--`
    is important. If it is too small, training takes too long. If it is too large,
    divergence occurs, and correct training cannot be achieved.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络训练中，学习`rate--η`在`equation--`中的值非常重要。如果它太小，训练会持续太长时间。如果它太大，训练会发散，无法达到正确的训练效果。
- en: There is an effective technique for the learning rate called **learning rate
    decay**. It uses a lower learning rate as training advances. This method is often
    used in neural network training. A neural network learns "much" first and learns
    "less" gradually.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有一种有效的学习率技术叫做**学习率衰减**。它随着训练的进行使用更低的学习率。这种方法在神经网络训练中经常使用。神经网络首先学习“很多”，然后逐渐学习“更少”。
- en: 'Reducing the learning rate gradually is the same as reducing the values of
    the learning rates for all the parameters collectively. AdaGrad ( *John Duchi,
    Elad Hazan, and Yoram Singer (2011): Adaptive Subgradient Methods for Online Learning
    and Stochastic Optimization. Journal of Machine Learning Research 12, Jul (2011),
    2121 – 2159.*) is an advanced version of this method. AdaGrad creates a custom-made
    value for each parameter.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '逐渐降低学习率等同于逐渐减小所有参数的学习率。AdaGrad（*John Duchi, Elad Hazan, and Yoram Singer (2011):
    Adaptive Subgradient Methods for Online Learning and Stochastic Optimization.
    Journal of Machine Learning Research 12, Jul (2011), 2121 – 2159.*）是这种方法的一个高级版本。AdaGrad为每个参数创建了一个定制的值。'
- en: 'AdaGrad adjusts the learning rate for each element of the parameter adaptively
    for training (the "Ada" in AdaGrad comes from "Adaptive"). Now, we will show AdaGrad''s
    update method with equations:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: AdaGrad会自适应地调整每个参数元素的学习率以进行训练（AdaGrad中的"Ada"来自于"Adaptive"）。现在，我们将通过方程展示AdaGrad的更新方法：
- en: '| ![73](img/Figure_6.5a.png) | (6.5) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| ![73](img/Figure_6.5a.png) | (6.5) |'
- en: '| ![74](img/Figure_6.5b.png) | (6.6) |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| ![74](img/Figure_6.5b.png) | (6.6) |'
- en: Just like SGD, W is the weight parameters to update, ![75](img/Figure_6.5c.png)
    is the gradients of the loss function for W, and η is the learning rate. Here,
    a new variable, h, appears. The h variable stores the sum of the squared gradient
    values thus far, as shown in equation 6.5 (⊙ in equation 6.5 indicates multiplication
    between array elements). When updating parameters, AdaGrad adjusts the scale of
    learning by multiplying ![76](img/Figure_6.5d.png). For the parameter element
    that moved significantly (i.e., was updated heavily), the learning rate becomes
    smaller. Thus, you can attenuate the learning rate for each parameter element
    by gradually reducing the learning rate of the parameter that moved significantly.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 就像SGD一样，W是需要更新的权重参数，![75](img/Figure_6.5c.png)是W的损失函数梯度，η是学习率。这里引入了一个新变量h。h变量存储了迄今为止梯度值的平方和，如方程6.5所示（方程6.5中的⊙表示数组元素之间的乘法）。在更新参数时，AdaGrad通过乘以![76](img/Figure_6.5d.png)来调整学习的规模。对于已经显著移动的参数元素（即已经进行了大量更新的参数），学习率会变小。因此，您可以通过逐渐减少那些显著移动的参数的学习率来衰减每个参数元素的学习率。
- en: Note
  id: totrans-64
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: 'AdaGrad records all the past gradients as the sum of squares. Therefore, as
    learning advances, the degree of update becomes small. When learning is conducted
    infinitely, the degree of update becomes 0, resulting in no update. The RMSProp
    (*Tieleman, T., & Hinton, G. (2012): Lecture 6.5—RMSProp: Divide the gradient
    by a running average of its recent magnitude. COURSERA: Neural Networks for Machine
    Learning*) method solves this problem. It does not add all the past gradients
    equally. It forgets the past gradients gradually and conducts addition so that
    the information about new gradients is clearly reflected. This reduces the scale
    of the past gradients exponentially, which is called the "exponential moving average."'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 'AdaGrad将所有过去的梯度记录为平方和。因此，随着学习的进行，更新的程度变小。当学习无限进行时，更新的程度变为0，导致没有更新。RMSProp（*Tieleman,
    T., & Hinton, G. (2012): Lecture 6.5—RMSProp: Divide the gradient by a running
    average of its recent magnitude. COURSERA: Neural Networks for Machine Learning*）方法解决了这个问题。它不会对所有过去的梯度进行等权重处理，而是逐渐遗忘过去的梯度，并进行加权处理，从而使新梯度的信息得以清晰反映。这样，过去梯度的规模会指数衰减，这就是所谓的“指数加权平均”。'
- en: 'Now, let''s implement AdaGrad. You can implement AdaGrad as follows (the source
    code is located at `common/optimizer.py`):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们来实现AdaGrad。您可以按如下方式实现AdaGrad（源代码位于`common/optimizer.py`）：
- en: '[PRE3]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Note that a small value of `1e-7` was added in the last line. This prevents
    division by `0` when `self.h[key]` contains `0`. In many deep learning frameworks,
    you can configure this small value as a parameter, but here, a fixed value, `1e-7`,
    is used.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最后一行添加了一个小的值`1e-7`。这可以防止当`self.h[key]`包含`0`时发生除以`0`的情况。在许多深度学习框架中，您可以将这个小值配置为一个参数，但在这里，使用了一个固定值`1e-7`。
- en: 'Now, let''s use AdaGrad to solve the optimization problem of equation 6.2\.
    The following image shows the result:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用AdaGrad来解决方程6.2中的优化问题。下图展示了结果：
- en: '![Figure 6.6: Update path for optimization by AdaGrad'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.6：AdaGrad优化的更新路径'
- en: '](img/fig06_6.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_6.jpg)'
- en: 'Figure 6.6: Update path for optimization by AdaGrad'
  id: totrans-72
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6.6：AdaGrad优化的更新路径
- en: The result shown in the preceding image shows that the parameters are moving
    efficiently to the minimum value. The parameters move a lot at first because the
    gradient in the y-axis direction is large. Adjustment is conducted in proportion
    to the large motion so that the update step becomes small. Thus, the degree of
    update in the y-axis direction is weakened, reducing the zigzag motion.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的图像显示了参数高效地向最小值移动。最初，参数移动较多，因为y轴方向的梯度很大。调整与大幅运动成比例进行，因此更新步长变小。这样，y轴方向的更新程度被减弱，从而减少了锯齿形的波动。
- en: Adam
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Adam
- en: In Momentum, the parameters move based on physical law, such as a ball rolled
    in a bowl. AdaGrad adjusts the update step adaptively for each parameter element.
    So, what happens when the two techniques, Momentum and AdaGrad, are combined?
    This is the basic idea of the technique called Adam (this explanation of Adam
    is intuitive and lacking some of the finer technical details. For a more granular
    definition, please see the original article).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在Momentum中，参数的更新基于物理法则，就像一个球在碗中滚动一样。AdaGrad为每个参数元素自适应地调整更新步长。那么，当Momentum和AdaGrad这两种技术结合时，会发生什么呢？这就是Adam技术的基本思想（这里对Adam的解释是直观的，缺少一些更细致的技术细节。如需更精确的定义，请参阅原文）。
- en: 'Adam is a new technique that was proposed in 2015\. The theory is slightly
    complicated. Intuitively, it is like a combination of Momentum and AdaGrad. By
    combining the advantages of these two techniques, we can expect to search the
    parameter space efficiently. The "bias correction" of hyperparameters is also
    a characteristic of Adam. For more details, please see the original paper (*Diederik
    Kingma and Jimmy Ba. (2014): Adam: A Method for Stochastic Optimization. arXiv:1412.6980[cs]
    (December 2014)*). It is implemented in Python as the `Adam` class in `common/optimizer.py`.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 'Adam 是一种新的技术，于2015年提出。其理论略显复杂。直观上来说，它类似于动量和AdaGrad的结合体。通过结合这两种技术的优势，我们可以期待有效地搜索参数空间。超参数的“偏差校正”也是Adam的一个特点。更多细节，请参阅原始论文（*Diederik
    Kingma 和 Jimmy Ba. (2014): Adam: A Method for Stochastic Optimization. arXiv:1412.6980[cs]
    (2014年12月)*）。在Python中实现为`common/optimizer.py`中的`Adam`类。'
- en: Now, let's use Adam to solve the optimization problem of equation 6.2\. The
    following figure shows the result.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用Adam来解决方程6.2的优化问题。下图显示了结果。
- en: '![Figure 6.7: Update path for optimization by Adam'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.7: Adam优化的更新路径'
- en: '](img/fig06_7.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_7.jpg)'
- en: 'Figure 6.7: Update path for optimization by Adam'
  id: totrans-80
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.7: Adam优化的更新路径'
- en: As shown in *Figure 6.7*, the update path by Adam moves as if a ball has been
    rolled in a bowl. The motion is similar to that in Momentum, but the left and
    right motions of the ball are smaller. This advantage is caused by the adaptive
    adjustment of the learning rate.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 6.7*所示，Adam的更新路径就像是把球滚进碗里一样移动。这种运动类似于动量，但球的左右运动较小。这种优势是由学习率的自适应调整引起的。
- en: Note
  id: totrans-82
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: Adam has three hyperparameters. The first is the learning rate (appearing as
    α in the paper). The others are the coefficient for the primary moment, β1, and
    the coefficient for the secondary moment, β2\. The article states that the standard
    values are 0.9 for β1 and 0.999 for β2, which are effective in many cases.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: Adam有三个超参数。第一个是学习率（在论文中表示为α）。其他两个是主动量的系数β1和次动量的系数β2。文章指出，标准值分别为β1为0.9，β2为0.999，在许多情况下非常有效。
- en: Which Update Technique Should We Use?
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 我们应该使用哪种更新技术？
- en: We have considered four-parameter updating techniques so far. Here, we will
    compare their results (the source code is located at `ch06/optimizer_compare_naive.py`).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经考虑了四种参数更新技术。在这里，我们将比较它们的结果（源代码位于`ch06/optimizer_compare_naive.py`）。
- en: 'As shown in *Figure 6.8*, different techniques use different paths to update
    the parameters. This image seems to show that AdaGrad is the best, but note that
    the results vary depending on the problems being solved. Naturally, the results
    also vary depending on the values of the hyperparameters (such as the learning
    rate):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如*图 6.8*所示，不同的技术使用不同的路径来更新参数。这幅图似乎显示AdaGrad是最好的，但请注意，结果因解决的问题而异。当然，结果也会因超参数的值（如学习率）而异：
- en: '![Figure 6.8: Comparison of optimization techniques – SGD, Momentum, AdaGrad,
    and Adam'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.8: 优化技术的比较 – SGD、动量、AdaGrad 和 Adam'
- en: '](img/fig06_8.jpg)'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_8.jpg)'
- en: 'Figure 6.8: Comparison of optimization techniques – SGD, Momentum, AdaGrad,
    and Adam'
  id: totrans-89
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: '图 6.8: 优化技术的比较 – SGD、动量、AdaGrad 和 Adam'
- en: 'So far, we have looked at four techniques: SGD, Momentum, AdaGrad, and Adam.
    But which should we use? Unfortunately, there is no one technique currently known
    that is good at solving all problems. Each has its own distinct characteristics
    and advantages, which make it better suited to certain problems over others. Therefore,
    it''s important to know which technique works best given a specific set of circumstances.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看过四种技术：SGD、动量、AdaGrad 和 Adam。但我们应该使用哪一种？遗憾的是，目前没有一种被广泛认可的技术能够解决所有问题。每种技术都有其独特的特点和优势，使其更适合某些问题而不适合其他问题。因此，了解在特定情况下哪种技术最有效非常重要。
- en: SGD is still used in a lot of research. Momentum and AdaGrad are also worth
    trying. Recently, many researchers and engineers seem to prefer Adam. This book
    mainly uses SGD and Adam. You can try the other techniques as you like.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: SGD仍然被广泛应用于许多研究中。动量和AdaGrad也值得尝试。最近，许多研究人员和工程师似乎更喜欢Adam。本书主要使用SGD和Adam。您可以根据需要尝试其他技术。
- en: Using the MNIST Dataset to Compare the Update Techniques
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用MNIST数据集比较更新技术
- en: 'For handwritten digit recognition, we will compare the four techniques we''ve
    described so far: SGD, Momentum, AdaGrad, and Adam. Let''s explore how each technique
    works in the progress of training. *Figure 6.9* shows the results (the source
    code is located at `h06/optimizer_compare_mnist.py`):'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.9: Using the MNIST dataset to compare the four update techniques
    – the horizontal axis indicates the iterations of learning, while the vertical
    axis indicates the values of the loss function'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_9.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.9: Using the MNIST dataset to compare the four update techniques –
    the horizontal axis indicates the iterations of learning, while the vertical axis
    indicates the values of the loss function'
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: This experiment used a five-layer neural network, and each layer had 100 neurons.
    ReLU was used as the activation function.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: The result of *Figure 6.9* shows that other techniques learned faster than SGD.
    It seems that the remaining three techniques learned similarly quickly. When we
    look closer, it seems that AdaGrad learned a little faster. In this experiment,
    note that the results are different depending on the hyperparameter of the learning
    rate and the structure of the neural network (the number of layers). However,
    generally, the other three techniques can learn faster than SGD and sometimes
    achieve better final recognition performance.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Initial Weight Values
  id: totrans-99
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The initial weight values are especially important in neural network training.
    What values are set as the initial weight values often determines the success
    or failure of neural network training. In this section, we will explain the recommended
    initial weight values, then conduct an experiment to check that they accelerate
    neural network learning.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: How About Setting the Initial Weight Values to 0?
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Later, we will look at a technique called weight decay, which reduces overfitting
    and improves generalization performance. In short, weight decay is a technique
    that reduces the values of the weight parameters to prevent overfitting.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: If we want the weights to be small, starting with the smallest possible initial
    values is probably a good approach. Here, we use an initial weight value such
    as `0.01 * np.random.randn(10, 100)`. This small value is the value generated
    from the Gaussian distribution multiplied by 0.01—a Gaussian distribution with
    a standard deviation of 0.01.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: If we want the weight values to be small, how about setting all the initial
    weight values to 0? This is a bad idea as it prevents us from training correctly.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: Why should the initial weight values not be 0? Or in other words, why should
    the weights not be uniform values? Well, because all weight values are updated
    uniformly (in the same way) in backpropagation. So, say that layers 1 and 2 have
    0 as their weights in a two-layer neural network. Then, in forward propagation,
    the same value is propagated to all the neurons in layer 2 because the weight
    of the input layer is 0\. When the same values are entered for all the neurons
    in layer 2, all the weights in layer 2 are updated similarly in backward propagation
    (please remember "backward propagation in a multiplication node"). Therefore,
    the weights are updated with the same value and become symmetrical values (duplicate
    values). Due to this, there is no meaning in having many weights. To prevent the
    weights from being uniform or breaking their symmetrical structure, random initial
    values are required.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么初始权重值不应该是0？换句话说，为什么权重不应该是统一值？因为在反向传播中，所有权重值会以相同的方式均匀更新。所以，假设在一个两层神经网络中，第一层和第二层的权重都是0。那么，在前向传播时，输入层的所有神经元的值都会被传递到第二层，因为输入层的权重是0。当相同的值输入到第二层的所有神经元时，在反向传播时，第二层的所有权重都会以相同的方式更新（请记住“在乘法节点中的反向传播”）。因此，权重以相同的值更新，并且变成对称值（重复值）。因此，拥有许多权重没有意义。为了防止权重统一或破坏其对称结构，需要使用随机初始值。
- en: Distribution of Activations in the Hidden Layers
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 隐藏层激活值的分布
- en: 'Observing the distribution of activations (referring here to the output data
    after the activation function, though some literature calls the data that flows
    between layers an "activation") in the hidden layers provides a lot of information.
    Here, we will conduct a simple experiment to see how the initial weight values
    change the activations in the hidden layers. We will enter some randomly generated
    data into a five-layer neural network (using a sigmoid function as the activation
    function) and show the data distribution of the activations in each layer in a
    histogram. This experiment is based on the CS231n (*CS231n: Convolutional Neural
    Networks for Visual Recognition* ([http://cs231n.github.io/](http://cs231n.github.io/)))
    course at Stanford University.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '观察隐藏层中激活值的分布（这里指的是激活函数后的输出数据，尽管一些文献称流经各层的数据为“激活”）提供了很多信息。在这里，我们将进行一个简单的实验，看看初始权重值如何改变隐藏层中的激活值。我们将一些随机生成的数据输入到一个五层神经网络中（使用sigmoid函数作为激活函数），并通过直方图展示每一层激活值的分布。这个实验基于斯坦福大学的CS231n课程（*CS231n:
    卷积神经网络与视觉识别* ([http://cs231n.github.io/](http://cs231n.github.io/)))。'
- en: 'The source code for the experiment is located at `ch06/weight_init_activation_histogram.py`.
    The following is part of this code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 实验的源代码位于`ch06/weight_init_activation_histogram.py`。以下是部分代码：
- en: '[PRE4]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Here, there are five layers and that each layer has 100 neurons. As input data,
    1,000 pieces of data are generated at random with Gaussian distribution and are
    provided to the five-layer neural network. A sigmoid function is used as the activation
    function, and the activation results of each layer are stored in the `activations`
    variable. Please note the weight scale. Here, a Gaussian distribution with a standard
    deviation of 1 is being used. The purpose of this experiment is to observe how
    the distribution of `activations` changes by changing this scale (standard deviation).
    Now, let''s show the data of each layer that is stored in `activations` in a histogram:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有五层，每一层有100个神经元。作为输入数据，随机生成1,000个数据点，服从高斯分布，并提供给这五层神经网络。使用sigmoid函数作为激活函数，每一层的激活结果存储在`activations`变量中。请注意权重的规模。这里使用标准差为1的高斯分布。这个实验的目的是通过改变这个尺度（标准差）来观察`activations`的分布如何变化。现在，让我们展示存储在`activations`中的每一层数据，并以直方图的形式呈现：
- en: '[PRE5]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Executing this code creates the histograms shown in the following image.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 执行此代码将生成下图所示的直方图。
- en: This image shows that the activations of each layer are mainly 0 and 1\. The
    sigmoid function that's being used here is an S-curve function. As the output
    of the sigmoid function approaches 0 (or 1), the value of the differential approaches
    0\. Therefore, when the data is mainly 0s and 1s, the values of the gradients
    in backward propagation get smaller until they vanish. This is a problem called
    **gradient vanishing**. In deep learning, where there's a large number of layers,
    gradient vanishing can be a more serious problem.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 该图显示，每一层的激活值主要为 0 和 1。这里使用的 sigmoid 函数是一种 S 曲线函数。当 sigmoid 函数的输出接近 0（或 1）时，微分的值会接近
    0。因此，当数据主要是 0 和 1 时，反向传播中的梯度值会变得越来越小，直到消失。这就是所谓的**梯度消失**问题。在深度学习中，层数较多时，梯度消失问题可能会变得更加严重。
- en: 'Next, let''s conduct the same experiment, but this time with the standard deviation
    of the weights as 0.01\. To set the initial weight values, you will need to modify
    the previous code, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们进行相同的实验，但这次权重的标准差为 0.01。为了设置初始权重值，您需要修改之前的代码，如下所示：
- en: '[PRE6]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![Figure 6.10: Distribution of the activations of each layer when a Gaussian
    distribution with a standard deviation of 1 is used for the initial weight values'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.10：当使用标准差为 1 的高斯分布作为初始权重值时，各层激活值的分布'
- en: '](img/fig06_10.jpg)'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_10.jpg)'
- en: 'Figure 6.10: Distribution of the activations of each layer when a Gaussian
    distribution with a standard deviation of 1 is used for the initial weight values'
  id: totrans-118
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.10：当使用标准差为 1 的高斯分布作为初始权重值时，各层激活值的分布
- en: 'Observe the results. The following image shows the distribution of the activations
    of each layer when a Gaussian distribution with a standard deviation of 0.01 is
    used:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 观察结果。以下图像显示了当使用标准差为 0.01 的高斯分布作为初始权重值时，各层激活值的分布：
- en: '![Figure 6.11: Distribution of the activations of each layer when a Gaussian
    distribution with a standard deviation of 0.01 is used for the initial weight
    values'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 6.11：当使用标准差为 0.01 的高斯分布作为初始权重值时，各层激活值的分布'
- en: '](img/fig06_11.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/fig06_11.jpg)'
- en: 'Figure 6.11: Distribution of the activations of each layer when a Gaussian
    distribution with a standard deviation of 0.01 is used for the initial weight
    values'
  id: totrans-122
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6.11：当使用标准差为 0.01 的高斯分布作为初始权重值时，各层激活值的分布
- en: Now, the activations concentrate around 0.5\. Unlike the previous example, they
    are not biased toward 0 and 1\. The problem of gradient vanishing does not occur.
    However, when activations are biased, it causes a large problem in terms of its
    representation. If multiple neurons output almost the same values, there is no
    meaning in the existence of multiple neurons. For example, when 100 neurons output
    almost the same values, one neuron can represent almost the same thing. Therefore,
    the biased activations cause a problem because representation is limited.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，激活值集中在 0.5 附近。与之前的例子不同，它们并不偏向于 0 和 1。梯度消失问题没有发生。然而，当激活值存在偏差时，会在表示能力上造成很大问题。如果多个神经元输出几乎相同的值，那么这些神经元就没有存在的意义。例如，当
    100 个神经元输出几乎相同的值时，一个神经元就可以代表几乎相同的内容。因此，偏置的激活值会导致问题，因为表示能力受到限制。
- en: Note
  id: totrans-124
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 注意
- en: The distribution of the activations in each layer needs to be spread properly.
    This is because, when moderately diverse data flows in each layer, a neural network
    learns efficiently. On the other hand, when biased data flows, training may not
    go well because of the gradient vanishing and "limited representation."
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 各层激活值的分布需要适当地分散。这是因为，当每一层的输入数据适度多样时，神经网络能够高效地学习。另一方面，当数据有偏时，训练可能会因为梯度消失和“有限的表示能力”而变得不顺利。
- en: 'Next, we will use the initial weight values that were recommended in a paper
    by Xavier Glorot et al. (*Xavier Glorot and Yoshua Bengio (2010): Understanding
    the difficulty of training deep feedforward neural networks. In Proceedings of
    the International Conference on Artificial Intelligence and Statistics (AISTATS2010).
    Society for Artificial Intelligence and Statistics*). This is called "Xavier initialization."
    Currently, the Xavier initializer is usually used in ordinary deep learning frameworks.
    For example, in the Caffe framework, you can specify the `xavier` argument for
    the initial weight setting to use the Xavier initializer.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: 'Xavier''s paper obtained the appropriate scale of weights so that the activation
    of each layer was spread similarly. It concluded that distribution with a standard
    deviation of ![6a](img/Figure_6.11a.png) should be used when the number of nodes
    in the previous layer is n (Xavier''s paper suggested setting values that consider
    both the number of input nodes in the previous layer and the number of output
    nodes in the next layer. However, in framework implementations such as Caffe,
    the values are only calculated based on the input nodes in the previous layer
    for simplification, as described here). This can be seen in the following diagram:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12: Xavier initializer – when n nodes in the previous layer are
    connected, a distribution with the standard deviation of  is used for initial
    values'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_12.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.12: Xavier initializer – when n nodes in the previous layer are connected,
    a distribution with the standard deviation of ![6b](img/Figure_6.12a.png) is used
    for initial values'
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'When the Xavier initializer is used, since the number of nodes in the previous
    layer is larger, the weight scale that is set for the initial values for the target
    nodes is smaller. Now, let''s use the Xavier initializer to complete some experiments.
    You only have to modify the initial weight value, as follows (the implementation
    is simplified here because the number of nodes is 100 in all the layers):'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '![Figure 6.13: Distribution of the activations of each layer when the Xavier
    initializer is used as the initial weight value'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_13.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.13: Distribution of the activations of each layer when the Xavier
    initializer is used as the initial weight value'
  id: totrans-135
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The preceding image shows the results when the Xavier initializer is used. It
    shows that distributions are spread more widely, although a higher layer has a
    more distorted shape. We can expect that training is conducted efficiently because
    the data that flows in each layer is spread properly, and the representation of
    the sigmoid function is not limited.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Also, the distributions of the upper layers are slightly distorted in terms
    of their shape. The distorted shape is improved when a `tanh` function (hyperbolic
    function) is used instead of a `sigmoid` function. Actually, when a `tanh` function
    is used, distributions will have a bell shape. The `tanh` function is an S-curve
    function, like a `sigmoid` function. The `tanh` function is symmetrical about
    the origin (0, 0), while the `sigmoid` function is symmetrical about `(x, y) =
    (0, 0.5)`. It is best to use the `tanh` function so that the activation function
    is symmetrical about the origin.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，上层的分布在形状上略有扭曲。使用`tanh`函数（双曲函数）代替`sigmoid`函数时，扭曲的形状有所改善。实际上，当使用`tanh`函数时，分布将呈现钟形曲线。`tanh`函数是一种S型曲线函数，类似于`sigmoid`函数。`tanh`函数关于原点(0,
    0)对称，而`sigmoid`函数则关于`(x, y) = (0, 0.5)`对称。最好使用`tanh`函数，这样激活函数就能关于原点对称。
- en: Initial Weight Values for ReLU
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: ReLU的初始权重值
- en: 'The Xavier initializer is based on the assumption that the activation function
    is linear. The Xavier initializer is suitable because the `sigmoid` and `tanh`
    functions are symmetrical and can be regarded as linear functions around their
    centers. Meanwhile, for ReLU, using the initial value is recommended. This is
    known as the He initializer and was recommended by Kaiming He and et. al. *(Kaiming
    He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun (2015): Delving Deep into Rectifiers:
    Surpassing Human-Level Performance on ImageNet Classification. In 1026 – 1034*).
    The He initializer uses a Gaussian distribution with a standard deviation of ![6d](img/Figure_6.13a.png)
    when the number of nodes in the previous layer is n. When we consider that the
    Xavier initializer is ![6e](img/Figure_6.13b.png), we can assume (intuitively)
    that the coefficient must be doubled to provide more spread because a negative
    area is 0 for ReLU.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Xavier初始化器基于激活函数是线性的假设。Xavier初始化器是合适的，因为`sigmoid`和`tanh`函数是对称的，可以在其中心附近被视为线性函数。与此同时，对于ReLU，建议使用初始值。这被称为He初始化器，由Kaiming
    He等人提出并推荐（*Kaiming He, Xiangyu Zhang, Shaoqing Ren, 和 Jian Sun (2015)：深入研究整流器：超越ImageNet分类中的人类水平表现。在1026
    – 1034*）。He初始化器使用标准差为![6d](img/Figure_6.13a.png)的高斯分布，当前一层的节点数为n时。如果我们考虑到Xavier初始化器是![6e](img/Figure_6.13b.png)，我们可以直观地假设系数必须加倍，以提供更广的分布，因为对于ReLU，负区域为0。
- en: Let's look at the distribution of activations when ReLU is used as the activation
    function. We will consider the results of three experiments after using a Gaussian
    distribution with a standard deviation of 0.01 (that is, `std=0.01`), the Xavier
    initializer, and the He initializer, which is specifically used for ReLU (*Figure
    6.14*).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看一下当ReLU作为激活函数时激活分布的情况。我们将考虑在使用标准差为0.01（即`std=0.01`）的高斯分布、Xavier初始化器和专为ReLU设计的He初始化器后进行的三个实验的结果（*图6.14*）。
- en: 'The results indicate that the activations of each layer are very small (the
    averages of the distributions are as follows: layer 1: 0.0396, layer 2: 0.00290,
    layer 3: 0.000197, layer 4: 1.32e-5, and layer 5: 9.46e-7) for `std=0.01`. When
    small data flows through a neural network, the gradients of the weights in backward
    propagation are also small. This is a serious problem as training will barely
    advance.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，当`std=0.01`时，每一层的激活值都非常小（各层分布的平均值如下：第1层：0.0396，第2层：0.00290，第3层：0.000197，第4层：1.32e-5，第5层：9.46e-7）。当小数据流过神经网络时，反向传播中权重的梯度也很小。这是一个严重的问题，因为训练几乎无法推进。
- en: Next, let's look at the results from using the Xavier initializer. This shows
    that the bias becomes larger little by little as the layers become deeper—as do
    the activations. Gradient vanishing will be a problem when it comes to training.
    On the other hand, for the He initializer, the spread of Gaussian distribution
    in each layer is similar. The spread of data is similar even when the layers are
    deeper. So, we can expect that appropriate values also flow for backward propagation.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们来看一下使用Xavier初始化器的结果。这表明，随着层数加深，偏置逐渐增大，激活值也一样。训练时会出现梯度消失的问题。另一方面，对于He初始化器，每一层中高斯分布的扩展是相似的。即使层数更深，数据的扩展也相似。因此，我们可以预期在反向传播时也能流动适当的值。
- en: In summary, when you use ReLU as the activation function, use the He initializer,
    and for S-curve functions such as `sigmoid` and `tanh`, use the Xavier initializer.
    As of the time of writing, this is the best practice.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，当使用ReLU作为激活函数时，应使用He初始化器；对于像`sigmoid`和`tanh`这样的S型曲线函数，应使用Xavier初始化器。至于目前写作时，这是最佳实践。
- en: Using the MNIST Dataset to Compare the Weight Initializers
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s use actual data to see how neural network learning is affected by different
    weight initializers. We will use `std=0.01`, the Xavier initializer, and the He
    initializer in our experiments (the source code is located at `ch06/weight_init_compare.py`).
    The following image shows the results:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14: Change of activation distribution by weight initializers when
    ReLU is used'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: as the activation function
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_14.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.14: Change of activation distribution by weight initializers when
    ReLU is used as the activation function'
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This experiment uses a five-layer neural network (100 neurons in each layer)
    and ReLU as the activation function. The results shown in the following image
    reveal that no learning is conducted for `std=0.01`. This is because small values
    (data near 0) flow in forward propagation, as we observed in the distribution
    of activations earlier. Thus, the gradients to obtain are also small in backward
    propagation, resulting in few updates occurring for the weights. On the other
    hand, training is performed smoothly for the Xavier and He initializers. The following
    image also shows that training advances fast for the He initializer:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15: Using the MNIST dataset to compare the weight initializers –
    the horizontal axis indicates the iterations of training, while the vertical axis
    indicates the values of the loss function'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_15.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.15: Using the MNIST dataset to compare the weight initializers – the
    horizontal axis indicates the iterations of training, while the vertical axis
    indicates the values of the loss function'
  id: totrans-154
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: As we have seen, the initial weight values are very important in neural network
    training. They often determine their success or failure. Although the importance
    of the initial weight values is sometimes overlooked, the starting (initial) value
    is important for everything.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalization
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous section, we observed the distribution of activations in each
    layer. We learned that the appropriate initial weight values provide a proper
    spread for the distribution of activations of each layer, thus enabling smooth
    training. So, how about adjusting the distribution of activations "forcefully"
    so that there's a proper spread in each layer?
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'This technique is based on the idea of batch normalization (*Sergey Ioffe and
    Christian Szegedy (2015): Batch Normalization: Accelerating Deep Network Training
    by Reducing Internal Covariate Shift. arXiv:1502.03167[cs] (February 2015)*).'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: Batch Normalization Algorithm
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Batch normalization (also known as batch norm) was first proposed in 2015\.
    Although batch norm is a new technique, it is widely used by many researchers
    and engineers. In fact, in competitions surrounding machine learning, batch norm
    often achieves excellent results.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'Batch norm attracts a lot of attention due to the following advantages:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: It can accelerate learning (it can increase the learning rate).
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is not as dependent on the initial weight values (you do not need to be cautious
    about the initial values).
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It reduces overfitting (it reduces the necessity of dropout).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first advantage is particularly attractive because deep learning takes a
    lot of time. With batch norm there's no need to be anxious about the initial weight
    values, and due to it reducing overfitting, it removes this cause of anxiety from
    deep learning.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'As we described earlier, the purpose of batch norm is to adjust the distribution
    of the activations in each layer so that it has a proper spread. To do that, the
    layer that normalizes data distribution is inserted into a neural network as the
    batch normalization layer (also known as the batch norm layer), as shown in the
    following diagram:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16: Neural network example that uses batch normalization (the batch
    norm layers'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: are shown in gray)
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_16.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.16: Neural network example that uses batch normalization (the batch
    norm layers are shown in gray)'
  id: totrans-170
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'As its name indicates, batch norm normalizes each mini-batch that is used for
    training. Specifically, it normalizes data so that the average is 0 and the variance
    is 1\. The following equation shows this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: '| ![77](img/Figure_6.16a.png) | (6.7) |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
- en: Here, a set of m input data, b ![78](img/Figure_6.16d.png), is treated as a
    mini-batch and its average, ![79](img/Figure_6.16e.png), and variance, ![80](img/Figure_6.16f.png),
    are calculated. The input data is normalized so that its average is 0 and its
    variance is 1 for the appropriate distribution. In equation 6.7, ε is a small
    value (such as 10e-7). This prevents division by 0.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Equation 6.7 simply converts the input data for a mini-batch, ![81](img/Figure_6.16g.png),
    into data with an average of 0 and a variance of 1, ![82](img/Figure_6.16h.png).
    By inserting this process before (or after) the activation function (see (*Sergey
    Ioffe and Christian Szegedy (2015): Batch Normalization: Accelerating Deep Network
    Training by Reducing Internal Covariate Shift. arXiv:1502.03167[cs] (February
    2015)*) and (*Dmytro Mishkin and Jiri Matas (2015): All you need is a good init.
    arXiv:1511.06422[cs] (November 2015)*) for a discussion (and experiments) on whether
    batch normalization should be inserted before or after the activation function),
    you can reduce the distribution bias of the data.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition, the batch norm layer converts the normalized data with a peculiar
    scale and shift. The following equation shows this conversion:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '| ![83](img/Figure_6.16i.png) | (6.8) |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
- en: Here, γ and β are parameters. They start with γ = 1 and β = 0 and will be adjusted
    to the appropriate values through training.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: This is the algorithm of batch norm. This algorithm provides the forward propagation
    in a neural network. By using a computational graph, as described in *Chapter
    5*, *Backpropagation*, we can represent batch norm as follows.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: 'We won''t go into detail about how to derive backward propagation in batch
    norm here because it is a little complicated. When you use a computational graph,
    such as the one shown in the following image, you can derive the backward propagation
    of batch norm relatively easily. Frederik Kratzert''s blog, *Understanding the
    Backward Pass through the Batch Normalization Layer* ([https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)),
    provides a detailed description of this. Please refer to it if you are interested:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17: Computational graph of batch normalization'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_17.jpg)'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.17: Computational graph of batch normalization'
  id: totrans-182
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-183
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Figure 6.17* is cited from reference, *Frederik Kratzert''s blog "Understanding
    the backward pass through Batch Normalization Layer"* ([https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html](https://kratzert.github.io/2016/02/12/understanding-the-gradient-flow-through-the-batch-normalization-layer.html)).'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating Batch Normalization
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's use the batch norm layer to conduct some experiments. First, we will
    use the MNIST dataset to see how the progress of learning changes with and without
    the batch norm layer (the source code can be found at `ch06/batch_norm_test.py`).
    *Figure 6.18* shows the result.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6.18* shows that batch norm accelerates training. Next, let''s see
    how the progress of training changes when various scales for the initial values
    are used. *Figure 6.19* contains graphs that show the progress of training when
    the standard deviations of the initial weight values are changed.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: This indicates that batch norm accelerates training in almost all cases. In
    fact, when batch norm is not used, training does not advance at all without a
    good scale of initial values.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
- en: As we have seen, using batch norm can accelerate training and provides robustness
    to the initial weight values ("robustness to the initial values" means having
    a little dependence on them). Batch norm will play an active part in many situations
    because it has such wonderful characteristics.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: Regularization
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Overfitting** often creates difficulties in machine learning problems. In
    overfitting, the model fits the training data too well and cannot properly handle
    other data that is not contained in the training data. Machine learning aims at
    generalizing performance. It is desirable for the model to properly recognize
    unknown data that is not contained in the training data. While you can create
    a complicated and representative model this way, reducing overfitting is also
    important:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18: Effect of batch norm – batch norm accelerates learning'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_18.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.18: Effect of batch norm – batch norm accelerates learning'
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Overfitting
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The main two causes of overfitting are as follows:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: The model has many parameters and is representative.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training data is insufficient.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we will generate overfitting by providing these two causes. Out of 60,000
    pieces of training data in the MNIST dataset, only 300 are provided, and a seven-layer
    network is used to increase the network''s complexity. It has 100 neurons in each
    layer. ReLU is used as the activation function:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19: The solid lines show the results of using batch norm, while
    the dotted lines show the results without it – the title of each graph indicates
    the standard deviation of the initial weight values'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_19.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.19: The solid lines show the results of using batch norm, while the
    dotted lines show the results without it – the title of each graph indicates the
    standard deviation of the initial weight values'
  id: totrans-202
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'The following is part of the code for this experiment (the source file is at
    `ch06/overfit_weight_decay.py`). First, the code loads the data:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The following code conducts training. Here, the recognition accuracy is calculated
    for each epoch for all the training data and all the test data:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `train_acc_list` and `test_acc_list` lists store the recognition accuracies
    for each epoch. An epoch indicates that all the training data has been used. Let's
    draw graphs based on these lists (`train_acc_list` and `test_acc_list`). The following
    plot shows the results.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'The recognition accuracies that were measured using the training data reached
    almost 100% after 100 epochs, but the recognition accuracies on the test data
    are far below 100%. These large differences are caused by overfitting the training
    data. This graph shows that the model cannot handle general data (test data) that
    was not used in training properly:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.20: Transition of recognition accuracies for the training data (train)
    and test data (test)'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_20.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.20: Transition of recognition accuracies for the training data (train)
    and test data (test)'
  id: totrans-211
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Weight Decay
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The **weight decay** technique has often been used to reduce overfitting. It
    avoids overfitting by imposing a penalty on large weights during training. Overfitting
    often occurs when a weight parameter takes a large value.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: As described earlier, the purpose of neural network training is to reduce the
    value of the loss function. For example, you can add the squared norm (L2 norm)
    of the weight to the loss function. Then, you could prevent the weight from being
    large. When the weights are W, the L2 norm of the weight decay is ![84](img/Figure_6.20a.png).
    This ![85](img/Figure_6.20b.png) is added to the loss function. Here, λ is the
    hyperparameter that controls the strength of regularization. If you set a larger
    value to λ, you can impose a stronger penalty on a large weight. ![88](img/Figure_6.20c.png)
    at the beginning of ![86](img/Figure_6.20b.png) is a constant for adjustment so
    that the differential of ![87](img/Figure_6.20b.png) is λW.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Weight decay adds ![89](img/Figure_6.20b.png) to the loss function for all weights.
    Therefore, the differential of the regularization term, λW, is added to the result
    of backpropagation when calculating the gradient of a weight.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: The L2 norm is the sum of squares of each element. In addition to the L2 norm,
    L1 and L ∞ norms also exist. The L1 norm is the sum of absolute values, that is,
    |w1| + |w2| + ... + |wn|. The L ∞ norm is also called the max norm. It is the
    largest among the absolute values of all the elements. You can use any of these
    norms as a regularization term. Each has its own characteristics, but we will
    only implement the L2 norm here since it's the most commonly used.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s conduct an experiment. We will apply the weight decay of λ= 0.1
    to the preceding experiment. The following plot shows the results (the network
    that supports weight decay is located at `common/multi_layer_net.py` and the code
    for the experiment is located at `ch06/overfit_weight_decay.py`):'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.21: Transition of recognition accuracies for the training data (train)
    and test data (test) when weight decay is used'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_21.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.21: Transition of recognition accuracies for the training data (train)
    and test data (test) when weight decay is used'
  id: totrans-220
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: The preceding image shows that the recognition accuracies of the training data
    and test data are different, but that the difference is smaller than in the one
    shown in *Figure 6.20* where weight decay was not used. This indicates that overfitting
    was reduced. Note that the recognition accuracies of the training data have not
    reached 100% (1.0).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: Dropout
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The previous section described the weight decay technique. It adds the L2 norm
    of the weights to the loss function to reduce overfitting. Weight decay is easy
    to implement and can reduce overfitting to some extent. However, as a neural network
    model becomes more complicated, weight decay is often insufficient. This is when
    the dropout technique (*N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever,
    and R. Salakhutdinov (2014): Dropout: A simple way to prevent neural networks
    from overfitting. The Journal of Machine Learning Research, pages 1929 – 1958,
    2014*) is often used.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: 'Dropout erases neurons at random during training. During training, it selects
    neurons in a hidden layer at random to erase them. As shown in the following image,
    the erased neurons do not transmit signals. During training, the neurons to be
    erased are selected at random each time data flows. During testing, the signals
    of all the neurons are propagated. The output of each neuron is multiplied by
    the rate of the erased neurons during training:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.22: Concept of dropout'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_22.jpg)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.22: Concept of dropout'
  id: totrans-227
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: '*Figure 6.22* is cited from reference, *N. Srivastava, G. Hinton, A. Krizhevsky,
    I. Sutskever, and R. Salakhutdinov (2014): Dropout: A simple way to prevent neural
    networks from overfitting. The Journal of Machine Learning Research pages 1929–1958,
    2014*.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: The left-hand image shows an ordinary neural network, while the right-hand image
    shows a network that dropout has been applied to. Dropout selects neurons at random
    and erases them to stop the transmission of subsequent signals.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement dropout. Simplicity is emphasized in the implementation
    here. If appropriate calculation is conducted during training, we only have to
    flow data through forward propagation (without multiplying the rate of the erased
    neurons). Such an implementation is conducted in deep learning frameworks. For
    efficient implementation, the dropout implemented in the Chainer framework, for
    example, may be useful:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Please note that, in each forward propagation, the neurons to erase are stored
    as `False` in `self.mask`. `self.mask` generates an array of the same shape as
    `x` at random and sets the elements to `True` when their values are larger than
    `dropout_ratio`. The behavior in backward propagation is the same as that in ReLU.
    If a neuron is passed a signal in forward propagation, it passes the received
    signal without changing it in backward propagation. If a neuron doesn't pass a
    signal in forward propagation, it stops the received signal in backward propagation.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
- en: We will use the MNIST dataset to validate the effect of dropout. The source
    code can be found in `ch06/overfit_dropout.py`. It uses the `Trainer` class to
    simplify implementation.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: The `Trainer` class is implemented in `common/trainer.py`. It conducts network
    training that has been conducted so far in this chapter. For details, please see
    `common/trainer.py` and `ch06/overfit_dropout.py`.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: To experiment with dropout, we'll use a seven-layer network (where 100 neurons
    exist in each layer and ReLU is used as the activation function), as in the previous
    experiment. One of the experiments will use dropout, while the other won't. The
    following image shows the results.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see, using dropout reduces the difference between the recognition
    accuracies of training data and test data. It also indicates that the recognition
    accuracy of the training data has not reached 100%. Due to this, you can use dropout
    to reduce overfitting, even in a representative network:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.23: The left-hand image shows the experiment without dropout, while
    the right-hand image shows the experiment with dropout (dropout_rate=0.15)'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_23.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.23: The left-hand image shows the experiment without dropout, while
    the right-hand image shows the experiment with dropout (dropout_rate=0.15)'
  id: totrans-240
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  id: totrans-241
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In machine learning, ensemble learning is often used in which multiple models
    learn separately, and their multiple outputs are averaged through prediction.
    For example, when we use it in a neural network, we prepare five networks with
    the same (or similar) structure and train each of them. Then, we average the five
    outputs during testing to obtain the result. Experiments have shown that ensemble
    learning improves a neural network's recognition accuracy by several percent.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
- en: Ensemble learning is close to dropout. Erasing neurons at random while training
    in dropout can be interpreted as providing a different model to learn data each
    time. While predicting, the output from the neurons is multiplied by the rate
    of the erasures (0.5, for example) to average the models. Thus, we can say that
    dropout simulates ensemble learning in one network.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
- en: Validating Hyperparameters
  id: totrans-244
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A neural network uses many hyperparameters, as well as parameters such as weights
    and biases. The hyperparameters here include the number of neurons in each layer,
    batch size, the learning rate for updating parameters, and weight decay. Setting
    the hyperparameters to inappropriate values deteriorates the performance of the
    model. The values of these hyperparameters are very important, but determining
    them usually requires a lot of trial and error. This section describes how to
    search for hyperparameter values as efficiently as possible.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
- en: Validation Data
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the dataset we've used so far, the training data and test data are separate.
    The training data is used to train a network, while the test data is used to evaluate
    generalization performance. Thus, you can determine whether or not the network
    conforms too well only to the training data (that is, whether overfitting occurs)
    and how large the generalization performance is.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
- en: We will use various hyperparameter settings for validation. Please note that
    you must not use test data to evaluate the performance of hyperparameters. This
    is very important but is often overlooked.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: So, why can't we use test data to evaluate the performance of hyperparameters?
    Well, if we use test data to adjust hyperparameters, the hyperparameter values
    will overfit the test data. In other words, it uses test data to check that the
    hyperparameter values are "good," so the hyperparameter values are adjusted so
    that they only fit the test data. Here, the model may provide low generalization
    performance and cannot fit other data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, we need to use verification data (called **validation data**) to
    adjust them. This validation data is used to evaluate the quality of our hyperparameters.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: Training data is used for learning parameters (weights and biases). Validation
    data is used to evaluate the performance of hyperparameters. Test data is used
    (once, ideally) at the end of training to check generalization performance.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
- en: 'Some datasets provide training data, validation data, and test data separately.
    Some provide only training data and test data, while some provide only one type
    of data. In that case, you must separate the data manually. For the MNIST dataset,
    the simplest way to obtain the validation data is to separate 20% of the training
    data beforehand and use that as validation data. The following code shows this:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Here, the input data and labeled data are shuffled before separating the training
    data. This is because some datasets may have biased data (for example, numbers
    "0" to "10" are arranged in this order). The `shuffle_dataset` function uses `np.random.shuffle`
    and is contained in `common/util.py`.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's use validation data to look at the technique that's used for optimizing
    hyperparameters.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: Optimizing Hyperparameters
  id: totrans-256
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What is important when optimizing hyperparameters is to gradually narrow down
    the range where "good" hyperparameter values exist. To do this, we will set a
    broad range initially, select hyperparameters at random from the range (sampling),
    and use the sampled values to evaluate the recognition accuracy. Next, we will
    repeat these steps several times and observe the result of the recognition accuracy.
    Based on the result, we will narrow down the range of "good" hyperparameter values.
    By repeating this procedure, we can gradually limit the range of appropriate hyperparameters.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
- en: 'It has been reported that random sampling before a search provides better results
    than a systematic search, such as a grid search, to optimize hyperparameters in
    a neural network (*James Bergstra and Yoshua Bengio (2012): Random Search for
    Hyper-Parameter Optimization. Journal of Machine Learning Research 13, Feb (2012),
    281 – 305*). This is because the degree by which the final recognition accuracy
    will be affected is different among different hyperparameters.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: Specifying a "broad" range of hyperparameters is effective. We will specify
    the range in "powers of 10," such as from 0.001 (10−3) to 1,000 (103) (this is
    also called "specifying on a log scale").
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: 'Please note that when optimizing hyperparameters, deep learning takes a lot
    of time (even a few days or weeks). Therefore, any hyperparameters that seem inappropriate
    must be abandoned while searching for them. When optimizing hyperparameters, it
    is effective to reduce the size of epoch for training to shorten the time that
    one evaluation takes. We discussed the optimization of hyperparameters previously.
    The following summarizes this discussion:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 0**'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: Specify the range of the hyperparameters.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
- en: Sample the hyperparameters from the range at random.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 2**'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: Use the hyperparameter values sampled in *Step 1* for training and use the validation
    data to evaluate the recognition accuracy (set small epochs).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 3**'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: Repeat *steps 1* and *2* a certain number of times (such as 100 times) and narrow
    down the range of hyperparameters based on the result of the recognition accuracy.
    When the range is narrowed down to some extent, select one hyperparameter value
    from it. This is one practical approach to optimizing hyperparameters.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Note
  id: totrans-269
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'However, you may feel that this approach is the "wisdom" of engineers rather
    than science. If you need a more refined technique for optimizing hyperparameters,
    you can use **Bayesian optimization**. It makes good use of mathematical theories
    such as Bayes'' theorem to provide stricter and more efficient optimization. For
    details, please see the paper *Practical Bayesian Optimization of Machine Learning
    Algorithms* (*Jasper Snoek, Hugo Larochelle, and Ryan P. Adams (2012): Practical
    Bayesian Optimization of Machine Learning Algorithms. In F. Pereira, C. J. C.
    Burges, L. Bottou, & K. Q. Weinberger, eds. Advances in Neural Information Processing
    Systems 25\. Curran Associates, Inc., 2951 – 2959*).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Hyperparameter Optimization
  id: totrans-271
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, let''s use the MNIST dataset to optimize some hyperparameters. We will
    look for two hyperparameters: the learning rate and the weight decay rate. The
    weight decay rate controls the strength of weight decay. This problem and solution
    are based on the *CS231n* (*CS231n: Convolutional Neural Networks for Visual Recognition*
    ([http://cs231n.github.io/](http://cs231n.github.io/))) course at Stanford University.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: 'As described earlier, hyperparameters are validated by sampling them at random
    from the range on a log scale, such as from 0.001 (10−3) to 1,000 (103). We can
    write this as `10 ** np.random.uniform(-3, 3)` in Python. This experiment will
    start with a range from 10−8 to 10−4 for the weight decay rate and from 10−6 to
    10−2 for the learning rate. In this case, we can write the random sampling of
    the hyperparameters as follows:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Here, the hyperparameters were sampled at random, and the sampled values were
    used for training. Then, training is repeated several times by using various hyperparameter
    values to find where the appropriate hyperparameters exist. Here, the details
    of implementation have been omitted, and only the result has been shown. The source
    code for optimizing hyperparameters is located at `ch06/hyperparameter_optimization.py`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have a range of 10−8 to 10−4 for the weight decay rate and a range
    of 10−6 to 10−2 for the learning rate, we get the following results. Here, we
    can see the transitions in learning the validation data in descending order of
    high-recognition accuracies:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.24: The solid lines show the recognition accuracies of the validation
    data, while the dotted lines show the recognition accuracies of the training data'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig06_24.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 6.24: The solid lines show the recognition accuracies of the validation
    data, while the dotted lines show the recognition accuracies of the training data'
  id: totrans-279
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'This indicates that the training advanced smoothly from `Best-1` to `Best-5`.
    Let''s check the hyperparameter values (that is, the learning rate and weight
    decay rate) of `Best-1` to `Best-5`. These are the results:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Here, we can see that when the learning rate was 0.001 to 0.01 and the weight
    decay rate was 10−8 to 10−6, learning advanced well. Due to this, the range of
    the hyperparameters where training is likely to succeed is observed to narrow
    the range of values. You can repeat the same procedure in the narrowed range.
    Thus, you can narrow the range where appropriate hyperparameters exist and select
    each of the final hyperparameters at a certain stage.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-283
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This chapter described some important techniques that are used for neural network
    training. How to update parameters, how to specify initial weight values, batch
    normalization, and dropout are all essential techniques that are used in modern
    neural networks. The techniques described here are often used in state-of-the-art
    deep learning. In this chapter, we learned about the following:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'Four famous methods for updating parameters: Momentum, AdaGrad, Adam, and SGD.'
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to specify initial weight values, which is very important if we wish to
    train correctly.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Xavier initializer and He initializer, which are effective as initial weight
    values.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch normalization accelerates training and provides robustness to the initial
    weight values.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weight decay and dropout are regularization techniques that are used to reduce overfitting.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To search for good hyperparameters, gradually narrowing down the range where
    appropriate values exist is an efficient method.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
