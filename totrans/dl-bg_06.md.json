["```py\nfrom sklearn.datasets import make_classification\n\nX, y = make_classification(n_samples=100, n_features=2, n_classes=2,\n                           n_informative=2, n_redundant=0, n_repeated=0,\n                           n_clusters_per_class=1, class_sep=1.5, \n                           random_state=5)\n```", "```py\ny[y==0] = -1\n```", "```py\nimport numpy as np\nX = np.append(np.ones((N,1)), X, 1)    \n```", "```py\nnp.sign(w.T.dot(X[n]))\n```", "```py\nN = 100 # number of samples to generate\nrandom.seed(a = 7) # add this to achieve for reproducibility\n\nX, y = make_classification(n_samples=N, n_features=2, n_classes=2,\n                           n_informative=2, n_redundant=0, n_repeated=0,\n                           n_clusters_per_class=1, class_sep=1.2, \n                           random_state=5)\n\ny[y==0] = -1\n\nX_train = np.append(np.ones((N,1)), X, 1) # add a column of ones\n\n# initialize the weights to zeros\nw = np.zeros(X_train.shape[1])\nit = 0\n\n# Iterate until all points are correctly classified\nwhile classification_error(w, X_train, y) != 0:\n  it += 1\n  # Pick random misclassified point\n  x, s = choose_miscl_point(w, X_train, y)\n  # Update weights\n  w = w + s*x\n\nprint(\"Total iterations: \", it)\n```", "```py\ndef classification_error(w, X, y):\n  err_cnt = 0\n  N = len(X)\n  for n in range(N):\n    s = np.sign(w.T.dot(X[n]))\n    if y[n] != s:\n      err_cnt += 1    # we could break here on large datasets\n  return err_cnt      # returns total number of errors\n```", "```py\ndef classification_error(w, X, y):\n  s = np.sign(X.dot(w))\n  return sum(s != y)\n```", "```py\ndef choose_miscl_point(w, X, y):\n  mispts = []\n  for n in range(len(X)):\n    if np.sign(w.T.dot(X[n])) != y[n]:\n      mispts.append((X[n], y[n]))\n  return mispts[random.randrange(0,len(mispts))]\n```", "```py\ndef choose_miscl_point(w, X, y):\n for idx in random.permutation(len(X)):\n   if np.sign(w.T.dot(X[idx])) != y[idx]:\n     return X[idx], y[idx]\n```", "```py\nTotal iterations: 14\n```", "```py\nX, y = make_classification(..., class_sep=2.0, ...)\n```", "```py\nX, y = make_classification(n_samples=N, n_features=2, n_classes=2,\n n_informative=2, n_redundant=0, n_repeated=0,\n n_clusters_per_class=1, class_sep=1.0, \n random_state=5)\n\ny[y==0] = -1\n\nX_train = np.append(np.ones((N,1)), X, 1) # add a column of ones\n\n# initialize the weights to zeros\nw = np.zeros(X_train.shape[1])\nit = 0\nbestW = {}\nbestW['err'] = N + 1 # dictionary to keep best solution\nbestW['w'] = []\nbestW['it'] = it\n\n# Iterate until all points are correctly classified\n#   or maximum iterations (i.e. 1000) are reached\nwhile it < 1000:\n err = classification_error(w, X_train, y)\n if err < bestW['err']:   # enter to save a new w\n bestW['err'] = err\n bestW['it'] = it\n bestW['w'] = list(w)\n if err == 0:  # exit loop if there are no errors\n break\n  it += 1\n  # Pick random misclassified point\n  x, s = choose_miscl_point(w, X_train, y)\n  # Update weights\n  w += s*x\n\nprint(\"Best found at iteration: \", bestW['it'])\nprint(\"Number of misclassified points: \", bestW['err'])\n```", "```py\nBest found at iteration: 95\nNumber of misclassified points: 1\n```"]