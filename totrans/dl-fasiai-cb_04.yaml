- en: '*Chapter 4*: Training Models with Text Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083), *Training
    Models with Tabular Data*, you went through a series of recipes that demonstrated
    how to use the facilities of fastai to train deep learning models on tabular data.
    In this chapter, we will examine how to take advantage of the fastai framework
    to train deep learning models on text datasets.
  prefs: []
  type: TYPE_NORMAL
- en: To explore deep learning with text data in fastai, we will start by taking a
    pre-trained **language model** (that is, a model that, when given a phrase, predicts
    what words come next) and fine-tuning it with the IMDb curated dataset. We will
    then use the resulting fine-tuned language model to create a **text classifier
    model** for the movie review use case represented by the IMDb dataset. The text
    classifier predicts the class of a phrase; in the movie review use case, it predicts
    whether a given phrase is **positive** or **negative**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we apply the same approach to a standalone (that is, non-curated)
    text dataset of Covid-related tweets. First, we will fine-tune the existing language
    model on the Covid tweets dataset. Then, we will use the fine-tuned language model
    to train a text classifier that predicts the class of a phrase according to the
    categories defined in the Covid tweets dataset: **extremely negative**, **negative**,
    **neutral**, **positive**, and **extremely positive**.'
  prefs: []
  type: TYPE_NORMAL
- en: The approach to training deep learning models on text datasets that is used
    in this chapter, also known as **ULMFiT**, was initially described by the creators
    of fastai in their paper entitled *Universal Language Model Fine-Tuning for Text
    Classification* [https://arxiv.org/abs/1801.06146](https://arxiv.org/abs/1801.06146).
    This approach introduced the concept of **transfer learning** to **natural language
    processing** (**NLP**).
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning is taking a model that has been trained on a large, general-purpose
    dataset and making it applicable to a specific use case by fine-tuning the large
    model with a smaller dataset that is specific to the use case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ULMFiT approach to transfer learning for NLP can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Start with a large language model trained on a large text dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tune this language model on a text dataset that is related to a specific
    use case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the fine-tuned language model to create a text classifier for the specific
    use case.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In this chapter, the large model is referred to as `AWD_LSTM`, and it has been
    trained on a big corpus taken from Wikipedia articles. We will fine-tune this
    large language model on datasets for two specific use cases: IMDb for movie reviews,
    and Covid tweets for social media posts regarding the Covid 19 pandemic. We then
    use each of the resulting fine-tuned language models to train text classifiers
    for each use case.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the recipes that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Training a deep learning language model with a curated IMDb text dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a deep learning classification model with a curated text dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a deep learning language model with a standalone text dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a deep learning text classifier with a standalone text dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test your knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensure that you have completed the setup sections from [*Chapter 1*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019),
    *Getting Started with fastai*, and have a working `ch4` folder. This folder contains
    the code samples described in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Some of the examples in this chapter will take over an hour to run.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Do not use Colab for these examples. With Colab you cannot control the GPU that
    you get for a session and these examples may run for many hours. For the examples
    in this chapter, use a for-pay GPU-enabled Gradient environment to ensure they
    complete in a reasonable time.
  prefs: []
  type: TYPE_NORMAL
- en: Training a deep learning language model with a curated IMDb text dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, you will go through the process of training a language model
    on a curated text dataset using fastai. We take a pre-existing language model
    that is packaged with fastai and fine-tune it with one of the curated text datasets,
    IMDb, that contains text samples for the movie review use case. The result will
    be a language model with the broad language capability of the pre-existing language
    model, along with the use case-specific details of the IMDb dataset. This recipe
    illustrates one of the breakthroughs made by the team that created fastai, that
    is, transfer learning applied to NLP.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the recipes so far in this book, we have recommended using the Gradient
    environment. You can use Gradient for this recipe and the instructions below include
    several workarounds to make the recipe work on Gradient. In particular, the pre-trained
    `AWD_LSTM` model will not be available in Gradient if its initial setup gets interrupted
    and the directory for the `IMDB` model is not writeable. If the setup of the pre-trained
    `AWD_LSTM` model gets interrupted, follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: In Colab, run the cells of the `text_model_training.ipynb` notebook up to and
    including the learner definition and training cell. Once you have done so, copy
    the contents of the `/root/.fastai/models/wt103-fwd` directory to a folder in
    your Drive environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the files you copied in the previous step to the `/storage/models/wt103-fwd`
    directory in your Gradient environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these steps, you should now be able to run the notebook for this recipe
    (and other recipes that make use of `AWD_LSTM`) in Gradient.
  prefs: []
  type: TYPE_NORMAL
- en: I am grateful for the opportunity to include the IMDB dataset featured in this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, Christopher
    Potts. (2011) *Learning Word Vectors for Sentiment Analysis* ([https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf](https://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf))
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will be running through the `text_model_training.ipynb`
    notebook to train a language model using the IMDb curated dataset. Once you have
    the notebook open in Colab, complete the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the notebook up to the `Training a language model` cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to define a `path` object associated with the IMDb curated
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You can see the directory structure for this dataset in the output of the `tree
    -d` command run in the `imdb` directory (`/storage/data/imdb` in Gradient). Note
    that the labels for the dataset (whether a review is positive or negative) are
    encoded by the directory in which the text sample is located. For example, the
    negative review text samples in the training dataset are contained in the `train/neg`
    directory.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following is the output of the `tree -d` command run in the `imdb` directory:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to define a `TextDataLoaders` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the `TextDataLoaders` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `path`: The `path` object (associated with the IMDb curated dataset) that
    you defined earlier in the notebook.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `valid`: Identifies the folder in the dataset''s directory structure that
    will be used to assess the performance of the model: `imdb/test`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `is_lm`: Set to `True` to indicate that this object will be used for a language
    model (as opposed to a text classifier).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `bs`: Specifies the batch size.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When you are training a language model with a large dataset such as IMDb, adjusting
    the `bs` value to be lower than the default batch size of `64` will be essential
    for avoiding memory errors, and that is why it is set to `16` in this `TextDataLoaders`
    definition.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to show a couple of items from a sample batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `max_n` argument specifies the number of sample batch items to show.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Note the output of this cell. The `text` column shows the original text. The
    `text_` column shows the same text shifted one token ahead, that is, it starts
    one word after the original text and ends one word past the original text. Given
    a sample such as one of the entries in the `text` column, the language model will
    predict the next word, as shown in the `text_` column. We can see the output of
    `show_batch()` in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Output of show_batch()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_4_1.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.1 – Output of show_batch()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define and train the deep learning model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the `language_model_learner` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `dls`: The `TextDataLoaders` object that is defined previously in this notebook.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `AWD_LSTM`: The pre-trained model to use as a basis for this model. This
    is the pre-trained language model incorporated with fastai that is trained with
    Wikipedia. If you are running this notebook on Colab, you can find the files that
    make up this model in the `/root/.fastai/models/wt103-fwd` directory after you
    have run this cell.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `metrics`: The performance metric to be optimized for the model, in this
    case, accuracy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here are the arguments for the `fine_tune` statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) The epoch number (first argument) specifies the number of epochs, that is,
    the number of times the algorithm goes through the full training data during the
    training process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The learning rate (second argument) specifies the learning rate for the training
    process. The learning rate is the rate at which the algorithm moves toward learning
    optimal parameters.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) Depending on your environment, it may take more than an hour for this cell
    to run to completion. I strongly recommend that you use a for-pay GPU-enabled
    Gradient environment for this example and specify at least 3 hours for the instance
    to ensure that it completes in a reasonable time and that the instance doesn't
    shut down while this cell is running.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) The `language_model_learner` definition includes a call to `to_fp16()` to
    specify mixed-precision training (summarized here: [https://docs.fast.ai/callback.fp16.html#Learner.to_fp16](https://docs.fast.ai/callback.fp16.html#Learner.to_fp16))
    to reduce the memory consumption of the training process and to prevent memory
    errors. Refer to the *There''s more…* section for more details.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of the `fine_tune` statement shows the accuracy of the model and
    the time taken to complete the fine-tuning, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Output of the fine_tune statement'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_4_2.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.2 – Output of the fine_tune statement
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to exercise the language model you just trained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for this invocation of the language model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) The input text sample `"what comes next"` (first argument) is the phrase
    that the model will complete. The language model will predict what words should
    follow this phrase.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `n_words`: This is the number of words that the language model is supposed
    to predict to complete the input phrase.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following screenshot shows an example of what the model''s prediction could
    look like:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.3 – The language model completes a phrase'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_4_3.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.3 – The language model completes a phrase
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to save the model. You can update the cell to specify
    the directory and filename to which to save the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to save the current path value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to assign a new value to the learner object path. The
    reason for doing this is that the default location for the model is not writeable
    on Gradient so you need to change the path value to a directory where you have
    write access:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to save the encoder subset of the model. This is the
    model minus the final layer. You will use this in the *Training a deep learning
    classification model with a curated text dataset* section when you train a text
    classifier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You have successfully applied transfer learning to train a
    language model on the curated IMDb dataset. Note that the idea of applying transfer
    learning to NLP like this was only described for the first time in 2018\. Now,
    thanks to the fastai framework, with just a few lines of code, you can take advantage
    of a technique that didn't exist scarcely 4 years ago!
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you have seen a simple example of how to train a language model
    with fastai deep learning using a curated text dataset. The language model is
    created by taking a model (`AWD_LSTM`) that has been pre-trained with the massive
    wiki dataset and then fine-tuning it using the IMDb dataset.
  prefs: []
  type: TYPE_NORMAL
- en: By taking advantage of transfer learning in this way, we end up with a language
    model that combines a good degree of capability on general-purpose English (thanks
    to the model pre-trained on the wiki dataset) as well as the capability to produce
    text that is specific to the use case of movie reviews (thanks to the IMDb dataset).
  prefs: []
  type: TYPE_NORMAL
- en: It's worthwhile to look a bit closer at the model in this recipe. A deeply detailed
    description of the model is beyond the scope of this book, so we will just focus
    on some highlights here.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the recipe, the model is defined as a `language_model_learner`
    object (documentation here: [https://docs.fast.ai/text.learner.html#language_model_learner](https://docs.fast.ai/text.learner.html#language_model_learner)).
    This object is a specialization of the fastai `learner` object which you first
    saw in the *Understanding the world in four applications: tables, text, recommender
    systems* section, and in the images of [*Chapter 1*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019)*,
    Getting Started with fastai*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The model in the recipe is based on the predefined `AWD_LSTM` model (documentation
    here: [https://docs.fast.ai/text.models.awdlstm.html#AWD_LSTM](https://docs.fast.ai/text.models.awdlstm.html#AWD_LSTM)).
    For this model, the output of `learn.summary()` shows only the high-level structure,
    including LSTM layers (fundamental to traditional NLP deep learning models) and
    dropout layers (used to reduce overfitting). Similarly, the output of `learn.model`
    for this model starts with layers for encoding (that is, transforming the input
    data to an intermediate representation used within the model) and ends with layers
    for decoding (that is, transforming the internal representation back to words).'
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this chapter, you will be working with some very large datasets, which means
    that you may need to take some extra steps to ensure that you don't run out of
    memory while you are preparing the datasets and training the model. Here, we'll
    describe some steps you can take to ensure that you train your fastai deep learning
    models on text datasets without running out of memory. We'll also go into more
    detail about how to save encoders in Gradient.
  prefs: []
  type: TYPE_NORMAL
- en: What happens if you run out of memory?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you are trying to train a large model, you may get an out of memory message
    such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'This message is saying that you have run out of memory on the GPU for your
    environment. What can you do if you encounter such a memory error? There are three
    steps you can take to get around this kind of memory error:'
  prefs: []
  type: TYPE_NORMAL
- en: Explicitly set the batch size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use mixed-precision training.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ensure that you have only one notebook active at a time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Memory error mitigation #1: Explicitly set the batch size'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To explicitly set the batch size, you can restart the kernel for your notebook
    and then update the definition of the `TextDataLoaders` object to set the `bs`
    parameter, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Setting the `bs` parameter explicitly specifies a batch size (the number of
    items on which the average loss is calculated) other than the default of 64\.
    By explicitly setting the batch size to a smaller value than the default, you
    limit the amount of memory consumed by each training epoch (that is, a complete
    iteration through the training data). When you set the value of `bs` explicitly
    like this, ensure that the value you set for the `bs` parameter is a multiple
    of 8.
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory error mitigation #2: Mixed-precision training'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another technique that you can use to control memory consumption is using `to_fp16()`
    function to the definition of the learner object, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'By specifying this call to `to_fp16()`, you allow the model to be trained using
    floating-point numbers that are less precise and are therefore expressed with
    less memory. The result is that the model training process consumes less memory.
    Refer to the fastai documentation for more details: [https://docs.fast.ai/callback.fp16.html#Learner.to_fp16](https://docs.fast.ai/callback.fp16.html#Learner.to_fp16).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Memory error mitigation #3: Stick to a single active notebook'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Finally, another approach that you can take to prevent running out of memory
    is to run a single notebook at a time. On Gradient, for example, if you have multiple
    notebooks active at the same time, then you can exhaust your available memory.
    If you take the steps of setting a smaller batch size in the `TextDataLoaders`
    object and specifying `to_fp16()` in the learner object and still get memory errors,
    shut down the kernel of all the notebooks except the one you are currently working
    on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In JuptyerLab in Gradient, you can shut down the kernel for a notebook by right-clicking
    on the notebook in the navigation pane and selecting **Shut Down Kernel** from
    the menu, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Shutting down a kernel in Gradient JupyterLab'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_4_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Shutting down a kernel in Gradient JupyterLab
  prefs: []
  type: TYPE_NORMAL
- en: Workaround to allow you to save encoders
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In addition to the memory tips we've just reviewed, there is one more tip you
    need to know for text models if you are using Gradient. On Gradient, you may run
    into a situation where you are not able to save and retrieve interim objects in
    the directory where fastai wants to save them.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, you need to save the encoder from the language model and then
    load that encoder when you train the text classifier. However, fastai forces you
    to save the encoder in the path for the dataset. The `save_encoder` function only
    lets you specify the unqualified file name, not the directory in which to save
    the encoder, as you can see in the following call to `save_encoder`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'At the same time, in Gradient, the directory for the IMDb dataset, `/storage/data/imdb`,
    is read-only. So, how can you save an encoder if the directory where it must be
    saved is not writeable? You can work around this problem by temporarily updating
    the learner''s `path` object, saving the encoder in the directory indicated by
    this temporary `path` value, and then setting the `path` object back to its original
    value, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Save the path value for your model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Change the path value for your model to a directory that you have write access
    to, for example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Save the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Change the path back to the original value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Training a deep learning classification model with a curated text dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we trained a language model using the curated text
    IMDb dataset. The model in the previous section predicted the next set of words
    that would follow a given set of words. In this section, we will take the language
    model that was fine-tuned on the IMDb dataset and use it to train a text classification
    model that classifies text samples that are specific to the movie review use case.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe makes use of the encoder that you trained in the previous section,
    so ensure that you have followed the steps in the recipe in that section, in particular,
    that you have saved the encoder from the trained language model.
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the previous section, you need to take some additional steps
    before you can run recipes in Gradient that use the language model pre-trained
    on the Wikipedia corpus. To ensure that you have access to the pre-trained language
    model that you need to use in this recipe, complete the following steps if the
    setup of AWD_LSTM was interrupted:'
  prefs: []
  type: TYPE_NORMAL
- en: In Colab, run the cells of the `text_model_training.ipynb` notebook up to and
    including the learner definition and training cell. Once you have done so, copy
    the contents of the `/root/.fastai/models/wt103-fwd` directory to a folder in
    your Drive environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the files you copied in the previous step to the `/storage/models/wt103-fwd`
    directory in your Gradient environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these steps, you will be able to run the notebook for this recipe (and
    other recipes that make use of `AWD_LSTM`) in Gradient.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will be running through the `text_classifier_model.ipynb`
    notebook to train a text classifier deep learning model using the `IMDb` curated
    dataset. Once you have the notebook open in Gradient, complete these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the notebook up to the `Define the text classifier` cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to define a `TextDataLoaders` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the `TextDataLoaders` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `path`: Defines the path of the dataset used to define the `TextDataLoaders`
    object'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `valid`: Identifies the folder in the dataset''s directory structure that
    will be used to assess the performance of the model: `imdb/test`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to see a sample of entries from a batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of `show_batch()` shows text samples along with the class (indicated
    in the `category` column). fastai knows that the class is encoded in the directory
    where the text sample is and correctly renders it in `show_batch()`, as seen in
    the following screenshot:![Figure 4.5 – Output of show_batch()
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '](img/B16216_4_5.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.5 – Output of show_batch()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define the text classifier model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the `text_classifier_learner`
    object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `dls_clas`: This is the `TextDataLoaders` object defined in the previous
    cell.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `AWD_LSTM`: This is the pre-trained model to use as a basis for this model.
    If you run this notebook in Colab, you can find the files that make up this model
    in the `/root/.fastai/models/wt103-fwd` directory after you have run this cell.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `metrics`: This is the performance metric to be optimized for the model,
    in this case, accuracy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'You need to load the encoder that you saved as part of the recipe in the previous
    section. The first step is to set the path for the `learn_clas` object so that
    it is the path in which the encoder is saved by running the following cell. Ensure
    that the directory specified is the directory where you saved the encoder in the
    previous recipe:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to load the encoder that you saved in the recipe in
    the *Training a deep learning language model with a curated text dataset* section
    to the `learn_clas` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for `fit_one_cycle`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) The argument epoch count (first argument) specifies that the training is
    run for `5` epochs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The argument learning rate (second argument) specifies that the learning
    rate is equal to `0.02`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell shows the results of the training, including the accuracy
    and the time taken for each epoch, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Results of training the text classification model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_4_6.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.6 – Results of training the text classification model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cells to get predictions on text strings that you expect to be negative
    and positive and observe whether the trained model makes the expected predictions,
    as shown in the following screenshot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Using the text classifier to get predictions on text strings'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_4_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – Using the text classifier to get predictions on text strings
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have taken the language model that was fine-tuned on the
    IMDb dataset and used it to train a text classification model that classifies
    text samples that are specific to the movie review use case.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can see the power of fastai by contrasting the code in this section, which
    defines a text classifier, with the code in the previous section, which defines
    a language model. There are only three differences in total:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the definition of the `TextDataLoaders` object, the following applies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) The language model has the `is_lm` argument to indicate that the model is
    a language model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The text classifier has the `label_col` argument to indicate which column
    in the dataset contains the category that is being predicted by the model. In
    the case of the text classifier defined in this section, the label for the dataset
    is encoded in the directory structure of the dataset rather than as a column in
    the dataset, so this parameter is not needed in the definition of the `TextDataLoaders`
    object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the definition of the model, the following applies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) The language model defines a `language_model_learner` object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The text classifier defines a `text_classifier_learner` object.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In getting a prediction from the model, the following applies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) The language model takes two arguments for its call to `learn.predict()`,
    the string on which to make the prediction, and the number of words to predict.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The text classifier takes one argument for its call to `learn.predict()`,
    the string whose class the model will predict.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: With just these three differences, fastai takes care of all the underlying differences
    between a language model and a text classifier.
  prefs: []
  type: TYPE_NORMAL
- en: There's more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you are using the Gradient environment and you are using a notebook with
    a cost, you will want to control how long the notebook is active to avoid paying
    for more time than you need. You can select the duration of your session when
    you start it up by selecting an hour value from the **Auto-Shutdown** menu, as
    shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Selecting a duration for a Gradient notebook session'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_4_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – Selecting a duration for a Gradient notebook session
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you end up selecting more time than you need and you are done with
    your session before the auto-shutdown time limit is reached. Should you explicitly
    shut down the session?
  prefs: []
  type: TYPE_NORMAL
- en: 'My experience has been that if you try to stop the instance in the Gradient
    notebook interface by selecting the **Stop Instance** button in the **Instance**
    view (as shown in *Figure 4.9*), you will risk putting your instance into a state
    where you cannot start it again easily:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Stop Instance button in the Instance view in Gradient'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_4_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 – Stop Instance button in the Instance view in Gradient
  prefs: []
  type: TYPE_NORMAL
- en: If you select **Stop Instance** and your instance gets into a state where you
    cannot start it again, then you will have to open a ticket with *Paperspace* support
    to fix your instance. After this happened to me a couple of times, I stopped using
    the **Stop Instance** button and just let the instance time out when I was finished
    working with it. You will save yourself time by never explicitly stopping your
    Gradient instance and instead just letting it time out when you are done with
    a session.
  prefs: []
  type: TYPE_NORMAL
- en: Training a deep learning language model with a standalone text dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous sections, we trained a language model and a text classifier
    using the curated text dataset IMDb. In this section and the next section, we
    will train a language model and a text classifier using a standalone text dataset,
    the *Kaggle Coronavirus tweets NLP – Text Classification* dataset described here:
    [https://www.kaggle.com/datatattle/covid-19-nlp-text-classification](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification).
    This dataset includes a selection of tweets related to the Covid-19 pandemic,
    along with categorization for the tweets according to the following five categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Extremely negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Negative
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neutral
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Positive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extremely positive
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of the language model trained on this dataset is to predict the subsequent
    words in a Covid-related tweet given a starting phrase. The goal of the text classification
    model trained on this dataset, as described in the *Training a deep learning text
    classifier with a standalone text dataset* section, is to predict which of the
    five categories a phrase belongs in.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As mentioned in previous sections in this chapter, you need to take some additional
    steps before you can run this recipe in Gradient to ensure that you have access
    to the pre-trained language model that you will use in this recipe. If you have
    not done so already, follow these steps to prepare your Gradient environment if
    the setup of AWD_LSTM was interrupted:'
  prefs: []
  type: TYPE_NORMAL
- en: In Colab, run the cells of the `text_model_training.ipynb` notebook up to and
    including the learner definition and training cell. Once you have done so, copy
    the contents of the `/root/.fastai/models/wt103-fwd` directory to a folder in
    your Drive environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Upload the files you copied in the previous step to the `/storage/models/wt103-fwd`
    directory in your Gradient environment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With these steps, you should now be able to run the notebook for this recipe
    (and other recipes that make use of `AWD_LSTM`) in Gradient.
  prefs: []
  type: TYPE_NORMAL
- en: 'Ensure that you have uploaded the files that make up the standalone Covid-related
    tweets dataset to your Gradient environment by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the `archive.zip` file from [https://www.kaggle.com/datatattle/covid-19-nlp-text-classification](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Unzip the downloaded `archive.zip` file to extract the `Corona_NLP_test.csv`
    and `Corona_NLP_train.csv` files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'From the terminal in your Gradient environment, make `/storage/archive` your
    current directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the `/storage/archive/covid_tweets` directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Make `/storage/archive/covid_tweets` your current directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create `test` and `train` directories in `/storage/archive/covid_tweets`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upload the files you extracted in *step 2* (`Corona_NLP_test.csv` and `Corona_NLP_train.csv`)
    to `/storage/archive/covid_tweets`. You can use the upload button in JupyterLab
    in Gradient to do the upload, but you need to do it in several steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) From the terminal in your Gradient environment, make `/notebooks` your current
    directory:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'temp your current folder, select the upload button (see *Figure 4.10*), and
    select the Corona_NLP_test.csv and Corona_NLP_train.csv files from your local
    system folder where you extracted them in *step 2*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 4.10 – Upload button in JupyterLab'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_4_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.10 – Upload button in JupyterLab
  prefs: []
  type: TYPE_NORMAL
- en: 'd) From the terminal in your Gradient environment, copy the `Corona_NLP_test.csv` file
    into the `/storage/archive/covid_tweets/test` directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'e) Copy the `Corona_NLP_train.csv` file into the `/storage/archive/covid_tweets/train`
    directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Once you have completed the steps to upload the files that make up the Covid-related
    tweets dataset, you should have the following directory structure in the `/storage/archive/covid_tweets`
    directory in your Gradient environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: With these preparation steps, you have brought the files that make up the dataset
    into the correct location in your Gradient environment to be used by a fastai
    model.
  prefs: []
  type: TYPE_NORMAL
- en: I am grateful for the opportunity to include the Covid tweets dataset in this
    book and I would like to thank the curators of this dataset and Kaggle for making
    the dataset available.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: Aman Miglani (2020). *Coronavirus tweets NLP - Text Classification* ([https://www.kaggle.com/datatattle/covid-19-nlp-text-classification](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification))
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will be running through the `text_standalone_dataset_lm.ipynb`
    notebook to train a language model using the Covid-related tweets standalone dataset.
    Once you have the notebook open in Gradient, complete these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the notebook up to the `Ingest the dataset` cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to define a path object for the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The argument for this path definition is the name of the root of the directory
    hierarchy in your Gradient environment into which you copied the CSV files for
    the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define a `df_train` dataframe to contain the contents
    of the `Corona_NLP_train.csv` file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the dataframe:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) The `path/'train/Corona_NLP_train.csv'` argument specifies the partially
    qualified filename for the training portion of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The `encoding = "ISO-8859-1"` argument specifies the encoding to use for
    the file. This encoding is selected to ensure that the content of the CSV file
    can be ingested into a dataframe without any errors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define a `TextDataLoaders` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the `TextDataLoaders` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `df_train`: The dataframe that you created in the previous step.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `path`: The path object for the dataset.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `text_col`: The column in the dataframe containing the text that will be
    used to train the model. For this dataset, the `OriginalTweet` column contains
    the text used to train the model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `is_lm`: An indicator that this model is a language model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define and train the deep learning model with a `language_model_learner`
    object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The definition of the `language_model_learner` object includes the call to
    `to_fp16()` to specify mixed-precision training (summarized here: [https://docs.fast.ai/callback.fp16.html#Learner.to_fp16](https://docs.fast.ai/callback.fp16.html#Learner.to_fp16))
    to reduce the memory consumption of the training process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here are the arguments for the definition of the `language_model_learner` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `dls`: The `TextDataLoaders` object that you defined in the previous step.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `AWD_LSTM`: The pre-trained model to use as a basis for this model. This
    is the pre-trained language model incorporated with fastai that is trained with
    Wikipedia.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `metrics`: The performance metric to be optimized for the model, in this
    case, accuracy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here are the arguments for the `fine_tune` statement:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) The epoch count argument (first argument) specifies the number of epochs
    for the training process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The learning rate argument (second argument) specifies the learning rate
    for the training process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The results of the training process, as shown in *Figure 4.11*, are displayed
    once the `fine_tune` statement has been run:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Results of the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_4_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.11 – Results of the training process
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to exercise the trained language model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The results are displayed as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Prediction of a language model trained on a standalone text
    dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_4_12.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.12 – Prediction of a language model trained on a standalone text dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to save the model. You can update the cell to specify
    the directory and filename to which to save the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to save the current path value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to assign a new value to the learner object path. The
    reason for doing this is that the default location for the model is not writeable
    on Gradient, so you need to change the path value to a directory where you have
    write access:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to save the encoder subset of the language model. This
    is the model minus the final layer. You will use this encoder in the next recipe
    when you train a text classifier on the Covid-related tweets standalone dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You used fastai to do transfer learning on top of an existing
    model with the Covid-related tweets standalone dataset to create a language model
    fine-tuned on that dataset. In the next section, you will use the encoder that
    you saved in the last step to fine-tune a text classifier trained on the standalone
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you have seen a simple example of how to train a language model
    with fastai using a standalone text dataset. The language model is created by
    taking an existing model (`AWD_LSTM`) that has been trained with the massive Wikipedia
    dataset and then fine-tuning it using the standalone Covid-related tweets dataset.
  prefs: []
  type: TYPE_NORMAL
- en: By taking advantage of transfer learning in this way, we end up with a language
    model that combines a good degree of capability in terms of general-purpose English
    (thanks to the model pre-trained on the wiki dataset) as well as the capability
    to produce text that is specific to the use case of social media related to the
    Covid-19 pandemic (thanks to the Covid tweets dataset). By following the recipe
    in this section, you can take advantage of fastai to apply this approach (transfer
    learning for NLP) on other text datasets for other use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Training a deep learning text classifier with a standalone text dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the *Training a deep learning language model with a standalone text dataset*
    section, we trained a language model using the standalone text dataset: the Kaggle
    Coronavirus tweets NLP – Text Classification dataset described here: [https://www.kaggle.com/datatattle/covid-19-nlp-text-classification](https://www.kaggle.com/datatattle/covid-19-nlp-text-classification).
    In this section, we will use this language model to create a text classifier trained
    with the Covid-related tweets dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This recipe makes use of the encoder that you trained in the *Training a deep
    learning language model with a standalone text dataset* section, so ensure that
    you have followed the steps in the recipe in that section. In particular, ensure
    that you have saved the encoder from the language model you trained in the previous
    section.
  prefs: []
  type: TYPE_NORMAL
- en: 'Also, make sure you have followed all the steps from the *Getting ready* sub-section
    of the previous section to ensure the following:'
  prefs: []
  type: TYPE_NORMAL
- en: That you have access to the `AWD_LSTM` model in your Gradient environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: That you have uploaded the files (`Corona_NLP_test.csv` and `Corona_NLP_train.csv`)
    that make up the standalone Covid-related tweets dataset to your Gradient environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will be running through the `text_standalone_dataset_classifier.ipynb`
    notebook to train a text classifier deep learning model using the Covid-related
    tweets dataset. Once you have the notebook open in Gradient, perform the following
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the notebook up to the `Ingest the dataset` cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to define a path object for the dataset. Note that the
    argument is the name of the root directory in your Gradient environment into which
    you copied the CSV files for the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to define a dataframe to contain the contents of the
    `Corona_NLP_train.csv` file (the training portion of the Covid-related tweets
    dataset):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the dataframe:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) The `path/'train/Corona_NLP_train.csv'` argument specifies the partially
    qualified filename for the training portion of the dataset.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The `encoding = "ISO-8859-1"` argument specifies the encoding to use for
    the file. This encoding is selected to ensure that the content of the CSV file
    can be ingested into a dataframe without any errors.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define a `TextDataLoaders` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the `TextDataLoaders` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `df_train`: The dataframe that you created in the previous step.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `path`: The path object for the dataset.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `text_col`: The column in the dataframe containing the text that will be
    used to train the model. For this dataset, the `OriginalTweet` column contains
    the text used to train the model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `label_col`: The column in the dataframe containing the labels that the
    text classifier will predict.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to see a batch from the `TextDataLoaders` object that
    you defined in the previous step:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this statement, the `text` and `category` columns, will be as
    follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Batch from the Covid tweets dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_4_13.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.13 – Batch from the Covid tweets dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define the `text_classifier_learner` object for the
    text classifier model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The definition of the `text_classifier_learner` object includes the call to
    `to_fp16()` to specify mixed-precision training (summarized here: [https://docs.fast.ai/callback.fp16.html#Learner.to_fp16](https://docs.fast.ai/callback.fp16.html#Learner.to_fp16))
    to reduce the memory consumption of the training process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here are the arguments for the definition of the `text_classifier_learner`
    object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `dls`: The `TextDataLoaders` object that you defined in the previous step.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `AWD_LSTM`: The pre-trained model to use as a basis for this model. This
    is the pre-trained language model incorporated with fastai that is trained with
    Wikipedia.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `metrics`: The performance metric to be optimized for the model, in this
    case, accuracy.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to assign a new value to the learner object path. The
    reason for doing this is to set the path to match the directory where you saved
    the encoder in the recipe in the previous section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to load the encoder that you saved in the recipe in
    the *Training a deep learning language model with a standalone text dataset* section
    to the `learn_clas` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to reset the value of the learner object path:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for `fit_one_cycle`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: a) The epoch count argument (first argument) specifies that the training is
    run for 1 epoch.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The learning rate argument (second argument) specifies that the learning
    rate is equal to 0.02.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell (as shown in *Figure 4.14*) shows the results of the
    training:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Output of the text classifier training'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_4_14.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 4.14 – Output of the text classifier training
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the cells to get predictions on text strings that you expect to be negative
    and positive and observe whether the trained model makes the expected predictions
    (as shown in *Figure 4.15*):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Using the text classifier to get predictions on text strings'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_4_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.15 – Using the text classifier to get predictions on text strings
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have taken advantage of the facilities of fastai to train
    a text classifier on a standalone dataset using transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The code for the text classifier model for the standalone Covid-related tweets
    dataset has some differences from the code for the text classifier model for the
    curated IMDb text dataset. Let's examine some of these differences.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the IMDb dataset, the `TextDataLoaders` definition does not include a `label_col`
    parameter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: 'By contrast, the `TextDataLoaders` definition for the standalone dataset includes
    both `text_col` and `label_col` parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'What''s the reason for these differences? First, for the `IMDb` dataset, we
    use the `from_folder` variation of `TextDataLoaders` because the dataset is organized
    as a collection of individual text files whose class is encoded by the directory
    that the file is in. Here is the directory structure of the IMDb dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'Consider one file from the IMDB dataset, `train/pos/9971_10.txt`:'
  prefs: []
  type: TYPE_NORMAL
- en: '*This film was Excellent, I thought that the original one was quiet mediocre.
    This one however got all the ingredients, a factory 1970 Hemi Challenger with
    4 speed transmission that really shows that Mother Mopar knew how to build the
    best muscle cars! I was in Chrysler heaven every time Kowalski floored that big
    block Hemi, and he sure did that a lot :)*'
  prefs: []
  type: TYPE_NORMAL
- en: How does fastai know the class of this review when we train the text classifier
    model? It knows because this file is in the `/pos` directory. Thanks to the flexibility
    of fastai, we simply have to pass the `path` value to the definition of the `TextDataLoaders`
    object and the fastai framework figures out the category of each text sample in
    the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s consider the standalone Covid-related tweets dataset. This dataset
    is packaged as CSV files that look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Sample from the Covid-related tweets dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_4_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.16 – Sample from the Covid-related tweets dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike the IMDb dataset, the Covid-related tweets dataset is encapsulated in
    just two files (one for the training dataset and one for the test dataset). The
    columns of these files have the information that fastai needs to train the model:'
  prefs: []
  type: TYPE_NORMAL
- en: The text of the sample – in the `OriginalTweet` column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The class (also known as the label) of the sample – in the `Sentiment` column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To tell fastai how to interpret this dataset, we need to explicitly tell it
    which column of the dataset the text is in and which column the class (or label)
    is in the definition of the `TextDataLoaders` object, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: The `IMDb` dataset is made up of thousands of individual text files spread across
    a complex set of directories that encode the class of each text file. By contrast,
    the Covid-related tweets dataset is made up of two CSV files that have the text
    samples and their classes as columns. Despite the differences in the organization
    of these two datasets, fastai can ingest them and prepare them to train a deep
    learning model with just a few tweaks to the definition of the `TextDataLoaders`
    object. fastai's ability to easily ingest datasets in a variety of different formats
    isn't just useful for text datasets; it is useful for all kinds of datasets. As
    you will see in [*Chapter 6*](B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152),
    *Training Models with Visual Data*, we really benefit from this ability when we
    deal with image datasets, which have many different kinds of organization.
  prefs: []
  type: TYPE_NORMAL
- en: Test your knowledge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you have worked through a number of extended examples of training fastai
    deep learning models with text datasets, you can try some variations to practice
    what you've learned.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that you have followed the *Getting ready* steps from the *Training a
    deep learning text classifier with a standalone text dataset* section to prepare
    your Gradient environment and upload the Covid-related tweets dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can follow the steps in this section to try some variations on the models
    that you trained with the Covid-related tweets dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make a copy of the `text_standalone_dataset_lm.ipynb` notebook that you worked
    through in the *Training a deep learning language model with a standalone text
    dataset* recipe. Give your new copy of the notebook the following name: `text_standalone_dataset_lm_combo.ipynb`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In your new notebook, in addition to creating a dataframe for the train CSV
    `Corona_NLP_train.csv` file, create a dataframe for the test CSV `Corona_NLP_test.csv`
    file by adding a cell to the notebook that looks like this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Use the pandas `concat` function to combine the two dataframes into a new dataframe
    called `df_combo`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now update the remainder of your new notebook to use `df_combo` instead of `df_train`
    and run the whole notebook to train a new language model. Do you notice any difference
    in the performance of the model?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In most model training situations, you need to ensure that you don't use the
    test dataset to train the model. Can you think of why you could get away with
    using the test set to train a language model like this?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Congratulations! You have completed your review of training fastai deep learning
    models on text datasets using fastai.
  prefs: []
  type: TYPE_NORMAL
