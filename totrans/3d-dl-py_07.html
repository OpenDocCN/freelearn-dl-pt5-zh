<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer107">
<h1 class="chapter-number" id="_idParaDest-78" lang="en-GB"><a id="_idTextAnchor081"/>6</h1>
<h1 id="_idParaDest-79" lang="en-GB"><a id="_idTextAnchor082"/>Exploring Neural Radiance Fields (NeRF)</h1>
<p lang="en-GB">In the previous chapter, you learned about Differentiable Volume Rendering where you reconstructed the 3D volume from several multi-view images. With this technique, you modeled a volume consisting of N x N x N voxels. The space requirement for storing this volume scale would therefore be O(N<span class="superscript" lang="">3</span>). This is undesirable, especially if we want to transmit this information over the network. Other methods can overcome such large disk space requirements, but they are prone to smoothing geometry and texture. Therefore, we cannot use them to model very complex or textured <span class="No-Break" lang="">scenes reliably.</span></p>
<p lang="en-GB">In this chapter, we are going to discuss a breakthrough new approach to representing 3D scenes, called <strong class="bold" lang="">Neural Radiance Fields</strong> (<strong class="bold" lang="">NeRF</strong>). This is <a id="_idIndexMarker275"/>one of the first techniques to model a 3D scene that requires less constant disk space and at the same time, captures the fine geometry and texture of <span class="No-Break" lang="">complex scenes.</span></p>
<p lang="en-GB">In this chapter, you will learn about the <span class="No-Break" lang="">following topics:</span></p>
<ul>
<li lang="en-GB"><span class="No-Break" lang="">Understanding NeRF</span></li>
<li lang="en-GB">Training a <span class="No-Break" lang="">NeRF model</span></li>
<li lang="en-GB">Understanding the NeRF <span class="No-Break" lang="">model architecture</span></li>
<li lang="en-GB">Understanding volume rendering with <span class="No-Break" lang="">radiance fields</span></li>
</ul>
<h1 id="_idParaDest-80" lang="en-GB"><a id="_idTextAnchor083"/>Technical requirements</h1>
<p lang="en-GB">In order to run the example code snippets in this book, you need to have a computer, ideally with a GPU with about 8 GB of GPU memory. Running code snippets only using CPUs is not impossible but will be extremely slow. The recommended computer configuration is <span class="No-Break" lang="">as follows:</span></p>
<ul>
<li lang="en-GB">A GPU device – for example, Nvidia GTX series or RTX series with at least 8 GB <span class="No-Break" lang="">of memory</span></li>
<li lang="en-GB"><span class="No-Break" lang="">Python 3.7+</span></li>
<li lang="en-GB">The PyTorch and <span class="No-Break" lang="">PyTorch3D libraries</span></li>
</ul>
<p lang="en-GB">The code snippets for this chapter can be found at <a href="https://github.com/PacktPublishing/3D-Deep-Learning-with-Python">https://github.com/PacktPublishing/3D-Deep-Learning-with-Python</a>. </p>
<h1 id="_idParaDest-81" lang="en-GB"><a id="_idTextAnchor084"/>Understanding NeRF</h1>
<p lang="en-GB">View synthesis<a id="_idIndexMarker276"/> is a long-standing problem in 3D computer vision. The challenge is to synthesize new views of a 3D scene using a small number of available 2D snapshots of the scene. It is particularly challenging because the view of a complex scene can depend on a lot of factors such as object artifacts, light sources, reflections, opacity, object surface texture, and occlusions. Any good representation should capture this information either implicitly or explicitly. Additionally, many objects have complex structures that are not completely visible from a certain viewpoint. The challenge is to construct complete information about the world given incomplete and <span class="No-Break" lang="">noisy information.</span></p>
<p lang="en-GB">As the name suggests, NeRF<a id="_idIndexMarker277"/> uses neural networks to model the world. As we will learn later in the chapter, NeRF uses neural networks in a very unconventional manner. It was a concept first developed by a team of researchers from UC Berkeley, Google Research, and UC San Diego. Because of their unconventional use of neural networks and the quality of the learned models, it has spawned multiple new inventions in the fields of view synthesis, depth sensing, and 3D reconstruction. It is therefore a very useful concept to understand as you navigate through the rest of this chapter <span class="No-Break" lang="">and book.</span></p>
<p lang="en-GB">In this section, first, we will explore the meaning of radiance fields and how we can use a neural network to represent these <span class="No-Break" lang="">radiance fields.</span></p>
<h2 id="_idParaDest-82" lang="en-GB"><a id="_idTextAnchor085"/>What is a radiance field?</h2>
<p lang="en-GB">Before we get to NeRF, let us understand what<a id="_idIndexMarker278"/> radiance fields are first. You see an object when the light from that object is processed by your body’s sensory system. The light from the object can either be generated by the object itself or reflected off it. </p>
<p lang="en-GB">Radiance is the standard metric for measuring the amount of light that passes through or is emitted from an area inside a particular solid angle. For our purposes, we can treat the radiance to be the intensity of a point in space when viewed in a particular direction. When capturing this information in RGB, the radiance will have three components corresponding to the colors Red, Green, and Blue. The radiance of a point in space can depend on many factors, including <span class="No-Break" lang="">the following:</span></p>
<ul>
<li lang="en-GB">Light sources illuminating <span class="No-Break" lang="">that point</span></li>
<li lang="en-GB">The existence of a surface (or volume density) that can reflect light at <span class="No-Break" lang="">that point</span></li>
<li lang="en-GB">The texture properties of <span class="No-Break" lang="">the surface</span></li>
</ul>
<p lang="en-GB">The following figure depicts the radiance value at a certain point in the 3D scene when viewed at a certain angle. The <a id="_idIndexMarker279"/>radiance field is just a collection of these radiance values at all points and viewing angles in the <span class="No-Break" lang="">3D scene:</span></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><img alt="Figure 6.1: The radiance (r, g, b) at a point (x, y, z) when viewed with certain viewing angles (θ, ∅) " height="680" src="image/B18217_06_1.png" width="906"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1: The radiance (r, g, b) at a point (x, y, z) when viewed with certain viewing angles (θ, ∅)</p>
<p lang="en-GB">If we know the <a id="_idIndexMarker280"/>radiance of all the points in a scene in all directions, we have all the visual information we need about the scene. This field of radiance values constitutes a radiance field. We can store the radiance field information as a volume in a 3D voxel grid data structure. We saw this in the previous chapter when discussing volume rendering. </p>
<h2 id="_idParaDest-83" lang="en-GB"><a id="_idTextAnchor086"/>Representing radiance fields with neural networks</h2>
<p lang="en-GB">In this<a id="_idIndexMarker281"/> section, we will explore a new way of<a id="_idIndexMarker282"/> using neural networks. In typical computer vision tasks, we use neural networks to map an input in the pixel space to an output. In the case of a discriminative model, the output is a class label. In the case of a generative model, the output is also in the pixel space. A NeRF model is neither <span class="No-Break" lang="">of these.</span></p>
<p lang="en-GB">NeRF uses a neural network to represent a volumetric scene function. This neural network takes a 5-dimensional input. These are the three spatial locations (x, y, z) and two viewing angles (θ, ∅). Its output is the volume density σ at (x, y, z) and the emitted color (r, g, b) of the point (x, y, z) when viewed from the viewing angle (θ, ∅). The emitted color is a proxy used to estimate the radiance at that point. In practice, instead of directly using (θ, ∅) to represent the viewing angle, NeRF uses the unit direction vector d in the 3D Cartesian coordinate system. These are equivalent representations of the <span class="No-Break" lang="">viewing angle.</span></p>
<p lang="en-GB">The model therefore maps any point in the 3D scene and a viewing angle to the volume density and radiance at that point. You can then use this model to synthesize views by querying the 5D coordinates along camera rays and using the volume rendering technique you learned about in the previous chapter to project the output colors and volume densities into <span class="No-Break" lang="">an image.</span></p>
<p lang="en-GB">With the following figure, let us find out how a neural network can be used to predict the density and radiance at a certain point (x, y, z) when viewed along a certain direction (<span class="No-Break" lang="">θ, </span><span class="No-Break" lang="">∅</span><span class="No-Break" lang="">):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer096">
<img alt="Figure 6.2: The inputs (x, y, z, θ, and ∅) are used to create separate harmonic embeddings for the spatial location and viewing angle first, forming the input to a neural network, and this neural network outputs the predicted density and radiance " height="491" src="image/B18217_06_2.jpg" width="1048"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2: The inputs (x, y, z, θ, and ∅) are used to create separate harmonic embeddings for the spatial location and viewing angle first, forming the input to a neural network, and this neural network outputs the predicted density and radiance</p>
<p lang="en-GB">Note that this is a fully connected network – typically, this is referred to as a <strong class="bold" lang="">Multi-Layer Perceptron</strong> (<strong class="bold" lang="">MLP</strong>). More<a id="_idIndexMarker283"/> importantly, this is not a convolutional neural network. We refer to this model as the <a id="_idIndexMarker284"/>NeRF model. A single NeRF model is optimized on a set of images from a single scene. Therefore, each model only knows the scene on which it is optimized. This is not the standard way to use a neural network, where we typically need the model to generalize unseen images. In the case of NeRF, we need the network to generalize unseen <span class="No-Break" lang="">viewpoints well.</span></p>
<p lang="en-GB">Now that you know what a NeRF is, let us look at how to use it to render new views <span class="No-Break" lang="">from it.</span></p>
<h1 id="_idParaDest-84" lang="en-GB"><a id="_idTextAnchor087"/>Training a NeRF model</h1>
<p lang="en-GB">In<a id="_idIndexMarker285"/> this section, we are going to train a simple NeRF model on images generated from the synthetic cow model. We are only going to instantiate the NeRF model without worrying about how it is implemented. The implementation details are covered in the next section. A single neural network (NeRF model) is trained to represent a single 3D scene. The following codes can be found in <strong class="source-inline" lang="">train_nerf.py</strong>, which can be found in this chapter’s GitHub repository. It is modified from a PyTorch3D tutorial. Let us go through the code to train a NeRF model on the synthetic <span class="No-Break" lang="">cow scene:</span></p>
<ol>
<li lang="en-GB">First, let us import the <span class="No-Break" lang="">standard modules:</span><p class="source-code" lang="en-GB">import torch</p><p class="source-code" lang="en-GB">import matplotlib.pyplot as plt</p></li>
<li lang="en-GB">Next, let<a id="_idIndexMarker286"/> us import the functions and classes used for rendering. These are <strong class="source-inline" lang="">pytorch3d</strong> <span class="No-Break" lang="">data structures:</span><p class="source-code" lang="en-GB">from pytorch3d.renderer import (</p><p class="source-code" lang="en-GB">FoVPerspectiveCameras,</p><p class="source-code" lang="en-GB">NDCMultinomialRaysampler,</p><p class="source-code" lang="en-GB">MonteCarloRaysampler,</p><p class="source-code" lang="en-GB">EmissionAbsorptionRaymarcher,</p><p class="source-code" lang="en-GB">ImplicitRenderer,</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">from utils.helper_functions import (generate_rotating_nerf,</p><p class="source-code" lang="en-GB">huber,</p><p class="source-code" lang="en-GB">sample_images_at_mc_locs)</p><p class="source-code" lang="en-GB">from nerf_model import NeuralRadianceField</p></li>
<li lang="en-GB">Next, we need to set up <span class="No-Break" lang="">the device:</span><p class="source-code" lang="en-GB">if torch.cuda.is_available():</p><p class="source-code" lang="en-GB">    device = torch.device("cuda:0")</p><p class="source-code" lang="en-GB">    torch.cuda.set_device(device)</p><p class="source-code" lang="en-GB">else:</p><p class="source-code" lang="en-GB">    device = torch.device("cpu")</p></li>
<li lang="en-GB">Next, let us import the utility functions that will let us generate synthetic training data and <span class="No-Break" lang="">visualize images:</span><p class="source-code" lang="en-GB">from utils.plot_image_grid import image_grid</p><p class="source-code" lang="en-GB">from utils.generate_cow_renders import generate_cow_renders</p></li>
<li lang="en-GB">We can now use these utility functions to generate camera angles, images, and silhouettes of the synthetic cow from multiple different angles. This will print the number of generated images, silhouettes, and <span class="No-Break" lang="">camera angles:</span><p class="source-code" lang="en-GB">target_cameras, target_images, target_silhouettes = generate_cow_renders(num_views=40, azimuth_range=180)</p><p class="source-code" lang="en-GB">print(f'Generated {len(target_images)} images/silhouettes/cameras.')</p></li>
<li lang="en-GB">As we <a id="_idIndexMarker287"/>have done in the previous chapter, let us define a ray sampler. We will use <strong class="source-inline" lang="">MonteCarloRaysampler</strong>. This generates rays from a random subset of pixels from the image plane. We need a random sampler here since we want to use a mini-batch gradient descent algorithm to optimize the model. This is a standard neural network optimization technique. Sampling rays systematically can result in optimization bias during each optimization step. This can lead to a worse model and increase the model training time. The ray sampler samples points uniformly along <span class="No-Break" lang="">the ray:</span><p class="source-code" lang="en-GB">render_size = target_images.shape[1] * 2</p><p class="source-code" lang="en-GB">volume_extent_world = 3.0</p><p class="source-code" lang="en-GB">raysampler_mc = MonteCarloRaysampler(</p><p class="source-code" lang="en-GB">    min_x = -1.0,</p><p class="source-code" lang="en-GB">    max_x = 1.0,</p><p class="source-code" lang="en-GB">    min_y = -1.0,</p><p class="source-code" lang="en-GB">    max_y = 1.0,</p><p class="source-code" lang="en-GB">    n_rays_per_image=750,</p><p class="source-code" lang="en-GB">    n_pts_per_ray=128,</p><p class="source-code" lang="en-GB">    min_depth=0.1,</p><p class="source-code" lang="en-GB">    max_depth=volume_extent_world,</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">Next, we will define the ray marcher. This uses the volume densities and colors of points sampled along the ray and renders the pixel value for that ray. For the ray marcher, we use <strong class="source-inline" lang="">EmissionAbsorptionRaymarcher</strong>. This implements the classical Emission-Absorption ray <span class="No-Break" lang="">marching algorithm:</span><p class="source-code" lang="en-GB">raymarcher = EmissionAbsorptionRaymarcher()</p></li>
<li lang="en-GB">We will now instantiate <strong class="source-inline" lang="">ImplicitRenderer</strong>. This composes the ray sampler and the ray marcher into a single <span class="No-Break" lang="">data structure:</span><p class="source-code" lang="en-GB">renderer_mc = ImplicitRenderer(raysampler=raysampler_mc, raymarcher=raymarcher)</p></li>
<li lang="en-GB">Let us<a id="_idIndexMarker288"/> look at the Huber loss function. This is defined in <strong class="source-inline" lang="">utils.helper_functions.huber</strong>. It is a robust alternative to the mean squared error function and is less sensitive <span class="No-Break" lang="">to outliers:</span><p class="source-code" lang="en-GB">def huber(x, y, scaling=0.1):</p><p class="source-code" lang="en-GB">    diff_sq = (x - y) ** 2</p><p class="source-code" lang="en-GB">    loss = ((1 + diff_sq / (scaling**2)).clamp(1e-4).sqrt() - 1) * float(scaling)</p><p class="source-code" lang="en-GB">    return loss</p></li>
<li lang="en-GB">We will now look at a helper function defined in <strong class="source-inline" lang="">utils.helper_functions.sample_images_at_mc_loss</strong> that is used to extract ground truth pixel values from target images. <strong class="source-inline" lang="">MonteCarloRaysampler</strong> samples rays passing through some <strong class="source-inline" lang="">x</strong> and <strong class="source-inline" lang="">y</strong> locations on the image. These are in <strong class="bold" lang="">Normalized Device Coordinates</strong> (<strong class="bold" lang="">NDCs</strong>). However, we<a id="_idIndexMarker289"/> need to sample from the image pixel coordinates. For this, we use the <strong class="source-inline" lang="">torch.nn.functional.grid_sample</strong> function. This uses interpolation techniques in the background to provide us with accurate pixel values. This is better than just mapping NDC coordinates to pixel coordinates and then sampling the one pixel that corresponds to an NDC coordinate value. In the NDC coordinate system, <strong class="source-inline" lang="">x</strong> and <strong class="source-inline" lang="">y</strong> both have a range of <strong class="source-inline" lang="">[-1, 1]</strong>. For example, (x, y) = (-1, -1) corresponds to the top left corner of <span class="No-Break" lang="">the image:</span><p class="source-code" lang="en-GB">def sample_images_at_mc_locs(target_images, sampled_rays_xy):</p><p class="source-code" lang="en-GB">    ba = target_images.shape[0]</p><p class="source-code" lang="en-GB">    dim = target_images.shape[-1]</p><p class="source-code" lang="en-GB">    spatial_size = sampled_rays_xy.shape[1:-1]</p><p class="source-code" lang="en-GB">    images_sampled = torch.nn.functional.grid_sample(</p><p class="source-code" lang="en-GB">        target_images.permute(0, 3, 1, 2), </p><p class="source-code" lang="en-GB">        -sampled_rays_xy.view(ba, -1, 1, 2),  # note the sign inversion</p><p class="source-code" lang="en-GB">        align_corners=True</p><p class="source-code" lang="en-GB">    )</p><p class="source-code" lang="en-GB">    return images_sampled.permute(0, 2, 3, 1).view(</p><p class="source-code" lang="en-GB">        ba, *spatial_size, dim</p><p class="source-code" lang="en-GB">    )</p></li>
<li lang="en-GB">While <a id="_idIndexMarker290"/>training the model, it is always useful to visualize the model output. Among many other uses, this will help us course-correct if we see that the model outputs are not changing over time. So far, we have used <strong class="source-inline" lang="">MonteCarloRaysampler</strong>, which is very useful while training the model, but this will not be useful when we want to render full images since it randomly samples rays. To view the full image, we need to systematically sample rays corresponding to all the pixels in the output frame. To achieve this, we are going to <span class="No-Break" lang="">use </span><span class="No-Break" lang=""><strong class="source-inline" lang="">NDCMultinomialRaysampler</strong></span><span class="No-Break" lang="">:</span><p class="source-code" lang="en-GB">render_size = target_images.shape[1] * 2</p><p class="source-code" lang="en-GB">volume_extent_world = 3.0</p><p class="source-code" lang="en-GB">raysampler_grid = NDCMultinomialRaysampler(</p><p class="source-code" lang="en-GB">    image_height=render_size,</p><p class="source-code" lang="en-GB">    image_width=render_size,</p><p class="source-code" lang="en-GB">    n_pts_per_ray=128,</p><p class="source-code" lang="en-GB">    min_depth=0.1,</p><p class="source-code" lang="en-GB">    max_depth=volume_extent_world,</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">We will<a id="_idIndexMarker291"/> now instantiate the <span class="No-Break" lang="">implicit renderer:</span><p class="source-code" lang="en-GB">renderer_grid = ImplicitRenderer(</p><p class="source-code" lang="en-GB">    raysampler=raysampler_grid, raymarcher=raymarcher,</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">In order to visualize the intermediate training results, let us define a helper function that takes the model and camera parameters as its input and compares it with the target image and its corresponding silhouette. If the rendered image is very large, it might not be possible to fit all the rays into the GPU memory at once. Therefore, we need to break them down into batches and run several forward passes on the model to get the output. We need to merge the output of the rendering into one coherent image. In order to keep it simple, we will just import the function here, but the full code is provided in the book’s <span class="No-Break" lang="">GitHub repository:</span><p class="source-code" lang="en-GB">from utils.helper_function import show_full_render</p></li>
<li lang="en-GB">We will now instantiate the NeRF model. To keep it simple, we are not presenting the model’s class definition here. You can find that in the chapter’s GitHub repository. Because the model structure is so important, we are going to discuss it in detail in a <span class="No-Break" lang="">separate section:</span><p class="source-code" lang="en-GB">from nerf_model import NeuralRadianceField</p><p class="source-code" lang="en-GB">neural_radiance_field = NeuralRadianceField()</p></li>
<li lang="en-GB">Let us now prepare to train the model. In order to reproduce the training, we should set the random seed used in <strong class="source-inline" lang="">torch</strong> to a fixed value. We then need to send all the variables to the device used for processing. Since this is a resource-intensive computational problem, we should ideally run it on a GPU-enabled machine. Running this on a CPU is extremely time-consuming and <span class="No-Break" lang="">not recommended:</span><p class="source-code" lang="en-GB">torch.manual_seed(1)</p><p class="source-code" lang="en-GB">renderer_grid = renderer_grid.to(device)</p><p class="source-code" lang="en-GB">renderer_mc = renderer_mc.to(device)</p><p class="source-code" lang="en-GB">target_cameras = target_cameras.to(device)</p><p class="source-code" lang="en-GB">target_images = target_images.to(device)</p><p class="source-code" lang="en-GB">target_silhouettes = target_silhouettes.to(device)</p><p class="source-code" lang="en-GB">neural_radiance_field = neural_radiance_field.to(device)</p></li>
<li lang="en-GB">We <a id="_idIndexMarker292"/>will now define the hyperparameters used to train the model. <strong class="source-inline" lang="">lr</strong> represents the learning rate, <strong class="source-inline" lang="">n_iter</strong> represents the number of training iterations (or steps), and <strong class="source-inline" lang="">batch_size</strong> represents the number of random cameras used in a mini-batch. The batch size here is chosen according to the GPU memory you have. If you find that you are running out of GPU memory, choose a smaller batch <span class="No-Break" lang="">size value:</span><p class="source-code" lang="en-GB">lr = 1e-3</p><p class="source-code" lang="en-GB">optimizer = torch.optim.Adam(neural_radiance_field.parameters(), lr=lr)</p><p class="source-code" lang="en-GB">batch_size = 6</p><p class="source-code" lang="en-GB">n_iter = 3000</p></li>
<li lang="en-GB">We are now ready to train our model. During each iteration, we should sample a mini-batch of <span class="No-Break" lang="">cameras randomly:</span><p class="source-code" lang="en-GB">loss_history_color, loss_history_sil = [], []</p><p class="source-code" lang="en-GB">for iteration in range(n_iter):</p><p class="source-code" lang="en-GB">    if iteration == round(n_iter * 0.75):</p><p class="source-code" lang="en-GB">        print('Decreasing LR 10-fold ...')</p><p class="source-code" lang="en-GB">        optimizer = torch.optim.Adam(</p><p class="source-code" lang="en-GB">            neural_radiance_field.parameters(), lr=lr * 0.1</p><p class="source-code" lang="en-GB">        )</p><p class="source-code" lang="en-GB">    optimizer.zero_grad()</p><p class="source-code" lang="en-GB">    batch_idx = torch.randperm(len(target_cameras))[:batch_size]</p><p class="source-code" lang="en-GB">    batch_cameras = FoVPerspectiveCameras(</p><p class="source-code" lang="en-GB">        R = target_cameras.R[batch_idx], </p><p class="source-code" lang="en-GB">        T = target_cameras.T[batch_idx], </p><p class="source-code" lang="en-GB">        znear = target_cameras.znear[batch_idx],</p><p class="source-code" lang="en-GB">        zfar = target_cameras.zfar[batch_idx],</p><p class="source-code" lang="en-GB">        aspect_ratio = target_cameras.aspect_ratio[batch_idx],</p><p class="source-code" lang="en-GB">        fov = target_cameras.fov[batch_idx],</p><p class="source-code" lang="en-GB">        device = device,</p><p class="source-code" lang="en-GB">    )</p></li>
<li lang="en-GB">During <a id="_idIndexMarker293"/>each iteration, first, we need to obtain the rendered pixel values and rendered silhouettes at the randomly sampled cameras using the NeRF model. These are predicted values. This is the forward propagation step. We want to compare these predictions to the ground truth to find out the training loss. Our loss is a mixture of two loss functions: A, a Huber loss on the predicted silhouette and the ground truth silhouette, and B, a Huber loss on the predicted color and the ground truth color. Once we obtain the loss value, we <a id="_idIndexMarker294"/>can backpropagate through the NeRF model and step through <span class="No-Break" lang="">the optimizer:</span><p class="source-code" lang="en-GB">    rendered_images_silhouettes, sampled_rays = renderer_mc(</p><p class="source-code" lang="en-GB">        cameras=batch_cameras, </p><p class="source-code" lang="en-GB">        volumetric_function=neural_radiance_field</p><p class="source-code" lang="en-GB">    )</p><p class="source-code" lang="en-GB">    rendered_images, rendered_silhouettes = (</p><p class="source-code" lang="en-GB">        rendered_images_silhouettes.split([3, 1], dim=-1)</p><p class="source-code" lang="en-GB">    )</p><p class="source-code" lang="en-GB">    </p><p class="source-code" lang="en-GB">    silhouettes_at_rays = sample_images_at_mc_locs(</p><p class="source-code" lang="en-GB">        target_silhouettes[batch_idx, ..., None], </p><p class="source-code" lang="en-GB">        sampled_rays.xys</p><p class="source-code" lang="en-GB">    )</p><p class="source-code" lang="en-GB">    sil_err = huber(</p><p class="source-code" lang="en-GB">        rendered_silhouettes, </p><p class="source-code" lang="en-GB">        silhouettes_at_rays,</p><p class="source-code" lang="en-GB">    ).abs().mean()</p><p class="source-code" lang="en-GB">    colors_at_rays = sample_images_at_mc_locs(</p><p class="source-code" lang="en-GB">        target_images[batch_idx], </p><p class="source-code" lang="en-GB">        sampled_rays.xys</p><p class="source-code" lang="en-GB">    )</p><p class="source-code" lang="en-GB">    color_err = huber(</p><p class="source-code" lang="en-GB">        rendered_images, </p><p class="source-code" lang="en-GB">        colors_at_rays,</p><p class="source-code" lang="en-GB">    ).abs().mean()</p><p class="source-code" lang="en-GB">    </p><p class="source-code" lang="en-GB">    loss = color_err + sil_err</p><p class="source-code" lang="en-GB">    loss_history_color.append(float(color_err))</p><p class="source-code" lang="en-GB">    loss_history_sil.append(float(sil_err))</p><p class="source-code" lang="en-GB">     </p><p class="source-code" lang="en-GB">    loss.backward()</p><p class="source-code" lang="en-GB">    optimizer.step()</p></li>
<li lang="en-GB">Let us<a id="_idIndexMarker295"/> visualize model performance after every 100 iterations. This will help us track model progress and terminate it if something unexpected is happening. This creates images in the same folder where you run <span class="No-Break" lang="">the code:</span><p class="source-code" lang="en-GB">    if iteration % 100 == 0:</p><p class="source-code" lang="en-GB">        show_idx = torch.randperm(len(target_cameras))[:1]</p><p class="source-code" lang="en-GB">        fig = show_full_render(</p><p class="source-code" lang="en-GB">        neural_radiance_field,</p><p class="source-code" lang="en-GB">        FoVPerspectiveCameras(</p><p class="source-code" lang="en-GB">            R = target_cameras.R[show_idx], </p><p class="source-code" lang="en-GB">            T = target_cameras.T[show_idx], </p><p class="source-code" lang="en-GB">            znear = target_cameras.znear[show_idx],</p><p class="source-code" lang="en-GB">            zfar = target_cameras.zfar[show_idx],</p><p class="source-code" lang="en-GB">            aspect_ratio = target_cameras.aspect_ratio[show_idx],</p><p class="source-code" lang="en-GB">            fov = target_cameras.fov[show_idx],</p><p class="source-code" lang="en-GB">            device = device), </p><p class="source-code" lang="en-GB">        target_images[show_idx][0],</p><p class="source-code" lang="en-GB">        target_silhouettes[show_idx][0],</p><p class="source-code" lang="en-GB">        renderer_grid,</p><p class="source-code" lang="en-GB">        loss_history_color,</p><p class="source-code" lang="en-GB">        loss_history_sil)</p><p class="source-code" lang="en-GB">    fig.savefig(f'intermediate_{iteration}')</p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer097">
<img alt="Figure 6.3: An intermediate visualization to keep track of model training " height="642" src="image/B18217_06_3.jpg" width="977"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3: An intermediate visualization to keep track of model training</p>
<ol>
<li lang="en-GB" value="20">After the <a id="_idIndexMarker296"/>optimization is finished, we take the final resulting volumetric model and render images from <span class="No-Break" lang="">new angles:</span><p class="source-code" lang="en-GB">from utils import generate_rotating_nerf</p><p class="source-code" lang="en-GB">with torch.no_grad():</p><p class="source-code" lang="en-GB">    rotating_nerf_frames = generate_rotating_nerf(neural_radiance_field, n_frames=3*5)</p><p class="source-code" lang="en-GB">image_grid(rotating_nerf_frames.clamp(0., 1.).cpu().numpy(), rows=3, cols=5, rgb=True, fill=True)</p><p class="source-code" lang="en-GB">plt.show()</p></li>
</ol>
<p lang="en-GB">Finally, the <a id="_idIndexMarker297"/>new rendered images are shown in the <span class="No-Break" lang="">figure here:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<img alt="Figure 6.4: Rendered images of the synthetic cow scene that our NeRF model learned " height="662" src="image/B18217_06_4.jpg" width="1094"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4: Rendered images of the synthetic cow scene that our NeRF model learned</p>
<p lang="en-GB">We trained a NeRF model on a synthetic cow scene in this section. In the next section, we will learn more about how the NeRF model is implemented by going through the code in <span class="No-Break" lang="">more detail.</span></p>
<h1 id="_idParaDest-85" lang="en-GB"><a id="_idTextAnchor088"/>Understanding the NeRF model architecture</h1>
<p lang="en-GB">So far, we have<a id="_idIndexMarker298"/> used the NeRF model class without fully knowing what it looks like. In this section, we will first visualize what the neural network looks like and then go through the code in detail and understand how it is implemented. </p>
<p lang="en-GB">The neural network takes the harmonic embedding of the spatial location (x, y, z) and the harmonic embedding of (θ, ∅) as its input and outputs the predicted density σ and the predicted color (r, g, b). The following figure illustrates the network architecture that we are going to implement in <span class="No-Break" lang="">this section:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer099">
<img alt="Figure 6.5: The simplified model architecture of the NeRF model " height="697" src="image/B18217_06_5.jpg" width="724"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5: The simplified model architecture of the NeRF model</p>
<p class="callout-heading" lang="en-GB">Note</p>
<p class="callout" lang="en-GB">The model architecture that we are going to implement is different from the original NeRF model architecture. In this implementation, we are implementing a simplified version of it. This simplified architecture makes it faster and easier <span class="No-Break" lang="">to train.</span></p>
<p lang="en-GB">Let us start defining the <strong class="source-inline" lang="">NeuralRadianceField</strong> class. We will now go through different parts of this class definition. For the full definition of the class, please refer to the code in the<a id="_idIndexMarker299"/> <span class="No-Break" lang="">GitHub repository:</span></p>
<ol>
<li lang="en-GB" value="1">Each input point is a 5-dimensional vector. It was found that training the model directly on this input performs poorly when representing high-frequency variation in color and geometry. This is because neural networks are known to be biased toward learning low-frequency functions. A good solution to this problem is to map the input space to a higher dimensional space and use that for training. This mapping function is a set of sinusoidal functions with fixed but <span class="No-Break" lang="">unique frequencies:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer100">
<img alt="" height="52" src="image/Formula_06_001.jpg" width="766"/>
</div>
</div>
<ol>
<li lang="en-GB" value="2">This function is applied to each of the components of the <span class="No-Break" lang="">input vector:</span><p class="source-code" lang="en-GB">class NeuralRadianceField(torch.nn.Module):</p><p class="source-code" lang="en-GB">    def __init__(self, n_harmonic_functions=60, n_hidden_neurons=256):</p><p class="source-code" lang="en-GB">        super().__init__()        </p><p class="source-code" lang="en-GB">        self.harmonic_embedding = HarmonicEmbedding(n_harmonic_functions)</p></li>
<li lang="en-GB">The neural network consists of an MLP backbone. It takes the embeddings of location (x, y, z) as its input. This is a fully connected network and the activation function used is <strong class="source-inline" lang="">softplus</strong>. The <strong class="source-inline" lang="">softplus</strong> function is a smoother version of the ReLU activation function. The output of the backbone is a vector with a size <span class="No-Break" lang="">of </span><span class="No-Break" lang=""><strong class="source-inline" lang="">n_hidden_neurons</strong></span><span class="No-Break" lang="">:</span><p class="source-code" lang="en-GB">        embedding_dim = n_harmonic_functions * 2 * 3</p><p class="source-code" lang="en-GB">        self.mlp = torch.nn.Sequential(</p><p class="source-code" lang="en-GB">            torch.nn.Linear(embedding_dim, n_hidden_neurons),</p><p class="source-code" lang="en-GB">            torch.nn.Softplus(beta=10.0),</p><p class="source-code" lang="en-GB">            torch.nn.Linear(n_hidden_neurons, n_hidden_neurons),</p><p class="source-code" lang="en-GB">            torch.nn.Softplus(beta=10.0),</p><p class="source-code" lang="en-GB">        )        </p></li>
<li lang="en-GB">We define a<a id="_idIndexMarker300"/> color layer that takes the output embeddings of the MLP backbone along with the ray direction input embeddings and outputs the RGB color of the input. We combine these inputs because the color output depends strongly on both the location of the point and the direction of viewing and therefore, it is important to provide shorter paths to make use of this <span class="No-Break" lang="">neural network:</span><p class="source-code" lang="en-GB">        self.color_layer = torch.nn.Sequential(</p><p class="source-code" lang="en-GB">            torch.nn.Linear(n_hidden_neurons + embedding_dim, n_hidden_neurons),</p><p class="source-code" lang="en-GB">            torch.nn.Softplus(beta=10.0),</p><p class="source-code" lang="en-GB">            torch.nn.Linear(n_hidden_neurons, 3),</p><p class="source-code" lang="en-GB">            torch.nn.Sigmoid(),</p><p class="source-code" lang="en-GB">        )</p></li>
<li lang="en-GB">Next, we define the <strong class="source-inline" lang="">density</strong> layer. The density of a point is just a function of <span class="No-Break" lang="">its location:</span><p class="source-code" lang="en-GB">        self.density_layer = torch.nn.Sequential(</p><p class="source-code" lang="en-GB">            torch.nn.Linear(n_hidden_neurons, 1),</p><p class="source-code" lang="en-GB">            torch.nn.Softplus(beta=10.0),</p><p class="source-code" lang="en-GB">        self.density_layer[0].bias.data[0] = -1.5        </p></li>
<li lang="en-GB">Now, we need some function to take the output of <strong class="source-inline" lang="">density_layer</strong> and predict the raw density:    <p class="source-code" lang="en-GB">    def _get_densities(self, features):</p><p class="source-code" lang="en-GB">        raw_densities = self.density_layer(features)</p><p class="source-code" lang="en-GB">        return 1 - (-raw_densities).exp()</p></li>
<li lang="en-GB">We do the<a id="_idIndexMarker301"/> same for obtaining colors at a certain point given the ray direction. We need to apply the positional encoding function to the ray direction input first. We should then concatenate it with the output of the <span class="No-Break" lang="">MLP backbone:</span><p class="source-code" lang="en-GB">    def _get_colors(self, features, rays_directions):</p><p class="source-code" lang="en-GB">        spatial_size = features.shape[:-1]        </p><p class="source-code" lang="en-GB">        rays_directions_normed = torch.nn.functional.normalize(</p><p class="source-code" lang="en-GB">            rays_directions, dim=-1</p><p class="source-code" lang="en-GB">        )</p><p class="source-code" lang="en-GB">        rays_embedding = self.harmonic_embedding(</p><p class="source-code" lang="en-GB">            rays_directions_normed</p><p class="source-code" lang="en-GB">        )</p><p class="source-code" lang="en-GB">        rays_embedding_expand = rays_embedding[..., None, :].expand(</p><p class="source-code" lang="en-GB">            *spatial_size, rays_embedding.shape[-1]</p><p class="source-code" lang="en-GB">        )        </p><p class="source-code" lang="en-GB">        color_layer_input = torch.cat(</p><p class="source-code" lang="en-GB">            (features, rays_embedding_expand),</p><p class="source-code" lang="en-GB">            dim=-1</p><p class="source-code" lang="en-GB">        )</p><p class="source-code" lang="en-GB">        return self.color_layer(color_layer_input)</p><p class="source-code" lang="en-GB"> </p></li>
<li lang="en-GB">We define the function for forward propagation. First, we obtain embeddings. Then, we pass them through the MLP backbone to obtain a set of features. We then use that to obtain the densities. We use the features and the ray directions to obtain the color. We return the densities <span class="No-Break" lang="">and colors:</span><p class="source-code" lang="en-GB">    def forward(</p><p class="source-code" lang="en-GB">        self, </p><p class="source-code" lang="en-GB">        ray_bundle: RayBundle,</p><p class="source-code" lang="en-GB">        **kwargs,</p><p class="source-code" lang="en-GB">    ):</p><p class="source-code" lang="en-GB">        rays_points_world = ray_bundle_to_ray_points(ray_bundle)        </p><p class="source-code" lang="en-GB">        embeds = self.harmonic_embedding(</p><p class="source-code" lang="en-GB">            rays_points_world</p><p class="source-code" lang="en-GB">        )</p><p class="source-code" lang="en-GB">        features = self.mlp(embeds)        </p><p class="source-code" lang="en-GB">        rays_densities = self._get_densities(features)</p><p class="source-code" lang="en-GB">        # rays_densities.shape = [minibatch x ... x 1]</p><p class="source-code" lang="en-GB">        rays_colors = self._get_colors(features, ray_bundle.directions)        </p><p class="source-code" lang="en-GB">        return rays_densities, rays_colors</p></li>
<li lang="en-GB">This <a id="_idIndexMarker302"/>function is used to allow for memory-efficient processing of input rays. First, the input rays are split into <strong class="source-inline" lang="">n_batches</strong> chunks and passed through the <strong class="source-inline" lang="">self.forward</strong> function one at a time in a <strong class="source-inline" lang="">for</strong> loop. Combined with disabling PyTorch gradient caching (<strong class="source-inline" lang="">torch.no_grad()</strong>), this allows us to render large batches of rays that do not all fit into GPU memory in a single forward pass. In our case, <strong class="source-inline" lang="">batched_forward</strong> is used to export a fully sized render of the radiance field for <span class="No-Break" lang="">visualization purposes:</span><p class="source-code" lang="en-GB">    def batched_forward(</p><p class="source-code" lang="en-GB">        self, </p><p class="source-code" lang="en-GB">        ray_bundle: RayBundle,</p><p class="source-code" lang="en-GB">        n_batches: int = 16,</p><p class="source-code" lang="en-GB">        **kwargs,        </p><p class="source-code" lang="en-GB">    ):</p><p class="source-code" lang="en-GB">        n_pts_per_ray = ray_bundle.lengths.shape[-1]  </p><p class="source-code" lang="en-GB">        spatial_size = [*ray_bundle.origins.shape[:-1], n_pts_per_ray]</p><p class="source-code" lang="en-GB">        # Split the rays to `n_batches` batches.</p><p class="source-code" lang="en-GB">        tot_samples = ray_bundle.origins.shape[:-1].numel()</p><p class="source-code" lang="en-GB">        batches = torch.chunk(torch.arange(tot_samples), n_batches)</p></li>
<li lang="en-GB">For each <a id="_idIndexMarker303"/>batch, we need to run a forward pass first and then extract the <strong class="source-inline" lang="">ray_densities</strong> and <strong class="source-inline" lang="">ray_colors</strong> separately to be returned as the outputs: <p class="source-code" lang="en-GB">        batch_outputs = [</p><p class="source-code" lang="en-GB">            self.forward(</p><p class="source-code" lang="en-GB">                RayBundle(</p><p class="source-code" lang="en-GB">                    origins=ray_bundle.origins.view(-1, 3)[batch_idx],</p><p class="source-code" lang="en-GB">                 directions=ray_bundle.directions.view(-1, 3)[batch_idx],</p><p class="source-code" lang="en-GB">                    lengths=ray_bundle.lengths.view(-1, n_pts_per_ray)[batch_idx],</p><p class="source-code" lang="en-GB">                    xys=None,</p><p class="source-code" lang="en-GB">                )</p><p class="source-code" lang="en-GB">            ) for batch_idx in batches</p><p class="source-code" lang="en-GB">        ]</p><p class="source-code" lang="en-GB">        rays_densities, rays_colors = [</p><p class="source-code" lang="en-GB">            torch.cat(</p><p class="source-code" lang="en-GB">                [batch_output[output_i] for batch_output in batch_outputs], dim=0</p><p class="source-code" lang="en-GB">            ).view(*spatial_size, -1) for output_i in (0, 1)]</p><p class="source-code" lang="en-GB">        return rays_densities, rays_colors</p></li>
</ol>
<p lang="en-GB">In this section, we<a id="_idIndexMarker304"/> went through the implementation of the NeRF model. To get a complete understanding of NeRF, we also need to explore the theoretical concepts underlying its use in rendering volumes. In the next section, we will explore this in <span class="No-Break" lang="">more detail.</span></p>
<h1 id="_idParaDest-86" lang="en-GB"><a id="_idTextAnchor089"/>Understanding volume rendering with radiance fields</h1>
<p lang="en-GB">Volume rendering<a id="_idIndexMarker305"/> allows you to create a 2D projection of a 3D image or scene. In this section, we will learn about rendering a 3D scene from different viewpoints. For the purposes of this section, assume that the NeRF model is fully trained and that it accurately maps the input coordinates (x, y, z, d­­­<span class="subscript" lang="">x</span>, d<span class="subscript" lang="">y</span>, d<span class="subscript" lang="">z</span>) to an output (r, g, b, σ). Here are the definitions <a id="_idIndexMarker306"/>of these input and <span class="No-Break" lang="">output </span><span class="No-Break" lang=""><a id="_idIndexMarker307"/></span><span class="No-Break" lang="">coordinates:</span></p>
<ul>
<li lang="en-GB"><strong class="bold" lang="">(x, y, z)</strong>: A point in the 3D scene in the <span class="No-Break" lang="">World Coordinates</span></li>
<li lang="en-GB"><strong class="bold" lang="">(d­­­x, dy, dz)</strong>: This is a unit vector that represents the direction along which we are viewing the point (x, <span class="No-Break" lang="">y, z)</span></li>
<li lang="en-GB"><strong class="bold" lang="">(r, g, b)</strong>: This is the radiance value (or the emitted color) of the point (x, <span class="No-Break" lang="">y, z)</span></li>
<li lang="en-GB"><strong class="bold" lang="">σ</strong>: The volume density at the point (x, <span class="No-Break" lang="">y, z)</span></li>
</ul>
<p lang="en-GB">In the previous chapter, you came to understand the concepts underlying volumetric rendering. You<a id="_idIndexMarker308"/> used the technique of ray sampling to get<a id="_idIndexMarker309"/> volume densities and colors from the volume. We called<a id="_idIndexMarker310"/> this volume sampling. In this chapter, we are going to use ray sampling on the radiance field to get the volume densities and colors. We can then perform ray marching to obtain the color intensities of that point. The ray marching technique used in the previous chapter and what is used in this chapter are conceptually similar. The difference is that 3D voxels are discrete representations of 3D space whereas radiance fields are a continuous representation of it (because we use a neural network to encode this representation). This slightly changes the way we accumulate color intensities along <span class="No-Break" lang="">a ray.</span></p>
<h2 id="_idParaDest-87" lang="en-GB"><a id="_idTextAnchor090"/>Projecting rays into the scene</h2>
<p lang="en-GB">Imagine placing a <a id="_idIndexMarker311"/>camera at a viewpoint and pointing it towards the 3D scene <a id="_idIndexMarker312"/>of interest. This is the scene on which the NeRF model is trained. To synthesize a 2D projection of the scene, we first send out of ray into the 3D scene originating from the viewpoint. </p>
<p lang="en-GB">The ray can be parameterized <span class="No-Break" lang="">as follows:</span></p>
<p lang="en-GB"><img alt="" height="45" src="image/Formula_06_002.png" width="269"/></p>
<p lang="en-GB">Here, r is the ray starting from the origin o and traveling along the direction d. It is parametrized by t, which can be varied in order to move to different points on the ray. Note that r is a 3D vector representing a point <span class="No-Break" lang="">in space.</span></p>
<h2 id="_idParaDest-88" lang="en-GB"><a id="_idTextAnchor091"/>Accumulating the color of a ray</h2>
<p lang="en-GB">We can use some <a id="_idIndexMarker313"/>well-known classical color rendering techniques to render the color of the ray. Before we do that, let us get a feeling for some <span class="No-Break" lang="">standard definitions:</span></p>
<ul>
<li lang="en-GB">Let us assume that we want to accumulate the color of the ray between tn (the near bound) and tf (the far bound). We do not care about the ray outside of <span class="No-Break" lang="">these bounds.</span></li>
<li lang="en-GB">We can think of volume density σ(r(t)) as the probability that the ray terminates at an infinitesimal point <span class="No-Break" lang="">around r(t).</span></li>
<li lang="en-GB">We can<a id="_idIndexMarker314"/> think of <img alt="" height="40" src="image/Formula_06_003.png" width="150"/> as the color at the point r(t) on the ray when viewed in along <span class="No-Break" lang="">direction d.</span></li>
<li lang="en-GB"><img alt="" height="89" src="image/Formula_06_004.png" width="180"/> will measure the accumulated volume density between tn and some <span class="No-Break" lang="">point t.</span></li>
<li lang="en-GB"><img alt="" height="88" src="image/Formula_06_005.png" width="390"/> will provide us with a notion of accumulated transmittance along the ray from tn to some point t. The higher the accumulated volume density, the lower the accumulated transmittance to the <span class="No-Break" lang="">point t.</span></li>
<li lang="en-GB">The expected color of the ray can now be defined <span class="No-Break" lang="">as follows:</span></li>
</ul>
<p lang="en-GB"><img alt="" height="153" src="image/Formula_06_006.png" width="720"/></p>
<p class="callout-heading" lang="en-GB">Important note</p>
<p class="callout" lang="en-GB">The volume density σ(r(t)) is a function of the point r(t). Most notably, this does not depend on the direction vector d. This is because volume density is a function of the physical location at which the point is located. The color <img alt="" height="47" src="image/Formula_06_007.png" width="165"/> is a function of both the point r(t) and the ray direction d. This is because the same point in space can have different colors when viewed from <span class="No-Break" lang="">different directions.</span></p>
<p lang="en-GB">Our NeRF model is<a id="_idIndexMarker315"/> a continuous function representing the radiance field of the scene. We can use it to obtain c(r(t), d) and σ(r(t)) at various points along the ray. There are many techniques for numerically estimating the integral C(r). While training and visualizing the outputs of the NeRF model, we used the standard <strong class="source-inline" lang="">EmissionAbsorptionRaymarcher</strong> method to accumulate the radiance along <span class="No-Break" lang="">a ray.</span></p>
<h1 id="_idParaDest-89" lang="en-GB"><a id="_idTextAnchor092"/>Summary</h1>
<p lang="en-GB">In this chapter, we came to understand how a neural network can be used to model and represent a 3D scene. This neural network is called the NeRF model. We then trained a simple NeRF model on a synthetic 3D scene. We then dug deeper into the NeRF model architecture and its implementation in code. We also understood the main components of the model. We then understood the principles behind rendering volumes with the NeRF model. The NeRF model is used to capture a single scene. Once we build this model, we can use it to render that 3D scene from different angles. It is logical to wonder whether there is a way to capture multiple scenes with a single model and whether we can predictably manipulate certain objects and attributes in the scene. This is our topic of exploration in the next chapter where we will explore the <span class="No-Break" lang="">GIRAFFE model.</span></p>
</div>
</div>

<div id="sbo-rt-content"><div>
<div class="Basic-Graphics-Frame" id="_idContainer108">
</div>
</div>
<div class="Content" id="_idContainer109">
<h1 id="_idParaDest-90" lang="en-GB"><a id="_idTextAnchor093"/>PART 3: State-of-the-art 3D Deep Learning Using PyTorch3D</h1>
<p lang="en-GB">This part of the book will be all about using PyTorch3D to implement state-of-the-art 3D deep learning models and algorithms. 3D computer vision technologies are making rapid progress in recent times and we will learn how to implement and use these state-of-the-art 3D deep learning models in the best <span class="No-Break" lang="">way possible.</span></p>
<p lang="en-GB">This part includes the <span class="No-Break" lang="">following chapter:</span></p>
<ul>
<li lang="en-GB"><a href="B18217_07.xhtml#_idTextAnchor094"><em class="italic" lang="">Chapter 7</em></a>, <em class="italic" lang="">Exploring Controllable Neural Feature Fields </em></li>
<li lang="en-GB"><a href="B18217_08.xhtml#_idTextAnchor108"><em class="italic" lang="">Chapter 8</em></a><em class="italic" lang="">, Modeling the Human Body in 3D</em></li>
<li lang="en-GB"><a href="B18217_09.xhtml#_idTextAnchor124"><em class="italic" lang="">Chapter 9</em></a><em class="italic" lang="">, Performing End-to-End View Synthesis with SynSin </em></li>
<li lang="en-GB"><a href="B18217_10.xhtml#_idTextAnchor134"><em class="italic" lang="">Chapter 10</em></a><em class="italic" lang="">, Mesh R-CNN</em></li>
</ul>
</div>
<div>
<div id="_idContainer110">
</div>
</div>
</div></body></html>