- en: '*Chapter 6*: Training Models with Visual Data'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has been successfully applied to many different types of data,
    including tabular data, text data, and recommender system data. You saw fastai's
    approach to these types of data in [*Chapter 3*](B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083),
    *Training Models with Tabular Data*, [*Chapter 4*](B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109),
    *Training Models with Text Data*, and [*Chapter 5*](B16216_05_Final_VK_ePub.xhtml#_idTextAnchor134),
    *Training Recommender Systems*. These types of data are all part of the story
    of deep learning, but **visual data** or **image data** is the type of data that
    is traditionally associated most closely with deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Visual data is also the type of data that is most thoroughly supported by the
    fastai framework. The fastai high-level API is mostly developed for visual data,
    and 70% of the curated fastai datasets are visual datasets. In this chapter, we
    will explore some of the features that fastai provides for exploring visual datasets
    and building high-performance deep learning models with image datasets.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will learn how to use the rich set of facilities available
    in fastai for preparing image datasets and using them to train deep learning models.
    In particular, you will learn how to create fastai deep learning models that classify
    images, that is, determine what objects are in images, and also how to use fastai
    to identify multiple objects in the same image.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are the recipes that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Training a classification model with a simple curated vision dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring a curated image location dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a classification model with a standalone vision dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a multi-image classification model with a curated vision dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Test your knowledge
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Ensure that you have completed the setup sections from [*Chapter 1*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019),
    *Getting Started with fastai*, and have a working Gradient instance or Colab set
    up. The recipes described in this chapter assume that you are using Gradient.
    Ensure that you have cloned the repo for this book from [https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook](https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook)
    and have access to the `ch6` folder. This folder contains the code samples described
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Training a classification model with a simple curated vision dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You may recall the first fastai model that you trained back in [*Chapter 1*](B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019),
    *Getting Started with fastai*. That model was trained on the MNIST dataset of
    hand-written digits. Given an image of a hand-written digit, that model was able
    to classify the image, that is, determine which of the digits from 0 to 9 were
    shown in the image.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, you are going to apply the same approach you saw in the MNIST
    model to another fastai curated dataset: the CIFAR dataset. This dataset, which
    is a subset of a larger curated CIFAR_100 dataset, is made up of 6,000 images
    organized into 10 categories. The model that you train in this section will be
    able to determine the category that an image from this dataset belongs to.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Confirm that you can open the `training_with_curated_image_datasets.ipynb` notebook
    in the `ch6` directory of your repo.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The images in the `CIFAR` dataset are quite small. In this section, we have
    rendered them in a larger size to make them easier to recognize in the context
    of the book, but the outcome is that they can look a bit blurry.
  prefs: []
  type: TYPE_NORMAL
- en: The `CIFAR` dataset featured in this section is introduced in the paper *Learning
    Multiple Layers of Features from Tiny Image*, Krizhevsky, 2009\. I am grateful
    for the opportunity to include this dataset in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: 'Alex Krizhevsky. (2009). *Learning Multiple Layers of Features from Tiny Image*. University
    of Toronto: [https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf](https://www.cs.toronto.edu/~kriz/learning-features-2009-TR.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will be running through the `training_with_curated_image_datasets.ipynb`
    notebook. Once you have the notebook open in your fastai environment, complete
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the notebook up to the `Ingest the dataset` cell to import
    the required libraries and set up your notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to define the `path` object for this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to examine the directory structure of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows the directory structure of the dataset, as shown in the following
    screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Output of path.ls()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_1.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.1 – Output of path.ls()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define an `ImageDataLoaders` object for this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the `ImageDataLoaders` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `path`: Specifies that the `ImageDataLoaders` object is defined using the
    `path` object you created in the previous cell'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `train=''train''`: Specifies that the training data is in the `/storage/data/cifar10/train`
    directory'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `valid=''test''`: Specifies that the validation data is in the `/storage/data/cifar10/test`
    directory'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to display a batch from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is a set of `4` items from a batch, showing images
    along with their corresponding categories, as shown in the following figures:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Output of show_batch()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_2.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.2 – Output of show_batch()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to examine the contents of the `train` subdirectory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows the structure of the `train` subdirectory, as shown in the
    following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Contents of the train subdirectory'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_3.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.3 – Contents of the train subdirectory
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to examine the contents of the `train/dog` subdirectory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows the structure of the `train/dog` subdirectory:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Contents of the train/dog subdirectory'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_4.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.4 – Contents of the train/dog subdirectory
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'If you want to take a different perspective as regards the directory structure
    of this dataset, you can use the `tree` command. To do this, from the Gradient
    terminal, enter the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: ├── test
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: │   ├── airplane
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: │   ├── automobile
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: │   ├── bird
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: │   ├── cat
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: │   ├── deer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: │   ├── dog
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: │   ├── frog
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: │   ├── horse
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: │   ├── ship
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: │   └── truck
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: └── train
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ├── airplane
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ├── automobile
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ├── bird
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ├── cat
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ├── deer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ├── dog
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ├── frog
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ├── horse
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ├── ship
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: └── truck
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to view a single item in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key elements of this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `img_files = get_image_files(path)`: Specifies that `path` is recursively
    examined and returns all the image files in the path. If you want more details
    about `get_image_files`, you can check out the documentation at the following
    link: [https://docs.fast.ai/data.transforms.html#get_image_files](https://docs.fast.ai/data.transforms.html#get_image_files).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `img = PILImage.create(img_files[100])`: Creates the image object, `img`,
    from a specific file returned by the previous statement.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell is one of the dataset files rendered as an image in
    the notebook, as shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.5 – An image from the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_5.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.5 – An image from the dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to display another image from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is another of the dataset files rendered as an image in the notebook
    as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Another image from the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_6.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.6 – Another image from the dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define the model as a `cnn_learner` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the `cnn_learner` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `dls`: Specifies that the model uses the `ImageDataLoaders` object, `dls`,
    that you defined in *Step 4*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `resnet18`: Specifies the pre-trained model to use as a starting point for
    this model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `loss_func=LabelSmoothingCrossEntropy()`: Specifies the loss function to
    use in the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `metrics=accuracy`: Specifies that `accuracy` is the performance metric
    that will be optimized in the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The argument indicates that the training run will be for `5` epochs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output displays the training loss, validation loss, and accuracy for each
    epoch, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.7 – Output of training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_7.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.7 – Output of training the model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s try out the trained model on some examples from the test dataset. First,
    run the following cell to define an object for one of the images in the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key elements of this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `img_files = get_image_files(path/"test"))`: Returns all the image files
    under the `test` directory'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `img = PILImage.create(img_files[700])`: Creates the image object, `img2`,
    from a specific file returned by the previous statement'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell is an image of a dog:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.8 – Dog image from the test dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_8.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.8 – Dog image from the test dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define an object for another one of the images in
    the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is an image of a bird:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.9 – Bird image from the test dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_9.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.9 – Bird image from the test dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have defined objects for a couple of images from the test dataset,
    let''s exercise the trained image classification model on them. First, run the
    following cell to apply the model to the dog image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is the model''s prediction, as shown in the following
    screenshot. Note that the model correctly predicts the category of the image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.10 – Image classification model''s prediction on the dog image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.10 – Image classification model's prediction on the dog image
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now see how the model does with the image of a bird. Run the following
    cell to apply the model to the bird image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is the model''s prediction, as shown in the following
    screenshot. Note that the model correctly predicts that the image is a bird:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.11 – Image classification model''s prediction on the bird image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.11 – Image classification model's prediction on the bird image
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have successfully exercised the model, let''s save it. Run the
    following cell to save the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is what the statements in this cell do:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `learn.path = Path(''/notebooks/temp'')`: Sets the path of the `learn` object
    to a directory that can be written to. Remember that in Gradient, by default,
    the path for the `learn` object is read-only, so you need to adjust the path to
    a writeable directory before you can save the model.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `learn.export(''cifar_apr20_2021.pkl'')`: Specifies that the name of the
    model to be saved is `cifar_apr20_2021.pkl`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After you have run this cell in Gradient, your model is saved in `/notebooks/temp/model/cifar_apr20_2021.pkl`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Congratulations! You have trained and exercised a fastai image classification
    model using a curated image dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Some important things are going on in the recipe in this section that are worth
    reviewing. In this section, we'll go over some of the details that might not have
    been evident in the main part of the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Labels are encoded in directory names and filenames
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, consider the names of the files that make up the dataset. The files
    in the `train/dog` subdirectory are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.12 – Files in the train/dog subdirectory'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.12 – Files in the train/dog subdirectory
  prefs: []
  type: TYPE_NORMAL
- en: 'The files in the `train/cat` subdirectory are shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.13 – Files in the train/cat subdirectory'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.13 – Files in the train/cat subdirectory
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at a sample file from each of these subdirectories. To display
    an image file from the `train/dog` subdirectory, run the following cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this cell is indeed an image of a dog, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.14 – A dog image from the training dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.14 – A dog image from the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'To display an image file from the `train/cat` subdirectory, run the following
    cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this cell is, as expected, an image of a cat, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.15 – A cat image from the training dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.15 – A cat image from the training dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'For this image classification problem, the model is trying to predict the label
    for an image, that is, the category, such as dog, deer, or bird, of the object
    that is shown in the image. For this dataset, the label is encoded in the following
    two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The name of the directory containing the image file**. As you can see in
    *Figure 6.14* and *Figure 6.15*, the label for an image is encoded in the name
    of the subdirectory where the image is located.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`xxxx_dog.png`, and the cat images have filenames of the form `xxxx_cat.png`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You used transfer learning to train the image classification model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In [*Chapter 4*](B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109), *Training
    Models with Text Data*, we used transfer learning to adapt an existing trained
    model to work with a dataset for a particular use case. You may not have noticed
    it, but we did the same thing in this recipe. There''s a clue in the cell where
    you defined the `cnn_learner` object as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'In the definition of `cnn_learner`, the `resnet18` argument is the pre-trained
    model that is used as a basis for the image classification model. The first time
    you run this cell, you will see a message like the one shown in *Figure 6.16*,
    indicating that the model is being set up in your environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.16 – Message you get the first time you run the cnn_learner definition'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.16 – Message you get the first time you run the cnn_learner definition
  prefs: []
  type: TYPE_NORMAL
- en: 'The second clue is in the cell where you train the model, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: For most of the models you have seen so far in this book, you trained the model
    using `learn.fit_one_cycle()`. Here, because we want to update an existing model
    for our particular use case, we use `learn.fine_tune()` instead. This is exactly
    what we did in [*Chapter 4*](B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109),
    *Training Models with Text Data*, for the language models we trained using transfer
    learning on top of the pre-trained `AWD_LSTM` model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Why did we use transfer learning for this use case instead of training the
    model from scratch? The simple answer is that we get decent performance from the
    model faster by using transfer learning. You can try it out yourself by making
    two changes to the `training_with_curated_image_datasets.ipynb` notebook and rerunning
    it. Here are the steps to follow to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Update the definition of the `cnn_learner` object to include the `pretrained=False`
    argument, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This change means that the model will be trained from scratch with the `CIFAR`
    dataset rather than by taking advantage of the pre-trained model.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Change the training statement to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: When you run the notebook after making these changes, this new model will not
    do a very good job of classifying images. Transfer learning works, and as Howard
    and Gugger explain in their book, it doesn't get nearly enough attention in standard
    introductions to deep learning. It's lucky for us that fastai is designed to make
    it easy to exploit the power of transfer learning, as shown by the image classification
    model that you trained in the *How to do it…* section of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: A closer look at the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Before moving on to the next recipe, it's worth taking a closer look at the
    model from this recipe. A deeply detailed description of the model is beyond the
    scope of this book, so we will just focus on some highlights here.
  prefs: []
  type: TYPE_NORMAL
- en: 'As shown in the recipe, the model is defined as a `cnn_learner` object (documentation
    here: [https://docs.fast.ai/vision.learner.html#cnn_learner](https://docs.fast.ai/vision.learner.html#cnn_learner))
    that uses the pre-trained `resnet18` model (documentation here: [https://pytorch.org/vision/stable/models.html](https://pytorch.org/vision/stable/models.html)).
    The output of `learn.summary()` for this model shows that the model includes a
    series of convolutional layers and associated layers. For a description of convolutional
    neural networks (CNNs), see: [https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/](https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/).'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring a curated image location dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Back in [*Chapter 2*](B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057), *Exploring
    and Cleaning Up Data with fastai*, we went through the process of ingesting and
    exploring a variety of datasets using fastai.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we are going to explore a special curated image dataset called
    `COCO_TINY`. This is an `CIFAR` dataset that we used in the *Training a classification
    model with a simple curated vision dataset* recipe, which had a single labeled
    object in each image, the images in image location datasets are labeled with bounding
    boxes (which indicate where in the image a particular object occurs) as well as
    the name of the object. Furthermore, images in the `COCO_TINY` dataset can contain
    multiple labeled objects, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.17 – Labeled image from an image location dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_17.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.17 – Labeled image from an image location dataset
  prefs: []
  type: TYPE_NORMAL
- en: In the recipe in this section, we'll ingest the dataset and apply its annotation
    information to create a `dataloaders` object for the dataset. The annotation information
    for each image can be quite complex. Each image can depict multiple labeled objects.
    For each labeled object in an image, the annotation information includes the filename
    of the image, the *x* and *y* coordinates of the bounding box that surrounds the
    object, and the label for the category of the object. For example, the image shown
    in *Figure 6.17* contains several **couch** objects and a **chair** object.
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that the images in the dataset don''t actually show the annotation. A
    typical image from the dataset looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.18 – Raw image from the COCO_TINY dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_18.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.18 – Raw image from the COCO_TINY dataset
  prefs: []
  type: TYPE_NORMAL
- en: There are no bounding boxes or labels in the image itself. That annotation information
    is contained in a separate file. The recipe in this section will show you how
    to combine the image files with the annotation information.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of this recipe, when we use `show_batch()` to display samples from
    the batch that incorporates annotation, the bounding boxes and labels are shown
    in the images. For example, *Figure 6.19* shows how the image from *Figure 6.18*
    looks when it is displayed by `show_batch()` – now you can see the bounding box
    and label for the **vase** object in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.19 – Annotated version of the image in Figure 6.18'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.19 – Annotated version of the image in Figure 6.18
  prefs: []
  type: TYPE_NORMAL
- en: In the recipe in this section, you will combine the image files from the `COCO_TINY`
    dataset, such as the image shown in *Figure 6.18*, with the information from the
    annotation file to get a `dataloaders` object that includes the images along with
    bounding boxes and labels for all the labeled objects in each image, as shown
    in *Figure 6.19*.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Confirm that you can open the `exploring_image_location_datasets.ipynb` notebook
    in the `ch6` directory of your repository.
  prefs: []
  type: TYPE_NORMAL
- en: The approach to examining the dataset shown in this recipe includes approaches
    inspired by this Kaggle kernel, https://www.kaggle.com/jachen36/coco-tiny-test-prediction,
    which demonstrates how to use fastai to explore an image location dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `COCO_TINY` dataset featured in this section is introduced in the paper
    *Microsoft COCO: Common Objects in Context*, Lin et al., 2014\. I am grateful
    for the opportunity to include an example using this dataset in this book.'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: 'Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick,
    James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár.
    (2014). *Microsoft COCO: Common Objects in Context*: [https://arxiv.org/abs/1405.0312](https://arxiv.org/abs/1405.0312).'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will be running through the `exploring_image_location_datasets.ipynb`
    notebook. Once you have the notebook open in your fastai environment, complete
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the notebook up to the `Ingest the dataset` cell to import
    the required libraries and set up your notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to define the `path` object for this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to examine the directory structure of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows the directory structure of the dataset, as shown in *Figure
    6.20*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.20 – Output of path.ls()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.20 – Output of path.ls()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The dataset consists of a set of image files in the `train` subdirectory and
    the `train.json` annotation file. In the next few steps of this recipe, we will
    take a closer look at what's in the `train.json` file.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to bring the `train.json` file into a series of Python
    dictionaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key parts of the code used in this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `data = json.load(json_file)`: Loads the contents of the whole `train.json`
    file into the `data` dictionary.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `categories = data[''categories'']`: Creates a separate list of dictionaries
    just for the category definitions. This dictionary defines the objects in the
    images.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `images = data[''images'']`: Creates a separate list of dictionaries just
    for image files.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `annotations = data[''annotations'']`: Creates a separate list of dictionaries
    just for the bounding boxes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to see the structure of each of the dictionaries you
    created in the previous cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell shows samples of the contents of each of the three
    dictionaries, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.21 – Contents of annotation dictionaries'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.21 – Contents of annotation dictionaries
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The dictionaries that we created in the previous step aren''t quite what we
    need to get complete annotations for the images. What we want is a consolidated
    annotation for each image that lists the bounding boxes for each object in the
    image along with the object''s category. We could do this manually by manipulating
    the dictionaries we created in the previous step, but fastai provides a very handy
    function called `get_annotations` that does the work for us. Run the following
    cell to define the annotation structures:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key parts of the code in this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `aget_annotations(path/''train.json'')`: Applies the `get_annotations` function
    to the `train.json` file to get an annotation structure. The output of this function
    is a list of filenames and a list of labeled bounding boxes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `dict(zip(image_files, bbox_lbl))`: Creates a dictionary that combines the
    file list and the labeled bounding box list output from the previous command.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to examine one of the elements of the annotation dictionary,
    `img_bbox_combo`, that you created in the previous cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell, as shown in the following screenshot, shows that elements
    of the dictionary are tuples made up of a list of bounding boxes (sets of 2 *x*
    and *y* coordinates that define the top-left and bottom-right points of the box
    around the object) and a list of corresponding object categories:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.22 – An element of img_bbox_combo'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.22 – An element of img_bbox_combo
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to see the image associated with the annotation you
    examined in the previous cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is the following image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.23 – Example image'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.23 – Example image
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define a function to return the bounding box associated
    with the input image file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to define a function to return the label (that is, the
    category) associated with the input image file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to define a function to return the image files in the
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to define a `DataBlock` object for the dataset using
    the functions that you defined in the previous three cells:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For almost all of the recipes so far in this book, we have used the top-level
    fastai API, which means we would be defining some kind of `dataloaders` object
    at this step. For this dataset, however, we need more flexibility than we can
    get from a `dataloaders` object, which is why we have defined a `DataBlock` object
    here. For details on `DataBlock` objects, refer to the following documentation:
    https://docs.fast.ai/data.block.html#DataBlock.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Here are the arguments for the definition of the `DataBlock` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `blocks=(ImageBlock, BBoxBlock, BBoxLblBlock)`: Specifies that the input
    to the model is images (`ImageBlock`) and the target has two parts: bounding boxes
    for object in the images (`BBoxBlock`), and labels (categories) associated with
    each of the bounding boxes (`BBoxLblBlock`).'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `get_items=get_image_files`: Specifies that the `get_image_files` function
    is called to get the input to the `DataBlock` object.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `get_y=[get_bbox, get_lbl]`: Specifies the functions that are applied to
    the results of `get_items`. The image filenames are sent as arguments to each
    of these functions. The first function, `get_bbox`, returns the list of bounding
    boxes associated with the image file according to the annotation information we
    ingested from the `train.json` file. The second function, `get_lbl`, returns the
    list of labels (categories) for the bounding boxes associated with the image file,
    according to the annotation information we ingested from the `train.json` file.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `n_inp=1`: Specifies the number of elements in the tuples specified in the
    `blocks` argument that should be considered part of the input, in our case, just
    the image files.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define a `dataloaders` object using the `DataBlock`
    object, `db`, you created in the previous cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the `dataloaders` definition:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `path`: Specifies that the path to use for the `dataloaders` object is the
    `path` object you defined for the dataset at the beginning of the notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `bs=32`: Specifies that the batch size is 32'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now it is finally time to see what batches for the `dataloaders` object look
    like. Run the following cell to see a sample of entries from a batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for `show_batch()`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `max_n=4`: Specifies the number of samples to show'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `figsize=(10,10)`: Specifies the dimensions of the samples'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of `show_batch()` is shown in the following image. Note that you
    see the images with the labeled objects highlighted with bounding boxes and annotation
    text. Also note that you may see different sample images when you run `show_batch()`
    in your notebook:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.24 – Sample image output by show_batch()'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.24 – Sample image output by show_batch()
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have successfully ingested and prepared an image location
    dataset, one of the most complex data manipulation tasks that you will tackle
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: After working through the recipe in this section, you may have some questions
    about the `COCO_TINY` dataset and the image location dataset in general. In this
    section, we will go through some of the questions that may have come up as you
    went through the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: What kinds of objects are annotated in the images in the dataset?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If you rerun the following cell several times, you will see a variety of annotated
    images:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'You can see an example of the output of this cell in *Figure 6.25*. Note that
    the annotations don''t cover every possible object in the images. For example,
    the animal in the first image isn''t annotated. Which objects are annotated for
    images in `COCO_TINY`? We''ll answer that question in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.25 – Output of show_batch()'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.25 – Output of show_batch()
  prefs: []
  type: TYPE_NORMAL
- en: 'When you ran the following cell to bring the annotation file, `train.json`,
    into a Python dictionary, you created a sub-dictionary called `categories`, which
    contains all the categories of objects annotated in the `COCO_TINY` dataset as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'To see what''s in the `categories` dictionary, run the following cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this cell is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.26 – The categories dictionary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.26 – The categories dictionary
  prefs: []
  type: TYPE_NORMAL
- en: 'From this dictionary, you can see that there are only six objects annotated
    for the images in the `COCO_TINY` dataset: **chair**, **couch**, **tv**, **remote**,
    **book**, and **vase**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you run `show_batch()` repeatedly, you will see that even objects that are
    only partially shown in an image are annotated. For example, the first image in
    *Figure 6.25* has an annotation for a chair, as shown in the following image,
    even though only part of the legs of the chair is shown in the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.27 – Close-up of the chair annotation'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.27 – Close-up of the chair annotation
  prefs: []
  type: TYPE_NORMAL
- en: 'You will also see that some images contain many annotated objects, and the
    bounding boxes for these objects can overlap, as shown in the following image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.28 – An image with multiple, overlapping bounding boxes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.28 – An image with multiple, overlapping bounding boxes
  prefs: []
  type: TYPE_NORMAL
- en: This subsection describes which objects are annotated in the images in `COCO_TINY`.
    As you rerun the `show_batch()` command repeatedly, you will see for yourself
    how complex the annotations can be for some objects.
  prefs: []
  type: TYPE_NORMAL
- en: How are the bounding boxes for annotated objects defined?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'While you were working through the recipe in this section, you may have asked
    yourself where the bounding boxes were defined. When you ran the following cell
    to bring the annotation file, `train.json`, into a Python dictionary, you created
    a sub-dictionary called `annotations` that contains all the annotations of objects
    annotated in the `COCO_TINY` dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'You can run the following cell to see the contents of a subset of the `annotations`
    dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of this cell shows examples of annotations for objects in a particular
    image, as shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.29 – Example of annotations'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_29.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.29 – Example of annotations
  prefs: []
  type: TYPE_NORMAL
- en: The values for the `bbox` keys in this dictionary define the bounding boxes
    for objects by specifying the *x* and *y* values at the extremes of the objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the following image. This image has a single annotated object – let''s
    see how to view the coordinates of its bounding box:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.30 – Example annotated image with a bounding box'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_30.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.30 – Example annotated image with a bounding box
  prefs: []
  type: TYPE_NORMAL
- en: 'The filename for this image is `000000071159.jpg`. To see the bounding box
    for this image, run the following cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'The output for this cell shows the bounding box coordinates from this image.
    If the image had multiple objects annotated in it, there would be a bounding box
    defined for each object:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.31 – Bounding box for the object in the example image in Figure
    6.30'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_31.png)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.31 – Bounding box for the object in the example image in Figure 6.30
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we have reviewed some of the details of the bounding box annotations
    in the `COCO_TINY` dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Why didn't we train a model using COCO_TINY?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unlike most of the recipes in this book, the recipe in this section did not
    include model training. After the effort we put into creating a `dataloaders`
    object for the `COCO_TINY` dataset, why didn't we go all the way and train a model
    with it?
  prefs: []
  type: TYPE_NORMAL
- en: 'The simple answer is that the fastai framework does not currently incorporate
    a simple way to train a model on an image location dataset such as `COCO_TINY`.
    If you want to attempt to train such a model, you can check out this repo for
    an approach, but be prepared to get into details well beyond the high-level fastai
    API that we have been exploring in this book so far: [https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0/blob/master/Computer%20Vision/06_Object_Detection.ipynb](https://github.com/muellerzr/Practical-Deep-Learning-for-Coders-2.0/blob/master/Computer%20Vision/06_Object_Detection.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Training a classification model with a standalone vision dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Training a classification model with a simple curated vision dataset*
    recipe, you went through the steps to ingest a fastai curated dataset and use
    it to train an image classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you will go through the same process for a standalone dataset
    called `fruits-360`. This dataset (described in more detail here: https://www.kaggle.com/moltean/fruits)
    contains over 90,000 images of fruits and vegetables organized into over 130 categories.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we'll begin by bringing this dataset into Gradient. Then we
    will work through the `training_with_standalone_image_datasets.ipynb` notebook
    to ingest the dataset and use it to train a fastai image classification model.
    Finally, we will see how well the trained model classifies images from the test
    set and save the trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensure that you have uploaded the `fruits-360` dataset to your Gradient environment
    by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Download `archive.zip` from [https://www.kaggle.com/moltean/fruits](https://www.kaggle.com/moltean/fruits).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Upload `archive.zip` to your Gradient environment. You can use the upload button
    in JupyterLab in Gradient to do the upload, but you need to do it in several steps
    as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'a) From the terminal in your Gradient environment, make `/notebooks` your current
    directory:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: temp your current folder, select the upload button (see *Figure 6.32*), and
    select archive.zip from your local system folder where you downloaded it in *Step
    1*:![Figure 6.32 – Upload button in JupyterLab
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_32.jpg)Figure 6.32 – Upload button in JupyterLab'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that you have uploaded `archive.zip` to the `/notebooks/temp` directory
    in your Gradient environment, make that directory your current directory by running
    the following command in the Gradient terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Unzip `archive.zip` into the `/storage/archive` directory by running the following
    command in the Gradient terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Confirm that you now have the dataset set up in your Gradient environment by
    running the following command from within the Gradient terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, run the following command to list the contents of the directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this command should look like what is shown in the following
    screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.33 – Contents of the fruits-360 directory'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_33.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.33 – Contents of the fruits-360 directory
  prefs: []
  type: TYPE_NORMAL
- en: With these preparation steps, you have moved the files that make up the `fruits-360`
    dataset to the correct location in your Gradient environment to be used by a fastai
    model.
  prefs: []
  type: TYPE_NORMAL
- en: The `fruits-360` dataset featured in this section is introduced in the Kaggle
    competition **Fruits 360** from 2020 ([https://www.kaggle.com/moltean/fruits](https://www.kaggle.com/moltean/fruits)).
    I am grateful for the opportunity to include an example using this dataset in
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: Mihai Oltean (2017-2020). *Fruits 360 – A dataset with 90,380 images of 131
    fruits and vegetables* ([https://mihaioltean.github.io](https://mihaioltean.github.io)).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will be running through the `training_with_standalone_image_datasets.ipynb`
    notebook. Once you have the notebook open in your fastai environment, complete
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the notebook up to the `Ingest the dataset` cell to import
    the required libraries and set up your notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to define the `path` object for this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to examine the directory structure of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows the directory structure of the dataset, as shown in the following
    screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.34 – Output of path.ls()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.34 – Output of path.ls()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define an `ImageDataLoaders` object for this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the `ImageDataLoaders` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `path`: Specifies that the `ImageDataLoaders` object is defined using the
    `path` object you created in the previous cell'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `train=''Training''`: Specifies that the training data is in the `/storage/archive/fruits-360/Training`
    directory'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `valid=''Test''`: Specifies that the validation data is in the `/storage/archive/fruits-360/Test`
    directory'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to display a batch from the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is a set of `4` items from a batch, showing images
    along with their corresponding categories, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.35 – Output of show_batch()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.35 – Output of show_batch()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to examine the contents of the `Training` subdirectory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows the structure of the `Training` subdirectory, as shown in
    the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B16216_6_36.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: Figure 6.36 – Contents of the Training subdirectory
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to see a single item in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key elements of this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `img_files = get_image_files(path)`: Specifies that `path` is recursively
    examined and returns all the image files in the path'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `img = PILImage.create(img_files[100])`: Creates the image object, `img`,
    from a specific file returned by the previous statement'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell is one of the dataset files rendered as an image in
    the notebook:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.37 – An image from the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_37.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.37 – An image from the dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define the model as a `cnn_learner` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments for the definition of the `cnn_learner` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `dls`: Specifies that the `cnn_learner` object is defined using the `ImageDataLoaders`
    object you defined earlier in this notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `resnet18`: Specifies the pre-trained model to use as a starting point for
    this model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `loss_func=LabelSmoothingCrossEntropy()`: Specifies the loss function to
    use in the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `metrics=accuracy`: Specifies that `accuracy` is the performance metric
    that will be optimized in the training process'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The argument indicates that the training run will be for `5` epochs.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output displays the training loss, validation loss, and accuracy for each
    epoch, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.38 – Output of training the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_38.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.38 – Output of training the model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s try out the trained model on some examples from the test dataset. First,
    run the following cell to define an object for one of the images in the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key elements of this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `img_files = get_image_files(path/"Test"))`: Returns all the image files
    under the `Test` directory'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `img = PILImage.create(img_files[700])`: Creates the image object, `img2`,
    from a specific file returned by the previous statement'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell is an image of a strawberry, as shown in the following
    screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.39 – An image of a strawberry from the test dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_39.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.39 – An image of a strawberry from the test dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define an object for another one of the images in
    the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is an image of a tomato, as shown in the following
    screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.40 – An image of a tomato from the test dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_40.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.40 – An image of a tomato from the test dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have defined objects for a couple of images from the test dataset,
    let''s exercise the trained image classification model on them. First, run the
    following cell to apply the model to the strawberry image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is the model''s prediction, as shown in the following
    screenshot. Note that the model correctly predicts the category of the image.
    The numbers shown in the `TensorImage` array correspond to the likelihood that
    the trained model ascribes to the image being in each of the categories:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.41 – Image classification model''s prediction on the image of the
    strawberry'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_41.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.41 – Image classification model's prediction on the image of the strawberry
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s see how the model does on the tomato image. Run the following cell to
    apply the model to the image of the tomato:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is the model''s prediction, as shown in the following
    screenshot. Note that the model correctly predicts that the image is a tomato:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.42 – Image classification model''s prediction on the image of a
    bird'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_42.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.42 – Image classification model's prediction on the image of a bird
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The trained model seems to be doing a good job of predicting the category of
    images from the test set, but there is still some ambiguity. The fruit and vegetable
    images can be ambiguous to a human, so let''s see how the trained model predicts
    the category of images where we know exactly what the category is. First, run
    the following cell to define an image of an avocado from the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key elements of this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `avocado_files = get_image_files(path/"Test/Avocado"))`: Returns all the
    image files under the `Test/Avocado` directory'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `avocado_img = PILImage.create(avocado_files[30])`: Creates the image object,
    `avocado_img`, from a specific avocado image file from the file set returned by
    the previous statement'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell is an image of an avocado, as shown in the following
    screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.43 – Image of an avocado from the test dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_43.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.43 – Image of an avocado from the test dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s get another image from a different directory in the test dataset. Run
    the following cell to define an image of a walnut from the test dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key elements of this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `walnut_files = get_image_files(path/"Test/Walnut")`: Returns all the image
    files under the `Test/Walnut` directory'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `walnut_img = PILImage.create(walnut_files[30])`: Creates the image object,
    `walnut_img`, from a specific walnut image file from the file set returned by
    the previous statement'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell is an image of a walnut, as shown in the following
    screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.44 – Image of a walnut from the test dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_44.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.44 – Image of a walnut from the test dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have defined objects for a couple of images from specific directories
    in the test dataset, let''s exercise the trained image classification model on
    them. First, run the following cell to apply the model to the avocado image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is the model''s prediction for the `avocado_img` image,
    as shown in the following screenshot. Note that the model correctly predicts the
    category of the image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.45 – Image classification model''s prediction on the image of the
    avocado'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_45.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.45 – Image classification model's prediction on the image of the avocado
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s see how the model does on the walnut image. Run the following cell to
    apply the model to the image of the walnut:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell is the model''s prediction, as shown in the following
    screenshot. Note that the model correctly predicts that the image is a walnut:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.46 – Image classification model''s prediction on the image of the
    walnut'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_46.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.46 – Image classification model's prediction on the image of the walnut
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have exercised the model to confirm that it makes good predictions
    for a small set of images, run the following cell to save the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell confirms that the model was saved in the `models` subdirectory
    of the dataset path, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.47 – Output of saving the model'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_47.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.47 – Output of saving the model
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have trained a fastai image classification model on a large
    standalone image dataset and exercised the trained model with a set of examples
    from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you compare the *Training a classification model with a simple curated vision
    dataset* recipe with the recipe in this section, you will notice that the code
    is very similar. In fact, once you have brought the `fruits-360` dataset into
    your Gradient environment, the code in the `training_with_standalone_image_datasets.ipynb`
    notebook is very close to the code in the `training_with_curated_image_datasets.ipynb`
    notebook. There are, however, some differences, as shown in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The path definition statement is different. For the curated dataset, the definition
    of the path statement has a `URLs` object as its argument:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Whereas the path statement for the standalone dataset has the `fruits-360`
    directory as its argument:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The test and train directories have different names between the two datasets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The standalone dataset is much bigger than the curated dataset, both in terms
    of the volume of images and the number of categories that the images are organized
    into.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Out of the box, the trained model for the standalone dataset shows higher accuracy
    (over 95%) than the curated dataset (~80%).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Despite these differences, it is remarkable that fastai can train a model to
    categorize images in the `fruits-360` dataset so easily. As you saw in the recipe
    in this section, you were able to train the model on the `fruits-360` dataset
    and get great performance, with just a few lines of code. I think the model you
    created in this recipe is a great example of the power and flexibility of fastai.
  prefs: []
  type: TYPE_NORMAL
- en: Training a multi-image classification model with a curated vision dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the *Training a classification model with a simple curated vision dataset*
    recipe, you went through the steps to ingest a fastai curated dataset and used
    it to train an image classification model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, you will go through the same process for another curated dataset
    called `PASCAL_2007`. This dataset (described in more detail here: http://host.robots.ox.ac.uk/pascal/VOC/)
    contains about 5,000 training images and the same number of test images. The dataset
    includes annotations that identify common objects that appear in each image. The
    identified objects are from 20 categories, including animals (cow, dog, cat, sheep,
    and horse), vehicles (boat, bus, train, airplane, bicycle, and car), and other
    items (person, sofa, bottle, and TV monitor).'
  prefs: []
  type: TYPE_NORMAL
- en: The images in the `CIFAR` dataset introduced in the *Training a classification
    model with a simple curated vision dataset* recipe had a single labeled object.
    By contrast, the images in `PASCAL_2007` can have zero, one, or more labeled objects.
    As you will see in this section, there are some challenges with training a model
    to predict multiple objects in the same image.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will ingest the `PASCAL_2007` dataset, including image annotations,
    explore the dataset, train a fastai model to categorize the images according to
    what objects are depicted in the images, and then exercise the trained model on
    some examples from the test dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Confirm that you can open the `training_with_curated_multi_image_classification_datasets.ipynb`
    notebook in the `ch6` directory of your repo.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `PASCAL_2007` dataset featured in this section is introduced in this paper
    – *The PASCAL Visual Object Classes (VOC) Challenge*: http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf.
    I am grateful for the opportunity to include an example using this dataset in
    this book.'
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citation
  prefs: []
  type: TYPE_NORMAL
- en: Mark Everingham, Luc Van Gool, Christopher K. I. Williams, John Winn, and Andrew
    Zisserman (2008). *The PASCAL Visual Object Classes (VOC) Challenge* (http://host.robots.ox.ac.uk/pascal/VOC/pubs/everingham10.pdf).
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, you will be running through the `training_with_curated_multi_image_classification_datasets.ipynb`
    notebook. Once you have the notebook open in your fastai environment, complete
    the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Run the cells in the notebook up to the `Ingest the dataset` cell to import
    the required libraries and set up your notebook.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the following cell to define the `path` object for this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to examine the directory structure of the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows the directory structure of the dataset, as shown in the following
    screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.48 – Output of path.ls()'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_48.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.48 – Output of path.ls()
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now review the key items in the directory structure of the `PASCAL_2007`
    dataset that we will be using in this recipe:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `/storage/data/pascal_2007/train`: A directory containing images for training
    the model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `/storage/data/pascal_2007/test`: A directory containing images for testing
    the trained model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `/storage/data/pascal_2007/train.json`: A file containing annotations for
    the training images'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `/storage/data/pascal_2007/test.json`: A file containing annotations for
    test images'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to bring the annotations in the `train.json` file into
    Python objects:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key parts of the code used in this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `data = json.load(json_file)`: Loads the contents of the whole `train.json`
    file into the `data` dictionary.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `categories = data[''categories'']`: Creates a separate list of dictionaries
    just for the category definitions. This dictionary defines the objects in the
    images in the dataset.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `images = data[''images'']`: Creates a separate list of dictionaries just
    for image files.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `annotations = data[''annotations'']`: Creates a separate list of dictionaries
    just for the annotations that specify the objects in the images and their bounding
    boxes.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you will see in later steps in this recipe, we will not be using the `categories`,
    `images`, and `annotations` lists of dictionaries to extract the annotations to
    feed into the fastai model training process. Instead, we will be using the fastai
    built-in API, `get_annotations`, to work directly with the annotations. The dictionaries
    we define in this cell will, however, be useful to help us to understand the details
    regarding the annotations.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to see some examples of annotations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell lists entries from each of the lists of dictionaries
    that you created in the previous cell. As shown in the following screenshot, the
    `categories` list of dictionaries contains the categories of objects that can
    appear in the images along with their IDs:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.49 – Entries from the categories list of dictionaries'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_49.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.49 – Entries from the categories list of dictionaries
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following screenshot shows that the `images` list of dictionaries lists
    the filenames of the image files in the training set along with their IDs and
    dimensions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.50 – Entries from the images list of dictionaries'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_50.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.50 – Entries from the images list of dictionaries
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The following screenshot shows that `annotations` lists the bounding boxes
    and category IDs of the objects that appear in the image with the given `image_id`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.51 – Entries from the annotations list of dictionaries'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_51.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.51 – Entries from the annotations list of dictionaries
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will use the fastai function, `get_annotations`, to get all the annotation
    information that we need for a given image, that is, the categories for the objects
    that are depicted in the image. Run the following cell to define the required
    annotation structures:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key parts of the code used in this cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `get_annotations(path/''train.json'')`: Applies the `get_annotations` function
    to the `train.json` file to get an annotation structure. The output of this function
    is a list of filenames and a list containing bounding boxes and object categories
    for each object in the image. The following screenshot shows an example of the
    contents of `bbox_lbl`, specifying the bounding boxes and categories for three
    objects:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.52 – Example bounding box and categories'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_52.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.52 – Example bounding box and categories
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `dict(zip(image_files, bbox_lbl))`: Creates a dictionary that combines the
    file list and the labeled bounding box list output from the previous command.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s now take a look at one of the image files from the training set along
    with its annotations. First, run the following cell to see the annotations for
    a specific image file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows the annotations associated with this image file:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.53 – Annotations for an image file'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_53.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.53 – Annotations for an image file
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to take a look at the image whose annotations we examined
    in the previous cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows the image associated with `image_files[5]`. As you can see
    in the following image, this image does indeed contain three planes, as indicated
    in the annotations you saw in the previous cell:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.54 – Image whose annotation indicates it contains three planes'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_54.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.54 – Image whose annotation indicates it contains three planes
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define the `get_category` function to get values
    out of the list of dictionaries you created earlier. We will use this function
    later to examine the annotations of images in the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the following cell to define the `get_lbl` function that takes a filename
    as input and returns the list of category names from the annotation structure
    for that file:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key parts of this function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `os.path.basename(filename)`: Returns the final part of the fully qualified
    `filename`. For example, if `filename` is the fully qualified name, `/storage/data/pascal_2007/train/006635.jpg`,
    `os.path.basename` returns `006635.jpg`. This conversion is required because the
    input image files will include fully qualified paths, but the `img_bbox_combo`
    structure is indexed with just the final part of the filename.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `img_bbox_combo[os.path.basename(filename)][1]`: Returns the categories
    associated with the `filename` image file.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to see an example of how `get_lbl` works:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output, as shown in the following screenshot, is a NumPy array containing
    the categories associated with the `/storage/data/pascal_2007/train/006635.jpg`
    image file:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.55 – Sample output of get_lbl'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_55.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.55 – Sample output of get_lbl
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It looks like a lot is going on in the image we used as an argument to `get_lbl`
    in the previous cell. Run the following cell to take a look at the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As shown in the following image, this image matches the annotation we saw in
    the previous cell. It does indeed contain a motorbike along with a bunch of people:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.56 – Image with a motorbike and several people'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_56.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.56 – Image with a motorbike and several people
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to examine the number of files in the training and test
    sets for this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output, shown in in the following screenshot, shows the number of files
    in each set:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.57 – Count of files in the training and test sets'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_57.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.57 – Count of files in the training and test sets
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cells to see the number of categories in the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output, shown in the following screenshot, shows the number of categories
    in the dataset:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.58 – Number of categories in the dataset'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_58.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.58 – Number of categories in the dataset
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define the `get_items` function that will be used
    in the `DataBlock` definition to get the input data files:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key items in this function''s definition:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `return_list = []`: Initializes the list of image files in the training
    set that have annotations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `empty_list = []`: Initializes the list of image files in the training set
    that do not have annotations.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `for file_path in get_image_files(path/''train'')`: Iterates through the
    files in the training set.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `file_id_list`: Is the list of file IDs in the annotation corresponding
    with the current `file_path`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'e) `if len(file_id_list) > 0`: Checks to see whether `file_id_list` has any
    entries. If it does, the function appends the current `file_path` to `return_list`.
    Otherwise, the function appends the current `file_path` to `empty_list`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'f) `return(return_list)`: This only returns the subset of image files that
    have annotations associated with them. If you include any of the image files that
    have no annotations associated with them, you will get an error in the step where
    you define a `dataloaders` object.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define a `DataBlock` object for the dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the key items in the definition of the `DataBlock` object:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `blocks=(ImageBlock, MultiCategoryBlock)`: Specifies the type of the input
    data (image files) and that the dataset has multi-category labels, that is, each
    image can have multiple annotated objects in it.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `get_items = get_items`: Specifies the function to apply to get the input
    items; in this case, the `get_items` function we defined in the previous cell
    that returns all the files in the training set that have annotations associated
    with them.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'c) `splitter=RandomSplitter()`: Tells fastai to create a validation set from
    randomly selected items from the training set, by default using 20% of the items
    in the training set.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'd) `get_y=[get_lbl]`: Specifies the function to get labels for the input, in
    this case, the `get_lbl` function. This function takes a filename as input and
    returns the list of categories from the annotations for that file.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'e) `item_tfms = RandomResizedCrop(168, min_scale=0.3)`: Specifies a transformation
    to apply during training. Because the image files in the training set are different
    sizes, we need to transform them all to a common size or we will get errors in
    `show_batch`. This transformation resizes the images by cropping them so they
    are all a common size.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'f) `n_inp=1`: Specifies which of the elements defined in the `blocks` clause
    of the definition should be considered inputs, in this case, `1` or just `ImageBlock`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define a `dataloaders` object using the `DataBlock`
    object, `db`, that you created in the previous cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments to the `dataloaders` definition:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `path`: Specifies that the source for the `dataloaders` object is the `path`
    object you created earlier in the notebook'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `bs=32`: Specifies that the batch size is `32`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The output of this cell, as shown in the following screenshot, shows the count
    of elements in `return_list` (the list of image files with valid annotations)
    and `empty_list` (the list of image files without valid annotations):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.59 – Output of the dataloader definition'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_59.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.59 – Output of the dataloader definition
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to display a set of samples from a batch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows a selection of images from the training set along with the
    categories for the objects in the images that are described in the annotation
    corresponding to the image files, as shown in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.60 – Results of show_batch'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_60.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.60 – Results of show_batch
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to define the model by specifying a `cnn_learner` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here are the arguments to the `cnn_learner` definition:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'a) `dls`: Specifies that the model is trained using the `dataloaders` object
    you defined in the previous cell'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'b) `resnet18`: Specifies that the model is based on the pre-trained `resnet18`
    mode.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to train the model for `10` epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output lists the training and validation loss for each epoch, as shown
    in the following screenshot. The performance of the model improves as the validation
    loss decreases:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.61 – Output of training the multi-category image classification
    model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_61.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.61 – Output of training the multi-category image classification model
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that we have trained the model, let''s exercise it on some images from
    the test set. To start with, run the following cell to prepare and display one
    of the images from the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This cell displays the following image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.62 – Image from the test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_62.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.62 – Image from the test set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to apply the trained model to the image displayed in
    the previous cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the output, as shown in the following screenshot, you can see that the
    model gets the right category for one of the objects in the image, but is wrong
    about the second object in the image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.63 – Model prediction on the image of the horse'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_63.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.63 – Model prediction on the image of the horse
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to prepare and display another image from the test set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This cell displays the following image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.64 – Image from the test set'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_64.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.64 – Image from the test set
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the following cell to apply the trained model to the image displayed in
    the previous cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'From the output, as shown in the following screenshot, you can see that the
    model gets the correct category for the object in the image:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.65 – Model prediction on the image of the cat'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_65.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.65 – Model prediction on the image of the cat
  prefs: []
  type: TYPE_NORMAL
- en: Congratulations! You have trained a fastai model that categorizes multiple objects
    in images.
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The recipe in this section is one of the longest, most complex recipes in this
    book. The following are some aspects of the recipe that may not have been evident
    to you as you worked through the steps.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe uses transfer learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you look at the code from *Step 19* and *Step 20* of the recipe, you will
    see a pattern similar to the model definition and training steps from the recipes
    in the *Training a classification model with a simple curated vision dataset*
    recipe and the *Training a classification model with a standalone vision dataset*
    recipe.
  prefs: []
  type: TYPE_NORMAL
- en: In all of these recipes, you specified a pre-trained model in the model definition
    and then trained the model using a `fine_tune` statement. Taking advantage of
    the pre-trained vision classification model for this recipe makes a lot of sense
    because we had a fairly small training set, as you will see in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: This recipe has a small training set for a deep learning model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'How big was the training set for this recipe? The initial training set is a
    little over 5,000 images, as you can see in the count of the training set from
    *Step 13* of the recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.66 – Count of items in the training and test sets'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_66.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.66 – Count of items in the training and test sets
  prefs: []
  type: TYPE_NORMAL
- en: 'However, in *Step 15*, where we defined the `get_items` function, we had to
    filter the list of training images to only include those that had valid category
    annotations. As you can see from the output of *Step 17*, where we define the
    `dataloaders` object and invoke the `get_items` function, less than half of the
    input training images have valid annotations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.67 – Count of items in return_list and empty_list'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_67.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.67 – Count of items in return_list and empty_list
  prefs: []
  type: TYPE_NORMAL
- en: What does this mean? It means that we have 2,500 images to train our model.
    Having only 2,500 images to train a complex model has consequences, as we will
    see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: The trained model from this recipe does not have great performance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model we trained in the recipe in this section does not have outstanding
    performance. It usually identifies one object in an image correctly, but it often
    doesn't identify more than one object, and when it does identify a second or third
    object, it often assigns these objects to an incorrect category. Why is the performance
    of this model not so good?
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we used only 2,500 images to train a complex model to categorize
    multiple objects in images into 20 categories. Compare this to the `fruits-360`
    dataset used in the *Training a classification model with a standalone vision
    dataset* recipe. The `fruits-360` dataset has over 90,000 images. To see how many
    files there are in the training set, run the following command in your Gradient
    environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: 'The following screenshot shows the output of this command:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.68 – Count of images in the fruits-360 training set'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16216_6_68.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 6.68 – Count of images in the fruits-360 training set
  prefs: []
  type: TYPE_NORMAL
- en: There are over 67,500 files in the training set for the `fruits-360` dataset.
    This means that for the first curated image dataset problem we saw in this chapter,
    we had 25 times as many training samples as we had for the recipe in this section,
    and in the first instance, we applied the model to the simpler problem of identifying
    a single object in each image. The relative lack of training images in the `PASCAL_2007`
    dataset could partially explain the mediocre performance of the model we trained
    with this dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Test your knowledge
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Of the four application areas that fastai explicitly supports (tabular, text,
    recommender systems, and image/vision), fastai provides the most thorough support
    for creating models that work with image datasets. In this chapter, we have just
    scratched the surface of what you can do with fastai and image datasets. In this
    section, you will get a chance to dig a bit deeper into one of the fastai image
    dataset recipes from this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensure that you have followed the *Training a multi-image classification model
    with a curated vision dataset* recipe. In this section, you will be adapting the
    notebook you worked through in that recipe to try some new variations on deep
    learning with image datasets.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'You can follow the steps in this section to try some variations on the image
    classification model that you trained with the `PASCAL_2007` dataset in the *Training
    a multi-image classification model with a curated vision dataset* recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make a copy of the `training_with_curated_multi_image_classification_datasets.ipynb`
    notebook that you worked through in the *Training a multi-image classification
    model with a curated vision dataset* recipe. Give your new copy of the notebook
    the following name: `training_with_curated_multi_image_classification_datasets_variations.ipynb`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Run the notebook up to and including the model definition cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Add the following cell immediately after this cell and run it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output of this cell lists the structure of the model. Note the summary
    of trainable and non-trainable parameters at the end of the output, as shown in
    the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.69 – Trainable parameter description at the end of the summary output
    before fine-tuning'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_69.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.69 – Trainable parameter description at the end of the summary output
    before fine-tuning
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Update the model training cell to train for 20 epochs and then run it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of this cell, as shown in the following screenshot, shows how the
    training loss and validation loss develop through the epochs. Note how the training
    loss decreases steadily right up to the 20th epoch, while the validation loss
    decreases up to the 10th epoch, after which it goes up and down. What problem
    is evident in a situation like this, where the training loss keeps dropping but
    the validation loss stops dropping?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.70 – Results of a 20-epoch training run'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_70.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.70 – Results of a 20-epoch training run
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The previous step shows us that training for more epochs won't improve the model's
    performance. You can validate this for yourself by running the rest of the notebook
    and comparing how well the model trained with 20 epochs predicts the objects in
    the selected test images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Add the following immediately after the model training cell and run it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output of this cell lists the structure of the model. Note the summary of
    trainable and non-trainable parameters at the end of the output, as shown in the
    following screenshot. Compare this output to the output from `summary()` prior
    to fine-tuning the model, as shown in *Step 3*. What has changed to explain the
    difference in the output of `summary()` between *Step 3* and this step?
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 6.71 – Trainable parameter description at the end of the summary output
    after fine-tuning'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B16216_6_71.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Figure 6.71 – Trainable parameter description at the end of the summary output
    after fine-tuning
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'In the *How it works…* section of the previous recipe, we looked at the size
    of the training set for `PASCAL_2007` and suggested that its small size could
    be a reason for the unimpressive performance of the model. One other potential
    culprit is the way that we are resizing the images in the definition of the `DataBlock`
    object, shown in the following cell:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The transformation that resizes the images is `RandomResizedCrop(168, min_scale=0.3)`.
    Update the `DataBlock` definition cell to try different image transformations.
    First, update the `RandomResizedCrop` function call to try a different image size
    and a different `min_scale` value. Train and exercise the model again on samples
    from the test set to see whether the performance changes for different image sizes
    and `min_scale` values.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Try using `Resize` rather than `RandomResizedCrop` as the transformation function
    (as shown in the following cell) and train the model again. See whether you get
    better results when you exercise the trained model on sample images from the training
    set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Congratulations! You have completed a review of training the fastai model on
    image datasets.
  prefs: []
  type: TYPE_NORMAL
