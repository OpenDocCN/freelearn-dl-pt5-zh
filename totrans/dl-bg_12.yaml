- en: Restricted Boltzmann Machines
  prefs: []
  type: TYPE_NORMAL
- en: Together, we have seen the power of unsupervised learning and hopefully convinced
    ourselves that it can be applied to different problems. We will finish the topic
    of unsupervised learning with an exciting approach known as **Restricted Boltzmann
    Machines** (**RBMs**). When we do not care about having a large number of layers,
    we can use RBMs to learn from the data and find ways to satisfy an energy function
    that will produce a model that is robust at representing input data.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter complements [Chapter 8](6677b8b1-806c-4c39-8c1e-371e83501acf.xhtml),
    *Deep Autoencoders*, by introducing the backward-forward nature of RBMs, while
    contrasting it with the forward-only nature of **Autoencoders** (**AEs**). This
    chapter compares RBMs and AEs in the problem of dimensionality reduction, using
    MNIST as the case study. Once you are finished with this chapter, you should be
    able to use an RBM using scikit-learn and implement a solution using a Bernoulli
    RBM. You will be able to perform a visual comparison of the latent spaces of an
    RBM and an AE, and also visualize the learned weights for an inspection of the
    inner workings of an RBM and an AE.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is organized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to RBMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning data representations with RBMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing RBMs and AEs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to RBMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RBMs are unsupervised models that can be used in different applications that
    require rich latent representations. They are usually used in a pipeline with
    a classification model with the purpose of extracting features from the data.
    They are based on **Boltzmann Machines** (**BMs**), which we discuss next (Hinton,
    G. E., and Sejnowski, T. J. (1983)).
  prefs: []
  type: TYPE_NORMAL
- en: BMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A BM can be thought of as an undirected dense graph, as depicted in *Figure
    10.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1dd88078-d42e-4544-8491-77f715f7696f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – A BM model
  prefs: []
  type: TYPE_NORMAL
- en: 'This undirected graph has some neural units that are modeled to be **visible**,
    ![](img/bc0fc9ef-6923-4dad-905f-d7c40116b3d7.png), and a set of neural units that
    are **hidden**, ![](img/ec3962a8-a124-46ce-b17f-ea74d72f1067.png). Of course,
    there could be many more than these. But the point of this model is that all neurons
    are connected to each other: they all *talk* among themselves. The training of
    this model will not be covered here, but essentially it is an iterative process
    where the input is presented in the visible layers, and every neuron (one at a
    time) adjusts its connections with other neurons to satisfy a loss function (usually
    based on an energy function), and the process repeats until the learning process
    is considered to be satisfactory.'
  prefs: []
  type: TYPE_NORMAL
- en: While the RB model was quite interesting and powerful, it took a very long time
    to train! Take into consideration that this was around in the early 1980s and
    performing computations on larger graphs than this and with larger datasets could
    have a significant impact on the training time. However, in 1983, G. E. Hinton
    and his collaborators proposed a simplification of the BM model by restricting
    the communication between neurons, as we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: RBMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The *restriction* of traditional BMs lies in the communication between neurons;
    that is, visible neurons can only talk to hidden neurons and hidden neurons can
    only talk to visible neurons, as depicted in *Figure 10.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/40d2dc56-c956-418d-a481-ea4ff53f5d42.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – An RBM model. Compare to the BM model in Figure 10.1
  prefs: []
  type: TYPE_NORMAL
- en: 'The graph shown in *Figure 10.2* is known as a **dense bipartite graph**. Perhaps
    you are thinking that it looks a lot like the typical dense neural networks that
    we have been using so far; however, it is not quite the same. The main difference
    is that all the neural networks that we have used only communicate information
    going forward from input (visible) to hidden layers, while an RBM can go both
    ways! The rest of the elements are familiar: we have weights and biases that need
    to be learned.'
  prefs: []
  type: TYPE_NORMAL
- en: If we stick to the simple model shown in *Figure 10.2*, we could explain the
    learning theory behind an RBM in simpler terms.
  prefs: []
  type: TYPE_NORMAL
- en: Let's interpret every single neural unit as a random variable whose current
    state depends on the state of other neural units.
  prefs: []
  type: TYPE_NORMAL
- en: This interpretation allows us to use sampling techniques related to **Markov
    Chain Monte Carlo** (**MCMC**) (Brooks, S., et al. (2011)); however, we will not
    go into the details of these in this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this interpretation, we can define an energy function for the model as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7eafb87d-5b01-4b10-ab01-98aad9a12ae3.png)'
  prefs: []
  type: TYPE_IMG
- en: 'where ![](img/da9561fe-3f4a-4973-9c06-1c311d126aae.png) denote the biases on
    a visible neural unit and a hidden neural unit, respectively. It turns out that
    we can also express the joint probability density function of neural an hidden
    units as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f43952f8-d909-4104-8a8e-c173c9c19d4b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'which has a simple marginal distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46d00194-e21e-4dd7-849b-273079cd4c45.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The denominator in the conditional and marginal is known as a normalizing factor
    that has only the effect of ensuring that probability values add up to one, and
    can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c442f2fe-1dfa-4915-8db1-90a8e5b4da7e.png)'
  prefs: []
  type: TYPE_IMG
- en: These formulations allow us to quickly find MCMC techniques for training; most
    notably, you will find in the literature that Contrastive Divergence involving
    Gibbs sampling is the most common approach (Tieleman, T. (2008)).
  prefs: []
  type: TYPE_NORMAL
- en: There are only a handful of RBMs implemented that are readily available for
    a learner to get started; one of them is a Bernoulli RBM that is available in
    scikit-learn, which we discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Bernoulli RBMs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: While the generalized RBM model does not make any assumptions about the data
    that it uses, a Bernoulli RBM does make the assumption that the input data represents
    values in the range [0,1] that can be interpreted as probability values. In the
    ideal case, values are in the set {0,1}, which is closely related to Bernoulli
    trials. If you are interested, there are other approaches that assume that the
    inputs follow a Gaussian distribution. You can find out more by reading Yamashita,
    T. et al. (2014).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are only a few datasets that we can use for this type of RBM; MNIST is
    an example that can be interpreted as binary inputs where the data is 0 when there
    are no digit traces and 1 where there is digit information. In scikit-learn, the
    `BernoulliRBM` model can be found in the neural network collection: `sklearn.neural_network`.'
  prefs: []
  type: TYPE_NORMAL
- en: Under the assumption of Bernoulli-like input distribution, this RBM model *approximately*
    optimizes the log likelihood using a method called **Persistent Contrastive Divergence**
    (**PCD**) (Tieleman, T., and Hinton, G. (2009)). It turned out that PCD was much
    faster than any other algorithm at the time and fostered discussions and much
    excitement that was soon overshadowed by the popularization of **backpropagation**
    compared to dense networks.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will implement a Bernoulli RBM on MNIST with the purpose
    of learning the representations of the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Learning data representations with RBMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that you know the basic idea behind RBMs, we will use the `BernoulliRBM`
    model to learn data representations in an unsupervised manner. As before, we will
    do this with the MNIST dataset to facilitate comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: For some people, the task of **learning representations** can be thought of
    as **feature engineering**. The latter has an explicability component to the term,
    while the former does not necessarily require us to prescribe meaning to the learned
    representations.
  prefs: []
  type: TYPE_NORMAL
- en: 'In scikit-learn, we can create an instance of the RBM by invoking the following
    instructions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The default parameters in the constructor of the RBM are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_components=256`, which is the number of hidden units, ![](img/73826389-c0a2-4f77-972f-ed916b23f0a9.png),
    while the number of visible units, ![](img/89773bbc-964d-486b-b725-57c2ef2aa47e.png),
    is inferred from the dimensionality of the input.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate=0.1` controls the strength of the learning algorithm with respect
    to updates, and it is recommended to explore it with values in the set {*1, 0.1,
    0.01, 0.001*}.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size=10` controls how many samples are used in the batch-learning algorithm.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`n_iter=10` controls the number of iterations that are run before we stop the
    learning algorithm. The nature of the algorithm allows it to keep going as much
    as we want; however, the algorithm usually finds good solutions in a few iterations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will only change the default number of components to make it 100\. Since
    the original number of dimensions in the MNIST dataset is 784 (because it consists
    of 28 x 28 images), having 100 dimensions does not seem like a bad idea.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the RBM with 100 components over MNIST training data loaded into `x_train`,
    we can do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The output during training might look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can look into the representations learned by invoking the `transform()`
    method on the MNIST test data, `x_test`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, there are 784 input dimensions, but the `r` variable will have
    100 dimensions. To visualize the test set in the latent space induced by the RBM,
    we can use UMAPs as we did before, which will produce the two-dimensional plot
    shown in *Figure 10.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/17976fd6-6978-4fe1-aa16-31bfe3ae1cce.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – UMAP representation of the learned representations by the RBM
    on MNIST test data
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code to produce this plot from the RBM feature space using UMAP is
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Compare *Figure 10.3* with the representations shown in previous chapters. From
    the figure, we can appreciate that there are clear class separations and clustering,
    while at the same time there are slight overlaps between classes. For example,
    there is some overlap between the numerals 3 and 8, which is to be expected since
    these numbers look alike. This plot also shows that the RBM generalizes very well
    since the data in *Figure 10.3* is coming from data that is unseen by the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can further inspect the weights (or *components*) learned by the RBM; that
    is, we can retrieve the weights associated with the visible layers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the `v` variable will be a 784 x 100 matrix describing the learned
    weights. We can visualize every single one of the neurons and reconstruct the
    weights associated with those neurons, which will look like the components in
    *Figure 10.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ac6b554-5cef-4031-b2af-73bc85f5a4b9.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Learned weights of the RBM
  prefs: []
  type: TYPE_NORMAL
- en: A close examination of *Figure 10.4* informs us that there are weights that
    pay attention to diagonal features, or circular features, or features that are
    very specific to specific digits and edges in general. The bottom row, for example,
    has features that appear to be associated with the numbers 2 and 6.
  prefs: []
  type: TYPE_NORMAL
- en: The weights shown in *Figure 10.4* can be used to transform the input space
    into richer representations that can later be used for classification in a pipeline
    that allows for this task.
  prefs: []
  type: TYPE_NORMAL
- en: 'To satisfy our learning curiosity, we could also look into the RBM and its
    states by sampling the network using the `gibbs()` method. This means that we
    could visualize what happens when we present the input to the visible layer and
    then what the response is from the hidden layer, and then use that as input again
    and repeat to see how the stimulus of the model changes. For example, run the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This will effectively produce a plot like the one shown in *Figure 5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8144dac0-0e70-4382-9d00-a6452b5336c7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – Gibbs sampling on an MNIST-based RBM
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10.5* shows the input in the first column, and the remaining 10 columns
    are successive sampling calls. Clearly, as the input is propagated back and forth
    within the RBM, it suffers from some slight deformations. Take row number five,
    corresponding to the digit 4; we can see how the input is being deformed until
    it looks like a number 2\. This information has no immediate effect on the learned
    features unless a strong deformation is observed at the first sampling call.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will use an AE to make a comparison with an RBM.
  prefs: []
  type: TYPE_NORMAL
- en: Comparing RBMs and AEs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now that we have seen how RBMs perform, a comparison with AEs is in order.
    To make this comparison fair, we can propose the closest configuration to an RBM
    that an AE can have; that is, we will have the same number of hidden units (neurons
    in the encoder layer) and the same number of neurons in the visible layer (the
    decoder layer), as shown in *Figure 10.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8fefc75-8a3e-40ae-8e91-2ab36d6fea70.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.6 – AE configuration that's comparable to RBM
  prefs: []
  type: TYPE_NORMAL
- en: 'We can model and train our AE using the tools we covered in [Chapter 7](480521d9-845c-4c0a-b82b-be5f15da0171.xhtml),
    *Autoencoders*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'There is nothing new here, except that we are training with only two dense
    layers that are large enough to provide nice representations. *Figure 10.7* depicts
    the UMAP visualization of the learned representations on the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/01853091-de4e-420d-a897-43d3f4b7e6b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.7 – AE-induced representations using a UMAP visualization
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding figure is produced with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: From *Figure 10.7*, you can see that the data is nicely clustered; although
    the clusters are closer together than in *Figure 10.3*, the within-cluster separations
    seems to be better. Similarly to the RBM, we can visualize the weights that were
    learned.
  prefs: []
  type: TYPE_NORMAL
- en: 'Every `Model` object in `tensorflow.keras` has a method called `get_weights()`
    that can retrieve a list with all the weights at every layer. Let''s run this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'It gives us access to the weights of the first layer and allows us to visualize
    them in the same way we did for the RBM weights. *Figure 10.8* shows the learned
    weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af57fcd9-06c3-4286-959e-699e42a5904a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.8 – AE weights
  prefs: []
  type: TYPE_NORMAL
- en: The weights shown in *Figure 10.8*, in comparison with those of the RBM in *Figure
    10.4*, have no noticeable digit-specific features. These features seem to be oriented
    to textures and edges in very unique regions. This is very interesting to see
    because it suggests that fundamentally different models will produce fundamentally
    different latent spaces.
  prefs: []
  type: TYPE_NORMAL
- en: If both RBMs and AEs produce interesting latent spaces, imagine what we could
    achieve if we use both of them in our deep learning projects! Try it!
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to prove that the AE achieves high-quality reconstructions as modeled,
    we can look at *Figure 10.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/37f7f0ac-e0ba-498e-80ac-6064f79c94a2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.9 – AE inputs (top row) and reconstructions (bottom row)
  prefs: []
  type: TYPE_NORMAL
- en: The reconstructions using 100 components seem to have high quality, as shown
    in *Figure 10.9*. This is, however, not possible for RBMs since their purpose
    is not to reconstruct data, necessarily, as we have explained in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This intermediate-level chapter has shown you the basic theory behind how RBMs
    work and their applications. We paid special attention to a Bernoulli RBM that
    operates on input data that may follow a Bernoulli-like distribution in order
    to achieve fast learning and efficient computations. We used the MNIST dataset
    to showcase how interesting the learned representations are for an RBM, and we
    visualized the learned weights as well. We concluded by comparing the RBM with
    a very simple AE and showed that both learned high-quality latent spaces while
    being fundamentally different models.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should be able to implement your own RBM model, visualize
    its learned components, and see the learned latent space by projecting (transforming)
    the input data and looking at the hidden layer projections. You should feel confident
    in using an RBM on large datasets, such as MNIST, and even perform a comparison
    with an AE.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is the beginning of a new group of chapters about supervised
    deep learning. [Chapter 11](03e9a734-fb56-485d-ae90-66fb98ecd4d1.xhtml), *Deep
    and Wide Neural Networks*, will get us started in a series of exciting and new
    topics surrounding supervised deep learning. The chapter will explain the differences
    in performance and the complexities of deep versus wide neural networks in supervised
    settings. It will introduce the concept of dense networks and sparse networks
    in terms of the connections between neurons. You can't miss it!
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Why can''t we perform data reconstructions with an RBM?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: RBMs are fundamentally different to AEs. An RBM aims to optimize an energy function,
    while an AE aims to optimize a data reconstruction function. Thus, we can't do
    reconstructions with RBMs. However, this fundamental difference allows for new
    latent spaces that are interesting and robust.
  prefs: []
  type: TYPE_NORMAL
- en: '**Can we add more layers to an RBM?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No. Not in the current model presented here. The concept of stacked layers of
    neurons fits the concept of deep AEs better.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is so cool about RBMs then?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They are simple. They are fast. They provide rich latent spaces. They have no
    equal at this point. The closest competitors are AEs.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Hinton, G. E., and Sejnowski, T. J. (1983, June). Optimal perceptual inference.
    In Proceedings of the *IEEE conference on Computer Vision and Pattern Recognition*
    (Vol. 448). IEEE New York.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brooks, S., Gelman, A., Jones, G., and Meng, X. L. (Eds.). (2011). *Handbook
    of Markov Chain Monte Carlo*. CRC press.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tieleman, T. (2008, July). Training restricted Boltzmann machines using approximations
    to the likelihood gradient. In Proceedings of the 25th *International Conference
    on Machine Learning* (pp. 1064-1071).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Yamashita, T., Tanaka, M., Yoshida, E., Yamauchi, Y., and Fujiyoshii, H. (2014,
    August). To be Bernoulli or to be Gaussian, for a restricted Boltzmann machine.
    In 2014 22nd *International Conference on Pattern Recognition* (pp. 1520-1525).
    IEEE.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tieleman, T., and Hinton, G. (2009, June). Using fast weights to improve persistent
    contrastive divergence. In Proceedings of the 26th Annual *International Conference
    on Machine Learning* (pp. 1033-1040).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
