["```py\npip install nltk\n```", "```py\n$ sudo pip install -U nltk\n```", "```py\nraw_txt = \"\"\"Welcome to the world of Deep Learning for NLP! \\\n             We're in this together, and we'll learn together. \\\n             NLP is amazing, \\\n             and Deep Learning makes it even more fun. \\\n             Let's learn!\"\"\"\n```", "```py\nimport nltk\nnltk.download('punkt')\nfrom nltk import tokenize\n```", "```py\ntokenize.sent_tokenize(raw_txt)\n```", "```py\n['Welcome to the world of Deep Learning for NLP!',\n \"We're in this together, and we'll learn together.\",\n 'NLP is amazing, and Deep Learning makes it even more fun.',\n \"Let's learn!\"]\n```", "```py\ntxt_sents = tokenize.sent_tokenize(raw_txt)\ntype(txt_sents), len(txt_sents)\n```", "```py\n(list, 4)\n```", "```py\ntxt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]\ntype(txt_words), type(txt_words[0])\n```", "```py\n(list, list)\n```", "```py\nprint(txt_words[:2])\n```", "```py\n[['Welcome', 'to', 'the', 'world', 'of', \n  'Deep', 'Learning', 'for', 'NLP', '!'], \n  ['We', \"'re\", 'in', 'this', 'together', \n  ',', 'and', 'we', \"'ll\", 'learn', 'together', '.']]\n```", "```py\ntxt_sents  = [sent.lower() for sent in txt_sents]\ntxt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]\n```", "```py\nprint(txt_words[:2])\n```", "```py\n[['welcome', 'to', 'the', 'world', 'of', \n  'deep', 'learning', 'for', 'nlp', '!'], \n ['we', \"'re\", 'in', 'this', 'together', \n  ',', 'and', 'we', \"'ll\", 'learn', 'together', '.']]\n```", "```py\nfrom string import punctuation\nlist_punct = list(punctuation)\nprint(list_punct)\n```", "```py\n['!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', \n '-', '.', '/', ':', ';', '<', '=', '>', '?', '@', \n '[', '\\\\', ']', '^', '_', '`', '{', '|', '}', '~']\n```", "```py\ndef drop_punct(input_tokens):\n    return [token for token in input_tokens \\\n            if token not in list_punct]\n```", "```py\ndrop_punct([\"let\",\".\",\"us\",\".\",\"go\",\"!\"])\n```", "```py\n['let', 'us', 'go']\n```", "```py\ntxt_words_nopunct = [drop_punct(sent) for sent in txt_words]\nprint(txt_words_nopunct)\n```", "```py\n[['welcome', 'to', 'the', 'world', 'of', \n  'deep', 'learning', 'for', 'nlp'], \n ['we', \"'re\", 'in', 'this', 'together', 'and', \n  'we', \"'ll\", 'learn', 'together'], \n ['nlp', 'is', 'amazing', 'and', \n  'deep', 'learning', 'makes', 'it', 'even', 'more', 'fun'], \n ['let', \"'s\", 'learn']]\n```", "```py\nimport nltk\nnltk.download(\"stopwords\")\nfrom nltk.corpus import stopwords\nlist_stop = stopwords.words(\"english\")\nlen(list_stop)\n```", "```py\n179\n```", "```py\nprint(list_stop[:50])\n```", "```py\n['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', \n 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', \n 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', \n 'himself', 'she', \"she's\", 'her', 'hers', 'herself', \n 'it', \"it's\", 'its', 'itself', 'they', 'them', \n 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', \n 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', \n 'was', 'were', 'be']\n```", "```py\n    import nltk\n    from nltk import tokenize\n    ```", "```py\n    raw_txt = \"\"\"Welcome to the world of deep learning for NLP! \\\n                 We're in this together, and we'll learn together. \\\n                 NLP is amazing, \\\n                 and deep learning makes it even more fun. \\\n                 Let's learn!\"\"\"\n    ```", "```py\n    txt_sents = tokenize.sent_tokenize(raw_txt.lower())\n    ```", "```py\n    txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]\n    ```", "```py\n    from string import punctuation\n    stop_punct = list(punctuation)\n    ```", "```py\n    from nltk.corpus import stopwords\n    stop_nltk = stopwords.words(\"english\")\n    ```", "```py\n    stop_final = stop_punct + stop_nltk\n    ```", "```py\n    def drop_stop(input_tokens):\n        return [token for token in input_tokens \\\n                if token not in stop_final]\n    ```", "```py\n    txt_words_nostop = [drop_stop(sent) for sent in txt_words]\n    ```", "```py\n    print(txt_words_nostop[0])\n    ```", "```py\n    ['welcome', 'world', 'deep', 'learning', 'nlp']\n    ```", "```py\nfrom nltk.stem import PorterStemmer\nstemmer_p = PorterStemmer()\n```", "```py\nprint(stemmer_p.stem(\"driving\"))\n```", "```py\ndrive\n```", "```py\ntxt = \"I mustered all my drive, drove to the driving school!\"\n```", "```py\ntokens = tokenize.word_tokenize(txt)\nprint([stemmer_p.stem(word) for word in tokens])\n```", "```py\n['I', 'muster', 'all', 'my', 'drive', ',', 'drove', 'to', \n 'the', 'drive', 'school', '!']\n```", "```py\nnltk.download('wordnet')\nfrom nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\n```", "```py\nlemmatizer.lemmatize(\"ponies\")\n```", "```py\n'pony'\n```", "```py\n[['welcome', 'world', 'deep', 'learning', 'nlp'],\n [\"'re\", 'together', \"'ll\", 'learn', 'together'],\n ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'],\n ['let', \"'s\", 'learn']]\n```", "```py\n    from nltk.stem import PorterStemmer\n    ```", "```py\n    stemmer_p = PorterStemmer()\n    ```", "```py\n    print([stemmer_p.stem(token) for token in txt_words_nostop[0]])\n    ```", "```py\n    ['welcome has been changed to welcom and learning to learn. This is consistent with the rules of the Porter stemming algorithm.\n    ```", "```py\n    txt_words_stem = [[stemmer_p.stem(token) for token in sent] \\\n                       for sent in txt_words_nostop]\n    ```", "```py\n    txt_words_stem\n    ```", "```py\n    [['welcom', 'world', 'deep', 'learn', 'nlp'],\n     [\"'re\", 'togeth', \"'ll\", 'learn', 'togeth'],\n     ['nlp', 'amaz', 'deep', 'learn', 'make', 'even', 'fun'],\n     ['let', \"'s\", 'learn']]\n    ```", "```py\nimport nltk\n```", "```py\nnltk.download()\n```", "```py\nalice_raw = nltk.corpus.gutenberg.raw('carroll-alice.txt')\n```", "```py\nalice_raw[:800]\n```", "```py\n\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n  \\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning\n  to get very tired of sitting by her sister on the\\nbank, \n  and of having nothing to do: once or twice she had peeped \n  into the\\nbook her sister was reading, but it had no pictures \n  or conversations in\\nit, 'and what is the use of a book,' \n  thought Alice 'without pictures or\\nconversation?'\n  \\n\\nSo she was considering in her own mind \n  (as well as she could, for the\\nhot day made her feel \n  very sleepy and stupid), whether the pleasure\\nof making \n  a daisy-chain would be worth the trouble of getting up \n  and\\npicking the daisies, when suddenly a White Rabbit \n  with pink eyes ran\\nclose by her.\\n\\nThere was nothing \n  so VERY remarkable in that; nor did Alice think \n  it so\\nVERY much out of the way to hear the Rabbit\"\n```", "```py\nalice_raw[:800]\n```", "```py\n\"[Alice's Adventures in Wonderland by Lewis Carroll 1865]\n  \\n\\nCHAPTER I. Down the Rabbit-Hole\\n\\nAlice was beginning \n  to get very tired of sitting by her sister on the\\nbank, \n  and of having nothing to do: once or twice she had peeped \n  into the\\nbook her sister was reading, but it had no pictures \n  or conversations in\\nit, 'and what is the use of a book,' \n  thought Alice 'without pictures or\\nconversation?\n  '\\n\\nSo she was considering in her own mind \n  (as well as she could, for the\\nhot day made her feel \n  very sleepy and stupid), whether the pleasure\\nof making \n  a daisy-chain would be worth the trouble of getting up \n  and\\npicking the daisies, when suddenly a White Rabbit \n  with pink eyes ran\\nclose by her.\\n\\nThere was nothing \n  so VERY remarkable in that; nor did Alice think \n  it so\\nVERY much out of the way to hear the Rabbit\"\n```", "```py\n[['alic', \"'s\", 'adventur', 'wonderland', 'lewi', 'carrol', \n  '1865', 'chapter', 'i.', 'rabbit-hol', 'alic', 'begin', \n  'get', 'tire', 'sit', 'sister', 'bank', 'noth', 'twice', \n  'peep', 'book', 'sister', 'read', 'pictur', 'convers', \n  \"'and\", 'use', 'book', 'thought', 'alic', \"'without\", \n  'pictur', 'convers'], \n ['consid', 'mind', 'well', 'could', 'hot', 'day', 'made', \n  'feel', 'sleepi', 'stupid', 'whether', 'pleasur', 'make', \n  'daisy-chain', 'would', 'worth', 'troubl', 'get', 'pick', \n  'daisi', 'suddenli', 'white', 'rabbit', \n  'pink', 'eye', 'ran', 'close'], \n ['noth', 'remark', 'alic', 'think', 'much', 'way', 'hear', \n  'rabbit', 'say', \"'oh\", 'dear'], \n ['oh', 'dear'], \n ['shall', 'late']]\n```", "```py\ntxt_words_nostop\n```", "```py\n[['welcome', 'world', 'deep', 'learning', 'nlp'],\n [\"'re\", 'together', \"'ll\", 'learn', 'together'],\n ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'],\n ['let', \"'s\", 'learn']]\n```", "```py\n    print(txt_words_nostop)\n    ```", "```py\n    [['welcome', 'world', 'deep', 'learning', 'nlp'], \n     [\"'re\", 'together', \"'ll\", 'learn', 'together'], \n     ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'], \n     ['let', \"'s\", 'learn']]\n    ```", "```py\n    target_terms = [\"nlp\",\"deep\",\"learn\"]\n    ```", "```py\n    def get_onehot(sent):\n        return [1 if term in sent else 0 for term in target_terms]\n    ```", "```py\n    one_hot_mat = [get_onehot(sent) for sent in txt_words_nostop]\n    ```", "```py\n    import numpy as np\n    np.array(one_hot_mat)\n    ```", "```py\n    array([[1, 1, 0],\n           [0, 0, 1],\n           [1, 1, 0],\n           [0, 0, 1]])\n    ```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\n```", "```py\ntxt_sents\n```", "```py\n['welcome to the world of deep learning for nlp!',\n \"we're in this together, and we'll learn together.\",\n 'nlp is amazing, and deep learning makes it even more fun.',\n \"let's learn!\"]\n```", "```py\nvectorizer = CountVectorizer(max_features = 5)\n```", "```py\nvectorizer.fit(txt_sents)\n```", "```py\nvectorizer.vocabulary_\n```", "```py\n{'deep': 1, 'we': 4, 'together': 3, 'and': 0, 'learn': 2}\n```", "```py\ntxt_dtm = vectorizer.fit_transform(txt_sents)\ntxt_dtm.toarray()\n```", "```py\narray([[0, 1, 0, 0, 0],\n       [1, 0, 1, 2, 2],\n       [1, 1, 0, 0, 0],\n       [0, 0, 1, 0, 0]], dtype=int64)\n```", "```py\ntxt_sents\n```", "```py\n['welcome to the world of deep learning for nlp!',\n \"we're in this together, and we'll learn together.\",\n 'nlp is amazing, and deep learning makes it even more fun.',\n \"let's learn!\"]\n```", "```py\ndef do_nothing(doc):\n    return doc\n```", "```py\nvectorizer = CountVectorizer(max_features=5,\n                             preprocessor=do_nothing,\n                             tokenizer=do_nothing)\n```", "```py\ntxt_dtm = vectorizer.fit_transform(txt_words_stem)\ntxt_dtm.toarray()\n```", "```py\narray([[0, 1, 1, 1, 0],\n       [1, 0, 1, 0, 2],\n       [0, 1, 1, 1, 0],\n       [0, 0, 1, 0, 0]], dtype=int64)\n```", "```py\nvectorizer.vocabulary_\n```", "```py\n{'deep': 1, 'learn': 2, 'nlp': 3, 'togeth': 4, \"'ll\": 0}\n```", "```py\ntxt_words_stem\n```", "```py\n[['welcom', 'world', 'deep', 'learn', 'nlp'],\n [\"'re\", 'togeth', \"'ll\", 'learn', 'togeth'],\n ['nlp', 'amaz', 'deep', 'learn', 'make', 'even', 'fun'],\n ['let', \"'s\", 'learn']]\n```", "```py\n['welcome to the world of deep learning for nlp!',\n \"we're in this together, and we'll learn together.\",\n 'nlp is amazing, and deep learning makes it even more fun.',\n \"let's learn!\"]\n```", "```py\n    from sklearn.feature_extraction.text import TfidfVectorizer\n    ```", "```py\n    vectorizer_tfidf = TfidfVectorizer(max_features=5)\n    ```", "```py\n    vectorizer_tfidf.fit(txt_sents)\n    ```", "```py\n    vectorizer_tfidf.vocabulary_\n    ```", "```py\n    {'deep': 1, 'we': 4, 'together': 3, 'and': 0, 'learn': 2}\n    ```", "```py\n    txt_tfidf = vectorizer_tfidf.transform(txt_sents)\n    ```", "```py\n    txt_tfidf.toarray()\n    ```", "```py\n    array([[0\\.        , 1\\.        , \n            0\\.        , 0\\.        , 0\\.        ],\n           [0.25932364, 0\\.        , 0.25932364, \n            0.65783832, 0.65783832],\n           [0.70710678, 0.70710678, 0\\.        , \n            0\\.        , 0\\.        ],\n           [0\\.        , 0\\.        , 1\\.        , \n            0\\.        , 0\\.        ]])\n    ```", "```py\n    vectorizer_tfidf.idf_\n    ```", "```py\n    array([1.51082562, 1.51082562, 1.51082562, \n           1.91629073, 1.91629073])\n    ```", "```py\n!pip install gensim\n```", "```py\nimport gensim.downloader as api\ndataset = api.load(\"text8\")\n```", "```py\nfrom gensim.models import word2vec\ndataset = word2vec.Text8Corpus(\"text8\")\n```", "```py\nnp.random.seed(1)\n```", "```py\nmodel = word2vec.Word2Vec(dataset)\n```", "```py\nprint(model.wv[\"animal\"])\n```", "```py\nlen(model.wv[\"animal\"])\n```", "```py\n100\n```", "```py\nmodel.wv.most_similar(\"animal\")\n```", "```py\n[('insect', 0.7598186135292053),\n ('animals', 0.729228138923645),\n ('aquatic', 0.6679497957229614),\n ('insects', 0.6522265672683716),\n ('organism', 0.6486647725105286),\n ('mammal', 0.6478426456451416),\n ('eating', 0.6435647010803223),\n ('ants', 0.6415578722953796),\n ('humans', 0.6414449214935303),\n ('feces', 0.6313734650611877)]\n```", "```py\nmodel.wv.most_similar(\"happiness\")\n```", "```py\n[('humanity', 0.7819231748580933),\n ('perfection', 0.7699881792068481),\n ('pleasure', 0.7422512769699097),\n ('righteousness', 0.7402842044830322),\n ('desires', 0.7374188899993896),\n ('dignity', 0.7189303040504456),\n ('goodness', 0.7103697657585144),\n ('fear', 0.7047020196914673),\n ('mankind', 0.7046756744384766),\n ('salvation', 0.6990150213241577)]\n```", "```py\nmodel.wv.most_similar(positive=['woman', 'king'], \\\n                      negative=['man'], topn=5)\n```", "```py\n[('queen', 0.6803990602493286),\n ('empress', 0.6331825852394104),\n ('princess', 0.6145625114440918),\n ('throne', 0.6131302714347839),\n ('emperor', 0.6064509153366089)]\n```", "```py\nmodel.wv.most_similar(positive=['uncle', 'woman'], \\\n                      negative=['man'], topn=5)\n```", "```py\n[('aunt', 0.8145735263824463),\n ('grandmother', 0.8067640066146851),\n ('niece', 0.7993890643119812),\n ('wife', 0.7965766787528992),\n ('widow', 0.7914236187934875)]\n```", "```py\n    v1 = model.wv['get']\n    ```", "```py\n    v2 = model.wv['happy']\n    ```", "```py\n    res1 = (v1+v2)/2\n    ```", "```py\n    v1 = model.wv['make']\n    v2 = model.wv['merry']\n    ```", "```py\n    res2 = (v1+v2)/2\n    ```", "```py\n    model.wv.cosine_similarities(res1, [res2])\n    ```", "```py\n    array([0.5798107], dtype=float32)\n    ```", "```py\nmodel = word2vec.Word2Vec(dataset, size=30)\n```", "```py\nmodel.wv.most_similar(positive=['woman', 'king'], \\\n                      negative=['man'], topn=5)\n```", "```py\n[('emperor', 0.8314059972763062),\n ('empress', 0.8250986933708191),\n ('son', 0.8157491683959961),\n ('prince', 0.8060941696166992),\n ('archbishop', 0.8003251552581787)]\n```", "```py\nmodel = word2vec.Word2Vec(dataset)\nmodel.wv.most_similar(\"oeuvre\", topn=5)\n```", "```py\n[('baglione', 0.7203884124755859),\n ('chateaubriand', 0.7119786143302917),\n ('kurosawa', 0.6956337690353394),\n ('swinburne', 0.6926312446594238),\n ('poetess', 0.6910216808319092)]\n```", "```py\nmodel_sg = word2vec.Word2Vec(dataset, sg=1)\nmodel_sg.wv.most_similar(\"oeuvre\", topn=5)\n```", "```py\n[('masterful', 0.8347533345222473),\n ('orchestration', 0.8149941563606262),\n ('mussorgsky', 0.8116796016693115),\n ('showcasing', 0.8080146312713623),\n ('lithographs', 0.805435299873352)]\n```", "```py\n    nltk.download('brown')\n    nltk.download('movie_reviews')\n    from nltk.corpus import brown, movie_reviews\n    ```", "```py\n    model_brown = word2vec.Word2Vec(brown.sents(), sg=1)\n    model_movie = word2vec.Word2Vec(movie_reviews.sents(), sg=1)\n    ```", "```py\n    model_brown.wv.most_similar('money', topn=5)\n    ```", "```py\n    [('job', 0.8477444648742676),\n     ('care', 0.8424298763275146),\n     ('friendship', 0.8394286632537842),\n     ('risk', 0.8268661499023438),\n     ('permission', 0.8243911862373352)]\n    ```", "```py\n    model_movie.wv.most_similar('money', topn=5)\n    ```", "```py\n    [('cash', 0.7299771904945374),\n     ('ransom', 0.7130625247955322),\n     ('record', 0.7028014063835144),\n     ('risk', 0.6977001428604126),\n     ('paid', 0.6940697431564331)]\n    ```", "```py\nfrom gensim.scripts.glove2word2vec import glove2word2vec\nglove_input_file = 'glove.6B.100d.txt'\nword2vec_output_file = 'glove.6B.100d.w2vformat.txt'\nglove2word2vec(glove_input_file, word2vec_output_file)\n```", "```py\nfrom gensim.models.keyedvectors import KeyedVectors\nglove_model = KeyedVectors.load_word2vec_format\\\n              (\"glove.6B.100d.w2vformat.txt\", binary=False)\n```", "```py\nglove_model.most_similar(\"money\", topn=5)\n```", "```py\n[('funds', 0.8508071899414062),\n ('cash', 0.848483681678772),\n ('fund', 0.7594833374023438),\n ('paying', 0.7415367364883423),\n ('pay', 0.740767240524292)]\n```", "```py\nglove_model.most_similar(positive=['woman', 'king'], \\\n                         negative=['man'], topn=5)\n```", "```py\n[('queen', 0.7698541283607483),\n ('monarch', 0.6843380928039551),\n ('throne', 0.6755737066268921),\n ('daughter', 0.6594556570053101),\n ('princess', 0.6520533561706543)]\n```", "```py\nmodel.wv.most_similar(positive=['woman', 'doctor'], \\\n                      negative=['man'], topn=5)\n```", "```py\n[('nurse', 0.6464251279830933),\n ('child', 0.5847542881965637),\n ('teacher', 0.569127082824707),\n ('detective', 0.5451491475105286),\n ('boyfriend', 0.5403486490249634)]\n```", "```py\nmodel.wv.most_similar(positive=['woman', 'smart'], \\\n                      negative=['man'], topn=5)\n```", "```py\n[('cute', 0.6156168580055237),\n ('dumb', 0.6035820245742798),\n ('crazy', 0.5834532976150513),\n ('pet', 0.582811713218689),\n ('fancy', 0.5697714686393738)]\n```"]