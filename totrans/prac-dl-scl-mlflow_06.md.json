["```py\n    # COMMAND ---------- \n    import mlflow\n    import torch\n    from flash.core.data.utils import download_data\n    from flash.text import TextClassificationData, TextClassifier\n    import torchmetrics\n    ```", "```py\n# MAGIC %md\n# MAGIC #### Notebooks for fine-tuning a pretrained language model to do text-based sentiment classification\n```", "```py\n# %%\ndownload_data(\"https://pl-flash-data.s3.amazonaws.com/imdb.zip\", \"./data/\")\ndatamodule = TextClassificationData.from_csv(\n    input_fields=\"review\",\n    target_fields=\"sentiment\",\n    train_file=\"data/imdb/train.csv\",\n    val_file=\"data/imdb/valid.csv\",\n    test_file=\"data/imdb/test.csv\"\n)\n```", "```py\n# %% Notebook for fine-tuning a pretrained language model and sentiment classification\n```", "```py\nconda create -n dl_model python==3.8.10\n```", "```py\nconda activate dl_model\n```", "```py\npip install -r requirements.txt\n```", "```py\npython fine_tuning.py\n```", "```py\nname: dl_model_chapter04\n```", "```py\nconda_env: conda.yaml\n```", "```py\nentry_points:\n```", "```py\n  main:\n```", "```py\n    parameters:\n```", "```py\n      pipeline_steps:\n```", "```py\n        description: Comma-separated list of dl pipeline steps to execute \n```", "```py\n        type: str\n```", "```py\n        default: all\n```", "```py\n    command: \"python main.py --steps {pipeline_steps}\"\n```", "```py\n  download_data:\n```", "```py\n    parameters:\n```", "```py\n      download_url:\n```", "```py\n        description: a url to download the data for fine tuning a text sentiment classifier\n```", "```py\n        type: str\n```", "```py\n        default: https://pl-flash-data.s3.amazonaws.com/imdb.zip\n```", "```py\n      local_folder:\n```", "```py\n        description: a local folder to store the downloaded data\n```", "```py\n        type: str\n```", "```py\n        default: ./data\n```", "```py\n      pipeline_run_name:\n```", "```py\n        description: an mlflow run name\n```", "```py\n        type: str\n```", "```py\n        default: chapter04\n```", "```py\n    command:\n```", "```py\n      \"python pipeline/download_data.py --download_url {download_url} --local_folder {local_folder} \\\n```", "```py\n      --pipeline_run_name {pipeline_run_name}\"\n```", "```py\n@click.command()\n```", "```py\n@click.option(\"--steps\", default=\"all\", type=str)\n```", "```py\ndef run_pipeline(steps):\n```", "```py\n    with mlflow.start_run(run_name='pipeline', nested=True) as active_run:\n```", "```py\n        download_run = mlflow.run(\".\", \"download_data\", parameters={})\n```", "```py\nif __name__ == \"__main__\":\n```", "```py\n    run_pipeline()\n```", "```py\nMLproject\n```", "```py\nconda.yaml\n```", "```py\nmain.py\n```", "```py\npipeline/download_data.py\n```", "```py\npipeline/fine_tuning_model.py\n```", "```py\npipeline/register_model.py\n```", "```py\nimport click\n```", "```py\nimport mlflow\n```", "```py\n@click.command()\n```", "```py\n@click.option(\"input\")\n```", "```py\ndef task(input):\n```", "```py\n    with mlflow.start_run() as mlrun:\n```", "```py\n        # Implement pipeline step logic here \n```", "```py\n        mlflow.log_parameter('parameter', parameter)\n```", "```py\n        mlflow.set_tag('pipeline_step', __file__)\n```", "```py\n        mlflow.log_artifacts(artifacts, artifact_path=\"data\")\n```", "```py\nif __name__ == '__main__':\n```", "```py\n    task()\n```", "```py\nimport click\n```", "```py\nimport mlflow\n```", "```py\nfrom flash.core.data.utils import download_data\n```", "```py\n@click.command()\n```", "```py\n@click.option(\"--download_url\")\n```", "```py\n@click.option(\"--local_folder\")\n```", "```py\n@click.option(\"--pipeline_run_name\")\n```", "```py\ndef task(download_url, local_folder, pipeline_run_name):\n```", "```py\n    with mlflow.start_run(run_name=pipeline_run_name) as mlrun:\n```", "```py\n        download_data(download_url, local_folder)\n```", "```py\n        mlflow.log_param(\"download_url\", download_url)\n```", "```py\n        mlflow.log_param(\"local_folder\", local_folder)\n```", "```py\n        mlflow.set_tag('pipeline_step', __file__)\n```", "```py\n        mlflow.log_artifacts(local_folder, artifact_path=\"data\")\n```", "```py\nif __name__ == '__main__':\n```", "```py\n    task()\n```", "```py\n        with mlflow.start_run(run_name='pipeline', nested=True) as active_run:\n```", "```py\n            download_run = mlflow.run(\".\", \"download_data\", parameters={})\n```", "```py\n            download_run = mlflow.tracking.MlflowClient().get_run(download_run.run_id)\n```", "```py\n            file_path_uri = download_run.data.params['local_folder']\n```", "```py\n            fine_tuning_run = mlflow.run(\".\", \"fine_tuning_model\", parameters={\"data_path\": file_path_uri})\n```", "```py\nfine_tuning_run_id = fine_tuning_run.run_id\n```", "```py\nregister_model_run = mlflow.run(\".\", \"register_model\", parameters={\"mlflow_run_id\": fine_tuning_run_id})\n```", "```py\n    with mlflow.start_run() as mlrun:\n```", "```py\n        logged_model = f'runs:/{mlflow_run_id}/model'\n```", "```py\n        mlflow.register_model(logged_model, registered_model_name)\n```", "```py\npython main.py\n```", "```py\nname: dl_model \n```", "```py\nchannels:\n```", "```py\n  - conda-forge\n```", "```py\ndependencies:\n```", "```py\n  - python=3.8.10\n```", "```py\n  - pip\n```", "```py\n  - pip:\n```", "```py\n    - -r requirements.txt\n```", "```py\nipykernel==6.4.1\n```", "```py\nlightning-flash[all]==0.5.0\n```", "```py\nmlflow==1.20.2\n```", "```py\ntransformers==4.9.2\n```", "```py\nboto3==1.19.7\n```", "```py\npytorch-lightning==1.4.9\n```", "```py\ndatasets==1.9.0\n```", "```py\nclick==8.0.3\n```", "```py\nconda env list\n```", "```py\nmlflow-95353930ddb7b60101df80a5d64ef8bf6204a808\n```", "```py\ncool-dl-utils @ git+ssh://git@github.com/cool_org/cool-dl-utils.git@81218891bbf5a447103884a368a75ffe65b17a44#egg=cool-dl-utils\n```", "```py\ngit+ssh://git@github.com/cool_org/cool-dl-utils.git@2.11.4\n```", "```py\n    df = spark.read \\\n      .format(\"delta\") \\\n      .option(\"timestampAsOf\", \"2020-11-01\") \\\n      .load(\"/path/to/my/table\")\n    ```", "```py\n    df = spark.read \\\n      .format(\"delta\") \\\n      .option(\"versionAsOf\", \"52\") \\\n      .load(\"/path/to/my/table\")\n    ```", "```py\n    imdb_train_df = spark.read.option('header', True).csv('dbfs:/FileStore/imdb/train.csv')\n    ```", "```py\n    imdb_train_df.write.format('delta').option(\"mergeSchema\", \"true\").mode(\"overwrite\").save('/imdb/training.delta')\n    ```", "```py\n    imdb_train_delta = spark.read.format('delta').load('/imdb/training.delta')\n    ```", "```py\n    train_data_version = spark.read.format(\"delta\").option(\"versionAsOf\", \"0\").load('/imdb/train.delta')  \n    ```", "```py\n    train_data_timestamped = spark.read.format(\"delta\").option(\"timestampAsOf\", \"2021-11-22T03:39:22\").load('/imdb/train.delta')  \n    ```", "```py\n    mlflow.log_parameter('file_path', '/imdb/train.delta')\n    mlflow.log_parameter('file_version', '0')\n    mlflow.log_parameter('file_timestamp', '2021-11-22T03:39:22') \n    ```"]