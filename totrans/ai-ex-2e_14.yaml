- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Preparing the Input of Chatbots with Restricted Boltzmann Machines (RBMs) and
    Principal Component Analysis (PCA)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the following chapters, we will explore chatbot frameworks and build chatbots.
    You will find that creating a chatbot structure only takes a few clicks. However,
    no chatbot can be built without designing the input to prepare the desired dialog
    flow. The goal of this chapter is to demonstrate how to extract features from
    a dataset and then use them to gather the basic information to build a chatbot
    in *Chapter 15*, *Setting up a Cognitive NLP UI/CUI Chatbot*.
  prefs: []
  type: TYPE_NORMAL
- en: The input of a dialog requires in-depth research and designing. In this chapter,
    we will build a **restricted Boltzmann machine** (**RBM**) that will analyze a
    dataset. In *Chapter 13*, *Visualizing Networks with TensorFlow 2.x and TensorBoard*,
    we examined the layers of a convolutional neural network (CNN) and displayed their
    outputs. This time, we will explore the weights of the RBM. We will go further
    and use the weights of the RBM as features. The weights of an RBM can be transformed
    into feature vectors for a **principal component analysis** (**PCA**) algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: We will use the feature vectors generated by the RBM to build a PCA display
    using TensorBoard Embedding Projector's functionality. We will then use the statistics
    obtained to lay the grounds for the inputs of a chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the whole process, we will use streaming platform data as an example of how
    this is done. Streaming has become a central activity of almost all smartphone
    owners. The problem facing Netflix, YouTube, Amazon, or any platform offering
    streaming services is to offer us the right video to watch. If a viewer watches
    a video, and the platform does not display a pertinent similar one to watch next,
    the viewer might choose to use another platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is divided into two parts:'
  prefs: []
  type: TYPE_NORMAL
- en: Building an RBM and then extending it to an automatic feature vector generator
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using PCA to represent the weights of an RBM as features. TensorFlow's Embedding
    Projector possesses an inbuilt PCA function. The statistics produced will provide
    the basis of the dialog structure for *Chapter 15*, *Setting Up a Cognitive NLP
    UI/CUI Chatbot*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's first define the basic terms we are using and our goals.
  prefs: []
  type: TYPE_NORMAL
- en: Defining basic terms and goals
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The goal of this chapter is to prepare data to create the input of a chatbot
    we will build in *Chapter 15*, *Setting Up a Cognitive NLP UI/CUI Chatbot*.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a chatbot requires preparation. We cannot just step into a project
    without a minimum amount of information. In our case, we will examine a dataset
    I created based on movie preferences. I did not choose to download huge datasets
    because we need to first focus on understanding the process and building a model
    using basic data.
  prefs: []
  type: TYPE_NORMAL
- en: The size of the datasets increase daily on an online platform. When we watch
    a movie on a streaming platform, on Netflix for example, we can like the movie
    or click on the thumbs-down button.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we approve or disapprove of a movie on an online platform, our preferences
    are recorded. The features of these movies provide valuable information for the
    platform, which can then display choices we prefer: action, adventure, romantic,
    comedy, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will first use an RBM to extract a description (such as
    action, adventure, or comedy, for example) of the movies watched by a user or
    a group of users. We will take the output weights produced by the RBM to create
    a file of features reflecting the user's preferences.
  prefs: []
  type: TYPE_NORMAL
- en: This file of features of a user's preferences can be considered as a "mental
    dataset" of a person. The name might seem strange at first. However, a "mental"
    representation of a person goes beyond the standard age, income, and other impersonal
    data. Features such as "love," "violence," and "horizons" (wider views, adventure)
    give us a deeper understanding of a person than information we can find on a driving
    license.
  prefs: []
  type: TYPE_NORMAL
- en: In the second part of the chapter, we will use the RBM's output of features
    of a person's "mind" as the input of a PCA. The PCA will calculate how the features
    relate to each other and how they vary, and we will display them in TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: We will then actually *see* a representation of a person's mind through the
    key features drawn from the RBM. This information will then be used to help us
    create a customized chatbot in *Chapter 15*.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on to the first phase and build an RBM.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing and building an RBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'RBMs are random and undirected graph models generally built with a visible
    and a hidden layer. They were used in a Netflix competition to predict future
    user behavior. The goal here is not to predict what a viewer will do but establish
    who the viewer is and store the data in a viewer''s profile-structured mind dataset.
    The input data represents the features to be trained to learn about viewer X.
    Each column represents a feature of X''s potential personality and tastes. Each
    line represents the features of a movie that X has watched. The following code
    (and this section) is in `RBM_01.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The goal of this RBM is to define a profile of X by computing the features of
    the movies watched. The input data could also be images, words, and other forms
    of data, as in any neural network.
  prefs: []
  type: TYPE_NORMAL
- en: First, we will explore the architecture and define what an energy-driven neural
    network is. Then, we will build an RBM from scratch in Python.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture of an RBM
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The RBM model used contains two layers: a visible layer and a hidden layer.
    Many types of RBMs exist, but generally, they contain the following properties:'
  prefs: []
  type: TYPE_NORMAL
- en: There is no connection between the visible units, which is why it is *restricted*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no connection between the hidden units enforcing the restricted property
    of the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is no direction as in a feedforward neural network (FNN), as explored
    in *Chapter 8*, *Solving the XOR Problem with a Feedforward Neural Network*. An RBM's
    model is thus an *undirected* graph.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The visible and hidden layers are connected by a weight matrix and a bias vector,
    which are the lines in the following diagram:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15438_14_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.1: The connection between visible and hidden units'
  prefs: []
  type: TYPE_NORMAL
- en: The network contains six visible and two hidden units, producing a weight matrix
    of 2×6 values to which we will add bias values.
  prefs: []
  type: TYPE_NORMAL
- en: You will note that there is no output. The system runs from the visible units
    to the hidden units and back. We are operating feature extraction with this type
    of network. In this chapter, for example, we will use the weights as features.
  prefs: []
  type: TYPE_NORMAL
- en: By forcing the network to represent its data contained in 6 units in 2 units
    through a weight matrix, the RBM creates feature representations. The hidden units,
    weights, and biases can be used for feature extraction.
  prefs: []
  type: TYPE_NORMAL
- en: An energy-based model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: An RBM is an energy-based model. The higher the energy, the lower the probability
    of obtaining the correct information; the lower the energy, the higher the probability
    – in other words, the higher the accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand this, let''s go back to the cup of tea we observed in *Chapter
    1*, *Getting Started with Next-Generation Artificial Intelligence through Reinforcement
    Learning*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.2: The complexity of a cup of tea'
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 1*, we observed a microstate of the cup through its global content
    and temperature. Then, we went on to use the Markov decision process (MDP) to
    run microstate calculations.
  prefs: []
  type: TYPE_NORMAL
- en: 'This time, we will focus on the temperature of the cup of tea. *x* will be
    the global temperature of all the molecules in the cup of tea:'
  prefs: []
  type: TYPE_NORMAL
- en: If *x* = 1, this means the temperature is very hot. The tea has just boiled.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *x* = 0.5, this means the temperature has gone down.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If *x* = 0.1, this means the temperature is still a bit warm, but the tea is
    cooling.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The higher the temperature, the more the molecules will be bouncing around in
    the cup with a high level of energy, making it feel hot.
  prefs: []
  type: TYPE_NORMAL
- en: However, the hotter it is, the closer to very hot, the lower the probability
    we can drink it.
  prefs: []
  type: TYPE_NORMAL
- en: 'This leads to a probability *p* for a temperature *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x* -> 1, *p* -> 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*x* -> 0, *p* -> 1'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, in an energy-driven system, we will strive to lower the energy
    level. Let's say we have a person with an unknown tolerance for hot drinks, and
    we want to wager whether they can drink our cup of tea. Nobody wants to drink
    cold (low-energy) tea, sure, but if our focus is on the likelihood of a person
    being able to drink the tea without finding it too hot (high-energy), then we
    want that tea to be as low-energy (that is, cool) as possible!
  prefs: []
  type: TYPE_NORMAL
- en: To illustrate the *p*(*x*) system of our cup of tea, we will use Euler's number
    *e*, which is equal to 2.718281\. *p*(*x*) is the probability that we can drink
    our cup of tea, with *p* being the probability, and *x* the temperature or energy.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin to introduce a simple energy function in which *p*(*x*) = *e*^((–)^x^):'
  prefs: []
  type: TYPE_NORMAL
- en: '*p*(*e*^((–1))) = 0.36'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p*(*e*^((–0.5))) = 0.60'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*p*(*e*^((–0.1))) = 0.90'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can see that as –*x* (energy) decreases, the probability *p*(*x*) increases.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the learning function of an RBM is to decrease the energy level
    by optimizing the weights and biases. By doing this, the RBM increases the probability
    that the hidden units, the weights, and biases are optimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'To calculate the energy of an RBM, we will take the complete architecture of
    the network into account. Let''s display our model again as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.3: The connection between visible and hidden units'
  prefs: []
  type: TYPE_NORMAL
- en: 'This RBM model contains the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: '*E*(*v*, *h*), which is the energy function that takes the visible units (input
    data) and hidden units into account.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*v*[i] = the states of the visible units (input).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*a*[i] = the biases of the visible units.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*h*[j] = the states of the hidden units.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b*[j] = the biases of the hidden units.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w*[ij] = the weight matrix.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With these variables in mind, we can define the energy function of an RBM for
    ![](img/B15438_14_001.png), ![](img/B15438_14_002.png), and *ij* as the lines
    and columns of the weight matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we've got a better idea of what an RBM is and the principles behind
    it, let's start to consider how to build an RBM from scratch using Python.
  prefs: []
  type: TYPE_NORMAL
- en: Building the RBM in Python
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will build an RBM using `RBM_01.py` from scratch using our bare hands with
    no pre-built library. The idea is to understand an RBM from top to bottom to see
    how it ticks. We will explore more RBM theory as we build the machine.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a class and the structure of the RBM
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'First, the RBM class is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The first function of the class will receive the number of hidden units (`2`)
    and the number of visible units (`6`).
  prefs: []
  type: TYPE_NORMAL
- en: 'The weight matrix is initialized with random weight values at line 20:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The bias units will now be inserted in the first row and the first column at
    line 27:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The goal of this model will be to observe the behavior of the weights. Observing
    the weights will determine how to interpret the result in this model based on
    calculations between the visible and hidden units.
  prefs: []
  type: TYPE_NORMAL
- en: The first row and column are the biases, as shown in the preceding code snippets.
    Only the weights will be analyzed for the profiling functions. The weights and
    biases are now in place.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a training function in the RBM class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On line 30, the training function is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'In this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '`self` is the class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data` is the 6×6 input array, containing 6 lines of movies and 6 columns of
    features of the movies:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The RBM model in this chapter is using **visible binary units**, as shown in
    the input, which is the training data of this model. The RBM will use the input
    as training data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'An RBM can contain other types of units: softmax units, Gaussian visible units,
    binomial units, rectified linear units, and more. Our model focuses on binary
    units.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`max_epochs` is the number of epochs that the RBM will run to train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`learning_rate` is the learning rate that will be applied to the weight matrix containing
    the weights and the biases.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will now insert bias units of `1` in the first column on line 35:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: There are other strategies to initialize biases. This is a trial-and-error process,
    depending on your project. In this case, bias units of `1` are sufficient to do
    the job.
  prefs: []
  type: TYPE_NORMAL
- en: Computing the hidden units in the training function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On line 37, we start training the RBM during `max_epochs` by computing the
    value of the hidden units:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The first phase is to focus on the hidden units. We activate the probabilities
    of the hidden units with our weight matrix using dot matrix multiplication:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we apply the logistic function as we saw in *Chapter 2*, *Building a
    Reward Matrix – Designing Your Datasets*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The logistic function called is on line 63:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We set the biases to `1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: We now have computed the first epoch of the probabilities of the hidden states
    with random weights.
  prefs: []
  type: TYPE_NORMAL
- en: Random sampling of the hidden units for the reconstruction and contractive divergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are many sampling methods, such as Gibbs sampling, for example, which
    has a randomized approach to avoid deterministic samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this model, we will choose a random sample that chooses the values of the
    hidden probabilities that exceed the values of a random sample of values. The
    `random.rand` function creates a random matrix with values between `0` and `1`,
    with a size of `num_examples`×`self.num_hidden+1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: This sample will be used for the **reconstruction** phase we will explore in
    the next section.
  prefs: []
  type: TYPE_NORMAL
- en: 'We also need to compute an association for the **contrastive divergence** (the
    function used to update the weight matrix) phase, which is explained hereinunder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'This dot product of the visible data units *v* × the hidden units *h* can be
    represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_004.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that the dot product has been implemented, we will build the reconstruction
    phase.
  prefs: []
  type: TYPE_NORMAL
- en: Reconstruction
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An RBM uses its input data as its training data, computes the hidden weights
    using a random weight matrix, and then *reconstructs* the visible units. Instead
    of an output layer as in other neural networks, an RBM reconstructs the visible
    units and compares them to the original data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code applies the same approach as for the hidden units described
    previously to generate visible units:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: These negative visible units will be used to evaluate the error level of the
    RBM, as explained here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have generated visible units with our sample of hidden unit states,
    we move on and generate the corresponding hidden states:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that `neg_associations` can be represented in the following form:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_005.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, we have done the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Computed positive hidden states using the visible units containing the training
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selected a random sample of those positive hidden states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reconstructed negative (generated from the hidden states, not the data) visible
    states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And, in turn, generated hidden states from the visible states produced
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have *reconstructed* visible states through this process. However, we need
    to evaluate the result and update the weights.
  prefs: []
  type: TYPE_NORMAL
- en: Contrastive divergence
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To update the weights, we do not use gradient descent. In this energy model,
    we use contrastive divergence, which can be expressed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_006.png)'
  prefs: []
  type: TYPE_IMG
- en: The letter ![](img/B15438_14_007.png) is the learning rate. The learning rate
    should be a small value and can be optimized throughout the training process.
    I applied a small value, 0.001 overall.
  prefs: []
  type: TYPE_NORMAL
- en: 'The source code for updating the weights is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: Over the epochs, the weights will adjust, bringing the energy and error level
    down and, hence, bringing the accuracy of the probabilities up.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we will display the error level and the energy value of the RBM
    throughout the epochs.
  prefs: []
  type: TYPE_NORMAL
- en: Error and energy function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'On line 56, the error function calculates the squared sum of the difference
    between the visible units provided by the data and the reconstructed visible units:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'For the energy function, we can use our original energy equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_008.png)'
  prefs: []
  type: TYPE_IMG
- en: In our code, we will not use the biases since we often set them to `1`.
  prefs: []
  type: TYPE_NORMAL
- en: We will also need a function to measure the evolution of the energy of the RBM.
  prefs: []
  type: TYPE_NORMAL
- en: 'The energy will be measured with a probabilistic function *p*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_009.png)'
  prefs: []
  type: TYPE_IMG
- en: '*Z* is a **partition function** for making sure that the sum of the probabilities
    of each *x* input does not exceed 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_010.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The partition function is the sum of all the individual probabilities of each
    *x*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_011.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The corresponding code will calculate the energy of the RBM, which will decrease
    over time as the RBM goes through the epochs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: You will note that neither the error function nor the energy function influences
    the training process. The training process is based on contrastive divergence.
  prefs: []
  type: TYPE_NORMAL
- en: The error and energy values will measure the efficiency of the model by providing
    some insight into the behavior of the RBM as it trains.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of these measurement values at the beginning of the process
    and at the end:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: At epoch 0, the error is high, and the energy is high, too.
  prefs: []
  type: TYPE_NORMAL
- en: At epoch 4999, the error is sufficiently low for the model to produce correct
    feature extraction values. The energy has significantly diminished.
  prefs: []
  type: TYPE_NORMAL
- en: Running the epochs and analyzing the results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once the RBM has optimized the weight-bias matrix for *n* epochs, the matrix
    will provide the following information for the profiler system of person X, for
    example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: An RBM model uses random values and will produce slightly different results
    each time it is trained.
  prefs: []
  type: TYPE_NORMAL
- en: The RBM will train the input and display the features added to X's profile.
  prefs: []
  type: TYPE_NORMAL
- en: 'The weights of the features have been trained for person X. The first line
    is the bias and examines columns 2 and 3\. The following six lines are the weights
    of X''s features:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: The weights (in bold) are lines 2 to 6 and columns 2 to 3\. The first line and
    first column are the biases.
  prefs: []
  type: TYPE_NORMAL
- en: 'The way to interpret the weights of an RBM remains a careful strategy to build.
    In this case, a creative approach is experimented with to determine marketing
    behavior. There are many other uses of an RBM, such as image processing, for example.
    In this case, the weight matrix will provide a profile of X by summing the weight
    lines of the feature, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The features are now labeled, as displayed in this output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: We can see that beyond standard movie classifications, X likes horizons somewhat,
    does not like violence, and likes action. X finds happiness and love important,
    but not family at this point.
  prefs: []
  type: TYPE_NORMAL
- en: The RBM has provided a personal profile of X—not a prediction, but getting ready
    for a suggestion through a chatbot or just building X's machine mind-dataset.
  prefs: []
  type: TYPE_NORMAL
- en: We have taken a dataset and extracted the main features from it using an RBM.
    The next step will be to use the weights as feature vectors for PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Using the weights of an RBM as feature vectors for PCA
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will be writing an enhanced version of `RBM_01.py`. `RBM_01.py`
    produces the feature vector of one viewer named X. The goal now is to extract
    the features of 12,000 viewers, for example, to have a sufficient number of feature
    vectors for PCA.
  prefs: []
  type: TYPE_NORMAL
- en: In `RBM_01.py`, viewer X's favorite movies were first provided in a matrix.
    The goal now is to produce a random sample of 12,000 viewer vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The first task at hand is to create an RBM launcher to run the RBM 12,000 times
    to simulate a random choice of viewers and their favorite movies, which are the
    ones the viewer liked. Then, the feature vector of each viewer will be stored.
  prefs: []
  type: TYPE_NORMAL
- en: '`RBM_launcher.py` first imports RBM as `rp`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The primary goal of `RBM_launcher.py` is to carry out the basic functions to
    run RBM. Once `RBM` is imported, the feature vector''s `.tsv` file is created:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: When `rp`, the `RBM` function imported as `rp`, is called, it will append the
    feature file.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to create the label file containing the metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: You will notice the use of the word "emotion." In this context, "emotion" refers
    to features for sentiment analysis in general, not human emotions in particular.
    Please read "emotions" in this context as sentiment analysis features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we are ready to run RBM 12,000+ times, for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '`rp.main()` calls the `main()` function in `RBM.py` that we will now enhance
    for this process.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will enhance `RBM_01.py` step-by-step in another file named `RBM.py`. We
    will adapt the code starting line 65 to create an RBM launcher option:'
  prefs: []
  type: TYPE_NORMAL
- en: 'A variable name `pt` is set to `0` or `1`, depending on whether we wish to
    display intermediate information:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: Since this is an automatic process, `pt` is set to `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The metadata of 10 movies is stored in `titles`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'A feature matrix of movies with six features per movie is created starting
    at line 71, with the same features as in `RBM_01.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Each line of the matrix contains a movie, and each column one of the six features
    of that movie. If the value is `0`, the feature is not present; if the value is
    `1`, the feature is present.
  prefs: []
  type: TYPE_NORMAL
- en: In the years to come, the number of features per movie will be extended to an
    indefinite number of features per movie to fine-tune our preferences.
  prefs: []
  type: TYPE_NORMAL
- en: 'An empty output matrix is created. In `RBM_01.py`, the result was provided.
    In this example, it will be filled with random choices:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the random movie selector will generate likes or dislikes per movie and
    per viewer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'We can choose whether to display the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The dialog output is the data collected by the platform through its like/dislike
    interface. The RBM runs its training session:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of the RBM training session are now processed from line 185 to
    line 239 to transform the weights obtained into feature vectors and the corresponding
    metadata:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'The goal is now to select the primary feature of a given movie chosen by a
    given viewer. This feature could be "love" or "violence" for example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The secondary feature is also very interesting. It often provides more information
    than the primary feature. A viewer will tend to view a certain type of movie.
    However, the secondary features vary from movie to movie. For example, suppose
    a young viewer likes action movies. "Violence" could be the primary feature, but
    the secondary feature could be "love" in one case or "family" in another. The secondary
    featured is stored in the feature vector of this viewer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'The metadata is saved in the label file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: This process will be repeated 12,000 times in this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature vector `features.tsv` file has been created:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.4: The feature vector file'
  prefs: []
  type: TYPE_NORMAL
- en: 'The feature vector `labels.tsv` metadata file matches the feature vector file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: You will note that "love" and "violence" appear often. This comes from the way
    I built the dataset based mostly on movies that contain action and some form of warm
    relationship between the characters, which is typical in movies for the younger
    generations.
  prefs: []
  type: TYPE_NORMAL
- en: Now that the feature vectors and the metadata file have been created, we can
    use PCA to represent the points.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: PCA is applied very efficiently to marketing by Facebook, Amazon, Google, Microsoft,
    IBM, and many other corporations, among other feature processing algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Probabilistic machine learning training remains efficient when targeting apparel,
    food, books, music, travel, cars, and other market consumer segments.
  prefs: []
  type: TYPE_NORMAL
- en: However, humans are not just consumers; they are human beings. When they contact websites
    or call centers, standard answers or stereotyped emotional tone analysis approaches
    can depend on one's nerves. When humans are in contact with doctors, lawyers,
    and other professional services, a touch of humanity is necessary if major personal
    crises occur.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of the PCA, in this context, is to extract key features to describe
    an individual or a population. The PCA phase will help us build a mental representation
    of X's profile, either to communicate with X or use X's mind as a powerful, *mindful*
    chatbot or decision-maker.
  prefs: []
  type: TYPE_NORMAL
- en: PCA isn't a simple concept, so let's take some time to understand it properly.
    We'll start with an intuitive explanation, and after that, we'll get into the
    mathematics behind it.
  prefs: []
  type: TYPE_NORMAL
- en: PCA takes data and represents it at a higher level.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine you are in your bedroom. You have some books, magazines,
    and music (maybe on your smartphone) around the room. If you consider your room as
    a 3D Cartesian coordinate system, the objects in your room are all in specific *x*,
    *y*, *z* coordinates.
  prefs: []
  type: TYPE_NORMAL
- en: For experimentation purposes, take your favorite objects and put them on your
    bed. Put the objects you like the most near one another, and your second choices
    a bit further away. If you imagine your bed as a 2D Cartesian space, you have
    just made your objects change dimensions. You have brought the objects that you
    value the most into a higher dimension. They are now more visible than the ones
    that have less value for you.
  prefs: []
  type: TYPE_NORMAL
- en: They are not in their usual place anymore; they are on your bed and at specific
    coordinates depending on your taste.
  prefs: []
  type: TYPE_NORMAL
- en: That is the philosophy of PCA. If the number of data points in the dataset is
    very large, the PCA of a "mental dataset" of one person will always be different
    from the PCA representation of another person, like DNA. A "mental dataset" is
    a collection of thoughts, images, words, and feelings of a given person. It is
    more than the classic age, gender, income, and other neutral features. A "mental
    dataset" will take us inside somebody's mind.
  prefs: []
  type: TYPE_NORMAL
- en: That is what a conceptual representation learning metamodel (CRLMM) is about
    as applied to a person's mental representation. Each person is different, and
    each person deserves a customized chatbot or bot treatment.
  prefs: []
  type: TYPE_NORMAL
- en: Mathematical explanation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The main steps in calculating PCA are important for understanding how to go
    from the intuitive approach to how TensorBoard Embedding Projector represents
    datasets using PCA.
  prefs: []
  type: TYPE_NORMAL
- en: Variance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Variance is when a value changes. For example, as the sun rises in summer, the
    temperature gets warmer and warmer. The variance is represented by the difference
    between the temperature at a given hour and then the temperature a few hours later.
    Covariance is when two variables change together. For example, the hotter it gets
    when we are outside, the more we will sweat to cool our bodies down.
  prefs: []
  type: TYPE_NORMAL
- en: '**Step 1**: Calculate the mean of the array `data1`. You can check this with
    `mathfunction.py`, as shown in the following function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The answer is 2.5\. The mean is not the median (the middle value of an array).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 2**: Calculate the mean of array `data2`. The mean calculation is executed
    with the following standard function:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The answer is ![](img/B15438_14_012.png). The bar above the *X* signifies that
    it is a mean.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Step 3**: Calculate the variance using the following equation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B15438_14_013.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, NumPy will calculate the variance with the absolute value of each *x*
    minus the mean, sum them up, and divide the sum by *n*, as shown in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: Some variances are calculated with *n* – 1 depending on the population of the
    dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'The result of the program for variances is as displayed in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: We can already see that `data2` varies a lot more than `data1`. Do they fit
    together? Are their variances close or not? Do they vary in the same way? Our
    goal in the following section is to find out whether two words, for example, will
    often be found together or close to one another, taking the output of the embedding
    program into account.
  prefs: []
  type: TYPE_NORMAL
- en: Covariance
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The covariance will tell us whether these datasets vary together or not. The
    equation follows the same philosophy as variance, but now both variances are joined
    to see whether they belong together:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_014.png)'
  prefs: []
  type: TYPE_IMG
- en: 'As with the variance, the denominator can be *n* – 1 depending on your model.
    Also, in this equation, the numerator is expanded to visualize the co-part of
    covariance, as implemented in the following array in `mathfunction.py`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'NumPy''s output is a covariance matrix, `a`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: If you increase some of the values of the dataset, it will increase the value
    of the parts of the matrix. If you decrease some of the values of the dataset,
    the elements of the covariance matrix will decrease.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at some of the elements of the matrix increase or decrease that way
    takes time and observation. What if we could find one or two values that would
    give us that information?
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvalues and eigenvectors
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To make sense of the covariance matrix, the eigenvector will point to the direction
    in which the covariances are going. The eigenvalues will express the magnitude
    or importance of a given feature.
  prefs: []
  type: TYPE_NORMAL
- en: To sum it up, an eigenvector will provide the direction and the eigenvalue of
    the importance for the covariance matrix, `a`. With those results, we will be
    able to represent the PCA with TensorBoard Embedding Projector in a multidimensional
    space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let `w` be an eigenvalue(s) of `a`. An eigenvalue(s) must satisfy the following
    equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: There must exist a vector, `v`, for which `dot(a,v)` is the same as `w*v`.
  prefs: []
  type: TYPE_NORMAL
- en: 'NumPy will do the math through the following function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'The eigenvalues are displayed (in ascending order) in the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we need the eigenvectors to see in which direction these values should
    be applied. NumPy provides a function to calculate both the eigenvalues and eigenvectors
    together. That is because eigenvectors are calculated using the eigenvalues of
    a matrix, as shown in this code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the program is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: Eigenvalues come in a 1D array with the eigenvalues of `a`.
  prefs: []
  type: TYPE_NORMAL
- en: Eigenvectors come in a 2D square array with the corresponding value (for each
    eigenvalue) in columns.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the feature vector
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The remaining step is to sort the eigenvalues from the highest to the lowest
    value. The highest eigenvalue will provide the principal component (most important).
    The eigenvector that goes with it will be its feature vector. You can choose to
    ignore the lowest values or features. In the dataset, there will be hundreds,
    and often thousands, of features to represent. Now we have the feature vector:'
  prefs: []
  type: TYPE_NORMAL
- en: feature vector = FV = {eigenvector[1], eigenvector[2] … *n*}
  prefs: []
  type: TYPE_NORMAL
- en: '*n* means that there could be many more features to transform into a PCA feature
    vector.'
  prefs: []
  type: TYPE_NORMAL
- en: Deriving the dataset
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The final step is to transpose the feature vector and original dataset and
    multiply the row feature vector by row data:'
  prefs: []
  type: TYPE_NORMAL
- en: Data that will be displayed = row of feature vector * row of data
  prefs: []
  type: TYPE_NORMAL
- en: Summing it up
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The highest value of eigenvalues is the principal component. The eigenvector
    will determine in which direction the data points will be oriented when multiplied
    by that vector.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorFlow's Embedding Projector to represent PCA
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: TensorBoard Embedding Projector offers an in-built PCA function that can be
    rapidly configured to fit our needs. TensorBoard can be called as a separate program
    or embedded in a program as we saw in *Chapter 13*, *Visualizing Networks with
    TensorFlow 2.x and TensorBoard*.
  prefs: []
  type: TYPE_NORMAL
- en: We will then extract key information on the viewer marketing segment that will
    be used to start building a chatbot in *Chapter 15*, *Setting Up a Cognitive NLP
    UI/CUI Chatbot*.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, go to this link: [https://projector.tensorflow.org/](https://projector.tensorflow.org/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the following functions, bear in mind that TensorBoard Embedding Projector
    is working at each step and that it might take some time depending on your machine.
    We load the data produced by `RBM_launcher.py` and `RBM.py` by clicking on **Load**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.5: The Load button'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the **Load data from your computer** windows appear, we load the feature
    vector `features.tsv` file by clicking on **Choose file**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.6: Loading the feature vector file'
  prefs: []
  type: TYPE_NORMAL
- en: 'We load the `labels.tsv` metadata file by clicking on **Choose file** in **Step
    2 (optional)**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.7: Loading a TSV file'
  prefs: []
  type: TYPE_NORMAL
- en: 'To obtain a good representation of our 12,000+ features, click on **Sphereize
    data,** which is not checked in default mode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.8: Sphereizing data'
  prefs: []
  type: TYPE_NORMAL
- en: 'We now choose label by **secondary_emotion**, color by **secondary_emotion**,
    along with edit by **secondary_emotion**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.9: Managing labels'
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a nice view of the data, we activate night mode so that the moon should
    be active:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.10: Activating the night mode'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, we have a nice PCA representation that turns like Earth, the
    ideas in our mind, or the minds of all of the viewers we are analyzing depending
    on how we use the features. The dots on the image are datapoints representing
    the features we calculated with an RBM and then represented with an image using
    PCA. It is like peeking inside the mind:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.11: A PCA representation of features'
  prefs: []
  type: TYPE_NORMAL
- en: The PCA representation of the features of a given person or a group of people
    provides vital information to create dialogs in a chatbot. Let's analyze the PCA
    to prepare data for a chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the PCA to obtain input entry points for a chatbot
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The goal is to gather some information to get started with our cognitive chatbot.
    We will use the filters provided by TensorBoard.
  prefs: []
  type: TYPE_NORMAL
- en: 'Choose **secondary_emotion** as the basis of our filters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.12: Filtering data'
  prefs: []
  type: TYPE_NORMAL
- en: 'The features we are analyzing are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: We need to see the statistics per feature.
  prefs: []
  type: TYPE_NORMAL
- en: 'We type the feature in TensorBoard''s search option, such as "love," for example,
    and then we click the down arrow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.13: TensorBoard''s search option'
  prefs: []
  type: TYPE_NORMAL
- en: 'The PCA representation changes its view in realtime:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_14_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 14.14: PCA representation of the RBM features'
  prefs: []
  type: TYPE_NORMAL
- en: There are 643 points for "love." Notice that the "love" points are grouped in
    a relatively satisfactory way. They are mostly in the same area of the image and
    not spread out all over the image. This grouping shows that the weights of the
    RBM provided features that turned out to be sufficiently correct in the PCA for
    this experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 'We repeat the process for each feature, to obtain the number of points per
    feature and visualize them. For the dataset supplied on GitHub for this chapter,
    we obtain:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Love: 643'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Happiness: 2267'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Family: 0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Horizons: 1521'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Action: 2976'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Violence: 4594'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Important**: This result will naturally change if `RBM_launcher.py` runs
    again since it''s a random viewer-movie choice process.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The results provide interesting information on the marketing segment we are
    targeting for the chatbot:'
  prefs: []
  type: TYPE_NORMAL
- en: Violence and action point to action movies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Family=0 points to younger viewers; teenagers, for example, more interested
    in action than creating a family.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering happiness and love are part of the horizons they are looking for.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is typical of superhero series and movies. Superheroes are often solitary
    individuals.
  prefs: []
  type: TYPE_NORMAL
- en: We will see how this works out when we build our chatbot in *Chapter 15*, *Setting
    Up a Cognitive NLP UI/CUI Chatbot*.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we prepared key information to create the input dialog of a
    chatbot. Using the weights of an RBM as features constituted the first step. We
    saw that we could use neural networks to extract features from datasets and represent
    them using the optimized weights.
  prefs: []
  type: TYPE_NORMAL
- en: Processing the likes/dislikes of a movie viewer reveals the features of the
    movies that, in turn, provide a mental representation of a marketing segment.
  prefs: []
  type: TYPE_NORMAL
- en: PCA chained to an RBM will generate a vector space that can be viewed in TensorBoard
    Embedding Projector in a few clicks.
  prefs: []
  type: TYPE_NORMAL
- en: Once TensorBoard was set up, we analyzed the statistics to understand the marketing
    segment the dataset originated from. By listing the points per feature, we found
    the main features that drove this marketing segment.
  prefs: []
  type: TYPE_NORMAL
- en: Having discovered some of the key features of the marketing segment we were
    analyzing, we can now move on to the next chapter and start building a chatbot
    for the viewers. At the same time, we will keep backdoors available in case the
    dialogs show that we need to fine-tune our feature vector statistics.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RBMs are based on directed graphs. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The hidden units of an RBM are generally connected to one another. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Random sampling is not used in an RBM. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: PCA transforms data into higher dimensions. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In a covariance matrix, the eigenvector shows the direction of the vector representing
    that matrix, and the eigenvalue shows the size of that vector. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is impossible to represent a human mind in a machine. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A machine cannot learn concepts, which is why classical applied mathematics
    is enough to make efficient artificial intelligence programs for every field.
    (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more on RBMs, refer to: [https://skymind.ai/wiki/restricted-boltzmann-machine](https://skymind.ai/wiki/restricted-boltzmann-machine)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The original reference site for the source code in this chapter can be found
    here: [https://github.com/echen/restricted-boltzmann-machines/blob/master/README.md](https://github.com/echen/restricted-boltzmann-machines/blob/master/README.md)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The original Geoffrey Hinton paper can be found here: [http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf](http://www.cs.toronto.edu/~hinton/absps/guideTR.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For more on PCA, refer to this link: [https://www.sciencedirect.com/topics/engineering/principal-component-analysis](https://www.sciencedirect.com/topics/engineering/principal-component-analysis)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ready-to-use RBM resources are located here: [https://pypi.org/project/pydbm/](https://pypi.org/project/pydbm/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
