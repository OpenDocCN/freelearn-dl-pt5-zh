- en: '*Chapter 18*: Challenges and Future Directions in Reinforcement Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this last chapter, we will summarize the end of our journey in this book:
    you have done a lot, so think of this as a celebration as well as a bird''s-eye
    view of your achievement. On the other hand, when you take your learnings to use
    **reinforcement learning** (**RL**) in real-world problems, you will likely encounter
    multiple challenges. After all, deep RL is still a fast-moving field making a
    lot of progress in solving these challenges. We have already mentioned most of
    them in the book and proposed solution approaches. We will briefly recap them
    and talk about additional future directions for RL. Finally, we will go over some
    resources and strategies for you to become an RL expert, which you are very well
    positioned for by coming this far.'
  prefs: []
  type: TYPE_NORMAL
- en: 'So, here is what we will cover in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: What you have achieved with this book
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges and future directions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suggestions for aspiring RL experts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Final words
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What you have achieved with this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'First of all, congratulations! You have come a long way beyond the fundamentals,
    acquiring the skills and the mindset to apply RL in the real world. Here is what
    we have done together so far:'
  prefs: []
  type: TYPE_NORMAL
- en: We spent a fair amount of time on bandit problems, which have a tremendous number
    of applications, not just in industry but also in academia as an auxiliary tool
    to solve other problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We went deeper into the theory than a typical applied book would to strengthen
    your foundations of RL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We covered many of the algorithms and architectures behind the most successful
    applications of RL.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We discussed advanced training strategies to get the most out of the advanced
    RL algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We did hands-on work with realistic case studies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Throughout this journey, we both implemented our versions of some of the algorithms,
    as well as utilized libraries, such as Ray and RLlib, which power many teams and
    platforms at the top tech companies for their RL applications.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You absolutely deserve to take a moment to celebrate your success!
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, once you are back, it is time to talk about what is ahead of you. RL is
    at the beginning of its rise. This means multiple things:'
  prefs: []
  type: TYPE_NORMAL
- en: First, it is an opportunity. You are now ahead of the game by making this investment
    and coming this far.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Second, since this is cutting edge, there are many challenges to be solved before
    RL becomes a mature, easy-to-use technology.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the next section, we will discuss what those challenges are. That way, you
    will recognize them when you see them, know that you are not alone, and can set
    your expectations accordingly in terms of what is needed (data, time, compute
    resource, and so on) to solve your problem. But you shouldn''t worry! RL is a
    very active and accelerating area of research, so our arsenal to tackle those
    challenges is getting stronger by the day. See the number of papers submitted
    to the NeurIPS conference on RL over the years, compiled and presented by Katja
    Hofmann, a prominent RL researcher, during the conference in 2019:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.1 – Number of RL contributions to the NeurIPS conference, compiled
    and presented by Katja Hofmann (source: Hofmann, 2019)'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_18_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 18.1 – Number of RL contributions to the NeurIPS conference, compiled
    and presented by Katja Hofmann (source: Hofmann, 2019)'
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, while we talk about the challenges, we also talk about the related
    research directions, so you will know where to look for the answers.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges and future directions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You may be wondering why we are back to talking about RL challenges after finishing
    an advanced-level book on this topic. Indeed, throughout the book, we presented
    many approaches to mitigate them. On the other hand, we cannot claim these challenges
    are solved. So, it is important to call them out and discuss future directions
    for each to provide you with a compass to navigate them. Let''s start our disccusion
    with one of the most important challenges: sample efficiency.'
  prefs: []
  type: TYPE_NORMAL
- en: Sample efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As you are now well aware, it takes a lot of data to train an RL model. OpenAI
    Five, which became a world-class player in the strategy game Dota 2, took 128,000
    CPUs and GPUs to train, over many months, collecting a total of 900 years' worth
    of game experience **per day** (OpenAI, 2018). RL algorithms are benchmarked on
    their performances after they are trained on over 10 billion Atari frames (Kapturowski,
    2019). This is certainly a lot of compute and resources just to play games. So,
    sample efficiency is one of the biggest challenges that face real-world RL applications.
  prefs: []
  type: TYPE_NORMAL
- en: Let's discuss the overall directions to mitigate this problem.
  prefs: []
  type: TYPE_NORMAL
- en: Sample-efficient algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An obvious direction is to try to create algorithms that are more sample efficient.
    Indeed, there is a big push in the research community to this end. We will increasingly
    compare the algorithms not just based on their best possible performance but also
    on how quickly and efficiently they reach those performance levels.
  prefs: []
  type: TYPE_NORMAL
- en: 'To this end, we will likely talk more and more about the following algorithm
    classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Off-policy methods**, which don''t require data to be collected under the
    most recent policy, giving them an edge over policy methods in terms of sample
    efficiency.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model-based methods**, which can be orders of magnitude more efficient than
    their model-free counterparts by leveraging the information they possess on environment
    dynamics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Models with informed priors**, which limit the hypothesis space of the models
    to a plausible set. Examples of this class use neural ordinary differential equations
    and Lagrangian neural networks in RL models (Du, 2020; Shen, 2020).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Specialized hardware and software architectures for distributed training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can expect the progress on the algorithmic frontier to be gradual and slow.
    For those of us who are excited and impatient enthusiasts, a quicker solution
    is to dump more compute resources into RL projects, get the most out of the existing
    ones. and train bigger and bigger models. So, it is only reasonable to expect
    what happened in the **natural language processing** (**NLP**) space to happen
    to RL too: NLP models went from being 8billion-parameter models to 17billion,
    then to 175billion in size with OpenAI''s GPT-3 in less than a year, thanks to
    the optimizations in training architectures and, of course, the supercomputers
    dedicated to the task:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 18.2 – Growth in the biggest NLP model sizes. The vertical axis is
    the number of parameters. Image modified from Rosset, 2020'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_18_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 18.2 – Growth in the biggest NLP model sizes. The vertical axis is the
    number of parameters. Image modified from Rosset, 2020
  prefs: []
  type: TYPE_NORMAL
- en: In addition, innovations in RL training architectures, such as in Ape-X and
    SEED RL (*Espeholt, 2019*), help existing RL algorithms to run more efficiently,
    a direction we can expect to see more progress in.
  prefs: []
  type: TYPE_NORMAL
- en: Machine teaching
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Machine teaching approaches, such as curriculum learning, reward shaping, and
    demonstration learning, aim to infuse context and expertise into RL training.
    They often lead to significant sample efficiency during training and in some cases
    are needed to make learning even more feasible. Machine teaching approaches will
    become increasingly popular in the near future to increase the sample efficiency
    in RL.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-task/meta/transfer learning
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since training an RL model from scratch could be very expensive, it only makes
    sense to reuse the models that are trained on other relevant tasks. Today, when
    we want to develop an application that involves image classification, for example,
    it is rare that we train a model from scratch. Instead, we use one of the pre-trained
    models and fine-tune it for our application.
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: 'The ONNX Model Zoo is a collection of pre-trained, state-of-the-art models
    in an open-standard format for popular tasks such as image classification and
    machine translation, which I highly recommend you take a look at: [https://github.com/onnx/models](https://github.com/onnx/models).'
  prefs: []
  type: TYPE_NORMAL
- en: We can expect to see similar repositories for RL tasks as well. On a relevant
    note, approaches such as multi-task learning, which is about training models for
    more than one task, and meta-learning, which is about training models that can
    be efficiently transferred to new tasks, will gain momentum and broader adaptation
    among RL researchers and practitioners.
  prefs: []
  type: TYPE_NORMAL
- en: RL-as-a-service
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: If you need to translate texts in your app programmatically, one approach is,
    as we mentioned, to use a pre-trained model. But what if you don't want to invest
    in maintaining these models? The answer is often to buy it as a service from companies
    such as Microsoft, Google, and Amazon. These companies have access to huge amounts
    of data and compute resources, and they constantly upgrade their models using
    cutting-edge machine learning approaches. Doing the same could be a daunting,
    even infeasible, or simply impractical undertaking for other businesses without
    the same resources and focus. We can expect to see RL-as-a-service trend in the
    industry.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sample efficiency is a tough nut to crack, but there are developments in multiple
    frontiers to achieve it, as we have summarized. Next, let''s talk about another
    major challenge: the need for good simulation models.'
  prefs: []
  type: TYPE_NORMAL
- en: Need for high-fidelity and fast simulation models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the biggest barriers in the way of the broad adaptation of RL in industry
    is the absence of simulations of processes that companies are interested in optimizing,
    either altogether or at sufficient fidelity. Creating these simulation models
    often requires a lot of investment, and in some complex tasks, the fidelity of
    a simulation model won't be high enough for a typical RL approach. To overcome
    these challenges, we can expect to see more developments in the following areas.
  prefs: []
  type: TYPE_NORMAL
- en: Offline RL to learn directly from data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Although most processes in industry don't have simulation models, it is much
    more common to have logs describing what happened in it. Offline RL approaches,
    which aim to learn policies directly from data, will become of higher and higher
    importance as RL finds its way into real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: Approaches to deal with generalization, partial observability, and non-stationarity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Even in cases where a simulation model of a process exists, it is quite rare
    for such a model to be of enough fidelity to naively train an RL model that would
    work in the real-world without additional considerations. This sim2real gap can
    be thought of as a form of partial observability, which is often handled through
    memory in the RL model architecture. Combined with generalization techniques,
    such as domain randomization and regularization, we are already seeing very successful
    applications where policies trained in a simulation are transferred to the real
    world. Dealing with non-stationarity is also closely related to the generalization
    capabilities of RL algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Online learning and fine-tuning on the edge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the important capabilities that will enable the successful use of RL
    approaches will be to be able to continue training after the deployment of a model
    on the edge. With that, we will be able to fine-tune the models that are trained
    on a simulation with actual data. In addition, this capability will help RL policies
    to adapt to the changing conditions in the environment.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, we will witness a shift from RL being a tool to use in video games
    to an alternative to the traditional control and decision-making methods, which
    will be facilitated by approaches that will remove the dependency on high-fidelity
    simulations.
  prefs: []
  type: TYPE_NORMAL
- en: High-dimensional action spaces
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: CartPole, the iconic testbed for RL, has only a few elements in its action space,
    like in most RL environments used in RL research. Real-life problems, however,
    can be quite complex in terms of the number of available actions to the agent,
    which also often depends on the state the agent is in. Approaches such as action
    embeddings, action masking, and action elimination will come in handy to tackle
    this challenge in such realistic scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Reward function fidelity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Crafting the right reward function that leads the agent to the desired behavior
    is a notoriously difficult undertaking in RL. Inverse RL approaches, which learn
    the reward from demonstrations, and curiosity-driven learning, which relies on
    intrinsic rewards, are promising methods to reduce the dependency on hand-engineering
    the reward function.
  prefs: []
  type: TYPE_NORMAL
- en: The challenge with reward function engineering is exacerbated when there are
    multiple and qualitative objectives. There is growing literature on multi-objective
    RL approaches that either deal with each subproblem individually or produce policies
    for a given mixture of objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Another challenge with respect to reward signals in RL environments is the delay
    and sparsity of rewards. For example, a marketing strategy controlled by an RL
    agent might observe rewards days, weeks, or even months after the action is taken.
    Approaches that deal with causality and credit assignment in RL are critical to
    be able to leverage RL in these environments.
  prefs: []
  type: TYPE_NORMAL
- en: These are all important branches to keep an eye on since real-world RL problems
    rarely have well-defined, dense, and scalar objectives.
  prefs: []
  type: TYPE_NORMAL
- en: Safety, behavior guarantees, and explainability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When training RL agents in a computer simulation, it is okay, in fact, needed,
    to try random and crazy actions to figure out better policies. For an RL model
    competing against world-class players in a board game, the worst scenario that
    can happen is to lose the game, perhaps embarrassingly. The risks are of a different
    category when an RL agent is in charge of a chemical process or a self-driving
    car. These are safety-critical systems where the room for error is little to none.
    In fact, this is one of the biggest disadvantages of RL methods compared to traditional
    control theory approaches that often come with theoretical guarantees and a solid
    understanding of the expected behavior. Research on constrained RL and safe exploration
    is, therefore, crucial to be able to use RL in such systems.
  prefs: []
  type: TYPE_NORMAL
- en: Even when the system is not safety-critical, such as in inventory replenishment
    problems, a related challenge is the explainability of the actions suggested by
    the RL agent in charge. Experts who oversee the decisions in these processes often
    demand explanations, especially when the suggested actions are counterintuitive.
    People tend to trade accuracy for explanation, which puts black-box approaches
    at a disadvantage. Deep learning has come a long way in terms of explainability,
    and RL will surely benefit from it. On the other hand, this will be an ongoing
    challenge for machine learning at large.
  prefs: []
  type: TYPE_NORMAL
- en: Reproducibility and sensitivity to hyperparameter choices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is one thing to train an RL model with the close oversight and guidance of
    many experts for a specific task and after many iterations, yet it is another
    thing to deploy multiple RL models in production for various environments, which
    are to be re-trained periodically and hands-off-the-wheel as new data come in.
    The consistency and resiliency of RL algorithms in terms of producing successful
    policies under a variety of conditions will be an increasingly important factor
    when benchmarking them for the research community, as well as for the practitioners
    who will get to deal with these models and their maintenance in real life.
  prefs: []
  type: TYPE_NORMAL
- en: Robustness and adversarial agents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Deep learning is known to be brittle about its representations. This lack of
    robustness allows adversarial agents to manipulate systems that rely on deep learning.
    This is a major concern and a very active area of research in the machine learning
    community. RL will surely piggyback on the developments in the broader machine
    learning research to address robustness issues in this field.
  prefs: []
  type: TYPE_NORMAL
- en: These challenges in RL are important to be aware of, especially for practitioners
    who want to use these tools to solve real-world problems, and this recap will
    hopefully help with that. We covered many of the solution approaches in the book
    and mentioned the overall directions in this section as well, so you know where
    to look for solutions. All of these are active areas of research, so whenever
    you encounter these challenges, it is a good idea to take a fresh look at the
    literature.
  prefs: []
  type: TYPE_NORMAL
- en: Before we wrap up, I would like to offer my two cents to aspiring RL experts.
  prefs: []
  type: TYPE_NORMAL
- en: Suggestions for aspiring RL experts
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is designed for an audience who already know the fundamentals of RL.
    Now that you have finished this book too, you are well-positioned to be an expert
    in RL. Having said that, RL is a big topic, and this book is really meant to be
    a compass and kickstarter for you. At this point, if you decide to go deeper to
    become an RL expert, I have some suggestions.
  prefs: []
  type: TYPE_NORMAL
- en: Go deeper into the theory
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In machine learning, models often fail to produce the expected level of results,
    at least at the beginning. One big factor that will help you pass these obstacles
    is to have a good foundation for the math behind the algorithms you are using
    to solve your problems. This will help you better understand the limitations and
    assumptions of those algorithms and identify whether they conflict with the realities
    of the problem at hand. To this end, here is some advice:'
  prefs: []
  type: TYPE_NORMAL
- en: It is never a bad idea to deepen your understanding of probability and statistics.
    Don't forget that all these algorithms are essentially statistical models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A solid understanding of the basic ideas in RL, such as Q-learning and the
    Bellman equation, is critical to have a good foundation to build modern RL on.
    This book serves this purpose to some extent. However, I highly recommend you
    read, multiple times, Rich Sutton and Andrew Barto''s book *Reinforcement Learning:
    An Introduction*, which is essentially the Bible of traditional RL.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Professor Sergey Levine's UC Berkeley course on deep RL, which this book benefited
    greatly from, is an excellent resource to go deeper into RL. This course is available
    at [http://rail.eecs.berkeley.edu/deeprlcourse/](http://rail.eecs.berkeley.edu/deeprlcourse/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another great resource, specific to multi-tasking and meta-learning, is Professor
    Chelsea Finn's Stanford course at [https://cs330.stanford.edu/](https://cs330.stanford.edu/).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Deep RL Bootcamp taught by the experts in the field is another excellent
    resource: [https://sites.google.com/view/deep-rl-bootcamp/home](https://sites.google.com/view/deep-rl-bootcamp/home).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you go through these resources, and refer back to them from time to time,
    you will notice your understanding of the topic becomes much deeper.
  prefs: []
  type: TYPE_NORMAL
- en: Follow good practitioners and research labs
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are excellent research labs focusing on RL, who also publish their findings
    in detailed blog posts that contain a lot of theoretical and practical insights.
    Here are some examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'OpenAI blog: [https://openai.com/blog/](https://openai.com/blog/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DeepMind blog: [https://deepmind.com/blog](https://deepmind.com/blog)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Berkeley AI Research** (**BAIR**) blog: [https://bair.berkeley.edu/blog](https://bair.berkeley.edu/blog)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microsoft Research RL group: [https://www.microsoft.com/en-us/research/theme/reinforcement-learning-group/](https://www.microsoft.com/en-us/research/theme/reinforcement-learning-group/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Google AI blog: [https://ai.googleblog.com/](https://ai.googleblog.com/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even if you don't read every single post, it is a good idea to monitor them
    regularly to stay synced with the trends of RL research.
  prefs: []
  type: TYPE_NORMAL
- en: Learn from papers and their good explanations
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A single year in AI research is like a year in dog years: a lot happens in
    it. So, the best way to stay up to date is really to follow the research in the
    area. This will also expose you to the theory and rigorous explanations of the
    methods. Now, there are two challenges that come with this:'
  prefs: []
  type: TYPE_NORMAL
- en: There are a ton of papers published every year, which makes it impossible to
    read them all.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It could be daunting to read equations and proofs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To address the former, I suggest you focus on papers accepted to conferences
    such as NeurIPS, ICLR, ICML, and AAAI. This will still amount to a lot, so you
    will still have to develop your own thresholds about what to read.
  prefs: []
  type: TYPE_NORMAL
- en: 'To address the latter, you can check whether there are good blog posts explaining
    the papers you would like to understand better. Some high-quality blogs (not specific
    to RL) to follow are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Lilian Weng''s blog: [https://lilianweng.github.io/lil-log/](https://lilianweng.github.io/lil-log/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Distill: [https://distill.pub/](https://distill.pub/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The Gradient: [https://thegradient.pub/](https://thegradient.pub/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Adrian Colyer''s The Morning Paper: [https://blog.acolyer.org/](https://blog.acolyer.org/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jay Alammar''s blog: [http://jalammar.github.io/](http://jalammar.github.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Christopher Olah''s blog (who is also in the Distill team): [https://colah.github.io/](https://colah.github.io/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jian Zhang''s blog: [https://medium.com/@jianzhang_23841](mailto:https://medium.com/@jianzhang_23841)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: I have personally learned a lot from these blogs, and still continue to learn
    from them.
  prefs: []
  type: TYPE_NORMAL
- en: Stay up to date with trends in other fields of deep learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most major developments in deep learning, such as the Transformer architecture,
    take only months to find their way into RL. Therefore, staying up to date with
    major trends in the broader machine learning and deep learning research will help
    you predict what is upcoming for RL. The blogs we listed in the previous section
    are a good way to follow these trends.
  prefs: []
  type: TYPE_NORMAL
- en: Read open source repos
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At this point, there are just too many algorithms in RL to explain line by
    line in a book. So, at some point, you need to develop that literacy and directly
    read good implementations of these algorithms. Here are my suggestions:'
  prefs: []
  type: TYPE_NORMAL
- en: The OpenAI Spinning Up website, [https://spinningup.openai.com/](https://spinningup.openai.com/),
    and repo, [https://github.com/openai/spinningup](https://github.com/openai/spinningup)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI Baselines: [https://github.com/openai/baselines](https://github.com/openai/baselines)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Stable Baselines: [https://github.com/hill-a/stable-baselines](https://github.com/hill-a/stable-baselines)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'DeepMind OpenSpiel: [https://github.com/deepmind/open_spiel](https://github.com/deepmind/open_spiel)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ray and RLlib: [https://github.com/ray-project/ray](https://github.com/ray-project/ray)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The *Hands-On* *Deep Reinforcement Learning* book repo by Maxim Lapan: [https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On](https://github.com/PacktPublishing/Deep-Reinforcement-Learning-Hands-On)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition to these, many papers now come with their own repos, as we used
    in some chapters in this book. There is a very nice website, [https://paperswithcode.com/](https://paperswithcode.com/),
    which you can use to identify such papers.
  prefs: []
  type: TYPE_NORMAL
- en: Practice!
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regardless of how much you read, you will truly learn only by practicing. So,
    try to get your hands dirty wherever possible. This could be through reproducing
    RL papers and algorithms, or even better, doing your own RL projects. The benefit
    you will get by going into the guts of an implementation cannot be replaced by
    anything else.
  prefs: []
  type: TYPE_NORMAL
- en: I hope this set of resources is helpful to you. To be clear, this is a lot to
    consume. It will take time to go over these, so set your targets realistically.
    Moreover, you will have to be selective about what to read and follow, a habit
    that you will develop over time.
  prefs: []
  type: TYPE_NORMAL
- en: Final words
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Well, it is time to wrap up. I would like to thank you for investing your time
    and effort into reading this book. I hope it has been beneficial for you. As a
    last word, I would like to emphasize that getting good at something takes a long
    time and there is no limit to how good you can become. Nobody is an expert at
    everything, even for subdisciplines such as RL or computer vision. you need to
    run. Consistency and continuity of your efforts will make the difference, no matter
    what your goal is. I wish you the best on this journey.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Hofmann, K. (2019). *Reinforcement Learning: Past, Present, and Future Perspectives*.
    Conference on Neural Information Processing Systems, Vancouver, Canada. URL: [https://slideslive.com/38922817/reinforcement-learning-past-present-and-future-perspectives](https://slideslive.com/38922817/reinforcement-learning-past-present-and-future-perspectives)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI (2018). *OpenAI Five*. OpenAI Blog. URL: [https://openai.com/blog/openai-five/](https://openai.com/blog/openai-five/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kapturowski, S., Ostrovski, G., Dabney, W., Quan, J., & Munos R. (2019). *Recurrent
    Experience Replay in Distributed Reinforcement Learning*. In International Conference
    on Learning Representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Espeholt, L., Marinier, R., Stanczyk, P., Wang, K., & Michalski, M. (2019).
    *SEED RL: Scalable and Efficient Deep-RL with Accelerated Central Inference*.
    arXiv.org, [http://arxiv.org/abs/1910.06591](http://arxiv.org/abs/1910.06591)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du, J., Futoma, J., & Doshi-Velez, F. (2020). *Model-based Reinforcement Learning
    for Semi-Markov Decision Processes with Neural ODEs*. arXiv.org, [https://arxiv.org/abs/2006.16210](https://arxiv.org/abs/2006.16210)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Shen, P. (2020). *Neural ODE for Differentiable Reinforcement Learning and
    Optimal Control: Cartpole Problem Revisited*. The startup. URL: [https://bit.ly/2RROQi3](https://bit.ly/2RROQi3)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosset, C. (2020). *Turing-NLG: A 17-billion-parameter language model by Microsoft*.
    Microsoft Research blog. URL: [https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/](https://www.microsoft.com/en-us/research/blog/turing-nlg-a-17-billion-parameter-language-model-by-microsoft/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dulac-Arnold, G., et al. (2020). *An Empirical Investigation of the Challenges
    of Real-World Reinforcement Learning*. arXiv:2003.11881 [Cs]. arXiv.org, [http://arxiv.org/abs/2003.11881](http://arxiv.org/abs/2003.11881)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dulac-Arnold, G., et al. (2019). *Challenges of Real-World Reinforcement Learning*.
    arXiv:1904.12901 [Cs, Stat]. arXiv.org, [http://arxiv.org/abs/1904.12901](http://arxiv.org/abs/1904.12901)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Irpan, A. (2018). *Deep Reinforcement Learning Doesn't Work Yet*. [http://www.alexirpan.com/2018/02/14/rl-hard.html](http://www.alexirpan.com/2018/02/14/rl-hard.html)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Levine, S. (2019). Deep Reinforcement Learning – CS285 Fa19 11/18/19, YouTube,
    [https://youtu.be/tzieElmtAjs?t=3336](https://youtu.be/tzieElmtAjs?t=3336). Accessed
    26 September 2020.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hoffmann, K. et al. (2020). *Challenges & Opportunities in Lifelong Reinforcement
    Learning*. ICML 2020\. URL: [https://slideslive.com/38930956/challenges-opportunities-in-lifelong-reinforcement-learning?ref=speaker-16425-latest](https://slideslive.com/38930956/challenges-opportunities-in-lifelong-reinforcement-learning?ref=speaker-16425-latest)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
