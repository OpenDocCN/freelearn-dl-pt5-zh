<html><head></head><body>
		<div>
			<div id="_idContainer085" class="Content">
			</div>
		</div>
		<div id="_idContainer086" class="Content">
			<h1 id="_idParaDest-77">3. <a id="_idTextAnchor085"/>Image Classification with Convolutional Neural Networks (CNNs)</h1>
		</div>
		<div id="_idContainer116" class="Content">
			<p class="callout-heading"><a id="_idTextAnchor086"/>Introduction</p>
			<p class="callout">In this chapter, we will study <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) and image classification. First, we will be introduced to the architecture of CNNs and how to implement them. We will then get hands-on experience of using TensorFlow to develop image classifiers. Finally, we will cover the concepts of transfer learning and fine-tuning and see how we can use state-of-the-art algorithms. </p>
			<p class="callout">By the end of this chapter, you will have a good understanding of what CNNs are and how programming with TensorFlow works. </p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor087"/>Introduction</h1>
			<p>In the previous chapters, we learned about traditional neural networks and a number of models, such as the perceptron. We learned how to train such models on structured data for regression or classification purposes. Now, we will learn how we can extend their application to the computer vision field.</p>
			<p>Not so long ago, computers were perceived as computing engines that could only process well-defined and logical tasks. Humans, on the other hand, are more complex since we have five basic senses that help us see things, hear noises, feel things, taste foods, and smell odors. Computers were only calculators that could operate large volumes of logical operations, but they couldn't deal with complex data. Compared to the abilities of humans, computers had very clear limitations.</p>
			<p>There were some rudimentary attempts to “<em class="italic">give sight</em>" to computers by processing and analyzing digital images. This field is called computer vision. But it was not until the advent of deep learning that we saw some incredible improvements and results. Nowadays, the field of computer vision has advanced to such an extent that, in some cases, computer vision AI systems are able to process and interpret certain types of images faster and more accurately than humans. You may have heard about the experiment where a group of 15 doctors in China competed against a deep learning system from the company BioMind AI for recognizing brain tumors from X-rays. The AI system took 15 minutes to accurately predict 87% of the 225 input images, while it took 30 minutes for the medical experts to achieve a score of 66% on the same pool of images. </p>
			<p>We've all heard about self-driving cars that can automatically make the right decisions depending on traffic conditions or drones that can detect sharks and automatically send alerts to lifeguards. All these amazing applications are possible thanks to the recent development of CNNs.</p>
			<p>Computer vision can be split into four different domains:</p>
			<ul>
				<li><strong class="bold">Image classification</strong>, where we need to recognize the main object in an image.</li>
				<li><strong class="bold">Image classification and localization</strong>, where we need to recognize and localize the main object in an image with a bounding box.</li>
				<li><strong class="bold">Object detection</strong>, where we need to recognize multiple objects in an image with bounding boxes.</li>
				<li><strong class="bold">Image segmentation</strong>, where we need to identify the boundaries of objects in an image.</li>
			</ul>
			<p>The following figure shows the difference between the four domains:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B15385_03_01.jpg" alt="Figure 3.1: Difference between the four domains of computer vision&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1: Difference between the four domains of computer vision</p>
			<p>In this chapter, we will only look at image classification, which is the most widely used application of CNN. This includes things such as car plate recognition, automatic categorization of the pictures taken with your mobile phone, or creating metadata used by search engines on databases of images.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If you're reading the print version of this book, you can download and browse the color versions of some of the images in this chapter by visiting the following link: <a href="https://packt.live/2ZUu5G2 ">https://packt.live/2ZUu5G2</a></p>
			<h1 id="_idParaDest-79">D<a id="_idTextAnchor088"/>igital Images</h1>
			<p>Humans can see through their eyes by transforming light into electrical signals that are then processed by the brain. But computers do not have physical eyes to capture light. They can only process information in digital forms composed of bits (0 or 1). So, to be able to “see", computers require a digitized version of an image. </p>
			<p>A digital image is formed by a two-dimensional matrix of pixels. For a grayscale image, each of these pixels can take a value between 0 and 255 that represents its intensity or level of gray. A digital image can be composed of one channel for a black and white image or three channels (red, blue, and green) for a color image:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B15385_03_02.jpg" alt="Figure 3.2: Digital representation of an image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2: Digital representation of an image</p>
			<p>A digital image is characterized by its dimensions (height, width, and channel):</p>
			<ul>
				<li><strong class="bold">Height:</strong> How many pixels there are on the vertical axis.</li>
				<li><strong class="bold">Width:</strong> How many pixels there are on the horizontal axis.</li>
				<li><strong class="bold">Channel:</strong> How many channels there are. If there is only one channel, an image will be in grayscale. If there are three channels, the image will be colored.</li>
			</ul>
			<p>The following digital image has dimensions (512, 512, 3).</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B15385_03_03.jpg" alt="Figure 3.3: Dimensions of a digital image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3: Dimensions of a digital image</p>
			<h1 id="_idParaDest-80">Ima<a id="_idTextAnchor089"/>ge Processing</h1>
			<p>Now that we know how a digital image is represented, let's discuss how computers can use this information to find patterns that will be used to classify an image or localize objects. So, in order to get any useful or actionable information from an image, a computer has to resolve an image into a recognizable or known pattern. As for any machine learning algorithm, computer vision needs some features in order to learn patterns. </p>
			<p>Unlike structured data, where each feature is well defined in advance and stored in separate columns, images don't follow any specific pattern. It is impossible to say, for instance, that the third line will always contain the eye of an animal or that the bottom left corner will always represent a red, round-shaped object. Images can be of anything and don't follow any structure. This is why they are considered to be unstructured data.</p>
			<p>However, images do contain features. They contain different shapes (lines, circles, rectangles, and so on), colors (red, blue, orange, yellow, and so on), and specific characteristics related to different types of objects (hair, wheel, leaves, and so on). Our eyes and brain can easily analyze and interpret all these features and identify objects in images. Therefore, we need to simulate the same analytical process for computers. This is where <strong class="bold">image filters</strong> (also called kernels) come into play.</p>
			<p>Image filters are small matrices specialized in detecting a defined pattern. For instance, we can have a filter for detecting vertical lines only and another one only for horizontal lines. Computer vision systems run such filters in every part of the image and generate a new image with the detected patterns highlighted. These kinds of generated images are called <strong class="bold">feature maps</strong>. An example of a feature map where an edge-detection filter is used is shown in the following figure:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B15385_03_04.jpg" alt="Figure 3.4: Example of a vertical edge feature map&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4: Example of a vertical edge feature map</p>
			<p>Such filters are widely used in image processing. If you've used Adobe Photoshop before (or any other image processing tool), you will have most likely used filters such as <em class="italic">Gaussian</em> and <em class="italic">Sharpen</em>. </p>
			<h2 id="_idParaDest-81">Conv<a id="_idTextAnchor090"/>olution Operations</h2>
			<p>Now that we know the basics of image processing, we can start our journey with CNNs. As we mentioned previously, computer vision relies on applying filters to an image to recognize different patterns or features and generate feature maps. But how are these filters applied to the pixels of an image? You could guess that there is some sort of mathematical operation behind it, and you would be absolutely right. This operation is called convolution.</p>
			<p>A convolution operation is composed of two stages:</p>
			<ul>
				<li>An element-wise product of two matrices</li>
				<li>A sum of the elements of a matrix</li>
			</ul>
			<p>Let's look at an example of how to convolute two matrices, A and B:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B15385_03_05.jpg" alt="Figure 3.5: Examples of matrices&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5: Examples of matrices</p>
			<p>First, we need to perform an element-wise multiplication with matrices A and B. We will get another matrix, C, as a result, with the following values:</p>
			<ul>
				<li>1st row, 1st column: 5 × 1 = 5</li>
				<li>1st row, 2nd column: 10 × 0 = 0</li>
				<li>1st row, 3rd column: 15 × (-1) = -15</li>
				<li>2nd row, 1st column: 10 × 2 = 20</li>
				<li>2nd row, 2nd column: 20 × 0 = 0</li>
				<li>2nd row, 3rd column: 30 × (-2) = -60</li>
				<li>3rd row, 1st column: 100 × 1 = 100</li>
				<li>3rd row, 2nd column: 150 × 0 = 0</li>
				<li>3rd row, 3rd column: 200 × (-1) = -200<p class="callout-heading">Note</p><p class="callout">An element-wise multiplication is different from a standard matrix multiplication, which operates at the row and column level rather than on each element. </p></li>
			</ul>
			<p>Finally, we just have to perform a sum on all elements of matrix C, which will give us the following:</p>
			<p>5+0-15+20+0-60+100+0-200 = -150</p>
			<p>The final result of the entire convolution operation on matrices A and B is -150, as shown in the following diagram:</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B15385_03_06.jpg" alt="Figure 3.6: Sequence of the convolution operation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6: Sequence of the convolution operation</p>
			<p>In this example, Matrix B is actually a filter (or kernel) called Sobel that is used for detecting vertical lines (there is also a variant for horizontal lines). Matrix A will be a portion of an image with the same dimensions as the filter (this is mandatory in order to perform element-wise multiplication).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">A filter is, in general, a square matrix such as (3,3) or (5,5).</p>
			<p>For a CNN, filters are actually parameters that will be learned (that is, defined) during the training process. So, the values of each filter that will be used will be set by the CNN itself. This is an important concept to go through before we learn how to train a CNN.</p>
			<h2 id="_idParaDest-82">Exerci<a id="_idTextAnchor091"/>se 3.01: Implementing a Convolution Operation</h2>
			<p>In this exercise, we will use TensorFlow to implement a convolution operation on two matrices: <strong class="source-inline">[[1,2,3],[4,5,6],[7,8,9]]</strong> and <strong class="source-inline">[[1,0,-1],[1,0,-1],[1,0,-1]]</strong>. Perform the following steps to complete this exercise:</p>
			<ol>
				<li>Open a new Jupyter Notebook file and name it <strong class="source-inline">Exercise 3.01</strong>.</li>
				<li>Import the <strong class="source-inline">tensorflow</strong> library:<p class="source-code">import tensorflow as tf</p></li>
				<li>Create a tensor called <strong class="source-inline">A</strong> from the first matrix, <strong class="source-inline">([[1,2,3],[4,5,6],[7,8,9]])</strong>. Print its value:<p class="source-code">A = tf.Variable([[1, 2, 3], [4, 5, 6], [7, 8, 9]])</p><p class="source-code">A</p><p>The output will be as follows:</p><p class="source-code">&lt;tf.Variable 'Variable:0' shape=(3, 3) dtype=int32, </p><p class="source-code">numpy=array([[1, 2, 3],</p><p class="source-code">             [4, 5, 6],</p><p class="source-code">             [7, 8, 9]])&gt;</p></li>
				<li>Create a tensor called <strong class="source-inline">B</strong> from the first matrix, <strong class="source-inline">([[1,0,-1],[1,0,-1],[1,0,-1]])</strong>. Print its value:<p class="source-code">B = tf.Variable([[1, 0, -1], [1, 0, -1], [1, 0, -1]])</p><p class="source-code">B</p><p>The output will be as follows:</p><p class="source-code">&lt;tf.Variable 'Variable:0' shape=(3, 3) dtype=int32, </p><p class="source-code">numpy=array([[ 1,  0, -1],</p><p class="source-code">             [ 1,  0, -1],</p><p class="source-code">             [ 1,  0, -1]])&gt;</p></li>
				<li>Perform an element-wise multiplication on <strong class="source-inline">A</strong> and <strong class="source-inline">B</strong> using <strong class="source-inline">tf.math.multiply()</strong>. Save the result in <strong class="source-inline">mult_out</strong> and print it:<p class="source-code">mult_out = tf.math.multiply(A, B)</p><p class="source-code">mult_out</p><p>The expected output will be as follows:</p><p class="source-code">&lt;tf.Tensor: id=19, shape=(3, 3), dtype=int32, </p><p class="source-code">numpy=array([[ 1,  0, -3],</p><p class="source-code">             [ 4,  0, -6],</p><p class="source-code">             [ 7,  0, -9]])&gt;</p></li>
				<li>Perform an element-wise sum on <strong class="source-inline">mult_out</strong> using <strong class="source-inline">tf.math.reduce_sum()</strong>. Save the result in <strong class="source-inline">conv_out</strong> and print it:<p class="source-code">conv_out = tf.math.reduce_sum(mult_out)</p><p class="source-code">conv_out</p><p>The expected output will be as follows:</p><p class="source-code">&lt;tf.Tensor: id=21, shape=(), dtype=int32, numpy=-6&gt;</p><p>The result of the convolution operation on the two matrices, <strong class="source-inline">[[1,2,3],[4,5,6],[7,8,9]]</strong> and <strong class="source-inline">[[1,0,-1],[1,0,-1],[1,0,-1]]</strong>, is <strong class="source-inline">-6</strong>.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/320pEfC">https://packt.live/320pEfC</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2ZdeLFr">https://packt.live/2ZdeLFr</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we used the built-in functions of TensorFlow to perform a convolution operation on two matrices. </p>
			<h2 id="_idParaDest-83">Stride<a id="_idTextAnchor092"/></h2>
			<p>So far, we have learned how to perform a single convolution operation. We learned that a convolution operation uses a filter of a specific size, say, (3, 3), that is, 3 × 3, and applies it on a portion of the image of a similar size. If we have a large image, let's say of size (512, 512), then we can just look at a very tiny part of the image.</p>
			<p>Taking tiny parts of the image at a time, we need to perform the same convolution operation on the entire space of a given image. To do so, we will apply a technique called sliding. As the name implies, sliding is where we apply the filter to an adjacent area of the previous convolution operation: we just slide the filter and apply convolution.</p>
			<p>If we start from the top-left corner of an image, we can slide the filter by one pixel at a time to the right. Once we get to the right edge, we can slide down the filter by one pixel. We repeat this sliding operation until we've applied convolution to the entire space of the image:</p>
			<p> </p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B15385_03_07.jpg" alt="Figure 3.7: Example of stride&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7: Example of stride</p>
			<p>Rather than sliding by 1 pixel only, we can choose a bigger sliding window, such as 2 or 3 pixels. The parameter defining the value of this sliding window is called <strong class="bold">stride</strong>. With a bigger stride value, there will be fewer overlapping pixels, but the resulting feature map will have smaller dimensions, so you will be losing a bit of information.</p>
			<p>In the preceding example, we applied a Sobel filter on an image that has been split horizontally with dark values on the left-hand side and white ones on the right-hand side. The resulting feature map has high values (800) in the middle, which indicates that the Sobel filter found a vertical line in that area. This is how sliding convolution helps to detect specific patterns in an image.</p>
			<h2 id="_idParaDest-84">Padding<a id="_idTextAnchor093"/></h2>
			<p>In the previous section, we learned how a filter can go through all the pixels of an image with pixel sliding. Combined with the convolution operation, this process helps to detect patterns (that is, extract features) in an image.</p>
			<p>Applying a convolution to an image will result in a feature map that has smaller dimensions than the input image. A technique called padding can be used in order to get the exact same dimensions for the feature map as for the input image. It consists of adding a layer of pixels with a value of <strong class="source-inline">0</strong> to the edge:</p>
			<div>
				<div id="_idContainer094" class="IMG---Figure">
					<img src="image/B15385_03_08.jpg" alt="Figure 3.8: Example of padding&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8: Example of padding</p>
			<p>In the preceding example, the input image has the dimensions (6,6). Once padded, its dimensions increased to (8,8). Now, we can apply convolution on it with a filter of size (3,3):</p>
			<div>
				<div id="_idContainer095" class="IMG---Figure">
					<img src="image/B15385_03_09.jpg" alt="Figure 3.9: Example of padded convolution&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.9: Example of padded convolution</p>
			<p>The resulting image after convoluting the padded image is (6,6) in terms of its dimensions, which is the exact same dimensions as for the original input image. The resulting feature map has high values in the middle of the image, just like the previous example without padding. So, the filter can still find the same pattern in the image. But you may notice now that we have very low values (-800) on the left edge. This is actually fine as lower values mean the filter hasn't found any pattern in this area.</p>
			<p>The following formulas can be used for calculating the output dimensions of a feature map after a convolution:</p>
			<div>
				<div id="_idContainer096" class="IMG---Figure">
					<img src="image/B15385_03_10.jpg" alt="Figure 3.10: Formulas for calculating the output dimensions of a feature map&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.10: Formulas for calculating the output dimensions of a feature map</p>
			<p>Here, we have the following:</p>
			<ul>
				<li><strong class="source-inline">w</strong>: Width of the input image</li>
				<li><strong class="source-inline">h</strong>: Height of the input image</li>
				<li><strong class="source-inline">p</strong>: Number of pixels used on each side for padding</li>
				<li><strong class="source-inline">f</strong>: Filter size</li>
				<li><strong class="source-inline">s</strong>: Number of pixels in the stride</li>
			</ul>
			<p>Let's apply this formula to the preceding example:</p>
			<ul>
				<li><strong class="source-inline">w</strong> = 6</li>
				<li><strong class="source-inline">h</strong> = 6</li>
				<li><strong class="source-inline">p</strong> = 1</li>
				<li><strong class="source-inline">f</strong> = 3</li>
				<li><strong class="source-inline">s</strong> = 1</li>
			</ul>
			<p>Then, calculate the output dimensions as follows:</p>
			<div>
				<div id="_idContainer097" class="IMG---Figure">
					<img src="image/B15385_03_11.jpg" alt="Figure 3.11: Output – dimensions of the feature map &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.11: Output – dimensions of the feature map </p>
			<p>So, the dimensions of the resulting feature map are (6,6).</p>
			<h1 id="_idParaDest-85">Convolution<a id="_idTextAnchor094"/>al Neural Networks</h1>
			<p>In <em class="italic">Chapter 2</em>, <em class="italic">Neural Networks</em>, you learned about traditional neural networks, such as perceptrons, that are composed of fully connected layers (also called dense layers). Each layer is composed of neurons that perform matrix multiplication, followed by a non-linear transformation with an activation function.</p>
			<p>CNNs are actually very similar to traditional neural networks, but instead of using fully connected layers, they use convolutional layers. Each convolution layer will have a defined number of filters (or kernels) that will apply the convolution operation with a given stride on an input image with or without padding and can be followed by an activation function.</p>
			<p>CNNs are widely used for image classification, where the network will have to predict the right class for a given input. This is exactly the same as classification problems for traditional machine learning algorithms. If the output can only be from two different classes, it will be a <strong class="bold">binary classification</strong>, such as recognizing dogs versus cats. If the output can be more than two classes, it will be a <strong class="bold">multi-class classification</strong> exercise, such as recognizing 20 different sorts of fruits.</p>
			<p>In order to make such predictions, the last layer of a CNN model needs to be a fully connected layer with the relevant activation function according to the type of prediction problem. You can use the following list of activation functions as a rule of thumb:</p>
			<div>
				<div id="_idContainer098" class="IMG---Figure">
					<img src="image/B15385_03_12.jpg" alt="Figure 3.12: List of activation functions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12: List of activation functions</p>
			<p>To gain a better perspective of its structure, here's what a simple CNN model looks like: </p>
			<p> </p>
			<div>
				<div id="_idContainer099" class="IMG---Figure">
					<img src="image/B15385_03_13.jpg" alt="Figure 3.13: Structure of a simple CNN model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13: Structure of a simple CNN model</p>
			<p>We have learned a lot about CNNs already. There is one more concept we need to go through in order to reduce the training time of a CNN before jumping into our first exercise: pooling layers.</p>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor095"/>Pooling Layers</h1>
			<p>Pooling layers are used to reduce the dimensions of the feature maps of convolution layers. But why do we need to perform such downsampling? One of the main reasons is to reduce the number of calculations that are performed in the networks. Adding multiple layers of convolution with different filters can have a significant impact on the training time. Also, reducing the dimensions of feature maps can eliminate some of the noise in the feature map and help us focus only on the detected pattern. It is quite typical to add a pooling layer after each convolutional layer in order to reduce the size of the feature maps. </p>
			<p>A pooling operation acts very similarly to a filter, but rather than performing a convolution operation, it uses an aggregation function such as average or max (max is the most widely used function in the current CNN architecture). For instance, <strong class="bold">max pooling</strong> will look at a specific area of the feature map and find the maximum values of its pixels. Then, it will perform a stride and find the maximum value among the neighbor pixels. It will repeat this process until it processes the entire image:</p>
			<p> </p>
			<div>
				<div id="_idContainer100" class="IMG---Figure">
					<img src="image/B15385_03_14.jpg" alt="Figure 3.14: Max pooling with stride 2 on an input image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14: Max pooling with stride 2 on an input image</p>
			<p>In the preceding example, we used a max pooling (which is the most widely used function for pooling) of size (2, 2) and a stride of 2. We looked at the top-left corner of the feature map and found the maximum value among the pixels, 6, 8, 1, and 2, and got a result of 8. Then, we slid the max pooling by a stride of 2 and performed the same operation on the group of pixels, that is, 6, 1, 7, and 4. We repeated the same operation on the bottom groups and got a new feature map of size (2,2).</p>
			<p>A CNN model with max pooling will look like this:</p>
			<p> </p>
			<div>
				<div id="_idContainer101" class="IMG---Figure">
					<img src="image/B15385_03_15.jpg" alt="Figure 3.15: Example of the CNN architecture with max pooling&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15: Example of the CNN architecture with max pooling</p>
			<p>For instance, the preceding model can be used for recognizing handwritten digits (from 0 to 9). There are three convolution layers in this model, followed by a max pooling layer. The final layers are fully connected and are responsible for making the predictions of the digit that's been detected.</p>
			<p>The overhead of adding pooling layers is much less than computing convolution. This is why they will speed up the training time.</p>
			<h2 id="_idParaDest-87">CNNs with Tenso<a id="_idTextAnchor096"/>rFlow and Keras </h2>
			<p>So far, you've learned a lot about how CNN works under the hood. Now, it is finally time to see how we can implement what we have learned. We will be using the Keras API from TensorFlow 2.0.</p>
			<p>The Keras API provides a high-level API for building your own CNN architecture. Let's look at the main classes we will be using for CNN.</p>
			<p>First, to create a convolution layer, we will need to instantiate a <strong class="source-inline">Conv2D()</strong> class and specify the number of kernels, their size, the stride, padding, and activation function:</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">layers.Conv2D(64, kernel_size=(3, 3), stride=(2,2), \</p>
			<p class="source-code">              padding="same", activation="relu")</p>
			<p>In the preceding example, we have created a convolution layer with <strong class="source-inline">64</strong> kernels that are <strong class="source-inline">(3, 3)</strong> in dimension with a stride of <strong class="source-inline">2</strong>, a padding to get the same output dimension as the input (<strong class="source-inline">padding='same'</strong>), and ReLU as the activation function.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can learn more about this class by going to TensorFlow's documentation website: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2D</a></p>
			<p>In order to add a max pooling layer, you will have to use the <strong class="source-inline">MaxPool2D()</strong> class and specify its dimensions and stride, as shown in the following code snippet:</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">layers.MaxPool2D(pool_size=(3, 3), strides=1)</p>
			<p>In the preceding code snippet, we have instantiated a max pooling layer of size <strong class="source-inline">(3,3)</strong> with a stride of <strong class="source-inline">1</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can learn more about this class by going to TensorFlow's documentation website: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D">https://www.tensorflow.org/api_docs/python/tf/keras/layers/MaxPool2D</a></p>
			<p>For a fully connected layer, we will use the <strong class="source-inline">Dense()</strong> class and specify the number of units and the activation function:</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">layers.Dense(units=1, activation='sigmoid')</p>
			<p>The preceding code shows us how to create a fully connected layer that has <strong class="source-inline">1</strong> output unit and uses <strong class="source-inline">sigmoid</strong> as the activation function.</p>
			<p>Finally, while manipulating input data, we may have to change its dimensions before feeding it to a CNN model. If we are using NumPy arrays, we can use the <strong class="source-inline">reshape</strong> method (as seen in <em class="italic">Chapter 1</em>, <em class="italic">Building Blocks of Deep Learning</em>), as follows: </p>
			<p class="source-code">features_train.reshape(60000, 28, 28, 1)</p>
			<p>Here, we have transformed the dimension of <strong class="source-inline">features_train</strong> to <strong class="source-inline">(60000, 28, 28, 1)</strong>, which corresponds to the format (number of observations, height, width, channel). This is needed when working with grayscale images to add the channel dimension. In this example, the dimensions of a grayscale image, <strong class="source-inline">(28,28)</strong>, will be reshaped to <strong class="source-inline">(28,28,1)</strong>, and there will be <strong class="source-inline">60000</strong> images in total.</p>
			<p>In TensorFlow, you can use the <strong class="source-inline">reshape</strong> method as follows:</p>
			<p class="source-code">from tensorflow.keras import layers</p>
			<p class="source-code">layers.Reshape((60000, 28, 28, 1))</p>
			<p>Now that we have learned how to design a CNN in TensorFlow, it's time to put this all into practice on the famous MNIST dataset.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can learn more about Reshape by going to TensorFlow's documentation website: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape">https://www.tensorflow.org/api_docs/python/tf/keras/layers/Reshape</a></p>
			<h2 id="_idParaDest-88">Exercise 3.02: <a id="_idTextAnchor097"/>Recognizing Handwritten Digits (MNIST) with CNN Using KERAS </h2>
			<p>In this exercise, we will be working on the MNIST dataset (which we worked on in <em class="italic">Chapter 2, Neural Networks</em>), which contains images of handwritten digits. However, this time, we will be using a CNN model. This dataset was originally shared by Yann Lecun, one of the most renowned deep learning researchers. We will build a CNN model and then train it to recognize handwritten digits. The CNN will be composed of two layers of convolution with 64 kernels each, followed by two fully connected layers that have 128 and 10 units, respectively.</p>
			<p>TensorFlow provides this dataset directly from its API. Perform the following steps to complete this exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can read more about this dataset on TensorFlow's website: <a href="https://www.tensorflow.org/datasets/catalog/mnist">https://www.tensorflow.org/datasets/catalog/mnist</a></p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file and name it <strong class="source-inline">Exercise 3.02</strong>.</li>
				<li>Import <strong class="source-inline">tensorflow.keras.datasets.mnist</strong> as <strong class="source-inline">mnist</strong>:<p class="source-code">import tensorflow.keras.datasets.mnist as mnist</p></li>
				<li>Load the <strong class="source-inline">mnist</strong> dataset using <strong class="source-inline">mnist.load_data()</strong> and save the results into <strong class="source-inline">(features_train, label_train), (features_test, label_test)</strong>:<p class="source-code">(features_train, label_train), (features_test, label_test) = \</p><p class="source-code">mnist.load_data()</p></li>
				<li>Print the content of <strong class="source-inline">label_train</strong>:<p class="source-code">label_train</p><p>The expected output will be as follows:</p><p class="source-code">array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)</p><p>The label column contains numeric values that correspond to the 10 handwritten digits: <strong class="source-inline">0</strong> to <strong class="source-inline">9</strong>.</p></li>
				<li>Print the shape of the training set:<p class="source-code">features_train.shape</p><p>The expected output will be as follows:</p><p class="source-code">(60000, 28, 28)</p><p>The training set is composed of <strong class="source-inline">60000</strong> observations of shape <strong class="source-inline">28</strong> by <strong class="source-inline">28</strong>. </p></li>
				<li>Print the <strong class="source-inline">shape</strong> of the testing set:<p class="source-code">features_test.shape</p><p>The expected output will be as follows:</p><p class="source-code">(10000, 28, 28)</p><p>The testing set is composed of <strong class="source-inline">10000</strong> observations of shape <strong class="source-inline">28</strong> by <strong class="source-inline">28</strong>.</p></li>
				<li>Reshape the training and testing sets with the dimensions <strong class="source-inline">(number_observations, 28, 28, 1)</strong>:<p class="source-code">features_train = features_train.reshape(60000, 28, 28, 1)</p><p class="source-code">features_test = features_test.reshape(10000, 28, 28, 1)</p></li>
				<li>Standardize <strong class="source-inline">features_train</strong> and <strong class="source-inline">features_test</strong> by dividing them by <strong class="source-inline">255</strong>:<p class="source-code">features_train = features_train / 255.0</p><p class="source-code">features_test = features_test / 255.0</p></li>
				<li>Import <strong class="source-inline">numpy</strong> as <strong class="source-inline">np</strong>, <strong class="source-inline">tensorflow</strong> as <strong class="source-inline">tf</strong>, and <strong class="source-inline">layers</strong> from <strong class="source-inline">tensorflow.keras</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import layers</p></li>
				<li>Set <strong class="source-inline">8</strong> as the seed for <strong class="source-inline">numpy</strong> and <strong class="source-inline">tensorflow</strong> using <strong class="source-inline">np.random_seed()</strong> and <strong class="source-inline">tf.random.set_seed()</strong>, respectively:<p class="source-code">np.random.seed(8)</p><p class="source-code">tf.random.set_seed(8)</p><p class="callout-heading">Note</p><p class="callout">The results may still differ slightly after setting the seeds.</p></li>
				<li>Instantiate a <strong class="source-inline">tf.keras.Sequential()</strong> class and save it to a variable called <strong class="source-inline">model</strong>:<p class="source-code">model = tf.keras.Sequential()</p></li>
				<li>Instantiate a <strong class="source-inline">layers.Conv2D()</strong> class with <strong class="source-inline">64</strong> kernels of shape <strong class="source-inline">(3,3)</strong>, <strong class="source-inline">activation='relu'</strong>, and <strong class="source-inline">input_shape=(28,28,1)</strong>, and save it to a variable called <strong class="source-inline">conv_layer1</strong>:<p class="source-code">conv_layer1 = layers.Conv2D(64, (3,3), activation='relu', \</p><p class="source-code">                            input_shape=(28, 28, 1))</p></li>
				<li>Instantiate a <strong class="source-inline">layers.Conv2D()</strong> class with <strong class="source-inline">64</strong> kernels of shape <strong class="source-inline">(3,3)</strong> and <strong class="source-inline">activation='relu'</strong> and save it to a variable called <strong class="source-inline">conv_layer2</strong>:<p class="callout-heading">Note</p><p class="callout">It is only required to specify the <strong class="source-inline">input_shape</strong> parameter for the first layer. For the following layers, CNN would infer it automatically.</p><p class="source-code">conv_layer2 = layers.Conv2D(64, (3,3), activation='relu')</p></li>
				<li>Instantiate a <strong class="source-inline">layers.Flatten()</strong> class with <strong class="source-inline">128</strong> neurons, <strong class="source-inline">activation='relu'</strong>, and save it to a variable called <strong class="source-inline">fc_layer1</strong>:<p class="source-code">fc_layer1 = layers.Dense(128, activation='relu')</p></li>
				<li>Instantiate a <strong class="source-inline">layers.Flatten()</strong> class with <strong class="source-inline">10</strong> neurons, <strong class="source-inline">activation='softmax'</strong>, and save it to a variable called <strong class="source-inline">fc_layer2</strong>:<p class="source-code">fc_layer2 = layers.Dense(10, activation='softmax')</p></li>
				<li>Add the four layers you just defined to the model using <strong class="source-inline">.add()</strong>, add a <strong class="source-inline">MaxPooling2D()</strong> layer of size <strong class="source-inline">(2,2)</strong> in between each of the convolution layers, and add a <strong class="source-inline">Flatten()</strong> layer before the first fully connected layer to flatten the feature maps:<p class="source-code">model.add(conv_layer1)</p><p class="source-code">model.add(layers.MaxPooling2D(2, 2))</p><p class="source-code">model.add(conv_layer2)</p><p class="source-code">model.add(layers.MaxPooling2D(2, 2))</p><p class="source-code">model.add(layers.Flatten())</p><p class="source-code">model.add(fc_layer1)</p><p class="source-code">model.add(fc_layer2)</p></li>
				<li>Instantiate a <strong class="source-inline">tf.keras.optimizers.Adam()</strong> class with <strong class="source-inline">0.001</strong> as the learning rate and save it to a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the neural network using <strong class="source-inline">.compile()</strong> with <strong class="source-inline">loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']</strong>:<p class="source-code">model.compile(loss='sparse_categorical_crossentropy', \</p><p class="source-code">              optimizer=optimizer, metrics=['accuracy'])</p></li>
				<li>Print the summary of the model:<p class="source-code">model.summary()</p><p>The expected output will be as follows:</p><div id="_idContainer102" class="IMG---Figure"><img src="image/B15385_03_16.jpg" alt="Figure 3.16: Summary of the model&#13;&#10;"/></div><p class="figure-caption">Figure 3.16: Summary of the model</p><p>The preceding summary shows us that there are more than 240,000 parameters to be optimized with this model.</p></li>
				<li>Fit the neural networks with the training set and specify <strong class="source-inline">epochs=5</strong>, <strong class="source-inline">validation_split=0.2</strong>, and <strong class="source-inline">verbose=2</strong>:<p class="source-code">model.fit(features_train, label_train, epochs=5,\</p><p class="source-code">          validation_split = 0.2, verbose=2)</p><p>The expected output will be as follows:</p><div id="_idContainer103" class="IMG---Figure"><img src="image/B15385_03_17.jpg" alt="Figure 3.17: Training output &#13;&#10;"/></div><p class="figure-caption">Figure 3.17: Training output </p><p>We trained our CNN on 48,000 samples, and we used 12,000 samples as the validation set. After training for five epochs, we achieved an accuracy score of <strong class="source-inline">0.9951</strong> for the training set and <strong class="source-inline">0.9886</strong> for the validation set. Our model is overfitting a bit.</p></li>
				<li>Let's evaluate the performance of the model on the testing set:<p class="source-code">model.evaluate(features_test, label_test)</p><p>The expected output will be as follows:</p><p class="source-code">10000/10000 [==============================] - 1s 86us/sample - </p><p class="source-code">loss: 0.0312 - accuracy: 0.9903 [0.03115778577708088, 0.9903]</p><p>With this, we've achieved an accuracy score of <strong class="source-inline">0.9903</strong> on the testing set. </p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2W2VLYl">https://packt.live/2W2VLYl</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3iKAVGZ">https://packt.live/3iKAVGZ</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we designed and trained a CNN architecture to recognize the images of handwritten digit images from the MNIST dataset and achieved an almost perfect score.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor098"/>Data Generator</h2>
			<p>In<a id="_idTextAnchor099"/> the previous exercise, we built our first multi-class CNN classifier on the MNIST dataset. We loaded the entire dataset into the model as it wasn't very big. But for bigger datasets, we will not be able to do this. Thankfully, Keras provides an API called <strong class="bold">data generator</strong> that we can use to load and transform data in batches. </p>
			<p>Data generators are also very useful for image classification. Sometimes, an image dataset comes in the form of a folder with predefined structures for the training and testing sets and for the different classes (all images that belong to a class will be stored in the same folder). The data generator API will be able to understand this structure and feed the CNN model properly with the relevant images and corresponding information. This will save you a lot of time as you won't need to build a custom pipeline to load images from the different folders. </p>
			<p>On top of this, data generators can divide the images into batches of images and feed them sequentially to the model. You don't have to load the entire dataset into memory in order to perform training. Let's see how they work.</p>
			<p>First, we need to import the <strong class="source-inline">ImageDataGenerator</strong> class from <strong class="source-inline">tensorflow.keras.preprocessing</strong>:</p>
			<p class="source-code">from tensorflow.keras.preprocessing.image \</p>
			<p class="source-code">import ImageDataGenerator</p>
			<p>Then, we can instantiate it by providing all the image transformations we want it to perform. In the following example, we will just normalize all the images from the training set by dividing them by <strong class="source-inline">255</strong> so that all the pixels will have a value between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>:</p>
			<p class="source-code">train_imggen = ImageDataGenerator(rescale=1./255)</p>
			<p>After this step, we will create a data generator by using the <strong class="source-inline">.flow_from_directory()</strong> method and will specify the path to the training directory, <strong class="source-inline">batch_size</strong>, the <strong class="source-inline">target_size</strong> of the image, the shuffle, and the type of class:</p>
			<p class="source-code">train_datagen = train_imggen.\</p>
			<p class="source-code">                flow_from_directory(batch_size=32, \</p>
			<p class="source-code">                                    directory=train_dir, \</p>
			<p class="source-code">                                    shuffle=True, \</p>
			<p class="source-code">                                    target_size=(100, 100), \</p>
			<p class="source-code">                                    class_mode='binary')</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You need to create a separate data generator for the validation set.</p>
			<p>Finally, we can train our model using the <strong class="source-inline">.fit_generator()</strong> method by providing the data generators for the training and validation sets, the number of epochs, and the number of steps per epoch, which corresponds to the number of images divided by the batch size (as integer):</p>
			<p class="source-code">model.fit_generator(train_data_gen, \</p>
			<p class="source-code">                    steps_per_epoch=total_train // batch_size, \</p>
			<p class="source-code">                    epochs=5, validation_data=val_data_gen, \</p>
			<p class="source-code">                    validation_steps=total_val // batch_size)</p>
			<p>This method is very similar to the <strong class="source-inline">.fit()</strong> method you saw earlier, but rather than training the CNN on the entire dataset in one go, it will train by batches of images using the data generator we defined. The number of steps defines how many batches will be required to process the entire dataset.</p>
			<p>Data generators are quite useful for loading data from folders and feeding the model in batches of images. But they can also perform some data processing, as shown in the following section.</p>
			<h2 id="_idParaDest-90">Exercise 3.03: Cl<a id="_idTextAnchor100"/>assifying Cats versus Dogs with Data Generators</h2>
			<p>In this exercise, we will be working on the cats versus dogs dataset, which contains images of dogs and cats. We will build two data generators for the training and validation sets and a CNN model to recognize images of dogs or cats. Perform the following steps to complete this exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset we'll be using is a modified version from the Kaggle cats versus dogs dataset: <a href="https://www.kaggle.com/c/dogs-vs-cats/data">https://www.kaggle.com/c/dogs-vs-cats/data</a>.The modified version, which only uses a subset of 25,000 images, has been provided by Google at <a href="https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip">https://storage.googleapis.com/mledu-datasets/cats_and_dogs_filtered.zip</a>. </p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file and name it <strong class="source-inline">Exercise 3.03</strong>.</li>
				<li>Import the <strong class="source-inline">tensorflow</strong> library:<p class="source-code">import tensorflow as tf</p></li>
				<li>Create a variable called <strong class="source-inline">file_url</strong> containing the link to the dataset:<p class="source-code">file_url = 'https://github.com/PacktWorkshops'\</p><p class="source-code">           '/The-Deep-Learning-Workshop/raw/master'\</p><p class="source-code">           '/Chapter03/Datasets/Exercise3.03'\</p><p class="source-code">           '/cats_and_dogs_filtered.zip'</p><p class="callout-heading">Note</p><p class="callout">In the aforementioned step, we are using the dataset stored at <a href="https://packt.live/3jZKRNw">https://packt.live/3jZKRNw</a>. If you have stored the dataset at any other URL, please change the highlighted path accordingly. Watch out for the slashes in the string below. Remember that the backslashes ( <strong class="source-inline">\</strong> ) are used to split the code across multiple lines, while the forward slashes ( <strong class="source-inline">/</strong> ) are part of the URL.</p></li>
				<li>Download the dataset using <strong class="source-inline">tf.keras.get_file</strong> with <strong class="source-inline">'cats_and_dogs.zip', origin=file_url, extract=True</strong> as parameters and save the result to a variable called <strong class="source-inline">zip_dir</strong>:<p class="source-code">zip_dir = tf.keras.utils.get_file('cats_and_dogs.zip', \</p><p class="source-code">                                   origin=file_url, extract=True)</p></li>
				<li>Import the <strong class="source-inline">pathlib</strong> library:<p class="source-code">import pathlib</p></li>
				<li>Create a variable called <strong class="source-inline">path</strong> containing the full path to the <strong class="source-inline">cats_and_dogs_filtered</strong> directory using <strong class="source-inline">pathlib.Path(zip_dir).parent</strong>:<p class="source-code">path = pathlib.Path(zip_dir).parent / 'cats_and_dogs_filtered'</p></li>
				<li>Create two variables called <strong class="source-inline">train_dir</strong> and <strong class="source-inline">validation_dir</strong> that take the full paths to the train and validation folders, respectively:<p class="source-code">train_dir = path / 'train'</p><p class="source-code">validation_dir = path / 'validation'</p></li>
				<li>Create four variables called <strong class="source-inline">train_cats_dir</strong>, <strong class="source-inline">train_dogs_dir</strong>, <strong class="source-inline">validation_cats_dir</strong>, and <strong class="source-inline">validation_dogs_dir</strong> that take the full paths to the cats and dogs folders for the train and validation sets, respectively:<p class="source-code">train_cats_dir = train_dir / 'cats'</p><p class="source-code">train_dogs_dir = train_dir /'dogs'</p><p class="source-code">validation_cats_dir = validation_dir / 'cats'</p><p class="source-code">validation_dogs_dir = validation_dir / 'dogs'</p></li>
				<li>Import the <strong class="source-inline">os</strong> package. We will need this in the next step in order to count the number of images from a folder:<p class="source-code">import os</p></li>
				<li>Create two variables called <strong class="source-inline">total_train</strong> and <strong class="source-inline">total_val</strong> that will get the number of images for the training and validation sets:<p class="source-code">total_train = len(os.listdir(train_cats_dir)) \</p><p class="source-code">                  + len(os.listdir(train_dogs_dir))</p><p class="source-code">total_val = len(os.listdir(validation_cats_dir)) \</p><p class="source-code">                + len(os.listdir(validation_dogs_dir))</p></li>
				<li>Import <strong class="source-inline">ImageDataGenerator</strong> from <strong class="source-inline">tensorflow.keras.preprocessing</strong>:<p class="source-code">from tensorflow.keras.preprocessing.image\</p><p class="source-code">import ImageDataGenerator</p></li>
				<li>Instantiate two <strong class="source-inline">ImageDataGenerator</strong> classes and call them <strong class="source-inline">train_image_generator</strong> and <strong class="source-inline">validation_image_generator</strong>. These will rescale the images by dividing them by <strong class="source-inline">255</strong>:<p class="source-code">train_image_generator = ImageDataGenerator(rescale=1./255)</p><p class="source-code">validation_image_generator = ImageDataGenerator(rescale=1./255)</p></li>
				<li>Create three variables called <strong class="source-inline">batch_size</strong>, <strong class="source-inline">img_height</strong>, and <strong class="source-inline">img_width</strong> that take the values <strong class="source-inline">16</strong>, <strong class="source-inline">100</strong>, and <strong class="source-inline">100</strong>, respectively:<p class="source-code">batch_size = 16</p><p class="source-code">img_height = 100</p><p class="source-code">img_width = 100</p></li>
				<li>Create a data generator called <strong class="source-inline">train_data_gen</strong> using <strong class="source-inline">.flow_from_directory()</strong> and specify the batch size, the path to the training folder, <strong class="source-inline">shuffle=True</strong>, the target size as <strong class="source-inline">(img_height, img_width)</strong>, and the class mode as <strong class="source-inline">binary</strong>:<p class="source-code">train_data_gen = train_image_generator.flow_from_directory\</p><p class="source-code">                 (batch_size=batch_size, directory=train_dir, \</p><p class="source-code">                  shuffle=True, \</p><p class="source-code">                  target_size=(img_height, img_width), \</p><p class="source-code">                  class_mode='binary')</p></li>
				<li>Create a data generator called <strong class="source-inline">val_data_gen</strong> using <strong class="source-inline">.flow_from_directory()</strong> and specify the batch size, paths to the validation folder, <strong class="source-inline">shuffle=True</strong>, the target size as <strong class="source-inline">(img_height, img_width)</strong>, and the class mode as <strong class="source-inline">binary</strong>:<p class="source-code">val_data_gen = validation_image_generator.flow_from_directory\</p><p class="source-code">              (batch_size=batch_size, \</p><p class="source-code">               directory=validation_dir, \</p><p class="source-code">               target_size=(img_height, img_width), \</p><p class="source-code">               class_mode='binary')</p></li>
				<li>Import <strong class="source-inline">numpy</strong> as <strong class="source-inline">np</strong>, <strong class="source-inline">tensorflow</strong> as <strong class="source-inline">tf</strong>, and <strong class="source-inline">layers</strong> from <strong class="source-inline">tensorflow.keras</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import layers</p></li>
				<li>Set <strong class="source-inline">8</strong> (this is totally arbitrary) as the <strong class="source-inline">seed</strong> for <strong class="source-inline">numpy</strong> and <strong class="source-inline">tensorflow</strong> using <strong class="source-inline">np.random_seed()</strong> and <strong class="source-inline">tf.random.set_seed()</strong>, respectively:<p class="source-code">np.random.seed(8)</p><p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Instantiate a <strong class="source-inline">tf.keras.Sequential()</strong> class into a variable called <strong class="source-inline">model</strong> with the following layers: A convolution layer with <strong class="source-inline">64</strong> kernels of shape <strong class="source-inline">3</strong>, <strong class="source-inline">ReLU</strong> as the activation function, and the required input dimensions; a max pooling layer; a convolution layer with <strong class="source-inline">128</strong> kernels of shape <strong class="source-inline">3</strong> and <strong class="source-inline">ReLU</strong> as the activation function; a max pooling layer; a flatten layer; a fully connected layer with <strong class="source-inline">128</strong> units and <strong class="source-inline">ReLU</strong> as the activation function; a fully connected layer with <strong class="source-inline">1</strong> unit and <strong class="source-inline">sigmoid</strong> as the activation function.<p>The code will look as follows:</p><p class="source-code">model = tf.keras.Sequential([</p><p class="source-code">    layers.Conv2D(64, 3, activation='relu', \</p><p class="source-code">                  input_shape=(img_height, img_width ,3)),\</p><p class="source-code">    layers.MaxPooling2D(),\</p><p class="source-code">    layers.Conv2D(128, 3, activation='relu'),\</p><p class="source-code">    layers.MaxPooling2D(),\</p><p class="source-code">    layers.Flatten(),\</p><p class="source-code">    layers.Dense(128, activation='relu'),\</p><p class="source-code">    layers.Dense(1, activation='sigmoid')])</p></li>
				<li>Instantiate a <strong class="source-inline">tf.keras.optimizers.Adam()</strong> class with <strong class="source-inline">0.001</strong> as the learning rate and save it to a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the neural network using <strong class="source-inline">.compile()</strong> with <strong class="source-inline">loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy']</strong>:<p class="source-code">model.compile(loss='binary_crossentropy', \</p><p class="source-code">              optimizer=optimizer, metrics=['accuracy'])</p></li>
				<li>Print a summary of the model using <strong class="source-inline">.summary()</strong>:<p class="source-code">model.summary()</p><p>The expected output will be as follows:</p><div id="_idContainer104" class="IMG---Figure"><img src="image/B15385_03_18.jpg" alt="Figure 3.18: Summary of the model&#13;&#10;"/></div><p class="figure-caption">Figure 3.18: Summary of the model</p><p>The preceding summary shows us that there are more than <strong class="source-inline">8,700,000</strong> parameters to be optimized with this model.</p></li>
				<li>Fit the neural networks with <strong class="source-inline">fit_generator()</strong> and provide the train and validation data generators, <strong class="source-inline">epochs=5</strong>, the steps per epoch, and the validation steps:<p class="source-code">model.fit_generator(train_data_gen, \</p><p class="source-code">                    steps_per_epoch=total_train // batch_size, \</p><p class="source-code">                    epochs=5, \</p><p class="source-code">                    validation_data=val_data_gen,\</p><p class="source-code">                    validation_steps=total_val // batch_size)</p><p>The expected output will be as follows:</p><div id="_idContainer105" class="IMG---Figure"><img src="image/B15385_03_19.jpg" alt="Figure 3.19: Training output&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.19: Training output</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The expected output will be close to the one shown. You may have slightly different accuracy values due to some randomness in weights initialization.</p>
			<p>We've trained our CNN for five epochs and achieved an accuracy score of <strong class="source-inline">0.85</strong> for the training set, and <strong class="source-inline">0.7113</strong> for the validation set. Our model is overfitting quite a lot. You may want to try the training with different architectures to see whether you can improve this score and reduce overfitting. You can also try feeding this model with some images of cats or dogs of your choice and see the output predictions.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31XQmp9">https://packt.live/31XQmp9</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2ZW10tW">https://packt.live/2ZW10tW</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<h1 id="_idParaDest-91"><a id="_idTextAnchor101"/>Data Augmentation</h1>
			<p>I<a id="_idTextAnchor102"/>n the previous section, you were introduced to data generators that can do a lot of the heavy lifting, such as feeding the model from folders rather than columnar data for you regarding data processing for neural networks. So far, we have seen how to create them, load data from a structured folder, and feed the model by batch. We only performed one image transformation with it: rescaling. However, data generators can perform many more image transformations.</p>
			<p>But why do we need to perform data augmentation? The answer is quite simple: to prevent overfitting. By performing data augmentation, we are increasing the number of images in a dataset. For one image, we can generate, for instance, 10 different variants of the same image. So, the size of your dataset will be multiplied by 10. </p>
			<p>Also, with data augmentation, we have a set of images with a broader range of visuals. For example, selfie pictures can be taken from different angles, but if your dataset only contains selfie pictures that are straight in terms of their orientation, your CNN model will not be able to interpret other images with different angles correctly. By performing data augmentation, you are helping your model generalize better to different types of images. However, as you may have guessed, there is one drawback: data augmentation will also increase the training time as you have to perform additional data transformations.</p>
			<p>Let's take a quick look at some of the different types of data argumentation that we can do.</p>
			<h2 id="_idParaDest-92">Horizontal Flipping<a id="_idTextAnchor103"/></h2>
			<p>Horizontal flipping returns an image that is flipped horizontally:</p>
			<div>
				<div id="_idContainer106" class="IMG---Figure">
					<img src="image/B15385_03_20.jpg" alt="Figure 3.20: Example of horizontal flipping&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.20: Example of horizontal flipping</p>
			<h2 id="_idParaDest-93"><a id="_idTextAnchor104"/>Vertical Flipping</h2>
			<p>Ve<a id="_idTextAnchor105"/>rtical flipping will flip an image vertically:</p>
			<div>
				<div id="_idContainer107" class="IMG---Figure">
					<img src="image/B15385_03_21.jpg" alt="Figure 3.21: Example of vertical flipping&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.21: Example of vertical flipping</p>
			<h2 id="_idParaDest-94"><a id="_idTextAnchor106"/>Zooming</h2>
			<p>An image can <a id="_idTextAnchor107"/>be zoomed in and provide different sizes of objects in the image:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/B15385_03_22.jpg" alt="Figure 3.22: Example of zooming&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.22: Example of zooming</p>
			<h2 id="_idParaDest-95"><a id="_idTextAnchor108"/>Horizontal Shifting</h2>
			<p>Ho<a id="_idTextAnchor109"/>rizontal shifting, as its name implies, will shift the image along the horizontal axis but keep it the same size. With this transformation, the image may be cropped, and new pixels need to be generated to fill the void. A common technique is to copy the neighboring pixels or to fill that space with black pixels:</p>
			<div>
				<div id="_idContainer109" class="IMG---Figure">
					<img src="image/B15385_03_23.jpg" alt="Figure 3.23: Example of horizontal shifting&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.23: Example of horizontal shifting</p>
			<h2 id="_idParaDest-96"><a id="_idTextAnchor110"/>Vertical Shifting</h2>
			<p>Verti<a id="_idTextAnchor111"/>cal shifting is similar to horizontal shifting, but along the vertical axis:</p>
			<div>
				<div id="_idContainer110" class="IMG---Figure">
					<img src="image/B15385_03_24.jpg" alt="Figure 3.24: Example of vertical shifting&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.24: Example of vertical shifting</p>
			<h2 id="_idParaDest-97"><a id="_idTextAnchor112"/>Rotating</h2>
			<p>A rotation with<a id="_idTextAnchor113"/> a particular angle can be performed on an image like so:</p>
			<div>
				<div id="_idContainer111" class="IMG---Figure">
					<img src="image/B15385_03_25.jpg" alt="Figure 3.25: Example of rotating&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.25: Example of<a id="_idTextAnchor114"/> rotating</p>
			<h2 id="_idParaDest-98"><a id="_idTextAnchor115"/>Shearing</h2>
			<p>Shearing transfo<a id="_idTextAnchor116"/>rms the image by moving one of the edges along the axis of the edge. After doing this, the image distorts from a rectangle to a parallelogram:</p>
			<div>
				<div id="_idContainer112" class="IMG---Figure">
					<img src="image/B15385_03_26.jpg" alt="Figure 3.26: Example of shearing&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.26: Example of shearing</p>
			<p>With <strong class="source-inline">Keras</strong>, all these data transformation techniques can be added to <strong class="source-inline">ImageDataGenerator</strong>:</p>
			<p class="source-code">from tensorflow.keras.preprocessing.image import ImageDataGenerator</p>
			<p class="source-code">ImageDataGenerator(rescale=1./255, \</p>
			<p class="source-code">                   horizontal_flip=True, zoom_range=0.2, \</p>
			<p class="source-code">                   width_shift_range=0.2, \</p>
			<p class="source-code">                   height_shift_range=0.2, \</p>
			<p class="source-code">                   shear_range=0.2, rotation_range=40, \</p>
			<p class="source-code">                   fill_mode='nearest')</p>
			<p>Now that we have a general understanding of data argumentation, let's look at how to implement it in our models in the following exercise.</p>
			<h2 id="_idParaDest-99">Exercise 3.04: Image Class<a id="_idTextAnchor117"/>ification (CIFAR-10) with Data Augmentation</h2>
			<p>In this exercise, we will be working on the CIFAR-10 dataset (Canadian Institute for Advanced Research), which is composed of 60,000 images of 10 different classes: airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships, and trucks. We will build a CNN model and use data augmentation to recognize these categories. Perform the following steps to complete this exercise:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can read more about this dataset on TensorFlow's website: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10">https://www.tensorflow.org/api_docs/python/tf/keras/datasets/cifar10</a>.</p>
			<ol>
				<li value="1">Open a new Jupyter Notebook file and name it <strong class="source-inline">Exercise 3.04</strong>.</li>
				<li>Import <strong class="source-inline">tensorflow.keras.datasets.cifar10</strong>:<p class="source-code">from tensorflow.keras.datasets import cifar10</p></li>
				<li>Load the CIFAR-10 dataset using <strong class="source-inline">cifar10.load_data()</strong> and save the results to <strong class="source-inline">(features_train, label_train), (features_test, label_test)</strong>:<p class="source-code">(features_train, label_train), (features_test, label_test) = \</p><p class="source-code">cifar10.load_data()</p></li>
				<li>Print the shape of <strong class="source-inline">features_train</strong>:<p class="source-code">features_train.shape</p><p>The expected output will be as follows:</p><p class="source-code"> (50000, 32, 32, 3)</p><p>The training set is composed of <strong class="source-inline">50000</strong> images that have the dimensions <strong class="source-inline">(32,32,3)</strong>.</p></li>
				<li>Create three variables called <strong class="source-inline">batch_size</strong>, <strong class="source-inline">img_height</strong>, and <strong class="source-inline">img_width</strong> that take the values <strong class="source-inline">16</strong>, <strong class="source-inline">32</strong>, and <strong class="source-inline">32</strong>, respectively:<p class="source-code">batch_size = 16</p><p class="source-code">img_height = 32</p><p class="source-code">img_width = 32</p></li>
				<li>Import <strong class="source-inline">ImageDataGenerator</strong> from <strong class="source-inline">tensorflow.keras.preprocessing</strong>:<p class="source-code">from tensorflow.keras.preprocessing.image import ImageDataGenerator</p></li>
				<li>Create an <strong class="source-inline">ImageDataGenerator</strong> instance called <strong class="source-inline">train_img_gen</strong> with data augmentation: rescaling (by dividing by 255), <strong class="source-inline">width_shift_range=0.1</strong>, <strong class="source-inline">height_shift_range=0.1</strong>, and horizontal flipping:<p class="source-code">train_img_gen = ImageDataGenerator\</p><p class="source-code">                (rescale=1./255, width_shift_range=0.1, \</p><p class="source-code">                 height_shift_range=0.1, horizontal_flip=True)</p></li>
				<li>Create an <strong class="source-inline">ImageDataGenerator</strong> instance called <strong class="source-inline">val_img_gen</strong> with rescaling (by dividing by 255):<p class="source-code">val_img_gen = ImageDataGenerator(rescale=1./255)</p></li>
				<li>Create a data generator called <strong class="source-inline">train_data_gen</strong> using the <strong class="source-inline">.flow()</strong> method and specify the batch size, features, and labels from the training set:<p class="source-code">train_data_gen = train_img_gen.flow\</p><p class="source-code">                 (features_train, label_train, \</p><p class="source-code">                 batch_size=batch_size)</p></li>
				<li>Create a data generator called <strong class="source-inline">val_data_gen</strong> using the <strong class="source-inline">.flow()</strong> method and specify the batch size, features, and labels from the testing set:<p class="source-code">val_data_gen = train_img_gen.flow\</p><p class="source-code">               (features_test, label_test, \</p><p class="source-code">                batch_size=batch_size)</p></li>
				<li>Import <strong class="source-inline">numpy</strong> as <strong class="source-inline">np</strong>, <strong class="source-inline">tensorflow</strong> as <strong class="source-inline">tf</strong>, and <strong class="source-inline">layers</strong> from <strong class="source-inline">tensorflow.keras</strong>:<p class="source-code">import numpy as np</p><p class="source-code">import tensorflow as tf</p><p class="source-code">from tensorflow.keras import layers</p></li>
				<li>Set <strong class="source-inline">8</strong> as the seed for <strong class="source-inline">numpy</strong> and <strong class="source-inline">tensorflow</strong> using <strong class="source-inline">np.random_seed()</strong> and <strong class="source-inline">tf.random.set_seed()</strong>:<p class="source-code">np.random.seed(8)</p><p class="source-code">tf.random.set_seed(8)</p></li>
				<li>Instantiate a <strong class="source-inline">tf.keras.Sequential()</strong> class into a variable called <strong class="source-inline">model</strong> with the following layers: a convolution layer with <strong class="source-inline">64</strong> kernels of shape <strong class="source-inline">3</strong>, ReLU as the activation function, and the necessary input dimensions; a max pooling layer; a convolution layer with <strong class="source-inline">128</strong> kernels of shape <strong class="source-inline">3</strong> and ReLU as the activation function; a max pooling layer; a flatten layer; a fully connected layer with <strong class="source-inline">128</strong> units and ReLU as the activation function; a fully connected layer with <strong class="source-inline">10</strong> units and Softmax as the activation function.<p>The code will be as follows:</p><p class="source-code">model = tf.keras.Sequential([</p><p class="source-code">        layers.Conv2D(64, 3, activation='relu', \</p><p class="source-code">                      input_shape=(img_height, img_width ,3)), \</p><p class="source-code">        layers.MaxPooling2D(), \</p><p class="source-code">        layers.Conv2D(128, 3, activation='relu'), \</p><p class="source-code">        layers.MaxPooling2D(), \</p><p class="source-code">        layers.Flatten(), \</p><p class="source-code">        layers.Dense(128, activation='relu'), \</p><p class="source-code">        layers.Dense(10, activation='softmax')])</p></li>
				<li>Instantiate a <strong class="source-inline">tf.keras.optimizers.Adam()</strong> class with <strong class="source-inline">0.001</strong> as the learning rate and save it to a variable called <strong class="source-inline">optimizer</strong>:<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p></li>
				<li>Compile the neural network using <strong class="source-inline">.compile()</strong> with <strong class="source-inline">loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']</strong>:<p class="source-code">model.compile(loss='sparse_categorical_crossentropy', \</p><p class="source-code">              optimizer=optimizer, metrics=['accuracy'])</p></li>
				<li>Fit the neural networks with <strong class="source-inline">fit_generator()</strong> and provide the train and validation data generators, <strong class="source-inline">epochs=5</strong>, the steps per epoch, and the validation steps:<p class="source-code">model.fit_generator(train_data_gen, \</p><p class="source-code">                    steps_per_epoch=len(features_train) \</p><p class="source-code">                                    // batch_size, \</p><p class="source-code">                    epochs=5, \</p><p class="source-code">                    validation_data=val_data_gen, \</p><p class="source-code">                    validation_steps=len(features_test) \</p><p class="source-code">                                     // batch_size)</p><p>The expected output will be as follows:</p><div id="_idContainer113" class="IMG---Figure"><img src="image/B15385_03_27.jpg" alt="Figure 3.27: Training logs for the model&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 3.27: Training logs for the model</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31ZLyQk">https://packt.live/31ZLyQk</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2OcmahS">https://packt.live/2OcmahS</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>In this exercise, we trained our CNN on five epochs, and we achieved an accuracy score of <strong class="source-inline">0.6713</strong> on the training set and <strong class="source-inline">0.6582</strong> on the validation set. Our model is overfitting slightly, but its accuracy score is quite low. You may wish to try this on different architectures to see whether you can improve this score by, for instance, adding more convolution layers.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The expected output for the preceding exercise will be close to the one shown (Figure 3.27). You may have slightly different accuracy values due to some randomness in weights initialization.</p>
			<h2 id="_idParaDest-100">Activity 3.01: Building a M<a id="_idTextAnchor118"/>ulticlass Classifier Based on the Fashion MNIST Dataset</h2>
			<p>In this activity, you will train a CNN to recognize images of clothing that belong to 10 different classes. You will apply some data augmentation techniques to reduce the risk of overfitting. You will be using the Fashion MNIST dataset provided by TensorFlow. Perform the following steps to complete this activity:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The original dataset was shared by Han Xiao. You can read more about this dataset on TensorFlow's website here: <a href="https://www.tensorflow.org/datasets/catalog/mnist">https://www.tensorflow.org/datasets/catalog/mnist</a></p>
			<ol>
				<li value="1">Import the Fashion MNIST dataset from TensorFlow.</li>
				<li>Reshape the training and testing sets.</li>
				<li>Create a data generator with the following data augmentation:<p class="source-code">rescale=1./255, </p><p class="source-code">rotation_range=40, </p><p class="source-code">width_shift_range=0.1, </p><p class="source-code">height_shift_range=0.1, </p><p class="source-code">shear_range=0.2, </p><p class="source-code">zoom_range=0.2, </p><p class="source-code">horizontal_flip=True, </p><p class="source-code">fill_mode='nearest'</p></li>
				<li>Create the neural network architecture with the following layers: A convolutional layer with <strong class="source-inline">Conv2D(64, (3,3), activation='relu')</strong> followed by <strong class="source-inline">MaxPooling2D(2,2)</strong>; a convolutional layer with <strong class="source-inline">Conv2D(64, (3,3), activation='relu')</strong> followed by <strong class="source-inline">MaxPooling2D(2,2)</strong>; a flatten layer; a fully connected layer with <strong class="source-inline">Dense(128, activation=relu)</strong>; a fully connected layer with <strong class="source-inline">Dense(10, activation='softmax')</strong>.</li>
				<li>Specify an Adam optimizer with a learning rate of <strong class="source-inline">0.001</strong>.</li>
				<li>Train the model.</li>
				<li>Evaluate the model on the testing set.</li>
			</ol>
			<p>The expected output will be as follows:</p>
			<div>
				<div id="_idContainer114" class="IMG---Figure">
					<img src="image/B15385_03_28.jpg" alt="Figure 3.28: Training logs for the model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.28: Training logs for the model</p>
			<p>The expected accuracy scores should be around <strong class="source-inline">0.82</strong> for the training and validation sets. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">The detailed steps for this activity, along with the solutions and additional commentary, are presented on page 394.</p>
			<h1 id="_idParaDest-101"><a id="_idTextAnchor119"/>Saving and Restoring Models</h1>
			<p><a id="_idTextAnchor120"/>In the previous section, we learned how we can use data augmentation to generate different variants of an image. This will increase the size of the dataset but will also help the model train on a wider variety of images and help it generalize better.</p>
			<p>Once you've trained your model, you will most likely want to deploy it in production and use it to make live predictions. To do so, you will need to save your model as a file. This file can then be loaded by your prediction service so that it can be used as an API or data science tool.</p>
			<p>There are different components of a model that can be saved:</p>
			<ul>
				<li>The model's architecture with all the network and layers used</li>
				<li>The model's trained weights</li>
				<li>The training configuration with the loss function, optimizer, and metrics</li>
			</ul>
			<p>In TensorFlow, you can save the entire model or each of these components separately. Let's learn how to do this.</p>
			<h3 id="_idParaDest-102"><a id="_idTextAnchor121"/>Saving the Entire Model</h3>
			<p>To s<a id="_idTextAnchor122"/>ave all the components into a single artifact, use the following code:</p>
			<p class="source-code">model.save_model(filepath='path_to_model/cnn_model')</p>
			<p>To load the saved model, use the following code:</p>
			<p class="source-code">loaded_model = tf.keras.models.load_model\</p>
			<p class="source-code">              (filepath='path_to_model/cnn_model')</p>
			<h3 id="_idParaDest-103">Saving the Architecture Only<a id="_idTextAnchor123"/></h3>
			<p>You can save just the architecture of the model as a <strong class="source-inline">json</strong> object. Then, you will need to use the <strong class="source-inline">json</strong> package to save it to a file, as shown in the following code snippet:</p>
			<p class="source-code">import json</p>
			<p class="source-code">config_json = model.to_json()</p>
			<p class="source-code">with open('config.json', 'w') as outfile:</p>
			<p class="source-code">    json.dump(config_json, outfile)</p>
			<p>Then, you will load it back using the <strong class="source-inline">json</strong> package:</p>
			<p class="source-code">import json</p>
			<p class="source-code">with open('config.json') as json_file:</p>
			<p class="source-code">    config_data = json.load(json_file)</p>
			<p class="source-code">loaded_model = tf.keras.models.model_from_json(config_data)</p>
			<h3 id="_idParaDest-104"><a id="_idTextAnchor124"/>Saving the Weights Only</h3>
			<p>You <a id="_idTextAnchor125"/>can save just the weights of the model as follows:</p>
			<p class="source-code">model.save_weights('path_to_weights/weights.h5')</p>
			<p>Then, you will load them back after instantiating the architecture of your new model:</p>
			<p class="source-code">new_model.load_weights('path_to_weights/weights.h5')</p>
			<p>This is particularly useful if you want to train your model even more later. You will load the saved weights and keep training your model and updating its weights further.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">.h5 is the file extension used by default by TensorFlow.</p>
			<h1 id="_idParaDest-105"><a id="_idTextAnchor126"/>Transfer Learning</h1>
			<p>So far, we<a id="_idTextAnchor127"/>'ve learned a lot about designing and training our own CNN models. But as you may have noticed, some of our models are not performing very well. This can be due to multiple reasons, such as the dataset being too small or our model requiring more training.</p>
			<p>But training a CNN takes a lot of time. It would be great if we could reuse an existing architecture that has already been trained. Luckily for us, such an option does exist, and it is called transfer learning. TensorFlow provides different implementations of state-of-the-art models that have been trained on the ImageNet dataset (over 14 million images).</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find the list of available pretrained models in the TensorFlow documentation: <a href="https://www.tensorflow.org/api_docs/python/tf/keras/applications">https://www.tensorflow.org/api_docs/python/tf/keras/applications</a></p>
			<p>To use a pretrained model, we need to import its implemented class. Here, we will be importing a <strong class="source-inline">VGG16</strong> model:</p>
			<p class="source-code">import tensorflow as tf</p>
			<p class="source-code">from tensorflow.keras.applications import VGG16</p>
			<p>Next, we will define the input dimensions of the images from our dataset. Let's say we have images of <strong class="source-inline">(100,100, 3)</strong>:</p>
			<p class="source-code">img_dim = (100, 100, 3)</p>
			<p>Then, we will instantiate a <strong class="source-inline">VGG16</strong> model:</p>
			<p class="source-code">base_model = VGG16(input_shape=img_dim, \</p>
			<p class="source-code">                   weights='imagenet', include_top=True)</p>
			<p>Now, we have a <strong class="source-inline">VGG16</strong> model trained on the <strong class="source-inline">ImageNet</strong> dataset. The <strong class="source-inline">include_top=True</strong> parameter is used to specify that we will be using the same last layers to predict ImageNet's 20,000 categories of images. </p>
			<p>Now, we can use this pretrained model to make predictions:</p>
			<p class="source-code">base_model.predict(input_img)</p>
			<p>But what if we want to use this pretrained model to predict different classes other than the ones from ImageNet? In this situation, we will need to replace the last fully connected layers of the pretrained models that are used for prediction and train them on the new classes. These last few layers are referred to as the top (or head) of the model. We can do this by specifying <strong class="source-inline">include_top=False</strong>:</p>
			<p class="source-code">base_model = VGG16(input_shape=img_dim, \</p>
			<p class="source-code">                   weights='imagenet', include_top=False)</p>
			<p>After this, we will need to freeze this model so that it can't be trained (that is, its weights will not be updated):</p>
			<p class="source-code">base_model.trainable = False</p>
			<p>Then, we will create a new fully connected layer with the parameter of our choice. In this example, we will add a <strong class="source-inline">Dense</strong> layer with <strong class="source-inline">20</strong> units and a <strong class="source-inline">softmax</strong> activation function:</p>
			<p class="source-code">prediction_layer = tf.keras.layers.Dense(20, activation='softmax')</p>
			<p>We will then add the new fully connected layer to our base model:</p>
			<p class="source-code">new_model = tf.keras.Sequential([base_model, prediction_layer])</p>
			<p>Finally, we will train this model, but only the weights for the last layer will be updated:</p>
			<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p>
			<p class="source-code">new_model.compile(loss='sparse_categorical_crossentropy', \</p>
			<p class="source-code">                  optimizer=optimizer, metrics=['accuracy'])</p>
			<p class="source-code">new_model.fit(features_train, label_train, epochs=5, \</p>
			<p class="source-code">              validation_split = 0.2, verbose=2)</p>
			<p>We just created a new model from a pretrained model and adapted it in order to make predictions for our own dataset. We achieved this by replacing the last layers according to the predictions we want to make. Then, we trained only these new layers to make the right predictions. Using transfer learning, you leveraged the existing weights of the <strong class="source-inline">VGG16</strong> model, which were trained on ImageNet. This has saved you a lot of training time and can significantly increase the performance of your model.</p>
			<h1 id="_idParaDest-106"><a id="_idTextAnchor128"/>Fine-Tuning</h1>
			<p>In the previous <a id="_idTextAnchor129"/>section, we learned how to apply transfer learning and use pretrained models to make predictions on our own dataset. With this approach, we froze the entire network and trained only the last few layers that were responsible for making the predictions. The convolutional layers stay the same, so all the filters are set in advance and you are just reusing them.</p>
			<p>But if the dataset you are using is very different from ImageNet, these pretrained filters may not be relevant. In this case, even using transfer learning will not help your model accurately predict the right outcomes. There is a solution for this, which is to only freeze a portion of the network and train the rest of the model rather than just the top layers, just like we do with <strong class="bold">transfer learning</strong>.</p>
			<p>In the early layers of the networks, the filters tend to be quite generic. For instance, you may find filters that detect horizontal or vertical lines at that stage. The filters closer to the end of the network (close to the top or head) are usually more specific to the dataset you are training on. So, these are the ones we want to retrain. Let's learn how we can achieve this in TensorFlow.</p>
			<p>First, let's instantiate a pretrained <strong class="source-inline">VGG16</strong> model:</p>
			<p class="source-code">base_model = VGG16(input_shape=img_dim, \</p>
			<p class="source-code">                   weights='imagenet', include_top=False)</p>
			<p>We will need to set the threshold for the layers so that they're frozen. In this example, we will freeze the first 10 layers:</p>
			<p class="source-code">frozen_layers = 10</p>
			<p>Then, we will iterate through these layers and freeze them individually:</p>
			<p class="source-code">for layer in base_model.layers[:frozen_layers]:</p>
			<p class="source-code">  layer.trainable = False</p>
			<p>Then, we will add our custom fully connected layer to our base model:</p>
			<p class="source-code">prediction_layer = tf.keras.layers.Dense(20, activation='softmax')</p>
			<p class="source-code">new_model = tf.keras.Sequential([base_model, prediction_layer])</p>
			<p>Finally, we will train this model:</p>
			<p class="source-code">optimizer = tf.keras.optimizers.Adam(0.001)</p>
			<p class="source-code">new_model.compile(loss='sparse_categorical_crossentropy', \</p>
			<p class="source-code">                  optimizer=optimizer, metrics=['accuracy'])</p>
			<p class="source-code">new_model.fit(features_train, label_train, epochs=5, \</p>
			<p class="source-code">              validation_split = 0.2, verbose=2)</p>
			<p>In this case, our model will train and update all the weights from the threshold layer we defined. They will use the pretrained weights as the initialized values for the first iteration.</p>
			<p>With this technique, called fine-tuning, you can still leverage pretrained models by partially training them to fit your dataset.</p>
			<h2 id="_idParaDest-107">Activity 3.02: Fruit Classif<a id="_idTextAnchor130"/>ication with Transfer Learning</h2>
			<p>In this activity, we will train a CNN to recognize images of fruits that belong to 120 different classes. We will use transfer learning and data augmentation to do so. We will be using the Fruits 360 dataset (<a href="https://arxiv.org/abs/1712.00580">https://arxiv.org/abs/1712.00580</a>), which was originally shared by Horea Muresan, Mihai Oltean, <em class="italic">Fruit recognition from images using deep learning, Acta Univ. Sapientiae, Informatica Vol. 10, Issue 1, pp. 26-42, 2018.</em> </p>
			<p>It contains more than 82,000 images of 120 different types of fruits. We will be using a subset of this dataset with more than 16,000 images. Perform the following steps to complete this activity:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset can be found here: <a href="https://packt.live/3gEjHsX ">https://packt.live/3gEjHsX</a></p>
			<ol>
				<li value="1">Import the dataset and unzip the file using TensorFlow.</li>
				<li>Create a data generator with the following data augmentation:<p class="source-code">rescale=1./255, </p><p class="source-code">rotation_range=40, </p><p class="source-code">width_shift_range=0.1, </p><p class="source-code">height_shift_range=0.1, </p><p class="source-code">shear_range=0.2, </p><p class="source-code">zoom_range=0.2, </p><p class="source-code">horizontal_flip=True, </p><p class="source-code">fill_mode='nearest'</p></li>
				<li>Load a pretrained <strong class="source-inline">VGG16</strong> model from TensorFlow.</li>
				<li>Add two fully connected layers on top of <strong class="source-inline">VGG16</strong>: A fully connected layer with <strong class="source-inline">Dense(1000, activation='relu')</strong> and a fully connected layer with <strong class="source-inline">Dense(120, activation='softmax')</strong>.</li>
				<li>Specify an Adam optimizer with a learning rate of <strong class="source-inline">0.001</strong>.</li>
				<li>Train the model.</li>
				<li>Evaluate the model on the testing set.</li>
			</ol>
			<p>The expected accuracy scores should be around <strong class="source-inline">0.89</strong> to <strong class="source-inline">0.91</strong> for the training and validation sets. The output will be similar to this:</p>
			<div>
				<div id="_idContainer115" class="IMG---Figure">
					<img src="image/B15385_03_29.jpg" alt="Figure 3.29: Expected output of the activity&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.29: Expected output of the activity</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The detailed steps for this activity, along with the solutions and additional commentary, are presented on page 398.</p>
			<h1 id="_idParaDest-108"><a id="_idTextAnchor131"/>Summary</h1>
			<p>We started our journe<a id="_idTextAnchor132"/>y in this chapter with an introduction to computer vision and image processing, where we learned the different applications of such technology, how digital images are represented, and analyzed this with filters.</p>
			<p>Then, we dived into the basic elements of CNN. We learned what a convolution operation is, how filters work in detecting patterns, and what stride and padding are used for. After understanding these building blocks, we learned how to use TensorFlow to design CNN models. We built our own CNN architecture to recognize handwritten digits.</p>
			<p>After this, we went through data generators and learned how they can feed our model with batches of images rather than loading the entire dataset. We also learned how they can perform data augmentation transformations to expand the variety of images and help the model generalize better.</p>
			<p>Finally, we learned about saving a model and its configuration, but also about how to apply transfer learning and fine-tuning. These techniques are very useful for reusing pretrained models and adapting them to your own projects and datasets. This will save you a lot of time as you won't have to train the model from scratch.</p>
			<p>In the next chapter, you will learn about another very interesting topic that is used for natural language processing: embeddings.</p>
		</div>
		<div>
			<div id="_idContainer117" class="Content">
			</div>
		</div>
	</body></html>