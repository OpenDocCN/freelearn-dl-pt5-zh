- en: Assessments
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Chapter 1: Introduction to Magenta and Generative Art'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Randomness.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Markov chain.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Algorave.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Long short-term memory** (**LSTM**).'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Autonomous systems generate music without operator input; assisting music systems
    will complement an artist while working.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Symbolic: sheet music, MIDI, MusicXML, AbcNotation. Sub-symbolic: raw audio
    (waveform), spectrogram.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '"Note On" and "Note Off" timing, pitch between 1 and 127 kHz, velocity, and
    channel.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At a sample rate of 96 kHz, the Nyquist frequency is 96 kHz/2 = 48 kHz and the
    frequency range is 0 to 48 kHz. This is worse for listening to audio since 28
    kHz of audio is lost on the ear (remember anything over 20 khz cannot be heard),
    and that sampling rate is not properly supported by much audio equipment. It is
    useful in recording and audio editing though.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A single musical note, A4, is played for 1 second loudly.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Drums, voice (melody), harmony (polyphony), and interpolation and manipulation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter 2: Generating Drum Sequences with the Drums RNN'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Given a current sequence, predict the score for the next note, then do a prediction
    for each step you want to generate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1) RNNs operate on sequences of vectors, for the input and output, which is
    good for sequential data such as a music score, and (2) keep an internal state
    composed of the previous output steps, which is good for doing a prediction based
    on past inputs, not only the current input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1) First, the hidden layer will get *h(t + 1)*, which is the output of the
    previous hidden layer, and (2) it will also receive *x(t + 2)*, which is the input
    of the current step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The number of bars generated will be 2 bars, or 32 steps, since we have 16
    steps per bar. At 80 QPM, each step takes 0.1875 seconds, because you take the
    number of seconds in a minute, divide by the QPM, and divide by the number of
    steps per quarter: 60 / 80 / 4 = 0.1875\. Finally, you have 32 steps at 0.1875
    seconds each, so the total time is 32 * 0.1875 = 6 seconds.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increasing the branch factor reduces the randomness, since you have more branches
    to choose from when selecting the best branch, but increasing the temperature
    will increase the randomness. Doing both at the same time will cancel out each
    other, we just don't know in what proportions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: At each step, the algorithm will generate four branches and keep two. At the
    last iteration, the beam search will search for the best branch in the graph by
    checking the remaining two nodes at each level multiplied by the number of steps
    of the generated graph (also the height of the tree), which is three. So we go
    through 2 * 3 nodes = 6 nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`NoteSequence`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The MIDI notes maps to the following classes: 36 maps to 0 (kick drum), 40
    maps to 1 (snare drum), 42 maps to 2 (closed hi-hat). The resulting index is calculated
    with 2⁰ + 2¹ + 2² = 7, so the resulting vector will be *v = [0, 0, 0, 0, 0, 0,
    1, 0, ... ]*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The bit representation of index 131 is `10000011` (in Python, you can use `"{0:b}".format(131)`
    to get that). This is represented as 2⁰ + 2¹ + 2⁷, which gives us the following
    classes: 0 (drum kit), 1 (snare drum) and 7 (crash cymbal). We then arbitrarily
    take the first element of each class: *{36, 38, 49}*.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter 3: Generating Polyphonic Melodies'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Vanishing gradients (values get multiplied by small values in each RNN step)
    and exploding gradients are common RNN problems that occur when training during
    the backpropagation step. LSTM provides a dedicated cell state that is modified
    by forget, input, and output gates to alleviate those problems.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Gated** **recurrent** **units** (**GRUs**) are simpler but less expressive
    memory cells, where the forget and input gates are combined into a single **update
    gate**.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For a 3/4 time signature, you need 3 steps per quarter note, times 4 steps per
    quarter note, which equals 12 steps per bar. For a binary step counter to count
    to 12, you need 5 bits (like for 4/4 time) that will only count to 12\. For 3
    lookbacks, you'll need to look at the past 3 bars, with each bar being 12 steps,
    so you have *[36, 24, 12]*.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The resulting vector is the sum of the previous step vectors, each applied with
    the attention mask, so we have 0.1 applied to *[1, 0, 0, 0]*, plus 0.5 applied
    to *[0, 1, 0, x]* giving *[0.10, 0.50, 0.00, 0.25]*. The value of x is 0.5, because
    0.5 times 0.5 equals 0.25.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A C major chord of one quarter note.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In Polyphony RNN, there are no note end events. If no `CONTINUED_NOTE` is used
    for a pitch in a step, it stops the note. In Perfomance RNN, a `NOTE_END 56` event
    would be used.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (1) Expressive timing using `TIME_SHIFT` events, present in all Performance
    RNN models, for example in the `performance` configuration, and (2) dynamic play
    using `VELOCITY` events, present in the `performance_with_dynamics` configuration.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It will change the number of iterations during the RNN steps call. A bigger
    number of notes per seconds will ask for more RNN steps during the generation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter 4: Latent Space Interpolation with MusicVAE'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The main use is dimensionality reduction, to force the network to learn important
    features, making it possible to reconstruct the original input. The downside of
    AE is that the latent space represented by the hidden layer is not continuous,
    making it hard to sample since the decoder won't be able to make sense of some
    of the points.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The reconstruction loss penalizes the network when it creates outputs that are
    different from the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In VAE, the latent space is continuous and smooth, making it possible to sample
    any point of the space and interpolate between two points. It is achieved by having
    the latent variables follow a probability distribution of P(z), often a Gaussian
    distribution.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The KL divergence measures how much two probability distributions diverge from
    each other. When combined with the reconstruction loss, it centers the clusters
    around 0 and makes them more or less close to one another.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We sample the normal distribution using `np.random.randn(4, 512)`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the direction between two points in the latent space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter 5: Audio Generation with NSynth and GANSynth'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You have to handle 16,000 samples per second (at least) and keep track of the
    general structure at a bigger time scale.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: NSynth is a WaveNet-style autoencoder that learns its own temporal embedding,
    making it possible to capture long term structure, and providing access to a useful
    hidden space.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The colors in the rainbowgram are the 16 dimensions of the temporal embedding.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Check the `timestretch` method in the `audio_utils.py` file in the chapter's
    code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: GANSynth uses upsampling convolutions, making the training and generation processing
    in parallel possible for the entire audio sample.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You need to sample the random normal distribution using `np.random.normal(size=[10,
    256])`, where 10 is the number of sampled instruments, and 256 is the size of
    the latent vector (given by the `latent_vector_size` configuration).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter 6: Data Preparation for Training'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MIDI is not a text format, so it is harder to use and modify, but it is extremely
    common. MusicXML is rather rare and cumbersome but has the advantage of being
    in text format. ABCNotation is also rather rare, but has the advantage of being
    in text format and closer to sheet music.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the code from `chapter_06_example_08.py`, and change the `program=43` in
    the extraction.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: There are 1,116 rock songs in LMD and 3,138 songs for jazz, blues, and country.
    Refer to `chapter_06_example_02.py` and `chapter_06_example_03.py` to see how
    to make statistics with genre information.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the `RepeatSequence` class in `melody_rnn_pipeline_example.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use the code from `chapter_06_example_09.py`. Yes, we can train a quantized
    model with it since the data preparation pipeline quantizes the input.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For small datasets, data augmentation plays an essential role in creating more
    data, because sometimes you just don't have more. For bigger datasets, it also
    plays a role by creating more relevant data and variations on existing data, which
    is good for the network training phase.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter 7: Training Magenta Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: See `chapter_07_example_03.py`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A network that underfits is a network that hasn't reached its optimum, meaning
    it won't predict well with the evaluation data, because it fits poorly the training
    data (for now). It can be fixed by letting it train long enough, by adding more
    network capacity, and more data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A network that overfits is a network that has learned to predict the input but
    cannot generalize to values outside of its training set. It can be fixed by adding
    more data, by reducing the network capacity, or by using regularization techniques
    such as dropout.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Early stopping.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Read *On Large-Batch Training for Deep Learning: Generalization Gap and Sharp
    Minima*, which explains that a larger batch size leads to sharp minimizers, which
    in turn leads to poorer generalization. Therefore it is worse in terms of efficiency,
    but might be better in terms of training time, since more data is processed at
    the same time.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A bigger network is a network that will be more precise in its prediction, so
    maximizing that should be important. The network size should also grow with the
    size (and quality) of the data. For example, a network that's too big for its
    data will likely overfit.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It helps with the exploding gradients problem because the weights will be multiplied
    by smaller values, limiting the possibility of having big gradients. Another way
    of doing this is by reducing the learning rate.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It can be used to launch training on more powerful machines and to launch multiple
    training sessions at the same time. Unfortunately, using cloud providers have
    a cost, meaning the more training time and power we use, the more costly our training
    will get.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter 8: Magenta in the Browser with Magenta.js'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We can train models using TensorFlow.js, but we cannot train models using Magenta.js.
    We need to train the models in Magenta using Python and import the resulting models
    in Magenta.js.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The Web Audio API enables audio synthesis in the browser using audio nodes for
    generation, transformation, and routing. The easiest way to use it is to use an
    audio framework such as Tone.js.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The method is `randomSample` and the argument is the pitch of the generated
    note. As an example, using 60 will result in a single note at MIDI pitch 60, or
    C4 in letter notation. This is also useful as a reference for pitching the note
    up or down using Tone.js.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The method is `sample` and the number of instruments depends on the model that
    is being used. In our example, we've used the `trio` model, which generates three
    instruments. Using a `melody` model will generate only one lead instrument.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Since JavaScript is single threaded, long synchronous computations that are
    launched in the UI thread will block its execution. Using a web worker makes it
    possible to execute code in another thread.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using the Web MIDI API in the browser, which is not well supported at the moment,
    or using Magenta.js in a Node.js process on the server side, making it easier
    to send MIDI to other processes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Chapter 9: Making Magenta Interact with Music Applications'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A DAW will have more functions geared towards music production such as recording,
    audio, MIDI editing, effects and mastering, and song composition. A software synthesizer
    like FluidSynth will have less functionalities, but have the advantage of being
    lightweight and easy to use.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Most music software won't open MIDI ports by themselves, so to send sequences
    back and forth between them we have to manually open ports.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: See the code in `chapter_09_example_05.py` in this chapter's code.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because syncing two pieces of software that have desynced requires restarting
    them. A MIDI clock enables syncing once per beat.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because Magenta Studio integrates with existing music production tools such
    as DAWs and doesn't require any technical knowledge, it makes AI-generated music
    available to a greater audience, which is ultimately the goal of Magenta.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Magenta Studio plugins and Magenta Studio standalone are both based on Magenta.js,
    packaged using Electron. Magenta Studio Plugins uses the Max MSP integration in
    Ableton Live to execute inside it.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
