- en: '19'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Architecting LLM Solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Large language models** (**LLMs**) have revolutionized the field of **natural
    language processing** (**NLP**) and **artificial intelligence** (**AI**), offering
    remarkable versatility in tackling a variety of tasks. However, realizing their
    full potential requires addressing certain challenges and developing effective
    LLM solutions. In this chapter, we’ll demystify the process of architecting LLM
    solutions, focusing on essential aspects such as memory, problem-solving capabilities,
    autonomous agents, and advanced tools for enhanced performance. We will be focusing
    on retrieval-augmented language models, which provide contextually relevant information,
    their practical applications, and methods to improve them further. Additionally,
    we’ll uncover the challenges, best practices, and evaluation methods to ensure
    the success of an LLM solution.'
  prefs: []
  type: TYPE_NORMAL
- en: Building upon these foundational concepts, this chapter will equip you with
    the knowledge and techniques necessary to create powerful LLM solutions tailored
    to your specific needs. By mastering the art of architecting LLM solutions, you
    will be better prepared to tackle complex challenges, optimize performance, and
    unlock the true potential of these versatile models in a wide range of real-world
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Overview of LLM solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling knowledge for LLM solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating LLM solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying challenges with LLM solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tackling challenges with LLM solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging LLMs to build autonomous agents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring LLM solution use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of LLM solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs excel in diverse tasks such as answering questions, machine translation,
    language modeling, sentiment analysis, and text summarization. They generate unstructured
    text but can be guided to produce structured output. LLM solutions harness this
    ability and leverage custom data from knowledge bases to create targeted, valuable
    outcomes for organizations and individuals. By properly streamlining processes
    and enhancing output quality objectively, an LLM solution can unlock the true
    potential of LLM-generated content, making it more powerful and practical across
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: The increasing accessibility of LLMs with pre-trained world knowledge has played
    a significant role in making these benefits more attainable for a broader audience.
    Thanks to various LLM providers and open source platforms, organizations and developers
    can now more easily adopt and integrate LLMs into their workflows. Prominent LLM
    providers, such as OpenAI (GPT-4 or GPT-3.5), Microsoft Azure, Google, and Amazon
    Bedrock, offer pre-trained models and APIs that can be seamlessly integrated into
    diverse applications. Additionally, the Hugging Face platform has made LLMs even
    more accessible by offering an extensive collection of open source models. Hugging
    Face provides a wide selection of pre-trained models and fine-tuning techniques
    while fostering an active community that continually contributes to enhancing
    LLMs.
  prefs: []
  type: TYPE_NORMAL
- en: As organizations and individuals harness the power of LLMs for their typical
    tasks and use cases, it is crucial to determine how to leverage custom knowledge
    effectively. This consideration ensures that LLMs are optimally utilized to address
    specific needs; this will be explored further in the *Handling knowledge for LLM
    solutions* section. By taking advantage of the increased accessibility and versatility
    of LLMs, organizations and individuals can unlock the full potential of these
    powerful models to drive innovation and improve outcomes.
  prefs: []
  type: TYPE_NORMAL
- en: 'Despite their impressive capabilities, LLMs face some limitations when it comes
    to solving more complex problems that they were not made to account for. Some
    of these limitations are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Inability to access up-to-date information on recent events
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tendency to hallucinate facts or generate imitative falsehoods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Difficulties in understanding low-resource languages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Lack of mathematical skills for precise calculations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unawareness of the progression of time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To overcome these limitations and enhance LLMs’ problem-solving capabilities,
    advanced solutions can be developed by incorporating the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time data integration**: By connecting LLMs to real-time data sources
    such as APIs, databases, or web services, the model can access up-to-date information
    and provide more accurate responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Existing tool integration**: Incorporating existing tools and APIs into the
    LLM architecture can extend its capabilities, allowing it to perform tasks that
    would otherwise be impossible or challenging for a standalone model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple agents with different personas and contexts**: Developing a multi-agent
    system where each agent possesses a unique persona and context can help address
    the challenges of diverse problem-solving scenarios. These agents can collaborate,
    share information, and provide more comprehensive and reliable solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 19**.1* shows an architecture that depicts the different approaches
    and methods that can be applied in an LLM solution and will be introduced in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.1 – LLM solution architecture](img/B18187_19_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.1 – LLM solution architecture
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will dive into the individual components listed
    in this LLM solution architecture and LLM solutions in general more comprehensively.
    We will start with how knowledge is handled for LLM solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Handling knowledge for LLM solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Domain knowledge is key in creating LLM solutions as it provides the background
    information and understanding they need to solve specific problems. This ultimately
    ensures the answers or actions the solutions come up with are on point and helpful.
    Domain knowledge needs to be included as context either as part of parametric
    memory, non-parametric memory, or a combination of both. Parametric memory refers
    to the parameters that are learned in an LLM. Non-parametric memory refers to
    an external library of knowledge, such as a list of documents, articles, or excerpts,
    that can be selectively chosen to be injected as part of the LLM context. This
    process is also referred to as an in-context learning method, knowledge retrieval,
    or information retrieval.
  prefs: []
  type: TYPE_NORMAL
- en: 'Non-parametric external knowledge can be provided to an LLM through either
    of the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '**As a latent conditioner in the cross-attention mechanism**: Latent conditioning
    involves generating latent feature vectors from the external knowledge and feeding
    it as part of the key and value vectors in the attention mechanism that were introduced
    in [*Chapter 6*](B18187_06.xhtml#_idTextAnchor092), *Understanding Neural Network
    Transformers*, while the original input is passed in as the query vector. This
    approach typically requires some form of fine-tuning the decoder part of the network
    in an encoder-decoder transformer architecture. Ideally, the fine-tuning process
    will build a decoder that can generalize to the domain of the intended external
    latent features and can attend to a variety of information. This approach allows
    the inclusion of any data modality as external knowledge. Notably, the **Retrieval
    Augmented Generation** (**RAG**) and **Retrieval-Enhanced Transformer** (**RETRO**)
    methods from published research papers [1][2] use this approach.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**As part of an LLM’s input prompt**: This is a straightforward process that
    doesn’t require any fine-tuning but can still benefit from it. This approach brings
    the lowest barrier of entry to leverage any custom domain knowledge in LLMs. However,
    this approach only supports knowledge represented in data modalities that can
    be effectively represented as textual data, such as text, numerical, categorical,
    and date data. Notably, the **Retrieval-Augmented Language Model Pre-Training**
    (**REALM**) method, as part of a published research paper [3], uses this approach
    for pre-training specifically and doesn’t use it as part of the final trained
    model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Both methods require a knowledge base to be established, as depicted in *Figure
    19**.2*, and a knowledge retrieval component that retrieves information from the
    knowledge base, as depicted in *Figure 19**.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 19.2 – Establishing a knowledge base](img/B18187_19_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.2 – Establishing a knowledge base
  prefs: []
  type: TYPE_NORMAL
- en: 'A short tabular summary of the REALM, RETRO, and RAG methods is presented in
    *Table 19.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Knowledge** **retrieval methods** | **Retriever training** | **Retrieval
    integration** |'
  prefs: []
  type: TYPE_TB
- en: '| RAG | Fine-tune with a frozen base network | Latent conditioner with cross-attention
    |'
  prefs: []
  type: TYPE_TB
- en: '| REALM | Full end-to-end training | Prepend to prompt specifically without
    a template |'
  prefs: []
  type: TYPE_TB
- en: '| RETRO | Fine-tune with a frozen base network | Latent conditioner with cross-attention
    |'
  prefs: []
  type: TYPE_TB
- en: Table 19.1 – Short overview of retrieval integration with LLM methods
  prefs: []
  type: TYPE_NORMAL
- en: A knowledge base requires that text data is pre-processed into appropriate logical
    chunks or segments. These segments are then transformed into embedding vectors
    using trained transformer models. Finally, a nearest neighbor index is constructed,
    enabling efficient retrieval of relevant information from the knowledge base.
    The nearest neighbor index can either be a simple KNN algorithm that computes
    raw distances between the prompt embedding vector, or an approximate KNN algorithm
    that approximates the distance computations. Both the index and the logical text
    chunks will then serve as the knowledge base, which can be used for retrieval.
    The method to perform retrieval can vary with different strategies, but the simplest
    form involves simply generating embedding from the prompt and returning the top
    *k* closest text chunks from the knowledge base using the index. These top *k*
    closest text chunks can then be included as part of the LLM prompt or as a latent
    conditioner.
  prefs: []
  type: TYPE_NORMAL
- en: For the approach of including the most relevant text chunks as part of the prompt,
    crafting a prompt template that can allow a specific spot to be inserted is the
    standard and can help organize information in a prompt properly. This can be as
    simple as using leading text such as `Context:`, following up with the retrieved
    relevant text chunks, and having new line separation before and after the context
    part of the prompt template.
  prefs: []
  type: TYPE_NORMAL
- en: While research papers often present published methods that encompass various
    aspects of the retrieval process in a single method, it is helpful to consider
    each component of building and using the knowledge base as separate, interchangeable
    parts. This allows for greater flexibility in selecting the most suitable components
    for specific situations. Moreover, although there is a published method known
    as RAG, it is worth noting that, in practice, the term RAG is commonly used to
    describe the general approach of integrating knowledge retrieval with LLMs, rather
    than referring solely to that specific method. Let’s briefly go through the three
    key method-based components that can be freely modified according to the use case.
    We will also choose orchestrator tools that help streamline the implementation
    of these components.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring chunking methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The text chunking process affects the efficiency of LLM context utilization
    and the quality of the resulting LLM generation. Choosing an appropriate chunking
    method depends on the following factors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**The embedding model used for embedding vector generation**: Different pre-trained
    embedding models may have different requirements or limitations when it comes
    to text chunking. Two such requirements are the supported context size and the
    typical text context size that was used during pre-training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The granularity of information needed for the expected prompts that will
    be made**: The level of detail or granularity required for the prompts can impact
    the choice of text chunking method. Depending on the specific use case, the method
    should be able to chunk the text into appropriate and concise segments that provide
    the necessary information for the LLM to generate accurate, concise, and relevant
    responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The nature of the text data used to build a knowledge base**: The characteristics
    of the text data itself can also influence the choice of text chunking method.
    For example, if the text data consists of long paragraphs or documents, a method
    that breaks the text into smaller chunks or sections may be more suitable. On
    the other hand, if the text data is already organized into logical segments, a
    method that preserves these segments may be preferred. Also, if the text data
    is Python code, it can be suitable to chunk the text by code methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are several methods for chunking text, including sentence, paragraph,
    entity, topic, and section chunking. These methods help organize the text into
    meaningful units that can be processed by the LLM. One notable method that is
    useful and readily available is the recursive chunking method from the LangChain
    library. This method allows you to adjust the granularity of the chunks by recursively
    splitting the text using an ordered list of text separators, a maximum chunk size,
    and the percent of overlap between chunks. The maximum chunk size should be tailored
    to the context size supported by the embedding model, ensuring that the generated
    chunks can be effectively processed. Meanwhile, incorporating an overlap percentage
    helps minimize the risk of missing critical information that could be located
    at the boundaries of chunks.
  prefs: []
  type: TYPE_NORMAL
- en: Many document-specific chunking methods are created based on this recursive
    chunking method by specifying the appropriate ordered list of text separators.
    Specifically, as of `langchain==0.0.314`, recursive methods have been created
    for Python code with `PythonCodeTextSplitter`, markdown documents with the `MarkdownTextSplitter`
    class, and LaTeX-formatted text with the `LatexTextSplitter` class.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s dive into embedding model choices.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring embedding models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embedding models play a crucial role in generating a knowledge base for LLM
    solutions. These models are responsible for encoding the semantic information
    of text into vector representations, which are then used to retrieve relevant
    information from the knowledge base.
  prefs: []
  type: TYPE_NORMAL
- en: One benchmark that provides insights into the performance of text embedding
    models is the `text-embedding-ada-002`.
  prefs: []
  type: TYPE_NORMAL
- en: When selecting an embedding model for knowledge base generation, it is essential
    to consider factors such as model size, embedding dimensions, and sequence length.
    Traditional embedding models such as GloVe offer high speed but may lack context
    awareness, resulting in lower average scores. On the other hand, models such as
    `all-mpnet-base-v2` and `all-MiniLM-L6-v2` strike a balance between speed and
    performance, providing satisfactory results. For maximum performance, larger models
    such as `bge-large-en-v1.5`, `ember-v1`, and `e5-large-v2` dominate the MTEB leaderboard,
    all with a 1.34 GB model size.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to note that the choice of embedding model depends on the specific
    task and dataset being used. Therefore, thoroughly exploring the various tabs
    of the MTEB leaderboard and considering the requirements of the knowledge base
    generation process can help in selecting the most suitable embedding model.
  prefs: []
  type: TYPE_NORMAL
- en: MTEB, with its extensive collection of datasets and evaluation metrics, serves
    as a valuable resource for researchers and practitioners in the field of NLP.
    By leveraging the insights provided by MTEB, developers can make informed decisions
    when choosing an embedding model for knowledge base generation in LLM solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Before we delve into exploring the knowledge base index types, it’s essential
    to remember that the choice of chunking method and embedding model shapes the
    construction of your knowledge base. Both components play a crucial part in how
    effectively the LLM can retrieve and utilize knowledge. Now, let’s dive deeper
    into the world of knowledge base index types and learn how they contribute to
    the efficiency of LLM solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the knowledge base index types
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The knowledge base index is the backbone of retrieval mechanisms in LLM solutions.
    It is the component that facilitates the efficient lookup of relevant information.
    While there are several ways of implementing this index, they all aim to provide
    a fast and efficient way of retrieving the most relevant text chunks from the
    knowledge base based on the input prompt.
  prefs: []
  type: TYPE_NORMAL
- en: 'Many options are available for building a knowledge base index. They range
    from manual code implementations to using various vector database libraries, service
    providers, and plugins. Some of these options are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '`faiss`, a library for efficient similarity search of dense vectors, and `scipy`,
    a library for pairwise distance computations. This allows for customization but
    may require more effort and expertise while requiring bigger RAM allocations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Service providers**: Various cloud providers offer vector database services.
    These include Pinecone, Chroma, Vespa, and Weaviate. These services handle the
    complexities of managing a vector database, providing scalable and robust solutions
    that can be easily integrated into your LLM architecture and solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Database tools with vector computation support**: Traditional database tools
    such as MongoDB, Neo4j, Redis, and PostgreSQL provide vector computation support
    through plugins. This can be a good option if you’re already using these tools
    in your tech stack and want to leverage their capabilities for your knowledge
    base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Plugins**: There are also plugins available, directly from LLM service providers,
    such as ChatGPT, that can help with the construction and maintenance of a knowledge
    base.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the right knowledge base index type depends on your specific requirements,
    such as the size of your knowledge base, the complexity of your retrieval needs,
    and the resources you have available. Consider factors such as scalability, ease
    of integration, cost, and the level of control you need over your knowledge base
    when making your choice. A recommendation is to only consider vector databases
    that are using the actual database technology or claim to do so when your knowledge
    base is big enough to matter. If your knowledge base is small, let’s say in the
    six-digit range, raw distance computations take less than 1 second if you make
    one prompt per compute in Python! Next, we will briefly discover orchestrator
    libraries for LLM solutions.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring orchestrator tools for LLM solutions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The process of architecting LLM solutions can be streamlined with the use of
    specific tools. Open sourced orchestrator libraries such as LangChain and LlamaIndex
    play a pivotal role in this context. Both tools simplify tasks such as setting
    up the knowledge base, integrating an LLM, and managing retrieval mechanisms.
    In general, an orchestrator significantly reduces the complexity and development
    time of LLM solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In addition to open sourced orchestrator tools, there are also paid options
    available that provide advanced features and support. Some of these DataRobot,
    Microsoft Azure, IBM Watson, LangSmith, OpenAI, and Google Vertex AI. These platforms
    offer a wide range of pre-built models, integrations, and tools that streamline
    the entire pipeline, from data ingestion to model deployment and monitoring.
  prefs: []
  type: TYPE_NORMAL
- en: As you continue to explore these tools and methods, it’s crucial to establish
    robust evaluation methods to measure the impact of these components on your solution,
    ensuring it meets its intended objectives. We’ll delve deeper into these methods
    in the upcoming section.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating LLM solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Evaluating LLM solutions is a crucial step in harnessing their full potential
    and ensuring their effectiveness in various applications. By implementing a comprehensive
    set of evaluation approaches, organizations can better assess the performance,
    accuracy, and overall quality of the results from an LLM solution, while also
    considering the associated costs, adherence to safety standards, and potential
    negative impact on users. In other words, doing this provides you with valuable
    insights to help make any informed decisions. To achieve a comprehensive evaluation,
    we can view evaluation methods as part of either a quantitative measure or a qualitative
    measure. Let’s dive into evaluation methods by these groups.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating LLM solutions through quantitative metrics
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Quantitative metrics can be aggregated throughout a provided evaluation dataset
    and can provide a more quick, comprehensive, and objective measure to compare
    multiple LLM solution setups. Here are some examples of quantitative metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Comprehension and fluency-based metrics**: Flesch Reading Ease, Coleman Liau
    Index, and SMOG readability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Facts-based metrics**: Any metrics that use the facts provided by a knowledge
    base for inference:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Factual consistency**: This refers to comparing generated text with facts
    stated in the knowledge base. It is important to note that relevant facts might
    not always be available in the knowledge base. This metric is also known as the
    extractiveness metric. To measure factual consistency, you can use either semantic
    similarity, which focuses on differences in the meaning of the text, or lexical
    similarity, which emphasizes matching words in the text.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Factual relevance**: This is about how relevant the provided facts are, without
    considering the LLM generation. This is possible when you have ranked relevant
    document labels.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generated text relevance/accuracy metric**: This metric evaluates the relevance
    and accuracy of the text generated by an LLM in comparison to an ideal ground
    truth. It can be computed using similarity metrics or self-evaluation techniques.
    Self-evaluation can be further broken down into the following areas:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**With access to token probabilities**: The average of log probabilities is
    used to assess the quality of the generated text. Higher log probabilities indicate
    that the model is more confident in its output, suggesting greater relevance and
    accuracy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Without access to token probabilities**: **SelfCheckGPT** is a method that
    can be employed to evaluate the generated text without relying on token probabilities.
    This approach leverages the LLM’s capabilities to assess the quality of its generated
    content, providing an alternative measure of relevance and accuracy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Runtime metrics**: The time taken to generate text, the number of tokens
    processed, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cost metrics**: The number of tokens generated, API call costs, hosting costs,
    and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Guardrail violations metrics**: The percentage of outputs that violate predefined
    standards. Examples of guardrails are toxicity levels and hate speech degree.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial performance metrics**: The performance measures in handling adversarial
    inputs. These were introduced more comprehensively in [*Chapter 14*](B18187_14.xhtml#_idTextAnchor206),
    *Analyzing* *Adversarial Performance*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias and fairness metrics**: Quantitative measures for assessing biases in
    the generated text. These were introduced more comprehensively in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196),
    *Exploring Bias* *and Fairness*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Any supervised classification or regression metrics**: This can be applied
    to the results or resulting actions from an LLM solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that quantitative methods such as generated text relevance and factual
    consistency metrics, which use similarity metrics to compare two sets of text,
    are not as reliable as supervised model metrics such as accuracy. These metrics
    should be taken with a grain of salt. Additionally, a nice bonus with quantitative
    metrics is that they can be used for monitoring a deployed model programmatically.
    Next, we will dive into qualitative manual evaluations.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating LLM solutions through qualitative evaluation methods
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Qualitative methods, which involve human feedback and manual assessments, complement
    quantitative measures and provide a comprehensive understanding of LLM performance.
    It can also sometimes be the only way for evaluation when there are no reference
    ground truth datasets. Here are some examples of qualitative LLM solution evaluation
    methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Human feedback scores**: These are the users’ ratings or rankings of generated
    responses to gauge effectiveness and relevance. Examples include grammar and coherence
    of text.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generated text relevance evaluation**: This involves manually assessing the
    generated text’s relevance to the given context or prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction explanations**: These assess the reasoning behind the generated
    text or predictions, which can help identify potential biases or faulty logic
    in the LLM solution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethical and legal compliance**: This ensures that the generated text adheres
    to ethical and legal guidelines through manual review.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By employing a combination of quantitative metrics and qualitative manual evaluations,
    organizations can gain a deeper understanding of LLM performance and identify
    potential areas for improvement. In general, try to treat LLM solutions as no
    different from any supervised machine learning projects and evaluate them vigorously,
    similarly to how you would in a supervised machine learning project. This holistic
    approach to evaluating LLM solutions not only ensures consistent performance and
    compliance but also helps in aligning these powerful models with specific needs
    and objectives, driving innovation and improving outcomes in various applications.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying challenges with LLM solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Despite their impressive capabilities, LLMs face challenges when solving complex
    real-world problems. In this section, we will explore some of the challenges faced
    by LLM solutions and discuss possible ways to tackle them. We will explore challenges
    by high-level groups, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Output and** **input limitations**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLMs just produce text**: Text output can help provide value for a lot of
    businesses. However, many other use cases require predictions and recommendations
    in entirely different formats.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The context size of an LLM is limited**: The issue is that with a large input
    size, you need exponentially more compute resources to train and predict. So,
    context size usually stays in a token range of one to three thousand. This issue
    should be prevalent only for use cases that require long context, as a few thousand
    context sizes should be enough for most use cases.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**An LLM is a text-specific model**: Other data modalities are not supported
    by default.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Repetitive retrieved information**: The information that’s retrieved from
    a knowledge base can be highly relevant but repetitive and numerous. As the context
    size of an LLM is limited, a risk arises when multiple pieces of information are
    placed as the context is repetitive and takes up most of the context limit quota.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge and** **information-related challenges**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Inability to access up-to-date information**: LLMs may not know about recent
    events or developments, leading to outdated or inaccurate information being provided
    in their responses.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Handling low-resource languages**: LLMs can struggle with understanding and
    processing languages with limited data or resources.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unawareness of the progression of time**: LLMs may not understand the concept
    of time, leading to confusion when dealing with time-sensitive information.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Information loss**: LLMs are shown to look at the beginning and end of sentences,
    but not so much the middle, and thus lose the most information placed in the middle.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Single index failures**: This challenge arises when an LLM lacks sufficient
    knowledge about a specific topic or area due to limitations in its training data.
    For instance, if you ask an LLM about a newly opened local restaurant that wasn’t
    covered in its training data, the LLM may provide limited or irrelevant information.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Incomplete content retrieval from documents:** When retrieval of a chunked
    sentence gets the right document, but the actual content needed is below the retrieved
    chunk in the same document, LLMs may not provide the complete or accurate information
    required by the user.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Example: In a documentation search for a software''s installation process,
    the LLM retrieves a section mentioning the installation, but the actual step-by-step
    instructions are located in the following section of the document. Consequently,
    the user only receives an overview without the necessary details for proper installation.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**The use irrelevant information in the context:** LLMs may use irrelevant
    information from their context as a basis for their output, essentially mimicking
    or echoing opinions found in the context even if they are not applicable or appropriate
    for the given situation. This phenomenon, referred to as sycophancy, can lead
    to misleading or unhelpful responses.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The global knowledge base summarization task can’t be executed accurately**:
    A retrieval process is unaware of the type of knowledge base it requests and thus
    can’t execute the global summarization task effectively.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy** **and reliability**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hallucinations**: LLMs can generate false or misleading information that
    may appear plausible but is not based on facts. This phenomenon is known as hallucination.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of mathematical skills**: LLMs often cannot perform precise calculations
    or solve complex mathematical problems. This issue is more widely known as it
    is slightly controversial, depending on how you look at it.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imitative falsehoods**: These are false statements that LLMs generate because
    they mimic common misconceptions found in the training data. Since the model learns
    from the data it’s trained on, it might inadvertently reproduce widely held but
    incorrect beliefs. For example, if many people believe that a specific food causes
    a particular illness, an LLM might generate a similar statement, even if it’s
    not scientifically accurate.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-imitative falsehoods**: These are false statements that arise due to
    the model’s inability to fully achieve its training objective. This includes hallucinations,
    which are statements that seem plausible but are incorrect. For instance, an LLM
    might generate a statement about a historical event that never occurred, but the
    statement may sound convincing to someone who is not knowledgeable about that
    specific event.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Runtime performance issues**: An LLM’s runtime can be slow. Additionally,
    by adding a knowledge base to it, the entire process can become slower than it
    already is.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethical implications and societal impacts**: The widespread adoption and
    deployment of LLMs comes with several ethical implications and societal impacts.
    As these models learn from vast amounts of data, they may inadvertently inherit
    biases present in the training data, leading to biased outputs, perpetuating stereotypes,
    or promoting misinformation. Furthermore, LLMs can generate content that may inadvertently
    promote harmful behavior, hate speech, or violate privacy concerns. The following
    ethical challenges are involved in the usage of an LLM solution:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias and fairness**: Ensuring that the LLM does not exhibit biased behavior
    or discriminate against specific user groups based on their race, gender, age,
    or other protected attributes. Consider the case that a bank uses an LLM to analyze
    loan applications and determine creditworthiness. The LLM has been trained on
    historical data, which may contain biases against certain ethnic groups. As a
    result, the LLM might reject loan applications from these groups at a higher rate,
    even when the applicants have good credit scores.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy concerns**: LLMs may inadvertently generate **personally identifiable
    information** (**PII**) or sensitive data in their outputs, which raises privacy
    concerns and potential legal issues. Consider the case where a healthcare organization
    uses an LLM to generate personalized health recommendations for its clients. The
    LLM can inadvertently include specific patient names and medical conditions in
    the generated advice, which then gets shared publicly, violating patient privacy.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Misinformation and disinformation**: LLMs can potentially generate misleading
    or false information, which can contribute to the spread of misinformation and
    disinformation. Consider the case where an LLM is used by a news agency to automatically
    summarize and publish news articles. The model unintentionally generates a summary
    that misrepresents the original story, leading to the spread of misinformation
    about a crucial business merger.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety**: Ensuring that the content generated by LLMs adheres to ethical
    guidelines, legal regulations, and community standards while avoiding promoting
    harmful or offensive content. Consider the case where an e-commerce platform uses
    an LLM to generate product descriptions for sellers. The LLM can create a description
    that promotes a potentially harmful product, such as a recalled item or an item
    that violates safety regulations, exposing the platform to legal and ethical issues.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparency and explainability**: Ensuring that the decisions made by LLMs
    are transparent, understandable, and justifiable to users and stakeholders. Consider
    the case where an insurance company uses an LLM to assess risk and determine premiums
    for customers. A customer receives a significantly higher premium and requests
    an explanation for the increase. The LLM’s decision-making process is, by itself,
    opaque and difficult to understand, making it challenging for the company to provide
    a clear and justifiable explanation.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have identified these challenges, let’s move on to the next section,
    where we will explore potential solutions and strategies to overcome these limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling challenges with LLM solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tackling the pesky challenges that LLMs face is key to unlocking their full
    potential and making them our trusty tools or sidekicks in solving real-world
    problems. Only by tackling these challenges can an LLM solution be formed objectively
    and effectively. In this section, we’ll dive into various complementary strategies
    that can help us tackle these challenges and boost the performance of LLMs by
    its high-level issue type. We will start with output and input limitations.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling the output and input limitation challenge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Navigating the output and input limitation challenges is vital for unlocking
    the full potential of LLMs, allowing them to efficiently process diverse data
    types, formats, and context sizes while delivering accurate and reliable results.
    The solutions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Customized pre-processing**: Design tailored pre-processing techniques to
    transform non-text data into a format that can be efficiently processed by LLMs.
    For example, design a structure that places structured tabular data as the LLM
    prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use context limit expansion neural network components**: Implement advanced
    neural network components such as LongLORA, which requires you to fine-tune an
    existing model, to expand the context window size, allowing LLMs to process larger
    amounts of information. However, it is essential to note that this option might
    not be available for external LLM providers and might only be feasible if you
    are considering hosting your own LLM model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLM context optimization**: Any wasted space or repetitive content limits
    the depth and breadth of the answers we can extract and generate. There are three
    possible methods here:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select only the most relevant and unique information to be included in the LLM’s
    context window. The maximal marginal relevance algorithm can be used to find a
    set of both relevant and unique sets of information from the distance scores.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider compressing and summarizing the information provided, which can also
    be done by an LLM, and then use the summarized information as context in the main
    LLM prompt.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply knowledge retrieval on demand instead of by default. This on-demand behavior
    can be enforced by treating the RAG as a tool and either teaching the LLM to use
    it via fine-tuning or in-context learning.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will tackle the challenges with knowledge and information.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling the knowledge- and information-related challenge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Addressing the output and input limitation challenges is crucial for enhancing
    the versatility and effectiveness of LLMs in solving a wider range of real-world
    problems across various data modalities and context sizes. The solutions are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time data integration**: Connecting LLMs to real-time data sources such
    as APIs, databases, or web services can help them access up-to-date information
    and provide more accurate responses. Incorporating relevant information from knowledge
    bases, using the RAG approach, is part of this solution. RAG can also help reduce
    hallucinations compared to if a model is fine-tuned with custom data if a rigorous
    strict prompt is made to instruct the LLM to not deviate from the context provided
    in the prompt.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tool integration**: Enhancing LLM architecture by integrating existing tools,
    APIs, and specialized algorithms can significantly extend their capabilities,
    allowing them to tackle tasks that are challenging or impossible for standalone
    models. Tools can be used to retrieve extra input context needed for the generation
    process. Alternatively, they can be used to accomplish specific tasks that the
    generated text tells them to do. Examples include leveraging external search engines,
    domain-specific APIs, and computational libraries to provide accurate responses,
    solve complex mathematical problems, or address queries related to real-time data.
    For LLMs such as GPT-3.5, which have API access, this can be achieved through
    effective few-shot prompting, while advanced models such as **Toolformer** and
    **WebGPT** by OpenAI showcase the potential of integrating external tools seamlessly
    into the LLM’s parametric memory and framework. WebGPT can browse the internet
    by detecting the Bing search engine identifier it generates and subsequently execute
    the search before continuing the generation it’s appended. Toolformer, on the
    other hand, is an LLM that can autonomously select and utilize APIs, integrating
    tools such as calculators, Q&A systems, search engines, translators, and calendars
    for improved generation. This is a key functionality of transforming an LLM into
    an agent that can accomplish real-world tasks.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[1,` `3,` `5,` `7,` `9,` `10,` `8,` `6,` `4,` `2]`, the LLM is encouraged
    to pay equal attention to all parts of the text, reducing the likelihood of missing
    valuable information placed in the middle.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Utilizing the surrounding information from the same document in LLM context:**
    This solution enhances the LLM''s understanding by incorporating additional information
    from the source document. Expanding the scope of retrieval to include surrounding
    text or metadata helps the LLM generate more accurate and comprehensive responses,
    ensuring it considers the broader context. This approach improves the LLM''s ability
    to address complex questions and provide well-informed responses, which effectively
    solves the documentation search use case issue.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filtering out irrelevant context using the LLM:** Before proceeding with
    the generation task, the LLM is employed to identify and remove any irrelevant
    context. This refined context is then used for generating responses. This seemingly
    simple and logical method has demonstrated its effectiveness in most cases as
    introduced in the paper [https://arxiv.org/abs/2311.11829v1](https://arxiv.org/abs/2311.11829v1).
    Moreover, the black box nature of this technique allows for easy implementation,
    contributing to more intuitive and natural LLM-generated content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularly building an up-to-date knowledge base**: To address the issue of
    single index failures, it is essential to maintain and update the LLM’s knowledge
    base regularly. This ensures that the LLM stays current with recent developments
    and can provide accurate information across a wide range of subjects, ultimately
    enhancing its reliability and effectiveness in solving real-world problems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Treat RAG as a tool an LLM can use dynamically based on its generation**:
    This will help solve the problem of not being able to perform summarization at
    the global level of a knowledge base. Similar to how Deadpool is aware of being
    a comic book character, we need the retrieval process to be aware of the type
    of knowledge base it is retrieving from, along with a special handler for summarization
    tasks. A bonus here is to allow the LLM to configure how many rows of relevant
    text to return to the scope of summarization that can be expanded and shrunk as
    required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-index retrieval**: To address the issue of single-index failures, a
    multi-index retrieval approach can be employed. This solution involves decomposing
    – or in other words, chunking – the user’s query into multiple components and
    retrieving information from various sources or knowledge indexes. This multi-faceted
    search strategy helps gather more diverse and comprehensive information, reducing
    the likelihood of overlooking relevant details due to a single index’s limitations.
    Consider a user asking about a rare bird species. Using a single index might yield
    limited information. With a multi-index retrieval approach, the LLM would do the
    following:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decompose the query into components (for example, habitat, diet, and appearance).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve data from various sources (for example, ornithology databases, nature
    websites, and social media).
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregate and synthesize the data to generate a comprehensive response.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Set up directed acyclic graph (DAG) workflows**: Setting up DAG workflows
    involves organizing a series of tasks or processes in a structured, non-circular
    sequence to efficiently process multiple sources of information and extend an
    LLM’s functionality. In the context of LLMs, a DAG workflow can be manually designed
    to connect various tools, APIs, and algorithms while addressing the challenges
    related to real-time data integration, tool integration, and multi-index retrieval.
    Let’s consider a use case where a user wants to plan a trip and needs information
    on various aspects of the destination, such as weather, attractions, and local
    cuisine. An LLM could use the DAG workflow to address this complex query efficiently.
    Here’s an example of a DAG workflow for LLMs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decompose the user’s query into sub-queries or components, specifically, weather
    forecast, top attractions, and cuisines topics.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For each subquery, identify relevant tools, APIs, or data sources. For weather
    forecast, we will retrieve data from a weather API. For top attractions, we will
    extract information from travel website knowledge base. For local cuisine, we
    will gather data from restaurant review website APIs.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply summarization to each fact separately before using it as part of the LLM’s
    input context.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the LLM generation process with the user query and the summarized facts.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Publish the results on a website through an API.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This DAG is depicted in *Figure 19**.3*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 19.3 – An example LLM DAG workflow](img/B18187_19_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 19.3 – An example LLM DAG workflow
  prefs: []
  type: TYPE_NORMAL
- en: By setting up a manual DAG workflow, an LLM can efficiently process information
    from multiple sources, leverage external tools and APIs, and provide accurate
    and reliable responses to a wide range of real-world problems.
  prefs: []
  type: TYPE_NORMAL
- en: This strategy helps the LLM provide a more accurate and detailed response, even
    when information is scarce or not readily available in a single index. The type
    of problem this solves is more commonly known as multi-hop question answering.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will tackle the challenges of accuracy and reliability.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling the challenges of accuracy and reliability
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensuring the accuracy and reliability of LLMs is crucial for building trust
    in their abilities and making them effective problem-solving tools. The solutions
    that can help solve accuracy and reliability-related challenges are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Treat the LLM solution as any modeling experiment**: Pair the LLM with a
    knowledge base, evaluate its performance using relevant metrics, and gather insights
    to fine-tune its capabilities iteratively according to the deep learning life
    cycle. This will help you choose a model that at least produces fewer hallucinations
    and can help you understand its effectiveness for your use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tune retriever embedding models**: This is instead of just depending
    on pre-trained embedding models or embedding model providers. This can improve
    the retrieval accuracy, thereby boosting the quality of LLM-generated responses.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt engineering**: Prompt engineering is the process of crafting effective
    and targeted prompts to guide a language model’s response, thereby improving its
    accuracy, relevance, and overall performance. Consider implementing the following
    techniques:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chain-of-thought (CoT)**: The method encourages LLMs to generate step-by-step
    reasoning traces, leading to more accurate and structured responses for tasks
    involving arithmetic, commonsense reasoning, and other problem-solving scenarios.
    By guiding the LLM through a series of reasoning steps, CoT helps reduce issues
    such as fact hallucination while enhancing the overall quality and coherence of
    the generated content.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ReAct**: This method is a framework that interleaves reasoning traces and
    task-specific actions, enabling LLMs to generate more reliable and factual responses.
    By incorporating dynamic reasoning and interaction with external sources, ReAct
    effectively addresses issues such as fact hallucination and error propagation,
    resulting in improved human interpretability and trustworthiness of LLMs.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt tuning**: Prompt tuning is a technique for refining LLM behavior by
    optimizing the input prompts using gradient-based methods, which allows for better
    control over the model’s responses, and leads to improved accuracy and relevance
    in various problem-solving tasks. By fine-tuning prompts, users can effectively
    guide the LLM to generate more desirable and context-specific outputs. This only
    applies to LLMs you can host yourself, however.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relying on well-engineered prompts**: Leverage published well-engineered
    prompts instead of crafting one of your own. This is a technique used by Langchain
    and Auto-GPT. AutoGPT is an open source Python application based on GPT-4\. It
    automates the execution of tasks without requiring multiple prompts, using AI
    agents to access the web and perform actions with minimal guidance. Unlike ChatGPT,
    AutoGPT can execute larger tasks such as creating websites and developing marketing
    strategies without needing step-by-step instructions. It has various applications,
    such as generating content, designing logos, and developing chatbots.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rejection sampling (best-of-n) reference**: Use rejection sampling techniques
    to improve the quality of generated responses by selecting the best response from
    multiple attempts. The best response can be evaluated through a chosen metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Re-ranking relevance distance scores from knowledge retrieval**: Knowledge
    retrieval is in the domain of recommendation systems. A common technique that’s
    used is to implement proper regression-based recommendation models to re-rank
    relevance distance scores. This can help provide more accurate and potentially
    more personalized relevant information with more contextual data. This technique
    is used by most real-world large-scale recommendation-based products, such as
    YouTube.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative retrieval and generation**: Use techniques such as self-ask, Active
    RAG, and ITER-RETGEN to generate temporary responses, evaluate their quality,
    and iteratively refine them using retrieved knowledge. This approach can reduce
    hallucinations and improve the quality of LLM-generated content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-agent systems**: Develop a multi-agent system composed of agents with
    unique personas and contexts to address diverse problem-solving scenarios. These
    agents can collaborate, share information, and provide more comprehensive and
    reliable solutions. An example of this is **AutoAgents**. AutoAgents is an innovative
    framework that adaptively generates and coordinates multiple specialized agents
    to build an AI team according to different tasks. The framework consists of two
    stages: drafting and execution. In the drafting stage, an agent team and execution
    plan are generated based on the input task, while the execution stage refines
    the plan through inter-agent collaboration and feedback to produce the outcome.
    AutoAgents can dynamically synthesize and coordinate multiple expert agents to
    form customized AI teams for diverse tasks. Experiments on open-ended question-answering
    and trivia creative writing tasks demonstrate the effectiveness of AutoAgents
    compared to existing methods. AutoAgents offers new perspectives for tackling
    complex tasks by assigning different roles to different tasks and promoting team
    cooperation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will dive into solutions for the runtime performance challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling the runtime performance challenge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The runtime performance challenge is a critical issue that can significantly
    impact the efficiency and effectiveness of language models. As LLMs continue to
    grow in complexity and scale, optimizing their runtime performance becomes more
    crucial than ever. The solutions to solve this issue are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Caching outputs**: Temporarily store results to avoid recomputing information,
    enabling faster response times and improved performance. This approach is particularly
    useful when dealing with repetitive or similar queries.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPUs and GPU inference accelerators**: This is only applicable for LLMs you
    host yourself. LLMs need to run with these components to run in a reasonable time.
    These were introduced in more detail in [*Chapter 15*](B18187_15.xhtml#_idTextAnchor217),
    *Deploying Deep Learning Models* *to Production*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scann` algorithm and the `faiss` *IVFPQFS* algorithm provide a good balance
    between index build time, index size, retrieval recall, and retrieval runtime.
    However, an approximate KNN algorithm is only required with large knowledge bases
    since the retrieval speed of a small knowledge base is already fast, which is
    less than 1 second. Typically, suitable data dimensions lie in the range of three-digit
    vector column sizes, and seven-digit vector row sizes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tackling the challenge of ethical implications and societal impacts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Addressing the ethical implications and societal impacts of LLM solutions is
    crucial for ensuring their responsible and sustainable deployment across various
    applications. By considering the ethical and societal consequences of LLM-generated
    content, developers can create models that respect user values, adhere to legal
    guidelines, and contribute positively to society.
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategies to tackle these challenges are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias and fairness mitigation**: Consider the following strategy in the context
    of the methods that were introduced in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196),
    *Exploring Bias* *and Fairness*:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data collection and preparation**: Ensure a diverse and representative dataset
    for fine-tuning LLM models. Balance sensitive attributes in the data, and eliminate
    or control potential biases that may arise from these attributes. Additionally,
    you can instruct the LLM to specifically not perpetuate bias in natural language
    as part of the input context. Better yet, empower users to define their preferences,
    values, and ethical guidelines, enabling LLMs to generate content that aligns
    with individual user needs and values.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias mitigation during fine-tuning**: Implement techniques such as counterfactual
    data augmentation, adversarial training, or re-sampling during the fine-tuning
    process to reduce the impact of biased features and improve fairness.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Post-processing**: Modify LLM-generated content using techniques such as
    equalized odds post-processing to ensure fairness in the outputs. This can be
    applied when using LLM providers such as OpenAI GPT-4 or fine-tuned open source
    models.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and evaluation**: Continuously monitor LLM-generated content for
    potential biases using bias and fairness metrics, and adjust the model as needed
    to ensure compliance with ethical guidelines and fairness requirements.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy-preserving techniques**: Adopt privacy-preserving approaches, such
    as differential privacy, federated learning, and homomorphic encryption, to protect
    sensitive information in the training data and generated content. Implement policies
    and guidelines to prevent the inadvertent disclosure of **Personal Identifiable
    Information** (**PII**) in LLM-generated content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fact-checking and credibility assessment**: Incorporate fact-checking and
    credibility assessment mechanisms into LLM solutions to reduce the risk of generating
    misleading or false information. This can involve integrating LLMs with external
    knowledge sources, such as knowledge databases, to verify the accuracy of generated
    content. You can also instruct the LLM to be humble and not return a statement
    if no facts can be used for verification.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content moderation and guardrails**: Implement content moderation techniques,
    such as keyword filtering, machine learning-based classifiers, and human-in-the-loop
    review processes, to prevent the generation of harmful or offensive content. Establish
    guardrails, such as toxicity thresholds or ethical guidelines, to ensure that
    LLM-generated content adheres to community standards and legal regulations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparency and explainability**: Develop methods for enhancing the transparency
    and explainability of LLM-generated content, such as providing reasoning traces,
    saliency maps, or counterfactual explanations. The concepts that were introduced
    in *Chapter 11**, Explaining Neural Network Predictions*, and [*Chapter 12*](B18187_12.xhtml#_idTextAnchor184)*,
    Interpreting Neural Networks*, can be applied to an LLM.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By implementing these strategies, developers can create LLM solutions that not
    only respect user values and adhere to legal guidelines but also contribute positively
    to society. Addressing the ethical implications and societal impacts of LLMs is
    an essential step toward building trust in the technology and ensuring its responsible
    and sustainable deployment across various applications.
  prefs: []
  type: TYPE_NORMAL
- en: With a comprehensive understanding of the challenges associated with LLMs mentioned,
    as well as their potential solutions, we can now turn our attention to addressing
    the overarching challenge of LLM solution adoption across organizations and industries.
  prefs: []
  type: TYPE_NORMAL
- en: Tackling the overarching challenge of LLM solution adoption
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One overarching challenge in realizing the full potential of LLM solutions lies
    in their adoption across organizations and industries. Similar to the adoption
    of any machine learning or deep learning solution, the key factor driving the
    adoption of LLMs is confidence. Confidence in the technology’s capabilities, its
    effectiveness in addressing specific use cases, and its ability to deliver tangible
    results are essential for widespread adoption.
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this challenge, it is crucial to systematically educate organizations
    about the power of LLMs, their diverse applications, and how they can be tailored
    to meet specific needs. This includes demonstrating the benefits of LLMs through
    real-world success stories, providing practical guidance on implementing LLM solutions,
    and offering support for organizations navigating the complexities of integrating
    LLMs into their workflows.
  prefs: []
  type: TYPE_NORMAL
- en: Building confidence in LLM solutions involves thoroughly evaluating their performance,
    addressing the challenges discussed earlier in this chapter, and ensuring that
    the solutions meet their intended objectives. By implementing a comprehensive
    set of evaluation approaches, including quantitative metrics and qualitative manual
    evaluations, organizations can better assess the performance, accuracy, and overall
    quality of the results from an LLM solution. These evaluations should be conducted
    iteratively, allowing for ongoing refinement and improvement of the LLM solution.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, addressing the challenges identified in this chapter, such as
    output and input limitations, knowledge and information-related challenges, accuracy
    and reliability issues, and runtime performance challenges, is essential for building
    confidence in LLM solutions. By leveraging the strategies and techniques discussed
    in this chapter, organizations can optimize the performance of LLMs and ensure
    their effectiveness in various applications.
  prefs: []
  type: TYPE_NORMAL
- en: Another essential aspect of building confidence in LLM solutions is effective
    communication and collaboration with stakeholders. This includes sharing evaluation
    results, discussing the benefits and potential limitations of LLMs, and addressing
    any concerns that stakeholders may have regarding the adoption of LLM solutions.
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, adopting LLM solutions successfully requires a combination of
    rigorous evaluation, addressing challenges, and effective communication with stakeholders.
    By treating the adoption of LLM solutions with the same level of care as any machine
    learning or deep learning solution, organizations can build confidence in the
    capabilities and performance of LLMs, unlocking their full potential in a wide
    range of real-world applications. And with that, we have explored the challenges
    that plague LLMs and their solutions in detail.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will dive deeper into leveraging LLMs to build
    autonomous agents, which can significantly expand and improve our problem-solving
    skills.
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging LLM to build autonomous agents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One promising area in which LLMs can be harnessed is the development of autonomous
    agents that can efficiently solve complex problems and interact with their environment.
    This section will focus on leveraging LLMs to build such agents and discuss the
    key aspects that contribute to their effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Autonomous agents are AI-driven entities that can perform tasks, make decisions,
    and interact with their environment independently. By incorporating LLMs into
    these agents, developers can create versatile and adaptive systems that can tackle
    a wide range of challenges. Here are some essential components of LLM-powered
    autonomous agents:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Planning and decision making**: LLMs can be utilized to generate plans and
    strategies that guide the agent’s actions, taking into account the context and
    goals.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observing and learning from the environment**: LLMs can be trained to observe
    and interpret the environment, learning from past experiences and adjusting their
    behavior accordingly.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaborative problem-solving**: Multi-agent systems can be developed, where
    each agent has a unique persona and context. These agents can collaborate, share
    information, and provide more comprehensive and reliable solutions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-refinement**: Autonomous agents can use LLMs to analyze their performance,
    identify areas for improvement, and refine their strategies and behaviors over
    time.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agents encompass parts and pieces of the solutions to the challenges identified
    in the *Identifying challenges with LLM solutions* section of this chapter. Additionally,
    the solutions that were introduced here can be combined to expand the scope of
    problems the overall architected LLM solution can cover. Examples of published
    agent methods that were also introduced earlier in this chapter are WebGPT, Toolformer,
    Auto-GPT, and AutoAgents. Autonomous agents that leverage LLMs are the key to
    making powerful LLM solutions. By combining the strengths of LLMs with the adaptability
    and decision-making capabilities of agents, developers can create groundbreaking
    systems that can revolutionize various domains and industries.
  prefs: []
  type: TYPE_NORMAL
- en: With a comprehensive understanding of LLM solutions and their potential applications,
    let’s explore some specific use cases where they can be employed effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring LLM solution use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore some fascinating real-world applications where
    LLM solutions can truly shine. This will give you a sense of how revolutionary
    LLM solutions are. The use cases are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Travel itinerary planner**: LLMs can be employed to develop advanced travel
    itinerary planners that generate personalized trip plans based on user preferences
    and constraints. By integrating LLMs with travel APIs, such as flight, hotel,
    and attraction databases, as well as real-time data sources such as weather and
    traffic information, these planners can provide context-aware recommendations
    tailored to individual traveler needs. Notably, companies such as Booking.com
    and Expedia integrated this into their products, and Agoda announced that they
    will be working on it.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intelligent tutoring systems**: LLMs can be used to develop intelligent tutoring
    systems that offer personalized learning experiences for students. By integrating
    LLMs with educational content, assessment tools, and learner data, these systems
    can generate targeted learning materials, provide real-time feedback, and adapt
    to individual learning needs. This enables a more efficient and engaging learning
    experience for students. Notably, Duolingo is a company that implemented such
    a solution in their gamified language learning product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated email responses**: LLM solutions can be employed to develop automated
    email response systems that handle various types of inquiries, such as customer
    support, sales inquiries, or general information requests. By integrating LLMs
    with email APIs, CRM systems, and relevant knowledge bases, email responses can
    be personalized, accurate, and contextually relevant. This helps businesses streamline
    their customer communication and provide efficient support. Notably, Nanonets
    AI is a company that implemented this as part of their product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code generation**: LLMs can be used to generate code snippets, algorithms,
    or entire software programs based on user input or specific requirements. Solutions
    such as GitHub Copilot leverage LLMs to assist developers in writing code, suggesting
    relevant code snippets, and completing sections of code based on context. By integrating
    LLMs with code repositories, programming language APIs, and domain-specific knowledge
    bases, code generation can be tailored to specific programming languages, frameworks,
    and use cases, improving developer productivity.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer support chatbots**: LLM solutions can be employed to develop advanced,
    context-aware chatbots that can handle customer inquiries and support requests
    more effectively. By integrating LLMs with **customer relationship management**
    (**CRM**) systems and knowledge bases, chatbots can provide personalized and accurate
    responses to customer queries. This helps businesses improve their customer support
    services, reduce response times, and increase customer satisfaction. Companies
    such as forethought.ai, Ada, and EBI.AI provide such a solution in their product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical diagnosis and treatment suggestions**: LLMs can be employed to develop
    advanced diagnostic tools that analyze patient symptoms, medical history, and
    relevant medical literature to suggest potential diagnoses and treatment options.
    By integrating LLMs with **electronic health record** (**EHR**) systems, medical
    databases, and domain-specific knowledge bases, these tools can help healthcare
    professionals make more informed decisions and improve patient outcomes. Notably,
    Harman, a Samsung company, implemented and offered such a solution as part of
    their offered services.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personal finance management**: LLMs can be used to develop intelligent personal
    finance management applications that provide tailored financial advice, budgeting
    suggestions, and investment recommendations based on user-specific financial goals
    and risk tolerance. By integrating LLMs with banking APIs, stock market data,
    and financial knowledge bases, these applications can offer context-aware financial
    planning and guidance to users. Although not exactly a service or product, Bloomberg
    has developed BloombergGPT, a 50-billion parameter large language model designed
    specifically for finance, which showcases the potential of LLMs in the financial
    domain.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creative content generation**: LLMs can be employed to generate creative
    content, such as stories, poetry, or music, based on user inputs, preferences,
    or inspirations. By integrating LLMs with databases of literary works, music libraries,
    and knowledge bases on creative techniques and styles, these applications can
    produce unique and engaging content that caters to individual artistic tastes
    and needs. Notably, Jasper built a platform to account for this use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legal document analysis and drafting**: LLMs can be used to develop advanced
    legal document analysis and drafting tools that assist legal professionals in
    reviewing contracts, identifying potential issues, and generating legal documents
    based on specific requirements. By integrating LLMs with legal databases, contract
    templates, and domain-specific knowledge bases, these tools can help streamline
    legal work and improve efficiency in the legal industry. Notably, netdocuments
    implemented this use case with their product.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smart home automation**: LLMs can be employed to develop intelligent home
    automation systems that understand natural language commands and adapt to user
    preferences and routines. By integrating LLMs with smart home devices, APIs, and
    user behavior data, these systems can provide a more intuitive and personalized
    home automation experience, enabling users to control their home environment with
    ease and convenience. Amazon Alexa is a prime example of this use case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each of these use cases, integrating LLMs with the relevant tools, APIs,
    and data sources ensures that the generated content, recommendations, and responses
    are accurate, contextually relevant, and tailored to specific needs, enhancing
    user experiences and providing valuable support in various domains.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored LLMs and their potential to address real-world
    problems and create value across various applications. We discussed the key aspects
    of architecting LLM solutions, such as handling knowledge, interacting with real-time
    data and tools, evaluating LLM solutions, identifying and addressing challenges,
    and leveraging LLMs to build autonomous agents. We also emphasized the importance
    of retrieval-augmented language models for providing contextually relevant information
    and examined various techniques and libraries to improve LLM solutions.
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed the limitations of LLMs, such as output and input limitations,
    knowledge and information-related challenges, accuracy and reliability issues,
    runtime performance challenges, ethical implications and societal impacts, and
    the overarching challenge of LLM solution adoption. To tackle these limitations,
    we presented various complementary strategies, such as real-time data integration,
    tool integration, prompt engineering, rejection sampling, multi-agent systems,
    runtime optimization techniques, bias and fairness mitigation, content moderation,
    and enabling LLM transparency and explainability. Lastly, we discussed leveraging
    LLMs to build autonomous agents, which can significantly expand and improve problem-solving
    abilities in diverse applications.
  prefs: []
  type: TYPE_NORMAL
- en: By understanding the intricacies of LLM solutions and applying the strategies
    and techniques discussed in this chapter, organizations and individuals can harness
    the full potential of LLMs to drive innovation and improve outcomes across various
    applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'By reading *The Deep Learning Architect Handbook*, you have gained invaluable
    insights into the various stages of the deep learning life cycle, exploring critical
    aspects from planning and data preparation to model deployment and governance.
    By reaching the end of this enlightening journey, you are now armed with the knowledge
    and skills to design, develop, and deploy effective deep learning solutions. To
    build upon this strong foundation, consider taking the following next steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Apply your newfound knowledge to real-world projects, either in your professional
    field or through open source contributions, to gain practical experience and deepen
    your understanding
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stay up-to-date with the latest research, trends, and breakthroughs in deep
    learning by attending conferences, following influential researchers, and reading
    research papers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore specialized areas within deep learning that interest you, such as reinforcement
    learning, generative adversarial networks, or few-shot learning, to further expand
    your expertise
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaborate with fellow deep learning enthusiasts and professionals, joining
    communities, discussion forums, and social media groups to exchange ideas, share
    experiences, and learn from each other
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider pursuing advanced courses, certifications, or even a degree in deep
    learning or a related field to enhance your education and qualifications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embrace the challenges and triumphs that lie ahead, for with the mastery of
    building deep learning models with intricate deep learning architectures, a keen
    understanding of bias and fairness, and the ability to monitor and maintain model
    performance, you are well-prepared to unleash the full potential of deep learning
    and drive innovation across a vast array of applications. Here’s to your continued
    success and growth in the world of deep learning!
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*RAG*: [https://doi.org/10.48550/arXiv.2005.11401](https://doi.org/10.48550/arXiv.2005.11401)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*RETRO*: [https://arxiv.org/pdf/2112.04426.pdf](https://arxiv.org/pdf/2112.04426.pdf)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*REALM*: [https://doi.org/10.48550/arXiv.2002.08909](https://doi.org/10.48550/arXiv.2002.08909)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
