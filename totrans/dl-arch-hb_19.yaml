- en: '19'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Architecting LLM Solutions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Large language models** (**LLMs**) have revolutionized the field of **natural
    language processing** (**NLP**) and **artificial intelligence** (**AI**), offering
    remarkable versatility in tackling a variety of tasks. However, realizing their
    full potential requires addressing certain challenges and developing effective
    LLM solutions. In this chapter, we’ll demystify the process of architecting LLM
    solutions, focusing on essential aspects such as memory, problem-solving capabilities,
    autonomous agents, and advanced tools for enhanced performance. We will be focusing
    on retrieval-augmented language models, which provide contextually relevant information,
    their practical applications, and methods to improve them further. Additionally,
    we’ll uncover the challenges, best practices, and evaluation methods to ensure
    the success of an LLM solution.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Building upon these foundational concepts, this chapter will equip you with
    the knowledge and techniques necessary to create powerful LLM solutions tailored
    to your specific needs. By mastering the art of architecting LLM solutions, you
    will be better prepared to tackle complex challenges, optimize performance, and
    unlock the true potential of these versatile models in a wide range of real-world
    applications.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following topics:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Overview of LLM solutions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Handling knowledge for LLM solutions
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating LLM solutions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying challenges with LLM solutions
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tackling challenges with LLM solutions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging LLMs to build autonomous agents
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring LLM solution use cases
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Overview of LLM solutions
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs excel in diverse tasks such as answering questions, machine translation,
    language modeling, sentiment analysis, and text summarization. They generate unstructured
    text but can be guided to produce structured output. LLM solutions harness this
    ability and leverage custom data from knowledge bases to create targeted, valuable
    outcomes for organizations and individuals. By properly streamlining processes
    and enhancing output quality objectively, an LLM solution can unlock the true
    potential of LLM-generated content, making it more powerful and practical across
    applications.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: The increasing accessibility of LLMs with pre-trained world knowledge has played
    a significant role in making these benefits more attainable for a broader audience.
    Thanks to various LLM providers and open source platforms, organizations and developers
    can now more easily adopt and integrate LLMs into their workflows. Prominent LLM
    providers, such as OpenAI (GPT-4 or GPT-3.5), Microsoft Azure, Google, and Amazon
    Bedrock, offer pre-trained models and APIs that can be seamlessly integrated into
    diverse applications. Additionally, the Hugging Face platform has made LLMs even
    more accessible by offering an extensive collection of open source models. Hugging
    Face provides a wide selection of pre-trained models and fine-tuning techniques
    while fostering an active community that continually contributes to enhancing
    LLMs.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 随着预训练世界知识的LLM逐渐变得更加普及，这在使更多人群能够获得这些好处方面发挥了重要作用。得益于各种LLM提供商和开源平台，组织和开发者现在可以更轻松地采纳并将LLM集成到他们的工作流程中。知名的LLM提供商，如OpenAI（GPT-4或GPT-3.5）、Microsoft
    Azure、Google和Amazon Bedrock，提供预训练模型和API，能够无缝集成到各种应用程序中。此外，Hugging Face平台通过提供丰富的开源模型库，使LLM更加易于访问。Hugging
    Face不仅提供了广泛的预训练模型和微调技术，还培养了一个积极的社区，持续贡献于LLM的改进。
- en: As organizations and individuals harness the power of LLMs for their typical
    tasks and use cases, it is crucial to determine how to leverage custom knowledge
    effectively. This consideration ensures that LLMs are optimally utilized to address
    specific needs; this will be explored further in the *Handling knowledge for LLM
    solutions* section. By taking advantage of the increased accessibility and versatility
    of LLMs, organizations and individuals can unlock the full potential of these
    powerful models to drive innovation and improve outcomes.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 随着组织和个人利用LLM的强大功能来完成日常任务和用例，至关重要的是要确定如何有效利用定制知识。这一考虑确保了LLM能够针对特定需求得到最优利用；这一点将在*LLM解决方案中的知识处理*部分进一步探讨。通过充分利用LLM日益增加的可达性和多功能性，组织和个人能够释放这些强大模型的全部潜力，推动创新并改善结果。
- en: 'Despite their impressive capabilities, LLMs face some limitations when it comes
    to solving more complex problems that they were not made to account for. Some
    of these limitations are as follows:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大语言模型（LLMs）具有令人印象深刻的能力，但在解决它们未被设计用来处理的复杂问题时，仍面临一些限制。以下是其中的一些限制：
- en: Inability to access up-to-date information on recent events
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无法访问最新事件的信息
- en: Tendency to hallucinate facts or generate imitative falsehoods
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 倾向于虚构事实或生成模仿的虚假信息
- en: Difficulties in understanding low-resource languages
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解低资源语言的困难
- en: Lack of mathematical skills for precise calculations
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缺乏进行精确计算的数学能力
- en: Unawareness of the progression of time
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对时间流逝的无意识
- en: 'To overcome these limitations and enhance LLMs’ problem-solving capabilities,
    advanced solutions can be developed by incorporating the following components:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这些限制并增强LLM的解决问题能力，可以通过加入以下组件来开发先进的解决方案：
- en: '**Real-time data integration**: By connecting LLMs to real-time data sources
    such as APIs, databases, or web services, the model can access up-to-date information
    and provide more accurate responses.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实时数据集成**：通过将LLM连接到实时数据源，如API、数据库或网络服务，模型可以获取最新的信息并提供更准确的回应。'
- en: '**Existing tool integration**: Incorporating existing tools and APIs into the
    LLM architecture can extend its capabilities, allowing it to perform tasks that
    would otherwise be impossible or challenging for a standalone model.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**现有工具集成**：将现有工具和API集成到LLM架构中，可以扩展其能力，使其执行一些单独模型难以或无法完成的任务。'
- en: '**Multiple agents with different personas and contexts**: Developing a multi-agent
    system where each agent possesses a unique persona and context can help address
    the challenges of diverse problem-solving scenarios. These agents can collaborate,
    share information, and provide more comprehensive and reliable solutions.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**具有不同人格和背景的多个代理**：开发一个多代理系统，每个代理具有独特的人格和背景，可以帮助解决不同问题情境下的挑战。这些代理可以协作、共享信息并提供更全面、可靠的解决方案。'
- en: '*Figure 19**.1* shows an architecture that depicts the different approaches
    and methods that can be applied in an LLM solution and will be introduced in this
    chapter:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*图19.1* 显示了一种架构，展示了可以在LLM解决方案中应用的不同方法和技巧，本章将介绍这些内容：'
- en: '![Figure 19.1 – LLM solution architecture](img/B18187_19_1.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图19.1 – LLM解决方案架构](img/B18187_19_1.jpg)'
- en: Figure 19.1 – LLM solution architecture
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.1 – LLM 解决方案架构
- en: In the next few sections, we will dive into the individual components listed
    in this LLM solution architecture and LLM solutions in general more comprehensively.
    We will start with how knowledge is handled for LLM solutions.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几个部分，我们将更全面地深入探讨此 LLM 解决方案架构中列出的各个组件以及 LLM 解决方案的整体内容。我们将从如何处理 LLM 解决方案中的知识开始。
- en: Handling knowledge for LLM solutions
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理 LLM 解决方案中的知识
- en: Domain knowledge is key in creating LLM solutions as it provides the background
    information and understanding they need to solve specific problems. This ultimately
    ensures the answers or actions the solutions come up with are on point and helpful.
    Domain knowledge needs to be included as context either as part of parametric
    memory, non-parametric memory, or a combination of both. Parametric memory refers
    to the parameters that are learned in an LLM. Non-parametric memory refers to
    an external library of knowledge, such as a list of documents, articles, or excerpts,
    that can be selectively chosen to be injected as part of the LLM context. This
    process is also referred to as an in-context learning method, knowledge retrieval,
    or information retrieval.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 域知识是创建 LLM 解决方案的关键，它提供了解决特定问题所需的背景信息和理解。这最终确保了解决方案得出的答案或行动是准确的且有帮助的。域知识需要作为上下文包含在内，作为参数化记忆、非参数化记忆的一部分，或两者的结合。参数化记忆指的是在
    LLM 中学习到的参数。非参数化记忆指的是外部知识库，如文档、文章或摘录列表，可以根据需要选择注入为 LLM 上下文的一部分。这个过程也被称为上下文学习方法、知识检索或信息检索。
- en: 'Non-parametric external knowledge can be provided to an LLM through either
    of the following options:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数化的外部知识可以通过以下任一方式提供给 LLM：
- en: '**As a latent conditioner in the cross-attention mechanism**: Latent conditioning
    involves generating latent feature vectors from the external knowledge and feeding
    it as part of the key and value vectors in the attention mechanism that were introduced
    in [*Chapter 6*](B18187_06.xhtml#_idTextAnchor092), *Understanding Neural Network
    Transformers*, while the original input is passed in as the query vector. This
    approach typically requires some form of fine-tuning the decoder part of the network
    in an encoder-decoder transformer architecture. Ideally, the fine-tuning process
    will build a decoder that can generalize to the domain of the intended external
    latent features and can attend to a variety of information. This approach allows
    the inclusion of any data modality as external knowledge. Notably, the **Retrieval
    Augmented Generation** (**RAG**) and **Retrieval-Enhanced Transformer** (**RETRO**)
    methods from published research papers [1][2] use this approach.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作为交叉注意力机制中的潜在条件**：潜在条件涉及从外部知识生成潜在特征向量，并将其作为键值向量的一部分输入到注意力机制中，该机制在[*第 6 章*](B18187_06.xhtml#_idTextAnchor092)《理解神经网络转换器》中已介绍，同时原始输入作为查询向量传入。此方法通常需要对网络的解码器部分进行某种形式的微调，尤其是在编码器-解码器的转换器架构中。理想情况下，微调过程将构建一个解码器，使其能够推广到目标外部潜在特征的领域，并能够关注各种信息。此方法允许将任何数据模态作为外部知识进行包含。值得注意的是，发布的研究论文中的**检索增强生成**（**RAG**）和**检索增强转换器**（**RETRO**）方法[1][2]采用了这种方法。'
- en: '**As part of an LLM’s input prompt**: This is a straightforward process that
    doesn’t require any fine-tuning but can still benefit from it. This approach brings
    the lowest barrier of entry to leverage any custom domain knowledge in LLMs. However,
    this approach only supports knowledge represented in data modalities that can
    be effectively represented as textual data, such as text, numerical, categorical,
    and date data. Notably, the **Retrieval-Augmented Language Model Pre-Training**
    (**REALM**) method, as part of a published research paper [3], uses this approach
    for pre-training specifically and doesn’t use it as part of the final trained
    model.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**作为 LLM 输入提示的一部分**：这是一个简单的过程，不需要任何微调，但仍然可以从中受益。这种方法为将任何自定义领域知识应用于 LLM 提供了最低的入门门槛。然而，这种方法仅支持以能够有效表示为文本数据的数据模态表示的知识，如文本、数值、类别和日期数据。值得注意的是，发布的研究论文中的**检索增强语言模型预训练**（**REALM**）方法[3]，特别用于预训练，并未作为最终训练模型的一部分使用。'
- en: 'Both methods require a knowledge base to be established, as depicted in *Figure
    19**.2*, and a knowledge retrieval component that retrieves information from the
    knowledge base, as depicted in *Figure 19**.1*:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法都需要建立一个知识库，如*图 19.2*所示，并且需要一个知识检索组件来从知识库中检索信息，如*图 19.1*所示：
- en: '![Figure 19.2 – Establishing a knowledge base](img/B18187_19_2.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 19.2 – 构建知识库](img/B18187_19_2.jpg)'
- en: Figure 19.2 – Establishing a knowledge base
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图 19.2 – 构建知识库
- en: 'A short tabular summary of the REALM, RETRO, and RAG methods is presented in
    *Table 19.1*:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*表 19.1* 简要总结了REALM、RETRO和RAG方法：'
- en: '| **Knowledge** **retrieval methods** | **Retriever training** | **Retrieval
    integration** |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| **知识** **检索方法** | **检索器训练** | **检索集成** |'
- en: '| RAG | Fine-tune with a frozen base network | Latent conditioner with cross-attention
    |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| RAG | 使用冻结的基础网络进行微调 | 带有交叉注意力的潜在调节器 |'
- en: '| REALM | Full end-to-end training | Prepend to prompt specifically without
    a template |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| REALM | 完整的端到端训练 | 特别地将其预先加入提示中，不使用模板 |'
- en: '| RETRO | Fine-tune with a frozen base network | Latent conditioner with cross-attention
    |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| RETRO | 使用冻结的基础网络进行微调 | 带有交叉注意力的潜在调节器 |'
- en: Table 19.1 – Short overview of retrieval integration with LLM methods
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 表 19.1 – 与LLM方法的检索集成简短概述
- en: A knowledge base requires that text data is pre-processed into appropriate logical
    chunks or segments. These segments are then transformed into embedding vectors
    using trained transformer models. Finally, a nearest neighbor index is constructed,
    enabling efficient retrieval of relevant information from the knowledge base.
    The nearest neighbor index can either be a simple KNN algorithm that computes
    raw distances between the prompt embedding vector, or an approximate KNN algorithm
    that approximates the distance computations. Both the index and the logical text
    chunks will then serve as the knowledge base, which can be used for retrieval.
    The method to perform retrieval can vary with different strategies, but the simplest
    form involves simply generating embedding from the prompt and returning the top
    *k* closest text chunks from the knowledge base using the index. These top *k*
    closest text chunks can then be included as part of the LLM prompt or as a latent
    conditioner.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 知识库要求将文本数据预处理成适当的逻辑块或段落。这些段落随后通过训练好的变换器模型转化为嵌入向量。最后，构建最近邻索引，使得能够高效地从知识库中检索相关信息。最近邻索引可以是简单的KNN算法，用来计算提示嵌入向量之间的原始距离，或者是近似KNN算法，近似计算距离。然后，索引和逻辑文本段落将作为知识库的一部分，可以用于检索。执行检索的方法可以根据不同策略有所不同，但最简单的形式是通过生成提示的嵌入向量，并使用索引返回知识库中与之最接近的前*k*个文本段落。这些前*k*个最接近的文本段落可以作为LLM提示的一部分或作为潜在调节器包含进去。
- en: For the approach of including the most relevant text chunks as part of the prompt,
    crafting a prompt template that can allow a specific spot to be inserted is the
    standard and can help organize information in a prompt properly. This can be as
    simple as using leading text such as `Context:`, following up with the retrieved
    relevant text chunks, and having new line separation before and after the context
    part of the prompt template.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于将最相关的文本切块作为提示的一部分的方式，制作一个可以插入特定内容的提示模板是标准做法，并有助于合理地组织提示中的信息。这可以像使用引导性文本`Context:`一样简单，接着是检索到的相关文本切块，并在提示模板的上下文部分之前和之后插入换行符。
- en: While research papers often present published methods that encompass various
    aspects of the retrieval process in a single method, it is helpful to consider
    each component of building and using the knowledge base as separate, interchangeable
    parts. This allows for greater flexibility in selecting the most suitable components
    for specific situations. Moreover, although there is a published method known
    as RAG, it is worth noting that, in practice, the term RAG is commonly used to
    describe the general approach of integrating knowledge retrieval with LLMs, rather
    than referring solely to that specific method. Let’s briefly go through the three
    key method-based components that can be freely modified according to the use case.
    We will also choose orchestrator tools that help streamline the implementation
    of these components.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管研究论文通常会提出涵盖检索过程各个方面的已发布方法，但将构建和使用知识库的每个组件视为独立的、可互换的部分是很有帮助的。这允许在特定情况下选择最合适的组件，从而具有更大的灵活性。此外，尽管存在一种已发布的方法被称为RAG，但值得注意的是，在实践中，RAG这一术语通常用来描述将知识检索与LLM集成的通用方法，而不仅仅指该特定方法。接下来，我们将简要介绍可以根据用例自由修改的三大基于方法的组件。我们还将选择帮助简化这些组件实现的协调工具。
- en: Exploring chunking methods
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索文本切块方法
- en: 'The text chunking process affects the efficiency of LLM context utilization
    and the quality of the resulting LLM generation. Choosing an appropriate chunking
    method depends on the following factors:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: '**The embedding model used for embedding vector generation**: Different pre-trained
    embedding models may have different requirements or limitations when it comes
    to text chunking. Two such requirements are the supported context size and the
    typical text context size that was used during pre-training.'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The granularity of information needed for the expected prompts that will
    be made**: The level of detail or granularity required for the prompts can impact
    the choice of text chunking method. Depending on the specific use case, the method
    should be able to chunk the text into appropriate and concise segments that provide
    the necessary information for the LLM to generate accurate, concise, and relevant
    responses.'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The nature of the text data used to build a knowledge base**: The characteristics
    of the text data itself can also influence the choice of text chunking method.
    For example, if the text data consists of long paragraphs or documents, a method
    that breaks the text into smaller chunks or sections may be more suitable. On
    the other hand, if the text data is already organized into logical segments, a
    method that preserves these segments may be preferred. Also, if the text data
    is Python code, it can be suitable to chunk the text by code methods.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are several methods for chunking text, including sentence, paragraph,
    entity, topic, and section chunking. These methods help organize the text into
    meaningful units that can be processed by the LLM. One notable method that is
    useful and readily available is the recursive chunking method from the LangChain
    library. This method allows you to adjust the granularity of the chunks by recursively
    splitting the text using an ordered list of text separators, a maximum chunk size,
    and the percent of overlap between chunks. The maximum chunk size should be tailored
    to the context size supported by the embedding model, ensuring that the generated
    chunks can be effectively processed. Meanwhile, incorporating an overlap percentage
    helps minimize the risk of missing critical information that could be located
    at the boundaries of chunks.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: Many document-specific chunking methods are created based on this recursive
    chunking method by specifying the appropriate ordered list of text separators.
    Specifically, as of `langchain==0.0.314`, recursive methods have been created
    for Python code with `PythonCodeTextSplitter`, markdown documents with the `MarkdownTextSplitter`
    class, and LaTeX-formatted text with the `LatexTextSplitter` class.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s dive into embedding model choices.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: Exploring embedding models
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Embedding models play a crucial role in generating a knowledge base for LLM
    solutions. These models are responsible for encoding the semantic information
    of text into vector representations, which are then used to retrieve relevant
    information from the knowledge base.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入模型在为 LLM 解决方案生成知识库中起着至关重要的作用。这些模型负责将文本的语义信息编码为向量表示，然后这些向量用于从知识库中检索相关信息。
- en: One benchmark that provides insights into the performance of text embedding
    models is the `text-embedding-ada-002`.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 一个提供文本嵌入模型性能洞察的基准是 `text-embedding-ada-002`。
- en: When selecting an embedding model for knowledge base generation, it is essential
    to consider factors such as model size, embedding dimensions, and sequence length.
    Traditional embedding models such as GloVe offer high speed but may lack context
    awareness, resulting in lower average scores. On the other hand, models such as
    `all-mpnet-base-v2` and `all-MiniLM-L6-v2` strike a balance between speed and
    performance, providing satisfactory results. For maximum performance, larger models
    such as `bge-large-en-v1.5`, `ember-v1`, and `e5-large-v2` dominate the MTEB leaderboard,
    all with a 1.34 GB model size.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择用于生成知识库的嵌入模型时，必须考虑模型大小、嵌入维度和序列长度等因素。传统的嵌入模型，如 GloVe，提供高速度，但可能缺乏上下文意识，导致较低的平均得分。另一方面，像
    `all-mpnet-base-v2` 和 `all-MiniLM-L6-v2` 这样的模型在速度和性能之间取得了平衡，提供了令人满意的结果。为了获得最佳性能，像
    `bge-large-en-v1.5`、`ember-v1` 和 `e5-large-v2` 这样的更大模型主导了 MTEB 排行榜，所有这些模型的大小为
    1.34 GB。
- en: It’s important to note that the choice of embedding model depends on the specific
    task and dataset being used. Therefore, thoroughly exploring the various tabs
    of the MTEB leaderboard and considering the requirements of the knowledge base
    generation process can help in selecting the most suitable embedding model.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，嵌入模型的选择取决于具体的任务和使用的数据集。因此，深入探索 MTEB 排行榜的不同选项，并考虑知识库生成过程的要求，有助于选择最合适的嵌入模型。
- en: MTEB, with its extensive collection of datasets and evaluation metrics, serves
    as a valuable resource for researchers and practitioners in the field of NLP.
    By leveraging the insights provided by MTEB, developers can make informed decisions
    when choosing an embedding model for knowledge base generation in LLM solutions.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: MTEB 通过其丰富的数据集和评估指标集合，为 NLP 领域的研究人员和实践者提供了宝贵的资源。通过利用 MTEB 提供的洞察，开发者可以在为 LLM
    解决方案选择嵌入模型时做出明智的决策。
- en: Before we delve into exploring the knowledge base index types, it’s essential
    to remember that the choice of chunking method and embedding model shapes the
    construction of your knowledge base. Both components play a crucial part in how
    effectively the LLM can retrieve and utilize knowledge. Now, let’s dive deeper
    into the world of knowledge base index types and learn how they contribute to
    the efficiency of LLM solutions.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探索知识库索引类型之前，必须记住，分块方法和嵌入模型的选择决定了知识库的构建方式。这两个组成部分在 LLM 能够高效地检索和利用知识方面发挥着重要作用。现在，让我们更深入地了解知识库索引类型，并学习它们如何提升
    LLM 解决方案的效率。
- en: Exploring the knowledge base index types
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索知识库索引类型
- en: The knowledge base index is the backbone of retrieval mechanisms in LLM solutions.
    It is the component that facilitates the efficient lookup of relevant information.
    While there are several ways of implementing this index, they all aim to provide
    a fast and efficient way of retrieving the most relevant text chunks from the
    knowledge base based on the input prompt.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 知识库索引是大规模语言模型（LLM）解决方案中检索机制的支柱。它是实现高效查找相关信息的组成部分。虽然实现这个索引的方法有很多种，但它们的目标都是提供一种快速高效的方式，从知识库中基于输入提示检索最相关的文本片段。
- en: 'Many options are available for building a knowledge base index. They range
    from manual code implementations to using various vector database libraries, service
    providers, and plugins. Some of these options are listed here:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 构建知识库索引有多种选择。它们从手动代码实现，到使用各种向量数据库库、服务提供商和插件都有。以下是一些可选方案：
- en: '`faiss`, a library for efficient similarity search of dense vectors, and `scipy`,
    a library for pairwise distance computations. This allows for customization but
    may require more effort and expertise while requiring bigger RAM allocations.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`faiss` 是一个用于高效相似度搜索稠密向量的库，`scipy` 是一个用于计算成对距离的库。这允许自定义，但可能需要更多的努力和专业知识，并且需要更大的内存分配。'
- en: '**Service providers**: Various cloud providers offer vector database services.
    These include Pinecone, Chroma, Vespa, and Weaviate. These services handle the
    complexities of managing a vector database, providing scalable and robust solutions
    that can be easily integrated into your LLM architecture and solution.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**服务提供商**：各种云服务提供商提供向量数据库服务，包括Pinecone、Chroma、Vespa和Weaviate。这些服务处理了管理向量数据库的复杂性，提供了可扩展且强大的解决方案，可以轻松地集成到你的LLM架构和解决方案中。'
- en: '**Database tools with vector computation support**: Traditional database tools
    such as MongoDB, Neo4j, Redis, and PostgreSQL provide vector computation support
    through plugins. This can be a good option if you’re already using these tools
    in your tech stack and want to leverage their capabilities for your knowledge
    base.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量计算的数据库工具**：传统的数据库工具，如MongoDB、Neo4j、Redis和PostgreSQL，通过插件提供向量计算支持。如果你已经在技术栈中使用这些工具，并希望利用它们的功能来构建知识库，这可能是一个不错的选择。'
- en: '**Plugins**: There are also plugins available, directly from LLM service providers,
    such as ChatGPT, that can help with the construction and maintenance of a knowledge
    base.'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**插件**：还有一些插件可以直接从LLM服务提供商获取，例如ChatGPT，它们可以帮助构建和维护知识库。'
- en: Choosing the right knowledge base index type depends on your specific requirements,
    such as the size of your knowledge base, the complexity of your retrieval needs,
    and the resources you have available. Consider factors such as scalability, ease
    of integration, cost, and the level of control you need over your knowledge base
    when making your choice. A recommendation is to only consider vector databases
    that are using the actual database technology or claim to do so when your knowledge
    base is big enough to matter. If your knowledge base is small, let’s say in the
    six-digit range, raw distance computations take less than 1 second if you make
    one prompt per compute in Python! Next, we will briefly discover orchestrator
    libraries for LLM solutions.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 选择正确的知识库索引类型取决于你的具体需求，比如知识库的大小、检索需求的复杂性以及你可用的资源。在做出选择时，要考虑可扩展性、集成的简便性、成本以及对知识库控制的程度。如果你的知识库已经足够大，建议只考虑使用实际数据库技术或声称使用这种技术的向量数据库。如果你的知识库比较小，例如六位数范围内，通过Python计算单次提示的原始距离计算时间少于1秒！接下来，我们将简要探索LLM解决方案的协调库。
- en: Exploring orchestrator tools for LLM solutions
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 探索LLM解决方案的协调工具
- en: The process of architecting LLM solutions can be streamlined with the use of
    specific tools. Open sourced orchestrator libraries such as LangChain and LlamaIndex
    play a pivotal role in this context. Both tools simplify tasks such as setting
    up the knowledge base, integrating an LLM, and managing retrieval mechanisms.
    In general, an orchestrator significantly reduces the complexity and development
    time of LLM solutions.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 构建LLM解决方案的过程可以通过使用特定工具进行简化。开源的协调库，如LangChain和LlamaIndex，在这个过程中发挥了重要作用。这两种工具简化了设置知识库、集成LLM和管理检索机制等任务。总的来说，协调工具显著减少了LLM解决方案的复杂性和开发时间。
- en: In addition to open sourced orchestrator tools, there are also paid options
    available that provide advanced features and support. Some of these DataRobot,
    Microsoft Azure, IBM Watson, LangSmith, OpenAI, and Google Vertex AI. These platforms
    offer a wide range of pre-built models, integrations, and tools that streamline
    the entire pipeline, from data ingestion to model deployment and monitoring.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 除了开源的协调工具外，还有一些付费选项可供选择，这些选项提供高级功能和支持。包括DataRobot、Microsoft Azure、IBM Watson、LangSmith、OpenAI
    和 Google Vertex AI。这些平台提供了广泛的预构建模型、集成和工具，能够简化整个流程，从数据摄取到模型部署和监控。
- en: As you continue to explore these tools and methods, it’s crucial to establish
    robust evaluation methods to measure the impact of these components on your solution,
    ensuring it meets its intended objectives. We’ll delve deeper into these methods
    in the upcoming section.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续探索这些工具和方法的同时，建立强大的评估方法至关重要，以衡量这些组件对你的解决方案的影响，确保它达成预期目标。我们将在接下来的部分深入探讨这些方法。
- en: Evaluating LLM solutions
  id: totrans-74
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 评估LLM解决方案
- en: Evaluating LLM solutions is a crucial step in harnessing their full potential
    and ensuring their effectiveness in various applications. By implementing a comprehensive
    set of evaluation approaches, organizations can better assess the performance,
    accuracy, and overall quality of the results from an LLM solution, while also
    considering the associated costs, adherence to safety standards, and potential
    negative impact on users. In other words, doing this provides you with valuable
    insights to help make any informed decisions. To achieve a comprehensive evaluation,
    we can view evaluation methods as part of either a quantitative measure or a qualitative
    measure. Let’s dive into evaluation methods by these groups.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 评估LLM解决方案是充分发挥其潜力并确保其在各种应用中有效性的关键步骤。通过实施一套全面的评估方法，组织可以更好地评估LLM解决方案的性能、准确性和整体质量，同时考虑相关的成本、安全标准的遵守情况，以及对用户的潜在负面影响。换句话说，这样做为你提供了有价值的见解，帮助做出有根据的决策。为了实现全面评估，我们可以将评估方法视为定量衡量或定性衡量的一部分。让我们按这些组别深入探讨评估方法。
- en: Evaluating LLM solutions through quantitative metrics
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 通过定量指标评估LLM解决方案
- en: 'Quantitative metrics can be aggregated throughout a provided evaluation dataset
    and can provide a more quick, comprehensive, and objective measure to compare
    multiple LLM solution setups. Here are some examples of quantitative metrics:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 定量指标可以在提供的评估数据集中进行汇总，能够提供一种更快速、全面和客观的衡量方式，用于比较多个LLM解决方案设置。以下是一些定量指标的例子：
- en: '**Comprehension and fluency-based metrics**: Flesch Reading Ease, Coleman Liau
    Index, and SMOG readability.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**理解和流利度相关的指标**：Flesch阅读难易度、Coleman Liau指数和SMOG可读性。'
- en: '**Facts-based metrics**: Any metrics that use the facts provided by a knowledge
    base for inference:'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于事实的指标**：任何使用知识库提供的事实进行推理的指标：'
- en: '**Factual consistency**: This refers to comparing generated text with facts
    stated in the knowledge base. It is important to note that relevant facts might
    not always be available in the knowledge base. This metric is also known as the
    extractiveness metric. To measure factual consistency, you can use either semantic
    similarity, which focuses on differences in the meaning of the text, or lexical
    similarity, which emphasizes matching words in the text.'
  id: totrans-80
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事实一致性**：指的是将生成的文本与知识库中陈述的事实进行比较。需要注意的是，相关的事实可能并不总是出现在知识库中。这个指标也被称为抽取性指标。要衡量事实一致性，可以使用语义相似度，侧重于文本含义上的差异，或者使用词汇相似度，强调文本中词汇的匹配。'
- en: '**Factual relevance**: This is about how relevant the provided facts are, without
    considering the LLM generation. This is possible when you have ranked relevant
    document labels.'
  id: totrans-81
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**事实相关性**：这是指所提供的事实的相关性，而不考虑LLM生成的内容。只有在你有排名的相关文档标签时，才可以实现这一点。'
- en: '**Generated text relevance/accuracy metric**: This metric evaluates the relevance
    and accuracy of the text generated by an LLM in comparison to an ideal ground
    truth. It can be computed using similarity metrics or self-evaluation techniques.
    Self-evaluation can be further broken down into the following areas:'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生成文本的相关性/准确性指标**：该指标评估由大语言模型（LLM）生成的文本与理想真实情况的相关性和准确性。可以通过相似度度量或自我评估技术来计算。自我评估可以进一步细分为以下几个方面：'
- en: '**With access to token probabilities**: The average of log probabilities is
    used to assess the quality of the generated text. Higher log probabilities indicate
    that the model is more confident in its output, suggesting greater relevance and
    accuracy.'
  id: totrans-83
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**有访问令牌概率**：通过计算对数概率的平均值来评估生成文本的质量。较高的对数概率表明模型对其输出更有信心，意味着相关性和准确性较高。'
- en: '**Without access to token probabilities**: **SelfCheckGPT** is a method that
    can be employed to evaluate the generated text without relying on token probabilities.
    This approach leverages the LLM’s capabilities to assess the quality of its generated
    content, providing an alternative measure of relevance and accuracy.'
  id: totrans-84
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**没有访问到令牌概率**：**SelfCheckGPT**是一种可以评估生成文本的方法，无需依赖令牌概率。该方法利用LLM的能力评估其生成内容的质量，提供了一种替代的相关性和准确性衡量方式。'
- en: '**Runtime metrics**: The time taken to generate text, the number of tokens
    processed, and so on.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**运行时指标**：生成文本所需的时间、处理的令牌数等。'
- en: '**Cost metrics**: The number of tokens generated, API call costs, hosting costs,
    and so on.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**成本指标**：生成的令牌数、API调用成本、托管成本等。'
- en: '**Guardrail violations metrics**: The percentage of outputs that violate predefined
    standards. Examples of guardrails are toxicity levels and hate speech degree.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial performance metrics**: The performance measures in handling adversarial
    inputs. These were introduced more comprehensively in [*Chapter 14*](B18187_14.xhtml#_idTextAnchor206),
    *Analyzing* *Adversarial Performance*.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias and fairness metrics**: Quantitative measures for assessing biases in
    the generated text. These were introduced more comprehensively in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196),
    *Exploring Bias* *and Fairness*.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Any supervised classification or regression metrics**: This can be applied
    to the results or resulting actions from an LLM solution.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that quantitative methods such as generated text relevance and factual
    consistency metrics, which use similarity metrics to compare two sets of text,
    are not as reliable as supervised model metrics such as accuracy. These metrics
    should be taken with a grain of salt. Additionally, a nice bonus with quantitative
    metrics is that they can be used for monitoring a deployed model programmatically.
    Next, we will dive into qualitative manual evaluations.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating LLM solutions through qualitative evaluation methods
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Qualitative methods, which involve human feedback and manual assessments, complement
    quantitative measures and provide a comprehensive understanding of LLM performance.
    It can also sometimes be the only way for evaluation when there are no reference
    ground truth datasets. Here are some examples of qualitative LLM solution evaluation
    methods:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: '**Human feedback scores**: These are the users’ ratings or rankings of generated
    responses to gauge effectiveness and relevance. Examples include grammar and coherence
    of text.'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generated text relevance evaluation**: This involves manually assessing the
    generated text’s relevance to the given context or prompt.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prediction explanations**: These assess the reasoning behind the generated
    text or predictions, which can help identify potential biases or faulty logic
    in the LLM solution.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethical and legal compliance**: This ensures that the generated text adheres
    to ethical and legal guidelines through manual review.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By employing a combination of quantitative metrics and qualitative manual evaluations,
    organizations can gain a deeper understanding of LLM performance and identify
    potential areas for improvement. In general, try to treat LLM solutions as no
    different from any supervised machine learning projects and evaluate them vigorously,
    similarly to how you would in a supervised machine learning project. This holistic
    approach to evaluating LLM solutions not only ensures consistent performance and
    compliance but also helps in aligning these powerful models with specific needs
    and objectives, driving innovation and improving outcomes in various applications.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: Identifying challenges with LLM solutions
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Despite their impressive capabilities, LLMs face challenges when solving complex
    real-world problems. In this section, we will explore some of the challenges faced
    by LLM solutions and discuss possible ways to tackle them. We will explore challenges
    by high-level groups, as follows:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管LLM具有令人印象深刻的能力，但在解决复杂的现实世界问题时，它们面临许多挑战。在本节中，我们将探讨LLM解决方案面临的一些挑战，并讨论可能的应对方式。我们将按高层次的群组来探讨这些挑战，如下所示：
- en: '**Output and** **input limitations**:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**输出和** **输入限制**：'
- en: '**LLMs just produce text**: Text output can help provide value for a lot of
    businesses. However, many other use cases require predictions and recommendations
    in entirely different formats.'
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM只是生成文本**：文本输出可以为许多企业提供价值。然而，许多其他用例需要以完全不同的格式进行预测和推荐。'
- en: '**The context size of an LLM is limited**: The issue is that with a large input
    size, you need exponentially more compute resources to train and predict. So,
    context size usually stays in a token range of one to three thousand. This issue
    should be prevalent only for use cases that require long context, as a few thousand
    context sizes should be enough for most use cases.'
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM的上下文大小有限**：问题在于，随着输入大小的增大，训练和预测所需的计算资源会呈指数级增加。因此，上下文大小通常保持在一到三千个标记的范围内。这个问题应该只在需要长上下文的用例中普遍存在，因为对于大多数用例，几千个上下文大小应该足够。'
- en: '**An LLM is a text-specific model**: Other data modalities are not supported
    by default.'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**LLM是文本特定模型**：默认情况下，其他数据模式不被支持。'
- en: '**Repetitive retrieved information**: The information that’s retrieved from
    a knowledge base can be highly relevant but repetitive and numerous. As the context
    size of an LLM is limited, a risk arises when multiple pieces of information are
    placed as the context is repetitive and takes up most of the context limit quota.'
  id: totrans-105
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**重复检索信息**：从知识库中检索到的信息可能高度相关，但却是重复且数量庞大的。由于LLM的上下文大小是有限的，当多个信息片段作为上下文时，可能会因为重复而占用大部分上下文限制配额，从而引发风险。'
- en: '**Knowledge and** **information-related challenges**:'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**知识和** **信息相关挑战**：'
- en: '**Inability to access up-to-date information**: LLMs may not know about recent
    events or developments, leading to outdated or inaccurate information being provided
    in their responses.'
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**无法访问最新信息**：LLM可能不了解最近的事件或发展，导致它们提供的信息过时或不准确。'
- en: '**Handling low-resource languages**: LLMs can struggle with understanding and
    processing languages with limited data or resources.'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**处理低资源语言**：LLM在理解和处理数据或资源有限的语言时可能会遇到困难。'
- en: '**Unawareness of the progression of time**: LLMs may not understand the concept
    of time, leading to confusion when dealing with time-sensitive information.'
  id: totrans-109
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对时间进程的无知**：LLM可能无法理解时间的概念，这会在处理与时间敏感的信息时引发混淆。'
- en: '**Information loss**: LLMs are shown to look at the beginning and end of sentences,
    but not so much the middle, and thus lose the most information placed in the middle.'
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信息丧失**：研究表明，LLM更倾向于关注句子的开头和结尾，而不是中间部分，因此会丧失放在中间的最重要信息。'
- en: '**Single index failures**: This challenge arises when an LLM lacks sufficient
    knowledge about a specific topic or area due to limitations in its training data.
    For instance, if you ask an LLM about a newly opened local restaurant that wasn’t
    covered in its training data, the LLM may provide limited or irrelevant information.'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**单一索引失败**：当LLM因其训练数据的限制而缺乏关于特定主题或领域的足够知识时，就会出现这个挑战。例如，如果你询问LLM关于一个新开张的本地餐馆，而这个餐馆没有出现在它的训练数据中，LLM可能会提供有限或不相关的信息。'
- en: '**Incomplete content retrieval from documents:** When retrieval of a chunked
    sentence gets the right document, but the actual content needed is below the retrieved
    chunk in the same document, LLMs may not provide the complete or accurate information
    required by the user.'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**从文档中检索不完整的内容**：当从分段句子中检索到正确的文档，但实际所需的内容位于同一文档中检索到的部分下方时，LLM可能无法提供用户所需的完整或准确的信息。'
- en: 'Example: In a documentation search for a software''s installation process,
    the LLM retrieves a section mentioning the installation, but the actual step-by-step
    instructions are located in the following section of the document. Consequently,
    the user only receives an overview without the necessary details for proper installation.'
  id: totrans-113
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 示例：在一个关于软件安装过程的文档搜索中，LLM检索到了提及安装的部分，但实际的逐步说明位于文档的下一部分。因此，用户只获得了一个概述，而没有得到进行正确安装所需的详细信息。
- en: '**The use irrelevant information in the context:** LLMs may use irrelevant
    information from their context as a basis for their output, essentially mimicking
    or echoing opinions found in the context even if they are not applicable or appropriate
    for the given situation. This phenomenon, referred to as sycophancy, can lead
    to misleading or unhelpful responses.'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The global knowledge base summarization task can’t be executed accurately**:
    A retrieval process is unaware of the type of knowledge base it requests and thus
    can’t execute the global summarization task effectively.'
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accuracy** **and reliability**:'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hallucinations**: LLMs can generate false or misleading information that
    may appear plausible but is not based on facts. This phenomenon is known as hallucination.'
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lack of mathematical skills**: LLMs often cannot perform precise calculations
    or solve complex mathematical problems. This issue is more widely known as it
    is slightly controversial, depending on how you look at it.'
  id: totrans-118
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Imitative falsehoods**: These are false statements that LLMs generate because
    they mimic common misconceptions found in the training data. Since the model learns
    from the data it’s trained on, it might inadvertently reproduce widely held but
    incorrect beliefs. For example, if many people believe that a specific food causes
    a particular illness, an LLM might generate a similar statement, even if it’s
    not scientifically accurate.'
  id: totrans-119
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Non-imitative falsehoods**: These are false statements that arise due to
    the model’s inability to fully achieve its training objective. This includes hallucinations,
    which are statements that seem plausible but are incorrect. For instance, an LLM
    might generate a statement about a historical event that never occurred, but the
    statement may sound convincing to someone who is not knowledgeable about that
    specific event.'
  id: totrans-120
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Runtime performance issues**: An LLM’s runtime can be slow. Additionally,
    by adding a knowledge base to it, the entire process can become slower than it
    already is.'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ethical implications and societal impacts**: The widespread adoption and
    deployment of LLMs comes with several ethical implications and societal impacts.
    As these models learn from vast amounts of data, they may inadvertently inherit
    biases present in the training data, leading to biased outputs, perpetuating stereotypes,
    or promoting misinformation. Furthermore, LLMs can generate content that may inadvertently
    promote harmful behavior, hate speech, or violate privacy concerns. The following
    ethical challenges are involved in the usage of an LLM solution:'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias and fairness**: Ensuring that the LLM does not exhibit biased behavior
    or discriminate against specific user groups based on their race, gender, age,
    or other protected attributes. Consider the case that a bank uses an LLM to analyze
    loan applications and determine creditworthiness. The LLM has been trained on
    historical data, which may contain biases against certain ethnic groups. As a
    result, the LLM might reject loan applications from these groups at a higher rate,
    even when the applicants have good credit scores.'
  id: totrans-123
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy concerns**: LLMs may inadvertently generate **personally identifiable
    information** (**PII**) or sensitive data in their outputs, which raises privacy
    concerns and potential legal issues. Consider the case where a healthcare organization
    uses an LLM to generate personalized health recommendations for its clients. The
    LLM can inadvertently include specific patient names and medical conditions in
    the generated advice, which then gets shared publicly, violating patient privacy.'
  id: totrans-124
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Misinformation and disinformation**: LLMs can potentially generate misleading
    or false information, which can contribute to the spread of misinformation and
    disinformation. Consider the case where an LLM is used by a news agency to automatically
    summarize and publish news articles. The model unintentionally generates a summary
    that misrepresents the original story, leading to the spread of misinformation
    about a crucial business merger.'
  id: totrans-125
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Safety**: Ensuring that the content generated by LLMs adheres to ethical
    guidelines, legal regulations, and community standards while avoiding promoting
    harmful or offensive content. Consider the case where an e-commerce platform uses
    an LLM to generate product descriptions for sellers. The LLM can create a description
    that promotes a potentially harmful product, such as a recalled item or an item
    that violates safety regulations, exposing the platform to legal and ethical issues.'
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparency and explainability**: Ensuring that the decisions made by LLMs
    are transparent, understandable, and justifiable to users and stakeholders. Consider
    the case where an insurance company uses an LLM to assess risk and determine premiums
    for customers. A customer receives a significantly higher premium and requests
    an explanation for the increase. The LLM’s decision-making process is, by itself,
    opaque and difficult to understand, making it challenging for the company to provide
    a clear and justifiable explanation.'
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have identified these challenges, let’s move on to the next section,
    where we will explore potential solutions and strategies to overcome these limitations.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: Tackling challenges with LLM solutions
  id: totrans-129
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tackling the pesky challenges that LLMs face is key to unlocking their full
    potential and making them our trusty tools or sidekicks in solving real-world
    problems. Only by tackling these challenges can an LLM solution be formed objectively
    and effectively. In this section, we’ll dive into various complementary strategies
    that can help us tackle these challenges and boost the performance of LLMs by
    its high-level issue type. We will start with output and input limitations.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Tackling the output and input limitation challenge
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Navigating the output and input limitation challenges is vital for unlocking
    the full potential of LLMs, allowing them to efficiently process diverse data
    types, formats, and context sizes while delivering accurate and reliable results.
    The solutions are as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '**Customized pre-processing**: Design tailored pre-processing techniques to
    transform non-text data into a format that can be efficiently processed by LLMs.
    For example, design a structure that places structured tabular data as the LLM
    prompt.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Use context limit expansion neural network components**: Implement advanced
    neural network components such as LongLORA, which requires you to fine-tune an
    existing model, to expand the context window size, allowing LLMs to process larger
    amounts of information. However, it is essential to note that this option might
    not be available for external LLM providers and might only be feasible if you
    are considering hosting your own LLM model.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLM context optimization**: Any wasted space or repetitive content limits
    the depth and breadth of the answers we can extract and generate. There are three
    possible methods here:'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select only the most relevant and unique information to be included in the LLM’s
    context window. The maximal marginal relevance algorithm can be used to find a
    set of both relevant and unique sets of information from the distance scores.
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider compressing and summarizing the information provided, which can also
    be done by an LLM, and then use the summarized information as context in the main
    LLM prompt.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Apply knowledge retrieval on demand instead of by default. This on-demand behavior
    can be enforced by treating the RAG as a tool and either teaching the LLM to use
    it via fine-tuning or in-context learning.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will tackle the challenges with knowledge and information.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
- en: Tackling the knowledge- and information-related challenge
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Addressing the output and input limitation challenges is crucial for enhancing
    the versatility and effectiveness of LLMs in solving a wider range of real-world
    problems across various data modalities and context sizes. The solutions are as
    follows:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: '**Real-time data integration**: Connecting LLMs to real-time data sources such
    as APIs, databases, or web services can help them access up-to-date information
    and provide more accurate responses. Incorporating relevant information from knowledge
    bases, using the RAG approach, is part of this solution. RAG can also help reduce
    hallucinations compared to if a model is fine-tuned with custom data if a rigorous
    strict prompt is made to instruct the LLM to not deviate from the context provided
    in the prompt.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tool integration**: Enhancing LLM architecture by integrating existing tools,
    APIs, and specialized algorithms can significantly extend their capabilities,
    allowing them to tackle tasks that are challenging or impossible for standalone
    models. Tools can be used to retrieve extra input context needed for the generation
    process. Alternatively, they can be used to accomplish specific tasks that the
    generated text tells them to do. Examples include leveraging external search engines,
    domain-specific APIs, and computational libraries to provide accurate responses,
    solve complex mathematical problems, or address queries related to real-time data.
    For LLMs such as GPT-3.5, which have API access, this can be achieved through
    effective few-shot prompting, while advanced models such as **Toolformer** and
    **WebGPT** by OpenAI showcase the potential of integrating external tools seamlessly
    into the LLM’s parametric memory and framework. WebGPT can browse the internet
    by detecting the Bing search engine identifier it generates and subsequently execute
    the search before continuing the generation it’s appended. Toolformer, on the
    other hand, is an LLM that can autonomously select and utilize APIs, integrating
    tools such as calculators, Q&A systems, search engines, translators, and calendars
    for improved generation. This is a key functionality of transforming an LLM into
    an agent that can accomplish real-world tasks.'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`[1,` `3,` `5,` `7,` `9,` `10,` `8,` `6,` `4,` `2]`, the LLM is encouraged
    to pay equal attention to all parts of the text, reducing the likelihood of missing
    valuable information placed in the middle.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Utilizing the surrounding information from the same document in LLM context:**
    This solution enhances the LLM''s understanding by incorporating additional information
    from the source document. Expanding the scope of retrieval to include surrounding
    text or metadata helps the LLM generate more accurate and comprehensive responses,
    ensuring it considers the broader context. This approach improves the LLM''s ability
    to address complex questions and provide well-informed responses, which effectively
    solves the documentation search use case issue.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filtering out irrelevant context using the LLM:** Before proceeding with
    the generation task, the LLM is employed to identify and remove any irrelevant
    context. This refined context is then used for generating responses. This seemingly
    simple and logical method has demonstrated its effectiveness in most cases as
    introduced in the paper [https://arxiv.org/abs/2311.11829v1](https://arxiv.org/abs/2311.11829v1).
    Moreover, the black box nature of this technique allows for easy implementation,
    contributing to more intuitive and natural LLM-generated content.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularly building an up-to-date knowledge base**: To address the issue of
    single index failures, it is essential to maintain and update the LLM’s knowledge
    base regularly. This ensures that the LLM stays current with recent developments
    and can provide accurate information across a wide range of subjects, ultimately
    enhancing its reliability and effectiveness in solving real-world problems.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Treat RAG as a tool an LLM can use dynamically based on its generation**:
    This will help solve the problem of not being able to perform summarization at
    the global level of a knowledge base. Similar to how Deadpool is aware of being
    a comic book character, we need the retrieval process to be aware of the type
    of knowledge base it is retrieving from, along with a special handler for summarization
    tasks. A bonus here is to allow the LLM to configure how many rows of relevant
    text to return to the scope of summarization that can be expanded and shrunk as
    required.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-index retrieval**: To address the issue of single-index failures, a
    multi-index retrieval approach can be employed. This solution involves decomposing
    – or in other words, chunking – the user’s query into multiple components and
    retrieving information from various sources or knowledge indexes. This multi-faceted
    search strategy helps gather more diverse and comprehensive information, reducing
    the likelihood of overlooking relevant details due to a single index’s limitations.
    Consider a user asking about a rare bird species. Using a single index might yield
    limited information. With a multi-index retrieval approach, the LLM would do the
    following:'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decompose the query into components (for example, habitat, diet, and appearance).
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Retrieve data from various sources (for example, ornithology databases, nature
    websites, and social media).
  id: totrans-151
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregate and synthesize the data to generate a comprehensive response.
  id: totrans-152
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Set up directed acyclic graph (DAG) workflows**: Setting up DAG workflows
    involves organizing a series of tasks or processes in a structured, non-circular
    sequence to efficiently process multiple sources of information and extend an
    LLM’s functionality. In the context of LLMs, a DAG workflow can be manually designed
    to connect various tools, APIs, and algorithms while addressing the challenges
    related to real-time data integration, tool integration, and multi-index retrieval.
    Let’s consider a use case where a user wants to plan a trip and needs information
    on various aspects of the destination, such as weather, attractions, and local
    cuisine. An LLM could use the DAG workflow to address this complex query efficiently.
    Here’s an example of a DAG workflow for LLMs:'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decompose the user’s query into sub-queries or components, specifically, weather
    forecast, top attractions, and cuisines topics.
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: For each subquery, identify relevant tools, APIs, or data sources. For weather
    forecast, we will retrieve data from a weather API. For top attractions, we will
    extract information from travel website knowledge base. For local cuisine, we
    will gather data from restaurant review website APIs.
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Apply summarization to each fact separately before using it as part of the LLM’s
    input context.
  id: totrans-156
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Execute the LLM generation process with the user query and the summarized facts.
  id: totrans-157
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Publish the results on a website through an API.
  id: totrans-158
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This DAG is depicted in *Figure 19**.3*:'
  id: totrans-159
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 19.3 – An example LLM DAG workflow](img/B18187_19_3.jpg)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
- en: Figure 19.3 – An example LLM DAG workflow
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: By setting up a manual DAG workflow, an LLM can efficiently process information
    from multiple sources, leverage external tools and APIs, and provide accurate
    and reliable responses to a wide range of real-world problems.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: This strategy helps the LLM provide a more accurate and detailed response, even
    when information is scarce or not readily available in a single index. The type
    of problem this solves is more commonly known as multi-hop question answering.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will tackle the challenges of accuracy and reliability.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Tackling the challenges of accuracy and reliability
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Ensuring the accuracy and reliability of LLMs is crucial for building trust
    in their abilities and making them effective problem-solving tools. The solutions
    that can help solve accuracy and reliability-related challenges are as follows:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '**Treat the LLM solution as any modeling experiment**: Pair the LLM with a
    knowledge base, evaluate its performance using relevant metrics, and gather insights
    to fine-tune its capabilities iteratively according to the deep learning life
    cycle. This will help you choose a model that at least produces fewer hallucinations
    and can help you understand its effectiveness for your use case.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fine-tune retriever embedding models**: This is instead of just depending
    on pre-trained embedding models or embedding model providers. This can improve
    the retrieval accuracy, thereby boosting the quality of LLM-generated responses.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt engineering**: Prompt engineering is the process of crafting effective
    and targeted prompts to guide a language model’s response, thereby improving its
    accuracy, relevance, and overall performance. Consider implementing the following
    techniques:'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Chain-of-thought (CoT)**: The method encourages LLMs to generate step-by-step
    reasoning traces, leading to more accurate and structured responses for tasks
    involving arithmetic, commonsense reasoning, and other problem-solving scenarios.
    By guiding the LLM through a series of reasoning steps, CoT helps reduce issues
    such as fact hallucination while enhancing the overall quality and coherence of
    the generated content.'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ReAct**: This method is a framework that interleaves reasoning traces and
    task-specific actions, enabling LLMs to generate more reliable and factual responses.
    By incorporating dynamic reasoning and interaction with external sources, ReAct
    effectively addresses issues such as fact hallucination and error propagation,
    resulting in improved human interpretability and trustworthiness of LLMs.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompt tuning**: Prompt tuning is a technique for refining LLM behavior by
    optimizing the input prompts using gradient-based methods, which allows for better
    control over the model’s responses, and leads to improved accuracy and relevance
    in various problem-solving tasks. By fine-tuning prompts, users can effectively
    guide the LLM to generate more desirable and context-specific outputs. This only
    applies to LLMs you can host yourself, however.'
  id: totrans-172
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Relying on well-engineered prompts**: Leverage published well-engineered
    prompts instead of crafting one of your own. This is a technique used by Langchain
    and Auto-GPT. AutoGPT is an open source Python application based on GPT-4\. It
    automates the execution of tasks without requiring multiple prompts, using AI
    agents to access the web and perform actions with minimal guidance. Unlike ChatGPT,
    AutoGPT can execute larger tasks such as creating websites and developing marketing
    strategies without needing step-by-step instructions. It has various applications,
    such as generating content, designing logos, and developing chatbots.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Rejection sampling (best-of-n) reference**: Use rejection sampling techniques
    to improve the quality of generated responses by selecting the best response from
    multiple attempts. The best response can be evaluated through a chosen metric.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Re-ranking relevance distance scores from knowledge retrieval**: Knowledge
    retrieval is in the domain of recommendation systems. A common technique that’s
    used is to implement proper regression-based recommendation models to re-rank
    relevance distance scores. This can help provide more accurate and potentially
    more personalized relevant information with more contextual data. This technique
    is used by most real-world large-scale recommendation-based products, such as
    YouTube.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Iterative retrieval and generation**: Use techniques such as self-ask, Active
    RAG, and ITER-RETGEN to generate temporary responses, evaluate their quality,
    and iteratively refine them using retrieved knowledge. This approach can reduce
    hallucinations and improve the quality of LLM-generated content.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-agent systems**: Develop a multi-agent system composed of agents with
    unique personas and contexts to address diverse problem-solving scenarios. These
    agents can collaborate, share information, and provide more comprehensive and
    reliable solutions. An example of this is **AutoAgents**. AutoAgents is an innovative
    framework that adaptively generates and coordinates multiple specialized agents
    to build an AI team according to different tasks. The framework consists of two
    stages: drafting and execution. In the drafting stage, an agent team and execution
    plan are generated based on the input task, while the execution stage refines
    the plan through inter-agent collaboration and feedback to produce the outcome.
    AutoAgents can dynamically synthesize and coordinate multiple expert agents to
    form customized AI teams for diverse tasks. Experiments on open-ended question-answering
    and trivia creative writing tasks demonstrate the effectiveness of AutoAgents
    compared to existing methods. AutoAgents offers new perspectives for tackling
    complex tasks by assigning different roles to different tasks and promoting team
    cooperation.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, we will dive into solutions for the runtime performance challenge.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Tackling the runtime performance challenge
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The runtime performance challenge is a critical issue that can significantly
    impact the efficiency and effectiveness of language models. As LLMs continue to
    grow in complexity and scale, optimizing their runtime performance becomes more
    crucial than ever. The solutions to solve this issue are as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: '**Caching outputs**: Temporarily store results to avoid recomputing information,
    enabling faster response times and improved performance. This approach is particularly
    useful when dealing with repetitive or similar queries.'
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPUs and GPU inference accelerators**: This is only applicable for LLMs you
    host yourself. LLMs need to run with these components to run in a reasonable time.
    These were introduced in more detail in [*Chapter 15*](B18187_15.xhtml#_idTextAnchor217),
    *Deploying Deep Learning Models* *to Production*.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scann` algorithm and the `faiss` *IVFPQFS* algorithm provide a good balance
    between index build time, index size, retrieval recall, and retrieval runtime.
    However, an approximate KNN algorithm is only required with large knowledge bases
    since the retrieval speed of a small knowledge base is already fast, which is
    less than 1 second. Typically, suitable data dimensions lie in the range of three-digit
    vector column sizes, and seven-digit vector row sizes.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tackling the challenge of ethical implications and societal impacts
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Addressing the ethical implications and societal impacts of LLM solutions is
    crucial for ensuring their responsible and sustainable deployment across various
    applications. By considering the ethical and societal consequences of LLM-generated
    content, developers can create models that respect user values, adhere to legal
    guidelines, and contribute positively to society.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'The strategies to tackle these challenges are as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: '**Bias and fairness mitigation**: Consider the following strategy in the context
    of the methods that were introduced in [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196),
    *Exploring Bias* *and Fairness*:'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data collection and preparation**: Ensure a diverse and representative dataset
    for fine-tuning LLM models. Balance sensitive attributes in the data, and eliminate
    or control potential biases that may arise from these attributes. Additionally,
    you can instruct the LLM to specifically not perpetuate bias in natural language
    as part of the input context. Better yet, empower users to define their preferences,
    values, and ethical guidelines, enabling LLMs to generate content that aligns
    with individual user needs and values.'
  id: totrans-188
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bias mitigation during fine-tuning**: Implement techniques such as counterfactual
    data augmentation, adversarial training, or re-sampling during the fine-tuning
    process to reduce the impact of biased features and improve fairness.'
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Post-processing**: Modify LLM-generated content using techniques such as
    equalized odds post-processing to ensure fairness in the outputs. This can be
    applied when using LLM providers such as OpenAI GPT-4 or fine-tuned open source
    models.'
  id: totrans-190
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Monitoring and evaluation**: Continuously monitor LLM-generated content for
    potential biases using bias and fairness metrics, and adjust the model as needed
    to ensure compliance with ethical guidelines and fairness requirements.'
  id: totrans-191
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy-preserving techniques**: Adopt privacy-preserving approaches, such
    as differential privacy, federated learning, and homomorphic encryption, to protect
    sensitive information in the training data and generated content. Implement policies
    and guidelines to prevent the inadvertent disclosure of **Personal Identifiable
    Information** (**PII**) in LLM-generated content.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fact-checking and credibility assessment**: Incorporate fact-checking and
    credibility assessment mechanisms into LLM solutions to reduce the risk of generating
    misleading or false information. This can involve integrating LLMs with external
    knowledge sources, such as knowledge databases, to verify the accuracy of generated
    content. You can also instruct the LLM to be humble and not return a statement
    if no facts can be used for verification.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Content moderation and guardrails**: Implement content moderation techniques,
    such as keyword filtering, machine learning-based classifiers, and human-in-the-loop
    review processes, to prevent the generation of harmful or offensive content. Establish
    guardrails, such as toxicity thresholds or ethical guidelines, to ensure that
    LLM-generated content adheres to community standards and legal regulations.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparency and explainability**: Develop methods for enhancing the transparency
    and explainability of LLM-generated content, such as providing reasoning traces,
    saliency maps, or counterfactual explanations. The concepts that were introduced
    in *Chapter 11**, Explaining Neural Network Predictions*, and [*Chapter 12*](B18187_12.xhtml#_idTextAnchor184)*,
    Interpreting Neural Networks*, can be applied to an LLM.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By implementing these strategies, developers can create LLM solutions that not
    only respect user values and adhere to legal guidelines but also contribute positively
    to society. Addressing the ethical implications and societal impacts of LLMs is
    an essential step toward building trust in the technology and ensuring its responsible
    and sustainable deployment across various applications.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: With a comprehensive understanding of the challenges associated with LLMs mentioned,
    as well as their potential solutions, we can now turn our attention to addressing
    the overarching challenge of LLM solution adoption across organizations and industries.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: Tackling the overarching challenge of LLM solution adoption
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One overarching challenge in realizing the full potential of LLM solutions lies
    in their adoption across organizations and industries. Similar to the adoption
    of any machine learning or deep learning solution, the key factor driving the
    adoption of LLMs is confidence. Confidence in the technology’s capabilities, its
    effectiveness in addressing specific use cases, and its ability to deliver tangible
    results are essential for widespread adoption.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: To overcome this challenge, it is crucial to systematically educate organizations
    about the power of LLMs, their diverse applications, and how they can be tailored
    to meet specific needs. This includes demonstrating the benefits of LLMs through
    real-world success stories, providing practical guidance on implementing LLM solutions,
    and offering support for organizations navigating the complexities of integrating
    LLMs into their workflows.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: Building confidence in LLM solutions involves thoroughly evaluating their performance,
    addressing the challenges discussed earlier in this chapter, and ensuring that
    the solutions meet their intended objectives. By implementing a comprehensive
    set of evaluation approaches, including quantitative metrics and qualitative manual
    evaluations, organizations can better assess the performance, accuracy, and overall
    quality of the results from an LLM solution. These evaluations should be conducted
    iteratively, allowing for ongoing refinement and improvement of the LLM solution.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, addressing the challenges identified in this chapter, such as
    output and input limitations, knowledge and information-related challenges, accuracy
    and reliability issues, and runtime performance challenges, is essential for building
    confidence in LLM solutions. By leveraging the strategies and techniques discussed
    in this chapter, organizations can optimize the performance of LLMs and ensure
    their effectiveness in various applications.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: Another essential aspect of building confidence in LLM solutions is effective
    communication and collaboration with stakeholders. This includes sharing evaluation
    results, discussing the benefits and potential limitations of LLMs, and addressing
    any concerns that stakeholders may have regarding the adoption of LLM solutions.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: In conclusion, adopting LLM solutions successfully requires a combination of
    rigorous evaluation, addressing challenges, and effective communication with stakeholders.
    By treating the adoption of LLM solutions with the same level of care as any machine
    learning or deep learning solution, organizations can build confidence in the
    capabilities and performance of LLMs, unlocking their full potential in a wide
    range of real-world applications. And with that, we have explored the challenges
    that plague LLMs and their solutions in detail.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we will dive deeper into leveraging LLMs to build
    autonomous agents, which can significantly expand and improve our problem-solving
    skills.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: Leveraging LLM to build autonomous agents
  id: totrans-206
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One promising area in which LLMs can be harnessed is the development of autonomous
    agents that can efficiently solve complex problems and interact with their environment.
    This section will focus on leveraging LLMs to build such agents and discuss the
    key aspects that contribute to their effectiveness.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: 'Autonomous agents are AI-driven entities that can perform tasks, make decisions,
    and interact with their environment independently. By incorporating LLMs into
    these agents, developers can create versatile and adaptive systems that can tackle
    a wide range of challenges. Here are some essential components of LLM-powered
    autonomous agents:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '**Planning and decision making**: LLMs can be utilized to generate plans and
    strategies that guide the agent’s actions, taking into account the context and
    goals.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Observing and learning from the environment**: LLMs can be trained to observe
    and interpret the environment, learning from past experiences and adjusting their
    behavior accordingly.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Collaborative problem-solving**: Multi-agent systems can be developed, where
    each agent has a unique persona and context. These agents can collaborate, share
    information, and provide more comprehensive and reliable solutions.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Self-refinement**: Autonomous agents can use LLMs to analyze their performance,
    identify areas for improvement, and refine their strategies and behaviors over
    time.'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Agents encompass parts and pieces of the solutions to the challenges identified
    in the *Identifying challenges with LLM solutions* section of this chapter. Additionally,
    the solutions that were introduced here can be combined to expand the scope of
    problems the overall architected LLM solution can cover. Examples of published
    agent methods that were also introduced earlier in this chapter are WebGPT, Toolformer,
    Auto-GPT, and AutoAgents. Autonomous agents that leverage LLMs are the key to
    making powerful LLM solutions. By combining the strengths of LLMs with the adaptability
    and decision-making capabilities of agents, developers can create groundbreaking
    systems that can revolutionize various domains and industries.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: With a comprehensive understanding of LLM solutions and their potential applications,
    let’s explore some specific use cases where they can be employed effectively.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: Exploring LLM solution use cases
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we will explore some fascinating real-world applications where
    LLM solutions can truly shine. This will give you a sense of how revolutionary
    LLM solutions are. The use cases are as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '**Travel itinerary planner**: LLMs can be employed to develop advanced travel
    itinerary planners that generate personalized trip plans based on user preferences
    and constraints. By integrating LLMs with travel APIs, such as flight, hotel,
    and attraction databases, as well as real-time data sources such as weather and
    traffic information, these planners can provide context-aware recommendations
    tailored to individual traveler needs. Notably, companies such as Booking.com
    and Expedia integrated this into their products, and Agoda announced that they
    will be working on it.'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intelligent tutoring systems**: LLMs can be used to develop intelligent tutoring
    systems that offer personalized learning experiences for students. By integrating
    LLMs with educational content, assessment tools, and learner data, these systems
    can generate targeted learning materials, provide real-time feedback, and adapt
    to individual learning needs. This enables a more efficient and engaging learning
    experience for students. Notably, Duolingo is a company that implemented such
    a solution in their gamified language learning product.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automated email responses**: LLM solutions can be employed to develop automated
    email response systems that handle various types of inquiries, such as customer
    support, sales inquiries, or general information requests. By integrating LLMs
    with email APIs, CRM systems, and relevant knowledge bases, email responses can
    be personalized, accurate, and contextually relevant. This helps businesses streamline
    their customer communication and provide efficient support. Notably, Nanonets
    AI is a company that implemented this as part of their product.'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Code generation**: LLMs can be used to generate code snippets, algorithms,
    or entire software programs based on user input or specific requirements. Solutions
    such as GitHub Copilot leverage LLMs to assist developers in writing code, suggesting
    relevant code snippets, and completing sections of code based on context. By integrating
    LLMs with code repositories, programming language APIs, and domain-specific knowledge
    bases, code generation can be tailored to specific programming languages, frameworks,
    and use cases, improving developer productivity.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Customer support chatbots**: LLM solutions can be employed to develop advanced,
    context-aware chatbots that can handle customer inquiries and support requests
    more effectively. By integrating LLMs with **customer relationship management**
    (**CRM**) systems and knowledge bases, chatbots can provide personalized and accurate
    responses to customer queries. This helps businesses improve their customer support
    services, reduce response times, and increase customer satisfaction. Companies
    such as forethought.ai, Ada, and EBI.AI provide such a solution in their product.'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Medical diagnosis and treatment suggestions**: LLMs can be employed to develop
    advanced diagnostic tools that analyze patient symptoms, medical history, and
    relevant medical literature to suggest potential diagnoses and treatment options.
    By integrating LLMs with **electronic health record** (**EHR**) systems, medical
    databases, and domain-specific knowledge bases, these tools can help healthcare
    professionals make more informed decisions and improve patient outcomes. Notably,
    Harman, a Samsung company, implemented and offered such a solution as part of
    their offered services.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Personal finance management**: LLMs can be used to develop intelligent personal
    finance management applications that provide tailored financial advice, budgeting
    suggestions, and investment recommendations based on user-specific financial goals
    and risk tolerance. By integrating LLMs with banking APIs, stock market data,
    and financial knowledge bases, these applications can offer context-aware financial
    planning and guidance to users. Although not exactly a service or product, Bloomberg
    has developed BloombergGPT, a 50-billion parameter large language model designed
    specifically for finance, which showcases the potential of LLMs in the financial
    domain.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Creative content generation**: LLMs can be employed to generate creative
    content, such as stories, poetry, or music, based on user inputs, preferences,
    or inspirations. By integrating LLMs with databases of literary works, music libraries,
    and knowledge bases on creative techniques and styles, these applications can
    produce unique and engaging content that caters to individual artistic tastes
    and needs. Notably, Jasper built a platform to account for this use case.'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Legal document analysis and drafting**: LLMs can be used to develop advanced
    legal document analysis and drafting tools that assist legal professionals in
    reviewing contracts, identifying potential issues, and generating legal documents
    based on specific requirements. By integrating LLMs with legal databases, contract
    templates, and domain-specific knowledge bases, these tools can help streamline
    legal work and improve efficiency in the legal industry. Notably, netdocuments
    implemented this use case with their product.'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Smart home automation**: LLMs can be employed to develop intelligent home
    automation systems that understand natural language commands and adapt to user
    preferences and routines. By integrating LLMs with smart home devices, APIs, and
    user behavior data, these systems can provide a more intuitive and personalized
    home automation experience, enabling users to control their home environment with
    ease and convenience. Amazon Alexa is a prime example of this use case.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each of these use cases, integrating LLMs with the relevant tools, APIs,
    and data sources ensures that the generated content, recommendations, and responses
    are accurate, contextually relevant, and tailored to specific needs, enhancing
    user experiences and providing valuable support in various domains.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-228
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored LLMs and their potential to address real-world
    problems and create value across various applications. We discussed the key aspects
    of architecting LLM solutions, such as handling knowledge, interacting with real-time
    data and tools, evaluating LLM solutions, identifying and addressing challenges,
    and leveraging LLMs to build autonomous agents. We also emphasized the importance
    of retrieval-augmented language models for providing contextually relevant information
    and examined various techniques and libraries to improve LLM solutions.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: We also discussed the limitations of LLMs, such as output and input limitations,
    knowledge and information-related challenges, accuracy and reliability issues,
    runtime performance challenges, ethical implications and societal impacts, and
    the overarching challenge of LLM solution adoption. To tackle these limitations,
    we presented various complementary strategies, such as real-time data integration,
    tool integration, prompt engineering, rejection sampling, multi-agent systems,
    runtime optimization techniques, bias and fairness mitigation, content moderation,
    and enabling LLM transparency and explainability. Lastly, we discussed leveraging
    LLMs to build autonomous agents, which can significantly expand and improve problem-solving
    abilities in diverse applications.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: By understanding the intricacies of LLM solutions and applying the strategies
    and techniques discussed in this chapter, organizations and individuals can harness
    the full potential of LLMs to drive innovation and improve outcomes across various
    applications.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'By reading *The Deep Learning Architect Handbook*, you have gained invaluable
    insights into the various stages of the deep learning life cycle, exploring critical
    aspects from planning and data preparation to model deployment and governance.
    By reaching the end of this enlightening journey, you are now armed with the knowledge
    and skills to design, develop, and deploy effective deep learning solutions. To
    build upon this strong foundation, consider taking the following next steps:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Apply your newfound knowledge to real-world projects, either in your professional
    field or through open source contributions, to gain practical experience and deepen
    your understanding
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stay up-to-date with the latest research, trends, and breakthroughs in deep
    learning by attending conferences, following influential researchers, and reading
    research papers
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explore specialized areas within deep learning that interest you, such as reinforcement
    learning, generative adversarial networks, or few-shot learning, to further expand
    your expertise
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaborate with fellow deep learning enthusiasts and professionals, joining
    communities, discussion forums, and social media groups to exchange ideas, share
    experiences, and learn from each other
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Consider pursuing advanced courses, certifications, or even a degree in deep
    learning or a related field to enhance your education and qualifications
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Embrace the challenges and triumphs that lie ahead, for with the mastery of
    building deep learning models with intricate deep learning architectures, a keen
    understanding of bias and fairness, and the ability to monitor and maintain model
    performance, you are well-prepared to unleash the full potential of deep learning
    and drive innovation across a vast array of applications. Here’s to your continued
    success and growth in the world of deep learning!
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-239
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*RAG*: [https://doi.org/10.48550/arXiv.2005.11401](https://doi.org/10.48550/arXiv.2005.11401)'
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*RETRO*: [https://arxiv.org/pdf/2112.04426.pdf](https://arxiv.org/pdf/2112.04426.pdf)'
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*REALM*: [https://doi.org/10.48550/arXiv.2002.08909](https://doi.org/10.48550/arXiv.2002.08909)'
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
