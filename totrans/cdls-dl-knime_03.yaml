- en: '*Chapter 2:* Data Access and Preprocessing with KNIME Analytics Platform'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before deep-diving into neural networks and deep learning architectures, it
    might be a good idea to get familiar with KNIME Analytics Platform and its most
    important functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover a few basic operations within KNIME Analytics
    Platform. Since every project needs data, we will first go through the basics
    of how to access data: from files or databases. In KNIME Analytics Platform, you
    can also access data from REST services, cloud repositories, specific industry
    formats, and more. We will leave the exploration of these other options to you.'
  prefs: []
  type: TYPE_NORMAL
- en: Data comes in a number of shapes and types. In the *Data Types and Conversions*
    section, we will briefly investigate the tabular nature of the KNIME data representation,
    the basic types of data in a data table, and how to convert from one type to another.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, after we have imported the data into a KNIME workflow, we will
    show some basic data operations, such as filtering, joining, concatenating, aggregating,
    and other commonly used data transformations.
  prefs: []
  type: TYPE_NORMAL
- en: The parameterization of a static workflow will conclude this very quick overview
    of the basic operations you can perform with KNIME Analytics Platform on your
    data.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will take you through the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data Types and Conversions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transforming Data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Parameterizing the Workflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's start with how to import data into a KNIME workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Accessing Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before starting with examples of how to access and import data into a KNIME
    workflow, let''s create the workflow:'
  prefs: []
  type: TYPE_NORMAL
- en: Click on the **File** item in the top menu or right-click on a folder, such
    as **LOCAL**, for example, in **KNIME Explorer**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Then, select the **New KNIME Workflow** option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Give it a name – for example, `Ch2_Workflow_Examples` – and a destination.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'An empty canvas will open in the central part of the KNIME workbench: the workflow
    editor.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'For this chapter, we will use toy data already available at installation. A
    set of workflows is installed together with the core KNIME Analytics Platform.
    You can find them in the `Example Workflows` folder (*Figure 2.1*) in the `TheData`
    sub-folder contains some free toy datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Structure of the Example Workflows folder in the KNIME Explorer
    panel](img/B16391_02_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – Structure of the Example Workflows folder in the KNIME Explorer
    panel
  prefs: []
  type: TYPE_NORMAL
- en: We will mainly use the datasets in the `Misc` sub-folder.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: In order to upload data to the **KNIME Explorer** panel, just copy it into a
    folder within the current workspace folder on your machine. The folder and its
    contents will then appear in **KNIME Explorer** in the list of workflows, servers,
    KNIME Hub spaces, and data available.
  prefs: []
  type: TYPE_NORMAL
- en: Reading Data from Files
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start with a classic: reading a **CSV-formatted** text file. To read
    a CSV-formatted text file, you need the **File Reader** node or its simplified
    version, the **CSV Reader** node. Let''s focus on the File Reader node, which,
    though more complex, is also more powerful and flexible. There are now two ways
    to create and configure a File Reader node.'
  prefs: []
  type: TYPE_NORMAL
- en: In the long way, you search for the File Reader node in the Node Repository;
    drag and drop it into the workflow editor; double-click it to open its configuration
    window, or alternatively, right-click it and then select **Configure**; and set
    the required settings, which at the very least require the file path via the **Browse**
    button (*Figure 2.2*).
  prefs: []
  type: TYPE_NORMAL
- en: In the short way, you just drag and drop your CSV-formatted file from the File
    Explorer panel into the workflow editor. This way automatically creates a File
    Reader node, fills up most of its configuration settings, including the file path,
    and keeps the configuration window open for further adjustments (*Figure 2.2*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Under the file path, there are some basic settings: whether to read the first
    row as column headers and/or the first column as `RowID`, the column delimiter
    for general text files, and how to deal with spaces and tabs.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Notice two more things in this configuration window of the File Reader node:
    the data preview and the **Advanced** button. The data preview in the lower part
    of the window allows you to see whether the dataset is being read properly. The
    **Advanced** button takes you to more advanced settings, such as enabling shorter
    lines, character encoding, quotes, and other similar preferences.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When using the short way to create and configure a **File Reader** node, in
    the preview panel in the node configuration window (*Figure 2.2*), you can see
    whether the automatic settings were sufficient or whether additional customization
    is necessary:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – The File Reader node and its configuration window](img/B16391_02_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – The File Reader node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: We drag and drop the `Demographics.csv` file from `Example Workflows/TheData/Misc`
    into the workflow editor. In the configuration window of the File Reader node,
    we see that the `CustomerKey` column is interpreted as the row ID of the data
    rows, rather than its own column. We need to disable the **read Row IDs** option
    to read the data properly. After the configuration is complete, we click **OK**;
    the node state moves to yellow and the node can now be executed.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'The automatic creation of the node and the configuration of its settings by
    file drag and drop works only for specific file extensions: `.csv` for a File
    Reader node, `.table` for a Table Reader node, `.xls` and .`xlsx` for an Excel
    Reader node, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, if we drag and drop the `ProductData2.xls` file from the KNIME Explorer
    panel to the workflow editor, an **Excel Reader** (**XLS**) node is created and
    automatically configured (*Figure 2.3*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – The Excel Reader (XLS) node and its configuration window](img/B16391_02_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – The Excel Reader (XLS) node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration window (*Figure 2.3*) is similar to the one of the **File
    Reader** node, but, of course, customized to deal with Excel files. Three items
    especially are different:'
  prefs: []
  type: TYPE_NORMAL
- en: The preview part is activated by a **refresh** button. You need to click on
    **refresh** to update the preview.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Column headers and row IDs are extracted from spreadsheet cells, identified
    with an alphabet letter (the column with the row IDs) and a row number (the row
    with the column headers), according to the Excel standards.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On top of the URL path, there is a menu with a default choice, **Custom URL**.
    This menu allows you to express the file path as an absolute path (**local file
    system**), as a path relative to a mountpoint (**Mountpoint**), as a path relative
    to one of the current locations (data, workflow, or mountpoint), or as a custom
    path (**Custom URL**). This feature will be soon extended to other reader nodes.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In our case, the automated configuration process does not include the column
    headers. We can see this from the preview segment. So, because we have the column
    headers in the first row, we adjust the `1`, refresh the preview, and click **OK**
    to save the changes and close the window.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's read the `SentimentAnalysis.table` file. `.table` files contain
    binary content in a KNIME proprietary format optimized for speed and size. These
    files are read by the Table Reader node. Since all the information about the file
    is already included in the file itself, the configuration window of the `SentimentAnalysis.table`
    file automatically generates a Table Reader node with a pre-configured URL.
  prefs: []
  type: TYPE_NORMAL
- en: To conclude this section, let's read the last files, `SentimentRating.csv` and
    `WebDataOldSystem.csv`, with two more File Reader nodes; then, let's add the name
    of the file in the comment under each node. Then, finally, let's group all these
    reader nodes inside an annotation explaining **Reading data from files** (*Figure
    2.9*).
  prefs: []
  type: TYPE_NORMAL
- en: '`Demographics.csv` contains the demographics of a number of customers, such
    as age and gender. Each customer is identified via a `CustomerKey` value. `ProductData2.xls`
    contains the products purchased by each customer, again identified via the `CustomerKey`
    value. `SentimentAnalysis.table` contains the sentiment expressed as text by the
    customer toward the company and the product, again identified via the `CustomerKey`
    value. `SentimentRating.csv` contains the mapping between the sentiment rating
    and the sentiment text. Finally, `WebdataOldSystem.csv` contains the old activity
    index by each customer, as classified in the old web system, before migration.'
  prefs: []
  type: TYPE_NORMAL
- en: Of course, if there is a dataset from before migration, we must have a newer
    dataset with data from the system after migration. This can be found in a database
    table in the `WebActivity.sqlite` SQLite database.
  prefs: []
  type: TYPE_NORMAL
- en: This leads us to the next section, where we will learn how to read data from
    a database.
  prefs: []
  type: TYPE_NORMAL
- en: Reading Data from Databases
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the Node Repository, there is a category named **DB**, dedicated to **database**
    operations. All database operations are performed according to the same sequence
    (*Figure 2.4*): connect to database, select the table to work on, build a SQL
    query, and import data according to the SQL query.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are nodes for each of these steps, as shown in *Figure 2.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Importing data from databases: connect, select, build SQL query,
    and import](img/B16391_02_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.4 – Importing data from databases: connect, select, build SQL query,
    and import'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s check these nodes one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`WebActivity.sqlite` file. The configuration window only requires the database
    file path since SQLite is a file-based database. All other settings have been
    preset in the node. Indeed, it is common to have some preset settings in dedicated
    connectors, and therefore dedicated connectors need fewer settings than the generic
    DB Connector node. A drag and drop of the `.sqlite` file automatically generates
    the **SQLite DB Connector** node with preloaded configuration settings.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Selecting the Table**: The **DB Table Selector** node allows you to select
    the table from the connected database to work on. If you are a SQL expert, the
    **Custom Query** flag allows you to create your own query for the subset of data
    to extract.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Build SQL Query**: If you are not a SQL expert, you can still build your
    SQL query to extract the subset of data. The DB nodes in the **DB/Query** category
    take a SQL query as input and add one more SQL queries on top of it. The node
    GUI is completely codeless and therefore there is no need to know any SQL code.
    So, for example, the configuration window of the **DB Row Filter** node presents
    a graphical editor on the right to build a row-filtering SQL query.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the following screenshot (*Figure 2.5*), record(s) of **CustomerKey = 11177**
    have been excluded:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – The GUI of the DB Row Filter node. This node builds a SQL query
    to filter out records without using any SQL script](img/B16391_02_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.5 – The GUI of the DB Row Filter node. This node builds a SQL query
    to filter out records without using any SQL script
  prefs: []
  type: TYPE_NORMAL
- en: '**Import Data**: Finally, the **DB Reader** node imports the data from the
    database connection according to the input SQL query. The DB Reader node has no
    configuration window since all the required SQL settings to import the data are
    contained in the SQL query at its input port. There are many other nodes, besides
    the DB Reader node, to import data from a database at the end of such a sequence.
    They are all in the **DB/Read/Write** category in the Node Configuration panel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Did you notice the node ports in *Figure 2.4*? We passed from the black triangle
    (data) to the red square (connection) to the brown square (SQL query). Only ports
    of the same type, transporting data of the same type, can be connected!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In order to inspect the results, after the successful execution of the DB Reader
    node, you can right-click the last node in the sequence – the one with a black
    triangle (data) port, in this case, the DB Reader node – and select the last item
    in the menu. This shows the output data table.
  prefs: []
  type: TYPE_NORMAL
- en: The database nodes only produce a SQL query. At the output port, you can still
    inspect the results of the query by right-clicking the node, selecting the last
    item in the menu, then clicking on the **Cache no of Rows** button in the **Table
    Preview** tab to temporarily visualize just the top rows in the selected number.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, we have also imported the last dataset, including customer web
    activity after migration to the new web system.
  prefs: []
  type: TYPE_NORMAL
- en: Let's spend a bit of time now on the data structure and data types.
  prefs: []
  type: TYPE_NORMAL
- en: Data Types and Conversions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you inspect any of the output data tables from any of the nodes described
    previously, you will see a table-like representation of the data. Here, each value
    is identified via `RowID`, the identification number for the record, and via a
    `CustomerKey 11000` is `M`, as identified via the `Gender` column header, and
    the row ID is `Row0`. In a reader node, the row ID and column header can be generated
    automatically or assigned from the values in a column or a row in the data.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is a screenshot of the data table output by the File Reader node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – A KNIME data table. Here, a cell is identified via its RowID
    value and column header](img/B16391_02_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.6 – A KNIME data table. Here, a cell is identified via its RowID value
    and column header
  prefs: []
  type: TYPE_NORMAL
- en: Each data column also has a data type, as you can see in *Figure 2.6* from the
    icons in the column headers. Basic data types are `true/false`), and **String**.
    However, more complex data types are also available, such as **Date&Time**, **Document**,
    **Image**, **Network**, and more. We will see some of these data types in the
    upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, a data column is not condemned to stay with that data type forever.
    If the condition exists, it can move to another data type. Some nodes are dedicated
    to conversions and can be found in the Node Repository under **Manipulation/Column/Convert
    & Replace**.
  prefs: []
  type: TYPE_NORMAL
- en: In the data that we have read, `CustomerKey` has been imported as a five-digit
    integer. However, it might be convenient to move from an integer type representation
    to a string type representation. For that, we use the **Number to String** node.
    The configuration window consists of an include/exclude framework to select those
    columns whose type needs changing. The opposite transformation is obtained with
    the **String to Number** node. The **Double to Int** node might also be useful
    for a transformation from double to integer.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The **String Manipulation** and **Math Formula** nodes, even though their primary
    task is data transformation, also present some conversion functionality.
  prefs: []
  type: TYPE_NORMAL
- en: We would like to draw your attention to the **Category To Number** node. This
    node comes in handy to discretize nominal classes and transform them into numbers,
    as neural networks only accept numbers as target classes.
  prefs: []
  type: TYPE_NORMAL
- en: Special data types, such as **Image** or **Date&Time**, offer their own conversion
    nodes. A very helpful node for that is the **String to Date&Time** node. **Date**
    or **Time** objects are often read as **String**, and this node converts them
    into the appropriate type object.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we want to consolidate all this customer information,
    starting with the web activity before and after the migration. In these two datasets,
    the columns describing web activity have different names: `First_WebActivity_`
    and `First(WebActivity)`. Let''s standardize them to the same name: `First_WebActivity_`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is what the **Column Rename** node does:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7 – The Column Rename node and its configuration window](img/B16391_02_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.7 – The Column Rename node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: The configuration window of the **Column Rename** node lists all the columns
    from the input data table on the left. Double-clicking on a column opens a frame
    on the right showing the current column name and requiring the new name and/or
    new type. All the nodes we have introduced in this section can be seen in the
    workflow in *Figure 2.13*.
  prefs: []
  type: TYPE_NORMAL
- en: Now, we are ready to concatenate the two web activity datasets and join all
    the other datasets by their `CustomerKey` values.
  prefs: []
  type: TYPE_NORMAL
- en: Transforming Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have read the data from files and databases. In this section, we will perform
    some operations to consolidate, filter, aggregate, and transform them. We will
    start with consolidation operations.
  prefs: []
  type: TYPE_NORMAL
- en: Joining and Concatenating
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The web activity dataset from the old system comes from a CSV file and, after
    column renaming, consists of two data columns: `CustomerKey` and `First_WebActivity_`.
    `First_WebActivity_` ranks how active a customer is on the company''s web site:
    `0` means `3` means **very active**.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The web activity dataset from the new web system comes from the SQLite database
    and consists of three columns: `CustomerKey`, `First_WebActivity_`, and `Count`.
    `Count` is just a progressive number associated with the data rows. It is not
    important for the upcoming analysis. We can decide later whether to remove it
    or keep it.'
  prefs: []
  type: TYPE_NORMAL
- en: It would be nice to have both rankings for the web activity, from the old and
    the new system, together in one single data table. For this, we use the **Concatenate**
    node. Two input data tables are placed together in the same output data table.
    Data cells belonging to columns with the same name are placed in the same output
    column. Data columns existing in only one of the tables can be retained (union
    of columns) or removed (intersection of columns), as set in the node configuration
    window. The node configuration window also offers a few strategies to deal with
    rows with the same row IDs existing in both input tables.
  prefs: []
  type: TYPE_NORMAL
- en: We concatenated the two web activity data tables and kept the union of data
    columns in the output data table.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The Concatenate node icon shows three dots in its lower-left corner. Clicking
    these three dots gives you the chance to add more input ports and therefore to
    concatenate more input data tables.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now move on to the sentiment analysis data. `SentimentAnalysis.table`
    produced a data table with `CustomerKey` and `SentimentAnalysis` columns. `SentimentAnalysis`
    includes the customer''s sentiment toward the company and product, expressed as
    text. `SentimentRating.csv` produced a data table with two columns: `SentimentAnalysis`
    and `SentimentRating`. Both columns express the customer sentiment: one in text
    and one in ranking ordinals. This is a mapping data table, translating text into
    ranking sentiment and vice versa. Depending on the kind of analysis we will run,
    we might need the text expression or the ranking expression. So, to be on the
    safe side, let''s join these two data tables together to have them all, `CustomerKey`,
    `SentimentAnalysis` (text), and `SentimentRating` (ordinals), in one data table
    only. This is obtained with the **Joiner** node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The Joiner node joins data cells from two input data tables together into the
    same data row, according to a key value. In our case, the key values are provided
    by the `SentimentAnalysis` columns present in both input data tables. So, each
    customer (`CustomerKey`) will have the `SentimentAnalysis` text value and the
    corresponding `SentimentRating` value. The Joiner node offers four different join
    modes: **inner join** (intersection of key values in the two tables), **left outer
    join** (all key values from the left/top table), **right outer join** (all key
    values from the right/bottom table), and **full outer join** (all key values from
    both tables).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 2.8*, you can find the two tabs of the configuration window of the
    Joiner node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – Configuration window of the Joiner node: the Joiner Settings
    and Column Selection tabs](img/B16391_02_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.8 – Configuration window of the Joiner node: the Joiner Settings and
    Column Selection tabs'
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration window of the Joiner node includes two tabs: **Joiner Settings**
    and **Column Selection**. The **Joiner Settings** tab exposes for selection the
    joiner mode and the data columns containing the key values for both input tables.
    The **Column Selection** tab sets the columns from both input tables to retain
    when building the final joint data rows. A few additional options are available
    to deal with columns with the same names in the two tables and to set what to
    do with the key columns after the joining is performed.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: There can be more than one level of key columns for the join. Just select the
    **+** button in the **Joiner Settings** tab to add more key columns. If you have
    more than one level of key columns, you can decide whether a join is performed
    if all key values match (**Match all of the following**) or if just one key value
    matches (**Match any of the following**), as set in the top radio buttons (*Figure
    2.8* on the left).
  prefs: []
  type: TYPE_NORMAL
- en: We joined the two sentiment tables using `SentimentAnalysis` as the key column
    in both tables and using a left outer join. The left outer join includes all key
    values from the left (upper) table (the customer table) and therefore makes sure
    that all sentiment values for all customers are retained in the output data table.
  prefs: []
  type: TYPE_NORMAL
- en: After joining `CustomerKey` with all the sentiment expressions, we will perform
    other similar join operations, multiple times, in cascade, using `CustomerKey`
    as the key column, to collect together the different pieces of data for the same
    customers in one single table (*Figure 2.13*).
  prefs: []
  type: TYPE_NORMAL
- en: 'If we inspect the output produced by the `Demographics.csv` file, we notice
    two data columns that are also provided by other files: `WebActivity` and `SentimentRating`.
    They are old columns and should be substituted with the same columns from the
    `SentimentAnalysis.table` file and the web activity files. We could remove these
    two columns in the **Column Selection** tab of the **Joiner** node. Alternatively,
    we can just filter those two columns out with a dedicated node.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how to filter columns and rows out of a data table.
  prefs: []
  type: TYPE_NORMAL
- en: Column and Row Filtering
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The **Column Filter** node is dedicated to filtering columns from the input
    data table. We can do that as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Manually select which columns to keep and which to exclude (**Manual Selection**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use a wildcard or a Regex expression to match the names of the columns to exclude
    or to keep (**Wildcard/Regex Selection**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Define one or more data types for the columns to include or exclude (**Type
    Selection**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All these options are available at the top of the configuration window of the
    **Column Filter** node. Selecting one of them changes the configuration window
    according to the required settings for that option. Here are the options.
  prefs: []
  type: TYPE_NORMAL
- en: '**Manual Selection**: Provides an include/exclude framework to move columns
    from one frame to the other to include or exclude input columns from the output
    data table (*Figure 2.9*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`*` for joker characters; for example, `R*` indicates all words starting with
    `R`, `R*a` indicates all words starting with `R` and ending with `a`, and so on.
    Regex refers to regular expressions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Type Selection**: This option provides a multiple choice for the data types
    of the columns to include.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The configuration window of the Column Filter node is shown in *Figure 2.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – The Column Filter node and its configuration window](img/B16391_02_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.9 – The Column Filter node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we have been filtering data by columns. The other flavor for data filtering
    is by rows. In this case, we want to remove or keep just some of the data rows
    in the table. For example, still working on the data from the `Demographics.csv`
    file, we might want to keep only the men in the dataset or remove all records
    with **CustomerKey 11177**. For this kind of filtering operation, there are many
    different nodes: **Row Filter**, **Row Filter (Labs)**, **Rule-based Row Filter**,
    **Reference Row Filter**, **Date&Time based Row Filter**, and more:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Row Filter** node is very simple and very powerful: on the right, the
    filtering condition and on the left the filtering mode.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filtering Condition** matches the content of cells in a data column with
    a condition. The input data column to match is selected at the top. The condition
    can consist of **pattern matching**, including wildcards and regex in the pattern
    expression; **range checking**, which is useful for numerical columns; and **missing
    value matching**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Filtering Mode** on the left sets whether to include or exclude the matching
    rows, matching by attribute value, row number, or **RowID**:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.10 – The Row Filter node and its configuration window](img/B16391_02_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.10 – The Row Filter node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: Here, we filter out, using the `CustomerKey` attribute has a value of `11177`.
  prefs: []
  type: TYPE_NORMAL
- en: A similar result could have been obtained using a `11177` into the lower port
    of the **Reference Row Filter** node from a **Table Creator** node.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Table Creator** node is an interesting node for temporary small data.
    It covers the role of an internal spreadsheet, which is where to store a few lines
    of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another group of very important nodes is the ones performing aggregations.
  prefs: []
  type: TYPE_NORMAL
- en: Aggregations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Aggregations are a very important part of any data preparation. Whether for
    dashboard or machine learning algorithms, some aggregation operations are usually
    necessary. There are two commonly used nodes for aggregations: the **GroupBy**
    node and the **Pivoting** node.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 2.11*, you can see the two tabs in the configuration window of the
    GroupBy node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – The two tabs of the GroupBy node''s configuration window:'
  prefs: []
  type: TYPE_NORMAL
- en: Groups and Manual Aggregation](img/B16391_02_011.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.11 – The two tabs of the GroupBy node''s configuration window: Groups
    and Manual Aggregation'
  prefs: []
  type: TYPE_NORMAL
- en: The **GroupBy** node isolates groups of data and on these groups calculates
    some measures, such as simple count, average, variance, percentages, and others.
    Identification of the groups happens in the tab named **Groups** of the configuration
    window; measure setting happens in one of the other tabs (*Figure 2.11*).
  prefs: []
  type: TYPE_NORMAL
- en: In the **Groups** tab, you select the data columns whose value combinations
    define the different groups of data. The node then creates one row for each group.
    For example, selecting the **Gender** column as the group column with distinct
    values of **male** and **female** means to identify those groups of data with
    **Gender** as **male** or **female**. Selecting the **Gender** (**male**/**female**)
    and **MaritalStatus** (**single**/**married**) columns as group columns means
    to identify the **single-female**, **single-male**, **married-female**, and **married-male**
    groups.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we need to select the measures we want to provide for these groups. Here
    we can proceed by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Manually selecting the columns and the measures to apply one by one (**Manual
    Aggregation**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the columns based on a pattern, including wildcard or Regex expressions,
    and the measures to apply to each set of columns (**Pattern Based Aggregation**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting the columns by type and the measures to apply to each set of columns
    (**Type Based Aggregation**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Each measure setting mode has its own tab in the configuration window (*Figure
    2.11*). In the `CustomerKey` column and the `Age` column. For `Gender` as the
    group column, we then get the number and the average age of women and men in the
    input table.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The **GroupBy** node offers a large number of measures. We have seen **Count**
    and **Mean**. However, we could have also used percentage, median, variance, number
    of missing values, sum, mode, minimum, maximum, first, last, kurtosis, concatenation
    of (distinct) values, correlation, and more. It is worth taking some time to investigate
    all the measurement methods available within the **GroupBy** node.
  prefs: []
  type: TYPE_NORMAL
- en: Like the `Gender` (`MaritalStatus` (`CustomerKey` data column. The final result
    is a table with **male**/**female** as the row IDs, **married**/**single** as
    the column headers, and the count of occurrences of each combination as the cell
    content.
  prefs: []
  type: TYPE_NORMAL
- en: This means that the distinct values in the group columns generate rows and the
    distinct values in the pivoting columns generate columns.
  prefs: []
  type: TYPE_NORMAL
- en: 'The configuration window of the **Pivoting** node then has three tabs: **Groups**
    to select the group columns, **Pivots** to select the pivoting columns, and **Manual
    Aggregation** to manually select data columns and the measures to calculate on
    them. If more than one manual aggregation is used, the resulting pivoting table
    has one column for each combination of aggregation method and pivot value.'
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the node returns the total aggregation based on only the group
    columns on the second output port and the total aggregation based on only the
    pivoted columns at the third output port.
  prefs: []
  type: TYPE_NORMAL
- en: Let's move on now to a few more very flexible and very powerful nodes to perform
    data transformation.
  prefs: []
  type: TYPE_NORMAL
- en: Math Formula and String Manipulation nodes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'KNIME Analytics Platform offers many nodes for data transformation. We cannot
    describe all of them here. So, while we leave the enjoyment of their discovery
    to you, we will describe two very powerful nodes here: the **String Manipulation**
    and **Math Formula** nodes.'
  prefs: []
  type: TYPE_NORMAL
- en: The **String Manipulation** node applies transformations on string values in
    data cells. The transformations are listed in the **Function** panel in the node
    configuration window (*Figure 2.12*). There, you can see the function and its
    possible syntaxes. If you select a function in the list, in the panel on the right,
    named **Description**, a full description of the function task and syntax appears.
    The transformation, however, is implemented in the **Expression** editor at the
    bottom of the window.
  prefs: []
  type: TYPE_NORMAL
- en: First, you select (double-click) a transformation from the `""`, or values from
    other columns in the input data table. Values from columns are inserted automatically
    with the right syntax with a double-click on the column name in the **Column List**
    panel on the left.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take an example:'
  prefs: []
  type: TYPE_NORMAL
- en: In the data table resulting from the `M`) and one for female (`F`), containing
    the number of occurrences and the average age for each group (`M/F`). Let's change
    `"M"` to `"Male"` and `"F"` to `Female"`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Then, we would use the `replace(str, search, replace)` function, where `str`
    indicates the column to work on, `search` the string to search in the cell value,
    and `replace` the string to use as a replacement. Double-clicking on the **Gender**
    column in the **Column List** panel and completing the expression by hand, we
    end up with the following expression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We get the following in a subsequent node:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The String Manipulation node and its configuration window are shown in *Figure
    2.12*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.12 – The String Manipulation node and its configuration window](img/B16391_02_012.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We would get a similar expression for `"F"` and `"Female"`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, we set to replace the original values in the `Gender` column with the
    new values, using the **Replace Column** option in the lower part of the configuration
    window.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is also possible to apply the same transformation to more than one input
    data column, with the **String Manipulation (Multi Column)** node. This node essentially
    works like the **String Manipulation** node. It just applies the set expression
    to all selected data columns. The lower part of its configuration window is the
    same as for the **String Manipulation** node. In the upper part, though, you can
    select all columns on which to apply the expression.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In the `$$CURRENTCOLUMN$$` general column name in the **Expression** editor.
    The very large number of string transformations in the **Function** list makes
    this node extremely powerful.
  prefs: []
  type: TYPE_NORMAL
- en: A node very similar to the String Manipulation node, even though working on
    a different task, is the **Math Formula** node. The Math Formula node implements
    a mathematical expression on the input data. Besides that, it works exactly the
    same as the String Manipulation node. In the configuration window, the available
    math functions are listed in the central **Function** panel. If a function from
    the list is selected, the description appears in the **Description** panel. The
    final expression is crafted in the **Expression** editor at the bottom. Insertion
    of column names in the **Expression** editor happens by double-clicking the column
    name in the **Column List** panel on the left. Nested mathematical functions are
    possible.
  prefs: []
  type: TYPE_NORMAL
- en: The **Math Formula (Multi Column)** node extends the **Math Formula** node to
    apply the same formula onto many selected columns.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2.13* shows the final workflow containing all the operations described
    in this chapter, which is also available on the KNIME Hub: [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%202/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%202/):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – Workflow that summarizes some data access, data conversion,
    and'
  prefs: []
  type: TYPE_NORMAL
- en: data transformation nodes available in KNIME Analytics Platform](img/B16391_02_013.jpg)
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13 – Workflow that summarizes some data access, data conversion, and
    data transformation nodes available in KNIME Analytics Platform
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have seen static transformations on data. What about having a different
    transformation for different conditions? Let's take the **Row Filter** node. Today,
    I might want to filter out the female occurrences from the data table, while tomorrow
    the male ones. How can I do that without having to change the configuration settings
    for all involved nodes at every run? The time has come to introduce you to **Flow
    Variables**.
  prefs: []
  type: TYPE_NORMAL
- en: Parameterizing the Workflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s consider a simple workflow: read the `Demographics.csv` file, filter
    all data rows with `Gender = M or F`, and replace `M` or `F` with `Male` or `Female`,
    respectively. Once we have decided whether to work on `M` or `F`, the workflow
    becomes quite simple and includes a `replace()` function:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s add one node that allows us to choose whether to work on `M` or `F`
    records: the **String Configuration** node. This node generates a flow variable.
    A flow variable is a parameter that travels with the data flow along the workflow
    branch and it can be used to overwrite settings in other nodes.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'As far as we are concerned, for now, two settings are important in the configuration
    window of this node: the default value and the variable name. Let''s use default
    value `M` for now, to work with `Gender = M` records, and let''s name the flow
    variable `gender_variable`.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executing the node creates a Flow Variable named `gender_variable` with value
    `M`:![Figure 2.14 – This workflow shows how to use flow variables](img/B16391_02_014.jpg)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Figure 2.14 – This workflow shows how to use flow variables
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Now, let's use the value of the `gender_variable` via the **V** button.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Did you notice the red connection between the **String Configuration** node
    and the **Row Filter** node? This is a **Flow Variable** connection. Flow variables
    are injected into nodes and branches via these connections.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: All nodes have hidden red circle ports for the input and output of flow variables.
    Clicking on the flow variable port of a node and releasing on another node brings
    out the hidden flow variable port and connects the nodes. Alternatively, in the
    context menu of each node, the **Show Flow Variable Ports** option makes them
    visible.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: After that, we create a small table with two rows, [`M, Male`] and [`F, Female`].
    We select the row corresponding to the value in the `gender_variable` flow variable,
    and we aim to replace the `M` or `F` character with the text. For this last part,
    we need to replace the hardcoded strings in the `M` or `F` character as a **Flow
    Variable**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now, we transform the `Male`/`Female` text into a new flow variable. We do this
    via the `gender_variable`, generated by the `Row0`, generated by the `M, Male`]
    or [`F, Female`], depending on what has been selected in the `$Gender$`) and flow
    variables (`$${Sgender_variable}$$`). Also, flow variables can be inserted automatically
    and with the right syntax in the **Expression** editor, by double-clicking on
    the flow variable name in the **Flow Variable List** panel on the left of the
    String Manipulation node's configuration window (*Figure 2.12*).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The benefit of using flow variables is clear. When we decide to use `F` instead
    of `M`, we just change the setting in the **String Configuration** node instead
    of checking and changing the setting in every single node.
  prefs: []
  type: TYPE_NORMAL
- en: We have shown only a small fraction of the nodes dealing with flow variables.
    You can explore more of these nodes in the **Workflow Control/Variables** category
    in the Node Repository panel.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We do not have space in this book to describe more of the many nodes available
    in KNIME Analytics Platform. We will leave this exploratory task to you.
  prefs: []
  type: TYPE_NORMAL
- en: KNIME Analytics Platform includes more than 2,000 nodes and covers a large variety
    of functionalities. However, the factotum nodes that work in most situations are
    much fewer in number, such as, for example, File Reader, Row Filter, GroupBy,
    Join, Concatenation, Math Formula, String Manipulation, Rule Engine, and more.
    We have described most of them in this chapter to give you a solid basis to build
    more complex workflows for deep learning, which we will do in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Questions and Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Check your level of understanding of the concepts presented in this chapter
    by answering the following questions:'
  prefs: []
  type: TYPE_NORMAL
- en: How can I read a text file with lines of variable length?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) By using the CSV Reader node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) By using the File Reader node
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) By using the File Reader node and the allow short lines enabled option
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) By using the File Reader node and the Limit Rows enabled option
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How can I filter records to the `Age > 42` column?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) By using the Column Filter node and selecting the `Age` column
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) By using the Row Filter node and pattern matching `=42` with the **Include**
    option on the right
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) By using the Row Filter node and range checking on, lower boundary *42*,
    with the **Include** option on the right
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) By using the Row Filter node and range checking on, lower boundary *42*,
    with the **Exclude** option on the right
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: How can I find the average sentiment rating for single women?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) By using a GroupBy node with `Gender` and `MaritalStatus` as group columns
    and the mean operation on the `SentimentRating` column
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) By using a GroupBy node with `Gender` as the group column and a count operation
    on the `CustomerKey` column
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) By using a GroupBy node with `CustomerKey` as the group column and a concatenate
    operation on the `SentimentAnalysis` column
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) By using a GroupBy node with `MaritalStatus` as the group column and a percent
    operation on the `SentimentRating` column
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Why do we need flow variables?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) To generate new values
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) To feed the necessary red connections
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) To populate the flow variables list in configuration windows
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) To parameterize the workflow
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
