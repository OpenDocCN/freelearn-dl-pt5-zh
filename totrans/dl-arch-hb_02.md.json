["```py\n    import numpy as np\n    from sklearn import datasets\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    def ReLU(x):\n      return np.maximum(x, 0)\n    ```", "```py\n    class MLP(object):\n      def __init__(\n        self, input_layer_size, hidden_layer_size, output_layer_size, seed=1234\n    ):\n        rng = np.random.RandomState(seed)\n        self.w1 = rng.normal(\n          size=(input_layer_size, hidden_layer_size)\n        )\n        self.b1 = np.zeros(hidden_layer_size)\n        self.w2 = rng.normal(\n          size=(hidden_layer_size, output_layer_size)\n        )\n        self.b2 = np.zeros(output_layer_size)\n        self.output_layer_size = output_layer_size\n        self.hidden_layer_size = hidden_layer_size\n    def forward_pass(self, x):\n        z1 = np.dot(x, self.w1) + self.b1\n        a1 = ReLU(z1)\n        z2 = np.dot(a1, self.w2)  + self.b2\n        a2 = z2\n        return z1, a1, z2, a2\n    ```", "```py\n    def ReLU_gradient(x):\n          return np.where(x > 0, 1, 0)\n    def backward_pass(self, a0, z1, a1, z2, a2, y):\n        number_of_samples = len(a2)\n        average_gradient_w2 = (\n          np.dot(a1.T, (a2 - y)) *\n          (2 / (number_of_samples * self.output_layer_size))\n        )\n        average_gradient_b2 = (\n          np.mean((a2 - y), axis=0) * (2 / self.output_layer_size)\n        )\n        average_gradient_w1 = np.dot(\n          a0.T, np.dot((a2 - y), self.w2.T) * ReLU_gradient(z1)\n        ) * 2 / (number_of_samples * self.output_layer_size)\n        average_gradient_b1 = np.mean(\n          np.dot((a2 - y), self.w2.T) * ReLU_gradient(z1), axis=0\n        ) *  2 / self.output_layer_size\n        return (\n          average_gradient_w2, average_gradient_b2, average_gradient_w1, average_gradient_b1\n        )\n    ```", "```py\n    def gradient_descent_step(\n        self, learning_rate, average_gradient_w2, average_gradient_b2, average_gradient_w1, average_gradient_b1\n      ):\n        self.w2 = self.w2 - learning_rate * average_gradient_w2\n        self.b2 = self.b2 - learning_rate * average_gradient_b2\n        self.w1 = self.w1 - learning_rate * average_gradient_w1\n        self.b1 = self.b1 - learning_rate * average_gradient_b1\n    ```", "```py\n    diabetes_data = datasets.load_diabetes(as_frame=True)\n    diabetes_df = diabetes_data['data']\n    ```", "```py\n    X = diabetes_df.values\n    ```", "```py\n    target = np.expand_dims(diabetes_data['target'], 1)\n    ```", "```py\n    X_train, X_val, y_train, y_val = train_test_split(\n      X, target, test_size=0.20, random_state=42\n    )\n    ```", "```py\n    mlp_model = MLP(\n      input_layer_size=len(diabetes_df.columns),\n      hidden_layer_size=20,\n      output_layer_size=target.shape[1]\n    )\n    ```", "```py\n    iterations = 100\n    training_error_per_epoch = []\n    validation_error_per_epoch = []\n    for i in range(iterations):\n      z1, a1, z2, a2 = mlp_model.forward_pass(X_train)\n      (\n        average_gradient_w2,\n        average_gradient_b2,\n        average_gradient_w1,\n        average_gradient_b1\n      ) = mlp_model.backward_pass(X_train, z1, a1, z2, a2, y_train)\n      mlp_model.gradient_descent_step(\n        learning_rate=0.1,\n        average_gradient_w2=average_gradient_w2,\n        average_gradient_b2=average_gradient_b2,\n        average_gradient_w1=average_gradient_w1,\n        average_gradient_b1=average_gradient_b1,\n      )\n      _, _, _, a2_val = mlp_model.forward_pass(X_val)\n      training_error_per_epoch.append(mean_squared_error(y_train, a2)\n      validation_error_per_epoch.append(\n        mean_squared_error(y_val, a2_val)\n      )\n    ```", "```py\n    plt.figure(figsize=(10, 6))\n    plt.plot(training_error_per_epoch)\n    plt.plot(validation_error_per_epoch,  linestyle = 'dotted')\n    plt.show()\n    ```", "```py\n    Import torch\n    import torch.nn as nn\n    import torch.nn.functional as F\n    ```", "```py\n    Class MLPPytorch(nn.Module):\n      def __init__(\n        self, input_layer_size, hidden_layer_size, output_layer_size\n      ):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(input_layer_size, hidden_layer_size)\n        self.fc2 = nn.Linear(hidden_layer_size, output_layer_size)\n    class MLPPytorch(nn.Module):\n      def __init__(\n        self, input_layer_size, hidden_layer_size, output_layer_size\n      ):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(input_layer_size, hidden_layer_size)\n        self.fc2 = nn.Linear(hidden_layer_size, output_layer_size)\n      def forward(self, x):\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n    ```", "```py\n    Net = MLPPytorch(\n      input_layer_size=len(diabetes_df.columns),\n      hidden_layer_size=10,\n      output_layer_size=y_train.shape[1],\n    )\n    ```", "```py\n    with torch.no_grad():\n           net.fc1.weight.copy_(\n        torch.from_numpy(mlp_model.w1.T)\n      )\n      net.fc1.bias.copy_(\n        torch.from_numpy(mlp_model.b1)\n      )\n      net.fc2.weight.copy_(\n        torch.from_numpy(mlp_model.w2.T)\n      )\n      net.fc2.bias.copy_(\n        torch.from_numpy(mlp_model.b2)\n      )\n    ```", "```py\n    torch_input = torch.from_numpy(X_train)\n    torch_target = torch.from_numpy(y_train)\n    ```", "```py\n    criterion = nn.MSELoss()\n    output = net(torch_input.float())\n    loss = criterion(output, torch_target.float())\n    loss.backward()\n    ```", "```py\n    np.testing.assert_almost_equal(output.detach().numpy(), a2, decimal=3)\n    np.testing.assert_almost_equal(net.fc2.weight.grad.numpy(), average_gradient_w2.T, decimal=3)\n    np.testing.assert_almost_equal(net.fc2.bias.grad.numpy(), average_gradient_b2, decimal=3)\n    np.testing.assert_almost_equal(net.fc1.weight.grad.numpy(), average_gradient_w1.T, decimal=3)\n    np.testing.assert_almost_equal(net.fc1.bias.grad.numpy(), average_gradient_b1, decimal=3)\n    ```"]