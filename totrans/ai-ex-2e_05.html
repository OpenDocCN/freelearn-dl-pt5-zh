<html><head></head><body>
  <div id="_idContainer086">
    <h1 class="chapterNumber">5</h1>
    <h1 id="_idParaDest-73" class="chapterTitle">How to Use Decision Trees to Enhance K-Means Clustering</h1>
    <p class="normal">This chapter addresses two critical issues. First, we will explore how to implement k-means clustering with dataset volumes that exceed the capacity of the given algorithm. Second, we will implement decision trees that verify the results of an ML algorithm that surpasses human analytic capacity. We will also explore the use of random forests.</p>
    <p class="normal">When facing such difficult problems, choosing the right model for the task often proves to be the most difficult task in ML. When we are given an unfamiliar set of features to represent, it can be a somewhat puzzling prospect. Then we have to get our hands dirty and try different models. An efficient estimator requires good datasets, which might change the course of the project.</p>
    <p class="normal">This chapter builds on the k-means clustering (or KMC) program developed in <em class="italics">Chapter 4</em>, <em class="italics">Optimizing Your Solutions with K-Means Clustering</em>. The issue of large datasets is addressed. This exploration will lead us into the world of the law of large numbers (LLN), the central limit theorem (CLT), the Monte Carlo estimator, decision trees, and random forests.</p>
    <p class="normal">Human intervention in a process such as the one described in this chapter is not only unnecessary, but impossible. Not only does machine intelligence surpass humans in many cases, but the complexity of a given problem itself often surpasses human ability, due to the complex and ever-changing nature of real-life systems. Thanks to machine intelligence, humans can deal with increasing amounts of data that would otherwise be impossible to manage.</p>
    <p class="normal">With our toolkit, we will build a solution to analyze the results of an algorithm without human intervention.</p>
    <p class="normal">This chapter covers the following topics:</p>
    <ul>
      <li class="list">Unsupervised learning with KMC</li>
      <li class="list">Determining if AI must or must not be used</li>
      <li class="list">Data volume issues</li>
      <li class="list">Defining the NP-hard characteristic of KMC</li>
      <li class="list">Random sampling concerning LLN, CLT, and the Monte Carlo estimator</li>
      <li class="list">Shuffling a training dataset</li>
      <li class="list">Supervised learning with decision trees and random forests</li>
      <li class="list">Chaining KMC to decision trees</li>
    </ul>
    <p class="normal">This chapter begins with unsupervised learning with KMC. We will explore methods to avoid running large datasets through random sampling. The output of the KMC algorithm will provide the labels for the supervised decision tree algorithm. The decision tree will verify the results of the KMC process, a task no human could do with large volumes of data.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">All the Python programs and files in this chapter are available at <a href="https://github.com/PacktPublishing/Artificial-Intelligence-By-Example-Second-Edition/tree/master/CH0"><span class="url">https://github.com/PacktPublishing/Artificial-Intelligence-By-Example-Second-Edition/tree/master/CH05</span></a>.</p>
      <p class="Information-Box--PACKT-">There is also a Jupyter notebook named <code class="Code-In-Text--PACKT-">COLAB_CH05.ipynb</code> that contains all of the Python programs in one run. You can upload it directly to Google Colaboratory (<a href="https://colab.research.google.com/"><span class="url">https://colab.research.google.com/</span></a>) using your Google Account.</p>
    </div>
    <h1 id="_idParaDest-74" class="title">Unsupervised learning with KMC with large datasets</h1>
    <p class="normal">KMC<a id="_idIndexMarker180"/> takes unlabeled data and <a id="_idIndexMarker181"/>forms clusters of data points. The names (integers) of these clusters provide a basis to then run a supervised learning algorithm such as a decision tree.</p>
    <p class="normal">In this section, we will see how to use KMC with large datasets.</p>
    <p class="normal">When facing a project with large unlabeled datasets, the first step consists of evaluating if machine learning will be feasible or not. Trying to avoid AI in a book on AI may seem paradoxical. However, in AI, as in real life, you should use the right tools at the right time. If AI is not necessary to solve a problem, do not use it.</p>
    <p class="normal">Use a <strong class="bold">proof of concept</strong> (<strong class="bold">POC</strong>) approach to <a id="_idIndexMarker182"/>see if a given AI project is possible or not. A POC costs much less than the project itself and helps to build a team that believes in the<a id="_idIndexMarker183"/> outcome. Or, the POC might show that it is too risky to go forward with an ML solution. Intractable problems exist. It's best to avoid spending months on something that will not work.</p>
    <p class="normal">The first step is exploring the data volume and the ML estimator model that will be used.</p>
    <p class="normal">If the POC proves that a particular ML algorithm will solve the problem at hand, the next thing to do is to address data volume issues. The POC shows that the model works on a sample dataset. Now, the implementation process can begin.</p>
    <p class="normal">Anybody who has run a<a id="_idIndexMarker184"/> machine learning algorithm with a large dataset on a laptop knows that it takes some time for a machine learning program to train and test these samples. A machine learning program or a deep learning convolutional neural network consumes a large amount of machine power. Even if you run an ANN <a id="_idIndexMarker185"/>using a <strong class="bold">GPU</strong> (short for <strong class="bold">graphics processing unit</strong>) hoping to get better performance than with CPUs, it still takes a lot of time for the training process to run through all the learning epochs. An epoch means that we have tried one set of weights, for example, to measure the accuracy of the result. If the accuracy is not sufficient, we run another epoch, trying other weights until the accuracy is sufficient.</p>
    <p class="normal">If you go on and you want to train your program on datasets exceeding 1,000,000 data points, for example, you will consume significant local machine power.</p>
    <p class="normal">Suppose you need to use a KMC algorithm in a corporation with hundreds of millions to billions of records of data coming from multiple SQL Server instances, Oracle databases, and a big data source. For example, suppose that you are working for the phone operating activity of a leading phone company. You must apply a KMC program to the duration of phone calls in locations around the world over a year. That represents millions of records per day, adding up to billions of records in a year.</p>
    <p class="normal">A machine learning KMC training program running billions of records will consume too much CPU/GPU and take too much time even if it works. On top of that, a billion records might only represent an insufficient amount of features. Adding more features will dramatically increase the size of such a dataset.</p>
    <p class="normal">The question now is to find out if KMC will even work with a dataset this size. A KMC problem is <strong class="bold">NP-hard</strong>. The <strong class="bold">P</strong> stands for <strong class="bold">polynomial</strong> and the <strong class="bold">N</strong> for <strong class="bold">non-deterministic</strong>.</p>
    <p class="normal">The solution to our volume problem requires some theoretical considerations. We need to identify the difficulty of the problems we are facing.</p>
    <h2 id="_idParaDest-75" class="title">Identifying the difficulty of the problem</h2>
    <p class="normal">We first need to<a id="_idIndexMarker186"/> understand what level of difficulty we are dealing with. One of the concepts that come in handy is NP-hard.</p>
    <h3 id="_idParaDest-76" class="title">NP-hard – the meaning of P</h3>
    <p class="normal">The P in <a id="_idIndexMarker187"/>NP-hard means that the time to solve or verify the solution of a P problem is polynomial (poly=many, nomial=terms). For example, <em class="italics">x</em><sup>3</sup> is a polynomial. The N means that the problem is non-deterministic.</p>
    <p class="normal">Once <em class="italics">x</em> is known, then <em class="italics">x</em><sup>3</sup> will be computed. For <em class="italics">x</em> = 3,000,000,000 and only 3 elementary calculations, this adds up to:</p>
    <p class="center">log <em class="italics">x</em><sup>3</sup> = 28.43</p>
    <p class="normal">It will take 10<sup>28.43</sup> calculations to compute this particular problem.</p>
    <p class="normal">It seems scary, but it isn't that scary for two reasons:</p>
    <ul>
      <li class="list">In the world of big data, the number can be subject to large-scale randomized sampling.</li>
      <li class="list">KMC can be trained in mini-batches (subsets of the dataset) to speed up computations.</li>
    </ul>
    <p class="normal">Polynomial time means that this time will be more or less proportional to the size of the input. Even if the time it takes to train the KMC algorithm remains a bit fuzzy, as long as the time it will take to verify the solution remains proportional thanks to the batch size of the input, the problem remains a polynomial.</p>
    <p class="normal">An exponential algorithm increases with the amount of data, not the number of calculations. An exponential function of this example would be <em class="italics">f</em>(<em class="italics">x</em>) = 3<sup style="font-style: italic;">x</sup> = 3<sup>3,000,000,000</sup> calculations. Such functions can often be broken down into multiple classical algorithms. Functions of this type exist in the corporate world, but they are out of the scope of this book.</p>
    <h3 id="_idParaDest-77" class="title">NP-hard – the meaning of non-deterministic</h3>
    <p class="normal">Non-deterministic<a id="_idIndexMarker188"/> problems require a heuristic approach, which implies some form of heuristics, such as a trial and error approach. We try a set of weights, for example, evaluate the result, and then go on until we find a satisfactory solution.</p>
    <h4 class="title">The meaning of hard</h4>
    <p class="normal">NP-hard can be transposed into an NP problem with some optimization. This means that NP-hard is as hard or harder than an NP problem.</p>
    <p class="normal">For example, we can use <a id="_idIndexMarker189"/>batches to control the size of the input, the calculation time, and the size of the outputs. That way, we can bring an NP-hard problem down to an NP problem.</p>
    <p class="normal">One way of creating batches to avoid running an algorithm on a dataset that will prove too large for it is to use random sampling.</p>
    <h2 id="_idParaDest-78" class="title">Implementing random sampling with mini-batches</h2>
    <p class="normal">A large portion of machine<a id="_idIndexMarker190"/> learning and deep learning <a id="_idIndexMarker191"/>contains random sampling in various forms. In this case, a training set of billions of elements will prove difficult, if not impossible, to implement without random sampling.</p>
    <p class="normal">Random sampling is used in many methods: Monte Carlo, stochastic gradient descent, random forests, and many algorithms. No matter what name the sampling takes, they share common concepts to various degrees, depending on the size of the dataset.</p>
    <p class="normal">Random sampling on large datasets can produce good results, but it requires relying on the LLN, which we will explore in the next section.</p>
    <h2 id="_idParaDest-79" class="title">Using the LLN</h2>
    <p class="normal">In probability, the<a id="_idIndexMarker192"/> LLN states that when dealing with very large volumes of data, significant samples can be effective enough to represent the whole dataset. For example, we are all familiar with polls that use small datasets.</p>
    <p class="normal">This principle, like all principles, has its merits and limits. But whatever the limitations, this law applies to everyday machine learning algorithms.</p>
    <p class="normal">In machine learning, sampling resembles polling. A smaller number of individuals represent a larger overall dataset.</p>
    <p class="normal">Sampling mini-batches and averaging them can prove as efficient as calculating the whole dataset as long as a scientific method is applied:</p>
    <ul>
      <li class="list">Training with mini-batches or subsets of data</li>
      <li class="list">Using an estimator in one form or another to measure the progression of the training session until a goal has been reached</li>
    </ul>
    <p class="normal">You may be surprised to read "until a goal has been reached" and not "until the optimal solution has been reached."</p>
    <p class="normal">The optimal solution may not represent the best solution. All the features and all the parameters are often not expressed. Finding a good solution will often be enough to classify or predict efficiently.</p>
    <p class="normal">The LLN explains why random functions are widely used in machine learning and deep learning. Random samples<a id="_idIndexMarker193"/> provide efficient results if they respect the CLT.</p>
    <h2 id="_idParaDest-80" class="title">The CLT</h2>
    <p class="normal">The LLN applied to the <a id="_idIndexMarker194"/>example of the KMC project must provide a reasonable set of centroids using random sampling. If the centroids are correct, then the random sample is reliable.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">A centroid<a id="_idIndexMarker195"/> is the geometrical center of a set of datasets, as explained in <em class="italics">Chapter 4</em>, <em class="italics">Optimizing Your Solutions with K-Means Clustering</em>.</p>
    </div>
    <p class="normal">This approach can now be extended to the CLT, which states that when training a large dataset, a subset of mini-batch samples can be sufficient. The following two conditions define the main properties of the CLT:</p>
    <ul>
      <li class="list">The variance between the data points of the subset (mini-batch) remains reasonable.</li>
      <li class="list">The normal distribution pattern with mini-batch variances remains close to the variance of the whole dataset.</li>
    </ul>
    <p class="normal">A Monte Carlo estimator, for example, can provide a good basis to see if the samples respect the CLT.</p>
    <h3 id="_idParaDest-81" class="title">Using a Monte Carlo estimator</h3>
    <p class="normal">The name <a id="_idIndexMarker196"/>Monte Carlo comes from the casinos in Monte Carlo and gambling. Gambling represents an excellent memoryless random example. No matter what happens before a gambler plays, prior knowledge provides no insight. For example, the gambler plays 10 games, losing some and winning some, creating a distribution of probabilities.</p>
    <p class="normal">The sum of the distribution of <em class="italics">f</em>(<em class="italics">x</em>) is computed. Then random samples are extracted from a dataset, for example, <em class="italics">x</em><sub>1</sub>, <em class="italics">x</em><sub>2</sub>, <em class="italics">x</em><sub>3</sub>,..., <em class="italics">x</em><sub style="font-style: italic;">n</sub>.</p>
    <p class="normal"><em class="italics">f</em>(<em class="italics">x</em>) can be estimated through the following equation:</p>
    <figure class="mediaobject"><img src="../Images/B15438_05_001.png" alt=""/></figure>
    <p class="normal">The estimator <img src="../Images/B15438_05_002.png" alt=""/> represents the average of the result of the predictions of a KMC algorithm or any implemented model.</p>
    <p class="normal">We have seen that <a id="_idIndexMarker197"/>a sample of a dataset can represent a full dataset, just as a group of people can represent a population when polling for elections, for example.</p>
    <p class="normal">Knowing that we can safely use random samples, just like in polling a population for elections, we can now process a full large dataset directly, or preferably with random samples.</p>
    <h2 id="_idParaDest-82" class="title">Trying to train the full training dataset</h2>
    <p class="normal">In <em class="italics">Chapter 4</em>, <em class="italics">Optimizing Your Solutions with K-Means Clustering</em>, a KMC algorithm with a six-cluster<a id="_idIndexMarker198"/> configuration produced six centroids (geometric centers), as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15438_05_01.png" alt="https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/02/KMeansClusters.jpg"/></figure>
    <p class="packt_figref">Figure 5.1: Six centroids</p>
    <p class="normal">The problem is now how to avoid costly machine resources to train this KMC dataset when dealing with large datasets. The solution is to take random samples from the dataset in <a id="_idIndexMarker199"/>the same way polling is done on a population for elections, for example.</p>
    <h2 id="_idParaDest-83" class="title">Training a random sample of the training dataset</h2>
    <p class="normal">The <code class="Code-In-Text--PACKT-">sampling/k-means_clustering_minibatch.py</code> program provides a way to verify<a id="_idIndexMarker200"/> the mini-batch solution.</p>
    <p class="normal">The program<a id="_idIndexMarker201"/> begins by loading the data in the following lines of code:</p>
    <pre class="programlisting"><code class="hljs routeros">dataset = pd.read_csv(<span class="hljs-string">'data.csv'</span>)
<span class="hljs-builtin-name">print</span> (dataset.head())
<span class="hljs-builtin-name">print</span>(dataset)
</code></pre>
    <p class="normal">Loading the dataset might create two problems:</p>
    <ul>
      <li class="list"><strong class="bold">The dataset is too large to be loaded in the first place.</strong> In this case, load the datasets batch by batch. Using this method, you can test the model on many batches to fine-tune your solution.</li>
      <li class="list"><strong class="bold">The dataset can be loaded, but the KMC algorithm chosen cannot absorb the volume of data.</strong> A good choice for the size of the mini-batch will solve this problem.</li>
    </ul>
    <p class="normal">Once a dataset has been loaded, the program will start the training process.</p>
    <p class="normal">A mini-batch dataset called <code class="Code-In-Text--PACKT-">dataset1</code> will be randomly created using Monte Carlo's large data volume principle with a mini-batch size of 1,000. Many variations of the Monte Carlo method apply to machine learning. For our example, it will be enough to use a random function to create the mini-batch:</p>
    <pre class="programlisting"><code class="hljs angelscript">n=<span class="hljs-number">1000</span>
dataset1=np.zeros(shape=(n,<span class="hljs-number">2</span>))
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range (n):
    j=randint(<span class="hljs-number">0</span>,<span class="hljs-number">4998</span>)
    dataset1[i][<span class="hljs-number">0</span>]=dataset.iloc[j,<span class="hljs-number">0</span>]
    dataset1[i][<span class="hljs-number">1</span>]=dataset.iloc[j,<span class="hljs-number">1</span>]
</code></pre>
    <p class="normal">Finally, the KMC algorithm runs on a standard basis, as shown in the following snippet:</p>
    <pre class="programlisting"><code class="hljs nix"><span class="hljs-comment">#II.Hyperparameters</span>
<span class="hljs-comment"># Features = 2 :implicit through the shape of the dataset (2 columns)</span>
<span class="hljs-attr">k</span> = <span class="hljs-number">6</span>
<span class="hljs-attr">kmeans</span> = KMeans(<span class="hljs-attr">n_clusters=k)</span>
<span class="hljs-comment">#III.K-means clustering algorithm</span>
<span class="hljs-attr">kmeans</span> = kmeans.fit(dataset1) <span class="hljs-comment">#Computing k-means clustering</span>
<span class="hljs-attr">gcenters</span> = kmeans.cluster_centers_ <span class="hljs-comment"># the geometric centers or centroids</span>
print(<span class="hljs-string">"The geometric centers or centroids:"</span>)
print(gcenters)
</code></pre>
    <p class="normal">The<a id="_idIndexMarker202"/> following screenshot, displaying the result <a id="_idIndexMarker203"/>produced, resembles the full dataset trained by KMC in <em class="italics">Chapter 4</em>, <em class="italics">Optimizing Your Solutions with K-Means Clustering</em>:</p>
    <figure class="mediaobject"><img src="../Images/B15438_05_02.png" alt="https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/02/MiniBatchOutput.jpg"/></figure>
    <p class="packt_figref">Figure 5.2: Output (KMC)</p>
    <p class="normal">The centroids obtained are consistent, as shown here:</p>
    <pre class="programlisting"><code class="hljs yaml"><span class="hljs-attr">The geometric centers or centroids:</span>
<span class="hljs-string">[[</span> <span class="hljs-number">19.6626506</span> <span class="hljs-number">14.37349398</span><span class="hljs-string">]</span>
 <span class="hljs-string">[</span> <span class="hljs-number">49.86619718</span> <span class="hljs-number">86.54225352</span><span class="hljs-string">]</span>
 <span class="hljs-string">[</span> <span class="hljs-number">65.39306358</span> <span class="hljs-number">54.34104046</span><span class="hljs-string">]</span>
 <span class="hljs-string">[</span> <span class="hljs-number">29.69798658</span> <span class="hljs-number">54.7852349</span> <span class="hljs-string">]</span>
 <span class="hljs-string">[</span> <span class="hljs-number">48.77202073</span> <span class="hljs-number">23.74611399</span><span class="hljs-string">]</span>
 <span class="hljs-string">[</span> <span class="hljs-number">96.14124294</span> <span class="hljs-number">82.44067797</span><span class="hljs-string">]]</span>
</code></pre>
    <p class="normal">The output will vary slightly at each run since this is a stochastic process. In this section, we broke the dataset down into random samples to optimize the training process. Another way to perform random sampling is to shuffle the dataset before training it.</p>
    <h2 id="_idParaDest-84" class="title">Shuffling as another way to perform random sampling</h2>
    <p class="normal">The <code class="Code-In-Text--PACKT-">sampling/k-means_clustering_minibatch_shuffling.py</code> program provides another way to solve a random sampling approach.</p>
    <p class="normal">KMC is an unsupervised training algorithm. As such, it trains <em class="italics">unlabeled</em> data. A single random computation does not consume a large amount of machine resources, but several random selections in a row can.</p>
    <p class="normal">Shuffling can<a id="_idIndexMarker204"/> reduce machine consumption costs. Proper shuffling of the data before starting training, just like shuffling cards before a poker game, will avoid repetitive and random mini-batch computations. In this model, the loading data phase and training phase do not change. However, instead of one or several random choices for <code class="Code-In-Text--PACKT-">dataset1</code>, the mini-batch dataset, we shuffle the complete dataset once before starting the training. The following code shows how to shuffle datasets:</p>
    <pre class="programlisting"><code class="hljs angelscript">sn=<span class="hljs-number">4999</span>
shuffled_dataset=np.zeros(shape=(sn,<span class="hljs-number">2</span>))
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range (sn):
    shuffled_dataset[i][<span class="hljs-number">0</span>]=dataset.iloc[i,<span class="hljs-number">0</span>]
    shuffled_dataset[i][<span class="hljs-number">1</span>]=dataset.iloc[i,<span class="hljs-number">1</span>]
</code></pre>
    <p class="normal">Then we select the first 1,000 shuffled records for training, as shown in the following code snippet:</p>
    <pre class="programlisting"><code class="hljs angelscript">n=<span class="hljs-number">1000</span>
dataset1=np.zeros(shape=(n,<span class="hljs-number">2</span>))
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range (n):
    dataset1[i][<span class="hljs-number">0</span>]=shuffled_dataset[i,<span class="hljs-number">0</span>]
    dataset1[i][<span class="hljs-number">1</span>]=shuffled_dataset[i,<span class="hljs-number">1</span>]
</code></pre>
    <p class="normal">The result in the following screenshot corresponds to the one with the full dataset and the random mini-batch dataset sample:</p>
    <figure class="mediaobject"><img src="../Images/B15438_05_03.png" alt="https://packt-type-cloud.s3.amazonaws.com/uploads/sites/2134/2018/02/Shuffled_mini_batch_training.jpg"/></figure>
    <p class="packt_figref">Figure 5.3: Full and random mini-batch dataset sample</p>
    <p class="normal">The centroids<a id="_idIndexMarker205"/> produced can provide first-level results to confirm the model, as shown in the following output.</p>
    <p class="normal">The geometric centers or centroids:</p>
    <pre class="programlisting"><code class="hljs json">[[ <span class="hljs-number">29.51298701</span> <span class="hljs-number">62.77922078</span>]
 [ <span class="hljs-number">57.07894737</span> <span class="hljs-number">84.21052632</span>]
 [ <span class="hljs-number">20.34337349</span> <span class="hljs-number">15.48795181</span>]
 [ <span class="hljs-number">45.19900498</span> <span class="hljs-number">23.95024876</span>]
 [ <span class="hljs-number">96.72262774</span> <span class="hljs-number">83.27737226</span>]
 [ <span class="hljs-number">63.54210526</span> <span class="hljs-number">51.53157895</span>]]
</code></pre>
    <div class="note">
      <p class="Information-Box--PACKT-">Using shuffling instead of random mini-batches has two advantages: limiting the number of mini-batch calculations and preventing training the same samples twice. If your shuffling algorithm works, you will only need to shuffle the dataset once. If not, you might have to go back and use random sampling, as explained in the previous section.</p>
    </div>
    <p class="normal">Random sampling and shuffling helped to solve one part of the dataset volume problem.</p>
    <p class="normal">However, now we must explore the other aspect of the implementation of a large dataset ML algorithm: verifying the results.</p>
    <h2 id="_idParaDest-85" class="title">Chaining supervised learning to verify unsupervised learning</h2>
    <p class="normal">This section explores<a id="_idIndexMarker206"/> how to verify the output of an unsupervised KMC algorithm with a supervised algorithm: a decision tree.</p>
    <p class="normal">KMC takes an input with no labels and produces an output with labels. The unsupervised process makes sense out of the chaos of incoming data.</p>
    <p class="normal">The example in this chapter focuses on two related features: location and distance. The clusters produced are location-distance subsets of data within the dataset. The input file contains two columns: distance and location. The output file contains three columns: distance, location, and a label (cluster number).</p>
    <p class="normal">The output file can thus be chained to a supervised learning algorithm, such as a decision tree. The decision tree will use the labeled data to produce a visual, white-box, machine thought process. Also, a decision tree can be trained to verify the results of the KMC algorithm. The process starts with preprocessing raw data.</p>
    <h3 id="_idParaDest-86" class="title">Preprocessing raw data</h3>
    <p class="normal">Earlier, it was shown<a id="_idIndexMarker207"/> that with large datasets, mini-batches will be necessary. Loading billions of records of data in memory is not an option. A random selection was applied in <code class="Code-In-Text--PACKT-">sampling/k-means_clustering_minibatch.py</code> as part of the KMC algorithm.</p>
    <p class="normal">However, since we are chaining our algorithms in a pipeline and since we are not training the model, we could take the random sampling function from <code class="Code-In-Text--PACKT-">sampling/k-means_clustering_minibatch.py</code> and isolate it:</p>
    <pre class="programlisting"><code class="hljs angelscript">n=<span class="hljs-number">1000</span>
dataset1=np.zeros(shape=(n,<span class="hljs-number">2</span>))
li=<span class="hljs-number">0</span>
<span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range (n):
    j=randint(<span class="hljs-number">0</span>,<span class="hljs-number">4999</span>)
    dataset1[li][<span class="hljs-number">0</span>]=dataset.iloc[j,<span class="hljs-number">0</span>]
    dataset1[li][<span class="hljs-number">1</span>]=dataset.iloc[j,<span class="hljs-number">1</span>]
    li+=<span class="hljs-number">1</span>
</code></pre>
    <p class="normal">The code could be applied to datasets extracted in a preprocessing phase from packs of data retrieved from a big data environment, for example. The preprocessing phase would be repeated in cycles. We will now explore the pipeline that goes from raw data to the output of the chained ML algorithms.</p>
    <h2 id="_idParaDest-87" class="title">A pipeline of scripts and ML algorithms</h2>
    <p class="normal">An ML <strong class="bold">pipeline</strong> will take raw data and perform dimension reduction or other preprocessing tasks that are not in the scope of this book. Preprocessing the data sometimes requires more than ML algorithms such as SQL scripts. Our exploration starts right after when ML algorithms<a id="_idIndexMarker208"/> such as KMC take over. However, a pipeline can run <a id="_idIndexMarker209"/>from raw data to ML using classical non-AI scripting as well.</p>
    <p class="normal">The pipeline described in the following sections can be broken down into three major steps, preceded by classical preprocessing scripting:</p>
    <ul>
      <li class="list"><strong class="bold">Step 0</strong>: A standard process performs a random sampling of a <strong class="bold">training dataset</strong> with classical preprocessing scripts before running the KMC program. This aspect is out of the scope of the ML process and this book. By doing this, we avoid overloading the ML Python programs. The training data will first be processed by a KMC algorithm and sent to the decision tree program.</li>
      <li class="list"><strong class="bold">Step 1</strong>: A KMC multi-ML program, <code class="Code-In-Text--PACKT-">kmc2dt_chaining.py</code>, reads the training dataset produced by step 0 using a <strong class="bold">saved model</strong> from <em class="italics">Chapter 4</em>, <em class="italics">Optimizing Your Solutions with K-Means Clustering</em>. The KMC program takes the unlabeled data, makes predictions, and produces a labeled output in a file called <code class="Code-In-Text--PACKT-">ckmc.csv</code>. The output label is the cluster number of a line of the dataset containing a distance and location.</li>
      <li class="list"><strong class="bold">Step 2</strong>: The decision tree program, <code class="Code-In-Text--PACKT-">decision_tree.py</code>, reads the output of the KMC predictions, <code class="Code-In-Text--PACKT-">ckmc.csv</code>. The decision tree algorithm trains its model and saves the trained model in a file called <code class="Code-In-Text--PACKT-">dt.sav</code>.<ul>
          <li class="Bullet-Within-Bullet--PACKT-"><strong class="bold">Step 2.1</strong>: The training phase is over. The pipeline now takes raw data retrieved by successive equal-sized datasets. This <strong class="bold">batch</strong> process will provide a fixed amount of data. Calculation time can be planned and mastered. This step is out of the scope of the ML process and this book.</li>
          <li class="Bullet-Within-Bullet-End--PACKT-"><strong class="bold">Step 2.2</strong>: A random sampling script processes the batch and produces a prediction dataset for the <strong class="bold">predictions</strong>.</li>
        </ul>
      </li>
      <li class="list"><strong class="bold">Step 3</strong>: <code class="Code-In-Text--PACKT-">kmc2dt_chaining.py</code> will now run a KMC algorithm that is chained to a decision tree that will verify the results of the KMC. The KMC algorithm produces predictions. The decision tree makes predictions on those predictions. The decision tree will also provide a visual graph in a PNG for users and administrators.</li>
    </ul>
    <p class="normal">Steps 2.1 to 3 can run on a twenty-four seven basis in a continuous process.</p>
    <p class="normal">It is important to note that random forests are an interesting alternative to the decision tree component. It can replace the decision tree algorithm in <code class="Code-In-Text--PACKT-">kmc2dt_chaining.py</code>. In the next section, we will explore random forests in this context with <code class="Code-In-Text--PACKT-">random_forest.py</code>.</p>
    <h3 id="_idParaDest-88" class="title">Step 1 – training and exporting data from an unsupervised ML algorithm</h3>
    <p class="normal"><code class="Code-In-Text--PACKT-">kmc2dt_chaining.py</code> can be<a id="_idIndexMarker210"/> considered as the <strong class="bold">chaining</strong> of a KMC program to a decision tree program that will verify the results. Each program forms a link of the chain.</p>
    <p class="normal">From a decision tree project's perspective, <code class="Code-In-Text--PACKT-">kmc2dt_chaining.py</code> can be considered as a <strong class="bold">pipeline</strong> taking unlabeled data and labeling it for the supervised decision tree program. A pipeline takes raw data and transforms it using more than one ML program.</p>
    <p class="normal">During the training phase of the chained model, <code class="Code-In-Text--PACKT-">kmc2dt_chaining.py</code> runs to provide datasets for the training of the decision tree. The parameter <code class="Code-In-Text--PACKT-">adt=0</code> limits the process to the KMC function, which is the first link of the chain. The decision tree in this program will thus not be activated in this phase.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">kmc2dt_chaining.py</code> will load the dataset, load the saved KMC mode, make predictions, and export the labeled result:</p>
    <ul>
      <li class="list"><strong class="bold">Load the dataset</strong>: <code class="Code-In-Text--PACKT-">data.csv</code>, the dataset file, is the same one used in <em class="italics">Chapter 4</em>, <em class="italics">Optimizing Your Solutions with K-Means Clustering</em>. The two features, <code class="Code-In-Text--PACKT-">location</code> and <code class="Code-In-Text--PACKT-">distance</code>, are loaded:
        <pre class="programlisting"><code class="hljs ini"><span class="hljs-attr">dataset</span> = pd.read_csv(<span class="hljs-string">'data.csv'</span>)
</code></pre>
      </li>
      <li class="list"><strong class="bold">Load the KMC model</strong>: The k-means cluster model, <code class="Code-In-Text--PACKT-">kmc_model.sav</code>, was saved by <code class="Code-In-Text--PACKT-">k-means_clustering_2.py</code> in <em class="italics">Chapter 4</em>, <em class="italics">Optimizing Your Solutions with K-Means Clustering</em>. It is now loaded using the <code class="Code-In-Text--PACKT-">pickle</code> module to save it:
        <pre class="programlisting"><code class="hljs ini"><span class="hljs-attr">kmeans</span> = pickle.load(open(<span class="hljs-string">'kmc_model.sav'</span>, <span class="hljs-string">'rb'</span>))
</code></pre>
      </li>
      <li class="list"><strong class="bold">Make predictions</strong>: No further training of the KMC model is required at this point. The model can run predictions on the mini-batches it receives. We can use an <strong class="bold">incremental</strong> process to verify the results on a large scale.<p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">If the data is not sufficiently scaled, other algorithms could be applied. In this case, the dataset does not require additional scaling. The KMC algorithm will make predictions on the sample and produce an output file for the decision tree.</p>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">For this example, the predictions will be generated line by line:</p>
        <pre class="programlisting"><code class="hljs lua">    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(<span class="hljs-number">0</span>,<span class="hljs-number">1000</span>):
        xf1=dataset.at[i,<span class="hljs-string">'Distance'</span>]
        xf2=dataset.at[i,<span class="hljs-string">'location'</span>]
        X_DL = <span class="hljs-string">[[xf1,xf2]]</span>
        <span class="highlight">prediction = kmeans.predict(X_DL)</span>
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The results are stored in a NumPy array:</p>
        <pre class="programlisting"><code class="hljs inform7">        p=str(prediction).strip('<span class="hljs-comment">[]</span>')
        p=int(p)
        kmcpred<span class="hljs-comment">[i]</span><span class="hljs-comment">[0]</span>=int(xf1)
        kmcpred<span class="hljs-comment">[i]</span><span class="hljs-comment">[1]</span>=int(xf2)
        kmcpred<span class="hljs-comment">[i]</span><span class="hljs-comment">[2]</span>=p;
</code></pre>
      </li>
      <li class="list"><strong class="bold">Export the labeled data</strong>: The <a id="_idIndexMarker211"/>predictions are saved in <a id="_idIndexMarker212"/>a file:
        <pre class="programlisting"><code class="hljs routeros">np.savetxt(<span class="hljs-string">'ckmc.csv'</span>, kmcpred, <span class="hljs-attribute">delimiter</span>=<span class="hljs-string">','</span>, <span class="hljs-attribute">fmt</span>=<span class="hljs-string">'%d'</span>)
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">This output file is special; it is now labeled:</p>
        <pre class="programlisting"><code class="hljs angelscript"><span class="hljs-number">80</span>,<span class="hljs-number">53</span>,<span class="hljs-number">5</span>
<span class="hljs-number">18</span>,<span class="hljs-number">8</span>,<span class="hljs-number">2</span>
<span class="hljs-number">55</span>,<span class="hljs-number">38</span>,<span class="hljs-number">0</span>
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The output file contains three columns:</p>
        <ul>
          <li class="Bullet-Within-Bullet--PACKT-">Column 1 = feature 1 = location; <code class="Code-In-Text--PACKT-">80</code>, for example, on the first line</li>
          <li class="Bullet-Within-Bullet--PACKT-">Column 2 = feature 2 = distance; <code class="Code-In-Text--PACKT-">53</code>, for example, on the first line</li>
          <li class="Bullet-Within-Bullet--PACKT-">Column 3 = label = cluster calculated; <code class="Code-In-Text--PACKT-">5</code>, for example, on the first line</li>
        </ul>
        <p class="Bullet-Without-Bullet-Within-Bullet-End--PACKT-">In our chained ML algorithms, this output data will become the input data of the next ML algorithm, the decision tree.</p>
      </li>
    </ul>
    <h3 id="_idParaDest-89" class="title">Step 2 – training a decision tree</h3>
    <p class="normal">In <em class="italics">Chapter 3</em>, <em class="italics">Machine Intelligence – Evaluation Functions and Numerical Convergence</em>, a decision tree was described and used to <a id="_idIndexMarker213"/>visualize a priority process. <code class="Code-In-Text--PACKT-">Decision_Tree_Priority.py</code> produced the following graph:</p>
    <figure class="mediaobject"><img src="../Images/B15438_05_04.png" alt="Une image contenant signe, texte  Description générée automatiquement"/></figure>
    <p class="packt_figref">Figure 5.4: Decision tree priorities</p>
    <p class="normal">The tree starts with a node with a high Gini value. The node is split into two, and each node below is a "leaf" in this case because Gini=0.</p>
    <p class="normal">The decision tree algorithm implemented in this book uses Gini impurity.</p>
    <p class="normal">Gini impurity <a id="_idIndexMarker214"/>represents the probability of a data point being incorrectly classified.</p>
    <p class="normal">A decision tree will start with the highest impurities. It will split the probability into two branches after having calculated a threshold.</p>
    <p class="normal">When a branch reaches a Gini impurity of 0, it reaches its <strong class="bold">leaf</strong>.</p>
    <p class="normal">Let's state that <em class="italics">k</em> is the probability of a data point being incorrectly classified.</p>
    <p class="normal">The dataset <em class="italics">X</em> from <em class="italics">Chapter 3</em>, <em class="italics">Machine Intelligence – Evaluation Functions and Numerical Convergence</em>, contains six data points. Four data points are low, and two data points are high:</p>
    <p class="center"><em class="italics">X</em> = {Low, Low, High, High, Low, Low}</p>
    <p class="normal">The equation of Gini impurity calculates the probability of each feature occurring and multiplies the result by 1—the probability of each feature occurring on the remaining values—as shown in the following equation:</p>
    <figure class="mediaobject"><img src="../Images/B15438_05_003.png" alt=""/></figure>
    <p class="normal">Applied to the <em class="italics">X</em> dataset with four lows out of six and two highs out of six, the result will be:</p>
    <p class="center"><em class="italics">G</em>(<em class="italics">k</em>) = (4/6) * (1 – 4/6) + (2/6) * (1 – 2/6)</p>
    <p class="center"><em class="italics">G</em>(<em class="italics">k</em>)=(0.66 * 0.33) + (0.33 * 0.66)</p>
    <p class="center"><em class="italics">G</em>(<em class="italics">k</em>)=0.222 + 0.222=0.444</p>
    <p class="normal">The probability that a data point will be incorrectly predicted is 0.444, as shown in the graph.</p>
    <p class="normal">The decision <a id="_idIndexMarker215"/>train is built on <strong class="bold">the gain of information</strong> on the features that contain the highest Gini impurity value.</p>
    <p class="normal">We will now explore the Python implementation of a decision tree to prepare it to be chained to the KMC program.</p>
    <h4 class="title">Training the decision tree</h4>
    <p class="normal">To train the<a id="_idIndexMarker216"/> decision tree, <code class="Code-In-Text--PACKT-">decision_tree.py</code> will load the dataset, train the model, make predictions, and save the model:</p>
    <ul>
      <li class="list"><strong class="bold">Load the dataset</strong>: Before loading the dataset, you will need to import the following modules:
        <pre class="programlisting"><code class="hljs capnproto"><span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd <span class="hljs-comment">#data processing</span>
<span class="hljs-keyword">from</span> sklearn.tree <span class="hljs-keyword">import</span> DecisionTreeClassifier <span class="hljs-comment">#the dt classifier</span>
<span class="hljs-keyword">from</span> sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split <span class="hljs-comment">#split the data into training data and testing data</span>
<span class="hljs-keyword">from</span> sklearn <span class="hljs-keyword">import</span> metrics <span class="hljs-comment">#measure prediction performance</span>
<span class="hljs-keyword">import</span> pickle <span class="hljs-comment">#save and load estimator models</span>
</code></pre>
      </li>
    </ul>
    <div class="note">
      <p>The versions of the modules will vary as the editors produce them and also depend on how often you update the versions and the code. For example, you might get a warning when you try to unpickle a KMC model from version 0.20.3 when using version 0.21.2. As long as it works, it is fine for educational purposes. However, in production, an administrator should have a database with the list of packages used and their versions.</p>
    </div>
    <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The dataset is loaded, labeled, and split into training and test datasets:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment">#loading dataset</span>
col_names = [<span class="hljs-string">'f1'</span>, <span class="hljs-string">'f2'</span>,<span class="hljs-string">'label'</span>]
df = pd.read_csv(<span class="hljs-string">"ckmc.csv"</span>, <span class="hljs-attribute">header</span>=None, <span class="hljs-attribute">names</span>=col_names)
<span class="hljs-builtin-name">print</span>(df.head())
<span class="hljs-comment">#defining features and label (classes)</span>
feature_cols = [<span class="hljs-string">'f1'</span>, <span class="hljs-string">'f2'</span>]
X = df[feature_cols] # Features
y = df.label # Target variable
<span class="hljs-builtin-name">print</span>(X)
<span class="hljs-builtin-name">print</span>(y)
<span class="hljs-comment"># splitting df (dataset) into training and testing data</span>
X_train, X_test, y_train, y_test = train_test_split(
    X, y, <span class="hljs-attribute">test_size</span>=0.3, <span class="hljs-attribute">random_state</span>=1) # 70% training <span class="hljs-keyword">and</span> 30% test
</code></pre>
    <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The data in this example will contain two features (location and distance) and a label (cluster number) provided by the output of the KMC algorithm. The following header shows how the dataset is structured:</p>
    <pre class="programlisting"><code class="hljs angelscript">   f1  f2  label
<span class="hljs-number">0</span>  <span class="hljs-number">80</span>  <span class="hljs-number">53</span>      <span class="hljs-number">5</span>
<span class="hljs-number">1</span>  <span class="hljs-number">18</span>   <span class="hljs-number">8</span>      <span class="hljs-number">2</span>
<span class="hljs-number">2</span>  <span class="hljs-number">55</span>  <span class="hljs-number">38</span>      <span class="hljs-number">0</span>
<span class="hljs-number">3</span>  <span class="hljs-number">74</span>  <span class="hljs-number">74</span>      <span class="hljs-number">5</span>
<span class="hljs-number">4</span>  <span class="hljs-number">17</span>   <span class="hljs-number">4</span>      <span class="hljs-number">2</span>
</code></pre>
    <ul>
      <li class="list"><strong class="bold">Training the model</strong>: Once the <a id="_idIndexMarker217"/>datasets are ready, the decision tree classifier is created and the model is trained:
        <pre class="programlisting"><code class="hljs ini"><span class="hljs-comment"># create the decision tree classifier</span>
<span class="hljs-attr">dtc</span> = DecisionTreeClassifier()
<span class="hljs-comment"># train the decision tree</span>
<span class="hljs-attr">dtc</span> = dtc.fit(X_train,y_train)
</code></pre>
      </li>
      <li class="list"><strong class="bold">Making predictions</strong>: Once the model is trained, predictions are made on the test dataset:
        <pre class="programlisting"><code class="hljs stylus"><span class="hljs-selector-id">#predictions</span> on X_test
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"prediction"</span>)</span></span>
y_pred = dtc.predict(X_test)
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(y_pred)</span></span>
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The predictions use the features to predict a cluster number in the test dataset, as shown in the following output:</p>
        <pre class="programlisting"><code class="hljs angelscript">prediction
[<span class="hljs-number">4</span> <span class="hljs-number">2</span> <span class="hljs-number">0</span> <span class="hljs-number">5</span> <span class="hljs-number">0.</span>..]
</code></pre>
      </li>
      <li class="list"><strong class="bold">Measuring the results with metrics</strong>: A key part of the process is to measure the results with metrics. If the accuracy approaches 1, then the KMC output chained to the decision tree algorithm is reliable:
        <pre class="programlisting"><code class="hljs lisp"># model accuracy
print(<span class="hljs-string">"Accuracy:"</span>,metrics.accuracy_score(<span class="hljs-name">y_test</span>, y_pred))
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet-End--PACKT-">In this example, the accuracy was 0.97. The model can predict the cluster of a distance and location with a 0.97 probability, which proves its efficiency. The training of the chained ML solution is over, and a prediction cycle begins.</p>
      </li>
    </ul>
    <p class="normal">You can generate a <a id="_idIndexMarker218"/>PNG file of the decision tree with <code class="Code-In-Text--PACKT-">decision_tree.py</code>. Uncomment the last paragraph of the program, which contains the export function:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-keyword">from</span> sklearn import tree
import pydotplus
<span class="hljs-attribute">graph</span>=1
<span class="hljs-keyword">if</span>(<span class="hljs-attribute">graph</span>==1):
    # Creating the graph <span class="hljs-keyword">and</span> exporting it
    dot_data = tree.export_graphviz(dtc, <span class="hljs-attribute">out_file</span>=None,
                                    <span class="hljs-attribute">filled</span>=<span class="hljs-literal">True</span>, <span class="hljs-attribute">rounded</span>=<span class="hljs-literal">True</span>,
                                    <span class="hljs-attribute">feature_names</span>=feature_cols,
                                    class_names=[<span class="hljs-string">'0'</span>,<span class="hljs-string">'1'</span>,<span class="hljs-string">'2'</span>,
                                                 <span class="hljs-string">'3'</span>,<span class="hljs-string">'4'</span>,<span class="hljs-string">'5'</span>])
    #creating graph
    graph = pydotplus.graph_from_dot_data(dot_data)
    #save graph
    <span class="hljs-attribute">image</span>=graph.create_png()
    graph.write_png(<span class="hljs-string">"kmc_dt.png"</span>)
</code></pre>
    <p class="normal">Note that once you have implemented this function, you can activate or deactivate it with the <code class="Code-In-Text--PACKT-">graph</code> parameter.</p>
    <p class="normal">The following image produced for this example can help you understand the thought process of the whole chained solution (KMC and the decision tree).</p>
    <figure class="mediaobject"><img src="../Images/B15438_05_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 5.5: Image output from the code example</p>
    <p class="normal">The image file, <code class="Code-In-Text--PACKT-">dt_kmc.png</code>, is available on GitHub in <code class="Code-In-Text--PACKT-">CH05</code>.</p>
    <h3 id="_idParaDest-90" class="title">Step 3 – a continuous cycle of KMC chained to a decision tree</h3>
    <p class="normal">The training of the <a id="_idIndexMarker219"/>chained KMC algorithm chained to a decision tree algorithm is over.</p>
    <p class="normal">The preprocessing phase using classical big data batch retrieval methods will continuously provide randomly sampled datasets with a script.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">kmc2dt_chaining.py</code> can focus on running KMC predictions and passing them on to decision tree predictions for white-box checking. The continuous process imports a dataset, loads saved models, predicts, and measures the results at the decision tree level.</p>
    <p class="normal">The chained process can run on a twenty-four seven basis if necessary.</p>
    <p class="normal">The modules used are required for both the KMC and the decision trees implemented in the training programs:</p>
    <pre class="programlisting"><code class="hljs xl">from sklearn.cluster <span class="hljs-keyword">import</span> KMeans
<span class="hljs-keyword">import</span> pandas <span class="hljs-keyword">as</span> pd
from matplotlib <span class="hljs-keyword">import</span> pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-keyword">import</span> pickle
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
from sklearn.<span class="hljs-keyword">tree</span> <span class="hljs-keyword">import</span> DecisionTreeClassifier
from sklearn.model_selection <span class="hljs-keyword">import</span> train_test_split
from sklearn <span class="hljs-keyword">import</span> metrics
</code></pre>
    <p class="normal">Let's run through the process:</p>
    <ul>
      <li class="list"><strong class="bold">Loading the KMC dataset and model</strong>: The KMC dataset has been prepared in the preprocessing phase. The trained model has been previously saved. The dataset is loaded with pandas and the model is loaded with <code class="Code-In-Text--PACKT-">pickle</code>:
        <pre class="programlisting"><code class="hljs ini"><span class="hljs-comment">#I.KMC. The prediction dataset and model</span>
<span class="hljs-attr">dataset</span> = pd.read_csv(<span class="hljs-string">'data.csv'</span>)
<span class="hljs-attr">kmeans</span> = pickle.load(open(<span class="hljs-string">'kmc_model.sav'</span>, <span class="hljs-string">'rb'</span>))
</code></pre>
      </li>
      <li class="list"><strong class="bold">Predicting and saving</strong>: The goal is to predict the batch dataset line by line and save the result in an array in a white-box approach that can be verified by the administrator of the system:
        <pre class="programlisting"><code class="hljs prolog">    for i in range(<span class="hljs-number">0</span>,<span class="hljs-number">1000</span>):
        xf1=dataset.at[i,<span class="hljs-string">'Distance'</span>]
        xf2=dataset.at[i,<span class="hljs-string">'location'</span>]
        <span class="hljs-symbol">X_DL</span> = [[xf1,xf2]]
        prediction = kmeans.predict(<span class="hljs-symbol">X_DL</span>)
        #print (i+<span class="hljs-number">1</span>, <span class="hljs-string">"The prediction for"</span>,<span class="hljs-symbol">X_DL</span>,<span class="hljs-string">" is:"</span>,
            str(prediction).strip(<span class="hljs-string">'[]'</span>))
        #print (i+<span class="hljs-number">1</span>, <span class="hljs-string">"The prediction for"</span>,
            str(<span class="hljs-symbol">X_DL</span>).strip(<span class="hljs-string">'[]'</span>), <span class="hljs-string">" is:"</span>,
            str(prediction).strip(<span class="hljs-string">'[]'</span>))
        p=str(prediction).strip(<span class="hljs-string">'[]'</span>)
        p=int(p)
        kmcpred[i][<span class="hljs-number">0</span>]=int(xf1)
        kmcpred[i][<span class="hljs-number">1</span>]=int(xf2)
        kmcpred[i][<span class="hljs-number">2</span>]=p
np.savetxt(<span class="hljs-string">'ckmc.csv'</span>, kmcpred, delimiter=<span class="hljs-string">','</span>, fmt=<span class="hljs-string">'%d'</span>)
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The <code class="Code-In-Text--PACKT-">ckmc.csv</code> file generated is the entry point of the next link of the chain: the <a id="_idIndexMarker220"/>decision tree.</p>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The two lines containing the <code class="Code-In-Text--PACKT-">print</code> instruction are commented for standard runs. However, you may wish to explore the outputs in detail if your code requires maintenance. That is why I recommend adding maintenance lines in the code.</p>
      </li>
      <li class="list"><strong class="bold">Loading the dataset for the decision tree</strong>: The dataset for the decision tree is loaded in the same way as in <code class="Code-In-Text--PACKT-">decision_tree.py</code>. A parameter activates the decision tree part of the code: <code class="Code-In-Text--PACKT-">adt=1</code>. The white-box quality control approach can thus be activated or deactivated.<p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The program loads the dataset, loads the model, and splits the data:</p>
        <pre class="programlisting"><code class="hljs routeros"><span class="hljs-keyword">if</span> <span class="hljs-attribute">adt</span>==1:
    #I.DT. The prediction dataset <span class="hljs-keyword">and</span> model
    col_names = [<span class="hljs-string">'f1'</span>, <span class="hljs-string">'f2'</span>,<span class="hljs-string">'label'</span>]
    # load dataset
    ds = pd.read_csv(<span class="hljs-string">'ckmc.csv'</span>, <span class="hljs-attribute">header</span>=None,
                     <span class="hljs-attribute">names</span>=col_names)
    #split dataset <span class="hljs-keyword">in</span> features <span class="hljs-keyword">and</span> target variable
    feature_cols = [<span class="hljs-string">'f1'</span>, <span class="hljs-string">'f2'</span>]
    X = ds[feature_cols] # Features
    y = ds.label # Target variable
    # Split dataset into training <span class="hljs-builtin-name">set</span> <span class="hljs-keyword">and</span> test <span class="hljs-builtin-name">set</span>
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, <span class="hljs-attribute">test_size</span>=0.3, <span class="hljs-attribute">random_state</span>=1) # 70% training <span class="hljs-keyword">and</span> 30% test
    # Load model
    dt = pickle.load(open(<span class="hljs-string">'dt.sav'</span>, <span class="hljs-string">'rb'</span>))
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">Although the dataset has been split, only the test data is used to verify the predictions of the decision tree and the quality of the KMC outputs.</p>
      </li>
      <li class="list"><strong class="bold">Predicting and measuring the results</strong>: The decision tree predictions are made and the accuracy of the results is measured:
        <pre class="programlisting"><code class="hljs nix">    <span class="hljs-comment">#Predict the response for the test dataset</span>
    <span class="hljs-attr">y_pred</span> = dt.predict(X_test)
    <span class="hljs-comment"># Model Accuracy</span>
    <span class="hljs-attr">acc=metrics.accuracy_score(y_test,</span> y_pred)
    print(<span class="hljs-string">"Accuracy:"</span>,round(acc,<span class="hljs-number">3</span>))
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">Once again, in this example, as in <code class="Code-In-Text--PACKT-">decision_tree.py</code>, the accuracy is 0.97.</p>
      </li>
      <li class="list"><strong class="bold">Double-checking the accuracy of the predictions</strong>: In the early days of a project or for <a id="_idIndexMarker221"/>maintenance purposes, double-checking is recommended. The function is activated or deactivated with a parameter named <code class="Code-In-Text--PACKT-">doublecheck</code>.<p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The prediction results are checked line by line against the original labels and measured:</p>
        <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment">#Double Check the Model's Accuracy</span>
    <span class="hljs-attribute">doublecheck</span>=1 #0 deactivated, 1 activated
    <span class="hljs-keyword">if</span> <span class="hljs-attribute">doublecheck</span>==1:
        <span class="hljs-attribute">t</span>=0
        <span class="hljs-attribute">f</span>=0
        <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(0,1000):
            <span class="hljs-attribute">xf1</span>=ds.at[i,<span class="hljs-string">'f1'</span>]
            <span class="hljs-attribute">xf2</span>=ds.at[i,<span class="hljs-string">'f2'</span>]
            <span class="hljs-attribute">xclass</span>=ds.at[i,<span class="hljs-string">'label'</span>]
            X_DL = [[xf1,xf2]]
            prediction =clf.predict(X_DL)
            <span class="hljs-attribute">e</span>=<span class="hljs-literal">False</span>
            <span class="hljs-keyword">if</span>(<span class="hljs-attribute">prediction</span>==xclass):
                <span class="hljs-attribute">e</span>=<span class="hljs-literal">True</span>
                t+=1
            <span class="hljs-keyword">if</span>(prediction!=xclass):
                <span class="hljs-attribute">e</span>=<span class="hljs-literal">False</span>
                f+=1
            <span class="hljs-builtin-name">print</span> (i+1,<span class="hljs-string">"The prediction for"</span>,X_DL,<span class="hljs-string">" is:"</span>,
                str(prediction).strip(<span class="hljs-string">'[]'</span>),
                <span class="hljs-string">"the class is"</span>,xclass,<span class="hljs-string">"acc.:"</span>,e)
        <span class="hljs-builtin-name">print</span>(<span class="hljs-string">"true:"</span>, t, <span class="hljs-string">"false"</span>, f, <span class="hljs-string">"accuracy"</span>,
            round(t/(t+f),3))
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The program will print out the predictions line by line, stating if they are <code class="Code-In-Text--PACKT-">True</code> or <code class="Code-In-Text--PACKT-">False</code>:</p>
        <pre class="programlisting"><code class="hljs basic"><span class="hljs-symbol">995 </span>The prediction <span class="hljs-keyword">for</span> [[<span class="hljs-number">85</span>, <span class="hljs-number">79</span>]]  is: <span class="hljs-number">4</span> the class is <span class="hljs-number">4</span> acc.: True
<span class="hljs-symbol">996 </span>The prediction <span class="hljs-keyword">for</span> [[<span class="hljs-number">103</span>, <span class="hljs-number">100</span>]]  is: <span class="hljs-number">4</span> the class is <span class="hljs-number">4</span> acc.: True
<span class="hljs-symbol">997 </span>The prediction <span class="hljs-keyword">for</span> [[<span class="hljs-number">71</span>, <span class="hljs-number">82</span>]]  is: <span class="hljs-number">5</span> the class is <span class="hljs-number">1</span> acc.: False
<span class="hljs-symbol">998 </span>The prediction <span class="hljs-keyword">for</span> [[<span class="hljs-number">50</span>, <span class="hljs-number">44</span>]]  is: <span class="hljs-number">0</span> the class is <span class="hljs-number">0</span> acc.: True
<span class="hljs-symbol">999 </span>The prediction <span class="hljs-keyword">for</span> [[<span class="hljs-number">62</span>, <span class="hljs-number">51</span>]]  is: <span class="hljs-number">5</span> the class is <span class="hljs-number">5</span> acc.: True
</code></pre>
      </li>
    </ul>
    <p class="normal">In this example, the<a id="_idIndexMarker222"/> accuracy is 0.99, which is high. Only 9 out of 1,000 predictions were <code class="Code-In-Text--PACKT-">False</code>. This result is not the same as the metrics function because it is a simple calculation that does not take mean errors or other factors into account. However, it shows that the KMC produced good results for this problem.</p>
    <p class="normal">Decision trees provide a good approach for the KMC. However, random forests take machine learning to another level and provide an interesting alternative, if necessary, to the use of decision trees.</p>
    <h2 id="_idParaDest-91" class="title">Random forests as an alternative to decision trees</h2>
    <p class="normal">Random forests<a id="_idIndexMarker223"/> open mind-blowing ML horizons. They are ensemble meta-algorithms. As an ensemble, they contain several decision trees. As meta-algorithms, they go beyond having one decision tree making predictions.</p>
    <p class="normal">To run an ensemble (several decision trees) as a meta-algorithm (several algorithms training and predicting the same data), the following module is required:</p>
    <pre class="programlisting"><code class="hljs angelscript"><span class="hljs-keyword">from</span> sklearn.ensemble <span class="hljs-keyword">import</span> RandomForestClassifier
</code></pre>
    <p class="normal">To understand how a random forest works as a meta-algorithm, let's focus on three key parameters in the following classifier:</p>
    <pre class="programlisting"><code class="hljs routeros">clf = RandomForestClassifier(<span class="hljs-attribute">n_estimators</span>=25, <span class="hljs-attribute">random_state</span>=None, <span class="hljs-attribute">bootstrap</span>=<span class="hljs-literal">True</span>)
</code></pre>
    <ul>
      <li class="list"><code class="Code-In-Text--PACKT-">n_estimators=25</code>: This parameter states the number of <strong class="bold">trees</strong> in the <strong class="bold">forest</strong>. Each tree will run its prediction. The main method to reach the final prediction is obtained by averaging the predictions of each tree.<p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">At each split of each tree, features are randomly permuted. The trees use different feature approaches.</p>
      </li>
      <li class="list"><code class="Code-In-Text--PACKT-">bootstrap=True</code>: When bootstrap is activated, a smaller sample is bootstrapped from the sample provided. Each tree thus bootstraps its own samples, adding more variety.</li>
      <li class="list"><code class="Code-In-Text--PACKT-">random_state=None</code>: When <code class="Code-In-Text--PACKT-">random_state=None</code> is activated, the random function uses <code class="Code-In-Text--PACKT-">np.random</code>.<p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">You can also use the other methods by consulting the scikit-learn documentation (see <em class="italics">Further reading</em> at the end of the chapter). My preference is to use <code class="Code-In-Text--PACKT-">np.random</code>. Note that to split the training state, I use scikit-learn's default random generator example with <code class="Code-In-Text--PACKT-">random_state=0</code>.</p>
        <p class="Bullet-Without-Bullet-Within-Bullet-End--PACKT-">This shows the importance of these small changes in parameters. After many tests, this is what I preferred. But maybe, in other cases, other <code class="Code-In-Text--PACKT-">random_state</code> values are preferable.</p>
      </li>
    </ul>
    <p class="normal">Beyond these key concepts and parameters, <code class="Code-In-Text--PACKT-">random_forest.py</code> can be built in a clear, straightforward way.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">random_forest.py</code> is built with the same structure as the KMC or decision tree program. It loads a dataset, prepares the features and target variables, splits the dataset into training and testing sub-datasets, predicts, and measures. <code class="Code-In-Text--PACKT-">random_forest.py</code> also contains a custom double-check function that will display each prediction, its status (<code class="Code-In-Text--PACKT-">True</code> or <code class="Code-In-Text--PACKT-">False</code>), and provide an independent accuracy rate.</p>
    <ul>
      <li class="list"><strong class="bold">Loading the dataset</strong>: <code class="Code-In-Text--PACKT-">ckmc.csv</code> was generated by the preceding KMC program. This time, it will be read by <code class="Code-In-Text--PACKT-">random_forest.py</code> instead of <code class="Code-In-Text--PACKT-">decision_tree.py</code>. The dataset is loaded, and the features and target variables <a id="_idIndexMarker224"/>are identified. Note the <code class="Code-In-Text--PACKT-">pp</code> variable, which will trigger the <code class="Code-In-Text--PACKT-">print</code> function or not. This is useful for switching from production mode to maintenance mode with a single variable change. Change <code class="Code-In-Text--PACKT-">pp=0</code> to <code class="Code-In-Text--PACKT-">pp=1</code> if you wish to switch to maintenance mode. In this case <code class="Code-In-Text--PACKT-">pp</code> is activated:
        <pre class="programlisting"><code class="hljs routeros"><span class="hljs-attribute">pp</span>=1 # <span class="hljs-builtin-name">print</span> information
<span class="hljs-comment"># load dataset</span>
col_names = [<span class="hljs-string">'f1'</span>, <span class="hljs-string">'f2'</span>,<span class="hljs-string">'label'</span>]
df = pd.read_csv(<span class="hljs-string">"ckmc.csv"</span>, <span class="hljs-attribute">header</span>=None, <span class="hljs-attribute">names</span>=col_names)
<span class="hljs-keyword">if</span> <span class="hljs-attribute">pp</span>==1:
    <span class="hljs-builtin-name">print</span>(df.head())
<span class="hljs-comment">#loading features and label (classes)</span>
feature_cols = [<span class="hljs-string">'f1'</span>, <span class="hljs-string">'f2'</span>]
X = df[feature_cols] # Features
y = df.label # Target variable
<span class="hljs-keyword">if</span> <span class="hljs-attribute">pp</span>==1:
    <span class="hljs-builtin-name">print</span>(X)
    <span class="hljs-builtin-name">print</span>(y)
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The program prints the labeled data:</p>
        <pre class="programlisting"><code class="hljs angelscript">   f1  f2  label
<span class="hljs-number">0</span>  <span class="hljs-number">80</span>  <span class="hljs-number">53</span>      <span class="hljs-number">5</span>
<span class="hljs-number">1</span>  <span class="hljs-number">18</span>   <span class="hljs-number">8</span>      <span class="hljs-number">2</span>
<span class="hljs-number">2</span>  <span class="hljs-number">55</span>  <span class="hljs-number">38</span>      <span class="hljs-number">0</span>
<span class="hljs-number">3</span>  <span class="hljs-number">74</span>  <span class="hljs-number">74</span>      <span class="hljs-number">5</span>
<span class="hljs-number">4</span>  <span class="hljs-number">17</span>   <span class="hljs-number">4</span>      <span class="hljs-number">2</span>
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The program prints the target cluster numbers to predict:</p>
        <pre class="programlisting"><code class="hljs angelscript">[<span class="hljs-number">1</span> <span class="hljs-number">5</span> <span class="hljs-number">5</span> <span class="hljs-number">5</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span>…]
</code></pre>
      </li>
      <li class="list"><strong class="bold">Splitting the data, creating the classifier, and training the model</strong>: The dataset is split into training and testing data. The random forest classifier is created with 25 estimators. Then the model is trained:
        <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment">#Divide the data into training and testing sets</span>
X_train, X_test, y_train, y_test = train_test_split(
    X, y, <span class="hljs-attribute">test_size</span>=0.2, <span class="hljs-attribute">random_state</span>=0)
<span class="hljs-comment">#Creating Random Forest Classifier and training</span>
clf = RandomForestClassifier(<span class="hljs-attribute">n_estimators</span>=25,
    <span class="hljs-attribute">random_state</span>=0)
clf.fit(X_train, y_train)
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The program is ready to predict.</p>
      </li>
      <li class="list"><strong class="bold">Predicting and measuring the accuracy of the trained model</strong>:
        <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment">#Predictions</span>
y_pred = clf.predict(X_test)
<span class="hljs-keyword">if</span> <span class="hljs-attribute">pp</span>==1:
    <span class="hljs-builtin-name">print</span>(y_pred)
<span class="hljs-comment">#Metrics</span>
<span class="hljs-attribute">ae</span>=metrics.mean_absolute_error(y_test, y_pred)
<span class="hljs-builtin-name">print</span>(<span class="hljs-string">'Mean Absolute Error:'</span>, round(ae,3))
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">The output results and metrics are satisfactory:</p>
        <pre class="programlisting"><code class="hljs yaml"><span class="hljs-attr">predictions:</span>
<span class="hljs-string">[1</span> <span class="hljs-number">5</span> <span class="hljs-number">5</span> <span class="hljs-number">5</span> <span class="hljs-number">2</span> <span class="hljs-number">1</span> <span class="hljs-number">3</span> <span class="hljs-number">3</span> <span class="hljs-number">0</span> <span class="hljs-number">5</span> <span class="hljs-number">3</span> <span class="hljs-number">5</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-number">4</span> <span class="hljs-number">3</span> <span class="hljs-number">1</span> <span class="hljs-number">3</span> <span class="hljs-number">2</span> <span class="hljs-number">2</span> <span class="hljs-string">…]</span>
<span class="hljs-attr">Mean Absolute Error:</span> <span class="hljs-number">0.165</span>
</code></pre>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">A mean error approaching 0 is an efficient result.</p>
      </li>
      <li class="list"><strong class="bold">Double-checking</strong>: A double-checking function<a id="_idIndexMarker225"/> is recommended in the early stages of an<a id="_idIndexMarker226"/> ML project and for maintenance purposes. The function is activated by the <code class="Code-In-Text--PACKT-">doublecheck</code> parameter as in <code class="Code-In-Text--PACKT-">kmc2dt_chaining.py</code>. Set <code class="Code-In-Text--PACKT-">doublecheck=1</code> if you wish to activate the maintenance mode. It is in fact, the same template:
        <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment">#Double Check the Model's Accuracy</span>
<span class="hljs-attribute">doublecheck</span>=0  # <span class="hljs-attribute">1</span>=<span class="hljs-literal">yes</span>, <span class="hljs-attribute">0</span>=<span class="hljs-literal">no</span>
<span class="hljs-keyword">if</span> <span class="hljs-attribute">doublecheck</span>==1:
    <span class="hljs-attribute">t</span>=0
    <span class="hljs-attribute">f</span>=0
    <span class="hljs-keyword">for</span> i <span class="hljs-keyword">in</span> range(0,1000):
        <span class="hljs-attribute">xf1</span>=df.at[i,<span class="hljs-string">'f1'</span>]
        <span class="hljs-attribute">xf2</span>=df.at[i,<span class="hljs-string">'f2'</span>]
        <span class="hljs-attribute">xclass</span>=df.at[i,<span class="hljs-string">'label'</span>]
        X_DL = [[xf1,xf2]]
        prediction =clf.predict(X_DL)
        <span class="hljs-attribute">e</span>=<span class="hljs-literal">False</span>
        <span class="hljs-keyword">if</span>(<span class="hljs-attribute">prediction</span>==xclass):
            <span class="hljs-attribute">e</span>=<span class="hljs-literal">True</span>
            t+=1
        <span class="hljs-keyword">if</span>(prediction!=xclass):
            <span class="hljs-attribute">e</span>=<span class="hljs-literal">False</span>
            f+=1
        <span class="hljs-keyword">if</span> <span class="hljs-attribute">pp</span>==1:
            <span class="hljs-builtin-name">print</span> (i+1,<span class="hljs-string">"The prediction for"</span>,X_DL,<span class="hljs-string">" is:"</span>,
                str(prediction).strip(<span class="hljs-string">'[]'</span>),<span class="hljs-string">"the class is"</span>,
                xclass,<span class="hljs-string">"acc.:"</span>,e)
    <span class="hljs-attribute">acc</span>=round(t/(t+f),3)
    <span class="hljs-builtin-name">print</span>(<span class="hljs-string">"true:"</span>,t,<span class="hljs-string">"false"</span>,f,<span class="hljs-string">"accuracy"</span>,acc)
    <span class="hljs-builtin-name">print</span>(<span class="hljs-string">"Absolute Error"</span>,round(1-acc,3))
</code></pre>
      </li>
    </ul>
    <p class="normal">The accuracy of the random forest is efficient. The output of the measurement is:</p>
    <pre class="programlisting"><code class="hljs yaml"><span class="hljs-attr">Mean Absolute Error:</span> <span class="hljs-number">0.085</span>
<span class="hljs-attr">true:</span> <span class="hljs-number">994</span> <span class="hljs-literal">false</span> <span class="hljs-number">6</span> <span class="hljs-string">accuracy</span> <span class="hljs-number">0.994</span>
<span class="hljs-string">Absolute</span> <span class="hljs-string">Error</span> <span class="hljs-number">0.006</span>
</code></pre>
    <p class="normal">The absolute error<a id="_idIndexMarker227"/> is a simple arithmetic approach that does not take mean errors or other factors into account. The score will vary from one run to another because of the random nature of the algorithm. However, in this case, 6 errors out of 1,000 predictions is a good result.</p>
    <p class="normal">Ensemble meta-algorithms such as random forests can replace the decision tree in <code class="Code-In-Text--PACKT-">kmc2dt_chaining.py</code> with just a few lines of code, as we just saw in this section, tremendously boosting the whole chained ML process.</p>
    <p class="normal">Chained ML algorithms using ensemble meta-algorithms are extremely powerful and efficient. In this chapter, we used a chained ML solution to deal with large datasets and perform automatic quality control on machine intelligence predictions.</p>
    <h1 id="_idParaDest-92" class="title">Summary</h1>
    <p class="normal">Although it may seem paradoxical, try to avoid AI before jumping into a project that involves millions to billions of records of data (such as SQL, Oracle, and big data). Try simpler classical solutions like big data methods. If the AI project goes through, LLN will lead to random sampling over the datasets, thanks to CLT.</p>
    <p class="normal">A pipeline of classical and ML processes will solve the volume problem, as well as the human analytic limit problem. The random sampling function does not need to run a mini-batch function included in the KMC program. Batches can be generated as a preprocessing phase using classical programs. These programs will produce random batches of equal size to the KMC NP-hard problem, transposing it into an NP problem.</p>
    <p class="normal">KMC, an unsupervised training algorithm, will transform unlabeled data into a labeled data output containing a cluster number as a label.</p>
    <p class="normal">In turn, a decision tree, chained to the KMC program, will train its model using the output of the KMC. The model will be saved just as the KMC model was saved. The random forests algorithm can replace the decision tree algorithm if it provides better results during the training phase of the pipeline.</p>
    <p class="normal">In production mode, a chained ML program containing the KMC trained model and the decision tree trained model can make classification predictions on fixed random sampled batches. Real-time metrics will monitor the quality of the process. The chained program, being continuous, can run twenty-four seven, providing reliable real-time results without human intervention.</p>
    <p class="normal">The next chapter explores yet another ML challenge: the increasing amount of language translations generated by global business and social communication.</p>
    <h1 id="_idParaDest-93" class="title">Questions</h1>
    <ol>
      <li class="list">The number of k clusters is not that important. (Yes | No)</li>
      <li class="list">Mini-batches and batches contain the same amount of data. (Yes | No)</li>
      <li class="list">K-means can run without mini-batches. (Yes | No)</li>
      <li class="list">Must centroids be optimized for result acceptance? (Yes | No)</li>
      <li class="list">It does not take long to optimize hyperparameters. (Yes | No)</li>
      <li class="list">It sometimes takes weeks to train a large dataset. (Yes | No)</li>
      <li class="list">Decision trees and random forests are unsupervised algorithms. (Yes | No)</li>
    </ol>
    <h1 id="_idParaDest-94" class="title">Further reading</h1>
    <ul>
      <li class="list">Decision trees: <a href="https://scikit-learn.org/stable/modules/tree.html"><span class="url">https://scikit-learn.org/stable/modules/tree.html</span></a></li>
      <li class="list">Random forests: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html"><span class="url">https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html</span></a></li>
    </ul>
  </div>
</body></html>