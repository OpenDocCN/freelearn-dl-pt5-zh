<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer087">
<h1 class="chapter-number" id="_idParaDest-68"><a id="_idTextAnchor067"/>3</h1>
<h1 id="_idParaDest-69"><a id="_idTextAnchor068"/>Regularization with Linear Models</h1>
<p>A huge part of <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>) is made up of linear models. Although sometimes considered less powerful than their nonlinear counterparts (such as tree-based models or deep learning models), linear models do address many concrete, valuable problems. Customer churn and advertising optimization are just a couple of problems where linear models may be the <span class="No-Break">right solution.</span></p>
<p>In this chapter, we will cover the <span class="No-Break">following recipes:</span></p>
<ul>
<li>Training a linear regression <span class="No-Break">with scikit-learn</span></li>
<li>Regularizing with <span class="No-Break">ridge regression</span></li>
<li>Regularizing with <span class="No-Break">lasso regression</span></li>
<li>Regularizing with elastic <span class="No-Break">net regression</span></li>
<li>Training a logistic <span class="No-Break">regression model</span></li>
<li>Regularizing a logistic <span class="No-Break">regression model</span></li>
<li>Choosing the <span class="No-Break">right regularization</span></li>
</ul>
<p>By the end of this chapter, we will have learned how to use and regularize some of the most commonly used <span class="No-Break">linear models.</span></p>
<h1 id="_idParaDest-70"><a id="_idTextAnchor069"/>Technical requirements</h1>
<p>In this chapter, besides loading data, you will learn how to fit and compute inferences with several linear models. In order to do so, the following libraries <span class="No-Break">are required:</span></p>
<ul>
<li><span class="No-Break">NumPy</span></li>
<li><span class="No-Break">Matplotlib</span></li>
<li><span class="No-Break">Scikit-learn</span></li>
</ul>
<h1 id="_idParaDest-71"><a id="_idTextAnchor070"/>Training a linear regression model with scikit-learn</h1>
<p>Linear regression<a id="_idIndexMarker096"/> is one the most basic <a id="_idIndexMarker097"/>ML models we can use, but it is very useful. Most people used linear regression in high school without talking about ML, and still use it on a regular basis within spreadsheets. In this recipe, we will explain the basics of linear regression, and then train and evaluate a linear regression model using scikit-learn on the California <span class="No-Break">housing dataset.</span></p>
<h2 id="_idParaDest-72"><a id="_idTextAnchor071"/>Getting ready</h2>
<p>Linear regression is not a complicated model, but it is still useful to understand what is under the hood to get the best out <span class="No-Break">of it.</span></p>
<p>The way linear regression works is pretty straightforward. Heading back to the real estate price example, if we consider a feature <em class="italic">x</em> such as the apartment surface and a label <em class="italic">y</em> such as the apartment price, a common solution would be to find <em class="italic">a</em> and <em class="italic">b</em> such that <em class="italic">y = ax + </em><span class="No-Break"><em class="italic">b</em></span><span class="No-Break">.</span></p>
<p>Unfortunately, this is not so simple in real life. There is usually no <em class="italic">a</em> and <em class="italic">b</em> that makes this equality always respected. It is more likely that we can define a function <strong class="source-inline">h(x)</strong> that aims to give a value as close as possible <span class="No-Break">to </span><span class="No-Break"><em class="italic">y</em></span><span class="No-Break">.</span></p>
<p>Also, we may have not just one feature <em class="italic">x</em>, but several features <em class="italic">x</em>1, <em class="italic">x</em>2,…, <em class="italic">x</em><span class="subscript">n</span>, representing apartment surface, location, floor, number of rooms, exponent features, and <span class="No-Break">so on.</span></p>
<p>By this logic, we would end up with a prediction <em class="italic">h(x)</em> that may look like <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer042">
<img alt="" height="118" src="image/Formula_03_001.jpg" width="370"/>
</div>
</div>
<p><img alt="" height="22" src="image/Formula_03_002.png" width="23"/><em class="italic"> </em>is the weight associated to the feature <img alt="" height="22" src="image/Formula_03_003.png" width="20"/> <em class="italic">x</em>j, and <em class="italic">b</em> is a bias term. This is just a generalization of the previous <em class="italic">y = ax + b</em> to <em class="italic">n</em> features. This formula allows a linear regression to predict virtually any <span class="No-Break">real number.</span></p>
<p>The goal of our ML model is to find the set of <em class="italic">w</em> and <em class="italic">b</em> values that minimizes prediction errors on the training set. By this, we mean finding the parameters <em class="italic">w</em> and <em class="italic">b</em> so that <em class="italic">h(x)</em> and <em class="italic">y</em> are as close <span class="No-Break">as possible.</span></p>
<p>One way to achieve<a id="_idIndexMarker098"/> that is to minimize the loss <em class="italic">L</em>, that can be defined here as a slightly modified <strong class="bold">mean squared </strong><span class="No-Break"><strong class="bold">error</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">MSE</strong></span><span class="No-Break">):</span></p>
<div>
<div class="IMG---Figure" id="_idContainer045">
<img alt="" height="123" src="image/Formula_03_004.jpg" width="511"/>
</div>
</div>
<p><img alt="" height="29" src="image/Formula_03_005.png" width="34"/> is the <a id="_idIndexMarker099"/>ground truth of the sample <em class="italic">i</em> in<a id="_idIndexMarker100"/> the training set, and <em class="italic">m</em> is the number of samples in the <span class="No-Break">training set.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The loss is usually a representation of the difference between the ground truth and the predictions. Hence, minimizing the loss allows the model to predict values that are as close as possible to the <span class="No-Break">ground truth.</span></p>
<p>Minimizing this mean squared error would allow us to find the set of <em class="italic">w</em> and <em class="italic">b</em> values so that the prediction <em class="italic">h(x)</em> is as close as possible to the ground truth <em class="italic">y</em>. Schematically, this can be represented as finding the <em class="italic">w</em> that minimizes the loss, as shown in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<img alt="Figure 3.1 – Loss function as a function of a parameter theta, having a global minimum at the cross" height="812" src="image/B19629_03_01.jpg" width="789"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Loss function as a function of a parameter theta, having a global minimum at the cross</p>
<p>The next question is: how <a id="_idIndexMarker101"/>do we find the set of values <a id="_idIndexMarker102"/>to minimize the loss? There are several ways of solving this problem. One commonly used technique in ML is <span class="No-Break">gradient descent.</span></p>
<p>What is gradient descent? In a few words, it is going down the curve to the minimum value in the <span class="No-Break">preceding figure.</span></p>
<p>How does this work? It’s a <span class="No-Break">multi-step process:</span></p>
<ol>
<li>Start with random values of the parameters <em class="italic">w</em> and <em class="italic">b</em>. Random values are usually defined using normal distribution centered on zero. This is why having scaled features may help significantly <span class="No-Break">for convergence.</span></li>
<li>Compute the loss for the given data and current values of <em class="italic">w</em> and <em class="italic">b</em>. As defined earlier, we may use the mean squared error to compute the <span class="No-Break">loss </span><span class="No-Break"><em class="italic">L</em></span><span class="No-Break">.</span></li>
</ol>
<p>The following figure is a good representation of the situation at <span class="No-Break">this point:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<img alt="Figure 3.2 – The loss function with the global minimum at the red cross, and a possible random initial state at the blue cross" height="803" src="image/B19629_03_02.jpg" width="780"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – The loss function with the global minimum at the red cross, and a possible random initial state at the blue cross</p>
<ol>
<li value="3">Compute the<a id="_idIndexMarker103"/> loss<a id="_idIndexMarker104"/> gradient with respect to each parameter <img alt="" height="51" src="image/Formula_03_006.png" width="64"/> . This is nothing more than the slope of the loss for a given parameter, which can be computed with the <span class="No-Break">following equations:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer050">
<img alt="" height="122" src="image/Formula_03_007.jpg" width="614"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer051">
<img alt="" height="124" src="image/Formula_03_008.jpg" width="519"/>
</div>
</div>
<p class="callout-heading">Note</p>
<p class="callout">One may notice that the slope is expected to decrease as we get closer to the minimum. Indeed, as we get close to the minimum, the error tends to zero and so does the slope, based on <span class="No-Break">these equations.</span></p>
<ol>
<li value="4">Apply gradient descent to parameters. Apply the gradient descent to parameters, with a user-defined learning rate <em class="italic">α</em>. This is computed using the <span class="No-Break">following formulas:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer052">
<img alt="" height="110" src="image/Formula_03_009.jpg" width="314"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer053">
<img alt="" height="110" src="image/Formula_03_010.jpg" width="275"/>
</div>
</div>
<p>This allows us to take a <a id="_idIndexMarker105"/>step toward the<a id="_idIndexMarker106"/> minimum, as represented in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<img alt="Figure 3.3 – The gradient descent allows us to take one step down the loss function, allowing us to get closer to the global minimum" height="803" src="image/B19629_03_03.jpg" width="780"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – The gradient descent allows us to take one step down the loss function, allowing us to get closer to the global minimum</p>
<ol>
<li value="5">Iterate through <em class="italic">steps 2 to 4</em> until convergence or max iteration. This would allow us to reach the optimal parameters, as represented in the <span class="No-Break">following figure:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer055">
<img alt="Figure 3.4 – With enough iterations and a convex loss function, the parameters will converge to the global minimum" height="803" src="image/B19629_03_04.jpg" width="780"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – With enough iterations and a convex loss function, the parameters will converge to the global minimum</p>
<p class="callout-heading">Note</p>
<p class="callout">A learning rate <em class="italic">α</em> that is too large would miss the global minimum, or even diverge, while one that is too small would take forever <span class="No-Break">to converge.</span></p>
<p>To complete<a id="_idIndexMarker107"/> this <a id="_idIndexMarker108"/>recipe, the following libraries have to be installed: <strong class="source-inline">numpy</strong>, <strong class="source-inline">matplotlib</strong>, and <strong class="source-inline">sklearn</strong>. They can be installed with <strong class="source-inline">pip</strong> in the terminal, with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install -U numpy sklearn matplotlib</pre>
<h2 id="_idParaDest-73"><a id="_idTextAnchor072"/>How to do it…</h2>
<p>Fortunately, all this procedure is fully implemented in scikit-learn, and the only thing you need to do is fully reuse this library. Let’s now train a linear regression on the California housing dataset provided <span class="No-Break">by scikit-learn.</span></p>
<ol>
<li><strong class="bold">Make the required imports</strong>: Here, we import NumPy for data manipulation. From scikit-learn, we do <span class="No-Break">several imports:</span><ul><li><strong class="source-inline">fetch_california_housing</strong>: A function that allows us to load <span class="No-Break">the dataset</span></li><li><strong class="source-inline">train_test_split</strong>: A function that allows us to split <span class="No-Break">the data</span></li><li><strong class="source-inline">StandardScaler</strong>: A class that allows us to rescale <span class="No-Break">the data</span></li><li><strong class="source-inline">LinearRegression</strong>: The class that <a id="_idIndexMarker109"/>contains <a id="_idIndexMarker110"/>the implementation of the <span class="No-Break">linear regression</span></li></ul></li>
</ol>
<p>Here is what the code <span class="No-Break">looks like:</span></p>
<pre class="source-code">
import numpy as np
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LinearRegression</pre>
<ol>
<li value="2"><strong class="bold">Load the California housing dataset</strong>: For pedagogical purposes, we will do a bit of feature engineering here and calculate the sum of all the features raised to the power of two: to do so, we simply concatenate the features <strong class="source-inline">X</strong> <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">X*X</strong></span><span class="No-Break">:</span><pre class="source-code">
# Load the California housing dataset</pre><pre class="source-code">
X, y = fetch_california_housing(return_X_y=True)</pre><pre class="source-code">
X = np.concatenate([X, X*X], axis=1)</pre></li>
<li><strong class="bold">Split the data</strong>: We split the data using the <strong class="source-inline">train_test_split</strong> function, with <strong class="source-inline">test_size=0.2</strong>, meaning we end up having 80% of the data in the training set, and 20% in the test set, split at random. This is <span class="No-Break">shown here:</span><pre class="source-code">
# Split the data</pre><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    X, y, test_size=0.2, random_state=0)</pre></li>
<li><strong class="bold">Prepare the data</strong>: Since we have only quantitative features here, the only preparation we need is rescaling. We can use the standard scaler of scikit-learn. We need to instantiate it, then fit it on the training set and transform the training set, and<a id="_idIndexMarker111"/> finally <a id="_idIndexMarker112"/>we transform the test set. Feel free to use any <span class="No-Break">other rescaler:</span><pre class="source-code">
# Rescale the data</pre><pre class="source-code">
scaler = StandardScaler()</pre><pre class="source-code">
X_train = scaler.fit_transform(X_train)</pre><pre class="source-code">
X_test = scaler.transform(X_test)</pre></li>
<li>Fit the model on the training set. The model must be instantiated, and here we use the default parameters, so nothing is specified. Once the model is instantiated, we can use the <strong class="source-inline">.fit()</strong> method on the <span class="No-Break">training set:</span><pre class="source-code">
# Fit the linear regression model</pre><pre class="source-code">
lr = LinearRegression()</pre><pre class="source-code">
lr.fit(X_train, y_train)</pre></li>
<li>Evaluate the model on both the training and test set. Here, we use the <strong class="source-inline">.score()</strong> method of the <strong class="source-inline">LinearRegression</strong> class, which provides <strong class="source-inline">R2-score</strong>, but you can use any other metric provided in <strong class="source-inline">sklearn.metrics</strong> that <span class="No-Break">suits regression:</span><pre class="source-code">
# Print the R2-score on train and test</pre><pre class="source-code">
print('R2-score on train set:', lr.score(X_train, y_train))</pre><pre class="source-code">
print('R2-score on test set:', lr.score(X_test, y_test))</pre></li>
</ol>
<p>Here is <span class="No-Break">the output:</span></p>
<pre class="source-code">
R2-score on train set: 0.6323843381852894 R2-score on test set: -1.2472000127402643</pre>
<p>As we can see, there is a significant difference between the train and test set’s scores, indicating model overfitting on the train set. To address this problem, regularization techniques will <a id="_idIndexMarker113"/>be <a id="_idIndexMarker114"/>proposed in the <span class="No-Break">following recipes.</span></p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor073"/>There’s more…</h2>
<p>Once the model has been trained, we can access all the parameters <em class="italic">w</em> (here, 16 values for 16 input features) as well as the intercept <em class="italic">b</em>, with the attributes, <strong class="source-inline">.coef_</strong>, and the <strong class="source-inline">.intercept_</strong> value of the <strong class="source-inline">LinearRegression</strong> <span class="No-Break">object respectively:</span></p>
<pre class="source-code">
print('w values:', lr.coef_)
print('b value:', lr.intercept_)</pre>
<p>Here is <span class="No-Break">the output:</span></p>
<pre class="source-code">
w values: [ 1.12882772e+00 -6.48931138e-02 -4.04087026e-01  4.87937619e-01  -1.69895164e-03 -4.09553062e-01 -3.72826365e+00 -8.38728583e+00  -2.67065542e-01  2.04856554e-01  2.46387700e-01 -3.19674747e-01   2.58750270e-03  3.91054062e-01  2.82040287e+00 -7.50771410e+00] b value: 2.072498958939411</pre>
<p>If we plot these values, we will notice that their values range between approximately <strong class="source-inline">-8</strong> and <strong class="source-inline">2</strong> on <span class="No-Break">this dataset:</span></p>
<pre class="source-code">
import matplotlib.pyplot as plt
plt.bar(np.arange(len(lr.coef_)), lr.coef_)
plt.xlabel('feature index')
plt.ylabel('weight value')
plt.show()</pre>
<p>Here is a visual representation <span class="No-Break">of this:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<img alt="Figure 3.5 – Learned values of each weight of the linear regression model. The range of the values is quite large, from -8 to 2" height="466" src="image/B19629_03_05.jpg" width="617"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – Learned values of each weight of the linear regression model. The range of the values is quite large, from -8 to 2</p>
<h2 id="_idParaDest-75"><a id="_idTextAnchor074"/>See also</h2>
<p>To have a full <a id="_idIndexMarker115"/>understanding<a id="_idIndexMarker116"/> of how to use linear regression using scikit-learn, it is good practice to check the official documentation<a id="_idIndexMarker117"/> of the <span class="No-Break">class: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml</span></a><span class="No-Break">.</span></p>
<p>We now have a good understanding of how linear regression works, and we will see in the next section how to regularize it <span class="No-Break">with penalization.</span></p>
<h1 id="_idParaDest-76"><a id="_idTextAnchor075"/>Regularizing with ridge regression</h1>
<p>A very common<a id="_idIndexMarker118"/> and useful way to regularize a linear regression is through penalization of the loss function. In this recipe, after reviewing what it means to add penalization to the loss function in the case of ridge regression, we will train a ridge model on the same California housing dataset as in the previous recipe, and see how it can improve the score thanks <span class="No-Break">to regularization.</span></p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor076"/>Getting ready</h2>
<p>One way to make sure that a model’s parameters are not going to overfit is to keep them close to zero: if the parameters do not have the possibility to evolve freely, they are less likely <span class="No-Break">to overfit.</span></p>
<p>To that end, ridge regression adds a new term (regularization term) to the <span class="No-Break">loss <img alt="" height="28" src="image/Formula_03_011.png" width="61"/>:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<img alt="" height="123" src="image/Formula_03_012.jpg" width="761"/>
</div>
</div>
<p>Where <img alt="" height="29" src="image/Formula_03_013.png" width="42"/>is the <em class="italic">L2</em> norm <span class="No-Break">of </span><span class="No-Break"><em class="italic">w</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<img alt="" height="117" src="image/Formula_03_014.jpg" width="262"/>
</div>
</div>
<p>With this loss, we intuitively understand that high values of weights <em class="italic">w</em> are not possible, and thus overfitting is less likely. Also, <em class="italic">𝜆</em> is a hyperparameter (it can be fine-tuned) allowing us to control the <span class="No-Break">regularization level:</span></p>
<ul>
<li>A high value of <em class="italic">𝜆</em> means <span class="No-Break">high regularization</span></li>
<li>A value of <em class="italic">𝜆</em><em class="italic">=0</em> means no regularization, for example, regular <span class="No-Break">linear regression</span></li>
</ul>
<p>The gradient descent formulas are slightly updated to <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<img alt="" height="125" src="image/Formula_03_015.jpg" width="761"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer062">
<img alt="" height="125" src="image/Formula_03_016.jpg" width="666"/>
</div>
</div>
<p>Let’s now see how to use ridge regression with scikit-learn: the same libraries required in the previous recipe must be installed: <strong class="source-inline">numpy</strong>, <strong class="source-inline">sklearn</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">matplotlib</strong></span><span class="No-Break">.</span></p>
<p>Also, we assume the data is already downloaded and prepared from the previous recipe. To download, split and prepare the data, refer to the <span class="No-Break">previous recipe.</span></p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor077"/>How to do it…</h2>
<p>Let’s assume we are<a id="_idIndexMarker119"/> reusing the same data from the previous recipe. We will just train and evaluate another model on this exact same data, including the feature engineering with the squared features. The related implementation in scikit-learn for ridge regression is the <strong class="source-inline">Ridge</strong> class, where the <strong class="source-inline">alpha</strong> class attribute is equivalent to the <em class="italic">𝜆</em> in the preceding equation. Let’s <span class="No-Break">use it:</span></p>
<ol>
<li>Import the <strong class="source-inline">Ridge</strong> class <span class="No-Break">from scikit-learn:</span><pre class="source-code">
from sklearn.linear_model import Ridge</pre></li>
<li>We then instantiate a ridge model. A regularization parameter of <strong class="source-inline">alpha=5000</strong> has been selected here, but every dataset may need a very specific hyperparameter value to perform best. Next, train the model on the training set (previously prepared) with the <strong class="source-inline">.fit()</strong> method, which is shown <span class="No-Break">as follows:</span><pre class="source-code">
# Fit the Ridge model ridge = Ridge(alpha=5000)</pre><pre class="source-code">
ridge.fit(X_train, y_train)</pre><pre class="source-code">
Ridge(alpha=5000)</pre></li>
<li>We then evaluate the model. Here, we compute and display the R2-score provided by the <strong class="source-inline">.score()</strong> of the ridge class, but any other regression metric could <span class="No-Break">be used:</span><pre class="source-code">
# Print the R2-score on train and test</pre><pre class="source-code">
print('R2-score on train set:', ridge.score(X_train, y_train))</pre><pre class="source-code">
print('R2-score on test set:', ridge.score(X_test, y_test))</pre></li>
</ol>
<p>Here is <span class="No-Break">the output:</span></p>
<pre class="source-code">
R2-score on train set: 0.5398290317808138 R2-score on test set: 0.5034148460338739</pre>
<p>We will notice that we are getting better results on the test set compared to the linear regression model (with no regularization) by allowing the R2-score on the test set to be slightly <span class="No-Break">above </span><span class="No-Break"><strong class="source-inline">0.5</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-79"><a id="_idTextAnchor078"/>There’s more…</h2>
<p>We can also print the<a id="_idIndexMarker120"/> weights and plot them, and compare those values to the ones of regular linear regression, <span class="No-Break">as follows:</span></p>
<pre class="source-code">
print('theta values:', ridge.coef_)
print('b value:', ridge.intercept_)</pre>
<p>Here is <span class="No-Break">the output:</span></p>
<pre class="source-code">
theta values: [ 0.43456599  0.06311698  0.00463607  0.00963748  0.00896739 -0.05894055  -0.17177956 -0.15109744  0.22933247  0.08516982  0.01842825 -0.01049763  -0.00358684  0.03935491 -0.17562536  0.1507696 ] b value: 2.07249895893891</pre>
<p>For visualization, we can also plot these values with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
plt.bar(np.arange(len(ridge.coef_)), ridge.coef_)
plt.xlabel('feature index') plt.ylabel('weight value')
plt.show()</pre>
<p> This code outputs <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer063">
<img alt="Figure 3.6 – Learned values of each weight of the ridge regression model. Note that some are positive, some negative, and none are purely equal to zero. Also, the range is much smaller than without regularization" height="411" src="image/B19629_03_06.jpg" width="556"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.6 – Learned values of each weight of the ridge regression model. Note that some are positive, some negative, and none are purely equal to zero. Also, the range is much smaller than without regularization</p>
<p>The weight values are now ranging from <strong class="source-inline">-0.2</strong> to .<strong class="source-inline">0.5</strong>, which is indeed much smaller than with <span class="No-Break">no penalization.</span></p>
<p>As expected, this <a id="_idIndexMarker121"/>adding regularization results in a test set R2-score that is much better and closer to the train set than <span class="No-Break">without regularization.</span></p>
<h2 id="_idParaDest-80"><a id="_idTextAnchor079"/>See also</h2>
<p>For more information about all the possible parameters and hyperparameters of the Ridge regression, you can take a look at the official scikit-learn <span class="No-Break">documentation: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-81"><a id="_idTextAnchor080"/>Regularizing with lasso regression</h1>
<p><strong class="bold">Lasso</strong> regression <a id="_idIndexMarker122"/>stands for <strong class="bold">Least Absolute Shrinkage and Selection Operator</strong>. This<a id="_idIndexMarker123"/> is a regularization method that is conceptually very close to ridge regression. In some cases, lasso regression outperforms ridge regression, which is why it’s useful to know what it does and how to use it. In this recipe, we will briefly explain what lasso regression is and then train a model using scikit-learn on the same California <span class="No-Break">housing dataset.</span></p>
<h2 id="_idParaDest-82"><a id="_idTextAnchor081"/>Getting ready</h2>
<p>Instead of using the L2-norm, lasso uses the L1-norm, so that the loss <img alt="" height="22" src="image/Formula_03_017.png" width="52"/> is <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<img alt="" height="124" src="image/Formula_03_018.jpg" width="729"/>
</div>
</div>
<p>While ridge regression tends to decrease weights close to zero quite smoothly, lasso is more drastic. Lasso, having a much steeper loss, tends to set weights to zero <span class="No-Break">quite quickly.</span></p>
<p>Just like the ridge regression recipe, we’ll use the same libraries and assume they are installed: <strong class="source-inline">numpy</strong>, <strong class="source-inline">sklearn</strong>, and <strong class="source-inline">matplotlib</strong>. Also, we’ll assume the data is already downloaded<a id="_idIndexMarker124"/> <span class="No-Break">and prepared.</span></p>
<h2 id="_idParaDest-83"><a id="_idTextAnchor082"/>How to do it…</h2>
<p>The scikit-learn implementation of lasso is available with the <strong class="source-inline">Lasso</strong> class. Like in the <strong class="source-inline">Ridge</strong> class, <strong class="source-inline">alpha</strong> is the term that allows control of <span class="No-Break">this regularization:</span></p>
<ul>
<li>The value of alpha is 0 means <span class="No-Break">no regularization</span></li>
<li>A large value of alpha means <span class="No-Break">high regularization</span></li>
</ul>
<p>Again, we will reuse the same, already prepared dataset that we used for linear regression and <span class="No-Break">ridge regression:</span></p>
<ol>
<li>Import the <strong class="source-inline">Lasso</strong> class <span class="No-Break">from scikit-learn:</span><pre class="source-code">
from sklearn.linear_model import Lasso</pre></li>
<li>We instantiate a lasso model with a value of <strong class="source-inline">alpha=0.2</strong>, which provides pretty good results and low overfitting, as we will see right away. However, feel free to test other values, as each dataset may have its very unique optimal value. Next, train the model on the training set using the <strong class="source-inline">.fit()</strong> method of the <span class="No-Break"><strong class="source-inline">Lasso</strong></span><span class="No-Break"> class:</span><pre class="source-code">
# Fit the Lasso model lasso = Lasso(alpha=0.02)</pre><pre class="source-code">
lasso.fit(X_train, y_train)</pre></li>
<li>Evaluate the lasso model on the training and test datasets, using the R2-score, implemented in the <strong class="source-inline">.score()</strong> method of the <span class="No-Break"><strong class="source-inline">Lasso</strong></span><span class="No-Break"> class:</span><pre class="source-code">
# Print the R2-score on train and test</pre><pre class="source-code">
print('R2-score on train set:', lasso.score(X_train, y_train))</pre><pre class="source-code">
print('R2-score on test set:', lasso.score(X_test, y_test))</pre></li>
</ol>
<p>This code outputs <span class="No-Break">the following:</span></p>
<pre class="source-code">
R2-score on train set: 0.5949103710772492
R2-score on test set: 0.57350350155955</pre>
<p>Here we see an<a id="_idIndexMarker125"/> improvement when compared to the linear regression with no penalization, and we have improved against the ridge regression too, having an R2-score of about <strong class="source-inline">0.57</strong> on the <span class="No-Break">test set.</span></p>
<h2 id="_idParaDest-84"><a id="_idTextAnchor083"/>There’s more…</h2>
<p>If we plot again for the weights instances, we now have the <span class="No-Break">following values:</span></p>
<pre class="source-code">
plt.bar(np.arange(len(lasso.coef_)), lasso.coef_)
plt.xlabel('feature index')
plt.ylabel('weight value')
plt.show()</pre>
<p>This outputs the plot in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.14</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<img alt="Figure 3.7 – Learned values of each weight of the lasso model. Note that, unlike the ridge model, several weights are set to zero" height="524" src="image/B19629_03_07.jpg" width="704"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.7 – Learned values of each weight of the lasso model. Note that, unlike the ridge model, several weights are set to zero</p>
<p>As expected, some <a id="_idIndexMarker126"/>values are set to 0, with an overall range of <strong class="source-inline">-0.5</strong> <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">0.7</strong></span><span class="No-Break">.</span></p>
<p>Lasso regularization allowed us to significantly improve performance on the test set in this case, while also <span class="No-Break">reducing overfitting.</span></p>
<h2 id="_idParaDest-85"><a id="_idTextAnchor084"/>See also</h2>
<p>Take a look at the official documentation for more information on the lasso <span class="No-Break">class: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.xhtml</span></a><span class="No-Break">.</span></p>
<p>Another technique called group lasso can be useful for regularization; find out more about it <span class="No-Break">here: </span><a href="https://group-lasso.readthedocs.io/en/latest/"><span class="No-Break">https://group-lasso.readthedocs.io/en/latest/</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-86"><a id="_idTextAnchor085"/>Regularizing with elastic net regression</h1>
<p>Elastic net<a id="_idIndexMarker127"/> regression, besides having a very fancy name, is nothing more than a combination of ridge and lasso penalization. It’s a regularization method that can be of help in some specific cases. Let’s have a look at what it means in terms of loss, and then train a model on the California <span class="No-Break">housing dataset.</span></p>
<h2 id="_idParaDest-87"><a id="_idTextAnchor086"/>Getting ready</h2>
<p>The idea with elastic net is to have both L1 and <span class="No-Break">L2 regularization.</span></p>
<p>This means that the loss <img alt="" height="24" src="image/Formula_03_019.png" width="89"/> is <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<img alt="" height="119" src="image/Formula_03_020.jpg" width="1046"/>
</div>
</div>
<p>The two hyperparameters, <img alt="" height="23" src="image/Formula_03_021.png" width="22"/> and <img alt="" height="23" src="image/Formula_03_022.png" width="23"/>, can <span class="No-Break">be fine-tuned.</span></p>
<p>We won’t go into detail on the equations for the gradient descent, since deriving them is straightforward as soon as ridge and lasso <span class="No-Break">are clear.</span></p>
<p>To train a model, we again need the <strong class="source-inline">sklearn</strong> library, which we already installed in previous recipes. Also, we again assume that the California housing dataset is already downloaded <span class="No-Break">and prepared.</span></p>
<h2 id="_idParaDest-88"><a id="_idTextAnchor087"/>How to do it…</h2>
<p>In scikit-learn, elastic net is implemented in the <strong class="source-inline">ElasticNet</strong> class. However, instead of having two hyperparameters, <img alt="" height="23" src="image/Formula_03_023.png" width="22"/> and <img alt="" height="22" src="image/Formula_03_024.png" width="22"/>, they are using two hyperparameters, <strong class="source-inline">alpha</strong> <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">l1_ratio</strong></span><span class="No-Break">:</span></p>
<ul>
<li><img alt="" height="26" src="image/Formula_03_025.png" width="166"/></li>
<li><img alt="" height="27" src="image/Formula_03_026.png" width="231"/></li>
</ul>
<p>Let’s now apply this to our already prepared California <span class="No-Break">housing dataset:</span></p>
<ol>
<li>Import the <strong class="source-inline">ElasticNet</strong> class <span class="No-Break">from scikit-learn:</span><pre class="source-code">
from sklearn.linear_model import ElasticNet</pre></li>
<li>Instantiate an elastic net model. Values of <strong class="source-inline">alpha=0.1</strong> and <strong class="source-inline">l1_ratio=0.5</strong> have been chosen, but other values can be tested. Then train the model on the training set, using the <strong class="source-inline">.fit()</strong> method of the <span class="No-Break"><strong class="source-inline">ElasticNet</strong></span><span class="No-Break"> class:</span><pre class="source-code">
# Fit the LASSO model</pre><pre class="source-code">
Elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)</pre><pre class="source-code">
elastic.fit(X_train, y_train)</pre><pre class="source-code">
ElasticNet(alpha=0.1)</pre></li>
<li>Evaluate the elastic net model on the training and test dataset, using the R2-score computed with the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">score()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
# Print the R2-score on train and test</pre><pre class="source-code">
print('R2-score on train set:', elastic.score(</pre><pre class="source-code">
    X_train, y_train))</pre><pre class="source-code">
print('R2-score on test set:', elastic.score(</pre><pre class="source-code">
    X_test, y_test))</pre></li>
</ol>
<p>Here is the output <span class="No-Break">for it:</span></p>
<pre class="source-code">
R2-score on train set: 0.539957010948829
R2-score on test set: 0.5134203748307193</pre>
<p>In this case, the <a id="_idIndexMarker128"/>results are not an improvement upon lasso regularization: perhaps a better fine-tuning of the hyperparameters is required to achieve <span class="No-Break">equivalent performance.</span></p>
<p>While more complicated to fine-tune because of having two hyperparameters, elastic net regression may offer more flexibility in regularization than the ridge or lasso regularizations. The use of hyperparameter optimization is the recommended method to find the right set of hyperparameters for any <span class="No-Break">specific task.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">In practice, elastic net regression is probably less widely used than the ridge and <span class="No-Break">lasso regressions.</span></p>
<h2 id="_idParaDest-89"><a id="_idTextAnchor088"/>See also</h2>
<p>The official documentation of scikit-learn for elastic net regression can be found <span class="No-Break">here: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-90"><a id="_idTextAnchor089"/>Training a logistic regression model</h1>
<p>Logistic regression is<a id="_idIndexMarker129"/> really close to linear regression conceptually. Once linear regression is fully understood, logistic regression is just a couple of tricks away. But unlike linear regression, logistic regression is most commonly used for <span class="No-Break">classification tasks.</span></p>
<p>Let’s first explain what logistic regression is, and then train a model on the breast cancer dataset <span class="No-Break">using scikit-learn.</span></p>
<h2 id="_idParaDest-91"><a id="_idTextAnchor090"/>Getting ready</h2>
<p>Unlike linear regression, logistic regression’s output is limited to a range of <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>. The first idea is exactly the same as linear regression, having for each feature <img alt="" height="20" src="image/Formula_03_028.png" width="18"/> a <span class="No-Break">parameter</span><span class="No-Break"> <img alt="" height="26" src="image/Formula_03_027.png" width="19"/></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer077">
<img alt="" height="120" src="image/Formula_03_029.jpg" width="293"/>
</div>
</div>
<p>There is one more step to limit the range to <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>, which is to apply the logistic function to this output <em class="italic">z</em>. As a reminder, the logistic function (also called the sigmoid function, although it’s a more generic function) is <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer078">
<img alt="" height="96" src="image/Formula_03_030.jpg" width="307"/>
</div>
</div>
<p>The logistic function has an S-shape, with values ranging from <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>, with a value of <strong class="source-inline">0.5</strong> on <em class="italic">x = 0</em>, as shown here in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.8</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer079">
<img alt="Figure 3.8 – Logistic function, ranging from 0 to 1 through 0.5 for x = 0" height="418" src="image/B19629_03_08.jpg" width="744"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.8 – Logistic function, ranging from 0 to 1 through 0.5 for x = 0</p>
<p>Finally, by applying the <a id="_idIndexMarker130"/>sigmoid function, we have the logistic regression prediction <em class="italic">h(x)</em> as <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer080">
<img alt="" height="101" src="image/Formula_03_031.jpg" width="635"/>
</div>
</div>
<p>This ensures an output value <em class="italic">h(x)</em> in the range of <strong class="source-inline">0</strong> to <strong class="source-inline">1</strong>. But it does not yet allow us to have a classification. The final step is to apply a threshold (for example, <strong class="source-inline">0.5</strong>) to have a <span class="No-Break">classification prediction:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer081">
<img alt="" height="99" src="image/Formula_03_032.jpg" width="653"/>
</div>
</div>
<p>As we did for linear regression, we now need to define a loss <strong class="source-inline">L</strong> that is to be minimized, in order to optimize the parameters <img alt="" height="26" src="image/Formula_03_033.png" width="21"/> and <em class="italic">b</em>. The commonly used loss<a id="_idIndexMarker131"/> is the so-called <strong class="bold">binary </strong><span class="No-Break"><strong class="bold">cross entropy</strong></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<img alt="" height="118" src="image/Formula_03_034.jpg" width="1040"/>
</div>
</div>
<p>We can see four extreme <span class="No-Break">cases here:</span></p>
<ul>
<li><em class="italic">if y = 1 and h(x)</em><em class="italic">≃</em><em class="italic">1: L  </em><em class="italic">≃</em><em class="italic"> 0</em></li>
<li><em class="italic">if y = 1 and h(x)</em><em class="italic">≃</em><em class="italic">0: L  </em><em class="italic">≃</em><em class="italic"> +∞</em></li>
<li><em class="italic">if y = 0 and h(x)</em><em class="italic">≃</em><em class="italic">0: L  </em><em class="italic">≃</em><em class="italic"> 0</em></li>
<li><em class="italic">if y = 0 and h(x)</em><em class="italic">≃</em><em class="italic">1: L </em><em class="italic">≃</em><em class="italic"> +∞</em></li>
</ul>
<p>So now for us to have the expected behavior, which is, a loss that tends to 0 indicating a highly<a id="_idIndexMarker132"/> accurate prediction, and that increases for a wrong prediction. This is represented in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<img alt="Figure 3.9 – Binary cross entropy, minimizing the error for both y = 0 and y = 1" height="446" src="image/B19629_03_09.jpg" width="548"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.9 – Binary cross entropy, minimizing the error for both y = 0 and y = 1</p>
<p>Again, one way to optimize logistic regression is through gradient descent, the exact same way as for linear regression. As a matter of fact, the equations are exactly the same, even if we can’t prove <span class="No-Break">it here.</span></p>
<p>To get ready for this recipe, all we need is to have the <strong class="source-inline">scikit-learn</strong> <span class="No-Break">library installed.</span></p>
<h2 id="_idParaDest-92"><a id="_idTextAnchor091"/>How to do it…</h2>
<p>Logistic regression is fully implemented in scikit-learn as the <strong class="source-inline">LogisticRegression</strong> class. Unlike linear regression in scikit-learn, logistic regression has regularization implemented directly into one single class. The following parameters are the ones to tweak in order to <span class="No-Break">fine-tune regularization:</span></p>
<ul>
<li><strong class="source-inline">penalty</strong>: This can be either <strong class="source-inline">'l1'</strong>, <strong class="source-inline">'l2'</strong>, <strong class="source-inline">'elasticnet'</strong>, <span class="No-Break">or </span><span class="No-Break"><strong class="source-inline">'none'</strong></span></li>
<li><strong class="source-inline">C</strong>: This is a float, inverse to regularization strength; the smaller the value, the greater <span class="No-Break">the regularization</span></li>
</ul>
<p>In this recipe, we will apply logistic regression with no regularization to the breast cancer dataset <a id="_idIndexMarker133"/>provided by scikit-learn. We will first load and prepare the data, and then train and evaluate the logistic regression model, <span class="No-Break">as follows:</span></p>
<ol>
<li>Import the <strong class="source-inline">load_breast_cancer</strong> function, which will allow us to load the dataset, and the <span class="No-Break"><strong class="source-inline">LogisticRegression</strong></span><span class="No-Break"> class:</span><pre class="source-code">
from sklearn.datasets import load_breast_cancer</pre><pre class="source-code">
from sklearn.linear_model import LogisticRegression</pre></li>
<li>Load the dataset using the <span class="No-Break"><strong class="source-inline">load_breast_cancer</strong></span><span class="No-Break"> function:</span><pre class="source-code">
# Load the dataset</pre><pre class="source-code">
X, y = load_breast_cancer(return_X_y=True)</pre></li>
<li>Split the dataset using the <strong class="source-inline">train_test_split</strong> function. We assume it has been imported in a previous recipe. Otherwise, you will need to import it with <strong class="source-inline">from sklearn.model_selection import train_test_split</strong>. We choose <strong class="source-inline">test_size=0.2</strong> so that we have a training size of 80% and a test size <span class="No-Break">of 20%:</span><pre class="source-code">
# Split the data</pre><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    X, y, test_size=0.2, random_state=42)</pre></li>
<li>We then prepare the data. Since the dataset is only composed of quantitative data, we just apply rescaling using the standard scaler: we instantiate it, fit it on the training set, and transform this same training set with <strong class="source-inline">fit_transform()</strong>. Finally, we rescale the test set with <strong class="source-inline">.transform()</strong>. Again, we assume the standard scaler has been imported from a previous recipe, otherwise, we need to import it with <strong class="source-inline">from sklearn.preprocessing </strong><span class="No-Break"><strong class="source-inline">import StandardScaler</strong></span><span class="No-Break">:</span><pre class="source-code">
# Rescale the data</pre><pre class="source-code">
scaler = StandardScaler()</pre><pre class="source-code">
X_train = scaler.fit_transform(X_train)</pre><pre class="source-code">
X_test = scaler.transform(X_test)</pre></li>
<li>Then we <a id="_idIndexMarker134"/>instantiate the logistic regression, and we specify <strong class="source-inline">penalty='none'</strong> here so that we don’t use any penalization of the loss for pedagogical reasons. Check out the next recipe to see how penalization works. Then we train the logistic regression model on the training set with the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">fit()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
# Fit the logistic regression model with no regularization</pre><pre class="source-code">
Lr = LogisticRegression(penalty='none')</pre><pre class="source-code">
lr.fit(X_train, y_train)</pre><pre class="source-code">
LogisticRegression(penalty='none')</pre></li>
<li>Evaluate the model on both the training and the test set. The <strong class="source-inline">.score()</strong> method of the <strong class="source-inline">LogisticRegression</strong> class uses the accuracy metric, but any other metric can <span class="No-Break">be used:</span><pre class="source-code">
# Print the accuracy score on train and test</pre><pre class="source-code">
print('Accuracy on train set:', lr.score(X_train, y_train))</pre><pre class="source-code">
print('Accuracy on test set:', lr.score(X_test, y_test))</pre></li>
</ol>
<p> This code outputs <span class="No-Break">the following:</span></p>
<pre class="source-code">
Accuracy on train set: 1.0 Accuracy on test set: 0.9385964912280702</pre>
<p>We are facing rather strong overfitting here, with classification accuracy of 100% on the training set but<a id="_idIndexMarker135"/> only about 94% on the test set. This is a good start, but in the next recipe, we will use regularization to help improve the <span class="No-Break">test accuracy.</span></p>
<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/>Regularizing a logistic regression model</h1>
<p>Logistic regression<a id="_idIndexMarker136"/> uses the same trick as linear regression to add regularization: it adds penalization to the loss. In this recipe, we will first briefly explain how penalization affects the loss, and how to add regularization using scikit-learn on the breast cancer dataset that we prepared in the <span class="No-Break">previous recipe.</span></p>
<h2 id="_idParaDest-94"><a id="_idTextAnchor093"/>Getting ready</h2>
<p>Just like linear regression, it is very easy to add a regularization term to the loss <strong class="source-inline">L</strong>, either an L1- or L2-norm of the parameters <em class="italic">w</em>. For example, the loss with an L2-norm would be <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<img alt="" height="120" src="image/Formula_03_035.jpg" width="1217"/>
</div>
</div>
<p>As we did for ridge regression, we’ve added a squared sum of the weights, with a hyperparameter in front of it. To keep as close as possible to the scikit-learn implementation, we will use 1/C instead of 𝜆 for the regularization hyperparameter, but the idea remains <span class="No-Break">the same.</span></p>
<p>In this recipe, we assume the following libraries are already installed from previous recipes: <strong class="source-inline">sklearn</strong> and <strong class="source-inline">matplotlib</strong>. Also, we assume the data from the breast cancer dataset is already loaded and prepared from the previous recipe, so that we can directly <span class="No-Break">reuse it.</span></p>
<h2 id="_idParaDest-95"><a id="_idTextAnchor094"/>How to do it…</h2>
<p>Let’s now try to improve the test accuracy we had on the previous recipe by adding <span class="No-Break">L2 regularization:</span></p>
<ol>
<li>Instantiate the logistic regression model. Here, we choose an L2 penalization and a regularization value of <strong class="source-inline">C=0.1</strong>. A lower value of <strong class="source-inline">C</strong> means <span class="No-Break">greater regularization:</span><pre class="source-code">
lr = LogisticRegression(penalty='l2', C=0.1)</pre></li>
<li>Fit the logistic regression model on the training set, with the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">fit()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
lr.fit(X_train, y_train)</pre><pre class="source-code">
LogisticRegression(C=0.1)</pre></li>
<li>Evaluate the model on<a id="_idIndexMarker137"/> both training and test set. We use the <strong class="source-inline">.score()</strong> method here, providing the <span class="No-Break">accuracy score:</span><pre class="source-code">
# Print the accuracy score on train and test</pre><pre class="source-code">
print('Accuracy on train set:', lr.score(</pre><pre class="source-code">
    X_train, y_train))</pre><pre class="source-code">
print('Accuracy on test set:', lr.score(</pre><pre class="source-code">
    X_test, y_test))</pre><pre class="source-code">
Accuracy on train set: 0.9802197802197802</pre><pre class="source-code">
Accuracy on test set: 0.9824561403508771</pre></li>
</ol>
<p>As we can see here, adding L2 regularization allowed us to climb up to 98% accuracy on the test set, which is quite an improvement from about 94% <span class="No-Break">without regularization.</span></p>
<p class="callout-heading">Another reminder</p>
<p class="callout">The best way to find the right regularization value for <strong class="source-inline">C</strong> is with <span class="No-Break">hyperparameter optimization.</span></p>
<p>Out of curiosity, we can plot the train and test accuracy here for several values of <span class="No-Break">regularization strength:</span></p>
<pre class="source-code">
accuracy_train = []
accuracy_test = []
c_values = [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30]
for c in c_values:
    lr = LogisticRegression(penalty='l2', C=c)
    lr.fit(X_train, y_train)
    accuracy_train.append(lr.score(X_train, y_train))
    accuracy_test.append(lr.score(X_test, y_test))
plt.plot(c_values, accuracy_train, label='train')
plt.plot(c_values, accuracy_test, label='test')
plt.legend()
plt.xlabel('C: inverse of regularization strength')
plt.ylabel('Accuracy')
plt.xscale('log')
plt.show()</pre>
<p>Here is the graph <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<img alt="Figure 3.10 – Accuracy as a function of the C parameter, for both the training and test sets" height="416" src="image/B19629_03_10.jpg" width="552"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.10 – Accuracy as a function of the C parameter, for both the training and test sets</p>
<p>This plot can <a id="_idIndexMarker138"/>actually be read from right to left. We can see that as the value of <strong class="source-inline">C</strong> decreases (thus increasing regularization), the train accuracy keeps on decreasing, as the regularization gets higher and higher. On the other hand, decreasing <strong class="source-inline">C</strong> (thus increasing regularization) first allows us to improve the test results: the model is generalizing more and more. But at some point, adding regularization (decreasing <strong class="source-inline">C</strong> more) does not help any further, and even hurts the test accuracy. Indeed, adding<a id="_idIndexMarker139"/> too much regularization creates a <span class="No-Break">high-bias model.</span></p>
<h2 id="_idParaDest-96"><a id="_idTextAnchor095"/>There’s more…</h2>
<p>The <strong class="source-inline">LogisticRegression</strong> implementation of scikit-learn allows us to not only use L2 penalization but also L1 and elastic net, just like linear regression, which allows us to have some flexibility on the best regularization for any given dataset <span class="No-Break">and task.</span></p>
<p>The official documentation can be checked for more <span class="No-Break">details: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-97"><a id="_idTextAnchor096"/>Choosing the right regularization</h1>
<p>Linear models <a id="_idIndexMarker140"/>share this regularization method with L1 and L2 penalization. The only difference in the implementation is the fact that linear regression has its own class for each regularization type, as <span class="No-Break">mentioned here:</span></p>
<ul>
<li><strong class="source-inline">LinearRegression</strong> for <span class="No-Break">no regularization</span></li>
<li><strong class="source-inline">RidgeRegression</strong> for <span class="No-Break">L2 regularization</span></li>
<li><strong class="source-inline">Lasso</strong> for <span class="No-Break">L1 regularization</span></li>
<li><strong class="source-inline">ElasticNet</strong> for both L1 <span class="No-Break">and L2</span></li>
</ul>
<p>Logistic regression has an integrated implementation, passing <strong class="source-inline">L1</strong> or <strong class="source-inline">L2</strong> as the <span class="No-Break">class parameter.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">With <strong class="bold">Support Vector Machines</strong> (<strong class="bold">SVMs</strong>), the <a id="_idIndexMarker141"/>scikit-learn’s implementation provides a <strong class="source-inline">C</strong> parameter for <strong class="source-inline">L2</strong> regularization for both the <strong class="source-inline">SVC</strong> classification class and the <strong class="source-inline">SVR</strong> <span class="No-Break">regression class.</span></p>
<p>But for linear regression as well as logistic regression, one question remains: should we use L1 or <span class="No-Break">L2 regularization?</span></p>
<p>In this recipe, we will provide some practical tips about whether to use L1 or L2 penalization in some cases and then we will perform a grid search on the breast cancer dataset using logistic regression to find the <span class="No-Break">best regularization.</span></p>
<h2 id="_idParaDest-98"><a id="_idTextAnchor097"/>Getting ready</h2>
<p>There is no absolute answer to what penalization is best. Most of the time, the only way to find out is by doing hyperparameter optimization. But in some cases, data itself or external constraints<a id="_idIndexMarker142"/> may give hints about which regularization to use. Let’s have a quick look. First, let’s compare L1 and L2 regularization, then let’s explore <span class="No-Break">hyperparameter optimization.</span></p>
<h3>L1 versus L2 regularization</h3>
<p>L1 and L2 regularization<a id="_idIndexMarker143"/> have<a id="_idIndexMarker144"/> intrinsic differences that can sometimes help us make an educated guess upfront, and then save computational time. Let’s have a look at <span class="No-Break">these differences.</span></p>
<p>As discussed, L1 regularization tends to set some weights to zero, and thus may allow feature selection. Thus, if we have a dataset with many features, it can be helpful to have this information, and then just remove those features in <span class="No-Break">the future.</span></p>
<p>Also, L1 regularization uses the absolute value of weights in the loss, making it more robust to outliers compared to L2 regularization. If our dataset could contain outliers, it is to <span class="No-Break">be considered.</span></p>
<p>Finally, in terms of computing speed, L2 regularization is adding a quadratic term and is as a consequence less computationally expensive than L1 regularization. If training speed is of concern, L2 regularization should be considered <span class="No-Break">before L1.</span></p>
<p>In a nutshell, we can consider the following cases to be ones where the choice can be made up front based on data or other constraints such as <span class="No-Break">computation resources:</span></p>
<ul>
<li>The data contains numerous features, many of lesser importance: <span class="No-Break">L1 regularization</span></li>
<li>The data contains many outliers: <span class="No-Break">L1 regularization</span></li>
<li>Training computation resources are of concern: <span class="No-Break">L2 regularization</span></li>
</ul>
<h3>Hyperparameter optimization</h3>
<p>A more practical<a id="_idIndexMarker145"/> and perhaps pragmatic approach is just to do hyperparameter optimization, with L1 or L2 being a hyperparameter (elastic net regression could be <span class="No-Break">added too).</span></p>
<p>We will use hyperparameter optimization with grid search as implemented by scikit-learn, optimizing a logistic regression model with both L1 and L2 regularization on the breast <span class="No-Break">cancer dataset.</span></p>
<p>For this recipe, we expect the <strong class="source-inline">scikit-learn</strong> library to be installed from a previous recipe. We also assume that the breast cancer data is already loaded and prepared from the <em class="italic">Training a logistic regression </em><span class="No-Break"><em class="italic">model</em></span><span class="No-Break"> recipe.</span></p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor098"/>How to do it…</h2>
<p>We will perform a<a id="_idIndexMarker146"/> grid search on a given set of hyperparameters, more specifically, L1 and L2 penalization with several values of <span class="No-Break">penalization, </span><span class="No-Break"><strong class="source-inline">C</strong></span><span class="No-Break">:</span></p>
<ol>
<li>First, we need to import the <strong class="source-inline">GridSearchCV</strong> class <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">sklearn</strong></span><span class="No-Break">:</span><pre class="source-code">
from sklearn.model_selection import GridSearchCV</pre></li>
<li>Then we define the parameter grid. This is the space of hyperparameters that we want to test. Here, we will try both L1 and L2 penalization, and for each penalization, we will try the <strong class="source-inline">C</strong> values in <strong class="source-inline">[0.01, 0.03, 0.06, 0.1, 0.3, 0.6]</strong> <span class="No-Break">as follows:</span><pre class="source-code">
# Define the hyperparameters we want to test param_grid = {</pre><pre class="source-code">
    'penalty': ['l1', 'l2'],</pre><pre class="source-code">
    'C': [0.01, 0.03, 0.06, 0.1, 0.3, 0.6] }</pre></li>
<li>Next, we instantiate the grid search object. Several parameters are <span class="No-Break">passed here:</span><ul><li><strong class="bold">The model to optimize</strong>: <strong class="source-inline">LogisticRegression</strong>, for which we specify the solver to be <strong class="source-inline">liblinear</strong> (check the <em class="italic">See also</em> section for more information) so that it can handle both L1 and L2 penalization. We assume the class is already imported from a previous recipe; otherwise, you can import it with <strong class="source-inline">from</strong> <strong class="source-inline">sklearn.linear_model</strong> <span class="No-Break"><strong class="source-inline">import</strong></span><span class="No-Break"> </span><span class="No-Break"><strong class="source-inline">LogisticRegression</strong></span><span class="No-Break">.</span></li><li><strong class="bold">The parameter grid</strong>: A dictionary with hyperparameters to explore as keys and the associated list of values to explore as dictionary values. We defined it in the previous step, containing all the values we want <span class="No-Break">to test.</span></li><li><strong class="bold">The scoring to optimize</strong>: We choose here <strong class="source-inline">scoring='accuracy'</strong>, but it can be any other <span class="No-Break">relevant metric.</span></li><li><strong class="bold">The number of cross-validation folds</strong>: We choose <strong class="source-inline">5</strong> cross-validation folds here with <strong class="source-inline">cv=5</strong>, as it is pretty standard, but depending on the size of the dataset, other values may be just <span class="No-Break">fine too:</span><pre class="source-code">
# Instantiate the grid search</pre><pre class="source-code">
object grid = GridSearchCV(</pre><pre class="source-code">
    LogisticRegression(solver='liblinear'),</pre><pre class="source-code">
    param_grid,</pre><pre class="source-code">
    scoring='accuracy',</pre><pre class="source-code">
    cv=5 )</pre></li></ul></li>
<li>Train the grid on <a id="_idIndexMarker147"/>the training data with the <strong class="source-inline">.fit()</strong> method. Then we can display the best hyperparameters found out of curiosity, using the <span class="No-Break"><strong class="source-inline">best_params_</strong></span><span class="No-Break"> attribute:</span><pre class="source-code">
# Fit and wait</pre><pre class="source-code">
grid.fit(X_train, y_train)</pre><pre class="source-code">
# Print the best set of hyperparameters</pre><pre class="source-code">
print('best hyperparameters:', grid.best_params_)</pre><pre class="source-code">
best hyperparameters: {'C': 0.06, 'penalty': 'l2'}</pre></li>
</ol>
<p>In this case, the hyperparameters appear to be <strong class="source-inline">C=0.06</strong> with L2 penalization. We can now evaluate <span class="No-Break">the model.</span></p>
<ol>
<li value="5">Evaluate the model on both the training and the test sets, using the <strong class="source-inline">.score()</strong> method that computes accuracy. Using <strong class="source-inline">.score()</strong> or <strong class="source-inline">.predict()</strong> directly on the grid object will automatically compute the best <span class="No-Break">model predictions:</span><pre class="source-code">
# Print the accuracy score on train and test</pre><pre class="source-code">
print('Accuracy on train set:', grid.score(</pre><pre class="source-code">
    X_train, y_train))</pre><pre class="source-code">
print('Accuracy on test set:', grid.score(</pre><pre class="source-code">
    X_test, y_test))</pre><pre class="source-code">
Accuracy on train set: 0.9824175824175824</pre><pre class="source-code">
Accuracy on test set: 0.9912280701754386</pre></li>
</ol>
<p>In this case, this improved the performance on the test set, although it’s not always that easy. But the method<a id="_idIndexMarker148"/> remains the same and can be applied to <span class="No-Break">any dataset.</span></p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor099"/>See also</h2>
<p>The grid search official documentation can be found <span class="No-Break">here: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.xhtml</span></a><span class="No-Break">.</span></p>
<p>More information about the solvers and penalizations available in scikit-learn can be found <span class="No-Break">here: </span><a href="https://scikit-learn.org/stable/modules/linear_model.xhtml#logistic-regression"><span class="No-Break">https://scikit-learn.org/stable/modules/linear_model.xhtml#logistic-regression</span></a><span class="No-Break">.</span></p>
</div>
</div></body></html>