<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Learning from Data
                </header>
            
            <article>
                
<p class="mce-root">Data preparation takes a great deal of time for complex datasets, as we saw in the previous chapter. However, time spent on data preparation is time well invested... this I can guarantee! In the same way, investing time in understanding the basic theory of learning from data is super important for any person that wants to join the field of deep learning. Understanding the fundamentals of learning theory will pay off whenever you read new algorithms or evaluate your own models. It will also make your life much easier when you get to the later chapters in this book. </p>
<p class="mce-root">More specifically, this chapter introduces the most elementary concepts around the theory of deep learning, including measuring performance on regression and classification as well as the identification of overfitting. It also offers some warnings about the sensibility of—and the need to optimize—model hyperparameters.</p>
<p class="mce-root">The outline of this chapter is as follows:</p>
<ul>
<li class="mce-root">Learning for a purpose</li>
<li class="mce-root">Measuring success and error</li>
<li class="mce-root">Identifying overfitting and generalization</li>
<li class="mce-root">The art behind learning</li>
<li class="mce-root">Ethical implications of training deep learning algorithms</li>
</ul>
<h1 id="uuid-f4672afa-5ffd-4b3a-8a98-3fbbfb2a28dc" class="mce-root">Learning for a purpose</h1>
<p>In <a href="https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=26&amp;action=edit">Chapter 3</a>, <em>Preparing Data</em>, we discussed how to prepare data for two major types of problems: <strong>regression</strong> and <strong>classification</strong>. In this section, we will cover the technical differences between classification and regression <span>in more detail</span><span>. These differences are important because they will limit the type of machine learning algorithms you can use to solve your problem.</span></p>
<h2 id="uuid-15d31fed-fbdb-48de-8cc3-a5aaa9613f68">Classification</h2>
<p>How do you know whether your problem is classification? The answer depends on two major factors: the <strong>problem</strong> you are trying to solve and the <strong>data</strong> you have to solve your problem. There might be other factors, for sure, but these two are by far the most significant.</p>
<p>If your purpose is to make a model that, given some input, will determine whether the response or output of the model is to distinguish between two or more distinct categories, then you have a<span> </span><span>classification</span><span> problem. Here is a non-exhaustive list of examples of classification problems:</span></p>
<ul>
<li>Given an image, indicate what number it contains (distinguish between 10 categories: 0-9 digits).</li>
<li><span>Given an image, indicate whether it contains a cat or not (distinguish between two categories: yes or no).</span>  </li>
<li>Given a sequence of readings about temperature, determine the season <span>(distinguish between four categories: the four seasons).</span></li>
<li>Given the text of a tweet, determine the sentiment <span>(distinguish between two categories: positive or negative).</span></li>
<li>Given an image of a person, determine the age group <span>(distinguish between five categories: &lt;18, 18-25, 26-35, 35-50, &gt;50).</span></li>
<li>Given an image of a dog, determine its breed (distinguish between 120 categories: those breeds that are internationally recognized).</li>
<li>Given an entire document, determine whether it has been tampered with (distinguish <span>between categories: authentic or altered).</span></li>
<li>Given satellite readings of a spectroradiometer, determine whether the geolocation matches the spectral signature of vegetation or not <span>(distinguish </span><span>between two categories: yes or no).</span></li>
</ul>
<p>As you can see from the examples in the list, there are different types of data for different types of problems. The data that we are seeing in these examples is known as <strong>labeled data</strong>.  </p>
<div class="packt_infobox">Unlabeled data is very common but is rarely used for classification problems without some type of processing that allows the matching of data samples to a category. For example, unsupervised clustering can be used on unlabeled data to assign the data to specific clusters (such as groups or categories); at which point, the data technically becomes "labeled data."</div>
<p>The other important thing to notice from the list is that we can categorize the classification problems into two major groups:</p>
<ul>
<li><strong>Binary classification</strong>: For classification between any two classes only</li>
<li><strong>Multi-class classification</strong>: For classification between more than just two classes</li>
</ul>
<p>This distinction may seem arbitrary but it is not; in fact, the type of classification will limit the type of learning algorithm you can use and the performance you can expect. To understand this a little better, let's discuss each classification separately.</p>
<h3 id="uuid-df92f3d9-377a-49c6-ab74-62f27962bfdd">Binary classification</h3>
<p>This type of classification is usually regarded as a much simpler problem than multiple classes. In fact, if we can solve the binary classification problem, we could, technically, solve the problem of multiple classes by deciding on a strategy to break down the problem into several binary classification problems (<em>Lorena, A. C.</em> et al., <em>2008</em>). </p>
<p>One of the reasons why this is considered a simpler problem is because of the algorithmic and mathematical foundations behind binary classification learning algorithms. Let's say that we have a binary classification problem, such as the Cleveland dataset explained in <a href="https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=26&amp;action=edit">Chapter 3</a>, <em>Preparing Data</em>. This dataset consists of 13 medical observations for each patient—we can call that <img class="fm-editor-equation" src="assets/4ae6e047-6db7-4cea-89a7-d59b4577f989.png" style="width:3.58em;height:1.17em;"/>. For each of these patient records, there is an associated label that indicates whether the patient has some type of heart disease (+1) or not (-1)—we will call that <img class="fm-editor-equation" src="assets/0f347d74-fa0b-4e7f-8523-3b84761dae43.png" style="width:6.75em;height:1.33em;"/>. So, an entire dataset, <img class="fm-editor-equation" src="assets/fbdaf353-eb1c-4455-a3f5-cbebb2f0d6e7.png" style="width:0.67em;height:0.75em;"/>, with <em>N </em>samples can be defined as a set of data and labels:</p>
<p style="padding-left: 270px"><img class="fm-editor-equation" src="assets/dff30c72-c9b3-4a6f-a20a-e3a3017c59e7.png" style="width:6.75em;height:1.50em;"/></p>
<p>Then, as discussed in <a href="https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=24&amp;action=edit">Chapter 1</a>, <em>Introduction to Machine Learning</em>, the whole point of learning is to use an algorithm that will find a way to map input data, <strong>x</strong>, to label the <em>y</em> correctly f<span>or all samples in </span><span><sub><img class="fm-editor-equation" src="assets/fbdaf353-eb1c-4455-a3f5-cbebb2f0d6e7.png" style="width:0.67em;height:0.75em;"/></sub></span><span> and to be able to further do so (hopefully) for samples outside of the known dataset, </span><img style="font-size: 1em;width:0.67em;height:0.75em;" class="fm-editor-equation" src="assets/fbdaf353-eb1c-4455-a3f5-cbebb2f0d6e7.png"/><span>. Using a perceptron and a corresponding <strong>Perceptron Learning Algorithm</strong> (<strong>PLA</strong>), what we want is to find the parameters <sub><img class="fm-editor-equation" src="assets/72310c26-8d2e-4d7b-8b0e-2694ac960fbf.png" style="width:2.58em;height:1.17em;"/></sub> that can satisfy the following:</span></p>
<p style="padding-left: 240px"><span><img class="fm-editor-equation" src="assets/a8cba13d-2fa4-48ee-907a-23d24d12ba66.png" style="width:9.75em;height:1.42em;"/></span></p>
<p>For all samples, <em>i</em> = 1, 2, ..., <em>N.</em> However, as we discussed in <a href="https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=24&amp;action=edit">Chapter 1</a>, <em>Introduction to Machine Learning</em>, the equation cannot be satisfied if the data is non-linearly separable. In that case, we can obtain an approximation, or a prediction, that is not necessarily the desired outcome; we will call such a prediction <img class="fm-editor-equation" src="assets/ca21520f-7620-4ce7-8ba6-ef31faaed1e9.png" style="width:0.67em;height:1.33em;"/>.</p>
<p>The whole point of a learning algorithm, then, becomes to reduce the differences between the desired target label, <img class="fm-editor-equation" src="assets/67f2f764-f8f9-4e62-abfd-5c9dddfe4114.png" style="width:0.50em;height:0.83em;"/>, and the prediction, <img class="fm-editor-equation" src="assets/dda3ec0e-9026-4af0-abb5-71bf7486e83c.png" style="width:0.67em;height:1.33em;"/>. In an ideal world, we want <img src="assets/46139ee6-41db-420e-a2e9-95cc864cda3a.png" style="width:3.67em;height:1.25em;"/> for all cases of <em>i</em><span> = 1, 2, ..., </span><em>N.</em> In cases of <em>i </em>where <img src="assets/7f1182f2-7155-4ec8-aca1-50a4033f4c85.png" style="width:3.42em;height:1.25em;"/>, the learning algorithm must make adjustments (that is, train itself) to avoid making such mistakes in the future by finding new parameters <img src="assets/2be5be4c-924d-462b-b31e-0dd3771de027.png" style="width:3.25em;height:1.50em;"/> that are hopefully better.</p>
<p>The science behind such algorithms varies from model to model, but the ultimate goals are usually the same:</p>
<ul>
<li>Reduce the number of errors, <img src="assets/dc0b01b2-c4f3-499b-8a4d-302e1d8fa01b.png" style="width:3.75em;height:1.25em;"/>, in every learning iteration.</li>
<li>Learn the model parameters in as few iterations (steps) as possible.</li>
<li>Learn the model parameters as fast as possible.</li>
</ul>
<p>Since most datasets deal with non-separable problems, the PLA is disregarded in favor of other algorithms that will converge faster and in fewer iterations. Many learning algorithms like this learn to adjust the parameters <img src="assets/10030282-7398-4858-8911-a2f401ae5315.png" style="width:3.25em;height:1.42em;"/> by taking specific steps to reduce the error, <img src="assets/fa1abe60-b158-4da2-b57c-73f2a10d40cf.png" style="width:4.08em;height:1.42em;"/>, based on derivatives with respect to the variability of the error and the choice of parameters. So, the most successful algorithms (in deep learning, at least) are those based on some type of gradient descent strategy (Hochreiter, S., et.al. 2001).</p>
<p>Now, let's go over the most basic iterative gradient strategy. Say that we want to learn the parameters <img class="fm-editor-equation" src="assets/293a24a9-8122-473a-b0e9-3f3c1f5f62e0.png" style="width:2.58em;height:1.17em;"/> given the dataset, <img src="assets/66b34a6d-fe6c-42c0-b839-75629dd44781.png" style="width:0.83em;height:0.83em;"/>. We will have to make a small adjustment to the problem formulation to make things a little easier. What we want is for <img class="fm-editor-equation" src="assets/4e82d5bd-8068-4b93-8586-84592a77fc05.png" style="width:4.58em;height:1.33em;"/> to be implied in the expression <img src="assets/4e4b289f-920a-4351-9622-14e68b8cacce.png" style="width:2.58em;height:1.25em;"/>. The only way this could work is if we set <img class="fm-editor-equation" src="assets/f1ff6cd9-db78-495b-b8ed-23cf890c12cb.png" style="width:12.75em;height:1.58em;"/>and <img src="assets/357ebb91-cfb5-4c22-b82d-49ddbb38b5f2.png" style="width:12.58em;height:1.67em;"/>.</p>
<p>With this simplification, we can simply search for <strong>w</strong>, which implies a search for <strong>b</strong> as well. Gradient descent with a fixed <em>learning rate</em> is as follows:</p>
<ol start="1">
<li>Initialize the weights to zero ( <img src="assets/1f4e3960-c78b-45c9-90e2-3921c8452c3a.png" style="width:2.92em;height:0.83em;"/>) and the iteration counter to zero (<img class="fm-editor-equation" src="assets/6ce01007-2b63-4bc4-8db8-4f1d0f7c33a1.png" style="width:2.75em;height:1.00em;"/>).</li>
<li>When <img src="assets/429a3278-fffb-4640-8730-c1c103a64135.png" style="width:4.58em;height:1.25em;"/>, do the following:</li>
</ol>
<ol>
<li style="padding-left: 60px">Calculate the gradient with respect to <img src="assets/23cda490-47ff-4f78-b5fc-195f6d6b2d0a.png" style="width:1.42em;height:0.92em;"/> and store it in <img src="assets/99c1192e-4889-40ad-9e1b-34a8ad598d5f.png" style="width:4.83em;height:0.92em;"/>.</li>
<li style="padding-left: 60px">Update <img src="assets/8074e322-e8f8-4e22-828c-06357c3902b8.png" style="width:1.17em;height:0.83em;"/> so that it looks like this: <img src="assets/1dfaa02c-ed8e-4bd1-aa7a-4ebd9003ccc8.png" style="width:7.58em;height:0.83em;"/>.</li>
<li style="padding-left: 60px">Increase the iteration counter and repeat.</li>
</ol>
<p> </p>
<p>There are a couple of things that need to be explained here:</p>
<ul>
<li>The gradient calculation, <img class="fm-editor-equation" src="assets/d6ee9c14-1d4a-4ce7-92eb-66e6f7639c98.png" style="width:3.92em;height:1.25em;"/>, is not trivial. For some specific machine learning models, it can be determined analytically; but in most cases, it must be determined numerically by using some of the latest algorithms.</li>
<li>We still need to define how the error,<img class="fm-editor-equation" src="assets/497c3550-a1d6-4dae-bbf2-c38fce7e4fe8.png" style="width:2.83em;height:1.17em;"/>, is calculated; but this will be covered in the next section of this chapter.</li>
<li>A learning rate, <img src="assets/1553a1e5-b84e-433e-8a44-39ec9c0a72ae.png" style="width:0.67em;height:1.00em;"/>, needs to be specified as well, which is a problem in itself. </li>
</ul>
<p>One way of looking at this last issue is that in order to find the parameter, <img src="assets/de1605e4-2c5a-4802-919c-f70cf201ca77.png" style="width:1.17em;height:0.83em;"/>, that minimizes the error, we need parameter <img src="assets/68663c10-bdf9-4cd2-8f73-e5c381771b62.png" style="width:0.67em;height:1.00em;"/>. Now, we could, when applying gradient descent, think about finding the <img src="assets/10ee9ff7-c459-40a1-ae7e-e9f5d4fdfbe5.png" style="width:0.67em;height:1.00em;"/><span> </span><span>parameter,</span><span> but we will then fall into an infinite cycle. We will not go into more detail about gradient descent and its learning rate since, nowadays, algorithms for gradient descent often include automatic calculations of it or adaptive ways of adjusting it (</span>Ruder, S. 2016<span>). </span></p>
<h3 id="uuid-b420744f-812e-454f-8eff-49a254c021b4">Multi-class classification</h3>
<p>Classifying into multiple categories can have an important effect on the performance of learning algorithms. In a general sense, the performance of a model will decrease with the number of classes it is required to recognize. The exception is if you have plenty of data and access to lots of computing power because if you do, you can overcome the limitations of poor datasets that have class imbalance problems and you can estimate massive gradients and make large calculations and updates to the model. Computing power may not be a limitation in the future, but at the moment it is. </p>
<p>The multiple classes problem can be solved by using strategies such as <strong>one versus one</strong> or <strong>one versus all</strong>.</p>
<p>In one versus all, you essentially have an expert binary classifier that is really good at recognizing one pattern from all the others and the implementation strategy is typically cascaded. An example is shown here:</p>
<pre class="mce-root">if classifierSummer says is Summer: you are done<br/>else:<br/> if classifierFall says is Fall: you are done<br/> else:<br/>   if classifierWinter says is Winter: you are done<br/>   else:<br/>     it must be Spring and, thus, you are done</pre>
<p class="mce-root">Here is a graphical explanation of this strategy. Suppose we have two-dimensional data that tells us something about the four seasons of the year, as shown:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/6ea29b49-ff32-4be0-a0cc-40d19d1896f6.png" style="width:21.83em;height:15.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.1 - Randomized two-dimensional data that could tell us something about the four seasons of the year</div>
<p>In this case of randomized two-dimensional data, we have four categories corresponding to the seasons of the year. Binary classification will not work directly. However, we could train expert binary classifiers that specialize in <em>one</em> specific category <em>versus all</em> the rest. If we train one binary classifier to determine whether data points belong to the <span class="packt_screen">Summer</span> category, using a simple perceptron, we could get the separating hyperplane shown here:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/ac4f8026-284f-43eb-89d9-1c25844ee256.png" style="width:22.00em;height:15.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.2: A PLA that is an expert in distinguishing from the Summer season data versus all the rest of the other seasons</div>
<p>Similarly, we can train the rest of the experts until we have enough to test our entire hypothesis; that is, until we are able to distinguish all the classes from each other. </p>
<p>Another alternative is to use classifiers that can handle multiple outputs; for example, decision trees or ensemble methods. But in the case of deep learning and neural networks, this refers to networks that can have multiple neurons in the output layer, such as the one depicted in <em>Figure 1.6</em> and <em>Figure 1.9</em> in <a href="https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=24&amp;action=edit">Chapter 1</a>, <em>Introduction to Machine Learning</em>. </p>
<p>The mathematical formulation of a multi-output neural network only changes slightly from a single-output one in that the output is no longer within a binary set of values, such as <img class="fm-editor-equation" src="assets/0f347d74-fa0b-4e7f-8523-3b84761dae43.png" style="width:5.92em;height:1.17em;"/>, but is now a vector of one-hot encoded values, such as <img class="fm-editor-equation" src="assets/3476b823-1083-4257-a7e0-71949ade683a.png" style="width:3.50em;height:1.25em;"/>. In this case, |<em>C</em>| denotes the size of the set <em>C</em>, which contains all the different class labels. For the previous example, <em>C </em>would contain the following: <em>C</em> = {'<em>Summer</em>', '<em>Fall</em>', '<em>Winter</em>', '<em>Spring</em>'}. Here is what each one-hot encoding would look like:</p>
<ul>
<li><strong>Summer</strong>: <img class="fm-editor-equation" src="assets/383c2835-581b-49ae-b2d7-745c43d5ed09.png" style="width:7.08em;height:1.42em;"/></li>
<li><strong>Fall</strong>: <img class="fm-editor-equation" src="assets/08a54e98-f381-4fd1-8834-8ae7c8c6a885.png" style="width:7.50em;height:1.50em;"/></li>
<li><strong>Winter</strong>: <img class="fm-editor-equation" src="assets/1688ddb7-c3c5-48eb-9514-54e26474bb93.png" style="width:7.08em;height:1.42em;"/></li>
<li><strong>Spring</strong>: <img class="fm-editor-equation" src="assets/6f45ab24-f3c6-45f1-b841-636832c00cf0.png" style="width:7.92em;height:1.58em;"/></li>
</ul>
<p>Every element in the target vector will correspond to the desired output of the four neurons. We should also point out that the dataset definition should <span>now</span><span> </span><span>reflect that both the sample input data and the labels are vectors:</span></p>
<p style="padding-left: 240px"><img class="fm-editor-equation" src="assets/556fdbd7-1f5b-4934-8441-d7a4d5a260d4.png" style="width:6.58em;height:1.42em;"/></p>
<p>Another way of dealing with the problem of multiple-class classification is by using <strong>regression</strong>. </p>
<h2 id="uuid-2bc76e09-f6b8-420b-9790-585c0ba6164d">Regression</h2>
<p class="mce-root">Previously, we specified that for binary classification, the target variable could take on a set of binary values; for example, <img class="fm-editor-equation" src="assets/0f347d74-fa0b-4e7f-8523-3b84761dae43.png" style="width:6.33em;height:1.25em;"/>. We also said that for multiple classification, we could modify the target variable to be a vector whose size depends on the number of classes, <img class="fm-editor-equation" src="assets/a5246959-0c32-4040-aa86-b99a914f8c09.png" style="width:3.25em;height:1.17em;"/>. Well, regression problems deal with cases where the target variable is any real value, <img class="fm-editor-equation" src="assets/8dbf0dcb-9e08-48c6-888e-9e62c7d77f96.png" style="width:2.83em;height:1.25em;"/>.</p>
<p>The implications here are very interesting because with a regression model and algorithm, we could <em>technically</em> do binary classification since the set of real numbers contains any binary set of numbers:</p>
<p style="padding-left: 270px"><img class="fm-editor-equation" src="assets/aeaca13b-8358-4c7e-8bf1-e50fae3e3f72.png" style="width:9.17em;height:1.33em;"/>.</p>
<p>Further, if we change <em>C</em><span> = {'<em>Summer</em>', '<em>Fall</em>', '<em>Winter</em>', '<em>Spring</em>'} to a numerical representation instead, such as C = {0,1,2,3}, then <em>technically</em>, we would again use </span>regression due to the same property:</p>
<p style="padding-left: 270px"><img class="fm-editor-equation" src="assets/c06b709f-7343-4fe6-9e3b-712fac9177ae.png" style="width:9.00em;height:1.25em;"/>.</p>
<div class="packt_tip">Although regression models can solve classification problems, it is recommended that you use models that are specialized in classification specifically and leave the regression models only for regression tasks.</div>
<p>Even if regression models can be used for classification (Tan, X., et.al. 2012), they are ideal for when the target variable is a real number. Here is a sample list of regression problems:</p>
<ul>
<li>When given an image, indicate how many people are in it (the output can be any integer &gt;=0).</li>
<li><span>When given an image, indicate the probability of it containing a cat (the output can be any real number between 0 and 1).</span></li>
<li>When given a sequence of readings about temperature, determine what the temperature actually feels like <span>(the output can be any integer whose range depends on the units).</span></li>
<li>When given the text of a tweet, determine the probability of it being offensive <span>(the output can be any real number between 0 and 1).</span></li>
<li>When given an image of a person, determine their age (the <span>output can be any positive integer, usually less than 100).</span></li>
<li><span>When given an entire document, determine the probable compression rate (the output can be any real number between 0 and 1).</span></li>
<li>When given satellite readings of a spectroradiometer, determine the corresponding infrared value<span> </span><span>(the output can be any real number).</span></li>
<li>When given the headlines of some major newspapers, determine the price of oil (the <span>output can be any real number &gt;=0).</span></li>
</ul>
<p>As you can see from this list, there are many possibilities due to the fact that the range of real numbers encompasses all integers and all positive and negative numbers, and even if the range is too broad for specific applications, the regression model can be scaled up or down to meet the range specifications. </p>
<p>To explain the potential of regression models, let's start with a basic <strong>linear regression</strong> model and in later chapters, we will cover more complex regression models based on deep learning.</p>
<p>The linear regression model tries to solve the following problem:</p>
<p style="padding-left: 240px" class="mce-root"><img class="fm-editor-equation" src="assets/970566f0-b7ac-4868-81dc-6b18e0b0fc2b.png" style="width:7.08em;height:1.42em;"/></p>
<p class="mce-root"><span>The problem is solved for </span><em>i</em><span> = 1, 2, ..., </span><em>N.</em><span> We could, however, use the same trick as before and include the calculation of <em>b </em>in the same equation. So, we can say that we are trying to solve the following problem:</span></p>
<p style="padding-left: 240px">  <img class="fm-editor-equation" src="assets/c802c25e-40e3-46c3-92bc-22fc0113e952.png" style="width:5.25em;height:1.42em;"/></p>
<p>Once again, we are trying to learn the parameters, <img class="fm-editor-equation" src="assets/d59f4bf8-7872-4734-8050-abd4ec68ecab.png" style="width:1.08em;height:0.83em;"/>, that yield <img class="fm-editor-equation" src="assets/562075f4-c6a5-4f29-a6e4-84f8c4f329c3.png" style="width:3.17em;height:1.17em;"/> for all cases of <em>i.</em> In the case of linear regression, the prediction, <img class="fm-editor-equation" src="assets/ca21520f-7620-4ce7-8ba6-ef31faaed1e9.png" style="width:0.75em;height:1.50em;"/>, should ideally <span>be</span><span> </span><span>equal to the true target value, </span><img style="font-size: 1em;width:0.75em;height:1.25em;" class="fm-editor-equation" src="assets/2a654437-6c4b-4da0-a2ad-8d800b7847ac.png"/><span>, if the input data, </span><img style="font-size: 1em;width:1.08em;height:0.83em;" class="fm-editor-equation" src="assets/5a5aa15c-23cf-45b3-801f-b4d8ca5449b8.png"/><span>, somehow describes a perfect straight line. But because this is very unlikely, there has to be a way of learning the parameters, </span><img style="font-size: 1em;width:1.17em;height:0.92em;" class="fm-editor-equation" src="assets/d1d74b08-42ed-48ee-b896-c39e9f170034.png"/><span>, even if </span><img style="font-size: 1em;width:3.42em;height:1.25em;" class="fm-editor-equation" src="assets/95a975fe-111d-454d-b981-b65ab5053683.png"/><span>. To achieve this, the linear regression learning algorithm begins by describing a low penalty for small mistakes and a larger penalty for big mistakes. This does make sense, right? It is very intuitive. </span></p>
<p>A natural way of penalizing mistakes in proportion to their size is by squaring the difference between the prediction and the target. Here is an example of when the difference is small: </p>
<p style="padding-left: 180px"><img class="fm-editor-equation" src="assets/3f4c1827-d243-4724-b234-61abbeae95c8.png" style="width:21.33em;height:1.42em;"/></p>
<p>Here is an example of when the difference is large:</p>
<p style="padding-left: 180px"><img class="fm-editor-equation" src="assets/879e3b84-a279-419f-bb57-1d71b48c3f14.png" style="width:18.83em;height:1.33em;"/></p>
<p>In both of these examples, the desired target value is <kbd>1</kbd>. In the first case, the predicted value of <kbd>0.98</kbd> is very close to the target and the squared difference is <kbd>0.0004</kbd>, which is small compared to the second case. The second prediction is off by <kbd>14.8</kbd>, which yields a squared difference of <kbd>219.4</kbd>. This seems reasonable and intuitive for building up a learning algorithm; that is, one that penalizes mistakes in proportion to how big or small they are. </p>
<p>We can formally define the overall average error in function of the choice of parameters <strong>w</strong> as the averaged sum of all squared errors, which is also known as the <strong>mean squared error (MSE)</strong>:</p>
<p style="padding-left: 270px"><img class="fm-editor-equation" src="assets/6b88425f-07b8-4cf1-8c8c-5e15ad59dcc4.png" style="width:9.75em;height:2.83em;"/>.</p>
<p>If we define the prediction in terms of the current choice of <img class="fm-editor-equation" src="assets/1d7cd898-6ee9-4952-9d43-b441c2412c88.png" style="width:0.92em;height:0.75em;"/> as <img class="fm-editor-equation" src="assets/2aa011c3-c882-4e20-a5b0-99e710545353.png" style="width:4.75em;height:1.33em;"/>, then we can rewrite the error function as follows:</p>
<p style="padding-left: 270px"><img class="fm-editor-equation" src="assets/b5a94efc-3f8c-47bc-8b0e-67bbf7851798.png" style="width:12.08em;height:3.08em;"/>.</p>
<p>This can be simplified in terms of the <img class="fm-editor-equation" src="assets/3f0e57b4-ce39-452d-ad17-78002c082199.png" style="width:1.17em;height:1.42em;"/>-norm (also known as the Euclidean norm, <img class="fm-editor-equation" src="assets/da7143d1-c3d3-4a85-aef9-2dd8b3aff0b3.png" style="width:1.67em;height:1.17em;"/>) by first defining a matrix of data <img class="fm-editor-equation" src="assets/15a5e1a8-a00b-478c-bd88-29ffbcd6374e.png" style="width:0.75em;height:0.75em;"/>, whose elements are data vector <img class="fm-editor-equation" src="assets/a0383757-b717-41b7-8e94-64b4fc666f00.png" style="width:0.67em;height:0.75em;"/>, and a vector of corresponding targets, as follows:</p>
<p style="padding-left: 210px"><img class="fm-editor-equation" src="assets/bdd9cc28-d833-4f38-b123-f42e24649ef5.png" style="width:11.83em;height:5.83em;"/>.</p>
<p>The simplification of the error is then as follows:</p>
<p style="padding-left: 240px"><img class="fm-editor-equation" src="assets/b8f14b75-d5fd-42ed-8486-9a0c2d91df10.png" style="width:10.08em;height:2.25em;"/></p>
<p>This can then <span>be</span><span> </span><span>expanded into the following important equation:</span></p>
<p style="padding-left: 180px"><img class="fm-editor-equation" src="assets/56c796c0-55f6-447a-b116-0a44e5b47bcf.png" style="width:18.83em;height:2.17em;"/>.</p>
<p>This is important because it facilitates the calculation of the derivative of the error, <img class="fm-editor-equation" src="assets/53799600-8705-4a59-ba5e-55275c4b2c2c.png" style="width:2.08em;height:1.00em;"/>, which is necessary for adjusting the parameters, <img class="fm-editor-equation" src="assets/04db5ca2-e2be-4220-8c6d-e87905762461.png" style="width:0.83em;height:0.67em;"/>, in the direction of the derivative and in proportion to the error. Now, following the basic properties of linear algebra, we can say that the derivative of the error (which is called a gradient since it yields a matrix) is the following:</p>
<p style="padding-left: 210px"><img class="fm-editor-equation" src="assets/ffea4ee4-25f6-4e04-be3f-e846f88d33f2.png" style="width:12.75em;height:2.17em;"/>.</p>
<p>Because we want to find the parameters that yield the smallest error, we can set the gradient to <kbd>0</kbd> and solve for <img class="fm-editor-equation" src="assets/c50dc76b-19c9-43bb-818a-a8a9f7c6ee3d.png" style="width:1.08em;height:0.83em;"/>.<strong> </strong>By setting the gradient to <kbd>0</kbd> and ignoring constant values, we arrive at the following:</p>
<p style="padding-left: 240px"><img class="fm-editor-equation" src="assets/5eb81dc2-15b3-4822-a5ae-a533d265eb0c.png" style="width:7.50em;height:1.42em;"/></p>
<p style="padding-left: 240px"><img class="fm-editor-equation" src="assets/7e5c5343-31fc-429b-8511-bc9a1c84fd1c.png" style="width:8.58em;height:1.33em;"/>.</p>
<p>These are called <strong>normal</strong><strong> equations</strong><em> </em>(Krejn, S. G. E. 1982). Then, if we simply use the term <img class="fm-editor-equation" src="assets/aa319bc1-8e1a-4349-8dc1-72a4206a1995.png" style="width:7.92em;height:1.25em;"/>, we arrive at the definition of a <strong>pseudo-inverse</strong> (Golub, G., and Kahan, W. 1965). The beauty of this is that we do not need to calculate the gradient iteratively to choose the best parameters, <img class="fm-editor-equation" src="assets/0c2d89de-6794-45ef-9e8d-537d24518945.png" style="width:1.08em;height:0.83em;"/>. As a matter of fact, because the gradient is analytic and direct, we can calculate <img class="fm-editor-equation" src="assets/b59769c0-1de8-4212-a587-0e18479a6b73.png" style="width:0.83em;height:0.67em;"/><strong> </strong>in one shot, as explained in this linear regression algorithm:</p>
<ol>
<li>From <img class="fm-editor-equation" src="assets/f94f1b86-f92e-4f64-99e2-2b8cd4d9a7e0.png" style="width:6.42em;height:1.42em;"/>, construct the pair, <img class="fm-editor-equation" src="assets/29abe296-f275-4635-a806-55a85636b84d.png" style="width:2.42em;height:1.00em;"/>.</li>
<li>Estimate the pseudo-inverse <img class="fm-editor-equation" src="assets/aa319bc1-8e1a-4349-8dc1-72a4206a1995.png" style="width:6.83em;height:1.08em;"/>.</li>
<li>Calculate and return <img class="fm-editor-equation" src="assets/1905d8ba-cdea-4768-af8c-e0528aec4962.png" style="width:3.92em;height:1.08em;"/>.</li>
</ol>
<p class="mce-root">To show this graphically, let's say that we have a system that sends a signal that follows a linear function; however, the signal, when it is transmitted, becomes contaminated with normal noise with a <kbd>0</kbd> mean and unit variance and we are only able to observe the noisy data, as shown:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/0aaa35da-96e2-4872-9d97-880766124f97.png" style="width:28.42em;height:20.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.3 - Data readings that are contaminated with random noise</div>
<p>If, say, a hacker reads this data and runs linear regression to attempt to determine the true function that produced this data before it was contaminated, then the data hacker would obtain the solution shown here:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/cb38549f-33ef-47a1-a783-777e0249d647.png" style="width:26.83em;height:19.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.4 - A linear regression solution to the problem of finding the true function given noisy data readings</div>
<p>Clearly, as the previous figure shows, the linear regression solution is very close to the true original linear function. In this particular example, a high degree of closeness can be observed since the data was contaminated with noise that follows a pattern of <strong>white noise</strong>; however, for different types of noise, the model may not perform as well as in this example. Furthermore, most regression problems are not linear at all; in fact, the most interesting regression problems are highly non-linear. Nonetheless, the basic learning principle is the same:</p>
<ul>
<li>Reduce the number of errors,<img class="fm-editor-equation" src="assets/53799600-8705-4a59-ba5e-55275c4b2c2c.png" style="width:2.08em;height:1.00em;"/>, in every learning iteration (or directly in one shot, such as in linear regression).</li>
<li>Learn the model parameters in as few iterations (steps) as possible.</li>
<li>Learn the model parameters as fast as possible.</li>
</ul>
<p>The other major component that guides the learning process is the way the success or error is calculated with respect to a choice of parameters, <img class="fm-editor-equation" src="assets/53799600-8705-4a59-ba5e-55275c4b2c2c.png" style="width:2.42em;height:1.17em;"/>. In the case of the PLA, it simply found a mistake and adjusted with respect to it. F<span>or multiple classes, this was through a process of gradient descent over some measure of error</span> and in linear regression, this was through direct gradient calculation using the MSE. But now, let's dive deeper into other types of error measures and successes that can be quantitative and qualitative.</p>
<h1 id="uuid-6785843d-4e3d-4c21-bf9f-c7d0c257cbc4" class="mce-root">Measuring success and error</h1>
<p>There is a wide variety of performance metrics that people use in deep learning models, such as accuracy, balanced error rate, mean squared error, and many others. To keep things organized, we will divide them into three groups: for binary classification, for multiple classes, and for regression.</p>
<h2 id="uuid-172a614b-974e-48a7-8d1f-350cefb102ce">Binary classification</h2>
<p>There is one essential tool used when analyzing and measuring the success of our models. It is known as a <strong>c</strong><strong>onfusion matrix</strong>. A confusion matrix is not only helpful in<span> </span><span>visually</span><span> displaying how a model makes predictions, but we can also retrieve other interesting information from it. The following diagram shows a template of a confusion matrix:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e700456f-66b7-4e67-91cc-b5cfa91d005d.png" style="width:31.75em;height:17.42em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.5 - A confusion matrix and the performance metrics derived from it</div>
<div class="packt_tip">A confusion matrix and all the metrics derived from it are a very important way of conveying how good your models are. You should bookmark this page and come back to it whenever you need it.</div>
<p>In the preceding confusion matrix, you will notice that it has<span> </span><span>two columns</span><span> in the vertical axis that indicate the true target values, while in the horizontal axis, it indicates the predicted value. The intersection of rows and columns indicates the relationship of what should have been predicted against what was actually predicted. Every entry in the matrix has a special meaning and can lead to other meaningful composite performance metrics.</span></p>
<p>Here is the list of metrics and what they mean:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Acronym</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
<td>
<p><strong>Interpretation</strong></p>
</td>
</tr>
<tr>
<td>
<p>TP</p>
</td>
<td>
<p><em>True Positive</em></p>
</td>
<td>
<p>This is when a data point was of the positive class and was correctly predicted to be of the positive class.</p>
</td>
</tr>
<tr>
<td>
<p>TN</p>
</td>
<td>
<p><em>True Negative</em></p>
</td>
<td>
<p><span>This is when a data point was of the negative class and was correctly predicted to be of the negative class.</span></p>
</td>
</tr>
<tr>
<td>
<p>FP</p>
</td>
<td>
<p><em>False Positive</em></p>
</td>
<td>
<p><span>This is when a data point was of the negative class and was incorrectly predicted to be of the positive class.</span></p>
</td>
</tr>
<tr>
<td>
<p>FN</p>
</td>
<td>
<p><em>False Negative</em></p>
</td>
<td>
<p><span>This is when a data point was of the positive class and was incorrectly predicted to be of the negative class.</span></p>
</td>
</tr>
<tr>
<td>
<p>PPV</p>
</td>
<td>
<p><em>Positive Predictive Value</em> or <em>Precision</em></p>
</td>
<td>
<p>This is the proportion of positive values that are predicted correctly<span> out of all the values predicted to be positive.</span></p>
</td>
</tr>
<tr>
<td>
<p>NPV</p>
</td>
<td>
<p><em>Negative Predictive Value</em></p>
</td>
<td>
<p><span><span>This is the proportion of negative values that are predicted correctly</span></span><span> out of all the values that are predicted to be negative.</span></p>
</td>
</tr>
<tr>
<td>
<p>FDR</p>
</td>
<td>
<p><em>False Discovery Rate</em></p>
</td>
<td>
<p>This is the proportion of incorrect predictions as false positives out of all the values that are predicted to be positive.</p>
</td>
</tr>
<tr>
<td>
<p>FOR</p>
</td>
<td>
<p><em>False Omission Rate</em></p>
</td>
<td>
<p>This is the proportion of incorrect predictions as false negatives out of all the values that are predicted to be negative.</p>
</td>
</tr>
<tr>
<td>
<p>TPR</p>
</td>
<td>
<p><em>True Positive Rate,</em> <em>Sensitivity</em>, <em>Recall</em>, <em>Hit Rate</em></p>
</td>
<td>
<p>This is the proportion of predicted positives that are actually positives out of all that should be positives.</p>
</td>
</tr>
<tr>
<td>
<p>FPR</p>
</td>
<td>
<p><em>False Positive Rate </em>or <em>Fall-Out</em></p>
</td>
<td>
<p><span>This is the proportion of predicted positives that are actually negatives out of all that should be negatives.</span></p>
</td>
</tr>
<tr>
<td>
<p>TNR</p>
</td>
<td>
<p><em><em>True Negative Rate</em></em><span>, <em>Specificity,</em> or <em>Selectivity</em></span></p>
</td>
<td>
<p><span>This is the proportion of predicted negatives that are actually negatives out of all that should be negatives.</span></p>
</td>
</tr>
<tr>
<td>
<p>FNR</p>
</td>
<td>
<p><em><em>False Negative Rate</em></em><em> </em><span>or </span><em>Miss Rate</em></p>
</td>
<td>
<p><span>This is the proportion of predicted negatives that are actually positives out of all that should be positives.</span></p>
</td>
</tr>
</tbody>
</table>
<p>Some of these can be a little bit obscure to understand; however, you don't have to memorize them now, you can always come back to this table.</p>
<p>There are other metrics that are a little bit complicated to calculate, such as the following:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Acronym</strong></p>
</td>
<td>
<p><strong>Description</strong></p>
</td>
<td>
<p><strong>Interpretation</strong></p>
</td>
</tr>
<tr>
<td>
<p>ACC</p>
</td>
<td>
<p><em>Accuracy</em></p>
</td>
<td>
<p>This is the rate of correctly predicting the positives and the negatives out of all the samples.</p>
</td>
</tr>
<tr>
<td>
<p><em>F</em><sub>1</sub></p>
</td>
<td>
<p><em>F</em><sub>1</sub><em>-Score</em></p>
</td>
<td>
<p>This is the average of the precision<em> </em>and sensitivity.</p>
</td>
</tr>
<tr>
<td>
<p>MCC</p>
</td>
<td>
<p><em>Matthews Correlation Coefficient</em></p>
</td>
<td>
<p>This is the correlation between the desired and the predicted classes.</p>
</td>
</tr>
<tr>
<td>
<p>BER</p>
</td>
<td>
<p><em>Balanced Error Rate</em></p>
</td>
<td>
<p>This is the average error rate for cases where there is a class imbalance.</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>I included, in this list of <em>complicated</em><em> </em>calculations, acronyms such as <strong>ACC</strong> and <strong>BER</strong>, which are acronyms that have a very intuitive meaning. The main issue is, however, that these will vary when we have multiple classes. So, their calculation will be slightly different in multiple classes. The rest of the metrics remain exclusive (as defined) to binary classification.</p>
<p>Before we discuss metrics for multiple classes, here are the formulas for calculating the previous metrics:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/5276ec41-4004-4150-8622-dcf51e6d52c6.png" style="width:14.58em;height:1.83em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/40867f27-e549-47ff-95e9-55fe50bf85ad.png" style="width:9.83em;height:2.17em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/29c0a645-a95a-43e3-9a0a-6e7b4f2d3152.png" style="width:19.17em;height:2.25em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/17310bd8-85d1-4b80-8976-ec16f0bdd58b.png" style="width:11.08em;height:2.08em;"/></p>
<p>In a general sense, you want <strong>ACC</strong>, <strong>F<sub>1</sub></strong>, and <strong>MCC</strong> to be high and <strong>BER</strong> to be low.</p>
<h2 id="uuid-d6ba982c-104e-44d9-963d-7f49e54062ae">Multiple classes</h2>
<p>When we go beyond simple binary classification, we often deal with multiple classes, such as <em>C</em> = {'<em>Summer</em>', '<em>Fall</em>', '<em>Winter</em>', '<em>Spring</em>'} or <em>C</em> = {0,1,2,3}. This can limit, to a certain point, the way we measure error or success.</p>
<p>Consider the confusion matrix for multiple classes shown here:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8b825d44-a56e-451e-8e4f-213c04487453.png" style="width:27.33em;height:17.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.6 - A confusion matrix for multiple classes</div>
<p>From the following diagram, it is evident that the notion of true positive or negative has disappeared since we no longer have just positive and negative classes, but also sets of finite classes:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f9d6ecb6-50a5-4079-84ed-85028b1436ab.png" style="width:9.17em;height:1.25em;"/></p>
<p>Individual classes, <img class="fm-editor-equation" src="assets/16d50594-4373-4352-bfa9-e7c0c228c469.png" style="width:0.92em;height:0.92em;"/>, can be strings or numbers, as long as they follow the rules of sets. That is, the set of classes, <img class="fm-editor-equation" src="assets/35755034-de0c-4834-9eb3-e8d7445dc5dd.png" style="width:0.75em;height:0.92em;"/>, must be finite and unique.</p>
<p>To measure ACC here, we will count all the elements in the main diagonal of the confusion matrix and divide it by the total number of samples:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cb518092-12fe-4555-986f-95df3fd0102d.png" style="width:6.83em;height:2.33em;"/></p>
<p>In this equation, <img class="fm-editor-equation" src="assets/7e28c26a-52a3-4d61-ba59-d9c1bd3b9356.png" style="width:1.08em;height:0.92em;"/> denotes the confusion matrix and <img class="fm-editor-equation" src="assets/130d0fd7-ce83-47e6-b8c3-df493c208c66.png" style="width:1.92em;height:1.08em;"/> denotes the trace operation; that is, the sum of the elements in the main diagonal of a square matrix. Consequently, the total error is <kbd>1-ACC</kbd>, but in the case of class imbalance, the error metric or plain accuracy may be deceiving. For this, we must use the BER metric, which for multiple classes can be defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/e9878abc-5d61-4822-8244-c05f31a27b88.png" style="width:13.25em;height:3.33em;"/></p>
<p>In this new formula for BER, <img class="fm-editor-equation" src="assets/69426ee5-6e9a-46cf-a421-8d20e26857e7.png" style="width:1.50em;height:1.08em;"/> refers to the element in the <em>j</em>th row and <em>i</em>th column of the confusion matrix, <img class="fm-editor-equation" src="assets/308bc727-30e0-4f7f-b86d-9ca99ac39cc7.png" style="width:1.42em;height:1.25em;"/>.</p>
<div class="packt_infobox">Some machine learning schools of thought use the rows of the confusion matrix to denote true labels and the columns to denote the predicted labels. The theory behind the analysis is the same and the interpretation is, too. Don't be alarmed that <kbd>sklearn</kbd> uses the flipped approach; this is irrelevant and you should not have any problems with following any discussions about this.</div>
<p>As an example, consider the dataset that was shown earlier in <em>Figure 4.1</em>. If we run a five-layered neural network classifier, we could obtain decision boundaries like this:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/170655fa-57cc-4652-bd30-d34d624664c8.png" style="width:29.33em;height:21.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.7 - Classification regions for a sample two-dimensional dataset with a five-layer neural net</div>
<p>Clearly, the dataset is not perfectly separable by a non-linear hyperplane; there are some data points that cross the boundaries for each class. In the previous graph, we can see that only the <em>Summer</em> class has no points that are incorrectly classified based on the classification boundaries.</p>
<p>However, this is more evident if we actually calculate and display the confusion matrix, shown here:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/dc71ff7b-05ae-410b-9ec5-b66bf34c08b0.png" style="width:22.92em;height:20.00em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.8 - A confusion matrix obtained from training errors on the sample two-dimensional dataset</div>
<p>In this case, the accuracy can be calculated as ACC=(25+23+22+24)/100, which yields an ACC of 0.94, which seems nice, and an error rate of 1-ACC = 0.06. This particular example has a slight class imbalance. Here are the samples for each class:</p>
<ul>
<li><span class="packt_screen">Summer</span>: 25</li>
<li><span class="packt_screen">Fall</span>: 25</li>
<li><span class="packt_screen">Winter</span>: 24</li>
<li><span class="packt_screen">Spring</span>: 26</li>
</ul>
<p>The <span class="packt_screen">Winter</span> group has fewer examples than the rest and the <span class="packt_screen">Spring</span> group has more examples than the rest. While this is a very small class imbalance, it can be enough to yield a deceivingly low error rate. We must now calculate the balanced error rate, BER.</p>
<p>BER can be calculated as follows: </p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/585f5334-7957-47d0-b576-3aec44b83eac.png" style="width:15.92em;height:2.25em;"/></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/366a761b-958b-4dea-bf40-3ca41bcaaa2a.png" style="width:28.58em;height:2.33em;"/></p>
<p>Here, the difference between the error rate and BER is a 0.01% under-estimation of the error. However, for classes that are highly imbalanced, the gap can be much larger and it is our responsibility to measure carefully and report the appropriate error measure, BER.</p>
<p>Another interesting fact about BER is that it intuitively is the counterpart of a balanced accuracy; this means that if we remove the <kbd>1–</kbd> term in the BER equation, we are left with the balanced accuracy. Further, if we examine the terms in the numerator, we can see that the fractions on it lead to class-specific accuracies; for example, the first class, <span class="packt_screen">Summer</span>, has a 100% accuracy, the second, <span class="packt_screen">Fall</span>, has a 92% accuracy, and so on.</p>
<p>In Python, the <kbd>sklearn</kbd> library has a class that can determine the confusion matrix automatically, given the true and predicted labels. The class is called <kbd>confusion_matrix</kbd> and it belongs to the <span><kbd>metrics</kbd> </span><span>super class </span><span>and we can use it as follows:</span></p>
<pre>from sklearn.metrics import confusion_matrix<br/>cm = confusion_matrix(y, y_pred)<br/>print(cm)</pre>
<p>If <kbd>y</kbd> contains the true labels, and <kbd>y_pred</kbd> contains the predicted labels, then the preceding instructions will output something like this:</p>
<pre>[[25 0 0 0]<br/> [ 0 23 1 1]<br/> [ 1 0 22 1]<br/> [ 0 1 1 24]]</pre>
<p>We can calculate BER by<span> </span><span>simply </span><span>doing this:</span></p>
<pre>BER = []<br/>for i in range(len(cm)):<br/> BER.append(cm[i,i]/sum(cm[i,:]))<br/>print('BER:', 1 - sum(BER)/len(BER))</pre>
<p>This will output the following:</p>
<pre>BER: 0.06006410256410266</pre>
<p>Alternatively, <kbd>sklearn</kbd> has a built-in function to calculate the balanced accuracy score in the same super class as the confusion matrix. The class is called <kbd>balanced_accuracy_score</kbd> and we can produce BER by doing the following:</p>
<pre>from sklearn.metrics import balanced_accuracy_score<br/>print('BER', 1- balanced_accuracy_score(y, y_pred))</pre>
<p>We get the following output:</p>
<pre>BER: 0.06006410256410266</pre>
<p>Let's now discuss the metrics for regression.</p>
<h2 id="uuid-3b2376a8-fd9d-4136-a613-91e920b9d98a">Regression</h2>
<p>The most popular metric is <strong>MSE</strong>, which we discussed earlier in this chapter when explaining how linear regression works. However, we explained it as a function of the choice of hyperparameters. Here, we will redefine it in a general sense as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/ce4606dc-3c6f-42b1-9172-4094989cc388.png" style="width:11.58em;height:3.42em;"/></p>
<p>Another metric that is very similar to MSE is <strong>mean absolute error</strong> (<strong>MAE</strong>). While MSE penalizes big mistakes <span>more (quadratically)</span><span> </span><span>and small errors much less, MAE penalizes everything in direct proportion to the absolute difference between what should be and what was predicted. This is a formal definition of MAE:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/240ba3a7-8194-473f-9e0f-5bda05e4c582.png" style="width:11.17em;height:3.42em;"/></p>
<p>Finally, out of the other measures for regression, the popular choice in deep learning is the <strong><em>R</em><sup>2</sup> score</strong>,<strong> </strong>also known as the <strong>coefficient of determination</strong>. This metric represents the proportion of variance, which is explained by the independent variables in the model. It measures how likely the model is to perform well on unseen data that follows the same statistical distribution as the training data. This is its definition:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/0f2c6d77-a573-4d0a-b3c6-b582c4f166c7.png" style="width:12.08em;height:3.33em;"/></p>
<p>The sample mean is defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/7de549fd-f078-464a-904d-b20a8675fead.png" style="width:7.92em;height:4.25em;"/></p>
<p>Scikit-learn has classes available for each one of these metrics, indicated in the following table:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Regression metric</strong></p>
</td>
<td>
<p><strong>Scikit-learn class</strong></p>
</td>
</tr>
<tr>
<td>
<p><em>R</em><sup>2</sup><span> </span>score</p>
</td>
<td>
<p><kbd>sklearn.metrics.r2_score</kbd></p>
</td>
</tr>
<tr>
<td>
<p>MAE</p>
</td>
<td>
<p><kbd><span>sklearn.metrics.mean_absolute_error</span></kbd></p>
</td>
</tr>
<tr>
<td>
<p>MSE</p>
</td>
<td>
<p><kbd><span>sklearn.metrics.</span>mean_squared_error</kbd></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>All of these classes take the true labels and predicted labels as input arguments.</p>
<p>As an example, if we take the data and linear regression model <span>shown in <em>Figure 4.3</em> and <em>Figure 4.4</em> </span><span>as input, </span><span>we can determine the three error metrics, as follows:</span></p>
<pre>from sklearn.metrics import mean_absolute_error<br/>from sklearn.metrics import mean_squared_error<br/>from sklearn.metrics import r2_score<br/><br/>r2 = r2_score(y,y_pred)<br/>mae = mean_absolute_error(y,y_pred)<br/>mse = mean_squared_error(y,y_pred)<br/><br/>print('R_2 score:', r2)<br/>print('MAE:', mae)<br/>print('MSE:', mse)</pre>
<p>The output of the preceding code is as follows:</p>
<pre>R_2 score: 0.9350586211501963<br/>MAE: 0.1259473720654865<br/>MSE: 0.022262066145814736</pre>
<p><span>The following graph shows the sample data used, along with the performance obtained. Clearly, the performance using the</span> <span>three </span><span>performance metrics</span><span> </span><span>is good:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d22cb47d-a379-462d-bf76-0acbf7ddda8e.png" style="width:27.83em;height:19.92em;"/></p>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.9 - Error metrics over a linear regression model on data contaminated with white noise</div>
<p>In general, you always want to have a determination coefficient that is as close to <kbd>1</kbd> as possible and all your errors (MSE and MAE) as close to <kbd>0</kbd> as possible. But while all of these are good metrics to report on our models, we need to be careful to report these metrics over <strong>unseen validation</strong> or <strong>test data</strong>. This is so that we accurately measure the generalization ability of the model and identify overfitting in our models before it becomes a catastrophic error.</p>
<h1 id="uuid-28ef3e02-df13-4220-9a04-77ae8feb3722" class="mce-root">Identifying overfitting and generalization</h1>
<p>Often, when we are in a controlled machine learning setting, we are given a dataset that we can use for training and a different set that we can use for testing. The idea is that you only run the learning algorithm on the <strong>training</strong> data, but when it comes to seeing how good your model is, you feed your model the <strong>test</strong> data and observe the output. It is typical for competitions and hackathons to give out the test data but withhold the labels associated with it because the winner will be selected based on how well the model performs on the test data and you don't want them to cheat by looking at the labels of the test data and making adjustments. If this is the case, we can use a <strong>validation</strong> dataset, which we can create by ourselves by separating a portion of the training data to be the validation data.</p>
<p>The whole point of having separate sets, namely a validation or test dataset, is to measure the performance on this data, knowing that our model was not trained with it. A model's ability to perform equally, or close to equally, well on unseen validation or test data is known as <strong>generalization.</strong> </p>
<div class="packt_infobox">Generalization is the ultimate goal of most learning algorithms; all of us professionals and practitioners of deep learning dream of achieving great generalization in all of our models. Similarly, our greatest nightmare is <strong>overfitting</strong>.</div>
<p>Overfitting is the opposite of generalization. It occurs when our models perform extremely well on the training data but when presented with validation or test data, the performance decreases significantly. This indicates that our model almost memorized the intricacies of the training data and missed the big picture generalities of the sample space that lead to good models.</p>
<p>In this and further chapters, we will follow these rules with respect to data splits:</p>
<ul>
<li>If we are given test data (with labels), we will train on the training set and report the performance based on the test set.</li>
<li>If we are not given test data (or if we have test data with no labels), we will split the training set, creating a validation set that we can report performance on using a cross-validation strategy.</li>
</ul>
<p>Let's discuss each scenario separately.</p>
<h2 id="uuid-3d96340a-86b5-40b6-bc50-7e31cc589723">If we have test data</h2>
<p>To begin this discussion, let's say that we have a deep learning model with a set of hyper parameters, <img class="fm-editor-equation" src="assets/fb401be2-c787-431a-ac8a-47d74004dcb3.png" style="width:0.58em;height:1.00em;"/>, which could be the weights of the model, the number of neurons, layers, the learning rate, the drop-out rate, and so on. Then, we can say that a model, <img class="fm-editor-equation" src="assets/d790c8ca-2267-4437-b9df-2eb6ca27611c.png" style="width:1.08em;height:0.92em;"/>, (with parameters <img class="fm-editor-equation" src="assets/cb15d854-ebcd-426f-9120-961cdcc51165.png" style="width:0.58em;height:1.00em;"/>) that is trained with training data, <img class="fm-editor-equation" src="assets/dff30c72-c9b3-4a6f-a20a-e3a3017c59e7.png" style="width:6.42em;height:1.42em;"/>, can have a training accuracy as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/43cdd752-074f-4433-a75a-6c958eb9dbda.png" style="width:9.67em;height:2.17em;"/></p>
<p>This is the training accuracy of a trained model on the training data. Consequently, if we are given labeled test data, <img class="fm-editor-equation" src="assets/b2cc358b-36e4-4460-a96e-6f6234315997.png" style="width:6.92em;height:1.50em;"/>, with <em>M</em> data points, we can simply estimate the <strong>test accuracy</strong> by calculating the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/b3b5f613-b3a4-40fb-9e26-a7097dd0ea7b.png" style="width:10.08em;height:2.25em;"/></p>
<p>One important property when reporting test accuracy usually holds true in most cases—all test accuracy is usually less than the training accuracy plus some noise caused by a poor selection of parameters:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1e080952-7eb2-4e89-9893-848d25b5e9ae.png" style="width:17.75em;height:1.33em;"/></p>
<p>This usually implies that if your test accuracy is significantly larger than your training accuracy, then there could be something wrong with the trained model. Also, we could consider the possibility that the test data is drastically different from the training data in terms of its statistical distribution and the multidimensional manifold that describes it.</p>
<p>In summary, reporting performance on the test set is very important if we have test data that was properly chosen. Nonetheless, it would be completely normal for the performance to be less than it was in training. However, if it is significantly lower, there could be a problem of overfitting and if it is significantly greater, then there could be a problem with the code, the model, and even the choice of test data. The problem of overfitting can be solved by choosing better parameters, <img class="fm-editor-equation" src="assets/cee96f36-9658-4621-9c99-9eadf7d7d500.png" style="width:0.50em;height:0.92em;"/>, or by choosing a different model, <img class="fm-editor-equation" src="assets/d01418e4-a07a-41a4-83dd-3c65bde9d857.png" style="width:1.08em;height:0.92em;"/>, which is discussed in the next section.</p>
<p>Now, let's<span> </span><span>briefly</span><span> discuss a case where we don't have test data or we have test data with no labels.</span></p>
<h2 id="uuid-ebf5b7e5-c399-43ee-8ac5-62c8f6f397f7">No test data? No problem – cross-validate</h2>
<p>Cross-validation is a technique that allows us to split the training data, <img class="fm-editor-equation" src="assets/dff30c72-c9b3-4a6f-a20a-e3a3017c59e7.png" style="width:6.75em;height:1.50em;"/>, into smaller groups for training purposes. The most important point to remember is that the splits are ideally made of an equal number of samples overall and that we want to rotate the choice of groups for training and validation sets. </p>
<p>Let's discuss the famous cross-validation strategy known as <strong><em>k</em>-fold cross-validation</strong> (Kohavi, R. 1995). The idea here is to divide the training data into <em>k</em> groups, which are (ideally) equally large, then select <em>k</em>-1 groups for training the model and measure the performance of the group that was left out. Then, change the groups each time until all the groups have been selected for testing.</p>
<p>In the previous sections, we discussed measuring performance using the standard accuracy, ACC, but we could use any performance metric. To show this, we will now calculate the MSE. This is how the <em>k</em>-fold cross-validation algorithm will look:</p>
<ol>
<li>Input the dataset, <img class="fm-editor-equation" src="assets/e304304b-b22e-44a6-bb16-5f682710918c.png" style="width:0.75em;height:0.83em;"/>, the model, <img class="fm-editor-equation" src="assets/17eb0626-a42f-4af0-a282-6e8b4763660f.png" style="width:0.83em;height:0.75em;"/>, the parameters, <img class="fm-editor-equation" src="assets/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png" style="width:0.50em;height:0.92em;"/>, and the number of folds, <img class="fm-editor-equation" src="assets/1d85f939-85aa-4c66-8a53-c5451f35ba70.png" style="width:0.83em;height:0.75em;"/>.</li>
<li>Divide the set of indices, <img class="fm-editor-equation" src="assets/7976dcab-c59d-4f7d-adc1-46aa541730e0.png" style="width:8.75em;height:1.17em;"/>, into <img class="fm-editor-equation" src="assets/7853dbb6-4144-4bdc-b6e9-f536bad0862d.png" style="width:0.92em;height:0.83em;"/>groups (ideally equal in size), <img class="fm-editor-equation" src="assets/1dab9af3-28c9-4288-b612-2871cba7b80e.png" style="width:3.17em;height:1.17em;"/>, such that <img class="fm-editor-equation" src="assets/09103fa9-8eb1-4d3c-bda9-34629dee8f42.png" style="width:10.25em;height:1.33em;"/>.</li>
<li>For each case of <img src="assets/3ae133ea-6268-4366-b1b1-8a00537970d7.png" style="width:7.42em;height:1.17em;"/>, do the following:</li>
</ol>
<ul>
<li style="padding-left: 30px">Select the indices for training as <sub><img class="fm-editor-equation" src="assets/84dc0f45-f878-48b2-ade1-11e12122e33d.png" style="width:12.08em;height:1.08em;"/></sub> and form the training set, <sub><img class="fm-editor-equation" src="assets/5bedbb94-b2d2-43fa-b526-2eda18f88551.png" style="width:6.42em;height:1.17em;"/></sub>.</li>
<li style="padding-left: 30px">Select the indices for validation as <span><img class="fm-editor-equation" src="assets/5e3df28e-3838-4ca1-b407-8f00962e9d85.png" style="width:4.92em;height:1.25em;"/></span> and form the validation set, <span><img class="fm-editor-equation" src="assets/107f4c15-a2f6-4f29-8a47-2393165addff.png" style="width:8.42em;height:1.58em;"/>.</span></li>
<li style="padding-left: 30px">Train the model with a choice of parameters over the training set: <img class="fm-editor-equation" src="assets/9b069e01-510c-4bad-b188-0df9797af76a.png" style="width:2.33em;height:0.92em;"/>.</li>
<li style="padding-left: 30px">Compute the error of the model, <sub><img class="fm-editor-equation" src="assets/40685b40-6076-4fba-b045-11a6d39face0.png" style="width:1.25em;height:1.08em;"/></sub>, on the validation set : <sub><img class="fm-editor-equation" src="assets/cb27f873-4b41-48ee-b205-215589f73273.png" style="width:16.25em;height:3.00em;"/></sub></li>
</ul>
<ol start="4">
<li>Return <img class="fm-editor-equation" src="assets/c8557d04-4efd-4275-845a-ca9183a4be72.png" style="width:3.00em;height:1.17em;"/> for all cases of <img class="fm-editor-equation" src="assets/355f6774-f073-4a73-9a57-c7df89a2cbbe.png" style="width:8.08em;height:1.25em;"/>.</li>
</ol>
<p class="mce-root">With this, we can calculate the cross-validation error (MSE) given by the following:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/f2e94af6-ec29-4bc0-9c9c-8cb59ba9ed2e.png" style="width:11.00em;height:3.67em;"/></p>
<p>We can also calculate its corresponding standard deviation:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/bba147e0-2d91-4e2c-bcb8-fb4db4c08f9a.png" style="width:16.42em;height:3.25em;"/>.</p>
<p>It is usually a good idea to look at the standard deviation of our performance metric—regardless of the choice—since it gives an idea of how consistent our performance on the validation sets is. Ideally, we would like to have a cross-validated MSE of <kbd>0</kbd>, <img class="fm-editor-equation" src="assets/5f2ba80e-d9bd-4d27-9965-a87fa2cb272a.png" style="width:3.75em;height:1.17em;"/>, and a standard deviation of <kbd>1</kbd>, <img class="fm-editor-equation" src="assets/51bcebac-4507-44fd-bae4-111feca2dbd9.png" style="width:4.92em;height:1.17em;"/>.</p>
<p>To explain this, we can use the regression example of the sample data contaminated by white noise, shown in <em>Figure 4.3</em> and <em>Figure 4.4</em>. To keep things simple for this example, we will use a total of 100 samples, <em>N</em>=100, and we will use 3 folds. We will use scikit-learn's <kbd>KFold</kbd> <span>class</span><span> </span><span>inside the</span> <kbd>model_selection</kbd> <span>super class and we will obtain the cross-validated MSE and its standard deviation.</span></p>
<p>To do this, we can use the following code and include other metrics as well:</p>
<pre>import numpy as np<br/>from sklearn.metrics import mean_absolute_error<br/>from sklearn.metrics import mean_squared_error<br/>from sklearn.metrics import r2_score<br/>from sklearn.model_selection import KFold<br/><br/># These will be used to save the performance at each split<br/>cv_r2 = []<br/>cv_mae = []<br/>cv_mse = []<br/><br/># Change this for more splits<br/>kf = KFold(n_splits=3)<br/>k = 0<br/><br/># Assuming we have pre-loaded training data X and targets y<br/>for S_D, S_V in kf.split(X):<br/>  X_train, X_test = X[S_D], X[S_V]<br/>  y_train, y_test = y[S_D], y[S_V]<br/><br/>  # Train your model here with X_train and y_train and...<br/>  # ... test your model on X_test saving the output on y_pred<br/><br/>  r2 = r2_score(y_test,y_pred)<br/>  mae = mean_absolute_error(y_test,y_pred)<br/>  mse = mean_squared_error(y_test,y_pred)<br/> <br/>  cv_r2.append(r2)<br/>  cv_mae.append(mae)<br/>  cv_mse.append(mse)<br/><br/>print("R_2: {0:.6}  Std: {1:0.5}".format(np.mean(cv_r2),np.std(cv_r2)))<br/>print("MAE: {0:.6}  Std: {1:0.5}".format(np.mean(cv_mae),np.std(cv_mae)))<br/>print("MSE: {0:.6}  Std: {1:0.5}".format(np.mean(cv_mse),np.std(cv_mse)))</pre>
<p class="mce-root"/>
<p>The result of this code will return something as follows:</p>
<pre>R_2: 0.935006  Std: 0.054835<br/>MAE: 0.106212  Std: 0.042851<br/>MSE: 0.0184534  Std: 0.014333</pre>
<p>These results are cross-validated and give a clearer picture of the generalization abilities of the model. For comparison purposes, see the results shown in <em>Figure 4.9</em>. You will notice that the results are very consistent between the performance measured before using the whole set in <em>Figure 4.9</em> and now, using only about 66% of the data (since we split it into three groups) for training and about 33% for testing, as shown:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/352f369c-e46e-403f-80db-daee8d4fc3d2.png" style="width:27.75em;height:19.33em;"/></p>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 4.10 - Cross-validated performance metrics with standard deviation in parenthesis</div>
<p>The previous graph shows the linear regression solution found for every split of the data as well as the true original function; you can see that the solutions found are fairly close to the true model, yielding a good performance, as measured by <strong><em>R</em><sup>2</sup></strong>, <strong>MAE</strong>, and <strong>MSE</strong>.</p>
<div class="packt_tip"><strong>Exercise</strong><br/>
Go ahead and change the number of folds, progressively increasing it, and document your observations. What happens to the cross-validated performances? Do they stay the same, increase, or decrease? What happens to the standard deviations of the cross-validated performances? Do they stay the same, increase, or decrease? What do you think this means?</div>
<p><span>Usually, cross-validation is used on a dataset, </span><img class="fm-editor-equation" src="assets/e304304b-b22e-44a6-bb16-5f682710918c.png" style="width:0.75em;height:0.83em;"/><span>, with a model, </span><img class="fm-editor-equation" src="assets/17eb0626-a42f-4af0-a282-6e8b4763660f.png" style="width:0.92em;height:0.83em;"/><span>, trained on parameters, </span><img class="fm-editor-equation" src="assets/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png" style="width:0.50em;height:0.83em;"/><span>. However, one of the greatest challenges in learning algorithms is finding the best set of parameters, <img class="fm-editor-equation" src="assets/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png" style="width:0.50em;height:0.83em;"/>, that can yield the best (test or cross-validated) performance. Many machine learning scientists believe choosing the set of parameters can be <strong>automated</strong> with some algorithms and others believe this is an <strong>art</strong> (Bergstra, J. S., et.al. 2011). </span></p>
<h1 id="uuid-b1c1ea57-76a2-46e9-a1f7-edc9d25cd65b" class="mce-root">The art behind learning</h1>
<p>For those of us who have spent decades studying machine learning, experience informs the way we choose parameters for our learning algorithms. But for those who are new to it, this is a skill that needs to be developed and this skill comes after learning how learning algorithms work. Once you have finished this book, I believe you will have enough knowledge to choose your parameters wisely. In the meantime, we can discuss some ideas for finding parameters automatically using standard and novel algorithms here. </p>
<p>Before we go any further, we need to make a distinction at this point and define two major sets of parameters that are important in learning algorithms. These are as follows:</p>
<ul>
<li><strong>Model parameters:</strong> These are parameters that represent the solution that the model represents. For example, in perceptron and linear regression, this would be vector <img class="fm-editor-equation" src="assets/5c1ca364-6e24-407a-8b15-dd412d3dda71.png" style="width:1.08em;height:0.83em;"/><strong> </strong>and scalar <img class="fm-editor-equation" src="assets/f8b8288d-5510-4840-9753-7b375f040e5a.png" style="width:0.50em;height:1.00em;"/>, while for a deep neural network, this would be a matrix of weights, <img class="fm-editor-equation" src="assets/74c8f74e-3b92-4f54-b1c7-85e2dcc4318e.png" style="width:1.08em;height:0.75em;"/>, and a vector of biases, <img class="fm-editor-equation" src="assets/9fe4b0b9-c94e-4df7-bb9b-9efa507ee211.png" style="width:0.67em;height:0.92em;"/>. For a convolutional network, this would be filter sets.</li>
<li><strong>Hyperparameters:</strong> These are parameters needed by the model to guide the learning process to search for a solution (model parameters) and are usually represented as <img class="fm-editor-equation" src="assets/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png" style="width:0.58em;height:1.00em;"/>. For example, in the PLA, a hyperparameter would be the maximum number of iterations; in a deep neural network, it would be the number of layers, the number of neurons, the activation function for the neurons, and the learning rate; and for a <strong>convolutional neural network</strong> (<strong>CNN</strong>), it would be the number of filters, the size of filters, the stride, the pooling size, and so on.</li>
</ul>
<p>Put in other words, the model parameters are determined, in part, by the choice of hyperparameters. Usually, unless there is a numerical anomaly, all learning algorithms will consistently find solutions (model parameters) for the same set of hyperparameters. So, one of the main tasks when learning is finding the best set of hyperparameters that will give us the best solutions.</p>
<p>To observe the effects of altering the hyperparameters of a model, let's once more consider the four-class classification problem of the seasons, shown earlier in <em>Figure 4.7</em>. We will assume that we are using a fully connected network, such as the one described in <a href="e3181710-1bb7-4069-825a-a235355bc116.xhtml"><em>Chapter 1</em></a>, <em>Introduction to Machine Learning</em>, and the hyperparameter we want to determine is the best number of layers. Just for didactic purposes, let's say that the number of neurons in each layer will increase exponentially in each layer, as shown:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Layer</strong></p>
</td>
<td>
<p><strong>Neurons in each layer</strong></p>
</td>
</tr>
<tr>
<td>
<p>1</p>
</td>
<td>
<p>(8)</p>
</td>
</tr>
<tr>
<td>
<p>2</p>
</td>
<td>
<p><span>(16, 8)</span></p>
</td>
</tr>
<tr>
<td>
<p>3</p>
</td>
<td>
<p><span>(32, 16, 8)</span></p>
</td>
</tr>
<tr>
<td>
<p>4</p>
</td>
<td>
<p><span>(64, 32, 16, 8)</span></p>
</td>
</tr>
<tr>
<td>
<p>5</p>
</td>
<td>
<p><span>(128, 64, 32, 16, 8)</span></p>
</td>
</tr>
<tr>
<td>
<p>6</p>
</td>
<td>
<p><span>(256, 128, 64, 32, 16, 8)</span></p>
</td>
</tr>
<tr>
<td>
<p>7</p>
</td>
<td>
<p><span>(512, 256, 128, 64, 32, 16, 8)</span></p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In the previous configuration, the first number in the brackets corresponds to the number of neurons closest to the input layer, while the last number in the brackets corresponds to the number of neurons closest to the output layer (which consists of 4 neurons, one per class). </p>
<p>So, the number of layers represents <img class="fm-editor-equation" src="assets/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png" style="width:0.58em;height:1.00em;"/>, in this example. If we loop through each configuration and determine the cross-validated BER, we can determine which architecture yields the best performance; that is, we are optimizing <img class="fm-editor-equation" src="assets/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png" style="width:0.58em;height:1.00em;"/> for performance. The results obtained will look as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td>
<p><strong>Layers – <sub><img src="assets/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png" style="width:0.75em;height:1.33em;"/></sub></strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>1</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>2</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>3</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>4</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>5</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>6</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p><strong>7</strong></p>
</td>
</tr>
<tr>
<td>
<p><strong>BER</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.275</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.104</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.100</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.096</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.067</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.079</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.088</p>
</td>
</tr>
<tr>
<td>
<p><strong>Standard deviation</strong></p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.22</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.10</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.08</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.10</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.05</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.04</p>
</td>
<td class="CDPAlignCenter CDPAlign">
<p>0.08</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>From the results, we can easily determine that the best architecture is one with five layers since it has the lowest BER and the second smallest standard deviation. We could, indeed, gather all the data at each split for each configuration and produce the box plot shown here:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/3c159f1a-3d99-48ee-8517-ac6a33da553e.png" style="width:32.83em;height:23.25em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.11 - A box plot of the cross-validated data optimizing the number of layers</div>
<p>This box plot illustrates a couple of important points. First, that there is a clear tendency of the model to reduce the BER as the number of layers increases up to <kbd>5</kbd>, then increases after that. This is very common in machine learning and it is known as the <strong>overfitting curve</strong>, which is usually a <em>u </em>shape (or <em>n </em>shape, for performance metrics that are better on higher values). The lowest point, in this case, would indicate the best set of hyperparameters (at <kbd>5</kbd>); anything to the left of that represents <strong>underfitting</strong> and anything to the right represents <strong>overfitting</strong>. The second thing that the box plot shows is that even if several models have a similar BER, we will choose the one that shows less variability and most consistency.</p>
<p>To illustrate the differences between underfitting, good fitting, and overfitting, we will show the decision boundaries produced by the worst underfit, the best fit, and the worst overfit. In this case, the worst underfit is one layer, the best fit is five layers, and the worst overfit is seven layers. Their respective decision boundaries are shown in <em>Figure 4.12</em>, <em>Figure 4.13</em>, and <em>Figure 4.14</em>, respectively:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/7e730ac9-4d3a-4868-9313-a783b113408f.png" style="width:23.25em;height:16.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.12 - Classification boundaries for a one-hidden-layer network that is underfitting</div>
<p>In the preceding graph, we can see that the underfit is clear since there are decision boundaries that prevent many datapoints from being classified correctly:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/5e2f7982-3a60-4d8d-941a-0dc2d14e0b42.png" style="width:23.83em;height:17.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 4.13 - Classification boundaries for a five-hidden-layer network that has a relatively good fit</span></div>
<p>Similarly, the previous graph shows the decision boundaries, but compared to <em>Figure 4.12</em>, these boundaries seem to provide a nicer separation of the data points for the different groups—a good fit:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/60fedb99-d5fc-4dfb-b89c-53f1318e7d43.png" style="width:25.92em;height:18.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 4.14 - Classification boundaries for a seven-hidden-layer network that is overfitting</span></div>
<p>If you look closely, <em>Figure 4.12</em> shows that some regions are designated very poorly, while in <em>figure 4.14</em>, the network architecture is trying <em>too hard</em> to classify all the examples perfectly, to the point where the outlier in the <em>Fall</em> class (the yellow points) that goes into the region of the <em>Winter</em> class (the blue points) has its own little region, which may have negative effects down the road. The classes in <em>Figure 4.13</em> seem to be robust against some of the outliers and have well-defined regions, for the most part.</p>
<p>As we progress through this book, we will deal with more complex sets of hyperparameters. Here we just dealt with one, but the theory is the same. This method of looking at the best set of hyperparameters is known as an exhaustive search. However, there are other ways of looking at parameters, such as performing a <strong>grid search.</strong></p>
<p>Suppose that you do not have a fixed way of knowing the number of neurons in each layer (as opposed to the earlier example); you only know that you would like to have something between <kbd>4</kbd> and <kbd>1024</kbd> neurons and something between <kbd>1</kbd> and <kbd>100</kbd> layers to allow deep or shallow models. In that case, you cannot do an exhaustive search; it would take too much time! Here, grid search<strong> </strong>is used as a solution that will sample the search space in—usually—equally-spaced regions.</p>
<p>For example, grid search can look at a number of neurons in the <kbd>[4, 1024]</kbd> <span>range</span><span> </span><span>on 10 equally spaced values—<kbd>4</kbd>, <kbd>117</kbd>, <kbd>230</kbd>, <kbd>344</kbd>, <kbd>457</kbd>, <kbd>570</kbd>, <kbd>684</kbd>, <kbd>797</kbd>, <kbd>910</kbd>, and <kbd>1024</kbd>—and the number of layers that is in the <kbd>[1,100]</kbd> </span><span>range</span><span> </span><span>on 10 equally spaced values—<kbd>1</kbd>, <kbd>12</kbd>, <kbd>23</kbd>, <kbd>34</kbd>, <kbd>45</kbd>, <kbd>56</kbd>, <kbd>67</kbd>, <kbd>78</kbd>, <kbd>89</kbd>, and <kbd>100</kbd>. Rather than looking at 1020*100=102,000 searches, it will look at 10*10=100, instead. </span></p>
<p>In <kbd>sklearn</kbd>, there is a class, <kbd>GridSearchCV</kbd>, that can return the best models and hyperparameters in cross-validation; it is part of the <kbd>model_selection</kbd> super class. The same class group has another class, called <kbd>RandomizedSearchCV</kbd>, which contains a methodology based on randomly searching the space. This is called <strong>random search.</strong></p>
<p>In <strong>random search</strong>, the premise is that it will look within the <kbd><span>[4, 1024]</span></kbd><span> </span><span>range </span><span>and the <kbd>[1,100]</kbd> range for neurons and layers, respectively, by randomly drawing numbers uniformly until it reaches a maximum limit of total iterations. </span></p>
<div class="packt_tip">Typically, if you know the range and distribution of the parameter search space, try a <strong>grid search</strong> approach on the space you believe is likely to have a better cross-validated performance. However, if you know very little or nothing about the parameter search space, use a <strong>random search</strong> approach. In practice, both of these methods work well.</div>
<p>There are other, more sophisticated methods that work well but whose implementation in Python is not yet standard, so we will not cover them in detail here. However, you should know about them:</p>
<ul>
<li>Bayesian hyperparameter optimization (Feurer, M., et.al. 2015)</li>
<li>Evolution theory-based hyperparameter optimization (Loshchilov, I., et.al. 2016)</li>
<li>Gradient-based hyperparameter optimization (Maclaurin, D., et.al. 2015)</li>
<li>Least squares-based hyperparameter optimization (Rivas-Perea, P., et.al. 2014)</li>
</ul>
<h1 id="uuid-d6b4a3af-b9bf-4355-83dc-96dce3e5ece9" class="mce-root">Ethical implications of training deep learning algorithms</h1>
<p>There are a few things that can be said about the ethical implications of training deep learning models. There is potential harm whenever you are handling data that represents human perceptions. But also, data about humans and human interaction has to be rigorously protected and examined carefully before creating a model that will generalize based on such data. Such thoughts are organized in the following sections.</p>
<h2 id="uuid-d64cd6b1-8c10-47ca-9c7f-1fe84c7ad5ec">Reporting using the appropriate performance measures</h2>
<p>Avoid faking good performance by picking the one performance metric that makes your model look good. It is not uncommon to read articles and reports of multi-class classification models that are trained over clear, class-imbalanced datasets but report the standard accuracy. Most likely, these models will report a high standard of accuracy since the models will be biased toward the over-sampled class and against the under-sampled groups. So, these types of models must report the balanced accuracy or the balanced error rate. </p>
<p>Similarly, for other types of classification and regression problems, you must report the appropriate performance metric. When in doubt, report as many performance metrics as you can. Nobody has ever complained about someone reporting model performance using too many metrics.</p>
<p>The consequences of not reporting the appropriate metrics go from having biased models that go undetected and are deployed into production systems with disastrous consequences to having misleading information that can be detrimental to our understanding of specific problems and how models perform. We must recall that what we do may affect others and we need to be vigilant. </p>
<h2 id="uuid-ac89ff66-83e1-43e0-8cb9-2bf523be7273">Being careful with outliers and verifying them</h2>
<p>Outliers are usually seen as bad things to work around during the learning process and I agree. Models should be robust against outliers, unless they are not really outliers. If we have some data and we don't know anything about it, it is a safe assumption to interpret outliers as anomalies. </p>
<p>However, if we know anything about the data (because we collected it, were given all the information about it, or know the sensors that produced it), then we can verify that outliers are really outliers. We must verify that they were the product of human error when typing data or produced by a faulty sensor, data conversion error, or some other artifact because if an outlier is not the product of any of these reasons, there is no reasonable basis for us to assume that it is an outlier. In fact, data like this gives us important information about situations that may not occur frequently but will eventually happen again and the model needs to respond properly.</p>
<p>Consider the data shown in the following figure. If we arbitrarily decide to ignore outliers without verification (such as in the top diagram), it may be that they are in fact not really outliers and the model will create a narrow decision space that ignores the outliers. The consequence, in this example, is that one point will be incorrectly classified as belonging to another group, while another point might be left out of the majority group:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/ad524a4e-52df-40bc-a71f-6c80e34dd76c.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 4.15 - Differences in the learned space of my models. The top diagram shows the ignoring outliers outcome. The bottom diagram shows the including outliers outcome</div>
<p>However, if we verify the data and discover that the outliers are completely valid input, the models might learn a better decision space that could potentially include the outliers. Nonetheless, this can yield a secondary problem where a point is classified as belonging to two different groups with different degrees of membership. While this is a problem, it is a much smaller risk than incorrectly classifying something. It is better to have, say, <span>60% certainty</span><span> </span><span>that a point belongs to one class and 40% certainty that it belongs to the other class, rather than classifying it incorrectly with 100% certainty. </span></p>
<p>If you think about it, models <span>that were built by ignoring outliers and then </span>deployed into government systems can cause discrimination problems. They may show bias against minority or protected population groups. If deployed into incoming school student selection, it could lead to the rejection of exceptional students. If deployed into DNA classification systems, it could incorrectly ignore the similarity of two very close DNA groups. Therefore, always verify outliers if you can.</p>
<h2 id="uuid-b0096942-3272-4d93-b4a8-0f4545bdd25f">Weight classes with undersampled groups</h2>
<p>If you have a class imbalance, as in <em>Figure 4.15</em>, I recommend you try to balance the classes by getting more data rather than reducing it. If this is not an option, look into algorithms that allow you to weight some classes differently, so as to even out the imbalance. Here are a couple of the most common techniques:</p>
<ul>
<li>On small datasets, use <kbd>sklearn</kbd> and the <kbd>class_weight</kbd> option. When training a model, it penalizes mistakes based on the provided weight for that class. There are a couple of automatic alternatives that you can look into that will also help, such as <kbd>class_weight="auto"</kbd> and <kbd>class_weight="balanced"</kbd>.</li>
<li>On large datasets where batch training is used, use Keras and the <kbd>BalancedBatchGenerator</kbd> class. This will prepare a selection of samples (batches) that is consistently balanced each time, thereby guiding the learning algorithm to consider all groups equally. The class is part of <kbd>imblearn.keras</kbd>.</li>
</ul>
<p>You should try to use these strategies every time you want to have a model that is not biased toward a majority group. The ethical implications of this are similar to the previous points already mentioned. But above all, we must protect life and treat people with respect; all people have an equal, infinite worth.</p>
<h1 id="uuid-668f5ff7-b5c6-40a6-8003-71f12dc436c4">Summary </h1>
<p>In this basic-level chapter, we discussed the basics of learning algorithms and their purpose. Then, we studied the most basic way of measuring success and failure through performance analysis using accuracies, errors, and other statistical devices. We also studied the problem of overfitting and the super important concept of generalization, which is its counterpart. Then, we discussed the art behind the proper selection of hyperparameters and strategies for their automated search.</p>
<p>After reading this chapter, you are now able to explain the technical differences between classification and regression and how to calculate different performance metrics, such as ACC, BER, MSE, and others, as appropriate for different tasks. Now, you are capable of detecting overfitting by using train, validation, and test datasets under cross-validation strategies, you can experiment with and observe the effects of altering the hyperparameters of a learning model. You are also ready to think critically about the precautions and devices necessary to prevent human harm caused by deep learning algorithms.</p>
<p>The next chapter is <a href="4e4b45a6-1924-4918-b2cd-81f0448fb213.xhtml"><em>Chapter 5</em></a>,<span> </span><em>Training a Single Neuron,</em><span> which revises and expands the concept of a neuron, which was introduced in <a href="e3181710-1bb7-4069-825a-a235355bc116.xhtml"><em>Chapter 1</em></a>, <em>Introduction to Machine Learning</em>, and shows its implementation in Python using different datasets to analyze the potential effects of different data; that is, linear and non-linearly separable data. However,</span><span> before we go there, please try to quiz yourself using the following questions. </span></p>
<h1 id="uuid-5fe6cc9b-fdcd-4e04-998c-7d2e12551b24">Questions and answers</h1>
<ol>
<li class="mce-root"><strong>When you did the exercise on cross-validation, what happened to the standard deviation and what does that mean?</strong></li>
</ol>
<p style="padding-left: 60px" class="mce-root">The standard deviation stabilizes and reduces on more folds. This means that the performance measurements are more reliable; it is an accurate measure of generalization or overfitting.</p>
<ol start="2">
<li class="mce-root"><strong>What is the difference between hyperparameters and model parameters?</strong></li>
</ol>
<p style="padding-left: 60px" class="mce-root">Model parameters are numerical solutions to a learning algorithm; hyperparameters are what the model needs to know in order to find a solution effectively.</p>
<ol start="3">
<li class="mce-root"><strong>Is a grid search faster than a randomized search for hyperparameters?</strong></li>
</ol>
<p style="padding-left: 60px" class="mce-root">It depends. If the choice of hyperparameters affects the computational complexity of the learning algorithm, then both could behave differently. However, in similar search spaces and in the amortized case, both should finish at about the same time.</p>
<ol start="4">
<li class="mce-root"><strong>Can I use a regression-based learning algorithm for a classification problem?</strong></li>
</ol>
<p style="padding-left: 60px" class="mce-root">Yes, as long as the labels, categories, or groups are mapped to a number in the set of real numbers.</p>
<ol start="5">
<li class="mce-root"><strong>Can I use a classification-based learning algorithm for a regression problem?</strong></li>
</ol>
<p style="padding-left: 60px" class="mce-root">No.</p>
<ol start="6">
<li><strong>Is the concept of a loss function the same as an error metric?</strong></li>
</ol>
<p style="padding-left: 60px">Yes and no. Yes, in the sense that a loss function will measure performance; however, the performance may not necessarily be with respect to the accuracy of classifying or regressing the data; it may be with respect to something else, such as the quality of groups or distances in information-theoretic spaces. For example, linear regression is based on the MSE algorithm as a loss function to minimize, while the loss function of the K-means algorithm is the sum of the squared distances of the data to their means, which it aims to minimize, but this does not necessarily mean it is an error. In the latter case, it is arguably meant as a cluster quality measure.</p>
<h1 id="uuid-0d869d63-5d22-4a9b-832f-8a513fff7bd3">References</h1>
<ul>
<li>Lorena, A. C., De Carvalho, A. C., &amp; Gama, J. M. (2008), A review on the combination of binary classifiers in multiclass problems, <em>Artificial Intelligence Review</em>, 30(1-4), 19</li>
<li><span><span>Hochreiter, S., Younger, A. S., &amp; Conwell, P. R. (2001, August), Learning to learn using gradient descent, in <em>International Conference on Artificial Neural Networks</em> (pp. 87-94), Springer: Berlin, H</span></span><span>eidelberg</span></li>
<li>Ruder, S. (2016), An overview of gradient descent optimization algorithms, <em>arXiv</em> <em>preprint</em> arXiv:1609.04747</li>
<li>Tan, X., Zhang, Y., Tang, S., Shao, J., Wu, F., &amp; Zhuang, Y. (2012, October), Logistic tensor regression for classification, in <em>International Conference on Intelligent Science and Intelligent Data Engineering</em> (pp. 573-581), Springer: Berlin, Heidelberg</li>
<li>Krejn, S. G. E. (1982), <em>Linear Equations in Banach Spaces,</em> Birkhäuser: Boston </li>
<li>Golub, G., &amp; Kahan, W. (1965), Calculating the singular values and pseudo-inverse of a matrix, <em>Journal of the Society for Industrial and Applied Mathematics</em>, Series B: Numerical Analysis, 2(2), (pp. 205-224)</li>
<li>Kohavi, R. (1995, August), A study of cross-validation and bootstrap for accuracy estimation and model selection, in <em>IJCAI</em>, 14(2), (pp. 1137-1145)</li>
<li>Bergstra, J. S., Bardenet, R., Bengio, Y., &amp; Kégl, B. (2011), Algorithms for hyper-parameter optimization, in <em>Advances in Neural Information Processing Systems,</em> (pp. 2546-2554)</li>
<li>Feurer, M., Springenberg, J. T., &amp; Hutter, F. (2015, February), Initializing Bayesian hyperparameter optimization via meta-learning, in <em>Twenty-Ninth AAAI Conference on Artificial Intelligence</em></li>
</ul>
<ul>
<li>Loshchilov, I., &amp; Hutter, F. (2016), CMA-ES for hyperparameter optimization of deep neural networks, <em>arXiv preprint</em> arXiv:1604.07269</li>
<li>Maclaurin, D., Duvenaud, D., &amp; Adams, R. (2015, June), Gradient-based hyperparameter optimization through reversible learning, in <em>International Conference on Machine Learning</em> (pp. 2113-2122)</li>
<li>Rivas-Perea, P., Cota-Ruiz, J., &amp; Rosiles, J. G. (2014), A nonlinear least squares quasi-Newton strategy for LP-SVR hyper-parameters selection, <em>International Journal of Machine Learning and Cybernetics</em>, 5(4), (pp.579-597)</li>
</ul>


            </article>

            
        </section>
    </body></html>