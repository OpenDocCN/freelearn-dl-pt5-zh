["```py\ndataset = pd.read_csv('data.csv')\nprint (dataset.head())\nprint(dataset) \n```", "```py\nn=1000\ndataset1=np.zeros(shape=(n,2))\nfor i in range (n):\n    j=randint(0,4998)\n    dataset1[i][0]=dataset.iloc[j,0]\n    dataset1[i][1]=dataset.iloc[j,1] \n```", "```py\n#II.Hyperparameters\n# Features = 2 :implicit through the shape of the dataset (2 columns)\nk = 6\nkmeans = KMeans(n_clusters=k)\n#III.K-means clustering algorithm\nkmeans = kmeans.fit(dataset1) #Computing k-means clustering\ngcenters = kmeans.cluster_centers_ # the geometric centers or centroids\nprint(\"The geometric centers or centroids:\")\nprint(gcenters) \n```", "```py\nThe geometric centers or centroids:\n[[ 19.6626506 14.37349398]\n [ 49.86619718 86.54225352]\n [ 65.39306358 54.34104046]\n [ 29.69798658 54.7852349 ]\n [ 48.77202073 23.74611399]\n [ 96.14124294 82.44067797]] \n```", "```py\nsn=4999\nshuffled_dataset=np.zeros(shape=(sn,2))\nfor i in range (sn):\n    shuffled_dataset[i][0]=dataset.iloc[i,0]\n    shuffled_dataset[i][1]=dataset.iloc[i,1] \n```", "```py\nn=1000\ndataset1=np.zeros(shape=(n,2))\nfor i in range (n):\n    dataset1[i][0]=shuffled_dataset[i,0]\n    dataset1[i][1]=shuffled_dataset[i,1] \n```", "```py\n[[ 29.51298701 62.77922078]\n [ 57.07894737 84.21052632]\n [ 20.34337349 15.48795181]\n [ 45.19900498 23.95024876]\n [ 96.72262774 83.27737226]\n [ 63.54210526 51.53157895]] \n```", "```py\nn=1000\ndataset1=np.zeros(shape=(n,2))\nli=0\nfor i in range (n):\n    j=randint(0,4999)\n    dataset1[li][0]=dataset.iloc[j,0]\n    dataset1[li][1]=dataset.iloc[j,1]\n    li+=1 \n```", "```py\n    dataset = pd.read_csv('data.csv') \n    ```", "```py\n    kmeans = pickle.load(open('kmc_model.sav', 'rb')) \n    ```", "```py\n     for i in range(0,1000):\n            xf1=dataset.at[i,'Distance']\n            xf2=dataset.at[i,'location']\n            X_DL = [[xf1,xf2]]\n            prediction = kmeans.predict(X_DL) \n    ```", "```py\n     p=str(prediction).strip('[]')\n            p=int(p)\n            kmcpred[i][0]=int(xf1)\n            kmcpred[i][1]=int(xf2)\n            kmcpred[i][2]=p; \n    ```", "```py\n    np.savetxt('ckmc.csv', kmcpred, delimiter=',', fmt='%d') \n    ```", "```py\n    80,53,5\n    18,8,2\n    55,38,0 \n    ```", "```py\n    import pandas as pd #data processing\n    from sklearn.tree import DecisionTreeClassifier #the dt classifier\n    from sklearn.model_selection import train_test_split #split the data into training data and testing data\n    from sklearn import metrics #measure prediction performance\n    import pickle #save and load estimator models \n    ```", "```py\n#loading dataset\ncol_names = ['f1', 'f2','label']\ndf = pd.read_csv(\"ckmc.csv\", header=None, names=col_names)\nprint(df.head())\n#defining features and label (classes)\nfeature_cols = ['f1', 'f2']\nX = df[feature_cols] # Features\ny = df.label # Target variable\nprint(X)\nprint(y)\n# splitting df (dataset) into training and testing data\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.3, random_state=1) # 70% training and 30% test \n```", "```py\n f1  f2  label\n0  80  53      5\n1  18   8      2\n2  55  38      0\n3  74  74      5\n4  17   4      2 \n```", "```py\n    # create the decision tree classifier\n    dtc = DecisionTreeClassifier()\n    # train the decision tree\n    dtc = dtc.fit(X_train,y_train) \n    ```", "```py\n    #predictions on X_test\n    print(\"prediction\")\n    y_pred = dtc.predict(X_test)\n    print(y_pred) \n    ```", "```py\n    prediction\n    [4 2 0 5 0...] \n    ```", "```py\n    # model accuracy\n    print(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred)) \n    ```", "```py\nfrom sklearn import tree\nimport pydotplus\ngraph=1\nif(graph==1):\n    # Creating the graph and exporting it\n    dot_data = tree.export_graphviz(dtc, out_file=None,\n                                    filled=True, rounded=True,\n                                    feature_names=feature_cols,\n                                    class_names=['0','1','2',\n                                                 '3','4','5'])\n    #creating graph\n    graph = pydotplus.graph_from_dot_data(dot_data)\n    #save graph\n    image=graph.create_png()\n    graph.write_png(\"kmc_dt.png\") \n```", "```py\nfrom sklearn.cluster import KMeans\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport pickle\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics \n```", "```py\n    #I.KMC. The prediction dataset and model\n    dataset = pd.read_csv('data.csv')\n    kmeans = pickle.load(open('kmc_model.sav', 'rb')) \n    ```", "```py\n     for i in range(0,1000):\n            xf1=dataset.at[i,'Distance']\n            xf2=dataset.at[i,'location']\n            X_DL = [[xf1,xf2]]\n            prediction = kmeans.predict(X_DL)\n            #print (i+1, \"The prediction for\",X_DL,\" is:\",\n                str(prediction).strip('[]'))\n            #print (i+1, \"The prediction for\",\n                str(X_DL).strip('[]'), \" is:\",\n                str(prediction).strip('[]'))\n            p=str(prediction).strip('[]')\n            p=int(p)\n            kmcpred[i][0]=int(xf1)\n            kmcpred[i][1]=int(xf2)\n            kmcpred[i][2]=p\n    np.savetxt('ckmc.csv', kmcpred, delimiter=',', fmt='%d') \n    ```", "```py\n    if adt==1:\n        #I.DT. The prediction dataset and model\n        col_names = ['f1', 'f2','label']\n        # load dataset\n        ds = pd.read_csv('ckmc.csv', header=None,\n                         names=col_names)\n        #split dataset in features and target variable\n        feature_cols = ['f1', 'f2']\n        X = ds[feature_cols] # Features\n        y = ds.label # Target variable\n        # Split dataset into training set and test set\n        X_train, X_test, y_train, y_test = train_test_split(\n            X, y, test_size=0.3, random_state=1) # 70% training and 30% test\n        # Load model\n        dt = pickle.load(open('dt.sav', 'rb')) \n    ```", "```py\n     #Predict the response for the test dataset\n        y_pred = dt.predict(X_test)\n        # Model Accuracy\n        acc=metrics.accuracy_score(y_test, y_pred)\n        print(\"Accuracy:\",round(acc,3)) \n    ```", "```py\n    #Double Check the Model's Accuracy\n        doublecheck=1 #0 deactivated, 1 activated\n        if doublecheck==1:\n            t=0\n            f=0\n            for i in range(0,1000):\n                xf1=ds.at[i,'f1']\n                xf2=ds.at[i,'f2']\n                xclass=ds.at[i,'label']\n                X_DL = [[xf1,xf2]]\n                prediction =clf.predict(X_DL)\n                e=False\n                if(prediction==xclass):\n                    e=True\n                    t+=1\n                if(prediction!=xclass):\n                    e=False\n                    f+=1\n                print (i+1,\"The prediction for\",X_DL,\" is:\",\n                    str(prediction).strip('[]'),\n                    \"the class is\",xclass,\"acc.:\",e)\n            print(\"true:\", t, \"false\", f, \"accuracy\",\n                round(t/(t+f),3)) \n    ```", "```py\n    995 The prediction for [[85, 79]]  is: 4 the class is 4 acc.: True\n    996 The prediction for [[103, 100]]  is: 4 the class is 4 acc.: True\n    997 The prediction for [[71, 82]]  is: 5 the class is 1 acc.: False\n    998 The prediction for [[50, 44]]  is: 0 the class is 0 acc.: True\n    999 The prediction for [[62, 51]]  is: 5 the class is 5 acc.: True \n    ```", "```py\nfrom sklearn.ensemble import RandomForestClassifier \n```", "```py\nclf = RandomForestClassifier(n_estimators=25, random_state=None, bootstrap=True) \n```", "```py\n    pp=1 # print information\n    # load dataset\n    col_names = ['f1', 'f2','label']\n    df = pd.read_csv(\"ckmc.csv\", header=None, names=col_names)\n    if pp==1:\n        print(df.head())\n    #loading features and label (classes)\n    feature_cols = ['f1', 'f2']\n    X = df[feature_cols] # Features\n    y = df.label # Target variable\n    if pp==1:\n        print(X)\n        print(y) \n    ```", "```py\n     f1  f2  label\n    0  80  53      5\n    1  18   8      2\n    2  55  38      0\n    3  74  74      5\n    4  17   4      2 \n    ```", "```py\n    [1 5 5 5 2 1 3 3…] \n    ```", "```py\n    #Divide the data into training and testing sets\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, y, test_size=0.2, random_state=0)\n    #Creating Random Forest Classifier and training\n    clf = RandomForestClassifier(n_estimators=25,\n        random_state=0)\n    clf.fit(X_train, y_train) \n    ```", "```py\n    #Predictions\n    y_pred = clf.predict(X_test)\n    if pp==1:\n        print(y_pred)\n    #Metrics\n    ae=metrics.mean_absolute_error(y_test, y_pred)\n    print('Mean Absolute Error:', round(ae,3)) \n    ```", "```py\n    predictions:\n    [1 5 5 5 2 1 3 3 0 5 3 5 3 2 2 4 3 1 3 2 2 …]\n    Mean Absolute Error: 0.165 \n    ```", "```py\n    #Double Check the Model's Accuracy\n    doublecheck=0  # 1=yes, 0=no\n    if doublecheck==1:\n        t=0\n        f=0\n        for i in range(0,1000):\n            xf1=df.at[i,'f1']\n            xf2=df.at[i,'f2']\n            xclass=df.at[i,'label']\n            X_DL = [[xf1,xf2]]\n            prediction =clf.predict(X_DL)\n            e=False\n            if(prediction==xclass):\n                e=True\n                t+=1\n            if(prediction!=xclass):\n                e=False\n                f+=1\n            if pp==1:\n                print (i+1,\"The prediction for\",X_DL,\" is:\",\n                    str(prediction).strip('[]'),\"the class is\",\n                    xclass,\"acc.:\",e)\n        acc=round(t/(t+f),3)\n        print(\"true:\",t,\"false\",f,\"accuracy\",acc)\n        print(\"Absolute Error\",round(1-acc,3)) \n    ```", "```py\nMean Absolute Error: 0.085\ntrue: 994 false 6 accuracy 0.994\nAbsolute Error 0.006 \n```"]