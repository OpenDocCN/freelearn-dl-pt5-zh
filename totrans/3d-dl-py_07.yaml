- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Neural Radiance Fields (NeRF)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, you learned about Differentiable Volume Rendering where
    you reconstructed the 3D volume from several multi-view images. With this technique,
    you modeled a volume consisting of N x N x N voxels. The space requirement for
    storing this volume scale would therefore be O(N3). This is undesirable, especially
    if we want to transmit this information over the network. Other methods can overcome
    such large disk space requirements, but they are prone to smoothing geometry and
    texture. Therefore, we cannot use them to model very complex or textured scenes
    reliably.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are going to discuss a breakthrough new approach to representing
    3D scenes, called **Neural Radiance Fields** (**NeRF**). This is one of the first
    techniques to model a 3D scene that requires less constant disk space and at the
    same time, captures the fine geometry and texture of complex scenes.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, you will learn about the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding NeRF
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a NeRF model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the NeRF model architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding volume rendering with radiance fields
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In order to run the example code snippets in this book, you need to have a
    computer, ideally with a GPU with about 8 GB of GPU memory. Running code snippets
    only using CPUs is not impossible but will be extremely slow. The recommended
    computer configuration is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: A GPU device – for example, Nvidia GTX series or RTX series with at least 8
    GB of memory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python 3.7+
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The PyTorch and PyTorch3D libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code snippets for this chapter can be found at [https://github.com/PacktPublishing/3D-Deep-Learning-with-Python](https://github.com/PacktPublishing/3D-Deep-Learning-with-Python).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding NeRF
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: View synthesis is a long-standing problem in 3D computer vision. The challenge
    is to synthesize new views of a 3D scene using a small number of available 2D
    snapshots of the scene. It is particularly challenging because the view of a complex
    scene can depend on a lot of factors such as object artifacts, light sources,
    reflections, opacity, object surface texture, and occlusions. Any good representation
    should capture this information either implicitly or explicitly. Additionally,
    many objects have complex structures that are not completely visible from a certain
    viewpoint. The challenge is to construct complete information about the world
    given incomplete and noisy information.
  prefs: []
  type: TYPE_NORMAL
- en: As the name suggests, NeRF uses neural networks to model the world. As we will
    learn later in the chapter, NeRF uses neural networks in a very unconventional
    manner. It was a concept first developed by a team of researchers from UC Berkeley,
    Google Research, and UC San Diego. Because of their unconventional use of neural
    networks and the quality of the learned models, it has spawned multiple new inventions
    in the fields of view synthesis, depth sensing, and 3D reconstruction. It is therefore
    a very useful concept to understand as you navigate through the rest of this chapter
    and book.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, first, we will explore the meaning of radiance fields and how
    we can use a neural network to represent these radiance fields.
  prefs: []
  type: TYPE_NORMAL
- en: What is a radiance field?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before we get to NeRF, let us understand what radiance fields are first. You
    see an object when the light from that object is processed by your body’s sensory
    system. The light from the object can either be generated by the object itself
    or reflected off it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Radiance is the standard metric for measuring the amount of light that passes
    through or is emitted from an area inside a particular solid angle. For our purposes,
    we can treat the radiance to be the intensity of a point in space when viewed
    in a particular direction. When capturing this information in RGB, the radiance
    will have three components corresponding to the colors Red, Green, and Blue. The
    radiance of a point in space can depend on many factors, including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Light sources illuminating that point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The existence of a surface (or volume density) that can reflect light at that
    point
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The texture properties of the surface
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following figure depicts the radiance value at a certain point in the 3D
    scene when viewed at a certain angle. The radiance field is just a collection
    of these radiance values at all points and viewing angles in the 3D scene:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1: The radiance (r, g, b) at a point (x, y, z) when viewed with
    certain viewing angles (θ, ∅) ](img/B18217_06_1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.1: The radiance (r, g, b) at a point (x, y, z) when viewed with certain
    viewing angles (θ, ∅)'
  prefs: []
  type: TYPE_NORMAL
- en: If we know the radiance of all the points in a scene in all directions, we have
    all the visual information we need about the scene. This field of radiance values
    constitutes a radiance field. We can store the radiance field information as a
    volume in a 3D voxel grid data structure. We saw this in the previous chapter
    when discussing volume rendering.
  prefs: []
  type: TYPE_NORMAL
- en: Representing radiance fields with neural networks
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, we will explore a new way of using neural networks. In typical
    computer vision tasks, we use neural networks to map an input in the pixel space
    to an output. In the case of a discriminative model, the output is a class label.
    In the case of a generative model, the output is also in the pixel space. A NeRF
    model is neither of these.
  prefs: []
  type: TYPE_NORMAL
- en: NeRF uses a neural network to represent a volumetric scene function. This neural
    network takes a 5-dimensional input. These are the three spatial locations (x,
    y, z) and two viewing angles (θ, ∅). Its output is the volume density σ at (x,
    y, z) and the emitted color (r, g, b) of the point (x, y, z) when viewed from
    the viewing angle (θ, ∅). The emitted color is a proxy used to estimate the radiance
    at that point. In practice, instead of directly using (θ, ∅) to represent the
    viewing angle, NeRF uses the unit direction vector d in the 3D Cartesian coordinate
    system. These are equivalent representations of the viewing angle.
  prefs: []
  type: TYPE_NORMAL
- en: The model therefore maps any point in the 3D scene and a viewing angle to the
    volume density and radiance at that point. You can then use this model to synthesize
    views by querying the 5D coordinates along camera rays and using the volume rendering
    technique you learned about in the previous chapter to project the output colors
    and volume densities into an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the following figure, let us find out how a neural network can be used
    to predict the density and radiance at a certain point (x, y, z) when viewed along
    a certain direction (θ, ∅):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2: The inputs (x, y, z, θ, and ∅) are used to create separate harmonic
    embeddings for the spatial location and viewing angle first, forming the input
    to a neural network, and this neural network outputs the predicted density and
    radiance ](img/B18217_06_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.2: The inputs (x, y, z, θ, and ∅) are used to create separate harmonic
    embeddings for the spatial location and viewing angle first, forming the input
    to a neural network, and this neural network outputs the predicted density and
    radiance'
  prefs: []
  type: TYPE_NORMAL
- en: Note that this is a fully connected network – typically, this is referred to
    as a **Multi-Layer Perceptron** (**MLP**). More importantly, this is not a convolutional
    neural network. We refer to this model as the NeRF model. A single NeRF model
    is optimized on a set of images from a single scene. Therefore, each model only
    knows the scene on which it is optimized. This is not the standard way to use
    a neural network, where we typically need the model to generalize unseen images.
    In the case of NeRF, we need the network to generalize unseen viewpoints well.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know what a NeRF is, let us look at how to use it to render new
    views from it.
  prefs: []
  type: TYPE_NORMAL
- en: Training a NeRF model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we are going to train a simple NeRF model on images generated
    from the synthetic cow model. We are only going to instantiate the NeRF model
    without worrying about how it is implemented. The implementation details are covered
    in the next section. A single neural network (NeRF model) is trained to represent
    a single 3D scene. The following codes can be found in `train_nerf.py`, which
    can be found in this chapter’s GitHub repository. It is modified from a PyTorch3D
    tutorial. Let us go through the code to train a NeRF model on the synthetic cow
    scene:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let us import the standard modules:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let us import the functions and classes used for rendering. These are
    `pytorch3d` data structures:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we need to set up the device:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let us import the utility functions that will let us generate synthetic
    training data and visualize images:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now use these utility functions to generate camera angles, images, and
    silhouettes of the synthetic cow from multiple different angles. This will print
    the number of generated images, silhouettes, and camera angles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we have done in the previous chapter, let us define a ray sampler. We will
    use `MonteCarloRaysampler`. This generates rays from a random subset of pixels
    from the image plane. We need a random sampler here since we want to use a mini-batch
    gradient descent algorithm to optimize the model. This is a standard neural network
    optimization technique. Sampling rays systematically can result in optimization
    bias during each optimization step. This can lead to a worse model and increase
    the model training time. The ray sampler samples points uniformly along the ray:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will define the ray marcher. This uses the volume densities and colors
    of points sampled along the ray and renders the pixel value for that ray. For
    the ray marcher, we use `EmissionAbsorptionRaymarcher`. This implements the classical
    Emission-Absorption ray marching algorithm:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now instantiate `ImplicitRenderer`. This composes the ray sampler and
    the ray marcher into a single data structure:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let us look at the Huber loss function. This is defined in `utils.helper_functions.huber`.
    It is a robust alternative to the mean squared error function and is less sensitive
    to outliers:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now look at a helper function defined in `utils.helper_functions.sample_images_at_mc_loss`
    that is used to extract ground truth pixel values from target images. `MonteCarloRaysampler`
    samples rays passing through some `x` and `y` locations on the image. These are
    in `torch.nn.functional.grid_sample` function. This uses interpolation techniques
    in the background to provide us with accurate pixel values. This is better than
    just mapping NDC coordinates to pixel coordinates and then sampling the one pixel
    that corresponds to an NDC coordinate value. In the NDC coordinate system, `x`
    and `y` both have a range of `[-1, 1]`. For example, (x, y) = (-1, -1) corresponds
    to the top left corner of the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'While training the model, it is always useful to visualize the model output.
    Among many other uses, this will help us course-correct if we see that the model
    outputs are not changing over time. So far, we have used `MonteCarloRaysampler`,
    which is very useful while training the model, but this will not be useful when
    we want to render full images since it randomly samples rays. To view the full
    image, we need to systematically sample rays corresponding to all the pixels in
    the output frame. To achieve this, we are going to use `NDCMultinomialRaysampler`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now instantiate the implicit renderer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to visualize the intermediate training results, let us define a helper
    function that takes the model and camera parameters as its input and compares
    it with the target image and its corresponding silhouette. If the rendered image
    is very large, it might not be possible to fit all the rays into the GPU memory
    at once. Therefore, we need to break them down into batches and run several forward
    passes on the model to get the output. We need to merge the output of the rendering
    into one coherent image. In order to keep it simple, we will just import the function
    here, but the full code is provided in the book’s GitHub repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now instantiate the NeRF model. To keep it simple, we are not presenting
    the model’s class definition here. You can find that in the chapter’s GitHub repository.
    Because the model structure is so important, we are going to discuss it in detail
    in a separate section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let us now prepare to train the model. In order to reproduce the training,
    we should set the random seed used in `torch` to a fixed value. We then need to
    send all the variables to the device used for processing. Since this is a resource-intensive
    computational problem, we should ideally run it on a GPU-enabled machine. Running
    this on a CPU is extremely time-consuming and not recommended:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will now define the hyperparameters used to train the model. `lr` represents
    the learning rate, `n_iter` represents the number of training iterations (or steps),
    and `batch_size` represents the number of random cameras used in a mini-batch.
    The batch size here is chosen according to the GPU memory you have. If you find
    that you are running out of GPU memory, choose a smaller batch size value:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We are now ready to train our model. During each iteration, we should sample
    a mini-batch of cameras randomly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'During each iteration, first, we need to obtain the rendered pixel values and
    rendered silhouettes at the randomly sampled cameras using the NeRF model. These
    are predicted values. This is the forward propagation step. We want to compare
    these predictions to the ground truth to find out the training loss. Our loss
    is a mixture of two loss functions: A, a Huber loss on the predicted silhouette
    and the ground truth silhouette, and B, a Huber loss on the predicted color and
    the ground truth color. Once we obtain the loss value, we can backpropagate through
    the NeRF model and step through the optimizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let us visualize model performance after every 100 iterations. This will help
    us track model progress and terminate it if something unexpected is happening.
    This creates images in the same folder where you run the code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '![Figure 6.3: An intermediate visualization to keep track of model training
    ](img/B18217_06_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.3: An intermediate visualization to keep track of model training'
  prefs: []
  type: TYPE_NORMAL
- en: 'After the optimization is finished, we take the final resulting volumetric
    model and render images from new angles:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, the new rendered images are shown in the figure here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4: Rendered images of the synthetic cow scene that our NeRF model
    learned ](img/B18217_06_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.4: Rendered images of the synthetic cow scene that our NeRF model
    learned'
  prefs: []
  type: TYPE_NORMAL
- en: We trained a NeRF model on a synthetic cow scene in this section. In the next
    section, we will learn more about how the NeRF model is implemented by going through
    the code in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the NeRF model architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have used the NeRF model class without fully knowing what it looks
    like. In this section, we will first visualize what the neural network looks like
    and then go through the code in detail and understand how it is implemented.
  prefs: []
  type: TYPE_NORMAL
- en: 'The neural network takes the harmonic embedding of the spatial location (x,
    y, z) and the harmonic embedding of (θ, ∅) as its input and outputs the predicted
    density σ and the predicted color (r, g, b). The following figure illustrates
    the network architecture that we are going to implement in this section:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5: The simplified model architecture of the NeRF model ](img/B18217_06_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 6.5: The simplified model architecture of the NeRF model'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The model architecture that we are going to implement is different from the
    original NeRF model architecture. In this implementation, we are implementing
    a simplified version of it. This simplified architecture makes it faster and easier
    to train.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us start defining the `NeuralRadianceField` class. We will now go through
    different parts of this class definition. For the full definition of the class,
    please refer to the code in the GitHub repository:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Each input point is a 5-dimensional vector. It was found that training the
    model directly on this input performs poorly when representing high-frequency
    variation in color and geometry. This is because neural networks are known to
    be biased toward learning low-frequency functions. A good solution to this problem
    is to map the input space to a higher dimensional space and use that for training.
    This mapping function is a set of sinusoidal functions with fixed but unique frequencies:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/Formula_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'This function is applied to each of the components of the input vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The neural network consists of an MLP backbone. It takes the embeddings of
    location (x, y, z) as its input. This is a fully connected network and the activation
    function used is `softplus`. The `softplus` function is a smoother version of
    the ReLU activation function. The output of the backbone is a vector with a size
    of `n_hidden_neurons`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define a color layer that takes the output embeddings of the MLP backbone
    along with the ray direction input embeddings and outputs the RGB color of the
    input. We combine these inputs because the color output depends strongly on both
    the location of the point and the direction of viewing and therefore, it is important
    to provide shorter paths to make use of this neural network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we define the `density` layer. The density of a point is just a function
    of its location:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need some function to take the output of `density_layer` and predict
    the raw density:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We do the same for obtaining colors at a certain point given the ray direction.
    We need to apply the positional encoding function to the ray direction input first.
    We should then concatenate it with the output of the MLP backbone:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We define the function for forward propagation. First, we obtain embeddings.
    Then, we pass them through the MLP backbone to obtain a set of features. We then
    use that to obtain the densities. We use the features and the ray directions to
    obtain the color. We return the densities and colors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This function is used to allow for memory-efficient processing of input rays.
    First, the input rays are split into `n_batches` chunks and passed through the
    `self.forward` function one at a time in a `for` loop. Combined with disabling
    PyTorch gradient caching (`torch.no_grad()`), this allows us to render large batches
    of rays that do not all fit into GPU memory in a single forward pass. In our case,
    `batched_forward` is used to export a fully sized render of the radiance field
    for visualization purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'For each batch, we need to run a forward pass first and then extract the `ray_densities`
    and `ray_colors` separately to be returned as the outputs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this section, we went through the implementation of the NeRF model. To get
    a complete understanding of NeRF, we also need to explore the theoretical concepts
    underlying its use in rendering volumes. In the next section, we will explore
    this in more detail.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding volume rendering with radiance fields
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Volume rendering allows you to create a 2D projection of a 3D image or scene.
    In this section, we will learn about rendering a 3D scene from different viewpoints.
    For the purposes of this section, assume that the NeRF model is fully trained
    and that it accurately maps the input coordinates (x, y, z, d­­­x, dy, dz) to
    an output (r, g, b, σ). Here are the definitions of these input and output coordinates:'
  prefs: []
  type: TYPE_NORMAL
- en: '**(x, y, z)**: A point in the 3D scene in the World Coordinates'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**(d­­­x, dy, dz)**: This is a unit vector that represents the direction along
    which we are viewing the point (x, y, z)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**(r, g, b)**: This is the radiance value (or the emitted color) of the point
    (x, y, z)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**σ**: The volume density at the point (x, y, z)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the previous chapter, you came to understand the concepts underlying volumetric
    rendering. You used the technique of ray sampling to get volume densities and
    colors from the volume. We called this volume sampling. In this chapter, we are
    going to use ray sampling on the radiance field to get the volume densities and
    colors. We can then perform ray marching to obtain the color intensities of that
    point. The ray marching technique used in the previous chapter and what is used
    in this chapter are conceptually similar. The difference is that 3D voxels are
    discrete representations of 3D space whereas radiance fields are a continuous
    representation of it (because we use a neural network to encode this representation).
    This slightly changes the way we accumulate color intensities along a ray.
  prefs: []
  type: TYPE_NORMAL
- en: Projecting rays into the scene
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Imagine placing a camera at a viewpoint and pointing it towards the 3D scene
    of interest. This is the scene on which the NeRF model is trained. To synthesize
    a 2D projection of the scene, we first send out of ray into the 3D scene originating
    from the viewpoint.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ray can be parameterized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_06_002.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, r is the ray starting from the origin o and traveling along the direction
    d. It is parametrized by t, which can be varied in order to move to different
    points on the ray. Note that r is a 3D vector representing a point in space.
  prefs: []
  type: TYPE_NORMAL
- en: Accumulating the color of a ray
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can use some well-known classical color rendering techniques to render the
    color of the ray. Before we do that, let us get a feeling for some standard definitions:'
  prefs: []
  type: TYPE_NORMAL
- en: Let us assume that we want to accumulate the color of the ray between tn (the
    near bound) and tf (the far bound). We do not care about the ray outside of these
    bounds.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can think of volume density σ(r(t)) as the probability that the ray terminates
    at an infinitesimal point around r(t).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can think of ![](img/Formula_06_003.png) as the color at the point r(t) on
    the ray when viewed in along direction d.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_06_004.png) will measure the accumulated volume density between
    tn and some point t.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_06_005.png) will provide us with a notion of accumulated transmittance
    along the ray from tn to some point t. The higher the accumulated volume density,
    the lower the accumulated transmittance to the point t.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The expected color of the ray can now be defined as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_06_006.png)'
  prefs: []
  type: TYPE_IMG
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The volume density σ(r(t)) is a function of the point r(t). Most notably, this
    does not depend on the direction vector d. This is because volume density is a
    function of the physical location at which the point is located. The color ![](img/Formula_06_007.png)
    is a function of both the point r(t) and the ray direction d. This is because
    the same point in space can have different colors when viewed from different directions.
  prefs: []
  type: TYPE_NORMAL
- en: Our NeRF model is a continuous function representing the radiance field of the
    scene. We can use it to obtain c(r(t), d) and σ(r(t)) at various points along
    the ray. There are many techniques for numerically estimating the integral C(r).
    While training and visualizing the outputs of the NeRF model, we used the standard
    `EmissionAbsorptionRaymarcher` method to accumulate the radiance along a ray.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we came to understand how a neural network can be used to model
    and represent a 3D scene. This neural network is called the NeRF model. We then
    trained a simple NeRF model on a synthetic 3D scene. We then dug deeper into the
    NeRF model architecture and its implementation in code. We also understood the
    main components of the model. We then understood the principles behind rendering
    volumes with the NeRF model. The NeRF model is used to capture a single scene.
    Once we build this model, we can use it to render that 3D scene from different
    angles. It is logical to wonder whether there is a way to capture multiple scenes
    with a single model and whether we can predictably manipulate certain objects
    and attributes in the scene. This is our topic of exploration in the next chapter
    where we will explore the GIRAFFE model.
  prefs: []
  type: TYPE_NORMAL
- en: 'PART 3: State-of-the-art 3D Deep Learning Using PyTorch3D'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This part of the book will be all about using PyTorch3D to implement state-of-the-art
    3D deep learning models and algorithms. 3D computer vision technologies are making
    rapid progress in recent times and we will learn how to implement and use these
    state-of-the-art 3D deep learning models in the best way possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part includes the following chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B18217_07.xhtml#_idTextAnchor094), *Exploring Controllable Neural
    Feature Fields*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B18217_08.xhtml#_idTextAnchor108)*, Modeling the Human Body in
    3D*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B18217_09.xhtml#_idTextAnchor124)*, Performing End-to-End View
    Synthesis with SynSin*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B18217_10.xhtml#_idTextAnchor134)*, Mesh R-CNN*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
