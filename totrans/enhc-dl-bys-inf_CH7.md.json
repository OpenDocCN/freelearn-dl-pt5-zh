["```py\n\nimport tensorflow as tf \nimport numpy as np \nimport matplotlib.pyplot as plt \nimport tensorflow_probability as tfp \nfrom sklearn.metrics import accuracy_score, mean_squared_error \nfrom sklearn.datasets import fetch_california_housing, load_diabetes \nfrom sklearn.model_selection import train_test_split \nimport seaborn as sns \nimport pandas as pd \nimport os \n\nfrom bayes_by_backprop import BBBRegressor \nfrom pbp import PBP \nfrom mc_dropout import MCDropout \nfrom ensemble import Ensemble \nfrom bdl_ablation_data import load_wine_quality, load_concrete \nfrom bdl_metrics import likelihood\n```", "```py\n\ndatasets = { \n\"california_housing\": fetch_california_housing(return_X_y=True, as_frame=True), \n\"diabetes\": load_diabetes(return_X_y=True, as_frame=True), \n\"wine_quality\": load_wine_quality(), \n\"concrete\": load_concrete(), \n}\n```", "```py\n\nmodels = { \n\"BBB\": BBBRegressor, \n\"PBP\": PBP, \n\"MCDropout\": MCDropout, \n\"Ensemble\": Ensemble, \n}\n```", "```py\n\nresults = { \n\"LL\": [], \n\"MSE\": [], \n\"Method\": [], \n\"Dataset\": [], \n}\n```", "```py\n\n# Parameters \nepochs = 10 \nbatch_size = 16 \nlogdir_base = \"profiling\"\n```", "```py\n\nfor dataset_key in datasets.keys(): \nX, y = datasets[dataset_key] \nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33) \n    ...\n```", "```py\n\n... \nfor model_key in models.keys(): \nlogdir = os.path.join(logdir_base, model_key + \"_train\") \nos.makedirs(logdir, exist_ok=True) \ntf.profiler.experimental.start(logdir) \n    ...\n```", "```py\n\n... \nmodel = models[model_key]() \n        model.fit(X_train, y_train, batch_size=batch_size, n_epochs=epochs)\n```", "```py\n\n... \ntf.profiler.experimental.stop() \nlogdir = os.path.join(logdir_base, model_key + \"_predict\") \nos.makedirs(logdir, exist_ok=True) \ntf.profiler.experimental.start(logdir) \n        ...\n```", "```py\n\n... \ny_divd, y_var = model.predict(X_test) \n\ntf.profiler.experimental.stop() \n\ny_divd = y_divd.reshape(-1) \ny_var = y_var.reshape(-1) \n\nmse = mean_squared_error(y_test, y_divd) \nll = likelihood(y_test, y_divd, y_var) \nresults[\"MSE\"].append(mse) \nresults[\"LL\"].append(ll) \nresults[\"Method\"].append(model_key) \nresults[\"Dataset\"].append(dataset_key) \ntf.keras.backend.clear_session() \n...\n```", "```py\n\n... \nresults = pd.DataFrame(results)\n```", "```py\n\nresults['NLL'] = -1*results['LL'] \n\ni = 1 \nfor dataset in datasets.keys(): \nfor metric in [\"NLL\", \"MSE\"]: \ndf_plot = results[(results['Dataset']==dataset)] \ndf_plot = groupedvalues = df_plot.groupby('Method').sum().reset_index() \nplt.subplot(3,2,i) \nax = sns.barplot(data=df_plot, x=\"Method\", y=metric) \nfor index, row in groupedvalues.iterrows(): \nif metric == \"NLL\": \nax.text(row.name, 0, round(row.NLL, 2), \ncolor='white', ha='center') \nelse: \nax.text(row.name, 0, round(row.MSE, 2), \ncolor='white', ha='center') \nplt.title(dataset) \nif metric == \"NLL\" and dataset == \"california_housing\": \nplt.ylim(0, 100) \ni+=1 \nfig = plt.gcf() \nfig.set_size_inches(10, 8) \nplt.tight_layout()\n```", "```py\n\ntensorboard --logdir profiling/BBB_train/\n```", "```py\n\nimport matplotlib.pyplot as plt \nimport numpy as np \nimport pandas as pd \nimport tensorflow as tf \nimport tensorflow_probability as tfp \nimport tensorflow_datasets as tfds\n```", "```py\n\nds = tfds.load('diamonds', split='train') \ndf = tfds.as_dataframe(ds)\n```", "```py\n\ndf = df[[\"features/carat\", \"price\"]]\n```", "```py\n\ntrain_df = df.sample(frac=0.8, random_state=0) \ntest_df = df.drop(train_df.index)\n```", "```py\n\ncarat = np.array(train_df['features/carat']) \nprice = np.array(train_df['price']) \ncarat_test = np.array(test_df['features/carat']) \nprice_test = np.array(test_df['price'])\n```", "```py\n\nNUM_TRAIN_SAMPLES = carat.shape[0]\n```", "```py\n\ndef plot_scatter(x_data, y_data, x_hat=None, y_hats=None, plot_std=False): \n# Plot the data as scatter points \nplt.scatter(x_data, y_data, color=\"k\", label=\"Data\") \n# Plot x and y values predicted by the model, if provided \nif x_hat is not None and y_hats is not None: \nif not isinstance(y_hats, list): \ny_hats = [y_hats] \nfor ind, y_hat in enumerate(y_hats): \nplt.plot( \nx_hat, \ny_hat.mean(), \ncolor=\"#e41a1c\", \nlabel=\"prediction\" if ind == 0 else None, \n) \n# Plot standard deviation, if requested \nif plot_std: \nfor ind, y_hat in enumerate(y_hats): \nplt.plot( \nx_hat, \ny_hat.mean() + 2 * y_hat.stddev(), \ncolor=\"#e41a1c\", \nlinestyle=\"dashed\", \nlabel=\"prediction + stddev\" if ind == 0 else None, \n) \nplt.plot( \nx_hat, \ny_hat.mean() - 2 * y_hat.stddev(), \ncolor=\"#e41a1c\", \nlinestyle=\"dashed\", \nlabel=\"prediction - stddev\" if ind == 0 else None, \n) \n# Plot x- and y-axis labels as well as a legend \nplt.xlabel(\"carat\") \nplt.ylabel(\"price\") \n    plt.legend()\n```", "```py\n\nplot_scatter(carat, price)\n```", "```py\n\nnormalizer = tf.keras.layers.Normalization(input_shape=(1,), axis=None) \nnormalizer.adapt(carat)\n```", "```py\n\ndef negloglik(y_true, y_divd): \n    return -y_divd.log_prob(y_true)\n```", "```py\n\nmodel = tf.keras.Sequential( \n[ \nnormalizer, \ntf.keras.layers.Dense(32, activation=\"relu\"), \ntf.keras.layers.Dense(1), \ntfp.layers.DistributionLambda( \nlambda t: tfp.distributions.Normal(loc=t, scale=1) \n), \n] \n)\n```", "```py\n\n# Compile \nmodel.compile(optimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik) \n# Fit \nmodel.fit(carat, price, epochs=100, verbose=0)\n```", "```py\n\n# Define range for model input \ncarat_hat = tf.linspace(carat_test.min(), carat_test.max(), 100) \n# Obtain model's price predictions on test data \nprice_hat = model(carat_hat) \n# Plot test data and model predictions \nplot_scatter(carat_test, price_test, carat_hat, price_hat)\n```", "```py\n\nmodel_aleatoric = tf.keras.Sequential( \n[ \nnormalizer, \ntf.keras.layers.Dense(32, activation=\"relu\"), \ntf.keras.layers.Dense(2), \ntfp.layers.DistributionLambda( \nlambda t: tfp.distributions.Normal( \nloc=t[..., :1], scale=1e-3 + tf.math.softplus(0.05 * t[..., 1:]) \n) \n), \n] \n)\n```", "```py\n\n# Compile \nmodel_aleatoric.compile( \noptimizer=tf.optimizers.Adam(learning_rate=0.05), loss=negloglik \n) \n# Fit \nmodel_aleatoric.fit(carat, price, epochs=100, verbose=0)\n```", "```py\n\ncarat_hat = tf.linspace(carat_test.min(), carat_test.max(), 100) \nprice_hat = model_aleatoric(carat_hat) \nplot_scatter( \ncarat_test, price_test, carat_hat, price_hat, plot_std=True, \n)\n```", "```py\n\ndef prior(kernel_size, bias_size=0, dtype=None): \nn = kernel_size + bias_size \nreturn tf.keras.Sequential( \n[ \ntfp.layers.VariableLayer(n, dtype=dtype), \ntfp.layers.DistributionLambda( \nlambda t: tfp.distributions.Independent( \ntfp.distributions.Normal(loc=t, scale=1), \nreinterpreted_batch_ndims=1, \n) \n), \n] \n    )\n```", "```py\n\ndef posterior(kernel_size, bias_size=0, dtype=None): \nn = kernel_size + bias_size \nc = np.log(np.expm1(1.0)) \nreturn tf.keras.Sequential( \n[ \ntfp.layers.VariableLayer(2 * n, dtype=dtype), \ntfp.layers.DistributionLambda( \nlambda t: tfp.distributions.Independent( \ntfp.distributions.Normal( \nloc=t[..., :n], \nscale=1e-5 + tf.nn.softplus(c + t[..., n:]), \n), \nreinterpreted_batch_ndims=1, \n) \n), \n] \n    )\n```", "```py\n\ndef build_epistemic_model(): \nmodel = tf.keras.Sequential( \n[ \nnormalizer, \ntfp.layers.DenseVariational( \n32, \nmake_prior_fn=prior, \nmake_posterior_fn=posterior, \nkl_weight=1 / NUM_TRAIN_SAMPLES, \nactivation=\"relu\", \n), \ntfp.layers.DenseVariational( \n1, \nmake_prior_fn=prior, \nmake_posterior_fn=posterior, \nkl_weight=1 / NUM_TRAIN_SAMPLES, \n), \ntfp.layers.DistributionLambda( \nlambda t: tfp.distributions.Normal(loc=t, scale=1) \n), \n] \n) \n  return model\n```", "```py\n\ncarat_subset = carat[:500] \nprice_subset = price[:500]\n```", "```py\n\n# Build \nmodel_epistemic = build_epistemic_model() \n# Compile \nmodel_epistemic.compile( \noptimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik \n) \n# Fit \nmodel_epistemic.fit(carat_subset, price_subset, epochs=100, verbose=0)\n```", "```py\n\ncarat_hat = tf.linspace(carat_test.min(), carat_test.max(), 100) \nprice_hats = [model_epistemic(carat_hat) for _ in range(10)] \nplot_scatter( \ncarat_test, price_test, carat_hat, price_hats, \n)\n```", "```py\n\n# Build \nmodel_epistemic_full = build_epistemic_model() \n# Compile \nmodel_epistemic_full.compile( \noptimizer=tf.optimizers.Adam(learning_rate=0.01), loss=negloglik \n) \n# Fit \nmodel_epistemic_full.fit(carat, price, epochs=100, verbose=0)\n```", "```py\n\ncarat_hat = tf.linspace(carat_test.min(), carat_test.max(), 100) \nprice_hats = [model_epistemic_full(carat_hat) for _ in range(10)] \nplot_scatter( \ncarat_test, price_test, carat_hat, price_hats, \n)\n```", "```py\n\n# Build model. \nmodel_epistemic_aleatoric = tf.keras.Sequential( \n[ \nnormalizer, \ntfp.layers.DenseVariational( \n32, \nmake_prior_fn=prior, \nmake_posterior_fn=posterior, \nkl_weight=1 / NUM_TRAIN_SAMPLES, \nactivation=\"relu\", \n), \ntfp.layers.DenseVariational( \n1 + 1, \nmake_prior_fn=prior, \nmake_posterior_fn=posterior, \nkl_weight=1 / NUM_TRAIN_SAMPLES, \n), \ntfp.layers.DistributionLambda( \nlambda t: tfp.distributions.Normal( \nloc=t[..., :1], scale=1e-3 + tf.math.softplus(0.05 * t[..., 1:]) \n) \n), \n] \n)\n```", "```py\n\nimport tensorflow as tf \nimport tensorflow_probability as tfp \nimport matplotlib.pyplot as plt \nimport numpy as np \nfrom sklearn.utils import shuffle \nfrom sklearn.metrics import roc_auc_score \nimport ddu_dirty_mnist \nfrom scipy.stats import entropy \ntfd = tfp.distributions\n```", "```py\n\ndirty_mnist_train = ddu_dirty_mnist.DirtyMNIST( \n\".\", \ntrain=True, \ndownload=True, \nnormalize=False, \nnoise_stddev=0 \n) \n\n# regular MNIST \ntrain_imgs = dirty_mnist_train.datasets[0].data.numpy() \ntrain_labels = dirty_mnist_train.datasets[0].targets.numpy() \n# AmbiguousMNIST \ntrain_imgs_amb = dirty_mnist_train.datasets[1].data.numpy() \ntrain_labels_amb = dirty_mnist_train.datasets[1].targets.numpy()\n```", "```py\n\ntrain_imgs, train_labels = shuffle( \nnp.concatenate([train_imgs, train_imgs_amb]), \nnp.concatenate([train_labels, train_labels_amb]) \n) \ntrain_imgs = np.expand_dims(train_imgs[:, 0, :, :], -1) \ntrain_labels = tf.one_hot(train_labels, 10)\n```", "```py\n\n(test_imgs, test_labels) = tf.keras.datasets.mnist.load_data()[1] \ntest_imgs = test_imgs / 255\\. \ntest_imgs = np.expand_dims(test_imgs, -1) \ntest_labels = tf.one_hot(test_labels, 10)\n```", "```py\n\nkl_divergence_function = lambda q, p, _: tfd.kl_divergence(q, p) / tf.cast( \n60000, dtype=tf.float32 \n) \n\nmodel = tf.keras.models.Sequential( \n[ \n*block(5), \n*block(16), \n*block(120, max_pool=False), \ntf.keras.layers.Flatten(), \ntfp.layers.DenseFlipout( \n84, \nkernel_divergence_fn=kl_divergence_function, \nactivation=tf.nn.relu, \n), \ntfp.layers.DenseFlipout( \n10, \nkernel_divergence_fn=kl_divergence_function, \nactivation=tf.nn.softmax, \n), \n] \n)\n```", "```py\n\ndef block(filters: int, max_pool: bool = True): \nconv_layer =  tfp.layers.Convolution2DFlipout( \nfilters, \nkernel_size=5, \npadding=\"same\", \nkernel_divergence_fn=kl_divergence_function, \nactivation=tf.nn.relu) \nif not max_pool: \nreturn (conv_layer,) \nmax_pool = tf.keras.layers.MaxPooling2D( \npool_size=[2, 2], strides=[2, 2], padding=\"same\" \n) \n    return conv_layer, max_pool\n```", "```py\n\nmodel.compile( \ntf.keras.optimizers.Adam(), \nloss=\"categorical_crossentropy\", \nmetrics=[\"accuracy\"], \nexperimental_run_tf_function=False, \n) \nmodel.fit( \nx=train_imgs, \ny=train_labels, \nvalidation_data=(test_imgs, test_labels), \nepochs=50 \n)\n```", "```py\n\n(_, _), (ood_imgs, _) = tf.keras.datasets.fashion_mnist.load_data() \nood_imgs = np.expand_dims(ood_imgs / 255., -1) \n\nambiguous_mnist_test = ddu_dirty_mnist.AmbiguousMNIST( \n\".\", \ntrain=False, \ndownload=True, \nnormalize=False, \nnoise_stddev=0 \n) \namb_imgs = ambiguous_mnist_test.data.numpy().reshape(60000, 28, 28, 1)[:10000] \namb_labels = tf.one_hot(ambiguous_mnist_test.targets.numpy(), 10).numpy()\n```", "```py\n\ndivds_id = [] \ndivds_ood = [] \ndivds_amb = [] \nfor _ in range(50): \ndivds_id.append(model(test_imgs)) \ndivds_ood.append(model(ood_imgs)) \ndivds_amb.append(model(amb_imgs)) \n# format data such that we have it in shape n_images, n_predictions, n_classes \ndivds_id = np.moveaxis(np.stack(divds_id), 0, 1) \ndivds_ood = np.moveaxis(np.stack(divds_ood), 0, 1) \ndivds_amb = np.moveaxis(np.stack(divds_amb), 0, 1)\n```", "```py\n\ndef total_uncertainty(divds: np.ndarray) -*>* np.ndarray: \nreturn entropy(np.mean(divds, axis=1), axis=-1) \n\ndef data_uncertainty(divds: np.ndarray) -*>* np.ndarray: \nreturn np.mean(entropy(divds, axis=2), axis=-1) \n\ndef knowledge_uncertainty(divds: np.ndarray) -*>* np.ndarray: \n    return total_uncertainty(divds) - data_uncertainty(divds)\n```", "```py\n\nlabels = [\"In-distribution\", \"Out-of-distribution\", \"Ambiguous\"] \nuncertainty_functions = [total_uncertainty, data_uncertainty, knowledge_uncertainty] \nfig, axes = plt.subplots(1, 3, figsize=(20,5)) \nfor ax, uncertainty in zip(axes, uncertainty_functions): \nfor scores, label in zip([divds_id, divds_ood, divds_amb], labels): \nax.hist(uncertainty(scores), bins=20, label=label, alpha=.8) \nax.title.set_text(uncertainty.__name__.replace(\"_\", \" \").capitalize()) \nax.legend(loc=\"upper right\") \nplt.legend() \nplt.savefig(\"uncertainty_types.png\", dpi=300) \nplt.show()\n```", "```py\n\ndef auc_id_and_amb_vs_ood(uncertainty): \nscores_id = uncertainty(divds_id) \nscores_ood = uncertainty(divds_ood) \nscores_amb = uncertainty(divds_amb) \nscores_id = np.concatenate([scores_id, scores_amb]) \nlabels = np.concatenate([np.zeros_like(scores_id), np.ones_like(scores_ood)]) \nreturn roc_auc_score(labels, np.concatenate([scores_id, scores_ood])) \n\nprint(f\"{auc_id_and_amb_vs_ood(total_uncertainty)=:.2%}\") \nprint(f\"{auc_id_and_amb_vs_ood(knowledge_uncertainty)=:.2%}\") \nprint(f\"{auc_id_and_amb_vs_ood(data_uncertainty)=:.2%}\") \n# output: \n# auc_id_and_amb_vs_ood(total_uncertainty)=91.81% \n# auc_id_and_amb_vs_ood(knowledge_uncertainty)=98.87% \n# auc_id_and_amb_vs_ood(data_uncertainty)=84.29%\n```", "```py\n\ndef auc_id_vs_amb(uncertainty): \nscores_id, scores_amb = uncertainty(divds_id), uncertainty(divds_amb) \nlabels = np.concatenate([np.zeros_like(scores_id), np.ones_like(scores_amb)]) \nreturn roc_auc_score(labels, np.concatenate([scores_id, scores_amb])) \n\nprint(f\"{auc_id_vs_amb(total_uncertainty)=:.2%}\") \nprint(f\"{auc_id_vs_amb(knowledge_uncertainty)=:.2%}\") \nprint(f\"{auc_id_vs_amb(data_uncertainty)=:.2%}\") \n# output: \n# auc_id_vs_amb(total_uncertainty)=94.71% \n# auc_id_vs_amb(knowledge_uncertainty)=87.06% \n# auc_id_vs_amb(data_uncertainty)=95.21%\n```"]