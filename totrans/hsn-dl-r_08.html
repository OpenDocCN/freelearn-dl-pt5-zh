<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Neural Collaborative Filtering Using Embeddings</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, you learned how to implement a <strong>multilayer perceptron</strong> (<strong>MLP</strong>) neural network for signal detection.</p>
<p>In this chapter, you will explore how to build a recommender system using collaborative filtering with neural network-based embeddings. We will briefly introduce recommender systems and then proceed from concept to implementation. Specifically, you will learn how to use the custom Keras API to construct a neural network-based recommender system with embedded layers to predict user ratings.</p>
<p>This chapter covers the following topics:</p>
<ul>
<li>Introducing recommender systems</li>
<li class="CDPAlignLeft CDPAlign" style="font-weight: 400">Collaborative filtering with neural networks</li>
<li class="CDPAlignLeft CDPAlign" style="font-weight: 400">Preparing, preprocessing, and exploring data</li>
<li>Performing exploratory data analysis</li>
<li class="CDPAlignLeft CDPAlign" style="font-weight: 400">Creating user and item embeddings</li>
<li>Building and training a neural recommender system </li>
<li class="CDPAlignLeft CDPAlign" style="font-weight: 400">Evaluating results and tuning hyperparameters</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>We will use the Keras (TensorFlow API) library in this chapter.</p>
<p>We will be using the <span><kbd>steam200k.csv</kbd> dataset. </span>This dataset was generated from publicly available Steam data, which is one of the world's most popular gaming hubs. This data contains a list of items (<kbd>game-title</kbd>), users (<kbd>user-id</kbd>), and two user behaviors (<kbd>own</kbd> and <kbd>value</kbd>), where <kbd>value</kbd> represents the number of hours played for each game. You can find the dataset at: <a href="https://www.kaggle.com/tamber/steam-video-games/version/1#steam-200k.csv">https://www.kaggle.com/tamber/steam-video-games/version/1#steam-200k.csv</a>.</p>
<p><span>You can find the code files of this chapter on</span><span> GitHub: </span><a href="https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R">https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R</a><span>.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Introducing recommender systems</h1>
                </header>
            
            <article>
                
<p>Recommender systems are information filtering systems designed to generate accurate and relevant item suggestions for users based on available data. Netflix, Amazon, YouTube, and Spotify are some popular services with recommender systems in commercial use today.</p>
<p>There are three primary types of recommender systems:</p>
<ul>
<li style="font-weight: 400"><strong>Collaborative filtering</strong>: Item recommendations reflect personalized preferences based on similarity to other users. Preferences can be <strong>explicit</strong> (item ratings) or <strong>implicit</strong> (item ratings per user-item interactions such as views, purchases, and so on).</li>
<li style="font-weight: 400"><strong>Content-based filtering</strong>: Item recommendations reflect contextual factors such as item attributes or user demographics; item suggestions can also use temporal factors such as location, date, and time where applicable.</li>
<li style="font-weight: 400"><strong>Hybrid</strong>: Item recommendations combine a variety (ensemble) of collaborative and content-based filtering methods, which have been used in notable competitions such as the Netflix Prize (2009).</li>
</ul>
<div class="packt_tip">See <a href="https://www.netflixprize.com/">https://www.netflixprize.com</a> for historical details about the Netflix Prize (2009) and various recommender system approaches.</div>
<p>Recommender systems often use data in the form of a sparse matrix of users and the items you wish to recommend to them. As its name suggests, a sparse matrix is a matrix whose data elements primarily comprise zero values.</p>
<p>Many recommender system algorithms seek to fill in a user-item interaction matrix with item suggestions based on various types of interactions between users and items. If there is no item preference or user interaction data available, this is frequently referred to as a <strong>cold start problem</strong>, which can be addressed with hybrid methods (collaborative and content-based filtering), contextual models (temporal, demographic, and metadata), as well as random item and feedback sampling strategies, among others. While these interventions are beyond the scope of this chapter, it is important to be aware of the diverse, experimental, and rapidly evolving types of techniques available.</p>
<p>For purposes of illustration, we will focus our attention on collaborative filtering, which is a popular technique that generates recommendations based on user-item interactions. Moreover, collaborative filtering is particularly suitable for our user-item dataset. I<span>n the absence of explicit ratings such as user-item preferences (for example, 1 to 5, and like or dislike), we will create</span> implicit preferences of user-item ratings based on the hours of gameplay, which is available in our dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Collaborative filtering with neural networks</h1>
                </header>
            
            <article>
                
<p><strong>Collaborative filtering</strong> (<strong>CF</strong>) is a core method used by recommender systems to filter suggestions by collecting and analyzing preferences about other similar users. CF techniques use available information and preference pattern data to make predictions (filters) about a particular user's interests.</p>
<p>The collaborative aspect of CF is associated with the notion that relevant recommendations are derived from other user preferences. CF also assumes that two individuals with similar preferences are more likely to share preferences for a particular item than two other individuals selected at random. Accordingly, the primary task of CF is to generate item suggestions (predictions) based on other (collaborative) similar users within the system.</p>
<p>To identify similar users and find ratings (preferences) of unrated items, recommender systems <em>typically</em> need an index of similarity between users and user-item preferences based on available input data. Traditional memory-based approaches include calculating similarity using distance metrics (cosine similarity, Jaccard), correlations (Pearson), or taking a weighted average of user preferences. Other machine learning approaches to determine user-item preferences of unrated items include generalized matrix factorization methods such as <strong>Principal Component Analysis</strong> (<strong>PCA</strong>), <strong>Singular Value Decomposition</strong> (<strong>SVD</strong>), and deep learning matrix factorization, among others.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring embeddings</h1>
                </header>
            
            <article>
                
<p><span>Broadly speaking, deep neural networks seek to minimize the loss (error) associated with non-linear data representations used for learning important features from input data. </span></p>
<p>In addition to traditional dimensionality reduction methods such as clustering and KNN or matrix factorization (PCA, clustering, and other probabilistic techniques), recommender systems can use neural network embeddings to support dimensionality reduction and distributed, non-linear data representations in scalable and efficient ways.</p>
<p><strong>Embeddings</strong> are low-dimensional representations (vectors) of continuous numbers learned from representations (vectors) of discrete input variables in neural networks. </p>
<p>Neural network embeddings offer several advantages such as the following:</p>
<ul>
<li style="font-weight: 400">Reduced computational time and costs (scalability)</li>
<li style="font-weight: 400">Decreased amount of input data required for some learning activation functions (sparsity)</li>
<li style="font-weight: 400">Representations of complex, non-linear relationships (flexibility)</li>
<li style="font-weight: 400">Automated feature importance and selection (efficiency)</li>
</ul>
<p>Let's take a look at an introductory example of how to prepare data in order to implement collaborative filtering using neural networks with embeddings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing, preprocessing, and exploring data</h1>
                </header>
            
            <article>
                
<p><span>Before we build a model, we need to first explore the input data to understand what is available for user-item recommendations.</span> In this section, we will prepare, process, and explore the <span>data, which includes users, items (games), and interactions (hours of gameplay), using the following steps:</span></p>
<ol>
<li>First, let's load some R packages for preparing and processing our input data:</li>
</ol>
<pre style="padding-left: 60px">library(keras)<br/>library(tidyverse)<br/>library(knitr)</pre>
<ol start="2">
<li>Next, let's load the data into R:</li>
</ol>
<pre style="padding-left: 60px">steamdata &lt;- read_csv("data/steam-200k.csv", col_names=FALSE)</pre>
<ol start="3">
<li>Let's inspect the input data using <kbd>glimpse()</kbd>:</li>
</ol>
<pre style="padding-left: 60px">glimpse(steamdata)</pre>
<p style="padding-left: 60px">This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-567 image-border" src="assets/fa9b5a1b-a7f1-4b6a-bd4a-bbebbf8df07e.png" style="width:36.25em;height:8.75em;"/></p>
<ol start="4">
<li><span>Let's manually add</span> column labels to organize this data:</li>
</ol>
<pre style="padding-left: 60px">colnames(steamdata) &lt;- c("user", "item", "interaction", "value", "blank")</pre>
<ol start="5">
<li>Let's remove any blank columns or extraneous whitespace characters:</li>
</ol>
<pre style="padding-left: 60px">steamdata &lt;- steamdata %&gt;% <br/>  filter(interaction == "play") %&gt;%<br/>  select(-blank) %&gt;%<br/>  select(-interaction) %&gt;% <br/>  mutate(item = str_replace_all(item,'[ [:blank:][:space:] ]',""))</pre>
<ol start="6">
<li>Now, we need to create sequential user and item IDs <span>so we can later specify an appropriate size for our lookup matrix via the following code:</span></li>
</ol>
<pre style="padding-left: 60px">users &lt;- steamdata %&gt;% select(user) %&gt;% distinct() %&gt;% rowid_to_column()<br/>steamdata &lt;- steamdata %&gt;% inner_join(users) %&gt;% rename(userid=rowid)<br/><br/>items &lt;- steamdata %&gt;% select(item) %&gt;% distinct() %&gt;% rowid_to_column()<br/>steamdata &lt;- steamdata %&gt;% inner_join(items) %&gt;% rename(itemid=rowid)</pre>
<ol start="7">
<li><span>Let's rename the <kbd>item</kbd> and <kbd>value</kbd> fields to clarify we are exploring user-item interaction data and implicitly defining user ratings based on the <kbd>value</kbd> field, which represents the total number of hours played for a particular game:</span></li>
</ol>
<pre style="padding-left: 60px">steamdata &lt;- steamdata %&gt;% rename(title=item, rating=value)</pre>
<ol start="8">
<li><span>This dataset contains user, item, and interaction data.</span> Let's use the following code to identify the number of users and items available for analysis:</li>
</ol>
<pre style="padding-left: 60px">n_users &lt;- steamdata %&gt;% select(userid) %&gt;% distinct() %&gt;% nrow()<br/>n_items &lt;- steamdata %&gt;% select(itemid) %&gt;% distinct() %&gt;% nrow()</pre>
<p style="padding-left: 60px"><span>We identified there are 11,350 users (players) and 3,598 items (games) to explore for analysis and recommendations. </span><span>Since we don't have explicit item ratings (yes/no, 1-5, for example), we will generate item (game) recommendations based on implicit feedback (hours of gameplay) for illustration purposes. </span><span>Alternatively, we could seek to acquire additional user-item data (such as contextual, temporal, or content), but we have enough baseline item-interaction data to build our preliminary CF-based recommender system with neural network embeddings.</span></p>
<ol start="9">
<li>Before proceeding, we need to normalize our rating (user-item interaction) data, which can be implemented using standard techniques such as min-max normalization:</li>
</ol>
<pre style="padding-left: 60px"># normalize data with min-max function<br/>minmax &lt;- function(x) {<br/>  return ((x - min(x)) / (max(x) - min(x)))<br/>}<br/><br/># add scaled rating value<br/>steamdata &lt;- steamdata %&gt;% mutate(rating_scaled = minmax(rating))</pre>
<ol start="10">
<li>Next, we will split the data into training and test data:</li>
</ol>
<pre style="padding-left: 60px"># split into training and test<br/>index &lt;- sample(1:nrow(steamdata), 0.8* nrow(steamdata))<br/>train &lt;- steamdata[index,] <br/>test &lt;- steamdata[-index,] </pre>
<ol start="11">
<li>Now we will create matrices of users, items, and ratings for the training and test data:</li>
</ol>
<pre style="padding-left: 60px"># create matrices of user, items, and ratings for training and test <br/>x_train &lt;- train %&gt;% select(c(userid, itemid)) %&gt;% as.matrix()<br/>y_train &lt;- train %&gt;% select(rating_scaled) %&gt;% as.matrix()<br/>x_test &lt;- test %&gt;% select(c(userid, itemid)) %&gt;% as.matrix()<br/>y_test &lt;- test %&gt;% select(rating_scaled) %&gt;% as.matrix()</pre>
<p><span>Prior to building our neural network models, we will first conduct <strong>Exploratory data analysis</strong></span> (<span><strong>EDA</strong></span>) <span>to better understand the scope, type, and characteristics of the underlying data.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Performing exploratory data analysis</h1>
                </header>
            
            <article>
                
<p>Recommender systems seek to use <span>available information and preference pattern data to generate predictions about a particular user's interests.</span></p>
<p><span>As a starting point, we can use EDA to identify important patterns and trends in the underlying data to inform our understanding and subsequent analysis: </span></p>
<ol>
<li>Let's identify the top 10 items based on implicit ratings constructed from user-item interaction data using the following code:</li>
</ol>
<pre style="padding-left: 60px"># user-item interaction exploratory data analysis (EDA)<br/>item_interactions &lt;- aggregate(<br/>    rating ~ title, data = steamdata, FUN = 'sum')<br/>item_interactions &lt;- item_interactions[<br/>    order(item_interactions$rating, decreasing = TRUE),]<br/>item_top10 &lt;- head(item_interactions, 10)<br/>kable(item_top10)</pre>
<p style="padding-left: 60px"><span><em>Dota 2</em> is the most popular item (game) by collective hours played:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-533 image-border" src="assets/e104ac05-7f83-4e51-9df8-abed47b00ab7.png" style="width:34.92em;height:19.00em;"/></p>
<ol start="2">
<li>Let's produce some summary statistics of user-item interactions to identify insights with the following code:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># average gamplay<br/>steamdata %&gt;% summarise(avg_gameplay = mean(rating))<br/><br/># median gameplay<br/>steamdata %&gt;% summarise(median_gameplay = median(rating))<br/><br/># top game by individual hours played<br/>topgame &lt;- steamdata %&gt;% arrange(desc(rating)) %&gt;% top_n(1,rating)<br/><br/># show top game by individual hours played<br/>kable(topgame)</pre>
<p style="padding-left: 60px">According to this exploratory analysis, Sid Meier's <em>Civilization V</em> is the most popular game by individual hours played:</p>
<p class="CDPAlignCenter CDPAlign"> <img class="alignnone size-full wp-image-669 image-border" src="assets/fd549d49-a53c-4123-9a22-cc43f45c77cc.png" style="width:51.00em;height:5.92em;"/></p>
<ol start="3">
<li class="CDPAlignLeft CDPAlign">Now, let's identify and visualize the top 10 games by hours played:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># top 10 games by hours played<br/>mostplayed &lt;- <br/>  steamdata %&gt;%<br/>  group_by(item) %&gt;%<br/>  summarise(hours=sum(rating)) %&gt;% <br/>  arrange(desc(hours)) %&gt;%<br/>  top_n(10, hours) %&gt;%<br/>  ungroup<br/><br/># show top 10 games by hours played<br/>kable(mostplayed)<br/><br/># reset factor levels for items<br/>mostplayed$item &lt;- droplevels(mostplayed$item)<br/><br/># top 10 games by collective hours played<br/>ggplot(mostplayed, aes(x=item, y=hours, fill = hours)) +<br/>  aes(x = fct_inorder(item)) +<br/>  geom_bar(stat = "identity") +<br/>  theme(axis.text.x = element_text(size=8, face="bold", angle=90)) +<br/>  theme(axis.ticks = element_blank()) +<br/>  scale_y_continuous(expand = c(0,0), limits = c(0,1000000)) + <br/>  labs(title="Top 10 games by collective hours played") +<br/>  xlab("game") +<br/>  ylab("hours")<br/><br/></pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-535 image-border" src="assets/d5bd6e83-92c3-4bff-b343-fa94a7bc9516.png" style="width:85.25em;height:75.58em;"/></p>
<ol start="4">
<li class="CDPAlignLeft CDPAlign">Next, let's identify the most popular games by total users:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># most popular games by total users<br/>mostusers &lt;-<br/>  steamdata %&gt;%<br/>  group_by(item) %&gt;%<br/>  summarise(users=n()) %&gt;% <br/>  arrange(desc(users)) %&gt;% <br/>  top_n(10, users) %&gt;% <br/>  ungroup<br/><br/># reset factor levels for items<br/>mostusers$item &lt;- droplevels(mostusers$item)<br/><br/># top 10 popular games by total users<br/>ggplot(mostusers, aes(x=item, y=users, fill = users)) +<br/>  aes(x = fct_inorder(item)) +<br/>  geom_bar(stat = "identity") +<br/>  theme(axis.text.x = element_text(size=8, face="bold", angle=90)) +<br/>  theme(axis.ticks = element_blank()) +<br/>  scale_y_continuous(expand = c(0,0), limits = c(0,5000)) + <br/>  labs(title="Top 10 popular games by total users") +<br/>  xlab("game") +<br/>  ylab("users")</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-536 image-border" src="assets/f09f6a4c-bd55-43e2-b7af-cf1a8dd61c02.png" style="width:40.92em;height:35.25em;"/></p>
<ol start="5">
<li class="mce-root">Now, let's calculate summary statistics of user-item interaction with the following code:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px">summary(steamdata$value)</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-537 image-border" src="assets/a621b44c-ce34-4533-bddd-04c70ee80ba8.png" style="width:26.92em;height:3.67em;"/></p>
<p class="mce-root" style="padding-left: 60px"><span>The summary statistics for overall user-item interaction show average (median) interaction is <kbd>4.5</kbd> hours and average (mean) interaction is <kbd>48.88</kbd> hours, which makes sense when you take into consideration the max (outlier) interaction value: <kbd>11,754</kbd> hours of Sid Meier's <em>Civilization V</em>!</span></p>
<ol start="6">
<li class="mce-root">Next, let's take a look at the distribution of items by individual hours played:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># plot item iteraction<br/>ggplot(steamdata, aes(x=steamdata$value)) +<br/>  geom_histogram(stat = "bin", binwidth=50, fill="steelblue") +<br/>  theme(axis.ticks = element_blank()) +<br/>  scale_x_continuous(expand = c(0,0)) + <br/>  scale_y_continuous(expand = c(0,0), limits = c(0,60000)) + <br/>  labs(title="Item interaction distribution") +<br/>  xlab("Hours played") +<br/>  ylab("Count")</pre>
<p style="padding-left: 60px"><span>Here is the resultant output of items by hours played:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-539 image-border" src="assets/7f5c977a-db29-4f36-aeb2-0b6cb17c5f46.png" style="width:35.17em;height:30.58em;"/></p>
<ol start="7">
<li>Since it's difficult to determine any clear user-item interaction patterns with this approach, let's re-examine items by hours played with a log transformation of hours to reveal any other distributional patterns:</li>
</ol>
<pre class="mce-root" style="padding-left: 60px"># plot item iteraction with log transformation<br/>ggplot(steamdata, aes(x=steamdata$value)) +<br/>  geom_histogram(stat = "bin", binwidth=0.25, fill="steelblue") +<br/>  theme(axis.ticks = element_blank()) +<br/>  scale_x_log10() +<br/>  labs(title="Item interaction distribution with log transformation") +<br/>  xlab("log(Hours played)") +<br/>  ylab("Count")</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-568 image-border" src="assets/d22db659-6848-4e12-962b-fc68142da942.png" style="width:42.08em;height:41.92em;"/></p>
<p>By applying a simple log transformation of hours played, we can clearly see the majority of games in this dataset are associated with 1,000 hours of gameplay or less.</p>
<p>Now that we have a better sense of the underlying data, let's focus our attention on building a neural network with embeddings to predict user ratings.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating user and item embeddings</h1>
                </header>
            
            <article>
                
<p><span>Recommender systems can use deep neural networks to support complex, non-linear data representations in flexible, scalable, and efficient ways.</span></p>
<p><span>Embeddings are low-dimensional representations (vectors) of continuous numbers learned from representations (vectors) of discrete input variables in neural networks.</span> <span>As previously noted in this chapter, recomme</span><span>nder systems</span> <em>typically</em> <span>need an index of similarity between users and user-item preferences to identify similar users and find</span> <span>ratings (preferences) of unrated items.</span></p>
<p><span>However, unlike traditional collaborative filtering approaches that use generalized matrix factorization methods to produce user-item affinity vectors, neural networks can store important information about user-item affinity in a latent (hidden) space using distributed, low-dimensional representational (embeddings).</span></p>
<p><span>Accordingly, as long as we have representations (embeddings) of users and items (games) accessible within the same latent space, we can determine the mutual importance of the relationship between users and items (games) using a dot product function.</span> <span>Presuming user and item vectors have already been normalized, this is effectively the same as using <strong>cosine similarity</strong>, cos(Θ), as a distance metric, where A<sub>i</sub> and B<sub>i</sub> are components of vector A and B, respectively:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1c38318b-5925-4998-a1ea-ea83007eb433.png" style="width:23.33em;height:3.92em;"/></p>
<p>By creating neural network embeddings for users and items, we can d<span>ecrease the amount of input data required for some learning activation functions, which is especially helpful with the data sparsity conditions typically encountered with user-item data in CF systems. In the next section, we will outline how to build, compile, and train a neural recommender system.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building and training a neural recommender system</h1>
                </header>
            
            <article>
                
<p><span>We are now going to build, compile, and train our model using our user-item ratings data. </span>Specifically, we will use Keras to construct a customized neural network with embedded layers (one for users and one for items) and a lambda function that computes the dot product to build a working prototype of a neural network-based recommender system:</p>
<ol>
<li>Let's get started using the following code:</li>
</ol>
<pre style="padding-left: 60px"># create custom model with user and item embeddings<br/>dot &lt;- function(<br/>  embedding_dim,<br/>  n_users,<br/>  n_items,<br/>  name = "dot"<br/>) {<br/>  keras_model_custom(name = name, function(self) {<br/>    self$user_embedding &lt;- layer_embedding(<br/>        input_dim = n_users+1,<br/>        output_dim = embedding_dim,<br/>        name = "user_embedding")<br/>    self$item_embedding &lt;- layer_embedding(<br/>        input_dim = n_items+1,<br/>        output_dim = embedding_dim,<br/>        name = "item_embedding")<br/>    self$dot &lt;- layer_lambda(<br/>        f = function(x)<br/>        k_batch_dot(x[[1]],x[[2]],axes=2),<br/>        name = "dot"<br/>    )<br/>    function(x, mask=NULL, training=FALSE) {<br/>      users &lt;- x[,1]<br/>      items &lt;- x[,2]<br/>      user_embedding &lt;- self$user_embedding(users) <br/>      item_embedding &lt;- self$item_embedding(items) <br/>      dot &lt;- self$dot(list(user_embedding, item_embedding))<br/>    }<br/>  })<br/>}</pre>
<p style="padding-left: 60px"><span>In the preceding code, we defined a custom model with user and item embeddings using the <kbd>keras_model_custom</kbd> function. You will notice that the input size of each embedding layer is initialized to the size of the input data (<kbd>n_users</kbd> and <kbd>n_items</kbd>, respectively).</span></p>
<ol start="2">
<li><span>In the following code, we define the size of the embedding parameter (<kbd>embedding_dim</kbd>) and </span>define <span>the architecture of our neural collaborative filtering model and vector representations (embeddings) to predict user ratings:</span></li>
</ol>
<pre style="padding-left: 60px"># initialize embedding parameter<br/>embedding_dim &lt;- 50<br/><br/># define model <br/>model &lt;- dot(<br/>  embedding_dim,<br/>  n_users,<br/>  n_items<br/>)</pre>
<ol start="3">
<li>Now, let's compile our model:</li>
</ol>
<pre style="padding-left: 60px"># compile model <br/>model %&gt;% compile(<br/>  loss = "mse",<br/>  optimizer = "adam"<br/>)</pre>
<ol start="4">
<li>Next, let's train our model with the following code:</li>
</ol>
<pre style="padding-left: 60px"># train model <br/>history &lt;- model %&gt;% fit(<br/>  x_train,<br/>  y_train,<br/>  epochs = 10,<br/>  batch_size = 500,<br/>  validation_data = list(x_test,y_test),<br/>  verbose = 1<br/>)</pre>
<p style="padding-left: 60px"><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-572 image-border" src="assets/3c5f7a78-aca3-4c1e-beaf-468f314d3df9.png" style="width:69.00em;height:33.50em;"/></p>
<ol start="5">
<li>Now, let's inspect the baseline architecture of our model for reference:</li>
</ol>
<pre style="padding-left: 60px">summary(model)</pre>
<p style="padding-left: 60px">Here is a printout of our model's architecture:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-573 image-border" src="assets/856206c9-0157-49b2-a53a-97e0561b7592.png" style="width:70.92em;height:22.58em;"/></p>
<p class="CDPAlignLeft CDPAlign">In the following section, we will evaluate the model results, tune parameters, and make some iterative adjustments to improve performance in terms of loss metrics.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Evaluating results and tuning hyperparameters</h1>
                </header>
            
            <article>
                
<p>Building a recommender system, evaluating its performance, and tuning the hyperparameters is a highly iterative process. Ultimately, the goal is to maximize the model's performance and results. Now that we have built and trained our baseline model, we can <span>monitor and evaluate its performance during the training process using the following code:</span></p>
<pre># evaluate model results<br/>plot(history)</pre>
<p><span>This results in the following model performance output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-574 image-border" src="assets/8b359981-0508-44fd-beb9-0b47e5e4f604.png" style="width:39.17em;height:31.67em;"/></p>
<p>In the following sections, we will experiment with tuning model parameters to improve its performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Hyperparameter tuning</h1>
                </header>
            
            <article>
                
<p>Let's try changing the <kbd>embedding_dim</kbd> hyperparameter to <kbd>32</kbd> and the <kbd>batch_size</kbd> hyperparameter to <kbd>50</kbd> to see if we get improved results:</p>
<pre># initialize embedding parameter<br/>embedding_dim &lt;- 32<br/><br/># train model <br/>history &lt;- model %&gt;% fit(<br/>  x_train,<br/>  y_train,<br/>  epochs = 10,<br/>  batch_size = 50,<br/>  validation_data = list(x_test,y_test),<br/>  verbose = 1)<br/><br/># show model<br/>summary(model)</pre>
<p><span>Here is a printout of the model's architecture:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-662 image-border" src="assets/8574203f-50c1-4e3e-b135-ecea2d9bc01d.png" style="width:71.67em;height:21.00em;"/></p>
<p>Now, we will plot the results as follows:</p>
<pre># evaluate results<br/>plot(history)</pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-663 image-border" src="assets/d9108ec2-4d64-43f3-be24-5603fd8b6936.png" style="width:59.33em;height:55.92em;"/></p>
<p>Unfortunately, these model performance results do not look significantly different than our baseline model, so let's explore some additional model configurations.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adding dropout layers </h1>
                </header>
            
            <article>
                
<p>In the following code, we will add dropout layers and encourage you to experiment with different dropout rates to see what empirically leads to optimal results:</p>
<pre># initialize embedding parameter<br/>embedding_dim &lt;- 64<br/><br/># create custom model with dropout layers<br/>dot_with_dropout &lt;- function(<br/>  embedding_dim,<br/>  n_users,<br/>  n_items,<br/>  name = "dot_with_dropout"<br/>) {<br/>  keras_model_custom(name = name, function(self) {<br/>    self$user_embedding &lt;- layer_embedding(<br/>      input_dim = n_users+1,<br/>      output_dim = embedding_dim,<br/>      name = "user_embedding")<br/>    self$item_embedding &lt;- layer_embedding(<br/>      input_dim = n_items+1,<br/>      output_dim = embedding_dim,<br/>      name = "item_embedding")<br/>    self$user_dropout &lt;- layer_dropout(<br/>        rate = 0.2)<br/>    self$item_dropout &lt;- layer_dropout(<br/>        rate = 0.4)<br/>    self$dot &lt;-<br/>      layer_lambda(<br/>        f = function(x)<br/>        k_batch_dot(x[[1]],x[[2]],axes=2),<br/>        name = "dot"<br/>      )<br/>    function(x, mask=NULL, training=FALSE) {<br/>      users &lt;- x[,1]<br/>      items &lt;- x[,2]<br/>      user_embedding &lt;- self$user_embedding(users) %&gt;%           <br/>          self$user_dropout()<br/>      item_embedding &lt;- self$item_embedding(items) %&gt;%     <br/>           self$item_dropout()<br/>      dot &lt;- self$dot(list(user_embedding,item_embedding))<br/>    }<br/>  })<br/>}</pre>
<p>In the preceding code, we added dropout layers using <kbd>layer_dropout()</kbd>, which adds some complexity to our preliminary model. In the following code, we define, compile, and train our custom model with dropout layers:</p>
<pre># define model <br/>model &lt;- dot_with_dropout(<br/>  embedding_dim,<br/>  n_users,<br/>  n_items)<br/><br/># compile model <br/>model %&gt;% compile(<br/>  loss = "mse",<br/>  optimizer = "adam"<br/>)<br/><br/># train model <br/>history &lt;- model %&gt;% fit(<br/>  x_train,<br/>  y_train,<br/>  epochs = 10,<br/>  batch_size = 50,<br/>  validation_data = list(x_test,y_test),<br/>  verbose = 1<br/>)</pre>
<p><span>This results in the following output:</span></p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-664 image-border" src="assets/69700d96-0c27-40f1-8a41-5e98765e010e.png" style="width:36.50em;height:16.58em;"/></p>
<p>Now, we print out a summary of our model, as follows:</p>
<pre>summary(model)</pre>
<p>Here's a summary of the model architecture:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-577 image-border" src="assets/cc77c7cb-1790-4300-b455-8cc778bf2d14.png" style="width:31.33em;height:12.75em;"/></p>
<p>Now, we will plot the results, as follows:</p>
<pre># evaluate results<br/>plot(history)</pre>
<p><span>This results in the following model performance output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-665 image-border" src="assets/f87a7148-3617-4da2-899d-65aed9c043e4.png" style="width:33.67em;height:31.75em;"/></p>
<p>While we added dropout layers, the observed model improvements are minimal.</p>
<p>Let's revisit our underlying assumptions and try another approach.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Adjusting for user-item bias</h1>
                </header>
            
            <article>
                
<p class="mce-root"><span>It is important to recognize that some users, in reality, may interact with items (games) differently than other users in terms of gaming frequency and affinity by proxy. This discrepancy, in turn, could potentially translate to different implicit ratings based on the user-item interaction (hours of gameplay) data available for this particular analysis.</span></p>
<p class="mce-root">Based on our previous findings, we will now modify the model to account for user and item biases by including embeddings for average users and items (games) using the following code:</p>
<pre># caculate minimum and max rating<br/>min_rating &lt;- steamdata %&gt;% summarise(min_rating = min(rating_scaled)) %&gt;% pull()<br/>max_rating &lt;- steamdata %&gt;% summarise(max_rating = max(rating_scaled)) %&gt;% pull()<br/><br/># create custom model with user, item, and bias embeddings<br/>dot_with_bias &lt;- function(<br/>  embedding_dim,<br/>  n_users,<br/>  n_items,<br/>  min_rating,<br/>  max_rating,<br/>  name = "dot_with_bias"<br/>) { <br/>keras_model_custom(name = name, function(self) {<br/>  self$user_embedding &lt;- layer_embedding(<br/>    input_dim = n_users+1,<br/>    output_dim = embedding_dim,<br/>    name = "user_embedding")<br/>  self$item_embedding &lt;- layer_embedding(<br/>    input_dim = n_items+1,<br/>    output_dim = embedding_dim,<br/>    name = "item_embedding")<br/>  self$user_bias &lt;- layer_embedding(<br/>    input_dim = n_users+1,<br/>    output_dim = 1,<br/>    name = "user_bias")<br/>  self$item_bias &lt;- layer_embedding(<br/>    input_dim = n_items+1,<br/>    output_dim = 1,<br/>    name = "item_bias")</pre>
<p>In the preceding code, we create a custom model with user and item embeddings (<kbd>user_embedding</kbd> and <kbd>item_embedding</kbd>) and embeddings for user bias and item bias (<kbd>user_bias</kbd> and <kbd>item_bias</kbd>). In the following code, we add dropout layers for both users and items and encourage you to experiment with different dropout rates for optimal results:</p>
<pre>  self$user_dropout &lt;- layer_dropout(<br/>    rate = 0.3)<br/>  self$item_dropout &lt;- layer_dropout(<br/>    rate = 0.5)<br/>  self$dot &lt;- layer_lambda(<br/>    f = function(x)<br/>    k_batch_dot(x[[1]],x[[2]],axes=2),<br/>    name = "dot")<br/>  self$dot_bias &lt;- layer_lambda(<br/>    f = function(x)<br/>    k_sigmoid(x[[1]]+x[[2]]+x[[3]]),<br/>    name = "dot_bias")<br/>  self$min_rating &lt;- min_rating<br/>  self$max_rating &lt;- max_rating<br/>  self$pred &lt;- layer_lambda(<br/>    f = function(x)<br/>    x * (self$max_rating - self$min_rating) + self$min_rating,<br/>    name = "pred")<br/>  function(x,mask=NULL,training=FALSE) {<br/>    users &lt;- x[,1]<br/>    items &lt;- x[,2]<br/>    user_embedding &lt;- self$user_embedding(users) %&gt;% self$user_dropout()<br/>    item_embedding &lt;- self$item_embedding(items) %&gt;% self$item_dropout()<br/>    dot &lt;- self$dot(list(user_embedding,item_embedding))<br/>    dot_bias &lt;- self$dot_bias(list(dot, self$user_bias(users), self$item_bias(items)))<br/>    self$pred(dot_bias)<br/>    }<br/>  })<br/>}</pre>
<p>Next, let's define, compile, and train our modified neural network model:</p>
<pre># define model <br/>model &lt;- dot_with_bias(<br/>  embedding_dim,<br/>  n_users,<br/>  n_items,<br/>  min_rating,<br/>  max_rating)<br/><br/># compile model <br/>model %&gt;% compile(<br/>  loss = "mse",<br/>  optimizer = "adam"<br/>)<br/><br/># train model <br/>history &lt;- model %&gt;% fit(<br/>  x_train,<br/>  y_train,<br/>  epochs = 10,<br/>  batch_size = 50,<br/>  validation_data = list(x_test,y_test),<br/>  verbose = 1)<br/>)</pre>
<p>This results in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-667 image-border" src="assets/ff39e817-e0be-4a77-95ba-624dcecb1ffd.png" style="width:69.58em;height:33.92em;"/></p>
<p>Now, we will print out the summary:</p>
<pre># summary model<br/>summary(model)</pre>
<p>By adding these additional layers of embeddings and tuning the hyperparameters, we have nearly doubled the total number of trainable parameters from our original baseline neural network, as reflected in the following model summary:</p>
<p class="CDPAlignCenter CDPAlign"><img class="alignnone size-full wp-image-581 image-border" src="assets/43bb18d7-548f-4fd3-92b3-44883005f12f.png" style="width:71.25em;height:41.42em;"/></p>
<p>Finally, we plot the results of the model as follows:</p>
<pre># evaluate results<br/>plot(history)</pre>
<p><span>This results in the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><span><img class="alignnone size-full wp-image-668 image-border" src="assets/d87d93c2-a800-44cc-96ee-774810eec733.png" style="width:43.50em;height:41.00em;"/></span></p>
<p>Through a succession of iterative configurations and empirically guided adjustments, we have improved overfitting relative to our previous models and achieved a notable RMSE below 0.1 on our validation dataset. With additional hyperparameter tuning and dropout layer rate configurations, we might be able to further improve the performance of this model. Future recommendations to build on this model would be to acquire and implement explicit rating data, as well as to experiment with additional contextual information and user demographic data to better understand the relationships and factors associated with user-item interactions.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, <span>you learned how to use the custom Keras API and embeddings to construct a deep neural network recommender system.</span> We briefly introduced collaborative filtering concepts and saw how to prepare data for building a custom neural network. During this iterative process, we created user and item embeddings, trained a deep neural network using embedded layers, tuned hyperparameters, and evaluated results using common performance metrics. In the next chapter, you will continue applying neural network approaches to other domains, such as natural language processing.</p>


            </article>

            
        </section>
    </body></html>