<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer060">
			<h1 id="_idParaDest-61"><em class="italic"><a id="_idTextAnchor060"/>Chapter 5</em>: Running DL Pipelines in Different Environments</h1>
			<p>It is critical to have the flexibility of running a <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) pipeline in different execution environments such as local or remote, on-premises, or in the cloud. This is because, during different stages of the DL development, there may be different constraints or preferences to either improve the velocity of the development or ensure security compliance. For example, it is desirable to do small-scale model experimentation in a local or laptop environment, while for a full hyperparameter tuning, we need to run the model on a cloud-hosted GPU cluster to get a quick turn-around time. Given the diverse execution environments in both hardware and software configurations, it used to be a challenge to achieve this kind of flexibility within a single framework. MLflow provides an easy-to-use framework to run DL pipelines at scale in different environments. We will learn how to do that in this chapter.</p>
			<p>In this chapter, we will first learn about the different DL pipeline execution scenarios and their execution environments. We will also learn how to run the different steps of the DL pipeline in different execution environments. Specifically, we will cover the following topics:</p>
			<ul>
				<li>An overview of different execution scenarios and environments</li>
				<li>Running locally with local code</li>
				<li>Running remote code in GitHub locally</li>
				<li>Running local code remotely in the cloud</li>
				<li>Running remotely in the cloud with remote code in GitHub</li>
			</ul>
			<p>By the end of this chapter, you will be comfortable setting up the DL pipelines to run either locally or remotely with different execution environments.</p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor061"/>Technical requirements</h1>
			<p>The following technical requirements are needed for completing the learning in this chapter:</p>
			<ul>
				<li>The code in this chapter can be found at the following GitHub URL: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter05">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter05</a>.</li>
				<li>Installation of the Databricks <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>) tool to access the Databricks platform remote execution of DL pipelines: <a href="https://github.com/databricks/databricks-cli">https://github.com/databricks/databricks-cli</a>.</li>
				<li>Access to a Databricks instance (must be the Enterprise version, as the Community version does not support remote execution) for learning how to run DL pipelines remotely on a cluster in Databricks.</li>
				<li>A full-fledged MLflow tracking server when running locally. This MLflow tracking server setup is the same as in previous chapters.</li>
			</ul>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor062"/>An overview of different execution scenarios and environments</h1>
			<p>In our previous chapters, we mainly focused on learning how to track DL pipelines using MLflow's tracking capabilities. Most of our execution environments are in a local environment, such as a <a id="_idIndexMarker221"/>local laptop or desktop environment. However, as we already know, the DL full life cycle consists of different stages where we may need to run the DL pipelines either entirely, partially, or as a single step in a different execution environment. Here are two typical examples:</p>
			<ul>
				<li>When accessing data for model training purposes, it is not uncommon to require the data to reside in an enterprise-security and privacy-compliant environment, where both the computation and the storage cannot leave a compliant boundary. </li>
				<li>When training a DL model, it is usually desirable to use a remote GPU cluster to maximize the efficiency of model training, where a local laptop usually does not have the required hardware capability.</li>
			</ul>
			<p>Both cases require a carefully defined <a id="_idIndexMarker222"/>execution environment that might be needed in one or multiple stages of the DL lifecycle. Note that this is not just a requirement to be flexible when moving from the development stage to a production environment, where the execution hardware and software configuration could be understandably different. It is also a requirement to be able to switch running environments during development stages or in different production environments without making major changes to the DL pipelines.</p>
			<p>Here, we classify the different <a id="_idIndexMarker223"/>scenarios and execution environments into the following four scenarios, based on the different combinations of the<a id="_idIndexMarker224"/> location of the source code of DL pipelines and <a id="_idIndexMarker225"/>target execution environments, as shown in the following table:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="Images/B18120_05_01.jpg" alt="Figure 5.1 – Four different scenarios of DL pipeline source codes and target execution environments&#13;&#10;" width="1475" height="432"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – Four different scenarios of DL pipeline source codes and target execution environments</p>
			<p><em class="italic">Figure 5.1</em> describes how either in development or production environments, we could encounter the possibilities of using either local or remote code to run in a different execution environment. Let's examine them one by one as follows:</p>
			<ul>
				<li><strong class="bold">Local source code running in a local target environment</strong>: This usually happens at the<a id="_idIndexMarker226"/> development stage, where modest computing power in a local environment is adequate to support quick prototyping or test runs for small changes in an existing pipeline. This is mostly the scenario we have been using in previous chapters for our MLflow experiments when learning how to track pipelines.</li>
				<li><strong class="bold">Local source code running in a remote target environment</strong>: This usually happens at the development stage or re-training of an existing DL model, where a <a id="_idIndexMarker227"/>GPU or other types of hardware accelerators, such as <strong class="bold">Tensor Processing Units</strong> (<strong class="bold">TPUs</strong>) or <strong class="bold">field-programmable gate arrays</strong> (<strong class="bold">FPGAs</strong>), are <a id="_idIndexMarker228"/>needed to perform computational and data-intensive model training or debugging prior to merging the GitHub repository (using local code change first).</li>
				<li><strong class="bold">Remote source code running in a local target environment</strong>: This usually happens when we don't have any changes in the code but the data has changed, either during the development stage or the production stage. For example, during the DL development stage, we could change the data with newly augmented training data<a id="_idIndexMarker229"/> either through some data augmentation techniques (for example, using <strong class="bold">AugLy</strong> to augment existing training data: <a href="https://github.com/facebookresearch/AugLy">https://github.com/facebookresearch/AugLy</a>) or newly annotated training data. During the production deployment step, we often need to run a regression test to evaluate a to-be-deployed DL pipeline against a hold-out regression testing dataset, so that we don't deploy a degraded model if the model performance accuracy metric does not meet the bar. In this case, the hold-out testing dataset is not usually big, so the execution can be done on the deployment server locally instead of launching to a remote cluster in a Databricks server.</li>
				<li><strong class="bold">Remote source code running in a remote target environment</strong>: This can happen in the development stage or production stage, where we want to use a fixed version of the DL pipeline code from GitHub to run in a remote GPU cluster to do model training, hyperparameter tuning, or re-training. Such large-scale execution can be <a id="_idIndexMarker230"/>time-consuming, and a remote GPU cluster could be very useful.</li>
			</ul>
			<p>Given the four different scenarios, it would be desirable to have a framework to be able to run the same DL pipeline with minimal configuration changes under these conditions. Prior to the arrival of MLflow, it took quite a lot of engineering and manual efforts to support these scenarios. MLflow<a id="_idIndexMarker231"/> provides an MLproject framework that supports all these four scenarios through the following three configurable mechanisms:</p>
			<ol>
				<li><strong class="bold">Entry points</strong>: We<a id="_idIndexMarker232"/> can define one or multiple entry points to execute different<a id="_idIndexMarker233"/> steps of a DL pipeline. For example, the following is an example to define a main entry point:<p class="source-code">entry_points:</p><p class="source-code">  <strong class="bold">main</strong>:</p><p class="source-code">    parameters:</p><p class="source-code">      <strong class="bold">pipeline_steps</strong>: { type: str, default: all }</p><p class="source-code">    command: "python main.py –pipeline_steps {pipeline_steps}"</p></li>
			</ol>
			<p>The entry point's name is <strong class="source-inline">main</strong>, which, by default, will be used when executing an MLflow run without specifying an entry point for an MLproject. Under this <strong class="source-inline">main</strong> entry point, there is a list of parameters. We can define the parameter's type and default value using a short syntax, as follows:</p>
			<p class="source-code">parameter_name: {type: data_type, default: value}</p>
			<p>We can also use a long syntax, as follows:</p>
			<p class="source-code">parameter_name:</p>
			<p class="source-code">  type: data_type</p>
			<p class="source-code">  default: value</p>
			<p>Here, we define <a id="_idIndexMarker234"/>only one parameter, called <strong class="source-inline">pipeline_steps</strong>, using<a id="_idIndexMarker235"/> the short syntax format with a <strong class="source-inline">str</strong> type and a default value of <strong class="source-inline">all</strong>.</p>
			<ol>
				<li value="2"><strong class="bold">Software and library dependencies</strong>: We can use one conda .<strong class="source-inline">yaml</strong> configuration file or a<a id="_idIndexMarker236"/> Docker image to define the software and library dependencies that can be used by the MLproject's entry points. Note that a single MLproject can either use a conda <strong class="source-inline">yaml</strong> file or a Docker image, but not both at the same time. Depending on the DL pipeline dependencies, sometimes using a conda .<strong class="source-inline">yaml</strong> file over a Docker image is preferred, since it is much more lightweight and easier to make changes without requiring additional Docker image storage locations and loading a large Docker image into memory in a resource-limited environment. However, a Docker image does sometimes have advantages if there are any Java packages (<strong class="source-inline">.jar</strong>) that are needed at runtime. If there are no such JAR dependencies, then it is preferred to have a conda .<strong class="source-inline">yaml</strong> file to specify the dependencies. Furthermore, as of MLflow version 1.22.0, running Docker-based projects on Databricks is not yet supported by the MLflow command line. If there are indeed any Java package dependencies, they can be installed <a id="_idIndexMarker237"/>using <strong class="bold">init scripts</strong> (for example, see the official documentation at <a href="https://docs.databricks.com/clusters/init-scripts.html#example-install-postgresql-jdbc-driver">https://docs.databricks.com/clusters/init-scripts.html#example-install-postgresql-jdbc-driver</a>). Thus, we will use conda .<strong class="source-inline">yaml</strong> configuration files to define execution environment dependencies in this book.</li>
				<li><strong class="bold">Hardware dependencies</strong>: We can<a id="_idIndexMarker238"/> use a cluster configuration JSON file to define the execution target backend environment, be it a GPU, CPU, or other types of clusters. This is only needed when the target backend execution environment <a id="_idIndexMarker239"/>is non-local, either in a Databricks server or a <strong class="bold">Kubernetes</strong> (<strong class="bold">K8s</strong>) cluster.</li>
			</ol>
			<p>Previously, we learned how to use MLproject to create a multiple-step DL pipeline running in a local environment in <a href="B18120_04_ePub.xhtml#_idTextAnchor050"><em class="italic">Chapter 4</em></a>, <em class="italic">Tracking Code and Data Versioning</em>, for tracking purposes. Now, we<a id="_idIndexMarker240"/> are going to learn how to use MLproject for supporting the different running scenarios outlined previously. </p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor063"/>Running locally with local code</h1>
			<p>Let's start with the first running<a id="_idIndexMarker241"/> scenario using the same <strong class="bold">Natural Language Processing (NLP)</strong> text sentiment classification example as the driving <a id="_idIndexMarker242"/>use case. You are advised to check out <a id="_idIndexMarker243"/>the following version of the source code from the GitHub location to follow along with the steps and learnings: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/26119e984e52dadd04b99e6f7e95f8dda8b59238/chapter05">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/26119e984e52dadd04b99e6f7e95f8dda8b59238/chapter05</a>. Note that this requires a specific Git hash committed version, as shown in the URL path. That means we are asking you to check out a specific committed version, not the main branch.</p>
			<p>Let's start with the DL pipeline that downloads the review data to local storage as a first execution exercise. Once you check out this chapter's code, you can type the following command line to execute the DL pipeline's first step:</p>
			<p class="source-code">mlflow run . --experiment-name='dl_model_chapter05' -P pipeline_steps='download_data'</p>
			<p>If we don't specify an entry point, it defaults to <strong class="source-inline">main</strong>. In this case, this is our desired behavior since we want to run the <strong class="source-inline">main</strong> entry point to start the parent DL pipeline. </p>
			<p>The <em class="italic">dot</em> means the current local directory. This tells MLflow to use the code in the current directory as the source to execute the project. If this command line runs successfully, you should be able to see the first two lines of output in the console as follows, which also reveal where the target execution environment is:</p>
			<p class="source-code">2022/01/01 19:15:37 INFO mlflow.projects.utils: === Created directory /var/folders/51/whxjy4r92dx18788yp11ycyr0000gp/T/tmp3qj2kws2 for downloading remote URIs passed to arguments of type 'path' ===</p>
			<p class="source-code">2022/01/01 19:15:37 INFO <strong class="bold">mlflow.projects.backend.local</strong>: === Running command 'source /Users/yongliu/opt/miniconda3/bin/../etc/profile.d/conda.sh &amp;&amp; conda activate mlflow-95353930ddb7b60101df80a5d64ef8bf6204a808 1&gt;&amp;2 &amp;&amp; python main.py --pipeline_steps download_data' in run with ID 'f7133b916a004c508e227f00d534e136' ===</p>
			<p>Note that the second output line shows <strong class="source-inline">mlflow.projects.backend.local</strong>, which means the target running environment is local. You may wonder where we define the local execution environment in our initial command line. It turns out that by default, the value for the parameter called <strong class="source-inline">--backend</strong> (or <strong class="source-inline">-b</strong>) is <strong class="source-inline">local</strong>. So, if we spell out the default values, the <strong class="source-inline">mlflow run</strong> command line will look like the following:</p>
			<p class="source-code">mlflow run . -e main -b local --<strong class="bold">experiment-name</strong>='dl_model_chapter05' -P pipeline_steps='download_data'</p>
			<p>Note that we<a id="_idIndexMarker244"/> also need to specify <strong class="source-inline">experiment-name</strong> in the<a id="_idIndexMarker245"/> command line or through an environment variable named <strong class="source-inline">MLFLOW_EXPERIMENT_NAME</strong> to define the experiment in which this project will run. Alternatively, you can specify an <strong class="source-inline">experiment-id</strong> parameter, or an environment variable named <strong class="source-inline">MLFLOW_EXPERIMENT_ID</strong>, to define the experiment integer ID that already exists. You only need to define either the ID or the name of the environment, but not both. It is common to define a human-readable experiment name and then query the experiment ID for that experiment in other parts of the code so that they will not be out of sync.</p>
			<p class="callout-heading">MLflow Experiment Name or ID for Running an MLproject </p>
			<p class="callout">To run an <a id="_idIndexMarker246"/>MLproject either using the CLI or the <strong class="source-inline">mlflow.run</strong> Python API, if we don't specify <strong class="source-inline">experiment-name</strong> or <strong class="source-inline">experiment-id</strong> through either an environment variable or a parameter assignment, it will default to the <strong class="source-inline">Default</strong> MLflow experiment. This is not desirable, as we want to organize our experiments into clearly separated experiments. In addition, once an MLproject starts running, any child runs will not be able to switch to a different experiment name or ID. So, the best practice will be always to specify an experiment name or an ID before launching an MLflow project run. </p>
			<p>Once you finish the run, you will see the output as in the following lines:</p>
			<p class="source-code">2022-01-01 19:15:48,249 &lt;Run: data=&lt;RunData: metrics={}, params={'download_url': 'https://pl-flash-data.s3.amazonaws.com/imdb.zip',</p>
			<p class="source-code"> 'local_folder': './data',</p>
			<p class="source-code"> 'mlflow run id': 'f9f74ebd80f246d58a5f7a3bfb3fc635',</p>
			<p class="source-code"> 'pipeline_run_name': 'chapter05'}, tags={'mlflow.gitRepoURL': 'git@github.com:PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow.git',</p>
			<p class="source-code"> 'mlflow.parentRunId': 'f7133b916a004c508e227f00d534e136',</p>
			<p>Note that this is<a id="_idIndexMarker247"/> a nested MLflow run since we first launch a <strong class="source-inline">main</strong> entry <a id="_idIndexMarker248"/>point that starts the whole pipeline (that's why there is <strong class="source-inline">mlflow.parentRunId</strong>), and then under this pipeline, we run one or multiple steps. Here, the step we run is called <strong class="source-inline">download_data</strong>, which is another entry point defined in the MLproject, but is invoked using the <strong class="source-inline">mlflow.run</strong> Python API, as follows, in the <strong class="source-inline">main.py</strong> file:</p>
			<pre class="source-code">download_run = mlflow.run(".", "download_data", parameters={})</pre>
			<p>Note that this also specifies which code source to use (<strong class="source-inline">local</strong>, since we specified a <em class="italic">dot</em>), and by default, a local execution environment. That's why you should be able to see the following lines in the console output:</p>
			<p class="source-code"> 'mlflow.project.backend': 'local',</p>
			<p class="source-code"> 'mlflow.project.entryPoint': 'download_data',</p>
			<p>You should also see a few other details of the run parameters for this entry point. The last two lines of the command line output should look like the following:</p>
			<p class="source-code">2022-01-01 19:15:48,269 finished mlflow pipeline run with a run_id = f7133b916a004c508e227f00d534e136</p>
			<p class="source-code">2022/01/01 19:15:48 INFO mlflow.projects: === Run (ID 'f7133b916a004c508e227f00d534e136') succeeded ===</p>
			<p>If you see this, you should feel proud that you have successfully run a pipeline with one step to completion. </p>
			<p>While this is<a id="_idIndexMarker249"/> something we have done before without <a id="_idIndexMarker250"/>knowing some of the details, the next section will allow us to run remote code in a local environment, where you will see the increasing flexibility and power of MLproject.</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor064"/>Running remote code in GitHub locally</h1>
			<p>Now, let's see <a id="_idIndexMarker251"/>how we run remote code<a id="_idIndexMarker252"/> from a GitHub repository on a<a id="_idIndexMarker253"/> local execution environment. This allows us to precisely run a specific version that has been checked into the GitHub repository using the commit hash. Let's use the same example as before by running a single <strong class="source-inline">download_data</strong> step of the DL pipeline that we have been using in this chapter. In the command line prompt, run the following command:</p>
			<p class="source-code">mlflow run https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow#chapter05 -v 26119e984e52dadd04b99e6f7e95f8dda8b59238  --experiment-name='dl_model_chapter05' -P pipeline_steps='download_data'</p>
			<p>Notice the difference between this command line and the one in the previous section. Instead of a <em class="italic">dot</em> to refer to a local copy of the code, we are pointing to a remote GitHub repository (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow</a>) and the folder name (<strong class="source-inline">chapter05</strong>) that contains the MLproject file we want to reference. The <strong class="source-inline">#</strong> symbol denotes the relative path to the root folder, according to MLflow's convention (see details on the MLflow documentation at this website: <a href="https://www.mlflow.org/docs/latest/projects.html#running-projects">https://www.mlflow.org/docs/latest/projects.html#running-projects</a>). We then define a version number by specifying the Git commit hash using the <strong class="source-inline">-v</strong> parameter. In this case, it is this version we have in the GitHub repository:</p>
			<p><a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/26119e984e52dadd04b99e6f7e95f8dda8b59238/chapter05">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/26119e984e52dadd04b99e6f7e95f8dda8b59238/chapter05</a></p>
			<p class="callout-heading"> Hidden Bug of Running an MLflow Project with GitHub's Main Branch</p>
			<p class="callout">When we<a id="_idIndexMarker254"/> omit the <strong class="source-inline">-v</strong> parameter in the MLflow run, MLflow will assume we want to use the default <strong class="source-inline">main</strong> branch of a GitHub project. However, MLflow's source code has a hardcoded reference to the <strong class="source-inline">main</strong> branch of a GitHub project as <strong class="source-inline">origin.refs.master</strong>, requiring the existence of a <strong class="source-inline">master</strong> branch in the GitHub project. This does not work in newer GitHub projects such as this book's project, since the default branch is called <strong class="source-inline">main</strong>, not <strong class="source-inline">master</strong> anymore, due to the recent changes introduced by GitHub (see details here: <a href="https://github.com/github/renaming">https://github.com/github/renaming</a>). So, at the time of writing this book, in the MLflow version 1.22.0, there is no way to run a default <strong class="source-inline">main</strong> branch of a GitHub project. We need to specifically declare the Git commit hash version when running an MLflow project in the GitHub repository.</p>
			<p>So, what happens<a id="_idIndexMarker255"/> when you use the code in a<a id="_idIndexMarker256"/> remote GitHub project <a id="_idIndexMarker257"/>repository when running an MLflow project? It becomes clear when you see the first line of the following console output:</p>
			<p class="source-code">2021/12/30 18:57:32 INFO mlflow.projects.utils: === Fetching project from https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow#chapter05 into <strong class="bold">/var/folders/51/whxjy4r92dx18788yp11ycyr0000gp/T/tmpdyzaa1ye</strong> ===</p>
			<p>This means that MLflow, on behalf of the user, starts to clone the remote project to a local temporary folder called <strong class="source-inline">/var/folders/51/whxjy4r92dx18788yp11ycyr0000gp/T/tmpdyzaa1ye</strong>.</p>
			<p>If you navigate to this temporary folder, you will see that the entire project content from GitHub has been cloned to this folder, not just the folder containing the ML project you want to run. </p>
			<p>The rest of the console output is as we have seen when using the local code. Once you finish the run with the <strong class="source-inline">download_data</strong> step, you should be able to find the downloaded data in the temporary folder under <strong class="source-inline">chapter05</strong>, since we define the local destination folder as a <strong class="source-inline">./data</strong> relative path in the ML project file:</p>
			<p class="source-code">local_folder: { type: str, default: ./data }</p>
			<p>MLflow automatically converts this to an absolute path, and it becomes a relative path to the cloned project folder under <strong class="source-inline">chapter05</strong>, since that's where the MLproject file resides. </p>
			<p>This capability to reference a remote GitHub project and run it in a local environment, whether this local environment is your laptop or a virtual machine in the cloud, is powerful. This enables automation through <strong class="bold">continuous integration and continuous deployment</strong> (<strong class="bold">CI/CD</strong>) since<a id="_idIndexMarker258"/> this can be directly invoked in a command line, which can then be scripted into a CI/CD script. The tracking part is also precise, since we have the Git commit hash logged in the MLflow tracking server, which allows us to know exactly which version of the code was executed.</p>
			<p>Note in both the scenarios we just covered, the execution environment is a local machine where the MLflow run command was issued. The MLflow project runs to completion <em class="italic">synchronously</em>, meaning it is a blocking call and it will run to completion and show you the progress in the console output in real time.</p>
			<p>However, there are <a id="_idIndexMarker259"/>additional running<a id="_idIndexMarker260"/> scenarios we need to <a id="_idIndexMarker261"/>support. For example, sometimes the machine where we issue the MLflow project run command is not powerful enough to support the computation we need, such as training a DL model with many epochs. Another scenario could be if the data to be downloaded or accessed for training is multiple gigabytes and you don't want to download it to your local laptop for model development. This requires us to be able to run the code in a remote cluster. Let's look at how we can do that in the next section.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor065"/>Running local code remotely in the cloud</h1>
			<p>In<a id="_idIndexMarker262"/> previous chapters, we ran all <a id="_idIndexMarker263"/>our code in a local laptop <a id="_idIndexMarker264"/>environment, and limited our DL fine-tuning step to only three epochs due to the limited power of a laptop. This serves the purpose of getting the code running and testing quickly in a local environment but does not serve to build an actual high-performance DL model. We really need to run the fine-tuning step in a remote GPU cluster. Ideally, we should only change some configuration and still issue the MLflow run command line in a local laptop console, but the actual pipeline will be submitted to a remote cluster in the cloud. Let's see how we can do this for our DL pipeline.</p>
			<p>Let's start with <a id="_idIndexMarker265"/>submitting code to run in a Databricks server. There are three prerequisites:</p>
			<ul>
				<li><strong class="bold">An Enterprise Databricks server</strong>: You <a id="_idIndexMarker266"/>need to have access to an Enterprise-licensed Databricks server or a free trial version of the Databricks server (<a href="https://docs.databricks.com/getting-started/try-databricks.html#sign-up-for-a-databricks-free-trial">https://docs.databricks.com/getting-started/try-databricks.html#sign-up-for-a-databricks-free-trial</a>) in the cloud. The <a id="_idIndexMarker267"/>Community<a id="_idIndexMarker268"/> version of Databricks does not support this <a id="_idIndexMarker269"/>remote execution.</li>
				<li><strong class="bold">The Databricks CLI</strong>: You <a id="_idIndexMarker270"/>need to set up the Databricks CLI where you issue the MLflow project run commands. To install it, simply run the following command:<p class="source-code"><strong class="bold">pip install databricks-cli</strong></p></li>
			</ul>
			<p>We also include this dependency in the <strong class="source-inline">requirements.txt</strong> file of <strong class="source-inline">chapter05</strong> when you check out the code for this chapter. </p>
			<ul>
				<li><strong class="bold">Access tokens for accessing the Databricks server</strong>: There are two ways to set up the tokens: using an<a id="_idIndexMarker271"/> environment variable, or using the Databricks command-line tool to generate a <strong class="source-inline">.databrickscfg</strong> file in your local home folder. You don't need both, but if you do have both, the one defined using environment variables will take a higher precedence when being picked up by the Databricks command line. The approach of using environment variables and generating access tokens is described in the <em class="italic">Setting up MLflow to interact with a remote MLflow server</em> section of <a href="B18120_01_ePub.xhtml#_idTextAnchor015"><em class="italic">Chapter 1</em></a>, <em class="italic">Deep Learning Life Cycle and MLOps Challenges</em>. Note these environment variables can be set up in the command line directly or can be put into<a id="_idIndexMarker272"/> your <strong class="source-inline">.bash_profile</strong> file if you are using a macOS or Linux machine. </li>
			</ul>
			<p>Here, we describe how we can use the Databricks command-line tool to generate a <strong class="source-inline">.databrickscfg</strong> file:</p>
			<ol>
				<li value="1">Run the<a id="_idIndexMarker273"/> following command to set up the token configuration:<p class="source-code"><strong class="bold">databricks configure --token</strong></p></li>
				<li>Follow the prompt to fill in the remote Databricks host URL and the access token:<p class="source-code"><strong class="bold">Databricks Host (should begin with https://): https://????</strong></p><p class="source-code"><strong class="bold">Token: dapi??????????</strong></p></li>
				<li>Now, if you check your<a id="_idIndexMarker274"/> local <a id="_idIndexMarker275"/>home folder, you <a id="_idIndexMarker276"/>should find a hidden file called <strong class="source-inline">.databrickscfg</strong>.</li>
			</ol>
			<p>If you open this<a id="_idIndexMarker277"/> file, you should be able to see something like the following:</p>
			<p class="source-code">[DEFAULT]</p>
			<p class="source-code">host = https://??????</p>
			<p class="source-code">token = dapi???????</p>
			<p class="source-code">jobs-api-version = 2.0 </p>
			<p>Note that the last line indicates the remote job submission and execution API version that the Databricks server is using.</p>
			<p>Now that you have the access set up correctly, let's see how we can run the DL pipeline remotely in the remote Databricks server using the following steps:</p>
			<ol>
				<li value="1">Since we are going<a id="_idIndexMarker278"/> to use the remote Databricks server, the local MLflow server we set up before no longer works. This means that we need to disable and comment out the following lines in the <strong class="source-inline">main.py</strong> file, which are only useful to the local MLflow server setup (check out the latest version of the code for <strong class="source-inline">chapter05</strong> from GitHub to follow the steps, at <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow.git">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow.git</a>):<p class="source-code">os.environ["MLFLOW_TRACKING_URI"] = http://localhost</p><p class="source-code">os.environ["MLFLOW_S3_ENDPOINT_URL"] = http://localhost:9000</p><p class="source-code">os.environ["AWS_ACCESS_KEY_ID"] = "minio"</p><p class="source-code">os.environ["AWS_SECRET_ACCESS_KEY"] = "minio123"</p></li>
			</ol>
			<p>Instead, we<a id="_idIndexMarker279"/> should use the following environment variable that can be defined in a <strong class="source-inline">.bash_profile</strong> file or directly executed in the command line:</p>
			<p class="source-code"><strong class="bold">export MLFLOW_TRACKING_URI="databricks"</strong></p>
			<p>This will use the <a id="_idIndexMarker280"/>MLflow<a id="_idIndexMarker281"/> tracking server on<a id="_idIndexMarker282"/> the Databricks server. If you don't specify this, it will default to a localhost but will fail since there is no localhost version of MLflow on the remote Databricks server. So, make sure you have this set up correctly. Now, we are ready to run our local code remotely.</p>
			<ol>
				<li value="2">Now, run the following command line to submit the local code to the remote Databricks server to run. We will just start with the <strong class="source-inline">download_data</strong> step, as follows:<p class="source-code"><strong class="bold">mlflow run . -b databricks --backend-config cluster_spec.json --experiment-name='/Shared/dl_model_chapter05' -P pipeline_steps ='download_data'</strong></p></li>
			</ol>
			<p>You will see this time that the command line has two new parameters: <strong class="source-inline">-b databricks</strong>, which specifies the backend as a Databricks server, and <strong class="source-inline">--backend-config cluster_spec.json</strong>, which details the cluster specification. The content of this <strong class="source-inline">cluster_spec.json</strong> file is as follows:</p>
			<p class="source-code">{</p>
			<p class="source-code">    "new_cluster": {</p>
			<p class="source-code">        "spark_version": "9.1.x-gpu-ml-scala2.12",</p>
			<p class="source-code">        "num_workers": 1,</p>
			<p class="source-code">        "node_type_id": "g4dn.xlarge"</p>
			<p class="source-code">    }</p>
			<p class="source-code">}</p>
			<p>This <strong class="source-inline">cluster_spec.json</strong> file is typically located in the same folder in which the MLproject <a id="_idIndexMarker283"/>file is located and<a id="_idIndexMarker284"/> needs to be predefined<a id="_idIndexMarker285"/> so that the<a id="_idIndexMarker286"/> MLflow run command can pick it up. The example we give here only defines a minimal set of parameters needed to create a job cluster on Databricks using AWS's GPU virtual machine as a single node, but you can create a much richer cluster specification if necessary (see the following <em class="italic">Cluster Specification for Databricks</em> box for more details).</p>
			<p class="callout-heading">Cluster Specification for Databricks</p>
			<p class="callout">When submitting<a id="_idIndexMarker287"/> jobs to Databricks, it requires the creation of a new job cluster, which is different from an interactive cluster that you already have, where you can run an interactive job by attaching a notebook. A cluster specification is defined by minimally specifying the Databricks runtime version, which in our current example is <strong class="source-inline">9.1.x-gpu-ml-scala2.12</strong>, the number of worker nodes, and the node type ID, as shown in our example. It is recommended to use the <strong class="bold">long-term support</strong> (<strong class="bold">LTS</strong>) version<a id="_idIndexMarker288"/> of the <a id="_idIndexMarker289"/>Databricks runtime (<a href="https://docs.databricks.com/release-notes/runtime/9.1ml.html">https://docs.databricks.com/release-notes/runtime/9.1ml.html</a>). The cluster node type depends on the cloud provider. Here, we use AWS's single GPU node (<strong class="source-inline">g4dn.xlarge</strong>) for learning purposes. There are many other configurations that<a id="_idIndexMarker290"/> you can define in<a id="_idIndexMarker291"/> this cluster <a id="_idIndexMarker292"/>specification, including storage and access permission, and <strong class="source-inline">init</strong> scripts. The easiest way to generate a working<a id="_idIndexMarker293"/> cluster specification JSON file is to use the Databricks portal UI to create a new cluster, where you can select the Databricks<a id="_idIndexMarker294"/> runtime version, cluster node types, and other parameters (<a href="https://docs.databricks.com/clusters/create.html">https://docs.databricks.com/clusters/create.html</a>). Then, you can get the JSON representation of the cluster by clicking on the JSON link on the top right of the <strong class="bold">Create Cluster</strong> UI page (see <em class="italic">Figure 5.2</em>).</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="Images/B18120_05_02.jpg" alt="Figure 5.2 - An example of creating a cluster on Databricks &#13;&#10;" width="1079" height="795"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 - An example of creating a cluster on Databricks </p>
			<p>Also notice that the <strong class="source-inline">experiment-name</strong> parameter in the preceding command no longer just takes an experiment name string but needs to include an absolute path in the Databricks workspace. This is different from the local MLflow tracking server. This convention must be followed to make this remote job submission work. Note that if you want to have several levels of subfolder structures, such as the following, then each subfolder must already exist in the Databricks server:</p>
			<p class="source-code">/rootPath/subfolder1/subfolder2/<strong class="bold">my_experiment_name</strong></p>
			<p>This means that the <strong class="source-inline">rootPath</strong>, <strong class="source-inline">subfolder1,</strong> and <strong class="source-inline">subfolder2</strong> folders must already exist. If not, the command line will fail since it cannot create the parent folder automatically on the Databricks server. That last string, <strong class="source-inline">my_experiment_name</strong>, can be automatically created if it does not already exist since that's the actual experiment name that will host all the experiment runs. Note that, in this example, we are using the command-line parameter to specify the experiment name, but it is also possible to use the environment variable to specify it, as follows:</p>
			<p class="source-code"><strong class="bold">export MLFLOW_EXPERIMENT_NAME=/Shared/dl_model_chapter05</strong></p>
			<ol>
				<li value="3">Once this <a id="_idIndexMarker295"/>command is <a id="_idIndexMarker296"/>executed, you will see <a id="_idIndexMarker297"/>a much shorter console output message this time compared with the previous run in a local environment. This is because when executing code this way, it runs <em class="italic">asynchronous</em>, which means the job is submitted to the remote <a id="_idIndexMarker298"/>Databricks server and immediately returns to the console without waiting. Let's look at the first three lines of the output:<p class="source-code"><strong class="bold">INFO: '/Shared/dl_model_chapter05' does not exist. Creating a new experiment</strong></p><p class="source-code"><strong class="bold">2022/01/06 17:35:32 INFO mlflow.projects.databricks: === Uploading project to DBFS path /dbfs/mlflow-experiments/427565/projects-code/f1cbec57b21eabfca52f417f8482054bbea22be 9205b5bbde461780d809924c2.tar.gz ===</strong></p><p class="source-code"><strong class="bold">2022/01/06 17:35:32 INFO mlflow.projects.databricks: === Finished uploading project to /dbfs/mlflow-experiments/427565/projects-code/f1cbec57b21eabfca52f417f8482054bbea22be 9205b5bbde461780d809924c2.tar.gz ===</strong></p></li>
			</ol>
			<p>The first line<a id="_idIndexMarker299"/> means that the experiment does not exist in the Databricks server, so it is being created. If you run this a second time, this will not show up. The second and third lines <a id="_idIndexMarker300"/>describe the process where MLflow<a id="_idIndexMarker301"/> packages the MLproject as a <strong class="source-inline">.tar.gz</strong> file and uploads it to the Databricks file server. Note that, unlike a GitHub project where it needs to check out the entire project from the repository, here, it only needs to package the <strong class="source-inline">chapter05</strong> folder since that's where our MLproject resides. This can be confirmed by looking at the job running logs<a id="_idIndexMarker302"/> in the Databricks cluster, which we will explain (where to get the job URL and how to look for the logs) in the next few paragraphs.</p>
			<p class="callout-heading">Synchronous and Asynchronous Running of MLproject</p>
			<p class="callout">The official MLflow <a id="_idIndexMarker303"/>run CLI does not support a parameter to specify <a id="_idIndexMarker304"/>the running of an MLflow project in asynchronous or synchronous mode. However, the MLflow run Python API does have a parameter called <strong class="source-inline">synchronous</strong>, which by default is set to be <strong class="source-inline">True</strong>. When using MLflow's CLI to run an MLflow job using Databricks as the backend, the default behavior is asynchronous. Sometimes, synchronous behavior of the CLI run command is desirable during CI/CD automation when you need to make sure the MLflow run completes successfully before moving to the next step. This cannot be done with the official MLflow run CLI, but you can write a wrapper CLI Python function to call MLflow's Python API with synchronous mode set to <strong class="source-inline">True</strong> and then use your own CLI Python command to run the MLflow job in synchronous mode. Also, note that <strong class="source-inline">mlflow.run()</strong> is the high-level fluent (object-oriented) API for the <strong class="source-inline">mlflow.projects.run()</strong> API. We use the <strong class="source-inline">mlflow.run()</strong> API extensively in this book for consistency. For details on the <a id="_idIndexMarker305"/>MLflow run Python API, see the official documentation page: <a href="https://www.mlflow.org/docs/latest/python_api/mlflow.projects.html#mlflow.projects.run">https://www.mlflow.org/docs/latest/python_api/mlflow.projects.html#mlflow.projects.run</a>. </p>
			<p>The next few lines of the output look similar to the following:</p>
			<p class="source-code">2022/01/06 17:48:31 INFO mlflow.projects.databricks: === Running entry point main of project . on Databricks ===</p>
			<p class="source-code">2022/01/06 17:48:31 INFO mlflow.projects.databricks: === Launched MLflow run as Databricks job run with ID <strong class="bold">279456</strong>. Getting run status page URL... ===</p>
			<p class="source-code">2022/01/06 17:48:31 INFO mlflow.projects.databricks: === Check the run's status at https://???.cloud.databricks.com#job/<strong class="bold">168339</strong>/run/1 ===</p>
			<p>These<a id="_idIndexMarker306"/> lines describe that the <a id="_idIndexMarker307"/>job has been <a id="_idIndexMarker308"/>submitted to the <a id="_idIndexMarker309"/>Databricks server and the job run ID and the job URL are shown in the last line (replace <strong class="source-inline">???</strong> with your actual Databricks URL to make this work for you). Notice that the MLflow run ID is <strong class="source-inline">279456</strong>, which is different from the ID you see in the job URL (<strong class="source-inline">168339</strong>). This is because the job URL is managed by the Databricks job management system and has a different way to generate and track each actual job. </p>
			<ol>
				<li value="4">Click the job URL link (<strong class="source-inline">https://???.cloud.databricks.com#job/168339/run/1</strong>) and check the status of this job, which will show the progress and standard output and error logs (see <em class="italic">Figure 5.3</em>). Usually, this<a id="_idIndexMarker310"/> page will <a id="_idIndexMarker311"/>take a<a id="_idIndexMarker312"/> few minutes to <a id="_idIndexMarker313"/>start showing the running progress since it needs to create a brand new cluster based on <strong class="source-inline">cluster_spec.json</strong> before it can start running the job.</li>
			</ol>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="Images/B18120_05_03.jpg" alt="Figure 5.3 – MLflow run job status page with standard output&#13;&#10;" width="643" height="874"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – MLflow run job status page with standard output</p>
			<p><em class="italic">Figure 5.3</em> shows the job was successfully finished (<strong class="bold">Status: Succeeded</strong>) and the standard output, which shows the content of the <strong class="source-inline">chapter05</strong> folder was uploaded and extracted in<a id="_idIndexMarker314"/> the <strong class="bold">Databricks File System</strong> (<strong class="bold">DBFS</strong>). As mentioned previously, only the MLproject we want to run was packaged, uploaded, and extracted in the DBFS, not the entire project repository.</p>
			<p>On the same job status page, you will also find the standard errors section, which shows the logs describing the pipeline step we wanted to run: <strong class="source-inline">download_data</strong>. These are not errors but just informational messages. All Python logs are aggregated here. See <em class="italic">Figure 5.4</em> for details:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="Images/B18120_05_04.jpg" alt="Figure 5.4 – MLflow job information logged on the job status page&#13;&#10;" width="1262" height="377"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – MLflow job information logged on the job status page</p>
			<p><em class="italic">Figure 5.4</em> shows<a id="_idIndexMarker315"/> the log that's very similar to <a id="_idIndexMarker316"/>what we see when we run in<a id="_idIndexMarker317"/> the local interactive<a id="_idIndexMarker318"/> environment, but now these runs were executed in the cluster we specified when we submitted the job. Note that the pipeline experiment ID is <strong class="source-inline">427565</strong> in <em class="italic">Figure 5.4</em>. You should be able to find the successfully completed MLflow DL pipeline runs in the integrated MLflow tracking server on the Databricks server, using the experiment ID <strong class="source-inline">427565</strong> in the following URL pattern:</p>
			<p><strong class="source-inline">https://[your databricks hostname]/#mlflow/experiments/427565</strong></p>
			<p>If you see the familiar tracking results as we have seen in previous chapters, give yourself a big hug since you just completed a major learning milestone in running local code in a remote Databricks cluster!</p>
			<p>Furthermore, we can run multiple steps of the DL pipeline using this approach without changing any code in the individual step's implementation. For example, if we want to run both the <strong class="source-inline">download_data</strong> and <strong class="source-inline">fine_tuning_model</strong> steps of the DL pipeline, we can issue the following command:</p>
			<p class="source-code">mlflow run . -b databricks --backend-config cluster_spec.json --experiment-name='/Shared/dl_model_chapter05' -P pipeline_steps='download_data,fine_tuning_model'</p>
			<p>The output console will show the following short messages:</p>
			<p class="source-code">2022/01/07 15:22:39 INFO mlflow.projects.databricks: === Uploading project to DBFS path /dbfs/mlflow-experiments/427565/projects-code/743cadfec82a55b8c76e9f27754cfdd516545b155254e990c2cc62650b8af959.tar.gz ===</p>
			<p class="source-code">2022/01/07 15:22:40 INFO mlflow.projects.databricks: === Finished uploading project to /dbfs/mlflow-experiments/427565/projects-code/743cadfec82a55b8c76e9f27754cfdd516545b155254e990c2cc62650b8af959.tar.gz ===</p>
			<p class="source-code">2022/01/07 15:22:40 INFO mlflow.projects.databricks: === Running entry point main of project . on Databricks ===</p>
			<p class="source-code">2022/01/07 15:22:40 INFO mlflow.projects.databricks: === Launched MLflow run as Databricks job run with ID 279540. Getting run status page URL... ===</p>
			<p class="source-code">2022/01/07 15:22:40 INFO mlflow.projects.databricks: === Check the run's status at https://?????.cloud.databricks.com#job/168429/run/1 ===</p>
			<p>You can then<a id="_idIndexMarker319"/> go to the job URL page <a id="_idIndexMarker320"/>shown in the last line of the console<a id="_idIndexMarker321"/> output and wait until it<a id="_idIndexMarker322"/> creates a new cluster and completes both steps. You should then be able to find both steps in the experiment folder logged in the MLflow tracking server, using the same experiment URL (since we use the same experiment name):</p>
			<p><strong class="source-inline">https://[your databricks hostname]/#mlflow/experiments/427565</strong></p>
			<p>Now that we know how to run local code in a remote Databricks cluster, we will learn how to run the code from a GitHub repository in a remote Databricks cluster.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor066"/>Running remotely in the cloud with remote code in GitHub</h1>
			<p>The most reliable <a id="_idIndexMarker323"/>way to <a id="_idIndexMarker324"/>reproduce a DL <a id="_idIndexMarker325"/>pipeline is to point <a id="_idIndexMarker326"/>to a specific version of the project code in GitHub and then run it in the cloud without invoking any local resources. This way, we know the exact version of the code as well as using the same running environment defined in the project. Let's see how this works with our DL pipeline. </p>
			<p>As a prerequisite and a reminder, the following three environment variables need to be set up before you issue the MLflow run command to complete this section of the learning:</p>
			<p class="source-code">export MLFLOW_TRACKING_URI=databricks</p>
			<p class="source-code">export DATABRICKS_TOKEN=[databricks_token]</p>
			<p class="source-code">export DATABRICKS_HOST='https://[your databricks host name/'</p>
			<p>We already know how to set up these environment variables from the last section. There is potentially one more setup needed, which is to allow your Databricks server to access your GitHub repository if it is non-public (see the following <em class="italic">GitHub Token for Databricks to Access a Non-Public or Enterprise Project Repository</em> box).</p>
			<p class="callout-heading">GitHub Token for Databricks to Access a Non-Public or Enterprise Project Repository</p>
			<p class="callout">To allow Databricks to <a id="_idIndexMarker327"/>access the project repository in GitHub, there is another token that's needed. This can be generated by going to your personal GitHub page (https://github.com/settings/tokens) and then following the steps described on this page (<a href="https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token">https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token</a>). You can then <a id="_idIndexMarker328"/>follow the instructions on the Databricks<a id="_idIndexMarker329"/> documentation website to set it up: <a href="https://docs.databricks.com/repos.html#configure-your-git-integration-with-databricks">https://docs.databricks.com/repos.html#configure-your-git-integration-with-databricks</a>.</p>
			<p>Now, let's run <a id="_idIndexMarker330"/>the project <a id="_idIndexMarker331"/>using the specific<a id="_idIndexMarker332"/> version in <a id="_idIndexMarker333"/>the GitHub repository for the full pipeline on the remote Databricks cluster:</p>
			<p class="source-code">mlflow run https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow#chapter05 -v 395c33858a53bcd8ac217a962ab81e148d9f1d9a -b databricks --backend-config cluster_spec.json --experiment-name='/Shared/dl_model_chapter05' -P pipeline_steps='all'</p>
			<p>We will then see the output as brief as six lines. Let's look at what the important information on each line shows and how this works:</p>
			<ol>
				<li value="1">The first line shows where the content of the project repository was downloaded to locally:<p class="source-code"><strong class="bold">2022/01/07 17:36:54 INFO mlflow.projects.utils: === Fetching project from https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow#chapter05 into</strong> <strong class="bold">/var/folders/51/whxjy4r92dx18788yp11ycyr0000gp/T/tmpzcepn5h5</strong> ===</p></li>
			</ol>
			<p>If we go to the temporary directory shown in this message on the local machine where we execute this command, we see that the entire repository is already downloaded to this folder: <strong class="source-inline">/var/folders/51/whxjy4r92dx18788yp11ycyr0000gp/T/tmpzcepn5h5</strong>. </p>
			<ol>
				<li value="2">The next two lines show the project content was zipped and uploaded to a DBFS folder on the Databricks server:<p class="source-code"><strong class="bold">2022/01/07 17:36:57 INFO mlflow.projects.databricks: === Uploading project to DBFS path /dbfs/mlflow-experiments/427565/projects-code/fba3d31e1895b78f40227b5965461faddb 61ec9df906fb09b161f74efaa90aa2.tar.gz ===</strong></p><p class="source-code"><strong class="bold">2022/01/07 17:36:57 INFO mlflow.projects.databricks: === Finished uploading project to /dbfs/mlflow-experiments/427565/projects-code/fba3d31e1895b78f40227b5965461faddb61ec 9df906fb09b161f74efaa90aa2.tar.gz ===</strong></p></li>
			</ol>
			<p>If we <a id="_idIndexMarker334"/>use the <a id="_idIndexMarker335"/>local <a id="_idIndexMarker336"/>command-line <a id="_idIndexMarker337"/>tool of Databricks, we can list this <strong class="source-inline">.tar.gz</strong> file as if it is a local file (but in fact, it is located remotely on the Databricks server):</p>
			<p class="source-code"><strong class="bold">databricks fs ls -l dbfs:/mlflow-experiments/427565/projects-code/fba3d31e1895b78f40227b5965461faddb61ec 9df906fb09b161f74efaa90aa2.tar.gz</strong></p>
			<p>You should see output similar to the following, which describes the attributes of the file (size, owner/group ID, and whether it is a file or directory):</p>
			<p class="source-code"><strong class="bold">file  3070  fba3d31e1895b78f40227b5965461faddb61ec 9df906fb09b161f74efaa90aa2.tar.gz  1641605818000</strong></p>
			<ol>
				<li value="3">The next line shows that it starts to run the <strong class="source-inline">main</strong> entry point for this GitHub project:<p class="source-code"><strong class="bold">2022/01/07 17:36:57 INFO mlflow.projects.databricks: === Running entry point main of project https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow#chapter05 on Databricks ===</strong></p></li>
			</ol>
			<p>Note the difference when we run the local code (it was a <em class="italic">dot</em> after the project, which means the current directory on the local system). Now, it lists the full path of the GitHub repository location.</p>
			<ol>
				<li value="4">The last two lines are like the previous section's output, where it lists out the job URL:<p class="source-code"><strong class="bold">2022/01/07 17:36:57 INFO mlflow.projects.databricks: === Launched MLflow run as Databricks job run with ID 279660. Getting run status page URL... ===</strong></p><p class="source-code"><strong class="bold">2022/01/07 17:36:57 INFO mlflow.projects.databricks: === Check the run's status at https://????.cloud.databricks.com#job/168527/run/1 ===</strong></p></li>
				<li>If we <a id="_idIndexMarker338"/>click the job <a id="_idIndexMarker339"/>URL in <a id="_idIndexMarker340"/>the last <a id="_idIndexMarker341"/>line of the console output, we will be able to see the following content on that website (<em class="italic">Figure 5.5</em>):</li>
			</ol>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="Images/B18120_05_05.jpg" alt="Figure 5.5 – MLflow run job status page using the code from the GitHub repository&#13;&#10;" width="1266" height="319"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – MLflow run job status page using the code from the GitHub repository</p>
			<p><em class="italic">Figure 5.5</em> shows the end status of this job. Notice that the title of the page now says <strong class="bold">MLflow Run for https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow#chapter05</strong>, instead of <strong class="bold">MLflow Run for .</strong> as shown in the previous section when using local code to run.</p>
			<p>The status of the job shows this was run successfully and you will also see that the results are logged in the experiment page as before, with all three steps finished. The model is also registered in the model registry as expected, in the Databricks server under the following URL:</p>
			<p><strong class="source-inline">https://[your_databricks_hostname]/#mlflow/models/dl_finetuned_model</strong></p>
			<p>In summary, the mechanism of how this approach works is shown in the following diagram (<em class="italic">Figure 5.6</em>):</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="Images/B18120_05_06.jpg" alt="Figure 5.6 – Summary view of running remote GitHub code in a remote Databricks cluster server&#13;&#10;" width="1346" height="622"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Summary view of running remote GitHub code in a remote Databricks cluster server</p>
			<p><em class="italic">Figure 5.6</em> shows<a id="_idIndexMarker342"/> that there<a id="_idIndexMarker343"/> are three <a id="_idIndexMarker344"/>different <a id="_idIndexMarker345"/>locations (a machine where we issue the MLflow run command, a remote Databricks server, and a remote GitHub project). When an MLflow run command is issued, the remote GitHub project source code is cloned to the machine where the MLflow run command was issued, and then uploaded to the remote Databricks server with a job submitted to execute the multiple steps of the DL pipeline. This is an asynchronous execution, and the status of the job needs to be monitored based on the job URL created. </p>
			<p class="callout-heading">Running an MLflow Project on Other Backends</p>
			<p class="callout">Right now, Databricks<a id="_idIndexMarker346"/> supports two types of remote running <a id="_idIndexMarker347"/>backend environments: Databricks and K8s. However, as of MLflow version 1.22.0 (<a href="https://www.mlflow.org/docs/latest/projects.html#run-an-mlflow-project-on-kubernetes-experimental">https://www.mlflow.org/docs/latest/projects.html#run-an-mlflow-project-on-kubernetes-experimental</a>), running MLflow projects on K8s is still in experimental mode and is subject to change. If you are interested in learning more about this, refer to the reference in the <em class="italic">Further reading</em> section to explore an example provided. There are also other <a id="_idIndexMarker348"/>third-party provided backends (also called community plugins) such as <strong class="source-inline">hadoop-yarn</strong> (<a href="https://github.com/criteo/mlflow-yarn">https://github.com/criteo/mlflow-yarn</a>). Due to the availability of Databricks in all major cloud providers and its maturity in supporting enterprise security-compliant production scenarios, this book currently focuses on learning about running MLflow projects remotely in a Databricks server.</p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor067"/>Summary</h1>
			<p>In this chapter, we have learned how to run a DL pipeline in different execution environments (local or remote Databricks clusters) using either local source code or GitHub project repository code. This is critical not just for reproducibility and flexibility in executing a DL pipeline, but also provides much better productivity and future automation possibility using CI/CD tools. The power to run one or multiple steps of a DL pipeline in remote resource-rich environments gives us the speed to execute large-scale computation and data-intensive jobs that are typically seen in production-quality DL model training and fine-tuning. This allows us to do hyperparameter tuning or cross-validation of a DL model if necessary. We will start to learn how to run large-scale hyperparameter tuning in the next chapter as our natural next step.</p>
			<h1 id="_idParaDest-69"><a id="_idTextAnchor068"/>Further reading</h1>
			<ul>
				<li>MLflow run projects parameters (for both command line and Python API): <a href="https://www.mlflow.org/docs/latest/projects.html#running-projects">https://www.mlflow.org/docs/latest/projects.html#running-projects</a></li>
				<li>MLflow run command line (CLI) documentation: <a href="https://www.mlflow.org/docs/latest/cli.html#mlflow-run">https://www.mlflow.org/docs/latest/cli.html#mlflow-run</a></li>
				<li>MLflow run projects on Databricks: <a href="https://www.mlflow.org/docs/latest/projects.html#run-an-mlflow-project-on-databricks">https://www.mlflow.org/docs/latest/projects.html#run-an-mlflow-project-on-databricks</a></li>
				<li>An example of running an MLflow project on K8s: <a href="https://github.com/SameeraGrandhi/mlflow-on-k8s/tree/master/examples/LogisticRegression">https://github.com/SameeraGrandhi/mlflow-on-k8s/tree/master/examples/LogisticRegression</a></li>
				<li>Running MLflow projects on Azure: <a href="https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-mlflow-projects">https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-mlflow-projects</a></li>
			</ul>
		</div>
	</div></body></html>