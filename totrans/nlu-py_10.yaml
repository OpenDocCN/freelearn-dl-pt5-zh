- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning Part 2 – Neural Networks and Deep Learning Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Neural networks** (**NNs**) have only became popular in **natural language
    understanding** (**NLU**) around 2010 but have since been widely applied to many
    problems. In addition, there are many applications of NNs to non-**natural language
    processing** (**NLP**) problems such as image classification. The fact that NNs
    are a general approach that can be applied across different research areas has
    led to some interesting synergies across these fields.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will cover the application of **machine learning** (**ML**)
    techniques based on NNs to problems such as NLP classification. We will also cover
    several different kinds of commonly used NNs—specifically, fully connected **multilayer
    perceptrons** (**MLPs**), **convolutional NNs** (**CNNs**), and **recurrent NNs**
    (**RNNs**)—and show how they can be applied to problems such as classification
    and information extraction. We will also discuss fundamental NN concepts such
    as hyperparameters, learning rate, activation functions, and epochs. We will illustrate
    NN concepts with a classification example using the TensorFlow/Keras libraries.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Basics of NNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Example—MLP for classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameters and tuning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving beyond MLPs—RNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Looking at another approach—CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Basics of NNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic concepts behind NNs have been studied for many years but have only
    fairly recently been applied to NLP problems on a large scale. Currently, NNs
    are one of the most popular tools for solving NLP tasks. NNs are a large field
    and are very actively researched, so we won’t be able to give you a comprehensive
    understanding of NNs for NLP. However, we will attempt to provide you with some
    basic knowledge that will let you apply NNs to your own problems.
  prefs: []
  type: TYPE_NORMAL
- en: NNs are inspired by some properties of the animal nervous system. Specifically,
    animal nervous systems consist of a network of interconnected cells, called *neurons*,
    that transmit information throughout the network with the result that, given an
    input, the network produces an output that represents a decision about the input.
  prefs: []
  type: TYPE_NORMAL
- en: '**Artificial NNs** (**ANNs**) are designed to model this process in some respects.
    The decision about how to react to the inputs is determined by a sequence of processing
    steps starting with units (*neurons*) that receive inputs and create outputs (or
    *fire*) if the correct conditions are met. When a neuron fires, it sends its output
    to other neurons. These next neurons receive inputs from a number of other neurons,
    and they in turn fire if they receive the right inputs. Part of the decision process
    about whether to fire involves *weights* on the neurons. The way that the NN learns
    to do its task—that is, the *training* process—is the process of adjusting the
    weights to produce the best results on the training data.'
  prefs: []
  type: TYPE_NORMAL
- en: The training process consists of a set of *epochs*, or passes through the training
    data, adjusting the weights on each pass to try to reduce the discrepancy between
    the result produced by the NN and the correct results.
  prefs: []
  type: TYPE_NORMAL
- en: The neurons in an NN are arranged in a series of layers, with the final layer—the
    output layer—producing the decision. Applying these concepts to NLP, we will start
    with an input text that is fed to the input layer, which represents the input
    being processed. Processing proceeds through all the layers, continuing through
    the NN until it reaches the output layer, which provides the decision—for example,
    is this movie review positive or negative?
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 10**.1* represents a schematic diagram of an NN with an input layer,
    two hidden layers, and an output layer. The NN in *Figure 10**.1* is a **fully
    connected NN** (**FCNN**) because every neuron receives inputs from every neuron
    in the preceding layer and sends outputs to every neuron in the following layer:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 10.1 – A\uFEFFn FCNN with two hidden layers](img/B19005_10_01.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – An FCNN with two hidden layers
  prefs: []
  type: TYPE_NORMAL
- en: 'The field of NNs uses a lot of specialized vocabulary, which can sometimes
    make it difficult to read documentation on the topic. In the following list, we’ll
    provide a brief introduction to some of the most important concepts, referring
    to *Figure 10**.1* as needed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Activation function**: The activation function is the function that determines
    when a neuron has enough inputs to fire and transmit its output to the neurons
    in the next layer. Some common activation functions are sigmoid and **rectified
    linear** **unit** (**ReLU**).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Backpropagation**: The process of training an NN where the loss is fed back
    through the network to train the weights.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch**: A batch is a set of samples that will be trained together.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Connection**: A link between neurons, associated with a weight that represents
    the strength of the connection. The lines between neurons in *Figure 10**.1* are
    connections.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convergence**: A network has converged when additional epochs do not appear
    to produce any reduction in the loss or improvements in accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout**: A technique for preventing overfitting by randomly removing neurons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Early stopping**: Ending training before the planned number of epochs because
    training appears to have converged.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epoch**: One pass through the training data, adjusting the weights to minimize
    loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Error**: The difference between the predictions produced by an NN and the
    reference labels. Measures how well the network predicts the classification of
    the data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exploding gradients**: Exploding gradients occur when gradients become unmanageably
    large during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Forward propagation**: Propagation of inputs forward through an NN from the
    input layer through the hidden layers to the output layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fully connected**: An FCNN is an NN “*with every neuron in one layer connecting
    to every neuron in the next layer*” ([https://en.wikipedia.org/wiki/Artificial_neural_network](https://en.wikipedia.org/wiki/Artificial_neural_network)),
    as shown in *Figure 10**.1*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gradient descent**: Optimizing weights by adjusting them in a direction that
    will minimize loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hidden layer**: A layer of neurons that is not the input or output layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameters**: Parameters that are not learned and are usually adjusted
    in a manual tuning process in order for the network to produce optimal results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Input layer**: The layer in an NN that receives the initial data. This is
    the layer on the left in *Figure 10**.1*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Layer**: A set of neurons in an NN that takes information from the previous
    layer and passes it on to the next layer. *Figure 10**.1* includes four layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning**: Assigning weights to connections in the training process in order
    to minimize loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate/adaptive learning rate**: Amount of adjustment to the weights
    after each epoch. In some approaches, the learning rate can adapt as the training
    progresses; for example, if learning starts to slow, it can be useful to decrease
    the learning rate.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Loss**: A function that provides a metric that quantifies the distance between
    the current model’s predictions and the goal values. The training process attempts
    to minimize loss.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MLP**: As described on *Wikipedia*, “*a fully connected class of feedforward
    artificial neural network (ANN). An MLP consists of at least three layers of nodes:
    an input layer, a hidden layer and an output layer*” ([https://en.wikipedia.org/wiki/Multilayer_perceptron](https://en.wikipedia.org/wiki/Multilayer_perceptron)).
    *Figure 10**.1* shows an example of an MLP.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neuron (unit)**: A unit in an NN that receives inputs and computes outputs
    by applying an activation function.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimization**: Adjustment to the learning rate during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output layer**: The final layer in an NN that produces a decision about the
    input. This is the layer on the right in *Figure 10**.1*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overfitting**: Tuning the network too closely to the training data so that
    it does not generalize to previously unseen test or validation data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Underfitting**: Underfitting occurs when an NN is unable to obtain good accuracy
    for training data. It can be addressed by using more training epochs or more layers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Vanishing gradients**: Gradients that become so small that the network is
    unable to make progress.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Weights**: A property of the connection between neurons that represents the
    strength of the connection. Weights are learned during training.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we will make these concepts concrete by going through an
    example of text classification with a basic MLP.
  prefs: []
  type: TYPE_NORMAL
- en: Example – MLP for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will review basic NN concepts by looking at the MLP, which is conceptually
    one of the most straightforward types of NNs. The example we will use is the classification
    of movie reviews into reviews with positive and negative sentiments. Since there
    are only two possible categories, this is a *binary* classification problem. We
    will use the *Sentiment Labelled Sentences Data Set* (*From Group to Individual
    Labels using Deep Features*, *Kotzias et al.*, *KDD 2015* [https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences)),
    available from the University of California, Irvine. Start by downloading the
    data and unzipping it into a directory in the same directory as your Python script.
    You will see a directory called `sentiment labeled sentences` that contains the
    actual data in a file called `imdb_labeled.txt`. You can install the data into
    another directory of your choosing, but if you do, be sure to modify the `filepath_dict`
    variable accordingly.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can take a look at the data using the following Python code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The output from the last `print` statement will include the first sentence in
    the corpus, its label (`1` or `0`—that is, positive or negative), and its source
    (`Internet Movie` `Database IMDB`).
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we will vectorize the corpus using the scikit-learn count `CountVectorizer`),
    which we saw earlier in [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144).
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code snippet shows the start of the vectorization process, where
    we set up some parameters for the vectorizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The `CountVectorizer` function has some useful parameters that control the maximum
    number of words that will be used to build the model, as well as make it possible
    to exclude words that are considered to be too frequent or too rare to be very
    useful in distinguishing documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to do the train-test split, as shown in the following code
    block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code shows the splitting of the training and test data, reserving
    20% of the total data for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `reviews` variable holds the actual documents, and the `y` variable holds
    their labels. Note that `X` and `y` are frequently used in the literature to represent
    the data and the categories in an ML problem, respectively, although we’re using
    `reviews` for the `X` data here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code shows the process of vectorizing the data, or converting
    each document to a numerical representation, using the vectorizer that was defined
    previously. You can review vectorization by going back to [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144).
  prefs: []
  type: TYPE_NORMAL
- en: The result is `X_train`, the count BoW of the dataset. You will recall the count
    BoW from [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144).
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to set up the NN. We will be using the Keras package, which
    is built on top of Google’s TensorFlow ML package. Here’s the code we need to
    execute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The code first prints the input dimension, which in this case is the number
    of words in each document vector. The input dimension is useful to know because
    it’s computed from the corpus, as well as the parameters we set in the `CountVectorizer`
    function. If it is unexpectedly large or small, we might want to change the parameters
    to make the vocabulary larger or smaller.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code defines the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The model built in the preceding code includes the input layer, two hidden layers,
    and one output layer. Each call to the `model.add()` method adds a new layer to
    the model. All the layers are dense because, in this fully connected network,
    every neuron receives inputs from every neuron in the previous layer, as illustrated
    in *Figure 10**.1*. The 2 hidden layers each contain 16 neurons. Why do we specify
    16 neurons? There is no hard and fast rule for how many neurons to include in
    the hidden layers, but a general approach would be to start with a smaller number
    since the training time will increase as the number of neurons increases. The
    final output layer will only have one neuron because we only want one output for
    this problem, whether the review is positive or negative.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another very important parameter is the **activation function**. The activation
    function is the function that determines how the neuron responds to its inputs.
    For all of the layers in our example, except the output layer, this is the ReLU
    activation function. The ReLU function can be seen in *Figure 10**.2*. ReLU is
    a very commonly used activation function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Values of the ReLU function for inputs between -15 and 15](img/B19005_10_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Values of the ReLU function for inputs between -15 and 15
  prefs: []
  type: TYPE_NORMAL
- en: One of the most important benefits of the ReLU function is that it is very efficient.
    It has also turned out to generally give good results in practice and is normally
    a reasonable choice as an activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other activation function that’s used in this NN is the sigmoid function,
    which is used in the output layer. We use the sigmoid function here because in
    this problem we want to predict the probability of a positive or negative sentiment,
    and the value of the sigmoid function will always be between `0` and `1`. The
    formula for the sigmoid function is shown in the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: S(x) =  1 _ 1 + e −x
  prefs: []
  type: TYPE_NORMAL
- en: 'A plot of the sigmoid function is shown in *Figure 10**.3*, and it is easy
    to see that its output value will always be between `0` and `1` regardless of
    the value of the input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Values of the sigmoid  function for inputs between -10 and
    10](img/B19005_10_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Values of the sigmoid function for inputs between -10 and 10
  prefs: []
  type: TYPE_NORMAL
- en: 'The sigmoid and ReLU activation functions are popular and practical activation
    functions, but they are only two examples of the many possible NN activation functions.
    If you wish to investigate this topic further, the following *Wikipedia* article
    is a good place to start: [https://en.wikipedia.org/wiki/Activation_function](https://en.wikipedia.org/wiki/Activation_function).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been defined, we can compile it, as shown in the following
    code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The `model.compile()` method requires the `loss`, `optimizer`, and `metrics`
    parameters, which supply the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: The `loss` parameter, in this case, tells the compiler to use `binary_crossentropy`
    to compute the loss. `categorical_crossentropy`, is used for problems when there
    are two or more label classes in the output. For example, if the task were to
    assign a star rating to reviews, we might have five output classes corresponding
    to the five possible star ratings, and in that case, we would use categorical
    cross-entropy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `optimizer` parameter adjusts the learning rate during training. We will
    not go into the mathematical details of `adam` here, but generally speaking, the
    optimizer we use here, `adam`, normally turns out to be a good choice.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, the `metrics` parameter tells the compiler how we will evaluate the
    quality of the model. We can include multiple metrics in this list, but we will
    just include `accuracy` for now. In practice, the metrics you use will depend
    on your problem and dataset, but `accuracy` is a good metric to use for the purposes
    of our example. In [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), we will explore
    other metrics and the reasons that you might want to select them in particular
    situations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It is also helpful to display a summary of the model to make sure that the
    model is structured as intended. The `model.summary()` method will produce a summary
    of the model, as shown in the following code snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: In this output, we can see that the network, consisting of four dense layers
    (which are the input layer, the two hidden layers, and the output layer), is structured
    as expected.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step is to fit or train the network, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Training is the iterative process of putting the training data through the network,
    measuring the loss, adjusting the weights to reduce the loss, and putting the
    training data through the network again. This step can be quite time-consuming,
    depending on the size of the dataset and the size of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Each cycle through the training data is an epoch. The number of epochs in the
    training process is a *hyperparameter*, which means that it’s adjusted by the
    developer based on the training results. For example, if the network’s performance
    doesn’t seem to be improving after a certain number of epochs, the number of epochs
    can be reduced since additional epochs are not improving the result. Unfortunately,
    there is no set number of epochs after which we can stop training. We have to
    observe the improvements in accuracy and loss over epochs to decide whether the
    system is sufficiently trained.
  prefs: []
  type: TYPE_NORMAL
- en: Setting the `verbose = True` parameter is optional but useful because this will
    produce a trace of the results after each epoch. If the training process is long,
    the trace can help you verify that the training is making progress. The batch
    size is another hyperparameter that defines how many data samples are to be processed
    before updating the model. When the following Python code is executed, with `verbose`
    set to `True`, at the end of every epoch, the loss, the accuracy, and the validation
    loss and accuracy will be computed. After training is complete, the `history`
    variable will contain information about the progress of the training process,
    and we can see plots of the training progress.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is important to display how the plots of accuracy and loss change with each
    epoch because it will give us an idea of how many epochs are needed to get this
    training to converge and will make it very clear when the data is overfitting.
    The following code shows how to plot the accuracy and loss changes over epochs::'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the results of the progress of our example through training over
    20 epochs in *Figure 10**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Accuracy and loss over 20 epochs of training](img/B19005_10_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.4 – Accuracy and loss over 20 epochs of training
  prefs: []
  type: TYPE_NORMAL
- en: Over the 20 epochs of training, we can see that the training accuracy approaches
    **1.0** and the training loss approaches **0**. However, this apparently good
    result is misleading because the really important results are based on the validation
    data. Because the validation data is not being used to train the network, it is
    the performance on the validation data that actually predicts how the network
    will perform in use. We can see from the plots of the changes in validation accuracy
    and loss that doing more training epochs after about 10 is not improving the model’s
    performance on the validation data. In fact, it is increasing the loss and therefore
    making the model worse. This is clear from the increase in the validation loss
    in the graph on the right in *Figure 10**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: Improving performance on this task will involve modifying other factors, such
    as hyperparameters and other tuning processes, which we will go over in the next
    section.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameters and tuning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Figure 10**.4* clearly shows that increasing the number of training epochs
    is not going to improve performance on this task. The best validation accuracy
    seems to be about 80% after 10 epochs. However, 80% accuracy is not very good.
    How can we improve it? Here are some ideas. None of them is guaranteed to work,
    but it is worth experimenting with them:'
  prefs: []
  type: TYPE_NORMAL
- en: If more training data is available, the amount of training data can be increased.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preprocessing techniques that can remove noise from the training data can be
    investigated—for example, stopword removal, removing non-words such as numbers
    and HTML tags, stemming and lemmatization, and lowercasing. Details on these techniques
    were covered in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changes to the learning rate—for example, lowering the learning rate might improve
    the ability of the network to avoid local minima.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decreasing the batch size.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Changing the number of layers and the number of neurons in each layer is something
    that can be tried, but having too many layers is likely to lead to overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding dropout by specifying a hyperparameter that defines the probability that
    the outputs from a layer will be ignored. This can help make the network more
    robust to overfitting.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improvements in vectorization—for example, by using **term frequency-inverse
    document frequency** (**TF-IDF**) instead of count BoW.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A final strategy for improving performance is to try some of the newer ideas
    in NNs—specifically, RNNs, CNNs, and transformers.
  prefs: []
  type: TYPE_NORMAL
- en: We will conclude this chapter by briefly reviewing RNNs and CNNs. We will cover
    transformers in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193).
  prefs: []
  type: TYPE_NORMAL
- en: Moving beyond MLPs – RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: RNNs are a type of NN that is able to take into account the order of items in
    an input. In the example of the MLP that was discussed previously, the vector
    representing the entire input (that is, the complete document) was fed to the
    NN at once, so the network had no way of taking into account the order of words
    in the document. However, this is clearly an oversimplification in the case of
    text data since the order of words can be very important to the meaning. RNNs
    are able to take into account the order of words by using earlier outputs as inputs
    to later layers. This can be especially helpful in certain NLP problems where
    the order of words is very important, such as **named entity recognition** (**NER**),
    **part-of-speech (POS) tagging**, or **slot labeling**.
  prefs: []
  type: TYPE_NORMAL
- en: 'A diagram of a unit of an RNN is shown in *Figure 10**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – A unit of an RNN](img/B19005_10_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.5 – A unit of an RNN
  prefs: []
  type: TYPE_NORMAL
- en: The unit is shown at time *t*. The input at time *t*, *x(t)*, is passed to the
    activation function as in the case of the MLP, but the activation function also
    receives the output from time *t-1*—that is, *x(t-1)*. For NLP, the earlier input
    would most likely have been the previous word. So, in this case, the input is
    the current word and one previous word. Using an RNN with Keras is very similar
    to the MLP example that we saw earlier, with the addition of a new RNN layer in
    the layer stack.
  prefs: []
  type: TYPE_NORMAL
- en: However, as the length of the input increases, the network will tend to *forget*
    information from earlier inputs, because the older information will have less
    and less influence over the current state. Various strategies have been designed
    to overcome this limitation, such as **gated recurrent units** (**GRUs**) and
    **long short-term memory** (**LSTM**). If the input is a complete text document
    (as opposed to speech), we have access not only to previous inputs but also to
    future inputs, and a bidirectional RNN can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will not cover these additional variations of RNNs here, but they do often
    improve performance on some tasks, and it would be worth researching them. Although
    there is a tremendous amount of resources available on this popular topic, the
    following *Wikipedia* article is a good place to start: [https://en.wikipedia.org/wiki/Recurrent_neural_network](https://en.wikipedia.org/wiki/Recurrent_neural_network).'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at another approach – CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs are very popular for image recognition tasks, but they are less often used
    for NLP tasks than RNNs because they don’t take into account the temporal order
    of items in the input. However, they can be useful for document classification
    tasks. As you will recall from earlier chapters, the representations that are
    often used in classification depend only on the words that occur in the document—BoW
    and TF-IDF, for example—so, effective classification can often be accomplished
    without taking word order into account.
  prefs: []
  type: TYPE_NORMAL
- en: To classify documents with CNNs, we can represent a text as an array of vectors,
    where each word is mapped to a vector in a space made up of the full vocabulary.
    We can use word2vec, which we discussed in [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144),
    to represent word vectors. Training a CNN for text classification with Keras is
    very similar to the training process that we worked through in MLP classification.
    We create a sequential model as we did earlier, but we add new convolutional layers
    and pooling layers.
  prefs: []
  type: TYPE_NORMAL
- en: We will not cover the details of using CNNs for classification, but they are
    another option for NLP classification. As in the case of RNNs, there are many
    available resources on this topic, and a good starting point is *Wikipedia* ([https://en.wikipedia.org/wiki/Convolutional_neural_network](https://en.wikipedia.org/wiki/Convolutional_neural_network)).
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have explored applications of NNs to document classification
    in NLP. We covered the basic concepts of NNs, reviewed a simple MLP, and applied
    it to a binary classification problem. We also provided some suggestions for improving
    performance by modifying hyperparameters and tuning. Finally, we discussed the
    more advanced types of NNs—RNNs and CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193), we will cover the currently
    best-performing techniques in NLP—transformers and pretrained models.
  prefs: []
  type: TYPE_NORMAL
