<html><head></head><body>
		<div>
			<div id="_idContainer047" class="Content">
			</div>
		</div>
		<div id="_idContainer048" class="Content">
			<h1 id="_idParaDest-44"><a id="_idTextAnchor045"/>2. Machine Learning versus Deep Learning</h1>
		</div>
		<div id="_idContainer072" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we will begin creating Artificial Neural Networks (ANNs) using the Keras library. Before utilizing the library for modeling, we will get an introduction to the mathematics that comprise ANNs—understanding linear transformations and how they can be applied in Python. You'll build a firm grasp of the mathematics that make up ANNs. By the end of this chapter, we will have applied that knowledge by building a logistic regression model with Keras.</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor046"/>Introduction</h1>
			<p>In the previous chapter, we discussed some applications of machine learning and even built models with the scikit-learn Python package. The previous chapter covered how to preprocess real-world datasets so that they can be used for modeling. To do this, we converted all the variables into numerical data types and converted <strong class="source-inline">categorical</strong> variables into <strong class="source-inline">dummy</strong> variables. We used the <strong class="source-inline">logistic regression</strong> algorithm to classify users of a website by their purchase intention from the <strong class="source-inline">online shoppers purchasing intention</strong> dataset. We advanced our model-building skills by adding <strong class="source-inline">regularization</strong> to the dataset to improve the performance of our models.</p>
			<p>In this chapter, we will continue learning how to build machine learning models and extend our knowledge so that we can build an <strong class="source-inline">Artificial Neural Network</strong> (<strong class="source-inline">ANN</strong>) with the Keras package. (Remember that <strong class="source-inline">ANNs</strong> represent a large class of machine learning algorithms that are so-called because their architecture resembles the neurons in the human brain.)</p>
			<p><strong class="source-inline">Keras</strong> is a machine learning library designed specifically for building neural networks. While scikit-learn's functionality spans a broader area of machine learning algorithms, the functionality of <strong class="source-inline">scikit-learn</strong> for neural networks is minimal.</p>
			<p><strong class="source-inline">ANNs</strong> can be used for the same machine learning tasks that other algorithms can perform, such as <strong class="source-inline">logistic regression</strong> for <strong class="source-inline">classification</strong> tasks, <strong class="source-inline">linear regression</strong> for <strong class="source-inline">regression</strong> problems, and <strong class="source-inline">k-means</strong> for <strong class="source-inline">clustering</strong>. Whenever we begin any machine learning problem, to determine what kind of task it is (<strong class="source-inline">regression</strong>, <strong class="source-inline">classification</strong>, or <strong class="source-inline">clustering</strong>), we need to ask the following questions:</p>
			<ul>
				<li><strong class="bold">What outcomes matter the most to me or my business?</strong> For example, if you are predicting the value of stock market indices, you could predict whether the price is higher or lower than the previous time point (which would be a <strong class="source-inline">classification</strong> task) or you could predict the value itself (which would be a <strong class="source-inline">regression</strong> problem). Each may lead to a different subsequent action or trading strategy. <p>The following plot shows a <strong class="source-inline">candlestick chart</strong>. It describes the price movements in financial data and is depicting a stock price. The colors represent whether the stock price increased (green) or decreased (red) in value over each period, and each candlestick shows the open, close, high, and low values of the data—important pieces of information for stock prices. </p><p class="callout-heading">Note</p><p class="callout">You can find the high-quality color images for this chapter at: <a href="https://packt.live/38nenXS">https://packt.live/38nenXS</a>.</p><p>One goal of modeling this data would be to predict what happens the following day. A <strong class="source-inline">classification</strong> task might predict a positive or negative change in the stock price and since there are only two possible values, this would be a binary classification task. Another option would be to predict the value of the stock the following day. Since the predicted value would be a <strong class="source-inline">continuous</strong> variable, this would be a <strong class="source-inline">regression</strong> task:</p></li>
			</ul>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B15777_02_01.jpg" alt="Figure 2.1: A candlestick chart indicating the movement of a stock index over the span of a month&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1: A candlestick chart indicating the movement of a stock index over the span of a month</p>
			<ul>
				<li><strong class="bold">Do we have the appropriately labeled data to train a model?</strong> For a supervised learning task, we must have at least some labeled data in order to train a model. For example, if we want to build a model to classify images into dog images and cat images, we would need training data, the images themselves, and labels for the data indicating whether they are dog images or cat images. ANNs often need a lot of data. For image classification, this can be millions of images to develop accurate, robust models. This may be a determining factor when deciding which algorithm is appropriate for a given task.</li>
			</ul>
			<p>ANNs are a type of machine learning algorithm that can be used to solve a task. They excel in certain aspects and have drawbacks in others, and these pros and cons should be considered before choosing this type of algorithm. Deep learning networks are distinguished from single-layer ANNs by their depth—the total number of hidden layers within the network.</p>
			<p>So, deep learning is really just a specific subgroup of machine learning that relies on ANNs with multiple layers. We encounter the results of deep learning on a regular basis, whether it's in image classification models such as the friend recognition models that help tag friends in your Facebook photos, or the recommendation algorithms that help suggest your next favorite songs on Spotify. Deep learning models are becoming more prevalent over traditional machine learning models for a variety of reasons, including the growing sizes of unstructured data that deep learning models excel at and lower computational costs.</p>
			<p>Choosing whether to use ANNs or traditional machine learning algorithms such as linear regression and decision trees for a particular task is a matter of experience and an understanding of the inner workings of the algorithm itself. As such, the benefits of using traditional machine learning algorithms or ANNs will be mentioned in the next section.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor047"/>Advantages of ANNs over Traditional Machine Learning Algorithms</h2>
			<ul>
				<li><strong class="bold">The best performance</strong>: For any supervised learning task, the best models have been ANNs that are trained on a lot of data. For example, in classification tasks such as classifying images from the <strong class="source-inline">ImageNet challenge</strong> (a large-scale visual recognition challenge for classifying images into <strong class="source-inline">1000 classes</strong>), ANNs can attain greater accuracy than humans.</li>
				<li><strong class="bold">Scale effectively with data</strong>: Traditional machine learning algorithms, such as <strong class="source-inline">logistic regression</strong> and <strong class="source-inline">decision trees</strong>, plateau in performance, whereas the ANN architecture is able to learn higher-level features—nonlinear combinations of the input features that may be important for classification or regression tasks. This allows ANNs to perform better when provided with large amounts of data - especially those ANNs with a deep architecture. For example, ANNs that perform well in the ImageNet challenge are provided with <strong class="source-inline">14 million images</strong> for training. The following figure shows the performance scaling with the amount of data for both deep learning algorithms and traditional machine learning algorithms:</li>
			</ul>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B15777_02_02.jpg" alt="Figure 2.2: Performance scaling with the amount of data for both deep learning algorithms and traditional machine learning algorithms&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2: Performance scaling with the amount of data for both deep learning algorithms and traditional machine learning algorithms</p>
			<ul>
				<li><strong class="bold">No need for feature engineering</strong>: ANNs are able to identify which features are important in modeling so that they are able to model directly from raw data. For example, in the binary classification of dog and cat images into their respective classes, there is no need to define features such as the color size or weight of the animal. The images themselves are sufficient for the ANN to successfully determine classification. In traditional machine learning algorithms, these features must be engineered in an iterative process that is manual and can be time-consuming.</li>
				<li><strong class="bold">Adaptable and transferable</strong>: Weights and features that are learned from ANNs can be applied to similar tasks. In computer vision tasks, pre-trained classification models can be used as the starting points for building models for other classification tasks. For example, VGG-16 is a <strong class="source-inline">16-layer deep learning model</strong> that's used by <strong class="source-inline">ImageNet</strong> to classify <strong class="source-inline">1000 random objects</strong>. The weights that are learned in the model can be transferred to classify other objects in significantly less time.</li>
			</ul>
			<p>However, there are some advantages of using traditional machine learning algorithms over ANNs, as explained in the following section.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor048"/>Advantages of Traditional Machine Learning Algorithms over ANNs</h2>
			<ul>
				<li><strong class="bold">Relatively good performance when the available training data is small</strong>: In order to attain high performance, ANNs require a lot of data, and the deeper the network, the more data is required. With the increase in layers, the number of parameters that need to be learned also increases. This results in more time to train on the training data to reach the optimal parameter values. For example, <strong class="source-inline">VGG-16</strong> has over <strong class="source-inline">138 million parameters</strong> and required <strong class="source-inline">14 million hand-labeled images</strong> to train and learn all the parameters.</li>
				<li><strong class="bold">Cost-effective</strong>: Both financially and computationally, deep networks can take a lot of computing power and time to train. This demands a lot of resources that may not be available to all. Moreover, these models are time-consuming to tune effectively and require a domain expert who's familiar with the inner workings of the model to achieve optimal performance.</li>
				<li><strong class="bold">Easy to interpret</strong>: Many traditional machine learning models are easy to interpret. So, identifying which feature had the most predictive power in the model is straightforward. This can be incredibly useful when working with non-technical team members who wish to understand and interpret the results of the model. ANNs are considered more of a <strong class="source-inline">black box</strong>, in that while they are successful in classifying images and other tasks, the understanding behind how the predictions are made is unintuitive and buried in layers of computations. As such, interpreting the results requires more effort.</li>
			</ul>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor049"/>Hierarchical Data Representation</h2>
			<p>One reason that ANNs are able to perform so well is that a large number of layers allows the network to learn representations of the data at many different levels. This is illustrated in the following diagram, in which the representation of an ANN being used to identify faces is shown. At lower levels of the model, simple features are learned, such as edges and gradients, as can be seen by looking at the features that were learned in the initial layers. As the model progresses, combinations of lower-level features activate to form face parts, and at later layers of the model, generic faces are learned. This is known as feature hierarchy and illustrates the power that this layered representation has for model building and interpretation.</p>
			<p>Many examples of input for real-world applications of deep neural networks involve images, video, and natural language text. The feature hierarchy that is learned by deep neural networks allows them to discover latent structures within unlabeled, unstructured data, such as images, video, and natural language text, which makes them useful for processing real-world data—most often raw and unprocessed.</p>
			<p>The following diagram shows an example of the learned representation of a deep learning model—lower features such as the <strong class="source-inline">edges</strong> and <strong class="source-inline">gradients</strong> activate together to form generic face shapes, which can be seen in the deeper layers:</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B15777_02_03.jpg" alt="Figure 2.3: Learned representation at various parts of a deep learning model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3: Learned representation at various parts of a deep learning model</p>
			<p>Since deep neural networks have become more accessible, various companies have started exploiting their applications. The following are some examples of some companies that use ANNs:</p>
			<ul>
				<li><strong class="bold">Yelp</strong>: Yelp uses deep neural networks to process, classify, and label their images more efficiently. Since photos are one important aspect of Yelp reviews, the company has placed an emphasis on classifying and categorizing them. This is achieved more efficiently with deep neural networks.</li>
				<li><strong class="bold">Clarifai</strong>: This cloud-based company is able to classify images and videos using deep neural network-based models.</li>
				<li><strong class="bold">Enlitic</strong>: This company uses deep neural networks to analyze medical image data such as X-rays or MRIs. The use of such networks in this application increases diagnostic accuracy and decreases diagnostic time and cost.</li>
			</ul>
			<p>Now that we understand the potential applications of using ANNs, we can understand the mathematics behind how they work. While they may seem intimidating and complex, they can be broken down into a series of linear and nonlinear transformations, which themselves are simple to understand. An ANN is created by sequentially combining a series of linear and nonlinear transformations. The next section discusses the basic components and operations involved in linear transformations that comprise the mathematics of ANNs.</p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor050"/>Linear Transformations</h1>
			<p>In this section, we will introduce linear transformations. Linear transformations are the backbone of modeling with ANNs. In fact, all the processes of ANN modeling can be thought of as a series of linear transformations. The working components of linear transformations are scalars, vectors, matrices, and tensors. Operations such as <strong class="source-inline">addition</strong>, <strong class="source-inline">transposition</strong>, and <strong class="source-inline">multiplication</strong> are performed on these components.</p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor051"/>Scalars, Vectors, Matrices, and Tensors</h2>
			<p><strong class="source-inline">Scalars</strong>, <strong class="source-inline">vectors</strong>, <strong class="source-inline">matrices</strong>, and <strong class="source-inline">tensors</strong> are the actual components of any deep learning model. Having a fundamental understanding of how to utilize these components, as well as the operations that can be performed on them, is key to understanding how ANNs operate. <strong class="source-inline">Scalars</strong>, <strong class="source-inline">vectors</strong>, and <strong class="source-inline">matrices</strong> are examples of the general entity known as a <strong class="source-inline">tensor</strong>, so the term <strong class="source-inline">tensors</strong> may be used throughout this chapter but may refer to any component. <strong class="source-inline">Scalars</strong>, <strong class="source-inline">vectors</strong>, and <strong class="source-inline">matrices</strong> refer to <strong class="source-inline">tensors</strong> with a specific number of dimensions. </p>
			<p>The rank of a <strong class="source-inline">tensor</strong> is an attribute that determines the number of dimensions the <strong class="source-inline">tensor</strong> spans. The definitions of each are listed here:</p>
			<ul>
				<li><strong class="bold">Scalar</strong>: They are single numbers and are an example of 0-order <strong class="source-inline">tensors</strong>. For instance, the temperature at any given point is a <strong class="source-inline">scalar</strong> field.</li>
				<li><strong class="bold">Vector</strong>: Vectors are one-dimensional arrays of single numbers and are an example of first-order <strong class="source-inline">tensors</strong>. The <strong class="source-inline">velocity</strong> of a given object is an example of a <strong class="source-inline">vector</strong> field since it will have a speed in the <strong class="source-inline">two (x,y)</strong> or <strong class="source-inline">three (x,y,z)</strong> dimensions.</li>
				<li><strong class="bold">Matrix</strong>: <strong class="source-inline">Matrices</strong> are rectangular arrays that span over two dimensions that consist of single numbers. They are an example of second-order <strong class="source-inline">tensors</strong>. An example of where <strong class="source-inline">matrices</strong> might be used is to store the <strong class="source-inline">velocity</strong> of a given object over time. One dimension of the <strong class="source-inline">matrix</strong> comprises the speed in the given directions, while the other <strong class="source-inline">matrix</strong> dimension is comprised of each given time point.</li>
				<li><strong class="bold">Tensor</strong>: <strong class="source-inline">Tensors</strong> are the general entities that encapsulate <strong class="source-inline">scalars</strong>, <strong class="source-inline">vectors</strong>, and <strong class="source-inline">matrices</strong>. In general, the name is reserved for <strong class="source-inline">tensors</strong> of order <strong class="source-inline">3</strong> or more. An example of where <strong class="source-inline">tensors</strong> might be used is to store the <strong class="source-inline">velocity</strong> of many objects over time. One dimension of the <strong class="source-inline">matrix</strong> comprises the speed in the given directions, another <strong class="source-inline">matrix</strong> dimension is given for each given time point, and a third dimension describes the various objects.</li>
			</ul>
			<p>The following diagram shows some examples of a <strong class="source-inline">scalar</strong>, a <strong class="source-inline">vector</strong>, a <strong class="source-inline">matrix</strong>, and a <strong class="source-inline">three-dimensional tensor</strong>:</p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B15777_02_04.jpg" alt="Figure 2.4: A visual representation of scalars, vectors, matrices, and tensors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4: A visual representation of scalars, vectors, matrices, and tensors</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor052"/>Tensor Addition</h2>
			<p><strong class="source-inline">Tensors</strong> can be added together to create new <strong class="source-inline">tensors</strong>. We will use the example of matrices in this chapter, but this concept can be extended to <strong class="source-inline">tensors</strong> with any rank. <strong class="source-inline">Matrices</strong> may be added to <strong class="source-inline">scalars</strong>, <strong class="source-inline">vectors</strong>, and other <strong class="source-inline">matrices</strong> under certain conditions.</p>
			<p>Two matrices may be added (or subtracted) together if they have the same shape. For such matrix-matrix addition, the resultant matrix is determined by the element-wise addition of the input matrices. The resultant matrix will, therefore, have the same shape as the two input matrices. We can define the matrix C = [c<span class="subscript">ij</span>] as the matrix sum <strong class="bold">C = A + B</strong>, where c<span class="subscript">ij</span> = a<span class="subscript">ij</span> + b<span class="subscript">ij</span> and each element in <strong class="bold">C</strong> is the sum of the same element in <strong class="bold">A</strong> and <strong class="bold">B</strong>. Matrix addition is commutative, which means that the order of <strong class="bold">A</strong> and <strong class="bold">B</strong> does not matter – <strong class="bold">A + B = B + A</strong>. Matrix addition is also associative, which means that the same result is achieved, even when the order of additions is different or even if the operation is applied more than once: <strong class="bold">A + (B + C) = (A + B) + C</strong>.</p>
			<p>The same matrix addition principles apply for <strong class="source-inline">scalars</strong>, <strong class="source-inline">vectors</strong>, and <strong class="source-inline">tensors</strong>. An example of this is as follows:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B15777_02_05.jpg" alt="Figure 2.5: An example of matrix-matrix addition&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5: An example of matrix-matrix addition</p>
			<p><strong class="source-inline">Scalars</strong> can also be added to <strong class="source-inline">matrices</strong>. Here, each element of the <strong class="source-inline">matrix</strong> is added to the <strong class="source-inline">scalar</strong> individually, as is shown in the below figure:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B15777_02_06.jpg" alt="Figure 2.6: An example of matrix-scalar addition&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6: An example of matrix-scalar addition</p>
			<p>It is possible to add vectors to matrices if the number of columns between the two matches each other. This is known as broadcasting.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor053"/>Exercise 2.01: Performing Various Operations with Vectors, Matrices, and Tensors</h2>
			<p class="callout-heading">Note</p>
			<p class="callout">For the exercises and activities within this chapter, you will need to have Python 3.7, Jupyter, and NumPy installed on your system. All the exercises and activities will be primarily developed in Jupyter notebooks. It is recommended to keep a separate notebook for different assignments unless advised not to. Use the following link to download them from this book's GitHub repository: <a href="https://packt.live/2vpc9rO">https://packt.live/2vpc9rO</a>.</p>
			<p>In this exercise, we are going to demonstrate how to create and work with <strong class="source-inline">vectors</strong>, <strong class="source-inline">matrices</strong>, and <strong class="source-inline">tensors</strong> within Python. We will assume that you have some familiarity with scalars. This can all be achieved with the NumPy library using the <strong class="source-inline">array</strong> and <strong class="source-inline">matrix</strong> functions. Tensors of any rank can be created with the NumPy <strong class="source-inline">array</strong> function. </p>
			<p>Before you begin, you should set up the files and folders for this chapter in your working directory using a similar structure and naming convention as you did in the previous chapter. You can verify your folder structure by comparing it to the GitHub repository, linked above.</p>
			<p>Follow these steps to perform this exercise:</p>
			<ol>
				<li>Open Jupyter Notebook to implement this exercise. Import the necessary dependency. Create a <strong class="source-inline">one-dimensional array</strong>, or a <strong class="source-inline">vector</strong>, as follows:<p class="source-code">import numpy as np</p><p class="source-code">vec1 = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10])</p><p class="source-code">vec1</p><p>The preceding code produces the following output:</p><p class="source-code">array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])</p></li>
				<li>Create a <strong class="source-inline">two-dimensional array</strong>, or <strong class="source-inline">matrix</strong>, with the <strong class="source-inline">array</strong> function:<p class="source-code">mat1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])</p><p class="source-code">mat1</p><p>The preceding code produces the following output:</p><p class="source-code">array([[ 1, 2, 3],</p><p class="source-code">       [ 4, 5, 6],</p><p class="source-code">       [ 7, 8, 9],</p><p class="source-code">       [10, 11, 12]])</p></li>
				<li>Use the <strong class="source-inline">matrix</strong> function to create matrices, which will show a similar output:<p class="source-code">mat2 = np.matrix([[1, 2, 3], [4, 5, 6], \</p><p class="source-code">                  [7, 8, 9], [10, 11, 12]])</p></li>
				<li>Create a <strong class="source-inline">three-dimensional array</strong>, or <strong class="source-inline">tensor</strong>, using the <strong class="source-inline">array</strong> function:<p class="source-code">ten1 = np.array([[[1, 2, 3], [4, 5, 6]], \</p><p class="source-code">                 [[7, 8, 9], [10, 11, 12]]])</p><p class="source-code">ten1</p><p>The preceding code produces the following output:</p><p class="source-code">array([[[ 1, 2, 3],</p><p class="source-code">        [ 4, 5, 6],</p><p class="source-code">        [[ 7, 8, 9],</p><p class="source-code">        [10, 11, 12]]])</p></li>
				<li>Determining the <strong class="source-inline">shape</strong> of a given <strong class="source-inline">vector</strong>, <strong class="source-inline">matrix</strong>, or <strong class="source-inline">tensor</strong> is important since certain operations, such as <strong class="source-inline">addition</strong> and <strong class="source-inline">multiplication</strong>, can only be applied to components of certain shapes. The shape of an n-dimensional array can be determined using the <strong class="source-inline">shape</strong> method. Write the following code to determine the <strong class="source-inline">shape</strong> of <strong class="source-inline">vec1</strong>:<p class="source-code">vec1.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(10, )</p></li>
				<li>Write the following code to determine the <strong class="source-inline">shape</strong> of <strong class="source-inline">mat1</strong>:<p class="source-code">mat1.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(4, 3)</p></li>
				<li>Write the following code to determine the <strong class="source-inline">shape</strong> of <strong class="source-inline">ten1</strong>:<p class="source-code">ten1.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(2, 2, 3)</p></li>
				<li>Create a <strong class="source-inline">matrix</strong> with <strong class="source-inline">four rows</strong> and <strong class="source-inline">three columns</strong> with whichever numbers you like. Print the resulting matrix to verify its <strong class="source-inline">shape</strong>:<p class="source-code">mat1 = np.matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])</p><p class="source-code">mat1</p><p>The preceding code produces the following output:</p><p class="source-code">matrix([[ 1, 2, 3],</p><p class="source-code">        [ 4, 5, 6],</p><p class="source-code">        [ 7, 8, 9],</p><p class="source-code">        [10, 11, 12]])</p></li>
				<li>Create another matrix with <strong class="source-inline">four rows</strong> and <strong class="source-inline">three columns</strong> with whichever numbers you like. Print the resulting matrix to verify its <strong class="source-inline">shape</strong>:<p class="source-code">mat2 = np.matrix([[2, 1, 4], [4, 1, 7], [4, 2, 9], [5, 21, 1]])</p><p class="source-code">mat2</p><p>The preceding code produces the following output:</p><p class="source-code">matrix([[ 2, 1, 4],</p><p class="source-code">        [ 4, 1, 7],</p><p class="source-code">        [ 4, 2, 9],</p><p class="source-code">        [ 5, 21, 1]])</p></li>
				<li>Add <strong class="source-inline">matrix 1</strong> and <strong class="source-inline">matrix 2</strong>:<p class="source-code">mat3 = mat1 + mat2</p><p class="source-code">mat3</p><p>The preceding code produces the following output:</p><p class="source-code">matrix([[ 3, 3, 7],</p><p class="source-code">        [ 8, 6, 13],</p><p class="source-code">        [ 11, 10, 18],</p><p class="source-code">        [ 15, 32, 13]])</p></li>
				<li>Add <strong class="source-inline">scalars</strong> to the <strong class="source-inline">arrays</strong> with the following code:<p class="source-code">mat1 + 4</p><p>The preceding code produces the following output:</p><p class="source-code">matrix([[ 5, 6, 7],</p><p class="source-code">        [ 8, 9, 10],</p><p class="source-code">        [ 11, 12, 13],</p><p class="source-code">        [ 14, 15, 16]])</p></li>
			</ol>
			<p>In this exercise, we learned how to perform various operations with <strong class="source-inline">vectors</strong>, <strong class="source-inline">matrices</strong>, and <strong class="source-inline">tensors</strong>. We also learned how to determine the <strong class="source-inline">shape</strong> of the <strong class="source-inline">matrix</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2NNQ7VA">https://packt.live/2NNQ7VA</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3eUDtQA">https://packt.live/3eUDtQA</a>.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor054"/>Reshaping</h2>
			<p>A <strong class="source-inline">tensor</strong> of any size can be reshaped as long as the number of total elements remains the same. For example, a <strong class="source-inline">(4x3) matrix</strong> can be reshaped into a <strong class="source-inline">(6x2) matrix</strong> since they both have a total of <strong class="source-inline">12</strong> elements. The <strong class="source-inline">rank</strong>, or <strong class="source-inline">number of dimensions</strong>, can also be changed in the <strong class="source-inline">reshaping</strong> process. For example, a <strong class="source-inline">(4x3) matrix</strong> can be reshaped into a <strong class="source-inline">(3x2x2) tensor</strong>. Here, the <strong class="source-inline">rank</strong> has changed from <strong class="source-inline">2</strong> to <strong class="source-inline">3</strong>. The <strong class="source-inline">(4x3) matrix</strong> can also be reshaped into a <strong class="source-inline">(12x1) vector</strong>, in which the <strong class="source-inline">rank</strong> has changed from <strong class="source-inline">2</strong> to <strong class="source-inline">1</strong>. </p>
			<p>The following diagram illustrates tensor reshaping—on the left is a tensor with <strong class="source-inline">shape (4x1x3)</strong>, which can be reshaped to a tensor of <strong class="source-inline">shape (4x3)</strong>. Here, the number of elements (<strong class="source-inline">12</strong>) has remained constant, though the <strong class="source-inline">shape</strong> and <strong class="source-inline">rank</strong> of the tensor have changed:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B15777_02_07.jpg" alt="Figure 2.7: Visual representation of reshaping a (4x1x3) tensor into a (4x3) tensor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7: Visual representation of reshaping a (4x1x3) tensor into a (4x3) tensor</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor055"/>Matrix Transposition</h2>
			<p>The <strong class="source-inline">transpose</strong> of a matrix is an operator that flips the matrix over its diagonal. When this occurs, the rows become the columns and vice versa. The transpose operation is usually denoted as a <strong class="source-inline">T</strong> superscript upon the matrix. Tensors of any rank can also be transposed:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B15777_02_08.jpg" alt="Figure 2.8: A visual representation of matrix transposition&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8: A visual representation of matrix transposition</p>
			<p>The following figure shows the matrix transposition properties of matrices <strong class="source-inline">A</strong> and <strong class="source-inline">B</strong>:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/B15777_02_09.jpg" alt="Figure 2.9: Matrix transposition properties where A and B are matrices&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9: Matrix transposition properties where A and B are matrices</p>
			<p>A square matrix (that is, a matrix with an equivalent number of rows and columns) is said to be symmetrical if the transpose of a matrix is equivalent to the original matrix.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor056"/>Exercise 2.02: Matrix Reshaping and Transposition</h2>
			<p>In this exercise, we are going to demonstrate how to reshape and transpose matrices. This will become important since some operations can only be applied to components if certain tensor dimensions match. For example, tensor multiplication can only be applied if the inner dimensions of the two tensors match. Reshaping or transposing tensors is one way to modify the dimensions of the tensor to ensure that certain operations can be applied. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a Jupyter notebook from the start menu to implement this exercise. Create a <strong class="source-inline">two-dimensional array</strong> with <strong class="source-inline">four rows</strong> and <strong class="source-inline">three columns</strong>, as follows:<p class="source-code">import numpy as np</p><p class="source-code">mat1 = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])</p><p class="source-code">mat1</p><p>This gives the following output:</p><p class="source-code">array([[ 1,  2,  3],</p><p class="source-code">       [ 4,  5,  6],</p><p class="source-code">       [ 7,  8,  9],</p><p class="source-code">       [10, 11, 12]])</p><p>We can confirm its shape by looking at the shape of the matrix:</p><p class="source-code">mat1.shape</p><p>The output is as follows:</p><p class="source-code">(4, 3)</p></li>
				<li>Reshape the array so that it has <strong class="source-inline">three rows</strong> and <strong class="source-inline">four columns</strong> instead, as follows:<p class="source-code">mat2 = np.reshape(mat1, [3,4])</p><p class="source-code">mat2</p><p>The preceding code produces the following output:</p><p class="source-code">array([[ 1, 2, 3, 4],</p><p class="source-code">       [ 5, 6, 7, 8],</p><p class="source-code">       [ 9, 10, 11, 12]])</p></li>
				<li>Confirm this by printing the <strong class="source-inline">shape</strong> of the array:<p class="source-code">mat2.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(3, 4)</p></li>
				<li>Reshape the matrix into a <strong class="source-inline">three-dimensional array</strong>, as follows:<p class="source-code">mat3 = np.reshape(mat1, [3,2,2])</p><p class="source-code">mat3</p><p>The preceding code produces the following output:</p><p class="source-code">array([[[ 1, 2],</p><p class="source-code">        [ 3, 4]],</p><p class="source-code">       [[ 5, 6],</p><p class="source-code">        [ 7, 8]],</p><p class="source-code">       [[ 9, 10],</p><p class="source-code">        [ 11, 12]]]) </p></li>
				<li>Print the <strong class="source-inline">shape</strong> of the array to confirm its dimensions:<p class="source-code">mat3.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(3, 2, 2)</p></li>
				<li>Reshape the matrix into a <strong class="source-inline">one-dimensional array</strong>, as follows:<p class="source-code">mat4 = np.reshape(mat1, [12])</p><p class="source-code">mat4</p><p>The preceding code produces the following output:</p><p class="source-code">array([ 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12])</p></li>
				<li>Confirm this by printing the <strong class="source-inline">shape</strong> of the array:<p class="source-code">mat4.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(12, )</p></li>
				<li>Taking the transpose of an array will flip it across its diagonal. For a one-dimensional array, a row-vector will be converted into a column vector and vice versa. For a two-dimensional array or matrix, each row becomes a column and vice versa. Call the transpose of an array using the <strong class="source-inline">T</strong> method:<p class="source-code">mat = np.matrix([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])</p><p class="source-code">mat.T</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer058" class="IMG---Figure"><img src="image/B15777_02_10.jpg" alt="Figure 2.10: Visual demonstration of the transpose function&#13;&#10;"/></div><p class="figure-caption">Figure 2.10: Visual demonstration of the transpose function</p></li>
				<li>Check the <strong class="source-inline">shape</strong> of the matrix and its transpose to verify that the dimensions have changed:<p class="source-code">mat.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(4, 3)</p></li>
				<li>Check the <strong class="source-inline">shape</strong> of the transposed matrix:<p class="source-code">mat.T.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(3, 4)</p></li>
				<li>Verify the matrix elements do not match when a matrix is reshaped, and a matrix is transposed:<p class="source-code">np.reshape(mat1, [3,4]) == mat1.T</p><p>The preceding code produces the following output:</p><p class="source-code">array([[ True, False, False, False],</p><p class="source-code">       [False, False, False, False],</p><p class="source-code">       [False, False, False, True]], dtype = bool)</p><p>Here, we can see that only the first and last elements match.</p></li>
			</ol>
			<p>In this section, we introduced some of the basic components of linear algebra, including scalars, vectors, matrices, and tensors. We also covered some basic manipulation of linear algebra components, such as addition, transposition, and reshaping. By doing so, we learned how to put these concepts into action by using functions in the <strong class="source-inline">NumPy</strong> library to perform these operations.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3gqBlR0">https://packt.live/3gqBlR0</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3eYCChD">https://packt.live/3eYCChD</a>.</p>
			<p>In the next section, we will extend our understanding of linear transformations by covering one of the most important transformations related to ANNs—matrix multiplication.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor057"/>Matrix Multiplication</h2>
			<p>Matrix multiplication is fundamental to neural network operations. While the rules for addition are simple and intuitive, the rules for multiplication for matrices and tensors are more complex. Matrix multiplication involves more than simple element-wise multiplication of the elements. Instead, a more complicated procedure is implemented that involves the entire row of one matrix and an entire column of the other. In this section, we will explain how multiplication works for two-dimensional tensors or matrices; however, tensors of higher orders can also be multiplied.</p>
			<p>Given a matrix, A = [a<span class="subscript">ij</span>]<span class="subscript">m x n</span>, and another matrix, B = [b<span class="subscript">ij</span>]<span class="subscript">n x p</span> , the product of the two matrices is C = AB = [C<span class="subscript">ij</span>]<span class="subscript">m x p</span>, and each element, c<span class="subscript">ij</span>, is defined element-wise as <img src="image/B15777_02_10a.png" alt="formula"/>. Note that the shape of the resultant matrix is the same as the outer dimensions of the matrix product or the number of rows of the first matrix and the number of columns of the second matrix. For the multiplication to work, the inner dimensions of the matrix product must match, or the number of columns of the first matrix and the number of columns of the second matrix. </p>
			<p>The concept of inner and outer dimensions of matrix multiplication can be seen in the following figure:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B15777_02_11.jpg" alt="Figure 2.11: A visual representation of the inner and outer dimensions in matrix multiplication&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.11: A visual representation of the inner and outer dimensions in matrix multiplication</p>
			<p>Unlike matrix addition, matrix multiplication is not commutative, which means that the order of the matrices in the product matters:</p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B15777_02_12.jpg" alt="Figure 2.12: Matrix multiplication is non-commutative&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.12: Matrix multiplication is non-commutative</p>
			<p>For example, let's say we have the following two matrices:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B15777_02_13.jpg" alt="Figure 2.13: Two matrices, A and B&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.13: Two matrices, A and B</p>
			<p>One way to construct the product is to have matrix <strong class="bold">A</strong> first, multiplied by <strong class="bold">B</strong>:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B15777_02_14.jpg" alt="Figure 2.14: Visual representation of matrix A multiplied by B&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.14: Visual representation of matrix A multiplied by B</p>
			<p>This results in a <strong class="source-inline">2x2</strong> matrix. Another way to construct the product is to have <strong class="bold">B</strong> first, multiplied by <strong class="bold">A</strong>:</p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B15777_02_15.jpg" alt="Figure 2.15: Visual representation of matrix B multiplied by A&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.15: Visual representation of matrix B multiplied by A</p>
			<p>Here, we can see that the matrix that was formed from the product <strong class="bold">BA</strong> is a <strong class="source-inline">3x3</strong> matrix and is very different from the matrix that was formed from the product <strong class="bold">AB</strong>.</p>
			<p>Scalar-matrix multiplication is much more straightforward and is simply the product of every element in the matrix multiplied by the scalar so that λA = [λa<span class="subscript">ij</span>]<span class="subscript">m x n</span>, where λ is a scalar and <strong class="bold">A</strong> is a matrix.</p>
			<p>In the following exercise, we will put our understanding into practice by performing matrix multiplication in Python utilizing the <strong class="source-inline">NumPy</strong> library.</p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor058"/>Exercise 2.03: Matrix Multiplication</h2>
			<p>In this exercise, we are going to demonstrate how to multiply matrices together. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a Jupyter notebook from the start menu to implement this exercise.<p>To demonstrate the fundamentals of matrix multiplication, begin with two matrices of the same shape:</p><p class="source-code">import numpy as np</p><p class="source-code">mat1 = np.array([[1, 2, 3], [4, 5, 6], \</p><p class="source-code">                 [7, 8, 9], [10, 11, 12]])</p><p class="source-code">mat2 = np.array([[2, 1, 4], [4, 1, 7], \</p><p class="source-code">                 [4, 2, 9], [5, 21, 1]])</p></li>
				<li>Since both matrices have the same shape and they are not square, they cannot be multiplied as is, otherwise, the inner dimensions of the product won't match. One way we could resolve this is to take the transpose of one of the matrices; then, we would be able to perform the multiplication. Take the transpose of the second matrix, which would mean that a <strong class="source-inline">(4x3) matrix</strong> is multiplied by a <strong class="source-inline">(3x4) matrix</strong>. The result would be a <strong class="source-inline">(4x4) matrix</strong>. Perform the multiplication using the <strong class="source-inline">dot</strong> method:<p class="source-code">mat1.dot(mat2.T)</p><p>The preceding code produces the following output:</p><p class="source-code">array([[ 16, 27, 35, 50],</p><p class="source-code">       [ 37, 63, 80, 131],</p><p class="source-code">       [ 58, 99, 125, 212],</p><p class="source-code">       [ 79, 135, 170, 293]])</p></li>
				<li>Take the transpose of the first matrix, which would mean that a <strong class="source-inline">(3x4) matrix</strong> is multiplied by a <strong class="source-inline">(4x3) matrix</strong>. The result would be a <strong class="source-inline">(3x3) matrix</strong>:<p class="source-code">mat1.T.dot(mat2)</p><p>The preceding code produces the following output:</p><p class="source-code">array([[ 96, 229, 105],</p><p class="source-code">       [ 111, 254, 126],</p><p class="source-code">       [ 126, 279, 147]])</p></li>
				<li>Reshape one of the arrays to make sure the inner dimension of the matrix multiplication matches. For example, we can reshape the first array to make it a <strong class="source-inline">(3x4) matrix</strong> instead of transposing. Note that the result is not the same as it is when transposing:<p class="source-code">np.reshape(mat1, [3,4]).dot(mat2)</p><p>The preceding code produces the following output:</p><p class="source-code">array([[ 42, 93, 49],</p><p class="source-code">       [ 102, 193, 133],</p><p class="source-code">       [ 162, 293, 217]])</p></li>
			</ol>
			<p>In this exercise, we learned how to multiply two matrices together. The same concept can be applied to tensors of all ranks, not just second-order tensors. Tensors of different ranks can even be multiplied together if their inner dimensions match. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/38p0RD7">https://packt.live/38p0RD7</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2VYI1xZ">https://packt.live/2VYI1xZ</a>.</p>
			<p>The next exercise demonstrates how to multiply three-dimensional tensors together.</p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor059"/>Exercise 2.04: Tensor Multiplication</h2>
			<p>In this exercise, we are going to apply our knowledge of matrix multiplication to higher-order tensors. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Open a Jupyter notebook from the start menu to implement this exercise. Begin by creating a three-dimensional tensor using the NumPy library and the <strong class="source-inline">array</strong> function. Import all the necessary dependencies:<p class="source-code">import numpy as np</p><p class="source-code">mat1 = np.array([[[1, 2, 3], [4, 5, 6]], [[1, 2, 3], [4, 5, 6]]])</p><p class="source-code">mat1</p><p>The preceding code produces the following output:</p><p class="source-code">array([[[ 1, 2, 3],</p><p class="source-code">        [ 4, 5, 6],</p><p class="source-code">        [[ 1, 2, 3],</p><p class="source-code">        [ 4, 5, 6]]])</p></li>
				<li>Confirm the shape using the <strong class="source-inline">shape</strong> method:<p class="source-code">mat1.shape</p><p>This tensor has the shape (2x2x3).</p></li>
				<li>Create a new <strong class="source-inline">three-dimensional tensor</strong> that we will be able to multiply the tensor by. Take the transpose of the original matrix:<p class="source-code">mat2 = mat1.T</p><p class="source-code">mat2</p><p>The preceding code produces the following output:</p><p class="source-code">array([[[ 1, 1],</p><p class="source-code">        [ 4, 4]],</p><p class="source-code">       [[ 2, 2],</p><p class="source-code">        [ 5, 5]],</p><p class="source-code">       [[ 3, 3],</p><p class="source-code">        [ 6, 6]]])</p></li>
				<li>Confirm the shape using the <strong class="source-inline">shape</strong> method:<p class="source-code">mat2.shape</p><p>This tensor has the shape (3x2x2).</p></li>
				<li>Take the <strong class="source-inline">dot</strong> product of the <strong class="source-inline">two matrices</strong>, as follows:<p class="source-code">mat3 = mat2.dot(mat1)</p><p class="source-code">mat3</p><p>The preceding code produces the following output:</p><p class="source-code">array([[[[ 5, 7, 9],</p><p class="source-code">         [ 5, 7, 9]],</p><p class="source-code">        [[ 20, 28, 36],</p><p class="source-code">         [ 20, 28, 36]]],</p><p class="source-code">       [[[ 10, 14, 18],</p><p class="source-code">         [ 10, 14, 18]],</p><p class="source-code">        [[ 25, 35, 45],</p><p class="source-code">         [ 25, 35, 45]]],</p><p class="source-code">       [[[ 15, 21, 27],</p><p class="source-code">         [ 15, 21, 27]],</p><p class="source-code">        [[ 30, 42, 54],</p><p class="source-code">         [ 30, 42, 54]]]])</p></li>
				<li>Look at the <strong class="source-inline">shape</strong> of this resultant tensor:<p class="source-code">mat3.shape</p><p>The preceding code produces the following output:</p><p class="source-code">(3, 2, 2, 3)</p><p>Now, we have a four-dimensional tensor.</p></li>
			</ol>
			<p>In this exercise, we learned how to perform matrix multiplication using the NumPy library in Python. While we do not have to perform matrix multiplication directly when we create ANNs with Keras, it is still useful to understand the underlying mathematics.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31G1rLn">https://packt.live/31G1rLn</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2AriZjn">https://packt.live/2AriZjn</a>.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor060"/>Introduction to Keras</h1>
			<p>Building ANNs involves creating layers of nodes. Each node can be thought of as a tensor of weights that are learned in the training process. Once the ANN has been fitted to the data, a prediction is made by multiplying the input data by the weight matrices layer by layer, applying any other linear transformation when needed, such as activation functions, until the final output layer is reached. The size of each weight tensor is determined by the size of the shape of the input nodes and the shape of the output nodes. For example, in a single-layer ANN, the size of our single hidden layer can be thought of as follows:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B15777_02_16.jpg" alt="Figure 2.16: Solving the dimensions of the hidden layer of a single-layer ANN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.16: Solving the dimensions of the hidden layer of a single-layer ANN</p>
			<p>If the input matrix of features has <strong class="source-inline">n</strong> rows, or observations, and <strong class="source-inline">m</strong> columns, or features, and we want our predicted target to have <strong class="source-inline">n</strong> rows (one for each observation) and one column (the predicted value), we can determine the size of our hidden layer by what is needed to make the matrix multiplication valid. Here is the representation of a single-layer ANN:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B15777_02_17.jpg" alt="Figure 2.17: Representation of a single-layer ANN&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.17: Representation of a single-layer ANN</p>
			<p>Here, we can determine that the weight matrix will be of size (<strong class="source-inline">mx1</strong>) to ensure the matrix multiplication is valid.</p>
			<p>If we have more than one hidden layer in an ANN, then we have much more freedom with the size of these weight matrices. In fact, the possibilities are endless, depending on how many layers there are and how many nodes we want in each layer. In practice, however, certain architecture designs work better than others, as we will be learning throughout this book.</p>
			<p>In general, Keras abstracts much of the linear algebra out of building neural networks so that users can focus on designing the architecture. For most networks, only the input size, output size, and the number of nodes in each hidden layer are needed to create networks in Keras.</p>
			<p>The simplest model structure in Keras is the <strong class="source-inline">Sequential</strong> model, which can be imported from <strong class="source-inline">keras.models</strong>. The model of the <strong class="source-inline">Sequential</strong> class describes an ANN that consists of a linear stack of layers. A <strong class="source-inline">Sequential</strong> model can be instantiated as follows:</p>
			<p class="source-code">from keras.models import Sequential</p>
			<p class="source-code">model = Sequential()</p>
			<p>Layers can be added to this model instance to create the structure of the model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before initializing your model, it is helpful to set a seed using the <strong class="source-inline">seed</strong> function in NumPy's random library and the <strong class="source-inline">set_seed</strong> function from TensorFlow's random library.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor061"/>Layer Types</h2>
			<p>The notion of layers is part of the Keras core API. A layer can be thought of as a composition of nodes, and at each node, a set of computations happen. In Keras, all the nodes of a layer can be initialized by simply initializing the layer itself. The individual operation of a generalized layer node can be seen in the following diagram. At each node, the input data is multiplied by a set of weights using matrix multiplication, as we learned earlier in this chapter. The sum of the product between the weights and the input is applied, which may or may not include a bias, as shown by the input node equal to <strong class="source-inline">1</strong> in the following diagram. Further functions may be applied to the output of this matrix multiplication, such as activation functions:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B15777_02_18.jpg" alt="Figure 2.18: A depiction of a layer node&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.18: A depiction of a layer node</p>
			<p>Some common layer types in Keras are as follows:</p>
			<ul>
				<li><strong class="bold">Dense</strong>: This is a fully connected layer in which all the nodes of the layer are directly connected to all the inputs and all the outputs. ANNs for classification or regression tasks on tabular data usually have a large percentage of their layers with this type in the architecture.</li>
				<li><strong class="bold">Convolutional</strong>: This layer type creates a convolutional kernel that is convolved with the input layer to produce a tensor of outputs. This convolution can occur in one or multiple dimensions. ANNs for the classification of images usually feature one or more convolutional layers in their architecture.</li>
				<li><strong class="bold">Pooling</strong>: This type of layer is used to reduce the dimensionality of an input layer. Common types of pooling include max pooling, in which the maximum value of a given window is passed through to the output, or average pooling, in which the average value of a window is passed through. These layers are often used in conjunction with a convolutional layer, and their purpose is to reduce the dimensions of the subsequent layers, allowing for fewer training parameters to be learned with little information loss.</li>
				<li><strong class="bold">Recurrent</strong>: Recurrent layers learn patterns from sequences, so each output is dependent on the results from the previous step. ANNs that model sequential data such as natural language or time-series data often feature one or more recurrent layer types.</li>
			</ul>
			<p>There are other layer types in Keras; however, these are the most common types when it comes to building models using Keras.</p>
			<p>Let's demonstrate how to add layers to a model by instantiating a model of the <strong class="source-inline">Sequential</strong> class and adding a <strong class="source-inline">Dense</strong> layer to the model. Successive layers can be added to the model in the order in which we wish the computation to be performed and can be imported from <strong class="source-inline">keras.layers</strong>. The number of units, or nodes, needs to be specified. This value will also determine the shape of the result from the layer. A <strong class="source-inline">Dense</strong> layer can be added to a <strong class="source-inline">Sequential</strong> model in the following way:</p>
			<p class="source-code">from keras.layers import Dense</p>
			<p class="source-code">from keras.models import Sequential</p>
			<p class="source-code">input_shape = 20</p>
			<p class="source-code">units = 1</p>
			<p class="source-code">model.add(Dense(units, input_dim=input_shape))</p>
			<p class="callout-heading">Note</p>
			<p class="callout">After the first layer, the input dimension does not need to be specified since it is determined from the previous layer.</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor062"/>Activation Functions</h2>
			<p>An activation function is generally applied to the output of a node to limit or bound its value. The value from each node is unbounded and may have any value, from negative to positive infinity. These can be troublesome within neural networks where the values of the weights and losses have been calculated and can head toward infinity and produce unusable results. Activation functions can help in this regard by bounding the value. Often, these activation functions push the value to two limits. Activation functions are also useful for deciding whether the node should be "fired" or not. Common activation functions are as follows:</p>
			<ul>
				<li>The <strong class="bold">Step</strong> function: The value is nonzero if it is above a certain threshold; otherwise, it is zero.</li>
				<li>The <strong class="bold">Linear</strong> function: <img src="image/B15777_02_18a.png" alt="formula"/>, which is a scalar multiplication of the input value.</li>
				<li>The <strong class="bold">Sigmoid</strong> function: <img src="image/B15777_02_18b.png" alt="formula"/>, such as a smoothed-out step function with smooth gradients. This activation function is useful for classification since the values are bound from zero to one.</li>
				<li>The <strong class="bold">Tanh</strong> function: <img src="image/B15777_02_18c.png" alt="formula"/>, which is a scaled version of the sigmoid with steeper gradients around <strong class="source-inline">x=0</strong>.</li>
				<li>The <strong class="bold">ReLU</strong> function: <img src="image/B15777_02_18d.png" alt="formula"/>, otherwise 0.</li>
			</ul>
			<p>Now that we have looked at some of the main components, we can begin to see how we might create useful neural networks out of these components. In fact, we can create a logistic regression model with all the concepts we have learned about in this chapter. A logistic regression model operates by taking the sum of the product of an input and a set of learned weights, followed by the output being passed through a logistic function. This can be achieved with a single-layer neural network with a sigmoid activation function.</p>
			<p>Activation functions can be added to models in the same manner that layers are added to models. The activation function will be applied to the output of the previous step in the model. A <strong class="source-inline">tanh</strong> activation function can be added to a <strong class="source-inline">Sequential</strong> model as follows:</p>
			<p class="source-code">from keras.layers import Dense, Activation</p>
			<p class="source-code">from keras.models import Sequential</p>
			<p class="source-code">input_shape = 20</p>
			<p class="source-code">units = 1</p>
			<p class="source-code">model.add(Dense(units, input_dim=input_shape))</p>
			<p class="source-code">model.add(Activation('tanh'))</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Activation functions can also be added to a model by including them as an argument when defining the layers.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor063"/>Model Fitting</h2>
			<p>Once a model's architecture has been created, the model must be compiled. The compilation process configures all the learning parameters, including which optimizer to use, the loss function to minimize, as well as optional metrics, such as accuracy, to calculate at various stages of the model training. Models are compiled using the <strong class="source-inline">compile</strong> method, as follows:</p>
			<p class="source-code">model.compile(optimizer='adam', loss='binary_crossentropy', \</p>
			<p class="source-code">              metrics=['accuracy'])</p>
			<p>After the model has been compiled, it is ready to be fit to the training data. This is achieved with an instantiated model using the <strong class="source-inline">fit</strong> method. Useful arguments when using the <strong class="source-inline">fit</strong> method are as follows:</p>
			<ul>
				<li><strong class="bold">X</strong>: The array of the training feature data to fit the data to.</li>
				<li><strong class="bold">y</strong>: The array of the training target data.</li>
				<li><strong class="bold">epochs</strong>: The number of epochs to run the model for. An epoch is an iteration over the entire training dataset.</li>
				<li><strong class="bold">batch_size</strong>: The number of training data samples to use per gradient update.</li>
				<li><strong class="bold">validation_split</strong>: The proportion of the training data to be used for validation that is evaluated after each epoch.</li>
				<li><strong class="bold">shuffle</strong>: Indicates whether to shuffle the training data before each epoch.</li>
			</ul>
			<p>The <strong class="source-inline">fit</strong> method can be used on a model in the following way:</p>
			<p class="source-code">history = model.fit(x=X_train, y=y_train['y'], \</p>
			<p class="source-code">                    epochs=10, batch_size=32, \</p>
			<p class="source-code">                    validation_split=0.2, shuffle=False)</p>
			<p>It is beneficial to save the output of calling the <strong class="source-inline">fit</strong> method of the model since it contains information on the model's performance throughout training, including the loss, which is evaluated after each epoch. If a validation split is defined, the loss is evaluated after each epoch on the validation split. Likewise, if any metrics are defined in training, they are also calculated after each epoch. It is useful to plot such loss and evaluation metrics to determine model performance as a function of the epoch. The model's loss as a function of the epoch can be visualized as follows:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">%matplotlib inline</p>
			<p class="source-code">plt.plot(history.history['loss'])</p>
			<p class="source-code">plt.show()</p>
			<p>Keras models can be evaluated by utilizing the <strong class="source-inline">evaluate</strong> method of the model instance. This method returns the loss and any metrics that were passed to the model for training. The method can be called as follows when evaluating an out-of-sample test dataset:</p>
			<p class="source-code">test_loss = model.evaluate(X_test, y_test['y'])</p>
			<p>These model-fitting steps represent the basic steps that need to be followed to build, train, and evaluate models using the Keras package. From here, there are an infinite number of ways to build and evaluate a model, depending on the task you wish to accomplish. In the following activity, we will create an ANN to perform the same task that we completed in <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Machine Learning with Keras.</em> In fact, we will recreate the logistic regression algorithm with ANNs. As such, we expect there to be similar performance between the two models.</p>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor064"/>Activity 2.01: Creating a Logistic Regression Model Using Keras</h2>
			<p>In this activity, we are going to create a basic model using the Keras library. We will perform the same classification task that we did in <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Machine Learning with Keras</em>. We will use the same online shopping purchasing intention dataset and attempt to predict the same variable.</p>
			<p>In the previous chapter, we used a logistic regression model to predict whether a user would purchase a product from a website when given various attributes about the online session's behavior and the attributes of the web page. In this activity, we will introduce the Keras library, though we'll continue to utilize the libraries we introduced previously, such as <strong class="source-inline">pandas</strong>, for easily loading in the data, and <strong class="source-inline">sklearn</strong>, for any data preprocessing and model evaluation metrics.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Preprocessed datasets have been provided for you to use for this activity. You can download them from <a href="https://packt.live/2ApIBwT">https://packt.live/2ApIBwT</a>.</p>
			<p>The steps to complete this activity are as follows:</p>
			<ol>
				<li value="1">Load in the processed feature and target datasets.</li>
				<li>Split the training and target data into training and test datasets. The model will be fit to the training dataset and the test dataset will be used to evaluate the model.</li>
				<li>Instantiate a model of the <strong class="source-inline">Sequential</strong> class from the <strong class="source-inline">keras.models</strong> library.</li>
				<li>Add a single layer of the <strong class="source-inline">Dense</strong> class from the <strong class="source-inline">keras.layers</strong> package to the model instance. The number of nodes should be equal to the number of features in the feature dataset.</li>
				<li>Add a sigmoid activation function to the model.</li>
				<li>Compile the model instance by specifying the optimizer to use, the loss metric to evaluate, and any other metrics to evaluate after each epoch.</li>
				<li>Fit the model to the training data, specifying the number of epochs to run for and the validation split to use.</li>
				<li>Plot the loss and other evaluation metrics with respect to the epoch that will be evaluated on the training and validation datasets.</li>
				<li>Evaluate the loss and other evaluation metrics on the test dataset.</li>
			</ol>
			<p>After implementing these steps, you should get the following expected output:</p>
			<p class="source-code">2466/2466 [==============================] - 0s 15us/step</p>
			<p class="source-code">The loss on the test set is 0.3632 and the accuracy is 86.902%</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 356.</p>
			<p>In this activity, we looked at some of the fundamental concepts of creating ANNs in Keras, including various layer types and activation functions. We used these components to create a simple logistic regression model using a package that gives us similar results to the logistic regression model we used in <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Machine Learning with Keras</em>. We learned how to build the model with the Keras library, train the model with a real-world dataset, and evaluate the performance of the model on a test dataset to provide an unbiased evaluation of the performance of the model.</p>
			<h1 id="_idParaDest-64"><a id="_idTextAnchor065"/>Summary</h1>
			<p>In this chapter, we covered the various types of linear algebra components and operations that pertain to machine learning. These components include scalars, vectors, matrices, and tensors. The operations that were applied to these tensors included addition, transposition, and multiplication—all of which are fundamental for understanding the underlying mathematics of ANNs.</p>
			<p>We also learned some of the basics of the Keras package, including the mathematics that occurs at each node. We replicated the model from the previous chapter, in which we built a logistic regression model to predict the same target from the online shopping purchasing intention dataset. However, in this chapter, we used the Keras library to create the model using an ANN instead of the scikit-learn logistic regression model. We achieved a similar level of accuracy using ANNs.</p>
			<p>The upcoming chapters of this book will use the same concepts we learned about in this chapter; however, we will continue building ANNs with the Keras package. We will extend our ANNs to more than a single layer by creating models that have multiple hidden layers. By adding multiple hidden layers to our ANNs, we will put the "deep" into "deep learning". We will also tackle the issues of underfitting and overfitting since they are related to training models with ANNs.</p>
		</div>
		<div>
			<div id="_idContainer073" class="Content">
			</div>
		</div>
	</body></html>