<html><head></head><body>
  <div id="_idContainer184">
    <h1 class="chapterNumber">10</h1>
    <h1 id="_idParaDest-189" class="chapterTitle">Conceptual Representation Learning</h1>
    <p class="normal">Understanding cutting-edge machine learning and deep learning theory only marks the beginning of your adventure. The knowledge you have acquired should help you become an AI visionary. Take everything you see as opportunities and see how AI can fit into your projects. Reach the limits and skydive beyond them.</p>
    <p class="normal">This chapter focuses on decision-making through visual representations and explains the motivation that led to <strong class="bold">conceptual representation learning</strong> (<strong class="bold">CRL</strong>) and <strong class="bold">metamodels</strong> (<strong class="bold">MM</strong>), which form <strong class="bold">CRLMMs</strong>.</p>
    <p class="normal">Concept learning is our human ability to partition the world from chaos to categories, classes, sets, and subsets. As a child and young adult, we acquire many classes of things and concepts. For example, once we understand what a "hole" is, we can apply it to anything we see that is somewhat empty: a black hole, a hole in the wall, a hole in a bank account if money is missing or overspent, and hundreds of other cases.</p>
    <p class="normal">By performing concept learning, we humans do not have to learn the same concept over and over again for each case. For example, a hole is a hole. So when we see a new situation such as a crater, we know it's just a "big" hole. I first registered a word2vector patent early in my career. Then I rapidly applied it to concept2vector algorithms. I then designed and developed the CRLMM method successfully for <strong class="bold">automatic planning and scheduling</strong> (<strong class="bold">APS</strong>) software, cognitive chatbots, and more, as we will see in the following chapters. The metamodel term means that I applied one single model to many different domains, just as we humans do.</p>
    <p class="normal">Conceptual representations also provide visual images of concepts. To plan, humans need to visualize necessary information (events, locations, and so on) and more critical <em class="italics">visual dimensions</em> such as <em class="italics">image concepts</em>. A human being thinks in <em class="italics">mental images</em>. When we think, mental images flow through our minds with numbers, sounds, odors, and sensations, transforming our environment into fantastic multidimensional representations similar to video clips.</p>
    <p class="normal">The following topics will be covered in this chapter:</p>
    <ul>
      <li class="list">An approach to CRLMM in three steps:<ul>
          <li class="Bullet-Within-Bullet--PACKT-">Transfer learning to avoid developing a new program for each variation of a similar case</li>
          <li class="Bullet-Within-Bullet--PACKT-">Domain learning to avoid developing a new program each time the domain changes</li>
          <li class="Bullet-Within-Bullet--PACKT-">The motivation for using CRLMM</li>
        </ul>
        <p class="Bullet-Without-Bullet-Within-Bullet--PACKT-">Over the years, I've successfully implemented CRL in C++, Java, and logic programming (Prolog) in various forms on corporate sites. In this chapter, I'll use Python to illustrate the approach with TensorFlow 2.x with the <strong class="bold">convolutional neural network (CNN)</strong> built in <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>.</p>
      </li>
      <li class="list">Transfer learning using a CNN model trained to generalize image recognition</li>
      <li class="list">Domain learning to extend image recognition trained in one field to another field</li>
    </ul>
    <p class="normal">We'll begin this chapter by looking at the benefits of transfer learning and how concept learning can boost this process.</p>
    <h1 id="_idParaDest-190" class="title">Generating profit with transfer learning</h1>
    <p class="normal">Transfer learning<a id="_idIndexMarker476"/> means that we can use a model we designed and <a id="_idIndexMarker477"/>trained in another similar case. This will make the model very profitable since we do not have to design a new model and write a new program for every new case. You will thus generate profit for your company or customer by lowering the cost of new implementations of your trained model. Think of a good AI model as a reusable tool when applied to similar cases. This is why concept learning, being more general and abstract, is profitable. That is how we humans adapt.</p>
    <p class="normal">When it comes to reasoning and thinking in general, we use mental images with some words. Our thoughts contain concepts, on which we build solutions. </p>
    <p class="normal">The trained model from <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>, can now classify images of a certain type. In this section, the trained model will be loaded and then generalized through transfer learning to classify similar images.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">You will notice that I did not <a id="_idIndexMarker478"/>use many images for the example. My goal was to explain <a id="_idIndexMarker479"/>the process, not to go into building large datasets, which is a task in itself. The primary goal is to <em class="italics">understand</em> CNNs and conceptual learning representations.</p>
    </div>
    <h2 id="_idParaDest-191" class="title">The motivation behind transfer learning</h2>
    <p class="normal">Transfer learning provides <a id="_idIndexMarker480"/>a cost-effective way of using trained models for other purposes within the same company, such as the food processing company described in <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>.</p>
    <p class="normal">This chapter describes how the food processing company used the model for other similar purposes.</p>
    <p class="normal">The company that succeeds in doing this will progressively generalize the use of the solution. By doing so, inductive abstraction will take place and lead to other AI projects, which will prove gratifying to the management of a corporation and the teams providing the solutions.</p>
    <h3 id="_idParaDest-192" class="title">Inductive thinking</h3>
    <p class="normal">Induction uses <a id="_idIndexMarker481"/>inferences to reach a conclusion. For example, a food processing conveyor belt with missing products will lead to packaging productivity problems. If an insufficient amount of products reaches the packaging section, this will slow down the whole production process.</p>
    <p class="normal">By observing similar problems in other areas of the company, inferences from managers will come up, such as <em class="italics">if insufficient amounts of products flow through the process, production will slow down</em>.</p>
    <h3 id="_idParaDest-193" class="title">Inductive abstraction</h3>
    <p class="normal">The project team in <a id="_idIndexMarker482"/>charge of improving efficiency in any company needs to find an <em class="italics">abstract representation</em> of a problem to implement a solution through organization or software. This book deals with the AI side of solving problems. Organizational processes need to define how AI will fit in, with several on-site meetings.</p>
    <h3 id="_idParaDest-194" class="title">The problem AI needs to solve</h3>
    <p class="normal">In this particular example, each section of the <a id="_idIndexMarker483"/>factory has an <strong class="bold">optimal production rate</strong> (<strong class="bold">OPR</strong>) defined per hour or per day, for example. The equation of an OPR per hour can be summed up as follows:</p>
    <p class="center">OPR : min(<em class="italics">p</em>(<em class="italics">s</em>)) &lt;= OPR &lt;= max(<em class="italics">p</em>(<em class="italics">s</em>))</p>
    <p class="normal">Where:</p>
    <ul>
      <li class="list"><em class="italics">p</em> is the production rate of a given section (the different production departments of a factory) <em class="italics">s</em>.</li>
      <li class="list"><em class="italics">p</em>(<em class="italics">s</em>) is the production rate of the section.</li>
      <li class="list">min(<em class="italics">p</em>(<em class="italics">s</em>)) is the historical minimum (trial and error over months of analysis). Under that level, the whole production process will slow down.</li>
      <li class="list">max(<em class="italics">p</em>(<em class="italics">s</em>)) is the historical maximum. Over that level, the whole production process will slow down as well.</li>
      <li class="list">OPR is the optimal production rate.</li>
    </ul>
    <p class="normal">The first time somebody sees this equation, it seems difficult to understand. The difficulty arises because you have to visualize the process, which is the goal of this chapter. Every warehouse, industry, and service uses production rates as a constraint to reach profitable levels.</p>
    <p class="normal">Visualization requires representation at two levels:</p>
    <ul>
      <li class="list">Ensuring that if a packaging department is not receiving enough products, it will have to slow down or even stop sometimes.</li>
      <li class="list">Ensuring that if a packaging department receives too many products, it will not be able to package them. If the input is a conveyor belt with no intermediate storage (present-day trends), then it will have to be slowed down, slowing down or stopping the processes before that point.</li>
    </ul>
    <p class="normal">In both cases, slowing down production leads to bad financial results and critical sales problems through late deliveries.</p>
    <p class="normal">In both cases, an <a id="_idIndexMarker484"/>OPR gap is a problem. To solve this problem, another level of abstraction is required. First, let's break down the OPR equation into two parts:</p>
    <p class="center">OPR &gt;= min(<em class="italics">p</em>(<em class="italics">s</em>))</p>
    <p class="center">OPR &lt;= max(<em class="italics">p</em>(<em class="italics">s</em>))</p>
    <p class="normal">Now let's find a higher control level through variance variable <em class="italics">v</em>:</p>
    <p class="center"><em class="italics">v</em><sub style="font-style: italic;">min</sub> = |OPR – min(<em class="italics">p</em>(<em class="italics">s</em>))|</p>
    <p class="center"><em class="italics">v</em><sub style="font-style: italic;">max</sub> = |OPR – max(<em class="italics">p</em>(<em class="italics">s</em>))|</p>
    <p class="normal"><em class="italics">v</em><sub style="font-style: italic;">min</sub> and <em class="italics">v</em><sub style="font-style: italic;">max</sub> are the absolute values of the variance in both situations (not enough products to produce and too many to produce respectively).</p>
    <p class="normal">The final representation is through a single control, detection, and learning rate (the Greek letter gamma):</p>
    <figure class="mediaobject"><img src="../Images/B15438_10_001.png" alt=""/></figure>
    <p class="normal">The variance between the optimal production rate of a given section of a company and its minimum speed (products per hour) will slow the following section down. If too few cakes (<em class="italics">v</em><sub style="font-style: italic;">min</sub>), for example, are produced, then the cake packaging department will be waiting and will have to stop. If too many cakes are produced (<em class="italics">v</em><sub style="font-style: italic;">max</sub>), then the section will have to slow down or stop. Both variances would create problems in a company that cannot manage intermediate storage easily, which is the case with the food processing industry.</p>
    <p class="normal">With this single <img src="../Images/B15438_10_002.png" alt=""/> concept, introduced in <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>, the TensorFlow 2.x CNN can start learning a fundamental production concept: what a physical gap is. Let's go back to humans. Once we understand that a gap is some kind of hole or empty space, we can identify and represent thousands of situations with that one gap concept that is here converted into a parameter named gamma (<img src="../Images/B15438_10_003.png" alt=""/>). Let's explore the concept and then implement it.</p>
    <h2 id="_idParaDest-195" class="title">The <img src="../Images/B15438_10_004.png" alt=""/> gap concept</h2>
    <p class="normal">Teaching the CNN the gap concept<a id="_idIndexMarker485"/> will help it extend its thinking power to many fields:</p>
    <ul>
      <li class="list">A gap in production, as explained before</li>
      <li class="list">A gap in a traffic lane for a self-driving vehicle to move into</li>
      <li class="list">Any incomplete, deficient area</li>
      <li class="list">Any opening or window</li>
    </ul>
    <p class="normal">Let's teach a CNN the <img src="../Images/B15438_10_005.png" alt=""/> gap concept, or simply, <img src="../Images/B15438_10_006.png" alt=""/>. The symbol <img src="../Images/B15438_10_007.png" alt=""/> of a gap is the Greek letter "gamma," so it is simply pronounced "gamma." We thus lead to teaching a CNN how to recognize a gap we will call gamma (<img src="../Images/B15438_10_005.png" alt=""/>). The goal is for a CNN to understand the abstract concept of an empty space, a hole represented by the word <em class="italics">gap</em> and the Greek letter gamma (<img src="../Images/B15438_10_005.png" alt=""/>).</p>
    <p class="normal">To achieve that goal, the CNN model that was trained and saved in <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>, now needs to be loaded and used. To <a id="_idIndexMarker486"/>grasp the implications of the <img src="../Images/B15438_10_010.png" alt=""/> concept, imagine the cost of not producing enough customer orders or having piles of unfinished products everywhere. The financial transposition of the physical gap is a profit <strong class="bold">variance</strong> on set goals. We all know the pain those variances lead to.</p>
    <h2 id="_idParaDest-196" class="title">Loading the trained TensorFlow 2.x model</h2>
    <p class="normal">The technical goal is <a id="_idIndexMarker487"/>to load and use the trained CNN model and then use the same model for other similar areas. The practical goal is to teach the CNN how to use the <img src="../Images/B15438_10_011.png" alt=""/> <strong class="bold">concept</strong> to enhance the thinking abilities of the scheduling, chatbot, and other applications.</p>
    <p class="normal">Loading the model has two main functions:</p>
    <ul>
      <li class="list">Loading the model to compile and classify new images without training the model</li>
      <li class="list">Displaying the parameters used layer by layer and displaying the weights reached during the learning and training phase</li>
    </ul>
    <p class="normal">In the following section, we will load and display the model without training it.</p>
    <h3 id="_idParaDest-197" class="title">Loading and displaying the model</h3>
    <p class="normal">A limited number of<a id="_idIndexMarker488"/> headers suffice to read a saved model with <code class="Code-In-Text--PACKT-">READ_MODEL.py</code>, as implemented in the following lines:</p>
    <pre class="programlisting"><code class="hljs haskell"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-title">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-title">from</span> tensorflow.keras <span class="hljs-keyword">import</span> datasets, layers, models
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-title">from</span> keras.preprocessing.image <span class="hljs-keyword">import</span> load_img
<span class="hljs-title">from</span> keras.preprocessing.image <span class="hljs-keyword">import</span> img_to_array
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-title">from</span> <span class="hljs-type">PIL</span> <span class="hljs-keyword">import</span> Image
<span class="hljs-meta">#Directory</span>
<span class="hljs-title">directory</span>='dataset/'
<span class="hljs-title">print</span>(<span class="hljs-string">"directory"</span>,directory)
</code></pre>
    <p class="normal">The <code class="Code-In-Text--PACKT-">model3.h5</code> model saved is now loaded from its file, as shown here:</p>
    <pre class="programlisting"><code class="hljs stylus">#____________________LOAD MODEL____________________________
loaded_model = keras<span class="hljs-selector-class">.models</span>.load_model(directory+<span class="hljs-string">"model/model3.h5"</span>)
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(loaded_model.summary()</span></span>)
</code></pre>
    <p class="normal">The loaded model needs to be compiled:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment"># __________________compile loaded model</span>
loaded_model.compile(<span class="hljs-attribute">loss</span>=<span class="hljs-string">'binary_crossentropy'</span>, <span class="hljs-attribute">optimizer</span>=<span class="hljs-string">'rmsprop'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <p class="normal">Reading and displaying the model is not a formality.</p>
    <p class="normal">Printing the structure provides useful information:</p>
    <pre class="programlisting"><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"GLOBAL MODEL STRUCTURE"</span>)</span></span>
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(loaded_model.summary()</span></span>)
</code></pre>
    <p class="normal">The trained <a id="_idIndexMarker489"/>model might or might not work on all datasets. In that case, the following output would point to problems that can be fixed through its structure, for example, as follows:</p>
    <pre class="programlisting"><code class="hljs markdown">MODEL STRUCTURE
Model: "sequential"
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
<span class="hljs-section">Layer (type)                 Output Shape              Param #   
=================================================================</span>
conv2d (Conv2D)              (None, 62, 62, 32)        896       
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
max_pooling2d (MaxPooling2D) (None, 31, 31, 32)        0         
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
conv2d_1 (Conv2D)            (None, 29, 29, 32)        9248      
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
max<span class="hljs-emphasis">_pooling2d_</span>1 (MaxPooling2 (None, 14, 14, 32)        0         
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
flatten (Flatten)            (None, 6272)              0         
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
dense (Dense)                (None, 128)               802944    
<span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span><span class="hljs-strong">_____</span>
<span class="hljs-section">dense_1 (Dense)              (None, 1)                 129       
=================================================================</span>
Total params: 813,217
Trainable params: 813,217
Non-trainable params: 0
</code></pre>
    <p class="normal">Once the global structure has been displayed, it is possible to look into the structure of each layer. For example, we can peek into the <code class="Code-In-Text--PACKT-">conv2d</code> layer:</p>
    <pre class="programlisting"><code class="hljs lasso">DETAILED MODEL STRUCTURE
{<span class="hljs-string">'name'</span>: <span class="hljs-string">'conv2d'</span>, <span class="hljs-string">'trainable'</span>: <span class="hljs-literal">True</span>, <span class="hljs-string">'batch_input_shape'</span>: (<span class="hljs-literal">None</span>, <span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>), <span class="hljs-string">'dtype'</span>: <span class="hljs-string">'float32'</span>, <span class="hljs-string">'filters'</span>: <span class="hljs-number">32</span>, <span class="hljs-string">'kernel_size'</span>: (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>), <span class="hljs-string">'strides'</span>: (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), <span class="highlight"><span class="hljs-string">'padding'</span></span>: <span class="hljs-string">'valid'</span>, <span class="hljs-string">'data_format'</span>: <span class="hljs-string">'channels_last'</span>, <span class="hljs-string">'dilation_rate'</span>: (<span class="hljs-number">1</span>, <span class="hljs-number">1</span>), <span class="hljs-string">'activation'</span>: <span class="hljs-string">'relu'</span>, <span class="hljs-string">'use_bias'</span>: <span class="hljs-literal">True</span>, <span class="hljs-string">'kernel_initializer'</span>: {<span class="hljs-string">'class_name'</span>: <span class="hljs-string">'GlorotUniform'</span>, <span class="hljs-string">'config'</span>: {<span class="hljs-string">'seed'</span>: <span class="hljs-literal">None</span>}}, <span class="hljs-string">'bias_initializer'</span>: {<span class="hljs-string">'class_name'</span>: <span class="hljs-string">'Zeros'</span>, <span class="hljs-string">'config'</span>: {}}, <span class="hljs-string">'kernel_regularizer'</span>: <span class="hljs-literal">None</span>, <span class="hljs-string">'bias_regularizer'</span>: <span class="hljs-literal">None</span>, <span class="hljs-string">'activity_regularizer'</span>: <span class="hljs-literal">None</span>, <span class="hljs-string">'kernel_constraint'</span>: <span class="hljs-literal">None</span>, <span class="hljs-string">'bias_constraint'</span>: <span class="hljs-literal">None</span>}
{<span class="hljs-string">'name'</span>: <span class="hljs-string">'max_pooling2d'</span>, <span class="hljs-string">'trainable'</span>: <span class="hljs-literal">True</span>, <span class="hljs-string">'dtype'</span>: <span class="hljs-string">'float32'</span>, <span class="hljs-string">'pool_size'</span>: (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), <span class="highlight"><span class="hljs-string">'padding'</span></span>: <span class="hljs-string">'valid'</span>, <span class="hljs-string">'strides'</span>: (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>), <span class="hljs-string">'data_format'</span>: <span class="hljs-string">'channels_last'</span>}
</code></pre>
    <p class="normal">Each parameter<a id="_idIndexMarker490"/> contains very useful information. For example, <code class="Code-In-Text--PACKT-">'padding':'valid'</code> means that padding has not been applied. In this model, the number and size of the kernels provide satisfactory results without padding, and the shape decreases to the final status layer (classification), as shown here:</p>
    <pre class="programlisting"><code class="hljs angelscript">initial shape (<span class="hljs-number">570</span>, <span class="hljs-number">597</span>, <span class="hljs-number">4</span>)
lay: <span class="hljs-number">1</span> filters shape (<span class="hljs-number">568</span>, <span class="hljs-number">595</span>, <span class="hljs-number">3</span>)
lay: <span class="hljs-number">2</span> Pooling shape (<span class="hljs-number">113</span>, <span class="hljs-number">119</span>, <span class="hljs-number">3</span>)
lay: <span class="hljs-number">3</span> filters shape (<span class="hljs-number">111</span>, <span class="hljs-number">117</span>, <span class="hljs-number">3</span>)
lay: <span class="hljs-number">4</span> pooling shape (<span class="hljs-number">22</span>, <span class="hljs-number">23</span>, <span class="hljs-number">3</span>)
lay: <span class="hljs-number">5</span> flatten shape (<span class="hljs-number">1518</span>,)
lay: <span class="hljs-number">6</span> dense shape (<span class="hljs-number">128</span>,)
lay: <span class="hljs-number">7</span> dense shape (<span class="hljs-number">1</span>,)
</code></pre>
    <p class="normal">However, suppose you want to control the output shape of a layer so that the spatial dimensions do not decrease faster than necessary. One reason could be that the next layer will explore the edges of the image and that we need to explore them with kernels that fit the shape.</p>
    <p class="normal">In that case, padding of size 1 can be added with <code class="Code-In-Text--PACKT-">0</code> values, as shown in the following matrix:</p>
    <table id="table001-7" class="No-Table-Style _idGenTablePara-1">
      <colgroup>
        <col/>
        <col/>
        <col/>
        <col/>
        <col/>
        <col/>
      </colgroup>
      <tbody>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">1</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">3</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">24</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">4</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">3</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">7</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">8</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">5</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">6</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">4</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">5</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">4</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">5</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">4</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">3</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content"><strong class="bold">1</strong></p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
        </tr>
        <tr class="No-Table-Style">
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
          <td class="No-Table-Style">
            <p class="content">0</p>
          </td>
        </tr>
      </tbody>
    </table>
    <p class="normal">A padding of size 2 would add two rows and columns around the initial shape.</p>
    <p class="normal">With that in mind, fine-tuning your training model by adding as many options as necessary will improve the quality of the results. The weights can be viewed by extracting them from the saved model file layer by layer, as shown in the following code snippet:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-builtin-name">print</span>(<span class="hljs-string">"WEIGHTS"</span>)
<span class="hljs-keyword">for</span> layer <span class="hljs-keyword">in</span> loaded_model.layers:
    weights = layer.get_weights() # list of numpy arrays
    <span class="hljs-builtin-name">print</span>(weights)
</code></pre>
    <p class="normal">Analyzing the weights<a id="_idIndexMarker491"/> used by the program will provide useful information about the way the optimization process was carried out by the program. Sometimes, a program will get stuck, and the weights might seem off track. After all, a CNN can contain imperfections like any other program.</p>
    <p class="normal">A look at the following output, for example, can help understand where the system went wrong:</p>
    <pre class="programlisting"><code class="hljs subunit">WEIGHTS
[array([[ 6.25981949e<span class="hljs-string">-03</span>,  2.35006157e<span class="hljs-string">-02</span>, <span class="hljs-string">-1</span>.28920656e<span class="hljs-string">-02</span>, ...,
        <span class="hljs-string">-8</span>.34930502e<span class="hljs-string">-03</span>,  2.00010985e<span class="hljs-string">-02</span>, <span class="hljs-string">-1</span>.84428487e<span class="hljs-string">-02</span>],
       [<span class="hljs-string">-1</span>.01672988e<span class="hljs-string">-02</span>,  1.87084991e<span class="hljs-string">-02</span>,  2.49958578e<span class="hljs-string">-02</span>, ...,
        <span class="hljs-string">-2</span>.92361379e<span class="hljs-string">-02</span>, <span class="hljs-string">-2</span>.33592112e<span class="hljs-string">-02</span>, <span class="hljs-string">-1</span>.64737436e<span class="hljs-string">-03</span>],
       [<span class="hljs-string">-2</span>.71108598e<span class="hljs-string">-02</span>,  2.53492035e<span class="hljs-string">-03</span>, <span class="hljs-string">-2</span>.90711448e<span class="hljs-string">-02</span>, ...,
</code></pre>
    <p class="normal">We can now use the loaded and checked model.</p>
    <h3 id="_idParaDest-198" class="title">Loading the model to use it</h3>
    <p class="normal">Loading the <a id="_idIndexMarker492"/>model with <code class="Code-In-Text--PACKT-">CNN_CONCEPT_STRATEGY.py</code> requires a limited number of headers, as follows:</p>
    <pre class="programlisting"><code class="hljs elm"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-title">from</span> tensorflow <span class="hljs-keyword">import</span> keras
<span class="hljs-title">from</span> tensorflow.keras <span class="hljs-keyword">import</span> datasets, layers, models
<span class="hljs-keyword">import</span> matplotlib.pyplot <span class="hljs-keyword">as</span> plt
<span class="hljs-title">from</span> keras.preprocessing.image <span class="hljs-keyword">import</span> load_img
<span class="hljs-title">from</span> keras.preprocessing.image <span class="hljs-keyword">import</span> img_to_array
<span class="hljs-keyword">import</span> numpy <span class="hljs-keyword">as</span> np
<span class="hljs-title">from</span> <span class="hljs-type">PIL</span> <span class="hljs-keyword">import</span> Image
</code></pre>
    <p class="normal">Loading the model is done by using the same code as in <code class="Code-In-Text--PACKT-">READ_MODEL.py</code>, described previously. Once you load it, compile the model with the <code class="Code-In-Text--PACKT-">model.compile</code> function, as follows:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment"># __________________compile loaded model</span>
loaded_model.compile(<span class="hljs-attribute">loss</span>=<span class="hljs-string">'binary_crossentropy'</span>, <span class="hljs-attribute">optimizer</span>=<span class="hljs-string">'rmsprop'</span>, metrics=[<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <p class="normal">The model used for this<a id="_idIndexMarker493"/> example and the image identification function has been implemented in two parts. First, we're loading and resizing the image with the following function, for example:</p>
    <pre class="programlisting"><code class="hljs leaf">def identify(target_image):
    filename = target_image
    original = load_img(filename, target_size=(64, 64))
    <span class="hljs-function"><span class="hljs-keyword">#</span><span class="hljs-title">print</span><span class="hljs-params">('<span class="hljs-variable">PIL</span> <span class="hljs-variable">image</span> <span class="hljs-variable">size</span>',<span class="hljs-variable">original</span>.<span class="hljs-variable">size</span>)</span></span>
    if(display==1):
        plt.imshow(original)
        plt.show()
    numpy_image = img_to_array(original)
    inputarray = numpy_image[np.newaxis,...] # extra dimension to fit model
    arrayresized=np.resize(inputarray,(64,64))
    <span class="hljs-function"><span class="hljs-keyword">#</span><span class="hljs-title">print</span><span class="hljs-params">('<span class="hljs-variable">Resized</span>',<span class="hljs-variable">arrayresized</span>)</span></span>
</code></pre>
    <p class="normal">The model expects another dimension in the input array to predict, so one is added to fit the model. In this example, one image at a time needs to be identified.</p>
    <p class="normal">I added the following two prediction methods and returned one:</p>
    <pre class="programlisting"><code class="hljs properties"><span class="hljs-comment">#___PREDICTION___</span>
    <span class="hljs-attr">prediction</span> = <span class="hljs-string">loaded_model.predict_proba(inputarray)</span>
    <span class="hljs-attr">return</span> <span class="hljs-string">prediction</span>
</code></pre>
    <p class="normal">There are two prediction methods because, basically, every component needs to be checked in a CNN during a project's implementation phase, to choose the best and fastest ones. To test <code class="Code-In-Text--PACKT-">prediction2</code>, just change the <code class="Code-In-Text--PACKT-">return</code> instruction.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Once a CNN is running, it can prove difficult to find out what went wrong. Checking the output of each layer and component while building the network saves fine-tuning time once the full-blown model produces thousands of results.</p>
    </div>
    <p class="normal">The following example detects product <img src="../Images/B15438_10_012.png" alt=""/> <strong class="bold">gaps</strong> on a conveyor belt in a food processing factory. The program loads the first image stored in the <code class="Code-In-Text--PACKT-">classify</code> directory to predict its value. The program describes the prediction:</p>
    <pre class="programlisting"><code class="hljs isbl"><span class="hljs-variable">MS1</span>=<span class="hljs-string">'productive'</span>
<span class="hljs-variable">MS2</span>=<span class="hljs-string">'gap'</span>
<span class="hljs-variable">s</span>=<span class="hljs-function"><span class="hljs-title">identify</span>(<span class="hljs-variable">directory</span>+<span class="hljs-string">'classify/img1.jpg'</span>)</span>
<span class="hljs-variable"><span class="hljs-keyword">if</span></span> (<span class="hljs-function"><span class="hljs-title">int</span>(<span class="hljs-variable">s</span>)==<span class="hljs-number">0</span>):
    <span class="hljs-title">print</span>(<span class="hljs-string">'Classified in class A'</span>)</span>
    <span class="hljs-function"><span class="hljs-title">print</span>(<span class="hljs-variable">MS1</span>)</span>
</code></pre>
    <p class="normal">The program<a id="_idIndexMarker494"/> displays (optional) the shaped image, as follows, which shows that the conveyor belt has a sufficient number of products at that point:</p>
    <figure class="mediaobject"><img src="../Images/B15438_10_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.1: Output (shaped image)</p>
    <p class="normal">The program then makes and displays its prediction <code class="Code-In-Text--PACKT-">0</code>, meaning no real gap has been found on the conveyor belt on this production line:</p>
    <pre class="programlisting"><code class="hljs lua">directory dataset/
Strategy model <span class="hljs-built_in">loaded</span> from training repository.
image dataset/classify/img1.jpg predict_proba: <span class="hljs-string">[[ 0.]]</span> predict: <span class="hljs-string">[[ 0.]]</span>
Classified <span class="hljs-keyword">in</span> class A
Productive
Seeking...
</code></pre>
    <p class="normal"><code class="Code-In-Text--PACKT-">Seeking...</code> means it is going to analyze the second image in the classify direction. It loads, displays, and predicts its value as shown in the following frame:</p>
    <figure class="mediaobject"><img src="../Images/B15438_10_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.2: Output (shaped image)</p>
    <p class="normal">The prediction (<em class="italics">value</em> = 1) correctly detected gaps on the conveyor belt, as shown in the following output:</p>
    <pre class="programlisting"><code class="hljs lua">image dataset/classify/img2.jpg predict_proba: <span class="hljs-string">[[ 1.]]</span> predict: <span class="hljs-string">[[ 1.]]</span>
Classified <span class="hljs-keyword">in</span> class B
gap
</code></pre>
    <p class="normal">Now that the predictions of the CNN have been verified, the implementation strategy needs approval. A <a id="_idIndexMarker495"/>CNN contains marvels of applied mathematics. CNNs epitomize deep learning themselves. A researcher could easily spend hundreds of hours studying them.</p>
    <p class="normal">However, applied mathematics in the business world requires profitability. As such, the components of CNNs appear to be ever-evolving concepts. Added kernels, activation functions, pooling, flattening, dense layers, and compiling and training methods act as a starting point for architectures, not as a finality.</p>
    <h4 class="title">Using transfer learning to be profitable or see a project stopped</h4>
    <p class="normal">At some point, a <a id="_idIndexMarker496"/>company will demand results and may shelve a project if those results are not delivered. If a spreadsheet represents a faster sufficient solution, a deep learning project will face potential competition and rejection. Many engineers learning artificial intelligence have to assume the role of standard SQL reporting experts before accessing real AI projects. Transfer learning is a profitable solution that can boost the credibility of an IT department.</p>
    <p class="normal">Transfer learning appears to be a solution to the present cost of building and training a CNN program. Your model might just pay off that way. The idea is to get a basic AI model rolling profits in fast for your customer and management. Then, you will have everybody's attention. To do that, you must define a strategy.</p>
    <h3 id="_idParaDest-199" class="title">Defining a strategy</h3>
    <p class="normal">If a deep learning CNN <a id="_idIndexMarker497"/>expert comes to a top manager saying that this CNN model can classify CIFAR-10 images of dogs, cats, cars, plants, and more, the answer will be, <em class="italics">so what? My 3-year-old child can too. In fact, so can my dog!</em></p>
    <p class="normal">The IT manager in that meeting might even blurt out something like, "We have all the decision tools we need right now, and our profits are increasing. Why would we invest in a CNN?"</p>
    <p class="normal">The core problem of marketing AI to real-world companies is that it relies upon a belief in the necessity of a CNN in the first place. Spreadsheets, SQL queries, standard automation, and software do 99% of the job. Most of the time, it does not take a CNN to replace many jobs; just an automated spreadsheet, a query, or standard, straightforward software is enough. Jobs have been sliced into simple-enough parts to replace humans with basic software for decades.</p>
    <p class="normal">Before presenting a CNN, a data scientist has to find out how much the company can earn using it.</p>
    <div class="note">
      <p class="Information-Box--PACKT-">Understanding, designing, building, and running a CNN does not mean much regarding business. All the hard work we put into understanding and running these complex programs will add up to nothing if we cannot prove that a solution will generate profit. Without profit, the implementation costs cannot be recovered, and nobody will listen to a presentation about even a fantastic program.</p>
    </div>
    <p class="normal">Applying a model efficiently means implementing it in one area of a company and then other domains for a good return on investment.</p>
    <h4 class="title">Applying the model</h4>
    <p class="normal">In a food processing <a id="_idIndexMarker498"/>company, for example, one of the packaging lines has a performance problem. Sometimes, randomly, some of the cakes are missing on the conveyor belt, as shown in the following frame:</p>
    <figure class="mediaobject"><img src="../Images/B15438_10_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.3: Food processing company example</p>
    <p class="normal">To start a cost-effective project, a cheap webcam could be installed over the conveyor belt. It'll take a random sample picture every 10 seconds and process it to find the holes shown in the interval in the center of the image. We can clearly see an empty space, a gap, a hole. If a hole is detected, it means some cakes have not made it to the conveyor belt (production errors).</p>
    <p class="normal">A 2% to 5% productivity<a id="_idIndexMarker499"/> rate increase could be obtained by automatically sending a signal to the production robot when some cakes are missing. The production robot will then send a signal to the production line to increase production to compensate for the missing units in real-time. This type of automatic control already exists in various forms of automated food production lines. However, this provides a low-cost way to start implementing this on a production line.</p>
    <h3 id="_idParaDest-200" class="title">Making the model profitable by using it for another problem</h3>
    <p class="normal">Let's say that the food <a id="_idIndexMarker500"/>processing experiment on the conveyor belt turns out to work well enough with dataset type <em class="italics">d</em><sub>1</sub> and the CNN model <em class="italics">M</em> to encourage generalization to another dataset, <em class="italics">d</em><sub>2</sub>, in the same company.</p>
    <p class="normal">Transfer learning consists of going from <em class="italics">M</em>(<em class="italics">d</em><sub>1</sub>) to <em class="italics">M</em>(<em class="italics">d</em><sub>2</sub>) using the same CNN model <em class="italics">M</em>, with some limited, cost-effective additional training. Variations will appear, but they can be solved by shifting a few parameters and working on the input data following some basic dataset preparation rules:</p>
    <ul>
      <li class="list"><strong class="bold">Overfitting</strong>: When the<a id="_idIndexMarker501"/> model fits the training data quickly with 100% accuracy, this may or may not be a problem. In the case of classifying holes on the conveyor belt, overfitting might not prove critical. The shapes are always the same, and the environment remains stable. However, in an unstable situation with all sorts of different images or products, then overfitting will limit the effectiveness of a system.</li>
      <li class="list"><strong class="bold">Underfitting</strong>: If the<a id="_idIndexMarker502"/> accuracy drops down to low levels, such as 20%, then the CNN will not work. The datasets and parameters need optimizing. Maybe the number of samples needs to be increased for <em class="italics">M</em>(<em class="italics">d</em><sub>2</sub>), or reduced, or split into different groups.</li>
      <li class="list"><strong class="bold">Regularization</strong>: Regularization, in <a id="_idIndexMarker503"/>general, involves the process of finding how to fix the generalization problem of <em class="italics">M</em>(<em class="italics">d</em><sub>2</sub>), not the training errors of <em class="italics">M</em>(<em class="italics">d</em><sub>2</sub>). Maybe an activation function needs some improvements, or the way the weights have been implemented requires attention.</li>
    </ul>
    <p class="normal">There is no limit to the number of methods you can apply to find a solution, just like standard software program improvements.</p>
    <h4 class="title">Where transfer learning ends and domain learning begins</h4>
    <p class="normal">Transfer learning can be used for similar types of objects or images in this example, as explained. The more similar images you can train within a company with the same model, the more return on investment (ROI) it will produce, and the more this company will ask you for more AI innovations.</p>
    <p class="normal">Domain learning takes a model such as the one described in <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>, and can generalize it. The generalization process will lead us to domain learning.</p>
    <h1 id="_idParaDest-201" class="title">Domain learning</h1>
    <p class="normal">This section on domain learning<a id="_idIndexMarker504"/> builds a bridge between classic transfer learning, as described previously, and another use of domain learning I have found profitable on corporate projects: teaching a machine a concept (CRLMM). In this chapter, we are focusing on teaching a machine to learn how to recognize a gap in situations other than at the food processing company.</p>
    <h2 id="_idParaDest-202" class="title">How to use the programs</h2>
    <p class="normal">You can read this whole chapter first to grasp the concepts or play with the programs first. Do as you feel is best for you. In any case, <code class="Code-In-Text--PACKT-">CNN_TDC_STRATEGY.py</code> loads trained models (you do not have to train them again for this chapter) and <code class="Code-In-Text--PACKT-">CNN_CONCEPT_STRATEGY.py</code> trains the models.</p>
    <h3 id="_idParaDest-203" class="title">The trained models used in this section</h3>
    <p class="normal">This <a id="_idIndexMarker505"/>section uses <code class="Code-In-Text--PACKT-">CNN_TDC_STRATEGY.py</code> to apply the trained models to the target concept images. <code class="Code-In-Text--PACKT-">READ_MODEL.py</code> (as shown previously) was converted into <code class="Code-In-Text--PACKT-">CNN_TDC_STRATEGY.py</code> by adding variable directory paths (for the <code class="Code-In-Text--PACKT-">model3.h5</code> files and images) and classification messages, as shown in the following code:</p>
    <pre class="programlisting"><code class="hljs ini"><span class="hljs-comment">#loads,traffic,food processing</span>
<span class="hljs-attr">A</span>=[<span class="hljs-string">'dataset_O/'</span>,<span class="hljs-string">'dataset_traffic/'</span>,<span class="hljs-string">'dataset/'</span>]
<span class="hljs-attr">MS1</span>=[<span class="hljs-string">'loaded'</span>,<span class="hljs-string">'jammed'</span>,<span class="hljs-string">'productive'</span>]
<span class="hljs-attr">MS2</span>=[<span class="hljs-string">'unloaded'</span>,<span class="hljs-string">'change'</span>,<span class="hljs-string">'gap'</span>]
<span class="hljs-comment">#____________________LOAD MODEL____________________________</span>
<span class="hljs-attr">loaded_model</span> = keras.models.load_model(directory+<span class="hljs-string">"model/model3.h5"</span>)....<span class="hljs-string">")
</span></code></pre>
    <p class="normal">The loaded model now targets the images to classify:</p>
    <pre class="programlisting"><code class="hljs ini"><span class="hljs-attr">s</span>=identify(directory+<span class="hljs-string">'classify/img1.jpg'</span>)
</code></pre>
    <p class="normal">Each subdirectory of the model contains four subdirectories:</p>
    <ul>
      <li class="list"><code class="Code-In-Text--PACKT-">classify</code>: Contains the images to classify</li>
      <li class="list"><code class="Code-In-Text--PACKT-">model</code>: The trained <code class="Code-In-Text--PACKT-">model3.h5</code> used to classify the images</li>
      <li class="list"><code class="Code-In-Text--PACKT-">test_set</code>: The test set of conceptual images</li>
      <li class="list"><code class="Code-In-Text--PACKT-">training_set</code>: The training set of conceptual images</li>
    </ul>
    <p class="normal">Now that we have<a id="_idIndexMarker506"/> explored the directory structure of our model, let's see how to use it in different situations.</p>
    <h3 id="_idParaDest-204" class="title">The trained model program</h3>
    <p class="normal">For this chapter, you do not need<a id="_idIndexMarker507"/> to train a model. It was already trained in <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>. The directory paths have become variables to access the subdirectories described previously. The paths can be called, as shown in the following code:</p>
    <pre class="programlisting"><code class="hljs routeros">A=[<span class="hljs-string">'dataset_O/'</span>,<span class="hljs-string">'dataset_traffic/'</span>,<span class="hljs-string">'dataset/'</span>]
<span class="hljs-attribute">scenario</span>=3 #reference <span class="hljs-keyword">to</span> A
<span class="hljs-attribute">directory</span>=A[scenario] #transfer learning parameter (choice of images)
<span class="hljs-builtin-name">print</span>(<span class="hljs-string">"directory"</span>,directory)
</code></pre>
    <p class="normal">You do not need to run training for this chapter. The models were trained and automatically stored in their respective subdirectories on the virtual machine delivered with the book. This means that when you need to detect gaps for various types of images, you can simply change the scenario to fit the type of images you will be receiving from the frames of a webcam: cakes, cars, fabric, or abstract symbols.</p>
    <div class="packt_tip">
      <p>For this chapter, focus on understanding the concepts. You can read the chapter without running the programs, open them without running them, or run them—whatever makes you comfortable. The main goal is to grasp the concepts to prepare for the subsequent chapters.</p>
    </div>
    <p class="normal">We have loaded the model and a scenario. Now, we are going to use our trained model to detect if a production line is loaded or underloaded.</p>
    <h2 id="_idParaDest-205" class="title">Gap – loaded or underloaded</h2>
    <p class="normal">The gap concept<a id="_idIndexMarker508"/> has just become a polysemy image-concept (polysemy means different meanings, as explained in <em class="italics">Chapter 6</em>, <em class="italics">Innovating AI with Google Translate</em>).</p>
    <p class="normal">In the cake situation, the <img src="../Images/B15438_10_013.png" alt=""/> gap was negative in its <em class="italics">g</em><sub>1</sub> subset of meaning and concepts applied to a CNN, relating it to negative images <em class="italics">n</em> + <em class="italics">g</em><sub>1</sub>:</p>
    <p class="center"><em class="italics">ng</em><sub>1</sub> = {missing, not enough, slowing production down … bad}</p>
    <p class="normal">The full-of-products image was positive, <em class="italics">p</em> + <em class="italics">g</em><sub>2</sub>:</p>
    <p class="center"><em class="italics">pg</em><sub>2</sub> = {good production flow, no gap}</p>
    <p class="normal">In this example, the CNN is learning how to distinguish an abstract representation, not simply an image, like for the cakes. Another subset of <img src="../Images/B15438_10_010.png" alt=""/> (the conceptual gap dataset) is loaded/underloaded. A "gap" is not a specific object but a general concept that can be applied to hundreds of different cases. This is why I use the term "conceptual gap dataset."</p>
    <p class="normal">The following abstract image is loaded. The squares represent production machines, and the arrows represent the load-in time. </p>
    <p class="normal">This means that the <em class="italics">x</em> axis represents time and the <em class="italics">y</em> axis represents machine production resources:</p>
    <figure class="mediaobject"><img src="../Images/B15438_10_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.4: Abstract image 1</p>
    <p class="normal">The CNN model runs and produces the following result:</p>
    <pre class="programlisting"><code class="hljs lua">directory dataset_O/
Strategy model <span class="hljs-built_in">loaded</span> from training repository.
image dataset_O/classify/img1.jpg predict_proba: <span class="hljs-string">[[ 0.]]</span> predict: <span class="hljs-string">[[ 0.]]</span>
Classified <span class="hljs-keyword">in</span> class A
<span class="hljs-built_in">loaded</span>
Seeking...
</code></pre>
    <p class="normal">The CNN recognizes this as a correctly loaded model. The task goes beyond classifying. The system needs to recognize this to make a decision.</p>
    <p class="normal">Another image produces a different result. In this case, an underloaded gap appears in the following screenshot:</p>
    <figure class="mediaobject"><img src="../Images/B15438_10_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.5: Abstract image 2</p>
    <p class="normal">And the CNN has <a id="_idIndexMarker509"/>a different output, as shown here:</p>
    <pre class="programlisting"><code class="hljs lua">Seeking...
image dataset_O/classify/img2.jpg predict_proba: <span class="hljs-string">[[ 1.]]</span> predict: <span class="hljs-string">[[ 1.]]</span>
Classified <span class="hljs-keyword">in</span> class
unloaded
</code></pre>
    <p class="normal">Read "unloaded" as "underloaded." Unloaded or underloaded represents empty spaces in any case. The gap concept <img src="../Images/B15438_10_015.png" alt=""/> has added two other subsets, <em class="italics">g</em><sub>3</sub> and <em class="italics">g</em><sub>4</sub>, to its dataset. We now have:</p>
    <figure class="mediaobject"><img src="../Images/B15438_10_016.png" alt=""/></figure>
    <p class="normal">The four <em class="italics">g</em><sub>1</sub> to <em class="italics">g</em><sub>4</sub> subsets of <img src="../Images/B15438_10_005.png" alt=""/> are:</p>
    <p class="center"><em class="italics">ng</em><sub>1</sub> = {missing, not enough, slowing production down … bad}</p>
    <p class="center"><em class="italics">pg</em><sub>2</sub> = <em class="italics">pg</em><sub>2</sub> = {good production flow, no gap}</p>
    <p class="center"><em class="italics">g</em><sub>3</sub> = {loaded}</p>
    <p class="center"><em class="italics">g</em><sub>4</sub> = {unloaded}</p>
    <p class="normal">The remaining <a id="_idIndexMarker510"/>problem will take some time to solve. <em class="italics">g</em><sub>4</sub> (gap) can sometimes represent an opportunity for a machine that does not have a good workload to be open to more production. In some cases, <em class="italics">g</em><sub>4</sub> becomes <em class="italics">pg</em><sub>4</sub> (<em class="italics">p</em> = positive). In other cases, it will become <em class="italics">ng</em><sub>4</sub> (<em class="italics">n</em> = negative) if production rates go down.</p>
    <p class="normal">In this section, we saw how to identify a "gap" in production lines. As explained, a "gap" is a generic concept for spaces everywhere. We will now explore jammed or "open" traffic lanes.</p>
    <h2 id="_idParaDest-206" class="title">Gap – jammed or open lanes</h2>
    <p class="normal">The model in this chapter can be extended to other domains. A self-driving car needs to recognize whether it is in a traffic jam or not. Also, a self-driving car has to know how to change lanes when it detects enough space (a gap) to do that.</p>
    <p class="normal">This produces two new subsets:</p>
    <p class="center"><em class="italics">g</em><sub>5</sub> = {traffic jam, heavy traffic … too much traffic}</p>
    <p class="center"><em class="italics">g</em><sub>6</sub> = {open lane, light traffic … normal traffic}</p>
    <p class="normal">The model now detects <em class="italics">g</em><sub>5</sub> (a traffic jam), as shown in the following screenshot:</p>
    <figure class="mediaobject"><img src="../Images/B15438_10_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.6: Traffic jam example</p>
    <p class="normal">The following output appears correctly:</p>
    <pre class="programlisting"><code class="hljs lua">directory dataset_traffic/
Strategy model <span class="hljs-built_in">loaded</span> from training repository.
image dataset_traffic/classify/img1.jpg predict_proba: <span class="hljs-string">[[ 0.]]</span> predict: <span class="hljs-string">[[ 0.]]</span>
Classified <span class="hljs-keyword">in</span> class A
jammed
</code></pre>
    <p class="normal"><em class="italics">g</em><sub>6</sub> comes out <a id="_idIndexMarker511"/>right as well, as shown in this screenshot:</p>
    <figure class="mediaobject"><img src="../Images/B15438_10_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.7: Traffic jam example</p>
    <p class="normal">A potential lane change has become possible, as detected by the following code:</p>
    <pre class="programlisting"><code class="hljs lua">Seeking...
image dataset_traffic/classify/img2.jpg predict_proba: <span class="hljs-string">[[ 1.]]</span> predict: <span class="hljs-string">[[ 1.]]</span>
Classified <span class="hljs-keyword">in</span> class B
change
</code></pre>
    <p class="normal">We have applied our CNN "gap" detection model to several types of images. We can now go deeper into the theory of conceptual datasets using "gaps" as an example.</p>
    <h2 id="_idParaDest-207" class="title">Gap datasets and subsets</h2>
    <p class="normal">At this point, the <img src="../Images/B15438_10_005.png" alt=""/> (the gap conceptual dataset) has begun <a id="_idIndexMarker512"/>to learn several subsets:</p>
    <figure class="mediaobject"><img src="../Images/B15438_10_019.png" alt=""/></figure>
    <p class="normal">In which:</p>
    <p class="center"><em class="italics">ng</em><sub>1</sub> = {missing, not enough, slowing production down … bad}</p>
    <p class="center"><em class="italics">pg</em><sub>2</sub> = <em class="italics">pg</em><sub>2</sub> = {good production flow, no gap}</p>
    <p class="center"><em class="italics">g</em><sub>2</sub> = {loaded}</p>
    <p class="center"><em class="italics">g</em><sub>3</sub> = {unloaded}</p>
    <p class="center"><em class="italics">pg</em><sub>4</sub> = {traffic jam, heavy traffic … too much traffic}</p>
    <p class="center"><em class="italics">ng</em><sub>5</sub> = {open lane, light traffic … normal traffic}</p>
    <p class="normal">Notice that <em class="italics">g</em><sub>2</sub> and <em class="italics">g</em><sub>3</sub> do not have labels yet. The food processing context provided the labels. Concept detection requires a context, which CRLMMs will provide.</p>
    <h3 id="_idParaDest-208" class="title">Generalizing the <img src="../Images/B15438_10_020.png" alt=""/> (the gap conceptual dataset)</h3>
    <p class="normal">The <a id="_idIndexMarker513"/>generalization of <img src="../Images/B15438_10_015.png" alt=""/> (the gap conceptual dataset) will provide a conceptual tool for metamodels.</p>
    <div class="note">
      <p class="Information-Box--PACKT-"><img src="../Images/B15438_10_022.png" alt=""/> (the gap conceptual dataset) refers to negative, positive, or undetermined space between two elements (objects, locations, or products on a production line).</p>
    </div>
    <p class="normal"><img src="../Images/B15438_10_023.png" alt=""/> (gamma) also refers to a gap in time: too long, not long enough, too short, or not short enough.</p>
    <p class="normal"><img src="../Images/B15438_10_023.png" alt=""/> represents the distance between two locations: too far or too close.</p>
    <p class="normal"><img src="../Images/B15438_10_025.png" alt=""/> can represent a <a id="_idIndexMarker514"/>misunderstanding or an understanding between two parties: a divergence of opinions or a convergence.</p>
    <p class="normal">All of these examples refer to gaps in space and time viewed as space.</p>
    <h2 id="_idParaDest-209" class="title">The motivation of conceptual representation learning metamodels applied to dimensionality</h2>
    <p class="normal">A CRLMM converts <a id="_idIndexMarker515"/>images into concepts. These abstract concepts will then be embedded in vectors that become logits for a softmax function, and in turn, will be converted into parameters for complex artificial intelligence programs' automatic scheduling, cognitive chatbots, and more.</p>
    <p class="normal">The advantage of a concept is that it can apply to many different areas. With just one concept, "gap" (a hole, empty space, and so on), you can describe hundreds if not thousands of cases.</p>
    <p class="normal">In some artificial intelligence projects, dimensionality reduction does not produce good results at all. When scheduling maintenance of airplanes, rockets, and satellite launchers, for example, thousands of features enter the system without leaving any out. A single missing screw in a rocket in the wrong place could cause a disaster. A single mistake in the engine of an airplane can cause an accident, a single component of a satellite can impair its precision.</p>
    <p class="normal">Dimensionality must be taken into account. Some use the expression "curse of dimensionality" and I often prefer the "blessing" of dimensionality. Let's have a look at both approaches.</p>
    <h3 id="_idParaDest-210" class="title">The curse of dimensionality</h3>
    <p class="normal">The number of<a id="_idIndexMarker516"/> features for a given project can reach large numbers. The following example contains 1,024 dimensions:</p>
    <figure class="mediaobject"><img src="../Images/B15438_10_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 10.8: The curse of dimensionality</p>
    <p class="normal">Each dot in the preceding representation represents a dimension that can be the features in an image for example.</p>
    <p class="normal">Asking a CNN to analyze thousands of features in an image, for example, might make it impossible to reach an accurate result. Since each layer is supposed to reduce the size of the data analyzed to extract important features, too many dimensions might make the training of the model impossible. Remember, each dimension can contain a feature that requires weights to be trained. If there are too many, the training becomes either too long or too difficult to calculate.</p>
    <p class="normal">Following standard CNN designs provides a good starting point. The limit of this approach occurs when the result does not meet expectations, such as in some of the cases we will look at in the upcoming chapters. In those cases, CRLMMs will increase the productivity of the solution, providing a useful abstract model.</p>
    <p class="normal">When a solution requires a large number of unreduced dimensions, kernels, pooling, and other dimension reduction methods cannot be applied, CRLMMs will provide a <em class="italics">pair of glasses</em> for the system. That's when the "blessing" of dimensionality comes in handy.</p>
    <h3 id="_idParaDest-211" class="title">The blessing of dimensionality</h3>
    <p class="normal">In some projects, when<a id="_idIndexMarker517"/> the model reaches limits that comprise a project, dimensionality is a blessing.</p>
    <p class="normal">Let's take an example of rocket manufacturing using our CNN model. We want to identify gaps on a surface. This surface contains tiles to protect the rocket from overheating when it goes through the atmosphere. A gap in those tiles could cause a fatal accident.</p>
    <p class="normal">If we take a few tiles out, take a picture and run it through our CNN model, it will <em class="italics">probably</em> detect a gap. The difference between that probability and an error could mean a critical failure of the rocket.</p>
    <p class="normal">This means that we might not want to reduce the number of features, which in turn need weights and add up to high numbers of dimensions.</p>
    <p class="normal">We could decide not to use pooling, which groups several dimensions into one as we saw. That could create calculations problems as we saw in the previous paragraph. Either there would be too many weights to calculate or the calculation could take too long.</p>
    <p class="normal">In that case, we could reduce the size of the frame to the smallest portion of the rocket component we are examining. We could decide that our camera will only scan the smallest surface possible at a time sending minimal-sized frames to our CNN.</p>
    <p class="normal">In that case, even with no pooling, the layers would contain more data, but the calculation would remain reasonable.</p>
    <p class="normal">The "blessing" of dimensionality, in this case, resides in the fact that by avoiding pooling (grouping), we are examining more details that can make our model much more reliable to detect small cracks since we would train it to see very small gaps.</p>
    <p class="normal">The curse of dimensionality usually leads to dimensionality reduction. But as we have just seen, it doesn't have to be so.</p>
    <h1 id="_idParaDest-212" class="title">Summary</h1>
    <p class="normal">In this chapter, the CNN architecture built in <em class="italics">Chapter 9</em>, <em class="italics">Abstract Image Classification with Convolutional Neural Networks (CNNs)</em>, was loaded to classify physical gaps in a food processing company. The model uses image concepts, taking CNNs to another level. Neural networks can tap into their huge cognitive potential, opening the doors to the future of AI.</p>
    <p class="normal">Then, the trained models were applied to transfer learning by identifying similar types of images. Some of those images represented concepts that led the trained CNN to identify <img src="../Images/B15438_10_026.png" alt=""/> concept gaps. Image concepts represent an avenue of innovative potential adding cognition to neural networks.</p>
    <p class="normal"><img src="../Images/B15438_10_006.png" alt=""/> concept gaps were applied to different fields using the CNN as a training and classification tool in domain learning.</p>
    <p class="normal"><img src="../Images/B15438_10_028.png" alt=""/> concept gaps have two main properties: negative n-gaps and positive p-gaps. To distinguish one from the other, a CRLMM provides a useful add-on. In the food processing company, installing a webcam on the right food processing conveyor belt provided a context for the system to decide whether a gap was positive or negative.</p>
    <p class="normal">With these concepts in mind, let's build a solution for <strong class="bold">advanced planning and scheduling</strong> (<strong class="bold">APS</strong>) in the next chapter.</p>
    <h1 id="_idParaDest-213" class="title">Questions</h1>
    <ol>
      <li class="list">The curse of dimensionality leads to reducing dimensions and features in machine learning algorithms. (Yes | No)</li>
      <li class="list">Transfer learning determines the profitability of a project. (Yes | No)</li>
      <li class="list">Reading <code class="Code-In-Text--PACKT-">model.h5</code> does not provide much information. (Yes | No)</li>
      <li class="list">Numbers without meaning are enough to replace humans. (Yes | No)</li>
      <li class="list">Chatbots prove that body language doesn't mean that much. (Yes | No)</li>
      <li class="list">Present-day ANNs provide enough theory to solve all AI requests. (Yes | No)</li>
      <li class="list">Chatbots can now replace humans in all situations. (Yes | No)</li>
      <li class="list">Self-driving cars have been approved and do not need conceptual training. (Yes | No)</li>
      <li class="list">Industries can implement AI algorithms for all of their needs. (Yes | No)</li>
    </ol>
    <h1 id="_idParaDest-214" class="title">Further reading</h1>
    <ul>
      <li class="list">More on Keras layers: <a href="https://keras.io/layers/about-keras-layers/"><span class="url">https://keras.io/layers/about-keras-layers/</span></a></li>
      <li class="list">More on concept learning: <a href="https://www.sciencedirect.com/topics/psychology/concept-learning"><span class="url">https://www.sciencedirect.com/topics/psychology/concept-learning</span></a></li>
      <li class="list">More on cognitive sciences, the brain, and the mind for conceptual models: <a href="http://catalog.mit.edu/schools/science/brain-cognitive-sciences/"><span class="url">http://catalog.mit.edu/schools/science/brain-cognitive-sciences/</span></a>, <a href="https://symsys.stanford.edu/undergraduatesconcentrations/cognitive-science-cogsci-concentration"><span class="url">https://symsys.stanford.edu/undergraduatesconcentrations/cognitive-science-cogsci-concentration</span></a></li>
    </ul>
  </div>
</body></html>