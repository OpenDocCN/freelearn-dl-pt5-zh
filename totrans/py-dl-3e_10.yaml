- en: '10'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Machine Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Operations (MLOps)
  prefs: []
  type: TYPE_NORMAL
- en: 'So far in this book, we have focused on the theory of **neural networks** (**NNs**),
    various NN architectures, and the tasks we can solve with them. This chapter is
    a little different because we’ll focus on some of the practical aspects of NN
    development. We’ll delve into this topic because the development and production
    deployment of ML models (and NNs in particular) have some unique challenges. We
    can split this process into three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training dataset creation**: Data collection, cleanup, storage, transformations,
    and feature engineering.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model development**: Experiment with different models and training algorithms
    and evaluate them.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Deployment**: Deploy trained models in the production environment and monitor
    their performance in computational and accuracy terms.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This multi-step complex pipeline presupposes some of the challenges when solving
    ML tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Diverse software toolkit**: Each step has multiple competing tools.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model development is hard**: Each training instance has a large number of
    variables. These could be modifications in the NN architecture, variations in
    the training hyperparameters (such as learning rate or momentum), or different
    training data distributions. On top of that, NNs have sources of randomness, such
    as weight initialization or data augmentation. Therefore, if we cannot reproduce
    earlier results, it won’t be easy to pinpoint the reason. Even if we have a bug
    in the code, it might not result in an easily detectable runtime exception. Instead,
    it might just deteriorate the model accuracy slightly. So that we don’t lose track
    of all the experiments, we need a robust tracking and monitoring system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Complex deployment and monitoring**: NNs require GPUs and batch-organized
    data for optimal performance. These requirements might collide with the real-world
    requirements of processing data in streams or sample-wise. In addition, the nature
    of the user data might change with time, which could cause **model drift**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring model deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ll implement the examples in this chapter using Python, PyTorch, **TensorFlow**
    (**TF**), and **Hugging Face** (**HF**) Transformers, among others. If you don’t
    have an environment set up with these tools, fret not – the examples are available
    as Jupyter Notebooks on Google Colab. You can find the code examples in this book’s
    GitHub repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter10](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter10).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding model development
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll discuss various tools that will help us manage the model
    development phase of the ML solution life cycle. Let’s start with the most important
    question – which NN framework should we choose?
  prefs: []
  type: TYPE_NORMAL
- en: Choosing an NN framework
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far in this book, we’ve mostly used PyTorch and TensorFlow. We can refer
    to them as **foundational** frameworks as these are the most important components
    of the entire NN software stack. They serve as a base for other components in
    the ML NN ecosystem, such as Keras or HF Transformers, which can use either of
    them as a backend (multi-backend support will come with Keras 3.0). In addition
    to TF, Google has also released JAX ([https://github.com/google/jax](https://github.com/google/jax)),
    a foundational library that supports GPU-accelerated NumPy operations and Autograd.
    Other popular libraries such as NumPy, pandas, and scikit-learn ([https://scikit-learn.org](https://scikit-learn.org))
    go beyond the scope of this book as they are not strictly related to NNs. Because
    of the importance of foundational libraries, they are the first and most important
    choice in our toolkit. But which one should we choose if we start a project from
    scratch?
  prefs: []
  type: TYPE_NORMAL
- en: PyTorch versus TensorFlow versus JAX
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s check the level of community adoption for these libraries. Our first stop
    is **Papers with Code** ([https://paperswithcode.com/](https://paperswithcode.com/)),
    which indexes ML papers, code, datasets, and results. The site also maintains
    the trend of paper implementations grouped by framework ([https://paperswithcode.com/trends](https://paperswithcode.com/trends)).
    As of September 2023, 57% of the new papers are using PyTorch. TF and JAX are
    distant second and third with 3% and 2%, respectively. This trend isn’t new –
    PyTorch was released in 2016, but it has already surpassed TF in 2019\. This particular
    data point indicates that PyTorch dominates cutting-edge research, which is what
    the most recent papers are. Therefore, if you want to always use the latest and
    greatest in the field, it’s a good idea to stick to PyTorch. Next, let’s look
    at the ML models, hosted on the HF platform ([https://huggingface.co/models](https://huggingface.co/models)),
    where we can also filter by project framework. Out of ~335,000 total models hosted,
    ~131,000 use PyTorch, ~10,000 use TF, and ~9,000 use JAX. Again, this is a strong
    result in favor of PyTorch. However, this is not the full picture, as these results
    are for public and open source projects. They are not necessarily indicative of
    what companies use in production. More representative of this could be PyPI Stats
    ([https://pypistats.org/](https://pypistats.org/)), which provides aggregate download
    information on Python packages available from the **Python Package Index** (**PyPi**,
    [https://pypi.org/](https://pypi.org/)). The picture here is a bit more nuanced
    – PyTorch has 11,348,753 downloads for the last month (August-September 2023)
    versus 16,253,288 for TF and 3,041,747 for JAX. However, we should be cautious
    with PyPi Stats because many automated processes (such as continuous integration)
    can inflate the PyPI download count, without indicating real-world use. In addition,
    the PyTorch download page advises installing the library through Conda ([https://conda.io/](https://conda.io/)).
    The monthly statistics show 759,291 PyTorch downloads versus 154,504 for TF and
    6,260 for JAX. Therefore, PyTorch leads here as well. Overall, my conclusion is
    that PyTorch is more popular than TF, but both libraries are used in production
    environments.
  prefs: []
  type: TYPE_NORMAL
- en: My advice, which you can take with as many pinches of salt as you wish, would
    be to select PyTorch if you start a project now. This is why this book has put
    more emphasis on PyTorch compared to TF. One exception to this rule is if your
    project runs on mobile or edge devices ([https://en.wikipedia.org/wiki/Edge_device](https://en.wikipedia.org/wiki/Edge_device))
    with limited computational power. TF has better support for such devices through
    the TF Lite library ([https://www.tensorflow.org/lite](https://www.tensorflow.org/lite)).
  prefs: []
  type: TYPE_NORMAL
- en: But ultimately, you can work with your preferred software stack and then convert
    your models into other libraries for deployment. We’ll see how this is possible
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Open Neural Network Exchange
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Open Neural Network Exchange** (**ONNX**, [https://onnx.ai/](https://onnx.ai/))
    provides an open source format for NN-based and traditional ML models (we’ll focus
    on NNs here). It defines an extensible computation graph model, built-in operators,
    and standard data types. In other words, ONNX provides a universal NN representation
    format, which allows us to convert models implemented with one library (for example,
    PyTorch) into others (such as TF), provided that both the source and target libraries
    support ONNX. In this way, you can train your model with one library and then
    convert it into another when deploying to production. This also makes sense because
    ONNX focuses on inference mode and not training (representing the training process
    using ONNX in experimental mode).'
  prefs: []
  type: TYPE_NORMAL
- en: 'ONNX represents an NN as a computational `onnx` (`!pip install onnx`) Python
    package. Let’s start:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Define the graph representation’s input (`X`, `A`, `B`) and output (`Y`) variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `make_tensor_value_info` declares a named graph I/O variables (`X` and
    `Y`) with a type (`elem_type`) and `shape`. `shape=[None]` means any shape, and
    `shape=[None, None]` means a two-dimensional tensor without specific dimension
    sizes. On the other hand, `A` and `B` are the function parameters (weights), and
    we initialize them with pre-defined values from NumPy arrays.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the graph operations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`mat_mul` represents matrix multiplication (`MatMul`) between the `X` and `A`
    input matrices into the `XA` output variable. `addition` sums the output of `mat_mul`,
    `XA`, with the bias, `B`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: ONNX operators
  prefs: []
  type: TYPE_NORMAL
- en: This example introduces the `MatMul` and `Add` ONNX operators. The full list
    of supported operators (available at [https://onnx.ai/onnx/operators/](https://onnx.ai/onnx/operators/))
    includes many other NN building blocks, such as activation functions, convolutions,
    pooling, and tensor operators (for example, `concat`, `pad`, `reshape`, and `flatten`).
    In addition, it supports the so-called `if` operator executes one subgraph or
    another, depending on a Boolean value. ONNX itself doesn’t implement the operators.
    Instead, the libraries that support it (such as PyTorch) have their own implementations.
    Conversely, the ONNX conversion will fail if your library model has operators
    that aren’t supported by ONNX.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now have the ingredients to define the computational `graph`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can see our computational graph in the following figure:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.1 – Linear regression ONNX computational graph](img/B19627_10_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.1 – Linear regression ONNX computational graph
  prefs: []
  type: TYPE_NORMAL
- en: 'Use `graph` to create an `onnx_model` instance. The model allows you to add
    additional metadata to the graph, such as docstring, version, author, and license,
    among others:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Check the model for consistency. This verifies that the input type or shapes
    match between the model components:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we can compute the output of the model for two random input samples
    with an instance of `ReferenceEvaluator`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The result of the computation is a NumPy array:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'ONNX allows us to serialize and deserialize both the model structure and its
    weights with **Protocol Buffers** (**protobuf**, [https://protobuf.dev/](https://protobuf.dev/)).
    Here’s how to do this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that we have introduced ONNX, let’s see how we can use it in practice by
    exporting PyTorch and TF models to ONNX.
  prefs: []
  type: TYPE_NORMAL
- en: 'In addition to `torch` and `tensorflow`, we’ll also need the `torchvision`,
    `onnx`, and `tf2onnx` ([https://github.com/onnx/tensorflow-onnx](https://github.com/onnx/tensorflow-onnx),
    `!pip install tf2onnx`) packages. Let’s start with PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load a pre-trained model (`MobileNetV3`, refer to [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146)):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, export the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Most parameters speak for themselves. `args=torch.randn(1, 3, 224, 224)` specifies
    a dummy tensor. This is necessary because the serializer might invoke the model
    once to infer the graph structure and tensor sizes. The dummy tensor will serve
    as input for this invocation. However, this exposes one of the limitations of
    the conversion process: if the model includes a dynamic computational graph, the
    converter will only convert the path of the current invocation. `export_params`
    tells the exporter to include the model weights, besides the model structure.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use ONNX to load the exported model and check it for consistency (spoiler:
    it works):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s do the same but with TF. Unlike PyTorch, TF doesn’t have out-of-the-box
    ONNX serialization support. Instead, we’ll use the `tf2onnx` package:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Load a pre-trained `MobileNetV3` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Serialize the model using `tf2onnx`. It follows the same principle as PyTorch,
    down to the dummy input tensor (`input_signature`), which is necessary for model
    invocation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once again, we can load the model with ONNX to verify its consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we can use `torch_model.onnx` or `tf_model.onnx`). This is a graphical
    viewer for NNs and other ML models. It exists as a web **user interface** (**UI**)
    or a standalone app. It supports ONNX, TensorFlow Lite, and PyTorch (experimental),
    among other libraries. For example, the following figure shows the initial **MobileNetV3**
    layers in detail, as visualized by Netron (the full model visualization is too
    large to display within this chapter):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – Netron visualization of the MobileNetV3 ONNX model file](img/B19627_10_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.2 – Netron visualization of the MobileNetV3 ONNX model file
  prefs: []
  type: TYPE_NORMAL
- en: Here, the input shape is 3×224×224, **W** is the shape of the convolutional
    filter, and **B** is the bias. We introduced the rest of the convolution attributes
    in [*Chapter 4*](B19627_04.xhtml#_idTextAnchor107).
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, neither PyTorch nor TF comes with the integrated ability to load
    ONNX models. However, there are open source packages that allow us to do this.
    Two of them are `onnx2torch` ([https://github.com/ENOT-AutoDL/onnx2torch](https://github.com/ENOT-AutoDL/onnx2torch))
    for PyTorch and `onnx2tf` ([https://github.com/PINTO0309/onnx2tf](https://github.com/PINTO0309/onnx2tf))
    for TF.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll focus on a tool that will ease the training process.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing TensorBoard
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**TensorBoard** (**TB**, [https://www.tensorflow.org/tensorboard/](https://www.tensorflow.org/tensorboard/),
    [https://github.com/tensorflow/tensorboard](https://github.com/tensorflow/tensorboard))
    is a TF-complement web-based tool that provides visualization and tooling for
    machine learning experiments. Some of its functions are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Metrics (such as loss and accuracy) tracking and visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Model graph visualization (similar to Netron)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A time series histogram of the change of weights, biases, or other tensors over
    time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Low-dimensional embedding projections
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TB can work with both TF/Keras and PyTorch, but it has better integration with
    TF (after all, it is developed by the TF team). In both cases, TB doesn’t communicate
    directly with the models during training. Instead, the training process stores
    its state and current progress in a special log file. TB tracks the file for changes
    and automatically updates its graphical interface with the latest information.
    In this way, it can visualize the training as it progresses. In addition, the
    file stores the entire training history to be displayed even after it finishes.
    To better understand how it works, we’ll add TB to the transfer learning computer
    vision examples that we introduced in [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146).
    As a quick recap, we’ll start with ImageNet pre-trained MobileNetV3 models. Then,
    we’ll use two transfer learning techniques, **feature engineering** and **fine-tuning**,
    to train these models to classify the CIFAR-10 dataset. TB will visualize the
    training.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the Keras example. We’ll only include the relevant part of
    the code, and not the full example, as we discussed it in [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146).
    More specifically, we’ll focus on the `train_model(model, epochs=5)` function,
    which takes the pre-trained `model` and the number of training `epochs` as parameters.
    The following is the function’s body (please note that the actual implementation
    has indentation):'
  prefs: []
  type: TYPE_NORMAL
- en: Initializing TensorBoard
  prefs: []
  type: TYPE_NORMAL
- en: This example assumes that TB is initialized and running (although the code works
    even if it is not available). We won’t include the initialization of TB because
    it differs depending on the environment. However, it is included in the Jupyter
    Notebook of this example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll configure the training of the pre-trained Keras model with the
    Adam optimizer, binary cross-entropy loss, and accuracy tracking:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we’ll add the special `tensorboard_callback`, which implements the TB
    connection:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The callback parameters are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`log_dir`: This instructs `tensorboard_callback` to write the log file in a
    unique time-stamped folder, `''logs/tb/'' + datetime.datetime.now().strftime(''%Y%m%d-%H%M%S'')`,
    located in the main `''logs/tb/''` folder. TB will simultaneously pick all training
    folders under `''logs/tb/''` and display them in its UI as unique training instances.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`update_freq=1`: Updates the log file once per epoch.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`histogram_freq=1`: Computes weight histograms once per epoch.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`write_graph=True`: Generates a graph visualization of the NN architecture.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`write_images=True`: Visualizes the model weights as an image.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`write_steps_per_second=True`: Logs the training steps per second.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`profile_batch=1`: Profiles the first batch to sample its compute characteristics.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Embeddings_freq=0`: The frequency (in epochs) at which embedding layers will
    be visualized (we don’t have embedding layers, so it’s disabled by default).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, we’ll run the training with the `model.fit` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We add `tensorboard_callback` to the list of `model` `callbacks`. The training
    process notifies each callback for various training events: start of training,
    end of training, start of testing, end of testing, start of epoch, end of epoch,
    start of batch, and end of batch. In turn, `tensorboard_callback` updates the
    log file according to its configuration and the current event.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The TB UI displays all the information in the log file. Although it’s too complex
    to include here, we can still show a snippet with accuracy:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 10.3 – Accuracy in the TB UI](img/B19627_10_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 10.3 – Accuracy in the TB UI
  prefs: []
  type: TYPE_NORMAL
- en: Here, TB displays the accuracy for four different experiments – train/test for
    feature engineering and train/test for fine-tuning.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s see how PyTorch integrates with TB. It provides a special `torch.utils.tensorboard.SummaryWriter`
    class, which writes entries directly to event log files in the `log_dir` folder
    to be consumed by TB. It follows the same principle as in Keras. The high-level
    API of `SummaryWriter` allows us to create an event file in `log_dir` and asynchronously
    add content to it. The main difference with Keras is that we’re responsible for
    adding the content, instead of an automated event listener doing it. Let’s see
    how that works in practice. As with Keras, we’ll use the computer vision transfer
    learning example from [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146). We’ll only
    focus on the relevant parts, but you can see the full example in the Jupyter Notebook
    in this book’s GitHub repository.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll initialize two `SummaryWriter` instances for the feature extractor
    fine-tuning modes. It doesn’t matter where we do it, so long as it happens before
    we start using them. As with Keras, each training instance has a unique time-stamped
    folder under `''logs/tb/''` (we’re only showing one initialization because they
    are identical):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'For the sake of clarity, we’ll include the initialization of the MobileNetV3
    pre-trained model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we’ll jump to the training (or testing) loop, where `train_loader`, an
    instance of `torch.utils.data.DataLoader`, yields pairs of `inputs` and `labels`
    mini-batches:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Within the loop, we can add the model graph to the log file. It takes the model
    and the input tensor as parameters to generate the visualization (hence the need
    to call `add_graph` in the training loop):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, at the end of the training loop, we’ll add the loss and the accuracy
    for the current `epoch` as scalar values:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: Each scalar value has a unique `tag` (besides the two tags in the code, we also
    have `tag='validation/loss'`). Note that `global_step` (equal to the epoch) stores
    `scalar_value` as a sequence within the same `tag`. In addition to graphs and
    scalars, `SummaryWriter` can add images, tensors, histograms, and embeddings,
    among others.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our introduction to TB. Next, we’ll learn how to develop NN models
    for edge devices.
  prefs: []
  type: TYPE_NORMAL
- en: Developing NN models for edge devices with TF Lite
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TF Lite is a TF-derived set of tools that allows us to run models on mobile,
    embedded, and edge devices. Its versatility is part of TF’s appeal for industrial
    applications (as opposed to research applications, where PyTorch dominates). The
    key paradigm of TF Lite is that the models run on-device, contrary to client-server
    architecture, where the model is deployed on remote, more powerful, hardware.
    This organization has the following implications (both good and bad):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low-latency execution**: The lack of server-round trip significantly reduces
    the model inference time and allows us to run real-time applications.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Privacy**: The user data never leaves the device.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Internet connectivity**: Internet connectivity is not required.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.tflite` file extension. Besides its small size, it allows us to access data
    directly without parsing/unpacking it first.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'TF Lite models support a subset of the TF Core operations and allow us to define
    custom ones:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Low power consumption**: The devices often run on battery.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Divergent training and inference**: NN training is a lot more computationally
    intensive compared to inference. Because of this, the model training runs on a
    different, more powerful, piece of hardware than the actual devices, where the
    models will run inference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In addition, TF Lite has the following key features:'
  prefs: []
  type: TYPE_NORMAL
- en: Multi-platform and multi-language support, including Android (Java), iOS (Objective-C
    and Swift) devices, web (JavaScript), and Python for all other environments. Google
    provides a TF Lite wrapper API called **MediaPipe Solutions** ([https://developers.google.com/mediapipe](https://developers.google.com/mediapipe),
    [https://github.com/google/mediapipe/](https://github.com/google/mediapipe/)),
    which supersedes the previous TF Lite API.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimized for performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It has end-to-end solution pipelines. TF Lite is oriented toward practical applications,
    rather than research. Because of this, it includes different pipelines for common
    ML tasks such as image classification, object detection, text classification,
    and question answering among others. The computer vision pipelines use modified
    versions of EfficientNet or MobileNet ([*Chapter 4*](B19627_04.xhtml#_idTextAnchor107)),
    and the natural language processing pipelines use BERT-based ([*Chapter* *7*](B19627_07.xhtml#_idTextAnchor202))
    models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, how does TF Lite model development work? First, we’ll select a model in
    one of the following ways:'
  prefs: []
  type: TYPE_NORMAL
- en: An existing pre-trained `.tflite` model ([https://tfhub.dev/s?deployment-format=lite](https://tfhub.dev/s?deployment-format=lite)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `.tflite` model with a custom training dataset. Model Maker only works with
    Python.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convert a full-fledged TF model into `.``tflite` format.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TFLite model metadata
  prefs: []
  type: TYPE_NORMAL
- en: 'The `.tflite` models may include optional metadata with three components:'
  prefs: []
  type: TYPE_NORMAL
- en: '-- **Human-readable part**: Provides additional information for the model.'
  prefs: []
  type: TYPE_NORMAL
- en: '-- **Input information**: Describes the input data format and the necessary
    pre-processing steps'
  prefs: []
  type: TYPE_NORMAL
- en: '-- **Output information**: Describes the output data format and the necessary
    post-processing steps.'
  prefs: []
  type: TYPE_NORMAL
- en: The last two parts can be leveraged by code generators (for example, Android
    code generator) to create ready-to-use model wrappers in the target platform.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s see how to use Model Maker to train a `.tflite` model and then
    use it to classify images. We’re only going to show relevant parts of the code,
    but the full example is available as a Jupyter Notebook in this book’s GitHub
    repository. Let’s start:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll create training and validation datasets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `dataset_path` is a path to the Flowers dataset ([https://www.tensorflow.org/datasets/catalog/tf_flowers](https://www.tensorflow.org/datasets/catalog/tf_flowers)),
    which contains 3,670 RGB low-resolution images of flowers, distributed in five
    classes (one subfolder per class). `data.split(0.9)` splits the dataset (instances
    of `image_classifier.Dataset`) into `train_data` (90% of the images) and `validation_data`
    (10% of the images) parts.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we’ll define the training hyperparameters – train for three epochs with
    a mini-batch size of 16 and export the trained model in the `export_dir` folder
    (other parameters are available as well):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we’ll define the model parameters (we’ll use `EfficientNet`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we’ll create a new model and we’ll run the training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This model achieves around 92% accuracy in three epochs. The training process
    creates a TB-compatible log file, so we’ll be able to track the progress with
    TB (available in the Jupyter Notebook).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we’ll export the model in `.tflite` format for the next phase of our
    example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have a trained model, we can use it to classify images. We’re going
    to use the `MediaPipe` Python API (which is different than Model Maker):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `classifier` is the pre-trained model, `generic_options` contains the
    file path to the `.tflite` model, and `cls_options` contains classification-specific
    options (we use the default values).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We’ll load five random flower images (one for each flower class, listed in
    `labels`) in a list called `image_paths` (not displayed here). We’ll classify
    each image, and we’ll compare its predicted label to the real one:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Predictably, the model classifies all images correctly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Next, we’ll learn how to optimize the training with mixed-precision computations.
  prefs: []
  type: TYPE_NORMAL
- en: Mixed-precision training with PyTorch
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We discussed mixed-precision training in the context of LLMs in [*Chapter 8*](B19627_08.xhtml#_idTextAnchor220).
    In this section, we’ll see how to use it in practice with PyTorch. Once again,
    we’ll use the transfer learning PyTorch example from [*Chapter 5*](B19627_05.xhtml#_idTextAnchor146)
    as a base for our implementation. All the code modifications are concentrated
    in the `train_model` function. We’ll only include `train_model` here, but the
    full example is available as a Jupyter Notebook in this book’s GitHub repository.
    The following is a shortened version of the function definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'We use a combination of two separate and unrelated mechanisms for mixed-precision
    training:'
  prefs: []
  type: TYPE_NORMAL
- en: '`torch.autocast`: This acts as a context manager (or decorator) and allows
    a region of the code to run in mixed precision. `device_type` specifies the device
    that `autocast` applies to. `dtype` specifies the data type with which the CUDA
    operations work. The PyTorch documentation suggests only wrapping the forward
    and loss computation with `torch.autocast`. The backward operations automatically
    run with the same data type as the forward ones.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`torch.cuda.amp.GradScaler`: When the forward pass uses `float16` precision
    operations, so does the backward pass, which computes the gradients. However,
    due to the lower precision, some gradient values will flush to zero. To prevent
    this, **gradient scaling** multiplies the NN’s loss by a scale factor and invokes
    a backward pass with the scaled value. Gradients flowing backward through the
    network are also scaled by the same factor. In this way, the entire backward pass
    uses a larger magnitude to prevent flushing to zero. Before the weight updates,
    the mechanism *unscales* the scaled gradient values, so the weight updates work
    with the actual values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes our introduction to the model development tools. Next, we’ll
    discuss some model deployment mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring model deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll discuss two basic model deployment examples. They’ll
    help you create simple, yet functional, proof-of-concept apps for your experiments.
    Let’s start.
  prefs: []
  type: TYPE_NORMAL
- en: Deploying NN models with Flask
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our first example, we’ll use Google Colab in combination with `prompt` parameter,
    generate an image with it, and return the image as a result.
  prefs: []
  type: TYPE_NORMAL
- en: According to its home page, Flask is a lightweight `localhost`), we won’t be
    able to access it. To solve this, we’ll need `flask-ngrok` ([https://ngrok.com/docs/using-ngrok-with/flask/](https://ngrok.com/docs/using-ngrok-with/flask/)),
    which will expose the server to the outside world (you’ll need a free `ngrok`
    registration and authentication token to run this example).
  prefs: []
  type: TYPE_NORMAL
- en: 'To satisfy all dependencies, we’ll need the `transformers`, `diffusers`, `accelerate`,
    and `flask-ngrok` packages. Let’s start:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll initialize the SD HF pipeline (`sd_pipe`) in the same way as we
    did in [*Chapter 9*](B19627_09.xhtml#_idTextAnchor236):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we’ll initialize our Flask `app`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here, `run_with_ngrok` indicates that the app will run with `ngrok`, but the
    actual `app` is not running yet (this will come at the end of this example). Since
    we don’t have access to Colab’s `localhost`, `ngrok` will make it possible to
    access it from our test client.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we’ll implement our `text-to-image` endpoint, which will process the
    prompts, which are coming in as web requests, and generate images based on them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The endpoint’s name is `/text-to-image` and it will process both `POST` and
    `GET` requests (the processing pipeline is the same). The function will parse
    the textual `prompt` parameter and it will feed it to `sd_pipe` to generate an
    `image` parameter (in the same way as in the [*Chapter 9*](B19627_09.xhtml#_idTextAnchor236)
    example). Finally, the `send_file` function will return the result of `image`
    to the client.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We can now start the Flask app with the `app.run()` command. It will initialize
    the Flask development server so that our endpoint will be ready to process requests.
    In addition, `ngrok` will expose the app to the outside world with a URL of the
    [http://RANDOM-SEQUENCE.ngrok.io/](http://RANDOM-SEQUENCE.ngrok.io/) type.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can use this URL to initiate a test request to the `text-to-image` endpoint
    (this is outside the Colab notebook):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can display the image with the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This concludes our REST API example. Next, we’ll deploy a model in a web environment
    with a UI.
  prefs: []
  type: TYPE_NORMAL
- en: Building ML web apps with Gradio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Gradio ([https://www.gradio.app/](https://www.gradio.app/)) is an open source
    Python library that allows us to build interactive web-based demos for our ML
    models. HF Spaces ([https://huggingface.co/spaces](https://huggingface.co/spaces))
    supports hosting Gradio apps. So, we can build a Gradio app on top of the HF infrastructure,
    which includes not only hosting but also has access to all available HF models
    ([https://huggingface.co/models](https://huggingface.co/models)).
  prefs: []
  type: TYPE_NORMAL
- en: We can create an HF space at [https://huggingface.co/new-space](https://huggingface.co/new-space).
    The space has a name (which will be its URL as well), a license, and an SDK. At
    the time of writing, HF Spaces supports Streamlit-based ([https://streamlit.io/](https://streamlit.io/)),
    Gradio-based, and Static instances. However, you can also deploy custom Docker
    containers for more flexibility.
  prefs: []
  type: TYPE_NORMAL
- en: Each new HF space has an associated Git repository. For example, the space of
    this example is located at [https://huggingface.co/spaces/ivan-vasilev/gradio-demo](https://huggingface.co/spaces/ivan-vasilev/gradio-demo),
    which is also the URL of its corresponding Git repository. The Gradio-based space
    expects a Python module called `app.py` in its root (in our case, the whole example
    will reside in `app.py`) and a `requirements.txt` file. Every time you push changes
    to the repository, the app will automatically pick them up and restart itself.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To replicate this example, you’ll need an HF account. HF Spaces has different
    hardware tiers. The basic one is free, but this particular example requires the
    GPU-enabled tier, which has an hourly cost. Therefore, if you want to run this
    example, you can duplicate it in your own account and enable the GPU tier.
  prefs: []
  type: TYPE_NORMAL
- en: 'Gradio starts with a central high-level class called `gradio.Interface`. Its
    constructor takes three main parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fn`: The main function, which will process the inputs and return outputs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`inputs`: One or more Gradio input components. These could be textual inputs,
    file uploads, or combo boxes, among others. You can specify the component as a
    class instance or via its string label. The number of inputs should match the
    number of `fn` parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`outputs`: One or more Gradio components, which will represent the result of
    the execution of `fn`. The number of outputs should match the number of values
    returned by `fn`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradio will automatically instantiate and arrange the UI components based on
    the `input` and `output` parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we’ll implement our example. We’ll use the same text-to-image SD scenario
    that we used in the *Deploying NN models with Flask* section. To avoid duplication,
    we’ll assume that the `sd_pipe` pipeline has already been initialized. Let’s start:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we’ll implement the `generate_image` function, which uses `prompt` to
    synthesize an image in a total of `inf_steps` steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we’ll initialize the `gradio.Interface` class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we discussed, the `inputs` and `outputs` `gr.Interface` parameters match
    the input/output signature of the `generate_image` function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Finally, we can run the app with the `interface.launch()` command. Here is
    what the responsive UI of the app looks like:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 10.4 – The SD Gradio app’s responsive UI, hosted on HF Spaces. Top:
    input components; bottom: generated image](img/B19627_10_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 10.4 – The SD Gradio app’s responsive UI, hosted on HF Spaces. Top:
    input components; bottom: generated image'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our introduction to Gradio and model deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we outlined three major components of the ML development life
    cycle – training dataset creation, model development, and model deployment. We
    focused on the latter two, starting with development. First, we discussed the
    popularity of the foundational NN frameworks. Then, we focused on several model
    development topics – the ONNX universal model representation format, the TB monitoring
    platform, the TF Lite mobile development library, and mixed precision PyTorch
    training. Next, we discussed two basic scenarios for model deployment – a REST
    service as a Flask app and an interactive web app with Gradio.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes this chapter and this book. I hope you’ve enjoyed the journey!
  prefs: []
  type: TYPE_NORMAL
