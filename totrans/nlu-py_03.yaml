- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Approaches to Natural Language Understanding – Rule-Based Systems, Machine Learning,
    and Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter will review the most common approaches to **natural language understanding**
    (**NLU**) and discuss both the benefits and drawbacks of each approach, including
    rule-based techniques, statistical techniques, and deep learning. It will also
    discuss popular pre-trained models such as **Bidirectional Encoder Representations
    from Transformers** (**BERT**) and its variants. We will learn that NLU is not
    a single technology; it includes a range of techniques, which are applicable to
    different goals.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Traditional machine-learning approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning approaches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-trained models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Considerations for selecting technologies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s begin!
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The basic idea behind **rule-based approaches** is that language obeys rules
    about how words are related to their meanings. For example, when we learn foreign
    languages, we typically learn specific rules about what words mean, how they’re
    ordered in sentences, and how prefixes and suffixes change the meanings of words.
    The rule-based approach to NLU operates on the premise that these kinds of rules
    can be provided to an NLU system so that the system can determine the meanings
    of sentences in the same way that a person does.
  prefs: []
  type: TYPE_NORMAL
- en: The rule-based approach was widely used in NLU from the mid-1950s through the
    mid-1990s until machine-learning-based approaches became popular. However, there
    are still NLU problems where rule-based approaches are useful, either on their
    own or when combined with other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: We will begin by reviewing the rules and data that are relevant to various aspects
    of language.
  prefs: []
  type: TYPE_NORMAL
- en: Words and lexicons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Nearly everyone is familiar with the idea of words, which are usually defined
    as units of language that can be spoken individually. As we saw in [*Chapter 1*](B19005_01.xhtml#_idTextAnchor016),
    in most, but not all, languages, words are separated by white space. The set of
    words in a language is referred to as the language’s **lexicon**. The idea of
    a lexicon corresponds to what we think of as a dictionary – a list of the words
    in a language. A computational lexicon also includes other information about each
    word. In particular, it includes its part or parts of speech. Depending on the
    language, it could also include information on whether the word has irregular
    forms (such as, for the irregular English verb “*eat*,” the past tense “*ate*”
    and the past participle “*eaten*” are irregular). Some lexicons also include semantic
    information such as words that are related in meaning to each word.
  prefs: []
  type: TYPE_NORMAL
- en: Part-of-speech tagging
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Traditional parts of speech as taught in schools include categories such as
    “*noun*”, “*verb*”, “*adjective*”, *preposition*, and so on. The parts of speech
    used in computational lexicons are usually more detailed than these since they
    need to express more specific information than what could be captured by traditional
    categories. For example, the traditional *verb* category in English is usually
    broken down into several different parts of speech corresponding to the different
    forms of the verb, such as the past tense and past participle forms. A commonly
    used set of parts of speech for English is the parts of speech from the Penn Treebank
    ([https://catalog.ldc.upenn.edu/LDC99T42](https://catalog.ldc.upenn.edu/LDC99T42)).
    Different languages will have different parts of speech categories in their computational
    lexicons.
  prefs: []
  type: TYPE_NORMAL
- en: A very useful task in processing natural language is to assign parts of speech
    to the words in a text. This is called **part-of-speech tagging** (**POS tagging**).
    *Table 3.1* shows an example of the Penn Treebank part-of-speech tags for the
    sentence “*We would like to book a flight from Boston* *to London*:”
  prefs: []
  type: TYPE_NORMAL
- en: '| **Word** | **Part** **of speech** | **Meaning of part of** **speech label**
    |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| we | PRP | Personal pronoun |'
  prefs: []
  type: TYPE_TB
- en: '| would | MD | Modal verb |'
  prefs: []
  type: TYPE_TB
- en: '| like | VB | Verb, base form |'
  prefs: []
  type: TYPE_TB
- en: '| to | TO | To (this word has its own part of speech) |'
  prefs: []
  type: TYPE_TB
- en: '| book | VB | Verb, base form |'
  prefs: []
  type: TYPE_TB
- en: '| a | DT | Determiner (article) |'
  prefs: []
  type: TYPE_TB
- en: '| flight | NN | Singular noun |'
  prefs: []
  type: TYPE_TB
- en: '| from | IN | Preposition |'
  prefs: []
  type: TYPE_TB
- en: '| Boston | NNP | Proper noun |'
  prefs: []
  type: TYPE_TB
- en: '| to | TO | To |'
  prefs: []
  type: TYPE_TB
- en: '| London | NNP | Proper noun |'
  prefs: []
  type: TYPE_TB
- en: Table 3.1 – Part-of-speech tags for “We would like to book a flight from Boston
    to London”
  prefs: []
  type: TYPE_NORMAL
- en: POS tagging is not just a matter of looking words up in a dictionary and labeling
    them with their parts of speech because many words have more than one part of
    speech. In our example, one of these is “*book*,” which is used as a verb in the
    example but is also commonly used as a noun. POS tagging algorithms have to not
    only look at the word itself but also consider its context in order to determine
    the correct part of speech. In this example, “*book*” follows “*to*,” which often
    indicates that the next word is a verb.
  prefs: []
  type: TYPE_NORMAL
- en: Grammar
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Grammar rules are the rules that describe how words are ordered in sentences
    so that the sentences can be understood and also so that they can correctly convey
    the author’s meaning. They can be written in the form of rules describing part-whole
    relationships between sentences and their components. For example, a common grammar
    rule for English says that a sentence consists of a noun phrase followed by a
    verb phrase. A full computational grammar for any natural language usually consists
    of hundreds of rules and is very complex. It isn’t very common now to build grammar
    from scratch; rather, grammar is already included in commonly used Python NLP
    libraries such as **natural language toolkit** (**NLTK**) and **spaCy**.
  prefs: []
  type: TYPE_NORMAL
- en: Parsing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finding the relationships between parts of a sentence is known as **parsing**.
    This involves applying the grammar rules to a specific sentence to show how the
    parts of the sentence are related to each other. *Figure 3**.1* shows the parsing
    of the sentence “*We would like to book a flight*.” In this style of parsing,
    known as **dependency parsing**, the relationships between the words are shown
    as arcs between the words. For example, the fact that “*we*” is the subject of
    the verb “*like*” is shown by an arc labeled **nsubj** connecting “*like*” to
    “*we*”.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Parsing for “We would like to book a flight”](img/B19005_03_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Parsing for “We would like to book a flight”
  prefs: []
  type: TYPE_NORMAL
- en: At this point, it isn’t necessary to worry about the details of parsing – we
    will discuss it in more detail in [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159).
  prefs: []
  type: TYPE_NORMAL
- en: Semantic analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Parsing involves determining how words are structurally related to each other
    in sentences, but it doesn’t say anything about their meanings or how their meanings
    are related. This kind of processing is done through **semantic analysis**. There
    are many approaches to semantic analysis—this is an active research field—but
    one way to think of semantic analysis is that it starts with the main verb of
    the sentence and looks at the relationships between the verb and other parts of
    the sentence, such as the subject, direct object, and related prepositional phrases.
    For example, the subject of “*like*” in *Figure 3**.1* is “*We*.” “*We*” could
    be described as the “*experiencer*” of “*like*” since it is described as experiencing
    “*liking.*” Similarly, the thing that is liked, “*to book a flight*,” could be
    described as the “*patient*” of “*like.*” Semantic analysis is most frequently
    done through the application of rules, but it can also be done with machine learning
    techniques, as described in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: Finding semantic relationships between the concepts denoted by words, independently
    of their roles in sentences, can also be useful. For example, we can think of
    a “*dog*” as a kind of “*animal*,” or we can think of “*eating*” as a kind of
    action. One helpful resource for finding these kinds of relationships is Wordnet
    ([https://wordnet.princeton.edu/](https://wordnet.princeton.edu/)), which is a
    large, manually prepared database describing the relationships between thousands
    of English words. *Figure 3**.2* shows part of the Wordnet information for the
    word “*airplane*,” indicating that an airplane is a kind of “*heavier-than-aircraft*,”
    which is a kind of “*aircraft*,” and so on, going all the way up to the very general
    category “*entity*:”
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Wordnet semantic hierarchy for the word “airplane”](img/B19005_03_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Wordnet semantic hierarchy for the word “airplane”
  prefs: []
  type: TYPE_NORMAL
- en: Pragmatic analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Pragmatic analysis** determines the meanings of words and phrases in context.
    For example, in long texts, different words can be used to refer to the same thing,
    or different things can be referred to with the same word. This is called **coreference**.
    For example, the sentence “*We want to book a flight from Boston to London*” could
    be followed by “*the flight needs to leave before 10 a.m.*” Pragmatic analysis
    determines that the flight that needs to leave before 10 a.m. is the same flight
    that we want to book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A very important type of pragmatic analysis is **named entity recognition**
    (**NER**), which links up references that occur in texts to corresponding entities
    in the real world. *Figure 3**.3* shows NER for the sentence “*Book a flight to
    London on United for less than 1,000 dollars*.” “*London*” is a named entity,
    which is labeled as a geographical location, “*United*” is labeled as an organization,
    and “*less than 1,000 dollars*” is labeled as a monetary amount:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – NER for “Book a flight to London on United for less than 1,000
    dollars”](img/B19005_03_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – NER for “Book a flight to London on United for less than 1,000
    dollars”
  prefs: []
  type: TYPE_NORMAL
- en: Pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In NLP applications, the steps we have just described are most often implemented
    as a **pipeline**; that is, a sequence of steps where the results of one step
    are the input to the next step. For example, a typical NLP pipeline might be as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Lexical lookup**: Look up the words in the application’s dictionary'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**POS tagging**: Assign parts of speech to each word in context'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Parsing**: Determine how the words are related to each other'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Semantic analysis**: Determine the meanings of the words and the overall
    meaning of the sentence'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pragmatic analysis**: Determine aspects of the meaning that depend on a broader
    context, such as the interpretations of pronouns'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One advantage of using pipelines is that each step can be implemented with different
    technologies, as long as the output of that step is in the format expected by
    the next step. So, pipelines are not only useful in rule-based approaches but
    also in the other techniques, which we will describe in the next sections.
  prefs: []
  type: TYPE_NORMAL
- en: More details on rule-based techniques will be provided in [*Chapter 8*](B19005_08.xhtml#_idTextAnchor159).
  prefs: []
  type: TYPE_NORMAL
- en: We will now turn to techniques that rely less on the rules of the language and
    more on machine learning with existing data.
  prefs: []
  type: TYPE_NORMAL
- en: Traditional machine learning approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'While rule-based approaches provide very fine-grained and specific information
    about language, there are some drawbacks to these approaches, which has motivated
    the development of alternatives. There are two major drawbacks:'
  prefs: []
  type: TYPE_NORMAL
- en: Developing the rules used in rule-based approaches can be a laborious process.
    Rule development can either be done by experts directly writing rules based on
    their knowledge of the language or, more commonly, the rules can be derived from
    examples of text that have been annotated with a correct analysis. Both of these
    approaches can be expensive and time-consuming.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rules are not likely to be universally applicable to every text that the system
    encounters. The experts who developed the rules might have overlooked some cases,
    the annotated data might not have examples of every case, and speakers can make
    errors such as false starts, which need to be analyzed although they aren’t covered
    by any rule. Written language can include spelling errors, which results in words
    that aren’t in the lexicon. Finally, the language itself can change, resulting
    in new words and new phrases that aren’t covered by existing rules.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For these reasons, rule-based approaches are primarily used as part of NLU pipelines,
    supplementing other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: 'Traditional machine learning approaches were motivated by problems in classification,
    where documents that are similar in meaning can be grouped. Two problems have
    to be solved in classification:'
  prefs: []
  type: TYPE_NORMAL
- en: Representing the documents in a training set in such a way that documents in
    the same categories have similar representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deciding how new, previously unseen documents should be classified based on
    their similarity to the documents in the training set
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Representations of documents are based on words. A very simple approach is to
    assume that the document should be represented simply as the set of words that
    it contains. This is called the **bag of words** (**BoW**) approach. The simplest
    way of using a BoW for document representation is to make a list of all the words
    in the corpus, and for each document and each word, state whether that word occurs
    in that document.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, suppose we have a corpus consisting of the three documents in
    *Figure 3**.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – A small corpus of restaurant queries](img/B19005_03_04New.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – A small corpus of restaurant queries
  prefs: []
  type: TYPE_NORMAL
- en: 'The entire vocabulary for this toy corpus is 29 words, so each document is
    associated with a list 29 items long that states, for each word, whether or not
    it appears in the document. The list represents *occurs* as `1` and *does not
    occur* as `0`, as shown in *Table 3.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **a** | **an** | **any** | **are** | **aren’t** | **away** | **Chinese**
    | **Eastern** | **…** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | … |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | … |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | 0 | 0 | 1 | 1 | 1 | 1 | 0 | 1 | … |'
  prefs: []
  type: TYPE_TB
- en: Table 3.2 – BoW for a small corpus
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 3.2* shows the BoW lists for the first eight words in the vocabulary
    for these three documents. Each row in *Table 3.2* represents one document. For
    example, the word “*a*” occurs once in the first document, but the word “*an*”
    does not occur, so its entry is *0*. This representation is mathematically a *vector*.
    Vectors are a powerful tool in NLU, and we will discuss them later in much more
    detail. The BoW representation might seem very simplistic (for example, it doesn’t
    take into account any information about the order of words). However, other variations
    on this concept are more powerful, which will be discussed later on in *Chapters
    9* to *12*.'
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The assumption behind the BoW approach is that the more words that two documents
    have in common, the more similar they are in meaning. This is not a hard-and-fast
    rule, but it turns out to be useful in practice.
  prefs: []
  type: TYPE_NORMAL
- en: For many applications, we want to group documents that are similar in meaning
    into different categories. This is the process of **classification**. If we want
    to classify a new document into one of these categories, we need to find out how
    similar its vector is to the vectors of the other documents in each category.
    For example, the sentiment analysis task discussed in [*Chapter 1*](B19005_01.xhtml#_idTextAnchor016)
    is the task of classifying documents into one of two categories – positive or
    negative sentiment regarding the topic of the text.
  prefs: []
  type: TYPE_NORMAL
- en: Many algorithms have been used for the classification of text documents. Naïve
    Bayes and **support vector machines** (**SVMs**), to be discussed in [*Chapter
    9*](B19005_09.xhtml#_idTextAnchor173), are two of the most popular. Neural networks,
    especially **recurrent neural networks** (**RNNs**), are also popular. Neural
    networks are briefly discussed in the next section and will be discussed in detail
    in [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184).
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we’ve summarized some approaches that are used in traditional
    machine learning. Now, we will turn our attention to the new approaches based
    on deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning approaches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Neural networks, and especially the large neural networks generally referred
    to as **deep learning**, have become very popular for NLU in the past few years
    because they significantly improve the accuracy of earlier methods.
  prefs: []
  type: TYPE_NORMAL
- en: The basic concept behind neural networks is that they consist of layers of connected
    units, called **neurons** in analogy to the neurons in animal nervous systems.
    Each neuron in a neural net is connected to other neurons in the neural net. If
    a neuron receives the appropriate inputs from other neurons, it will fire, or
    send input to another neuron, which will in turn fire or not fire depending on
    other inputs that it receives. During the training process, weights on the neurons
    are adjusted to maximize classification accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3**.5* shows an example of a four-layer neural net performing a sentiment
    analysis task. The neurons are circles connected by lines. The first layer, on
    the left, receives a text input. Two hidden layers of neurons then process the
    input, and the result (*positive*) is produced by the single neuron in the final
    output layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – A four-layer neural network trained to perform sentiment analysis
    on product reviews](img/B19005_03_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – A four-layer neural network trained to perform sentiment analysis
    on product reviews
  prefs: []
  type: TYPE_NORMAL
- en: Although the concepts behind neural networks have existed for many years, the
    implementation of neural networks large enough to perform significant tasks has
    only been possible within the last few years because of the limitations of earlier
    computing resources. Their current popularity is largely due to the fact that
    they are often more accurate than earlier approaches, especially given sufficient
    training data. However, the process of training a neural net for large-scale tasks
    can be complex and time-consuming and can require the services of expert data
    scientists. In some cases, the additional accuracy that a neural net provides
    is not enough to justify the additional expense of developing the system.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning and neural networks will be discussed in detail in [*Chapter 10*](B19005_10.xhtml#_idTextAnchor184).
  prefs: []
  type: TYPE_NORMAL
- en: Pre-trained models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most recent approach to NLU is based on the idea that much of the information
    required to understand natural language can be made available to many different
    applications by processing generic text (such as internet text) to create a baseline
    model for the language. Some of these models are very large and are based on tremendous
    amounts of data. To apply these models to a specific application, the generic
    model is adapted to the application through the use of application-specific training
    data, through a process called **fine-tuning**. Because the baseline model already
    contains a vast amount of general information about the language, the amount of
    training data can be considerably less than the training data required for some
    of the traditional approaches. These popular technologies include BERT and its
    many variations and **Generative Pre-trained Transformers** (**GPTs**) and their
    variations.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-trained models will be discussed in detail in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193).
  prefs: []
  type: TYPE_NORMAL
- en: Considerations for selecting technologies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter has introduced four classes of NLU technologies:'
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical machine learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning and neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pre-trained models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How should we decide which technology or technologies should be employed to
    solve a specific problem? The considerations are largely practical and have to
    do with the costs and effort required to create a working solution. Let’s look
    at the characteristics of each approach.
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 3.3* lists the four approaches to NLU that we’ve reviewed in this chapter
    and how they compare with respect to developer expertise, the amount of data required,
    the training time, accuracy, and cost. As *Table 3.3* shows, every approach has
    advantages and disadvantages. For small or simple problems that don’t require
    large amounts of data, the rule-based, deep learning, or pre-trained approaches
    should be strongly considered, at least for part of the pipeline. While pre-trained
    models are accurate and have relatively low development costs, developers may
    prefer to avoid the costs of cloud services or the costs of managing large models
    on their local computer resources:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Developer** **expertise** | **Amount of** **data required** | **Training
    time** | **Accuracy** | **Cost** |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- | --- | --- | --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| Rule-based | High (linguists or domain experts) | Small amount of domain-specific
    data | Large amount of time for experts to write rules | High if rules are accurate
    | Rule development may be costly; computer time costs are low |'
  prefs: []
  type: TYPE_TB
- en: '| Statistical | Medium – use standard tools; some NLP/data science expertise
    required | Medium amount of domain-specific data | Large amount of time for annotation
    | Medium | Data annotation may be costly; computer time costs are low |'
  prefs: []
  type: TYPE_TB
- en: '| Deep learning | High (data scientists) | Large amount of domain-specific
    data | Large amount of time for annotation; additional computer time for training
    models | Medium-high | Charges for some cloud services or local computer resources
    |'
  prefs: []
  type: TYPE_TB
- en: '| Pre-trained | Medium – use standard tools with some data science expertise
    | Small amount of domain-specific data | Medium amount of time to label data for
    fine-tuning models | High | Charges for some cloud services or local computer
    resources |'
  prefs: []
  type: TYPE_TB
- en: Table 3.3 – Comparison between general approaches to NLU
  prefs: []
  type: TYPE_NORMAL
- en: The most important considerations are the problem that’s being addressed and
    what the acceptable costs are. It should also be kept in mind that choosing one
    or another technology isn’t a permanent commitment, especially for approaches
    that rely on annotated data, which can be used for more than one approach.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we surveyed the various techniques that can be used in NLU
    applications and learned several important skills.
  prefs: []
  type: TYPE_NORMAL
- en: We learned about what rule-based approaches are and the major rule-based techniques,
    including topics such as POS tagging and parsing. We then learned about the important
    traditional machine learning techniques, especially the ways that text documents
    can be represented numerically. Next, we focused on the benefits and drawbacks
    of the more modern deep learning techniques and the advantages of pre-trained
    models.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will review the basics of getting started with NLU –
    installing Python, using Jupyter Labs and GitHub, using NLU libraries such as
    NLTK and spaCy, and how to choose between libraries.
  prefs: []
  type: TYPE_NORMAL
