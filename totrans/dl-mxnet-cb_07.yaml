- en: '7'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '7'
- en: Optimizing Models with Transfer Learning and Fine-Tuning
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用迁移学习与微调优化模型
- en: As models grow in size (the depth and number of processing modules per layer),
    training them grows exponentially as more time is spent per epoch, and typically,
    more epochs are required to reach optimum performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着模型规模的增大（每层的深度和处理模块数量），训练它们所需的时间呈指数增长，通常为了达到最佳性能，需要更多的训练轮次（epoch）。
- en: For this reason, **MXNet** provides state-of-the-art pre-trained models via
    **GluonCV** and **GluonNLP** libraries. As we have seen in previous chapters,
    these models can help us solve a variety of problems when our final dataset is
    similar to the one the selected model has been pre-trained on.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，**MXNet**通过**GluonCV**和**GluonNLP**库提供了最先进的预训练模型。正如我们在前几章中所见，当我们的最终数据集与所选模型的预训练数据集相似时，这些模型可以帮助我们解决各种问题。
- en: However, sometimes this is not good enough, and our final dataset might have
    some nuances that the pre-trained model is not picking up. In these cases, it
    would be ideal to combine the stored knowledge of the pre-trained model with our
    final dataset. This is called transfer learning, where the knowledge of our pre-trained
    model is transferred to a new task (final dataset).
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有时候这还不够，最终数据集可能存在一些微妙的差异，预训练模型并未能捕捉到这些差异。在这种情况下，将预训练模型所存储的知识与我们的最终数据集相结合是理想的做法。这就是迁移学习，我们将预训练模型的知识转移到一个新的任务（最终数据集）上。
- en: In this chapter, we will learn how to use GluonCV and GluonNLP, which are MXNet
    Gluon libraries that are specific to **Computer Vision** (**CV**) and **Natural
    Language Processing** (**NLP**), respectively. We will also learn how to retrieve
    pre-trained models from their model zoos, and how to optimize our own networks
    by transferring the learnings from these pre-trained models.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将学习如何使用MXNet Gluon库中的GluonCV和GluonNLP，分别针对**计算机视觉**（**CV**）和**自然语言处理**（**NLP**）。我们还将学习如何从它们的模型库中获取预训练模型，并通过迁移这些预训练模型的学习成果来优化我们自己的网络。
- en: 'Specifically, we will cover the following topics in our recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将在本章中涵盖以下主题：
- en: Understanding transfer learning and fine-tuning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解迁移学习与微调
- en: Improving performance for classifying images
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升图像分类性能
- en: Improving performance for segmenting images
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升图像分割性能
- en: Improving performance for translating English to German
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提升从英语翻译到德语的性能
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    technical requirements apply:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 除了*前言*中指定的技术要求外，以下技术要求适用：
- en: Ensure that you have completed the first recipe, *Installing MXNet, Gluon, GluonCV
    and GluonNLP*, from [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016), *Up and Running*
    *with MXNet*.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了第一章的食谱，*安装MXNet、Gluon、GluonCV和GluonNLP*，[*第1章*](B16591_01.xhtml#_idTextAnchor016)，*开始使用MXNet*。
- en: Ensure that you have completed [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with Computer Vision*, and [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121),
    *Understanding Text with Natural* *Language Processing*.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*，以及[*第6章*](B16591_06.xhtml#_idTextAnchor121)，*理解自然语言处理中的文本*。
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch07](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch07).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下GitHub链接中找到：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch07](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch07)。
- en: 'Furthermore, you can access each recipe directly from Google Colab; for example,
    the first recipe of this chapter can be found here: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch07/7_1_Understanding_Transfer_Learning_and_Fine_Tuning.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch07/7_1_Understanding_Transfer_Learning_and_Fine_Tuning.ipynb).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以直接从Google Colab访问每个食谱；例如，本章的第一个食谱可以在这里找到：[https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch07/7_1_Understanding_Transfer_Learning_and_Fine_Tuning.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch07/7_1_Understanding_Transfer_Learning_and_Fine_Tuning.ipynb)。
- en: Understanding transfer learning and fine-tuning
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解迁移学习与微调
- en: In the previous chapters, we saw how we could leverage MXNet, GluonCV, and GluonNLP
    to retrieve pre-trained models in certain datasets (such as ImageNet, MS COCO,
    and IWSLT2015) and use them for our specific tasks and datasets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will introduce a methodology called **transfer learning**,
    which will allow us to combine the information from pre-trained models (on general
    knowledge datasets) and the information from the new domain (the dataset from
    the task we want to solve). There are two main significant advantages to this
    approach. On the one hand, pre-training datasets are typically large-scale (ImageNet-22k
    has 14 million images), and using a pre-trained model saves us that training time.
    On the other hand, we use our specific dataset not only for evaluation but also
    for training the model, improving its performance in the desired scenario. As
    we will discover, there is not always an easy way to achieve this, as it requires
    the capability to obtain a sizable dataset, or even one right way, as it might
    not yield the expected results. We will also explore the optional next step after
    transfer learning, called fine-tuning, where we will try to use our specific dataset
    to modify the model parameters even further. We will put both techniques to the
    test.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As for previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will be looking at the following steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: Introducing transfer learning
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describing the advantages of transfer learning and when to use it
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Understanding the fundamentals of representation learning
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Focusing on practical applications
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s dive into each of these steps.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: Introducing transfer learning
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous chapters, we learned how to train deep learning neural networks
    from scratch, exploring problems in CV and NLP. As introduced in [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052),
    *Solving Regression Problems*, deep learning neural networks try to imitate the
    biological networks in our brains. One interesting point of view is that when
    we (and our brains) learn new tasks, we leverage previous knowledge we have acquired
    in a very strong way. For example, a very good tennis player will become a relatively
    good player at squash with a few hours of play. Transfer learning is a field of
    study that contains different techniques to achieve similar results as in this
    example.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 7.1 – Comparison between traditional \uFEFFmachine \uFEFFlearning\
    \ (ML) and transfer learning](img/B16591_07_1.jpg)"
  id: totrans-31
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Comparison between traditional machine learning (ML) and transfer
    learning
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 7**.1*, we can see a comparison between both paradigms where, in
    transfer learning, the approach to solving Task 2 leverages the knowledge acquired
    while solving Task 1\. This implies, however, that to solve a single desired task
    (Task 2), we are training the model twice (for Task 1 and for Task 2 later). In
    practice, as we will see in the next steps, we will work with pre-trained models
    from MXNet’s GluonCV and GluonNLP model zoos, and therefore, we will only have
    to train the model once, for Task 2.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.1*中，我们可以看到两种范式的对比，其中迁移学习解决任务 2 的方法利用了在解决任务 1 时获得的知识。然而，这意味着要解决单个目标任务（任务
    2），我们需要训练两次模型（分别为任务 1 和任务 2）。实际上，正如我们接下来的步骤所示，我们将使用来自 MXNet 的 GluonCV 和 GluonNLP
    模型库中的预训练模型，因此我们只需为任务 2 训练一次模型。
- en: Describing the advantages of transfer learning and when to use it
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 描述迁移学习的优势以及何时使用它
- en: 'There are several reasons why using transfer learning offers advantages:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用迁移学习具有多种优势，原因有很多：
- en: '**Faster**: As we leverage pre-trained models from model zoos, the training
    will converge much faster than training from scratch, requiring much fewer epochs
    and less time.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更快**：通过利用来自模型库的预训练模型，训练过程会比从头开始训练更快收敛，所需的训练轮次和时间都会大大减少。'
- en: '**More general**: Typically, pre-trained models have been trained in large-scale
    datasets (such as ImageNet); therefore, the parameters (weights) learned are generalistic
    and can then be reused for a large number of tasks. It is an objective that outputs
    from the feature extraction part of the pre-trained model (also known as **representations**),
    learned by training using large-scale datasets that are general and domain-invariant
    (can be reused).'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更通用**：通常，预训练模型是使用大规模数据集（如 ImageNet）进行训练的，因此学习到的参数（权重）具有广泛的适用性，能够重用于大量任务。这个目标是通过使用大规模数据集训练得到的、既通用又不依赖特定领域的特征提取部分（也称为**表示**）来实现的。'
- en: '**Requires less data**: To adapt a pre-trained model for a given new task,
    the amount of data required is much less than for training that specific model
    architecture from scratch. This is because representations can be reused (as mentioned
    in the previous point).'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**需要更少的数据**：为了将预训练模型适配到新的任务上，所需的数据量远低于从头开始训练该模型架构的数量。这是因为表示（如前述）可以被重用。'
- en: '**More environmentally friendly**: As the training time, datasets, and compute
    requirements for transfer learning are much lower than training from scratch,
    less pollution is required to train a model.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**更环保**：由于迁移学习所需的训练时间、数据集和计算资源远低于从头开始训练，训练模型所需的污染也大大减少。'
- en: '**Performance improvements**: It has been proven (for example, in [https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf))
    that using transfer learning with small-scale datasets yields strong performance
    improvements, and on large-scale datasets, the same performance point is achieved
    much faster than training from scratch.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性能提升**：已有研究证明（例如，[https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf](https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Oquab_Learning_and_Transferring_2014_CVPR_paper.pdf)）迁移学习在小规模数据集上能带来显著的性能提升，在大规模数据集上，迁移学习能比从头训练更快地达到相同的性能水平。'
- en: In *Figure 7**.2*, different methods to compute representations are analyzed,
    and although specialized networks can reach better performance, this is only possible
    if large-scale datasets, high-end compute resources, and longer training times
    are given.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.2*中，分析了计算表示的不同方法，尽管专用网络可以达到更好的性能，但这只有在拥有大规模数据集、高端计算资源和更长训练时间的情况下才能实现。
- en: '![Figure 7.2 – Comparing different approaches for representations](img/B16591_07_2.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.2 – 比较不同的表示学习方法](img/B16591_07_2.jpg)'
- en: Figure 7.2 – Comparing different approaches for representations
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 – 比较不同的表示学习方法
- en: 'In a more general setting, there are different ways to achieve transfer learning,
    as shown in the following figure:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在更广泛的场景下，迁移学习有多种实现方法，如下图所示：
- en: '![Figure 7.3 – Different types of transfer learning](img/B16591_07_3.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.3 – 不同类型的迁移学习](img/B16591_07_3.jpg)'
- en: Figure 7.3 – Different types of transfer learning
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 – 不同类型的迁移学习
- en: In *Figure 7**.3*, we can see the different types of transfer learning, depending
    on the similarity of the source and target domain and the availability of source
    and target data. In this chapter, we will explore the usual setting of having
    a pre-trained model in a similar domain to our intended task (equal source and
    target domain), and the tasks will be slightly different, with some amount of
    labeled data in the target domain (**inductive** **transfer learning**).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.3*中，我们可以看到不同类型的迁移学习，这取决于源领域和目标领域的相似性以及源领域和目标领域数据的可用性。在本章中，我们将探讨在与我们目标任务相似的领域中使用预训练模型的常见设置（源领域和目标领域相同），并且任务会稍有不同，同时在目标领域有一定量的标注数据（**归纳**
    **迁移学习**）。
- en: 'Andrew Ng, chief scientist of Baidu and co-founder of Google Brain, said the
    following in a tutorial in NIPS 2016 called *Nuts and Bolts of Building AI Applications
    Using Deep Learning*: “*In the next few years, we’ll see a lot of concrete value
    driven through transfer learning*,” and he was right.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 百度首席科学家、Google Brain的联合创始人Andrew Ng在2016年NIPS的一个教程中说：“*在未来几年，我们将看到通过迁移学习带来大量具体的价值*，”他是对的。
- en: Understanding the fundamentals of representation learning
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解表示学习的基本原理
- en: In this section, we will answer the question, from a more theoretical point
    of view, about how to use transfer learning and why it works. In [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with Computer Vision*, and [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121),
    *Understanding Text with Natural Language Processing*, we introduced the concept
    of **representations** for features in images using GluonCV and for words/sentences
    in text using GluonNLP.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将从更理论的角度回答如何使用迁移学习以及它为何有效的问题。在[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*，和[*第6章*](B16591_06.xhtml#_idTextAnchor121)，*使用自然语言处理理解文本*中，我们介绍了使用GluonCV提取图像特征的**表示**概念，以及使用GluonNLP提取文本中单词/句子的表示概念。
- en: 'We can revisit, in *Figure 7**.4*, the usual architecture of a CNN architecture:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在*图7.4*中回顾CNN架构的常见结构：
- en: '![Figure 7.4 – Refresher of Convolutional Neural Networks (CNNs)](img/B16591_07_4.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图7.4 – 卷积神经网络（CNNs）的复习](img/B16591_07_4.jpg)'
- en: Figure 7.4 – Refresher of Convolutional Neural Networks (CNNs)
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 – 卷积神经网络（CNNs）的复习
- en: 'In *Figure 7**.5*, we can revisit the usual Transformer architecture:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.5*中，我们可以回顾Transformer架构的常见结构：
- en: '![Figure 7.5 – Refresh of the Transformer architecture (encoder on the left,
    and decoder on the right)](img/B16591_07_5.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图7.5 – Transformer架构的复习（左侧是编码器，右侧是解码器)](img/B16591_07_5.jpg)'
- en: Figure 7.5 – Refresh of the Transformer architecture (encoder on the left, and
    decoder on the right)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 – Transformer架构的复习（左侧是编码器，右侧是解码器）
- en: The underlying idea is common in both fields; for example, the feature extractor
    part of CNNs and the encoder in Transformers are representations, and the training
    of these network sections is called **representation learning**, an active field
    of study due to the capability of being able to train these networks in both supervised
    and unsupervised settings.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这一基本思想在两个领域中都是共通的；例如，CNN的特征提取部分和Transformer中的编码器就是表示，而这些网络部分的训练被称为**表示学习**，这是一项积极的研究领域，因为它具备在监督和无监督设置下训练这些网络的能力。
- en: 'The main idea behind transfer learning is to transfer the representations learned
    in a task to a different task; therefore, we will typically follow the next steps:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习背后的主要思想是将一个任务中学习到的表示迁移到另一个任务中；因此，我们通常会遵循以下步骤：
- en: Retrieve a pre-trained model from MXNet’s Model Zoo (GluonCV or GluonNLP).
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从MXNet的模型库中获取预训练模型（GluonCV或GluonNLP）。
- en: Remove the last layers (typically, a classifier). Keep the parameters in the
    rest of the layers frozen (not updatable during training).
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除最后的层（通常是分类器）。将其他层的参数冻结（在训练过程中不可更新）。
- en: Add new layers (a new classifier) corresponding to the new task
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 添加新的层（新的分类器），以适应新任务
- en: Train the updated model (only the new layers, not frozen, will be updated during
    training) with the target data.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用目标数据训练更新后的模型（只有新添加的层是可更新的，其他冻结的层在训练期间不可更新）。
- en: If we have enough labeled data for the task that we want to solve (target task),
    another step (that can be done after the previous step or substituting it) is
    called **fine-tuning**.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有足够的标注数据来解决我们想要解决的任务（目标任务），另一个步骤（可以在前一步之后执行，也可以替代前一步）叫做**微调**。
- en: 'Fine-tuning takes into account that the representations originally learned
    might not fit perfectly with the target task and, therefore, could also improve
    with updating. In this scenario, the steps are as follows:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 微调考虑到原始学习的表示可能无法完美适应目标任务，因此，通过更新也能得到改进。在这种情况下，步骤如下：
- en: Unfreeze the weights of the representation network.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解冻表示网络的权重。
- en: Retrain the network with target data, typically with a smaller learning rate
    as the representations should be close (same domain).
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用目标数据重新训练网络，通常使用较小的学习率，因为表示应该接近（同一领域）。
- en: Both processes (transfer learning and fine-tuning) are summarized visually in
    *Figure 7**.6*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 两个过程（迁移学习和微调）在*图 7.6*中有直观总结。
- en: '![Figure 7.6 – Transfer learning and fine-tuning](img/B16591_07_6.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.6 – 迁移学习与微调](img/B16591_07_6.jpg)'
- en: Figure 7.6 – Transfer learning and fine-tuning
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.6 – 迁移学习与微调
- en: Both processes can be applied sequentially, with adequate **hyperparameters**
    for each one.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 两个过程可以按顺序应用，每个过程都有适当的**超参数**。
- en: Focusing on practical applications
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 重点关注实际应用
- en: 'In this section, we will use what we have learned so far about representation
    learning and we will apply it to a practical example: detecting cats and dogs.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用到目前为止所学的表示学习内容，并将其应用于一个实际的示例：检测猫和狗。
- en: 'To do this, we will retrieve a model from the **GluonCV Model Zoo**; we will
    remove the classifier (last layers) and keep the feature extraction stage. We
    will then analyze how the representations of the cats and dogs have been learned.
    To load the model, we can use this code snippet:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将从**GluonCV模型库**中检索一个模型；我们将去除分类器（最后的几层），保留特征提取阶段。然后，我们将分析猫和狗的表示是如何被学习的。要加载模型，我们可以使用以下代码片段：
- en: '[PRE0]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In the previous code snippet, for the `pretrained` parameter, we have assigned
    the value of `True`, indicating that we want the pretrained weights to be retrieved
    (and not only the architecture of the model).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，对于`pretrained`参数，我们已将其赋值为`True`，表示我们希望加载预训练权重（而不仅仅是模型的架构）。
- en: When trained correctly, CNNs learn hierarchical representations of the features
    of the images in the training dataset, with each progressive layer learning more
    and more complex patterns. Therefore, when an image is processed (when processing
    on successive layers), the network can compute more complex patterns associated
    with the network.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 当正确训练时，CNN会学习训练数据集中图像特征的层次化表示，每一层逐渐学习越来越复杂的模式。因此，当图像被处理时（在连续的层中进行处理），网络能够计算与网络相关的更复杂的模式。
- en: 'Now, we can use a new MXNet library, MXBoard (see the recipe for installation
    instructions), with this model to evaluate the different steps that a dog image
    goes through and see some examples of how a pre-trained model computes its representations:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用一个新的MXNet库，MXBoard（请参阅安装说明中的食谱），使用此模型来评估狗图像经过的不同步骤，并查看一些预训练模型计算其表示的示例：
- en: '![Figure 7.7 – Cat and dog representations – convolutional filters](img/B16591_07_7.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.7 – 猫与狗的表示 – 卷积滤波器](img/B16591_07_7.jpg)'
- en: Figure 7.7 – Cat and dog representations – convolutional filters
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 – 猫与狗的表示 – 卷积滤波器
- en: In *Figure 7**.7*, we can see the convolutional filters corresponding to the
    first convolutional layer of a ResNet152 pre-trained network (on ImageNet). Please
    note how these filters focus on simple patterns such as specific shapes (vertical
    and horizontal lines, circles, and so on) and specific colors (red blobs).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7.7*中，我们可以看到对应于ResNet152预训练网络（在ImageNet上）的第一层卷积层的卷积滤波器。请注意这些滤波器如何专注于简单的模式，如特定的形状（垂直和水平线、圆形等）和特定的颜色（红色斑点）。
- en: 'Let’s analyze the results with a specific image:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一张特定的图像来分析结果：
- en: '![Figure 7.8 – Example image of a dog](img/B16591_07_8.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.8 – 一只狗的示例图像](img/B16591_07_8.jpg)'
- en: Figure 7.8 – Example image of a dog
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.8 – 一只狗的示例图像
- en: 'We select an image from our *Dogs vs. Cats* dataset, such as the dog depicted
    in *Figure 7**.8*. When passing this image through our network, we will find results
    similar to the following:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从*Dogs vs. Cats*数据集中选择一张图像，例如*图 7.8*中描绘的狗。当将这张图像通过我们的网络时，我们将得到类似以下的结果：
- en: '![Figure 7.9 – Output from convolutional filters](img/B16591_07_9.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.9 – 卷积滤波器输出](img/B16591_07_9.jpg)'
- en: Figure 7.9 – Output from convolutional filters
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.9 – 卷积滤波器输出
- en: In *Figure 7**.9*, we can see the output of the filters in *Figure 7**.7* for
    our dog example. Note how different outputs highlight simple shapes such as the
    eyes or the legs (larger values, closer to white).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图7.9*中，我们可以看到我们狗的示例在*图7.7*中的过滤器输出。注意不同的输出如何突出显示简单的形状，比如眼睛或腿部（较大的值，接近白色）。
- en: 'Finally, as the image traverses the network, its features are more and more
    compressed, yielding (for ResNet152) a final vector of 2,048 elements. This vector
    can be computed easily with networks retrieved using MXNet’s Model Zoo:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，随着图像在网络中传播，它的特征会越来越压缩，最终得到（对于ResNet152）一个包含2,048个元素的向量。这个向量可以通过使用MXNet的模型库轻松计算：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This code excerpt provides the following output:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码示例会输出以下结果：
- en: '[PRE2]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see, we have a `2048` element.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，我们得到了一个`2048`元素。
- en: How it works…
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它是如何工作的...
- en: In this recipe, we introduced the concepts of transfer learning and fine-tuning.
    We explained when it made sense to use these two different techniques and their
    advantages.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在本篇食谱中，我们介绍了迁移学习和微调的概念。我们解释了何时使用这两种不同技术及其优点。
- en: We also explored when these techniques can be useful and their connections to
    representation learning, explaining how representations play a significant role
    in the knowledge being transferred when using these techniques. We used a new
    library, **MXBoard**, to produce visualizations for the representations.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还探讨了这些技术何时有用及其与表征学习的关系，解释了在使用这些技术时，表征在知识转移中的重要作用。我们使用了一个新的库，**MXBoard**，来生成表征的可视化。
- en: Moreover, we intuitively and practically showed how to apply these techniques
    to CV and NLP tasks and computed a representation for a specific example.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们直观且实践地展示了如何将这些技术应用于计算机视觉和自然语言处理任务，并为一个具体示例计算了表征。
- en: There’s more...
  id: totrans-97
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多...
- en: 'Transfer learning, including fine-tuning, is an active field of study. In this
    recipe, we have only covered the most useful scenario for deep learning, inductive
    transfer learning. For a more comprehensive but still easy-to-read introduction,
    I recommend reading *Transfer learning: a friendly introduction*, which can be
    found at: [https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00652-w](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00652-w).'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习，包括微调，是一个活跃的研究领域。在这个食谱中，我们仅涵盖了深度学习中最有用的场景——归纳迁移学习。想了解更全面但仍易于阅读的介绍，我推荐阅读*迁移学习：友好的介绍*，可以在以下网址找到：[https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00652-w](https://journalofbigdata.springeropen.com/articles/10.1186/s40537-022-00652-w)。
- en: 'Moreover, the concept of transferring knowledge from one system to another
    is not new, and there are references to concepts such as **learning to learn**
    and **knowledge transfer** as early as 1995, when a **Neural Information Processing
    Systems** (**NeurIPS**) workshop on the topic was presented. A summary of the
    workshop can be found here: http://socrates.acadiau.ca/courses/comp/dsilver/nips95_ltl/nips95.workshop.pdf.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，知识从一个系统转移到另一个系统的概念并不新颖，早在1995年就有**学习如何学习**和**知识转移**等概念的引用，那时曾有一个关于这个主题的**神经信息处理系统**（**NeurIPS**）研讨会。该研讨会的总结可以在这里找到：http://socrates.acadiau.ca/courses/comp/dsilver/nips95_ltl/nips95.workshop.pdf。
- en: 'Furthermore, as introduced 21 years later in the same venue, Andrew Ng was
    able to correctly foresee the importance of transfer learning. His 2016 NeurIPS
    tutorial can be found here (jump to 1h 37m for the transfer learning quote): [https://www.youtube.com/watch?v=F1ka6a13S9I](https://www.youtube.com/watch?v=F1ka6a13S9I).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，正如21年后在同一场合中介绍的，Andrew Ng正确预见了迁移学习的重要性。他的2016年NeurIPS教程可以在这里找到（跳转到1小时37分钟查看迁移学习相关内容）：[https://www.youtube.com/watch?v=F1ka6a13S9I](https://www.youtube.com/watch?v=F1ka6a13S9I)。
- en: Improving performance for classifying images
  id: totrans-101
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 提升图像分类的性能
- en: After introducing transfer learning and fine-tuning in the previous recipe,
    in this one, we will apply it to **image classification**, a CV task.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一篇食谱中介绍了迁移学习和微调后，在本篇中，我们将其应用于**图像分类**，这是一个计算机视觉任务。
- en: In the second recipe, *Classifying images with MXNet – GluonCV Model Zoo, AlexNet,
    and ResNet*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing Images
    with Computer Vision*, we saw how we could use GluonCV to retrieve pre-trained
    models and use them directly for an image classification task. In the first instance,
    we looked at training them from scratch, effectively only leveraging past knowledge
    by using the architecture of the pre-trained model, without leveraging any past
    knowledge contained in the pre-trained weights, which were re-initialized, deleting
    any past information. Afterward, the pre-trained models were used directly for
    the task, effectively also leveraging the weights/parameters of the model.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个配方中，*使用MXNet进行图像分类——GluonCV模型库，AlexNet和ResNet*，在[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*，我们已经看到如何使用GluonCV检索预训练模型，并直接用于图像分类任务。在第一次的实例中，我们展示了如何从零开始训练模型，实际上仅利用预训练模型的架构，而没有利用任何包含在预训练权重中的过去知识，这些权重已被重新初始化，删除了任何历史信息。之后，预训练模型直接用于任务，实际上也利用了模型的权重/参数。
- en: In this recipe, we will combine the weights/parameters of the model with the
    target dataset, applying the techniques introduced in this chapter, transfer learning
    and fine-tuning. The dataset used for the pre-training was `Dogs vs` `Cats` dataset.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将把模型的权重/参数与目标数据集结合，应用本章介绍的技术——迁移学习和微调。用于预训练的数据集是`Dogs vs` `Cats`数据集。
- en: Getting ready
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As for previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 和前面的章节一样，在这个配方中，我们将使用一些矩阵运算和线性代数，但一点也不难。
- en: 'Furthermore, we will be working with text datasets; therefore, we will revisit
    some concepts already seen in the second recipe, *Classifying images with MXNet:
    GluonCV Model Zoo, AlexNet, and ResNet*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with* *Computer Vision*.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还将处理文本数据集；因此，我们将重新审视在第二个配方中已看到的一些概念，*使用MXNet进行图像分类：GluonCV模型库，AlexNet和ResNet*，在[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*。
- en: How to do it...
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will be looking at the following steps:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将查看以下步骤：
- en: Revisiting the *ImageNet-1k* and *Dogs vs.* *Cats* datasets
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新审视*ImageNet-1k*和*Dogs vs.* *Cats*数据集
- en: Training a **ResNet** model from scratch with *Dogs* *vs Cats*
  id: totrans-111
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从头开始训练一个**ResNet**模型，使用*Dogs* *vs Cats*数据集
- en: Using a pre-trained ResNet model to optimize performance via transfer learning
    from *ImageNet-1k* to *Dogs* *vs Cats*
  id: totrans-112
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预训练的ResNet模型，通过迁移学习从*ImageNet-1k*到*Dogs* *vs Cats*优化性能
- en: Fine-tuning our pre-trained ResNet model on *Dogs* *vs Cats*
  id: totrans-113
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对*Dogs* *vs Cats*数据集上的预训练ResNet模型进行微调
- en: Let’s look at these steps in detail next.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将详细介绍这些步骤。
- en: Revisiting the ImageNet-1k and Dogs vs Cats datasets
  id: totrans-115
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新审视ImageNet-1k和Dogs vs Cats数据集
- en: '*ImageNet-1k* and *Dogs vs Cats* are both image classification datasets; however,
    they are quite different. *ImageNet-1k* is a large-scale dataset containing ~1.2
    million images labeled into 1,000 classes and has been used extensively in research
    and academia for benchmarking. *Dogs vs Cats* is a small-scale dataset containing
    1,400 images depicting either a dog or a cat, and its fame is mostly due to a
    Kaggle competition launched in 2013.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '*ImageNet-1k*和*Dogs vs Cats*都是图像分类数据集；然而，它们有很大的不同。*ImageNet-1k*是一个大规模数据集，包含约120万张图像，按1000个类别进行标签，广泛用于研究和学术界的基准测试。*Dogs
    vs Cats*是一个小规模数据集，包含1400张描绘狗或猫的图像，其知名度主要来自于2013年启动的Kaggle竞赛。'
- en: MXNet GluonCV does not provide methods to directly download any of the datasets.
    However, we do not need the *ImageNet-1k* dataset (its size is ~133 GB), only
    the pre-trained parameters for our chosen model. The pre-trained models can be
    downloaded directly from the MXNet GluonCV Model Zoo, we have seen examples in
    previous chapters and we will use them again in this one.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet GluonCV不提供直接下载数据集的方法。然而，我们不需要*ImageNet-1k*数据集（它的大小约为133GB），只需要我们选择的模型的预训练参数。预训练模型可以直接从MXNet
    GluonCV模型库下载，我们在前面的章节中见过例子，在本章中也将再次使用它们。
- en: 'Here are some examples from *ImageNet-1k*:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一些来自*ImageNet-1k*的示例：
- en: '![Figure 7.10 – ImageNet-1k examples](img/B16591_07_10.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.10 – ImageNet-1k示例](img/B16591_07_10.jpg)'
- en: Figure 7.10 – ImageNet-1k examples
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 – ImageNet-1k示例
- en: The source of the preceding figure is [https://cs.stanford.edu/people/karpathy/cnnembed/](https://cs.stanford.edu/people/karpathy/cnnembed/).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 上图的来源是[https://cs.stanford.edu/people/karpathy/cnnembed/](https://cs.stanford.edu/people/karpathy/cnnembed/)。
- en: 'For *Dogs vs Cats*, all the information on how to retrieve the dataset can
    be found in the second recipe, *Classifying images with MXNet: GluonCV Model Zoo,
    AlexNet, and ResNet*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing
    Images with Computer Vision*. Taking that recipe’s code as a reference, we can
    display some examples:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于*猫狗*数据集，关于如何获取数据集的所有信息都可以在第二个配方中找到，即*使用 MXNet 进行图像分类：GluonCV 模型动物园、AlexNet
    和 ResNet*，在[*第 5 章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*。根据该配方的代码作为参考，我们可以显示一些示例：
- en: '![Figure 7.11 – Dogs vs Cats dataset](img/B16591_07_11.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.11 – 猫狗数据集](img/B16591_07_11.jpg)'
- en: Figure 7.11 – Dogs vs Cats dataset
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 – 猫狗数据集
- en: In *Figure 7**.10* and *Figure 7**.11*, we can see how some images from *ImageNet-1k*
    resemble some of the images from *Dogs* *vs Cats*.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 7**.10*和*图 7**.11*中，我们可以看到*ImageNet-1k*中的一些图像与*猫狗*数据集中的一些图像相似。
- en: Training a ResNet model from scratch with Dogs vs Cats
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从头开始训练一个 ResNet 模型，使用猫狗数据集
- en: 'As described in the second recipe, *Classifying images with MXNet: GluonCV
    Model Zoo, AlexNet, and ResNet*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with Computer Vision*, we will be using **softmax cross-entropy**
    as the loss function and **accuracy** and the **confusion matrix** as evaluation
    metrics.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如第二个配方中所述，*使用 MXNet 进行图像分类：GluonCV 模型动物园、AlexNet 和 ResNet*，在[*第 5 章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*，我们将使用**softmax
    交叉熵**作为损失函数，以及**accuracy**和**混淆矩阵**作为评估指标。
- en: 'We have the following evolution in the training using the ResNet model:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 ResNet 模型进行训练的演变如下：
- en: '![Figure 7.12 – ResNet training evolution (training loss and validation loss,
    and validation accuracy) – training from scratch](img/B16591_07_12.jpg)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.12 – ResNet 训练演变（训练损失和验证损失，以及验证精度）– 从头开始训练](img/B16591_07_12.jpg)'
- en: Figure 7.12 – ResNet training evolution (training loss and validation loss,
    and validation accuracy) – training from scratch
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 – ResNet 训练演变（训练损失和验证损失，以及验证精度）– 从头开始训练
- en: 'Furthermore, for the best iteration, the `accuracy` value obtained in the test
    set is as follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在最佳迭代中，测试集中得到的`accuracy`值如下：
- en: '[PRE3]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The confusion matrix is as follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵如下：
- en: '![Figure 7.13 – Confusion matrix in Dogs vs Cats for a ResNet model trained
    from scratch](img/B16591_07_13.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.13 – 从头开始训练的 ResNet 模型在猫狗数据集中的混淆矩阵](img/B16591_07_13.jpg)'
- en: Figure 7.13 – Confusion matrix in Dogs vs Cats for a ResNet model trained from
    scratch
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.13 – 从头开始训练的 ResNet 模型在猫狗数据集中的混淆矩阵
- en: Both the accuracy value obtained (75%) and *Figure 7**.13* show quite average
    performance after training for several epochs (100, in this example). You are
    encouraged to run your own experiments trying out different hyperparameters.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过多次训练（例如，100 次）后，获得的准确度值（75%）和*图 7**.13*显示出相当平均的性能。鼓励您运行自己的实验，尝试不同的超参数设置。
- en: 'Qualitatively, we can also check how well our model is performing with an example
    image. In our case, we chose the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 定性上，我们还可以检查我们的模型在一个示例图像中的表现。在我们的案例中，我们选择了以下内容：
- en: '![Figure 7.14 – Qualitative example of Cats vs Dogs, specifically a cat](img/B16591_07_14.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图 7.14 – 猫狗定性示例，具体为猫](img/B16591_07_14.jpg)'
- en: Figure 7.14 – Qualitative example of Cats vs Dogs, specifically a cat
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.14 – 猫狗定性示例，具体为猫
- en: 'We can check the output of our model by running this image through it with
    the following code snippet:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过以下代码片段运行这张图像，检查我们模型的输出：
- en: '[PRE4]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'These code statements will give us the following output:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这些代码语句将给出以下输出：
- en: '[PRE5]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As can be seen from the results, the image has been correctly classified as
    a cat.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如结果所示，图像已正确分类为猫。
- en: Using a pre-trained ResNet model to optimize performance via transfer learning
    from ImageNet-1k to Dogs vs Cats
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用预训练的 ResNet 模型，通过从 ImageNet-1k 到猫狗数据集的迁移学习来优化性能
- en: 'In the previous recipe, we trained a new model from scratch using our dataset.
    However, this has two important drawbacks:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的配方中，我们使用我们的数据集从头开始训练了一个新模型。然而，这有两个重要的缺点：
- en: A large amount of data is required for training from scratch.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从头开始训练需要大量的数据。
- en: The training process can take a long time due to the large size of the dataset
    and the number of epochs needed for the model to learn the task.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于数据集的大尺寸和模型学习任务所需的周期数，训练过程可能需要很长时间。
- en: 'Therefore, in this recipe, we will follow a different approach: we will use
    pre-trained models from MXNet GluonCV to solve the task. These models have been
    trained in *ImageNet-1k*, a dataset that contains the classes we are interested
    in (cats and dogs); therefore, we can use those learned representations and easily
    transfer them to *Dogs vs Cats* (same domain).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'For a ResNet model, use the following:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As we can see in the previous code snippet, following the discussion in this
    chapter’s first recipe, *Understanding transfer learning and fine-tuning*, for
    the `pretrained` parameter, we have assigned the value of `True`, indicating that
    we want the pre-trained weights to be retrieved (and not only the architecture
    of the model).
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to adequately evaluate the improvements that transfer learning brings,
    we are going to evaluate our pre-trained model directly (the source task is *ImageNet-1k*)
    before applying transfer learning to *Dogs vs Cats* and after applying it. Therefore,
    using our pre-trained model as is, we obtain the following:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The confusion matrix is as follows:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.15 – Confusion matrix in Dogs vs Cats for a pre-trained ResNet model](img/B16591_07_15.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: Figure 7.15 – Confusion matrix in Dogs vs Cats for a pre-trained ResNet model
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, our pre-trained Transformer model is already showing good performance
    values as it is the same domain; however, simply using a pre-trained model does
    not yield better performance than training from scratch. The great advantage of
    using pre-trained models is the time savings, as loading one just takes a few
    lines of code.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check how well our model is performing qualitatively with the same
    image example. Note how the code slightly differs from the previous qualitative
    image excerpt, as now we need to convert ImageNet classes (the output of our ResNet50
    pre-trained model) to our classes (`0` for cats and `1` for dogs). The new code
    is given as follows:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'These code statements will give us the following output:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As can be seen from the result, the image has been correctly classified as a
    cat.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a baseline for comparison, let’s apply transfer learning to
    our task. From the first recipe, *Understanding transfer learning and fine-tuning*,
    the first step was to retrieve a pre-trained model from the MXNet Model Zoo (GluonCV
    or GluonNLP), which we have already done.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: The second step was to remove the last layers (typically, a classifier), keeping
    the parameters in the rest of the layers frozen (not updatable during training),
    so let’s do it!
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: 'We can replace the classifier with the following snippet:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We can freeze the ResNet feature extraction layers with the following snippet:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'We can replace the classifier with the following snippet:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now, we can apply the usual training process with *Dogs vs Cats*, and we have
    the following evolution in the training using the ResNet model:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.16 – ResNet training evolution (training loss and validation loss)
    – transfer learning](img/B16591_07_16.jpg)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
- en: Figure 7.16 – ResNet training evolution (training loss and validation loss)
    – transfer learning
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the accuracy obtained in the test set
    is as follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The confusion matrix is as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.17 – Confusion matrix in Dogs vs Cats for a ResNet model with transfer
    learning](img/B16591_07_17.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
- en: Figure 7.17 – Confusion matrix in Dogs vs Cats for a ResNet model with transfer
    learning
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Compared with our previous experiment of training from scratch, this experiment
    yields much higher performance, and it took us literally minutes to get this model
    to start working well for us in our intended task, whereas the training required
    for the previous experiment took hours and required several tries to tune the
    hyperparameters, which can then turn into several days of effort in total.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As can be seen from the result, the image has been correctly classified as a
    cat.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning our pre-trained ResNet model on Dogs vs Cats
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous recipe, we *froze* the parameters in the encoder layers. However,
    as the dataset we are currently working with (*Dogs vs Cats*) has enough data
    samples, we can *unfreeze* those parameters and train the model, effectively allowing
    the new training process to update the representations (with transfer learning,
    we were working directly with the representations learned for *ImageNet-1k*).
    This process is called fine-tuning.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two variants of fine-tuning:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: Applying transfer learning by freezing the layers and unfreezing them afterward
    (fine-tuning after transfer learning)
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directly applying fine-tuning without the preliminary step of freezing the layers
    (fine-tuning directly)
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s compute both experiments and draw conclusions by comparing the results.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first experiment, we can take the network obtained in the previous
    recipe, unfreeze the layers, and restart the training. In MXNet, to unfreeze the
    encoder parameters, we can run the following snippet:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now, we can apply the usual training process with *Dogs vs Cats*, and we have
    the following evolution in the training using the ResNet model:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.18 – ResNet training evolution (training loss and validation loss)
    – fine-tuning after transfer learning](img/B16591_07_18.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
- en: Figure 7.18 – ResNet training evolution (training loss and validation loss)
    – fine-tuning after transfer learning
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the accuracy obtained in the test set
    is as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The confusion matrix is as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.19 – Confusion matrix in Dogs vs Cats for a ResNet model with fine-tuning
    after transfer learning](img/B16591_07_19.jpg)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
- en: Figure 7.19 – Confusion matrix in Dogs vs Cats for a ResNet model with fine-tuning
    after transfer learning
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
- en: Compared with our previous experiment of transfer learning, this experiment
    yields worse performance. This is due to a combination of the size of the dataset
    and the hyperparameters chosen. You are encouraged to try your own experiments.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: As can be seen from the results, the image has been correctly classified as
    a cat.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue now with the second fine-tuning experiment where, instead of
    applying transfer learning, we apply fine-tuning directly to the whole model (no
    frozen layers).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to again retrieve the pre-trained ResNet model for *ImageNet-1k*, with
    the following code snippet for MXNet GluonCV:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'And now, without freezing, we can apply the training process, which will update
    all layers of our ResNet model, giving the following loss curves:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.20 – ResNet training evolution (training loss and validation loss)
    – fine-tuning without freezing](img/B16591_07_20.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
- en: Figure 7.20 – ResNet training evolution (training loss and validation loss)
    – fine-tuning without freezing
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the accuracy obtained in the test set
    is as follows:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The value is similar to the previous experiment with transfer learning. For
    the confusion matrix, we have the following:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.21 – Confusion matrix in Dogs vs Cats for a ResNet model with fine-tuning
    without freezing](img/B16591_07_21.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
- en: Figure 7.21 – Confusion matrix in Dogs vs Cats for a ResNet model with fine-tuning
    without freezing
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, compared with our previous fine-tuning experiment, we can see
    how this experiment yields higher performance. Empirically, this has been proven
    to be a repeatable result and has been indicated to be because initially freezing
    the encoder allows for the decoder to learn (using the encoder representations)
    the new task at hand. From an information flow point of view, in this step, there
    is a knowledge transfer from the feature extraction stage to the classifier. In
    the secondary step when the feature extraction stage is unfrozen, the learned
    parameters from the classifier perform auxiliary transfer learning – this time,
    from the classifier to the feature extraction stage.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As can be seen from the results, the image has been correctly classified as
    a cat.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-219
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we applied the techniques of transfer learning and fine-tuning,
    introduced at the beginning of the chapter, to the task of image classification,
    which was also presented previously, in the second recipe, *Classifying images
    with MXNet: GluonCV Model Zoo, AlexNet, and ResNet*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with* *Computer Vision*.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: 'We revisited two known datasets, *ImageNet-1k* and *Dogs vs Cats*, which we
    intended to combine using knowledge transfer based on the former dataset and refining
    that knowledge with the latter. Moreover, this was achieved by leveraging the
    tools that MXNet GluonCV provided:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
- en: A pre-trained ResNet model for *ImageNet-1k*
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools for easy-to-use access to *Dogs* *vs Cats*
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, we continued using the loss functions and metrics introduced for
    image classification, softmax cross-entropy, accuracy, and the confusion matrix.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'Having all these tools readily available within MXNet and GluonCV allowed us
    to run the following experiments with just a few lines of code:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: Training a model from scratch in *Dogs* *vs Cats*
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a pre-trained model to optimize performance via transfer learning from
    *ImageNet-1k* to *Dogs* *vs Cats*
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning our pre-trained model on *Dogs vs Cats* (with and without freezing
    layers)
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After running the different experiments, we obtained an effective tie between
    transfer learning and fine-tuning directly (accuracies of 0.985 and 0.98, respectively).
    The actual results obtained when running these experiments might differ based
    on model architecture, datasets, and hyperparameters chosen, so you are encouraged
    to try out different techniques and variations.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transfer learning, including fine-tuning, is an active field of research. A
    recent paper published in 2022 explores the latest advances in image classification.
    The paper is titled *Deep Transfer Learning for Image Classification: A survey*,
    and can be found here: [https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey](https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey).'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more general approach to CV use cases, a recent paper was published,
    *Transfer Learning Methods as a New Approach in Computer Vision Tasks with Small
    Datasets*, where the problem of small datasets is evaluated, and these techniques
    are applied to solve medical imaging tasks. It can be found here: [https://www.researchgate.net/publication/344943295_Transfer_Learning_Methods_as_a_New_Approach_in_Computer_Vision_Tasks_with_Small_Datasets](https://www.researchgate.net/publication/344943295_Transfer_Learning_Methods_as_a_New_Approach_in_Computer_Vision_Tasks_with_Small_Datasets).'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Improving performance for segmenting images
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will apply transfer learning and fine-tuning to **semantic
    segmentation**, a CV task.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: 'In the fourth recipe, *Segmenting objects in images with MXNet: PSPNet and
    DeepLab-v3*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing Images
    with Computer Vision*, we saw how we could use GluonCV to retrieve pre-trained
    models and use them directly for a semantic segmentation task, effectively leveraging
    past knowledge by using the architecture and the weights/parameters of the pre-trained
    model.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will continue leveraging the weights/parameters of the model,
    obtained for a task consisting of classifying images among a set of 21 classes
    using semantic segmentation models. The dataset used for the pre-training was
    *MS COCO* (source task) and we will run several experiments to evaluate our models
    in a new (target) task, using the *Penn-Fudan Pedestrian* dataset. In these experiments,
    we will also include knowledge from the target dataset to improve our semantic
    classification performance.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-237
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As for previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we will be working with text datasets; therefore, we will revisit
    some concepts already seen in the fourth recipe, *Segmenting objects in images
    with MXNet: PSPNet and DeepLab-v3*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with* *Computer Vision*.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-240
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will be looking at the following steps:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the *MS COCO* and *Penn-Fudan* *Pedestrian* datasets
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training a **DeepLab-v3** model from scratch with *Penn-Fudan Pedestrian*
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a pre-trained DeepLab-v3 model to optimize performance via transfer learning
    from *MS COCO* to *Penn-Fudan Pedestrian*
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tuning our pre-trained DeepLab-v3 model on *Penn-Fudan Pedestrian*
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s look at these steps in detail next.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
- en: Revisiting the MS COCO and Penn-Fudan Pedestrian datasets
  id: totrans-247
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*MS COCO* and *Penn-Fudan Pedestrian* are both object detection and semantic
    segmentation datasets; however, they are quite different. *MS COCO* is a large-scale
    dataset containing ~150k images labeled into 80 classes (21 main ones) and has
    been used extensively in research and academia for benchmarking. *Penn-Fudan Pedestrian*
    is a small-scale dataset containing 170 images of 423 pedestrians. For this recipe,
    we will focus on the semantic segmentation task.'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
- en: MXNet GluonCV does not provide methods to directly download any of the datasets.
    However, we do not need the *MS COCO* dataset (its size is ~19 GB), only the pre-trained
    parameters for our chosen model.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples from *MS COCO*:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.22 – MS COCO example](img/B16591_07_22.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
- en: Figure 7.22 – MS COCO example
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
- en: 'For *Penn-Fudan Pedestrian*, all the information on how to retrieve the dataset
    can be found in the fourth recipe, *Segmenting objects in images with MXNet: PSPNet
    and DeepLab-v3*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing
    Images with Computer Vision*. Taking that recipe’s code as a reference, we can
    display some examples:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.23 – Penn-Fudan Pedestrian dataset examples](img/B16591_07_23.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
- en: Figure 7.23 – Penn-Fudan Pedestrian dataset examples
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: From *Figures 7.22* and *7.23*, we can see how some images from *MS COCO* resemble
    some of the images from *Penn-Fudan Pedestrian*.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: Training a DeepLab-v3 model from scratch with Penn-Fudan Pedestrian
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As described in the fourth recipe, *Segmenting objects in images with MXNet:
    PSPNet and DeepLab-v3*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing
    Images with Computer Vision*, we will be using softmax cross-entropy as the loss
    function and pixel accuracy and **Mean Intersection over Union** (**mIoU**) as
    evaluation metrics.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'By following the code in our recipe, we have the following evolution while
    training from scratch our *DeepLab-v3* model:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.24 – DeepLab-v3 training evolution (training loss and validation
    loss) – training from scratch](img/B16591_07_24.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
- en: Figure 7.24 – DeepLab-v3 training evolution (training loss and validation loss)
    – training from scratch
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the pixel accuracy and mIoU values obtained
    in the test set are as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Even after training for 40 epochs, the evaluation values obtained do not show
    strong performance (an mIoU value of only 0.65).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
- en: 'Qualitatively, we can also check how well our model is performing with an example
    image. In our case, we chose the following:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.25 – Image example of Penn-Fudan Pedestrian for qualitative results](img/B16591_07_25.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: Figure 7.25 – Image example of Penn-Fudan Pedestrian for qualitative results
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'We can check the output of our model by running this image through it with
    the following code snippet:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The preceding code snippet shows the ground truth segmentations and the prediction
    from our model:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.26 – Ground truth and prediction from DeepLab-v3 trained from scratch](img/B16591_07_26.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
- en: Figure 7.26 – Ground truth and prediction from DeepLab-v3 trained from scratch
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen from the results, the pedestrians have only started to be correctly
    segmented. To improve the results, we will need to train for more epochs and/or
    adjust the hyperparameters. However, a better, faster, and simpler approach would
    be to use transfer learning and fine-tuning.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
- en: Using a pre-trained DeepLab-v3 model to optimize performance via transfer learning
    from MS COCO to Penn-Fudan Pedestrian
  id: totrans-274
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous recipe, we trained a new model from scratch using our dataset.
    However, this has three important drawbacks:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
- en: A large amount of data is required for training from scratch.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training process can take a very long time due to the large size of the
    dataset and the number of epochs needed for the model to learn the task.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The compute resources required might be expensive or difficult to procure.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, in this recipe, we will follow a different approach. We will use
    pre-trained models from the MXNet GluonCV Model Zoo to solve the task. These models
    have been trained in *MS COCO*, a dataset that contains the classes we are interested
    in (`person` in this case); therefore, we can use those learned representations
    and easily transfer them to *Penn-Fudan Pedestrian* (same domain).
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: 'For a DeepLab-v3 model, we have the following:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As we can see in the preceding code snippet, following the discussion in this
    chapter’s first recipe, *Understanding Transfer-Learning and Fine-Tuning*, for
    the `pretrained` parameter, we have assigned the value of `True`, indicating that
    we want the pretrained weights to be retrieved (and not only the architecture
    of the model).
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to evaluate adequately the improvements that transfer learning brings,
    we are going to directly evaluate our pre-trained model in our target task (the
    task source is *MS COCO*) before applying transfer learning to *Penn-Fudan Pedestrian*
    and after applying it. Therefore, using our pre-trained model as is, we obtain
    the following:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As we can see, our pre-trained Transformer model is already showing good performance
    values as it is in the same domain. Moreover, the great advantage of using pre-trained
    models is the time savings, as loading a pre-trained model just takes a few lines
    of code.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.27 – Ground truth and prediction from a DeepLab-v3 pre-trained model](img/B16591_07_27.jpg)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
- en: Figure 7.27 – Ground truth and prediction from a DeepLab-v3 pre-trained model
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
- en: 'As can be seen from the results, the pedestrians have been correctly segmented.
    Please note a side advantage of using pre-trained models in *Figure 7**.27*: in
    the ground truth image, the people in the background were not segmented, but the
    pre-trained model correctly picked them up (which might explain the low mIoU values).'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a baseline for comparison, let’s apply transfer learning to
    our task. In the first recipe, *Understanding transfer learning and fine-tuning*,
    the first step was to retrieve a pre-trained model from the MXNet Model Zoo (GluonCV
    or GluonNLP), which we have already done.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
- en: The second step is to remove the last layers (typically, a classifier), keeping
    the parameters in the rest of the layers frozen (not updatable during training),
    so let’s do it!
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: 'We can freeze the *DeepLab-v3* feature extraction layers with the following
    snippet:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Furthermore, we will also need to replace the segmentation task head. Previously,
    it supported 21 classes from *MS COCO*. For our experiments, two classes are enough,
    `background` and `person`. This is done with the following snippet:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now, we can apply the usual training process with *Penn-Fudan Pedestrian*,
    and we have the following evolution in the training using the *DeepLab-v3* model:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.28 – DeepLab-v3 training evolution (training loss and validation
    loss) – transfer learning](img/B16591_07_28.jpg)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
- en: Figure 7.28 – DeepLab-v3 training evolution (training loss and validation loss)
    – transfer learning
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the evaluation metrics obtained in the
    test set are as follows:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Compared with our previous experiments of training from scratch and pre-training,
    this experiment yields slightly better performance, and it took us literally minutes
    to get this model to start working for us in our intended task, whereas the training
    required for the training from scratch experiment took hours and required several
    tries to tune the hyperparameters, which turned into several days of effort in
    total.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.29 – Ground truth and prediction from the DeepLab-v3 pre-trained
    model with transfer learning](img/B16591_07_29.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
- en: Figure 7.29 – Ground truth and prediction from the DeepLab-v3 pre-trained model
    with transfer learning
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen from the results, the pedestrians have been correctly segmented.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning our pre-trained DeepLab-v3 model on Penn-Fudan Pedestrian
  id: totrans-306
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous recipe, we *froze* the parameters in the encoder layers. However,
    with the dataset we are currently working with (*Penn-Fudan Pedestrian*), we can
    *unfreeze* those parameters and train the model, effectively allowing the new
    training process to update the representations (with transfer learning, we were
    working directly with the representations learned for *MS COCO*). As introduced
    in this chapter, this process is called fine-tuning.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two variants of fine-tuning:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: Apply transfer learning by freezing the layers and unfreezing them afterward.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directly apply fine-tuning without the preliminary step of freezing the layers.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s compute both experiments and draw conclusions by comparing the results.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first experiment, we can take the network obtained in the previous
    recipe, unfreeze the layers, and restart the training. In MXNet, to unfreeze the
    encoder parameters, we can run the following snippet:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, we can apply the usual training process with *Penn-Fudan Pedestrian*,
    and we have the following evolution in the training using the *DeepLab-v3* model:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.30 – DeepLab-v3 training evolution (training loss and validation
    loss) – fine-tuning after transfer learning](img/B16591_07_30.jpg)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
- en: Figure 7.30 – DeepLab-v3 training evolution (training loss and validation loss)
    – fine-tuning after transfer learning
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the evaluation metrics obtained in the
    test set are as follows:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Compared with our previous experiment in transfer learning, this experiment
    yields ~3% better performance in mIoU, a very good increase taking into account
    the low training time invested.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.31 – Ground truth and prediction from the DeepLab-v3 pre-trained
    model with fine-tuning after transfer learning](img/B16591_07_31.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
- en: Figure 7.31 – Ground truth and prediction from the DeepLab-v3 pre-trained model
    with fine-tuning after transfer learning
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen from the results, the pedestrians have been correctly segmented.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue now with the second fine-tuning experiment, in which we do not
    apply transfer learning (no frozen layers) and, instead, apply fine-tuning directly
    to the whole model.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to retrieve the pre-trained *DeepLab-v3* model for *MS COCO*, with
    the following code snippet for MXNet GluonCV:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'And now, without freezing, we can apply the training process, which will update
    all layers of our *DeepLab-v3* model:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.32 – DeepLab-v3 training evolution (training loss and validation
    loss) – fine-tuning without freezing](img/B16591_07_32.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
- en: Figure 7.32 – DeepLab-v3 training evolution (training loss and validation loss)
    – fine-tuning without freezing
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the evaluation metrics obtained in the
    test set are as follows:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Compared with our previous fine-tuning experiment, we can see how these experiments
    yield very similar performance. Empirically, it has been proven that this fine-tuning
    experiment can also yield slightly lower results because initially freezing the
    encoder allows for the decoder to learn (using the encoder representations) the
    new task at hand. From a point of view, in this step, there is a knowledge transfer
    from the encoder to the decoder. In a secondary step, when the encoder is unfrozen,
    the learned parameters from the decoder perform auxiliary transfer learning, this
    time from thedecoder to the encoder.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check how well our model is performing qualitatively with the same
    image example and code. The output is given as follows:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.33 – Ground truth and prediction from the DeepLab-v3 pre-trained
    model with fine-tuning without freezing](img/B16591_07_33.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
- en: Figure 7.33 – Ground truth and prediction from the DeepLab-v3 pre-trained model
    with fine-tuning without freezing
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
- en: As can be seen from the results, the pedestrians have been correctly segmented,
    although, as mentioned, if we look at the person on the right, the arm closer
    to the person on the left could be segmented better. As discussed, sometimes this
    version of fine-tuning yields slightly lower results than other approaches.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-337
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we applied the techniques of transfer learning and fine-tuning,
    introduced at the beginning of the chapter, to the task of image classification,
    which was also presented previously, in the fourth recipe, *Segmenting objects
    in images with MXNet: PSPNet and DeepLab-v3*, in [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with* *Computer Vision*.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'We revisited two known datasets, *MS COCO* and *Penn-Fudan Pedestrian*, which
    we intended to combine using knowledge transfer based on the former dataset and
    refining that knowledge with the latter. Moreover, MXNet GluonCV provided the
    following:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: A pre-trained *DeepLab-v3* model for *MS COCO*
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tools for easy-to-use access to *Penn-Fudan Pedestrian*
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, we continued using the loss functions and metrics introduced for
    semantic segmentation, softmax cross-entropy, pixel accuracy, and mIoU.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
- en: 'Having all these tools readily available within MXNet and GluonCV allowed us
    to run the following experiments with just a few lines of code:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: Training a model from scratch with *Penn-Fudan Pedestrian*
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a pre-trained model to optimize performance via transfer learning from
    *MS COCO* to *Penn-Fudan Pedestrian*
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning our pre-trained model on *Penn-Fudan Pedestrian* (with and without
    freezing layers)
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After running the different experiments and taking into account the qualitative
    results and the quantitative results, transfer learning (with a pixel accuracy
    of 0.95 and mIoU of 0.88) has been the best experiment for our task. The actual
    results obtained when running these experiments might differ based on model architecture,
    datasets, and hyperparameters chosen, so you are encouraged to try out different
    techniques and variations.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  id: totrans-348
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Transfer learning, including fine-tuning, is an active field of research. A
    recent paper published in 2022 explores the latest advances in image classification.
    The paper is titled *Deep Transfer Learning for Image Classification: A survey*,
    and can be found here: [https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey](https://www.researchgate.net/publication/360782436_Deep_transfer_learning_for_image_classification_a_survey).'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: 'An interesting paper that combines transfer learning and semantic segmentation
    is *Semantic Segmentation with Transfer Learning for Off-Road Autonomous Driving*,
    in which a change of domain is also studied by the usage of synthetic data. It
    can be found here: [https://www.researchgate.net/publication/333647772_Semantic_Segmentation_with_Transfer_Learning_for_Off-Road_Autonomous_Driving](https://www.researchgate.net/publication/333647772_Semantic_Segmentation_with_Transfer_Learning_for_Off-Road_Autonomous_Driving).'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
- en: 'A more general overview is given in this paper: *Learning Transferable Knowledge
    for Semantic Segmentation with Deep Convolutional Neural Network*, accepted for
    **Computer Vision and Pattern Recognition** (**CVPR**) symposium in 2016\. It
    can be found here: [https://openaccess.thecvf.com/content_cvpr_2016/papers/Hong_Learning_Transferrable_Knowledge_CVPR_2016_paper.pdf](https://openaccess.thecvf.com/content_cvpr_2016/papers/Hong_Learning_Transferrable_Knowledge_CVPR_2016_paper.pdf).'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: Improving performance for translating English to German
  id: totrans-352
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipes, we have seen how we can leverage pre-trained models
    and new datasets for transfer learning and fine-tuning applied to CV tasks. In
    this recipe, we will follow a similar approach, but with an NLP task, translating
    from English to German.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: In the fourth recipe, *Translating text from Vietnamese to English*, in [*Chapter
    6*](B16591_06.xhtml#_idTextAnchor121), *Understanding Text with Natural Language
    Processing*, we saw how we could use GluonNLP to retrieve pre-trained models and
    use them directly for a translation task, training them from scratch, effectively
    only leveraging past knowledge by using the architecture of the pre-trained model.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will also leverage the weights/parameters of the model, obtained
    for a task consisting of translating text from English to German using **machine
    translation** models. The dataset that we will use for pre-training will be *WMT2014*
    (task source), and we will run several experiments to evaluate our models in a
    new (target) task, using the dataset *WMT2016* dataset (with a ~20% increased
    vocabulary of words and sentences for German-English pairs).
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-356
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As for previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we will be working with text datasets; therefore, we will revisit
    some concepts already seen in the fourth recipe, *Understanding text datasets
    – load, manage, and visualize Enron Emails dataset* from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-359
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will be looking at the following steps:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the *WMT2014* and *WMT2016* datasets
  id: totrans-361
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Training a Transformer model from scratch with *WMT2016*
  id: totrans-362
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using a pre-trained Transformer model to optimize performance via transfer learning
    from *WMT2014* to *WMT2016*
  id: totrans-363
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tuning our pre-trained Transformer model on *WMT2016*
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s look at these steps in detail next.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
- en: Introducing the WMT2014 and WMT2016 datasets
  id: totrans-366
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '*WMT2014* and *WMT2016* are multi-modal (multi-language) translation datasets,
    including Chinese, English, and German corpus. *WMT2014* was first introduced
    in 2014 in *Proceedings of the Ninth Workshop on Statistical Machine Translation*,
    as part of the evaluation campaign of the translation models. This workshop was
    upgraded to its own conference in 2016, and *WMT2016* was introduced as part of
    the evaluation campaign of translation models in *Proceedings of the First Conference
    on Machine Translation*. Both datasets are very similar, retrieving information
    from news sources, and the largest difference is the corpus (the size of the vocabulary
    used for both). WMT2014 is about ~140k distinct words, whereas WMT2016 is slightly
    larger with ~150k words, and specifically for German-English pairs, an increase
    of ~20% of words and sentences.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: 'MXNet GluonNLP provides ready-to-use versions of these datasets. For our case,
    we will work with *WMT2016*, which only contains *train* and *test* splits. We
    will further split the test set to obtain *validation* and *test* splits. Here
    is the code to load the dataset:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here is the code to generate *validation* and *test* splits:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'After splitting, our *WMT2016* datasets provide the following data:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: From the large number of instances on each of the datasets, we can confirm that
    these are suitable for our experiments.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
- en: Training a Transformer model from scratch in WMT2016
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As described in the fourth recipe, *Translating text from Vietnamese to English*,
    in [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121), *Understanding Text with Natural
    Language Processing*, we will be using **Perplexity** for our *per-batch computations*
    in training, and **BLEU** for *per-epoch computations*, which will show us the
    evolution of our training process, as part of the typically used training and
    validation losses. We will also use them for quantitative evaluation, and for
    qualitative evaluation, we will choose a sentence (feel free to use any other
    sentence you can come up with).
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
- en: 'We have the following evolution in the training using the Transformer model:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.34 – Transformer training evolution (training loss) – training from
    scratch](img/B16591_07_34.jpg)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
- en: Figure 7.34 – Transformer training evolution (training loss) – training from
    scratch
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied
    by 100) obtained in the test set are as follows:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Current **State-of-the-Art** (**SOTA**) models can yield above 30 points in
    the BLEU score; we reach about halfway with 10 epochs, reaching SOTA performance
    in ~30 epochs.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
- en: 'Qualitatively, we can also check how well our model is performing with a sentence
    example. In our case, we chose: `"I learn new things every day"`, and this can
    be verified with the following code:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'These code statements will give us the following output:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: The German sentence means *I think that’s the case here*; therefore, as can
    be seen from this result, the text has not been correctly translated from English
    to German, and we would need to invest more time in training to achieve the right
    results.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
- en: Using a pre-trained Transformer model to optimize performance via transfer learning
    from WMT2014 to WMT2016
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the previous recipe, we trained a new model from scratch using our dataset.
    However, this has two important drawbacks:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: A large amount of data is required for training from scratch.
  id: totrans-390
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training process can take a very long time due to the large size of the
    dataset and the number of epochs needed for the model to learn the task.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Therefore, in this recipe, we will follow a different approach. We will use
    pre-trained models from MXNet GluonNLP to solve the task. These models have been
    trained on *WMT2014* a very similar dataset, so the representations learned for
    this task can be easily transferred to *WMT2016* (same domain).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: 'For a Transformer model, we have the following:'
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'The output shows us the size of the vocabulary of the *WMT2014* dataset (the
    pre-trained English to German translation task):'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: This is a subset of the whole corpus available for *WMT2014*. As we can also
    see in the preceding code snippet, following the discussion in this chapter’s
    first recipe, *Understanding transfer learning and fine-tuning*, for the `pretrained`
    parameter, we have assigned the value of `True`, indicating that we want the pretrained
    weights to be retrieved (and not only the architecture of the model).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to evaluate adequately the improvements that transfer learning brings,
    we are going to directly evaluate our pre-trained model (task source is *WMT2014*)
    before applying transfer learning to *WMT2016* and after applying it. Therefore,
    using our pre-trained model as is, we obtain the following:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As we can see, our pre-trained Transformer model is already showing very good
    performance values as it is the same domain; however, simply using a pre-trained
    model does not yield SOTA performance, which can be achieved if training from
    scratch. The great advantage of using pre-trained models is the time and compute
    savings as loading a pre-trained model just takes a few lines of code.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check how well our model is performing qualitatively with the same
    sentence example and code. The output is given as follows:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The German sentence means *I learn new things that arise in every case*; therefore,
    as can be seen from the results, the text has not yet been correctly translated
    from English to German, but this time, was much closer than our previous experiment.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a baseline for comparison, let’s apply transfer learning to
    our task. In the first recipe, *Understanding transfer learning and fine-tuning*,
    the first step was to retrieve a pre-trained model from the MXNet Model Zoo (GluonCV
    or GluonNLP), which we have already done.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
- en: The second step is to remove the last layers (typically, a classifier), keeping
    the parameters in the rest of the layers frozen (not updatable during training),
    so let’s do it!
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
- en: 'We can freeze all parameters except the classifier with the following snippet,
    keeping the parameters frozen (we will unfreeze them in a later experiment):'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Now, we can apply the usual training process with *WMT2016*, and we have the
    following evolution in the training using the Transformer model:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.35 – Transformer training evolution (training loss) – transfer learning](img/B16591_07_35.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
- en: Figure 7.35 – Transformer training evolution (training loss) – transfer learning
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied
    by 100) obtained in the test set are as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Compared with our previous experiments, this experiment yields slightly lower
    numerical performance; however, it took us literally minutes to get this model
    to start working for us in our intended task, whereas training from scratch in
    our previous experiment took hours and required several tries to tune the hyperparameters,
    becoming several days of effort in total.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also check how well our model is performing qualitatively with the same
    sentence example and code. The output is given as follows:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: The German sentence means *I learn new things every time*; therefore, as can
    be seen from the results, the text has been almost correctly translated from English
    to German, improving from our previous experiment (pre-trained model), although
    the (better) quantitative results were suggesting otherwise.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: Fine-tuning our pre-trained Transformer model on WMT2016
  id: totrans-417
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous recipe, we froze all the parameters except the classifier. However,
    as the dataset we are currently working with (*WMT2016*) has enough data samples,
    we can unfreeze those parameters and train the model, effectively allowing the
    new training process to update the representations (with transfer learning, we
    were working directly with the representations learned for *WMT2014*). This process,
    as we know, is called fine-tuning.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two variants of fine-tuning:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: Apply transfer learning by freezing the layers and unfreezing them afterward.
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Directly apply fine-tuning without the preliminary step of freezing the layers.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s compute both experiments and draw conclusions by comparing the results.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
- en: 'For the first experiment, we can take the network obtained in the previous
    recipe, unfreeze the layers, and restart the training. In MXNet, to unfreeze the
    encoder parameters, we can run the following snippet:'
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-424
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Now, we can apply the usual training process with *WMT2016*, and we have the
    following evolution in the training using the Transformer model:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.36 – Transformer training evolution (training loss) – fine-tuning
    after transfer learning](img/B16591_07_36.jpg)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
- en: Figure 7.36 – Transformer training evolution (training loss) – fine-tuning after
    transfer learning
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied
    by 100) obtained in the test set are as follows:'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  id: totrans-429
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Compared with our previous experiment in transfer learning, this experiment
    yields a slightly worse quantitative performance.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
- en: 'Qualitatively, we can also check how well our model is performing with a sentence
    example. In our case, we chose `"I learn new things every day"`, and the output
    obtained is as follows:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  id: totrans-432
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The German sentence means *I learn something new every time*; therefore, as
    can be seen from the results, the text has been almost correctly translated from
    English to German.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
- en: Let’s continue now with the second fine-tuning experiment, where we do not apply
    transfer learning (no frozen layers), and instead apply fine-tuning directly to
    the whole model.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to retrieve again the pre-trained Transformer model for *WMT2014*,
    with the following code snippet for MXNet GluonNLP:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'And now, without freezing, we can apply the training process, which will update
    all layers of our Transformer model:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.37 – Transformer training evolution (training loss) – fine-tuning
    without freezing](img/B16591_07_37.jpg)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
- en: Figure 7.37 – Transformer training evolution (training loss) – fine-tuning without
    freezing
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the best iteration, the loss, perplexity, and BLEU score (multiplied
    by 100) obtained in the test set are as follows:'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  id: totrans-441
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Compared with our previous fine-tuning experiment, we can see how this experiment
    yields a slightly higher performance. Empirically, however, the opposite results
    were expected (for this experiment to yield a slightly lower performance). This
    has been proven to be a repeatable result because initially freezing the encoder
    allows for the decoder to learn (using the encoder representations) the new task
    at hand. From a point of view, in this step, there is a knowledge transfer from
    the encoder to the decoder. In a secondary step, when the encoder is unfrozen,
    the learned parameters from the decoder perform auxiliary transfer learning –
    this time, from the decoder to the encoder.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
- en: 'Qualitatively, we can also check how well our model is performing with a sentence
    example. In our case, we chose `"I learn new things every day"`, and the output
    obtained is as follows:'
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  id: totrans-444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: The German sentence means *I learn new things every time*; therefore, as can
    be seen from the results, the text has been almost correctly translated from English
    to German.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
- en: How it works…
  id: totrans-446
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we applied the techniques of transfer learning and fine-tuning,
    introduced at the beginning of the chapter, to the task of machine translation,
    which was also presented previously, in the fourth recipe, *Translating text from
    Vietnamese to English*, in [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121), *Understanding
    Text with Natural* *Language Processing*.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: 'We explored two new datasets, *WMT2014* and *WMT2016*, which, among other language
    pairs, support translations between German and English. Moreover, MXNet GluonNLP
    provided the following:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: A pre-trained Transformer model for *WMT2014*
  id: totrans-449
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A data loader ready to be used with *WMT2016*
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, we continued using the metrics introduced for machine translation,
    perplexity, and BLEU.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: 'Having all these tools readily available within MXNet and GluonNLP allowed
    us to run the following experiments with just a few lines of code:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
- en: Training a model from scratch with *WMT2016*
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using a pre-trained model to optimize performance via transfer learning from
    *WMT2014* to *WMT2016*
  id: totrans-454
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fine-tuning our pre-trained model on *WMT2016* (with and without freezing layers)
  id: totrans-455
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We compared the results and derived the best approach for this particular task,
    which was applying transfer learning and fine-tuning afterward.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  id: totrans-457
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we introduced two new datasets, *WMT2014* and *WMT2016*. These
    datasets were introduced as challenges in the **Workshop on Statistical Machine
    Translation** (**WMT**) conference. The results for 2014 and 2016 are the following:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: '**Findings of the 2014 Workshop on Statistical Machine** **Translation:** https://aclanthology.org/W14-3302.pdf'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Findings of the 2016 Conference on Machine Translation (****WMT16):** [https://aclanthology.org/W16-2301.pdf](https://aclanthology.org/W16-2301.pdf)'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Transfer learning, including fine-tuning, for machine translation is an active
    area of research. A paper published in 2020 explores its applications, titled
    *In Neural Machine Translation, What Does Transfer Learning Transfer?* and can
    be found here: [https://aclanthology.org/2020.acl-main.688.pdf](https://aclanthology.org/2020.acl-main.688.pdf).'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: 'For a more general approach to NLP use cases, a recent paper was published,
    *A Survey on Transfer Learning in Natural Language Processing*, and can be found
    here: [https://www.researchgate.net/publication/342801560_A_Survey_on_Transfer_Learning_in_Natural_Language_Processing](https://www.researchgate.net/publication/342801560_A_Survey_on_Transfer_Learning_in_Natural_Language_Processing).'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
