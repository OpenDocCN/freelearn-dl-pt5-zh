<html><head></head><body>
<div id="_idContainer101">
<h1 class="chapter-number" id="_idParaDest-144"><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.1.1">9</span></h1>
<h1 id="_idParaDest-145"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.2.1">Exploring Unsupervised Deep Learning</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Unsupervised learning works with data that does not have any labels. </span><span class="koboSpan" id="kobo.3.2">More broadly, unsupervised learning aims to uncover the intrinsic patterns hidden within the data. </span><span class="koboSpan" id="kobo.3.3">The most rigorous and expensive part of a supervised machine learning project is the labels required for a given data. </span><span class="koboSpan" id="kobo.3.4">In the real world, there is tons of unlabeled data available with tons of information that could be learned from. </span><span class="koboSpan" id="kobo.3.5">Frankly, it’s impossible to obtain labels for all of the data that exist in the world. </span><span class="koboSpan" id="kobo.3.6">Unsupervised learning is the key to unlocking the potential of the abundant unlabeled digital data we have today. </span><span class="koboSpan" id="kobo.3.7">Let’s explore a hypothetical situation below to understand </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">this better.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">Imagine that it costs 1 USD and 1 minute to obtain a label for a row of data for whatever use case it could be, and a single unit of information can be obtained through supervised learning. </span><span class="koboSpan" id="kobo.5.2">To get 10,000 units of information, 10,000 USD would need to be spent, and 10,000 minutes need to be contributed to obtain 10,000 pieces of labeled data. </span><span class="koboSpan" id="kobo.5.3">Both time and money are painful things to burn. </span><span class="koboSpan" id="kobo.5.4">However, for unsupervised learning, it costs 0 USD and 0 minutes to obtain 0.01 units of information through the same data without a label. </span><span class="koboSpan" id="kobo.5.5">Since the amount of data is not impeded by time or money, we can easily get 100 times more data than the 10,000 samples and get the same information that can be learned through a model. </span><span class="koboSpan" id="kobo.5.6">When money and time aren’t an issue, the amount of information your model can learn is endless, assuming that your unsupervised learning model has the capacity and ability to </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">do so.</span></span></p>
<p><span class="koboSpan" id="kobo.7.1">Deep learning provides a competitive edge to the unsupervised learning field, given the huge capacity and ability to learn complex information. </span><span class="koboSpan" id="kobo.7.2">If we can crack the code of unsupervised deep learning, it will set the stage for models that can come close to general intelligence one day! </span><span class="koboSpan" id="kobo.7.3">In this chapter, we will explore the notable components of unsupervised deep learning by taking a look at the </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.9.1">Exploring unsupervised deep </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">learning applications</span></span></li>
<li><span class="koboSpan" id="kobo.11.1">Creating pretrained network weights for </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">downstream tasks</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Creating general representations through unsupervised </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">deep learning</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Exploring </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">zero-shot learning</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Exploring the dimensionality reduction component of unsupervised </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">deep learning</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Detecting anomalies in </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">external data</span></span></li>
</ul>
<h1 id="_idParaDest-146"><a id="_idTextAnchor151"/><span class="koboSpan" id="kobo.21.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.22.1">This chapter includes some practical implementations in the </span><strong class="bold"><span class="koboSpan" id="kobo.23.1">Python</span></strong><span class="koboSpan" id="kobo.24.1"> programming language. </span><span class="koboSpan" id="kobo.24.2">To complete it, you will need to have a computer with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">libraries installed:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.26.1">pandas</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.27.1">CLIP</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.28.1">numpy</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.29.1">pytorch</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.30.1">pillow</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.31.1">You can find the code files for this chapter on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">at </span></span><a href="https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_9"><span class="No-Break"><span class="koboSpan" id="kobo.33.1">https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_9</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.34.1">.</span></span></p>
<h1 id="_idParaDest-147"><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.35.1">Exploring unsupervised deep learning applications</span></h1>
<p><span class="koboSpan" id="kobo.36.1">Today, practitioners have been able to </span><a id="_idIndexMarker709"/><span class="koboSpan" id="kobo.37.1">leverage unsupervised deep learning to tap into their unlabeled data to achieve either one of the following use cases. </span><span class="koboSpan" id="kobo.37.2">These have been put in descending order in terms of their impact </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">and usefulness:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.39.1">Creating pretrained network weights for </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">downstream tasks</span></span></li>
<li><span class="koboSpan" id="kobo.41.1">Creating general representations that can be used as-is in downstream supervised tasks by predictive </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">supervised models</span></span></li>
<li><span class="koboSpan" id="kobo.43.1">Achieving one-shot and </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">zero-shot learning</span></span></li>
<li><span class="koboSpan" id="kobo.45.1">Performing </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">dimensionality reduction</span></span></li>
<li><span class="koboSpan" id="kobo.47.1">Detect anomalies in </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">external data</span></span></li>
<li><span class="koboSpan" id="kobo.49.1">Clustering the provided training data </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">into groups</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.51.1">To start, note that pure clustering is still a core application of unsupervised learning in general, but not for deep learning. </span><strong class="bold"><span class="koboSpan" id="kobo.52.1">Clustering</span></strong><span class="koboSpan" id="kobo.53.1"> is</span><a id="_idIndexMarker710"/><span class="koboSpan" id="kobo.54.1"> where unlabeled data is grouped into multiple arbitrary clusters or classes. </span><span class="koboSpan" id="kobo.54.2">This will be useful in use cases such as customer segmentation for targeted responses, or topic modeling to figure out trendy topics people are discussing on social media. </span><span class="koboSpan" id="kobo.54.3">In clustering, the relationship between the unlabeled data samples is leveraged to find groups of data that are close together. </span><span class="koboSpan" id="kobo.54.4">Some clustering techniques group this data by assuming a spherical distribution in each cluster, such </span><a id="_idIndexMarker711"/><span class="koboSpan" id="kobo.55.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.56.1">K-means</span></strong><span class="koboSpan" id="kobo.57.1">. </span><span class="koboSpan" id="kobo.57.2">Some other clustering techniques are more adaptive and can find clusters of multiple distributions and</span><a id="_idIndexMarker712"/><span class="koboSpan" id="kobo.58.1"> sizes, such </span><span class="No-Break"><span class="koboSpan" id="kobo.59.1">as </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.60.1">HDBSCAN</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.62.1">Deep learning methods have not been able to produce any significant improvement from non-deep learning clustering methods such as the simple </span><em class="italic"><span class="koboSpan" id="kobo.63.1">k</span></em><span class="koboSpan" id="kobo.64.1">-means algorithm or the HDBSCAN algorithm. </span><span class="koboSpan" id="kobo.64.2">However, efforts have been made to utilize clustering itself to aid the unsupervised pretraining of neural network models. </span><span class="koboSpan" id="kobo.64.3">To realize it in a neural network model as a component, the clustering model has to be differentiable so that gradients can still be propagated to the entire network. </span><span class="koboSpan" id="kobo.64.4">These methods are not superior to non-deep learning techniques such as </span><em class="italic"><span class="koboSpan" id="kobo.65.1">k</span></em><span class="koboSpan" id="kobo.66.1">-means or HDBSCAN and are simply a variation so that the concept of clustering can be realized in a neural network. </span><span class="koboSpan" id="kobo.66.2">An example application of clustering in unsupervised </span><a id="_idIndexMarker713"/><span class="koboSpan" id="kobo.67.1">pretraining is </span><strong class="bold"><span class="koboSpan" id="kobo.68.1">SwaV</span></strong><span class="koboSpan" id="kobo.69.1">, which will be introduced in the next section. </span><span class="koboSpan" id="kobo.69.2">However, for completeness, one example of a neural network-based clustering algorithm that is used traditionally is </span><a id="_idIndexMarker714"/><span class="koboSpan" id="kobo.70.1">self-organizing maps, but the network itself is not considered a </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">deep network.</span></span></p>
<p><span class="koboSpan" id="kobo.72.1">In the next few sections, we will discover the other five applications more comprehensively, ordered by their impact and usefulness, as </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">shown previously.</span></span></p>
<h1 id="_idParaDest-148"><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.74.1">Creating pretrained network weights for downstream tasks</span></h1>
<p><span class="koboSpan" id="kobo.75.1">Also known as</span><a id="_idIndexMarker715"/><span class="koboSpan" id="kobo.76.1"> unsupervised</span><a id="_idIndexMarker716"/><span class="koboSpan" id="kobo.77.1"> transfer learning, this method is analogous to supervised transfer learning and naturally reaps the same benefits as described in the </span><em class="italic"><span class="koboSpan" id="kobo.78.1">Transfer learning</span></em><span class="koboSpan" id="kobo.79.1"> section in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.80.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.81.1">, </span><em class="italic"><span class="koboSpan" id="kobo.82.1">Exploring Supervised Deep Learning</span></em><span class="koboSpan" id="kobo.83.1">. </span><span class="koboSpan" id="kobo.83.2">But as a recap, let’s go through an analogy. </span><span class="koboSpan" id="kobo.83.3">Imagine you’re a chef who has spent years learning how to cook a variety of dishes, from pasta and steak to desserts. </span><span class="koboSpan" id="kobo.83.4">One day, you’re asked to cook a new dish you’ve never tried before; let’s call it “Dish X.” </span><span class="koboSpan" id="kobo.83.5">Instead of starting from scratch, you use your prior knowledge and experience to simplify the process. </span><span class="koboSpan" id="kobo.83.6">You know how to chop vegetables, how to use the oven, and how to adjust the heat, so you don’t have to relearn all of these steps. </span><span class="koboSpan" id="kobo.83.7">You can focus your energy on learning the specific ingredients and techniques required for Dish X This is similar to how transfer learning works in machine learning, which applies to both unsupervised learning and supervised learning. </span><span class="koboSpan" id="kobo.83.8">A model that has already been trained on a related task can be used as a starting point, allowing the model to learn new tasks more quickly </span><span class="No-Break"><span class="koboSpan" id="kobo.84.1">and effectively.</span></span></p>
<p><span class="koboSpan" id="kobo.85.1">Besides being able to use the unlimited amount of unlabeled data available in the world, unsupervised transfer learning also bears another benefit. </span><span class="koboSpan" id="kobo.85.2">Labels in supervised learning often hold biases that the model will adopt. </span><span class="koboSpan" id="kobo.85.3">The biases acquired through learning can obstruct the acquisition of more generalized knowledge that would be more useful for downstream tasks, to varying extents. </span><span class="koboSpan" id="kobo.85.4">Other than biases in the labels, there are also situations where labels are wrong. </span><span class="koboSpan" id="kobo.85.5">Being unsupervised means that the model is stripped of any possibility of learning biases or errors from any labels. </span><span class="koboSpan" id="kobo.85.6">However, note that biases are more prominent in some datasets. </span><span class="koboSpan" id="kobo.85.7">A dataset with a complex task that has quality-related labels derived qualitatively from human judgment tends to have more biases compared to a simple task such as classifying whether a picture has a face </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">or not.</span></span></p>
<p><span class="koboSpan" id="kobo.87.1">Now, let’s dive into the techniques. </span><span class="koboSpan" id="kobo.87.2">Unsupervised transfer learning has been rated as the most impactful and useful application due to contributions that were made in the NLP field. </span><span class="koboSpan" id="kobo.87.3">Transformers, introduced in </span><a href="B18187_06.xhtml#_idTextAnchor092"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.88.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.89.1">, </span><em class="italic"><span class="koboSpan" id="kobo.90.1">Understanding Neural Network Transformers</span></em><span class="koboSpan" id="kobo.91.1">, paved the way for the paradigm to pre-train your model with an unsupervised learning technique more commonly known as </span><strong class="bold"><span class="koboSpan" id="kobo.92.1">self-supervised learning</span></strong><span class="koboSpan" id="kobo.93.1">. </span><span class="koboSpan" id="kobo.93.2">How</span><a id="_idIndexMarker717"/><span class="koboSpan" id="kobo.94.1"> this method is categorized here is a matter of perspective and not everybody would agree with it. </span><span class="koboSpan" id="kobo.94.2">Self-supervised learning leverages only the relationship of co-occurring data to pre-train a neural network with relational knowledge without labels. </span><span class="koboSpan" id="kobo.94.3">Seeing it this way, check out the following question, and decide your </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">own answer.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.96.1">Try it yourself</span></p>
<p class="callout"><span class="koboSpan" id="kobo.97.1">Traditional unsupervised learning data preprocessing techniques such as </span><strong class="bold"><span class="koboSpan" id="kobo.98.1">Principal Component Analysis</span></strong><span class="koboSpan" id="kobo.99.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.100.1">PCA</span></strong><span class="koboSpan" id="kobo.101.1">) leverage</span><a id="_idIndexMarker718"/><span class="koboSpan" id="kobo.102.1"> the relationship of co-occurring data to build new features that can better represent impactful patterns. </span><span class="koboSpan" id="kobo.102.2">Do you see PCA as its own category with self-supervising, under the umbrella of supervised learning, or under the umbrella of </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">unsupervised learning?</span></span></p>
<p class="callout"><em class="italic"><span class="koboSpan" id="kobo.104.1">The author’s </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.105.1">answer: unsupervised</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">!</span></span></p>
<p><span class="koboSpan" id="kobo.107.1">The unsupervised learning techniques that are used in transformers are masked language modeling and next-sentence prediction. </span><span class="koboSpan" id="kobo.107.2">These tasks help the model learn the relationships between words and sentences, allowing it to better understand the meaning and context of language data. </span><span class="koboSpan" id="kobo.107.3">A model that has been trained on masked language modeling and next-sentence prediction tasks can use its understanding of language to perform better on a variety of NLP downstream tasks. </span><span class="koboSpan" id="kobo.107.4">These are proven by the SoTA predictive</span><a id="_idIndexMarker719"/><span class="koboSpan" id="kobo.108.1"> performances </span><a id="_idIndexMarker720"/><span class="koboSpan" id="kobo.109.1">on various datasets from leading transformers today, such as DeBERTa, as introduced in </span><a href="B18187_06.xhtml#_idTextAnchor092"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.110.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.111.1">, </span><em class="italic"><span class="koboSpan" id="kobo.112.1">Understanding Neural </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.113.1">Network Transformers</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.115.1">Let’s briefly go through other </span><a id="_idIndexMarker721"/><span class="koboSpan" id="kobo.116.1">examples of </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">unsupervised pre-training:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.118.1">A simple framework for contrastive learning of visual representations</span></strong><span class="koboSpan" id="kobo.119.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.120.1">SimCLR</span></strong><span class="koboSpan" id="kobo.121.1">): SimCLR utilizes a method </span><a id="_idIndexMarker722"/><span class="koboSpan" id="kobo.122.1">called </span><strong class="bold"><span class="koboSpan" id="kobo.123.1">contrastive learning</span></strong><span class="koboSpan" id="kobo.124.1"> to pretrain </span><a id="_idIndexMarker723"/><span class="koboSpan" id="kobo.125.1">convolutional neural networks. </span><span class="koboSpan" id="kobo.125.2">Contrastive learning is a key technique in unsupervised deep learning that helps neural networks learn representations of data by optimizing the distance between related features. </span><span class="koboSpan" id="kobo.125.3">The core idea behind contrastive learning is to bring features of similar data points closer together and push features of dissimilar data points further apart in the feature space, using a contrastive loss function. </span><span class="koboSpan" id="kobo.125.4">While there are various forms of contrastive loss today, the general idea is to minimize the distance between similar examples and maximize the distance between dissimilar examples. </span><span class="koboSpan" id="kobo.125.5">This distance can be measured in various ways, such as Euclidean distance or cosine distance. </span><span class="koboSpan" id="kobo.125.6">Although this method requires labels for learning and is technically a supervised learning loss, the features learned, along with the selection of similar and dissimilar samples for label-free samples, make this loss function a crucial technique in unsupervised deep learning. </span><span class="koboSpan" id="kobo.125.7">The simplest representation of such a contrastive loss is </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">as follows:</span></span><ul><li><span class="koboSpan" id="kobo.127.1">For </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">dissimilar samples:</span></span></li></ul></li>
</ul>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.129.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.130.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.131.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.132.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.133.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.134.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.135.1">d</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.136.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.137.1">s</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.138.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.139.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.140.1">n</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.141.1">c</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.142.1">e</span></span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.143.1">For </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">similar samples:</span></span></li>
</ul>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.145.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.146.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.147.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.148.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.149.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.150.1">d</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.151.1">i</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.152.1">s</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.153.1">t</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.154.1">a</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.155.1">n</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.156.1">c</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.157.1">e</span></span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.158.1">    SimCLR focuses on image data and uses crafted image augmentation techniques to generate image pairs that could optimize the network to produce closer features. </span><span class="koboSpan" id="kobo.158.2">Random cropping and random color distortions are the most general augmentations that can be useful setups for most image datasets to perform unsupervised pre-training </span><span class="No-Break"><span class="koboSpan" id="kobo.159.1">with SimCLR.</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.160.1">Swapping assignments between multiple views of the same image</span></strong><span class="koboSpan" id="kobo.161.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.162.1">SwaV</span></strong><span class="koboSpan" id="kobo.163.1">): SwaV adopts a similar concept to</span><a id="_idIndexMarker724"/><span class="koboSpan" id="kobo.164.1"> SimCLR in utilizing image augmentations with convolutional neural networks. </span><span class="koboSpan" id="kobo.164.2">It also uses the concept of clustering and embeddings to optimize the model to produce features that make sure the two images are mapped to the same </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">feature space.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.166.1">The learning technique is executed </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">as follows:</span></span></p><ol><li class="upper-roman"><span class="koboSpan" id="kobo.168.1">A pre-set amount of cluster number is determined, </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">called </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.170.1">K</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.172.1">Featurize the two images with the same convolutional </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">neural network.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.174.1">The two sets of features will then be assigned independently to specific clusters by using an external technique called Optimal Transport Solver using an embedding layer that represents the representative features of the </span><em class="italic"><span class="koboSpan" id="kobo.175.1">K</span></em><span class="koboSpan" id="kobo.176.1"> clusters. </span><span class="koboSpan" id="kobo.176.2">Two features will always be assigned to </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">different clusters.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.178.1">Dot products between the convolutional features and all the cluster embeddings are computed and a softmax operation </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">is applied.</span></span></li><li class="upper-roman"><span class="koboSpan" id="kobo.180.1">The assigned clusters for both image features will then be swapped, where cross-entropy between the swapped cluster assignments and the resulting values from the softmax operation will be used to optimize the weights of both the CNN and the </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">embeddings layer.</span></span></li></ol><p class="list-inset"><span class="koboSpan" id="kobo.182.1">     The idea is to</span><a id="_idIndexMarker725"/><span class="koboSpan" id="kobo.183.1"> jointly learn the embedding weights and convolutional network weights that consistently categorize</span><a id="_idIndexMarker726"/><span class="koboSpan" id="kobo.184.1"> together multiple augmentations of the same image. </span><span class="koboSpan" id="kobo.184.2">The technique can be described as contrastive clustering. </span><span class="koboSpan" id="kobo.184.3">Both SwaV and SimCLR are competitively close in multiple downstream </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">task performances.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.186.1">SEER</span></strong><span class="koboSpan" id="kobo.187.1">: SEER is the </span><a id="_idIndexMarker727"/><span class="koboSpan" id="kobo.188.1">combination of SwaV, an extremely high amount of unlabeled data for images at the billion scales instead of </span><a id="_idIndexMarker728"/><span class="koboSpan" id="kobo.189.1">the more common million scale, and using high-capacity models to pre-train using random, uncurated, and unlabeled images. </span><span class="koboSpan" id="kobo.189.2">This allowed SEER to achieve SoTA downstream supervised task performance and outperformed both SimCLR and </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">SwaV alone.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.191.1">UP-DETR, by the researchers from SCTU and Tencent Wechat AI</span></strong><span class="koboSpan" id="kobo.192.1">: This method pre-trains the transformer that has an encoder-decoder architecture with CNN features for image object detection tasks in an unsupervised way. </span><span class="koboSpan" id="kobo.192.2">UP-DETR managed to improve the performance of transformers on downstream supervised image object detection datasets. </span><span class="koboSpan" id="kobo.192.3">The interesting thing to remember here is that it structured the network in a way that allowed random image patches to be fed separately to the decoder to predict the bounding box of these patches on the original image. </span><span class="koboSpan" id="kobo.192.4">The original image is fed into the encoder part of the transformer and combined with the random image patches at the </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">decoder part.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.194.1">Wav2vec 2.0</span></strong><span class="koboSpan" id="kobo.195.1">: Wav2vec 2.0 showed how feasible it is to train a reliable speech recognition model with limited amounts of labeled data by leveraging self-supervised pretraining as a pretext task. </span><span class="koboSpan" id="kobo.195.2">It also uses contrastive, loss that is simply the cosine similarity between samples. </span><span class="koboSpan" id="kobo.195.3">The method uses CNNs as an audio feature extractor and quantizes the representations into a discrete array of values that can be trained before passing it into a transformer. </span><span class="koboSpan" id="kobo.195.4">The unsupervised tasks of masked speech modeling</span><a id="_idIndexMarker729"/><span class="koboSpan" id="kobo.196.1"> and contrastive loss are applied here. </span><span class="koboSpan" id="kobo.196.2">Let’s look at how these two methods can </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">be combined:</span></span><ol><li class="lower-roman"><span class="koboSpan" id="kobo.198.1">A random location of the quantized latent speech representations </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">is masked.</span></span></li><li class="lower-roman"><span class="koboSpan" id="kobo.200.1">The transformers output at the same location of the masked quantized latent speech representation will be used as the prediction of the missing masked quantized latent </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">speech representation.</span></span></li><li class="lower-roman"><span class="koboSpan" id="kobo.202.1">A few parts of the non-masked quantized latent representations of the same sample will be used to compute the contrastive loss against the predicted missing masked quantized latent speech representation, which effectively enforces parts of the same audio sample to be in a similar </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">latent domain.</span></span></li></ol><p class="list-inset"><span class="koboSpan" id="kobo.204.1">This process </span><a id="_idIndexMarker730"/><span class="koboSpan" id="kobo.205.1">is </span><a id="_idIndexMarker731"/><span class="koboSpan" id="kobo.206.1">demonstrated in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.207.1">Figure 9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.208.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">:</span></span></p><div class="IMG---Figure" id="_idContainer097"><span class="koboSpan" id="kobo.210.1"><img alt="" role="presentation" src="image/B18187_09_01.jpg"/></span></div></li>
</ul>
<p class="IMG---Figure"><span class="koboSpan" id="kobo.211.1">To PD: This image is sent for redraw.</span></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.212.1">Figure 9.1 – Wav2vec 2.0 model structure</span></p>
<p><span class="koboSpan" id="kobo.213.1">The methods we’ve introduced here were meant for a specific modality. </span><span class="koboSpan" id="kobo.213.2">However, with some effort methods, they can be adapted to other modalities to replicate performance. </span><span class="koboSpan" id="kobo.213.3">For example, augmentations were used for image-based methods along with contrastive learning. </span><span class="koboSpan" id="kobo.213.4">To adapt this method to text-based modality, using text augmentations that preserve the meaning, such as word replacement or back translation, should work well. </span><span class="koboSpan" id="kobo.213.5">The modalities involved in the unsupervised methods introduced here were images, text data, and audio data. </span><span class="koboSpan" id="kobo.213.6">These modalities were chosen due to their generalizability factor for downstream tasks. </span><span class="koboSpan" id="kobo.213.7">Other forms of modality, such as graph data or numerical data, are highly customizable to individual use cases where there is fundamentally no information that can be transferred to downstream tasks. </span><span class="koboSpan" id="kobo.213.8">Before you attempt to run an unsupervised deep learning method to create pre-trained weights, consider listing the information that can be transferred and evaluate qualitatively whether it makes sense </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">to proceed.</span></span></p>
<p><span class="koboSpan" id="kobo.215.1">But what if the pre-trained network weights are already capable of producing very generalizable features across different domains? </span><span class="koboSpan" id="kobo.215.2">Just like how CNNs trained on the ImageNet dataset in a supervised way can be used as feature extractors, there is no limiting the immediate usage of networks trained by unsupervised </span><a id="_idIndexMarker732"/><span class="koboSpan" id="kobo.216.1">methods </span><a id="_idIndexMarker733"/><span class="koboSpan" id="kobo.217.1">such as SwaV or Wav2vec 2.0 as feature extractors. </span><span class="koboSpan" id="kobo.217.2">Feel free to try it out yourself! </span><span class="koboSpan" id="kobo.217.3">However, a few unsupervised learning techniques use neural networks that are made to use their generated features instead of their weights directly. </span><span class="koboSpan" id="kobo.217.4">In the next section, we will discover </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">exactly that.</span></span></p>
<h1 id="_idParaDest-149"><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.219.1">Creating general representations through unsupervised deep learning</span></h1>
<p><span class="koboSpan" id="kobo.220.1">The representations </span><a id="_idIndexMarker734"/><span class="koboSpan" id="kobo.221.1">that are learned through unsupervised deep learning can be directly used as-is in downstream supervised tasks by predictive supervised models or consumed directly by end users. </span><span class="koboSpan" id="kobo.221.2">There are a handful of generally impactful unsupervised methods that utilize neural networks that are meant to be used primarily as feature extractors. </span><span class="koboSpan" id="kobo.221.3">Let’s take a look at a couple of unsupervised </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">feature extractors:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.223.1">Unsupervised pre-trained word tokenizers</span></strong><span class="koboSpan" id="kobo.224.1">: These</span><a id="_idIndexMarker735"/><span class="koboSpan" id="kobo.225.1"> are used heavily by variants of the transformers architecture and </span><a id="_idIndexMarker736"/><span class="koboSpan" id="kobo.226.1">were introduced in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.227.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.228.1">, </span><em class="italic"><span class="koboSpan" id="kobo.229.1">Exploring Supervised Deep Learning,</span></em><span class="koboSpan" id="kobo.230.1"> in the </span><em class="italic"><span class="koboSpan" id="kobo.231.1">Representing text data for supervised deep </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.232.1">learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.233.1"> section.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.234.1">Unsupervised pre-trained word embeddings</span></strong><span class="koboSpan" id="kobo.235.1">: These methods leverage unsupervised learning and attempt to </span><a id="_idIndexMarker737"/><span class="koboSpan" id="kobo.236.1">perform language modeling, similar to masked language modeling in transformers. </span><span class="koboSpan" id="kobo.236.2">However, word embeddings-based methods have been overtaken by transformer-based pretraining with sub-word-based text tokenization in terms of metric performance. </span><span class="koboSpan" id="kobo.236.3">The word embeddings method still stays relevant today due to the runtime efficiency it has over transformer-based methods. </span><span class="koboSpan" id="kobo.236.4">Note that not every project has the GPU available that is needed to run a big transformer in a reasonable runtime. </span><span class="koboSpan" id="kobo.236.5">Some projects only have access to CPU processing, and word embeddings provide the perfect inference runtime versus metric performance tradeoff. </span><span class="koboSpan" id="kobo.236.6">Additionally, word embeddings are a natural solution for some use cases that require words as a result, such as word-to-word translation from one language into another language, or even finding synonyms or antonyms. </span><span class="koboSpan" id="kobo.236.7">Examples of methods that produce pre-trained word embeddings are </span><strong class="bold"><span class="koboSpan" id="kobo.237.1">fastText</span></strong><span class="koboSpan" id="kobo.238.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.239.1">word2vec</span></strong><span class="koboSpan" id="kobo.240.1">. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.241.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.242.1">.2</span></em><span class="koboSpan" id="kobo.243.1"> exemplifies</span><a id="_idIndexMarker738"/><span class="koboSpan" id="kobo.244.1"> the </span><a id="_idIndexMarker739"/><span class="koboSpan" id="kobo.245.1">architecture of word </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">embedding methods:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer098">
<span class="koboSpan" id="kobo.247.1"><img alt="Figure 9.2 – The word embeddings architecture with a two-layer MLP and trainable embeddings" src="image/B18187_09_02.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.248.1">Figure 9.2 – The word embeddings architecture with a two-layer MLP and trainable embeddings</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.249.1">The task is either to predict the middle word based on the summed embeddings of the surrounding words or to predict the surrounding words based on the embeddings of the current word. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.250.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.251.1">.2</span></em><span class="koboSpan" id="kobo.252.1"> shows the former case. </span><span class="koboSpan" id="kobo.252.2">After training the word embeddings with the MLP with a dictionary of N wor</span><a id="_idIndexMarker740"/><span class="koboSpan" id="kobo.253.1">ds, the MLP is then dumped, and the word embeddings are saved as a dictionary for simple look-up utilization during inference. </span><span class="koboSpan" id="kobo.253.2">FastText differs from word2vec by using not words but subwords to generate embeddings and thus can better handle missing words. </span><span class="koboSpan" id="kobo.253.3">A word embedding for FastText is produced by summing up embeddings of subwords that form the full</span><a id="_idIndexMarker741"/><span class="koboSpan" id="kobo.254.1"> word. </span><span class="koboSpan" id="kobo.254.2">Go to </span><a href="https://github.com/facebookresearch/fastText"><span class="koboSpan" id="kobo.255.1">https://github.com/facebookresearch/fastText</span></a><span class="koboSpan" id="kobo.256.1"> to learn how to use word embeddings that have pre-trained for 157 languages, or how to pre-train FastText embeddings on your </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">custom dataset!</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.258.1">Autoencoders</span></strong><span class="koboSpan" id="kobo.259.1">: Autoencoders </span><a id="_idIndexMarker742"/><span class="koboSpan" id="kobo.260.1">are</span><a id="_idIndexMarker743"/><span class="koboSpan" id="kobo.261.1"> encoder-decoder architectures that can be trained to denoise data and reduce the dimensionality of the data while optimizing it to be reconstructible. </span><span class="koboSpan" id="kobo.261.2">They are generally trained to extract useful and core information by limiting the number of features in the bottleneck section of the architecture right after the encoder and right before the decoder. </span><span class="koboSpan" id="kobo.261.3">Go back to </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.262.1">Chapter 5</span></em></span><span class="koboSpan" id="kobo.263.1">, </span><em class="italic"><span class="koboSpan" id="kobo.264.1">Understanding Autoencoders</span></em><span class="koboSpan" id="kobo.265.1">, to find </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">out more!</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.267.1">Contrastive Language-Image Pretraining</span></strong><span class="koboSpan" id="kobo.268.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.269.1">CLIP</span></strong><span class="koboSpan" id="kobo.270.1">): CLIP is a method that trains a text language transformer encoder and a CNN image encoder with contrastive learning. </span><span class="koboSpan" id="kobo.270.2">It provides a dataset consisting of around 400 million</span><a id="_idIndexMarker744"/><span class="koboSpan" id="kobo.271.1"> image text pairs constructed by combining multiple publicly available datasets. </span><span class="koboSpan" id="kobo.271.2">This method produces a powerful image and text feature encoder that can be used independently. </span><span class="koboSpan" id="kobo.271.3">This method became a main part of the current SoTA of text-to-image methods by assisting during training to optimize to generate an image that has CLIP-encoded image embeddings</span><a id="_idIndexMarker745"/><span class="koboSpan" id="kobo.272.1"> close to the CLIP-encoded text embeddings. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.273.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.274.1">.3</span></em><span class="koboSpan" id="kobo.275.1"> shows the </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">CLIP architecture:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer099">
<span class="koboSpan" id="kobo.277.1"><img alt="Figure 9.3 – CLIP architecture" src="image/B18187_09_03.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.278.1">Figure 9.3 – CLIP architecture</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.279.1">A small implementation</span><a id="_idIndexMarker746"/><span class="koboSpan" id="kobo.280.1"> detail here is that the outputs of the image encoder and text encoder are fed separately to a linear layer so that the number of features matches up for contrastive learning loss to be computed. </span><span class="koboSpan" id="kobo.280.2">Specifically, for CLIP, the contrastive loss is applied in the </span><span class="No-Break"><span class="koboSpan" id="kobo.281.1">following way:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.282.1">
cosine_similarity = np.dot(Image_embeddings, Text_embeddings.T) * np.exp(learned_variable)
labels = np.arange(number_of_samples)
loss_image = cross_entropy_loss(cosine_similarity, labels, axis=0)
loss_text = cross_entropy_loss(cosine_similarity, labels, axis=1)
loss = (loss_image + loss_text)/2</span></pre> <p class="list-inset"><span class="koboSpan" id="kobo.283.1">Notice that cross-entropy is still applied after applying pairwise cosine similarity between the image and text embeddings, which exemplifies the many variations of contrastive loss, where the core workhorse still comes down to using the distance metric. </span><span class="koboSpan" id="kobo.283.2">Since CLIP leverages co-occurring data, it does not belong to the self-supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">learning sub-category.</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.285.1">Generative models</span></strong><span class="koboSpan" id="kobo.286.1">: Let’s look at some examples of </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">these models:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.288.1">Transformer models</span></strong><span class="koboSpan" id="kobo.289.1">. </span><span class="koboSpan" id="kobo.289.2">Examples </span><a id="_idIndexMarker747"/><span class="koboSpan" id="kobo.290.1">include </span><a id="_idIndexMarker748"/><span class="koboSpan" id="kobo.291.1">GPT-3 and ChatGPT. </span><span class="koboSpan" id="kobo.291.2">Both are transformer models that are trained with masked language modeling and next-sentence prediction tasks. </span><span class="koboSpan" id="kobo.291.3">The difference with ChatGPT is that it is fine-tuned using reinforcement learning through human feedback and training. </span><span class="koboSpan" id="kobo.291.4">Both generate new text data by predicting in an </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">autoregressive manner.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.293.1">Text to image generators</span></strong><span class="koboSpan" id="kobo.294.1">. </span><span class="koboSpan" id="kobo.294.2">Examples </span><a id="_idIndexMarker749"/><span class="koboSpan" id="kobo.295.1">include DALL.E 2 and Stable Diffusion. </span><span class="koboSpan" id="kobo.295.2">Both methods utilize the diffusion model. </span><span class="koboSpan" id="kobo.295.3">At a high level, the Stable Diffusion method slowly generates an extremely high-quality image from a base image full </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">of noise.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.297.1">Generative adversarial networks</span></strong><span class="koboSpan" id="kobo.298.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.299.1">GANs</span></strong><span class="koboSpan" id="kobo.300.1">): GANs </span><a id="_idIndexMarker750"/><span class="koboSpan" id="kobo.301.1">utilize two neural network components during </span><a id="_idIndexMarker751"/><span class="koboSpan" id="kobo.302.1">training, called </span><strong class="bold"><span class="koboSpan" id="kobo.303.1">discriminators</span></strong><span class="koboSpan" id="kobo.304.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.305.1">generators</span></strong><span class="koboSpan" id="kobo.306.1">. </span><span class="koboSpan" id="kobo.306.2">The discriminator</span><a id="_idIndexMarker752"/><span class="koboSpan" id="kobo.307.1"> is a classifier that is meant to discern fake from real images. </span><span class="koboSpan" id="kobo.307.2">The generator and discriminator are trained iteratively until a point where the discriminator can’t discern that the image generated by the generator is fake. </span><span class="koboSpan" id="kobo.307.3">GANs can generate good-quality images but are generally surpassed by diffusion-based models as they produce much </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">higher-quality images.</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.309.1">It’s important to note that the methods to learn and create feature representations with unsupervised learning are not limited to those based on neural networks. </span><span class="koboSpan" id="kobo.309.2">PCA and TF-IDF, which are considered data pre-processing techniques, also belong in this category. </span><span class="koboSpan" id="kobo.309.3">However, the key difference between them and deep learning-based methods is that the latter require more training time but offer better </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">generalization capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.311.1">The key in unsupervised representation learning is leveraging relationships between co-occurring data. </span><span class="koboSpan" id="kobo.311.2">The techniques that were introduced in the last two sections have public repositories that can be utilized immediately. </span><span class="koboSpan" id="kobo.311.3">For </span><a id="_idIndexMarker753"/><span class="koboSpan" id="kobo.312.1">most of them, there are already pre-trained weights that you can leverage out of the box to either fine-tune further on downstream supervised tasks or use as plain feature extractors. </span><span class="koboSpan" id="kobo.312.2">In the next section, we will explore a special type of utilization of CLIP called </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">zero-shot learning.</span></span></p>
<h1 id="_idParaDest-150"><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.314.1">Exploring zero-shot learning</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.315.1">Zero-shot learning</span></strong><span class="koboSpan" id="kobo.316.1"> is a</span><a id="_idIndexMarker754"/><span class="koboSpan" id="kobo.317.1"> paradigm that involves utilizing a trained machine learning model to tackle new tasks without training and learning directly on the new task. </span><span class="koboSpan" id="kobo.317.2">The method implements transfer learning at its core but instead of requiring additional learning in the downstream task, no learning is done. </span><span class="koboSpan" id="kobo.317.3">The method that we will be using to realize zero-shot learning here is CLIP as a base and thus is an extension of an unsupervised </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">learning method.</span></span></p>
<p><span class="koboSpan" id="kobo.319.1">CLIP can be used to perform zero-shot learning on a wide variety of downstream tasks. </span><span class="koboSpan" id="kobo.319.2">To recap, CLIP is pre-trained with the task of image-text retrieval. </span><span class="koboSpan" id="kobo.319.3">So long as CLIP is applied to downstream tasks without any additional learning process, it can be considered as zero-shot learning. </span><span class="koboSpan" id="kobo.319.4">The tested use cases include tasks such as object character recognition, action recognition in videos, geo-localization based on images, and many types of fine-grained image object classification. </span><span class="koboSpan" id="kobo.319.5">Additionally, there are basic ways people have been testing and giving demos on zero-shot learning for </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">object detection.</span></span></p>
<p><span class="koboSpan" id="kobo.321.1">In this chapter, we will implement a non-documented zero-shot application of CLIP, which is image object counting. </span><span class="koboSpan" id="kobo.321.2">Counting means the model will be performing regression. </span><span class="koboSpan" id="kobo.321.3">Let’s start </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">the implementation:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.323.1">First, let’s import all the necessary libraries. </span><span class="koboSpan" id="kobo.323.2">We will be using the open source </span><strong class="source-inline"><span class="koboSpan" id="kobo.324.1">clip</span></strong><span class="koboSpan" id="kobo.325.1"> library from </span><a href="https://github.com/openai/CLIP"><span class="koboSpan" id="kobo.326.1">https://github.com/openai/CLIP</span></a><span class="koboSpan" id="kobo.327.1"> to utilize a pretrained version of CLIP using a visual </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">transformer model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.329.1">
import os
from tqdm import tqdm
import numpy as np
import clip
import torch
from PIL import Image
import pandas as pd</span></pre></li> <li><span class="koboSpan" id="kobo.330.1">Next, we will load the pretrained CLIP model in either CPU mode or GPU mode if the CUDA toolkit </span><span class="No-Break"><span class="koboSpan" id="kobo.331.1">is installed:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.332.1">
device = "cuda" if torch.cuda.is_available() else "cpu"
model, preprocess = clip.load('ViT-B/32', device)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.333.1">The model variable we</span><a id="_idIndexMarker755"/><span class="koboSpan" id="kobo.334.1"> loaded here is a class that contains the individual methods to encode images and text with the encoder model loaded with pretrained weights. </span><span class="koboSpan" id="kobo.334.2">Additionally, the preprocess variable is a method that needs to be executed on the input before it’s fed to the image encoder. </span><span class="koboSpan" id="kobo.334.3">It performs normalization on the data that was used </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">during training.</span></span></p></li> <li><span class="koboSpan" id="kobo.336.1">To predict with this model more conveniently, we will create a helper method with the </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">following structure.</span></span><pre class="source-code"><span class="koboSpan" id="kobo.338.1">
def predict_with_pil_image(image_input, clip_labels, top_k=5):
  return similarity_score_probability, indices</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.339.1">Remember that CLIP is trained to generate the image encoder and text encoder outputs in a way that they’re mapped into the same feature space. </span><span class="koboSpan" id="kobo.339.2">Matching text descriptions to an image will produce text and image features that are close together in distance. </span><span class="koboSpan" id="kobo.339.3">Since the metric we used is cosine similarity, the higher the similarity value, the lower the distance. </span><span class="koboSpan" id="kobo.339.4">The main technique to realize zero-shot learning from CLIP is to think of multiple descriptions that represent a certain label and choose the label that has the highest similarity score against an image. </span><span class="koboSpan" id="kobo.339.5">The similarity score will then be normalized against all the other labels to obtain a probability score. </span><span class="koboSpan" id="kobo.339.6">Additionally, </span><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">top_k</span></strong><span class="koboSpan" id="kobo.341.1"> is used to control how many top highest similarity score text description indices and scores to return. </span><span class="koboSpan" id="kobo.341.2">We will come back to how to design the descriptions objectively for zero-shot learning later, once we’ve defined the code that will belong in the predict method we </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">defined previously.</span></span></p></li> <li><span class="koboSpan" id="kobo.343.1">The first part of this method will be to preprocess the provided single image input and multiple text descriptions, called </span><strong class="source-inline"><span class="koboSpan" id="kobo.344.1">clip_labels</span></strong><span class="koboSpan" id="kobo.345.1">. </span><span class="koboSpan" id="kobo.345.2">The image will be preprocessed according to the provided preprocessor, while the multiple text descriptions will be tokenized according to the sub-word tokenizer used in the text transformer provided by the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.346.1">clip</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.347.1"> library:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.348.1">
image_input = preprocess(cars_image).unsqueeze(0).to(
device)
text_inputs = torch.cat([clip.tokenize(cl) for cl in clip_labels]).to(device)</span></pre></li> <li><span class="koboSpan" id="kobo.349.1">Next, we will encode the preprocessed image and text inputs using the image encoder and text </span><span class="No-Break"><span class="koboSpan" id="kobo.350.1">encoder, respectively:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.351.1">
with torch.no_grad():
  image_features = model.encode_image(image_input)
  text_features = model.encode_text(text_inputs)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.352.1">In PyTorch, remember that we need to make sure no gradients are computed during inference mode as we are not training the model and don’t want that extra computation or </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">RAM wastage.</span></span></p></li> <li><span class="koboSpan" id="kobo.354.1">Now that the image and </span><a id="_idIndexMarker756"/><span class="koboSpan" id="kobo.355.1">text features with the same column dimensions have been extracted, we will compute the cosine similarity score of the provided image input features against all the text description </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">label features:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.357.1">
image_features /= image_features.norm(dim=-1, keepdim=True)
text_features /= text_features.norm(dim=-1, keepdim=True)
similarity_score_probability = (100.0 * image_features @ text_features.T).softmax(dim=-1)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.358.1">In addition to the similarity score, the similarity among all the text descriptions is normalized using Softmax so that the scores will all add up to one. </span><span class="koboSpan" id="kobo.358.2">View this as a way to compare the similarity scores against other samples. </span><span class="koboSpan" id="kobo.358.3">This will essentially convert distance scores into a multiclass prediction setting where the provided text descriptions as labels are all the </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">possible classes.</span></span></p></li> <li><span class="koboSpan" id="kobo.360.1">Next, we will extract the </span><strong class="source-inline"><span class="koboSpan" id="kobo.361.1">top_k</span></strong><span class="koboSpan" id="kobo.362.1"> highest similarity score and return their similarity score probability and indices to indicate which label the score </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">belongs to:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.364.1">
percentages, indices = similarity[0].topk(5)
similarity_score_probability = percentages.numpy() * 100.0
indices = indices.numpy()</span></pre></li> <li><span class="koboSpan" id="kobo.365.1">Now that the method is complete, we can load an image using Pillow, create some descriptions, and feed it into the method to perform zero-shot learning! </span><span class="koboSpan" id="kobo.365.2">In this tutorial, we will work on object counting. </span><span class="koboSpan" id="kobo.365.3">This can range from counting how many cars to properly account for parking availability to counting how many people to account for personnel that are required to service the people. </span><span class="koboSpan" id="kobo.365.4">In this tutorial, we will count the number of paper clips as a pet project. </span><span class="koboSpan" id="kobo.365.5">Note that this can easily be extended to other counting datasets and projects. </span><span class="koboSpan" id="kobo.365.6">We will use the dataset at </span><a href="https://www.kaggle.com/datasets/jeffheaton/count-the-paperclips?resource=download"><span class="koboSpan" id="kobo.366.1">https://www.kaggle.com/datasets/jeffheaton/count-the-paperclips?resource=download</span></a><span class="koboSpan" id="kobo.367.1"> to achieve this. </span><span class="koboSpan" id="kobo.367.2">Be sure to download the dataset in the same folder where the code exists. </span><span class="koboSpan" id="kobo.367.3">Let’s load up a simple version with fewer paper clips </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">than 2:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.369.1">
image = Image.open('clips-data-2020/clips/clips-25001.png')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.370.1">We will get the </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">following output:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer100">
<span class="koboSpan" id="kobo.372.1"><img alt="Figure 9.4 – Example paper clip counting from easy (A), medium (B), to hard (C)" src="image/B18187_09_04.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.373.1">Figure 9.4 – Example paper clip counting from easy (A), medium (B), to hard (C)</span></p>
<p class="list-inset"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.374.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.375.1">.4 B</span></em><span class="koboSpan" id="kobo.376.1"> and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.377.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.378.1">.4 C</span></em><span class="koboSpan" id="kobo.379.1"> are harder examples that we will explore after we go through predictions for </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.380.1">Figure </span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.381.1">9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.382.1">.4 A</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">.</span></span></p>
<ol>
<li value="9"><span class="koboSpan" id="kobo.384.1">Now, we need multiple sets</span><a id="_idIndexMarker757"/><span class="koboSpan" id="kobo.385.1"> of text descriptions to account for all the possible counts of paper clips. </span><span class="koboSpan" id="kobo.385.2">A simple description that just uses numbers as text is not descriptive enough to even get close to a good similarity score. </span><span class="koboSpan" id="kobo.385.3">Instead, let’s add some additional text along with the number, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">following code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.387.1">
raw_labels = list(range(100))
clip_labels = ['{} number of paper clips where some paper clips are partially occluded'.format(label) for label in raw_labels]</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.388.1">In this code, we use all numbers from 0 to 99 with the same surrounding text. </span><span class="koboSpan" id="kobo.388.2">More effort can be put into designing more variations of the pretext needed. </span><span class="koboSpan" id="kobo.388.3">It is also possible to utilize multiple pretexts with the same raw labels. </span><span class="koboSpan" id="kobo.388.4">The more descriptive the text is of the image, the more likely there will be a description that produces the closest features to </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">the image.</span></span></p></li> <li><span class="koboSpan" id="kobo.390.1">Let’s run a prediction on this example and see how </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">it performs:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.392.1">
percentages, indices = predict_with_pil_image(image, clip_labels)
print("\nTop 5 predictions:\n")
for percent, index in zip(percentages, indices):
    print(f"{raw_labels[index]}: {percent:.2f}%")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.393.1">This produces the </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">following</span></span><span class="No-Break"><a id="_idIndexMarker758"/></span><span class="No-Break"><span class="koboSpan" id="kobo.395.1"> results:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.396.1">Top 5 predictions: 2: 4.64%, 4: 4.13% ,3: 4.03%, 0: 3.83%, 1: 3.67%</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.397.1">It accurately predicted two </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">paper clips!</span></span></p></li> <li><span class="koboSpan" id="kobo.399.1">Now, let’s go through two more harder examples from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.400.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.401.1">.4 B</span></em><span class="koboSpan" id="kobo.402.1"> and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.403.1">Figure </span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.404.1">9</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.405.1">.4 C</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.407.1">
medium_image = Image.open('clips-data-2020/clips/clips-25086.png'))
hard_image = Image.open('clips-data-2020/clips/clips-25485.png')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.408.1">Predicting on </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.409.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.410.1">.4 B</span></em><span class="koboSpan" id="kobo.411.1"> produces the </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">following results:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.413.1">Top 5 predictions: 4: 3.98%, 3: 3.78%, 2: 3.71%, 6: 3.23%, 5: 3.16%</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.414.1">This produces an error of 1 since the image contains three paper clips. </span><span class="koboSpan" id="kobo.414.2">Predicting on </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.415.1">Figure 9</span></em></span><em class="italic"><span class="koboSpan" id="kobo.416.1">.4 C</span></em><span class="koboSpan" id="kobo.417.1"> produces the </span><span class="No-Break"><span class="koboSpan" id="kobo.418.1">following results:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.419.1">Top 5 predictions: 18: 1.32%, 9: 1.29%, 0: 1.26%, 19: 1.22% ,16: 1.20%</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.420.1">This produces an error of two since the image contains 16 paper clips. </span><span class="koboSpan" id="kobo.420.2">Not bad </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">at all!</span></span></p></li> <li><span class="koboSpan" id="kobo.422.1">Now, let’s evaluate the mean error on a partitioned validation dataset of </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">1,000 examples:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.424.1">
testing_data = pd.read_csv('train.csv')
errors = []
for idx, row in tqdm(testing_data.iterrows(), total=1000):
     image = Image.open(
      'clips-data-2020/clips/clips-     {}.png'.format(row['id'])
     )
    percentages, indices = predict_with_pil_image(image, clip_labels, 1)
    errors.append(abs(row['clip_count'] - raw_labels[indices[0]]))
    if idx == 1000:
        break
print('{} average count error'.format(np.mean(errors)))</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.425.1">This results in a </span><strong class="source-inline"><span class="koboSpan" id="kobo.426.1">23.8122</span></strong><span class="koboSpan" id="kobo.427.1"> average</span><a id="_idIndexMarker759"/><span class="koboSpan" id="kobo.428.1"> count error. </span><span class="koboSpan" id="kobo.428.2">All of a sudden, this doesn’t look that usable. </span><span class="koboSpan" id="kobo.428.3">In some of the examples, the model couldn’t properly count clips that were partially occluded, even when in the description it was specified that there would be some partially occluded. </span><span class="koboSpan" id="kobo.428.4">It might make sense to add a description stating that the clips are in different sizes, or even that it is on top of a piece of lined paper. </span><span class="koboSpan" id="kobo.428.5">Try it for yourself and do </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">some experiments!</span></span></p>
<p><span class="koboSpan" id="kobo.430.1">Training a ridge regressor model with a pretrained SqueezeNet featurizer on this dataset separately and validated on the same validation partition in </span><em class="italic"><span class="koboSpan" id="kobo.431.1">step 12</span></em><span class="koboSpan" id="kobo.432.1"> results in a root mean squared error of </span><strong class="source-inline"><span class="koboSpan" id="kobo.433.1">1.9102</span></strong><span class="koboSpan" id="kobo.434.1">. </span><span class="koboSpan" id="kobo.434.2">This shows that zero-shot learning by itself is not as reliable as a supervised model for this use case, but it might be able to discern and predict nicely on simpler images. </span><span class="koboSpan" id="kobo.434.3">In the paper, the authors emphasized that CLIP-based zero-shot learning can work well and achieve performance close to supervised learning techniques in some specific use cases and datasets. </span><span class="koboSpan" id="kobo.434.4">However, in most use cases and datasets, CLIP-based zero-shot learning still falls way behind proper supervised learning methods. </span><span class="koboSpan" id="kobo.434.5">This tutorial is an example of where it could work, but it doesn’t work so reliably that it can be utilized in the real world. </span><span class="koboSpan" id="kobo.434.6">However, it does show promise in the base unsupervised method that CLIP was trained in. </span><span class="koboSpan" id="kobo.434.7">A few more years and some more research down the line, and we’ll be sure to see an even better performance</span><a id="_idIndexMarker760"/><span class="koboSpan" id="kobo.435.1"> that will likely be generally the same supervised learning </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">or better!</span></span></p>
<p><span class="koboSpan" id="kobo.437.1">Next, let’s explore the dimensionality reduction component of </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">unsupervised learning.</span></span></p>
<h1 id="_idParaDest-151"><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.439.1">Exploring the dimensionality reduction component of unsupervised deep learning</span></h1>
<p><span class="koboSpan" id="kobo.440.1">Dimensionality reduction is a technique</span><a id="_idIndexMarker761"/><span class="koboSpan" id="kobo.441.1"> that can be useful in cases where a faster runtime is needed to train and perform inference on your model or when the model has a hard time learning from too much data. </span><span class="koboSpan" id="kobo.441.2">The most well-known unsupervised deep learning method for dimensionality reduction is based on autoencoders, which we discussed in </span><a href="B18187_05.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.442.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.443.1">, </span><em class="italic"><span class="koboSpan" id="kobo.444.1">Understanding Autoencoders</span></em><span class="koboSpan" id="kobo.445.1">. </span><span class="koboSpan" id="kobo.445.2">A typical autoencoder network is trained to reproduce the input data as an unsupervised learning method. </span><span class="koboSpan" id="kobo.445.3">This is done through the encoder-decoder structure. </span><span class="koboSpan" id="kobo.445.4">At inference time, using only the encoder will allow you to perform dimensionality reduction as the outputs of the encoder will contain the most compact representation, which can fully reconstruct the original input data. </span><span class="koboSpan" id="kobo.445.5">Autoencoders can support different modalities, with one modality at any one time, which makes it a very versatile unsupervised dimensionality </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">reduction method.</span></span></p>
<p><span class="koboSpan" id="kobo.447.1">Other examples include unsupervised methods to create word embeddings that use shallow neural networks, such as FastText or Word2vec. </span><span class="koboSpan" id="kobo.447.2">The number of unique words that exist in a language is huge. </span><span class="koboSpan" id="kobo.447.3">Even if this gets scaled down to the total amount of unique words in the training data, this can balloon up to 100,000 words easily. </span><span class="koboSpan" id="kobo.447.4">One simple way to encode the words is by using one-hot-encoding or TF-IDF. </span><span class="koboSpan" id="kobo.447.5">Both methods produce 100,000 column features when the dataset contains 100,000 unique words. </span><span class="koboSpan" id="kobo.447.6">This will easily blow up the RAM</span><a id="_idIndexMarker762"/><span class="koboSpan" id="kobo.448.1"> requirements and can make or break the potential of a solution. </span><span class="koboSpan" id="kobo.448.2">However, word embeddings can be tuned to have the desired amount of feature columns as you can choose the embedding size during pretraining with the language model </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">you’re using.</span></span></p>
<p><span class="koboSpan" id="kobo.450.1">Finally, let’s learn how to detect anomalies in </span><span class="No-Break"><span class="koboSpan" id="kobo.451.1">external data.</span></span></p>
<h1 id="_idParaDest-152"><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.452.1">Detecting anomalies in external data</span></h1>
<p><strong class="bold"><span class="koboSpan" id="kobo.453.1">Anomaly detection</span></strong><span class="koboSpan" id="kobo.454.1"> is also considered to be an important application</span><a id="_idIndexMarker763"/><span class="koboSpan" id="kobo.455.1"> of unsupervised learning</span><a id="_idIndexMarker764"/><span class="koboSpan" id="kobo.456.1"> in general. </span><span class="koboSpan" id="kobo.456.2">Anomaly detection can be used in cases where you want to perform any kind of filtering of your existing data, called </span><strong class="bold"><span class="koboSpan" id="kobo.457.1">outlier detection</span></strong><span class="koboSpan" id="kobo.458.1">, and also act as a real-time detector</span><a id="_idIndexMarker765"/><span class="koboSpan" id="kobo.459.1"> during the inference</span><a id="_idIndexMarker766"/><span class="koboSpan" id="kobo.460.1"> stage given new external data, known as </span><strong class="bold"><span class="koboSpan" id="kobo.461.1">novelty detection</span></strong><span class="koboSpan" id="kobo.462.1">. </span><span class="koboSpan" id="kobo.462.2">Here are some examples of end user use cases of </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">anomaly detection:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.464.1">Removing noise</span><a id="_idIndexMarker767"/><span class="koboSpan" id="kobo.465.1"> in your dataset that will be fed into a supervised feature learning process to enable more </span><span class="No-Break"><span class="koboSpan" id="kobo.466.1">stable learning.</span></span></li>
<li><span class="koboSpan" id="kobo.467.1">Removing defective products in the production line. </span><span class="koboSpan" id="kobo.467.2">This can range from the manufacturing production of semiconductor wafers to </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">egg production.</span></span></li>
<li><span class="koboSpan" id="kobo.469.1">Fraud prevention by detecting </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">anomalous transactions.</span></span></li>
<li><span class="koboSpan" id="kobo.471.1">Scam detection through SMS, email, or direct </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">messenger platforms.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.473.1">Anomaly detection is a two-class or binary problem. </span><span class="koboSpan" id="kobo.473.2">This means that an alternative way people approach these example use cases listed is to use supervised learning and collect a bunch of negative samples and positive samples. </span><span class="koboSpan" id="kobo.473.3">The supervised learning approach can work well when a good-quality dataset containing negative/anomalous data has been collected and labeled because usually, there will already be a good amount of positive/normal data. </span><span class="koboSpan" id="kobo.473.4">Go back to the </span><em class="italic"><span class="koboSpan" id="kobo.474.1">Preparing data</span></em><span class="koboSpan" id="kobo.475.1"> section of </span><a href="B18187_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.476.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.477.1">, </span><em class="italic"><span class="koboSpan" id="kobo.478.1">Deep Learning Life Cycle</span></em><span class="koboSpan" id="kobo.479.1">, to recap what it means to have quality data for machine learning! </span><span class="koboSpan" id="kobo.479.2">However, in reality, we can’t possibly capture all the possible variations and types of negative samples that can occur. </span><span class="koboSpan" id="kobo.479.3">Anomaly detection-specific algorithms are built to handle anomalies or negative samples more generally and can extrapolate well to </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">unseen examples.</span></span></p>
<p><span class="koboSpan" id="kobo.481.1">One of the more widely known deep learning methods to achieve anomaly detection on external data that’s not used for training</span><a id="_idIndexMarker768"/><span class="koboSpan" id="kobo.482.1"> is the autoencoder model. </span><span class="koboSpan" id="kobo.482.2">The model performs something called a </span><strong class="bold"><span class="koboSpan" id="kobo.483.1">one-class classifier</span></strong><span class="koboSpan" id="kobo.484.1">, analogous to the traditional one-class support vector machines. </span><span class="koboSpan" id="kobo.484.2">This means that autoencoders can be trained to reconstruct the original data that’s fed into it to achieve either or a combination of four tasks – dimensionality reduction, noise remover, general featurizer, and anomaly detection. </span><span class="koboSpan" id="kobo.484.3">Depending on the objective at hand, the autoencoder should be trained differently. </span><span class="koboSpan" id="kobo.484.4">For dimensionality</span><a id="_idIndexMarker769"/><span class="koboSpan" id="kobo.485.1"> reduction, noise remover, and general featurizer, the autoencoder</span><a id="_idIndexMarker770"/><span class="koboSpan" id="kobo.486.1"> should be trained normally as introduced. </span><span class="koboSpan" id="kobo.486.2">For anomaly detection, the following things need to be done to ensure it achieves the </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">desired behavior:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.488.1">The data without anomalies is to be used as the training data. </span><span class="koboSpan" id="kobo.488.2">If you know there might be anomalies, use a simple outlier detection algorithm such as the local outlier factor, and remove outliers in </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">the data.</span></span></li>
<li><span class="koboSpan" id="kobo.490.1">Use a combination of anomalous and non-anomalous data to form the validation and holdout partition. </span><span class="koboSpan" id="kobo.490.2">If such categorization is not known beforehand, use an outlier-based anomaly detection algorithm from </span><em class="italic"><span class="koboSpan" id="kobo.491.1">step 1</span></em><span class="koboSpan" id="kobo.492.1"> to label </span><span class="No-Break"><span class="koboSpan" id="kobo.493.1">the data.</span></span></li>
<li><span class="koboSpan" id="kobo.494.1">If </span><em class="italic"><span class="koboSpan" id="kobo.495.1">step 2</span></em><span class="koboSpan" id="kobo.496.1"> is somehow not possible, train and overfit the training data using the mean squared error loss. </span><span class="koboSpan" id="kobo.496.2">This is so that you can make sure the model will only be able to reconstruct the data that has the same characteristics as the training data. </span><span class="koboSpan" id="kobo.496.3">Overfitting can be done by training and making sure the training reconstruction loss becomes lower </span><span class="No-Break"><span class="koboSpan" id="kobo.497.1">each epoch.</span></span></li>
<li><span class="koboSpan" id="kobo.498.1">If </span><em class="italic"><span class="koboSpan" id="kobo.499.1">step 2</span></em><span class="koboSpan" id="kobo.500.1"> is possible, train, validate and evaluate with the given cross-validation partitioning strategy and follow the tips introduced in the </span><em class="italic"><span class="koboSpan" id="kobo.501.1">Training supervised deep learning models effectively</span></em><span class="koboSpan" id="kobo.502.1"> section in the </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">previous chapter.</span></span></li>
<li><span class="koboSpan" id="kobo.504.1">The predict function of the autoencoder for testing, validation, and inference can be set based on how well the model can reproduce the input data indicated through the mean squared error. </span><span class="koboSpan" id="kobo.504.2">A threshold will need to be used as a cut-off mechanism to determine which data is anomalous or not. </span><span class="koboSpan" id="kobo.504.3">The higher the mean squared error, the more probable it is that the provided data is anomalous. </span><span class="koboSpan" id="kobo.504.4">Since anomaly detection is a binary classification problem, once labels are available, perform performance analysis by using ROC curves, confusion metrics, and the log loss error using different mean squared error thresholds. </span><span class="koboSpan" id="kobo.504.5">This will be introduced more comprehensively in the </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">next chapter.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.506.1">In anomaly detection in general, note that anomalies are a vague description of what it means to be an anomaly. </span><span class="koboSpan" id="kobo.506.2">Different algorithms encode their definition of what an anomaly is given a dataset, its feature space, and its feature distribution. </span><span class="koboSpan" id="kobo.506.3">When an algorithm does not work well for your predefined case of anomalies, it does not mean that the algorithm is not a good model in general. </span><span class="koboSpan" id="kobo.506.4">The autoencoder approach won’t always be the best method that captures your intuition of anomalies. </span><span class="koboSpan" id="kobo.506.5">When it comes to unsupervised anomaly detection learning, make sure you train a bunch of models with proper hyperparameter settings</span><a id="_idIndexMarker771"/><span class="koboSpan" id="kobo.507.1"> and analyze individually which models match</span><a id="_idIndexMarker772"/><span class="koboSpan" id="kobo.508.1"> your intuition of what makes </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">data anomalous.</span></span></p>
<h1 id="_idParaDest-153"><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.510.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.511.1">Deep learning has made significant contributions to the field of unsupervised learning, leading to the development of several innovative methods. </span><span class="koboSpan" id="kobo.511.2">But by far the most impactful method is unsupervised pretraining, which leverages the abundance of free data available on the internet today to improve the model performance of the downstream supervised tasks and create generalizable representations. </span><span class="koboSpan" id="kobo.511.3">With deeper research and time, unsupervised learning will aid in closing the gap toward general artificial intelligence. </span><span class="koboSpan" id="kobo.511.4">Overall, deep learning has been a valuable tool in the unsupervised learning domain, helping practitioners make the most of the large amounts of free data available on the </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">internet today.</span></span></p>
<p><span class="koboSpan" id="kobo.513.1">In the next chapter, we will dive into the first chapter of the second part of this book, which is meant to introduce methods that provide insights about a trained deep </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">learning model.</span></span></p>
</div>


<div class="Content" id="_idContainer102">
<h1 id="_idParaDest-154" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor159"/><span class="koboSpan" id="kobo.1.1">Part 2 – Multimodal Model Insights</span></h1>
<p><span class="koboSpan" id="kobo.2.1">In this part of the book, we delve into the fascinating world of multimodal model insights, taking you on a comprehensive journey through various aspects of evaluating, interpreting, and securing deep learning models. </span><span class="koboSpan" id="kobo.2.2">This part offers a comprehensive understanding of various facets of model assessment and enhancement while emphasizing the importance of responsible and effective AI deployment in real-world applications. </span><span class="koboSpan" id="kobo.2.3">Throughout these chapters, you will explore methods for evaluating and understanding model predictions, interpreting neural networks, and addressing ethical and security concerns, such as bias, fairness, and </span><span class="No-Break"><span class="koboSpan" id="kobo.3.1">adversarial performance.</span></span></p>
<p><span class="koboSpan" id="kobo.4.1">By the end of this part, you will have a solid understanding of the importance of model evaluation, interpretation, and security, enabling you to create robust, reliable, and equitable deep learning systems and solutions that not only excel in performance but also consider ethical implications and potential vulnerabilities while standing the test of time for </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">real-world applications.</span></span></p>
<p><span class="koboSpan" id="kobo.6.1">This part contains the </span><span class="No-Break"><span class="koboSpan" id="kobo.7.1">following chapters:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.8.1">Chapter 10</span></em><span class="koboSpan" id="kobo.9.1">, </span><em class="italic"><span class="koboSpan" id="kobo.10.1">Exploring Model Evaluation Methods</span></em></li>
<li><em class="italic"><span class="koboSpan" id="kobo.11.1">Chapter 11</span></em><span class="koboSpan" id="kobo.12.1">, </span><em class="italic"><span class="koboSpan" id="kobo.13.1">Explaining Neural Network Predictions</span></em></li>
<li><em class="italic"><span class="koboSpan" id="kobo.14.1">Chapter 12</span></em><span class="koboSpan" id="kobo.15.1">, </span><em class="italic"><span class="koboSpan" id="kobo.16.1">Interpreting Neural Networks</span></em></li>
<li><em class="italic"><span class="koboSpan" id="kobo.17.1">Chapter 13</span></em><span class="koboSpan" id="kobo.18.1">, </span><em class="italic"><span class="koboSpan" id="kobo.19.1">Exploring Bias and Fairness</span></em></li>
<li><em class="italic"><span class="koboSpan" id="kobo.20.1">Chapter 14</span></em><span class="koboSpan" id="kobo.21.1">, </span><em class="italic"><span class="koboSpan" id="kobo.22.1">Analyzing Adversarial Performance</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer103">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer104">
</div>
</div>
</body></html>