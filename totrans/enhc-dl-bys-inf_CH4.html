<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch009.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="chapter-4-introducing-bayesian-deep-learning" class="level1 chapterHead" data-number="9">
<h1 class="chapterHead" data-number="9"><span class="titlemark">Chapter 4</span><br/>
<span id="x1-490004"></span>Introducing Bayesian Deep Learning</h1>
<p>In <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a>, <em>Fundamentals of</em> <em>Bayesian Inference</em>, we saw how traditional methods for Bayesian inference can be used to produce model uncertainty estimates, and we introduced the properties of well-calibrated and well-principled methods for uncertainty estimation. While these traditional methods are powerful in many applications, <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a> also highlighted some of their limitations with respect to scaling. In <em>Chapter 3,</em> <em>Fundamentals of Deep Learning</em>, we saw the impressive things DNNs are capable of given large amounts of data; but we also learned that they aren’t perfect. In particular, they often lack robustness for out-of-distribution data – a major concern when we consider the deployment of these methods in real-world applications.</p>
<div class="IMG---Figure">
<img src="../media/file79.png" alt="PIC"/> <span id="x1-49001r1"></span> <span id="x1-49002"></span></div>
<p class="IMG---Caption">Figure 4.1: BDL combines the strengths of both deep learning and traditional Bayesian inference 
</p>
<p>BDL looks to ameliorate the shortcomings of both traditional Bayesian inference and standard DNNs, using the strengths from one method to address the weaknesses of the other. The fundamental idea is pretty straightforward: our DNNs gain uncertainty estimates, and so can be implemented more robustly, and our Bayesian inference methods gain the scalability and high-dimensional non-linear representation learning of DNNs.</p>
<p>While conceptually this is quite intuitive, practically it’s not a case of just gluing things together. As the model complexity increases, so does the computational cost of Bayesian inference – making certain methods for Bayesian inference (such as via sampling) intractable.</p>
<p>In this chapter, we’ll introduce the concept of an ideal <strong>Bayesian Neural</strong> <strong>Network</strong> (<strong>BNN</strong>) and discuss its limitations, and we’ll learn about how we can use BNNs to create more robust deep learning systems. In particular, we’ll be covering the following:</p>
<ul>
<li><p>The ideal BNN</p></li>
<li><p>BDL fundamentals</p></li>
<li><p>Tools for BDL</p></li>
</ul>
<p><span id="x1-49003r87"></span></p>
<section id="technical-requirements-2" class="level2 sectionHead" data-number="9.1">
<h2 class="sectionHead" data-number="9.1" id="sigil_toc_id_45"><span class="titlemark">4.1 </span> <span id="x1-500001"></span>Technical requirements</h2>
<p>To complete the practical tasks in this chapter, you will need a Python 3.8 environment with the <code>SciPy</code> stack and the following additional Python packages installed:</p>
<ul>
<li><p>TensorFlow 2.0</p></li>
<li><p>TensorFlow Probability</p></li>
<li><p>Seaborn plotting library</p></li>
</ul>
<p>All of the code for this book can be found in the GitHub repository for the book: <a href="https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference" class="url"><span class="No-Break">https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference</span></a>. <span id="x1-50002r90"></span></p>
</section>
<section id="the-ideal-bnn" class="level2 sectionHead" data-number="9.2">
<h2 class="sectionHead" data-number="9.2" id="sigil_toc_id_46"><span class="titlemark">4.2 </span> <span id="x1-510002"></span>The ideal BNN</h2>
<p>As we saw in the previous chapter, a standard neural <span id="dx1-51001"></span>network comprises multiple layers. Each of these layers comprises a number of perceptrons – and these perceptrons comprise a multiplicative component (weight) and an additive component (bias). Each weight and bias parameter comprises a single parameter – or point estimate – and, in combination, these parameters transform the input to the perceptron. As we’ve seen, multiple layers of perceptrons are capable of achieving impressive feats when trained via backpropagation. However, these point estimates contain very limited information – let’s take a look.</p>
<p>Generally speaking, the goal of deep learning is to find (potentially very, very many) parameter values that best map a set of inputs onto a set of outputs. That is, given some data, for each parameter in our network, we’ll choose the parameter that best describes the data. This often boils down to taking the mean – or expectation – of the candidate parameter values. Let’s see what this may look like for a single parameter in a neural network:</p>
<div class="IMG---Figure">
<img src="../media/file80.png" alt="PIC"/> <span id="x1-51002r2"></span> <span id="x1-51003"></span></div>
<p class="IMG---Caption">Figure 4.2: A table of values illustrating how parameters are averaged in machine learning models 
</p>
<p>To understand this better, we’ll use a table to illustrate the relationship between input values, model parameters, and output values. The table shows, for five example input values (first column), what the ideal parameter (second column) would be to obtain the target output value (fourth column). In this context, ideal here simply means that the input value multiplied by the ideal parameter will exactly equal the target output value. Because we need to find a single value that best maps our input data to our output data, we end up taking the expectation (or mean) of our ideal parameters.</p>
<p>As we see here, taking the mean of these parameters is the compromise our model needs to make in order to find a parameter value that best fits all five data points in the example. This is the compromise that is made with traditional deep learning – by using distributions, rather than point estimates, BDL can improve on this. If we look at our standard deviation (<em>σ</em>) values, we get an idea of how the variation in the <em>ideal</em> parameter values (and thus the variance in the input values) translates to a variation in the loss. So, what happens if we have a poor selection of parameter values?</p>
<div class="IMG---Figure">
<img src="../media/file81.png" alt="PIC"/> <span id="x1-51004r3"></span> <span id="x1-51005"></span></div>
<p class="IMG---Caption">Figure 4.3: A table of values illustrating how parameter <em>σ</em> increases for poor sets of parameters 
</p>
<p>If we compare <em>Figure</em> <a href="#x1-51002r2"><em>4.2</em></a> and <em>Figure</em> <a href="#x1-51004r3"><em>4.3</em></a>, we see how a significant variance in parameter values can lead to poorer approximation from the model, and that larger <em>σ</em> can be indicative of an error (at least for well-calibrated models). While in practice things are a little more complicated, what we see here is essentially what’s happening in every parameter of a deep <span id="dx1-51006"></span>learning model: parameter distributions are distilled down to point estimates, losing information in the process. In BDL, we’re interested in harnessing the additional information from these parameter distributions, using it for more robust training and for the creation of uncertainty-aware models.</p>
<p>BNNs look to achieve this by modeling the distribution over neural network parameters. In the ideal case, the BNN would be able to learn any arbitrary distribution for every parameter in the network. At inference time, we would sample from the NN to obtain a distribution of output values. Using the sampling methods introduced in <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of</em> <em>Bayesian Inference</em></a>, we would repeat this process until we have obtained a statistically sufficient number of samples from which we could assume a good approximation of our output distribution. We could then use this output distribution to infer something about our input data, whether that be classifying speech content or performing regression on house prices.</p>
<p>Because we’d have parameter distributions, rather than point estimates, our ideal BNN<span id="dx1-51007"></span> would produce precise uncertainty estimates. These would tell us how likely the parameter values are given the input data. In doing so, they would allow us to detect cases where our input data deviates from the data seen at training time, and to quantify the degree of this deviation by how far a given sample of values lies from the distribution learned at training time. With this information, we would be able to handle our neural network outputs more intelligently – for example, if they’re highly uncertain, then we could fall back to some safe, pre-defined behavior. This concept of interpreting model predictions based on uncertainties should be familiar: we saw this in <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a>, where we learned that high uncertainties are indicative of erroneous model predictions.</p>
<p>Looking back to <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a> again, we saw that sampling quickly becomes computationally intensive. Now imagine sampling from a distribution for each parameter in an NN – even if we take a relatively small network such as MobileNet (an architecture specifically designed to be more computationally efficient), we’re still looking at an enormous 4.2 million parameters. Performing this kind of sampling-based inference on such a network would be incredibly computationally intensive, and this would be even worse for other network architectures (for example, AlexNet has 60 million parameters!).</p>
<p>Because of this intractability, BDL <span id="dx1-51008"></span>methods make use of various approximations in order to facilitate uncertainty quantification. In the next section, we’ll learn about some of the fundamental principles applied to make uncertainty estimates possible with DNNs. <span id="x1-51009r91"></span></p>
</section>
<section id="bdl-fundamentals" class="level2 sectionHead" data-number="9.3">
<h2 class="sectionHead" data-number="9.3" id="sigil_toc_id_47"><span class="titlemark">4.3 </span> <span id="x1-520003"></span>BDL fundamentals</h2>
<p>Throughout the rest of the book, we will introduce a <span id="dx1-52001"></span>range of methods necessary to make BDL possible. There are a number of common themes present through these methods. We’ll cover these here, so that we have a good understanding of these concepts when we encounter them later on.</p>
<p>These concepts include the following:</p>
<ul>
<li><p><strong>Gaussian assumptions</strong>: With many BDL methods, we use <span id="dx1-52002"></span>Gaussian assumptions to make things computationally tractable</p></li>
<li><p><strong>Uncertainty sources</strong>: We’ll take a look at the different <span id="dx1-52003"></span>sources of uncertainty, and how we can determine the contributions of these sources for some BDL methods</p></li>
<li><p><strong>Likelihoods</strong>: We were introduced to likelihoods in <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a>, and here we’ll learn more about the importance of likelihood as a metric for <span id="dx1-52004"></span>evaluating the calibration of probabilistic models</p></li>
</ul>
<p>Let’s look at each of these in the following subsections. <span id="x1-52005r84"></span></p>
<section id="gaussian-assumptions" class="level3 subsectionHead" data-number="9.3.1">
<h3 class="subsectionHead" data-number="9.3.1" id="sigil_toc_id_48"><span class="titlemark">4.3.1 </span> <span id="x1-530001"></span>Gaussian assumptions</h3>
<p>In the ideal case described previously, we talked <span id="dx1-53001"></span>about learning distributions for each neural network parameter. While realistically <span id="dx1-53002"></span>each parameter would follow a specific non-Gaussian distribution, this would make an already difficult problem even <em>more</em> difficult. This is because, for a BNN, we’re interested in learning two key probabilities:</p>
<ul>
<li><p>The probability of the weights <em>W</em> given some data <em>D</em>:</p>
<div class="math-display">
<img src="../media/file82.jpg" class="math-display" alt="P (W |D ) "/>
</div></li>
<li><p>The probability of some output <em>ŷ</em> given some input <strong>x</strong>:</p>
<div class="math-display">
<img src="../media/file83.jpg" class="math-display" alt="P (yˆ|x) "/>
</div></li>
</ul>
<p>Obtaining these probabilities for arbitrary probability distributions would involve solving intractable integrals. Gaussian integrals, on the other hand, have closed-form solutions – making them a very popular choice for approximating distributions.</p>
<p>For this reason, it’s common in BDL to assume that we can closely approximate the true underlying distribution of our weights with Gaussian distributions (similarly to what we’ve seen in <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a>). Let’s see what this would look like – taking our typical linear perceptron model:</p>
<div class="math-display">
<img src="../media/file84.jpg" class="math-display" alt="z = f(x) = βX + ξ "/>
</div>
<p>Here, <em>x</em> is our input to the perceptron, <em>β</em> is our learned weight value, <em>ξ</em> is our learned bias value, and <em>z</em> is the value that is returned (typically passed to the next layer). With a Bayesian approach, we turn our parameters <em>β</em> and <em>ξ</em> into distributions, rather than point estimates, such that:</p>
<div class="math-display">
<img src="../media/file85.jpg" class="math-display" alt="β ≈ 𝒩 (μ β,σβ) "/>
</div>
<div class="math-display">
<img src="../media/file86.jpg" class="math-display" alt="ξ ≈ 𝒩 (μ ξ,σξ) "/>
</div>
<p>The learning process would now involve learning four parameters instead of two, as each Gaussian is described by two parameters: the mean (<em>μ</em>) and <span id="dx1-53003"></span>standard deviation (<em>σ</em>). Doing this for each perceptron in our neural network, we <span id="dx1-53004"></span>end up doubling the number of parameters we need to learn – we can see this illustrated, starting with <em>Figure</em> <a href="#x1-53005r4"><em>4.4</em></a>:</p>
<div class="IMG---Figure">
<img src="../media/DNN-standard.JPG" class="graphics" alt="PIC"/> <span id="x1-53005r4"></span> <span id="x1-53006"></span></div>
<p class="IMG---Caption">Figure 4.4: An illustration of a standard DNN 
</p>
<p>Introducing one-dimensional Gaussian distributions for our weights, our network becomes as follows:</p>
<div class="IMG---Figure">
<img src="../media/DNN-bayesian.JPG" class="graphics" alt="PIC"/> <span id="x1-53007r5"></span> <span id="x1-53008"></span></div>
<p class="IMG---Caption">Figure 4.5: An illustration of a BNN with Gaussian priors over the weights 
</p>
<p>In <em>Chapter 5, Principled Approaches for Bayesian Deep Learning</em>, we’ll see methods that do exactly this. While this does increase the computational complexity and memory footprint of our network, it makes the process of Bayesian inference with NNs manageable – making it a very worthwhile trade-off.</p>
<p>So, what is it we’re actually trying to capture in these uncertainty estimates? In <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a>, we saw how uncertainty varies according to the sample of data used for training – but what are the sources of this uncertainty, and why is it important in deep learning applications? Let’s continue on to the next section to find out. <span id="x1-53009r95"></span></p>
</section>
<section id="sources-of-uncertainty" class="level3 subsectionHead" data-number="9.3.2">
<h3 class="subsectionHead" data-number="9.3.2" id="sigil_toc_id_49"><span class="titlemark">4.3.2 </span> <span id="x1-540002"></span>Sources of uncertainty</h3>
<p>As we saw in <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a>, and as we’ll see later on in the book, we typically deal with uncertainties as scalar variables associated with a parameter or output. These <span id="dx1-54001"></span>variables represent the variation in the parameter or output of interest, but while they are just scalar variables, there are multiple sources contributing to their values. These sources of uncertainty fall into two categories:</p>
<ul>
<li><p><strong>Aleatoric uncertainty</strong>, otherwise <span id="dx1-54002"></span>known as observational <span id="dx1-54003"></span>uncertainty or <span id="dx1-54004"></span>data uncertainty, is the uncertainty associated with our inputs. It describes the variation in our <strong>observations</strong>, and as such is <strong>irreducible</strong>.</p></li>
<li><p><strong>Epistemic uncertainty</strong>, otherwise known as <span id="dx1-54005"></span>model uncertainty, is the <span id="dx1-54006"></span>uncertainty that stems from our model. In the case of <span id="dx1-54007"></span>machine learning, this is the <span id="dx1-54008"></span>variance associated with the parameters of our model that <em>does not</em> stem from the observations, and is instead a product of the model, or how the model is trained. For example, in <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a>, we saw how different priors affected the uncertainty produced by Gaussian processes. This is an example of how model parameters influence the epistemic uncertainty – in this case, because they explicitly modify how the model interprets the relationship between different data points.</p></li>
</ul>
<p>We can build an intuition of these concepts through some simple examples. Let’s say we have a basket of fruit containing apples and bananas. If we measure the height and length of some apples and bananas, we’ll see that apples are generally round, and that bananas are generally long, as illustrated in <em>Figure</em> <a href="#x1-54010r6"><em>4.6</em></a>. We know from our observations that the exact dimensions of each fruit varies: we accept that there is randomness, or stochasticity, associated with the <span id="dx1-54009"></span>measurements of any given distribution of apples, but we know that they will all be roughly similar. This is the <strong>irreducible uncertainty</strong>: the inherent uncertainty in the data.</p>
<div class="IMG---Figure">
<img src="../media/aleatoric-uncertainty-illustration.JPG" class="graphics" alt="PIC"/> <span id="x1-54010r6"></span> <span id="x1-54011"></span></div>
<p class="IMG---Caption">Figure 4.6: An illustration of aleatoric uncertainty, using fruit shapes as an example 
</p>
<p>We can make use of this information to build a model to classify fruit as either apples or bananas according to these input features. But what <span id="dx1-54012"></span>happens if we mainly train our model on apples, with only a few measurements for bananas? This is illustrated in <em>Figure</em> <a href="#x1-54013r7"><em>4.7</em></a>.</p>
<div class="IMG---Figure">
<img src="../media/epistemic-uncertainty-illustration.JPG" class="graphics" alt="PIC"/> <span id="x1-54013r7"></span> <span id="x1-54014"></span></div>
<p class="IMG---Caption">Figure 4.7: An illustration of high epistemic uncertainty based on our fruit example 
</p>
<p>Here, we see that – because of limited data – our model has incorrectly classified bananas as apples. While these data points fall within our model’s <code>apple</code> boundary, we also see that they lie very far from the other apples, meaning that, although they’re classified as apples, our model (if it’s Bayesian) will have a high predictive uncertainty associated with these data points. This epistemic uncertainty is very useful in practical applications: it gives us an indication of when we can trust our model, and when we should be cautious about our model’s predictions. Unlike aleatoric uncertainty, epistemic uncertainty is <strong>reducible</strong> – if we give our model more examples of bananas, its class boundaries will improve, and the epistemic uncertainty will approach the aleatoric uncertainty.</p>
<div class="IMG---Figure">
<img src="../media/epistemic-uncertainty-illustration2.JPG" class="graphics" alt="PIC"/> <span id="x1-54016r8"></span> <span id="x1-54017"></span></div>
<p class="IMG---Caption">Figure 4.8: Illustration of low epistemic uncertainty 
</p>
<p>In <em>Figure</em> <a href="#x1-54016r8"><em>4.8</em></a>, we see that the epistemic uncertainty has reduced significantly now that our model has observed more data, and it’s looking a lot more like the aleatoric uncertainty illustrated in <em>Figure</em> <a href="#x1-54010r6"><em>4.6</em></a>. Epistemic uncertainty is therefore incredibly useful, both for indicating how much we can trust our model, and as a means of improving <span id="dx1-54018"></span>our model’s performance.</p>
<p>As deep learning approaches are increasingly applied in mission-critical and safety-critical applications, it’s crucial that the methods we use can estimate the degree of epistemic uncertainty associated with their predictions. To illustrate this, let’s change the domain of our example from <em>Figure</em> <a href="#x1-54013r7"><em>4.7</em></a>: instead of classifying fruit, we’re now classifying whether a jet engine is operating within safe parameters, as shown in <em>Figure</em> <a href="#x1-54019r9"><em>4.9</em></a>.</p>
<div class="IMG---Figure">
<img src="../media/epistemic-uncertainty-illustration-engine-failure.JPG" class="graphics" alt="PIC"/> <span id="x1-54019r9"></span> <span id="x1-54020"></span></div>
<p class="IMG---Caption">Figure 4.9: An illustration of high epistemic uncertainty in a safety-critical application 
</p>
<p>Here, we see that our epistemic <span id="dx1-54021"></span>uncertainty could be a life-saving indicator of engine failure. Without this uncertainty estimate, our model would assume that all is fine, even though the temperature of the engine is unusual given the other parameters – this could lead to catastrophic consequences. Fortunately, because of our uncertainty estimates, our model is able to tell us that something is wrong, despite the fact that it’s never encountered this situation before.</p>
<section id="separating-sourcing-of-uncertainty" class="level4 likesubsubsectionHead" data-number="9.3.2.1">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="9.3.2.1">Separating sourcing of uncertainty</h4>
<p>In this section, we’ve been introduced to two <span id="dx1-55001"></span>sources of uncertainty, and we’ve seen how epistemic uncertainty can be very useful for understanding how to interpret our model’s outputs. So, you may be wondering: is it possible to separate our sources of uncertainty?</p>
<p>Generally speaking, there are limited guarantees when trying to decompose uncertainty into epistemic and aleatoric components, but some models allow us to obtain a good approximation of this. Ensemble methods provide a particularly good illustrative example.</p>
<p>Let’s say we have an ensemble of <em>M</em> models that produce the predictive posterior <em>P</em>(<em>y</em><span class="cmsy-10x-x-109">|</span><strong>x</strong><em>,D</em>) for some input <strong>x</strong> and output <em>y</em> from data <em>D</em>. For a given input, our prediction will have entropy:</p>
<div class="math-display">
<img src="../media/file93.jpg" class="math-display" alt=" 1 ∑M m m H [P (y|x, D)] ≈ H [M- P (y|x,𝜃 )],𝜃 ∼ p(𝜃|D ) m=1 "/>
</div>
<p>Here, <em>H</em> denotes entropy, and <em>𝜃</em> denotes our model parameters. This is a formal exdivssion of concepts we’ve already covered, showing that the entropy (in other words, uncertainty) of our predictive posterior will be high when our aleatoric and/or epistemic uncertainty is high. This, therefore, represents our <strong>total</strong> <strong>uncertainty</strong>, which is the <span id="dx1-55002"></span>uncertainty we’ll be working with throughout this book. We can represent this in a manner more consistent with what we’ll be encountering in the book – in terms of our predictive standard deviation <em>σ</em>:</p>
<div class="math-display">
<img src="../media/file94.jpg" class="math-display" alt="σ = σa + σe "/><span id="x1-550002"></span>
</div>
<p>Where <em>a</em> and <em>e</em> denote aleatoric and epistemic uncertainty, respectively.</p>
<p>Because we’re working with ensembles, we can go a step further than our total uncertainty. Ensembles are unique in that each model learns something slightly different from the data, due to different data or parameter initialization. As we get an uncertainty estimate for each model, we can take the expectation (in other words, the average) of these uncertainty estimates:</p>
<div class="math-display">
<img src="../media/file95.jpg" class="math-display" alt=" ∑M 𝔼 [H [P(y|x,𝜃)]] ≈ -1- H [P (y|x,𝜃m )],𝜃m ∼ p(𝜃|D ) p(𝜃|D) M m=1 "/>
</div>
<p>This gives us our <strong>expected data uncertainty</strong> – an estimate of our <span id="dx1-55003"></span>aleatoric uncertainty. This approximate measure of aleatoric uncertainty becomes more accurate as ensemble size increases. This is possible because of the way ensemble members learn from different subsets of data. If there is no epistemic uncertainty, then the models are consistent, meaning their outputs are identical, and the total uncertainty exclusively comprises the aleatoric uncertainty.</p>
<p>If, on the other hand, there is some epistemic uncertainty, then our total uncertainty comprises both aleatoric and epistemic uncertainty. We can use the expected data uncertainty to determine how much epistemic uncertainty is present in our total uncertainty. We do this using <strong>mutual information</strong>, which is given by:</p>
<div class="math-display">
<img src="../media/file96.jpg" class="math-display" alt="I[y,𝜃|x,D ] = H [P (y|x, D)]− 𝔼p (𝜃|D )[H [P(y|x,𝜃)]] "/>
</div>
<p>We can also exdivss this in terms of equation <a href="#x1-550002">4.3.2</a>:</p>
<div class="math-display">
<img src="../media/file97.jpg" class="math-display" alt="I[y,𝜃|x,D ] = σe = σ − σa "/>
</div>
<p>As we can see, the concept is pretty straightforward: simply subtract our aleatoric uncertainty from our total uncertainty! The ability to estimate the aleatoric uncertainty can make ensemble methods more attractive for uncertainty quantification, as it allows us to decompose uncertainty, thus providing additional information we don’t usually have access to. In <em>Chapter 6, Bayesian</em> <em>Inference with a Standard Deep Learning Toolbox</em>, we’ll learn more about ensemble techniques for BDL. For non-ensemble methods, we just have the general predictive uncertainty, <em>σ</em> (the combined aleatoric and epistemic uncertainty), which is suitable in most cases.</p>
<p>In the next section, we’ll see how we can incorporate uncertainties in how we evaluate our models, and how they can be incorporated in the loss function to improve model training. <span id="x1-55004r98"></span></p>
</section>
</section>
<section id="going-beyond-maximum-likelihood-the-importance-of-likelihoods" class="level3 subsectionHead" data-number="9.3.3">
<h3 class="subsectionHead" data-number="9.3.3" id="sigil_toc_id_50"><span class="titlemark">4.3.3 </span>Going beyond maximum likelihood: the importance of likelihoods</h3>
<p>In the previous section, we saw how <span id="dx1-56001"></span>uncertainty quantification can help to <span id="dx1-56002"></span>avoid potentially hazardous scenarios in real-world applications of machine learning. Going back even further to <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a> and <a href="CH3.xhtml#x1-350003"><em>Chapter 3</em></a>, <a href="CH3.xhtml#x1-350003"><em>Fundamentals of Deep Learning</em></a>, we were introduced to the concept of calibration, and shown how well-calibrated methods’ uncertainties increase as data at inference deviates from training data – a concept illustrated in <em>Figure</em> <a href="#x1-54013r7"><em>4.7</em></a>.</p>
<p>While it’s easy to illustrate the concept of calibration with simple data – as we saw in <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a> (through <em>Figure</em> <a href="CH2.xhtml#x1-31029r21"><em>2.21</em></a>) – unfortunately, it’s not easy or practical to do this in most applications. A much more practical approach to understanding how well-calibrated a given method would be to use a metric that incorporates its uncertainty – and this is exactly what we get with <strong>likelihood</strong>.</p>
<p>Likelihood is the probability that some <span id="dx1-56003"></span>parameters describe some data. As mentioned earlier, we typically work with Gaussian distributions to make things tractable – so we’re interested in Gaussian likelihood: the likelihood that the parameters of a Gaussian fit some observed data. The equation for Gaussian likelihood is as follows:</p>
<div class="math-display">
<img src="../media/file98.jpg" class="math-display" alt=" 1 (y − μ)2 p(y) = √----exp {− ----2--} 2π σ 2σ "/><span id="x1-560003"></span>
</div>
<p>Let’s see what these distributions would look like for the parameter values we saw earlier in <em>Figures</em> <a href="#x1-51002r2"><em>4.2</em></a> and <a href="#x1-51004r3"><em>4.3</em></a>:</p>
<div class="IMG---Figure">
<img src="../media/file99.png" alt="PIC"/> <span id="x1-56004r10"></span> <span id="x1-56005"></span></div>
<p class="IMG---Caption">Figure 4.10: The plot of Gaussian distributions corresponding to the parameter sets from Figures <a href="#x1-51002r2">4.2</a> and <a href="#x1-51004r3">4.3</a> 
</p>
<p>Visualizing these two distributions highlights the difference in uncertainty between the two parameter sets: our first set of parameters has high probability (solid line), whereas our second set of parameters has low probability (dotted line). But what <span id="dx1-56006"></span>does this mean for the resulting likelihood values associated with our model’s outputs? To investigate these, we need to plug these values into equation <a href="#x1-560003">4.3.3</a>. To do this, we’ll need a value for <em>y</em>. We’ll use the mean of our target values: 24<em>.</em>03. For our <em>μ</em> and <em>σ</em> values, we’ll take the means and standard deviations of the predicted output values, respectively:</p>
<div class="math-display">
<img src="../media/file100.jpg" class="math-display" alt=" 1 (24.03 − 24.01)2 p(𝜃1) = √---------exp { − ----------2----} = 0.29 2π × 1.37 2 × 1.37 "/>
</div>
<div class="math-display">
<img src="../media/file101.jpg" class="math-display" alt=" -----1----- (24.03−--31.11)2 − 5 p(𝜃2) = √2-π-× 1.78 exp {− 2× 1.782 } = 7.88× 10 "/>
</div>
<p>We see here that we have a much higher likelihood score for our first set of parameters (<em>𝜃</em><sub><span class="cmr-8">1</span></sub>) than for our second (<em>𝜃</em><sub><span class="cmr-8">2</span></sub>). This is consistent with <em>Figure</em> <a href="#x1-56004r10"><em>4.10</em></a>, and indicates that, given the data, parameters <em>𝜃</em><sub><span class="cmr-8">1</span></sub> have a higher probability than parameters <em>𝜃</em><sub><span class="cmr-8">2</span></sub> – in other words, parameters <em>𝜃</em><sub><span class="cmr-8">1</span></sub> do a better job of mapping the inputs to the outputs.</p>
<p>These examples illustrate the impact of incorporating uncertainty estimates, allowing us to compute the likelihood of the data. While our error has increased somewhat due to the poorer mean prediction, our likelihood has decreased more dramatically – falling by many orders of magnitude. This tells us that these parameters are doing a very poor job of describing the data, and it does so in a more principled way than simply computing the error between our outputs and our targets.</p>
<p>An important feature of likelihood is that it <span id="dx1-56007"></span>balances a model’s accuracy with its uncertainty. Models that are over-confident have low uncertainty on data for which they have incorrect predictions, and likelihood penalizes them for this overconfidence. Similarly, well-calibrated models are confident on data for which they have correct predictions, and uncertain on data for which they have incorrect predictions. While the models will still be penalized for the incorrect predictions, they will also be rewarded for being uncertain in the right places, and not being over-confident. To see this in practice, we can again use the target output value from the tables shown in <em>Figure</em> <a href="#x1-51002r2"><em>4.2</em></a> and <em>Figure</em> <a href="#x1-51004r3"><em>4.3</em></a>: <em>y</em> = 24<em>.</em>03, but we’ll also use an incorrect prediction: <em>ŷ</em> = 5<em>.</em>00. As we can see, this produces a pretty significant error of <span class="cmsy-10x-x-109">|</span><em>y</em> <span class="cmsy-10x-x-109">−</span><em>ŷ</em><span class="cmsy-10x-x-109">| </span>= <span class="cmsy-10x-x-109">|</span>24<em>.</em>03 <span class="cmsy-10x-x-109">− </span>5<em>.</em>00<span class="cmsy-10x-x-109">| </span>= 19<em>.</em>03. Let’s take a look at what happens to our likelihood as we increase our <em>σ</em><sup><span class="cmr-8">2</span></sup> value associated with this prediction:</p>
<div class="IMG---Figure">
<img src="../media/file102.png" alt="PIC"/> <span id="x1-56008r11"></span> <span id="x1-56009"></span></div>
<p class="IMG---Caption">Figure 4.11: A plot of likelihood values with increasing variance 
</p>
<p>As we see here, our likelihood value is very small when <em>σ</em><sup><span class="cmr-8">2</span></sup> = 0<em>.</em>00, but increases as <em>σ</em><sup><span class="cmr-8">2</span></sup> increases to around 0<em>.</em>15, before falling off again. This demonstrates that, given an incorrect prediction, some uncertainty is better than none when it comes to likelihood values. Thus, using likelihoods allows us to train better calibrated models.</p>
<p>Similarly, we can see that if we fix our uncertainty, in this case to <em>σ</em><sup><span class="cmr-8">2</span></sup> = 0<em>.</em>1, and vary our predictions, our likelihood peaks at the correct value, falling off in either direction as our predictions <em>ŷ</em> become less accurate and our error <span class="cmsy-10x-x-109">|</span><em>y</em> <span class="cmsy-10x-x-109">−</span><em>ŷ</em><span class="cmsy-10x-x-109">|</span> grows:</p>
<div class="IMG---Figure">
<img src="../media/likelihood-varying-predictions.png" alt="PIC"/> <span id="x1-56010r12"></span> <span id="x1-56011"></span></div>
<p class="IMG---Caption">Figure 4.12: A plot of likelihood values with varying predictions 
</p>
<p>Practically, we don’t usually use the likelihood, but instead use the <strong>negative</strong> <strong>log-likelihood</strong> (<strong>NLL</strong>). We make it negative because, with <span id="dx1-56012"></span>loss functions, we are interested in <span id="dx1-56013"></span>finding the minima, rather than the maxima. We use the log because this allows us to use addition, rather than multiplication, which makes things more computationally efficient (making use of the logarithmic identity <em>log</em>(<em>a</em> <span class="cmsy-10x-x-109">∗ </span><em>b</em>) = <em>log</em>(<em>a</em>) + <em>log</em>(<em>b</em>)). The equation that we’ll typically be using is therefore:</p>
<div class="math-display">
<img src="../media/file103.jpg" class="math-display" alt=" 2 N LL (y) = − log{-1--}− (y-−-μ)- 2πσ 2 σ2 "/><span id="x1-56010r3"></span>
</div>
<p>Now that we’re familiar with the core concepts of uncertainty and likelihood, we’re ready for the next section, where we’ll learn how to work with probabilistic concepts in code using the TensorFlow Probability library. <span id="x1-56014r94"></span></p>
</section>
</section>
<section id="tools-for-bdl" class="level2 sectionHead" data-number="9.4">
<h2 class="sectionHead" data-number="9.4" id="sigil_toc_id_51"><span class="titlemark">4.4 </span> <span id="x1-570004"></span>Tools for BDL</h2>
<p>In this chapter, as well as in <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a>, we’ve seen a lot of <span id="dx1-57001"></span>equations involving probability. While it’s possible to create BDL models without a probability library, having a library that supports some of the fundamental functions makes things much easier. As we’re using <span id="dx1-57002"></span>TensorFlow for the examples in this book, we’ll be using the <strong>TensorFlow</strong> <strong>Probability</strong> (<strong>TFP</strong>) library to help us with some of these probabilistic components. In this section, we’ll introduce TFP and show how it can be used to easily implement many of the concepts we’ve seen in <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a> and <a href="#x1-490004"><em>Chapter 4</em></a>, <a href="#x1-490004"><em>Introducing Bayesian Deep</em> <em>Learning</em></a>.</p>
<p>Much of the content up to this point has been about introducing the concept of working with distributions. As such, the first TFP module we’ll learn about is the <code>distributions</code> module. Let’s take a look:</p>
<pre id="fancyvrb33" class="fancyvrb"><span id="x1-57010r1"></span> 
<code><span>import</span><span id="textcolor544"><span> </span></span><span>tensorflow_probability</span><span id="textcolor545"><span> </span></span><span>as</span><span id="textcolor546"><span> </span></span><span>tfp</span> <span id="x1-57012r2"></span> </code>
<code><span>tfd</span><span id="textcolor547"><span> </span></span><span id="textcolor548"><span>=</span></span><span id="textcolor549"><span> </span></span><span>tfp.distributions</span> <span id="x1-57014r3"></span> </code>
<code><span>mu</span><span id="textcolor550"><span> </span></span><span id="textcolor551"><span>=</span></span><span id="textcolor552"><span> </span></span><span id="textcolor553"><span>0</span></span> <span id="x1-57016r4"></span> </code>
<code><span>sigma</span><span id="textcolor554"><span> </span></span><span id="textcolor555"><span>=</span></span><span id="textcolor556"><span> </span></span><span id="textcolor557"><span>1.5</span></span> <span id="x1-57018r5"></span> </code>
<code><span>gaussian_dist</span><span id="textcolor558"><span> </span></span><span id="textcolor559"><span>=</span></span><span id="textcolor560"><span> </span></span><span>tfd.Normal(loc</span><span id="textcolor561"><span>=</span></span><span>mu,</span><span id="textcolor562"><span> </span></span><span>scale</span><span id="textcolor563"><span>=</span></span><span>sigma)</span></code></pre>
<p>Here, we have a simple example of initializing a Gaussian (or normal) distribution using the <code>distributions</code> module. We can now sample from this distribution – we’ll visualize the distribution of our samples using <code>seaborn</code> and <code>matplotlib</code>:</p>
<pre id="fancyvrb34" class="fancyvrb"><span id="x1-57028r1"></span> 
<code><span>import</span><span id="textcolor564"><span> </span></span><span>seaborn</span><span id="textcolor565"><span> </span></span><span>as</span><span id="textcolor566"><span> </span></span><span>sns</span> <span id="x1-57030r2"></span> </code>
<code><span>samples</span><span id="textcolor567"><span> </span></span><span id="textcolor568"><span>=</span></span><span id="textcolor569"><span> </span></span><span>gaussian_dist.sample(</span><span id="textcolor570"><span>1000</span></span><span>)</span> <span id="x1-57032r3"></span> </code>
<code><span>sns.histplot(samples,</span><span id="textcolor571"><span> </span></span><span>stat</span><span id="textcolor572"><span>=</span></span><span id="textcolor573"><span>"probability"</span></span><span>,</span><span id="textcolor574"><span> </span></span><span>kde</span><span id="textcolor575"><span>=</span></span><span>True)</span> <span id="x1-57034r4"></span> </code>
<code><span>plt.show()</span></code></pre>
<p>This produces the following plot:</p>
<div class="IMG---Figure">
<img src="../media/file104.png" alt="PIC"/> <span id="x1-57035r13"></span> <span id="x1-57036"></span></div>
<p class="IMG---Caption">Figure 4.13: A probability distribution of samples drawn from a Gaussian distribution using TFP 
</p>
<p>As we can see, the samples follow a Gaussian distribution defined by our parameters <em>μ</em> = 0 and <em>σ</em> = 1<em>.</em>5. The TFD distribution classes also have <span id="dx1-57037"></span>methods for useful functions such as <strong>Probability Density Function</strong> (<strong>PDF</strong>) and <strong>Cumulative Density Function</strong> (<strong>CDF</strong>). Let’s take a look, starting <span id="dx1-57038"></span>with computing the PDF over a range of values:</p>
<pre id="fancyvrb35" class="fancyvrb"><span id="x1-57050r1"></span> 
<code><span>pdf_range</span><span id="textcolor576"><span> </span></span><span id="textcolor577"><span>=</span></span><span id="textcolor578"><span> </span></span><span>np.arange(</span><span id="textcolor579"><span>-4</span></span><span>,</span><span id="textcolor580"><span> </span></span><span id="textcolor581"><span>4</span></span><span>,</span><span id="textcolor582"><span> </span></span><span id="textcolor583"><span>0.1</span></span><span>)</span> <span id="x1-57052r2"></span> </code>
<code><span>pdf_values</span><span id="textcolor584"><span> </span></span><span id="textcolor585"><span>=</span></span><span id="textcolor586"><span> </span></span><span>[]</span> <span id="x1-57054r3"></span> </code>
<code><span id="textcolor587"><span>for</span></span><span id="textcolor588"><span> </span></span><span>x</span><span id="textcolor589"><span> </span></span><span>in</span><span id="textcolor590"><span> </span></span><span>pdf_range</span><span id="textcolor591"><span>:</span></span> <span id="x1-57056r4"></span> </code>
<code><span id="textcolor592"><span> </span><span> </span><span> </span><span> </span></span><span>pdf_values.append(gaussian_dist.prob(x))</span> <span id="x1-57058r5"></span> </code>
<code><span>plt.figure(figsize</span><span id="textcolor593"><span>=</span></span><span>(</span><span id="textcolor594"><span>10</span></span><span>,</span><span id="textcolor595"><span> </span></span><span id="textcolor596"><span>5</span></span><span>))</span> <span id="x1-57060r6"></span> </code>
<code><span>plt.plot(pdf_range,</span><span id="textcolor597"><span> </span></span><span>pdf_values)</span> <span id="x1-57062r7"></span> </code>
<code><span>plt.title(</span><span id="textcolor598"><span>"Probability</span><span> density</span><span> function"</span></span><span>,</span><span id="textcolor599"><span> </span></span><span>fontsize</span><span id="textcolor600"><span>=</span></span><span id="textcolor601"><span>"15"</span></span><span>)</span> <span id="x1-57064r8"></span> </code>
<code><span>plt.xlabel(</span><span id="textcolor602"><span>"x"</span></span><span>,</span><span id="textcolor603"><span> </span></span><span>fontsize</span><span id="textcolor604"><span>=</span></span><span id="textcolor605"><span>"15"</span></span><span>)</span> <span id="x1-57066r9"></span> </code>
<code><span>plt.ylabel(</span><span id="textcolor606"><span>"probability"</span></span><span>,</span><span id="textcolor607"><span> </span></span><span>fontsize</span><span id="textcolor608"><span>=</span></span><span id="textcolor609"><span>"15"</span></span><span>)</span> <span id="x1-57068r10"></span> </code>
<code><span>plt.show()</span></code></pre>
<p>With the <span id="dx1-57069"></span>divceding code, we’ll produce the following plot:</p>
<div class="IMG---Figure">
<img src="../media/file105.png" alt="PIC"/> <span id="x1-57070r14"></span> <span id="x1-57071"></span></div>
<p class="IMG---Caption">Figure 4.14: A plot of probability density function values for a range of inputs spanning <em>x</em> = <span class="cmsy-10x-x-109">−</span>4 to <em>x</em> = 4 
</p>
<p>Similarly, we can also compute the CDF:</p>
<pre id="fancyvrb36" class="fancyvrb"><span id="x1-57083r1"></span> 
<code><span>cdf_range</span><span id="textcolor610"><span> </span></span><span id="textcolor611"><span>=</span></span><span id="textcolor612"><span> </span></span><span>np.arange(</span><span id="textcolor613"><span>-4</span></span><span>,</span><span id="textcolor614"><span> </span></span><span id="textcolor615"><span>4</span></span><span>,</span><span id="textcolor616"><span> </span></span><span id="textcolor617"><span>0.1</span></span><span>)</span> <span id="x1-57085r2"></span> </code>
<code><span>cdf_values</span><span id="textcolor618"><span> </span></span><span id="textcolor619"><span>=</span></span><span id="textcolor620"><span> </span></span><span>[]</span> <span id="x1-57087r3"></span> </code>
<code><span id="textcolor621"><span>for</span></span><span id="textcolor622"><span> </span></span><span>x</span><span id="textcolor623"><span> </span></span><span>in</span><span id="textcolor624"><span> </span></span><span>cdf_range</span><span id="textcolor625"><span>:</span></span> <span id="x1-57089r4"></span> </code>
<code><span id="textcolor626"><span> </span><span> </span><span> </span><span> </span></span><span>cdf_values.append(gaussian_dist.cdf(x))</span> <span id="x1-57091r5"></span> </code>
<code><span>plt.figure(figsize</span><span id="textcolor627"><span>=</span></span><span>(</span><span id="textcolor628"><span>10</span></span><span>,</span><span id="textcolor629"><span> </span></span><span id="textcolor630"><span>5</span></span><span>))</span> <span id="x1-57093r6"></span> </code>
<code><span>plt.plot(cdf_range,</span><span id="textcolor631"><span> </span></span><span>cdf_values)</span> <span id="x1-57095r7"></span> </code>
<code><span>plt.title(</span><span id="textcolor632"><span>"Cumulative</span><span> density</span><span> function"</span></span><span>,</span><span id="textcolor633"><span> </span></span><span>fontsize</span><span id="textcolor634"><span>=</span></span><span id="textcolor635"><span>"15"</span></span><span>)</span> <span id="x1-57097r8"></span> </code>
<code><span>plt.xlabel(</span><span id="textcolor636"><span>"x"</span></span><span>,</span><span id="textcolor637"><span> </span></span><span>fontsize</span><span id="textcolor638"><span>=</span></span><span id="textcolor639"><span>"15"</span></span><span>)</span> <span id="x1-57099r9"></span> </code>
<code><span>plt.ylabel(</span><span id="textcolor640"><span>"CDF"</span></span><span>,</span><span id="textcolor641"><span> </span></span><span>fontsize</span><span id="textcolor642"><span>=</span></span><span id="textcolor643"><span>"15"</span></span><span>)</span> <span id="x1-57101r10"></span> </code>
<code><span>plt.show()</span></code></pre>
<p>Compared with the PDF, the CDF produces cumulative probability values, from 0 to 1, as we see in the following plot:</p>
<div class="IMG---Figure">
<img src="../media/file106.png" alt="PIC"/> <span id="x1-57102r15"></span> <span id="x1-57103"></span></div>
<p class="IMG---Caption">Figure 4.15: Cumulative density function values for a range of inputs spanning <em>x</em> = <span class="cmsy-10x-x-109">−</span>4 to <em>x</em> = 4 
</p>
<p>The <code>tfp.distributions</code> classes also give us <span id="dx1-57105"></span>easy access to the parameters of the distributions, for example, we can recover the parameters of our Gaussian distribution via the following:</p>
<pre id="fancyvrb37" class="fancyvrb"><span id="x1-57109r1"></span> 
<code><span>mu</span><span id="textcolor645"><span> </span></span><span id="textcolor646"><span>=</span></span><span id="textcolor647"><span> </span></span><span>gaussian_dist.mean()</span> <span id="x1-57111r2"></span> </code>
<code><span>sigma</span><span id="textcolor648"><span> </span></span><span id="textcolor649"><span>=</span></span><span id="textcolor650"><span> </span></span><span>gaussian_dist.stddev()</span></code></pre>
<p>Note that these will return <code>tf.Tensor</code> objects, but the NumPy values can be accessed easily via the <code>.numpy()</code> function, for example:</p>
<pre id="fancyvrb38" class="fancyvrb"><span id="x1-57117r1"></span> 
<code><span>mu</span><span id="textcolor653"><span> </span></span><span id="textcolor654"><span>=</span></span><span id="textcolor655"><span> </span></span><span>mu.numpy()</span> <span id="x1-57119r2"></span> </code>
<code><span>sigma</span><span id="textcolor656"><span> </span></span><span id="textcolor657"><span>=</span></span><span id="textcolor658"><span> </span></span><span>sigma.numpy()</span></code></pre>
<p>This gives us two NumPy scalars for our <code>mu</code> and <code>sigma</code> variables: 0<em>.</em>0 and 1<em>.</em>5, respectively.</p>
<p>Just as we can compute the probability, and thus obtain the PDF, using the <code>prob()</code> function, we can also easily compute the log probability, or log likelihood, using the <code>log_prob()</code> function. This makes things a little easier than coding the full likelihood equation (for instance, equation <a href="#x1-56010r3">4.3.3</a>) each time:</p>
<pre id="fancyvrb39" class="fancyvrb"><span id="x1-57128r1"></span> 
<code><span>x</span><span id="textcolor659"><span> </span></span><span id="textcolor660"><span>=</span></span><span id="textcolor661"><span> </span></span><span id="textcolor662"><span>5</span></span> <span id="x1-57130r2"></span> </code>
<code><span>log_likelihood</span><span id="textcolor663"><span> </span></span><span id="textcolor664"><span>=</span></span><span id="textcolor665"><span> </span></span><span>gaussian_dist.log_prob(x)</span> <span id="x1-57132r3"></span> </code>
<code><span>negative_log_likelihood</span><span id="textcolor666"><span> </span></span><span id="textcolor667"><span>=</span></span><span id="textcolor668"><span> </span></span><span id="textcolor669"><span>-</span></span><span>log_likelihood</span></code></pre>
<p>Here, we first obtain the log likelihood for some value <em>x</em> = 5, and then obtain the NLL, such as would be used in the context of gradient descent.</p>
<p>As we continue through the book, we’ll learn more about what TFP has to offer – using the <code>distributions</code> module to sample from parameter distributions, and exploring the powerful <code>tfp.layers</code> module, which implements probabilistic versions of common neural network layers. <span id="x1-57135r108"></span></p>
</section>
<section id="summary-3" class="level2 sectionHead" data-number="9.5">
<h2 class="sectionHead" data-number="9.5" id="sigil_toc_id_52"><span class="titlemark">4.5 </span> <span id="x1-580005"></span>Summary</h2>
<p>In this chapter, we were introduced to the fundamental concepts that we’ll need to progress through the book and learn how to implement and use BNNs. Most crucially, we learned about the ideal BNN, which introduced us to the core ideas underlying BDL, and the computational difficulties of achieving this in practice. We also covered the fundamental practical methods used in BDL, giving us a grounding in the concepts that allow us to implement computationally tractable BNNs.</p>
<p>The chapter also introduced the concept of uncertainty sources, describing the difference between data and model uncertainty, how these contribute to total uncertainty, and how we can estimate the contributions of different types of uncertainty with various models. We also introduced one of the most fundamental components in probabilistic inference – the likelihood function – and learned about how it can help us to train better principled and better calibrated models. Lastly, we were introduced to TensorFlow Probability: a powerful library for probabilistic inference, and a crucial component of the practical examples later in the book.</p>
<p>Now that we’ve covered these fundamentals, we’re ready to see how the concepts we’ve encountered so far can be applied in the implementation of several key BDL models. We’ll learn about the advantages and disadvantages of these approaches, and how to apply them to a variety of real-world problems. Continue on to <a href="CH5.xhtml#x1-600005"><em>Chapter 5</em></a>, <a href="CH5.xhtml#x1-600005"><em>Principled Approaches for Bayesian</em> <em>Deep Learning</em></a>, where we’ll learn about two key principled approaches for BDL. <span id="x1-58001r112"></span></p>
</section>
<section id="further-reading-2" class="level2 sectionHead" data-number="9.6">
<h2 class="sectionHead" data-number="9.6" id="sigil_toc_id_53"><span class="titlemark">4.6 </span> <span id="x1-590006"></span>Further reading</h2>
<p>This chapter has introduced the material necessary to start working with BDL; however, there are many resources that go into more depth on the topics of uncertainty sources. The following are a few recommendations for readers interested in exploring the theory and code in more depth:</p>
<ul>
<li><p><em>Machine Learning: A Probabilistic Perspective, Murphy</em>: Kevin Murphy’s extremely popular book on machine learning has become a staple for students and researchers in the field. This book provides a detailed treatment of machine learning from a probabilistic standpoint, unifying concepts from statistics, machine learning, and Bayesian probability.</p></li>
<li><p><em>TensorFlow Probability</em> <em>Tutorials</em>: in this book, we’ll see how TensorFlow Probability can be used to develop BNNs, but their website includes a wide array of tutorials addressing probabilistic programming more generally: <a href="https://www.tensorflow.org/probability/overview" class="url"><span class="No-Break">https://www.tensorflow.org/probability/overview</span></a></p></li>
<li><p><em>Pyro Tutorials</em>: Pyro is a PyTorch-based library for probabilistic programming – it’s another powerful tool for Bayesian inference, and the Pyro website has many excellent tutorials and examples of probabilistic inference: <a href="https://pyro.ai/" class="url"><span class="No-Break">https://pyro.ai/</span></a>.</p></li>
</ul>
<p><span id="x1-59001r88"></span></p>
</section>
</section>
</body>
</html>