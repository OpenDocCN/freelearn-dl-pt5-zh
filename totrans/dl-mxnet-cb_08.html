<html><head></head><body>
<div id="_idContainer203">
<h1 class="c apter-number" id="_idParaDest-171"><a id="_idTextAnchor172"/><span class="koboSpan" id="kobo.1.1">8</span></h1>
<h1 id="_idParaDest-172"><a id="_idTextAnchor173"/><span class="koboSpan" id="kobo.2.1">Improving Training Performance with MXNet</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In previous chapters, we have leveraged MXNet capabilities to solve computer vision and </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">Natural Language Processing</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">NLP</span></strong><span class="koboSpan" id="kobo.7.1">) tasks. </span><span class="koboSpan" id="kobo.7.2">In those chapters, the focus was on obtaining the maximum performance out of pre-trained models, leveraging the </span><em class="italic"><span class="koboSpan" id="kobo.8.1">Model Zoos</span></em><span class="koboSpan" id="kobo.9.1"> from </span><strong class="source-inline"><span class="koboSpan" id="kobo.10.1">GluonCV</span></strong><span class="koboSpan" id="kobo.11.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.12.1">GluonNLP</span></strong><span class="koboSpan" id="kobo.13.1">. </span><span class="koboSpan" id="kobo.13.2">We trained these models using different approaches: </span><em class="italic"><span class="koboSpan" id="kobo.14.1">from scratch</span></em><span class="koboSpan" id="kobo.15.1">, </span><em class="italic"><span class="koboSpan" id="kobo.16.1">transfer learning</span></em><span class="koboSpan" id="kobo.17.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.18.1">fine-tuning</span></em><span class="koboSpan" id="kobo.19.1">. </span><span class="koboSpan" id="kobo.19.2">In this chapter, we will focus on improving the performance of the training process itself and accelerating how we can obtain </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">those results.</span></span></p>
<p><span class="koboSpan" id="kobo.21.1">To achieve the objective of optimizing the performance of our training loops, MXNet contains different features. </span><span class="koboSpan" id="kobo.21.2">We have already briefly used some of those features such as the concept of </span><strong class="bold"><span class="koboSpan" id="kobo.22.1">lazy evaluation</span></strong><span class="koboSpan" id="kobo.23.1">, which was introduced in </span><a href="B16591_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.24.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.25.1">. </span><span class="koboSpan" id="kobo.25.2">We will revisit it in this chapter, in combination with automatic parallelization. </span><span class="koboSpan" id="kobo.25.3">Moreover, we will optimize how to access data efficiently, leveraging Gluon DataLoaders in different contexts (CPU, GPU) to perform </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">data transforms.</span></span></p>
<p><span class="koboSpan" id="kobo.27.1">Moreover, we will explore how to combine multiple GPUs to accelerate training, making use of techniques such as data parallelization for optimal performance. </span><span class="koboSpan" id="kobo.27.2">We will also explore how we can use different data types with </span><strong class="bold"><span class="koboSpan" id="kobo.28.1">Automatic Mixed Precision</span></strong><span class="koboSpan" id="kobo.29.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.30.1">AMP</span></strong><span class="koboSpan" id="kobo.31.1">), allowing </span><strong class="source-inline"><span class="koboSpan" id="kobo.32.1">MXNet</span></strong><span class="koboSpan" id="kobo.33.1"> to dynamically optimize the different </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">data formats.</span></span></p>
<p><span class="koboSpan" id="kobo.35.1">Finally, using problems already explored in the book, we will apply all these techniques with examples. </span><span class="koboSpan" id="kobo.35.2">For our computer vision task, we will choose image segmentation, and for our NLP task, we will choose translating text from English </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">to German.</span></span></p>
<p><span class="koboSpan" id="kobo.37.1">Specifically, this chapter is structured with the </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">following recipes:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.39.1">Introducing training </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">optimization features</span></span></li>
<li><span class="koboSpan" id="kobo.41.1">Optimizing training for </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">image segmentation</span></span></li>
<li><span class="koboSpan" id="kobo.43.1">Optimizing training for translating text from English </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">to German</span></span></li>
</ul>
<h1 id="_idParaDest-173"><a id="_idTextAnchor174"/><span class="koboSpan" id="kobo.45.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.46.1">Apart from the technical requirements specified in the </span><em class="italic"><span class="koboSpan" id="kobo.47.1">Preface</span></em><span class="koboSpan" id="kobo.48.1">, the following technical </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">requirements apply:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.50.1">Ensure that you have completed the </span><em class="italic"><span class="koboSpan" id="kobo.51.1">Installing MXNet, Gluon, GluonCV and GluonNLP</span></em><span class="koboSpan" id="kobo.52.1"> recipe from </span><a href="B16591_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.53.1">Chapter 1</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.54.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.55.1">Ensure that you have completed </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.56.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.57.1"> and </span><a href="B16591_06.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.58.1">Chapter 6</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.59.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.60.1">Ensure that you have completed </span><a href="B16591_07.xhtml#_idTextAnchor148"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.61.1">Chapter 7</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.62.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.63.1">The code for this chapter can be found at the following GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">URL: </span></span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch08"><span class="No-Break"><span class="koboSpan" id="kobo.65.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch08</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.66.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.67.1">Furthermore, you can access each recipe directly from Google Colab. </span><span class="koboSpan" id="kobo.67.2">For example, the code for the first recipe of this chapter can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">here: </span></span><a href="https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch08/8_1_Introducing_training_optimization_features.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.69.1">https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch08/8_1_Introducing_training_optimization_features.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.70.1">.</span></span></p>
<h1 id="_idParaDest-174"><a id="_idTextAnchor175"/><span class="koboSpan" id="kobo.71.1">Introducing training optimization features</span></h1>
<p><span class="koboSpan" id="kobo.72.1">In the previous chapters, we </span><a id="_idIndexMarker958"/><span class="koboSpan" id="kobo.73.1">saw how we could leverage </span><em class="italic"><span class="koboSpan" id="kobo.74.1">MXNet</span></em><span class="koboSpan" id="kobo.75.1">, </span><em class="italic"><span class="koboSpan" id="kobo.76.1">GluonCV</span></em><span class="koboSpan" id="kobo.77.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.78.1">GluonNLP</span></em><span class="koboSpan" id="kobo.79.1"> to retrieve pre-trained models in certain datasets (such as </span><strong class="bold"><span class="koboSpan" id="kobo.80.1">ImageNet</span></strong><span class="koboSpan" id="kobo.81.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.82.1">MS COCO</span></strong><span class="koboSpan" id="kobo.83.1">, or </span><strong class="bold"><span class="koboSpan" id="kobo.84.1">IWSLT2015</span></strong><span class="koboSpan" id="kobo.85.1">) and </span><a id="_idIndexMarker959"/><span class="koboSpan" id="kobo.86.1">use them for our specific tasks</span><a id="_idIndexMarker960"/><span class="koboSpan" id="kobo.87.1"> and</span><a id="_idIndexMarker961"/><span class="koboSpan" id="kobo.88.1"> datasets. </span><span class="koboSpan" id="kobo.88.2">Furthermore, we used transfer learning and fine-tuning techniques to improve the performance on </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">those tasks/datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.90.1">In this recipe, we will introduce (and revisit) several concepts and features that will optimize our training loops, after which we will analyze the </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">trade-offs involved.</span></span></p>
<h2 id="_idParaDest-175"><a id="_idTextAnchor176"/><span class="koboSpan" id="kobo.92.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.93.1">Similar to the previous chapters, in this recipe, we will be using some matrix operations and linear algebra, but it will not be hard at all, as you will find lots of examples and code snippets to facilitate </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">your learning.</span></span></p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor177"/><span class="koboSpan" id="kobo.95.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.96.1">In this recipe, we</span><a id="_idIndexMarker962"/><span class="koboSpan" id="kobo.97.1"> will work through the </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.99.1">Working with lazy evaluation and </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">automatic parallelization</span></span></li>
<li><span class="koboSpan" id="kobo.101.1">Optimizing DataLoaders: GPU preprocessing and </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">CPU threads</span></span></li>
<li><span class="koboSpan" id="kobo.103.1">Training with </span><strong class="source-inline"><span class="koboSpan" id="kobo.104.1">Float32</span></strong><span class="koboSpan" id="kobo.105.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.106.1">Float16</span></strong><span class="koboSpan" id="kobo.107.1">, and Automatic </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">Mixed Precision</span></span></li>
<li><span class="koboSpan" id="kobo.109.1">Training with multiple GPUs and </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">data parallelization</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.111.1">Let’s dive into each of </span><span class="No-Break"><span class="koboSpan" id="kobo.112.1">these steps.</span></span></p>
<h3><span class="koboSpan" id="kobo.113.1">Working with lazy evaluation and automatic parallelization</span></h3>
<p><span class="koboSpan" id="kobo.114.1">In the </span><em class="italic"><span class="koboSpan" id="kobo.115.1">NumPy and MXNet NDArrays</span></em><span class="koboSpan" id="kobo.116.1"> recipe of </span><a href="B16591_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.117.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.118.1">, we introduced lazy evaluation, the</span><a id="_idIndexMarker963"/><span class="koboSpan" id="kobo.119.1"> strategy that MXNet follows when computing operations. </span><span class="koboSpan" id="kobo.119.2">This strategy is optimal </span><a id="_idIndexMarker964"/><span class="koboSpan" id="kobo.120.1">for large compute loads, where the actual calculation is deferred until the values are </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">actually needed.</span></span></p>
<p><span class="koboSpan" id="kobo.122.1">Furthermore, by not executing the computation of the operations until they are needed, MXNet can also parallelize some of those computations, meaning the data involved is not sequentially processed. </span><span class="koboSpan" id="kobo.122.2">This process is done automatically and is very useful when sharing data across multiple hardware resources such as CPUs </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">and GPUs.</span></span></p>
<p><span class="koboSpan" id="kobo.124.1">As a toy example, we can run some experiments with matrix multiplication. </span><span class="koboSpan" id="kobo.124.2">Our first experiment will run the generation of four matrices and then a combination of multiplications among them. </span><span class="koboSpan" id="kobo.124.3">After each computation, we will force the computation to be finalized (by adding calls to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.125.1">wait_to_read()</span></strong><span class="koboSpan" id="kobo.126.1"> function). </span><span class="koboSpan" id="kobo.126.2">We will compute the results for two configurations. </span><span class="koboSpan" id="kobo.126.3">The initial configuration will be to force MXNet to work with one thread (</span><strong class="source-inline"><span class="koboSpan" id="kobo.127.1">NaiveEngine</span></strong><span class="koboSpan" id="kobo.128.1">). </span><span class="koboSpan" id="kobo.128.2">With this configuration, the computation took </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">this long:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.130.1">
Time (s): 134.3672107196594</span></pre> <p><span class="koboSpan" id="kobo.131.1">The second configuration tested will be MXNet in its usual, default configuration (</span><strong class="source-inline"><span class="koboSpan" id="kobo.132.1">ThreadedEnginePerDevice</span></strong><span class="koboSpan" id="kobo.133.1">, with four CPU threads). </span><span class="koboSpan" id="kobo.133.2">With this configuration, the computation took </span><span class="No-Break"><span class="koboSpan" id="kobo.134.1">this long:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.135.1">
Time (s): 135.26983547210693</span></pre> <p><span class="koboSpan" id="kobo.136.1">As we can see, forcing each computation to be finalized before moving to the next one (</span><strong class="source-inline"><span class="koboSpan" id="kobo.137.1">wait_to_read()</span></strong><span class="koboSpan" id="kobo.138.1"> calls) is counter-productive when working in a </span><span class="No-Break"><span class="koboSpan" id="kobo.139.1">multi-threading configuration.</span></span></p>
<p><span class="koboSpan" id="kobo.140.1">Our </span><a id="_idIndexMarker965"/><span class="koboSpan" id="kobo.141.1">second experiment will be very similar; however, this time, we will remove all calls to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.142.1">wait_to_read()</span></strong><span class="koboSpan" id="kobo.143.1"> function. </span><span class="koboSpan" id="kobo.143.2">We will only ensure that all calculations for the multiplication of the matrices are finalized before computing the time taken. </span><span class="koboSpan" id="kobo.143.3">For the initial configuration (</span><em class="italic"><span class="koboSpan" id="kobo.144.1">NaiveEngine</span></em><span class="koboSpan" id="kobo.145.1">), the computation takes the following amount </span><span class="No-Break"><span class="koboSpan" id="kobo.146.1">of time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.147.1">
Time (s): 134.47382940625321</span></pre> <p><span class="koboSpan" id="kobo.148.1">As expected, this is a very similar duration to only working with one thread, as all computations </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">are sequential.</span></span></p>
<p><span class="koboSpan" id="kobo.150.1">With our second configuration (</span><em class="italic"><span class="koboSpan" id="kobo.151.1">ThreadedEnginePerDevice</span></em><span class="koboSpan" id="kobo.152.1">, with four CPU threads), the computation for this second experiment took the following amount </span><span class="No-Break"><span class="koboSpan" id="kobo.153.1">of time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.154.1">
Time (s): 111.36750531196594</span></pre> <p><span class="koboSpan" id="kobo.155.1">The results show that when using multiple threads (the default automatic configuration for MXNet), we achieved a ~20% improvement (improvements can be even higher with different workloads more suited </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">for multi-threading).</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.157.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.158.1">Please note how in the code, we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.159.1">mx.nd.waitall()</span></strong><span class="koboSpan" id="kobo.160.1"> function to verify that all computations had been strictly completed before computing the time these </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">operations took.</span></span></p>
<h3><span class="koboSpan" id="kobo.162.1">Optimizing DataLoaders – GPU preprocessing and CPU threads</span></h3>
<p><span class="koboSpan" id="kobo.163.1">In the </span><em class="italic"><span class="koboSpan" id="kobo.164.1">Understanding image datasets – loading, managing, and visualizing the fashion MNIST dataset</span></em><span class="koboSpan" id="kobo.165.1"> recipe of </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.166.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.167.1">, we </span><a id="_idIndexMarker966"/><span class="koboSpan" id="kobo.168.1">introduced </span><strong class="bold"><span class="koboSpan" id="kobo.169.1">Gluon DataLoader</span></strong><span class="koboSpan" id="kobo.170.1">, an efficient mechanism to generate batch sizes to feed into our models for training </span><span class="No-Break"><span class="koboSpan" id="kobo.171.1">and evaluation.</span></span></p>
<p><span class="koboSpan" id="kobo.172.1">DataLoader has two important roles to play in our data preprocessing. </span><span class="koboSpan" id="kobo.172.2">On the one hand, as we have explored in previous chapters, our models are optimized for parallel data processing, meaning that we can ingest several samples (for example, images for an image segmentation task) at </span><a id="_idIndexMarker967"/><span class="koboSpan" id="kobo.173.1">the same time in the same </span><em class="italic"><span class="koboSpan" id="kobo.174.1">batch</span></em><span class="koboSpan" id="kobo.175.1"> and it will be processed in parallel by the </span><em class="italic"><span class="koboSpan" id="kobo.176.1">GPU</span></em><span class="koboSpan" id="kobo.177.1">. </span><span class="koboSpan" id="kobo.177.2">This parameter is called </span><em class="italic"><span class="koboSpan" id="kobo.178.1">batch size</span></em><span class="koboSpan" id="kobo.179.1">. </span><span class="koboSpan" id="kobo.179.2">On the other hand, samples typically need to be preprocessed in order to maximize the performance of the model (for example, images are resized and its values allocated to </span><strong class="source-inline"><span class="koboSpan" id="kobo.180.1">[0,</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.181.1">1]</span></strong><span class="koboSpan" id="kobo.182.1"> from </span><strong class="source-inline"><span class="koboSpan" id="kobo.183.1">[0,</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.184.1">255]</span></strong><span class="koboSpan" id="kobo.185.1">). </span><span class="koboSpan" id="kobo.185.2">These operations are time-consuming and optimizing them can save large amounts of time </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">and compute.</span></span></p>
<p><span class="koboSpan" id="kobo.187.1">Let’s analyze</span><a id="_idIndexMarker968"/><span class="koboSpan" id="kobo.188.1"> the effect of the preprocessing of the data in the GPU, compared to the general, default behavior of using the CPU. </span><span class="koboSpan" id="kobo.188.2">As a baseline, let’s compute how long it takes to just load the dataset using only the CPU. </span><span class="koboSpan" id="kobo.188.3">We select the validation split of a segmentation dataset, and the result is </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.190.1">
Time (s): 24.319150686264038</span></pre> <p><span class="koboSpan" id="kobo.191.1">However, when loading the dataset, we typically apply certain </span><strong class="source-inline"><span class="koboSpan" id="kobo.192.1">transform</span></strong><span class="koboSpan" id="kobo.193.1"> operations that maximize our network performance. </span><span class="koboSpan" id="kobo.193.2">The usual transform operations including image resizing, cropping, transforming to tensors, and normalizing can be defined in MXNet with the </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.195.1">
input_transform_fn = mx.gluon.data.vision.transforms.Compose([
mx.gluon.data.vision.transforms.Resize(image_size, keep_ratio=True),
mx.gluon.data.vision.transforms.CenterCrop(image_size), mx.gluon.data.vision.transforms.ToTensor(),
mx.gluon.data.vision.transforms.Normalize([.485, .456, .406], [.229, .224, .225])
])</span></pre> <p><span class="koboSpan" id="kobo.196.1">With these transform operations applied when processing the validation split of a segmentation dataset using only the CPU, the processing time is </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.198.1">
Time (s): 38.973774433135986</span></pre> <p><span class="koboSpan" id="kobo.199.1">As we can see, the processing time has increased by more than 50%, from ~24s to ~39s. </span><span class="koboSpan" id="kobo.199.2">However, when we leverage the GPU for the data preprocessing, the processing time is </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.201.1">
Time (s): 25.39602303504944</span></pre> <p><span class="koboSpan" id="kobo.202.1">As we can see, the GPU-based preprocessing operations have an overhead that is almost </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">negligible (&lt;5%).</span></span></p>
<p><span class="koboSpan" id="kobo.204.1">Furthermore, performing</span><a id="_idIndexMarker969"/><span class="koboSpan" id="kobo.205.1"> the preprocessing in the GPU has another advantage: the data can be kept stored in the GPU for our models to process, whereas when preprocessing with the CPU, we need to send a copy of the data to the GPU memory, which can take a significant amount of time. </span><span class="koboSpan" id="kobo.205.2">If we actually measure our end-to-end preprocessing pipeline, combining the data preprocessing with the copy operation to the GPU memory, these are the results. </span><span class="koboSpan" id="kobo.205.3">With the CPU only, the end-to-end processing time is </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.207.1">
Time (s): 67.73443150520325</span></pre> <p><span class="koboSpan" id="kobo.208.1">As we can see, the copy time is significant, taking the whole pipeline more than 1 minute. </span><span class="koboSpan" id="kobo.208.2">However, the result when using the GPU is </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.210.1">
Time (s): 23.22727918624878</span></pre> <p><span class="koboSpan" id="kobo.211.1">This shows a significant improvement (&lt;40%) in the time it took for the full preprocessing. </span><span class="koboSpan" id="kobo.211.2">In summary, this was due to two factors: the fact that the preprocessing operations are faster in the GPU, and secondly, that the data needs to be copied to the GPU at the end of the process so that our models (which are also stored in the GPU) process the </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">data efficiently.</span></span></p>
<p><span class="koboSpan" id="kobo.213.1">The most important drawback of this approach is the need to keep the full dataset in the GPU. </span><span class="koboSpan" id="kobo.213.2">Typically, GPU memory space is optimized for each batch that you use for training or inference, not the whole dataset. </span><span class="koboSpan" id="kobo.213.3">This is the reason why this approach typically finishes with the processed data being copied back out of the GPU memory space and into the CPU </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">memory space.</span></span></p>
<p><span class="koboSpan" id="kobo.215.1">However, there are situations where keeping the data in the GPU memory space might be the right approach – for example, when you are experimenting with datasets, and maybe loading several datasets and testing different preprocessing pipelines. </span><span class="koboSpan" id="kobo.215.2">In these situations you want fast turn-around times for your experiments and, therefore, speed is the right variable to optimize for. </span><span class="koboSpan" id="kobo.215.3">Moreover, sometimes you are not working with the full training/validation/test splits of a dataset, but just a part of it (again, for example, for experiments). </span><span class="koboSpan" id="kobo.215.4">In these cases, optimizing for speed makes sense </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">as well.</span></span></p>
<p><span class="koboSpan" id="kobo.217.1">For other, more production-oriented environments, the right approach is to preprocess in GPU memory space but keep the data (copying back) in CPU memory space. </span><span class="koboSpan" id="kobo.217.2">In this scenario, the results </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">vary slightly:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.219.1">
Time (s): 34.58254957199097</span></pre> <p><span class="koboSpan" id="kobo.220.1">As we can see, the </span><a id="_idIndexMarker970"/><span class="koboSpan" id="kobo.221.1">preprocessing step being done in the GPU is still a significant increase (~50%) in performance, even taking into account the necessary data movements from the CPU to the GPU </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">and back.</span></span></p>
<p><span class="koboSpan" id="kobo.223.1">Now, we will take a deeper look at how we can leverage two important parameters that Gluon DataLoader takes as input: the number of workers and the batch size. </span><span class="koboSpan" id="kobo.223.2">The number of workers is the number of threads that DataLoader will launch in parallel (multi-threading) for data preprocessing. </span><span class="koboSpan" id="kobo.223.3">Batch size, as mentioned, is the number of samples that will be processed </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">in parallel.</span></span></p>
<p><span class="koboSpan" id="kobo.225.1">These parameters are directly related to the number of cores the CPU has, and can be optimized to use the available HW for maximum performance. </span><span class="koboSpan" id="kobo.225.2">To find out how many cores our CPU has, Python provides a very </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">simple API:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.227.1">
import multiprocessing
multiprocessing.cpu_count()</span></pre> <p><span class="koboSpan" id="kobo.228.1">In the environment selected, the number of cores available is shown </span><span class="No-Break"><span class="koboSpan" id="kobo.229.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.230.1">
4</span></pre> <p><span class="koboSpan" id="kobo.231.1">Combining the usage of the CPU and the GPU, we can compute the best performance taking into account different values for the number of workers and the batch size. </span><span class="koboSpan" id="kobo.231.2">The results for the selected environment are </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer196">
<span class="koboSpan" id="kobo.233.1"><img alt="Figure 8.1 – Runtim﻿e(s) versus Batch Size for different computation regimes (CPU/GPU and number of workers)" src="image/B16591_08_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.234.1">Figure 8.1 – Runtime(s) versus Batch Size for different computation regimes (CPU/GPU and number of workers)</span></p>
<p><span class="koboSpan" id="kobo.235.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.236.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.237.1">.1</span></em><span class="koboSpan" id="kobo.238.1">, we can </span><a id="_idIndexMarker971"/><span class="koboSpan" id="kobo.239.1">conclude three </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">important aspects:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.241.1">A GPU preprocessing pipeline (data processing plus memory storage) is much faster (+50% runtime improvement), even when copying back </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">the data.</span></span></li>
<li><span class="koboSpan" id="kobo.243.1">When combining both the GPU and CPU, as we are only working with one GPU in this environment, it bottlenecks when we copy the data back to the CPU, as it’s done per sample (not </span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">per batch).</span></span></li>
<li><span class="koboSpan" id="kobo.245.1">If working only with a CPU, adding workers improves the processing time. </span><span class="koboSpan" id="kobo.245.2">However, the limit is the number of threads. </span><span class="koboSpan" id="kobo.245.3">Adding more workers than threads (four in our case) will give no improvement in performance. </span><span class="koboSpan" id="kobo.245.4">An increase in the batch size yields better performance until a given number (8 in our case) and won’t improve performance further </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">than that.</span></span></li>
</ul>
<p class="callout- eading"><span class="koboSpan" id="kobo.247.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.248.1">When working with the GPU, the MXNet Gluon DataLoader only supports the value </span><strong class="source-inline"><span class="koboSpan" id="kobo.249.1">0</span></strong><span class="koboSpan" id="kobo.250.1"> (zero) for the number </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">of workers.</span></span></p>
<h3><span class="koboSpan" id="kobo.252.1">Training with Float32, Float16, and automatic mixed precision</span></h3>
<p><span class="koboSpan" id="kobo.253.1">In the previous recipes, we have seen how to optimize our training loops by using different approaches to optimize the CPU and the GPU for maximum performance for a given model. </span><span class="koboSpan" id="kobo.253.2">In this recipe, we will explore how our data inputs, our model parameters, and the different arithmetic calculations around them are computed, and how we can </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">optimize them.</span></span></p>
<p><span class="koboSpan" id="kobo.255.1">First of all, let’s understand how </span><a id="_idIndexMarker972"/><span class="koboSpan" id="kobo.256.1">computations work. </span><span class="koboSpan" id="kobo.256.2">The default data type for the data inputs and the model parameters is </span><strong class="source-inline"><span class="koboSpan" id="kobo.257.1">Float32</span></strong><span class="koboSpan" id="kobo.258.1">, as can be verified (see the recipe code), which yields the </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">following output:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.260.1">
Input data type: &lt;class 'numpy.float32'&gt; Model Parameters data type: &lt;class 'numpy.float32'&gt;</span></pre> <p><span class="koboSpan" id="kobo.261.1">This output </span><a id="_idIndexMarker973"/><span class="koboSpan" id="kobo.262.1">indicates, as expected, that the data type of our data inputs and our model parameters is </span><strong class="source-inline"><span class="koboSpan" id="kobo.263.1">Float32</span></strong><span class="koboSpan" id="kobo.264.1"> (single-precision). </span><span class="koboSpan" id="kobo.264.2">But what does </span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">this mean?</span></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.266.1">Float32</span></strong><span class="koboSpan" id="kobo.267.1"> indicates two things: on the one hand, that it is a data type that supports decimal numbers using a floating radix point, and on the other hand, that 32 bits are used to store a single number in this format. </span><span class="koboSpan" id="kobo.267.2">The most important features of this format are </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.269.1">The ability to represent large numbers, from 10</span><span class="superscript"><span class="koboSpan" id="kobo.270.1">-45</span></span> <span class="No-Break"><span class="koboSpan" id="kobo.271.1">to 10</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.272.1">+38</span></span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.273.1">Variable precision</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.274.1">Using </span><strong class="source-inline"><span class="koboSpan" id="kobo.275.1">Float32</span></strong><span class="koboSpan" id="kobo.276.1"> as </span><a id="_idIndexMarker974"/><span class="koboSpan" id="kobo.277.1">the data type has numerous advantages, mostly connected to its variable precision. </span><span class="koboSpan" id="kobo.277.2">However, the training process is an iterative optimization process, for which many of the calculations involved </span><a id="_idIndexMarker975"/><span class="koboSpan" id="kobo.278.1">do not require the precision of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">Float32</span></strong><span class="koboSpan" id="kobo.280.1"> data type. </span><span class="koboSpan" id="kobo.280.2">We could afford, in a controlled way, to trade off some precision if it allowed us to speed up the training process. </span><span class="koboSpan" id="kobo.280.3">One of the ways we can achieve that balanced trade-off is with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.281.1">Float16</span></strong><span class="koboSpan" id="kobo.282.1"> data type (half-precision). </span><span class="koboSpan" id="kobo.282.2">Similarly to </span><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">Float32</span></strong><span class="koboSpan" id="kobo.284.1">, the most important features of </span><strong class="source-inline"><span class="koboSpan" id="kobo.285.1">Float16</span></strong><span class="koboSpan" id="kobo.286.1"> are </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.288.1">The ability to represent large numbers, from 2</span><span class="superscript"><span class="koboSpan" id="kobo.289.1">-24</span></span> <span class="No-Break"><span class="koboSpan" id="kobo.290.1">to 2</span></span><span class="No-Break"><span class="superscript"><span class="koboSpan" id="kobo.291.1">+16</span></span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.292.1">Variable precision</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.293.1">As an example of the loss of precision, we can display the approximated value of 1/3 in both formats with this </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">code excerpt:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.295.1">
a = mx.nd.array([1/3], dtype=np.float32)
 b = a.astype(np.float16)
print("1/3 as Float32: {0:.30f}".format(a.asscalar()))
print("1/3 as Float16: {0:.30f}".format(b.asscalar()))</span></pre> <p><span class="koboSpan" id="kobo.296.1">This yields </span><span class="No-Break"><span class="koboSpan" id="kobo.297.1">the</span></span><span class="No-Break"><a id="_idIndexMarker976"/></span><span class="No-Break"><span class="koboSpan" id="kobo.298.1"> following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.299.1">
1/3 as Float32: 0.333333343267440795898437500000
1/3 as Float16: 0.333251953125000000000000000000</span></pre> <p><span class="koboSpan" id="kobo.300.1">As we can see, none of the </span><a id="_idIndexMarker977"/><span class="koboSpan" id="kobo.301.1">representations are exact, with </span><strong class="source-inline"><span class="koboSpan" id="kobo.302.1">Float32</span></strong><span class="koboSpan" id="kobo.303.1"> yielding higher precision as expected, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.304.1">Float16</span></strong><span class="koboSpan" id="kobo.305.1"> having more limited accuracy, but potentially enough for some use cases (such as model training, as we will </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">prove shortly).</span></span></p>
<p><span class="koboSpan" id="kobo.307.1">As mentioned, this loss of </span><a id="_idIndexMarker978"/><span class="koboSpan" id="kobo.308.1">accuracy is a trade-off, where </span><a id="_idIndexMarker979"/><span class="koboSpan" id="kobo.309.1">we obtain large speed gains in our training loops. </span><span class="koboSpan" id="kobo.309.2">To enable </span><strong class="source-inline"><span class="koboSpan" id="kobo.310.1">Float16</span></strong><span class="koboSpan" id="kobo.311.1"> (half-precision) for our training loops, we need to apply certain changes to our code. </span><span class="koboSpan" id="kobo.311.2">First of all, we need to update our model parameters to </span><strong class="source-inline"><span class="koboSpan" id="kobo.312.1">Float16</span></strong><span class="koboSpan" id="kobo.313.1">, which we can do with one simple line </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.315.1">
deeplab_ft_direct_f16.cast('float16')</span></pre> <p><span class="koboSpan" id="kobo.316.1">After this, when our model is going to process the data and ground truth, these need to be updated to </span><strong class="source-inline"><span class="koboSpan" id="kobo.317.1">Float16</span></strong><span class="koboSpan" id="kobo.318.1"> too, so in our training loop, we add the </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">following lines:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.320.1">
data  = data.astype('float16', copy=False)
 label = label.astype('float16', copy=False)</span></pre> <p><span class="koboSpan" id="kobo.321.1">With these changes, we can now run an experiment to compare the performance of both training loops. </span><span class="koboSpan" id="kobo.321.2">For example, we are going to fine-tune a DeepLabv3 pre-trained model in an image segmentation task (see the </span><em class="italic"><span class="koboSpan" id="kobo.322.1">Improving performance for segmenting images </span></em><span class="koboSpan" id="kobo.323.1">recipe of </span><a href="B16591_07.xhtml#_idTextAnchor148"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.324.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.325.1">). </span><span class="koboSpan" id="kobo.325.2">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.326.1">Float32</span></strong><span class="koboSpan" id="kobo.327.1">, we obtain the </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">following results:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.329.1">
Training time for 10 epochs: 594.4833037853241 / Best validation loss: 0.6800425</span></pre> <p><span class="koboSpan" id="kobo.330.1">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.331.1">Float16</span></strong><span class="koboSpan" id="kobo.332.1">, these are the results </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">we obtained:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.334.1">
Training time for 10 epochs: 199.80901980400085 / Best validation loss: nan</span></pre> <p><span class="koboSpan" id="kobo.335.1">Unfortunately, for </span><strong class="source-inline"><span class="koboSpan" id="kobo.336.1">Float16</span></strong><span class="koboSpan" id="kobo.337.1">, although our training time took ~1/3</span><span class="superscript"><span class="koboSpan" id="kobo.338.1">rd</span></span><span class="koboSpan" id="kobo.339.1"> than the </span><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">Float32</span></strong><span class="koboSpan" id="kobo.341.1"> training loop, it did </span><a id="_idIndexMarker980"/><span class="koboSpan" id="kobo.342.1">not converge. </span><span class="koboSpan" id="kobo.342.2">This is due to </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">several reasons:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.344.1">Limited support for large numbers, as any integer larger than </span><strong class="source-inline"><span class="koboSpan" id="kobo.345.1">65519</span></strong><span class="koboSpan" id="kobo.346.1"> is represented </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">as infinity</span></span></li>
<li><span class="koboSpan" id="kobo.348.1">Limited support for small numbers, as any positive decimal number smaller than </span><strong class="source-inline"><span class="koboSpan" id="kobo.349.1">1e-7</span></strong><span class="koboSpan" id="kobo.350.1"> is represented as </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.351.1">0</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.352.1"> (zero)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.353.1">Thankfully, MXNet</span><a id="_idIndexMarker981"/><span class="koboSpan" id="kobo.354.1"> offers a solution that automatically combines the best of </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">both worlds:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.356.1">Applying </span><strong class="source-inline"><span class="koboSpan" id="kobo.357.1">Float32</span></strong><span class="koboSpan" id="kobo.358.1"> (single-precision) where it </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">is necessary</span></span></li>
<li><span class="koboSpan" id="kobo.360.1">Applying </span><strong class="source-inline"><span class="koboSpan" id="kobo.361.1">Float16</span></strong><span class="koboSpan" id="kobo.362.1"> (half-precision) where it is not, for </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">runtime optimization</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.364.1">This approach is</span><a id="_idIndexMarker982"/><span class="koboSpan" id="kobo.365.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.366.1">Automatic Mixed Precision</span></strong><span class="koboSpan" id="kobo.367.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.368.1">AMP</span></strong><span class="koboSpan" id="kobo.369.1">), and in order to enable it, we just </span><a id="_idIndexMarker983"/><span class="koboSpan" id="kobo.370.1">need to make a few </span><a id="_idIndexMarker984"/><span class="koboSpan" id="kobo.371.1">changes in our code. </span><span class="koboSpan" id="kobo.371.2">First of all, before creating our model, we need to initialize </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">the library:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.373.1">
amp.init()</span></pre> <p><span class="koboSpan" id="kobo.374.1">Then, after initializing our trainer/optimizer, we need to link it </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">with AMP:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.376.1">
amp.init_trainer(trainer)</span></pre> <p><span class="koboSpan" id="kobo.377.1">And finally, in order to prevent underflow or overflow, we need to enable </span><strong class="bold"><span class="koboSpan" id="kobo.378.1">loss scaling</span></strong><span class="koboSpan" id="kobo.379.1">, an </span><a id="_idIndexMarker985"/><span class="koboSpan" id="kobo.380.1">MXNet feature that allows for the loss computed during the training loop to be adjusted (multiplied or divided) to always be in the range supported by the </span><strong class="source-inline"><span class="koboSpan" id="kobo.381.1">Float16</span></strong><span class="koboSpan" id="kobo.382.1"> data type. </span><span class="koboSpan" id="kobo.382.2">This is done quite conveniently in the </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">training loop:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.384.1">
with amp.scale_loss(loss, trainer) as scaled_loss:
mx.autograd.backward(scaled_loss)</span></pre> <p><span class="koboSpan" id="kobo.385.1">When we apply these changes and repeat the previous experiment for </span><strong class="source-inline"><span class="koboSpan" id="kobo.386.1">Float16</span></strong><span class="koboSpan" id="kobo.387.1"> (now with AMP enabled), we obtain the </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">following results:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.389.1">
Training time for 10 epochs: 217.64903020858765 / Best validation loss: 0.7082735</span></pre> <p><span class="koboSpan" id="kobo.390.1">As we can see, we obtained very similar results for the validation loss in a much shorter amount of </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">time (~33%).</span></span></p>
<p><span class="koboSpan" id="kobo.392.1">As the memory</span><a id="_idIndexMarker986"/><span class="koboSpan" id="kobo.393.1"> footprint of our training loop is now approximately half of what it was before, we can typically either double the size of our model (more layers and larger resolutions), or double our batch size, as the GPU memory consumed will be the same in this case compared to a full </span><strong class="source-inline"><span class="koboSpan" id="kobo.394.1">Float32</span></strong><span class="koboSpan" id="kobo.395.1"> training loop. </span><span class="koboSpan" id="kobo.395.2">Running the same experiment with a double batch size yields the </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">following results:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.397.1">
Training time for 10 epochs: 218.82141995429993 / Best validation loss: 0.18198483</span></pre> <p><span class="koboSpan" id="kobo.398.1">As we can see, increasing the batch size has an excellent effect on the performance of our training loop, with a much lower validation loss, and still benefiting from a significantly smaller amount of training </span><span class="No-Break"><span class="koboSpan" id="kobo.399.1">time (~33%).</span></span></p>
<p><span class="koboSpan" id="kobo.400.1">However, typically, as a </span><strong class="bold"><span class="koboSpan" id="kobo.401.1">Machine Learning Engineer</span></strong><span class="koboSpan" id="kobo.402.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.403.1">MLE</span></strong><span class="koboSpan" id="kobo.404.1">) or </span><strong class="bold"><span class="koboSpan" id="kobo.405.1">Data Scientist </span></strong><span class="koboSpan" id="kobo.406.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.407.1">DS</span></strong><span class="koboSpan" id="kobo.408.1">), we will </span><a id="_idIndexMarker987"/><span class="koboSpan" id="kobo.409.1">work with </span><a id="_idIndexMarker988"/><span class="koboSpan" id="kobo.410.1">large amounts of data and large models, running</span><a id="_idIndexMarker989"/><span class="koboSpan" id="kobo.411.1"> training loops expected to last for hours or days. </span><span class="koboSpan" id="kobo.411.2">Therefore, it is very common for MLEs/DSs at work to start training loops just before the end of the working day, leaving the training running in the background, and coming back the next working day to analyze and evaluate the results. </span><span class="koboSpan" id="kobo.411.3">In such an environment, it is actually a better strategy to optimize performance given an expected training time. </span><span class="koboSpan" id="kobo.411.4">With MXNet, we can optimize our training parameters for this as well. </span><span class="koboSpan" id="kobo.411.5">For example, we could adjust the training time by doubling the number of epochs. </span><span class="koboSpan" id="kobo.411.6">In this case, the experiment yields the </span><span class="No-Break"><span class="koboSpan" id="kobo.412.1">following results:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.413.1">
Training time for 10 epochs: 645.7392318248749 / Best validation loss: 0.16439788</span></pre> <p><span class="koboSpan" id="kobo.414.1">Compared to a vanilla </span><strong class="source-inline"><span class="koboSpan" id="kobo.415.1">Float32</span></strong><span class="koboSpan" id="kobo.416.1"> training loop, these results are excellent. </span><span class="koboSpan" id="kobo.416.2">However, let’s not forget that the actual results depend on the specific task, datasets, model, hyperparameters, and so on. </span><span class="koboSpan" id="kobo.416.3">You are encouraged to try different options and hyperparameters with toy training loops to find the solution that works best for </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">each case.</span></span></p>
<h3><span class="koboSpan" id="kobo.418.1">Training with multiple GPUs and data parallelization</span></h3>
<p><span class="koboSpan" id="kobo.419.1">In this</span><a id="_idIndexMarker990"/><span class="koboSpan" id="kobo.420.1"> recipe, we will leverage having </span><a id="_idIndexMarker991"/><span class="koboSpan" id="kobo.421.1">multiple GPUs in our environment to optimize our training further. </span><span class="koboSpan" id="kobo.421.2">MXNet and Gluon allow us to update our training loops to include multiple GPUs </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">very easily.</span></span></p>
<p><span class="koboSpan" id="kobo.423.1">From a high-level perspective, there are two paradigms to leverage </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">multiple GPUs:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.425.1">Model parallelization</span></strong><span class="koboSpan" id="kobo.426.1">: The </span><a id="_idIndexMarker992"/><span class="koboSpan" id="kobo.427.1">model is split into parts and each part is deployed to a specific GPU. </span><span class="koboSpan" id="kobo.427.2">This paradigm is very useful when the model does not fit in a </span><span class="No-Break"><span class="koboSpan" id="kobo.428.1">single GPU.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.429.1">Data parallelization</span></strong><span class="koboSpan" id="kobo.430.1">: The </span><a id="_idIndexMarker993"/><span class="koboSpan" id="kobo.431.1">data batches are split into parts and each part is deployed to a specific GPU that can perform a forward and a backward pass using that </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">data fully.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.433.1">We will work exclusively with data parallelization as it is the most common use case, yielding high speed-ups, and is also the most convenient given the simplicity of </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">its approach.</span></span></p>
<p><span class="koboSpan" id="kobo.435.1">In order to apply data </span><a id="_idIndexMarker994"/><span class="koboSpan" id="kobo.436.1">parallelization, we will need to make modifications to our training loop, </span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">as follows:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.438.1">Setting the context</span></strong><span class="koboSpan" id="kobo.439.1">: The context is now a list, where each element is a specific </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">GPU context.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.441.1">Initializing our model in those contexts</span></strong><span class="koboSpan" id="kobo.442.1">: In data parallelization, each GPU will store a copy of all the </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">model parameters.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.444.1">Adapting hyperparameters</span></strong><span class="koboSpan" id="kobo.445.1">: Batch size is typically set to the largest possible without filling up the GPU memory. </span><span class="koboSpan" id="kobo.445.2">When working with several GPUs in parallel, this number can typically be multiplied by the number of GPUs in the context. </span><span class="koboSpan" id="kobo.445.3">However, this also has a side effect on the learning rate, which must be multiplied by the same number to keep gradient updates in the </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">same range.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.447.1">Distributing the data</span></strong><span class="koboSpan" id="kobo.448.1">: Each GPU must have a slice of each batch and run the forward and backward passes </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">with it.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.450.1">Computing the losses and updating the gradients</span></strong><span class="koboSpan" id="kobo.451.1">: Each GPU will compute the losses associated with their slice of each batch. </span><span class="koboSpan" id="kobo.451.2">MXNet automatically </span><a id="_idIndexMarker995"/><span class="koboSpan" id="kobo.452.1">combines the losses and computes the gradients that are distributed to each GPU to update their </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">model copy.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.454.1">Displaying results</span></strong><span class="koboSpan" id="kobo.455.1">: Statistics such as the training loss and the validation loss are typically computed and accumulated during each batch and visualized at the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.456.1">each epoch.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.457.1">Let’s see some</span><a id="_idIndexMarker996"/><span class="koboSpan" id="kobo.458.1"> examples of how to apply each of </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">these steps.</span></span></p>
<p><span class="koboSpan" id="kobo.460.1">For example, to set the context in an environment with four GPUs is very easy with MXNet and just requires one line </span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.462.1">
ctx_list = [mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3)]</span></pre> <p><span class="koboSpan" id="kobo.463.1">Initializing the model and custom layers is as easy as that. </span><span class="koboSpan" id="kobo.463.2">For our environment, this is how we can initialize a Deeplabv3 network with a </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.464.1">ResNet-101</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.465.1"> backbone:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.466.1">
deeplab_ft_direct_f32 = gcv.model_zoo.get_model('deeplab_resnet101_coco', pretrained=True, ctx=ctx_list)
 [...]
deeplab_ft_direct_f32.head.initialize(ctx=ctx_list)</span></pre> <p><span class="koboSpan" id="kobo.467.1">To update the hyperparameters, we just need to compute the number of GPUs in the context and update the previously computed batch size and learning rates. </span><span class="koboSpan" id="kobo.467.2">For our example, this simply means adding/modifying some lines </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.469.1">
num_gpus = len(ctx_list)
 [...]
batch_size_per_gpu = 4
batch_size = len(ctx_list) * batch_size_per_gpu
 [...]
trainer = mx.gluon.Trainer(deeplab_ft_direct_f32.collect_params(), "sgd", {"learning_rate": 0.5})</span></pre> <p><span class="koboSpan" id="kobo.470.1">In</span><a id="_idIndexMarker997"/><span class="koboSpan" id="kobo.471.1"> order to distribute the data evenly across every GPU, MXNet and Gluon have a very convenient function, </span><strong class="source-inline"><span class="koboSpan" id="kobo.472.1">split_and_load()</span></strong><span class="koboSpan" id="kobo.473.1">, which automatically allocates the data according to the number of GPUs in the context. </span><span class="koboSpan" id="kobo.473.2">For our environment, this is done </span><span class="No-Break"><span class="koboSpan" id="kobo.474.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.475.1">
data_list   = mx.gluon.utils.split_and_load(data, ctx_list=ctx_list)
 label_list  = mx.gluon.utils.split_and_load(label, ctx_list=ctx_list)</span></pre> <p><span class="koboSpan" id="kobo.476.1">To compute the losses and update the gradients, the data distributed in each GPU is processed in parallel using a loop. </span><span class="koboSpan" id="kobo.476.2">As MXNet provides automatic parallelization, the calls are not blocking and each GPU computes its outputs and losses independently. </span><span class="koboSpan" id="kobo.476.3">Furthermore, MXNet combines those losses to generate the full gradient updates, and redistributes this to each GPU, and all of this is done automatically. </span><span class="koboSpan" id="kobo.476.4">We can achieve all this with just a few lines </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.478.1">
with mx.autograd.record():
outputs = [model(data_slice) for data_slice in data_list]
losses = [loss_fn(output[0], label_slice) for output, label_slice in zip(outputs, label_list)]
for loss in losses:
loss.backward()
trainer.step(batch_size)</span></pre> <p><span class="koboSpan" id="kobo.479.1">Lastly, in order to display the loss computations, each GPU loss needs to be processed and combined. </span><span class="koboSpan" id="kobo.479.2">Using automatic parallelization, this can be achieved with just one line </span><span class="No-Break"><span class="koboSpan" id="kobo.480.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.481.1">
current_loss = sum([l.sum().asscalar() for l in losses])</span></pre> <p><span class="koboSpan" id="kobo.482.1">With these simple steps, we have been able to modify our training loop to support multiple GPUs, and we are now ready to measure the performance increase of </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">these changes.</span></span></p>
<p><span class="koboSpan" id="kobo.484.1">As a </span><a id="_idIndexMarker998"/><span class="koboSpan" id="kobo.485.1">reminder, using one GPU, we reached the following performance (with a batch size </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">of four):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.487.1">
Training time for 10 epochs: 647.753002166748 / Best validation loss: 0.0937674343585968</span></pre> <p><span class="koboSpan" id="kobo.488.1">In our environment, with 4 GPUs, we could increase the batch size to 16, the results of which would be </span><span class="No-Break"><span class="koboSpan" id="kobo.489.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.490.1">
Training time for 10 epochs: 177.23532104492188 / Best validation loss: 0.082047363743186</span></pre> <p><span class="koboSpan" id="kobo.491.1">As expected, we have been able to reduce the time spent in training to ~25% (the expected reduction when going from 1 GPU to 4 GPUs, with some expected loss due to the data distribution) while maintaining our validation scores (even slightly </span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">improving them).</span></span></p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor178"/><span class="koboSpan" id="kobo.493.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.494.1">In this recipe, we take a </span><a id="_idIndexMarker999"/><span class="koboSpan" id="kobo.495.1">deeper look at how MXNet and Gluon can help us optimize our training loops. </span><span class="koboSpan" id="kobo.495.2">We have leveraged our HW (CPUs and GPUs) to address each of the steps in the </span><span class="No-Break"><span class="koboSpan" id="kobo.496.1">training loop:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.497.1">We revisited how lazy evaluation and automatic parallelization mechanisms work together to optimize all </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">MXNet-based flows.</span></span></li>
<li><span class="koboSpan" id="kobo.499.1">We leveraged all our CPU threads to load data and optimized that process further via preprocessing in GPU. </span><span class="koboSpan" id="kobo.499.2">We also compared the trade-offs between speed and </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">memory optimizations.</span></span></li>
<li><span class="koboSpan" id="kobo.501.1">We analyzed different data types and combined the accuracy and precision of </span><strong class="source-inline"><span class="koboSpan" id="kobo.502.1">Float32</span></strong><span class="koboSpan" id="kobo.503.1"> with the speed-ups of </span><strong class="source-inline"><span class="koboSpan" id="kobo.504.1">Float16</span></strong><span class="koboSpan" id="kobo.505.1"> where possible, </span><span class="No-Break"><span class="koboSpan" id="kobo.506.1">using AMP.</span></span></li>
<li><span class="koboSpan" id="kobo.507.1">We increased the performance of our training loops by using multiple GPUs (assuming our HW has these </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">devices available).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.509.1">We compared each of these scenarios by running two experiments, comparing the performance before a specific optimization to the performance afterward, emphasizing potential trade-offs that have to be taken into account when using these optimizations. </span><span class="koboSpan" id="kobo.509.2">In the recipes that follow, we will apply all these optimization techniques concurrently to optimize two </span><a id="_idIndexMarker1000"/><span class="koboSpan" id="kobo.510.1">familiar tasks: </span><strong class="bold"><span class="koboSpan" id="kobo.511.1">image segmentation</span></strong><span class="koboSpan" id="kobo.512.1"> and </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.513.1">text translation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">.</span></span></p>
<h2 id="_idParaDest-178"><a id="_idTextAnchor179"/><span class="koboSpan" id="kobo.515.1">There’s more…</span></h2>
<p><span class="koboSpan" id="kobo.516.1">All the optimization features shown in this recipe have been thoroughly described in the research literature. </span><span class="koboSpan" id="kobo.516.2">The following are some introductory links to start understanding each of the features </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">in depth:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.518.1">Lazy evaluation and automatic </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.519.1">parallelization:</span></strong></span><span class="No-Break"> </span><a href="https://cljdoc.org/d/org.apache.mxnet.contrib.clojure/clojure-mxnet-linux-cpu/1.4.1/doc/ndarray-imperative-tensor-operations-on-cpu-gpu#lazy-evaluation-and-automatic-parallelization"><span class="No-Break"><span class="koboSpan" id="kobo.520.1">https://cljdoc.org/d/org.apache.mxnet.contrib.clojure/clojure-mxnet-linux-cpu/1.4.1/doc/ndarray-imperative-tensor-operations-on-cpu-gpu#lazy-evaluation-and-automatic-parallelization</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.521.1">Gluon </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.522.1">DataLoaders: </span></strong></span><a href="https://mxnet.apache.org/versions/master/api/python/docs/tutorials/getting-started/crash-course/5-datasets.html"><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.523.1">https:</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">//mxnet.apache.org/versions/master/api/python/docs/tutorials/getting-started/crash-course/5-datasets.html</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.525.1">AMP:</span></strong></span><span class="No-Break"> </span><a href="https://medium.com/apache-mxnet/simplify-mixed-precision-training-with-mxnet-amp-dc2564b1c7b0"><span class="No-Break"><span class="koboSpan" id="kobo.526.1">https://medium.com/apache-mxnet/simplify-mixed-precision-training-with-mxnet-amp-dc2564b1c7b0</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.527.1">Training with multiple </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.528.1">GPUs: </span></strong></span><a href="https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/getting-started/crash-course/6-use_gpus.html"><span class="No-Break"><span class="koboSpan" id="kobo.529.1">https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/getting-started/crash-course/6-use_gpus.html</span></span></a></li>
</ul>
<h1 id="_idParaDest-179"><a id="_idTextAnchor180"/><span class="koboSpan" id="kobo.530.1">Optimizing training for image segmentation</span></h1>
<p><span class="koboSpan" id="kobo.531.1">In the previous recipe, we saw how we could leverage MXNet and Gluon to optimize the training of our models with a variety of different techniques. </span><span class="koboSpan" id="kobo.531.2">We understood how we can jointly use lazy evaluation and automatic parallelization for parallel processing. </span><span class="koboSpan" id="kobo.531.3">We saw how to improve the performance of our DataLoaders by combining preprocessing in the CPU and GPU, and how using half-precision (</span><strong class="source-inline"><span class="koboSpan" id="kobo.532.1">Float16</span></strong><span class="koboSpan" id="kobo.533.1">) in combination with AMP can halve our training times. </span><span class="koboSpan" id="kobo.533.2">Lastly, we explored how to take advantage of multiple GPUs to further reduce </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">training times.</span></span></p>
<p><span class="koboSpan" id="kobo.535.1">Now, we can revisit a problem we have been working with throughout the book: </span><strong class="bold"><span class="koboSpan" id="kobo.536.1">image segmentation</span></strong><span class="koboSpan" id="kobo.537.1">. </span><span class="koboSpan" id="kobo.537.2">We have worked on this task in recipes from previous chapters. </span><span class="koboSpan" id="kobo.537.3">In the </span><em class="italic"><span class="koboSpan" id="kobo.538.1">Segmenting objects semantically with MXNet Model Zoo – PSPNet and DeepLabv3</span></em><span class="koboSpan" id="kobo.539.1"> recipe in </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.540.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.541.1">, we learned how to use pre-trained models from GluonCV Model Zoo, and introduced the task and the datasets that we will be using in this recipe: </span><strong class="bold"><span class="koboSpan" id="kobo.542.1">MS COCO</span></strong><span class="koboSpan" id="kobo.543.1"> and the </span><strong class="bold"><span class="koboSpan" id="kobo.544.1">Penn-Fudan Pedestrian</span></strong><span class="koboSpan" id="kobo.545.1"> dataset. </span><span class="koboSpan" id="kobo.545.2">Furthermore, in the </span><em class="italic"><span class="koboSpan" id="kobo.546.1">Improving performance for segmenting images</span></em><span class="koboSpan" id="kobo.547.1"> recipe in </span><a href="B16591_07.xhtml#_idTextAnchor148"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.548.1">Chapter 7</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.549.1">, Optimizing Models with Transfer Learning and Fine-Tuning </span></em><span class="koboSpan" id="kobo.550.1">we compared the different approaches that we could take when dealing with a target dataset, training our models from scratch, or leveraging the existing knowledge of pre-trained models and adjusting it for our task using the different modalities of transfer learning </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">and fine-tuning.</span></span></p>
<p><span class="koboSpan" id="kobo.552.1">In this recipe, we will apply all these optimization techniques for the specific task of training an image </span><span class="No-Break"><span class="koboSpan" id="kobo.553.1">segmentation model.</span></span></p>
<h2 id="_idParaDest-180"><a id="_idTextAnchor181"/><span class="koboSpan" id="kobo.554.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.555.1">Similar to previous chapters, in this recipe, we will be using some matrix operations and linear algebra, but it will not be hard at all, as you will find lots of examples and code snippets to facilitate </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">your learning.</span></span></p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor182"/><span class="koboSpan" id="kobo.557.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.558.1">In this recipe, we will be looking at the </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.560.1">Revisiting our current preprocessing and </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">training pipeline</span></span></li>
<li><span class="koboSpan" id="kobo.562.1">Applying training </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">optimization techniques</span></span></li>
<li><span class="koboSpan" id="kobo.564.1">Analyzing </span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">the results</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.566.1">Let’s dive into each of </span><span class="No-Break"><span class="koboSpan" id="kobo.567.1">these steps.</span></span></p>
<h3><span class="koboSpan" id="kobo.568.1">Revisiting our current preprocessing and training pipeline</span></h3>
<p><span class="koboSpan" id="kobo.569.1">In the </span><em class="italic"><span class="koboSpan" id="kobo.570.1">Improving performance for segmenting images</span></em><span class="koboSpan" id="kobo.571.1"> recipe in </span><a href="B16591_07.xhtml#_idTextAnchor148"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.572.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.573.1">, we processed the data with the </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">following approach:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.575.1">Loaded the data from storage into the </span><em class="italic"><span class="koboSpan" id="kobo.576.1">CPU </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.577.1">memory space</span></em></span></li>
<li><span class="koboSpan" id="kobo.578.1">Preprocessed the data using </span><span class="No-Break"><span class="koboSpan" id="kobo.579.1">the </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.580.1">CPU</span></em></span></li>
<li><span class="koboSpan" id="kobo.581.1">Used the </span><strong class="bold"><span class="koboSpan" id="kobo.582.1">default parameters</span></strong><span class="koboSpan" id="kobo.583.1"> to process the data </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">during training</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.585.1">This was a </span><a id="_idIndexMarker1001"/><span class="koboSpan" id="kobo.586.1">valid approach to compare the different training alternatives available to us (training from scratch, pre-trained models, transfer learning, and fine-tuning) without adding complexity to the experiments. </span><span class="koboSpan" id="kobo.586.2">For example, this approach worked quite well to introduce and evaluate the technique of </span><span class="No-Break"><span class="koboSpan" id="kobo.587.1">fine-tuning directly.</span></span></p>
<p><span class="koboSpan" id="kobo.588.1">Following the aforementioned approach on the dataset selected for this recipe (</span><em class="italic"><span class="koboSpan" id="kobo.589.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.590.1">), the CPU-based preprocessing took the following amount </span><span class="No-Break"><span class="koboSpan" id="kobo.591.1">of time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.592.1">
Pre-processing time (s): 0.12470602989196777</span></pre> <p><span class="koboSpan" id="kobo.593.1">Furthermore, when combined with the necessary step of reloading the data in batches and copying it to the GPU, we obtain the </span><span class="No-Break"><span class="koboSpan" id="kobo.594.1">following performance:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.595.1">
Data-Loading in GPU time (s): 0.4085373878479004</span></pre> <p><span class="koboSpan" id="kobo.596.1">After the preprocessing, the next step is the training process. </span><span class="koboSpan" id="kobo.596.2">As described, we will evaluate the effect of our training optimizations directly by using the technique of fine-tuning. </span><span class="koboSpan" id="kobo.596.3">In combination with this approach, we will use the </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">following hyperparameters:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.598.1">
# Epochs &amp; Batch Size
epochs = 10
batch_size = 4
# Define Optimizer and Hyper Parameters
trainer = mx.gluon.Trainer(deeplab_ft_direct_naive.collect_params(), "sgd", {"learning_rate": 0.1})</span></pre> <p><span class="koboSpan" id="kobo.599.1">In these conditions, the training process duration and performance achieved were </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.601.1">
Training time for 10 epochs (s): 638.9948952198029 / Best validation loss: 0.09416388</span></pre> <p><span class="koboSpan" id="kobo.602.1">As we can see, we got an excellent validation performance (~0.09) in a little over </span><span class="No-Break"><span class="koboSpan" id="kobo.603.1">10 minutes.</span></span></p>
<p><span class="koboSpan" id="kobo.604.1">The evolution </span><a id="_idIndexMarker1002"/><span class="koboSpan" id="kobo.605.1">of the training loss and the validation loss across each epoch looks </span><span class="No-Break"><span class="koboSpan" id="kobo.606.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer197">
<span class="koboSpan" id="kobo.607.1"><img alt="Figure 8.2 – Revisiting training: training loss versus validation loss" src="image/B16591_08_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.608.1">Figure 8.2 – Revisiting training: training loss versus validation loss</span></p>
<p><span class="koboSpan" id="kobo.609.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.610.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.611.1">.2</span></em><span class="koboSpan" id="kobo.612.1">, we can see the evolution of the training and validation loss. </span><span class="koboSpan" id="kobo.612.2">As explored throughout the chapters, we select the model that provide the minimal validation loss (in this case, this was achieved in the last epoch, </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">epoch 10).</span></span></p>
<p><span class="koboSpan" id="kobo.614.1">After the training is completed, we can verify the overall performance in the test split of our dataset. </span><span class="koboSpan" id="kobo.614.2">From a quantitative point of view, these are the results </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">we obtained:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.616.1">
PixAcc:  0.9627800347222222
mIoU  :  0.9070747450272697</span></pre> <p><span class="koboSpan" id="kobo.617.1">As expected, we got excellent results by training for just a limited number of epochs (10 in </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">this case).</span></span></p>
<p><span class="koboSpan" id="kobo.619.1">From a </span><a id="_idIndexMarker1003"/><span class="koboSpan" id="kobo.620.1">qualitative point of view, this is what </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">we have:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer198">
<span class="koboSpan" id="kobo.622.1"><img alt="Figure 8.3 – Revisiting training: GroundTruth example and Prediction post-training" src="image/B16591_08_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.623.1">Figure 8.3 – Revisiting training: GroundTruth example and Prediction post-training</span></p>
<p><span class="koboSpan" id="kobo.624.1">As expected, the results show how the model has learned to focus on the people in the foreground, avoiding the ones in </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">the background.</span></span></p>
<h3><span class="koboSpan" id="kobo.626.1">Applying training optimization techniques</span></h3>
<p><span class="koboSpan" id="kobo.627.1">In the </span><em class="italic"><span class="koboSpan" id="kobo.628.1">Introducing training optimization features</span></em><span class="koboSpan" id="kobo.629.1"> recipe at the beginning of this chapter, we</span><a id="_idIndexMarker1004"/><span class="koboSpan" id="kobo.630.1"> showed how different optimization techniques could improve the performance of the different steps we take when training a machine learning model, including preprocessing the data and training and evaluating </span><span class="No-Break"><span class="koboSpan" id="kobo.631.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.632.1">In this section, we will show how, with MXNet and Gluon and just a few lines of code, we can easily apply all the techniques we’ve been </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">introduced to.</span></span></p>
<p><span class="koboSpan" id="kobo.634.1">As shown in the first recipe of this chapter, MXNet applies by default the best policy (</span><strong class="source-inline"><span class="koboSpan" id="kobo.635.1">ThreadedEnginePerDevice</span></strong><span class="koboSpan" id="kobo.636.1">) to optimize lazy evaluation and automatic parallelization, taking into account the number of CPU threads available, so there is no need for us to apply any changes here (please note that this technique is also applied automatically when working with </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">multiple GPUs).</span></span></p>
<p><span class="koboSpan" id="kobo.638.1">We also showed </span><a id="_idIndexMarker1005"/><span class="koboSpan" id="kobo.639.1">how we could optimize our data preprocessing pipeline by combining the usage of CPU threads and GPUs, taking into account the number of devices available for each, and optimizing accordingly. </span><span class="koboSpan" id="kobo.639.2">For this experiment, specific HW was chosen with the </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">following characteristics:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.641.1">
Number of CPUs: 16
Number of GPUs: 4</span></pre> <p><span class="koboSpan" id="kobo.642.1">In order to use this optimization technique, we had to apply some changes to our code. </span><span class="koboSpan" id="kobo.642.2">Specifically, we define the GPUs available </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">for use:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.644.1">
# Context variable is now a list,
 # with each element corresponding to a GPU device
ctx_list = [mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3)]
 num_gpus = len(ctx_list)</span></pre> <p><span class="koboSpan" id="kobo.645.1">Furthermore, in our preprocessing pipeline, we now need a specific step that takes the data from CPU memory space and copies it to GPU </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">memory space:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.647.1">
p_train_gpu = mx.gluon.data.SimpleDataset(
    [(data.as_in_context(ctx_list[idx % num_gpus]), label.as_in_context(ctx_list[idx % num_gpus]))
     for idx, (data, label) in enumerate(pedestrian_train_dataset)])
 p_val_gpu   = mx.gluon.data.SimpleDataset(
    [(data.as_in_context(ctx_list[idx % num_gpus]), label.as_in_context(ctx_list[idx % num_gpus]))
     for idx, (data, label) in enumerate(pedestrian_val_dataset)])
 p_test_gpu  = mx.gluon.data.SimpleDataset(
    [(data.as_in_context(ctx_list[idx % num_gpus]), label.as_in_context(ctx_list[idx % num_gpus]))
     for idx, (data, label) in enumerate(pedestrian_test_dataset)])
p_train_opt = p_train_gpu.transform(train_val_transform, lazy=False)
 p_val_opt   = p_val_gpu.transform(train_val_transform, lazy=False)
 p_test_opt  = p_test_gpu.transform(test_transform, lazy=False)</span></pre> <p><span class="koboSpan" id="kobo.648.1">As discussed in </span><a id="_idIndexMarker1006"/><span class="koboSpan" id="kobo.649.1">the first recipe of this chapter, in a typical production-oriented environment, we do not want to keep the data in the GPU, occupying precious GPU memory. </span><span class="koboSpan" id="kobo.649.2">It is usual to optimize the batch size for the GPU memory available, and to load the data from the CPU memory space into the GPU memory space in batches using </span><em class="italic"><span class="koboSpan" id="kobo.650.1">MXNet Gluon DataLoaders</span></em><span class="koboSpan" id="kobo.651.1">. </span><span class="koboSpan" id="kobo.651.2">Therefore, for our GPU-based preprocessing pipeline to be </span><a id="_idIndexMarker1007"/><span class="koboSpan" id="kobo.652.1">complete, we need a final step to copy the data back into the CPU </span><span class="No-Break"><span class="koboSpan" id="kobo.653.1">memory space:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.654.1">
to_cpu_fn = lambda x: x.as_in_context(mx.cpu())</span></pre> <p><span class="koboSpan" id="kobo.655.1">With these code changes, our optimal preprocessing pipeline is ready, and we can continue with the next optimization technique: applying </span><strong class="source-inline"><span class="koboSpan" id="kobo.656.1">Float16</span></strong><span class="koboSpan" id="kobo.657.1"> optimizations, </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">including AMP.</span></span></p>
<p><span class="koboSpan" id="kobo.659.1">As shown in the first recipe of this chapter, in order to enable this technique, we just need a few changes in our code. </span><span class="koboSpan" id="kobo.659.2">First of all, we initialize </span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">the library:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.661.1">
# AMP
amp.init()</span></pre> <p><span class="koboSpan" id="kobo.662.1">Secondly, we attach the trainer/optimizer to </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">the library:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.664.1">
amp.init_trainer(trainer)</span></pre> <p><span class="koboSpan" id="kobo.665.1">And lastly, due to the limitations of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.666.1">Float16</span></strong><span class="koboSpan" id="kobo.667.1"> data type, there is a risk of gradients over/under-flowing; therefore, we need to adjust (scale) the loss accordingly, which can be done automatically with these lines </span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.669.1">
with amp.scale_loss(losses, trainer) as scaled_losses: mx.autograd.backward(scaled_losses)</span></pre> <p><span class="koboSpan" id="kobo.670.1">With these three simple changes, we have updated our training loop to work efficiently with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.671.1">Float16</span></strong><span class="koboSpan" id="kobo.672.1"> data type (</span><span class="No-Break"><span class="koboSpan" id="kobo.673.1">when appropriate).</span></span></p>
<p><span class="koboSpan" id="kobo.674.1">Please note in</span><a id="_idIndexMarker1008"/><span class="koboSpan" id="kobo.675.1"> the preceding code snippet how we are now working with a list of losses, instead of a single instance. </span><span class="koboSpan" id="kobo.675.2">This is due to our next and last training optimization technique: working with </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.676.1">multiple GPUs</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.678.1">As we will see, working with multiple GPUs optimally implies working with them in parallel, and therefore, computing losses and executing the training backward pass in parallel, yielding the losses list described in the </span><span class="No-Break"><span class="koboSpan" id="kobo.679.1">previous paragraph.</span></span></p>
<p><span class="koboSpan" id="kobo.680.1">In order to work with multiple GPUs in parallel, we need to define the new context as a list (seen before for preprocessing, and shown here again </span><span class="No-Break"><span class="koboSpan" id="kobo.681.1">for convenience):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.682.1">
# Context variable is now a list,
 # with each element corresponding to a GPU device
ctx_list = [mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3)
 num_gpus = len(ctx_list)</span></pre> <p><span class="koboSpan" id="kobo.683.1">As we now have multiple GPUs, we can increase our batch size to optimally use the available GPU </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">memory space:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.685.1">
batch_size = len(ctx_list) * batch_size_per_gpu</span></pre> <p><span class="koboSpan" id="kobo.686.1">Furthermore, when reading from Gluon DataLoaders, we need to split the batches of data across the GPUs. </span><span class="koboSpan" id="kobo.686.2">Thankfully, Gluon also provides a function that simplifies that action. </span><span class="koboSpan" id="kobo.686.3">We just need the following lines of code to be added (for each training and </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">validation batch):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.688.1">
data_list  = mx.gluon.utils.split_and_load(data , ctx_list=ctx_list)
label_list = mx.gluon.utils.split_and_load(label, ctx_list=ctx_list)</span></pre> <p><span class="koboSpan" id="kobo.689.1">As mentioned, this split across GPUs allows us to compute in parallel the model outputs and the losses associated with those outputs (a measure of the difference between the actual outputs and the expected outputs). </span><span class="koboSpan" id="kobo.689.2">This </span><a id="_idIndexMarker1009"/><span class="koboSpan" id="kobo.690.1">can be achieved with the following lines </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.692.1">
outputs = [model(data_slice) for data_slice in data_list]
losses = [loss_fn(output[0], label_slice) for output, label_slice in zip(outputs, label_list)]</span></pre> <p><span class="koboSpan" id="kobo.693.1">And lastly, we compute the backward pass used to update the weights of our model (combined with the scaled loss </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">of AMP):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.695.1">
with amp.scale_loss(losses, trainer) as scaled_losses:
mx.autograd.backward(scaled_losses)</span></pre> <p><span class="koboSpan" id="kobo.696.1">With these minimal code changes, we now have an optimal preprocessing and training pipeline and can run our experiments to analyze the </span><span class="No-Break"><span class="koboSpan" id="kobo.697.1">performance changes.</span></span></p>
<h3><span class="koboSpan" id="kobo.698.1">Analyzing the results</span></h3>
<p><span class="koboSpan" id="kobo.699.1">In the </span><a id="_idIndexMarker1010"/><span class="koboSpan" id="kobo.700.1">previous sections, we revisited the previous performance of our preprocessing and training pipelines, and we reviewed how we had to apply the necessary changes for our training optimization techniques, specifically for our image </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">segmentation task.</span></span></p>
<p><span class="koboSpan" id="kobo.702.1">Our preprocessing pipeline steps are now </span><span class="No-Break"><span class="koboSpan" id="kobo.703.1">the following:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.704.1">Load the data from storage into CPU </span><span class="No-Break"><span class="koboSpan" id="kobo.705.1">memory space.</span></span></li>
<li><span class="koboSpan" id="kobo.706.1">Preprocess the data using </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">the GPU.</span></span></li>
<li><span class="koboSpan" id="kobo.708.1">Copy back the data to CPU </span><span class="No-Break"><span class="koboSpan" id="kobo.709.1">memory space.</span></span></li>
<li><span class="koboSpan" id="kobo.710.1">Use the optimized parameters to process the data </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">during training.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.712.1">For our experiments, we are going to use the technique of </span><span class="No-Break"><span class="koboSpan" id="kobo.713.1">fine-tuning directly.</span></span></p>
<p><span class="koboSpan" id="kobo.714.1">Applying the approach described earlier on the dataset selected for this recipe (</span><em class="italic"><span class="koboSpan" id="kobo.715.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.716.1">), the preprocessing took the following amount </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">of time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.718.1">
Pre-processing time (s): 0.10713815689086914</span></pre> <p><span class="koboSpan" id="kobo.719.1">An end-to-end preprocessing pipeline must take into account the process of batching using the </span><em class="italic"><span class="koboSpan" id="kobo.720.1">Gluon DataLoader</span></em><span class="koboSpan" id="kobo.721.1"> to load the data – in our case, into multiple GPUs </span><span class="No-Break"><span class="koboSpan" id="kobo.722.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.723.1">
Data-Loading in GPU time (s): 0.18216562271118164</span></pre> <p><span class="koboSpan" id="kobo.724.1">Compared </span><a id="_idIndexMarker1011"/><span class="koboSpan" id="kobo.725.1">to the initial section of this recipe (where the preprocessing took </span><strong class="source-inline"><span class="koboSpan" id="kobo.726.1">0.4</span></strong><span class="koboSpan" id="kobo.727.1"> seconds), we can see how, even with the added overhead of copying back the data to the CPU memory space, we have improved the preprocessing performance by &gt;</span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">2 times.</span></span></p>
<p><span class="koboSpan" id="kobo.729.1">After the preprocessing, the next step is the training process. </span><span class="koboSpan" id="kobo.729.2">As described, we will evaluate the effect of our training optimizations using the technique of fine-tuning directly. </span><span class="koboSpan" id="kobo.729.3">In combination with this approach, we use the </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">following hyperparameters:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.731.1">
# Epochs &amp; Batch Size
epochs = 10
batch_size_per_gpu = 4
batch_size = len(ctx_list) * batch_size_per_gpu
# Define Optimizer and Hyper Parameters
trainer = mx.gluon.Trainer(deeplab_ft_direct_opt.collect_params(), "sgd", {"learning_rate": 0.5})</span></pre> <p><span class="koboSpan" id="kobo.732.1">Please note how, by adding multiple GPUs to the training process, we can increase the batch size (multiplied by the number of GPUs), and we can also increase the learning rate (from 0.1 to 0.5). </span><span class="koboSpan" id="kobo.732.2">In these conditions, the training process duration and performance achieved were </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.734.1">
Training time for 10 epochs: 59.86336851119995 / Best validation loss: 0.08904324161509672</span></pre> <p><span class="koboSpan" id="kobo.735.1">As can be seen, we got excellent validation performance (~0.09) in less than 1 minute. </span><span class="koboSpan" id="kobo.735.2">When comparing with the results obtained in the recipe, we can see how there was a minimal decrease in the loss (a positive change that we will confirm with our performance analysis shortly), but the largest improvement by far was a &gt;10x decrease in the training time. </span><span class="koboSpan" id="kobo.735.3">This improvement is due to all the training optimization techniques that we have applied. </span><span class="koboSpan" id="kobo.735.4">In a nutshell, each of the optimizations provided the </span><span class="No-Break"><span class="koboSpan" id="kobo.736.1">following improvements:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.737.1">Using 4 GPUs</span></strong><span class="koboSpan" id="kobo.738.1">: Provided a 4x decrease </span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">in time</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.740.1">Using Float16 and AMP</span></strong><span class="koboSpan" id="kobo.741.1">: Provided a 2x decrease (</span><span class="No-Break"><span class="koboSpan" id="kobo.742.1">8x combined)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.743.1">Preprocessing the datasets</span></strong><span class="koboSpan" id="kobo.744.1">: Provided a 1.25x decrease (&gt;</span><span class="No-Break"><span class="koboSpan" id="kobo.745.1">10x combined)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.746.1">The evolution</span><a id="_idIndexMarker1012"/><span class="koboSpan" id="kobo.747.1"> of the training loss and the validation loss across each epoch was </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer199">
<span class="koboSpan" id="kobo.749.1"><img alt="Figure 8.4 – Optimized training: training loss versus validation loss" src="image/B16591_08_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.750.1">Figure 8.4 – Optimized training: training loss versus validation loss</span></p>
<p><span class="koboSpan" id="kobo.751.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.752.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.753.1">.4</span></em><span class="koboSpan" id="kobo.754.1">, we can see the evolution of the training and validation losses. </span><span class="koboSpan" id="kobo.754.2">As explored throughout the chapters so far, we select the model that provided the minimal validation loss (in this case, achieved in the last epoch, </span><span class="No-Break"><span class="koboSpan" id="kobo.755.1">epoch 10).</span></span></p>
<p><span class="koboSpan" id="kobo.756.1">After training is completed, we can verify the overall performance in the test split of our dataset. </span><span class="koboSpan" id="kobo.756.2">From a quantitative point of view, these are the results </span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">we obtained:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.758.1">
PixAcc:  0.9679262152777778
mIoU  :  0.9176786683400912</span></pre> <p><span class="koboSpan" id="kobo.759.1">As expected, we </span><a id="_idIndexMarker1013"/><span class="koboSpan" id="kobo.760.1">got excellent results just by training for a limited number of epochs (10 in this case). </span><span class="koboSpan" id="kobo.760.2">We can also confirm that the minimal improvement in the validation loss provided a minimal improvement in our test metrics (compared with 0.96/0.91 in our </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">initial experiment).</span></span></p>
<p><span class="koboSpan" id="kobo.762.1">From a qualitative point of view, we have </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer200">
<span class="koboSpan" id="kobo.764.1"><img alt="Figure 8.5 – Optimized training: GroundTruth example and Prediction post-training" src="image/B16591_08_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.765.1">Figure 8.5 – Optimized training: GroundTruth example and Prediction post-training</span></p>
<p><span class="koboSpan" id="kobo.766.1">As expected, the results show how the model has learned to focus on the different people in the foreground, avoiding the ones in </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">the background.</span></span></p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor183"/><span class="koboSpan" id="kobo.768.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.769.1">In this recipe, we</span><a id="_idIndexMarker1014"/><span class="koboSpan" id="kobo.770.1"> applied the different training optimization techniques seen in the first recipe of this chapter, leveraging our HW (CPUs and GPUs) to address each of the steps in the </span><span class="No-Break"><span class="koboSpan" id="kobo.771.1">training loop:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.772.1">We revisited how lazy evaluation and automatic parallelization mechanisms worked together to optimize all </span><span class="No-Break"><span class="koboSpan" id="kobo.773.1">MXNet-based flows.</span></span></li>
<li><span class="koboSpan" id="kobo.774.1">We leveraged all our CPU threads to load data and optimized that process further via preprocessing in the GPU. </span><span class="koboSpan" id="kobo.774.2">We also compared the trade-offs between speed and </span><span class="No-Break"><span class="koboSpan" id="kobo.775.1">memory optimizations.</span></span></li>
<li><span class="koboSpan" id="kobo.776.1">We analyzed different data types and combined the accuracy and precision of </span><strong class="source-inline"><span class="koboSpan" id="kobo.777.1">Float32</span></strong><span class="koboSpan" id="kobo.778.1"> with the speed-ups of </span><strong class="source-inline"><span class="koboSpan" id="kobo.779.1">Float16</span></strong><span class="koboSpan" id="kobo.780.1"> where possible, </span><span class="No-Break"><span class="koboSpan" id="kobo.781.1">using AMP.</span></span></li>
<li><span class="koboSpan" id="kobo.782.1">We increased the performance of our training loops by using multiple GPUs (assuming our HW has these </span><span class="No-Break"><span class="koboSpan" id="kobo.783.1">devices available).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.784.1">We compared</span><a id="_idIndexMarker1015"/><span class="koboSpan" id="kobo.785.1"> each of these scenarios applied specifically to the task of image segmentation, running two experiments. </span><span class="koboSpan" id="kobo.785.2">In the first experiment, we did not apply any of the training optimization techniques described in the previous recipe, following the approach seen in previous chapters of the book. </span><span class="koboSpan" id="kobo.785.3">In the second experiment, we applied all the techniques in parallel, trying to optimize as much as </span><span class="No-Break"><span class="koboSpan" id="kobo.786.1">we could.</span></span></p>
<p><span class="koboSpan" id="kobo.787.1">This proved quite useful, delivering similar algorithmic performance, with 10x improvement in training time (from 10 minutes to 1 minute). </span><span class="koboSpan" id="kobo.787.2">This was mostly due to using multiple GPUs (4x decrease), leveraging </span><strong class="source-inline"><span class="koboSpan" id="kobo.788.1">Float16</span></strong><span class="koboSpan" id="kobo.789.1"> AMP (2x decrease), and the optimized preprocessing (</span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">1.25x decrease).</span></span></p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor184"/><span class="koboSpan" id="kobo.791.1">There’s more…</span></h2>
<p><span class="koboSpan" id="kobo.792.1">We have described, implemented, executed, and evaluated several training optimization techniques. </span><span class="koboSpan" id="kobo.792.2">However, there are even more advanced techniques that can be leveraged to achieve the optimal </span><span class="No-Break"><span class="koboSpan" id="kobo.793.1">training loop.</span></span></p>
<p><span class="koboSpan" id="kobo.794.1">One such technique is </span><strong class="bold"><span class="koboSpan" id="kobo.795.1">learning rate schedules</span></strong><span class="koboSpan" id="kobo.796.1">. </span><span class="koboSpan" id="kobo.796.2">Throughout </span><a id="_idIndexMarker1016"/><span class="koboSpan" id="kobo.797.1">the book, we have been working with constant learning rates. </span><span class="koboSpan" id="kobo.797.2">However, there are multiple advantages of using a dynamically adjusted learning rate. </span><span class="koboSpan" id="kobo.797.3">Some of them are </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.799.1">Warmup</span></strong><span class="koboSpan" id="kobo.800.1">: When </span><a id="_idIndexMarker1017"/><span class="koboSpan" id="kobo.801.1">working with pre-trained models, it’s not advisable to start with a large learning rate. </span><span class="koboSpan" id="kobo.801.2">The initial epochs must be used for the gradients to start adjusting. </span><span class="koboSpan" id="kobo.801.3">This can be thought of as a way of </span><em class="italic"><span class="koboSpan" id="kobo.802.1">adjusting the model from the source task to the target task</span></em><span class="koboSpan" id="kobo.803.1">, retaining and leveraging the knowledge from the previous task, so smaller learning rates </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">are recommended.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.805.1">Decay</span></strong><span class="koboSpan" id="kobo.806.1">: In optimal training loops, as</span><a id="_idIndexMarker1018"/><span class="koboSpan" id="kobo.807.1"> the model learns the expected representation of inputs to outputs, the objective of the training is to produce finer and finer improvements. </span><span class="koboSpan" id="kobo.807.2">Smaller learning rates achieve better performance at these stages (smaller and more stable weight updates). </span><span class="koboSpan" id="kobo.807.3">Therefore, a decaying learning rate is preferred after a </span><span class="No-Break"><span class="koboSpan" id="kobo.808.1">few epochs.</span></span></li>
</ul>
<p><em class="italic"><span class="koboSpan" id="kobo.809.1">Dive into Deep Learning</span></em><span class="koboSpan" id="kobo.810.1"> provides great insights on how to implement these techniques in </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">MXNet: </span></span><a href="https://d2l.ai/chapter_optimization/lr-scheduler.html"><span class="No-Break"><span class="koboSpan" id="kobo.812.1">https://d2l.ai/chapter_optimization/lr-scheduler.html.</span></span></a></p>
<h1 id="_idParaDest-184"><a id="_idTextAnchor185"/><span class="koboSpan" id="kobo.813.1">Optimizing training for translating text from English to German</span></h1>
<p><span class="koboSpan" id="kobo.814.1">In the first </span><a id="_idIndexMarker1019"/><span class="koboSpan" id="kobo.815.1">recipe of this chapter, we saw how we could leverage MXNet and Gluon to optimize the training of our models, applying different techniques. </span><span class="koboSpan" id="kobo.815.2">We understood how to jointly use lazy evaluation and automatic parallelization for parallel processing and improved the performance of our DataLoaders by combining preprocessing in the CPU and GPU. </span><span class="koboSpan" id="kobo.815.3">We saw how using half-precision (</span><strong class="source-inline"><span class="koboSpan" id="kobo.816.1">Float16</span></strong><span class="koboSpan" id="kobo.817.1">) in combination with AMP can halve our training times, and explored how to take advantage of multiple GPUs for further reduced </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">training times.</span></span></p>
<p><span class="koboSpan" id="kobo.819.1">Now, we can revisit a problem we have been working with throughout the book, that of </span><strong class="bold"><span class="koboSpan" id="kobo.820.1">translating text from English to German</span></strong><span class="koboSpan" id="kobo.821.1">. </span><span class="koboSpan" id="kobo.821.2">We have worked with translation tasks in recipes in previous chapters. </span><span class="koboSpan" id="kobo.821.3">In the </span><em class="italic"><span class="koboSpan" id="kobo.822.1">Translating text from Vietnamese to English</span></em><span class="koboSpan" id="kobo.823.1"> recipe from </span><a href="B16591_06.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.824.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.825.1">, we introduced the task of translating text, while also learning how to use pre-trained models from GluonCV Model Zoo. </span><span class="koboSpan" id="kobo.825.2">Furthermore, in the </span><em class="italic"><span class="koboSpan" id="kobo.826.1">Improving performance for translating English to German</span></em><span class="koboSpan" id="kobo.827.1"> recipe from </span><a href="B16591_07.xhtml#_idTextAnchor148"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.828.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.829.1">, we introduced the datasets that we will be using in this recipe: </span><em class="italic"><span class="koboSpan" id="kobo.830.1">WMT2014</span></em><span class="koboSpan" id="kobo.831.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.832.1">WMT2016</span></em><span class="koboSpan" id="kobo.833.1">, and compared the different approaches that we could take when dealing with a target dataset, training our models from scratch or leveraging past knowledge from pre-trained models and adjusting it for our task, using the different modalities of transfer learning </span><span class="No-Break"><span class="koboSpan" id="kobo.834.1">and fine-tuning.</span></span></p>
<p><span class="koboSpan" id="kobo.835.1">Therefore, in this recipe, we will apply all these optimization techniques for the specific task of training an </span><em class="italic"><span class="koboSpan" id="kobo.836.1">English-to-German text </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.837.1">translation model</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.838.1">.</span></span></p>
<h2 id="_idParaDest-185"><a id="_idTextAnchor186"/><span class="koboSpan" id="kobo.839.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.840.1">As in previous </span><a id="_idIndexMarker1020"/><span class="koboSpan" id="kobo.841.1">chapters, in this recipe, we will be using some matrix operations and linear algebra, but it will not be difficult to understand </span><span class="No-Break"><span class="koboSpan" id="kobo.842.1">at all.</span></span></p>
<h2 id="_idParaDest-186"><a id="_idTextAnchor187"/><span class="koboSpan" id="kobo.843.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.844.1">In this recipe, we will work through the </span><span class="No-Break"><span class="koboSpan" id="kobo.845.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.846.1">Revisiting our current preprocessing and </span><span class="No-Break"><span class="koboSpan" id="kobo.847.1">training pipeline</span></span></li>
<li><span class="koboSpan" id="kobo.848.1">Applying training </span><span class="No-Break"><span class="koboSpan" id="kobo.849.1">optimization techniques</span></span></li>
<li><span class="koboSpan" id="kobo.850.1">Analyzing </span><span class="No-Break"><span class="koboSpan" id="kobo.851.1">the results</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.852.1">Let’s dive into each of </span><span class="No-Break"><span class="koboSpan" id="kobo.853.1">these steps.</span></span></p>
<h3><span class="koboSpan" id="kobo.854.1">Revisiting our current preprocessing and training pipeline</span></h3>
<p><span class="koboSpan" id="kobo.855.1">In the</span><em class="italic"><span class="koboSpan" id="kobo.856.1"> Improving performance for translating English to German</span></em><span class="koboSpan" id="kobo.857.1"> recipe from </span><a href="B16591_07.xhtml#_idTextAnchor148"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.858.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.859.1">, we processed the data with the </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">following approach:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.861.1">Loaded the data from storage into CPU </span><span class="No-Break"><span class="koboSpan" id="kobo.862.1">memory space</span></span></li>
<li><span class="koboSpan" id="kobo.863.1">Preprocessed the data </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">using CPU</span></span></li>
<li><span class="koboSpan" id="kobo.865.1">Used the default parameters to process the data </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">during training</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.867.1">This was a </span><a id="_idIndexMarker1021"/><span class="koboSpan" id="kobo.868.1">valid approach to compare the different training alternatives available for us (training from scratch, pre-trained models, transfer learning, and fine-tuning) without adding complexity to the experiments. </span><span class="koboSpan" id="kobo.868.2">For example, this approach worked quite well to introduce and evaluate the technique of fine-tuning, which is the technique that we have selected to work with in </span><span class="No-Break"><span class="koboSpan" id="kobo.869.1">this recipe.</span></span></p>
<p><span class="koboSpan" id="kobo.870.1">Applying the approach described earlier on the dataset selected for this recipe (</span><em class="italic"><span class="koboSpan" id="kobo.871.1">WMT2016</span></em><span class="koboSpan" id="kobo.872.1">), the CPU-based preprocessing took the following amount </span><span class="No-Break"><span class="koboSpan" id="kobo.873.1">of time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.874.1">
Pre-processing time (s): 2.697735548019409</span></pre> <p><span class="koboSpan" id="kobo.875.1">Furthermore, when combined with the necessary step of reloading the data in batches and copying it to the GPU, we obtain the </span><span class="No-Break"><span class="koboSpan" id="kobo.876.1">following performance:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.877.1">
Data-Loading in GPU time (s): 27.328779935836792</span></pre> <p><span class="koboSpan" id="kobo.878.1">After the </span><a id="_idIndexMarker1022"/><span class="koboSpan" id="kobo.879.1">preprocessing, the next step is the training process. </span><span class="koboSpan" id="kobo.879.2">As described, we will evaluate the effect of our training optimizations using the technique of fine-tuning directly. </span><span class="koboSpan" id="kobo.879.3">In combination with this approach, we use the </span><span class="No-Break"><span class="koboSpan" id="kobo.880.1">following hyperparameters:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.881.1">
# Epochs &amp; Batch Size
hparams.epochs = 5
hparams.lr = 0.00003
# hparam.batch_size = 256</span></pre> <p><span class="koboSpan" id="kobo.882.1">In these conditions, the training process duration and performance achieved were </span><span class="No-Break"><span class="koboSpan" id="kobo.883.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.884.1">
Training time for 5 epochs: 11406.558312892914 / Best validation loss: 1.4029905894300159</span></pre> <p><span class="koboSpan" id="kobo.885.1">As we can see, we got an excellent validation performance (~1.4) for a training time of ~</span><span class="No-Break"><span class="koboSpan" id="kobo.886.1">3 hours.</span></span></p>
<p><span class="koboSpan" id="kobo.887.1">The evolution of the training loss and the validation loss across each epoch looked </span><span class="No-Break"><span class="koboSpan" id="kobo.888.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer201">
<span class="koboSpan" id="kobo.889.1"><img alt="Figure 8.6 – Revisiting training: training loss versus validation loss" src="image/B16591_08_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.890.1">Figure 8.6 – Revisiting training: training loss versus validation loss</span></p>
<p><span class="koboSpan" id="kobo.891.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.892.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.893.1">.6</span></em><span class="koboSpan" id="kobo.894.1">, we </span><a id="_idIndexMarker1023"/><span class="koboSpan" id="kobo.895.1">can see the evolution of the training and validation loss. </span><span class="koboSpan" id="kobo.895.2">As explored throughout the chapters, we select the model that provide the minimal validation loss (in this case, it was achieved in the first epoch, </span><span class="No-Break"><span class="koboSpan" id="kobo.896.1">epoch 1).</span></span></p>
<p><span class="koboSpan" id="kobo.897.1">After the training is completed, we can verify the overall performance in the test split of our dataset. </span><span class="koboSpan" id="kobo.897.2">From a quantitative point of view, these are the results </span><span class="No-Break"><span class="koboSpan" id="kobo.898.1">we obtained:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.899.1">
WMT16 test loss: 1.28; test bleu score: 27.05</span></pre> <p><span class="koboSpan" id="kobo.900.1">As expected, we got excellent results just by training for a limited number of epochs (10 in </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">this case).</span></span></p>
<p><span class="koboSpan" id="kobo.902.1">From a qualitative point of view, we can also check how well our model is performing by testing it with an example sentence. </span><span class="koboSpan" id="kobo.902.2">In our case, we chose </span><strong class="source-inline"><span class="koboSpan" id="kobo.903.1">I learn new things every day</span></strong><span class="koboSpan" id="kobo.904.1">, and the output obtained is </span><span class="No-Break"><span class="koboSpan" id="kobo.905.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.906.1">
Qualitative Evaluation: Translating from English to German
Expected translation:
 Ich lerne neue Dinge jeden Tag.
 </span><span class="koboSpan" id="kobo.906.2">In English:
 I learn new things every day.
 </span><span class="koboSpan" id="kobo.906.3">The German translation is:
 Immer wieder erfährt ich Neues.</span></pre> <p><span class="koboSpan" id="kobo.907.1">The </span><a id="_idIndexMarker1024"/><span class="koboSpan" id="kobo.908.1">German sentence obtained in the output (</span><strong class="source-inline"><span class="koboSpan" id="kobo.909.1">Immer wieder erfährt ich Neues</span></strong><span class="koboSpan" id="kobo.910.1">) means </span><strong class="source-inline"><span class="koboSpan" id="kobo.911.1">I'm always learning new things</span></strong><span class="koboSpan" id="kobo.912.1">, and therefore, as can be seen from the results, the text has been almost perfectly translated from English </span><span class="No-Break"><span class="koboSpan" id="kobo.913.1">to German.</span></span></p>
<h3><span class="koboSpan" id="kobo.914.1">Applying training optimization techniques</span></h3>
<p><span class="koboSpan" id="kobo.915.1">In the </span><em class="italic"><span class="koboSpan" id="kobo.916.1">Introducing training optimization features</span></em><span class="koboSpan" id="kobo.917.1"> recipe at the beginning of this chapter, we </span><a id="_idIndexMarker1025"/><span class="koboSpan" id="kobo.918.1">showed how different optimization techniques could improve the performance of the different steps we take when training a machine learning model, including preprocessing the data and training and evaluating </span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.920.1">In this section, we will show how, with MXNet and Gluon and just a few lines of code, we can easily apply all of the techniques we’ve been </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">introduced to.</span></span></p>
<p><span class="koboSpan" id="kobo.922.1">As shown in the first recipe of this chapter, MXNet applies by default the best policy (</span><strong class="source-inline"><span class="koboSpan" id="kobo.923.1">ThreadedEnginePerDevice</span></strong><span class="koboSpan" id="kobo.924.1">) to optimize lazy evaluation and automatic parallelization, taking into account the number of CPU threads available, so there is no need for us to apply any changes here (please note that this technique is also applied automatically when working with </span><span class="No-Break"><span class="koboSpan" id="kobo.925.1">multiple GPUs).</span></span></p>
<p><span class="koboSpan" id="kobo.926.1">We have shown how we could optimize our data preprocessing pipeline by combining the usage of CPU threads and GPUs, taking into account the number of devices available for each and optimizing accordingly. </span><span class="koboSpan" id="kobo.926.2">For this experiment, specific HW was chosen with the </span><span class="No-Break"><span class="koboSpan" id="kobo.927.1">following characteristics:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.928.1">
Number of CPUs: 16
Number of GPUs: 4</span></pre> <p><span class="koboSpan" id="kobo.929.1">In order to apply this optimization technique, we had to apply some changes to our code. </span><span class="koboSpan" id="kobo.929.2">Specifically, we defined the GPUs available </span><span class="No-Break"><span class="koboSpan" id="kobo.930.1">for use:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.931.1">
# Context variable is now a list,
 # with each element corresponding to a GPU device
ctx_list = [mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3)]
 num_gpus = len(ctx_list)</span></pre> <p><span class="koboSpan" id="kobo.932.1">Furthermore, in our </span><a id="_idIndexMarker1026"/><span class="koboSpan" id="kobo.933.1">preprocessing pipeline, we now need a specific step that takes the data from the CPU memory space and copies it to the GPU </span><span class="No-Break"><span class="koboSpan" id="kobo.934.1">memory space:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.935.1">
wmt2016_train_data_processed_gpu = mx.gluon.data.SimpleDataset([(mx.nd.array(data).as_in_context(ctx_list[idx % num_gpus]), mx.nd.array(label).as_in_context(ctx_list[idx % num_gpus])) for idx, (data, label) in enumerate(wmt2016_train_data_processed)])
wmt2016_train_data_processed_gpu = mx.gluon.data.SimpleDataset([(mx.nd.array(data).as_in_context(ctx_list[idx % num_gpus]), mx.nd.array(label).as_in_context(ctx_list[idx % num_gpus])) for idx, (data, label) in enumerate(wmt2016_train_data_processed)])
wmt2016_val_data_processed_gpu = mx.gluon.data.SimpleDataset([(mx.nd.array(data).as_in_context(ctx_list[idx % num_gpus]), mx.nd.array(label).as_in_context(ctx_list[idx % num_gpus])) for idx, (data, label) in enumerate(wmt2016_val_data_processed)])
wmt2016_ test _data_processed_gpu = mx.gluon.data.SimpleDataset([(mx.nd.array(data).as_in_context(ctx_list[idx % num_gpus]), mx.nd.array(label).as_in_context(ctx_list[idx % num_gpus])) for idx, (data, label) in enumerate(wmt2016_ test _data_processed)])</span></pre> <p><span class="koboSpan" id="kobo.936.1">As </span><a id="_idIndexMarker1027"/><span class="koboSpan" id="kobo.937.1">discussed in the first recipe of this chapter, in a typical production-oriented environment, we do not want to keep the data in the GPU as it occupies precious GPU memory. </span><span class="koboSpan" id="kobo.937.2">It is usual to optimize the batch size for the GPU memory available, and to load the data from the CPU memory space into the GPU memory space in batches using MXNet Gluon DataLoaders. </span><span class="koboSpan" id="kobo.937.3">Therefore, for our GPU-based preprocessing pipeline to be complete, we need a final step to copy the data back into the CPU memory space. </span><span class="koboSpan" id="kobo.937.4">As introduced in the </span><em class="italic"><span class="koboSpan" id="kobo.938.1">Improving performance for translating English to German</span></em><span class="koboSpan" id="kobo.939.1"> recipe from </span><a href="B16591_07.xhtml#_idTextAnchor148"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.940.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.941.1">, we are using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.942.1">ShardedDataLoader</span></strong><span class="koboSpan" id="kobo.943.1"> class from MXNet</span><strong class="source-inline"><span class="koboSpan" id="kobo.944.1"> GluonNLP</span></strong><span class="koboSpan" id="kobo.945.1"> library. </span><span class="koboSpan" id="kobo.945.2">This class performs that data transfer back to the CPU memory </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">space automatically.</span></span></p>
<p><span class="koboSpan" id="kobo.947.1">However, as will be seen in our experiments, when working with multiple GPUs, performance is better when working directly with MXNet Gluon DataLoaders, as these are designed to be parallelized </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">optimally afterward.</span></span></p>
<p><span class="koboSpan" id="kobo.949.1">With these code changes, our optimal preprocessing pipeline is ready, and we can continue with the next optimization technique: applying </span><strong class="source-inline"><span class="koboSpan" id="kobo.950.1">Float16</span></strong><span class="koboSpan" id="kobo.951.1"> optimizations, </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">including AMP.</span></span></p>
<p><span class="koboSpan" id="kobo.953.1">As shown in the first recipe of this chapter, in order to enable this technique, we just need a few changes in our code. </span><span class="koboSpan" id="kobo.953.2">First of all, we initialize </span><span class="No-Break"><span class="koboSpan" id="kobo.954.1">the library:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.955.1">
# AMP
amp.init()</span></pre> <p><span class="koboSpan" id="kobo.956.1">Secondly, we attach the trainer/optimizer to </span><span class="No-Break"><span class="koboSpan" id="kobo.957.1">the library:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.958.1">
amp.init_trainer(trainer)</span></pre> <p><span class="koboSpan" id="kobo.959.1">In the previous recipe, when dealing with images, we described how, due to the risk of gradients over/under-flowing, there was a need to adjust (scale) the loss accordingly. </span><span class="koboSpan" id="kobo.959.2">This is not necessary for our use case; therefore, we do not apply </span><strong class="bold"><span class="koboSpan" id="kobo.960.1">loss </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.961.1">scaling</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.962.1"> here.</span></span></p>
<p><span class="koboSpan" id="kobo.963.1">With these two simple changes, we have updated our training loop to work efficiently with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.964.1">Float16</span></strong><span class="koboSpan" id="kobo.965.1"> data type (</span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">when appropriate).</span></span></p>
<p><span class="koboSpan" id="kobo.967.1">Finally, we can apply our next and last training optimization technique: working with </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">multiple GPUs.</span></span></p>
<p><span class="koboSpan" id="kobo.969.1">As we will see, working </span><a id="_idIndexMarker1028"/><span class="koboSpan" id="kobo.970.1">with multiple GPUs optimally implies working with them in parallel, and therefore, computing losses and executing the training backward pass in parallel, yielding the losses list described in the </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">previous paragraph.</span></span></p>
<p><span class="koboSpan" id="kobo.972.1">In order to work with multiple GPUs in parallel, we need to define the new context as a list (seen before for preprocessing, and shown here again </span><span class="No-Break"><span class="koboSpan" id="kobo.973.1">for convenience):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.974.1">
# Context variable is now a list,
 # with each element corresponding to a GPU device
ctx_list = [mx.gpu(0), mx.gpu(1), mx.gpu(2), mx.gpu(3)]
 num_gpus = len(ctx_list)</span></pre> <p><span class="koboSpan" id="kobo.975.1">As we now have multiple GPUs, we can increase our batch size to optimally use the available GPU </span><span class="No-Break"><span class="koboSpan" id="kobo.976.1">memory space:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.977.1">
batch_size = len(ctx_list) * batch_size_per_gpu</span></pre> <p><span class="koboSpan" id="kobo.978.1">Furthermore, when reading from Gluon DataLoaders, we need to split the batches of data across the GPUs. </span><span class="koboSpan" id="kobo.978.2">Thankfully, Gluon also provides a function that simplifies that action. </span><span class="koboSpan" id="kobo.978.3">We just need the following lines of code to be added (for each training and </span><span class="No-Break"><span class="koboSpan" id="kobo.979.1">validation batch):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.980.1">
src_seq_list = mx.gluon.utils.split_and_load(src_seq, ctx_list=ctx_list, even_split=False)
 tgt_seq_list = mx.gluon.utils.split_and_load(tgt_seq, ctx_list=ctx_list, even_split=False)
 src_valid_length_list = mx.gluon.utils.split_and_load(src_valid_length, ctx_list=ctx_list, even_split=False)
 tgt_valid_length_list = mx.gluon.utils.split_and_load(tgt_valid_length, ctx_list=ctx_list, even_split=False)</span></pre> <p><span class="koboSpan" id="kobo.981.1">As mentioned, this split across the GPUs allows us to compute in parallel the model outputs and the losses associated with those outputs (a measure of the difference between the actual outputs and the expected outputs). </span><span class="koboSpan" id="kobo.981.2">This can be achieved with the following lines </span><span class="No-Break"><span class="koboSpan" id="kobo.982.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.983.1">
out_slice, _ = wmt_transformer_model_ft_direct_opt(
src_seq_slice,
tgt_seq_slice[:, :-1],
src_valid_length_slice,
tgt_valid_length_slice - 1)
loss = loss_function(out_slice, tgt_seq_slice[:, 1:], tgt_valid_length_slice - 1)</span></pre> <p><span class="koboSpan" id="kobo.984.1">Typically, in </span><a id="_idIndexMarker1029"/><span class="koboSpan" id="kobo.985.1">order to finalize our updates to work with multiple GPUs in the training loop, we would need to apply further changes to our loss scaling. </span><span class="koboSpan" id="kobo.985.2">However, as discussed, for our use case, this is </span><span class="No-Break"><span class="koboSpan" id="kobo.986.1">not necessary.</span></span></p>
<p><span class="koboSpan" id="kobo.987.1">With these minimal code changes, we now have an optimal preprocessing and training pipeline, and we can run the required experiments to analyze the </span><span class="No-Break"><span class="koboSpan" id="kobo.988.1">performance changes.</span></span></p>
<h3><span class="koboSpan" id="kobo.989.1">Analyzing the results</span></h3>
<p><span class="koboSpan" id="kobo.990.1">In the </span><a id="_idIndexMarker1030"/><span class="koboSpan" id="kobo.991.1">previous sections, we revisited the previous performance of our preprocessing and training pipelines and reviewed how we had to apply the necessary changes for our training optimization techniques, specifically for our task of translating text from English </span><span class="No-Break"><span class="koboSpan" id="kobo.992.1">to German.</span></span></p>
<p><span class="koboSpan" id="kobo.993.1">Our preprocessing pipeline steps are now </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">the following:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.995.1">Load the data from storage into the CPU </span><span class="No-Break"><span class="koboSpan" id="kobo.996.1">memory space.</span></span></li>
<li><span class="koboSpan" id="kobo.997.1">Preprocess the data using the GPU (although as we will see, we will change this to </span><span class="No-Break"><span class="koboSpan" id="kobo.998.1">the CPU).</span></span></li>
<li><span class="koboSpan" id="kobo.999.1">Copy back the data to the CPU memory space (won’t </span><span class="No-Break"><span class="koboSpan" id="kobo.1000.1">be necessary).</span></span></li>
<li><span class="koboSpan" id="kobo.1001.1">Use the optimized parameters to process the data </span><span class="No-Break"><span class="koboSpan" id="kobo.1002.1">during training.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1003.1">For our experiments, we are going to use the technique of </span><span class="No-Break"><span class="koboSpan" id="kobo.1004.1">fine-tuning directly.</span></span></p>
<p><span class="koboSpan" id="kobo.1005.1">Following the </span><a id="_idIndexMarker1031"/><span class="koboSpan" id="kobo.1006.1">aforementioned approach on the dataset selected for this recipe (</span><em class="italic"><span class="koboSpan" id="kobo.1007.1">WMT2016</span></em><span class="koboSpan" id="kobo.1008.1">), the GPU-based preprocessing took the following amount </span><span class="No-Break"><span class="koboSpan" id="kobo.1009.1">of time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1010.1">
Pre-processing time (s): 50.427586793899536</span></pre> <p><span class="koboSpan" id="kobo.1011.1">An end-to-end preprocessing pipeline must take into account the process of batching using the Gluon DataLoader to load the data (in our case, </span><a id="_idIndexMarker1032"/><span class="koboSpan" id="kobo.1012.1">into multiple GPUs), giving us the </span><span class="No-Break"><span class="koboSpan" id="kobo.1013.1">following performance:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1014.1">
Data-Loading in GPU time (s): 72.83465576171875</span></pre> <p><span class="koboSpan" id="kobo.1015.1">Compared to the initial section of this recipe (where the preprocessing took 27 seconds), we can see how, in this case, preprocessing in the GPU has not been effective. </span><span class="koboSpan" id="kobo.1015.2">This is due to the nature of the text data, which is not as straightforward to parallelize as it is with images, </span><span class="No-Break"><span class="koboSpan" id="kobo.1016.1">for example.</span></span></p>
<p><span class="koboSpan" id="kobo.1017.1">In this scenario, a CPU-based preprocessing pipeline is best, avoiding the Gluon NLP</span><em class="italic"> </em><strong class="source-inline"><span class="koboSpan" id="kobo.1018.1">ShardedDataLoader</span></strong><span class="koboSpan" id="kobo.1019.1"> class and using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1020.1">Gluon DataLoader</span></strong><span class="koboSpan" id="kobo.1021.1"> class instead (which is better suited for parallelizing). </span><span class="koboSpan" id="kobo.1021.2">Applying this pipeline, we get the </span><span class="No-Break"><span class="koboSpan" id="kobo.1022.1">following results:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1023.1">
Data-Loading in CPU with Gluon DataLoaders time (s): 24.988255500793457</span></pre> <p><span class="koboSpan" id="kobo.1024.1">This gives us a minimal edge (2 seconds), but, as mentioned, this is the best we can get with the usage of Gluon DataLoader and its </span><span class="No-Break"><span class="koboSpan" id="kobo.1025.1">parallelization capabilities.</span></span></p>
<p><span class="koboSpan" id="kobo.1026.1">After the preprocessing, the next step is the training process. </span><span class="koboSpan" id="kobo.1026.2">As described, we will evaluate the effect of our training optimizations using the technique of fine-tuning directly. </span><span class="koboSpan" id="kobo.1026.3">In combination with this approach, we use the </span><span class="No-Break"><span class="koboSpan" id="kobo.1027.1">following hyperparameters:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1028.1">
# Epochs &amp; Batch Size
hparams.epochs = 5
hparams.lr = 0.0001
# hparams.batch_size = num_gpus * 256</span></pre> <p><span class="koboSpan" id="kobo.1029.1">Please note how by adding multiple GPUs to the training process, we can increase the batch size (multiplied by the number of GPUs), and we can also increase the learning rate (from 0.00003 to 0.0001). </span><span class="koboSpan" id="kobo.1029.2">In these conditions, the training process duration and achieved performance is </span><span class="No-Break"><span class="koboSpan" id="kobo.1030.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1031.1">
Training time for 5 epochs: 1947.1244320869446 / Best validation loss: 1.2199710432327155</span></pre> <p><span class="koboSpan" id="kobo.1032.1">As we </span><a id="_idIndexMarker1033"/><span class="koboSpan" id="kobo.1033.1">can see, we got excellent validation performance (~1.4) with training that took ~3 hours. </span><span class="koboSpan" id="kobo.1033.2">When compared to the results obtained in the initial section of this recipe, we can see how there was a minimal decrease in the loss (a positive change, which we will confirm with our performance analysis shortly), but the largest improvement by far has been a 5.5x decrease in the training time. </span><span class="koboSpan" id="kobo.1033.3">This improvement is due to all the training optimization techniques that we have applied. </span><span class="koboSpan" id="kobo.1033.4">In a nutshell, each of the optimizations provided the </span><span class="No-Break"><span class="koboSpan" id="kobo.1034.1">following improvements:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1035.1">Using 4 GPUs</span></strong><span class="koboSpan" id="kobo.1036.1">: Provided a 4x decrease (</span><span class="No-Break"><span class="koboSpan" id="kobo.1037.1">as expected).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1038.1">Using Float16 and AMP</span></strong><span class="koboSpan" id="kobo.1039.1">: Provided a 1.4x decrease (5.5x combined), which is less than the 2x decrease expected, due to the small number of operations to be reduced to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1040.1">Float16</span></strong><span class="koboSpan" id="kobo.1041.1"> without compromising </span><span class="No-Break"><span class="koboSpan" id="kobo.1042.1">algorithmic performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1043.1">Preprocessing the datasets</span></strong><span class="koboSpan" id="kobo.1044.1">: In this case, there were </span><span class="No-Break"><span class="koboSpan" id="kobo.1045.1">negligible improvements.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1046.1">The evolution of the training loss and the validation loss across each epoch looked </span><span class="No-Break"><span class="koboSpan" id="kobo.1047.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer202">
<span class="koboSpan" id="kobo.1048.1"><img alt="Figure 8.7 – Optimized training: training loss versus validation loss" src="image/B16591_08_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1049.1">Figure 8.7 – Optimized training: training loss versus validation loss</span></p>
<p><span class="koboSpan" id="kobo.1050.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1051.1">Figure 8</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1052.1">.7</span></em><span class="koboSpan" id="kobo.1053.1">, we can</span><a id="_idIndexMarker1034"/><span class="koboSpan" id="kobo.1054.1"> see the evolution of the training and validation losses. </span><span class="koboSpan" id="kobo.1054.2">As explored throughout the chapters, we select the model that provided the minimal validation loss (in this case, achieved in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1055.1">first epoch).</span></span></p>
<p><span class="koboSpan" id="kobo.1056.1">After training is completed, we can verify the overall performance in the test split of our dataset. </span><span class="koboSpan" id="kobo.1056.2">From a quantitative point of view, these are the results </span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">we obtained:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1058.1">
WMT16 test loss: 1.27; test bleu score: 28.20</span></pre> <p><span class="koboSpan" id="kobo.1059.1">As expected, we got excellent results just by training for a limited number of epochs (5 in this case). </span><span class="koboSpan" id="kobo.1059.2">We can also confirm how the minimal improvement in the validation loss provided a minimal improvement in our test metrics (compared to 27.05 as </span><span class="No-Break"><span class="koboSpan" id="kobo.1060.1">initially obtained).</span></span></p>
<p><span class="koboSpan" id="kobo.1061.1">From a qualitative point of view, we can also check how well our model is performing by testing it with an example sentence. </span><span class="koboSpan" id="kobo.1061.2">In our case, we chose </span><strong class="source-inline"><span class="koboSpan" id="kobo.1062.1">I learn new things every day</span></strong><span class="koboSpan" id="kobo.1063.1">, and the output obtained is </span><span class="No-Break"><span class="koboSpan" id="kobo.1064.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1065.1">
Qualitative Evaluation: Translating from English to German
Expected translation:
 Ich lerne neue Dinge.
 </span><span class="koboSpan" id="kobo.1065.2">In English:
 I learn new things every day.
 </span><span class="koboSpan" id="kobo.1065.3">The German translation is:
 Ich lerne jedes Mal Neues.</span></pre> <p><span class="koboSpan" id="kobo.1066.1">The </span><a id="_idIndexMarker1035"/><span class="koboSpan" id="kobo.1067.1">German sentence obtained in the output (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1068.1">Ich lerne jedes Mal Neues</span></strong><span class="koboSpan" id="kobo.1069.1">) means </span><strong class="source-inline"><span class="koboSpan" id="kobo.1070.1">I learn something new every time</span></strong><span class="koboSpan" id="kobo.1071.1">, and therefore, as can be seen from the results, the text has been almost perfectly translated from English </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">to German.</span></span></p>
<h2 id="_idParaDest-187"><a id="_idTextAnchor188"/><span class="koboSpan" id="kobo.1073.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.1074.1">In this recipe, we </span><a id="_idIndexMarker1036"/><span class="koboSpan" id="kobo.1075.1">applied the different training optimization techniques seen in the first recipe of this chapter, leveraging our HW (CPUs and GPUs) to address each of the steps in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1076.1">training loop:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1077.1">We revisited how lazy evaluation and automatic parallelization mechanisms work together to optimize all </span><span class="No-Break"><span class="koboSpan" id="kobo.1078.1">MXNet-based flows.</span></span></li>
<li><span class="koboSpan" id="kobo.1079.1">We leveraged all our CPU threads to load data and tested to optimize that process further via preprocessing in the GPU. </span><span class="koboSpan" id="kobo.1079.2">In this case, it was shown how a CPU-based preprocessing pipeline in combination with Gluon DataLoader was the </span><span class="No-Break"><span class="koboSpan" id="kobo.1080.1">optimal approach.</span></span></li>
<li><span class="koboSpan" id="kobo.1081.1">We analyzed different data types and combined the accuracy and precision of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1082.1">Float32</span></strong><span class="koboSpan" id="kobo.1083.1"> with the speed-ups of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1084.1">Float16</span></strong><span class="koboSpan" id="kobo.1085.1">, and where </span><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">possible, AMP.</span></span></li>
<li><span class="koboSpan" id="kobo.1087.1">We increased the performance of our training loops by using multiple GPUs (assuming our HW has these </span><span class="No-Break"><span class="koboSpan" id="kobo.1088.1">devices available).</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1089.1">We compared each of these scenarios applied specifically to the task of </span><em class="italic"><span class="koboSpan" id="kobo.1090.1">translating text from English to German</span></em><span class="koboSpan" id="kobo.1091.1">, running two experiments. </span><span class="koboSpan" id="kobo.1091.2">In the first experiment, we did not apply any of the training optimization techniques described, following the approaches seen in </span><a id="_idIndexMarker1037"/><span class="koboSpan" id="kobo.1092.1">previous chapters of the book. </span><span class="koboSpan" id="kobo.1092.2">In the second experiment, we applied all the techniques in parallel, trying to optimize as much as </span><span class="No-Break"><span class="koboSpan" id="kobo.1093.1">we could.</span></span></p>
<p><span class="koboSpan" id="kobo.1094.1">This proved quite useful, delivering similar algorithmic performance, with a 5.5x improvement in training time (from 3 hours to 30 minutes). </span><span class="koboSpan" id="kobo.1094.2">This was mostly due to using multiple GPUs (4x decrease) and leveraging </span><strong class="source-inline"><span class="koboSpan" id="kobo.1095.1">Float16</span></strong><span class="koboSpan" id="kobo.1096.1"> and AMP (1.4x decrease), whereas the optimized preprocessing provided </span><span class="No-Break"><span class="koboSpan" id="kobo.1097.1">negligible improvements.</span></span></p>
<h2 id="_idParaDest-188"><a id="_idTextAnchor189"/><span class="koboSpan" id="kobo.1098.1">There’s more…</span></h2>
<p><span class="koboSpan" id="kobo.1099.1">We have described, implemented, executed, and evaluated several training optimization techniques. </span><span class="koboSpan" id="kobo.1099.2">However, there are even more advanced techniques that can be leveraged to achieve the optimal </span><span class="No-Break"><span class="koboSpan" id="kobo.1100.1">training loop.</span></span></p>
<p><span class="koboSpan" id="kobo.1101.1">One such technique is </span><strong class="bold"><span class="koboSpan" id="kobo.1102.1">Reinforcement Learning from Human Feedback </span></strong><span class="koboSpan" id="kobo.1103.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.1104.1">RLHF</span></strong><span class="koboSpan" id="kobo.1105.1">), where a </span><em class="italic"><span class="koboSpan" id="kobo.1106.1">human-in-the-loop</span></em><span class="koboSpan" id="kobo.1107.1"> process </span><a id="_idIndexMarker1038"/><span class="koboSpan" id="kobo.1108.1">is introduced. </span><span class="koboSpan" id="kobo.1108.2">In this process, after a model has been trained, a person is presented with different options for output by the model (for example, different potential translations) and they rank those responses according to how they better represent the original sentence. </span><span class="koboSpan" id="kobo.1108.3">These human inputs are then used to train a reward model that scores the output of the model and selects the one with the highest score. </span><span class="koboSpan" id="kobo.1108.4">This </span><a id="_idIndexMarker1039"/><span class="koboSpan" id="kobo.1109.1">technique has been proven to be extremely powerful. </span><span class="koboSpan" id="kobo.1109.2">As an example, </span><strong class="bold"><span class="koboSpan" id="kobo.1110.1">OpenAI</span></strong><span class="koboSpan" id="kobo.1111.1"> developed </span><strong class="bold"><span class="koboSpan" id="kobo.1112.1">ChatGPT</span></strong><span class="koboSpan" id="kobo.1113.1"> on</span><a id="_idIndexMarker1040"/><span class="koboSpan" id="kobo.1114.1"> top of </span><a id="_idIndexMarker1041"/><span class="koboSpan" id="kobo.1115.1">the </span><strong class="bold"><span class="koboSpan" id="kobo.1116.1">GPT-3</span></strong><span class="koboSpan" id="kobo.1117.1"> language model </span><span class="No-Break"><span class="koboSpan" id="kobo.1118.1">using RLHF.</span></span></p>
<p><span class="koboSpan" id="kobo.1119.1">To learn more about </span><em class="italic"><span class="koboSpan" id="kobo.1120.1">ChatGPT</span></em><span class="koboSpan" id="kobo.1121.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1122.1">RLHF</span></em><span class="koboSpan" id="kobo.1123.1">, the following article is </span><span class="No-Break"><span class="koboSpan" id="kobo.1124.1">recommended: </span></span><a href="https://huyenchip.com/2023/05/02/rlhf.html"><span class="No-Break"><span class="koboSpan" id="kobo.1125.1">https://huyenchip.com/2023/05/02/rlhf.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1126.1">.</span></span></p>
</div>
</body></html>