["```py\n    aws sagemaker-runtime invoke-endpoint \\\n        --endpoint-name my-endpoint \\\n        --content-type 'application/json' \\\n        --body '{\"instances\": [1.0, 2.0, 5.0]}' \\\n        --custom-attributes 'tfs-model-name=other_model\n    ```", "```py\nimport tensorflow as tf\nimport tensorflow_hub as hub\nmodel_handle = \"https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2\"\nclassifier = hub.load(model_handle)\n```", "```py\nprobabilities = tf.nn.softmax(classifier(image)).numpy()\n```", "```py\n    model1\n        |--[model_version_number]\n            |--variables\n            |--saved_model.pb\n    model2\n        |--[model_version_number]\n            |--assets\n            |--variables\n            |--saved_model.pb\n    code\n        |--inference.py\n        |--requirements.txt\n```", "```py\nmodel_name = \"efficientnetv2-s\"\nmodel_dir = f\"./{model_name}/1\"\ncode_dir = f\"./{model_name}/code\"\nos.makedirs(model_dir, exist_ok=False)\nos.makedirs(code_dir, exist_ok=False)\ntf.saved_model.save(classifier, model_dir)\n```", "```py\ndef handler(data, context):\n    if context.request_content_type == \"application/json\":\n        instance = json.loads(data.read().decode(\"utf-8\"))\n    else:\n        raise ValueError(\n            415,\n            'Unsupported content type \"{}\"'.format(\n                context.request_content_type or \"Unknown\"\n            ),\n        )\n    if USE_GRPC:\n        prediction = _predict_using_grpc(context, instance)\n    else:\n        inst_json = json.dumps({\"instances\": instance})\n        response = requests.post(context.rest_uri, data=inst_json)\n        if response.status_code != 200:\n            raise Exception(response.content.decode(\"utf-8\"))\n        prediction = response.content\n    response_content_type = context.accept_header\n    return prediction, response_content_type\n```", "```py\nfrom tensorflow_serving.apis import predict_pb2\nfrom tensorflow_serving.apis import prediction_service_pb2_grpc\ndef _predict_using_grpc(context, instance):\n    grpc_request = predict_pb2.PredictRequest()\n    grpc_request.model_spec.name = \"model\"\n    grpc_request.model_spec.signature_name = \"serving_default\"\n    options = [\n        (\"grpc.max_send_message_length\", MAX_GRPC_MESSAGE_LENGTH),\n        (\"grpc.max_receive_message_length\", MAX_GRPC_MESSAGE_LENGTH),\n    ]\n    channel = grpc.insecure_channel(f\"0.0.0.0:{context.grpc_port}\", options=options)\n    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\ngrpc_request.inputs[\"input_1\"].CopyFrom(tf.make_tensor_proto(instance))\n    result = stub.Predict(grpc_request, 10)\n    output_shape = [dim.size for dim in result.outputs[\"output_1\"].tensor_shape.dim]\n    np_result = np.array(result.outputs[\"output_1\"].float_val).reshape(output_shape)\n    return json.dumps({\"predictions\": np_result.tolist()})\n```", "```py\nUSE_GRPC = True if os.getenv(\"USE_GRPC\").lower() == \"true\" else False \n```", "```py\n! cp 1_src/inference.py $code_dir\n! cp 1_src/requirements.txt $code_dir\n! tar -C \"$PWD\" -czf model.tar.gz  efficientnetv2-s/\n```", "```py\n    import sagemaker\n    from sagemaker import get_execution_role\n    sagemaker_session = sagemaker.Session()\n    role = get_execution_role() \n    bucket = sagemaker_session.default_bucket()\n    prefix = 'tf-serving'\n    s3_path = 's3://{}/{}'.format(bucket, prefix)\n    model_data = sagemaker_session.upload_data('model.tar.gz',\n                                               bucket,\n                                               os.path.join(prefix, 'model'))\n    ```", "```py\n    from sagemaker.tensorflow import TensorFlowModel\n    env = {\n           \"SAGEMAKER_TFS_ENABLE_BATCHING\":\"true\",\n           \"SAGEMAKER_TFS_MAX_BATCH_SIZE\":\"4\",\n           \"SAGEMAKER_TFS_BATCH_TIMEOUT_MICROS\":\"100000\",\n           \"SAGEMAKER_TFS_NUM_BATCH_THREADS\":\"6\",\n           \"SAGEMAKER_TFS_MAX_ENQUEUED_BATCHES\":\"6\",\n           \"USE_GRPC\":\"true\" # to switch between TFS REST and gRCP API\n           }\n    tensorflow_serving_model = TensorFlowModel(model_data=model_data,\n                                     name=\"efficientnetv2-1\",\n                                     role=role,\n                                     framework_version='2.8',\n                                     env=env,\n    sagemaker_session=sagemaker_session)\n    ```", "```py\nresponse_remote = predictor.predict(image.numpy())\nprobabilities = np.array(response_remote['predictions'])\ntop_5 = tf.argsort(probabilities, axis=-1, direction=\"DESCENDING\")[0][:5].numpy()\nnp_classes = np.array(classes)\n# Some models include an additional 'background' class in the predictions, so\n# we must account for this when reading the class labels.\nincludes_background_class = probabilities.shape[1] == 1001\nfor i, item in enumerate(top_5):\n  class_index = item if includes_background_class else item + 1\n  line = f'({i+1}) {class_index:4} - {classes[class_index]}: {probabilities[0][top_5][i]}'\n  print(line)\n```", "```py\n    model.tar.gz/\n    |- model_weights.pth\n    |- other_model_artifacts\n    |- code/\n      |- inference.py\n      |- requirements.txt  # optional\n    ```", "```py\n    mkdir distilbert-base-uncased-distilled-squad\n    wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/pytorch_model.bin -P distilbert-base-uncased-distilled-squad\n    wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer.json -P distilbert-base-uncased-distilled-squad\n    wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer_config.json -P distilbert-base-uncased-distilled-squad\n    wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/vocab.txt -P distilbert-base-uncased-distilled-squada\n    wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/config.json -P distilbert-base-uncased-distilled-squad\n    tar -C \"$PWD\" -czf distilbert-base-uncased-distilled-squad.tar.gz  distilbert-base-uncased-distilled-squad/\n    ```", "```py\n    import sagemaker\n    from sagemaker import get_execution_role\n    sagemaker_session = sagemaker.Session()\n    role = get_execution_role()\n    bucket = sagemaker_session.default_bucket()\n    prefix = 'torchserve'\n    s3_path = 's3://{}/{}'.format(bucket, prefix)\n    model_data = sagemaker_session.upload_data('distilbert-base-uncased-distilled-squad.tar.gz',bucket,os.path.join(prefix, 'model-artifacts'))\n    ```", "```py\n    from sagemaker.pytorch import PyTorchModel\n    env = {\n        \"SAGEMAKER_TS_BATCH_SIZE\": \"2\",\n        \"SAGEMAKER_TS_MAX_BATCH_DELAY\": \"1000\",\n        \"SAGEMAKER_TS_RESPONSE_TIMEOUT\" : \"120\",\n        \"SAGEMAKER_TS_MIN_WORKERS\" : \"1\",\n        \"SAGEMAKER_TS_MAX_WORKERS\" : \"2\"\n        }\n    model = PyTorchModel(model_data=model_data,\n                       role=role, \n                       entry_point='pipeline_predictor.py',\n                       source_dir='2_src',\n                       framework_version='1.9.0',\n                       py_version='py38',\n                       env=env,\n                       sagemaker_session=sagemaker_session)\n    ```", "```py\n    from sagemaker.serializers import JSONSerializer\n    from sagemaker.deserializers import JSONDeserializer\n    remote_predictor = model.deploy(initial_instance_count=1, instance_type=\"ml.g4dn.4xlarge\", serializer=JSONSerializer(), deserializer=JSONDeserializer())\n    ```", "```py\n    remote_predictor.predict(data)\n    ```", "```py\n    Model config: \n    { \"model\": { \"1.0\": { \"defaultVersion\": true, \"marName\": \"model.mar\", \"minWorkers\": 1, \"maxWorkers\": 2, \"batchSize\": 3, \"maxBatchDelay\": 100000, \"responseTimeout\": 120 } } }\n    ```", "```py\n    import torch\n    import torch_tensorrt\n    import os\n    torch.hub._validate_not_a_forked_repo = lambda a, b, c: True\n    MODEL_NAME = \"resnet50\"\n    MODEL_VERSION = \"1\"\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    # load model\n    model = (torch.hub.load(\"pytorch/vision:v0.10.0\", MODEL_NAME, pretrained=True).eval().to(device))\n    ```", "```py\n    # Compile with Torch TensorRT;\n    trt_model = torch_tensorrt.compile(\n        model,\n        inputs=[\n            torch_tensorrt.Input(\n                min_shape=(1, 3, 224, 224),\n                opt_shape=(8, 3, 224, 224),\n                max_shape=(16, 3, 224, 224),\n                dtype=torch.float32,\n            )\n        ],\n        enabled_precisions={ torch.float32 },\n    )\n    ```", "```py\n    # Save the model\n    model_dir = os.path.join(os.getcwd(), \"3_src\", MODEL_NAME, MODEL_VERSION)\n    os.makedirs(model_dir, exist_ok=True)\n    print(model_dir)\n    torch.jit.save(trt_model, os.path.join(model_dir, \"model.pt\"))\n    ```", "```py\nname: \"resnet50\"\nplatform: \"pytorch_libtorch\"\nmax_batch_size : 128\ninput [\n  {\n    name: \"input__0\"\n    data_type: TYPE_FP32\n    dims: [ 3, 224, 224 ]\n  }\n]\noutput [\n  {\n    name: \"output__0\"\n    data_type: TYPE_FP32\n    dims: [ 1, 1000 ,1, 1]\n  }\n]\ndynamic_batching {\n   preferred_batch_size: 128\n   max_queue_delay_microseconds: 1000\n }\ninstance_group {\n  count: 1\n  kind: KIND_GPU\n}\n```", "```py\nresnet50\n|- 1\n  |- model.pt\n|- config.pbtxt\n```", "```py\n    account_id_map = {\n     # <REDACTED_FOR_BREVITY>\n    }\n    region = boto3.Session().region_name\n    if region not in account_id_map.keys():\n        raise(\"UNSUPPORTED REGION\")\n    base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n    triton_image_uri = \"{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.05-py3\".format(\n        account_id=account_id_map[region], region=region, base=base)\n    ```", "```py\n    unique_id = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n    sm_model_name = \"triton-resnet50-\" + unique_id\n    container = {\n        \"Image\": triton_image_uri,\n        \"ModelDataUrl\": model_data,\n        \"Environment\": {\"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME\": \"resnet50\"},\n    }\n    create_model_response = sm_client.create_model(\n        ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container\n    )\n    ```", "```py\n    endpoint_config_name = \"triton-resnet50-\" + unique_id\n    create_endpoint_config_response = sm_client.create_endpoint_config(\n        EndpointConfigName=endpoint_config_name,\n        ProductionVariants=[\n            {\n                \"InstanceType\": \"ml.g4dn.4xlarge\",\n                \"InitialVariantWeight\": 1,\n                \"InitialInstanceCount\": 1,\n                \"ModelName\": sm_model_name,\n                \"VariantName\": \"AllTraffic\",\n            }\n        ],)\n    ```", "```py\n    endpoint_name = \"triton-resnet50-\" + unique_id\n    create_endpoint_response = sm_client.create_endpoint(\n        EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)\n    ```", "```py\npayload = {\n    \"inputs\": [\n        {\n            \"name\": \"input__0\",\n            \"shape\": [1, 3, 224, 224],\n            \"datatype\": \"FP32\",\n            \"data\": get_sample_image(),\n        }\n    ]\n}\nresponse = runtime_sm_client.invoke_endpoint(   EndpointName=endpoint_name, ContentType=\"application/octet-stream\", Body=json.dumps(payload))\npredictions = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n```"]