- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Solving Regression Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned how to set up and run MXNet, work with
    Gluon and DataLoaders, and visualize datasets for regression, classification,
    image, and text problems. We also discussed the different learning methodologies
    (supervised learning, unsupervised learning, and reinforcement learning). In this
    chapter, we are going to focus on supervised learning, where the expected outputs
    are known for at least some examples. Depending on the given type of these outputs,
    supervised learning can be decomposed into regression and classification. Regression
    outputs are numbers from a continuous distribution (such as predicting the stock
    price of a public company), whereas classification outputs are defined from a
    known set (for example, identifying whether an image corresponds to a mouse, a
    cat, or a dog).
  prefs: []
  type: TYPE_NORMAL
- en: Classification problems can be seen as a subset of regression problems, and
    therefore, in this chapter, we will start working with the latter ones. We will
    learn why these problems are suitable for deep learning models with an overview
    of the equations that define these problems. We will learn how to create suitable
    models and how to train them, emphasizing the choice of hyperparameters. We will
    end each section by evaluating the models according to our data, as expected in
    supervised learning, and we will see the different evaluation criteria for regression
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following recipes in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the math of regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining loss functions and evaluation metrics for regression
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating regression models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    are some of the additional requirements needed for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you have completed *Recipe, Installing MXNet, Gluon, GluonCV and
    GluonNLP* from [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016) *Up and Running*
    *with MXNet*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ensure that you have completed *Recipe 1, Toy dataset for regression – load,
    manage, and visualize a house sales dataset* from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029)*,
    Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch03](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch03).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, you can access each recipe directly from Google Colab, for example,
    for the first recipe of this chapter: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch03/3_1_Understanding_Maths_for_Regression_Models.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch03/3_1_Understanding_Maths_for_Regression_Models.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the math of regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous chapter, **regression** problems are a type of **supervised
    learning** problem whose output is a number from a continuous distribution, such
    as the price of a house or the predicted value of a company stock price.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest model we can use for a regression problem is a **linear regression**
    model. However, these models are extremely powerful for simple problems, as their
    parameters can be trained and are very fast and explainable, given the small number
    of parameters involved. As we will see, this number of parameters is completely
    dependent on the number of features we use.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting property of linear regression models is that they can be
    represented by neural networks, and as neural networks will be the basis for most
    models that we will be using throughout the book, this is the linear regression
    model based on neural networks that we will be using.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest neural network model is known as the **Perceptron**, and this is
    the building block we will be working on not only in this recipe, but throughout
    the whole chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Before jumping in to understand our model, let me mention that for the math
    part of this recipe, we will encounter a little bit of matrix operations and linear
    algebra, but it will not be hard at all.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will work through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Modeling a biological neuron mathematically
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining a regression model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Describing basic activation functions
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initializing the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Modeling a biological neuron mathematically
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The Perceptron was first introduced by American psychologist Frank Rosenblatt
    in 1958 at Cornell Aeronautical Laboratory, and it was an initial attempt at replicating
    how information is processed by the neurons in our brain.
  prefs: []
  type: TYPE_NORMAL
- en: Rosenblatt analyzed a biological neuron and developed a mathematical model that
    behaved similarly. To make a comparison between these architectures, we will start
    with a very simple model of a neuron.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.1 – Biological neuron](img/B16591_03_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Biological neuron
  prefs: []
  type: TYPE_NORMAL
- en: 'As we can see in *Figure 3**.1*, the neuron is composed of three main parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dendrites**: Where the neuron receives inputs from other neurons. Depending
    on how strong the connection is, an input will be increased or decreased in the
    dendrites.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cell Body** or **Soma**: Contains the nucleus, which is the structure that
    receives all the inputs from the dendrites and processes them. The nucleus might
    trigger an electrical message to be communicated to other neurons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Axon/Axon Terminals**: This is the output structure, which communicates messages
    with other neurons.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Rosenblatt took the preceding simplified model of the neuron and assigned to
    it certain mathematical properties:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – Perceptron](img/B16591_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – Perceptron
  prefs: []
  type: TYPE_NORMAL
- en: '**Weights**: This will simulate the behavior of dendrites by multiplying the
    inputs or features with a set of weights (*W* in *Figure 3**.2*, while *j* refers
    to any neuron in the model).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sum** and **bias**: The combination of input signals done in the nucleus
    will be modeled as a sum with a bias (*θ* in *Figure 3**.2*) and a processing
    function, called the **activation function**. (We will describe these functions
    in the following step.)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Output**: Either connections to other neurons, or the direct output of the
    whole model (*o* in *Figure 3**.2*).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing *Figure 3**.1* and *Figure 3**.2*, we can see the similarities between
    the simplified model of the biological neuron and the Perceptron. Furthermore,
    we can also see how all these parts are connected together, from processing the
    input to delivering the output.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a regression model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Therefore, from a mathematical perspective using matrix multiplication, we can
    write the following equations for the model *y = f(W*⋅*X + b)*, where *W* is the
    weight vector *[W*1*, W*2*, …. W*n*]*, (n is the number of features), *X* is the
    feature vector *[X*1*, X*2*, …. X*n*]*, *b* is the bias term, and *f()* is the
    activation function. For the regression case we will be dealing with, we will
    work with a linear activation function, where the output is equal to the input.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in our case, the activation function is the identity function (output
    equal to the input), we have *y = W**⋅**X +* *b*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can easily achieve this with MXNet and its NDArray library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: And that’s it! That’s our *y* neural network output in terms of the *X* inputs
    and the *W* and *b* parameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: In the original paper by Rosenblatt, the expected output was either 0 or 1 (classification
    problem) and to fulfill this requirement, the activation function was defined
    as the step function (0 if the input is smaller than 0, or 1 if the input is larger
    than or equal to 0). This was one of the strongest limitations to Rosenblatt’s
    neuron model, and different activation functions were proposed later that improve
    the behavior of the model.
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning networks, we do not use a single neurons (Perceptrons) as our
    model. Typically, several layers of Perceptrons are stacked together, and the
    number of layers is also known as the **depth** of the network.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – Deep learning network](img/B16591_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – Deep learning network
  prefs: []
  type: TYPE_NORMAL
- en: These networks are quite powerful and have been proven to match or surpass human-level
    performance in several fields, including image recognition in computer vision
    and sentiment analysis in natural language processing.
  prefs: []
  type: TYPE_NORMAL
- en: Describing basic activation functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The most common activation functions for regression problems are the linear
    activation function and the **ReLU** activation function. Let’s briefly describe
    them.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Activation functions for regression](img/B16591_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Activation functions for regression
  prefs: []
  type: TYPE_NORMAL
- en: Linear Activation Function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: In this function, the output is equal to the input. It is not bounded, and therefore
    it is suitable for unbounded numerical outputs, as is the case with the output
    for the regression problems.
  prefs: []
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The **Rectified Linear Unit** activation function is very similar to the linear
    activation function: its output is equal to the input, but in this case, only
    when the input is larger than 0; the output is 0 otherwise. This function is suitable
    to only pass positive information to the next layers (sparse activation), and
    also provides better gradient propagation. Therefore its use is very common in
    intermediate layers on deep learning networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: As we will see in the next recipes, training involves computing iteratively
    new gradients and using these computations to update the model parameters. When
    using activation functions such as the sigmoid, the larger the number of layers,
    the smaller the gradient becomes. This problem is known as the vanishing gradient
    problem, and the ReLU activation function behaves better in those scenarios.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have defined our model and its behavior theoretically, but we have
    not used our problem framing or our dataset to define it. In this section, we
    will start working at a more practical level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step in defining our model is to decide which features (inputs) we
    are going to work with. We will continue using the House Sales Dataset we met
    in *Recipe 1, Toy dataset for regression – load, manage, and visualize house sales
    dataset* in [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029)*, Working with MXNet
    and Visualizing Datasets: Gluon and DataLoader*. This dataset contained data for
    21,613 houses, including prices and 19 input features. Although in our model we
    will work with all the input features, in the aforementioned recipe, we saw that
    the three non-correlated features that primarily contributed to the house price
    were as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Square feet of living space
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Grade
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of bathrooms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For an initial study, we will work with these three features. After selecting
    these features, if we show the first five houses, we will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – Filtered features for house prices](img/B16591_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – Filtered features for house prices
  prefs: []
  type: TYPE_NORMAL
- en: One of the paths that we did not exploit when analyzing this dataset previously
    is that `grade` is not a continuous feature as the others are; its values come
    from a discrete set of values. This type of feature is called a categorical feature,
    and can be nominal or ordinal. A nominal feature is a class name – for example,
    we could have the architectural style of the house as a feature, and the values
    of that feature could be Victorian, Art Deco, Craftsman, and so on. Ordinal features,
    on the other hand, are class numbers. In our case, `grade` is an ordinal feature
    consisting of numerical values in an order (1 is worst, 13 is best).
  prefs: []
  type: TYPE_NORMAL
- en: In both cases, categorical features can be represented numerically in different
    ways that will help our models better learn the relationship between that particular
    feature and the output. There are several approaches to dealing with categorical
    features. In this example, we are going to work with one of the simplest approaches,
    a **one-hot** **encoding** scheme.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: You will find more information on working with categorical data in the *There’s
    more* section at the end of this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'With a one-hot encoding scheme, each of the categories is decomposed into its
    own feature and will be assigned a binary value accordingly. In our case, `grade`
    contains integer values from 1 to 13, and therefore, we add 13 new features to
    our input vector. Each of these new features will have a value of 0 or 1\. For
    example, for a house of grade 1, the feature vector looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – One-hot encoding for grade feature](img/B16591_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – One-hot encoding for grade feature
  prefs: []
  type: TYPE_NORMAL
- en: If we take a close look at *Figure 3**.6*, we will see that there is no one-hot
    encoding for the grade column corresponding to the value 2\. This is because no
    actual house in our dataset had that grade, and therefore, it has not been added
    as a feature.
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the final number of features is 14 and we have 1 output, the price
    of the house.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have defined the input (the features) and output dimensions, we
    can initialize our model. We will explore this in more detail in the next recipes,
    but it is useful to show a glimpse of how this would look:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the `Weights` vector has 14 components (the number of features)
    and the `Bias` vector has 1 component.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that our model is initialized, we can use it to estimate the price of the
    first house, which can be seen in *Figure 3**.6* to be around 2.2 million dollars.
    With our current model, the estimated house price is (in $):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'As we have the expected price, we can compute some error metrics. In this case,
    I have chosen the absolute error and the error relative to the actual price. These
    quantities can be easily computed in Python:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'The values obtained for the errors are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, 2.6k dollars is a very small price for a house of 1,180 sqft,
    even though it only has 1 bathroom and is of an average grade (7). This means
    our error metrics have very large values suggesting a ~99% error rate. This means
    that either we are not evaluating our model properly (in this case, we just used
    1 value, we might have been unlucky) or we only used the initialized parameters
    that did not give us an accurate estimation. We need to improve our model parameters
    using a process called **training** to improve our evaluation metrics. We will
    explore these topics in detail in the next recipes.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regression models can be as complex as the model designer wants. They can have
    as many layers as necessary to model adequately the relationships between the
    input features and the desired output values.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we described how a biological neuron works inside our brain
    and simplified it to derive a simple mathematical model that we could use for
    our regression problem. In our case, we only used one layer, typically called
    the input layer, and we defined the weights and bias as its parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we learned how to initialize our model, explored the effect initialization
    has on the weights and bias, and saw how we can use our data to evaluate the model.
    We will develop all these topics further in the next recipes.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we briefly introduced several topics. We started by describing
    Rosenblatt’s Perceptron. If you want to read the original paper, here is the link:
    [https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf](https://www.ling.upenn.edu/courses/cogs501/Rosenblatt1958.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Although in this recipe and the following ones we are going to work with some
    equations, we will use libraries and code that will allow us to focus on the actual
    outputs and their relationships with the inputs. However, for the interested reader,
    here is a refresher: [https://machinelearningmastery.com/gentle-introduction-linear-algebra/](https://machinelearningmastery.com/gentle-introduction-linear-algebra/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we analyzed the input features in more detail, specifically working
    with `grade`, a categorical feature, using one-hot encoding. There are different
    ways to work with categorical data, which are explored at this link: https://towardsdatascience.com/understanding-feature-engineering-part-2-categorical-data-f54324193e63.'
  prefs: []
  type: TYPE_NORMAL
- en: For more information regarding initialization and evaluation, please continue
    reading the recipes that follow in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Defining loss functions and evaluation metrics for regression
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we defined our input features, described our model,
    and initialized it. At that point, we passed the features vector of a house to
    predict the price, calculated the output, and compared it against the expected
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'At the end of the previous recipe, the comparison of the expected output and
    the actual output of the model intuitively provided us with an idea of how good
    our model was. This is what it means to “evaluate” our model: we assessed the
    model’s performance. However, that evaluation is not complete for several reasons,
    as we did not correctly take into account several factors:'
  prefs: []
  type: TYPE_NORMAL
- en: We only evaluated the model on one house – what about the others? How can we
    take all houses into account in our evaluation?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the difference between values an accurate measurement of model error? What
    other operations make sense?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will cover how we can assess (that is, evaluate) the performance
    of our models, and study functions that are suitable for this matter.
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we will introduce a concept that will be very important when we
    optimize (i.e., train) our models: loss functions.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before defining some useful functions for evaluating regression models and
    computing their losses, let’s specify two required and three desirable properties
    of the functions that we’ll use to evaluate our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[Required] **Continuous**: Obviously, we would like our evaluation functions
    to not be undefined on some potential error value, which will allow us to use
    these functions on a large set of pairs (expected output, actual model output).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Required] **Symmetric**: This is easily explained with an example. Let’s say
    that the price of a house is 2.2 million dollars – we would like our evaluation
    function to evaluate the model in the same way whether the output was 2 million
    dollars or 2.4 million dollars, as both values are the same distance from the
    expected value, just in different directions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Desirable] **Robust**: Again, this is easier to explain with an example. Taking
    the same example as in the previous point, imagine we have 2 outputs of 2.4 and
    2.8 million dollars. The error is already going to be large when compared with
    the expected output of 2.2 million dollars, so we would not want to make the error
    even larger due to the loss/evaluation function. From a mathematical perspective,
    we don’t want the error to grow exponentially, or it could make the computations
    diverge to **Not a Number (NaN**). With robust functions, large errors do not
    make computations diverge.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Desirable] **Differentiable**: This is the least intuitive of all the properties.
    Typically, we would aim to achieve error rates of as close as possible to zero.
    However, that is a theoretical scenario that only happens when we have enough
    data to describe a problem perfectly, and when the model is large enough that
    it can represent the mapping from the data to the output values. In reality, we
    can never be sure that we are complying with neither of these previous assumptions,
    and therefore, the unrealistic expectation of zero error evolves to become the
    minimum error possible for our data and our model. We can only detect the minimum
    values of a function by calculating its differential function, hence this **differentiable**
    property. A small stroke of luck, though, is that differentiability implies continuity,
    therefore if our function can satisfy property #4, it automatically satisfies
    property #1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[Desirable] **Simple**: The simpler the function that satisfies all the properties,
    the better, because that way we can understand the results more intuitively and
    it will not be costly, computationally speaking.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Not only evaluation functions must satisfy these criteria. As we will see in
    the next recipe, they must also be satisfied by a very important function for
    training, the *loss function*.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s discuss some evaluation and loss functions and analyze their advantages
    and disadvantages. The functions we will describe are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mean squared error
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Smooth L1 loss
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Mean absolute error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first function we are going to study is *almost* perfect according to the
    five properties described earlier. The intuitive idea of this function is to use
    the difference between values as an indicator of the distance or error between
    those values. We apply the `abs` function, meaning absolute value, to make it
    symmetrical:'
  prefs: []
  type: TYPE_NORMAL
- en: MAE =  1 _ n  ∑ j=1 n | y j −  ˆ y  j|
  prefs: []
  type: TYPE_NORMAL
- en: 'When plotted, the function produces the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.7 – \uFEFFMAE graph](img/B16591_03_7.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – MAE graph
  prefs: []
  type: TYPE_NORMAL
- en: 'If we analyze this function according to the properties defined previously,
    we see that all properties except #4 are fulfilled; unfortunately, the function
    is not differentiable at point 0\. As we will see in the next recipe, this is
    particularly challenging when this function is used as a loss function. However,
    when evaluating our models, this is not required as the evaluation process does
    not need differentiation, and **Mean Absolute Error** (**MAE**) is considered
    a typical regression metric in evaluation.'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: This function can be used for evaluation and can also be used as a loss function
    (by itself or as a regularization term). In this case, it is common to call it
    L1 loss or term explain how this relates to the rest of the sentence, as it computes
    the L1 distance of the vectors corresponding to the expected output and the actual
    output.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another similar metric is the **Mean Absolute Percentage Error** (**MAPE**).
    In this metric, each output error is normalized by the expected output:'
  prefs: []
  type: TYPE_NORMAL
- en: MAPE =  100% _ n  ∑ i=1 n | y i −  ˆ y  i _ y i |
  prefs: []
  type: TYPE_NORMAL
- en: Mean squared error
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To solve the differentiability issue, the **Mean Squared Error** (**MSE**)function
    is very similar to the MAE, but increases the order from 1 to 2 in the difference
    term. The intuitive idea is to use the simplest quadratic differentiable function
    (*x*2):'
  prefs: []
  type: TYPE_NORMAL
- en: MSE =  1 _ n  ∑ i=1 n (Y i −  ˆ Y ) 2
  prefs: []
  type: TYPE_NORMAL
- en: 'When plotted, the function produces the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.8 – \uFEFFMSE graph](img/B16591_03_8.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – MSE graph
  prefs: []
  type: TYPE_NORMAL
- en: 'If we analyze this function according to the properties defined, we see that
    all properties except #3 are fulfilled. Unfortunately, the function is not as
    robust as the MAE. Large errors grow exponentially, and therefore, this evaluation
    function is much more susceptible to outliers, as one single data point with a
    very large error can cause the squared error for that value to be quite large,
    and therefore it will make a large contribution to the MSE, leading to erroneous
    conclusions.'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: This function can be used for loss functions (by itself or as a regularization
    term). In this case. It is common to call it L2 loss or term (ridge regression),
    as it computes the L2 distance of the vectors corresponding to the expected output
    and the actual output.
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to have the same units as the output variable, the MSE can have the
    squared root applied. This evaluation metric is called **Root Mean Squared** **Error**
    (**RMSE**):'
  prefs: []
  type: TYPE_NORMAL
- en: RMSE = √ _ ∑ i=1 n  ( ˆ y  i − y i) 2 _ n
  prefs: []
  type: TYPE_NORMAL
- en: Smooth mean absolute error/smooth L1 loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Can’t we have the best of both worlds? Of course we can!
  prefs: []
  type: TYPE_NORMAL
- en: 'By combining both functions – the MSE for small values of the error and the
    MAE for the large values of the error – we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: smooth   L 1 (x) = { 0.5 x 2 if |x| < 1  |x| − 0.5 otherwise
  prefs: []
  type: TYPE_NORMAL
- en: 'When plotted, the function produces the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Smooth mean absolute error graph](img/B16591_03_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Smooth mean absolute error graph
  prefs: []
  type: TYPE_NORMAL
- en: If we analyze this function according to the properties defined, we see that
    all properties are fulfilled.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: This function can be used for loss functions. In this case, it is common to
    call it smooth L1 loss.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the first recipe of this chapter, we designed our first regression model
    based on a simple biological neuron. We initialized its parameters randomly and
    performed our first naive evaluation. The result was not good, and we conjectured
    that this was due to two reasons: our evaluation mechanism was not robust enough
    and our model parameters had not been optimized.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we explored how to improve on the first reason: evaluation.
    We covered three of the most important evaluation metrics and mentioned their
    relationship with loss functions, which we will explore in detail in the next
    recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we discussed which evaluation metrics are better, exploring how the
    MAE is robust but unfortunately not differentiable, and the MSE is differentiable
    but allows outliers to steer the metric (which is not ideal). We concluded the
    recipe by combining the functions to get the best of both worlds.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A very interesting set of evaluation functions that we did not explore in this
    recipe is the coefficient of determination and its extensions. However, this set
    is only used for linear regression modeling. More information can be found at
    the following links:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Coefficient of** **determination**[https://en.wikipedia.org/wiki/Coefficient_of_determination](https://en.wikipedia.org/wiki/Coefficient_of_determination)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistics By** **Jim** [https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/](https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Furthermore, there are generally many different functions that can be used
    for evaluation and loss in regression problems; you can refer to this link for
    further details: [https://machine-learning-note.readthedocs.io/en/latest/basic/loss_functions.html](https://machine-learning-note.readthedocs.io/en/latest/basic/loss_functions.html)'
  prefs: []
  type: TYPE_NORMAL
- en: Training regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In supervised learning, training is the process of optimizing the parameters
    of a model towards a specific objective. It is typically the most complex and
    the most time-consuming step in solving a deep learning problem statement.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will visit the basic concepts involved in training a model.
    We will apply them to solve the regression model we previously defined in this
    chapter, combined with the usage of the functions we discussed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will predict house prices using the dataset seen in *Recipe 1, Toy dataset
    for regression – load, manage, and visualize house sales dataset* from [*Chapter
    2*](B16591_02.xhtml#_idTextAnchor029)*, Working with MXNet and Visualizing Datasets:
    Gluon* *and DataLoader.*'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a number of concepts that we should get familiar with to understand
    this recipe. These concepts define how the training will proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Loss function**: The training process is an iterative optimization process.
    As the training progresses, the model is expected to perform better against an
    operation that compares the expected output with the actual output of the model.
    That operation is the loss function, also known as the objective, target, or cost
    function, which is being optimized during the training process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizer**: With each iteration of the training process, each parameter
    of the model is updated by a quantity (calculated using the loss function). The
    optimizer is an algorithm that defines how that quantity is calculated. The most
    important hyperparameter of the optimizer is the **learning rate**, which is the
    multiplier applied to the calculated quantity to update the parameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dataset split**: Stopping the training when the model can perform at its
    best in the real world is critical to achieving success in deep learning projects.
    One method to accomplish this is to split our dataset into training, validation,
    and test sets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epochs**: This is the number of iterations for which the training process
    will run.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch size**: The number of training samples analyzed at a time to produce
    an estimation of the gradient.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will create our own training loop and evaluate how each
    hyperparameter influences the training. To achieve this, we will follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Improving the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the loss function and optimizer
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Splitting our dataset
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Analyzing fairness and diversity
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the number of epochs and the batch size
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Putting everything together for a training loop
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improving the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: To solve this problem, the architecture we explored in the previous recipes
    (Perceptron) will not be enough. We will stack several Perceptrons together and
    connect them through different layers. This architecture is known as a **Multi-Layer
    Perceptron** (**MLP**). We will define a network architecture with three hidden
    layers that are fully connected (dense) and use the **ReLU** activation function
    (introduced in the first recipe of the chapter) with 128, 1,024, and 128 neurons,
    respectively, and an output layer with the corresponding 1 single output. The
    last layer is left without an activation function, also called the linear activation
    function (*y =* *x*).
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, the house sales dataset is a very complex problem, and finding
    solutions that generalize well (i.e., work sufficiently well in the real world)
    isn’t easy. For this purpose, we have included two new advanced features in the
    model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch normalization**: With this step in the process, for each mini-batch,
    the input distribution is standardized. This helps with training convergence and
    generalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout**: This method consists of disabling neurons of the network randomly
    (given a probability). This helps reduce overfitting (this concept will be explained
    in the next recipe) and improve generalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Our code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'One important thing to note is that the input to our model has also been modified:'
  prefs: []
  type: TYPE_NORMAL
- en: The numerical inputs have been scaled to produce inputs with zero mean and unit
    variance. This improves the convergence of the training algorithm.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The categorial input (grade) has been one-hot encoded. We introduced this concept
    in *Recipe 4, Toy dataset for text tasks – load, manage, and visualize enron emails
    dataset* from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029)*, Working with MXNet
    and Visualizing Datasets: Gluon* *and DataLoader*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This increases the number of features to 30\. As the dataset contains ~20k
    rows, this provides around 600k data points. Let’s compare this to the number
    of parameters in our model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The number of trainable parameters in our model is ~270k. The number of data
    points is approximately two times the number of trainable parameters in our model.
    Typically, this is the minimum for a successful model and ideally, we would like
    to work with a dataset size of ~10 times the number of the parameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Even though the comparison between the data points available and the number
    of parameters of the model is a very useful one, different architectures have
    different requirements in terms of data. As usual, experimentation (trial and
    error) is the key to finding the right balance.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last important point about our model is the initialization method, as with
    multilayer networks, random initialization might not yield the best results. The
    most common methods nowadays are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Xavier initialization**: Takes into account the number of inputs and the
    number of outputs when computing the variance'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MSRA PReLU** or **Kaiming initialization**: The Xavier initialization method
    has some issues with ReLU activation functions, so this method is preferred'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MXNet provides very simple access to these functionalities, in this case, for
    MSRA PReLU initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Initialization methods provide values for the weights and bias so that the model
    activation functions are not initialized on saturated (flat) regions. The intuition
    is to have these weights and biases with zero mean and unit variance. For the
    statistical analysis, a link is provided in *There’s more...*.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the loss function and optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we saw in the previous recipe, the smooth L1 (also known as Huber) loss function
    will work quite well.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several optimizers that have been proven to work well for **supervised**
    **learning** problems:'
  prefs: []
  type: TYPE_NORMAL
- en: '`batch_size` parameter when we worked with **DataLoader** in [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029)*,
    Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Momentum/Nesterov accelerated gradient**: Gradient descent can have a problem
    with stability and can start jumping and getting trapped in local minima. One
    method to avoid these issues is to consider the past steps that the algorithm
    has taken, which is achieved with these two optimizers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adagrad**/**Adadelta**/**RMSprop**: GD uses the same learning rate for all
    parameters, without taking into account the frequency with which they are updated.
    Adagrad and these other optimizers deal with this issue by adjusting the learning
    rate by parameter. However, Adagrad learning rates decrease over time and may
    yield values close to zero that do not perform any further updates. To solve this
    problem, Adadelta and RMSprop were developed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adam**/**AdaMax**/**Nadam**: These state-of-the-art optimizers combine both
    of the improvements from gradient descent: past-step calculations and adaptive
    learning rates. Adam uses the L2 norm for the exponentially weighted average of
    the gradients, whereas AdaMax uses the infinity norm (max operation). Nadam replaces
    the momentum component in Adam with the Nesterov momentum to accelerate convergence.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MXNet and Gluon provides very simple interfaces to define the loss function
    and the optimizer. With the following two lines of code, we are choosing the Huber
    loss function and the Adam optimizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Splitting our dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the most important things to consider in all data science projects is
    the performance of a trained model on data outside the dataset we are going to
    work with. In supervised learning, for training and evaluation, we work with data
    knowing the desired (expected) output, so how can we make sure that when using
    our model on new data, without a known output, it is going to perform as expected?
  prefs: []
  type: TYPE_NORMAL
- en: 'We deal with this issue by splitting our dataset into three parts:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Training set**: The training set is used during training to compute the updates
    of the model parameters.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Validation set**: The validation set is used during training to check every
    epoch how the model has improved (or not) with those updates previously calculated.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Test set**: Finally, once the training has finished, we can compute its performance
    on *unseen data*, which is the test set, the only part of the dataset that was
    not used to improve the model during training.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Furthermore, in order to have stable training that will allow our model to
    work properly with data outside our dataset, the data split needs to be computed
    taking into account the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Size of the splits**: This depends on the amount of data available and the
    task. Typical percentage splits for training/validation/test data are 60/20/20
    or 80/10/10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Which data points to select for each of the splits**: The key here is to
    have a balanced dataset. For example, in our house prices dataset, we do not want
    to have only houses with two and three bedrooms on the training set, then four-bedroom
    houses in the validation set, and finally houses with five or more bedrooms in
    the test set. Ideally, each set should be an accurate representation of the dataset.
    This is very important for sensitive datasets where fairness and diversity must
    be considered.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can achieve these splits easily, in this case, using a function from the
    well-known library **scikit-learn**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding code snippet, we do our three-way split for train, validation,
    and test data in two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: We assign 20% of the dataset to the test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The remaining 80% will be split equally between the validation and test sets.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Analyzing fairness and diversity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Imagine for a moment that we work for a real-estate website, and we manage
    the data science team. There is a compelling feature that will drive traffic to
    our website: when homeowners want to sell a property, they can fill in some data
    about their house and will be able to see a machine learning-optimized estimate
    of the price at which they should put their house up for selling, with data indicating
    that it will be sold within the next 3 months at that price. This feature sounds
    really cool as homeowners can fine-tune the asking price of the house they want
    to sell, and potential buyers will see reasonable prices according to market.'
  prefs: []
  type: TYPE_NORMAL
- en: However, we suddenly realize we do not have enough data yet for houses with
    two or fewer bathrooms, and we know that feature is highly sensitive for our model.
    Deploying this model for real-life properties would mean that houses with two
    or fewer bathrooms could be valued closer to valuations of houses with more bathrooms
    by our model, simply because that is all the data our model has been able to see.
    This would mean that for the cheapest houses, the most affordable ones for low-income
    families, we will be increasing their prices *unfairly*, which would be a serious
    problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Our model cannot know better because we have not shown it better. In this scenario,
    what options do we have? These are the ones that might suit a real-world situation:'
  prefs: []
  type: TYPE_NORMAL
- en: Be confident about the robustness of our model and deploy it in production regardless.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convince business leaders not to deploy the model until we have all the data
    we need.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deploy the model in production, but only allow sellers to use it for houses
    with three bathrooms or more.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The first option is the least adequate of all, however, it is actually the
    most frequently applied one, due to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: It is inconvenient to work on a project for several months and delay it right
    before it is expected to launch. Management typically does not expect nor want
    this kind of news and it could put some jobs at risk.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, the most common reason for this to happen is that the error in the
    data goes unnoticed. There is never enough time to verify that the data is accurate/fair/diverse
    enough and the focus shifts to delivering an optimized model as soon as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The second option is difficult to argue for a similar reason as the first option,
    and the third option might look good on paper, but is actually very dangerous.
    If I found myself in that situation, I would not choose the third option, simply
    because we cannot be sure that the data is diverse and fair across all features,
    so a proper data quality assessment is required. If we found this kind of error
    this late in a project, it is because not enough focus was put on data quality.
    This typically happens with companies that have been recording or storing large
    amounts of data and now want to do some machine learning project with it, instead
    of designing data collection operations with a clear objective. This is one of
    the most common reasons why machine learning projects fail in such companies.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let us take a look at how our dataset looks from the point of view of fairness
    and diversity:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, as seen in *Recipe 1, Toy dataset for regression – load, manage and
    visualize house sales dataset* from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029)*,
    Working with MXNet and Visualizing Datasets: Gluon and DataLoader*, we will start
    with the price distribution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – Price distribution across the training, validation, and test
    sets](img/B16591_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – Price distribution across the training, validation, and test sets
  prefs: []
  type: TYPE_NORMAL
- en: Although we can see a small dip in houses with prices lower than $500k, the
    price distribution is fairly represented across the three datasets and no manual
    modifications are necessary.
  prefs: []
  type: TYPE_NORMAL
- en: 'The living space in square foot looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – Living Sqft plots for the training, validation, and test sets](img/B16591_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – Living Sqft plots for the training, validation, and test sets
  prefs: []
  type: TYPE_NORMAL
- en: The largest differences we can see here are due to a very small number of high-valued
    houses. We can even consider these outliers, and if the parameters of our training
    are well chosen, this should not harm our prediction capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of bathrooms looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – Number of bathrooms across the training, validation, and test
    sets](img/B16591_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – Number of bathrooms across the training, validation, and test
    sets
  prefs: []
  type: TYPE_NORMAL
- en: This distribution is quite well represented in our validation and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Grade looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – Grade for training, validation, and test sets](img/B16591_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – Grade for training, validation, and test sets
  prefs: []
  type: TYPE_NORMAL
- en: This distribution is also well represented in our validation and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'Grouping all the individual analyses, we can conclude that our training, validation,
    and test sets are fairly good representations of our full dataset. We must remember
    though that our dataset has its own limitations (although I have to say, for the
    selected features, it is quite well represented):'
  prefs: []
  type: TYPE_NORMAL
- en: 'Price: [75k$, 7.7M$]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Living Sqft: [290, 13540]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number of Bathrooms [0, 8]
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Grade: [1, 13], but lacking 2 as we pointed out earlier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining the number of epochs and batch size
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Number of epochs** refers to the number of iterations the training algorithm
    will run. Depending on the complexity of the problem and the optimizer and hyperparameters
    chosen, this number can vary from very low (say 5-10) to very high (thousands
    of iterations).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Batch size** refers to the number of training samples analyzed at the same
    time to estimate the error gradient. In *Recipe 3, Toy dataset for image tasks
    – load, manage and visualize iris dataset* in [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029)*,
    Working with MXNet and Visualizing Datasets: Gluon and DataLoader*, we introduced
    this concept as a means to optimize memory usage; the smaller the batch size,
    the less memory required. Furthermore, this speeds up computation of the gradient;
    the larger the batch size, the faster computations will run (if memory permits).
    Typical values range from 32 to 2,048 samples.'
  prefs: []
  type: TYPE_NORMAL
- en: Putting everything together for a training loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The training loop is the iterative process that runs the optimizer to calculate/estimate
    the gradient so that on each iteration, the error computed from the loss function
    (the objective of the optimizer) is reduced. As mentioned, each iteration is called
    an epoch. And for each iteration, the full training set is accessed in batches
    to compute the gradient.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, as we will see, it is interesting to compute the loss function
    for the validation set. In our case, we will also compute the loss function for
    the test set, as it will provide us with specific details on how our model will
    perform.
  prefs: []
  type: TYPE_NORMAL
- en: In order to understand the difference in behavior when modifying the hyperparameters,
    we are going to run the training loop for our house price prediction dataset several
    times, modifying just one hyperparameter per table and keeping the rest of the
    variables constant (unless otherwise noted).
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer and learning rate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As discussed earlier, the chosen optimizer for the training loop and the learning
    rate are intimately related, as for some optimizers (such as SGD) the learning
    rate is kept constant, whereas for others (such as Adam) it varies from a given
    starting point.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The best optimizer depends on several factors, and nothing trumps trial and
    error. I strongly suggest to try a few and see which one fits best. In my experience,
    SGD and Adam are typically the ones that work best, including this problem, the
    prediction of house prices.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s analyze how the training loss and validation loss vary for the SGD optimizer
    by varying the **learning rate** (**LR**) while keeping the other parameters constant:
    *Epochs = 100, Batch Size = 128, Loss fn =* *HuberLoss*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Loss for the SGD optimizer when varying the learning rate](img/B16591_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Loss for the SGD optimizer when varying the learning rate
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 3**.14*, we can conclude that for the SGD optimizer, an LR value
    of between 10-1 and 1.0 is optimal. Furthermore, we can see that for very large
    values of LR (> 2.0) the algorithm diverges. That’s why, when searching for optimal
    values for the LR, it’s better to start small.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s analyze how the training loss and validation loss vary for the Adam optimizer
    by varying the **learning rate** (**LR**) and keeping the other parameters constant:
    *Epochs = 100, Batch Size = 128, Loss fn =* *HuberLoss*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – Loss for Adam optimizer when varying the learning rate](img/B16591_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – Loss for Adam optimizer when varying the learning rate
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 3**.15*, we can conclude that for the Adam optimizer, an LR value
    of between 10-4 and 10-3 is optimal. As Adam calculates gradients differently,
    it is more difficult to make it diverge than SGD.
  prefs: []
  type: TYPE_NORMAL
- en: Adam requires smaller values for the learning rate because it *adapts* the value
    as the training process evolves.
  prefs: []
  type: TYPE_NORMAL
- en: Batch size
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s analyze how the training loss and validation loss vary for the Adam optimizer
    by varying the batch size, keeping the other parameters constant: *Epochs = 100,
    LR = 10-2, Loss fn =* *HuberLoss*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – Loss for the Adam optimizer when varying batch size](img/B16591_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – Loss for the Adam optimizer when varying batch size
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 3**.16,* we can conclude that for the Adam optimizer, a batch size
    value of between 64 and 1,024 provides the best results.
  prefs: []
  type: TYPE_NORMAL
- en: Epochs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Another hyperparameter is the number of epochs, meaning the number of times
    the optimizer is going to process the full training set.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s analyze how the training loss and validation loss vary for the Adam optimizer
    varying the epochs, keeping the other parameters constant: *LR = 10-2, Batch Size
    = 128, Loss fn =* *HuberLoss*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Loss for the Adam optimizer when varying the number of epochs](img/B16591_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Loss for the Adam optimizer when varying the number of epochs
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 3**.17*, we can conclude that around 100-200 epochs is good for
    our problem. With these values, it is very likely the best result will be achieved
    earlier than that.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On our journey toward solving our regression problem, we learned in this recipe
    how to update our model hyperparameters optimally. We understood the role that
    each hyperparameter plays in the training loop and we performed some ablation
    studies for each individual hyperparameter. This helped us understand how our
    training and validation losses behaved when we modified each hyperparameter individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our current problem and the chosen model, we verified that the best set
    of hyperparameters was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimizer: Adam'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Learning Rate: 10-2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batch Size: 128'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of epochs: 200'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the training loop, these hyperparameters gave us a training loss
    of 0.10 and a validation loss of 0.10.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With our model definition, we introduced three new concepts: **batch normalization**,
    **dropout**, and **scaling**. I find the following links useful to understand
    these advanced topics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Introduction to batch** **normalization**: [https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/](https://machinelearningmastery.com/batch-normalization-for-training-of-deep-neural-networks/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Batch normalization research** **paper**: [https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Introduction to** **dropout**: [https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/](https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Dropout research** **paper**: [https://jmlr.org/papers/v15/srivastava14a.html](https://jmlr.org/papers/v15/srivastava14a.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Scaling**: [https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/](https://machinelearningmastery.com/how-to-improve-neural-network-stability-and-modeling-performance-with-data-scaling/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the subject of initialization, this article explores the Xavier and Kaiming
    methods in detail (includes links to the research papers): [https://pouannes.github.io/blog/initialization/](https://pouannes.github.io/blog/initialization/).'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we explored in depth how two optimizers, **SGD** and **Adam**,
    behave. These are two of the most important and best performant optimizers; however,
    there are many more, and some could work better for your specific problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'One excellent resource to learn about which optimizers are implemented in MXNet
    and their characteristics is the official documentation: [https://mxnet.apache.org/versions/1.6/api/python/docs/tutorials/packages/optimizer/index.html](https://mxnet.apache.org/versions/1.6/api/python/docs/tutorials/packages/optimizer/index.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To compare the behavior and performance of each optimizer, I personally like
    the visualizations shown in this link (optimizers section): [https://towardsdatascience.com/on-optimization-of-deep-neural-networks-21de9e83e1](https://towardsdatascience.com/on-optimization-of-deep-neural-networks-21de9e83e1).'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we worked with optimizers and their hyperparameters. Hyperparameter
    choice is a very complex problem, and it always requires a little bit of trial
    and error with each problem, verifying that the training loop works. A rule of
    thumb when selecting hyperparameters is to read research papers that tackle similar
    problems to yours and start with the hyperparameters proposed in those papers.
    You can then move from that starting point and see what works best for your particular
    case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Apart from the training loss and the validation loss at the end of the training
    loop, we also provided a third loss value, *best validation loss*, we will explore
    what this value means and how it is calculated in the next recipe. This all maps
    to a question we have not answered properly yet: *when do I stop my training loop?*
    We will address this question in the next recipe.'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating regression models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous recipe, we learned how to choose our training hyperparameters
    to optimize our training. We also verified how those choices affected the training
    and validation losses. In this recipe, we are going to explore how those choices
    affect our actual evaluation in the real world. The observant reader will have
    noticed that we split the dataset into three different sets: training, validation,
    and test. However, during our training, we only used the training set and the
    validation set. In this recipe, we will emulate some real-world behavior of our
    model by running it on the unseen data, the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When evaluating a model, we can perform qualitative evaluation and quantitative
    evaluation:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Qualitative evaluation** is the selection of one or more random (or not so
    random, depending on what we are looking for) samples and analyzing the result,
    verifying whether it matches our expectations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantitative evaluation** deals with computing the outputs for a large number
    of inputs and calculating statistics about them (typically the mean), hence we
    will compute the MAE and MAPE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Furthermore, we are going to take a look at how training can have a large influence
    on the evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before jumping in to model evaluation, let’s discuss how we can measure our
    model training performance. Therefore, the steps in this recipe are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring training performance – overfitting
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Qualitative evaluation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quantitative evaluation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measuring training performance – overfitting
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Deep learning networks are quite powerful, surpassing human-level performance
    on a variety of problems. However, when not kept in check, these networks can
    also provide incorrect and unexpected results. One of the most important and frequent
    errors happens when the network, using its full capability, memorizes the samples
    that are being shown (the training set), yielding excellent results for that data.
    However, in this scenario, the network has simply memorized the training samples,
    and when deployed in a real-world use case it is going to perform poorly. This
    type of error is called **overfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: Thankfully, there is a very successful strategy to deal with overfitting, and
    we have already touched on it. It starts with splitting our full dataset into
    a training set and a validation set, which we did in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'From a theoretical point of view, training and validation losses typically
    have behaviors similar to that shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.18 – Losses versus epochs – ideal](img/B16591_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – Losses versus epochs – ideal
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3**.18*, we can see how the training and validation losses typically
    evolve (an idealized portrayal). The training loss continues decreasing as the
    training progresses, always optimizing (albeit more slowly as the number of epochs
    increases). The validation loss, however, reaches a point where it does not decrease
    further, but rather increases. At the lowest level of validation loss is where
    the model has reached its peak performance and where we should stop the learning
    process (early).
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s examine how this kind of behavior looks in the real world. For our problem,
    the training loss and the validation loss evolve as the training progresses, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.19 – Losses versus epochs – real](img/B16591_03_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – Losses versus epochs – real
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 3**.19*, the validation loss is much noisier than in
    the ideal world, and early stopping is much more difficult to achieve successfully.
    A very easy implementation is to save the model every time the validation loss
    decreases. This way, we are always certain that given a number of epochs the training
    is going to be run, the model with the best (lowest) validation loss will be saved.
    This was the method implemented in the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To verify our model is behaving similarly to what we expect (yielding a low
    error when predicting a house price), one easy approach is to run our model for
    a random input from the test set (unseen data). This can easily be done with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding code snippet yields the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As expected, the error rate is quite reasonable (just 2.45%!).
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Although I tried to keep the code as reproducible as possible, including setting
    the seeds for all random processes, there might be some sources of randomness.
    This means that your results might be different, but typically the order of magnitude
    of errors will be similar.
  prefs: []
  type: TYPE_NORMAL
- en: Quantitative evaluation – MAE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s calculate the **MAE** function, as described in the *Defining loss functions
    and evaluation metrics for regression* recipe earlier in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The MAE is $81k. Taking into account that prices varied from $75k to $7.7 million,
    this error seems reasonable. Do not forget that estimating house prices is a hard
    problem!
  prefs: []
  type: TYPE_NORMAL
- en: Quantitative evaluation – MAPE
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The value provided by the MAE is good to get an idea of how small or large
    the errors are in our model’s predictions. However, it does not provide a very
    meaningful merit figure, as the same MAE could have been achieved in different
    ways:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Small errors for all houses*: As houses increase in price, the absolute error
    number will be higher, and hence an $80k MAE might be quite good.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Very large errors for cheap houses*: In this case, an $80k MAE will mean that
    for the cheapest houses, the error might be 2-3 times, or even worse, the actual
    price of the house. This scenario would be very bad.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In general, we can add to the MAE another figure, similarly calculated to provide
    a **relative** error rate, instead of relying solely on an **absolute** value.
    For our model, we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Looks like our model is not behaving too badly, yielding a MAPE of 16%!
  prefs: []
  type: TYPE_NORMAL
- en: Quantitative evaluation – thresholds and percentage
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Another question we could consider for evaluating our model is the following:
    *For how many houses (in %) did we accurately predict* *the price?*'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume we consider that we accurately predicted the price of a house
    if the predicted price error is less than 25%. In our case, this gives us the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This calculation gives us an 81%, well done!
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we could plot the percentage of houses we correctly predicted
    as a function of the error threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Percentage of correct estimations](img/B16591_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – Percentage of correct estimations
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3**.20*, we can see, as expected, that considering an error of 25%
    to deem a prediction accurate, our model yields 80%+ correct predictions.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we explored how to evaluate our regression model. To properly
    do this, we revisited the decision made previously to split our full dataset into
    a training set, a validation set, and a test set.
  prefs: []
  type: TYPE_NORMAL
- en: During training, we used the training set to calculate the gradients to update
    our model parameters, and the validation set to confirm the real-world behavior.
    Afterward, to evaluate our model performance, we used the test set, which was
    the only remaining set of unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: We discovered the value of describing our model behavior qualitatively by calculating
    the output of random samples, and of quantitatively evaluating our model performance
    by exploring calculations and graphs of MAE and MAPE.
  prefs: []
  type: TYPE_NORMAL
- en: We ended the recipe by defining what constitutes an accurate prediction by setting
    a threshold and plotting the behavior of the model by modifying the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Deep learning has surpassed human-level performance on multiple tasks. However,
    evaluating models properly is paramount to verify how models will perform when
    deployed in production environments in the real world. I found interesting this
    small list of tasks where human-level performance has been reached by AI: https://venturebeat.com/2017/12/08/6-areas-where-artificial-neural-networks-outperform-humans/.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When evaluation is not performed properly, models may not behave as expected.
    Two of the most significant large-scale problems of this type (at Google in 2015
    and Microsoft in 2016, respectively) are detailed in the following articles:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Google Mistakenly Tags Black People as ‘Gorillas,’ Showing Limits of** **Algorithms:**
    https://www.wsj.com/articles/BL-DGB-42522'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Statistics By** **Jim:** [https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/](https://statisticsbyjim.com/regression/r-squared-invalid-nonlinear-regression/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Unfortunately, although these issues are less and less frequent nowadays, they
    still exist. A database that contains these issues has been published and is updated
    whenever one of these issues is reported: [https://incidentdatabase.ai/.](https://incidentdatabase.ai/)'
  prefs: []
  type: TYPE_NORMAL
- en: 'To prevent these issues, Google wrote a set of principles to develop Responsible
    AI. I strongly recommend all AI practitioners to abide by them: [https://ai.google/principles/](https://ai.google/principles/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this stage, we have completed our journey through a complete regression
    problem: we explored our regression dataset, decided on our evaluation metrics,
    and defined and initialized our model. We understood the best hyperparameter combination
    of optimizer, learning rate, batch size, and epochs, and trained it with early
    stopping. Lastly, we concluded by evaluating our model qualitatively and quantitatively.'
  prefs: []
  type: TYPE_NORMAL
