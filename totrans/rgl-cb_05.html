<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer149">
<h1 class="chapter-number" id="_idParaDest-136"><a id="_idTextAnchor136"/>5</h1>
<h1 id="_idParaDest-137"><a id="_idTextAnchor137"/>Regularization with Data</h1>
<p>Even though there are plenty of regularization methods for models (with each model having a unique set of hyperparameters), sometimes, the most effective regularization comes from the data itself. Indeed, sometimes, even the most powerful model can’t have good performance if the data is not transformed <span class="No-Break">properly beforehand.</span></p>
<p>In this chapter, we’ll look at some methods that help regularize models <span class="No-Break">from data:</span></p>
<ul>
<li>Hashing high <span class="No-Break">cardinality features</span></li>
<li><span class="No-Break">Aggregating features</span></li>
<li>Undersampling an <span class="No-Break">imbalanced dataset</span></li>
<li>Oversampling an <span class="No-Break">imbalanced dataset</span></li>
<li>Resampling imbalanced data <span class="No-Break">with SMOTE</span></li>
</ul>
<h1 id="_idParaDest-138"><a id="_idTextAnchor138"/>Technical requirements</h1>
<p>In this chapter, you will apply several tricks to data, as well as resample datasets or download new data via the command line. To do so, you will need the <span class="No-Break">following libraries:</span></p>
<ul>
<li><span class="No-Break">NumPy</span></li>
<li><span class="No-Break">pandas</span></li>
<li><span class="No-Break">scikit-learn</span></li>
<li><span class="No-Break">imbalanced-learn</span></li>
<li><span class="No-Break">category_encoders</span></li>
<li><span class="No-Break">Kaggle API</span></li>
</ul>
<h1 id="_idParaDest-139"><a id="_idTextAnchor139"/>Hashing high cardinality features</h1>
<p>High cardinality <a id="_idIndexMarker236"/>features are qualitative features with many<a id="_idIndexMarker237"/> possible values. High cardinality features may appear in many applications, such as a country in a customer database, a phone model in advertising, or vocabulary in NLP applications. High cardinality issues can be manifold: not only may they lead to a very highly dimensional dataset, but they can also evolve as more and more values become available. Indeed, even if the data for the number of countries or vocabulary is arguably quite stable, there are new phone models every week, if not <span class="No-Break">every day.</span></p>
<p>Hashing is a very popular and useful way to deal with such problems. In this recipe, we’ll see what it is and how to use it in practice on a dataset to predict whether employees will leave <span class="No-Break">a company.</span></p>
<h2 id="_idParaDest-140"><a id="_idTextAnchor140"/>Getting started</h2>
<p>Hashing is a very useful trick in computer science in general, and it is widely used in cryptography or blockchain, for example. It is also useful in machine learning when dealing with high cardinality features. It does not necessarily help with regularization per se, but it can sometimes be a <span class="No-Break">side effect.</span></p>
<h3>What is hashing?</h3>
<p>Hashing is ofte<a id="_idIndexMarker238"/>n used in machine learning at the production level for dealing with high cardinality features. High cardinality features tend to have a growing number of possible outcomes. This can include things such as a mobile phone model, a software version, an item ID, and so on. In such cases, using one-hot encoding on high cardinality features may lead to <span class="No-Break">several problems:</span></p>
<ul>
<li>The required space is not fixed and can’t <span class="No-Break">be controlled</span></li>
<li>We need to figure out how to encode a <span class="No-Break">new value</span></li>
</ul>
<p>Using hashing instead of one-hot encoding can address <span class="No-Break">these limitations.</span></p>
<p>To do so, we must use a hash function that converts an input into a controlled output. One well-known hash function is <strong class="source-inline">md5</strong>. If we apply <strong class="source-inline">md5</strong> to some strings, we’ll get the <span class="No-Break">following results:</span></p>
<pre class="source-code">
from hashlib import md5
print('hashing of "regularization" -&gt;',
    md5(b'regularization').hexdigest())
print('hashing of "regularized" -&gt;',
    md5(b'regularized').hexdigest())
print('hashing of "machine learning" -&gt;',
    md5(b'machine learning').hexdigest())</pre>
<p>The output will look <span class="No-Break">like this:</span></p>
<pre class="source-code">
hashing of "regularization" -&gt; 04ef847b5e35b165c190ced9d91f65da
hashing of "regularized" -&gt; bb02c45d3c38892065ff71198e8d2f89
hashing of "machine learning" -&gt; e04d1bcee667afb8622501b9a4b4654d</pre>
<p>As we can see, hashing<a id="_idIndexMarker239"/> has several <span class="No-Break">interesting properties:</span></p>
<ul>
<li>No matter the input size, the output size <span class="No-Break">is fixed</span></li>
<li>Two similar inputs may lead to very <span class="No-Break">different outputs</span></li>
</ul>
<p>These properties allow hash functions to be very effective when used with high cardinality features. All we have to do <span class="No-Break">is this:</span></p>
<ol>
<li>Choose a <span class="No-Break">hash function.</span></li>
<li>Define the expected space dimension of <span class="No-Break">the output.</span></li>
<li>Encode our feature with <span class="No-Break">that function.</span></li>
</ol>
<p>Of course, there are some drawbacks <span class="No-Break">to hashing:</span></p>
<ul>
<li>There may be collisions – two different inputs may have the same output (even if this does not necessarily hurt performance if it’s not <span class="No-Break">that severe)</span></li>
<li>We may want similar inputs to have similar outputs (a well-chosen hashing function can<a id="_idIndexMarker240"/> have such <span class="No-Break">a property)</span></li>
</ul>
<h3>Required installations</h3>
<p>We need to do <a id="_idIndexMarker241"/>some preparation for this recipe. Since we will download a Kaggle dataset, first, we need to install the <span class="No-Break">Kaggle API:</span></p>
<ol>
<li>Install the library <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">pip</strong></span><span class="No-Break">:</span><pre class="source-code">
<strong class="bold">pip install kaggle</strong></pre></li>
<li>If you haven’t already done so, create a Kaggle account <span class="No-Break">at </span><a href="https://www.kaggle.com"><span class="No-Break">www.kaggle.com</span></a><span class="No-Break">.</span></li>
<li>Go to your profile page and create your API token by clicking <strong class="bold">Create New API Token</strong>. This should download a <strong class="source-inline">kaggle.json</strong> file to <span class="No-Break">your computer:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer140">
<img alt="Figure 5.1 – Screenshot of the Kaggle website" height="438" src="image/B19629_05_01.jpg" width="986"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – Screenshot of the Kaggle website</p>
<ol>
<li value="4">You need to move the freshly downloaded <strong class="source-inline">kaggle.json</strong> file to <strong class="source-inline">~/.kaggle</strong> via the following <span class="No-Break">command line:</span><pre class="source-code">
<strong class="bold">mkdir ~/.kaggle &amp;&amp; mv kaggle.json ~/.kaggle/.</strong></pre></li>
<li>You can now download the dataset with the following <span class="No-Break">command line:</span><pre class="source-code">
<strong class="bold">kaggle datasets download -d reddynitin/aug-train</strong></pre></li>
<li>We should now have a file called <strong class="source-inline">aug-train.zip</strong>, which contains the data we will use for this recipe. We also need to install the <strong class="source-inline">category_encoders</strong>, <strong class="source-inline">pandas</strong>, and <strong class="source-inline">sklearn</strong> libraries <a id="_idIndexMarker242"/>with the following <span class="No-Break">command line:</span><pre class="source-code">
pip install category_encoders pandas scikit-learn.</pre></li>
</ol>
<h2 id="_idParaDest-141"><a id="_idTextAnchor141"/>How to do it...</h2>
<p>In this recipe, we will load <a id="_idIndexMarker243"/>and quickly prepare the dataset (quickly in<a id="_idIndexMarker244"/> the sense that more data preparation could lead to better results), and then apply a logistic regression model to this classification task. On the selected dataset, the <strong class="source-inline">city</strong> feature has 123 possible outcomes, so it can be considered a high cardinality feature. Also, we can fairly assume that the production data could contain more cities, so the hashing trick would make <span class="No-Break">sense here:</span></p>
<ol>
<li>Import the required modules, functions, and classes: <strong class="source-inline">pandas</strong> for loading the data, <strong class="source-inline">train_test_split</strong> for splitting the data, <strong class="source-inline">StandardScaler</strong> for rescaling quantitative features, <strong class="source-inline">HashingEncoder</strong> for encoding qualitative features, and <strong class="source-inline">LogisticRegression</strong> as <span class="No-Break">the model:</span><pre class="source-code">
Import numpy as np</pre><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.preprocessing import StandardScaler, OneHotEncoder</pre><pre class="source-code">
from category_encoders.hashing import HashingEncoder</pre><pre class="source-code">
from sklearn.linear_model import LogisticRegression</pre></li>
<li>Load the dataset with <strong class="source-inline">pd.read_csv()</strong>. Note that we do not need to unzip the dataset first since the zip only contains one CSV file – <strong class="source-inline">pandas</strong> will take care of that <span class="No-Break">for us:</span><pre class="source-code">
df = pd.read_csv('aug-train.zip')</pre><pre class="source-code">
print('number of unique values for the feature city',</pre><pre class="source-code">
    df['city'].nunique())</pre></li>
</ol>
<p>As we can see, the <strong class="source-inline">city</strong> feature has <strong class="source-inline">123</strong> possible values in <span class="No-Break">the dataset:</span></p>
<pre class="source-code">
<strong class="bold">number of unique values for the feature city 123</strong></pre>
<ol>
<li value="3">Remove any missing data. We take a very brutal policy here: we remove all the features that have a large amount of missing data, and then we remove all the rows with remaining missing data. This is not a recommended approach in general since we lose a lot of potentially useful information. Since dealing with missing data isn’t the subject here, we will take this <span class="No-Break">simplistic approach:</span><pre class="source-code">
df = df.drop(columns=['gender', 'major_discipline',</pre><pre class="source-code">
    'company_size', 'company_type'])</pre><pre class="source-code">
df = df.dropna()</pre></li>
<li>Split the data<a id="_idIndexMarker245"/> into <a id="_idIndexMarker246"/>training and test sets with the <span class="No-Break"><strong class="source-inline">train_test_split</strong></span><span class="No-Break"> function:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    df.drop(columns=['target']), df['target'],</pre><pre class="source-code">
    stratify=df['target'], test_size=0.2,</pre><pre class="source-code">
    random_state=0</pre><pre class="source-code">
)</pre></li>
<li>Select and rescale any quantitative features. We will use the standard scaler to rescale the selected quantitative features, but any other scaler may work <span class="No-Break">fine too:</span><pre class="source-code">
quanti_feats = ['city_development_index', 'training_hours']</pre><pre class="source-code">
# Instantiate the scaler</pre><pre class="source-code">
scaler = StandardScaler()</pre><pre class="source-code">
# Select quantitative features</pre><pre class="source-code">
X_train_quanti = X_train[quanti_feats]</pre><pre class="source-code">
X_test_quanti = X_test[quanti_feats]</pre><pre class="source-code">
# Rescale quantitative features</pre><pre class="source-code">
X_train_quanti = scaler.fit_transform(X_train_quanti)</pre><pre class="source-code">
X_test_quanti = scaler.transform(X_test_quanti)</pre></li>
<li>Select and prepare “regular” qualitative features. Here, we will use the one-hot encoder<a id="_idIndexMarker247"/> from <strong class="source-inline">scikit-learn</strong>, though we could also apply the hashing trick to <span class="No-Break">those features:</span><pre class="source-code">
quali_feats = ['relevent_experience',</pre><pre class="source-code">
    'enrolled_university', 'education_level',</pre><pre class="source-code">
    'experience', 'last_new_job']</pre><pre class="source-code">
quali_feats = ['last_new_job']</pre><pre class="source-code">
# Instantiate the one hot encoder</pre><pre class="source-code">
encoder = OneHotEncoder()</pre><pre class="source-code">
# Select qualitative features to one hot encode</pre><pre class="source-code">
X_train_quali = X_train[quali_feats]</pre><pre class="source-code">
X_test_quali = X_test[quali_feats]</pre><pre class="source-code">
# Encode those features</pre><pre class="source-code">
X_train_quali = encoder.fit_transform(</pre><pre class="source-code">
    X_train_quali).toarray()</pre><pre class="source-code">
X_test_quali = encoder.transform(</pre><pre class="source-code">
    X_test_quali).toarray()</pre></li>
<li>Encode the high cardinality <strong class="source-inline">'city'</strong> feature with hashing. Since there are currently <strong class="source-inline">123</strong> possible values for this feature, we could use only 7 bits to encode the whole space of possibilities. This is what is denoted by the <strong class="source-inline">n_components=7</strong> parameter. For safety, we could set it to 8 or more bits, to consider a growing set <a id="_idIndexMarker248"/>of<a id="_idIndexMarker249"/> possible cities in <span class="No-Break">the data:</span><pre class="source-code">
high_cardinality_feature = ['city']</pre><pre class="source-code">
# Instantiate the hashing encoder</pre><pre class="source-code">
hasher = HashingEncoder(n_components=7)</pre><pre class="source-code">
# Encode the city feature with hashing</pre><pre class="source-code">
X_train_hash = hasher.fit_transform(</pre><pre class="source-code">
    X_train[high_cardinality_feature])</pre><pre class="source-code">
X_test_hash = hasher.fit_transform(</pre><pre class="source-code">
    X_test[high_cardinality_feature])</pre><pre class="source-code">
# Display the result on the training set</pre><pre class="source-code">
X_train_hash.head()</pre></li>
</ol>
<p>The output will look something <span class="No-Break">like this:</span></p>
<pre class="source-code">
  col_0    col_1    col_2    col_3    col_4    col_5    col_6
18031     1       0         0       0       0         0              0
16295     0       0         0       1        0         0              0
7679      0       0         0        0       0         1              0
18154     0       0         1        0        0        0              0
10843     0       0         0        0        0        1              0</pre>
<p class="callout-heading">Note</p>
<p class="callout">As we can see, all the values are encoded in seven columns, spanning 2^7 = 128 <span class="No-Break">possible values.</span></p>
<ol>
<li value="8">Concatenate all <a id="_idIndexMarker250"/>the <a id="_idIndexMarker251"/><span class="No-Break">prepared data:</span><pre class="source-code">
X_train = np.concatenate([X_train_quali,</pre><pre class="source-code">
    X_train_quanti, X_train_hash], 1)</pre><pre class="source-code">
X_test = np.concatenate([X_test_quali,</pre><pre class="source-code">
    X_test_quanti, X_test_hash], 1)</pre></li>
<li>Instantiate and train the logistic regression model. Here, we will use the default hyperparameters proposed by <strong class="source-inline">scikit-learn</strong> for <span class="No-Break">logistic regression:</span><pre class="source-code">
lr = LogisticRegression()</pre><pre class="source-code">
lr.fit(X_train, y_train)</pre></li>
<li>Print the accuracy for both the training and test sets using the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">score()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
print('Accuracy train set:', lr.score(X_train,</pre><pre class="source-code">
    y_train))</pre><pre class="source-code">
print('Accuracy test set:', lr.score(X_test, y_test))</pre></li>
</ol>
<p>The output will look something <span class="No-Break">like this:</span></p>
<pre class="source-code">
<strong class="bold">Accuracy train set: 0.7812087988342239</strong>
<strong class="bold">Accuracy test set: 0.7826810990840966</strong></pre>
<p>As we can see, we have <a id="_idIndexMarker252"/>an<a id="_idIndexMarker253"/> accuracy of about 78% on the test set, with no <span class="No-Break">apparent overfitting.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">It is possible that adding some features (for example, with feature engineering) could help improve the model since the model in itself seems to have no room for much improvement based on the fact there is <span class="No-Break">no overfitting.</span></p>
<h2 id="_idParaDest-142"><a id="_idTextAnchor142"/>See also</h2>
<ul>
<li>The official documentation of the category encoders <span class="No-Break">library: </span><a href="https://contrib.scikit-learn.org/category_encoders/"><span class="No-Break">https://contrib.scikit-learn.org/category_encoders/</span></a></li>
<li>The category encoders page about <span class="No-Break">hashing: </span><a href="https://contrib.scikit-learn.org/category_encoders/hashing.xhtml"><span class="No-Break">https://contrib.scikit-learn.org/category_encoders/hashing.xhtml</span></a></li>
</ul>
<h1 id="_idParaDest-143"><a id="_idTextAnchor143"/>Aggregating features</h1>
<p>When you’re <a id="_idIndexMarker254"/>looking at high cardinality features, one possible solution is to reduce the actual cardinality of that feature. Here, aggregating is one possible solution, and it may work very well in some cases. In this recipe, we will explain what aggregating is and discuss when we should use it. Once we’ve done that, we will <span class="No-Break">apply it.</span></p>
<h2 id="_idParaDest-144"><a id="_idTextAnchor144"/>Getting ready</h2>
<p>When dealing with high cardinality features, one-hot encoding leads to high-dimensionality datasets. Because of the so-called curse of dimensionality, the ability for models to generalize properly can be a real issue for one-hot encoded high cardinality features, even with very large training datasets. Thus, aggregating is a way to lower the dimensionality of the one-hot encoding, and then lower the risk <span class="No-Break">of overfitting.</span></p>
<p>There are several ways to aggregate. Let’s, for example, assume that we have a database of clients that contains the “phone model” feature, which consists of many of the possible phone models (that is, hundreds). There could be at least two ways of aggregating such <span class="No-Break">a feature:</span></p>
<ul>
<li><strong class="bold">By occurrence probability</strong>: Any model appearing less than X% in the data is considered <span class="No-Break">as “others”</span></li>
<li><strong class="bold">By a given similarity</strong>: We could gather models by generation, brand, or <span class="No-Break">even price</span></li>
</ul>
<p>These methods have their pros <span class="No-Break">and cons:</span></p>
<ul>
<li><strong class="bold">Pros</strong>: Aggregating by occurrence is simple, works all the time, and does not require any subject <span class="No-Break">matter knowledge</span></li>
<li><strong class="bold">Cons</strong>: Aggregating by a given similarity can be more relevant but requires knowledge about the feature that may not be available, or it could take too long (for example, if there are millions <span class="No-Break">of values)</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">Aggregating is also sometimes useful when there is a long tail distribution of values for a feature, which means that some values appear a lot, while many others appear only a small fraction of <span class="No-Break">the time.</span></p>
<p>In this recipe, we will apply aggregating to a dataset that contains many cities as features but with no information about the city names. This will leave us with the only option being to aggregate by occurrence. We will reuse the same dataset as in the previous recipe, so we will require the Kaggle API. For that, please refer to the previous recipe. Using the Kaggle API, the dataset can be downloaded with the <span class="No-Break">following command:</span></p>
<pre class="source-code">
kaggle datasets download -d reddynitin/aug-train</pre>
<p>We will also need the <strong class="source-inline">pandas</strong> and <strong class="source-inline">scikit-learn</strong> libraries, which can be installed with the <span class="No-Break">following command:</span></p>
<pre class="source-code">
pip install pandas scikit-learn.</pre>
<h2 id="_idParaDest-145"><a id="_idTextAnchor145"/>How to do it...</h2>
<p>We will use the same <a id="_idIndexMarker255"/>dataset as in the previous recipe. To prepare for this recipe, we will aggregate the cities based on a given threshold on their occurrences in the dataset, and then train and evaluate a model on <span class="No-Break">this data:</span></p>
<ol>
<li>Import the required modules, functions, and classes: <strong class="source-inline">pandas</strong> for loading the data, <strong class="source-inline">train_test_split</strong> for splitting the data, <strong class="source-inline">StandardScaler</strong> for rescaling quantitative features, <strong class="source-inline">OneHotEncoder</strong> for encoding qualitative features, and <strong class="source-inline">LogisticRegression</strong> as <span class="No-Break">the model:</span><pre class="source-code">
Import numpy as np</pre><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.preprocessing import OneHotEncoder, StandardScaler</pre><pre class="source-code">
from sklearn.linear_model import LogisticRegression</pre></li>
<li>Load the dataset with <strong class="source-inline">pandas</strong>. There’s no need to unzip the file first – this is all handled <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">pandas</strong></span><span class="No-Break">:</span><pre class="source-code">
df = pd.read_csv('aug-train.zip')</pre></li>
<li>Remove any missing data. As we did in the previous recipe, we will use a simple policy, remove all features that have a large amount of missing data, and then remove the rows with <span class="No-Break">missing data:</span><pre class="source-code">
df = df.drop(columns=['gender', 'major_discipline',</pre><pre class="source-code">
    'company_size', 'company_type'])</pre><pre class="source-code">
df = df.dropna()</pre></li>
<li>Split the data <a id="_idIndexMarker256"/>into training and test sets with the <span class="No-Break"><strong class="source-inline">train_test_split</strong></span><span class="No-Break"> function:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    df.drop(columns=['target']), df['target'],</pre><pre class="source-code">
    stratify=df['target'], test_size=0.2,</pre><pre class="source-code">
    random_state=0</pre><pre class="source-code">
)</pre></li>
<li>Rescale any quantitative features with the standard scaler provided <span class="No-Break">by </span><span class="No-Break"><strong class="source-inline">scikit-learn</strong></span><span class="No-Break">:</span><pre class="source-code">
quanti_feats = ['city_development_index',</pre><pre class="source-code">
    'training_hours']</pre><pre class="source-code">
scaler = StandardScaler()</pre><pre class="source-code">
X_train_quanti = X_train[quanti_feats]</pre><pre class="source-code">
X_test_quanti = X_test[quanti_feats]</pre><pre class="source-code">
X_train_quanti = scaler.fit_transform(X_train_quanti)</pre><pre class="source-code">
X_test_quanti = scaler.transform(X_test_quanti)</pre></li>
<li>Now, we must aggregate the <span class="No-Break"><strong class="source-inline">city</strong></span><span class="No-Break"> feature:</span><pre class="source-code">
# Get only cities above threshold</pre><pre class="source-code">
threshold = 0.1</pre><pre class="source-code">
kept_cities = X_train['city'].value_counts(</pre><pre class="source-code">
    normalize=True)[X_train['city'].value_counts(</pre><pre class="source-code">
    normalize=True) &gt; threshold].index</pre><pre class="source-code">
# Update all cities below threshold as 'other'</pre><pre class="source-code">
X_train.loc[~X_train['city'].isin(kept_cities),</pre><pre class="source-code">
    'city'] = 'other'</pre><pre class="source-code">
X_test.loc[~X_test['city'].isin(kept_cities),</pre><pre class="source-code">
    'city'] = 'other'</pre></li>
<li>Prepare the <a id="_idIndexMarker257"/>qualitative features with one-hot encoding, including the newly aggregated <span class="No-Break"><strong class="source-inline">city</strong></span><span class="No-Break"> feature:</span><pre class="source-code">
# Get qualitative features</pre><pre class="source-code">
quali_feats = ['city', 'relevent_experience',</pre><pre class="source-code">
    'enrolled_university', 'education_level',</pre><pre class="source-code">
    'experience', 'last_new_job']</pre><pre class="source-code">
X_train_quali = X_train[quali_feats]</pre><pre class="source-code">
X_test_quali = X_test[quali_feats]</pre><pre class="source-code">
# Instantiate the one hot encoder</pre><pre class="source-code">
encoder = OneHotEncoder()</pre><pre class="source-code">
# Apply one hot encoding</pre><pre class="source-code">
X_train_quali = encoder.fit_transform(</pre><pre class="source-code">
    X_train_quali).toarray()</pre><pre class="source-code">
X_test_quali = encoder.transform(</pre><pre class="source-code">
    X_test_quali).toarray()</pre></li>
<li>Concatenate the quantitative and qualitative features <span class="No-Break">back together:</span><pre class="source-code">
X_train = np.concatenate([X_train_quali,</pre><pre class="source-code">
    X_train_quanti], 1)</pre><pre class="source-code">
X_test = np.concatenate([X_test_quali, X_test_quanti], 1)</pre></li>
<li>Instantiate<a id="_idIndexMarker258"/> and train the logistic regression model. Here, we will just keep the default hyperparameters of <span class="No-Break">the model:</span><pre class="source-code">
lr = LogisticRegression()</pre><pre class="source-code">
lr.fit(X_train, y_train)</pre></li>
<li>Compute and print the model’s accuracy on both the training and <span class="No-Break">test sets:</span><pre class="source-code">
print('Accuracy train set:', lr.score(X_train, y_train))</pre><pre class="source-code">
print('Accuracy test set:', lr.score(X_test, y_test))</pre></li>
</ol>
<p>The output will look <span class="No-Break">like this:</span></p>
<pre class="source-code">
<strong class="bold">Accuracy train set: 0.7805842759003538</strong>
<strong class="bold">Accuracy test set: 0.774909797391063</strong></pre>
<p class="callout-heading">Note</p>
<p class="callout">For this specific case, aggregating did not seem to help much in terms of giving us more robust results, but at the very least it has helped the model be less unpredictable and robust to <span class="No-Break">new cities.</span></p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor146"/>There’s more...</h2>
<p>Since the aggregation code may have looked complicated, let’s take a look at what <span class="No-Break">we did.</span></p>
<p>So, we have the <strong class="source-inline">city</strong> feature, which has many possible values; there’s a frequency for each value in the training set. These can be computed with the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">value_counts(normalize=True)</strong></span><span class="No-Break"> method:</span></p>
<pre class="source-code">
df['city'].value_counts(normalize=True)</pre>
<p>This will give us the <span class="No-Break">following output:</span></p>
<pre class="source-code">
city_103         0.232819
city_21           0.136227
city_16           0.081659
city_114         0.069613
city_160         0.045354
                                  ...
city_111         0.000167
city_129         0.000111
city_8              0.000111
city_140         0.000056
city_171         0.000056
Name: city, Length: 123, dtype: float64</pre>
<p>It appears that, in the entire dataset, <strong class="source-inline">city_103</strong> is the value more than 23% of the time, while other <a id="_idIndexMarker259"/>values such as <strong class="source-inline">city_111</strong> appear less than 1% of the time. We will just apply a threshold to those values so that we get the list of cities appearing more than the <span class="No-Break">given threshold:</span></p>
<pre class="source-code">
df['city'].value_counts(normalize=True) &gt; 0.05</pre>
<p>This will give us the <span class="No-Break">following output:</span></p>
<pre class="source-code">
city_103           True
city_21              True
city_16              True
city_114           True
city_160         False
                             ...
city_111         False
city_129         False
city_8              False
city_140         False
city_171         False
Name: city, Length: 123, dtype: bool</pre>
<p>Now, all we have to do is get the index (that is, the city name) of all the true values. This is exactly what we <a id="_idIndexMarker260"/>can do with the following <span class="No-Break">full line:</span></p>
<pre class="source-code">
kept_cities = df['city'].value_counts(normalize=True)[
    df['city'].value_counts(normalize=True) &gt; 0.05].index
kept_cities</pre>
<p>This shows the <span class="No-Break">following output:</span></p>
<pre class="source-code">
Index(['city_103', 'city_21', 'city_16', 'city_114'], dtype='object')</pre>
<p>As expected, this returns list of cities that occur more than <span class="No-Break">the threshold.</span></p>
<h1 id="_idParaDest-147"><a id="_idTextAnchor147"/>Undersampling an imbalanced dataset</h1>
<p>A typical case in<a id="_idIndexMarker261"/> machine learning is what we call an imbalanced dataset. An imbalanced dataset simply means that for a given class, some occurrences are much more likely than others, hence the lack of balance. There are plenty of cases of imbalanced datasets: rare diseases in medicine, customer behavior, <span class="No-Break">and more.</span></p>
<p>In this recipe, we will propose one possible way to handle imbalanced datasets: undersampling. After explaining this process, we will apply it to a credit card fraud <span class="No-Break">detection dataset.</span></p>
<h2 id="_idParaDest-148"><a id="_idTextAnchor148"/>Getting ready</h2>
<p>The problem with imbalanced data is that it may bias the results of a machine learning model. Let’s assume we’re undertaking a classification task of detecting rare diseases present in only 1% of a dataset. A common pitfall with such data is to have a model predicting as always healthy as it would still have 99% accuracy. So, it would be very likely for a machine learning model to minimize <span class="No-Break">its losses.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">In such situations, other metrics such as the F1-score or <strong class="bold">ROC Area Under Curve</strong> (<strong class="bold">ROC AUC</strong>) are usually <span class="No-Break">more relevant.</span></p>
<p>One way to prevent this from happening is to undersample the dataset. More specifically, we can undersample the overrepresented class by removing some samples <span class="No-Break">of it:</span></p>
<ul>
<li>We keep all the samples of the <span class="No-Break">underrepresented class</span></li>
<li>We keep only a subsample of the <span class="No-Break">overrepresented class</span></li>
</ul>
<p>By doing this, we can artificially balance the dataset and avoid the pitfalls of an imbalanced dataset. For example, let’s say we have a dataset composed of the <span class="No-Break">following attributes:</span></p>
<ul>
<li>100 samples <span class="No-Break">with disease</span></li>
<li>9,900 samples with <span class="No-Break">no disease</span></li>
</ul>
<p>A perfectly balanced undersampling would give us the following results in <span class="No-Break">the dataset:</span></p>
<ul>
<li>100 samples <span class="No-Break">with disease</span></li>
<li>100 randomly selected samples with <span class="No-Break">no disease</span></li>
</ul>
<p>Of course, the drawback is that we lose a lot of data in <span class="No-Break">the process.</span></p>
<p>For this recipe, we first need to download the dataset. To do so, we will use the Kaggle API (refer to the <em class="italic">Hashing high cardinality features</em> recipe to learn how to install it). The dataset can be downloaded with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
kaggle datasets download -d mlg-ulb/creditcardfraud</pre>
<p>The following libraries are also needed: <strong class="source-inline">pandas</strong> for loading the data, <strong class="source-inline">scikit-learn</strong> for modeling, <strong class="source-inline">matplotlib</strong> for displaying the data, and <strong class="source-inline">imbalanced-learn</strong> for undersampling. They can be installed with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install pandas scikit-learn matplotlib imbalanced-learn</pre>
<h2 id="_idParaDest-149"><a id="_idTextAnchor149"/>How to do it...</h2>
<p>In this recipe, we will<a id="_idIndexMarker262"/> apply undersampling to a credit card fraud dataset. This is a rather extreme case of an imbalanced dataset since only about 0.18% of the samples <span class="No-Break">are positive:</span></p>
<ol>
<li>Import the required modules, classes, <span class="No-Break">and functions:</span><ul><li><strong class="source-inline">pandas</strong> for data loading <span class="No-Break">and manipulation</span></li><li><strong class="source-inline">train_test_split</strong> for <span class="No-Break">data splitting</span></li><li><strong class="source-inline">StandardScaler</strong> for data rescaling (the dataset holds only <span class="No-Break">quantitative features)</span></li><li><strong class="source-inline">RandomUnderSampler</strong> <span class="No-Break">for undersampling</span></li><li><strong class="source-inline">LogisticRegression</strong> <span class="No-Break">for modeling</span></li><li><strong class="source-inline">roc_auc_score</strong> for displaying the ROC and ROC <span class="No-Break">AUC computations:</span><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.preprocessing import StandardScaler</pre><pre class="source-code">
from imblearn.under_sampling import RandomUnderSampler</pre><pre class="source-code">
from sklearn.linear_model import LogisticRegression</pre><pre class="source-code">
from sklearn.metrics import roc_auc_score</pre></li></ul></li>
<li>Load the data with <strong class="source-inline">pandas</strong>. We can load the ZIP file directly. We will also display the<a id="_idIndexMarker263"/> relative amount of each label: we have around 99.8% of regular transactions compared to less than 0.18% of <span class="No-Break">fraudulent transactions:</span><pre class="source-code">
df = pd.read_csv('creditcardfraud.zip')</pre><pre class="source-code">
df['Class'].value_counts(normalize=True)</pre></li>
</ol>
<p>The output will look <span class="No-Break">like this:</span></p>
<pre class="source-code">
<strong class="bold">0         0.998273</strong>
<strong class="bold">1         0.001727</strong>
<strong class="bold">Name: Class, dtype: float64</strong></pre>
<ol>
<li value="3">Split the data into training and test sets. The need to stratify the label in such cases can <span class="No-Break">be critical:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    df.drop(columns=['Class']), df['Class'],</pre><pre class="source-code">
    test_size=0.2, random_state=0,</pre><pre class="source-code">
    stratify=df['Class'])</pre></li>
<li>Apply random undersampling, up to a 10% sampling strategy. This means we must undersample the overrepresented class until there is a 10 to 1 ratio in the class balance. We could go up to a 1 to 1 ratio, but this would be at the cost of even more dropped data. This ratio is defined by the <strong class="source-inline">sampling_strategy=0.1</strong> parameter. We <a id="_idIndexMarker264"/>must also set the random state <span class="No-Break">for reproducibility:</span><pre class="source-code">
# Instantiate the object with a 10% strategy</pre><pre class="source-code">
rus = RandomUnderSampler(sampling_strategy=0.1,</pre><pre class="source-code">
    random_state=0)</pre><pre class="source-code">
# Undersample the train dataset</pre><pre class="source-code">
X_train, y_train = rus.fit_resample(X_train, y_train)</pre><pre class="source-code">
# Check the balance</pre><pre class="source-code">
y_train.value_counts()</pre></li>
</ol>
<p>This gives us the <span class="No-Break">following output:</span></p>
<pre class="source-code">
<strong class="bold">0         3940</strong>
<strong class="bold">1           394</strong>
<strong class="bold">Name: Class, dtype: int64</strong></pre>
<p>After undersampling, we end up with <strong class="source-inline">3940</strong> regular transaction samples compared to <strong class="source-inline">394</strong> <span class="No-Break">fraudulent transactions.</span></p>
<ol>
<li value="5">Rescale the data using a <span class="No-Break">standard scaler:</span><pre class="source-code">
# Scale the data</pre><pre class="source-code">
scaler = StandardScaler()</pre><pre class="source-code">
X_train = scaler.fit_transform(X_train)</pre><pre class="source-code">
X_test = scaler.transform(X_test)</pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Arguably, we could apply rescaling before resampling. This would give more weight to the overrepresented class when rescaling but would not be considered <span class="No-Break">data leakage.</span></p>
<ol>
<li value="6">Instantiate <a id="_idIndexMarker265"/>and train the logistic regression model on the <span class="No-Break">training set:</span><pre class="source-code">
lr = LogisticRegression()</pre><pre class="source-code">
lr.fit(X_train, y_train)</pre></li>
<li>Compute the ROC AUC on both the training and test sets. To do so, we need the predicted probabilities for each sample, which we can get with the <strong class="source-inline">predict_proba()</strong> method, as well as the imported <span class="No-Break"><strong class="source-inline">roc_auc_score()</strong></span><span class="No-Break"> function:</span><pre class="source-code">
# Get the probas</pre><pre class="source-code">
y_train_proba = lr.predict_proba(X_train)[:, 1]</pre><pre class="source-code">
y_test_proba = lr.predict_proba(X_test)[:, 1]</pre><pre class="source-code">
# Display the ROC AUC</pre><pre class="source-code">
print('ROC AUC training set:', roc_auc_score(y_train,</pre><pre class="source-code">
    y_train_proba))</pre><pre class="source-code">
print('ROC AUC test set:', roc_auc_score(y_test,</pre><pre class="source-code">
    y_test_proba))</pre></li>
</ol>
<p>This returns <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">ROC AUC training set: 0.9875041871730784</strong>
<strong class="bold">ROC AUC test set: 0.9731067071595099</strong></pre>
<p>We got a ROC AUC of <a id="_idIndexMarker266"/>about 97% on the test set and close to 99% on the <span class="No-Break">training set.</span></p>
<h2 id="_idParaDest-150"><a id="_idTextAnchor150"/>There’s more...</h2>
<p>Optionally, we can<a id="_idIndexMarker267"/> plot the ROC curves for both the training and test sets. To do so, we can use the <strong class="source-inline">roc_curve()</strong> function <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">scikit-learn</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
# Display the ROC curve
fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)
fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)
plt.plot(fpr_test, tpr_test, label='test')
plt.plot(fpr_train, tpr_train, label='train')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.legend()
plt.show()</pre>
<div>
<div class="IMG---Figure" id="_idContainer141">
<img alt="Figure 5.2 – ROC curve for the train and test sets. Plot produced by the code" height="425" src="image/B19629_05_02.jpg" width="560"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – ROC curve for the train and test sets. Plot produced by the code</p>
<p>As we can see, while <a id="_idIndexMarker268"/>the ROC AUC is very similar for the training and test sets, the curve for the test set is a bit lower. This means that, as expected, the model is <span class="No-Break">overfitting slightly.</span></p>
<p>Note that fine-tuning <strong class="source-inline">sampling_strategy</strong> might be helpful to get <span class="No-Break">better results.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">To optimize the sampling strategy and the model hyperparameters at the same time, you can use scikit-learn’s <span class="No-Break"><strong class="source-inline">Pipeline</strong></span><span class="No-Break"> class.</span></p>
<h2 id="_idParaDest-151"><a id="_idTextAnchor151"/>See also</h2>
<ul>
<li>The documentation for <span class="No-Break"><strong class="source-inline">RandomUnderSampler</strong></span><span class="No-Break">: </span><a href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml"><span class="No-Break">https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml</span></a></li>
<li>The documentation for <span class="No-Break"><strong class="source-inline">Pipeline</strong></span><span class="No-Break">: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.xhtml</span></a></li>
<li>Here’s a great code example of a two-step pipeline hyperparameter <span class="No-Break">optimization: </span><a href="https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.xhtml%0D"><span class="No-Break">https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.xhtml</span></a></li>
</ul>
<h1 id="_idParaDest-152"><a id="_idTextAnchor152"/>Oversampling an imbalanced dataset</h1>
<p>Another solution <a id="_idIndexMarker269"/>when dealing with imbalanced datasets is random oversampling. This is the opposite of random undersampling. In this recipe, we’ll learn how to use it on the credit card fraud <span class="No-Break">detection dataset.</span></p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor153"/>Getting ready</h2>
<p>Random oversampling can be seen as the opposite of random undersampling: the idea is to duplicate samples of the underrepresented dataset to rebalance <span class="No-Break">the dataset.</span></p>
<p>As for the previous recipe, let’s assume a 1%-99% imbalanced dataset that contains <span class="No-Break">the following:</span></p>
<ul>
<li>100 samples <span class="No-Break">with disease</span></li>
<li>9,900 samples with <span class="No-Break">no disease</span></li>
</ul>
<p>To apply oversampling to this dataset using a 1/1 strategy (so, a perfectly balanced dataset), we would need to have 99 duplicates of each sample of the disease class. So, the oversampled dataset would need to contain <span class="No-Break">the following:</span></p>
<ul>
<li>9,900 samples with disease (100 original samples duplicated 99 times <span class="No-Break">on average)</span></li>
<li>9,900 samples with <span class="No-Break">no disease</span></li>
</ul>
<p>We can easily guess the pros and cons of <span class="No-Break">this method:</span></p>
<ul>
<li><strong class="bold">Pro</strong>: Unlike undersampling, we <a id="_idIndexMarker270"/>do not waste any data from the overrepresented class, which means our model can be trained on the full picture of the data <span class="No-Break">we have</span></li>
<li><strong class="bold">Con</strong>: We may have a lot <a id="_idIndexMarker271"/>of duplicates of the underrepresented class, leading to potential overfitting on <span class="No-Break">this data</span></li>
</ul>
<p>Fortunately, we can choose a rebalancing strategy below 1/1 so that the underrepresented data duplication can <span class="No-Break">be limited.</span></p>
<p>For this recipe, we need to download the dataset. If you completed the <em class="italic">Undersampling an imbalanced dataset</em> recipe, you don’t need to do <span class="No-Break">anything else.</span></p>
<p>Otherwise, using the Kaggle API (refer to the <em class="italic">Hashing high cardinality features</em> recipe to learn how<a id="_idIndexMarker272"/> to install it), we need to download the dataset via the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
kaggle datasets download -d mlg-ulb/creditcardfraud</pre>
<p>The following libraries are also needed: <strong class="source-inline">pandas</strong> for loading the data, <strong class="source-inline">scikit-learn</strong> for modeling, <strong class="source-inline">matplotlib</strong> for displaying the data, and <strong class="source-inline">imbalanced-learn</strong> for the oversampling part. They can be installed via the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install pandas scikit-learn matplotlib imbalanced-learn.</pre>
<h2 id="_idParaDest-154"><a id="_idTextAnchor154"/>How to do it...</h2>
<p>In this recipe, we will apply oversampling to the credit card <span class="No-Break">fraud dataset:</span></p>
<ol>
<li>Import the required modules, classes, <span class="No-Break">and functions:</span><ul><li><strong class="source-inline">pandas</strong> for data loading <span class="No-Break">and manipulation</span></li><li><strong class="source-inline">train_test_split</strong> for <span class="No-Break">data splitting</span></li><li><strong class="source-inline">StandardScaler</strong> for data rescaling (the dataset only contains <span class="No-Break">quantitative features)</span></li><li><strong class="source-inline">RandomOverSampler</strong> <span class="No-Break">for oversampling</span></li><li><strong class="source-inline">LogisticRegression</strong> <span class="No-Break">for modeling</span></li><li><strong class="source-inline">roc_auc_score</strong> for displaying the ROC and ROC <span class="No-Break">AUC computations:</span><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.preprocessing import StandardScaler</pre><pre class="source-code">
from imblearn.over_sampling import RandomOverSampler</pre><pre class="source-code">
from sklearn.linear_model import LogisticRegression</pre><pre class="source-code">
from sklearn.metrics import roc_auc_score</pre></li></ul></li>
<li>Load the data with pandas. We can load the ZIP file directly. As we did in the previous recipe, we <a id="_idIndexMarker273"/>will display the relative amount of each label to remind us that we have about 99.8% of regular transactions and less than 0.18% of <span class="No-Break">fraudulent transactions:</span><pre class="source-code">
df = pd.read_csv('creditcardfraud.zip')</pre><pre class="source-code">
df['Class'].value_counts(normalize=True)</pre></li>
</ol>
<p>This prints <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">0         0.998273</strong>
<strong class="bold">1         0.001727</strong>
<strong class="bold">Name: Class, dtype: float64</strong></pre>
<ol>
<li value="3">Split the data into training and test sets. We must specify stratification on the labels to make sure the balance is still <span class="No-Break">the same:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    df.drop(columns=['Class']), df['Class'],</pre><pre class="source-code">
    test_size=0.2, random_state=0,</pre><pre class="source-code">
    stratify=df['Class'])</pre></li>
<li>Apply random oversampling with a 10% sampling strategy. This means that we oversample the underrepresented class until there is a 10 to 1 ratio in the class balance. This ratio is defined by the <strong class="source-inline">sampling_strategy=0.1</strong> parameter. We must also<a id="_idIndexMarker274"/> set the random state <span class="No-Break">for reproducibility:</span><pre class="source-code">
# Instantiate the oversampler with a 10% strategy</pre><pre class="source-code">
ros = RandomOverSampler(sampling_strategy=0.1,</pre><pre class="source-code">
    random_state=0)</pre><pre class="source-code">
# Overersample the train dataset</pre><pre class="source-code">
X_train, y_train = ros.fit_resample(X_train, y_train)</pre><pre class="source-code">
# Check the balance</pre><pre class="source-code">
y_train.value_counts()</pre></li>
</ol>
<p>This outputs <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">0         227451</strong>
<strong class="bold">1           22745</strong>
<strong class="bold">Name: Class, dtype: int64</strong></pre>
<p>After oversampling, we now have <strong class="source-inline">227451</strong> regular transactions in the training set (which is unchanged) versus <strong class="source-inline">22745</strong> <span class="No-Break">fraudulent transactions.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">It is possible to change the sampling strategy. As usual, it is a matter of doing a tradeoff: a greater sampling strategy means more duplicated samples for more balance, while a smaller sampling strategy means fewer duplicated samples but <span class="No-Break">less balance.</span></p>
<ol>
<li value="5">Rescale the data using a <span class="No-Break">standard scaler:</span><pre class="source-code">
# Scale the data</pre><pre class="source-code">
scaler = StandardScaler()</pre><pre class="source-code">
X_train = scaler.fit_transform(X_train)</pre><pre class="source-code">
X_test = scaler.transform(X_test)</pre></li>
<li>Instantiate and<a id="_idIndexMarker275"/> train the logistic regression model on the <span class="No-Break">training set:</span><pre class="source-code">
lr = LogisticRegression()</pre><pre class="source-code">
lr.fit(X_train, y_train)</pre></li>
<li>Compute the ROC AUC on both the training and test sets. To do so, we need the predicted probabilities for each sample, which we can get by using the <strong class="source-inline">predict_proba()</strong> method, as well as the imported <span class="No-Break"><strong class="source-inline">roc_auc_score()</strong></span><span class="No-Break"> function:</span><pre class="source-code">
# Get the probas</pre><pre class="source-code">
y_train_proba = lr.predict_proba(X_train)[:, 1]</pre><pre class="source-code">
y_test_proba = lr.predict_proba(X_test)[:, 1]</pre><pre class="source-code">
# Display the ROC AUC</pre><pre class="source-code">
print('ROC AUC training set:', roc_auc_score(y_train,</pre><pre class="source-code">
     y_train_proba))</pre><pre class="source-code">
print('ROC AUC test set:', roc_auc_score(y_test,</pre><pre class="source-code">
    y_test_proba))</pre></li>
</ol>
<p>This returns <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">ROC AUC training set: 0.9884952360756659</strong>
<strong class="bold">ROC AUC test set: 0.9721115830969416</strong></pre>
<p>The results are quite<a id="_idIndexMarker276"/> comparable to the ones we obtained with undersampling. However, this does not mean that these two techniques are <span class="No-Break">always equal.</span></p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor155"/>There’s more...</h2>
<p>Optionally, just <a id="_idIndexMarker277"/>like we did in the <em class="italic">Undersampling an imbalanced dataset</em> recipe, we can plot the ROC curves for both the training and test sets using the <strong class="source-inline">roc_curve()</strong> function <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">scikit-learn</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
# Display the ROC curve
fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)
fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)
plt.plot(fpr_test, tpr_test, label='test')
plt.plot(fpr_train, tpr_train, label='train')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.legend()
plt.show()</pre>
<div>
<div class="IMG---Figure" id="_idContainer142">
<img alt="Figure 5.3 – ROC curve for the train and test sets. Plot produced by the code" height="425" src="image/B19629_05_03.jpg" width="560"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – ROC curve for the train and test sets. Plot produced by the code</p>
<p>In this case, the ROC<a id="_idIndexMarker278"/> AUC curve for the test set is clearly below the one for the training set, which means that the model is <span class="No-Break">overfitting slightly.</span></p>
<h2 id="_idParaDest-156"><a id="_idTextAnchor156"/>See also</h2>
<p>The documentation for <strong class="source-inline">RandomUnderSampler</strong> is available <span class="No-Break">at</span><span class="No-Break"> </span><a href="https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml"><span class="No-Break">https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-157"><a id="_idTextAnchor157"/>Resampling imbalanced data with SMOTE</h1>
<p>Finally, a more <a id="_idIndexMarker279"/>complex solution for dealing with <a id="_idIndexMarker280"/>imbalanced datasets is a method called SMOTE. After explaining the SMOTE algorithm, we will apply this method to the credit card fraud <span class="No-Break">detection dataset.</span></p>
<h2 id="_idParaDest-158"><a id="_idTextAnchor158"/>Getting ready</h2>
<p><strong class="bold">SMOTE</strong> stands for <strong class="bold">Synthetic Minority Oversampling TEchnique</strong>. As its name suggests, it<a id="_idIndexMarker281"/> creates synthetic samples for an underrepresented class. But how exactly does it create <span class="No-Break">synthetic data?</span></p>
<p>This method uses the k-NN algorithm on the underrepresented class. The SMOTE algorithm can be summarized with the <span class="No-Break">following steps:</span></p>
<ol>
<li>Randomly pick a sample, <img alt="" height="18" src="image/F_05_001.png" width="19"/>, in the <span class="No-Break">minority class.</span></li>
<li>Using k-NN, randomly pick one of the k-nearest neighbors of <img alt="" height="19" src="image/F_05_002.png" width="19"/> in the minority class. Let’s call this <span class="No-Break">sample <img alt="" height="18" src="image/F_05_003.png" width="27"/>.</span></li>
<li>Compute the new synthetic sample, <img alt="" height="24" src="image/F_05_004.png" width="221"/>, with 𝜆 being randomly drawn in the [0, <span class="No-Break">1] range:</span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer147">
<img alt="Figure 5.4 – Visual representation of SMOTE" height="662" src="image/B19629_05_04.jpg" width="662"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – Visual representation of SMOTE</p>
<p>Compared to <a id="_idIndexMarker282"/>random<a id="_idIndexMarker283"/> oversampling, this method is more complex since it has one hyperparameter: the number of nearest neighbors, <em class="italic">k</em>, to consider. This method also<a id="_idIndexMarker284"/> comes with pros <span class="No-Break">and cons:</span></p>
<ul>
<li><strong class="bold">Pro</strong>: Unlike random oversampling, it limits the risks of overfitting on the underrepresented class since samples are <span class="No-Break">not duplicated</span></li>
<li><strong class="bold">Con</strong>: Creating synthetic data is a risky bet; nothing assures you that it has a meaning and would ever be likely on <span class="No-Break">real data</span></li>
</ul>
<p>To complete this recipe, you will need to download the credit card fraud dataset if you haven’t done so already (check out the <em class="italic">Undersampling an imbalanced dataset</em> or <em class="italic">Oversampling an imbalanced dataset</em> recipe to learn how to <span class="No-Break">do this).</span></p>
<p>Using the Kaggle API (refer to the <em class="italic">Hashing high cardinality features</em> recipe to learn how to install it), we have to download the dataset via the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
kaggle datasets download -d mlg-ulb/creditcardfraud</pre>
<p>The following libraries are needed: <strong class="source-inline">pandas</strong> for loading the data, <strong class="source-inline">scikit-learn</strong> for modeling, <strong class="source-inline">matplotlib</strong> for displaying the data, and <strong class="source-inline">imbalanced-learn</strong> for undersampling. They can be installed via the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install pandas scikit-learn matplotlib imbalanced-learn.</pre>
<h2 id="_idParaDest-159"><a id="_idTextAnchor159"/>How to do it...</h2>
<p>In this recipe, we<a id="_idIndexMarker285"/> will<a id="_idIndexMarker286"/> apply SMOTE to the credit card <span class="No-Break">fraud dataset:</span></p>
<ol>
<li>Import the required modules, classes, <span class="No-Break">and functions:</span><ul><li><strong class="source-inline">pandas</strong> for data loading <span class="No-Break">and manipulation</span></li><li><strong class="source-inline">train_test_split</strong> for <span class="No-Break">data splitting</span></li><li><strong class="source-inline">StandardScaler</strong> for data rescaling (the dataset contains only <span class="No-Break">quantitative features)</span></li><li><strong class="source-inline">SMOTE</strong> for the <span class="No-Break">SMOTE oversampling</span></li><li><strong class="source-inline">LogisticRegression</strong> <span class="No-Break">for modeling</span></li><li><strong class="source-inline">roc_auc_score</strong> for displaying the ROC and ROC <span class="No-Break">AUC computations:</span><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.preprocessing import StandardScaler</pre><pre class="source-code">
from imblearn.over_sampling import SMOTE</pre><pre class="source-code">
from sklearn.linear_model import LogisticRegression</pre><pre class="source-code">
from sklearn.metrics import roc_auc_score</pre></li></ul></li>
<li>Load the data with <strong class="source-inline">pandas</strong>. We can load the ZIP file directly. As we did in the previous two recipes, we will display the relative amount of each label. Again, we will have about 99.8% of regular transactions and less than 0.18% of <span class="No-Break">fraudulent transactions:</span><pre class="source-code">
df = pd.read_csv('creditcardfraud.zip')</pre><pre class="source-code">
df['Class'].value_counts(normalize=True)</pre></li>
</ol>
<p>The output will look <span class="No-Break">like this:</span></p>
<pre class="source-code">
<strong class="bold">0         0.998273</strong>
<strong class="bold">1         0.001727</strong>
<strong class="bold">Name: Class, dtype: float64</strong></pre>
<ol>
<li value="3">Split the data<a id="_idIndexMarker287"/> into<a id="_idIndexMarker288"/> training and test sets. We must specify stratification on the labels to make sure the balance is still <span class="No-Break">the same:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    df.drop(columns=['Class']), df['Class'],</pre><pre class="source-code">
    test_size=0.2, random_state=0,</pre><pre class="source-code">
    stratify=df['Class'])</pre></li>
<li>Apply SMOTE with a 10% sampling strategy with the <strong class="source-inline">sampling_strategy=0.1</strong> parameter. By doing this, we will generate synthetic data of the underrepresented class until there is a 10 to 1 ratio in the class balance. We must also set the random state <span class="No-Break">for reproducibility:</span><pre class="source-code">
# Instantiate the SLOT with a 10% strategy</pre><pre class="source-code">
smote = SMOTE(sampling_strategy=0.1, random_state=0)</pre><pre class="source-code">
# Overersample the train dataset</pre><pre class="source-code">
X_train, y_train = smote.fit_resample(X_train,</pre><pre class="source-code">
    y_train)</pre><pre class="source-code">
# Check the balance</pre><pre class="source-code">
y_train.value_counts()</pre></li>
</ol>
<p>With this, we will get the <span class="No-Break">following output:</span></p>
<pre class="source-code">
<strong class="bold">0         227451</strong>
<strong class="bold">1           22745</strong>
<strong class="bold">Name: Class, dtype: int64</strong></pre>
<p>After oversampling, we now have <strong class="source-inline">227451</strong> regular transactions in the training set (which <a id="_idIndexMarker289"/>is <a id="_idIndexMarker290"/>unchanged) versus <strong class="source-inline">22745</strong> fraudulent transactions, including many synthetically <span class="No-Break">generated samples.</span></p>
<ol>
<li value="5">Rescale the data using a <span class="No-Break">standard scaler:</span><pre class="source-code">
# Scale the data</pre><pre class="source-code">
scaler = StandardScaler()</pre><pre class="source-code">
X_train = scaler.fit_transform(X_train)</pre><pre class="source-code">
X_test = scaler.transform(X_test)</pre></li>
<li>Instantiate and train the logistic regression model on the <span class="No-Break">training set:</span><pre class="source-code">
lr = LogisticRegression()</pre><pre class="source-code">
lr.fit(X_train, y_train)</pre></li>
<li>Compute the ROC AUC on both the training and test sets. To do so, we need the predicted probabilities for each sample, which we can get by using the <strong class="source-inline">predict_proba()</strong> method, as well as the imported <span class="No-Break"><strong class="source-inline">roc_auc_score()</strong></span><span class="No-Break"> function:</span><pre class="source-code">
# Get the probas</pre><pre class="source-code">
y_train_proba = lr.predict_proba(X_train)[:, 1]</pre><pre class="source-code">
y_test_proba = lr.predict_proba(X_test)[:, 1]</pre><pre class="source-code">
# Display the ROC AUC</pre><pre class="source-code">
print('ROC AUC training set:', roc_auc_score(y_train,</pre><pre class="source-code">
    y_train_proba))</pre><pre class="source-code">
print('ROC AUC test set:', roc_auc_score(y_test,</pre><pre class="source-code">
    y_test_proba))</pre></li>
</ol>
<p>Now, the <a id="_idIndexMarker291"/>output <a id="_idIndexMarker292"/>should be <span class="No-Break">as follows:</span></p>
<pre class="source-code">
<strong class="bold">ROC AUC training set: 0.9968657635906649</strong>
<strong class="bold">ROC AUC test set: 0.9711737923925902</strong></pre>
<p>The results are slightly different from the ones we got for random undersampling and oversampling. While the performances on the test set are quite similar, there seems to be more overfitting in this case. There are several possible explanations for such results, with one of them being that the synthetic samples were not very helpful for <span class="No-Break">the model.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The results that we got for the resampling strategies on this dataset are not necessarily representative of the results we would get on any other dataset. Moreover, we had to fine-tune the sampling strategies and models to get a proper <span class="No-Break">performance comparison.</span></p>
<h2 id="_idParaDest-160"><a id="_idTextAnchor160"/>There’s more...</h2>
<p>Optionally, we<a id="_idIndexMarker293"/> can <a id="_idIndexMarker294"/>plot the ROC curves for both the training and test sets using the <strong class="source-inline">roc_curve()</strong> function <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">scikit-learn</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve
# Display the ROC curve
fpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)
fpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)
plt.plot(fpr_test, tpr_test, label='test')
plt.plot(fpr_train, tpr_train, label='train')
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.legend()
plt.show()</pre>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer148">
<img alt="Figure 5.5 – ROC curve for the train and test sets after using SMOTE" height="425" src="image/B19629_05_05.jpg" width="560"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – ROC curve for the train and test sets after using SMOTE</p>
<p>Compared to<a id="_idIndexMarker295"/> random<a id="_idIndexMarker296"/> undersampling and oversampling, overfitting appears to be even <span class="No-Break">clearer here.</span></p>
<h2 id="_idParaDest-161"><a id="_idTextAnchor161"/>See also</h2>
<p>The official documentation for SMOTE can be found <span class="No-Break">at</span><span class="No-Break"> </span><a href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.xhtml"><span class="No-Break">https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.xhtml</span></a><span class="No-Break">.</span></p>
<p>It is not recommended that you apply this implementation to categorical features as it assumes that a feature value for a sample can be any linear combination of values of other samples. This is not true for <span class="No-Break">categorical features.</span></p>
<p>Working implementations for categorical features have also been proposed, including <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">SMOTENC</strong>: For working with datasets that contain both categorical and non-categorical <span class="No-Break">features: </span><a href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.xhtml"><span class="No-Break">https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.xhtml</span></a></li>
<li><strong class="bold">SMOTEN</strong>: For working with datasets that contain only categorical <span class="No-Break">features: </span><a href="https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.xhtml"><span class="No-Break">https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.xhtml</span></a></li>
</ul>
</div>
</div></body></html>