["```py\n    import torch\n    import math\n    import numpy as np\n    from pytorch3d.renderer import (\n        FoVPerspectiveCameras,\n        PointLights,\n        look_at_view_transform,\n        NDCGridRaysampler,\n    )\n    ```", "```py\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        torch.cuda.set_device(device)\n    else:\n        device = torch.device(\"cpu\")\n    ```", "```py\n    num_views: int = 10\n    azimuth_range: float = 180\n    elev = torch.linspace(0, 0, num_views)\n    azim = torch.linspace(-azimuth_range, azimuth_range, num_views) + 180.0\n    lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\n    R, T = look_at_view_transform(dist=2.7, elev=elev, azim=azim)\n    cameras = FoVPerspectiveCameras(device=device, R=R, T=T)\n    ```", "```py\n    image_size = 64\n    volume_extent_world = 3.0\n    raysampler = NDCGridRaysampler(\n        image_width=image_size,\n        image_height=image_size,\n        n_pts_per_ray=50,\n        min_depth=0.1,\n        max_depth=volume_extent_world,\n    )\n    ```", "```py\n    ray_bundle = raysampler(cameras)\n    ```", "```py\n    print('ray_bundle origins tensor shape = ', ray_bundle.origins.shape)\n    print('ray_bundle directions shape = ', ray_bundle.directions.shape)\n    print('ray_bundle lengths = ', ray_bundle.lengths.shape)\n    print('ray_bundle xys shape = ', ray_bundle.xys.shape)\n    ```", "```py\n        ray_bundle origins tensor shape =  torch.Size([10, 64, 64, 3])\n        ray_bundle directions shape =  torch.Size([10, 64, 64, 3])\n        ray_bundle lengths =  torch.Size([10, 64, 64, 50])\n        ray_bundle xys shape =  torch.Size([10, 64, 64, 2])\n        ```", "```py\n    torch.save({\n        'ray_bundle': ray_bundle\n    }, 'ray_sampling.pt')\n    ```", "```py\n    import torch\n    from pytorch3d.structures import Volumes\n    from pytorch3d.renderer.implicit.renderer import VolumeSampler\n    ```", "```py\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        torch.cuda.set_device(device)\n    else:\n        device = torch.device(\"cpu\")\n    ```", "```py\n    checkpoint = torch.load('ray_sampling.pt')\n    ray_bundle = checkpoint.get('ray_bundle')\n    ```", "```py\n    batch_size = 10\n    densities = torch.zeros([batch_size, 1, 64, 64, 64]).to(device)\n    colors = torch.zeros(batch_size, 3, 64, 64, 64).to(device)\n    voxel_size = 0.1\n    volumes = Volumes(\n        densities=densities,\n        features=colors,\n        voxel_size=voxel_size\n    )\n    ```", "```py\n    volume_sampler = VolumeSampler(volumes = volumes, sample_mode = \"bilinear\")\n    rays_densities, rays_features = volume_sampler(ray_bundle)\n    ```", "```py\n    print('rays_densities shape = ', rays_densities.shape)\n    print('rays_features shape = ', rays_features.shape)\n    ```", "```py\n    rays_densities shape =  torch.Size([10, 64, 64, 50, 1])\n    rays_features shape =  torch.Size([10, 64, 64, 50, 3])\n    ```", "```py\n    torch.save({\n        'rays_densities': rays_densities,\n        'rays_features': rays_features\n    }, 'volume_sampling.pt')\n    ```", "```py\n    import torch\n    from pytorch3d.renderer.implicit.raymarching import EmissionAbsorptionRaymarcher\n    ```", "```py\n    checkpoint = torch.load('volume_sampling.pt')\n    rays_densities = checkpoint.get('rays_densities')\n    rays_features = checkpoint.get('rays_features')\n    ```", "```py\n    ray_marcher = EmissionAbsorptionRaymarcher()\n    image_features = ray_marcher(rays_densities = rays_densities, rays_features = rays_features)\n    ```", "```py\n    print('image_features shape = ', image_features.shape)\n    ```", "```py\n    image_features shape =  torch.Size([10, 64, 64, 4])\n    ```", "```py\n    import os\n    import sys\n    import time\n    import json\n    import glob\n    import torch\n    import math\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from PIL import Image\n    from pytorch3d.structures import Volumes\n    from pytorch3d.renderer import (\n        FoVPerspectiveCameras,\n        VolumeRenderer,\n        NDCGridRaysampler,\n        EmissionAbsorptionRaymarcher\n    )\n    from pytorch3d.transforms import so3_exp_map\n    from plot_image_grid import image_grid\n    from generate_cow_renders import generate_cow_renders\n    ```", "```py\n    if torch.cuda.is_available():\n        device = torch.device(\"cuda:0\")\n        torch.cuda.set_device(device)\n    else:\n        device = torch.device(\"cpu\")\n    ```", "```py\n    target_cameras, target_images, target_silhouettes = generate_cow_renders(num_views=40)\n    ```", "```py\n    render_size = 128\n    volume_extent_world = 3.0\n    raysampler = NDCGridRaysampler(\n        image_width=render_size,\n        image_height=render_size,\n        n_pts_per_ray=150,\n        min_depth=0.1,\n        max_depth=volume_extent_world,\n    )\n    ```", "```py\n    raymarcher = EmissionAbsorptionRaymarcher()\n    renderer = VolumeRenderer(\n        raysampler=raysampler, raymarcher=raymarcher,\n    )\n    ```", "```py\n    class VolumeModel(torch.nn.Module):\n        def __init__(self, renderer, volume_size=[64] * 3, voxel_size=0.1):\n            super().__init__()\n            self.log_densities = torch.nn.Parameter(-4.0 * torch.ones(1, *volume_size))\n            self.log_colors = torch.nn.Parameter(torch.zeros(3, *volume_size))\n            self._voxel_size = voxel_size\n            self._renderer = renderer\n        def forward(self, cameras):\n            batch_size = cameras.R.shape[0]\n            densities = torch.sigmoid(self.log_densities)\n            colors = torch.sigmoid(self.log_colors)\n            volumes = Volumes(\n                densities=densities[None].expand(\n                    batch_size, *self.log_densities.shape),\n                features=colors[None].expand(\n                    batch_size, *self.log_colors.shape),\n                voxel_size=self._voxel_size,\n            )\n            return self._renderer(cameras=cameras, volumes=volumes)[0]\n    ```", "```py\n    def huber(x, y, scaling=0.1):\n        diff_sq = (x - y) ** 2\n        loss = ((1 + diff_sq / (scaling ** 2)).clamp(1e-4).sqrt() - 1) * float(scaling)\n        return loss\n    ```", "```py\n    target_cameras = target_cameras.to(device)\n    target_images = target_images.to(device)\n    target_silhouettes = target_silhouettes.to(device)\n    ```", "```py\n    volume_size = 128\n    volume_model = VolumeModel(\n        renderer,\n        volume_size=[volume_size] * 3,\n        voxel_size=volume_extent_world / volume_size,\n    ).to(device)\n    ```", "```py\n    lr = 0.1\n    optimizer = torch.optim.Adam(volume_model.parameters(), lr=lr)\n    batch_size = 10\n    n_iter = 300\n    ```", "```py\n    for iteration in range(n_iter):\n        if iteration == round(n_iter * 0.75):\n            print('Decreasing LR 10-fold ...')\n            optimizer = torch.optim.Adam(\n                volume_model.parameters(), lr=lr * 0.1\n            )\n        optimizer.zero_grad()\n        batch_idx = torch.randperm(len(target_cameras))[:batch_size]\n        # Sample the minibatch of cameras.\n        batch_cameras = FoVPerspectiveCameras(\n            R=target_cameras.R[batch_idx],\n            T=target_cameras.T[batch_idx],\n            znear=target_cameras.znear[batch_idx],\n            zfar=target_cameras.zfar[batch_idx],\n            aspect_ratio=target_cameras.aspect_ratio[batch_idx],\n            fov=target_cameras.fov[batch_idx],\n            device=device,\n        )\n        rendered_images, rendered_silhouettes = volume_model(\n            batch_cameras\n        ).split([3, 1], dim=-1)\n        sil_err = huber(\n            rendered_silhouettes[..., 0], target_silhouettes[batch_idx],\n        ).abs().mean()\n        color_err = huber(\n            rendered_images, target_images[batch_idx],\n        ).abs().mean()\n        loss = color_err + sil_err\n        loss.backward()\n        optimizer.step()\n    ```", "```py\n    with torch.no_grad():\n     rotating_volume_frames = ge erate_rotating_volume(volume_model, n_frames=7 * 4)\n    image_grid(rotating_volume_frames.clamp(0., 1\\. .cpu().numpy(), rows=4, cols=7, rgb=True, fill=True)\n    plt.savefig('rotating_volume.png')\n    plt.show()\n    ```"]