<html><head></head><body>
		<div id="_idContainer039">
			<h1 id="_idParaDest-50"><em class="italic"><a id="_idTextAnchor051"/>Chapter 3</em>: Automating the Machine Learning Pipeline with AutoKeras</h1>
			<p>Automating the machine learning pipeline involves automating a series of processes such as <strong class="bold">data exploration</strong>, <strong class="bold">data preprocessing</strong>, <strong class="bold">feature engineering</strong>, <strong class="bold">algorithm selection</strong>, <strong class="bold">model training</strong>, and <strong class="bold">hyperparameter tuning</strong>.<strong class="bold"> </strong></p>
			<p>This chapter explains the standard machine learning pipeline and how to automate some of them with <strong class="bold">AutoKeras</strong>. We will also describe the main data preparation best practices to apply before training a model. The post-data preparation steps are performed by AutoKeras and we will see them in depth in later chapters.</p>
			<p>As we saw in the first chapter, AutoKeras can automate all pipeline modeling steps by applying hyperparameter optimization and <strong class="bold">Neural Architecture Search</strong> (<strong class="bold">NAS</strong>), but some data preprocessing before these steps must be done by hand or with other tools.</p>
			<p>We will explain the data representations expected by our model, as well as the basic preprocessing techniques that AutoKeras applies. By the end of this chapter, you will have learned the main data formats and techniques for feeding your models in a suitable and optimal way.</p>
			<p> The main topics that will be covered are as follows:</p>
			<ul>
				<li>Understanding tensors</li>
				<li>Preparing the data to feed deep learning models </li>
				<li>Loading data into AutoKeras in multiple formats</li>
				<li>Splitting your dataset for training and evaluation</li>
			</ul>
			<p>In this chapter, we will look at some basic preprocessing techniques and how to use the AutoKeras helpers to apply them, but first, let's explain what kind of data structures are expected by our model and how are they represented.</p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor052"/>Understanding tensors</h1>
			<p>In the MNIST example, the digit images were<a id="_idIndexMarker151"/> stored in NumPy matrices, also called tensors. These tensors are the basic data structures for machine learning models. Now that we know what fuels our models, let's dig deeper into understanding what tensors are and their different types.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor053"/>What is a tensor?</h2>
			<p>A tensor is basically a <a id="_idIndexMarker152"/>multi-dimensional array of numbers, usually floating-point numbers of N dimensions (also called <em class="italic">axes</em>).</p>
			<p>A tensor is defined by three key attributes: the number of axes or rank, the dimension of each axes or shape, and the type of data it contains. Let's explain them in detail:</p>
			<ul>
				<li><strong class="bold">Rank (axes number)</strong>: This is also called a dimension in <strong class="source-inline">numpy</strong> nomenclature (<strong class="source-inline">ndim</strong>).  For instance, a scalar (a single number) has no <a id="_idIndexMarker153"/>axes, a vector (a list of numbers) has one, a matrix (a list of vectors) has two, and a 3D tensor (a list of matrices) has three. Let's look at a practical example:<p class="source-code">&gt;&gt;&gt; import numpy as np </p><p class="source-code">&gt;&gt;&gt; x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]) </p><p class="source-code">&gt;&gt;&gt; x.ndim</p><p class="source-code">2</p><p>In the previous code snippet, we created a matrix and printed its rank (<strong class="source-inline">2</strong>).</p></li>
				<li><strong class="bold">Shape (each axis dimension)</strong>: This is the dimension of each axis and returns a tuple with the lengths of the corresponding <a id="_idIndexMarker154"/>array dimensions. In the case of a matrix, the first item would correspond to the number of rows and the second item would correspond to the number of columns, as shown in the following code:<p class="source-code">&gt;&gt;&gt; import numpy as np </p><p class="source-code">&gt;&gt;&gt; x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]) </p><p class="source-code">&gt;&gt;&gt; x.shape </p><p class="source-code">(4, 3) </p><p>This time, we have created a matrix and printed its shape (4 rows, 3 columns).</p></li>
				<li><strong class="bold">Data type</strong>: The type of data that's<a id="_idIndexMarker155"/> contained in the tensor is usually floating-point numbers because the computer works in a more optimal way with this type of data. For instance, in the following matrix, the stored items are integers:<p class="source-code">&gt;&gt;&gt; import numpy as np </p><p class="source-code">&gt;&gt;&gt; x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]]) </p><p class="source-code">&gt;&gt;&gt; x.dtype </p><p class="source-code">dtype('int64')Here we have created a matrix and printed the type of its items (int64)</p></li>
			</ul>
			<p>Now that we've explained the key attributes of a tensor, let's see what types of tensors we can use.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor054"/>Types of tensors </h2>
			<p>Based on their dimensions, we can classify<a id="_idIndexMarker156"/> tensors like so:</p>
			<ul>
				<li><strong class="bold">Scalar (N=0)</strong>: A tensor containing just one <a id="_idIndexMarker157"/>number is called a scalar; let's create<a id="_idIndexMarker158"/> one:<p class="source-code">&gt;&gt;&gt; import numpy as np </p><p class="source-code">&gt;&gt;&gt; t = np.array(123) </p><p class="source-code">&gt;&gt;&gt; t </p><p class="source-code">array(123) </p><p class="source-code">&gt;&gt;&gt; v.ndim </p><p class="source-code">0</p><p>By creating a scalar and printing its rank, we can see its value is 0.</p></li>
				<li><strong class="bold">Vector (N=1)</strong>: A 1D tensor is a <a id="_idIndexMarker159"/>called vector. It's an array of numbers of 1 dimension, as shown in the<a id="_idIndexMarker160"/> following code:<p class="source-code">&gt;&gt;&gt; x = np.array([1, 2, 3]) </p><p class="source-code">&gt;&gt;&gt; xarray([1, 2, 3]) </p><p class="source-code">&gt;&gt;&gt; x.ndim </p><p class="source-code">1</p><p>Here, we have created a vector of 1 dimension and printed its rank.</p></li>
				<li><strong class="bold">Matrix (N=2)</strong>: A 2D tensor is<a id="_idIndexMarker161"/> called a matrix. It's a<a id="_idIndexMarker162"/> vectors array of dimension 2 and its two axes are called <em class="italic">rows</em> and <em class="italic">columns</em>. You can imagine a matrix as a grid of numbers. The following is a NumPy matrix:<p class="source-code">&gt;&gt;&gt; x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]) </p><p class="source-code">&gt;&gt;&gt; x.ndim </p><p class="source-code">2 </p><p>In the case of a matrix, the rank that's returned is 2. as shown in the previous block code.</p><p>The rows are the first axis items, while the columns are the second axis items, so the first row is <strong class="source-inline">[1, 2, 3]</strong> and the first column is <strong class="source-inline">[1, 4, 7]</strong>, respectively.</p></li>
				<li><strong class="bold">3D tensors (N=3)</strong>: A 3D tensor is an array of matrices. This tensor is typically used to represent images using a 3-matrice <a id="_idIndexMarker163"/>array, with each matrix representing a color (red, green, or blue) of the pixel. You can imagine it as a cube filled with numbers. Let's create one using the following code:<p class="source-code">&gt;&gt;&gt; x = np.array([[[1, 2, 3], [4, 5, 6], [7, 8, 9]], [[10, 11, 12], [13, 14, 15], [16, 17, 18]], [[19, 20, 21], [22, 23, 24], [25, 26, 27]]]) </p><p class="source-code"> &gt;&gt;&gt; x.ndim </p><p class="source-code"> 3</p><p>Here, we can see that the rank that's returned for the 3D tensor is 3.</p></li>
				<li><strong class="bold">4D tensors (N=4)</strong>: 4D tensors are arrays of 3D tensors. This complex structure is often used to store video, which is <a id="_idIndexMarker164"/>basically a batch of frames, where each frame is an image represented by a 3D tensor.</li>
			</ul>
			<p>The following is a visual representation of these ranks:</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B16953_03_01.jpg" alt="Figure 3.1 – A visual representation of tensors with different ranks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 3.1 – A visual representation of tensors with different ranks</p>
			<p>A 3D tensor can store an RGB image or frame, while a 4D tensor can contain a video as an array of frames.</p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor055"/>Preparing the data to feed deep learning models</h1>
			<p>In the previous <a id="_idIndexMarker165"/>chapter, we explained that AutoKeras is a framework that specializes in deep learning that uses neural networks as a learning engine. We also learned how to create end-to-end classifier/regressor models for the MNIST dataset of handwritten digits as input data. This dataset had already been preprocessed to be used by the model, which means all the images had the same attributes (same size, color, and so on), but this is not always the case.</p>
			<p>Once we know what a tensor is, we are ready to learn how to feed our neural networks. Most of the data preprocessing techniques are domain-specific, and we will explain them in the following chapters when we need to use them in specific examples. But first, we will present some fundamentals that are the basis for each specific technique.</p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor056"/>Data preprocessing operations for neural network models</h2>
			<p>In this section, we will look at some <a id="_idIndexMarker166"/>of the operations we can use to transform the raw input data into a more appropriate format. This will allow us to feed the neural network in order to improve the learning performance of the model.</p>
			<p>The main data preprocessing operations are feature engineering, data normalization, data vectorization, and missing values processing. Let's look at them in detail:</p>
			<ul>
				<li><strong class="bold">Feature engineering</strong>: This is the<a id="_idIndexMarker167"/> process of extracting features from raw data using the domain knowledge of human experts, in such a way that these extracted features improve the performance of our models. <p>In traditional machine learning, function engineering is critical but with deep learning, this process is not as important because neural networks can automatically extract the relevant characteristics from the raw<a id="_idIndexMarker168"/> input data. However, there are cases where function engineering is still crucial, such as when we don't have a large dataset, the input data is structured, or we have limited resources. In these cases, this step is key to achieving our goals.</p></li>
				<li><strong class="bold">Data normalization</strong>: Neural networks work much better with small input values, usually between 0 and 1. It is easier for the model to learn from small numbers <a id="_idIndexMarker169"/>because the learning algorithms are based on gradient updates being made to its weight parameters, in such a way that small values will cause faster updates, thus speeding up the process, while larger values will slow it down. Usually, the dataset comes with larger values, so before we incorporate them into our model, we need to change and scale them to a range of 0 to 1. This technique is called normalization. AutoKeras already <a id="_idIndexMarker170"/>does this for us. In the previous digit classification example, the dataset contained images encoded as integers from 0 to 255. However, we fed our model without performing normalization because AutoKeras did it for us automatically.</li>
				<li><strong class="bold">Data vectorization</strong>: As we explained <a id="_idIndexMarker171"/>previously, neural networks <a id="_idIndexMarker172"/>work with tensors. Each source that feeds your model, such as texts, images, or sounds, must be converted into a tensor through a process called vectorization. This<a id="_idIndexMarker173"/> process converts the raw input data into floating-point number vectors that are more suitable for algorithms. In the <strong class="source-inline">MINST</strong> example shown in the previous chapter, the dataset has already been vectorized, so this process was not necessary. </li>
				<li><strong class="bold">Missing values processing</strong>: Datasets<a id="_idIndexMarker174"/> often contain missing <a id="_idIndexMarker175"/>values in some records. How should your model handle these incomplete records? Generally, for deep learning models, initializing missing values to 0 is a common practice, as long as 0 is not already a significant value. Once the neural network model learns that 0 means a missing value, it will ignore it every time. It is important to note that if your model will be exposed to missing values in the real world and you trained it without them, it will not learn to ignore them. So, a common practice in this case is to artificially generate missing values to force your model to learn how to handle them.</li>
			</ul>
			<p>Now that we've learned about the main data structures and their <strong class="source-inline">transform</strong> operations, we will look at what data formats are supported by AutoKeras and what utilities it has to convert raw data into a more suitable format.</p>
			<h1 id="_idParaDest-56"><a id="_idTextAnchor057"/>Loading data into AutoKeras in multiple formats</h1>
			<p>As we mentioned previously, AutoKeras performs normalization automatically. However, in the following chapters, you will <a id="_idIndexMarker176"/>see that you can create your model in a more personalized way by stacking blocks. More specifically, you can use special blocks to normalize your data.</p>
			<p>Now, let's look at the different data structures that we can use to feed our model.</p>
			<p>AutoKeras models accept three types of input:</p>
			<ul>
				<li>A <strong class="bold">NumPy array</strong> is an array<a id="_idIndexMarker177"/> that's commonly used by <strong class="bold">Scikit-Learn</strong> and many other Python-based libraries. This is always<a id="_idIndexMarker178"/> the fastest option, as long as your data fits in memory.</li>
				<li><strong class="bold">Python generators</strong> load batches <a id="_idIndexMarker179"/>of data from disk to memory, so this is a good option when the entire dataset does not fit in memory.</li>
				<li><strong class="bold">TensorFlow Dataset</strong> is a high-performance <a id="_idIndexMarker180"/>option that is similar to Python generators, but it is best suited for deep learning and large datasets. This is because data can be streamed from disk or from a distributed filesystem.</li>
			</ul>
			<p>You can prepare your data in one of these formats before feeding it to your AutoKeras model. If you are working with large datasets and need to train in GPUs, the best choice is to use the <strong class="bold">TensorFlow Dataset</strong> object, because<a id="_idIndexMarker181"/> they have many advantages in terms of performance and versatility, such as the following:</p>
			<ul>
				<li>It can perform asynchronous preprocessing and data queuing.</li>
				<li>It provides GPU memory data preloading, so that it is available after the GPU finishes processing the previous batch.</li>
				<li>It provides transformation primitives, so that you can apply a function to each element of the dataset that's generating a new transformed dataset.</li>
				<li>A cache that maintains the latest batches that have been read from the dataset in memory.</li>
				<li>You can load from several sources (<strong class="bold">NumPy arrays</strong>, <strong class="bold">Python generators</strong>, <strong class="bold">CSV files</strong>, text files, folders, and so on).</li>
			</ul>
			<p>The following diagram represents all the different data sources that can use a TensorFlow Dataset object as input:</p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B16953_03_02.jpg" alt="Figure 3.2 – A visual representation of a TensorFlow Dataset object's input sources"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2 – A visual representation of a TensorFlow Dataset object's input sources</p>
			<p>AutoKeras has very useful utilities to<a id="_idIndexMarker182"/> help you convert raw data on disk into a TensorFlow Dataset:</p>
			<ul>
				<li><strong class="source-inline">autokeras.image_dataset_from_directory</strong> converts image files stored in a directory in a specific way into a tagged dataset of image tensors. Let's learn how to process a images directory.<p>The following directory is well-structured, which means we can feed it to AutoKeras. There's a subfolder for each class of images:</p><p class="source-code">main_directory/</p><p class="source-code">...class_a/</p><p class="source-code">......a_image_1.jpg</p><p class="source-code">......a_image_2.jpg</p><p class="source-code">...class_b/</p><p class="source-code">......b_image_1.jpg</p><p class="source-code">......b_image_2.jpg</p><p>Now, we must pass this folder path to the <strong class="source-inline">autokeras</strong> function in order to create a dataset from the images directory:</p><p class="source-code">autokeras.image_dataset_from_directory(</p><p class="source-code">    main_directory,</p><p class="source-code">    batch_size=32,</p><p class="source-code">    color_mode="rgb",</p><p class="source-code">    image_size=(256, 256),</p><p class="source-code">    interpolation="bilinear",</p><p class="source-code">    shuffle=True,</p><p class="source-code">    seed=None,</p><p class="source-code">    validation_split=None,</p><p class="source-code">    subset=None,</p><p class="source-code">)</p><p>There are several<a id="_idIndexMarker183"/> parameters, but only the path directory (<strong class="source-inline">main_directory</strong>) is required; the rest of the parameters are set by default. We will explain them in more detail in later chapters.</p></li>
				<li><strong class="source-inline">autokeras.text_dataset_from_directory</strong> generates a Tensorflow Dataset from text files stored in a directory in a specific way. As we saw previously with the images, we have to create a subfolder for every category:<p class="source-code"># Directory structure</p><p class="source-code">main_directory/</p><p class="source-code">...class_a/</p><p class="source-code">......a_text_1.txt</p><p class="source-code">......a_text_2.txt</p><p class="source-code">...class_b/</p><p class="source-code">......b_text_1.txt</p><p class="source-code">......b_text_2.txt</p><p class="source-code"># Create a dataset from the texts directory</p><p class="source-code">autokeras.text_dataset_from_directory(directory, batch_size=32, max_length=None, shuffle=True, seed=None, validation_split=None, subset=None)</p><p>As we mentioned previously, with images, only the path directory (<strong class="source-inline">directory</strong>) is required; the rest of the parameters, if they haven't been initialized, will be set by default. We will explain these in more detail in later chapters as well.</p></li>
			</ul>
			<p>Furthermore, AutoKeras can work with CSV files by directly passing the filename as a parameter to its structured data models; that is, <strong class="source-inline">autokeras.StructuredDataClassifier</strong> and <strong class="source-inline">autokeras.StructuredDataRegressor</strong>. Now that we <a id="_idIndexMarker184"/>know what kind of data is best suited for AutoKeras and what utilities it has for preprocessing it, we will learn how to divide our dataset so that we can properly evaluate and test our model.</p>
			<h1 id="_idParaDest-57"><a id="_idTextAnchor058"/>Splitting your dataset for training and evaluation</h1>
			<p>To evaluate a model, you<a id="_idIndexMarker185"/> must divide your dataset into three subsets: a training set, a validation set, and a test set. During the training phase, AutoKeras will train your model with the training dataset, while using the validation dataset to evaluate its performance. Once you are ready, the final evaluation will be done using the test dataset.  </p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor059"/>Why you should split your dataset</h2>
			<p>Having a separate test dataset that is not <a id="_idIndexMarker186"/>used during training is really important to avoid information leaks.  </p>
			<p>As we mentioned previously, the validation set is used to tune the hyperparameters of your model based on the performance of the model, but some information about the validation data is filtered into the model. Due to this, you run the risk of ending up with a model that works artificially well with the validation data, because that's what you trained it for. However, the actual performance of the model is due to us using previously unseen data, not validation data, so we must use a different and never-before-seen dataset to <a id="_idIndexMarker187"/>evaluate our model. This is known as the test dataset.  </p>
			<p>To avoid information leaks, it is very important that your model has never had access to any information about the test set, not even indirectly. This is why it's so important to have a separate test dataset.</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor060"/>How to split your dataset </h2>
			<p>In the MNIST example in the<a id="_idIndexMarker188"/> previous chapter, we did not split the dataset explicitly because the <strong class="source-inline">load_data</strong> method did this split for us. However, often, these datasets are just one set of records that you will have to split. The following is a visual representation of dataset splitting: </p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B16953_03_03.jpg" alt="Figure 3.3 – A visual representation of dataset splitting"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3 – A visual representation of dataset splitting</p>
			<p>When AutoKeras is training a model, it will reserve 20% of the training set for validation by default, but you can always define a different percent using <strong class="source-inline">validation_split param</strong> in the <strong class="source-inline">fit</strong> function. In the following code, we are using this parameter to split the training data and using the last 15% as <a id="_idIndexMarker189"/>validation data:</p>
			<p class="source-code">reg.fit(x_train, y_train,validation_split=0.15)  </p>
			<p>We can also manually create the validation dataset and pass it as <strong class="source-inline">validation_data  param: split = 5000</strong>:<strong class="source-inline">  </strong></p>
			<p class="source-code">x_val = x_train[split:] </p>
			<p class="source-code">y_val = y_train[split:] </p>
			<p class="source-code">x_train = x_train[:split] </p>
			<p class="source-code">y_train = y_train[:split] </p>
			<p class="source-code">reg.fit(x_train, </p>
			<p class="source-code">        y_train, </p>
			<p class="source-code">        epochs=2, </p>
			<p class="source-code">        validation_data=(x_val, y_val))</p>
			<p>You can also use to split the <strong class="source-inline">train_test_split</strong> function:</p>
			<p class="source-code">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, ...) </p>
			<p>Now, let's summarize what we learned in this chapter.</p>
			<h1 id="_idParaDest-60"><a id="_idTextAnchor061"/>Summary</h1>
			<p>In this chapter, we learned about tensors, the main data structures for networks, some data preprocessing operations for neural networks, and the AutoKeras data formats that are supported, as well as its data-preprocessing utilities. Finally, we learned how to split a dataset in a quick and easy way. Now, you are ready to power your AutoKeras models in the most appropriate way.</p>
			<p>In the next chapter, we will learn how AutoKeras works with images. We will also introduce some techniques we can use to extract specific characteristics from images and how to apply them.</p>
		</div>
	</body></html>