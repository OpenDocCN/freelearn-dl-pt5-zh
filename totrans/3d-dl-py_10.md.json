["```py\n    git clone https://github.com/facebookresearch/synsin.git\n    cd synsin/\n    conda create –-name synsin_env –-file requirements.txt\n    conda activate synsin_env\n    ```", "```py\n    elif opt.dataset == 'kitti':\n       opt.min_z = 1.0\n       opt.max_z = 50.0\n       opt.train_data_path = (\n           './DATA/dataset_kitti/'\n       )\n       from data.kitti import KITTIDataLoader\n       return KITTIDataLoader\n    ```", "```py\n    bash ./download_models.sh\n    ```", "```py\nwget https://dl.fbaipublicfiles.com/synsin/checkpoints/realestate/synsin.pth\n```", "```py\nwget https://dl.fbaipublicfiles.com/synsin/checkpoints/readme.txt\n```", "```py\n    python train.py --batch-size 32 \\\n          --folder 'temp' --num_workers 4  \\\n          --resume --dataset 'kitti' --use_inv_z \\\n          --use_inverse_depth \\\n          --accumulation 'alphacomposite' \\\n          --model_type 'zbuffer_pts' \\\n          --refine_model_type 'resnet_256W8UpDown64'  \\\n          --norm_G 'sync:spectral_batch' \\\n          --gpu_ids 0,1 --render_ids 1 \\\n          --suffix '' --normalize_image --lr 0.0001\n    ```", "```py\n    export KITTI=${KITTI_HOME}/dataset_kitti/images\n    python evaluation/eval_kitti.py --old_model ${OLD_MODEL} --result_folder ${TEST_FOLDER}\n    ```", "```py\n    python evaluation/evaluate_perceptualsim.py \\\n         --folder ${TEST_FOLDER} \\\n         --pred_image im_B.png \\\n         --target_image im_res.png \\\n         --output_file kitti_results\n    ```", "```py\nPerceptual similarity for ./DATA/dataset_kitti/test_folder/:  2.0548\nPSNR for /DATA/dataset_kitti/test_folder/:  16.7344\nSSIM for /DATA/dataset_kitti/test_folder/:  0.5232\n```", "```py\nimport torch\nimport torch.nn as nn\nimport sys\nsys.path.insert(0, './synsin')\nimport os\nos.environ['DEBUG'] = '0'\nfrom synsin.models.networks.sync_batchnorm import convert_model\nfrom synsin.models.base_model import BaseModel\nfrom synsin.options.options import get_model\n```", "```py\n          torch.backends.cudnn.enabled = True\n          opts = torch.load(model_path)['opts']\n          opts.render_ids = [1]\n           torch_devices = [int(gpu_id.strip()) for gpu_id in opts.gpu_ids.split(\",\")]\n           device = 'cuda:' + str(torch_devices[0])\n    ```", "```py\n    model = get_model(opts)\n    if 'sync' in opts.norm_G:\n    model = convert_model(model)\n    model = nn.DataParallel(model,       torch_devices[0:1]).cuda()\n    else:\n        model = nn.DataParallel(model, torch_devices[0:1]).cuda()\n    ```", "```py\n    #  Load the original model to be tested\n    model_to_test = BaseModel(model, opts)\n    model_to_test.load_state_dict(torch.load(MODEL_PATH)['state_dict'])\n    model_to_test.eval()\n    print(\"Loaded model\")\n    ```", "```py\n    import matplotlib.pyplot as plt\n    import quaternion\n    import torch\n    import torch.nn as nn\n    import torchvision.transforms as transforms\n    from PIL import Image\n    from set_up_model_for_inference import synsin_model\n    ```", "```py\n     model_to_test = synsin_model(path_to_model)\n        # Load the image\n        transform = transforms.Compose([\n            transforms.Resize((256,256)),\n            transforms.ToTensor(),\n            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n        if isinstance(test_image, str):\n            im = Image.open(test_image)\n        else:\n            im = test_image\n        im = transform(im)\n    ```", "```py\n        # Parameters for the transformation\n        theta = -0.15\n        phi = -0.1\n        tx = 0\n        ty = 0\n        tz = 0.1\n    RT = torch.eye(4).unsqueeze(0)\n    # Set up rotation\n    RT[0,0:3,0:3] = torch.Tensor(quaternion.as_rotation_matrix(quaternion.from_rotation_vector([phi, theta, 0])))\n    # Set up translation\n    RT[0,0:3,3] = torch.Tensor([tx, ty, tz])\n    ```", "```py\n    batch = {\n        'images' : [im.unsqueeze(0)],\n        'cameras' : [{\n            'K' : torch.eye(4).unsqueeze(0),\n            'Kinv' : torch.eye(4).unsqueeze(0)\n        }]\n    }\n    # Generate a new view of the new transformation\n    with torch.no_grad():\n        pred_imgs = model_to_test.model.module.forward_angle(batch, [RT])\n        depth = nn.Sigmoid()(model_to_test.model.module.pts_regressor(batch['images'][0].cuda()))\n    ```", "```py\n        fig, axis = plt.subplots(1,3, figsize=(10,20))\n        axis[0].axis('off')\n        axis[1].axis('off')\n        axis[2].axis('off')\n        axis[0].imshow(im.permute(1,2,0) * 0.5 + 0.5)\n        axis[0].set_title('Input Image')\n        axis[1].imshow(pred_imgs[0].squeeze().cpu().permute(1,2,0).numpy() * 0.5 + 0.5)\n        axis[1].set_title('Generated Image')\n        axis[2].imshow(depth.squeeze().cpu().clamp(max=0.04))\n        axis[2].set_title('Predicted Depth')\n    ```", "```py\n      # Parameters for the transformation\n        theta = 0.15\n        phi = 0.1\n        tx = 0\n        ty = 0\n        tz = 0.1\n    ```", "```py\n    from inference_unseen_image import inference\n    from PIL import Image\n    import numpy as np\n    import imageio\n    def create_gif(model_path, image_path, save_path, theta = -0.15, phi = -0.1, tx = 0,\n                  ty = 0, tz = 0.1, num_of_frames = 5):\n        im = inference(model_path, test_image=image_path, theta=theta,\n                       phi=phi, tx=tx, ty=ty, tz=tz)\n        frames = []\n        for i in range(num_of_frames):\n            im = Image.fromarray((im * 255).astype(np.uint8))\n            frames.append(im)\n            im = inference(model_path, im, theta=theta,\n                       phi=phi, tx=tx, ty=ty, tz=tz)\n        imageio.mimsave(save_path, frames,  duration=1)\n    ```"]