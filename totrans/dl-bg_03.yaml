- en: Setup and Introduction to Deep Learning Frameworks
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you are now familiar with **machine learning** (**ML**) and **deep
    learning** (**DL**) - this is great! You should feel ready to begin making the
    preparations for writing and running your own programs. This chapter helps you
    in the process of setting up TensorFlow and Keras, and introduces their usefulness
    and purpose in deep learning. Dopamine is presented as the new reinforcement learning
    framework that we will use later on. This chapter also briefly introduces other
    deep learning libraries that are important to know.
  prefs: []
  type: TYPE_NORMAL
- en: 'The topics that will be covered in this chapter are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Colaboratory
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction and setup of TensorFlow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction and setup of Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Dopamine
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other deep learning libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Colaboratory
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is Colaboratory? Colaboratory is a web-based research tool for doing machine
    learning and deep learning. It is essentially like Jupyter Notebook. Colaboratory is
    becoming very popular these days as it requires no setup.
  prefs: []
  type: TYPE_NORMAL
- en: Throughout this book, we will be using Python 3 running on Colaboratory which
    will have installed all the libraries we may need.
  prefs: []
  type: TYPE_NORMAL
- en: 'Colaboratory is free to use and is compatible with most major browsers. The
    company in charge of the development of the Colaboratory tool is Google^™. As
    opposed to Jupyter notebooks, in Colaboratory you are running everything on the
    cloud and not on your own computer. Here is the catch: you need a Google account
    since all the Colaboratory notebooks are saved into your personal Google Drive
    space. However, if you do not have a Google account, you can still continue reading
    to see how you can install every piece of Python library you will need to run
    things on your own. Still, I highly recommend you create a Google account, if
    only just to learn deep learning using the Colaboratory notebooks of this book.'
  prefs: []
  type: TYPE_NORMAL
- en: 'When you run your code on Colaboratory, it runs on a dedicated virtual machine,
    and here is the fun part: you can have a GPU allocated to use! Or you can also
    use a CPU if you want. Whenever you are not running something, Colaboratory will
    deallocate resources (you know, because we all want to work), but you can reconnect
    them at any time.'
  prefs: []
  type: TYPE_NORMAL
- en: If you are ready, go ahead and navigate to this link: [https://colab.research.google.com/](https://colab.research.google.com/)
  prefs: []
  type: TYPE_NORMAL
- en: If you are interested in more information and a further introduction to Colaboratory,
    search for *Welcome to Colaboratory!*. Now that you have accessed the previous
    link, let us get started with TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: From now on, we will refer to **Colaboratory** as **Colab** for short. This
    is actually how people refer to it.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction and setup of TensorFlow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**TensorFlow** (**TF**) has in its name the word *Tensor*, which is a synonym
    of vector. TF, thus, is a Python framework that is designed to excel at vectorial
    operations pertaining to the modeling of neural networks. It is the most popular
    library for machine learning.'
  prefs: []
  type: TYPE_NORMAL
- en: As data scientists, we have a preference towards TF because it is free, opensource
    with a strong user base, and it uses state-of-the-art research on the graph-based
    execution of tensor operations.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let us now begin with instructions to set up or verify that you have the proper
    setup:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin the installation of TF, run the following command in your Colaboratory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This will install about 20 libraries that are required to run TF, including `numpy`,
    for example.
  prefs: []
  type: TYPE_NORMAL
- en: Notice the exclamation mark (!) at the beginning of the command? This is how
    you will run shell commands on Colaboratory. For example, say that you want to
    remove a file named `model.h5`, then you would issue the command `!rm model.h5`.
  prefs: []
  type: TYPE_NORMAL
- en: 'If the execution of the installation ran properly, you will be able to run
    the following command, which will print the version of TF that is installed on
    your Colaboratory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This version of TF is the current version of TF at the time of writing this
    book. However, we all know that TF versions change frequently and it is likely
    that there will be a new version of TF when you are reading this book. If that
    is the case, you can install a specific version of TF as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: We are assuming that you are familiar with Python, thus, we will trust you with
    the responsibility of matching the proper libraries to the versions that we are
    using in this book. This is not difficult and can easily be done as shown previously,
    for example, using the `==` sign to specify the version. We will be showing the
    versions used as we continue.
  prefs: []
  type: TYPE_NORMAL
- en: TensorFlow with GPU support
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Colaboratory, by default, has GPU support automatically enabled for TensorFlow.
    However, if you have access to your own system with a GPU and want to set up TensorFlow
    with GPU support, the installation is very simple. Just type the following command
    on your personal system:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Notice, however, that this assumes that you have set up all the necessary drivers
    for your system to give access to the GPU. However, fear not, there is plenty
    of documentation about this process that can be searched on the internet, for
    example, [https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu).
    If you run into any problems and you need to move forward, I highly recommend
    that you come back and do the work on Colaboratory, as it is the easiest way to
    learn.
  prefs: []
  type: TYPE_NORMAL
- en: Let us now address how TensorFlow works and how its graph paradigm makes it
    very robust.
  prefs: []
  type: TYPE_NORMAL
- en: Principles behind TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This book is for absolute beginners in deep learning. As such, here is what
    we want you to know about how TF works. TF creates a graph that contains the execution
    from its input tensors, up to the highest level of abstraction of operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, let''s say that we have tensors ***x*** and ***w*** that are known
    input vectors, and that we have a known constant *b*, and say that you want to
    perform this operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/af40ade0-45f9-4dcb-acc5-f5c7296f964b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'If we create this operation by declaring and assigning tensors, the graph will
    look like the one in *Figure 2.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1d9b0258-46f6-44c7-a14d-425d966d2247.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 - Example of a tensor multiplication and addition operation
  prefs: []
  type: TYPE_NORMAL
- en: In this figure, there is a tensor multiplication operation, *mul*, whose result
    is a scalar and needs to be added, *add*, with another scalar, *b*. Note that
    this might be an intermediate result and, in real computing graphs, the outcome
    of this goes up higher in the execution tree. For more detailed information on
    how TF uses graphs, please refer to this paper (Abadi, M., et.al., 2016).
  prefs: []
  type: TYPE_NORMAL
- en: In a nutshell, TF finds the best way to execute tensor operations delegating
    specific parts to GPUs if available, or otherwise parallelizing operations on
    the CPU cores if available. It is open source with a growing community of users
    around the world. Most deep learning professionals know about TF.
  prefs: []
  type: TYPE_NORMAL
- en: Now let us discuss how to set up Keras and how it abstracts TensorFlow functionalities.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction and setup of Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you search on the internet for sample TensorFlow code, you will find that
    it may not be super easy to understand or follow. You can find tutorials for beginners
    but, in reality, things can get complicated very easily and editing someone else's
    code can be very difficult. Keras comes as an API solution to develop deep learning
    Tensorflow model prototypes with relative ease. In fact, Keras supports running
    not only on top of TensorFlow, but also over CNTK and Theano.
  prefs: []
  type: TYPE_NORMAL
- en: We can think of Keras as an abstraction to actual TensorFlow models and methods.
    This symbiotic relationship has become so popular that TensorFlow now unofficiallyencourages
    its use for those who are beginning to use TensorFlow. Keras is very user friendly,
    it is easy to follow in Python, and it is easy to learn in a general sense.
  prefs: []
  type: TYPE_NORMAL
- en: Setup
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To set up Keras on your Colab, do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Run the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The system will proceed to install the necessary libraries and dependencies.
    Once finished, type and run the following code snippet:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs a confirmation message of it using TensorFlow as the backend as
    well as the latest version of Keras, which at the time of writing this book is
    2.2.4\. Thus, the output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Principles behind Keras
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are two major ways in which Keras provides functionality to its users:
    a sequential model and the Functional API.'
  prefs: []
  type: TYPE_NORMAL
- en: 'These can be summarized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sequential model**: This refers to a way of using Keras that allows you to
    linearly (or sequentially) stack layer instances. A layer instance, in this case,
    has the same meaning as in our previous discussions in [Chapter 1](e3181710-1bb7-4069-825a-a235355bc116.xhtml), *Introduction
    to Machine Learning*. That is, a layer has some type of input, some type of behavior
    or main model operation, and some type of output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Functional API**: This is the best way to go deeper in defining more complex
    models, such as merge models, models with multiple outputs, models with multiple
    shared layers, and many other possibilities. Don''t worry, these are advanced
    topics that will become clear in further chapters. The Functional API paradigm
    gives the coder more freedom to do different innovative things.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can think of the sequential model as an easy way of starting with Keras,
    and the Functional API as the way to go for more complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Remember the shallow neural network from [Chapter 1](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=25&action=edit#post_24), *Introduction
    to Machine Learning*? Well, this is how you would do that model using the sequential
    model paradigm in Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The first two lines of code import the `Sequential` model and the `Dense` and
    `Activation` layers, respectively. A `Dense` layer is a fully connected neural
    network, whereas an `Activation` layer is a very specific way of invoking a rich
    set of activation functions, such as ReLU and SoftMax, as in the previous example
    (these will be explained in detail later).
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you could do the same model, but using the `add()` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This second way of writing the code for the neural model looks more linear,
    while the first one looks more like a Pythonic way to do so with a list of items.
    It is really the same thing and you will probably develop a preference for one
    way or the other. However, remember, both of the previous examples use the Keras
    sequential model.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, just for comparison purposes, this is how you would code the exact same
    neural network architecture, but using the Keras Functional API paradigm:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If you are an experienced programmer, you will notice that the Functional API
    style allows more flexibility. It allows you to define input tensors to use them
    as input to different pieces of the model, if needed. However, using the Functional
    API does assume that you are familiar with the sequential model. Therefore, in
    this book, we will start with the sequential model and move forward with the Functional
    API paradigm as we make progress toward more complex neural models.
  prefs: []
  type: TYPE_NORMAL
- en: Just like Keras, there are other Python libraries and frameworks that allow
    us to do machine learning with relatively low difficulty. At the time of writing
    this book, the most popular is Keras and the second most popular is PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to PyTorch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: At the time of writing this book, PyTorch is the third most popular overall
    deep learning framework. Its popularity has been increasing in spite of being
    relatively new in the world compared to TensorFlow. One of the interesting things
    about PyTorch is that it allows some customizations that TensorFlow does not.
    Furthermore, PyTorch has the support of Facebook™.
  prefs: []
  type: TYPE_NORMAL
- en: 'Although this book covers TensorFlow and Keras, I think it is important for
    all of us to remember that PyTorch is a good alternative and it looks very similar
    to Keras. As a mere reference, here is how the exact same shallow neural network
    we showed earlier would look if coded in PyTorch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The similarities are many. Also, the transition from Keras to PyTorch should
    not be too difficult for the motivated reader, and it could be a nice skill to
    have in the future. However, for now, most of the interest of the community is
    on TensorFlow and all its derivatives, especially Keras. If you want to know more
    about the beginnings and basic principles of PyTorch, you might find this reading
    useful (Paszke, A., et.al., 2017).
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to Dopamine
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An interesting recent development in the world of deep reinforcement learning
    is Dopamine. Dopamine is a framework for the fast prototyping of deep reinforcement
    learning algorithms. This book will deal very briefly with reinforcement learning,
    but you need to know how to install it.
  prefs: []
  type: TYPE_NORMAL
- en: Dopamine is known for being easy to use for new users in the world of reinforcement
    learning. Also, although it is not an official product of Google, most of its
    developers are Googlers. In its current state, at the time of writing this book,
    the framework is very compact and provides ready-to-use algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'To install Dopamine, you can run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'You can test the correct installation of Dopamine by simply executing the following
    command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: This provides no output, unless there are errors. Usually, Dopamine will make
    use of a lot of libraries outside of it to allow doing many more interesting things.
    Right now, some of the most interesting things one can do with reinforcement learning
    is to train agents with reward policies, which has direct applications in gaming.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, see *Figure 2.2*, which displays a time snapshot of a video
    game as it learns, using policies that reinforce desired behavior depending on
    the actions taken by an agent:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/63c1363e-7bef-466b-92ce-e1314d83ce37.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 - Sample visualization of Dopamine's agent in a reinforcement learning
    problem in gaming
  prefs: []
  type: TYPE_NORMAL
- en: An agent in reinforcement learning is the piece that decides what action to
    take next. The agent accomplishes this by observing the world and the rules of
    the world. The more defined the rules are, the more constrained the result will
    be. If the rules are too loose, the agent may not make good decisions on what
    actions to take.
  prefs: []
  type: TYPE_NORMAL
- en: Although this book does not dive a great deal into reinforcement learning, we
    will cover an interesting gaming application in the last chapter of the book.
    For now, you can read the following white paper for more information about Dopamine
    (Castro, P. S., et.al., 2018).
  prefs: []
  type: TYPE_NORMAL
- en: Other deep learning libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Besides the big two, TensorFlow and Keras, there are other competitors that
    are making their way in the world of deep learning. We already discussed PyTorch,
    but there are more. Here we talk about them briefly.
  prefs: []
  type: TYPE_NORMAL
- en: Caffe
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Caffe is also a popular framework developed at UC Berkeley (Jia, Y., et.al.
    2014). It became very popular in 2015-2016\. A few employers still demand this
    skillset and scholarly articles still mention its usage. However, its usage is
    in decay in part due to the major success of TF and the accessibility of Keras.
  prefs: []
  type: TYPE_NORMAL
- en: For more information about Caffe, visit: [https://caffe.berkeleyvision.org](https://caffe.berkeleyvision.org).
  prefs: []
  type: TYPE_NORMAL
- en: Note also the existence of Caffe2, which is developed by Facebook and is open
    source. It was built based on Caffe, but now Facebook has its new champion, PyTorch.
  prefs: []
  type: TYPE_NORMAL
- en: Theano
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Theano was developed by Yoshua Bengio's group at the University of Montreal
    in 2007 (Al-Rfou, R., *et.al. *2016). Theano has a relatively old user base that
    probably saw the rise of TF. The latest major release was made in late 2017 and,
    although there are no clear plans of new major releases, updates are still being
    made by the community.
  prefs: []
  type: TYPE_NORMAL
- en: 'For more information about Theano, please visit:'
  prefs: []
  type: TYPE_NORMAL
- en: '[http://deeplearning.net/software/theano/](http://deeplearning.net/software/theano/)'
  prefs: []
  type: TYPE_NORMAL
- en: Honorable mentions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are other alternatives out there that may not be as popular, for a variety
    of reasons, but are worth mentioning here in case their future changes. These
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Name** | **Developed by** | **More information** |'
  prefs: []
  type: TYPE_TB
- en: '| MXNET | Apache | [https://mxnet.apache.org/](https://mxnet.apache.org/) |'
  prefs: []
  type: TYPE_TB
- en: '| CNTK | Microsoft | [https://cntk.ai](https://cntk.ai) |'
  prefs: []
  type: TYPE_TB
- en: '| Deeplearning4J | Skymind | [https://deeplearning4j.org/](https://deeplearning4j.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| Chainer | Preferred Networks | [https://chainer.org/](https://chainer.org/)
    |'
  prefs: []
  type: TYPE_TB
- en: '| FastAI | Jeremy Howard | [https://www.fast.ai/](https://www.fast.ai/) |'
  prefs: []
  type: TYPE_TB
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This introductory chapter showed how to set up the necessary libraries to run
    TensorFlow, Keras, and Dopamine. Hopefully, you will use Colabs to make things
    easier for you to learn. You also learned the basic mindset and design concept
    behind these frameworks. Although such frameworks are the most popular at the
    time of writing this book, there are other competitors out there, which we also
    introduced briefly.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you are all set to begin the journey to mastering deep learning.
    Our first milestone is to know how to prepare data for deep learning applications.
    This item is crucial for the success of the model. No matter how good the models
    are and how deep they are, if the data is not properly formatted or treated, it
    can lead to catastrophic performance results. For that reason, we will now go
    to [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml), *Preparing Data.* In
    that chapter, you will learn how to take a dataset and prepare it for the specific
    task you are trying to solve with a specific type of deep learning model. However,
    before you go there, please try to quiz yourself with the following questions.
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Does Colab run on my personal computer?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No, it runs in the cloud, but with some skill and setup, you could connect it
    to your own personal cloud.
  prefs: []
  type: TYPE_NORMAL
- en: '**Does Keras use GPUs? **'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes. Since Keras runs on TensorFlow (in the setup of this book) and TensorFlow
    uses GPUs, then Keras also does.
  prefs: []
  type: TYPE_NORMAL
- en: '**What are the two main coding paradigms in Keras?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (A) Sequential model; (B) Functional API.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why do we care about Dopamine?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Because there are only a few reinforcement learning frameworks you can trust
    out there, and Dopamine is one of them.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., Devin, M.,
    Ghemawat, S., Irving, G., Isard, M., and Kudlur, M. (2016). *Tensorflow: A system
    for large-scale machine learning.* In*12th {USENIX} Symposium on Operating Systems
    Design and Implementation* ({OSDI} 16) (pp. 265-283).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Paszke, A., Gross, S., Chintala, S., Chanan, G., Yang, E., DeVito, Z., Lin,
    Z., Desmaison, A., Antiga, L. and Lerer, A. (2017). *Automatic differentiation
    in pytorch.*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Castro, P. S., Moitra, S., Gelada, C., Kumar, S., and Bellemare, M. G. (2018).
    *Dopamine: A research framework for deep reinforcement learning*. arXiv preprint
    arXiv:1812.06110.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama,
    S., and Darrell, T. (2014, November). *Caffe: Convolutional architecture for fast
    feature embedding.* In *Proceedings of the 22nd ACM* *international conference
    on Multimedia* (pp. 675-678). *ACM*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Al-Rfou, R., Alain, G., Almahairi, A., Angermueller, C., Bahdanau, D., Ballas,
    N., Bastien, F., Bayer, J., Belikov, A., Belopolsky, A. and Bengio, Y. (2016).
    *Theano: A Python framework for fast computation of* *mathematical expressions*. arXiv
    preprint arXiv:1605.02688.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
