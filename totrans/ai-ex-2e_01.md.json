["```py\n# Markov Decision Process (MDP) - The Bellman equations adapted to\n# Reinforcement Learning\nimport numpy as ql\n# R is The Reward Matrix for each state\nR = ql.matrix([ [0,0,0,0,1,0],\n                [0,0,0,1,0,1],\n                [0,0,100,1,0,0],\n                [0,1,1,0,1,0],\n                [1,0,0,1,0,0],\n                [0,1,0,0,0,0] ]) \n```", "```py\nnext_action = int(ql.random.choice(PossibleAction,1))\nreturn next_action \n```", "```py\n# The Bellman equation\n    Q[current_state, action] = R[current_state, action] +\n        gamma * MaxValue \n```", "```py\nimport numpy as ql\nR = ql.matrix([ [0,0,0,0,1,0],\n                [0,0,0,1,0,1],\n                [0,0,100,1,0,0],\n                [0,1,1,0,1,0],\n                [1,0,0,1,0,0],\n                [0,1,0,0,0,0] ])\nQ = ql.matrix(ql.zeros([6,6]))\ngamma = 0.8 \n```", "```py\nagent_s_state = 1\n# The possible \"a\" actions when the agent is in a given state\ndef possible_actions(state):\n    current_state_row = R[state,]\n    possible_act = ql.where(current_state_row >0)[1]\n    return possible_act\n# Get available actions in the current state\nPossibleAction = possible_actions(agent_s_state) \n```", "```py\ndef ActionChoice(available_actions_range):\n    if(sum(PossibleAction)>0):\n        next_action = int(ql.random.choice(PossibleAction,1))\n    if(sum(PossibleAction)<=0):\n        next_action = int(ql.random.choice(5,1))\n    return next_action\n# Sample next action to be performed\naction = ActionChoice(PossibleAction) \n```", "```py\ndef reward(current_state, action, gamma):\n    Max_State = ql.where(Q[action,] == ql.max(Q[action,]))[1]\n    if Max_State.shape[0] > 1:\n        Max_State = int(ql.random.choice(Max_State, size = 1))\n    else:\n        Max_State = int(Max_State)\n    MaxValue = Q[action, Max_State]\n\n    # Q function\n    Q[current_state, action] = R[current_state, action] +\n        gamma * MaxValue\n# Rewarding Q matrix\nreward(agent_s_state,action,gamma) \n```", "```py\nfor i in range(50000):\n    current_state = ql.random.randint(0, int(Q.shape[0]))\n    PossibleAction = possible_actions(current_state)\n    action = ActionChoice(PossibleAction)\n    reward(current_state,action,gamma)\n\n# Displaying Q before the norm of Q phase\nprint(\"Q :\")\nprint(Q)\n# Norm of Q\nprint(\"Normed Q :\")\nprint(Q/ql.max(Q)*100) \n```", "```py\n# Markov Decision Process (MDP) â€“ The Bellman equations adapted to\n# Reinforcement Learning with the Q action-value(reward) matrix\nimport numpy as ql\n# R is The Reward Matrix for each state\nR = ql.matrix([ [0,0,0,0,1,0],\n                [0,0,0,1,0,1],\n                [0,0,100,1,0,0],\n                [0,1,1,0,1,0],\n                [1,0,0,1,0,0],\n                [0,1,0,0,0,0] ]) \n```", "```py\nQ :\n[[ 0\\. 0\\. 0\\. 0\\. 258.44 0\\. ]\n [ 0\\. 0\\. 0\\. 321.8 0\\. 207.752]\n [ 0\\. 0\\. 500\\. 321.8 0\\. 0\\. ]\n [ 0\\. 258.44 401\\. 0\\. 258.44 0\\. ]\n [ 207.752 0\\. 0\\. 321.8 0\\. 0\\. ]\n [ 0\\. 258.44 0\\. 0\\. 0\\. 0\\. ]]\nNormed Q :\n[[ 0\\. 0\\. 0\\. 0\\. 51.688 0\\. ]\n [ 0\\. 0\\. 0\\. 64.36 0\\. 41.5504]\n [ 0\\. 0\\. 100\\. 64.36 0\\. 0\\. ]\n [ 0\\. 51.688 80.2 0\\. 51.688 0\\. ]\n [ 41.5504 0\\. 0\\. 64.36 0\\. 0\\. ]\n [ 0\\. 51.688 0\\. 0\\. 0\\. 0\\. ]] \n```", "```py\n\"\"\"# Improving the program by introducing a decision-making process\"\"\"\nnextc=-1\nnextci=-1\nconceptcode=[\"A\",\"B\",\"C\",\"D\",\"E\",\"F\"] \n```", "```py\norigin=int(input(\n    \"index number origin(A=0,B=1,C=2,D=3,E=4,F=5): \")) \n```", "```py\nindex number origin(A=0,B=1,C=2,D=3,E=4,F=5): 5 \n```", "```py\nConcept Path\n-> F\n-> B\n-> D\n-> C \n```"]