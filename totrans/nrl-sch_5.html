<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer062">
<h1 class="chapter-number" id="_idParaDest-64"><a id="_idTextAnchor068"/>5</h1>
<h1 id="_idParaDest-65"><a id="_idTextAnchor069"/>Multiple Search Modalities</h1>
<p>With the benefits of deep learning and artificial intelligence, we can encode any kind of data into <strong class="bold">vectors</strong>. This allows us to create a search system that uses any kind of data as a query and returns any kind of data as a search result. </p>
<p>In this chapter, we will introduce the rising topic of the <strong class="bold">multimodal search problem</strong>. You will see different data modalities and how to work with them. You will see how text, images, and audio documents can be transformed into vectors, and how to implement search systems independently of the data modality. You will also see the differences between the concepts of <strong class="bold">multimodality</strong> and <strong class="bold">cross-modality</strong>.</p>
<p>In this chapter, we’re going to cover the following main topics:</p>
<ul>
<li>How to represent documents of different data types</li>
<li>How to encode multimodal documents</li>
<li>Cross-modal and multimodal searches</li>
</ul>
<p>By the end of this chapter, you will have a solid understanding of how cross-modal and multimodal searches work, and how easy it is to process data of different modalities in Jina. </p>
<h1 id="_idParaDest-66"><a id="_idTextAnchor070"/>Technical requirements</h1>
<p>This chapter has the following technical requirements:</p>
<ul>
<li>A laptop with a minimum of 4 GB of RAM (8 GB or more is preferred)</li>
<li>Python installed with <em class="italic">3.7</em>, <em class="italic">3.8</em>, or <em class="italic">3.9</em> on a Unix-like operating system, such as macOS or Ubuntu</li>
</ul>
<h1 id="_idParaDest-67"><a id="_idTextAnchor071"/>Introducing multimodal documents</h1>
<p>Over<a id="_idIndexMarker331"/> the last decade, various types of data, such as <strong class="bold">texts</strong>, <strong class="bold">images</strong>, and <strong class="bold">audio</strong>, have been growing rapidly on the internet. Commonly, different types of data are associated with one piece of content. For example, images often also have textual tags and captions to describe the content. Therefore, the content has two modalities: image and text. A movie clip with subtitles has three modalities: image, audio, and text.</p>
<p>Jina<a id="_idIndexMarker332"/> is a <strong class="bold">data-type-agnostic framework</strong>, letting <a id="_idIndexMarker333"/>you work with any type of data and develop cross-modal and multimodal search systems. To better understand what this implies, it makes sense to first show how to represent documents of different data types, and then show how to represent multimodal documents in Jina.</p>
<h2 id="_idParaDest-68"><a id="_idTextAnchor072"/>Text document</h2>
<p>To<a id="_idIndexMarker334"/> represent a textual document in Jina is quite easy. You can do it simply by using the following code:</p>
<pre class="source-code">from docarray import Document
doc = Document(text='Hello World.')</pre>
<p>In some cases, one document can include thousands of words. But a long document with thousands of words is hard to search; some finer granularity would be nice. You can do this by segmenting a long document into smaller <em class="italic">chunks</em>. For example, let’s segment this simple document by using the <strong class="source-inline">!</strong> mark:</p>
<pre class="source-code">from jina import Document
d = Document(text='नमस्ते दुनिया!你好世界!こんにちは世界!Привет мир!')
d.chunks.extend([Document(text=c) for c in d.text.split('!')])</pre>
<p>This creates five subdocuments under the original document and stores them under <strong class="source-inline">.chunks</strong>. To see that more clearly, you can visualize it via <strong class="source-inline">d.display()</strong>, whose output is shown in the following figure:</p>
<div>
<div class="IMG---Figure" id="_idContainer053">
<img alt="Figure 5.1 – An example of a text document with chunk-level subdocuments " height="916" src="image/Figure_5.01_B17488.jpg" width="1250"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.1 – An example of a text document with chunk-level subdocuments</p>
<p>You can also <a id="_idIndexMarker335"/>print out each subdocument’s text attributes by using the <strong class="source-inline">.texts</strong> sugar attribute:</p>
<pre class="source-code">print(d.chunks.texts)</pre>
<p>This will output the following:</p>
<p class="source-code">['नमस्ते दुनिया', '你好世界', 'こんにちは世界', 'Привет мир', '']</p>
<p>That’s all you need to know about representing textual documents in Jina!</p>
<h2 id="_idParaDest-69"><a id="_idTextAnchor073"/>Image document</h2>
<p>Compared to<a id="_idIndexMarker336"/> textual data, image data is more universal and easier to comprehend. Image data is often<a id="_idIndexMarker337"/> just an <strong class="bold">N-dimensional array</strong> (<strong class="bold">ndarray</strong>) – strictly speaking, not just any ndarray, but an ndarray with <strong class="source-inline">ndim=2</strong> or <strong class="source-inline">ndim=3</strong> and <strong class="source-inline">dtype=uint8</strong>. Each element in that ndarray represents a pixel value between <strong class="source-inline">0</strong> and <strong class="source-inline">255</strong> on a certain channel at a certain position. For example, a colored JPG image of 256x300 can be represented as a <strong class="source-inline">[256, 300, 3]</strong> ndarray. You may ask why <strong class="source-inline">3</strong> is in the last dimension. It is because it represents the <strong class="source-inline">R</strong>, <strong class="source-inline">G</strong>, and <strong class="source-inline">B</strong> channels of each pixel. Some images have a different number of channels. For example, a PNG with a transparent background has four channels, where the extra channel represents opacity. A grayscale image has only one channel, which represents the luminance (a measure representing the proportions of black and white). </p>
<p>In Jina, you <a id="_idIndexMarker338"/>can load image data by specifying the image URI and then convert it into <strong class="source-inline">.tensor</strong> using the Document API. As an example, we will use the following code to load a PNG apple image (as shown in <em class="italic">Figure 5.2</em>):</p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<img alt="Figure 5.2 – An example PNG image located in the apple.png local file " height="432" src="image/Figure_5.02_B17488.jpg" width="472"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.2 – An example PNG image located in the apple.png local file</p>
<pre class="source-code">from docarray import Document
d = Document(uri='apple.png')
d.load_uri_to_image_tensor()
print(d.content_type, d.tensor)</pre>
<p>You will get the following output:</p>
<p class="source-code">tensor [[[255 255 255]</p>
<p class="source-code">  [255 255 255]</p>
<p class="source-code">  [255 255 255]</p>
<p class="source-code">  ...</p>
<p>Now the image content is converted into a document’s <strong class="source-inline">.tensor</strong> field, which can then be used for further processing. Some help functions can be used to process the image data. You can resize it (that is, downsample/upsample) and normalize it. You can switch the channel axis of <strong class="source-inline">.tensor</strong> to meet certain requirements of some framework, and finally, you can chain all these processing steps together in one line. For example, the image can be normalized and the color axis should be placed first, not last. You can perform such image transformations with the following code:</p>
<pre class="source-code">from docarray import Document
d = (
    Document(uri='apple.png')
    .load_uri_to_image_tensor()
    .set_image_tensor_shape(shape=(224, 224))
    .set_image_tensor_normalization()
    .set_image_tensor_channel_axis(-1, 0)
)
print(d.content_type, d.tensor.shape)</pre>
<p>You can also dump <strong class="source-inline">.tensor</strong> back to a PNG image file by using the following:</p>
<pre class="source-code">d.save_image_tensor_to_file('apple-proc.png', channel_axis=0)</pre>
<p>Note that the channel axis is now switched to <strong class="source-inline">0</strong> because of the processing steps we just conducted. Finally, you will get the resulting image shown in <em class="italic">Figure 5.3</em>:</p>
<div>
<div class="IMG---Figure" id="_idContainer055">
<img alt="Figure 5.3 – The resulting image after resizing and normalizing " height="224" src="image/Figure_5.03_B17488.jpg" width="224"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.3 – The resulting image after resizing and normalizing</p>
<h2 id="_idParaDest-70"><a id="_idTextAnchor074"/>Audio document</h2>
<p>As an <a id="_idIndexMarker339"/>important format for storing information, digital audio data can be a soundbite, music, a ringtone, or background noise. It often comes in <strong class="source-inline">.wav</strong> and <strong class="source-inline">.mp3</strong> formats, where the sound waves are digitized by sampling them at discrete intervals. To load a <strong class="source-inline">.wav</strong> file as a document in Jina, you can simply use the following code:</p>
<pre class="source-code">from docarray import Document
d = Document(uri='hello.wav')
d.load_uri_to_audio_tensor()
print(d.content_type, d.tensor.shape)</pre>
<p>You will see the following output:</p>
<p class="source-code">tensor [-0.00061035 -0.00061035 -0.00082397 ...  0.00653076  0.00595093 0.00631714]</p>
<p>As shown in the preceding example, the data from the <strong class="source-inline">.wav</strong> file is converted to a one-dimension (mono) ndarray, in which each element is generally expected to lie in the range [-1.0, +1.0] scale. You are by no means restricted to using Jina-native methods for audio processing. Here are some command-line tools, programs, and libraries that you can use for more advanced handling of audio data:</p>
<ul>
<li><strong class="bold">FFmpeg</strong> (<a href="https://ffmpeg.org/">https://ffmpeg.org/</a>): This is <a id="_idIndexMarker340"/>a free, open source project for handling <a id="_idIndexMarker341"/>multimedia files and streams.</li>
<li><strong class="bold">Pydub</strong> (<a href="https://github.com/jiaaro/pydub">https://github.com/jiaaro/pydub</a>): This <a id="_idIndexMarker342"/>manipulates audio with a simple and easy-to-use high-level interface.</li>
<li><strong class="bold">Librosa</strong> (<a href="https://librosa.github.io/librosa/">https://librosa.github.io/librosa/</a>): This<a id="_idIndexMarker343"/> is a Python package for <a id="_idIndexMarker344"/>music and audio analysis.</li>
</ul>
<h2 id="_idParaDest-71"><a id="_idTextAnchor075"/>Multimodal document</h2>
<p>So far, you <a id="_idIndexMarker345"/>have learned how to represent different data modalities in Jina. However, in the real world, data often comes in a form that combines multiple modalities, such as video, which typically includes at least <em class="italic">image</em> and <em class="italic">audio</em>, as well as <em class="italic">text</em> in the form of subtitles. Now, it is very interesting to know how to represent multimodal data.</p>
<p>A Jina document can be nested vertically via chunks. It is intuitive to put data of different modalities into subdocuments in chunks. For example, you can create a fashion product document<a id="_idIndexMarker346"/> that has two modalities, including a dress image and a product description.</p>
<div>
<div class="IMG---Figure" id="_idContainer056">
<img alt="Figure 5.4 – An example of a fashion product document with two modalities " height="276" src="image/Figure_5.04_B17488.jpg" width="231"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.4 – An example of a fashion product document with two modalities</p>
<p>You can do it simply by using the following code:</p>
<pre class="source-code">from jina import Document 
text_doc = Document(text='Men Town Round Red Neck T-Shirts') 
image_doc = Document(uri='tshirt.jpg').load_uri_to_image_tensor()
fashion_doc = Document(chunks=[text_doc, image_doc])</pre>
<p>Now, the example fashion product (as shown in <em class="italic">Figure 5.4</em>) is represented as a Jina document, which has two chunk-level documents representing the product’s description and dress image, respectively. You can also use <strong class="source-inline">fashion_doc.display()</strong> to produce the visualization, as shown in <em class="italic">Figure 5.5</em>: </p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<img alt="Figure 5.5 – An example of a fashion product document with two chunk-level documents " height="556" src="image/Figure_5.05_B17488.jpg" width="1232"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.5 – An example of a fashion product document with two chunk-level documents</p>
<p class="callout-heading">Important Note</p>
<p class="callout">You may think that different modalities correspond to different kinds of data (images and text in this case). However, this is not accurate. For example, you can do a cross-modal search by searching for images from different perspectives or searching for matching titles for given paragraph text. Therefore, we can consider that a modality is related to a given data distribution from which the data may come.</p>
<p>So far, we have <a id="_idIndexMarker347"/>learned how to represent a single piece of text, image, and audio data, as well as representing multimodal data as a Jina document. In the following section, we will show how to get the embedding of each document. </p>
<h1 id="_idParaDest-72"><a id="_idTextAnchor076"/>How to encode multimodal documents</h1>
<p>After<a id="_idIndexMarker348"/> defining the document for different types of data, the next step is to encode the documents into vector embeddings using a model. Formally, embedding was a multi-dimension of a document (often a <strong class="source-inline">[1, D]</strong> vector), which was designed to contain the content information of a document. With current advances in the performance of all the deep learning methods, even general-purpose models (for example, CNN models trained on ImageNet) can be used to extract meaningful feature vectors. In the following sections, we will show how to encode embedding <a id="_idIndexMarker349"/>for documents of different modalities. </p>
<h2 id="_idParaDest-73"><a id="_idTextAnchor077"/>Encoding text documents</h2>
<p>To convert<a id="_idIndexMarker350"/> textual documents into vectors, we can use the<a id="_idIndexMarker351"/> pretrained Bert model (<a href="https://www.sbert.net/docs/pretrained_models.xhtml">https://www.sbert.net/docs/pretrained_models.xhtml</a>) provided by Sentence <a id="_idIndexMarker352"/>Transformer (<a href="https://www.sbert.net/">https://www.sbert.net/</a>), as shown in the following example:</p>
<pre class="source-code">from docarray import DocumentArray
from sentence_transformers import SentenceTransformer
da = DocumentArray(...)
model = SentenceTransformer('all-MiniLM-L6-v2')
da.embeddings = model.encode(da.texts)
print(da.embeddings.shape)</pre>
<p>As a result, each document in the input <strong class="source-inline">DocumentArray</strong> will have an embedding with a 384-dimensional dense vector space after <strong class="source-inline">.encode(...)</strong> has been completed.</p>
<h2 id="_idParaDest-74"><a id="_idTextAnchor078"/>Encoding image documents</h2>
<p>For encoding<a id="_idIndexMarker353"/> image documents, we can use a pretrained model from Pytorch for embedding. As an example, we will use <a id="_idIndexMarker354"/>the <strong class="bold">ResNet50</strong> network (<a href="https://arxiv.org/abs/1512.03385">https://arxiv.org/abs/1512.03385</a>) for object classification on images <a id="_idIndexMarker355"/>provided by <strong class="bold">torchvision</strong> (<a href="https://pytorch.org/vision/stable/models.xhtml">https://pytorch.org/vision/stable/models.xhtml</a>):</p>
<pre class="source-code">from docarray import DocumentArray
import torchvision
da = DocumentArray(...)
model = torchvision.models.resnet50(pretrained=True)
da.embed(model)
print(da.embeddings.shape)</pre>
<p>In this way, we’ve <a id="_idIndexMarker356"/>successfully encoded an image document into its feature vector representation. The feature vector generated is the output activations of the neural network (a vector of 1,000 components).</p>
<p class="callout-heading">Important Note</p>
<p class="callout">You might have noticed that in the preceding example, we use <strong class="source-inline">.embed()</strong> for embeddings. Usually, when <strong class="source-inline">DocumentArray</strong> has <strong class="source-inline">.tensors</strong> set, you can use this API for encoding documents. You can specify <strong class="source-inline">.embed(..., device='cuda')</strong> when working with a GPU. The device name identifier depends on the model framework that you are using.</p>
<h2 id="_idParaDest-75"><a id="_idTextAnchor079"/>Encoding audio documents</h2>
<p>To<a id="_idIndexMarker357"/> encode the sound clips into vectors, we chose <a id="_idIndexMarker358"/>the <strong class="bold">VGGish</strong> model (<a href="https://arxiv.org/abs/1609.09430">https://arxiv.org/abs/1609.09430</a>) from Google Research. We will use the pretrained model <a id="_idIndexMarker359"/>from <strong class="bold">torchvggish</strong> (<a href="https://github.com/harritaylor/torchvggish">https://github.com/harritaylor/torchvggish</a>) to get the feature embeddings for audio data:</p>
<pre class="source-code">import torch
from docarray import DocumentArray
model = torch.hub.load('harritaylor/torchvggish', 'vggish')
model.eval()
for d in da:
    d.embedding = model(d.uri)[0]
print(da.embeddings.shape)</pre>
<p>The <a id="_idIndexMarker360"/>returned embeddings for each sound clip are a matrix of the size <em class="italic">K</em> x 128, where <em class="italic">K</em> is the number of examples in the log mel spectrogram and roughly corresponds to the length of audio in seconds. Therefore, each 4-second audio clip in the chunks is represented by four 128-dimensional vectors.</p>
<p>We have now learned how to encode embeddings for different modalities of documents. In the following section, we will show you how to search for data by using multiple modalities. This can be useful when trying to find data that is not easily represented in a single modality. For example, you might use an image search to find data that is textual in nature.</p>
<h1 id="_idParaDest-76"><a id="_idTextAnchor080"/>Cross-modal and multimodal searches</h1>
<p>Now that we know how<a id="_idIndexMarker361"/> to work with multimodal data, we can describe <strong class="bold">cross-modal </strong>and <strong class="bold">multimodal</strong> searches. Before that, I would like to first describe<a id="_idIndexMarker362"/> the <strong class="bold">unimodal</strong> (single-modality) search. In general, unimodal search means processing a single modality of data at both index and query time. For example, in an image <a id="_idIndexMarker363"/>search retrieval, the returned search <a id="_idIndexMarker364"/>results are also images based on the given image query. </p>
<p>So far, we already know how to encode document content into feature vectors to create embeddings. In the index, each document with the content of an image, text, or audio can be represented as embedding vectors and stored in an indexe. In the query, the query document can also be represented as an embedding, which can then be used to identify similar documents via some similarity scores such as cosine, Euclidean distance, and so on. <em class="italic">Figure 5.6</em> illustrates the unified matching view of the search problem:</p>
<div>
<div class="IMG---Figure" id="_idContainer058">
<img alt="Figure 5.6 – An illustration of the unified matching view for the search problem " height="345" src="image/Figure_5.06_B17488.jpg" width="339"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.6 – An illustration of the unified matching view for the search problem</p>
<p>More formally, searching <a id="_idIndexMarker365"/>can be considered as constructing a matching model that calculates the matching degree between input query documents and documents in the search. With this unified matching view, the matching models for <strong class="bold">unimodal</strong>, <strong class="bold">multimodal</strong>, and <strong class="bold">cross-modal</strong> searches<a id="_idIndexMarker366"/> bear even more resemblance to each other in terms of architecture and methodology, as reflected in the techniques: embedding the inputs (queries and documents) as distributed representations, combining neural network components to represent the relations between data of different modalities, and training the model parameters in an end-to-end manner.</p>
<h2 id="_idParaDest-77"><a id="_idTextAnchor081"/>Cross-modal search</h2>
<p>In a <a id="_idIndexMarker367"/>unimodal search, the search is designed to deal with a single data type, making it less flexible and more fragile regarding the input of different data types. Beyond unimodal search, <strong class="bold">cross-modal search</strong> aims to take one type of data as the query to retrieve relevant data of another type, such as image-text, video-text, and audio-text cross-modal searches. For example, as shown in <em class="italic">Figure 5.7</em>, we can devise a text-to-image search system that retrieves images based on short text descriptions as queries:</p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer059">
<img alt="Figure 5.7 – A cross-modal search system to look for images from captions " height="577" src="image/Figure_5.07_B17488.jpg" width="299"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.7 – A cross-modal search system to look for images from captions</p>
<p>More recently, cross-modal search <a id="_idIndexMarker368"/>has attracted considerable attention due to the rapid growth of multimodal data. As multimodal data grows, it becomes difficult for users to search for information of interest effectively and efficiently. So far, there have been various search methods proposed for searching multimodal data. However, these search techniques are mostly single modality-based, which converts cross-modal search into keyword-based search. This can be expensive because you need a person to write those keywords, and also, information about multimodal content is not always available. We need to look for another solution! <strong class="bold">C</strong><strong class="bold">ross-modal</strong> search aims to identify relevant data across different modalities. The main challenge in cross-modal search is how to measure the content similarity between different modalities of data. Various methods have been proposed to deal with such a problem. One common way is to generate feature vectors from different modalities in the same latent space, such that newly generated features can be applied in the computation of distance metrics. </p>
<p>To achieve this goal, usually, two <a id="_idIndexMarker369"/>very common architectures for deep metric learning (Siamese and triplet networks) can be utilized. They both share the idea that different subnetworks (which may or may not share weights) receive different inputs at the same time (positive and negative pairs for Siamese networks, and positive, negative, and anchor documents for triplets), and try to project their own feature vectors onto a common latent space where the contrastive loss is computed and its error propagated to all the subnetworks. </p>
<p>Positive pairs are pairs of objects (images, text, or any document) that are semantically related and expected to remain close in the projection space. On the other hand, negative pairs are pairs of documents that should be apart. </p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<img alt="Figure 5.8 – The schema of the deep metric learning process with a triplet network and anchor " height="406" src="image/Figure_5.08_B17488.jpg" width="1028"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.8 – The schema of the deep metric learning process with a triplet network and anchor</p>
<p>As shown in <em class="italic">Figure 5.8</em>, an example of a cross-modal search between an image and text, the subnetwork used to extract image features<a id="_idIndexMarker370"/> is a <strong class="bold">ResNet50</strong> architecture with weights pretrained on ImageNet, while for the text embedding, the output of a hidden layer from a<a id="_idIndexMarker371"/> pretrained <strong class="bold">Bert</strong> model is used. And recently, a new deep metric learning pretrained model, <strong class="bold">Contrastive Language-Image Pretraining</strong> (<strong class="bold">CLIP</strong>), was proposed, which is a neural network trained <a id="_idIndexMarker372"/>on a variety of image-text pairs. It is trained to learn visual concepts from natural language with the help of text snippets and image pairs from the internet. It can perform zero-shot learning by encoding text labels and images in the same <a id="_idIndexMarker373"/>semantic space and creating a standard embedding for both modalities. With the CLIP-style model, both images and query texts can be mapped into the same latent space, so that they can be compared using a similarity measure. </p>
<h2 id="_idParaDest-78"><a id="_idTextAnchor082"/>Multimodal search</h2>
<p>Compared<a id="_idIndexMarker374"/> to unimodal and cross-modal searches, <strong class="bold">multimodal search</strong> aims to enable multimodal data as the query input. The search queries can be composed of a combination of text input, image input, and other modalities of input. It is intuitive to combine different modalities of information for improving search performance. Imagine an e-commerce search scenario that takes two types of query information: an image and a text. For example, if you were searching for pants, the image would be a picture of pants, and the text would be something like “tight” and “blue.” In this case, the search query is composed of two modalities (text and image). We can refer to this search scenario as a multimodal search.</p>
<p>To allow for a multimodal search, two approaches are widely used in practice to fuse multiple modalities in the search: early fusion (which fuses features from multiple modalities as query input) and late fusion (which fuses the search results from different modalities at the very end).</p>
<p>Specifically, the early-fusion method fuses the features extracted from data of different modalities. As shown in <em class="italic">Figure 5.9</em>, features of two different modalities (image and text) resulting from different models are fed into a fusion operator. To combine features simply, we can use the feature concatenation as the fusion operator to produce the feature for multimodal data. Another fusion option involves the projection of different modalities into a common embedding space. And then, we can directly add the features from the data of different modalities. </p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"> </p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<img alt="Figure 5.9 – Early fusion, the fusion of multimodal features as the query input " height="437" src="image/Figure_5.09_B17488.jpg" width="786"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 5.9 – Early fusion, the fusion of multimodal features as the query input</p>
<p>After combining <a id="_idIndexMarker375"/>the features, we can use the same method devised for unimodal search to resolve the multimodal search problem. This feature fusion approach suffers from one significant limitation: it would be hard to define the right balance between the importance of various modalities in the context of a user query. To overcome this limitation, an end-to-end neural network can be trained to model the joint multimodal space. However, modeling this joint multimodal space requires a complex training strategy and thoroughly annotated datasets. </p>
<p>In practice, to address the abovementioned shortcomings, we can simply use the late-fusion approach to separate search per modality, and then fuse the search results from different modalities, for example, with a linear combination of the retrieval scores of all modalities per document. While late fusion has been proven to be robust, it has a few issues: appropriate weights of modalities are not a trivial problem, and there is a primary modality issue. For example, in a text-image multimodal search, when the results are assessed by the user based on visual similarity only, the influence of textual scores may worsen the visual quality of the end results. </p>
<p>The main difference between these two search modes is that for cross-modal, there is a direct mapping between a single document and a vector in embedding space, while for multimodal, this does not hold true, since two or more documents might be combined into a single vector.</p>
<h1 id="_idParaDest-79"><a id="_idTextAnchor083"/>Summary</h1>
<p>This chapter describes the concept of multimodal data, and cross-modal and multimodal search problems. First, we introduced multimodal data and how to represent it in Jina. Then, we learned how to use a deep neural network to get the vector features from data of different modalities. Finally, we introduced cross-modal and multimodal search systems. This unlocks a lot of powerful search patterns and makes it easier to understand how to implement cross-modal and multimodal search applications with Jina.</p>
<p>In the next chapter, we will introduce some basic practical examples to explain how to use Jina to implement search applications. </p>
</div>
</div>


<div id="sbo-rt-content"><div class="Content" id="_idContainer063">
<h1 id="_idParaDest-80"><a id="_idTextAnchor084"/>Part 3: How to Use Jina for Neural Search</h1>
</div>
<div id="_idContainer064">
<p>In this part, you will use all the knowledge learned so far and you will see step-by-step guides on how to build a search system for different modalities, either for text, images, audio, or cross- and multi-modality. The following chapters are included in this part: </p>
<ul>
<li><a href="B17488_06.xhtml#_idTextAnchor085"><em class="italic">Chapter 6</em></a>, <em class="italic">Basic Practical Examples with Jina</em></li>
<li><a href="B17488_07.xhtml#_idTextAnchor101"><em class="italic">Chapter 7</em></a>, <em class="italic">Exploring Advanced Use Cases of Jina</em></li>
</ul>
</div>
<div>
<div id="_idContainer065">
</div>
</div>
<div>
<div id="_idContainer066">
</div>
</div>
</div>
</body></html>