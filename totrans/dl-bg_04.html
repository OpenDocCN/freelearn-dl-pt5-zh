<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Preparing Data
                </header>
            
            <article>
                
<p class="mce-root">Now that you have successfully prepared your system to learn about deep learning, see <a href="0b6e1f9c-280c-4107-aa1b-862b99f991c8.xhtml">Chapter 2</a>, <em>Setup and Introduction to Deep Learning Frameworks</em>, we will proceed to give you important guidelines about data that you may encounter frequently when practicing deep learning. When it comes to learning about deep learning, having well-prepared datasets will help you to focus more on designing your models rather than preparing your data. However, everyone knows that this is not a realistic expectation and if you ask any data scientist or machine learning professional about this, they will tell you that an important aspect of modeling is knowing how to prepare your data. Knowing how to deal with your data and how to prepare it will save you many hours of work that you can spend fine-tuning your models. Any time spent preparing your data is time well invested indeed.</p>
<p class="mce-root">This chapter will introduce you to the main concepts behind data processing to make it useful in deep learning. It will cover essential concepts of formatting outputs and inputs that are categorical or real-valued, and techniques for augmenting data or reducing the dimensions of data. At the end of the chapter, you should be able to handle the most common data manipulation techniques that can lead to successful choices of deep learning methodologies down the road.</p>
<p class="mce-root">Specifically, this chapter discusses the following:</p>
<ul>
<li>Binary data and binary classification</li>
<li>Categorical data and multiple classes</li>
<li>Real-valued data and univariate regression</li>
<li>Altering the distribution of data</li>
<li>Data augmentation</li>
<li>Data dimensionality reduction</li>
<li>Ethical implications of manipulating data</li>
</ul>
<h1 id="uuid-81d77ccf-90ad-4411-88c4-e95947eae0c0">Binary data and binary classification</h1>
<p>In this section, we will focus all our efforts on <strong>preparing</strong> data with binary inputs or targets. By binary, of course, we mean values that can be represented as either 0 or 1. Notice the emphasis on the words <em>represented as</em>. The reason is that a column may contain data that is not necessarily a 0 or a 1, but could be interpreted as or represented by a 0 or a 1.</p>
<p>Consider the following fragment of a dataset:</p>
<table style="border-collapse: collapse;width: 100%;height: 166px" border="1">
<tbody>
<tr>
<td style="width: 20%" class="CDPAlignCenter CDPAlign">
<p><em>x</em><sub>1</sub></p>
</td>
<td style="width: 23.2138%" class="CDPAlignCenter CDPAlign">
<p><em>x</em><sub>2</sub></p>
</td>
<td style="width: 22.7862%" class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td style="width: 28%" class="CDPAlignCenter CDPAlign">
<p><em>y</em></p>
</td>
</tr>
<tr>
<td style="width: 20%" class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td style="width: 23.2138%" class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td style="width: 22.7862%" class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td style="width: 28%" class="CDPAlignCenter CDPAlign">
<p>a</p>
</td>
</tr>
<tr>
<td style="width: 20%" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td style="width: 23.2138%" class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td style="width: 22.7862%" class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td style="width: 28%" class="CDPAlignCenter CDPAlign">
<p>a</p>
</td>
</tr>
<tr>
<td style="width: 20%" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td style="width: 23.2138%" class="CDPAlignCenter CDPAlign">
<p>5</p>
</td>
<td style="width: 22.7862%" class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td style="width: 28%" class="CDPAlignCenter CDPAlign">
<p>b</p>
</td>
</tr>
<tr>
<td style="width: 20%" class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td style="width: 23.2138%" class="CDPAlignCenter CDPAlign">
<p>7</p>
</td>
<td style="width: 22.7862%" class="CDPAlignCenter CDPAlign">
<p>...</p>
</td>
<td style="width: 28%" class="CDPAlignCenter CDPAlign">
<p>b</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>In this short dataset example with only four rows, the column<strong> </strong><em>x</em><sub>1</sub> has values that are clearly binary and are either <em>0</em> or a <em>1</em>. However, <em>x</em><sub>2</sub>, at first glance, may not be perceived as binary, but if you pay close attention, the only values in that column are either <em>5</em> or <em>7</em>. This means that the data can be correctly and uniquely mapped to a set of two values. Therefore, we could map <em>5</em> to <em>0</em>, and <em>7</em> to <em>1</em>, or vice versa; it does not really matter.</p>
<p>A similar phenomenon is observed in the target output value, <em>y</em>, which also contains unique values that can be mapped to a set of size two. And we can do such mapping by assigning, say, <em>b</em> to <em>0</em>, and <em>a</em> to <em><strong>1</strong></em>. </p>
<div class="packt_tip">If you are going to map from strings to binary, always make sure to check what type of data your specific models can handle. For example, in some Support Vector Machine implementations, the preferred values for targets are -1 and 1. This is still binary but in a different set. Always double-check before deciding what mapping you will use.</div>
<p>In the next sub-section, we will deal specifically with binary targets using a dataset as a case study.</p>
<h2 id="uuid-96341356-638b-487e-a8b2-b21b9d631b27">Binary targets on the Cleveland Heart Disease dataset</h2>
<p class="mce-root">The <em>Cleveland Heart Disease</em> (Cleveland 1988) dataset contains patient data for 303 subjects. Some of the columns in the dataset have missing values; we will deal with this, too. The dataset contains 13 columns that include cholesterol and age.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">The target is to detect whether a subject has heart disease or not, thus, is binary. The problem we will deal with is that the data is encoded with values from 0 to 4, where 0 indicates the absence of heart disease and the range 1 to 4 indicates some type of heart disease.</p>
<p class="mce-root">We will use the portion of the dataset identified as <kbd>Cleveland</kbd>, which can be downloaded from this link: <a href="https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data" target="_blank">https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data</a></p>
<p>The attributes of the dataset are as follows:</p>
<table style="border-collapse: collapse;width: 100%" border="1">
<tbody>
<tr>
<td style="width: 14.5616%">
<p><strong>Column</strong></p>
</td>
<td style="width: 84.5489%">
<p><strong>Description</strong></p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x</em><sub>1</sub></p>
</td>
<td style="width: 84.5489%">
<p>Age</p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x</em><sub>2</sub></p>
</td>
<td style="width: 84.5489%">
<p>Sex</p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x</em><sub>3</sub></p>
</td>
<td style="width: 84.5489%">
<p>Chest pain type:</p>
<p class="p1"><span class="s1">  1: typical angina</span></p>
<p class="p1"><span class="s1"><span>  </span>2: atypical angina</span></p>
<p class="p1"><span class="s1"><span>  </span>3: non-anginal pain</span></p>
<p class="p1"><span class="s1"><span>  </span>4: asymptomatic</span></p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x<sub>4</sub></em></p>
</td>
<td style="width: 84.5489%">
<p>Resting blood pressure (in mm Hg on admission to the hospital)</p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x</em><sub>5</sub></p>
</td>
<td style="width: 84.5489%">
<p>Serum cholesterol in mg/dl</p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x<sub>6</sub></em></p>
</td>
<td style="width: 84.5489%">
<p>Fasting blood sugar &gt; 120 mg/dl:</p>
<p><span>  </span>1 = true</p>
<p><span>  </span>0 = false</p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x<sub>7</sub></em></p>
</td>
<td style="width: 84.5489%">
<p>Resting electrocardiographic results:</p>
<p><span>  </span>0: normal</p>
<p><span>  </span>1: having ST-T wave abnormality</p>
<p><span>  </span>2: showing probable or definite left ventricular hypertrophy</p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x<sub>8</sub></em></p>
</td>
<td style="width: 84.5489%">
<p><span>Maximum heart rate achieved</span></p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x<sub>9</sub></em></p>
</td>
<td style="width: 84.5489%">
<p class="mce-root"><span>Exercise-induced angina:<br/></span></p>
<p class="mce-root"><span>  1 = yes<br/></span></p>
<p class="mce-root"><span>  0 = no</span></p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x<sub>10</sub></em></p>
</td>
<td style="width: 84.5489%">
<p class="mce-root"><span>ST depression induced by exercise relative to rest <br/></span></p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x<sub>11</sub></em></p>
</td>
<td style="width: 84.5489%">
<p class="mce-root"><span>The slope of the peak exercise ST segment:<br/></span></p>
<p class="mce-root"><span>  1: upsloping<br/></span></p>
<p class="mce-root"><span>  2: flat<br/></span></p>
<p class="mce-root"><span>  3: downsloping<br/></span></p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x<sub>12</sub></em></p>
</td>
<td style="width: 84.5489%">
<p class="mce-root"><span>Number of major vessels (0-3) colored by fluoroscopy<br/></span></p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>x<sub>13</sub></em></p>
</td>
<td style="width: 84.5489%">
<p class="mce-root"><span>Thal:<br/></span><span>  3 = normal<br/></span> <span>  6 = fixed defect<br/></span><span>  7 = reversible defect<br/></span></p>
</td>
</tr>
<tr>
<td style="width: 14.5616%">
<p><em>y</em></p>
</td>
<td style="width: 84.5489%">
<p class="mce-root">Diagnosis of heart disease (angiographic disease status):<br/>
  0: &lt; 50% diameter narrowing<br/>
  1: &gt; 50% diameter narrowing</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>Let's follow the next steps in order to read the dataset into a pandas DataFrame and clean it:</p>
<ol>
<li>In our Google Colab, we will first download the data using the <kbd>wget</kbd> command as follows:</li>
</ol>
<pre style="padding-left: 60px">!wget https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data</pre>
<p style="padding-left: 60px">This, in turn, downloads the file <kbd>processed.cleveland.data</kbd> to the default directory for Colab. This can be verified by inspecting the <span class="packt_screen">Files</span> tab on the left side of Colab. Please note that the preceding instruction is all one single line that, unfortunately, is very long.</p>
<ol start="2">
<li>Next, we load the dataset using pandas to verify that the dataset is readable and accessible. </li>
</ol>
<div class="packt_infobox">Pandas is a Python library that is very popular among data scientists and machine learning scientists. It makes it easy to load and save datasets, to replace missing values, to retrieve basic statistical properties on data, and even perform transformations. Pandas is a lifesaver and now most other libraries for machine learning accept pandas as a valid input format.</div>
<p style="padding-left: 60px">Run the following commands in Colab to load and display some data:</p>
<pre style="padding-left: 60px">import pandas as pd<br/>df = pd.read_csv('processed.cleveland.data', header=None)<br/>print(df.head())</pre>
<p style="padding-left: 60px">The <kbd>read_csv()</kbd> <span>function </span>loads a file that is formatted as <strong>c</strong><strong>omma-separated values</strong> (<strong>CSV</strong>). We use the argument <kbd>header=None</kbd> to tell pandas that the data does not have any actual headers; if omitted, pandas will use the first row of the data as the names for each column, but we do not want that in this case.</p>
<p class="mce-root"/>
<p style="padding-left: 60px">The loaded data is stored in a variable called <kbd>df</kbd>, which can be any name, but I think it is easy to remember because pandas stores the data in a DataFrame object. Thus, <kbd>df</kbd> seems like an appropriate, short, memorable name for the data. However, if we work with multiple DataFrames, then it would be more convenient to name all of them differently with a name that describes the data they contain.</p>
<p style="padding-left: 60px">The <kbd>head()</kbd> method that operates over a DataFrame is analog to a <kbd>unix</kbd> command that retrieves the first few lines of a file. On a DataFrame, the <kbd>head()</kbd> method returns the first five rows of data. If you wish to retrieve more, or fewer, rows of data, you can specify an integer as an argument to the method. Say, for example, that you want to retrieve the first three rows, then you would do <kbd>df.head(3)</kbd>.</p>
<p style="padding-left: 60px">The results of running the preceding code are as follows:</p>
<pre style="padding-left: 60px">    0   1   2     3     4   5   6     7   8    9   10  11  12  13<br/>0  63.  1.  1.  145.  233.  1.  2.  150.  0.  2.3  3.  0.  6.   0<br/>1  67.  1.  4.  160.  286.  0.  2.  108.  1.  1.5  2.  3.  3.   2<br/>2  67.  1.  4.  120.  229.  0.  2.  129.  1.  2.6  2.  2.  7.   1<br/>3  37.  1.  3.  130.  250.  0.  0.  187.  0.  3.5  3.  0.  3.   0<br/>4  41.  0.  2.  130.  204.  0.  2.  172.  0.  1.4  1.  0.  3.   0</pre>
<p style="padding-left: 60px">Here are a few things to observe and remember for future reference:</p>
<ul>
<li>On the left side, there is an unnamed column that has rows with consecutive numbers, 0, 1, ..., 4. These are the indices that pandas assigns to each row in the dataset. These are unique numbers. Some datasets have unique identifiers, such as a filename for an image.</li>
<li>On the top, there is a row that goes from 0, 1, ..., 13. These are the column identifiers. These are also unique and can be set if they are given to us. </li>
<li>At the intersection of every row and column, we have values that are either floating-point decimals or integers. The entire dataset contains decimal numbers except for column 13, which is our target and contains integers.</li>
</ul>
<ol start="3">
<li>Because we will use this dataset as a binary classification problem, we now need to change the last column to contain only binary values: 0 and 1. We will preserve the original meaning of 0, that is,<em> </em>no heart disease, and anything greater than or equal to 1 will be mapped to 1, indicating the diagnosis of some type of heart disease. We will run the following instructions:</li>
</ol>
<pre style="padding-left: 60px">print(set(df[13]))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p style="padding-left: 60px">The instruction <kbd>df[13]</kbd> looks at the DataFrame and retrieves all the rows of the column whose index is <kbd>13</kbd>. Then, the <kbd>set()</kbd> method over all the rows of column 13 will create a set of all the unique elements in the column. In this way, we can know how many different values there are so that we can replace them. The output is as follows:</p>
<pre style="padding-left: 60px"><span>{0, 1, 2, 3, 4}</span></pre>
<p style="padding-left: 60px">From this, we know that 0 is no heart disease and 1 implies heart disease. However, 2, 3, and 4 need to be mapped to 1, because they, too, imply positive heart disease. We can make this change by executing the following commands:</p>
<pre style="padding-left: 60px">df[13].replace(to_replace=[2,3,4], value=1, inplace=True)<br/>print(df.head())<br/>print(set(df[13]))</pre>
<p>Here, the <kbd>replace()</kbd> <span>function </span>works on the DataFrame to replace specific values. In our case, it took three arguments:</p>
<ul>
<li><kbd>to_replace=[2,3,4]</kbd> denotes the list of items to search for, in order to replace them.</li>
<li><kbd>value=1</kbd> denotes the value that will replace every matched entry .</li>
<li><kbd>inplace=True</kbd> indicates to pandas that we want to make the changes on the column.</li>
</ul>
<div class="packt_tip">In some cases, pandas DataFrames behave like an immutable object, which, in this case, makes it necessary to use the <kbd>inplace=True</kbd> argument. If we did not use this argument, we would have to do something like this.<br/>
<kbd>df[13] = df[13].replace(to_replace=[2,3,4], value=1)</kbd>, which is not a problem for experienced pandas users. This means that you should be comfortable doing this either way.<br/>
The main problem for people beginning to use pandas is that it does not <em>always</em> behave like an immutable object. Thus, you should keep all the pandas documentation close to you: <a href="https://pandas.pydata.org/pandas-docs/stable/index.html">https://pandas.pydata.org/pandas-docs/stable/index.html</a></div>
<p>The output for the preceding commands is the following:</p>
<pre>    0   1   2     3     4   5   6     7   8    9   10  11  12  13<br/>0  63.  1.  1.  145.  233.  1.  2.  150.  0.  2.3  3.  0.  6.   0<br/>1  67.  1.  4.  160.  286.  0.  2.  108.  1.  1.5  2.  3.  3.   <strong>1</strong><br/>2  67.  1.  4.  120.  229.  0.  2.  129.  1.  2.6  2.  2.  7.   1<br/>3  37.  1.  3.  130.  250.  0.  0.  187.  0.  3.5  3.  0.  3.   0<br/>4  41.  0.  2.  130.  204.  0.  2.  172.  0.  1.4  1.  0.  3.   0<br/><span><br/>{0, 1}</span></pre>
<p>First, notice that when we print the first five rows, the thirteenth column now exclusively has the values 0 or 1. You can compare this to the original data to verify that the number in bold font actually changed. We also verified, with <kbd>set(df[13])</kbd>, that the set of all unique values of that column is now only <kbd>{0, 1}</kbd>, which is the desired target.</p>
<p>With these changes, we could use the dataset to train a deep learning model and perhaps improve the existing documented performance [<span>Detrano, R., <em>et al.</em> </span><span>(1989)]</span>.</p>
<p>The same methodology can be applied to make any other column have binary values in the set we need. As an exercise<span>, let's do another example with the famous <kbd>MNIST</kbd> dataset.</span></p>
<h2 id="uuid-4bc064fa-6afd-4b0c-9891-ad19e00be740">Binarizing the MNIST dataset</h2>
<p>The MNIST dataset is well known in the deep learning community (<span>Deng, L. (2012))</span>. It is composed of thousands of images of handwritten digits. Figure 3.1 shows eight samples of the MNIST dataset:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/7de1c72c-0fef-46ad-93e0-19d39bafad3b.png" style="width:18.92em;height:12.00em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><br/>
Figure 3.1 – Eight samples of the MNIST dataset. The number on top of each image corresponds to the target class</div>
<p>As you can see, the samples in this dataset are messy and are very real. Every image has a size of 28 x 28 pixels. And there are only 10 target classes, one for each digit, 0, 1, 2, ..., 9. The complication here is usually that some digits may look similar to others; for example, 1 and 7, or 0 and 6. However, most deep learning algorithms have successfully solved the classification problem with high accuracy. </p>
<p>From <em>Figure 3.1</em>, a close inspection will reveal that the values are not exactly zeros and ones, that is, binary. In fact, the images are 8-bit grayscale, in the range [0-255]. As mentioned earlier, this is no longer a problem for most advanced deep learning algorithms. However, for some algorithms, such as <strong>Restricted Boltzmann Machines</strong> (<strong>RMBs</strong>), the input data needs to be in binary format [0,1] because that is how the algorithm works, traditionally. </p>
<p>Thus, we will do two things:</p>
<ul>
<li>Binarize the images, so as to have binary inputs</li>
<li>Binarize the targets, to make it a binary classification problem</li>
</ul>
<p>For this example, we will arbitrarily select two numerals only, 7 and 8, as our target classes.</p>
<h3 id="uuid-76f36ba8-43a0-47ad-84f9-608ad229196f">Binarizing the images</h3>
<p>The binarization process is a common step in image processing. It is formally known as image thresholding because we need a threshold to decide which values become zeros and ones. For a full survey about this topic, please consult (<span>Sezgin, M., and Sankur, B. (2004)). This is all to say that there is a science behind picking the perfect threshold that will minimize the range conversion error from [0, 255] down to [0, 1]. </span></p>
<p>However, since this is not a book about image processing, we will arbitrarily set a threshold of 128. Thus, any value below 128 will become a zero, and any value greater than or equal to 128 will become a one. </p>
<p>This step can be easily done by using indexing in Python. To proceed, we will display a small portion of the dataset to make sure the data is transformed correctly. We will do this by executing the following commands in the next steps:</p>
<ol>
<li>To load the dataset and verify its dimensionality (shape), run the following command:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.datasets import fetch_openml<br/>mnist = fetch_openml('mnist_784')<br/>print(mnist.data.shape)<br/>print(mnist.target.shape)</pre>
<p style="padding-left: 60px">The following is the output:</p>
<pre style="padding-left: 60px"><span>(70000, 784)<br/>(70000,)</span></pre>
<p style="padding-left: 60px">The first thing to notice is that we are using a machine learning library known as <kbd>scikit learn</kbd> or <kbd>sklearn</kbd> in Python. It is one of the most used libraries for general-purpose machine learning. The <kbd>MNIST</kbd> dataset is loaded using the <kbd>fetch_openml()</kbd> method, which requires an argument with the identifier of the dataset to be loaded, which in this case is <span><kbd>'mnist_784'</kbd>. The number <kbd>784</kbd> comes from the size of <kbd>MNIST</kbd> images, which is 28 x 28 pixels and can be interpreted as a vector of 784 elements rather than a matrix of 28 columns and 28 rows. By verifying the <kbd>shape</kbd> property, we can see that the dataset has 70,000 images represented as vectors of size 784, and the targets are in the same proportion. </span></p>
<div class="packt_infobox">Please note here that, as opposed to the previous section where we used a dataset loaded into pandas, in this example, we use the data directly as lists or arrays of lists. You should feel comfortable manipulating both pandas and raw datasets. </div>
<ol start="2">
<li>To actually do the binarization by verifying the data before and after, run the following:</li>
</ol>
<pre style="padding-left: 60px">print(mnist.data[0].reshape(28, 28)[10:18,10:18])<br/>mnist.data[mnist.data &lt; 128] = 0<br/>mnist.data[mnist.data &gt;=128] = 1<br/>print(mnist.data[0].reshape(28, 28)[10:18,10:18])</pre>
<p style="padding-left: 60px">This will output the following:</p>
<pre style="padding-left: 60px">[[ 1. 154. 253.  90.   0.   0.   0.   0.]<br/> [ 0. 139. 253. 190.   2.   0.   0.   0.]<br/> [ 0.  11. 190. 253.  70.   0.   0.   0.]<br/> [ 0.   0.  35. 241. 225. 160. 108.   1.]<br/> [ 0.   0.   0.  81. 240. 253. 253. 119.]<br/> [ 0.   0.   0.   0.  45. 186. 253. 253.]<br/> [ 0.   0.   0.   0.   0.  16.  93. 252.]<br/> [ 0.   0.   0.   0.   0.   0.   0. 249.]]<br/><br/>[[ 0. 1. 1. 0. 0. 0. 0. 0.]<br/> [ 0. 1. 1. 1. 0. 0. 0. 0.]<br/> [ 0. 0. 1. 1. 0. 0. 0. 0.]<br/> [ 0. 0. 0. 1. 1. 1. 0. 0.]<br/> [ 0. 0. 0. 0. 1. 1. 1. 0.]<br/> [ 0. 0. 0. 0. 0. 1. 1. 1.]<br/> [ 0. 0. 0. 0. 0. 0. 0. 1.]<br/> [ 0. 0. 0. 0. 0. 0. 0. 1.]]</pre>
<p>The instruction <kbd>data[0].reshape(28, 28)[10:18,10:18]</kbd> is doing three things:</p>
<ol>
<li><kbd>data[0]</kbd> returns the first image as an array of size (1, 784).</li>
<li><kbd>reshape(28, 28)</kbd> resizes the (1, 784) array as a (28, 28) matrix, which is the actual image; this can be useful to display the actual data, for example, to produce <em>Figure 3.1</em>. </li>
<li><kbd>[10:18,10:18]</kbd> takes only a subset of the (28, 28) matrix at positions 10 to 18 for both columns and rows; this more or less corresponds to the center area of the image and it is a good place to look at what is changing.</li>
</ol>
<p>The preceding is for looking at the data only, but the actual changes are done in the next lines. The line <kbd>mnist.data[mnist.data &lt; 128] = 0</kbd> uses Python indexing. The instruction <kbd>mnist.data &lt; 128</kbd> returns a multidimensional array of Boolean values that <kbd>mnist.data[ ]</kbd> uses as indices on which to set the value to zero. The key is to do so for all values strictly less than 128. And the next line does the same, but for values greater than or equal to 128.</p>
<p>By inspecting the output, we can confirm that the data has successfully changed and has been thresholded, or binarized.</p>
<h3 id="uuid-2909c10c-64be-420d-bc6a-ca93ac9dbddd">Binarizing the targets</h3>
<p>We will binarize the targets by following the next two steps:</p>
<ol>
<li>First, we will discard image data for other numerals and we will only keep 7 and 8. Then, we will map 7 to 0 and 8 to 1. These commands will create new variables, <kbd>X</kbd><span> and</span> <kbd>y</kbd><span>, that will hold only the numerals 7 and 8:</span></li>
</ol>
<pre style="padding-left: 60px">X = mnist.data[(mnist.target == '7') | (mnist.target == '8')]<br/>y = mnist.target[(mnist.target == '7') | (mnist.target == '8')]<br/>print(X.shape)<br/>print(y.shape)</pre>
<p style="padding-left: 60px">This will output the following:</p>
<pre style="padding-left: 60px">(14118, 784)<br/>(14118)</pre>
<p style="padding-left: 60px">Notice the use of the <kbd>OR</kbd> operator, <kbd>|</kbd>, to logically take two sets of Boolean indices and produce one with the <kbd>OR</kbd> operator. These indices are used to produce a new dataset. The shape of the new dataset contains a little over 14,000 images.</p>
<ol start="2">
<li> To <span>map 7 to 0 and 8 to 1, </span>we can run the following command:</li>
</ol>
<pre style="padding-left: 60px">print(y[:10])<br/>y = [0 if v=='7' else 1 for v in y]<br/>print(y[:10])</pre>
<p style="padding-left: 60px">This outputs the following:</p>
<pre style="padding-left: 60px">['7' '8' '7' '8' '7' '8' '7' '8' '7' '8']<br/>[0, 1, 0, 1, 0, 1, 0, 1, 0, 1]</pre>
<p>The instruction <kbd>[0 if v=='7' else 1 for v in y]</kbd> checks every element in <kbd>y</kbd>, and if an element is <kbd>'7'</kbd>, then it returns a <kbd>0</kbd>, otherwise (for example, when it is <kbd>'8'</kbd>), it returns a <kbd>1</kbd>. As the output suggests, choosing the first 10 elements, the data is binarized to the set {<kbd>0</kbd>, <kbd><span><span>1</span></span></kbd>}.</p>
<div class="packt_tip">Remember, the target data in <kbd>y</kbd> was already binary in the sense that it only had two sets of unique possible numbers {<kbd>7</kbd>, <kbd>8</kbd>}. But we made it binary to the set {<kbd>0</kbd>, <kbd>1</kbd>} because often this is better when we use different deep learning algorithms that calculate very specific types of loss functions.</div>
<p>With this, the dataset is ready to use with binary and general classifiers. But what if we actually want to have multiple classes, for example, to detect all 10 digits of the <kbd>MNIST</kbd> dataset and not just 2? Or what if we have features, columns, or inputs that are not numeric but are categorical? The next section will help you prepare the data in these cases. </p>
<h1 id="uuid-67e43aaa-b695-45c8-9060-43a8c8cdbfde">Categorical data and multiple classes</h1>
<p>Now that you know how to binarize data for different purposes, we can look into other types of data, such as categorical or multi-labeled data, and how to make them numeric. Most advanced deep learning algorithms, in fact, only accept numerical data. This is merely a design issue that can easily be solved later on, and it is not a big deal because <span>you will learn </span>there are easy ways to take categorical data and convert it to a meaningful numerical representation. </p>
<div class="packt_infobox"><strong>Categorical data</strong> has information embedded as distinct categories. These categories can be represented as numbers or as strings. For example, a dataset that has a column named <kbd>country</kbd> with items such as "India", "Mexico", "France", and "U.S". Or, a dataset with zip codes such as 12601, 85621, and 73315. The former is <strong>non-numeric</strong> categorical data, and the latter is <strong>numeric</strong> categorical data. Country names would need to be converted to a number to be usable at all, but zip codes are already numbers that are meaningless as mere numbers. Zip codes would be more meaningful, from a machine learning perspective, if we converted them to latitude and longitude coordinates; this would better capture places that are closer to each other than using plain numbers.</div>
<p>To begin, we will address the issue of converting string categories to plain numbers and then we will convert those to numbers in a format called <strong>one-hot encoding</strong>.</p>
<h2 id="uuid-de80f514-1320-4a07-b115-556dc8e95d91">Converting string labels to numbers</h2>
<p>We will take the <kbd>MNIST</kbd> dataset again and use its string labels, <em>0</em>, <em>1</em>, ..., <em>9</em>, and convert them to numbers. We can achieve this in many different ways:</p>
<ul>
<li>We could simply map all strings to integers with one simple command, <kbd>y = list(map(int, mnist.target))</kbd>, and be done. The variable <kbd>y</kbd> now contains only a list of integers such as <kbd>[8, 7, 1, 2, ... ]</kbd>. But this will only solve the problem for this particular case; you need to learn something that will work for all cases. So, let's not do this.</li>
<li>We could do some hard work by iterating over the data 10 times – <kbd>mnist.target = [0 if v=='0' else v for v in mnist.target]</kbd> – doing this for every numeral. But again, this (and other similar things) will work only for this case. Let's not do this.</li>
<li>We could use scikit-learn's <kbd>LabelEncoder()</kbd> method, which will take any list of labels and map them to a number. This will work for all cases.</li>
</ul>
<p>Let's use the <kbd>scikit</kbd> method by following these steps:</p>
<ol>
<li>Run the following code:</li>
</ol>
<pre style="padding-left: 60px">from sklearn import preprocessing<br/>le = preprocessing.LabelEncoder()<br/>print(sorted(list(set(mnist.target))))<br/><br/>le.fit(sorted(list(set(mnist.target))))</pre>
<p style="padding-left: 60px">This produces the following output:</p>
<pre style="padding-left: 60px">['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']<br/><br/>LabelEncoder()</pre>
<p style="padding-left: 60px">The <kbd>sorted(list(set(mnist.target)))</kbd> command does three things:</p>
<ul>
<li style="padding-left: 60px"><kbd>set(mnist.target)</kbd> retrieves the set of unique values in the data, for example, <kbd>{'8', '2', ..., '9'}</kbd>.</li>
<li style="padding-left: 60px"><kbd>list(set(mnist.target))</kbd> simply converts the set into a list because we need a list or an array for the <kbd>LabelEncoder()</kbd> method.</li>
<li style="padding-left: 60px"><kbd>sorted(list(set(mnist.target)))</kbd> is important here so that <em>0</em> maps to 0 and not to have <em>8</em> map to 0, and so on. It sorts the list, and the result looks like this - <kbd>['0', '1', ..., '9']</kbd>.</li>
</ul>
<p style="padding-left: 60px">The <kbd>le.fit()</kbd> method takes a list (or an array) and produces a map (a dictionary) to be used forward (and backward if needed) to encode labels, or strings, into numbers. It stores this in a <kbd>LabelEncoder</kbd> object.</p>
<ol start="2">
<li>Next, we could test the encoding as follows:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">print(le.transform(["9", "3", "7"]) )<br/><br/>list(le.inverse_transform([2, 2, 1]))</pre>
<p style="padding-left: 60px">This will output the following:</p>
<pre style="padding-left: 60px">[9 3 7]<br/><br/>['2', '2', '1']</pre>
<p style="padding-left: 60px">The <kbd>transform()</kbd> method transforms a string-based label into a number, whereas the <kbd>inverse_transform()</kbd> method takes a number and returns the corresponding string label or category. </p>
<div class="packt_tip">Any attempt to map to and from an unseen category or number will cause a <kbd>LabelEncoder</kbd> object to produce an error. Please be diligent in providing the list of all possible categories to the best of your knowledge.</div>
<ol start="3">
<li>Once the <kbd>LabelEncoder</kbd> object is fitted and tested, we can simply run the following instruction to encode the data:</li>
</ol>
<pre style="padding-left: 60px">print("Before ", mnist.target[:3])<br/>y = le.transform(mnist.target)<br/>print("After ", y[:3])</pre>
<p style="padding-left: 60px">This will output the following:</p>
<pre style="padding-left: 60px">Before ['5' '0' '4']<br/>After [5 0 4]</pre>
<p>The new encoded labels are now in <kbd>y</kbd> and ready to be used.</p>
<div class="packt_infobox">This method of encoding a label to an integer is also known as <strong>Ordinal Encoding.</strong></div>
<p>This methodology should work for all labels encoded as strings, for which you can simply map to numbers without losing context. In the case of the <kbd>MNIST</kbd> dataset, we can map <em>0</em> to 0 and <em>7</em> to 7 without losing context. Other examples of when you can do this include the following:</p>
<ul>
<li><strong>Age groups</strong>: ['18-21', '22-35', '36+'] to [0, 1, 2]</li>
<li><strong>Gender</strong>: ['male', 'female'] to [0, 1]</li>
<li><strong>Colors</strong>: ['red', 'black', 'blue', ...] to [0, 1, 2, ...]</li>
<li><strong>Studies</strong>: ['primary', 'secondary', 'high school', 'university'] to [0, 1, 2, 3]</li>
</ul>
<p>However, we are making one big assumption here: the labels encode no special meaning in themselves. As we mentioned earlier, zip codes could be simply encoded to smaller numbers; however, they have a geographical meaning, and doing so might negatively impact the performance of our deep learning algorithms. Similarly, in the preceding list, if studies require a special meaning that indicates that a <em>university</em> degree is much higher or more important than a <em>primary</em> degree, then perhaps we should consider different number mappings. Or perhaps we want our learning algorithms to <em>learn</em> such intricacies by themselves! In such cases, we should then use the well-known strategy of one-hot encoding.</p>
<h2 id="uuid-9bb88236-567f-47a8-ae37-648b5075217f">Converting categories to one-hot encoding</h2>
<p>Converting categories to one-hot encoding is better in most cases in which the categories or labels may have special meanings with respect to each other. In such cases, it has been reported to outperform ordinal encoding [Potdar, K., <em>et al.</em> (2017)]. </p>
<p>The idea is to represent each label as a Boolean state having independent columns. Take, for example, a column with the following data:</p>
<table style="border-collapse: collapse;width: 100%;height: 200px" border="1">
<tbody>
<tr>
<td style="width: 281px" class="CDPAlignCenter CDPAlign">
<p><strong>Gender</strong></p>
</td>
</tr>
<tr>
<td style="width: 281px" class="CDPAlignCenter CDPAlign">
<p>'female'</p>
</td>
</tr>
<tr>
<td style="width: 281px" class="CDPAlignCenter CDPAlign">
<p>'male'</p>
</td>
</tr>
<tr>
<td style="width: 281px" class="CDPAlignCenter CDPAlign">
<p>'male'</p>
</td>
</tr>
<tr>
<td style="width: 281px" class="CDPAlignCenter CDPAlign">
<p>'female'</p>
</td>
</tr>
<tr>
<td style="width: 281px" class="CDPAlignCenter CDPAlign">
<p>'female'</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>This can be uniquely transformed, using one-hot encoding, into the following new piece of data:</p>
<table style="border-collapse: collapse;width: 100%;height: 195px" border="1">
<tbody>
<tr>
<td style="width: 144.4px" class="CDPAlignCenter CDPAlign">
<p><strong>Gender_Female</strong></p>
</td>
<td style="width: 125.2px" class="CDPAlignCenter CDPAlign">
<p><strong>Gender_Male</strong></p>
</td>
</tr>
<tr>
<td style="width: 144.4px" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td style="width: 125.2px" class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
</tr>
<tr>
<td style="width: 144.4px" class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td style="width: 125.2px" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
</tr>
<tr>
<td style="width: 144.4px" class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
<td style="width: 125.2px" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
</tr>
<tr>
<td style="width: 144.4px" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td style="width: 125.2px" class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
</tr>
<tr>
<td style="width: 144.4px" class="CDPAlignCenter CDPAlign">
<p>1</p>
</td>
<td style="width: 125.2px" class="CDPAlignCenter CDPAlign">
<p>0</p>
</td>
</tr>
</tbody>
</table>
<p> </p>
<p>As you can see, the binary bit is <em>hot</em> (is one) only if the label corresponds to that specific row and it is zero otherwise. Notice also that we renamed the columns to keep track of which label corresponds to which column; however, this is merely a recommended format and is not a formal rule.</p>
<p>There are a number of ways we can do this in Python. If your data is in a pandas DataFrame, then you can simply do <kbd>pd.get_dummies(df, prefix=['Gender'])</kbd>, assuming your column is in <kbd>df</kbd> and you want to use <kbd>Gender</kbd> as a prefix.</p>
<p>To reproduce the exact results as discussed in the preceding table, follow these steps:</p>
<ol>
<li>Run the following command:</li>
</ol>
<pre style="padding-left: 60px">import pandas as pd<br/>df=pd.DataFrame({'Gender': ['female','male','male',<br/>                            'female','female']})<br/>print(df)</pre>
<p style="padding-left: 60px">This will output the following:</p>
<pre style="padding-left: 60px">  Gender<br/>0 female<br/>1 male<br/>2 male<br/>3 female<br/>4 female</pre>
<ol start="2">
<li>Now simply do the encoding by running the following command:</li>
</ol>
<pre style="padding-left: 60px">pd.get_dummies(df, prefix=['Gender'])</pre>
<p style="padding-left: 60px">And this is produced:</p>
<pre style="padding-left: 60px">  Gender_female  Gender_male<br/>0             1            0<br/>1             0            1<br/>2             0            1<br/>3             1            0<br/>4             1            0</pre>
<div class="packt_infobox">A fun, and perhaps obvious, property of this encoding is that the <kbd>OR</kbd> and <kbd>XOR</kbd> operations along the rows of all the encoded columns will always be one, and the <kbd>AND</kbd> operation will yield zeros.</div>
<p style="padding-left: 60px">For cases in which the data is not a pandas DataFrame, for example, MNIST targets, we can use scikit-learn's <kbd>OneHotEncoder.transform()</kbd> method. </p>
<p style="padding-left: 60px">A <kbd>OneHotEncoder</kbd> object has a constructor that will automatically initialize everything to reasonable assumptions and determines most of its parameters using the <kbd>fit()</kbd> method. It determines the size of the data, the different labels that exist in the data, and then creates a dynamic mapping that we can use with the <kbd>transform()</kbd> method. </p>
<p style="padding-left: 60px">To do a one-hot encoding of the <kbd>MNIST</kbd> targets, we can do this:</p>
<pre style="padding-left: 60px">from sklearn.preprocessing import OneHotEncoder<br/>enc = OneHotEncoder()<br/>y = [list(v) for v in mnist.target] # reformat for sklearn<br/>enc.fit(y)<br/><br/>print('Before: ', y[0])<br/>y = enc.transform(y).toarray()<br/>print('After: ', y[0])<br/>print(enc.get_feature_names())</pre>
<p style="padding-left: 60px">This will output the following:</p>
<pre style="padding-left: 60px">Before: ['5']<br/>After: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]<br/>['x0_0' 'x0_1' 'x0_2' 'x0_3' 'x0_4' 'x0_5' 'x0_6' 'x0_7' 'x0_8' 'x0_9']</pre>
<p>This code includes our classic sanity check in which we verify that label <kbd>'5'</kbd> was in fact converted to a row vector with 10 columns, of which number <kbd>6</kbd> is <em>hot</em>. It works, as expected. The new dimensionality of <kbd>y</kbd> is <em>n</em> rows and 10 columns.</p>
<div class="packt_tip">This is the preferred format for the targets that use deep learning methods on MNIST. One-hot encoding targets are great for neural networks that will have exactly one neuron per class. In this case, one neuron per digit. Each neuron will need to learn to predict one-hot encoded behavior, that is, only one neuron should fire up (be "hot") while the others should be inhibited. </div>
<p> </p>
<p>The preceding process can be repeated exactly to convert any other columns into one-hot encoding, provided that they contain categorical data. </p>
<p>Categories, labels, and specific mappings to integers or bits are very helpful when we want to classify input data into those categories, labels, or mappings. But what if we want to have input data that maps to continuous data? For example, data to predict a person's IQ by looking at their responses; or predicting the price of electricity depending on the input data about weather and the seasons. This is known as data for <strong>regression</strong>, which we will cover next.</p>
<h1 id="uuid-b4690847-d723-45a2-be12-84f622b825d2">Real-valued data and univariate regression</h1>
<p>Knowing how to deal with categorical data is very important when using classification models based on deep learning; however, knowing how to prepare data for regression is as important. Data that contains continuous-like real values, such as temperature, prices, weight, speed, and others, is suitable for regression; that is, if we have a dataset with columns of different types of values, and one of those is real-valued data, we could perform regression on that column. This implies that we could use all the rest of the dataset to predict the values on that column. This is known as <strong>univariate regression</strong>, or regression on one variable.</p>
<p>Most machine learning methodologies work better if the data for regression is <strong>normalized</strong>. By that, we mean that the data will have special statistical properties that will make calculations more stable. This is critical for many deep learning algorithms that suffer from vanishing or exploding gradients (<span>Hanin, B. (2018)</span>). For example, in calculating a gradient in a neural network, an error needs to be propagated backward from the output layer to the input layer; but if the output layer has a large error and the range of values (that is their <strong>distribution</strong>) is also large, then the multiplications going backward can cause overflow on variables, which would ruin the training process. </p>
<p>To overcome these difficulties, it is desirable to normalize the distribution of variables that can be used for regression, or variables that are real-valued. The normalization process has many variants, but we will limit our discussion to two main methodologies, one that sets specific statistical properties of the data, and one that sets specific ranges on the data.</p>
<h2 id="uuid-8a9b6f4d-2c8a-4ff5-bd69-243eea437400">Scaling to a specific range of values</h2>
<p>Let's go back to the heart disease dataset discussed earlier in this chapter. If you pay attention, many of those variables are real-valued and would be ideal for regression; for example, <em>x</em><sub>5</sub> and <em>x</em><sub>10</sub>. </p>
<div class="packt_tip">All variables are suitable for regression. This means that, technically, we can predict on any numeric data. The fact that some values are real-valued makes them more appealing for regression for a number of reasons. For example, the fact that the values in that column have a meaning that goes beyond integers and natural numbers. </div>
<p>Let's focus on <em>x</em><sub>5 </sub>and<span> </span><em>x</em><sub>10</sub>, which are the variables for measuring the cholesterol level and ST depression induced by exercise relative to rest, respectively. What if we want to change the original research question the doctors intended, which was to study heart disease based on different factors? What if now we want to use all the factors, including knowing whether patients have heart disease or not, to determine or predict their cholesterol level? We can do that with regression on <em>x</em><sub>5</sub>.</p>
<p>So, to prepare the data on <em>x</em><sub>5</sub> and<span> </span><em>x</em><sub>10</sub>, we will go ahead and scale the data. For verification purposes, we will retrieve descriptive statistics on the data before and after the scaling of the data.</p>
<p>To reload the dataset and display descriptive statistics, we can do the following:</p>
<pre>df = pd.read_csv('processed.cleveland.data', header=None)<br/>df[[4,9]].describe()</pre>
<p>In this case, index, <kbd>4</kbd> and <kbd>9</kbd> correspond to<span> </span><em>x</em><sub>5</sub><span> and</span><span> </span><em>x</em><sub>10</sub>, and the <kbd>describe()</kbd> method outputs the following information:</p>
<pre>                 4            9<br/>count   303.000000   303.000000<br/>mean    246.693069     1.039604<br/>std      51.776918     1.161075<br/>min     <strong>126.000000</strong>     <strong>0.000000</strong><br/>25%     211.000000     0.000000<br/>50%     241.000000     0.800000<br/>75%     275.000000     1.600000<br/>max     <strong>564.000000</strong>     <strong>6.200000</strong></pre>
<p>The most notable properties are the mean, and maximum/minimum values contained in that column. These will change once we scale the data to a different range. If we visualize the data as a scatter plot with respective histograms, it looks like <em>Figure 3.2</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/71a5370e-2596-445b-8ad1-90ec5f7e721a.png" style="width:36.17em;height:33.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.2 – Scatter plot of the two columns<span> </span><em>x</em><sub>5</sub><span> and</span><span> </span><em>x</em><sub>10</sub> and their corresponding histograms</div>
<p>As can be seen from <em>Figure 3.2</em>, the ranges are quite different, and the distribution of the data is different as well. The new desired range here is a minimum of 0 and a maximum of 1. This range is typical when we scale the data. And it can be achieved using scikit-learn's <kbd>MinMaxScaler</kbd> object as follows:</p>
<pre>from sklearn.preprocessing import MinMaxScaler<br/>scaler = MinMaxScaler()<br/>scaler.fit(df[[4,9]])<br/>df[[4,9]] = scaler.transform(df[[4,9]])<br/>df[[4,9]].describe()</pre>
<p>This will output the following:</p>
<pre>                4            9<br/>count  303.000000   303.000000<br/>mean     0.275555     0.167678<br/>std      0.118212     0.187270<br/>min      <strong>0.000000</strong>     <strong>0.000000</strong><br/>25%      0.194064     0.000000<br/>50%      0.262557     0.129032<br/>75%      0.340183     0.258065<br/>max      <strong>1.000000</strong>     <strong>1.000000</strong></pre>
<p>What the <kbd>fit()</kbd> method does internally is to determine what the current min and max values are for the data. Then, the <kbd>transform()</kbd> method uses that information to remove the minimum and divide by the maximum to achieve the desired range. As can be seen, the new descriptive statistics have changed, which can be confirmed by looking at the range in the axes of <em>Figure 3.3</em>:</p>
<div class="CDPAlignCenter packt_figref CDPAlign"><img src="assets/3ddba0fa-3bb9-478d-a6f6-b4739023945d.png" style="width:37.08em;height:33.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.3 – <span>Scatter plot of the newly scaled columns</span><span> </span><em>x</em><sub>5</sub><span> and</span><span> </span><em>x</em><sub>10</sub><span> and their corresponding histograms </span></div>
<p>Notice, however, if you pay close attention, that the distribution of the data has not changed. That is, the histograms of the data in <em>Figure 3.2</em> and <em>Figure 3.3</em> are still the same. And this is a very important fact because, usually, you do not want to change the distribution of the data.</p>
<h2 id="uuid-93b5c075-0cd0-47a4-bcd8-bc7b9441c32b">Standardizing to zero mean and unit variance</h2>
<p>Another way of preprocessing real-valued data is by making it have zero mean and unit variance. This process is referred to by many names, such as normalizing, z-scoring, centering, or standardizing.</p>
<p>Let's say that <strong><em>x</em></strong>=[<em>x</em><sub>5</sub><span>,</span><span> </span><em>x</em><sub>10</sub>], from our features above, then we can standardize <em><strong>x</strong></em><strong> </strong>as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/55d3b551-c4fa-4283-8e6c-ad964388b135.png" style="width:6.75em;height:2.83em;"/></p>
<p>Here, <em>µ</em> is a vector corresponding to the means of each column on <em><strong>x</strong></em>, and <em>σ</em> is a vector of standard deviations of each column in <em><strong>x</strong></em>.</p>
<p>After the standardization of <em><strong>x</strong></em>, if we recompute the mean and standard deviation, we should get a mean of zero and a standard deviation of one. In Python, we do the following:</p>
<pre>df[[4,9]] = (df[[4,9]]-df[[4,9]].mean())/df[[4,9]].std()<br/>df[[4,9]].describe()</pre>
<p>This will output the following:</p>
<pre>                   4                9<br/>count   3.030000e+02     3.030000e+02<br/>mean    <strong>1.700144e-16    -1.003964e-16</strong><br/>std     <strong>1.000000e+00     1.000000e+00</strong><br/>min    -2.331021e+00    -8.953805e-01<br/>25%    -6.893626e-01    -8.953805e-01<br/>50%    -1.099538e-01    -2.063639e-01<br/>75%     5.467095e-01     4.826527e-01<br/>max     6.128347e+00     4.444498e+00</pre>
<p>Notice that after normalization, the mean is, for numerical purposes, zero. And the standard deviation is one. The same thing can be done, of course, using the scikit-learn <kbd>StandardScaler</kbd> object as follows:</p>
<pre>from sklearn.preprocessing import StandardScaler<br/>scaler = StandardScaler()<br/>scaler.fit(df[[4,9]])<br/>df[[4,9]] = scaler.transform(df[[4,9]])</pre>
<p>This will yield the same results with negligible numerical differences. For practical purposes, both methods will achieve the same thing.</p>
<div class="packt_tip">Although both ways of normalizing are appropriate, in the DataFrame directly or using a <kbd>StandardScaler</kbd> object, you should prefer using the <span><kbd>StandardScaler</kbd> object if you are working on a production application. Once the <kbd>StandardScaler</kbd> object uses the <kbd>fit()</kbd> method, it can be used on new, unseen, data easily by re-invoking <kbd>transform()</kbd> method; however, if we do it directly on the pandas DataFrame, we will have to manually store the mean and standard deviation somewhere and reload it every time we need to standardize new data.</span></div>
<p> </p>
<p>Now, for comparison purposes, <em>Figure 3.4</em> depicts the new ranges after the normalization of the data. If you look at the axes closely, you will notice that the position of the zero values are where most of the data is, that is, where the mean is. Therefore, the cluster of data is centered around a mean of zero:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/fbe8723e-6944-46f1-90e9-3656d0333c96.png" style="width:36.42em;height:33.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 3.4 – </span><span>Scatter plot of the standardized columns</span><span> </span><em>x</em><sub>5</sub><span> and</span><span> </span><em>x</em><sub>10</sub><span> and their corresponding histograms </span></div>
<p>Notice, again, that in <em>Figure 3.4</em>, after applying the standardization process, the distribution of the data still does not change. But what if you actually want to change the distribution of the data? Keep reading on to the next section.</p>
<h1 id="uuid-80eced41-84b0-479f-9da3-83c79ef8df78">Altering the distribution of data</h1>
<p>It has been demonstrated that changing the distribution of the targets, particularly in the case of regression, can have positive benefits in the performance of a learning algorithm (Andrews, D. F., et al. (1971)).</p>
<p>Here, we'll discuss one particularly useful transformation known as <strong>Quantile Transformation</strong>. This methodology aims to look at the data and manipulate it in such a way that its histogram follows either a <strong>normal</strong> distribution or a <strong>uniform</strong> distribution. It achieves this by looking at estimates of quantiles. </p>
<p>We can use the following commands to transform the same data as in the previous section:</p>
<pre>from sklearn.preprocessing import QuantileTransformer<br/>transformer = QuantileTransformer(output_distribution='normal')<br/>df[[4,9]] = transformer.fit_transform(df[[4,9]])</pre>
<p>This will effectively map the data into a new distribution, namely, a normal distribution. </p>
<div class="packt_infobox">Here, the term <strong>normal distribution</strong> refers to a Gaussian-like <strong>probability density function</strong> (<strong>PDF</strong>). This is a classic distribution found in any statistics textbook. It is usually identified by its bell-like shape when plotted. </div>
<p>Note that we are also using the <kbd>fit_transform()</kbd> method, which does both <kbd>fit()</kbd> and <kbd>transform()</kbd> at the same time, which is convenient.</p>
<p>As can be seen in <em>Figure 3.5</em>, the variable related to cholesterol data, <em>x</em><sub>5</sub>, was easily transformed into a normal distribution with a bell shape. However, for <em>x</em><sub>10</sub>, the heavy presence of data in a particular region causes the distribution to have a bell shape, but with a long tail, which is not ideal:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/86423dc1-72ee-4d89-b0dc-0a3c8fbabc76.png" style="width:44.08em;height:40.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.5 –<span> </span><span>Scatter plot of the normally transformed columns</span><span> </span><em>x</em><sub>5</sub><span> and</span><span> </span><em>x</em><sub>10</sub><span> and their corresponding Gaussian-like histograms </span></div>
<p>The process of transforming the data for a uniform distribution is very similar. We simply need to make a small change in one line, on the <kbd>QuantileTransformer()</kbd> constructor, as follows:</p>
<pre>transformer = QuantileTransformer(output_distribution='<strong>uniform</strong>')</pre>
<p>Now, the data is transformed into a uniform distribution, as shown in <em>Figure 3.6</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/5052596d-f5ba-4467-b5f6-19630b3510d8.png" style="width:43.92em;height:39.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 3.6 –</span><span> </span><span>Scatter plot of the uniformly transformed columns</span><span> </span><em>x</em><sub>5</sub><span> and</span><span> </span><em>x</em><sub>10</sub><span> and their corresponding uniform histograms </span></div>
<p>From the figure, we can see that the data has been uniformly distributed across each variable. Once again, the clustering of data in a particular region has the effect of causing a large concentration of values in the same space, which is not ideal. This artifact also creates a gap in the distribution of the data that is usually difficult to handle, unless we use techniques to augment the data, which we'll discuss next.</p>
<h1 id="uuid-ae0ca1cc-a3c8-4aa8-ac24-0e0c2004a8a1">Data augmentation</h1>
<p>Now that you have learned how to process the data to have specific distributions, it is important for you to know about data augmentation, which is usually associated with missing data or high-dimensional data. Traditional machine learning algorithms may have problems dealing with data where the number of dimensions surpasses the number of samples available. The problem is not particular to all deep learning algorithms, but some algorithms have a much more difficult time learning to model a problem that has more variables to figure out than samples to work on. We have a few options to correct that: either we reduce the dimensions or variables (see the following section) or we increase the samples in our dataset (this section).</p>
<p>One of the tools for adding more data is known as <strong>data augmentation </strong>(Van Dyk, D. A., and Meng, X. L. (2001)).  In this section, we will use the <kbd>MNIST</kbd> dataset to exemplify a few techniques for data augmentation that are particular to images but can be conceptually extended to other types of data.</p>
<p>We will cover the basics: adding noise, rotating, and rescaling. That is, from one original example, we will produce three new, different images of numerals. We will use the image processing library known as <kbd>scikit image</kbd>.</p>
<h2 id="uuid-d86207e8-2f01-4fb6-84d5-19179805b3ac">Rescaling</h2>
<p>We begin by reloading the <kbd>MNIST</kbd> dataset as we have done before:</p>
<pre>from sklearn.datasets import fetch_openml<br/>mnist = fetch_openml('mnist_784')</pre>
<p>Then we can simply invoke the <kbd>rescale()</kbd> method to create a rescaled image. The whole purpose behind resizing an image is to rescale it back to its original size because this makes the image look like a small resolution image of the original. It loses some of its characteristics in the process, but it can actually make a more robust deep learning model. That is, a model robust to the scale of objects, or in this case, the scale of numerals: </p>
<pre>from skimage.transform import rescale<br/>x = mnist.data[0].reshape(28,28)</pre>
<p>Once we have <kbd>x</kbd> as the original image from which we will augment, we can do the scaling down and up as follows:</p>
<pre class="mce-root">s = rescale(x, 0.5, multichannel=False)<br/>x_= rescale(s, 2.0, multichannel=False)</pre>
<p>Here, the augmented image (rescaled) is in <kbd>x_</kbd><em>.  </em>Notice that, in this case, the image is downscaled by a factor of two (50%) and then upscaled, also by a factor of two (200%). The <kbd>multichannel</kbd> argument is set to <kbd>false</kbd> since the images have only one single channel, meaning they are grayscale.</p>
<div class="packt_tip">When rescaling, be careful of rescaling by factors that give you exact divisions. For example, a 28 x 28 image that is downscaled by a factor of 0.5 goes down to 14 x 14; this is good. But if we downscale by a factor of 0.3, it will go down to 8.4 x 8.4, which goes up to 9 x 9; this is not good because it can add unnecessary complications. Keep it simple.</div>
<p>Besides rescaling, we can also modify the existing data slightly so as to have variations of the existing data without deviating much from the original, as we'll discuss next.</p>
<h2 id="uuid-6ea184e9-21ae-43b6-a86b-abe9feb55fb3">Adding noise</h2>
<p>Similarly, we can also contaminate the original image with additive Gaussian noise. This creates random patterns all over the image to simulate a camera problem or noisy acquisition. Here, we use it to also augment our dataset and, in the end, to produce a deep learning model that is robust against noise.</p>
<p>For this, we use the <kbd>random_noise()</kbd> method as follows:</p>
<pre>from skimage.util import random_noise<br/>x_ = random_noise(x)</pre>
<p>Once again, the augmented image (noisy) is in <kbd>x_</kbd>. </p>
<p>Besides noise, we can also change the perspective of an image slightly so as to preserve the original shape at a different angle, as we'll discuss next.</p>
<h2 id="uuid-102e01bc-e2f5-4cc5-b410-3a1bc39a6f6d">Rotating</h2>
<p>We can use a plain rotation effect on the images to have even more data. The rotation of images is a crucial part of learning good features from images. Larger datasets contain, naturally, many versions of images that are slightly rotated or fully rotated. If we do not have such images in our dataset, we can manually rotate them and augment our data.</p>
<p>For this, we use the <kbd>rotate()</kbd> method like so:</p>
<pre>from skimage.transform import rotate<br/>x_ = rotate(x, 22)</pre>
<p>In this example, the number <kbd>22</kbd> specifies the angle of rotation: </p>
<div class="packt_tip">When you are augmenting your dataset, you may want to consider having multiple rotations at random angles.</div>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/039c1b15-877e-421d-884e-3903881019c0.png" style="width:18.25em;height:11.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.7 – A<span>n example of the images produced with the preceding data augmentation techniques</span></div>
<p class="CDPAlignLeft CDPAlign">The first column is the original numeral of the MNIST dataset. The second column shows the effect of rescaling. The third column shows the original plus additive Gaussian noise. The last column shows a rotation of 20 degrees <span>(top)</span> and -20 degrees (bottom).</p>
<h2 id="uuid-1aa57c95-16ef-4204-9995-29e32639657a">Other augmentation techniques</h2>
<p>For image datasets, there are other ideas for augmenting data that include the following:</p>
<ul>
<li>Changing the projection of the image</li>
<li>Adding compression noise (quantizing the image)</li>
<li>Other types of noise besides Gaussian, such as salt and pepper, or multiplicative noise</li>
<li>The translation of the image by different distances at random</li>
</ul>
<p>But the most robust augmentation would be a combination of all of these!</p>
<p>Images are fun because they are highly correlated in local areas. But for general non-image datasets, such as the heart disease dataset, we can augment data in other ways, for example:</p>
<ul>
<li>Adding low-variance Gaussian noise </li>
<li>Adding compression noise (quantization)</li>
<li>Drawing new points from a calculated probability density function over the data</li>
</ul>
<p>For other special datasets, such as text-based data, we can also do the following:</p>
<ul>
<li>Replace some words with synonyms</li>
<li>Remove some words</li>
<li>Add words that contain errors</li>
<li>Remove punctuation (only if you do not care about proper language structures)</li>
</ul>
<p>For more information on this and many other augmentation techniques, consult online resources on the latest advances pertaining to your specific type of data.</p>
<p>Let's now dive into some techniques for dimensionality reduction that can be used to alleviate the problem of high-dimensional and highly correlated datasets.</p>
<h1 id="uuid-f2bcd2c2-0bf4-4b94-8ec9-12848276a5e9">Data dimensionality reduction</h1>
<p>As pointed out before, if we have the problem of having more dimensions (or variables) than samples in our data, we can either augment the data or reduce the dimensionality of the data. Now, we will address the basics of the latter. </p>
<p>We will look into reducing dimensions both in supervised and unsupervised ways with both small and large datasets.</p>
<h2 id="uuid-662872cc-92d4-40b8-9938-f33e71757c94">Supervised algorithms</h2>
<p>Supervised algorithms for dimensionality reduction are so called because they take the labels of the data into account to find better representations. Such methods often yield good results. Perhaps the most popular kind is called <strong>linear discriminant analysis</strong> (<strong>LDA</strong>), which we'll discuss next.</p>
<h3 id="uuid-4049906e-3e15-4cd1-92ca-fc067ebf927f">Linear discriminant analysis</h3>
<p>Scikit learn has a <kbd>LinearDiscriminantAnalysis</kbd> class that can easily perform dimensionality reduction on a desired number of components.</p>
<p>By <strong>number of components</strong>, the number of dimensions desired is understood. The name comes from <strong>principal component analysis</strong> (<strong>PCA</strong>), which is a statistical approach that determines the eigenvectors and eigenvalues of the centered covariance matrix of a dataset; then, the largest eigenvalues associated with specific eigenvectors are known to be the most important, <em>principal</em>, components. When we use PCA to reduce to a specific number of components, we say that we want to keep those components that are the most important in a space induced by the eigenvalues and eigenvectors of the covariance matrix of the data. </p>
<p>LDA and other dimensionality reduction techniques also have a similar philosophy in which they aim to find low-dimensional spaces (based on the number of components desired) that can better represent the data based on other properties of the data. </p>
<p>If we use the heart disease dataset as an example, we can perform LDA to reduce the entire dataset from 13 dimensions to 2 dimensions, all the while using the labels [0, 1, 2, 3, 4] to inform the LDA algorithm how to better separate the groups represented by those labels.</p>
<p>To achieve this, we can follow these steps:</p>
<ol>
<li>First, we reload the data and drop the missing values:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.discriminant_analysis import LinearDiscriminantAnalysis<br/>df = pd.read_csv('processed.cleveland.data', header=None)<br/>df = df.apply(pd.to_numeric, errors='coerce').dropna()</pre>
<p style="padding-left: 60px">Notice that we did not have to deal with missing values before on the heart disease dataset because pandas automatically ignores missing values. But here, because we are strictly converting data into numbers, missing values will be converted to <kbd>NaN</kbd> since we are specifying <kbd>errors='coerce'</kbd>, which forces any errors in the conversion to become <kbd>NaN</kbd>. Consequently, with <kbd>dropna()</kbd>, we ignore rows with those values from our dataset because they will cause LDA to fail.</p>
<ol start="2">
<li>Next, we prepare the <kbd>X</kbd> and <kbd>y</kbd> variables to contain the data and targets, respectively, and we perform LDA as follows:</li>
</ol>
<pre style="padding-left: 60px" class="mce-root">X = df[[0,1,2,3,4,5,6,7,8,9,10,11,12]].values<br/>y = df[13].values<br/><br/>dr = LinearDiscriminantAnalysis(n_components=2)<br/>X_ = dr.fit_transform(X, y)</pre>
<p>In this example, <kbd>X_</kbd> contains the entire dataset represented in two dimensions, as given by <kbd>n_components=2</kbd>. The choice of two components is simply to illustrate graphically how the data looks. But you can change this to any number of components you desire.</p>
<p><em>Figure 3.8</em> depicts how the 13-dimensional dataset looks if compressed, or reduced, down to two dimensions:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/81bc6b69-a2d6-4c3d-ab8a-038d7f2b0bbe.png" style="width:33.33em;height:30.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.8 – Reducing dimensions from 13 to 2 using LDA</div>
<p>Notice how the values with 0 (no heart disease) are mostly clustered toward the left side, while the rest of the values (that is, 1, 2, 3, and 4, which represent heart disease) seem to cluster toward the right side. This is a nice property that was not observed in <em>Figures</em> <em>3.2</em> to <em>3.6</em> when we picked two columns out of the 13. </p>
<p>Technically speaking, the relevant information of the 13 dimensions is still contained in the LDA-induced two dimensions. If the data seems to be separable in these low-dimensional representations, a deep learning algorithm may have a good chance of learning representations to classify or regress on the data with high performance. </p>
<p>While LDA can offer a very nice way to perform dimensionality reduction informed by the labels in the data, we might not always <span>have labeled data, or we may not want to use the labels that we have. In those cases </span>we can, and we should, explore other robust methodologies that require no label information, such as unsupervised techniques, which we'll discuss next.</p>
<h2 id="uuid-0c15a1c7-9302-4a6b-9f43-c6e295dd1de0"><span>Unsupervised</span> techniques</h2>
<p>Unsupervised techniques are the most popular methods because they need no prior information about labels. We begin with a kernelized version of PCA and then we move on to methods that operate on larger datasets.</p>
<h3 id="uuid-acaf113e-7904-44b8-bfe1-fb01a7081bc4">Kernel PCA</h3>
<p>This variant of PCA uses kernel methods to estimate distances, variances, and other parameters to determine the major components of the data (Schölkopf, B., et al. (1997)). It may take a bit more time to produce a solution than regular PCA, but it is very much worth using it over traditional PCA.</p>
<p>The <kbd>KernelPCA</kbd> class of scikit-learn can be used as follows:</p>
<pre>from sklearn.decomposition import KernelPCA<br/><br/>dr = KernelPCA(n_components=2, kernel='linear')<br/>X_ = dr.fit_transform(X)</pre>
<p class="mce-root">Again, we use two dimensions as the new space, and we use a <kbd>'linear'</kbd> kernel. Other popular choices for the kernel include the following:</p>
<ul>
<li><kbd>'rbf'</kbd> for a radial basis function kernel</li>
<li><kbd>'poly'</kbd> for a polynomial kernel</li>
</ul>
<div class="packt_tip">Personally, I like the <kbd>'rbf'</kbd> kernel in general, because it is more powerful and robust. But oftentimes, you spend valuable time trying to determine the best value for the parameter <em>γ</em>, which is how wide the bell of the radial basis function is. If you have the time, try <kbd>'rbf'</kbd> and experiment with the parameter <kbd>gamma</kbd>.</div>
<p class="mce-root"/>
<p>The result of using kernel PCA is shown in <em>Figure 3.9</em>. The diagram again shows a clustering arrangement of the negative class (no heart disease, a value of 0) toward the bottom left of the KPCA-induced space. The positive class (heart disease, values ≥ 1) tends to cluster upward: </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/88fdcb86-dfba-4e09-8355-7df6981cc28c.png" style="width:33.33em;height:30.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 3.9 – Reducing dimensions with kernel PCA from 13 down to 2</div>
<p>Compared to <em>Figure 3.8</em>, LDA produces a slightly better space where the groups can be separated. However, KPCA does a good job in spite of now knowing the actual target classes. Now, LDA and KPCA might take no time on small datasets, but what if we have a lot of data? We will discuss some options next.</p>
<h3 id="uuid-f01e6b39-b12e-48a6-bbfd-6e5d451c8172">Large datasets</h3>
<p>The previous examples will work well with moderate-sized datasets. However, when dealing with very large datasets, that is, with many dimensions or many samples, some algorithms may not function at their best. In the worst case, they will fail to produce a solution. The next two unsupervised algorithms are designed to function well for large datasets by using a technique called <strong>batch training</strong>. This technique is well known and has been applied in machine learning successfully (Hinton, G. E. (2012)). </p>
<p>The main idea is to divide the dataset into small (mini) batches and partially make progress toward finding a global solution to the problem at hand.</p>
<h4 id="uuid-a7d9f674-83a6-424b-bdad-a208204e2187">Sparse PCA</h4>
<p>We'll first look into a sparse-coding version of PCA available in scikit-learn as <kbd>MiniBatchSparsePCA</kbd>. This algorithm will determine the best transformation into a subspace that satisfies a sparsity constraint.</p>
<div class="packt_infobox"><strong>Sparsity</strong> is a property of matrices (or vectors) in which most of the elements are zeros. The opposite of sparsity is density. We like sparsity in deep learning because we do a lot of tensor (vector) multiplications, and if some of the elements are zeros, we do not have to perform those multiplications, thus saving time and optimizing for speed. </div>
<p>Follow the next steps in order to use the <kbd>MNIST</kbd> dataset and reduce its dimensions, since it has 784 dimensions and 70,000 samples. It is large enough, but even larger datasets can also be used:</p>
<ol>
<li>We begin by reloading the data and preparing it for the sparse PCA encoding:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.datasets import fetch_openml<br/>mnist = fetch_openml('mnist_784')<br/><br/>X = mnist.data</pre>
<ol start="2">
<li>Then we perform the dimensionality reduction as follows:</li>
</ol>
<pre style="padding-left: 60px">from sklearn.decomposition import MiniBatchSparsePCA<br/><br/>dr = MiniBatchSparsePCA(n_components=2, batch_size=50, <br/>                        normalize_components=True)<br/>X_ = dr.fit_transform(X)</pre>
<p>Here, the <kbd>MiniBatchSparsePCA()</kbd> constructor takes three arguments:</p>
<ul>
<li><kbd>n_components</kbd>, which we set to 2 for visualization purposes.</li>
<li><kbd>batch_size</kbd> determines how many samples the algorithm will use at a time. We set it to <kbd>50</kbd>, but larger numbers may cause the algorithm to slow down.</li>
<li><kbd>normalize_components</kbd> refers to the preprocessing of the data by <em>centering</em> it, that is, making it have a zero mean and a unit variance; we recommend doing this every time, especially if you have data that is highly correlated, such as images.</li>
</ul>
<p>The <kbd>MNIST</kbd> dataset transformed using sparse PCA looks as depicted in <em>Figure 3.10</em>:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/452ad7f6-1aeb-4443-a433-7b36d4a3e64c.png" style="width:27.67em;height:23.67em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.10 – MNIST dataset reduced to two dimensions using sparse PCA</div>
<p class="CDPAlignLeft CDPAlign">As you can see, the separation between classes is not perfectly clear. There are some definitive clusters of digits, but it does not seem like a straightforward task due to the overlap between groups. This is caused in part by the fact that many digits may look alike. It would make sense to have the numerals 1 and 7 clustered together (the left side up and down), or 3 and 8 (the middle and up). </p>
<p>But let's also use another popular and useful algorithm called Dictionary Learning.</p>
<h4 id="uuid-66da95de-1834-4e44-82d9-044a6d428228">Dictionary Learning</h4>
<p>Dictionary Learning is the process of learning the basis of transformations, called <strong>dictionaries</strong>, by using a process that can easily scale to very large datasets (Mairal, J., et al. (2009)).</p>
<div class="packt_infobox">This was not possible with PCA-based algorithms, but this technique remains powerful and recently received the <em>Test of Time</em> award at one of the major conferences in the world, the <em>2019</em> <em>Intern</em><em>ational Conference in Machine Learning. </em></div>
<p>The algorithm is available in scikit-learn through the <kbd>MiniBatchDictionaryLearning</kbd> class. We can use it as follows:</p>
<pre>from sklearn.decomposition import MiniBatchDictionaryLearning<br/><br/>dr = MiniBatchDictionaryLearning(n_components=2, batch_size=50)<br/>X_ = dr.fit_transform(X)</pre>
<p>The constructor <kbd>MiniBatchDictionaryLearning()</kbd> takes on similar arguments as <kbd>MiniBatchSparsePCA()</kbd> with the same meaning. The results of the learned space are shown in <em>Figure 3.11</em>:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/c2c39450-0acb-48c5-9e7a-a5fcad716e5b.png" style="width:30.25em;height:25.83em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 3.11 – Dimensionality reduction of MNIST data down to two dimensions using Dictionary Learning</div>
<p class="mce-root">As can be seen, there is a significant overlap among classes even if there are clearly defined clusters. This could lead to poor performance results if this data, the two-dimensional data, is used as input to train a classifier. This does not mean that algorithms are bad, necessarily. What this could mean is that, maybe, two dimensions are not the best choice of final dimensions. Continue reading to learn more about this.</p>
<h2 id="uuid-a2805412-e06b-4f09-b666-415ff74d99b0" class="mce-root">Regarding the number of dimensions</h2>
<p>Reducing dimensions is not always a necessary step. But it is highly recommended for data that is highly correlated, for example, images.</p>
<p>All the discussed dimensionality reduction techniques actually strive to remove redundant information in the data and preserve the important content. If we ask an algorithm to reduce the dimensions of our non-correlated, non-redundant dataset from 13 dimensions to 2, that sounds a bit risky; perhaps 8 or 9 would be a better choice.</p>
<p>No serious-minded machine learner would try to reduce a non-correlated, non-redundant dataset with 784 dimensions to only 2. Even if the data is highly correlated and redundant, like the <kbd>MNIST</kbd> dataset, asking to go from 784 down to 2 is a big stretch. It is a very risky decision that may get rid of important, discriminant, relevant information; perhaps 50 or 100 would be a better choice. </p>
<p>There is no general way of finding which amount of dimensions is good. It is a process that requires experimentation. If you want to become good at this, you must do your due diligence and at least try two or more experiments with different dimensions. </p>
<h1 id="uuid-f53316be-189a-4a3a-965c-8f3e1e8ef0fd">Ethical implications of manipulating data</h1>
<p><span>There are many ethical implications and risks when manipulating data that you need to know. We live in a world where most deep learning algorithms will have to be corrected, by re-training them, because it was found that they were biased or unfair. That is very unfortunate; you want to be a person who exercises responsible AI and produces carefully thought out models. </span></p>
<p><span>When manipulating data, be careful about removing outliers from the data just because you think they are decreasing your model's performance. Sometimes, outliers represent information about protected groups or minorities, and removing those perpetuates unfairness and introduces bias toward the majority groups. Avoid removing outliers unless you are absolutely sure that they are errors caused by faulty sensors or human error. </span></p>
<p><span>Be careful of the way you transform the distribution of the data. Altering the distribution is fine in most cases, but if you are dealing with demographic data, you need to pay close attention to what you are transforming.</span></p>
<p>When dealing with demographic information such as gender, encoding female and male as 0 and 1 could be risky if we are considering proportions; we need to be careful not to promote equality (or inequality) that does not reflect the reality of the community that will use your models. The exception is when our current reality shows unlawful discrimination, exclusion, and bias. Then, our models (based on our data) should not reflect this reality, but the lawful reality that our community wants. That is, we will prepare good data to create models not to perpetuate societal problems, but models that will reflect the society we want to become.</p>
<h1 id="uuid-06757e7e-0ed3-40bf-bf53-442f2edfee9d">Summary </h1>
<p>In this chapter, we discussed many data manipulation techniques that we will come back to use all the time. It is good for you to spend time doing this now rather than later. It will make our modeling of deep learning architectures easier. </p>
<p>After reading this chapter, you are now able to manipulate and produce binary data for classification or for feature representation. You also know how to deal with categorical data and labels and prepare it for classification or regression. When you have real-valued data, you now know how to identify statistical properties and how to normalize such data. If you ever have the problem of data that has non-normal or non-uniform distributions, now you know how to fix that. And if you ever encounter problems of not having enough data, you learned a few data augmentation techniques. Toward the end of this chapter, you learned some of the most popular dimensionality reduction techniques. You will learn more of these along the road, for example, when we talk about autoencoders, which can be used for dimensionality reduction as well. But sit tight, we will get there in due time.</p>
<p>For now, we will continue our journey toward the next introductory topic about basic machine learning. <a href="7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml">Chapter 4</a>,<span> </span><em>Learning from Data,</em><span> introduces the most elementary concepts around the theory of deep learning, including measuring performance on regression and classification, as well as the identification of overfitting. However,</span><span> before we go there, please try to quiz yourself with the following questions. </span></p>
<p class="mce-root"/>
<p class="mce-root"/>
<h1 id="uuid-e3f4a8b9-c6a9-46d2-997b-6ad417c7a0f1">Questions and answers</h1>
<ol>
<li class="mce-root"><strong>Which variables of the heart dataset are suitable for regression?</strong></li>
</ol>
<p style="padding-left: 60px" class="mce-root">Actually, all of them. But the ideal ones are those that are real-valued.</p>
<ol start="2">
<li class="mce-root"><strong>Does the scaling of the data change the distribution of the data? </strong></li>
</ol>
<p style="padding-left: 60px" class="mce-root">No. The distribution remains the same. Statistical metrics such as the mean and variance may change, but the distribution remains the same.</p>
<ol start="3">
<li class="mce-root"><strong>What is the main difference between supervised and unsupervised dimensionality reduction methods?</strong></li>
</ol>
<p style="padding-left: 60px" class="mce-root">Supervised algorithms use the target labels, while unsupervised algorithms do not need that information.</p>
<ol start="4">
<li class="mce-root"><strong>When is it better to use batch-based dimensionality reduction?</strong></li>
</ol>
<p style="padding-left: 60px" class="mce-root">When you have very large datasets.</p>
<h1 id="uuid-1d13a967-220b-4249-b5ed-4d0a507653b8">References</h1>
<ul>
<li><span>Cleveland Heart Disease Dataset (1988). Principal investigators:<br/>
a. Hungarian Institute of Cardiology. Budapest: Andras Janosi, M.D.<br/>
b. University Hospital, Zurich, Switzerland: William Steinbrunn, M.D.<br/>
c. University Hospital, Basel, Switzerland: Matthias Pfisterer, M.D.<br/>
d. V.A. Medical Center, Long Beach and Cleveland Clinic Foundation: Robert Detrano, M.D., Ph.D.</span></li>
<li><span>Detrano, R., Janosi, A., Steinbrunn, W., Pfisterer, M., Schmid, J.J., Sandhu, S., Guppy, K.H., Lee, S. and Froelicher, V., (1989). International application of a new probability algorithm for the diagnosis of coronary artery disease. </span><em>The American journal of cardiology</em><span>, </span>64<span>(5), 304-310.</span></li>
<li><span>Deng, L. (2012). The MNIST database of handwritten digit images for machine learning research (best of the web). </span><em>IEEE Signal Processing Magazine</em><span>, </span>29<span>(6), 141-142.</span></li>
<li>Sezgin, M., and Sankur, B. (2004). Survey over image thresholding techniques and quantitative performance evaluation. <em>Journal of Electronic imaging</em>, 13(1), 146-166.</li>
</ul>
<ul>
<li>Potdar, K., Pardawala, T. S., and Pai, C. D. (2017). A comparative study of categorical variable encoding techniques for neural network classifiers. <em>International Journal of Computer Applications</em>, 175(4), 7-9.</li>
<li><span>Hanin, B. (2018). Which neural net architectures give rise to exploding and vanishing gradients?. In<em> </em></span><em>Advances in Neural Information Processing Systems</em><span> (pp. 582-591).</span></li>
<li>Andrews, D. F., Gnanadesikan, R., and Warner, J. L. (1971). Transformations of multivariate data. <em>Biometrics</em>, 825-840.</li>
<li><span>Van Dyk, D. A., and Meng, X. L. (2001). The art of data augmentation. </span><em>Journal of Computational and Graphical Statistics</em><span>, </span>10<span>(1), 1-50.</span></li>
<li>Schölkopf, B., Smola, A., and Müller, K. R. (1997, October). Kernel principal component analysis. In <em>International conference on artificial neural networks</em> (pp. 583-588). Springer, Berlin, Heidelberg.</li>
<li>Hinton, G. E. (2012). A practical guide to training restricted Boltzmann machines. In <em>Neural networks: Tricks of the trade</em> (pp. 599-619). Springer, Berlin, Heidelberg.</li>
<li>Mairal, J., Bach, F., Ponce, J., and Sapiro, G. (June, 2009). Online dictionary learning for sparse coding. In <em>Proceedings of the 26th annual international conference on machine learning</em> (pp. 689-696). ACM.</li>
</ul>


            </article>

            
        </section>
    </body></html>