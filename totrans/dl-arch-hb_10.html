<html><head></head><body>
<div id="_idContainer107">
<h1 class="chapter-number" id="_idParaDest-155"><a id="_idTextAnchor160"/><a id="_idTextAnchor161"/><span class="koboSpan" id="kobo.1.1">10</span></h1>
<h1 id="_idParaDest-156"><a id="_idTextAnchor162"/><span class="koboSpan" id="kobo.2.1">Exploring Model Evaluation Methods</span></h1>
<p><span class="koboSpan" id="kobo.3.1">A trained deep learning model without any form of validation cannot be deployed to production. </span><span class="koboSpan" id="kobo.3.2">Production, in the context of the machine learning software domain, refers to the deployment and operation of a machine learning model in a live environment for actual consumption of its predictions. </span><span class="koboSpan" id="kobo.3.3">More broadly, model evaluation serves as a critical component in any deep learning project. </span><span class="koboSpan" id="kobo.3.4">Typically, a deep learning project will result in many models being built, and a final model will be chosen to serve in a production environment. </span><span class="koboSpan" id="kobo.3.5">A good model evaluation process for any project leads to </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.5.1">A better-performing final model through model comparisons </span><span class="No-Break"><span class="koboSpan" id="kobo.6.1">and metrics</span></span></li>
<li><span class="koboSpan" id="kobo.7.1">Fewer production prediction mishaps by understanding common </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">model pitfalls</span></span></li>
<li><span class="koboSpan" id="kobo.9.1">More </span><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.10.1">closely aligned practitioner and final model behaviors through </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">model insights</span></span></li>
<li><span class="koboSpan" id="kobo.12.1">A higher probability of project success through success </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">metric evaluation</span></span></li>
<li><span class="koboSpan" id="kobo.14.1">A final model that is less biased and fairer and produces more </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">trusted predictions</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.16.1">Generally, the model evaluation process leads to more informed decisions across the entire machine learning life cycle. </span><span class="koboSpan" id="kobo.16.2">In this first chapter of </span><em class="italic"><span class="koboSpan" id="kobo.17.1">Part 2</span></em><span class="koboSpan" id="kobo.18.1"> of the book, we will discuss all the categories of model evaluation that will help to achieve these benefits. </span><span class="koboSpan" id="kobo.18.2">Additionally, we will dive deep into some of these categories in the next four chapters, which all belong to </span><em class="italic"><span class="koboSpan" id="kobo.19.1">Part 2</span></em><span class="koboSpan" id="kobo.20.1"> of the book. </span><span class="koboSpan" id="kobo.20.2">Specifically, we will cover the following topics in </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.22.1">Exploring the different model </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">evaluation methods</span></span></li>
<li><span class="koboSpan" id="kobo.24.1">Engineering the base model </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">evaluation metric</span></span></li>
<li><span class="koboSpan" id="kobo.26.1">Exploring custom metrics and </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">their applications</span></span></li>
<li><span class="koboSpan" id="kobo.28.1">Exploring statistical tests for comparing </span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">model metrics</span></span></li>
<li><span class="koboSpan" id="kobo.30.1">Relating the evaluation metric </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">to success</span></span></li>
<li><span class="koboSpan" id="kobo.32.1">Directly optimizing </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">the metric</span></span></li>
</ul>
<h1 id="_idParaDest-157"><a id="_idTextAnchor164"/><span class="koboSpan" id="kobo.34.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.35.1">For this chapter, we will have a practical implementation using the Python programming language. </span><span class="koboSpan" id="kobo.35.2">To complete it, you will only need to install the </span><strong class="source-inline"><span class="koboSpan" id="kobo.36.1">matplotlib</span></strong><span class="koboSpan" id="kobo.37.1"> library </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">in Python.</span></span></p>
<p><span class="koboSpan" id="kobo.39.1">The code files are available on GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">at </span></span><a href="https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_10"><span class="No-Break"><span class="koboSpan" id="kobo.41.1">https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_10</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.42.1">.</span></span></p>
<h1 id="_idParaDest-158"><a id="_idTextAnchor165"/><span class="koboSpan" id="kobo.43.1">Exploring the different model evaluation methods</span></h1>
<p><span class="koboSpan" id="kobo.44.1">Most practitioners </span><a id="_idIndexMarker773"/><span class="koboSpan" id="kobo.45.1">are familiar with accuracy-related metrics. </span><span class="koboSpan" id="kobo.45.2">This is the most basic evaluation method. </span><span class="koboSpan" id="kobo.45.3">Typically, for supervised problems, a practitioner will treat an accuracy-related metric as the golden source of truth. </span><span class="koboSpan" id="kobo.45.4">In the context of model evaluation, the term “accuracy metrics” is often used to collectively refer to various performance metrics such as accuracy, F1 score, recall, precision, and mean squared error. </span><span class="koboSpan" id="kobo.45.5">When coupled with a suitable cross-validation partitioning strategy, using metrics as a standalone evaluation strategy can go a long way in most projects. </span><span class="koboSpan" id="kobo.45.6">In deep learning, accuracy-related metrics are typically used to monitor the progress of the model at each epoch. </span><span class="koboSpan" id="kobo.45.7">The monitoring process can subsequently be extended to perform early stopping to stop training the model when it doesn’t improve anymore and to determine when to reduce the learning rate. </span><span class="koboSpan" id="kobo.45.8">Additionally, the best model weights can be loaded at the end of the training process defined by the weights that achieved the best metric score on the </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">validation dataset.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">Accuracy-related metrics alone do not provide a complete picture of a machine learning model’s capabilities and behavior. </span><span class="koboSpan" id="kobo.47.2">This is particularly true in unsupervised projects where accuracy metrics are superficial and only relevant to specific distributions. </span><span class="koboSpan" id="kobo.47.3">Gaining a more complete understanding of a model allows you to make more informed decisions across the machine’s life cycle. </span><span class="koboSpan" id="kobo.47.4">Some examples of the ways you can gain more understanding of the model includethe following: </span></p>
<ul>
<li><span class="koboSpan" id="kobo.48.1">Model insights can be a proxy to assess the accuracy of the data being used. </span><span class="koboSpan" id="kobo.48.2">If the data is deemed to be inaccurate or has some slight flaws, you can transition back into the data preparation stage of the machine learning </span><span class="No-Break"><span class="koboSpan" id="kobo.49.1">life cycle.</span></span></li>
<li><span class="koboSpan" id="kobo.50.1">In cases where bias is detected, the model may need to be retrained to remove </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">the bias.</span></span></li>
<li><span class="koboSpan" id="kobo.52.1">It is also important to evaluate whether the model can recognize patterns in the way that domain experts do. </span><span class="koboSpan" id="kobo.52.2">If it cannot, a different model or data preparation method may </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">be required.</span></span></li>
<li><span class="koboSpan" id="kobo.54.1">It is necessary to consider whether the model can exhibit common sense in its predictions. </span><span class="koboSpan" id="kobo.54.2">If not, it may be necessary to apply special post-processing techniques to enforce </span><span class="No-Break"><span class="koboSpan" id="kobo.55.1">common sense.</span></span></li>
<li><span class="koboSpan" id="kobo.56.1">Exposing other performance metrics such as inference speed and model size can be critical when choosing a model. </span><span class="koboSpan" id="kobo.56.2">You don’t want a model that is too slow or too big to fit into your targeted </span><span class="No-Break"><span class="koboSpan" id="kobo.57.1">production environment.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.58.1">In addition to</span><a id="_idIndexMarker774"/><span class="koboSpan" id="kobo.59.1"> helping a project progress toward success, gathering insights from a model can also help identify potential issues early on. </span><span class="koboSpan" id="kobo.59.2">In the machine learning life cycle, which was introduced in </span><a href="B18187_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.60.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.61.1">, </span><em class="italic"><span class="koboSpan" id="kobo.62.1">Deep Learning Life Cycle</span></em><span class="koboSpan" id="kobo.63.1">, it is important to remember that projects may fail during planning or when delivering model insights. </span><span class="koboSpan" id="kobo.63.2">Failing is a natural part of the machine learning process and, in fact, many projects are not meant to succeed. </span><span class="koboSpan" id="kobo.63.3">This could be due to a variety of factors, such as data engineering not being suited for machine learning or the task being too complex. </span><span class="koboSpan" id="kobo.63.4">However, it is important to fail fast and dump the project in order to be able to redirect resources to other use cases that have a higher chance of success. </span><span class="koboSpan" id="kobo.63.5">In cases where the project is critical and cannot be dumped, identifying the root cause of the failure quickly can improve the execution efficiency of the project by diverting resources to fix it and cyclically transitioning between the stages of the machine learning life cycle. </span><span class="koboSpan" id="kobo.63.6">In order for a project to fail fast, you have to have a responsible and confident way to be able to determine whether the model is not working. </span><span class="koboSpan" id="kobo.63.7">In summary, this ability to fail quickly can be very beneficial, as it saves time and resources that might have otherwise </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">been wasted.</span></span></p>
<p><span class="koboSpan" id="kobo.65.1">The following list shows a sufficient range of methods that can be utilized to evaluate deep learning </span><a id="_idIndexMarker775"/><span class="koboSpan" id="kobo.66.1">models, with some of them being general methods that can work for non-deep learning </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">models too:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.68.1">Evaluation metric engineering</span></strong><span class="koboSpan" id="kobo.69.1">: While evaluation metrics are commonly used in many</span><a id="_idIndexMarker776"/><span class="koboSpan" id="kobo.70.1"> projects, the practice of evaluation metric engineering is often overlooked. </span><span class="koboSpan" id="kobo.70.2">In this chapter, we will take a closer look at metric engineering and explore the process of selecting appropriate evaluation metrics. </span><span class="koboSpan" id="kobo.70.3">We’ll start by discussing foundational baseline evaluation metrics that are suitable for various types of problems. </span><span class="koboSpan" id="kobo.70.4">Then, we’ll move on to explore how to upgrade the baseline evaluation metric to one that is specific to the domain and use case of the project. </span><span class="koboSpan" id="kobo.70.5">So, in short, this chapter will help you understand the importance of metric engineering and guide you through the process of selecting the right metrics for </span><span class="No-Break"><span class="koboSpan" id="kobo.71.1">your project.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.72.1">Learning curves</span></strong><span class="koboSpan" id="kobo.73.1">: Learning </span><a id="_idIndexMarker777"/><span class="koboSpan" id="kobo.74.1">curves determine the level of fit for a deep </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">learning model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.76.1">Lift charts</span></strong><span class="koboSpan" id="kobo.77.1">: A lift chart </span><a id="_idIndexMarker778"/><span class="koboSpan" id="kobo.78.1">offers a visual representation of the performance of a predictive model. </span><span class="koboSpan" id="kobo.78.2">It shows how much better the model is at predicting positive outcomes compared to </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">random chance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.80.1">Receiver operating characteristic</span></strong><span class="koboSpan" id="kobo.81.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.82.1">ROC</span></strong><span class="koboSpan" id="kobo.83.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.84.1">curves</span></strong><span class="koboSpan" id="kobo.85.1">: A graphical representation </span><a id="_idIndexMarker779"/><span class="koboSpan" id="kobo.86.1">of the performance of a binary classification model. </span><span class="koboSpan" id="kobo.86.2">They plot the </span><strong class="bold"><span class="koboSpan" id="kobo.87.1">true positive rate</span></strong><span class="koboSpan" id="kobo.88.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.89.1">TPR</span></strong><span class="koboSpan" id="kobo.90.1">) against the </span><strong class="bold"><span class="koboSpan" id="kobo.91.1">false positive rate</span></strong><span class="koboSpan" id="kobo.92.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.93.1">FPR</span></strong><span class="koboSpan" id="kobo.94.1">) at various </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">classification thresholds.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.96.1">Confusion matrix</span></strong><span class="koboSpan" id="kobo.97.1">: A confusion matrix</span><a id="_idIndexMarker780"/><span class="koboSpan" id="kobo.98.1"> is a performance evaluation tool that measures the classification accuracy of a machine learning model. </span><span class="koboSpan" id="kobo.98.2">It compares the predicted and actual outcomes of a model’s predictions and presents them in a </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">matrix format.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.100.1">Feature importances</span></strong><span class="koboSpan" id="kobo.101.1">: This is the process of determining which features in a dataset have the most </span><a id="_idIndexMarker781"/><span class="koboSpan" id="kobo.102.1">influence on the output of a machine learning model. </span><span class="koboSpan" id="kobo.102.2">Additionally, it is useful for identifying the most important factors in a given problem and can help improve the model’s </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">overall performance.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.104.1">A/B testing</span></strong><span class="koboSpan" id="kobo.105.1">: A/B testing </span><a id="_idIndexMarker782"/><span class="koboSpan" id="kobo.106.1">for machine learning involves comparing the performance of two different models or any algorithms on a specific task, in order to determine which model performs better in practice. </span><span class="koboSpan" id="kobo.106.2">This can help practitioners make more informed decisions about which models to use or how to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">existing models.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.108.1">Cohort analysis</span></strong><span class="koboSpan" id="kobo.109.1">: Cohort analysis is</span><a id="_idIndexMarker783"/><span class="koboSpan" id="kobo.110.1"> a technique for evaluating the performance of a model on different subgroups or cohorts of users. </span><span class="koboSpan" id="kobo.110.2">It can help identify whether the model is performing differently for different groups and can be useful for understanding how to improve the model for </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">specific segments.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.112.1">Residual analysis</span></strong><span class="koboSpan" id="kobo.113.1">: Residual analysis is a technique used to check the goodness of fit of a </span><a id="_idIndexMarker784"/><span class="koboSpan" id="kobo.114.1">regression model by examining the difference between the observed values and the predicted values (</span><strong class="bold"><span class="koboSpan" id="kobo.115.1">residuals</span></strong><span class="koboSpan" id="kobo.116.1">). </span><span class="koboSpan" id="kobo.116.2">It helps identify patterns or outliers in the residuals that may indicate areas for improvement in </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">the model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.118.1">Confidence intervals</span></strong><span class="koboSpan" id="kobo.119.1">: Confidence intervals are a measure of the uncertainty in an estimate. </span><span class="koboSpan" id="kobo.119.2">They </span><a id="_idIndexMarker785"/><span class="koboSpan" id="kobo.120.1">can be used to determine the range of values in which the true performance of a model is likely to fall with a certain level of confidence. </span><span class="koboSpan" id="kobo.120.2">Confidence intervals can be useful for comparing the performance of different models or for determining whether the performance of a model is </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">statistically significant.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.122.1">Gathering insights from predictions</span></strong><span class="koboSpan" id="kobo.123.1">: This will be covered in </span><a href="B18187_11.xhtml#_idTextAnchor172"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.124.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.125.1">, </span><em class="italic"><span class="koboSpan" id="kobo.126.1">Explaining Neural </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.127.1">Network Predictions</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.129.1">Interpreting neural networks</span></strong><span class="koboSpan" id="kobo.130.1">: This will be covered in </span><a href="B18187_12.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.131.1">Chapter 12</span></em></span></a><span class="koboSpan" id="kobo.132.1">, </span><em class="italic"><span class="koboSpan" id="kobo.133.1">Interpreting </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.134.1">Neural Networks</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.136.1">Bias and fairness analysis</span></strong><span class="koboSpan" id="kobo.137.1">: This will be covered in </span><a href="B18187_13.xhtml#_idTextAnchor196"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.138.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.139.1">, </span><em class="italic"><span class="koboSpan" id="kobo.140.1">Exploring Bias </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.141.1">and Fairness</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.143.1">Adversarial analysis</span></strong><span class="koboSpan" id="kobo.144.1">: This will be covered in </span><a href="B18187_14.xhtml#_idTextAnchor206"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.145.1">Chapter 14</span></em></span></a><span class="koboSpan" id="kobo.146.1">, </span><em class="italic"><span class="koboSpan" id="kobo.147.1">Analyzing </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.148.1">Adversarial Performance</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.150.1">In this book, we will only be covering methods that relate to neural networks in some way. </span><span class="koboSpan" id="kobo.150.2">In the next section, we will start with the engineering baseline evaluation method, which is the model </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">evaluation metric.</span></span></p>
<h1 id="_idParaDest-159"><a id="_idTextAnchor166"/><span class="koboSpan" id="kobo.152.1">Engineering the base model evaluation metric</span></h1>
<p><span class="koboSpan" id="kobo.153.1">Engineering a </span><a id="_idIndexMarker786"/><span class="koboSpan" id="kobo.154.1">metric for your use case is a skill that is often overlooked. </span><span class="koboSpan" id="kobo.154.2">This is most likely because most projects work on a publicly available dataset, which almost always already has a metric proposed. </span><span class="koboSpan" id="kobo.154.3">This includes projects on Kaggle and many public datasets people use to benchmark against. </span><span class="koboSpan" id="kobo.154.4">However, this does not happen in real life and a metric doesn’t just get served to you. </span><span class="koboSpan" id="kobo.154.5">Let’s explore this topic further here and gain </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">this skillset.</span></span></p>
<p><span class="koboSpan" id="kobo.156.1">The model evaluation metric is the </span><a id="_idIndexMarker787"/><span class="koboSpan" id="kobo.157.1">first evaluation method that is essential in supervised projects, excluding unsupervised-based projects. </span><span class="koboSpan" id="kobo.157.2">There are a few baseline metrics that exist to be the </span><em class="italic"><span class="koboSpan" id="kobo.158.1">de facto</span></em><span class="koboSpan" id="kobo.159.1"> metrics depending on the problem and target type. </span><span class="koboSpan" id="kobo.159.2">Additionally, there are also more customized versions of these baseline metrics that are catered to special objectives. </span><span class="koboSpan" id="kobo.159.3">For example, generative-based tasks can be evaluated through a special human-based opinion score called the mean opinion score. </span><span class="koboSpan" id="kobo.159.4">The recommended go-to strategy here is to always start with a baseline metric and work your way up to metrics that reflect how errors should be distributed properly in different conditions in your use case, similar to how it is recommended to </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">build models.</span></span></p>
<p><span class="koboSpan" id="kobo.161.1">Here</span><a id="_idIndexMarker788"/><span class="koboSpan" id="kobo.162.1"> are </span><a id="_idIndexMarker789"/><span class="koboSpan" id="kobo.163.1">baseline metrics for </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">different conditions:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.165.1">Binary </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.166.1">classification problems</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.168.1">Accuracy</span></strong><span class="koboSpan" id="kobo.169.1">: This</span><a id="_idIndexMarker790"/><span class="koboSpan" id="kobo.170.1"> is the percentage of correctly classified examples (true positives and true negatives) out of all examples. </span><span class="koboSpan" id="kobo.170.2">It is the most widely known evaluation metric across any domain. </span><span class="koboSpan" id="kobo.170.3">However, in reality, this is a skewed metric that can cloud the actual positive prediction performance of the model due to the natural oversupply of negatives in most datasets. </span><span class="koboSpan" id="kobo.170.4">If there are 99 negative examples and 1 positive example, predicting negative all the time without a model can get you 99% accuracy! </span><span class="koboSpan" id="kobo.170.5">Accuracy still remains</span><a id="_idIndexMarker791"/><span class="koboSpan" id="kobo.171.1"> the main method for model evaluation, but it is not practically used. </span><span class="koboSpan" id="kobo.171.2">When somebody says the model is </span><em class="italic"><span class="koboSpan" id="kobo.172.1">accurate</span></em><span class="koboSpan" id="kobo.173.1">, they probably aren’t using the </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">accuracy metric.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.175.1">Precision</span></strong><span class="koboSpan" id="kobo.176.1">: This is</span><a id="_idIndexMarker792"/><span class="koboSpan" id="kobo.177.1"> the proportion of true positives among all predicted positive examples. </span><span class="koboSpan" id="kobo.177.2">It is a robust alternative that focuses on false positives. </span><span class="koboSpan" id="kobo.177.3">A prediction threshold is needed here for binary </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">classification projects.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.179.1">Recall</span></strong><span class="koboSpan" id="kobo.180.1">: This </span><a id="_idIndexMarker793"/><span class="koboSpan" id="kobo.181.1">is the proportion of true positives among all actual positive examples. </span><span class="koboSpan" id="kobo.181.2">It is a robust alternative that focuses on false negatives. </span><span class="koboSpan" id="kobo.181.3">A prediction threshold is needed here for binary </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">classification projects.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.183.1">F1 score</span></strong><span class="koboSpan" id="kobo.184.1">: The F1 score is</span><a id="_idIndexMarker794"/><span class="koboSpan" id="kobo.185.1"> the harmonic mean of precision and recall. </span><span class="koboSpan" id="kobo.185.2">It provides a balanced measure of a model’s performance. </span><span class="koboSpan" id="kobo.185.3">The formula of harmonic </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">mean is</span></span></li></ul></li>
</ul>
<p><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.187.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.188.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.189.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.190.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.191.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.192.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.193.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.194.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.195.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.196.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.197.1">x</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.198.1">1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.199.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.200.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.201.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.202.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.203.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.204.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.205.1">x</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.206.1">2</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.207.1">…</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.208.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.209.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.210.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.211.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.212.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.213.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.214.1">n</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.215.1"> </span></span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.216.1">    where is the total number of samples and is the individual sample value. </span><span class="koboSpan" id="kobo.216.2">There is also the F2 score that weighs recall more than precision. </span><span class="koboSpan" id="kobo.216.3">Use the F1 score if you care about both false positives and false negatives equally, and use the F2 score if you care about false negatives more than false positives. </span><span class="koboSpan" id="kobo.216.4">For example, in an intrusion detection use case, if you want to capture any intruders, you can’t afford to have false negatives, but you can afford to have false positives, so using the F2 score would be better. </span><span class="koboSpan" id="kobo.216.5">Note that the harmonic mean is utilized instead of the arithmetic mean to ensure that extreme values are penalized. </span><span class="koboSpan" id="kobo.216.6">For example, a recall of 1.0 and a precision of 0.01 will result in 0.012 instead of something close to 0.5. </span><span class="koboSpan" id="kobo.216.7">A prediction threshold is </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">needed here.</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.218.1">Area under the receiver operating characteristic curve</span></strong><span class="koboSpan" id="kobo.219.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.220.1">AUC ROC</span></strong><span class="koboSpan" id="kobo.221.1">): This is </span><a id="_idIndexMarker795"/><span class="koboSpan" id="kobo.222.1">the area under the </span><a id="_idIndexMarker796"/><span class="koboSpan" id="kobo.223.1">ROC curve, which provides a measure of a model’s ability to distinguish between positive and negative examples. </span><span class="koboSpan" id="kobo.223.2">No threshold is needed here. </span><span class="koboSpan" id="kobo.223.3">It is recommended for use when the positive and negative classes are balanced so you don’t need to tune the prediction threshold like for F1 </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">and F2.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.225.1">Mean average precision</span></strong><span class="koboSpan" id="kobo.226.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.227.1">mAP</span></strong><span class="koboSpan" id="kobo.228.1">): This</span><a id="_idIndexMarker797"/><span class="koboSpan" id="kobo.229.1"> is an extension on top of the precision metric where instead of a single threshold, multiple thresholds are used to compute precision and averaged up to obtain the more robust precision value at different thresholds. </span><span class="koboSpan" id="kobo.229.2">For multiple classes, average precision is computed independently and averaged up to </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">obtain mAP.</span></span></li>
</ul>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.231.1">Multiclass </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.232.1">classification problems</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.233.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.234.1">Macro</span></strong><span class="koboSpan" id="kobo.235.1">: Calculate </span><a id="_idIndexMarker798"/><span class="koboSpan" id="kobo.236.1">any </span><a id="_idIndexMarker799"/><span class="koboSpan" id="kobo.237.1">binary classification metric for each class individually and then </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">average them</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.239.1">Micro</span></strong><span class="koboSpan" id="kobo.240.1">: Determine </span><a id="_idIndexMarker800"/><span class="koboSpan" id="kobo.241.1">the overall true positive and false positive rates by considering the highest predicted class, and then use these rates to calculate the </span><span class="No-Break"><span class="koboSpan" id="kobo.242.1">overall precision</span></span></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.243.1">Regression problems</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.244.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.245.1">Mean squared error</span></strong><span class="koboSpan" id="kobo.246.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.247.1">MSE</span></strong><span class="koboSpan" id="kobo.248.1">): This</span><a id="_idIndexMarker801"/><span class="koboSpan" id="kobo.249.1"> is the </span><a id="_idIndexMarker802"/><span class="koboSpan" id="kobo.250.1">average of the squared differences between predicted and </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">actual values.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.252.1">Root mean squared error</span></strong><span class="koboSpan" id="kobo.253.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.254.1">RMSE</span></strong><span class="koboSpan" id="kobo.255.1">): This is</span><a id="_idIndexMarker803"/><span class="koboSpan" id="kobo.256.1"> the square root of the MSE. </span><span class="koboSpan" id="kobo.256.2">It provides values at the same scale as the target data and is recommended </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">over MSE.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.258.1">Mean absolute error</span></strong><span class="koboSpan" id="kobo.259.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.260.1">MAE</span></strong><span class="koboSpan" id="kobo.261.1">): This is the average of the absolute differences </span><a id="_idIndexMarker804"/><span class="koboSpan" id="kobo.262.1">between predicted and actual values. </span><span class="koboSpan" id="kobo.262.2">Use this over RMSE when you care about </span><a id="_idIndexMarker805"/><span class="koboSpan" id="kobo.263.1">differences between predicted and actual labels without caring about </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">its sign.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.265.1">R-squared</span></strong><span class="koboSpan" id="kobo.266.1">: A </span><a id="_idIndexMarker806"/><span class="koboSpan" id="kobo.267.1">measure of how well the model fits the data, which ranges from 0 (poor fit) to 1 (</span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">perfect fit).</span></span></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.269.1">Multilabel</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.271.1">Label ranking average precision</span></strong><span class="koboSpan" id="kobo.272.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.273.1">LRAP</span></strong><span class="koboSpan" id="kobo.274.1">): This is the average precision of each</span><a id="_idIndexMarker807"/><span class="koboSpan" id="kobo.275.1"> ground truth label </span><a id="_idIndexMarker808"/><span class="koboSpan" id="kobo.276.1">assigned to a particular sample. </span><span class="koboSpan" id="kobo.276.2">It takes into consideration the ranking of labels predicted against the ground truth labels and assigns</span><a id="_idIndexMarker809"/><span class="koboSpan" id="kobo.277.1"> scores appropriately according to how far or close a ground truth label is in the ranks. </span><span class="koboSpan" id="kobo.277.2">LRAP is beneficial for use cases such as movie recommendation systems, where predicting multiple labels and their rankings is important. </span><span class="koboSpan" id="kobo.277.3">It evaluates the model’s ability to</span><a id="_idIndexMarker810"/><span class="koboSpan" id="kobo.278.1"> predict the correct labels and their order of relevance, making it an ideal metric for tasks that require accurate and meaningful rankings, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">genre recommendations.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.280.1">Image and video labels</span></strong><span class="koboSpan" id="kobo.281.1">: Raw image and video frames, when used directly as labels, require</span><a id="_idIndexMarker811"/><span class="koboSpan" id="kobo.282.1"> their own set of custom metrics that can provide more meaningful evaluations compared to standard regression metrics. </span><span class="koboSpan" id="kobo.282.2">These are </span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">as follows:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.284.1">Peak signal-to-noise ratio</span></strong><span class="koboSpan" id="kobo.285.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.286.1">PSNR</span></strong><span class="koboSpan" id="kobo.287.1">): This is a measure of the quality of a reconstructed</span><a id="_idIndexMarker812"/><span class="koboSpan" id="kobo.288.1"> image or video based on the difference between the original and the </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">reconstructed image.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.290.1">Structural similarity index</span></strong><span class="koboSpan" id="kobo.291.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.292.1">SSIM</span></strong><span class="koboSpan" id="kobo.293.1">): This is a measure of the structural similarity</span><a id="_idIndexMarker813"/><span class="koboSpan" id="kobo.294.1"> between two images or </span><span class="No-Break"><span class="koboSpan" id="kobo.295.1">video frames.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.296.1">Text labels</span></strong><span class="koboSpan" id="kobo.297.1">: Although </span><a id="_idIndexMarker814"/><span class="koboSpan" id="kobo.298.1">standard classification loss and metrics can be used for text prediction tasks, some metrics can measure much higher-level concepts that are more in tune with human intuition. </span><span class="koboSpan" id="kobo.298.2">These are </span><span class="No-Break"><span class="koboSpan" id="kobo.299.1">as follows:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.300.1">Bleu score</span></strong><span class="koboSpan" id="kobo.301.1">: This is a</span><a id="_idIndexMarker815"/><span class="koboSpan" id="kobo.302.1"> measure of the similarity between machine-generated text and human-generated text based on </span><span class="No-Break"><span class="koboSpan" id="kobo.303.1">n-gram overlap.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.304.1">Word error rate</span></strong><span class="koboSpan" id="kobo.305.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.306.1">WER</span></strong><span class="koboSpan" id="kobo.307.1">): This </span><a id="_idIndexMarker816"/><span class="koboSpan" id="kobo.308.1">is a measure of the error rate in automatic speech recognition systems based on the number of errors in </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">word sequences.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.310.1">Human quality-based metrics</span></strong><span class="koboSpan" id="kobo.311.1">: These</span><a id="_idIndexMarker817"/><span class="koboSpan" id="kobo.312.1"> are non-programmatically computable</span><a id="_idIndexMarker818"/><span class="koboSpan" id="kobo.313.1"> metrics that can only be evaluated by </span><span class="No-Break"><span class="koboSpan" id="kobo.314.1">humans manually:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.315.1">Mean opinion score</span></strong><span class="koboSpan" id="kobo.316.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.317.1">MOS</span></strong><span class="koboSpan" id="kobo.318.1">): This is</span><a id="_idIndexMarker819"/><span class="koboSpan" id="kobo.319.1"> a subjective quality rating given by human observers, which can be used to validate and calibrate objective </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">quality metrics</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.321.1">User engagement</span></strong><span class="koboSpan" id="kobo.322.1">: Metrics </span><a id="_idIndexMarker820"/><span class="koboSpan" id="kobo.323.1">such as time spent on a website or app, click-through rate, or bounce rate can be used to measure user engagement </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">and satisfaction</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.325.1">Task completion rate</span></strong><span class="koboSpan" id="kobo.326.1">: This is </span><a id="_idIndexMarker821"/><span class="koboSpan" id="kobo.327.1">the proportion of users who successfully complete a given task or goal, which can be used to evaluate the usability and effectiveness of a product </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">or service</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.329.1">The baseline metrics</span><a id="_idIndexMarker822"/><span class="koboSpan" id="kobo.330.1"> are a set of common metrics that should be your first choices depending on your problem types and conditions. </span><span class="koboSpan" id="kobo.330.2">With that said, the choice of which base metric to use still depends on the specific problem and the trade-offs between the different aspects of the model’s performance that are important for the task at hand. </span><span class="koboSpan" id="kobo.330.3">Here are step-by-step recommendations on how</span><a id="_idIndexMarker823"/><span class="koboSpan" id="kobo.331.1"> to actually choose and utilize an appropriate model </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">evaluation metric:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.333.1">Understand the problem. </span><span class="koboSpan" id="kobo.333.2">Consider the nature of the problem, the data, and the desired outcomes. </span><span class="koboSpan" id="kobo.333.3">This is a key step that will help you identify the key criteria that are most important for </span><span class="No-Break"><span class="koboSpan" id="kobo.334.1">your task.</span></span><ol><li class="Alphabets"><span class="koboSpan" id="kobo.335.1">Be mindful of the quality of the data. </span><span class="koboSpan" id="kobo.335.2">The pillars of data quality (representativeness, consistency, comprehensiveness, uniqueness, fairness, and validity) introduced in </span><a href="B18187_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.336.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.337.1">, </span><em class="italic"><span class="koboSpan" id="kobo.338.1">Deep Learning Life Cycle</span></em><span class="koboSpan" id="kobo.339.1">, will all affect what the metric actually represents. </span><span class="koboSpan" id="kobo.339.2">If you are evaluating a model’s performance on a bad dataset, then the chosen metric may not reflect the model’s true performance on </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">real-world data.</span></span></li><li class="Alphabets"><span class="koboSpan" id="kobo.341.1">Consider the perspective of the users who will be interacting with the model or the output of the model. </span><span class="koboSpan" id="kobo.341.2">What are their expectations and requirements? </span><span class="koboSpan" id="kobo.341.3">What are the relevant quality factors that need to be taken </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">into account?</span></span></li><li class="Alphabets"><span class="koboSpan" id="kobo.343.1">Define clear objectives for what the predictions need to accomplish and the opposite of what they need </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">to do.</span></span></li></ol></li>
<li><span class="koboSpan" id="kobo.345.1">Choose a</span><a id="_idIndexMarker824"/><span class="koboSpan" id="kobo.346.1"> metric that aligns with your defined objective. </span><span class="koboSpan" id="kobo.346.2">The metric you choose should align with your overall objective. </span><span class="koboSpan" id="kobo.346.3">For example, in a binary classification medical use case for detecting cancer, making a false positive diagnosis of cancer can ruin many years of a patient’s life, so choose a metric such as precision to allow you to quantitatively reduce the number of </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">false positives.</span></span><ol><li class="Alphabets"><span class="koboSpan" id="kobo.348.1">Consider base metrics that are commonly used in similar problems and how they might need to be adapted or modified to suit the current problem. </span><span class="koboSpan" id="kobo.348.2">Are there any unique aspects of the problem that require a different type of metric or a modification of an </span><span class="No-Break"><span class="koboSpan" id="kobo.349.1">existing metric?</span></span></li><li class="Alphabets"><span class="koboSpan" id="kobo.350.1">A single metric may not always capture the full performance of a model. </span><span class="koboSpan" id="kobo.350.2">Consider using multiple metrics that evaluate different aspects of the model’s performance. </span><span class="koboSpan" id="kobo.350.3">A clear example of this is the creation of an F1 score that combines two metrics: precision </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">and recall.</span></span></li></ol></li>
<li><span class="koboSpan" id="kobo.352.1">Consider the trade-offs between metrics. </span><span class="koboSpan" id="kobo.352.2">For example, in a binary classification project, increasing recall performance by modifying the model’s prediction threshold can adversely affect precision. </span><span class="koboSpan" id="kobo.352.3">Evaluate the trade-offs (if any) between the metrics and choose the one that best aligns with </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">your objectives.</span></span></li>
<li><span class="koboSpan" id="kobo.354.1">Cross-validate the metric. </span><span class="koboSpan" id="kobo.354.2">Making sure the metric is computed on a validation and holdout set instead of just the training set is essential to estimating the model’s real-world performance. </span><span class="koboSpan" id="kobo.354.3">Computing the metric on both the training data and validation data in every epoch will also allow you to visualize the learning curve using the chosen metric directly instead of using the </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">utilized loss.</span></span></li>
<li><span class="koboSpan" id="kobo.356.1">Consider optimizing your model to the metric directly. </span><span class="koboSpan" id="kobo.356.2">Some metrics can be approximately reproduced in the deep learning libraries directly and utilized as loss. </span><span class="koboSpan" id="kobo.356.3">Using direct optimization can sometimes help you get a better model specifically for the metric you’ve chosen. </span><span class="koboSpan" id="kobo.356.4">However, there might also be some pitfalls that can happen. </span><span class="koboSpan" id="kobo.356.5">We will discuss this more extensively later in the </span><em class="italic"><span class="koboSpan" id="kobo.357.1">Directly optimizing the </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.358.1">metric</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.359.1"> section.</span></span></li>
<li><span class="koboSpan" id="kobo.360.1">Build, evaluate, and </span><a id="_idIndexMarker825"/><span class="koboSpan" id="kobo.361.1">compare many models against the evaluation metric by iteratively improving it or just using a diverse variety of techniques. </span><span class="koboSpan" id="kobo.361.2">The process of building a machine learning model might just be the shortest process in the entire ML lifecycle. </span><span class="koboSpan" id="kobo.361.3">But remember that the amount of effort you put into the building and experimentation process can determine whether a project fails or succeeds. </span><span class="koboSpan" id="kobo.361.4">Building a variety of models and iteratively improving them can be a hard and time-consuming task. </span><span class="koboSpan" id="kobo.361.5">Having boilerplate code that can adapt to most use cases can make this process way faster and more seamless so that you can put your effort into more pressing issues in a project. </span><span class="koboSpan" id="kobo.361.6">Alternatively, you can also consider using AutoML tools to consistently get a variety of models trained for each use case. </span><span class="koboSpan" id="kobo.361.7">Look forward to the last chapter in this book where we will get a feel for how AutoML tools can streamline the </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">model-building process!</span></span></li>
<li><span class="koboSpan" id="kobo.363.1">Make sure you define a success criterion that relates to the chosen metric. </span><span class="koboSpan" id="kobo.363.2">To translate a chosen evaluation metric into an actual success metric, it is important to establish a clear understanding of what constitutes success for the given problem. </span><span class="koboSpan" id="kobo.363.3">This will typically involve defining a threshold or target level of performance that the model needs to achieve in order to be considered successful. </span><span class="koboSpan" id="kobo.363.4">Sometimes, the success criteria can have a more fine-grained definition based on specific types of error or specific groups of data. </span><span class="koboSpan" id="kobo.363.5">We will explore this in depth later in </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">this chapter.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.365.1">By carefully considering </span><a id="_idIndexMarker826"/><span class="koboSpan" id="kobo.366.1">these recommendations, it is possible to select a unique metric that accurately measures the relevant aspects of the problem. </span><span class="koboSpan" id="kobo.366.2">Ultimately, this can help to develop more accurate and effective machine learning solutions that will lead to better performance and more </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">successful outcomes.</span></span></p>
<p><span class="koboSpan" id="kobo.368.1">Base metrics</span><a id="_idIndexMarker827"/><span class="koboSpan" id="kobo.369.1"> are a group of metrics that are commonly used by many practitioners to evaluate the performance of a model. </span><span class="koboSpan" id="kobo.369.2">However, in some cases, a particular problem may have additional criteria and unique behaviors that need to be considered when assessing model performance. </span><span class="koboSpan" id="kobo.369.3">In such situations, it may be necessary to adopt alternative or customized metrics that better suit the specific needs of the problem at hand. </span><span class="koboSpan" id="kobo.369.4">Base metrics can be further adapted to the additional ideals you want to use to judge your models. </span><span class="koboSpan" id="kobo.369.5">The next topic will explore custom metrics and their applications, including when it is appropriate to use them as the most suitable metric for a specific </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">use case.</span></span></p>
<h1 id="_idParaDest-160"><a id="_idTextAnchor167"/><span class="koboSpan" id="kobo.371.1">Exploring custom metrics and their applications</span></h1>
<p><span class="koboSpan" id="kobo.372.1">Base metrics </span><a id="_idIndexMarker828"/><span class="koboSpan" id="kobo.373.1">are generally sufficient to meet the requirements of most use cases. </span><span class="koboSpan" id="kobo.373.2">However, custom metrics build upon base metrics and incorporate additional goals that are specific to a given scenario. </span><span class="koboSpan" id="kobo.373.3">It’s helpful to think of base metrics as a bachelor’s degree and custom metrics as a master’s or PhD degree. </span><span class="koboSpan" id="kobo.373.4">It’s perfectly fine to use only base metrics if they meet your needs and you don’t have any </span><span class="No-Break"><span class="koboSpan" id="kobo.374.1">additional requirements.</span></span></p>
<p><span class="koboSpan" id="kobo.375.1">Custom ideals often arise naturally early on in a project and are highly dependent on the specific use case. </span><span class="koboSpan" id="kobo.375.2">Most real use cases don’t expose their chosen metrics to the public, even when the prediction </span><a id="_idIndexMarker829"/><span class="koboSpan" id="kobo.376.1">of the model is meant to be utilized publicly, such as </span><strong class="bold"><span class="koboSpan" id="kobo.377.1">Open AI</span></strong><span class="koboSpan" id="kobo.378.1">’s </span><strong class="bold"><span class="koboSpan" id="kobo.379.1">ChatGPT</span></strong><span class="koboSpan" id="kobo.380.1">. </span><span class="koboSpan" id="kobo.380.2">However, in machine learning competitions, companies with real use cases accompanied by data publish their chosen metric publicly to find the best model that can be built. </span><span class="koboSpan" id="kobo.380.3">In such a setting for a project, the company that hosts the competition is incentivized to perform good-quality metric engineering work that reflects its ideals for its use case. </span><span class="koboSpan" id="kobo.380.4">A metric can affect the resulting best model and will ultimately cost the company money when it doesn’t engineer a good metric that matches their ideals. </span><span class="koboSpan" id="kobo.380.5">Some competitions provide prizes of up to </span><span class="No-Break"><span class="koboSpan" id="kobo.381.1">100,000 USD!</span></span></p>
<p><span class="koboSpan" id="kobo.382.1">In this section, we will present some common and publicly shared custom ideals along with associated metrics and use cases from machine learning competitions that could be useful for you to consider, regardless of your particular </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">use case:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.384.1">Ideals</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.385.1">Use case</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.386.1">Custom metric</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.387.1">For time-series regression point-based forecasting, the targets are seasonal and can fluctuate widely based on the season the data is in. </span><span class="koboSpan" id="kobo.387.2">We want a metric that can make sure errors aren’t weighted heavily to any </span><span class="No-Break"><span class="koboSpan" id="kobo.388.1">one season.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.389.1">M5 forecasting—accuracy (Kaggle)</span></strong><span class="koboSpan" id="kobo.390.1">: This involves predicting the number of Walmart retail goods units sold. </span><span class="koboSpan" id="kobo.390.2">The competition provided sales time-series data from Walmart that followed a hierarchical structure, beginning at the item level and progressing to department, product category, and store levels. </span><span class="koboSpan" id="kobo.390.3">The data was generously provided and covered three regions in the United States: California, Texas, </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">and Wisconsin.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.392.1">Weighted root mean squared scaled error </span></strong><span class="koboSpan" id="kobo.393.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.394.1">WRMSSE</span></strong><span class="koboSpan" id="kobo.395.1">): The main part here is the RMSSE, which is a modification of RMSE. </span><span class="koboSpan" id="kobo.395.2">Before applying the root of MSE, RMSSE divides the standard MSE by the MSE that uses most recent observation as ground truth. </span><span class="koboSpan" id="kobo.395.3">This makes sure all RMSE from any season will be scaled into the same range </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">of values.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style" rowspan="2">
<p><span class="koboSpan" id="kobo.397.1">Some labels/classes don’t matter as much in reality, either because they don’t occur as much or they just don’t impact post-prediction decisions as much. </span><span class="koboSpan" id="kobo.397.2">Don’t judge the model too much on unimportant labels; put further emphasis on the errors of more </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">important labels/classes.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.399.1">RSNA intracranial hemorrhage detection (Kaggle)</span></strong><span class="koboSpan" id="kobo.400.1">: This is a multi-label classification problem to detect the location and type of any hemorrhage present in a medical image of the human brain. </span><span class="koboSpan" id="kobo.400.2">A special label called </span><strong class="source-inline"><span class="koboSpan" id="kobo.401.1">any</span></strong><span class="koboSpan" id="kobo.402.1"> was made to account for all other types of hemorrhage not accounted for with a specific label. </span><span class="koboSpan" id="kobo.402.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.403.1">any</span></strong><span class="koboSpan" id="kobo.404.1"> label had 2-3 times more data than any other </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">label alone.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.406.1">Weighted multilabel log loss</span></strong><span class="koboSpan" id="kobo.407.1">: The </span><strong class="source-inline"><span class="koboSpan" id="kobo.408.1">any</span></strong><span class="koboSpan" id="kobo.409.1"> label was weighted more than any other label even though it had more data. </span><span class="koboSpan" id="kobo.409.2">This shows that the significance of a label for a metric is not exclusive to the scarcity of the data associated with the label in a dataset; it really depends on the specific </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">problem context.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.411.1">M5 Forecasting—accuracy (Kaggle)</span></strong><span class="koboSpan" id="kobo.412.1">: This involves predicting the number of Walmart retail goods units sold. </span><span class="koboSpan" id="kobo.412.2">The competition provided time-series sales data from Walmart that followed a hierarchical structure, beginning at the item level and progressing to department, product category, and store levels. </span><span class="koboSpan" id="kobo.412.3">The data was generously provided and covered three regions in the United States: California, Texas, </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">and Wisconsin.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.414.1">WRMSSE</span></strong><span class="koboSpan" id="kobo.415.1">: The competition author valued more unit sales forecast from products that provided more significant sales in dollars. </span><span class="koboSpan" id="kobo.415.2">The weight for each product is obtained by using the sales volumes for the product in the last 28 observations of the training sample (sum of units sold multiplied by their </span><span class="No-Break"><span class="koboSpan" id="kobo.416.1">respective prices).</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.417.1">Walmart Recruiting—Store Sales Forecasting (Kaggle)</span></strong><span class="koboSpan" id="kobo.418.1">: This involves forecasting the sales of </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">Walmart goods.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.420.1">Weighted mean absolute error</span></strong><span class="koboSpan" id="kobo.421.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.422.1">WMAE</span></strong><span class="koboSpan" id="kobo.423.1">): Walmart weighed holiday weeks forecast error more than non-holiday weeks by five-fold, as they have much higher sales during </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">holiday weeks.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style" rowspan="2">
<p><span class="koboSpan" id="kobo.425.1">We don’t really care too much about small errors, as they can be tolerated, but we care about big errors because they can result in the triggering of unwanted actions by consuming </span><span class="No-Break"><span class="koboSpan" id="kobo.426.1">the predictions.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.427.1">Google Analytics customer revenue prediction (Kaggle)</span></strong><span class="koboSpan" id="kobo.428.1">: This is a regression problem about predicting the total revenue generated by a customer for an online store. </span><span class="koboSpan" id="kobo.428.2">The revenue values were highly skewed and contained many zero values, which made it challenging to evaluate the performance of the participating models using traditional metrics such as MSE </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">or MAE.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.430.1">Root mean squared log error (RMSLE)</span></strong><span class="koboSpan" id="kobo.431.1">: RMSLE applies a logarithmic transformation to the predicted and actual values before computing RMSE, which helps to penalize large errors more heavily than small errors. </span><span class="koboSpan" id="kobo.431.2">A natural way of doing this is to simply train the model to predict the log values of the target and apply RMSE to </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">achieve RMSLE.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.433.1">Diabetic retinopathy detection (Kaggle)</span></strong><span class="koboSpan" id="kobo.434.1">: This is a multiclass problem about predicting whether high-resolution retina images have diabetic retinopathy or not. </span><span class="koboSpan" id="kobo.434.2">The problem has five classes: one for no disease and the other four for different severities of </span><span class="No-Break"><span class="koboSpan" id="kobo.435.1">the disease.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.436.1">Quadratic Weighted Kappa (QWK) score</span></strong><span class="koboSpan" id="kobo.437.1">: The Kappa score is a statistical measure that provides a single value to quantify the degree of agreement between predicted and actual labels in multi-class classification tasks. </span><span class="koboSpan" id="kobo.437.2">The quadratic weighing mechanism enables the Kappa score to become more robust to minor errors and more sensitive to larger ones. </span><span class="koboSpan" id="kobo.437.3">The quadratic weighting scheme can address issues where the Kappa score may be inflated by a high proportion of agreement in easy-to-classify categories while still providing an accurate representation of the level of agreement for the more </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">difficult cases.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style"/>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.439.1">Two Sigma Connect</span></strong><strong class="bold"><span class="koboSpan" id="kobo.440.1"> Rental listing inquiries (Kaggle)</span></strong><span class="koboSpan" id="kobo.441.1">: This involves predicting how popular an apartment rental listing is based on the listing content, such as text descriptions, photos, number of bedrooms, </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">and price.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.443.1">Log loss</span></strong><span class="koboSpan" id="kobo.444.1">: Log loss is an evaluation metric that emphasizes wrong predictions by penalizing models that are confident about </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">incorrect predictions.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.446.1">In a multiclass problem, we have a higher tolerance for our multiclass model in our use case where we can consume several of the top predicted classes instead of using the single most probable class to maximize the true positive hit rate performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.447.1">the model.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.448.1">Airbnb new user bookings (Kaggle)</span></strong><span class="koboSpan" id="kobo.449.1">: This is a multiclass problem for predicting the country in which a new user will make their first booking with Airbnb (including a no-booking class). </span><span class="koboSpan" id="kobo.449.2">The predicted class will allow Airbnb to share more personalized content with their community, decrease the average time to first booking, and better </span><span class="No-Break"><span class="koboSpan" id="kobo.450.1">forecast demand.</span></span></p>
<p><span class="koboSpan" id="kobo.451.1">Airbnb had a relaxed requirement for the multiclass predictions a model produced, where they could perform personalized content based on the top five countries instead of a single country to improve the true positive </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">hit rate.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.453.1">Normalized discounted cumulative gain (NDCG) of top-k classes</span></strong><span class="koboSpan" id="kobo.454.1">: NDCG is a measure of the ranking effectiveness of top-k classes, usually used for recommendation use cases. </span><span class="koboSpan" id="kobo.454.2">This metric pairs nicely with the tolerance Airbnb has for multiclass predictions. </span><span class="koboSpan" id="kobo.454.3">It would work well for any multiclass use case that can tolerate using several top predicted classes instead of the single predicted </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">top class.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.456.1">For multi-object video-based tracking use cases, we want to use a metric that penalizes undesired tracking behaviors, such as false object identification and failure to track objects consistently over time. </span><span class="koboSpan" id="kobo.456.2">Additionally, we need a metric that can be computed by the multiple trajectories created by a model through time against a fixed set of ground truth </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">tracking trajectories.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.458.1">Multi-camera people tracking (AICity)</span></strong> <a href="https://www.aicitychallenge.org/2023-data-and-evaluation/"><span class="koboSpan" id="kobo.459.1">https://www.aicitychallenge.org/2023-data-and-evaluation/</span></a><span class="koboSpan" id="kobo.460.1">: The dataset includes multiple camera feeds captured in various settings, including warehouse environments within a building, as well as synthetic data generated using the NVIDIA Omniverse Platform in multiple indoor settings. </span><span class="koboSpan" id="kobo.460.2">To enhance the diversity and size of our dataset for </span><strong class="source-inline"><span class="koboSpan" id="kobo.461.1">Track 1</span></strong><span class="koboSpan" id="kobo.462.1">, we have created a large-scale synthetic dataset of animated people. </span><span class="koboSpan" id="kobo.462.2">All camera feeds in our dataset are high-resolution (1080p) feeds with frame rates of 30 frames </span><span class="No-Break"><span class="koboSpan" id="kobo.463.1">per second.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.464.1">Identity F1 score (IDF1)</span></strong><span class="koboSpan" id="kobo.465.1">: IDF1 handles this by evaluating the overall performance of the tracking algorithm based on how well it matches predicted tracks to ground truth tracks, taking into account the identity of each object over time. </span><span class="koboSpan" id="kobo.465.2">The algorithm penalizes false positives, false negatives, and identity switches, which occur when the algorithm incorrectly identifies two detections as the same object or incorrectly assigns two different identities to the same object over time. </span><span class="koboSpan" id="kobo.465.3">Most importantly, the model can dynamically determine matching trajectories based on a version of the </span><strong class="bold"><span class="koboSpan" id="kobo.466.1">intersection over union</span></strong><span class="koboSpan" id="kobo.467.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.468.1">IOU</span></strong><span class="koboSpan" id="kobo.469.1">) algorithm between predicted and ground </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">truth trajectories.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.471.1">We care about bias in our model’s performance a lot more than the overall </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">model’s performance.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.473.1">Jigsaw unintended bias in toxicity classification (Kaggle)</span></strong> <a href="https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/overview/evalu"><span class="koboSpan" id="kobo.474.1">https://www.kaggle.com/competitions/jigsaw-unintended-bias-in-toxicity-classification/overview/evaluation</span></a><span class="koboSpan" id="kobo.475.1">: This involves predicting the different intensities of toxicity in text data. </span><span class="koboSpan" id="kobo.475.2">The data contains identities that the competition author wishes to optimize against in the name of bias. </span><span class="koboSpan" id="kobo.475.3">The identities </span><span class="No-Break"><span class="koboSpan" id="kobo.476.1">involved are</span></span></p>
<p><span class="koboSpan" id="kobo.477.1">male, female, transgender, other gender, heterosexual, homosexual (gay or lesbian), bisexual, other sexual orientation, Christian, Jewish, Muslim, Hindu, Buddhist, atheist, other religion, black, white, Asian, Latino, other race or ethnicity, physical disability, intellectual or learning disability, psychiatric or mental illness, and </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">other disability.</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.479.1">Bias-focused AUC</span></strong><span class="koboSpan" id="kobo.480.1">: The competition focuses on a weighted combination of multiple AUC metric that are computed on a different subset of the dataset based on specific identities that can be mentioned in the text data row. </span><span class="koboSpan" id="kobo.480.2">Overall, AUC is combined equally with the next three </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">bias-focused metrics.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.482.1">Identity-based subgroup AUC</span></strong><span class="koboSpan" id="kobo.483.1">: This involves analyzing a dataset that only includes comments mentioning a specific identity subgroup. </span><span class="koboSpan" id="kobo.483.2">A low score in this metric indicates that the model struggles to distinguish between toxic and non-toxic comments related to </span><span class="No-Break"><span class="koboSpan" id="kobo.484.1">that identity.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style" rowspan="2"/>
<td class="No-Table-Style" rowspan="2"/>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.485.1">Background positive, subgroup negative (BPSN) AUC</span></strong><span class="koboSpan" id="kobo.486.1">: This involves evaluating the model’s performance on non-toxic examples mentioning the identity and toxic examples that do not. </span><span class="koboSpan" id="kobo.486.2">A low score in this metric suggests that the model may incorrectly label non-toxic examples related to the identity </span><span class="No-Break"><span class="koboSpan" id="kobo.487.1">as toxic.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.488.1">Background negative, subgroup positive (BNSP) AUC</span></strong><span class="koboSpan" id="kobo.489.1">: This involves assessing the model’s performance on toxic examples mentioning the identity and non-toxic examples that do not. </span><span class="koboSpan" id="kobo.489.2">A low score in this metric indicates that the model may incorrectly label toxic examples related to the identity </span><span class="No-Break"><span class="koboSpan" id="kobo.490.1">as non-toxic.</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.491.1">Table 10.1 – A table of custom ideals with example use cases and metrics used</span></p>
<p><span class="koboSpan" id="kobo.492.1">In the</span><a id="_idIndexMarker830"/><span class="koboSpan" id="kobo.493.1"> second ideal, the general metric idea is to apply weights to any metric you’d like to choose based on what you deem more important. </span><span class="koboSpan" id="kobo.493.2">Additionally, weights can be applied more flexibly to any auxiliary data that does not act as an input to a model, nor as a target to the model. </span><span class="koboSpan" id="kobo.493.3">Finally, for the last ideal, look forward to </span><a href="B18187_13.xhtml#_idTextAnchor196"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.494.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.495.1">, </span><em class="italic"><span class="koboSpan" id="kobo.496.1">Exploring Bias and Fairness</span></em><span class="koboSpan" id="kobo.497.1">, to discover methods to optimize </span><span class="No-Break"><span class="koboSpan" id="kobo.498.1">against bias.</span></span></p>
<p><span class="koboSpan" id="kobo.499.1">While the examples of custom ideals and metrics we’ve discussed are useful guidelines, it’s important to remember that there are many different metrics and ideals that may be relevant to your specific use case. </span><span class="koboSpan" id="kobo.499.2">Don’t be afraid to dig deeper into your problem domain and identify metrics that are unique to your </span><span class="No-Break"><span class="koboSpan" id="kobo.500.1">particular situation.</span></span></p>
<p><span class="koboSpan" id="kobo.501.1">The examples we’ve given can serve as a helpful cheat sheet for developing custom metrics that are tailored to your needs. </span><span class="koboSpan" id="kobo.501.2">By understanding the reasoning behind the use of these special metrics in specific domains, you can gain insight into the factors that should be considered when evaluating model performance. </span><span class="koboSpan" id="kobo.501.3">Ultimately, the key is to choose metrics that align with your goals and capture the most important aspects of your </span><span class="No-Break"><span class="koboSpan" id="kobo.502.1">problem domain.</span></span></p>
<p><span class="koboSpan" id="kobo.503.1">Next, we will discuss a robust strategy to compare the metric performance of different models across multiple metric values computed from different cross-validation folds or </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">dataset partitions.</span></span></p>
<h1 id="_idParaDest-161"><a id="_idTextAnchor168"/><span class="koboSpan" id="kobo.505.1">Exploring statistical tests for comparing model metrics</span></h1>
<p><span class="koboSpan" id="kobo.506.1">In </span><a id="_idIndexMarker831"/><span class="koboSpan" id="kobo.507.1">machine learning, metric-based model evaluation often involves using averages of aggregated metrics from different folds or partitions, such as holdout and validation sets, to compare the performance of various models. </span><span class="koboSpan" id="kobo.507.2">However, relying solely on these average performance metrics may not provide a comprehensive assessment of a model’s performance and generalizability. </span><span class="koboSpan" id="kobo.507.3">A more robust approach to model evaluation is the incorporation of statistical hypothesis tests, which assess whether observed differences in performance are statistically significant or due to </span><span class="No-Break"><span class="koboSpan" id="kobo.508.1">random chance.</span></span></p>
<p><span class="koboSpan" id="kobo.509.1">Statistical hypothesis tests are procedures used to determine whether observed data provides sufficient evidence to reject a null hypothesis in favor of an alternative hypothesis, helping to quantify the likelihood that the observed differences are due to random chance or a genuine effect. </span><span class="koboSpan" id="kobo.509.2">In statistical tests, the null hypothesis (H0) is a default assumption that states there is no effect or relationship between variables, serving as a basis for comparison against the alternative hypothesis with the goal of determining whether the observed data provides enough evidence to reject this default assumption. </span><span class="koboSpan" id="kobo.509.3">For the purpose of comparing model metric performance across multiple partitions and datasets, the null hypothesis is typically that there is no difference between the performances of the models, while the alternative hypothesis is that there </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">are differences.</span></span></p>
<p><span class="koboSpan" id="kobo.511.1">Overall, statistical tests offer a formal framework to objectively determine whether differences in performance are significant or due to chance. </span><span class="koboSpan" id="kobo.511.2">Additionally, statistical tests offer a comprehensive understanding of model performance by accounting for variability and uncertainty in metrics. </span><span class="koboSpan" id="kobo.511.3">The following table shows common statistical tests that you can consider using, along with the Python code needed to execute it, how to interpret the result, and when to </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">use it:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table002-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.513.1">Statistical test</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.514.1">Python code</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.515.1">Result interpretation</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.516.1">Recommended use</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.517.1">Paired t-test</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="source-inline"><span class="koboSpan" id="kobo.518.1">from scipy.stats </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.519.1">import ttest_rel</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.520.1"> t_stat, p_value = </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.521.1">ttest_rel(model1_scores, model2_scores)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.522.1">If </span><strong class="source-inline"><span class="koboSpan" id="kobo.523.1">p_value</span></strong><span class="koboSpan" id="kobo.524.1"> &lt; 0.05, there’s a significant difference between </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">the models.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.526.1">Use this when comparing two dependent samples with normally </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">distributed differences.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.528.1">Mann-Whitney </span><span class="No-Break"><span class="koboSpan" id="kobo.529.1">U test</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="source-inline"><span class="koboSpan" id="kobo.530.1">from scipy.stats </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.531.1">import mannwhitneyu</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.532.1"> u_stat, p_value = </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.533.1">mannwhitneyu(model1_scores, model2_scores)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.534.1">If </span><strong class="source-inline"><span class="koboSpan" id="kobo.535.1">p_value</span></strong><span class="koboSpan" id="kobo.536.1"> &lt; 0.05, there’s a significant difference between </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">the models.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.538.1">Use this when comparing two independent samples with non-normally distributed data or </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">ordinal data.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.540.1">Analysis of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.541.1">variance</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.542.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.543.1">ANOVA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">)</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="source-inline"><span class="koboSpan" id="kobo.545.1">from scipy.stats import f_oneway f_stat, p_value = f_oneway(model1_scores, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.546.1">model2_scores, model3_scores)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.547.1">If </span><strong class="source-inline"><span class="koboSpan" id="kobo.548.1">p_value</span></strong><span class="koboSpan" id="kobo.549.1"> &lt; 0.05, there’s a significant difference among </span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">the models.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.551.1">Use this when comparing three or more independent samples with normally distributed data and </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">equal variances.</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.553.1">Kruskal-Wallis </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">H test</span></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="source-inline"><span class="koboSpan" id="kobo.555.1">from scipy.stats </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.556.1">import kruskal</span></strong></span></p>
<p><strong class="source-inline"><span class="koboSpan" id="kobo.557.1"> h_stat, p_value = kruskal(model1_scores, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.558.1">model2_scores, model3_scores)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.559.1">If </span><strong class="source-inline"><span class="koboSpan" id="kobo.560.1">p_value</span></strong><span class="koboSpan" id="kobo.561.1"> &lt; 0.05, there are significant differences between </span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">the models.</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.563.1">Use this when comparing three or more independent samples with non-normally distributed data or </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">ordinal data.</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.565.1">Table 10.2 – Common statistical tests with details on their Python implementation, result interpretations, and recommendations on when to use them</span></p>
<p><span class="koboSpan" id="kobo.566.1">These</span><a id="_idIndexMarker832"/><span class="koboSpan" id="kobo.567.1"> recommendations can help you choose the appropriate statistical test based on the conditions and assumptions of your data, such as the number of samples, the type of data (dependent or independent), and the distribution of the data (normal </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">or non-normal).</span></span></p>
<p><span class="koboSpan" id="kobo.569.1">Next, we will discuss how the outcome of metric engineering can be converted to a </span><span class="No-Break"><span class="koboSpan" id="kobo.570.1">success criterion.</span></span></p>
<h1 id="_idParaDest-162"><a id="_idTextAnchor169"/><span class="koboSpan" id="kobo.571.1">Relating the evaluation metric to success</span></h1>
<p><span class="koboSpan" id="kobo.572.1">Defining </span><a id="_idIndexMarker833"/><span class="koboSpan" id="kobo.573.1">success in a machine learning project is crucial and should be done at the early stages of the project as introduced in the </span><em class="italic"><span class="koboSpan" id="kobo.574.1">Defining success</span></em><span class="koboSpan" id="kobo.575.1"> section in </span><a href="B18187_01.xhtml#_idTextAnchor015"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.576.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.577.1">, </span><em class="italic"><span class="koboSpan" id="kobo.578.1">Deep Learning Life Cycle</span></em><span class="koboSpan" id="kobo.579.1">. </span><span class="koboSpan" id="kobo.579.2">Success can be defined as achieving higher-level objectives, such as improving the efficiency of processes or increasing the accuracy of processes in comparison to manual labor. </span><span class="koboSpan" id="kobo.579.3">In some rare cases, machine learning can enable processes that were previously impossible due to human limitations. </span><span class="koboSpan" id="kobo.579.4">The ultimate success of achieving these objectives is to save costs or earn more revenue for </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">an organization.</span></span></p>
<p><span class="koboSpan" id="kobo.581.1">A model with a metric performance score of 0.80 F1 score or 0.00123 RMSE doesn’t really mean anything at face value and has to be translated to something tangible in the use case. </span><span class="koboSpan" id="kobo.581.2">For instance, one should answer questions such as what estimated model score can allow the project to achieve the targeted cost savings or revenue improvements. </span><span class="koboSpan" id="kobo.581.3">Quantifying the success that can be obtained from model performance is essential, particularly as machine learning projects can be expensive to execute. </span><span class="koboSpan" id="kobo.581.4">Sometimes, the return on investment can be low if the model fails to perform at a certain level. </span><span class="koboSpan" id="kobo.581.5">After selecting the evaluation metric, it’s important to establish a metric threshold for success that is realistic, achievable, and based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">business objective.</span></span></p>
<p><span class="koboSpan" id="kobo.583.1">As a finale to the topic, let’s go through an example workflow to relate the evaluation metric to success based on a hypothetical </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">use case.</span></span></p>
<p><span class="koboSpan" id="kobo.585.1">Let’s consider a use case to identify defective products in a manufacturing process using image data. </span><span class="koboSpan" id="kobo.585.2">Let’s assume that the cost of producing a product is $50 and it has a retail price of $200. </span><span class="koboSpan" id="kobo.585.3">If that product is defective and it makes it through to a customer, this will result in an additional chargeback of $1,000 and the return of the $200 paid by the customer to compensate for the defective product, which may have caused harm. </span><span class="koboSpan" id="kobo.585.4">On the other hand, if a good product is identified as defective and scrapped, it results in a cost of $250; $50 is used to produce it and $200 is lost opportunity cost as it </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">is scrapped.</span></span></p>
<p><span class="koboSpan" id="kobo.587.1">Let’s say that we have a dataset of 10,000 product images, which is the amount of products produced a month. </span><span class="koboSpan" id="kobo.587.2">If we don’t use a model and produce and ship all 10,000 products, we would have 95 defective products, which would cost $500,000 in production costs (10,000 x $50) and $95,000 in chargeback costs, resulting in a total cost of $595,000. </span><span class="koboSpan" id="kobo.587.3">After selling all the non-defective products, the company will gain $1,981,000 in sales (9,905 x $200). </span><span class="koboSpan" id="kobo.587.4">The total money earned will </span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">be $1,386,000.</span></span></p>
<p><span class="koboSpan" id="kobo.589.1">Consider a case in which we use a trained deep learning model to classify these images as either good (negative) or defective (positive). </span><span class="koboSpan" id="kobo.589.2">To make sure it is worth using a deep learning model to optimize this process, let’s say the company needs to gain at least $120,000 a year in cash to make this worthwhile. </span><span class="koboSpan" id="kobo.589.3">Let’s also consider that maintaining a machine learning model costs $20,000 per month. </span><span class="koboSpan" id="kobo.589.4">The threshold in any metric needs to relate to the result of making at least $30,000 gains </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">per month.</span></span></p>
<p><span class="koboSpan" id="kobo.591.1">Say we </span><a id="_idIndexMarker834"/><span class="koboSpan" id="kobo.592.1">want to use an F score-based metric. </span><span class="koboSpan" id="kobo.592.2">Since false negatives affect the total money more than false positives, we might want to use the F2 score instead of the F1 score. </span><span class="koboSpan" id="kobo.592.3">But will models with the same metric score exhibit different monetary returns for either of the two metrics? </span><span class="koboSpan" id="kobo.592.4">This is essential to understand so that a proper success threshold can be set. </span><span class="koboSpan" id="kobo.592.5">Let’s attempt to use Python code to analyze </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">score behavior:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.594.1">First, we define the number of actual positives and actual negatives and the </span><span class="No-Break"><span class="koboSpan" id="kobo.595.1">total data:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.596.1">
actual_positive = 95
total_data = 10000
actual_negative = total_data – actual_positive</span></pre></li> <li><span class="koboSpan" id="kobo.597.1">Next, let’s define methods to compute precision, recall, F1 score, and </span><span class="No-Break"><span class="koboSpan" id="kobo.598.1">F2 score:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.599.1">
def precision(tp, fp):
     denominator = tp + fp
     if denominator == 0:
           return 0
     return tp/ denominator
def recall(tp, fn):
     denominator = tp + fn
     if denominator == 0:
           return 0
     return tp/denominator
def f1score(tp, fp, fn):
     prec = precision(tp, fp)
     rec = recall(tp, fn)
     denominator = prec+ rec
     if denominator == 0:
           return 0
     return 2 * (prec * rec) / denominator
def fbeta(tp, fp, fn, beta=0.5):
     prec = precision(tp, fp)
     rec = recall(tp, fn)
     denominator = beta**2 * prec + rec
     if denominator == 0:
           return 0
     return (1+beta**2) * (prec * rec) / denominator</span></pre></li> <li><span class="koboSpan" id="kobo.600.1">Next, let’s</span><a id="_idIndexMarker835"/><span class="koboSpan" id="kobo.601.1"> define the method to compute the final cash we </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">will have:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.603.1">
def compute_total_cash(tp, fp, fn, tn):
     return tp * -50 + fp * -50 + fn * -1050 + tn * 150</span></pre></li> <li><span class="koboSpan" id="kobo.604.1">Let’s use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.605.1">compute_total_cash</span></strong><span class="koboSpan" id="kobo.606.1"> method to compute the baseline cash for the current setup where no model is used. </span><span class="koboSpan" id="kobo.606.2">This is so that we can find out the cash success threshold required for a model to be considered valuable enough to </span><span class="No-Break"><span class="koboSpan" id="kobo.607.1">be used:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.608.1">
baseline_cash = compute_total_cash(0, 0, actual_positive, actual_negative)
threshold_cash_line = baseline_cash + 30000</span></pre></li> <li><span class="koboSpan" id="kobo.609.1">Next, we</span><a id="_idIndexMarker836"/><span class="koboSpan" id="kobo.610.1"> will simulate every possible combination of true positives (</span><strong class="source-inline"><span class="koboSpan" id="kobo.611.1">tp</span></strong><span class="koboSpan" id="kobo.612.1">), false positives (</span><strong class="source-inline"><span class="koboSpan" id="kobo.613.1">fp</span></strong><span class="koboSpan" id="kobo.614.1">), true negatives (</span><strong class="source-inline"><span class="koboSpan" id="kobo.615.1">tn</span></strong><span class="koboSpan" id="kobo.616.1">), and false negatives (</span><strong class="source-inline"><span class="koboSpan" id="kobo.617.1">fn</span></strong><span class="koboSpan" id="kobo.618.1">) and compute the F1 and </span><span class="No-Break"><span class="koboSpan" id="kobo.619.1">F2 scores:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.620.1">
f1_scores = []
f2_scores = []
total_cash = []
for tp in range(0, 96):
     fn = 95 - tp
     for fp in range(0, actual_negative):
           tn = total_data - tp - fp - fn
           f1_scores.append(f1score(tp, fp, fn))
           f2_scores.append(fbeta(tp, fp, fn, beta=2.0))
           total_cash.append(compute_total_cash(tp, fp, fn, tn)</span></pre></li> <li><span class="koboSpan" id="kobo.621.1">Now, let’s plot both of the scores independently against the total cash return while drawing horizontal lines using </span><strong class="source-inline"><span class="koboSpan" id="kobo.622.1">threshold_cash_line</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.623.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.624.1">baseline_cash</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.626.1">
import matplotlib.pyplot as plt
fig, axs = plt.subplots(2, figsize=(18, 15))
axs[0].scatter(f1_scores, total_cash, alpha=0.01)
axs[1].scatter(f2_scores, total_cash, alpha=0.01)
for i in range(2):
     axs[i].axhline(y=baseline_cash, color='r', linestyle='dotted')
     axs[i].axhline(y=threshold_cash_line, color='y', linestyle='dashdot')</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.627.1">This will result in the </span><span class="No-Break"><span class="koboSpan" id="kobo.628.1">following figure:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer105">
<span class="koboSpan" id="kobo.629.1"><img alt="" role="presentation" src="image/B18187_10.01_(A).jpg"/></span>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer106">
<span class="koboSpan" id="kobo.630.1"><img alt="Figure 10.1 – Cash ﻿versus F1 score and cash ﻿versus F2 score" src="image/B18187_10.01_(B).jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.631.1">Figure 10.1 – Cash versus F1 score and cash versus F2 score</span></p>
<ol>
<li value="7"><span class="koboSpan" id="kobo.632.1">The </span><a id="_idIndexMarker837"/><span class="koboSpan" id="kobo.633.1">figure suggests that the F2 score has huge fluctuations in the lower score range even though it weights recall higher. </span><span class="koboSpan" id="kobo.633.2">The goal here is to make sure a metric can be properly linked to success, so having wider cash fluctuations with the same score isn’t a desirable trait. </span><span class="koboSpan" id="kobo.633.3">F1 score would probably be the wiser choice here. </span><span class="koboSpan" id="kobo.633.4">Using the topmost horizontal line (which references the minimum 30,000 cash threshold we need to hit) as a reference, we want to find a point where it is not even possible to get a lower score than the threshold in the F1 score graph. </span><span class="koboSpan" id="kobo.633.5">Roughly, a 0.65 F1 score should guarantee that the model can produce </span><span class="No-Break"><span class="koboSpan" id="kobo.634.1">a score.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.635.1">This example demonstrates the level of analysis required to properly choose a metric and find a threshold that can be directly linked to success while taking into consideration both monetary profit and loss. </span><span class="koboSpan" id="kobo.635.2">However, it is important to note that not all machine learning projects can be measured in terms of dollar cost. </span><span class="koboSpan" id="kobo.635.3">Some projects may not be directly related to cash, and that is perfectly acceptable. </span><span class="koboSpan" id="kobo.635.4">However, to be successful, it is</span><a id="_idIndexMarker838"/><span class="koboSpan" id="kobo.636.1"> important to quantify the value of the model in a way that stakeholders can understand. </span><span class="koboSpan" id="kobo.636.2">If nobody understands the value that the model provides, the project is unlikely to </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">be successful.</span></span></p>
<p><span class="koboSpan" id="kobo.638.1">Next, let’s dive into the idea of directly optimizing the metric in a deep </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">learning model.</span></span></p>
<h1 id="_idParaDest-163"><a id="_idTextAnchor170"/><span class="koboSpan" id="kobo.640.1">Directly optimizing the metric</span></h1>
<p><span class="koboSpan" id="kobo.641.1">The</span><a id="_idIndexMarker839"/><span class="koboSpan" id="kobo.642.1"> loss and the metric used to train a deep learning model are two separate components. </span><span class="koboSpan" id="kobo.642.2">One of the tricks you can use to improve a model’s accuracy performance against the chosen metric is to directly optimize against it instead of just monitoring performance for the purpose of choosing the best performing model weights and using early stopping. </span><span class="koboSpan" id="kobo.642.3">In other words, using the metric as a </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">loss directly!</span></span></p>
<p><span class="koboSpan" id="kobo.644.1">By directly optimizing for the metric of interest, the model has a chance to improve in a way that is relevant to the end goal rather than optimizing for a proxy loss function that may not be directly related to the ultimate performance of the model. </span><span class="koboSpan" id="kobo.644.2">This simply means that the model can result in a much better performance when using the metric as a </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">loss directly.</span></span></p>
<p><span class="koboSpan" id="kobo.646.1">However, not all metrics can be used as a loss, as not all metrics can be differentiable. </span><span class="koboSpan" id="kobo.646.2">Remember that backpropagation requires all functions used to be differentiable so that gradients can be computed to update the neural network weights. </span><span class="koboSpan" id="kobo.646.3">Note that discontinuous methods are not all differentiable. </span><span class="koboSpan" id="kobo.646.4">Here are some common discontinuous functions that are not differentiable, along with NumPy methods to look out for </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">easy identification:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.648.1">Max and min functions</span></strong><span class="koboSpan" id="kobo.649.1">: NumPy </span><a id="_idIndexMarker840"/><span class="koboSpan" id="kobo.650.1">methods to look out for are </span><strong class="source-inline"><span class="koboSpan" id="kobo.651.1">np.min</span></strong><span class="koboSpan" id="kobo.652.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.653.1">np.max</span></strong><span class="koboSpan" id="kobo.654.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.655.1">np.argmin</span></strong><span class="koboSpan" id="kobo.656.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.657.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.658.1">np.argmax</span></strong></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.659.1">Clipping functions</span></strong><span class="koboSpan" id="kobo.660.1">: NumPy </span><a id="_idIndexMarker841"/><span class="koboSpan" id="kobo.661.1">methods to look out for </span><span class="No-Break"><span class="koboSpan" id="kobo.662.1">are </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.663.1">np.clip</span></strong></span></li>
<li><span class="koboSpan" id="kobo.664.1">Other functions in NumPy to look out for are </span><strong class="source-inline"><span class="koboSpan" id="kobo.665.1">np.sign</span></strong><span class="koboSpan" id="kobo.666.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.667.1">np.piecewise</span></strong><span class="koboSpan" id="kobo.668.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.669.1">np.digitize</span></strong><span class="koboSpan" id="kobo.670.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.671.1">np.searchsorted</span></strong><span class="koboSpan" id="kobo.672.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.673.1">np.histogram</span></strong><span class="koboSpan" id="kobo.674.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.675.1">np.fft</span></strong><span class="koboSpan" id="kobo.676.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.677.1">np.count_nonzero</span></strong><span class="koboSpan" id="kobo.678.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.679.1">np.round</span></strong><span class="koboSpan" id="kobo.680.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.681.1">np.cumsum</span></strong><span class="koboSpan" id="kobo.682.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.683.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.684.1">np.percentile</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.685.1">Additionally, using a metric as a loss function can sometimes lead to suboptimal performance because the metric does not always capture all aspects of the problem that the model needs to learn in order to perform well. </span><span class="koboSpan" id="kobo.685.2">Some important aspects of a problem may be difficult to measure directly or to include in a metric. </span><span class="koboSpan" id="kobo.685.3">These aspects may form the foundation needed for a model to learn before it can proceed to slowly get better at the chosen metric. </span><span class="koboSpan" id="kobo.685.4">For example, in image recognition, the model needs to learn to </span><a id="_idIndexMarker842"/><span class="koboSpan" id="kobo.686.1">recognize more abstract features, such as texture, lighting, or viewpoint before it can attempt to get better at accuracy. </span><span class="koboSpan" id="kobo.686.2">If these features are not captured in the metric, the model may not learn to recognize them, resulting in suboptimal performance. </span><span class="koboSpan" id="kobo.686.3">A good solution here is to experiment with more conventional loss functions initially and then fine-tune the model using either only the metric as a loss or a combination of the original loss and the metric as </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">a loss.</span></span></p>
<p><span class="koboSpan" id="kobo.688.1">While using a metric as a loss function can be beneficial in some cases, it’s not a surefire method of improving performance. </span><span class="koboSpan" id="kobo.688.2">The efficacy of this approach largely depends on the specific use case and the complexity of the problem being addressed. </span><span class="koboSpan" id="kobo.688.3">The performance boost achieved through this method might be minimal and too nuanced for some projects to even consider. </span><span class="koboSpan" id="kobo.688.4">However, when used successfully, it can lead to meaningful improvements </span><span class="No-Break"><span class="koboSpan" id="kobo.689.1">in performance.</span></span></p>
<h1 id="_idParaDest-164"><a id="_idTextAnchor171"/><span class="koboSpan" id="kobo.690.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.691.1">In this chapter, we briefly explored an overview of different model evaluation methods and how they can be used to measure the performance of a deep learning model. </span><span class="koboSpan" id="kobo.691.2">We started with the topic of metric engineering among all the introduced methods. </span><span class="koboSpan" id="kobo.691.3">We introduced common base model evaluation metrics. </span><span class="koboSpan" id="kobo.691.4">On top of this, we discussed the limitations of using base model evaluation metrics and introduced the concept of engineering a model evaluation metric tailored to the specific problem at hand. </span><span class="koboSpan" id="kobo.691.5">We also explored the idea of optimizing directly against the evaluation metric by using it as a loss function. </span><span class="koboSpan" id="kobo.691.6">While this approach can be beneficial, it is important to consider the potential pitfalls and limitations, as well as the specific use case for which this approach may </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">be appropriate.</span></span></p>
<p><span class="koboSpan" id="kobo.693.1">The evaluation of deep learning models requires careful consideration of appropriate evaluation methods, metrics, and statistical tests. </span><span class="koboSpan" id="kobo.693.2">Hopefully, after reading through this chapter, I have helped ease your journey into metric engineering, encouraged you to take the first step toward deeper metric engineering by following the guidelines provided, and highlighted the importance of metric engineering as a valuable component to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">model evaluation.</span></span></p>
<p><span class="koboSpan" id="kobo.695.1">However, some information can be hidden behind a single metric value no matter how good and appropriate the final chosen metric is. </span><span class="koboSpan" id="kobo.695.2">In the next chapter, we will cover a key method that can help uncover either wanted or unwanted hidden behaviors of your neural model based on how your neural network model </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">makes predictions.</span></span></p>
</div>
</body></html>