<html><head></head><body>
		<div>
			<div id="_idContainer074" class="Content">
			</div>
		</div>
		<div id="_idContainer075" class="Content">
			<h1 id="_idParaDest-65"><a id="_idTextAnchor066"/>3. Deep Learning with Keras</h1>
		</div>
		<div id="_idContainer094" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, you will experiment with different neural network architectures. You will create Keras sequential models—building single-layer and multi-layer models—and evaluate the performance of trained models. Networks of different architectures will help you understand overfitting and underfitting. By the end of this chapter, you will have explored early stopping that can be used to combat overfitting to the training data.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor067"/>Introduction</h1>
			<p>In the previous chapter, you learned about the mathematics of neural networks, including <strong class="source-inline">linear transformations</strong> with <strong class="source-inline">scalars</strong>, <strong class="source-inline">vectors</strong>, <strong class="source-inline">matrices</strong>, and <strong class="source-inline">tensors</strong>. Then, you implemented your first neural network using Keras by building a logistic regression model to classify users of a website into those who will purchase from the website and those who will not.</p>
			<p>In this chapter, you will extend your knowledge of building neural networks using Keras. This chapter covers the basics of deep learning and will provide you with the necessary foundations so that you can build highly complex neural network architectures. We will start by extending the <strong class="source-inline">logistic regression</strong> model to a simple single-layer neural network and then proceed to more complicated neural networks with multiple hidden layers. </p>
			<p>In this process, you will learn about the underlying basic concepts of neural networks, including forward propagation for making predictions, computing loss, backpropagation for computing derivatives of loss with respect to model parameters, and, finally, gradient descent for learning about optimal parameters for the model. You will also learn about the various choices that are available so that you can build and train a neural network in terms of <strong class="source-inline">activation functions</strong>, <strong class="source-inline">loss functions</strong>, and <strong class="source-inline">optimizers</strong>.</p>
			<p>Furthermore, you will learn how to evaluate your model while understanding issues such as <strong class="bold">overfitting</strong> and <strong class="bold">underfitting</strong>, all while looking at how they can impact the performance of your model and how to detect them. You will learn about the drawbacks of evaluating a model on the same dataset that's used for training, as well as the alternative approach of holding back a part of the available dataset for evaluation purposes. Subsequently, you will learn how to compare the model error rate on each of these two subsets of the dataset that can be used to detect problems such as high bias and high variance in the model. Lastly, you will learn about a technique called <strong class="bold">early stopping</strong> to reduce overfitting, which is again based on comparing the model's error rate to the two subsets of the dataset.</p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor068"/>Building Your First Neural Network</h1>
			<p>In this section, you will learn about the representations and concepts of deep learning, such as <strong class="bold">forward propagation</strong>—the propagation of data through the network, multiplying the input values by the weight of each connection for every node, and <strong class="bold">backpropagation</strong>—the calculation of the gradient of the loss function with respect to the weights in the matrix, and <strong class="bold">gradient descent</strong>—the optimization algorithm that's used to find the minimum of the loss function. </p>
			<p>We will not delve deeply into these concepts as it isn't required for this book. However, this coverage will essentially help anyone who wants to apply deep learning to a problem.</p>
			<p>Then, we will move on to implementing neural networks using Keras. Also, we will stick to the simplest case, which is a neural network with a single hidden layer. You will learn how to define a model in Keras, choose the <strong class="bold">hyperparameters</strong>—the parameters of the model that are set before training the model—and then train your model. At the end of this section, you will have the opportunity to practice what you have learned by implementing a neural network in Keras so that you can perform classification on a dataset and observe how neural networks outperform simpler models such as logistic regression.</p>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor069"/>Logistic Regression to a Deep Neural Network</h2>
			<p>In <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Machine Learning with Keras</em>, you learned about the logistic regression model, and then how to implement it as a sequential model using Keras in <em class="italic">Chapter 2</em>, <em class="italic">Machine Learning versus Deep Learning</em>. Technically speaking, logistic regression involves a very simple neural network with only one hidden layer and only one node in its hidden layer.</p>
			<p>An overview of the logistic regression model with two-dimensional input can be seen in the following image. What you see in this image is called one <strong class="bold">node</strong> or <strong class="bold">unit</strong> in the deep learning world, which is represented by the green circle. As you may have noticed, there are some differences between logistic regression terminology and deep learning terminology. In logistic regression, we call the parameters of the model <strong class="bold">coefficients</strong> and <strong class="bold">intercepts</strong>. In deep learning models, the parameters are referred to as <strong class="bold">weights</strong> (w) and <strong class="bold">biases</strong> (b):</p>
			<p> </p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B15777_03_01.jpg" alt="Figure 3.1: Overview of the logistic regression model with a two-dimensional input&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.1: Overview of the logistic regression model with a two-dimensional input</p>
			<p>At each node/unit, the inputs are multiplied by some weights and then a bias term is added to the sum of these weighted inputs. This can be seen in the calculation above the node in the preceding image. The <strong class="source-inline">inputs</strong> are <strong class="source-inline">X1</strong> and <strong class="source-inline">X2</strong>, the <strong class="source-inline">weights</strong> are <strong class="source-inline">W1</strong> and <strong class="source-inline">W2</strong>, and the <strong class="source-inline">bias</strong> is <strong class="source-inline">b</strong>. Next, a nonlinear function (for example, a sigmoid function in the case of a logistic regression model) is applied to the sum of the weighted inputs and the bias term is used to compute the final output of the node. In the calculation shown in the preceding image, this is <strong class="source-inline">σ</strong>. In deep learning, the nonlinear function is called the <strong class="bold">activation function</strong> and the output of the node is called the <strong class="bold">activation</strong> of that node.</p>
			<p>It is possible to build a single-layer neural network by stacking logistic regression nodes/units on top of each other in a layer, as shown in the following image. Every value at the input layers, <strong class="source-inline">X1</strong> and <strong class="source-inline">X2</strong>, is passed to all three nodes at the hidden layer:</p>
			<div>
				<div id="_idContainer077" class="IMG---Figure">
					<img src="image/B15777_03_02.jpg" alt="Figure 3.2: Overview of a single-layer neural network with a two-dimensional input &#13;&#10;and a hidden layer of size 3&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.2: Overview of a single-layer neural network with a two-dimensional input and a hidden layer of size 3</p>
			<p>It is also possible to build multi-layer neural networks by stacking multiple layers of processing nodes after one another, as shown in the following image. The following image shows a two-layer neural network with two-dimensional input:</p>
			<div>
				<div id="_idContainer078" class="IMG---Figure">
					<img src="image/B15777_03_03.jpg" alt="Figure 3.3: Overview of a two-layer neural network with a two-dimensional input&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.3: Overview of a two-layer neural network with a two-dimensional input</p>
			<p>The preceding two images show the most common way of representing a neural network. Every neural network consists of an <strong class="bold">input layer</strong>, an <strong class="bold">output layer</strong>, and one or many <strong class="bold">hidden layers</strong>. If there is only one hidden layer, the network is called a <strong class="bold">shallow neural network</strong>. On the other hand, neural networks with many hidden layers are called <strong class="bold">deep neural networks</strong>, and the process of training them is called <strong class="bold">deep learning</strong>. </p>
			<p><em class="italic">Figure 3.2</em> shows a neural network with only one hidden layer, so this would be a shallow neural network, whereas the neural network in <em class="italic">Figure 3.3</em> has two hidden layers, so it is a deep neural network. The input layers are generally on the left. In the case of <em class="italic">Figure 3.3</em>, these are features <strong class="source-inline">X1</strong> and <strong class="source-inline">X2</strong>, and they are input into the first hidden layer, which has three nodes. The arrows represent the weight values that are applied to the input. At the second hidden layer, the result of the first hidden layer becomes the input to the second hidden layer. The arrows between the first and second hidden layers represent the weights. The output is generally the layer on the far right and, in the case of <em class="italic">Figure 3.3</em>, is represented by the layer labeled <strong class="source-inline">Y</strong>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">In some resources, you may see that a network, such as the one shown in the preceding image, is referred to as a <strong class="bold">four-layer network</strong>. This is because the input and output layers are counted along with the hidden layers. However, the more common convention is to count only the hidden layers, so the network we mentioned previously will be referred to as a two-layer network.</p>
			<p>In a deep learning setting, the number of nodes in the input layer is equal to the number of features of the input data, and the number of nodes in the output layer is equal to the number of dimensions of the output data. However, you need to select the number of nodes in the hidden layers or the size of the hidden layers. If you choose a larger size layer, the model becomes more flexible and will be able to model more complex functions. This increase in flexibility comes at the cost of the need for more training data and more computations to train the model on. The parameters that are required to be selected by the developer are called <strong class="source-inline">hyperparameters</strong> and include parameters such as the number of layers and the number of nodes in each layer. Common hyperparameters to be chosen include the number of epochs to train for and the loss function to use. </p>
			<p>In the next section, we will cover <strong class="source-inline">activation functions</strong> that are applied after each hidden layer.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor070"/>Activation Functions</h2>
			<p>In addition to the size of the layer, you need to choose an activation function for each hidden layer that you add to the model, and also do the same for the output layer. We learned about the sigmoid activation function in the logistic regression model. However, there are more options for activation functions that you can choose from when building a neural network in Keras. For example, the sigmoid activation function is a good choice as the activation function on the output layer for binary classification tasks since the result of a sigmoid function is bounded between 0 and 1. Some commonly used activation functions for deep learning are <strong class="bold">sigmoid</strong>/<strong class="bold">logistic</strong>, <strong class="bold">tanh</strong> (<strong class="bold">hyperbolic tangent</strong>), and <strong class="bold">Rectified Linear Unit</strong> (<strong class="bold">ReLU</strong>).</p>
			<p>The following image shows a <strong class="source-inline">sigmoid</strong> activation function:</p>
			<div>
				<div id="_idContainer079" class="IMG---Figure">
					<img src="image/B15777_03_04.jpg" alt="Figure 3.4: Sigmoid activation function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.4: Sigmoid activation function</p>
			<p>The following image shows a <strong class="source-inline">tanh</strong> activation function:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/B15777_03_05.jpg" alt="Figure 3.5: tanh activation function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.5: tanh activation function</p>
			<p>The following image shows a <strong class="source-inline">ReLU</strong> activation function:</p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/B15777_03_06.jpg" alt="Figure 3.6: ReLU activation function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.6: ReLU activation function</p>
			<p>As you can see in <em class="italic">Figures 3.4</em> and <em class="italic">3.5</em>, the output of a sigmoid function is always between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, and the output of tanh is always between <strong class="source-inline">-1</strong> and <strong class="source-inline">1</strong>. This makes <strong class="source-inline">tanh</strong> a better choice for hidden layers since it keeps the average of the outputs in each layer close to zero. In fact, <strong class="source-inline">sigmoid</strong> is only a good choice for the <strong class="source-inline">activation function</strong> of the output layer when building a <strong class="source-inline">binary classifier</strong> since its output can be interpreted as the probability of a given input belonging to one class. </p>
			<p>Therefore, <strong class="source-inline">tanh</strong> and <strong class="source-inline">ReLU</strong> are the most common choices of activation function for hidden layers. It turns out that the learning process is faster when using the <strong class="source-inline">ReLU activation function</strong> because it has a fixed derivative (or slope) for an input greater than <strong class="source-inline">0</strong>, and a slope of <strong class="source-inline">0</strong> everywhere else.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can read more about all the available choices for activation functions in Keras here: <a href="https://keras.io/activations/">https://keras.io/activations/</a>.</p>
			<h2 id="_idParaDest-70"><a id="_idTextAnchor071"/>Forward Propagation for Making Predictions</h2>
			<p>Neural networks make a prediction about the output by performing <strong class="bold">forward propagation</strong>. Forward propagation entails the computations that are performed on the input in every layer of a neural network until the output layer is reached. It is best to understand forward propagation through an example.</p>
			<p>Let's go through forward propagation equations one by one for a two-layer neural network (shown in the following image) where the input data is two-dimensional, and the output data is a one-dimensional binary class label. The activation functions for layer 1 and layer 2 will be tanh, and the activation function in the output layer is sigmoid.</p>
			<p>The following image shows the weights and biases for each layer as matrices and vectors with proper indexes. For each layer, the number of rows in the weight's matrix is equal to the number of nodes in the previous layer, and the number of columns is equal to the number of nodes in that layer. </p>
			<p>For example, <strong class="source-inline">W1</strong> has two rows and three columns because the input to layer 1 is the input layer, <strong class="source-inline">X</strong>, which has two columns, and layer 1 has three nodes. Likewise, <strong class="source-inline">W2</strong> has three rows and three columns because the input to layer 2 is layer 1, which has two nodes, and layer 2 has five nodes. The bias, however, is always a vector with a size equal to the number of nodes in that layer. The total number of parameters in a deep learning model is equal to the total number of elements in all the weights' matrices and the biases' vectors:</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/B15777_03_07.jpg" alt="Figure 3.7: A two-layer neural network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.7: A two-layer neural network</p>
			<p>An example of performing all the steps for forward propagation according to the neural network outlined in the preceding image is as follows.</p>
			<p><strong class="bold">Steps to perform forward propagation</strong>:</p>
			<ol>
				<li><strong class="source-inline">X</strong> is the network input to the network in the preceding image, so it is the input for the first hidden layer. First, the input matrix, <strong class="source-inline">X</strong>, is the matrix multiplied by the weight matrix for layer 1, <strong class="source-inline">W1</strong>, and the bias, <strong class="source-inline">b1</strong>, is added:<p><em class="italic">z1 = X*W1 + b1</em></p></li>
				<li>Next, the layer 1 output is computed by applying an activation function to <em class="italic">z1</em>, which is the output of the previous step:<p><em class="italic">a1 = tanh(z1)</em></p></li>
				<li><strong class="source-inline">a1</strong> is the output of layer 1 and is called the <strong class="source-inline">activation of layer 1</strong>. The output of layer 1 is, in fact, the <strong class="source-inline">input</strong> for layer 2. Next, the activation of layer 1 is the matrix multiplied by the weight matrix for layer 2, <strong class="source-inline">W2</strong>, and the bias, <strong class="source-inline">b2</strong>, is added:<p><em class="italic">z2 = a1 * W2 + b2</em></p></li>
				<li>The layer 2 output/activation is computed by applying an activation function to <strong class="source-inline">z2</strong>:<p><em class="italic">a2 = tanh(z2)</em></p></li>
				<li>The output of layer 2 is, in fact, the input for the next layer (the network output layer here). Following this, the activation of layer 2 is the matrix multiplied by the weight matrix for the output layer, <strong class="source-inline">W3</strong>, and the bias, <strong class="source-inline">b3</strong>, is added:<p><em class="italic">z3 = a2 * W3 + b3</em></p></li>
				<li>Finally, the network output, Y, is computed by applying the sigmoid activation function to <strong class="source-inline">z3</strong>:<p><em class="italic">Y = sigmoid(z3)</em></p></li>
			</ol>
			<p>The total number of parameters in this model is equal to the sum of the number of elements in <strong class="source-inline">W1</strong>, <strong class="source-inline">W2</strong>, <strong class="source-inline">W3</strong>, <strong class="source-inline">b1</strong>, <strong class="source-inline">b2</strong>, and <strong class="source-inline">b3</strong>. Therefore, the number of parameters can be calculated by summing the parameters in each of the parameters in weight matrices and biases, which is equal to 6 + 15 + 5 + 3 + 5 + 1 = 35. These are the parameters that need to be learned in the process of deep learning. </p>
			<p>Now that we have learned about the forward propagation step, we have to evaluate our model and compare it to the real target values. To achieve that, we will use a loss function, which we will cover in the next section. Here, we will learn about some common loss functions that we can use for classification and regression tasks.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor072"/>Loss Function</h2>
			<p>When learning the optimal parameters (weights and biases) of a model, we need to define a function to measure error. This function is called the <strong class="bold">loss function</strong> and it provides us with a measure of how different network-predicted outputs are from the real outputs in the dataset.</p>
			<p>The loss function can be defined in several different ways, depending on the problem and the goal. For example, in the case of a classification problem, one common way to define loss is to compute the proportion of misclassified inputs in the dataset and use that as the probability of the model making an error. On the other hand, in the case of a regression problem, the loss function is usually defined by computing the distance between the predicted outputs and their corresponding real outputs, and then averaging over all the examples in the dataset.</p>
			<p>Brief descriptions of some commonly used loss functions that are available in Keras are as follows:</p>
			<ul>
				<li><strong class="bold">mean_squared_error</strong> is a loss function for regression problems that calculates <strong class="source-inline">(real output – predicted output)^2</strong> for each example in the dataset and then returns their average.</li>
				<li><strong class="bold">mean_absolute_error</strong> is a loss function for regression problems that calculates <strong class="source-inline">abs (real output – predicted output)</strong> for each example in the dataset and then returns their average.</li>
				<li><strong class="bold">mean_absolute_percentage_error</strong> is a loss function for regression problems that calculates <strong class="source-inline">abs [(real output – predicted output) / real output]</strong> for each example in the dataset and then returns their average, multiplied by 100%.</li>
				<li><strong class="bold">binary_crossentropy</strong> is a loss function for two-class/binary classification problems. In general, the cross-entropy loss is used for calculating the loss for models where the output is a probability number between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>.</li>
				<li><strong class="bold">categorical_crossentropy</strong> is a loss function for multi-class (more than two classes) classification problems.<p class="callout-heading">Note</p><p class="callout">You can read more about all the available choices for loss functions in Keras here: <a href="https://keras.io/losses/">https://keras.io/losses/</a>.</p></li>
			</ul>
			<p>During the training process, we keep changing the model parameters until the minimum difference between the model-predicted outputs and the real outputs is reached. This is called an <strong class="bold">optimization process</strong>, and we will learn more about how it works in later sections. For neural networks, we use backpropagation to compute the derivatives of the loss function with respect to the weights.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor073"/>Backpropagation for Computing Derivatives of Loss Function</h2>
			<p><strong class="bold">Backpropagation</strong> is the process of performing the chain rule of calculus from the output layer to the input layer of a neural network in order to compute the derivatives of the loss function with respect to the model parameters in each layer. The derivative of a function is simply the slope of that function. We are interested in the slope of the loss function because it provides us with the direction in which model parameters need to change in order for the loss value to be minimized.</p>
			<p>The chain rule of calculus states that if, for example, <strong class="source-inline">z</strong> is a function of <strong class="source-inline">y</strong>, and <strong class="source-inline">y</strong> is a function of <strong class="source-inline">x</strong>, then the derivative of <strong class="source-inline">z</strong> with respect to <strong class="source-inline">x</strong> can be reached by multiplying the derivative of <strong class="source-inline">z</strong> with respect to <strong class="source-inline">y</strong> by the derivative of <strong class="source-inline">y</strong> with respect to <strong class="source-inline">x</strong>. This can be written as follows:</p>
			<p><em class="italic">dz/dx = dz/dy * dy/dx</em></p>
			<p>In deep neural networks, the loss function is a function of predicted outputs. We can show this through the equation given here:</p>
			<p><em class="italic">loss = L(y_predicted)</em></p>
			<p>On the other hand, according to forward propagation equations, the output predicted by the model is a function of the model parameters—that is, the weights and biases in each layer. Therefore, according to the chain rule of calculus, the derivative of the loss with respect to the model parameters can be computed by multiplying the derivative of the loss with respect to the predicted output by the derivative of the predicted output with respect to the model parameters.</p>
			<p>In the next section, we will learn how the optimal weight parameters are modified when given the derivatives of the loss function with respect to the weights.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor074"/>Gradient Descent for Learning Parameters</h2>
			<p>In this section, we will learn how a deep learning model learns its optimal parameters. Our goal is to update the weight parameters so that the loss function is minimized. This will be an iterative process in which we continue to update the weight parameters so that the loss function is at a minimum. This process is called <strong class="bold">learning parameters</strong> and it is done through the use of an optimization algorithm. One very common optimization algorithm that's used for learning parameters in machine learning is <strong class="bold">gradient descent</strong>. Let's see how gradient descent works.</p>
			<p>If we plot the average of loss over all the examples in the dataset for all the possible values of the model parameters, it is usually a convex shape (such as the one shown in the following plot). In gradient descent, our goal is to find the minimum point (<strong class="source-inline">Pt</strong>) on the plot. The algorithm starts by initializing the model parameters with some random values (<strong class="source-inline">P1</strong>). Then, it computes the loss and the derivatives of the loss with respect to the parameters at that point. As we mentioned previously, the derivative of a function is, in fact, the slope of the function. After computing the slope at an initial point, we have the direction in which we need to update the parameters.</p>
			<p>The hyperparameter, called the <strong class="bold">learning rate</strong> (<strong class="bold">alpha</strong>), determines how big a step the algorithm will take from the initial point. After selecting the proper alpha value, the algorithm updates the parameters from their initial values to the new values (shown as point <strong class="source-inline">P2</strong> in the following plot). As shown in the following plot, <strong class="source-inline">P2</strong> is closer to the target point, and if we keep moving in that direction, we will eventually get to the target point, <strong class="source-inline">Pt</strong>. The algorithm computes the slope of the function again at <strong class="source-inline">P2</strong> and takes another step. </p>
			<p>This process is repeated until the slope is equal to zero and therefore no direction for further movement is provided:</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B15777_03_08.jpg" alt="Figure 3.8: A schematic view of the gradient descent algorithm finding the set &#13;&#10;of parameters that minimize loss&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.8: A schematic view of the gradient descent algorithm finding the set of parameters that minimize loss</p>
			<p>The pseudocode for the gradient descent algorithm is provided here:</p>
			<p class="source-code">Initialize all the weights (w) and biases (b) arbitrarily</p>
			<p class="source-code">Repeat Until converge {</p>
			<p class="source-code">Compute loss given w and b</p>
			<p class="source-code">Compute derivatives of loss with respect to w (dw), and with respect to b (db) using backpropagation</p>
			<p class="source-code">Update w to w – alpha * dw</p>
			<p class="source-code">Update b to b – alpha * db</p>
			<p class="source-code">}</p>
			<p>To summarize, the following steps are repeated when training a deep neural network (after initializing the parameters to some random values):</p>
			<ol>
				<li value="1">Use forward propagation and the current parameters to predict the outputs for the entire dataset.</li>
				<li>Use the predicted outputs to compute the loss over all the examples.</li>
				<li>Use backpropagation to compute the derivatives of the loss with respect to the weights and biases at each layer.</li>
				<li>Update the weights and biases using the derivative values and the learning rate.</li>
			</ol>
			<p>What we discussed here was the standard gradient descent algorithm, which computes the loss and the derivatives using the entire dataset in order to update the parameters. There is another version of gradient descent called <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>), which computes the loss and the derivatives each time using a subset or a batch of data examples only; therefore, its learning process is faster than standard gradient descent.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Another common choice is an optimization algorithm called <strong class="bold">Adam</strong>. Adam usually outperforms <strong class="source-inline">SGD</strong> when training deep learning models. As we've already learned, <strong class="source-inline">SGD</strong> uses a single hyperparameter (called a <strong class="bold">learning rate</strong>) to update the parameters. However, Adam improves this process by using a learning rate, a weighted average of gradients, and a weighted average of squared gradients to update the parameters at each iteration.</p>
			<p>Usually, when building a neural network, you need to choose two hyperparameters (called the <strong class="bold">batch size</strong> and the number of <strong class="bold">epochs</strong>) for your optimization process. The <strong class="source-inline">batch_size</strong> argument determines the number of data examples to be included at each iteration of the optimization algorithm. <strong class="source-inline">batch_size=None</strong> is equivalent to the standard version of gradient descent, which uses the entire dataset in each iteration. The <strong class="source-inline">epochs</strong> argument determines how many times the optimization algorithm passes through the entire training dataset before it stops. </p>
			<p>For example, imagine we have a dataset of size <strong class="source-inline">n=400</strong>, and we choose <strong class="source-inline">batch_size=5</strong> and <strong class="source-inline">epochs=20</strong>. In this case, the optimizer will have <strong class="source-inline">400/5 = 80</strong> iterations in one pass through the entire dataset. Since it is supposed to go through the entire dataset <strong class="source-inline">20</strong> times, it will have <strong class="source-inline">80 * 20</strong> iterations in total.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">When building a model in Keras, you need to choose the type of optimizer to be used when training your model. There are some other options other than SGD and Adam available in Keras. You can read more about all the possible options for optimizers in Keras here: <a href="https://keras.io/optimizers/">https://keras.io/optimizers/</a>.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">All the activities and exercises in this chapter will be developed in a Jupyter notebook. Please download this book's GitHub repository, along with all the prepared templates, from <a href="https://packt.live/39pOUMT">https://packt.live/39pOUMT</a>.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor075"/>Exercise 3.01: Neural Network Implementation with Keras</h2>
			<p>In this exercise, you will learn the step-by-step process of implementing a neural network using Keras. Our simulated dataset represents various measurements of trees, such as height, the number of branches, the girth of the trunk at the base, and more, that are found in a forest. Our goal is to classify the records into either deciduous or coniferous type trees based on the measurements given. First, execute the following code block to load a simulated dataset of <strong class="source-inline">10000</strong> records that consist of two classes, representing the two tree species, where each data example has <strong class="source-inline">10</strong> feature values:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">X = pd.read_csv('../data/tree_class_feats.csv')</p>
			<p class="source-code">y = pd.read_csv('../data/tree_class_target.csv')</p>
			<p class="source-code"># Print the sizes of the dataset</p>
			<p class="source-code">print("Number of Examples in the Dataset = ", X.shape[0])</p>
			<p class="source-code">print("Number of Features for each example = ", X.shape[1]) </p>
			<p class="source-code">print("Possible Output Classes = ", np.unique(y))</p>
			<p><strong class="bold">Expected output</strong>:</p>
			<p class="source-code">Number of Examples in the Dataset = 10000</p>
			<p class="source-code">Number of Features for each example = 10</p>
			<p class="source-code">Possible Output Classes = [0 1]</p>
			<p>Since each data example in this dataset can only belong to one of the two classes, this is a binary classification problem. Binary classification problems are very important and very common in real-life scenarios. For example, let's assume that the examples in this dataset represent the measurement results for <strong class="source-inline">10000</strong> trees from a forest. The goal is to build a model using this dataset to predict whether the species of each tree that's measured is a deciduous or coniferous species of tree. The <strong class="source-inline">10</strong> features for the trees can include predictors such as height, number of branches, and girth of the trunk at the base. </p>
			<p>The output class <strong class="source-inline">0</strong> means that the tree is a coniferous species of tree, while the output class <strong class="source-inline">1</strong> means that the tree is a deciduous species of tree.</p>
			<p>Now, let's go through the steps for building and training a Keras model to perform the classification:</p>
			<ol>
				<li value="1">Set a seed in <strong class="source-inline">numpy</strong> and <strong class="source-inline">tensorflow</strong> and define your model as a Keras sequential model. <strong class="source-inline">Sequential</strong> models are, in fact, stacks of layers. After defining the model, we can add as many layers to it as desired:<p class="source-code">from keras.models import Sequential</p><p class="source-code">from tensorflow import random</p><p class="source-code">np.random.seed(42)</p><p class="source-code">random.set_seed(42)</p><p class="source-code">model = Sequential()</p></li>
				<li>Add one hidden layer of size <strong class="source-inline">10</strong> with an activation function of type <strong class="source-inline">tanh</strong> to your model (remember that the input dimension is equal to <strong class="source-inline">10</strong>). There are different types of layers available in Keras. For now, we will use only the simplest type of layer, called the <strong class="source-inline">Dense</strong> layer. A Dense layer is equivalent to the <strong class="source-inline">fully connected layers</strong> that we have seen in all the examples so far:<p class="source-code">from keras.layers import Dense, Activation</p><p class="source-code">model.add(Dense(10, activation='tanh', input_dim=10))</p></li>
				<li>Add another hidden layer, this time of size <strong class="source-inline">5</strong> and with an activation function of type <strong class="source-inline">tanh</strong>, to your model. Please note that the input dimension argument is only provided for the first layer since the input dimension for the next layers is known:<p class="source-code">model.add(Dense(5, activation='tanh'))</p></li>
				<li>Add the output layer with the <strong class="source-inline">sigmoid</strong> activation function. Please note that the number of units in the output layer is equal to the output dimension:<p class="source-code">model.add(Dense(1, activation='sigmoid'))</p></li>
				<li>Ensure that the loss function is binary cross-entropy and that the optimizer is <strong class="source-inline">SGD</strong> for training the model using the <strong class="source-inline">compile()</strong> method and print out a summary of the model to see its architecture:<p class="source-code">model.compile(optimizer='sgd', loss='binary_crossentropy', \</p><p class="source-code">              metrics=['accuracy'])</p><p class="source-code">model.summary()</p><p>The following image shows the output of the preceding code:</p><div id="_idContainer084" class="IMG---Figure"><img src="image/B15777_03_09.jpg" alt="Figure 3.9: A summary of the model that was created&#13;&#10;"/></div><p class="figure-caption">Figure 3.9: A summary of the model that was created</p></li>
				<li>Train your model for <strong class="source-inline">100</strong> epochs and set a <strong class="source-inline">batch_size</strong> equal to 5 and a <strong class="source-inline">validation_split</strong> equal to <strong class="source-inline">0.2</strong>, and then set <strong class="source-inline">shuffle</strong> equal to <strong class="source-inline">false</strong> using the <strong class="source-inline">fit()</strong> method. Remember that you need to pass the input data, <strong class="source-inline">X</strong>, and its corresponding outputs, <strong class="source-inline">y</strong>, to the <strong class="source-inline">fit()</strong> method to train the model. Also, keep in mind that training a network may take a long time, depending on the size of the dataset, the size of the network, the number of epochs, and the number of CPUs or GPUs available. Save the results to a variable named <strong class="source-inline">history</strong>:<p class="source-code">history = model.fit(X, y, epochs=100, batch_size=5, \</p><p class="source-code">                    verbose=1, validation_split=0.2, \</p><p class="source-code">                    shuffle=False)</p><p>The <strong class="source-inline">verbose</strong> argument can take any of these three values: <strong class="source-inline">0</strong>, <strong class="source-inline">1</strong>, or <strong class="source-inline">2</strong>. By choosing <strong class="source-inline">verbose=0</strong>, no information will be printed during training. <strong class="source-inline">verbose=1</strong> will print a full progress bar at every iteration, while <strong class="source-inline">verbose=2</strong> will print only the epoch number:</p><div id="_idContainer085" class="IMG---Figure"><img src="image/B15777_03_10.jpg" alt="Figure 3.10: The loss details of the last 5 epochs out of 400&#13;&#10;"/></div><p class="figure-caption">Figure 3.10: The loss details of the last 5 epochs out of 400</p></li>
				<li>Print the accuracy and loss of the model on the training and validation data as a function of the epoch:<p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code"># Plot training &amp; validation accuracy values</p><p class="source-code">plt.plot(history.history['accuracy'])</p><p class="source-code">plt.plot(history.history['val_accuracy'])</p><p class="source-code">plt.title('Model accuracy')</p><p class="source-code">plt.ylabel('Accuracy')</p><p class="source-code">plt.xlabel('Epoch')</p><p class="source-code">plt.legend(['Train', 'Validation'], loc='upper left')</p><p class="source-code">plt.show()</p><p class="source-code"># Plot training &amp; validation loss values</p><p class="source-code">plt.plot(history.history['loss'])</p><p class="source-code">plt.plot(history.history['val_loss'])</p><p class="source-code">plt.title('Model loss')</p><p class="source-code">plt.ylabel('Loss')</p><p class="source-code">plt.xlabel('Epoch')</p><p class="source-code">plt.legend(['Train', 'Validation'], loc='upper left')</p><p class="source-code">plt.show()</p><p>The following image shows the output of the preceding code:</p><div id="_idContainer086" class="IMG---Figure"><img src="image/B15777_03_11.jpg" alt="Figure 3.11: The model's accuracy and loss as a function of an epoch during &#13;&#10;the training process&#13;&#10;"/></div><p class="figure-caption">Figure 3.11: The model's accuracy and loss as a function of an epoch during the training process</p></li>
				<li>Use your trained model to predict the output class for the first 10 input data examples (<strong class="source-inline">X.iloc[0:10,:]</strong>):<p class="source-code">y_predicted = model.predict(X.iloc[0:10,:])</p><p>You can print the predicted classes using the following code block:</p><p class="source-code"> # print the predicted classes</p><p class="source-code">print("Predicted probability for each of the "\</p><p class="source-code">      "examples belonging to class 1: "),</p><p class="source-code">print(y_predicted)</p><p class="source-code">print("Predicted class label for each of the examples: "), </p><p class="source-code">print(np.round(y_predicted))</p><p><strong class="bold">Expected output</strong>:</p><p class="source-code">Predicted probability for each of the examples belonging to class 1:</p><p class="source-code">[[0.00354007]</p><p class="source-code"> [0.8302744 ]</p><p class="source-code"> [0.00316998]</p><p class="source-code"> [0.95335543]</p><p class="source-code"> [0.99479216]</p><p class="source-code"> [0.00334176]</p><p class="source-code"> [0.43222323]</p><p class="source-code"> [0.00391936]</p><p class="source-code"> [0.00332899]</p><p class="source-code"> [0.99759173]</p><p class="source-code">Predicted class label for each of the examples:</p><p class="source-code">[[0.]</p><p class="source-code"> [1.]</p><p class="source-code"> [0.]</p><p class="source-code"> [1.]</p><p class="source-code"> [1.]</p><p class="source-code"> [0.]</p><p class="source-code"> [0.]</p><p class="source-code"> [0.]</p><p class="source-code"> [0.]</p><p class="source-code"> [1.]]</p><p>Here, we used the trained model to predict the output for the first 10 tree species in the dataset. As you can see, the model predicted that the second, fourth, fifth, and tenth trees were predicted as the species of class 1, which is deciduous.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YX3fxX">https://packt.live/2YX3fxX</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/38pztVR">https://packt.live/38pztVR</a>.</p></li>
			</ol>
			<p>Please note that you can extend these steps by adding more hidden layers to your network. In fact, you can add as many layers as you want to your model before adding the output layer. However, the input dimension argument is only provided for the first layer since the input dimension for the next layers is known. Now that you have learned how to implement a neural network in Keras, you are ready to practice with them further by implementing a neural network that can perform classification in the following activity.</p>
			<h2 id="_idParaDest-75"><a id="_idTextAnchor076"/>Activity 3.01: Building a Single-Layer Neural Network for Performing Binary Classification</h2>
			<p>In this activity, we will use a Keras sequential model to build a binary classifier. The simulated dataset provided represents the testing results of the production of aircraft propellers. Our target variable will be the results of the manual inspection of the propellers, designated as either "pass" (represented as a value of 1) or "fail" (represented as a value of 0). </p>
			<p>Our goal is to classify the testing results into either "pass" or "fail" classes to match the manual inspections. We will use models with different architectures and observe the visualization of the different models' performance. This will help you gain a better sense of how going from one processing unit to a layer of processing units changes the flexibility and performance of the model. </p>
			<p>Assume that this dataset contains two features representing the test results of two different tests inspecting the aircraft propellers of over <strong class="source-inline">3000</strong> propellers (the two features are normalized to have a mean of zero). The output is the likelihood of the propeller passing the test, with 1 representing a pass and zero representing a fail. The company would like to rely less on time-consuming, error-prone manual inspections of the aircraft propellers and shift resources to developing automated tests to assess the propellers faster. Therefore, the goal is to build a model that can predict whether an aircraft propeller will pass the manual inspection when given the results from the two tests. In this activity, you will first build a logistic regression model, then a single-layer neural network with three units, and finally a single-layer neural network with six units, to perform the classification. Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import the required packages:<p class="source-code"># import required packages from Keras</p><p class="source-code">from keras.models import Sequential </p><p class="source-code">from keras.layers import Dense, Activation </p><p class="source-code">import numpy as np</p><p class="source-code">import pandas as pd</p><p class="source-code">from tensorflow import random</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code"># import required packages for plotting</p><p class="source-code">import matplotlib.pyplot as plt </p><p class="source-code">import matplotlib</p><p class="source-code">%matplotlib inline </p><p class="source-code">import matplotlib.patches as mpatches</p><p class="source-code"># import the function for plotting decision boundary</p><p class="source-code">from utils import plot_decision_boundary</p><p class="callout-heading">Note </p><p class="callout">You will need to download the <strong class="source-inline">utils.py</strong> file from the GitHub repository and save it into your activity folder in order for the utils import statement to work correctly. You can find the file here: <a href="https://packt.live/31EumPY">https://packt.live/31EumPY</a>.</p></li>
				<li>Set up a seed for a random number generator so that the results will be reproducible:<p class="source-code">"""</p><p class="source-code">define a seed for random number generator so the result will be reproducible</p><p class="source-code">"""</p><p class="source-code">seed = 1</p><p class="callout-heading">Note</p><p class="callout">The triple-quotes ( <strong class="source-inline">"""</strong> ) shown in the code snippet above are used to denote the start and end points of a multi-line code comment. Comments are added into code to help explain specific bits of logic.</p></li>
				<li>Load the dataset using the <strong class="source-inline">read_csv</strong> function from the <strong class="source-inline">pandas</strong> library. Print the <strong class="source-inline">X</strong> and <strong class="source-inline">Y</strong> sizes and the number of examples in the training dataset using <strong class="source-inline">feats.shape</strong>, <strong class="source-inline">target.shape</strong>, and <strong class="source-inline">feats.shape[0]</strong>:<p class="source-code">feats = pd.read_csv('outlier_feats.csv')</p><p class="source-code">target = pd.read_csv('outlier_target.csv')</p><p class="source-code">print("X size = ", feats.shape)</p><p class="source-code">print("Y size = ", target.shape)</p><p class="source-code">print("Number of examples = ", feats.shape[0])</p></li>
				<li>Plot the dataset using the following code:<p class="source-code">plt.scatter(feats[:,0], feats[:,1], \</p><p class="source-code">            s=40, c=Y, cmap=plt.cm.Spectral)</p></li>
				<li>Implement a logistic regression model as a sequential model in Keras. Remember that the activation function for binary classification needs to be sigmoid. </li>
				<li>Train the model with <strong class="source-inline">optimizer='sgd'</strong>, <strong class="source-inline">loss='binary_crossentropy'</strong>, <strong class="source-inline">batch_size = 5</strong>, <strong class="source-inline">epochs = 100</strong>, and <strong class="source-inline">shuffle=False</strong>. Observe the loss values in each iteration by using <strong class="source-inline">verbose=1</strong> and <strong class="source-inline">validation_split=0.2</strong>. </li>
				<li>Plot the decision boundary of the trained model using the following code:<p class="source-code">plot_decision_boundary(lambda x: model.predict(x), \</p><p class="source-code">                       X_train, y_train)</p></li>
				<li>Implement a single-layer neural network with three nodes in the hidden layer and the <strong class="source-inline">ReLU activation function</strong> for <strong class="source-inline">200</strong> epochs. It is important to remember that the activation function for the output layer still needs to be sigmoid since it is a binary classification problem. Choosing <strong class="source-inline">ReLU</strong> or having no activation function for the output layer will not produce outputs that can be interpreted as class labels. Train the model with <strong class="source-inline">verbose=1</strong> and observe the loss in every iteration. After the model has been trained, plot the decision boundary and evaluate the loss and accuracy on the test dataset.</li>
				<li>Repeat <em class="italic">step 8</em> for the hidden layer of <strong class="source-inline">size 6</strong> and <strong class="source-inline">400</strong> epochs and compare the final loss value and the decision boundary plot.</li>
				<li>Repeat <em class="italic">steps 8</em> and <em class="italic">9</em> using the <strong class="source-inline">tanh</strong> activation function for the hidden layer and compare the results with the models with <strong class="source-inline">relu</strong> activation. Which activation function do you think is a better choice for this problem?<p class="callout-heading">Note</p><p class="callout">The solution for this activity can be found on page 362.</p></li>
			</ol>
			<p>In this activity, you observed how stacking multiple processing units in a layer can create a much more powerful model than a single processing unit. This is the basic reason why neural networks are such powerful models. You also observed that increasing the number of units in the layer increases the flexibility of the model, meaning a non-linear separating decision boundary can be estimated more precisely. </p>
			<p>However, a model with more processing units takes longer to learn the patterns, requires more epochs to be trained, and can overfit the training data. As such, neural networks are computationally expensive models. You also observed that using the tanh activation function results in a slower training process in comparison to using the <strong class="source-inline">ReLU activation function</strong>.</p>
			<p>In this section, we created various models and trained them on our data. We observed that some models performed better than others by evaluating them on the data that they were trained on. In the next section, we learn about some alternative methods we can use to evaluate our models that provide an unbiased evaluation.</p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor077"/>Model Evaluation</h1>
			<p>In this section, we will move on to multi-layer or deep neural networks while learning about techniques for assessing the performance of a model. As you may have already realized, there are many hyperparameter choices to be made when building a deep neural network. </p>
			<p>Some of the challenges of applied deep learning include how to find the right values for the number of hidden layers, the number of units in each hidden layer, the type of activation function to use for each layer, and the type of optimizer and loss function for training the network. Model evaluation is required when making these decisions. By performing model evaluation, you can say whether a specific deep architecture or a specific set of hyperparameters is working poorly or well on a particular dataset, and therefore decide whether to change them or not. </p>
			<p>Furthermore, you will learn about <strong class="source-inline">overfitting</strong> and <strong class="source-inline">underfitting</strong>. These are two very important issues that can arise when building and training deep neural networks. Understanding the concepts of overfitting and underfitting and whether they are happening in practice is essential when it comes to finding the right deep neural network for a particular problem and improving its performance as much as possible.</p>
			<h2 id="_idParaDest-77"><a id="_idTextAnchor078"/>Evaluating a Trained Model with Keras</h2>
			<p>In the previous activity, we plotted the decision boundary of the model by predicting the output for every possible value of the input. Such visualization of model performance was possible because we were dealing with two-dimensional input data. The number of features or measurements in the input space is almost always way more than two, and so visualization by 2D plotting is not an option. One way to figure out how well a model is doing on a particular dataset is to compute the overall loss when predicting outputs for many examples. This can be done by using the <strong class="source-inline">evaluate()</strong> method in Keras, which receives a set of inputs (<strong class="source-inline">X</strong>) and their corresponding outputs (<strong class="source-inline">y</strong>), and calculates and returns the overall loss of the model on the inputs, <strong class="source-inline">X</strong>.</p>
			<p>For example, let's consider a case of building a neural network with two hidden layers of sizes <strong class="source-inline">8</strong> and <strong class="source-inline">4</strong>, respectively, in order to perform binary or two-class classification. The available data points and their corresponding class labels are stored in <strong class="source-inline">X</strong>, <strong class="source-inline">y</strong> arrays. We can build and train the mentioned model as follows:</p>
			<p class="source-code">model = Sequential()</p>
			<p class="source-code">model.add(Dense(8, activation='tanh', input_dim=2))</p>
			<p class="source-code">model.add(Dense(4, activation='tanh'))</p>
			<p class="source-code">model.add(Dense(1, activation='sigmoid'))</p>
			<p class="source-code">model.compile(optimizer='sgd', loss='binary_crossentropy')</p>
			<p class="source-code">model.fit(X, y, epochs=epochs, batch_size=batch_size)</p>
			<p>Now, instead of using <strong class="source-inline">model.predict()</strong> to predict the output for a given set of inputs, we can evaluate the overall performance of the model by calculating the loss on the whole dataset by writing the following:</p>
			<p class="source-code">model.evaluate(X, y, batch_size=None, verbose=0)</p>
			<p>If you include other metrics, such as accuracy, when defining the <strong class="source-inline">compile()</strong> method for the model, the <strong class="source-inline">evaluate()</strong> method will return those metrics along with the loss when it is called. For example, if we add metrics to the <strong class="source-inline">compile()</strong> arguments, as shown in the following code, then calling the <strong class="source-inline">evaluate()</strong> method will return the overall loss and the overall accuracy of the trained model on the whole dataset:</p>
			<p class="source-code">model.compile(optimizer='sgd', loss='binary_crossentropy', \</p>
			<p class="source-code">              metrics=['accuracy'])</p>
			<p class="source-code">model.evaluate(X, y, batch_size=None, verbose=0)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can check out all the possible options for the <strong class="source-inline">metrics</strong> argument in Keras here: <a href="https://keras.io/metrics/">https://keras.io/metrics/</a>.</p>
			<p>In the next section, we will learn about splitting the dataset into training and test datasets. Much like we did in <em class="italic">Chapter 1</em>, <em class="italic">Introduction to Machine Learning with Keras</em>, training on separate data for evaluation can provide an unbiased evaluation of your model's performance.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor079"/>Splitting Data into Training and Test Sets</h2>
			<p>In general, evaluating a model on the same dataset that has been used for training the model is a methodological mistake. Since the model has been trained to reduce the errors on this dataset, performing an evaluation on it will result in a biased estimation of the model performance. In other words, the error rate on the dataset that has been used for training is always an underestimation of the error rate on new unseen examples. </p>
			<p>On the other hand, when building a machine learning model, the goal is not to achieve good performance on the training data only, but to achieve good performance on future examples that the model has not seen during training. That is why we are interested in evaluating the performance of a model using a dataset that has not been used for training the model.</p>
			<p>One way to achieve this is to split the available dataset into two sets: a training set and a test set. The training set is used to train the model, while the test set is used for performance evaluation. More precisely, the role of the training set is to provide enough examples for the model that it will learn the relations and patterns in the data, while the role of the test set is to provide us with an unbiased estimation of the model performance on new unseen examples. The common practice in machine learning is to perform <strong class="source-inline">70%-30%</strong> or <strong class="source-inline">80%-20%</strong> splitting for training-test sets. This is usually the case for relatively small datasets. When dealing with a dataset with millions of examples in which the goal is to train a large deep neural network, the training-test splitting can be done using <strong class="source-inline">98%-2%</strong> or <strong class="source-inline">99%-1%</strong> ratios. </p>
			<p>The following image shows the division of a dataset into a training set and test set. Notice that there is no overlap between the <strong class="source-inline">training</strong> set and <strong class="source-inline">test</strong> set:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/B15777_03_12.jpg" alt="Figure 3.12: Illustration of splitting a dataset into training and test sets&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.12: Illustration of splitting a dataset into training and test sets</p>
			<p>You can easily perform splitting on your dataset using scikit-learn's <strong class="source-inline">train_test_split</strong> function. For example, the following code will perform a <strong class="source-inline">70%-30%</strong> training-test split on the dataset:</p>
			<p class="source-code">from sklearn.model_selection import train_test_split</p>
			<p class="source-code">X_train, X_test, \</p>
			<p class="source-code">y_train, y_test = train_test_split(X, y, test_size=0.3, \</p>
			<p class="source-code">                                   random_state=None)</p>
			<p>The <strong class="source-inline">test_size</strong> argument represents the proportion of the dataset to be kept in the test set, so it should be between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>. By assigning an <strong class="source-inline">int</strong> to the <strong class="source-inline">random_state</strong> argument, you can choose the seed to be used to generate the random split between the training and test sets.</p>
			<p>After splitting the data into training and test sets, we can change the code from the previous section by providing only the training set as an argument to <strong class="source-inline">fit()</strong>:</p>
			<p class="source-code">model = Sequential()</p>
			<p class="source-code">model.add(Dense(8, activation='tanh', input_dim=2))</p>
			<p class="source-code">model.add(Dense(4, activation='tanh'))</p>
			<p class="source-code">model.add(Dense(1, activation='sigmoid'))</p>
			<p class="source-code">model.compile(optimizer='sgd', \</p>
			<p class="source-code">              loss='binary_crossentropy')</p>
			<p class="source-code">model.fit(X_train, y_train, epochs=epochs, \</p>
			<p class="source-code">          batch_size=batch_size)</p>
			<p>Now, we can compute the model error rate on the training set and the test set separately:</p>
			<p class="source-code">model.evaluate(X_train, y_train, batch_size=None, \</p>
			<p class="source-code">               verbose=0)</p>
			<p class="source-code">model.evaluate(X_test, y_test, batch_size=None, \</p>
			<p class="source-code">               verbose=0)</p>
			<p>Another way of doing the splitting is by including the <strong class="source-inline">validation_split</strong> argument for the <strong class="source-inline">fit()</strong> method in Keras. For example, by only changing the <strong class="source-inline">model.fit(X, y)</strong> line in the code from the previous section to <strong class="source-inline">model.fit(X, y, validation_split=0.3)</strong>, the model will keep the last 30% of the data examples in a separate test set. It will only train the model on the other 70% of the samples, and it will evaluate the model on the training set and the test set at the end of each epoch. In doing so, it would be possible to observe the changes in the training error rate, as well as the test error rate as the training progresses.</p>
			<p>The reason that we want to have an unbiased evaluation of our model is so that we can see where there is room for improvement. Since neural networks have so many parameters to learn and can learn complex functions, they can often overfit to the training data and learn the noise in the training data, which can prevent the model from performing well on new, unseen data. The next section will explore these concepts in detail.</p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor080"/>Underfitting and Overfitting</h2>
			<p>In this section, you will learn about two issues you may face when building a machine learning model that needs to fit into a dataset. These issues are called <strong class="bold">overfitting</strong> and <strong class="bold">underfitting</strong> and are similar to the concepts of <strong class="bold">bias</strong> and <strong class="bold">variance</strong> for a model. </p>
			<p>In general, if a model is not flexible enough to learn the relations and patterns in a dataset, there will be a high training error. We can call such a model a model with high bias. On the other hand, if a model is too flexible for a given dataset, it will learn the noise in the training data, as well as the relations and patterns in the data. Such a system will cause a large increase in the test error in comparison to the training error. We mentioned previously that it is always the case that the test error is slightly higher than the training error. </p>
			<p>However, having a large gap between the test error and the training error is an indicator of a system with high variance. In data analysis, neither of these situations (<strong class="source-inline">high bias</strong> and <strong class="source-inline">high variance</strong>) are desirable. In fact, the aim is to find the model with the lowest possible amount of bias and variance at the same time.</p>
			<p>For example, let's consider a dataset that represents the normalized locations of the sightings of two species of butterfly, as shown in the following plot. The goal is to find a model that can separate these two species of the butterfly when given the location of their sighting. Clearly, the separating line between the two classes is not linear. Therefore, if we choose a simple model such as logistic regression (a neural network with one hidden layer of size one) to perform the classification on this dataset, we will get a linear separating line/decision boundary between two classes that is unable to capture the true pattern in the dataset:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/B15777_03_13.jpg" alt="Figure 3.13: Two-dimensional data points of two different classes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.13: Two-dimensional data points of two different classes</p>
			<p>The following plot illustrates the decision boundary that's achieved by such a model. By evaluating this model, it will be observed that the training error rate is high and that the test error rate is slightly higher than the training error. Having a high training error rate is indicative of a model with high bias while having a slight difference between the training error and test error is representative of a low-variance model. This is a clear case of underfitting; the model fails to fit the true separating line between the two classes:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/B15777_03_14.jpg" alt="Figure 3.14: Underfitting&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.14: Underfitting</p>
			<p>If we increase the flexibility of the neural network by adding more layers to it and increase the number of units in each layer, we can train a better model and succeed in capturing the non-linearity in the decision boundary. Such a model can be seen in the following plot. This is a model with a low training error rate and low-test error rate (again, the test error rate is slightly higher than the training error rate). Having a low training error rate and a slight difference between the test error rate and the training error rate is indicative of a model with low bias and low variance. A model with low bias and low variance represents the right amount of fitting for a given dataset:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/B15777_03_15.jpg" alt="Figure 3.15: Correct fit&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.15: Correct fit</p>
			<p>But what will happen if we increase the flexibility of the neural network even more? By adding too much flexibility to the model, it will learn not only the patterns and relations in the training data but also the noise in them. In other words, the model will fit each individual training example as opposed to fitting only to the overall trends and relations in them. The following plot shows such a system. Evaluating this model will show a very low training error rate and a high-test error rate (with a large difference between the training error rate and test error rate). This is a model with low bias and high variance, and this situation is called overfitting:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/B15777_03_16.jpg" alt="Figure 3.16: Overfitting&#13;&#10;"/>
				</div>
			</div>
			<p> </p>
			<p class="figure-caption">Figure 3.16: Overfitting</p>
			<p>Evaluating the model on both the training set and the test set and comparing their error rates provide valuable information on whether the current model is right for a given dataset. Also, in cases where the current model is not fitting the dataset correctly, it is possible to determine whether it is overfitting or underfitting to the data and change the model accordingly to find the right model. For example, if the model is underfitting, you can make the network larger. On the other hand, if the model is overfitting, you can reduce the overfitting by making the network smaller or providing more training data to it. There are many methods that can be implemented to prevent underfitting or overfitting in practice, one of which we will explore in the next section.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor081"/>Early Stopping</h2>
			<p>Sometimes, the flexibility of a model is right for the dataset but overfitting or underfitting is still happening. This is because we are training the model for either too many iterations or too few iterations. When using an iterative optimizer such as <strong class="source-inline">gradient descent</strong>, the optimizer tries to fit the training data better and better in every iteration. Therefore, if we keep updating the parameters after the patterns in the data are learned, it will start fitting to the individual data examples.</p>
			<p>By observing the training and test error rates in every iteration, it is possible to determine when the network is starting to overfit to the training data and stop the training process before this happens. Regions associated with <strong class="source-inline">underfitting</strong> and <strong class="source-inline">overfitting</strong> have been labeled on the following plot. The correct number of iterations for training the model can be determined from the region at which the test error rate has its lowest value. We labeled this region as the right fit on the plot and it can be seen that, in this region, both the <strong class="source-inline">training error rate</strong> and the <strong class="source-inline">test error rate</strong> are low:</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="image/B15777_03_17.jpg" alt="Figure 3.17: Plot of training error rate and test error rate while training a model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.17: Plot of training error rate and test error rate while training a model</p>
			<p>You can easily store the values for training loss and test loss in every epoch while training with Keras. To do this, you need to provide the test set as the <strong class="source-inline">validation_data</strong> argument when defining the <strong class="source-inline">fit()</strong> method for the model and store it in a <strong class="source-inline">history</strong> dictionary:</p>
			<p class="source-code">history = model.fit(X_train, y_train, validation_data=(X_test, y_test))</p>
			<p>You can plot the values stored in <strong class="source-inline">history</strong> later to find the correct number of iterations to train your model with:</p>
			<p class="source-code">import matplotlib.pyplot as plt </p>
			<p class="source-code">import matplotlib</p>
			<p class="source-code"># plot training loss</p>
			<p class="source-code">plt.plot(history.history['loss'])</p>
			<p class="source-code"># plot test loss</p>
			<p class="source-code">plt.plot(history.history['val_loss'])</p>
			<p>In general, since deep neural networks are highly flexible models, the chance of overfitting happening is very high. There is a whole group of techniques, called <strong class="bold">regularization</strong> techniques, that have been developed to reduce overfitting in machine learning models in general, and deep neural networks in particular. You will learn more about these techniques in <em class="italic">Chapter 5</em>, <em class="italic">Improving Model Accuracy</em>. In the next activity, we will put our understanding into practice and attempt to find the optimal number of epochs to train for so as to prevent overfitting.</p>
			<h2 id="_idParaDest-81">Activity 3.02: Ad<a id="_idTextAnchor082"/>vanced Fibrosis Diagnosis with Neural Networks</h2>
			<p>In this activity, you are going to use a real dataset to predict whether a patient has advanced fibrosis based on measurements such as age, gender, and BMI. The dataset consists of information for 1,385 patients who underwent treatment dosages for hepatitis C. For each patient, <strong class="source-inline">28</strong> different attributes are available, as well as a class label, which can only take two values: <strong class="source-inline">1</strong>, indicating advanced fibrosis, and <strong class="source-inline">0</strong>, indicating no indication of advanced fibrosis. This is a binary/two-class classification problem with an input dimension equal to <strong class="source-inline">28</strong>.</p>
			<p>In this activity, you will implement different deep neural network architectures to perform this classification. Plot the trends in the training error rates and test error rates and determine how many epochs the final classifier needs to be trained for:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset that's being used in this activity can be found here: <a href="https://packt.live/39pOUMT">https://packt.live/39pOUMT</a>.</p>
			<div>
				<div id="_idContainer093" class="IMG---Figure">
					<img src="image/B15777_03_18.jpg" alt="Figure 3.18: Schematic view of the binary classifier for a diabetes diagnosis&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 3.18: Schematic view of the binary classifier for a diabetes diagnosis</p>
			<p>Follow these steps to complete this activity:</p>
			<ol>
				<li value="1">Import all the necessary dependencies. Load the dataset from the <strong class="source-inline">data</strong> subfolder of the <strong class="source-inline">Chapter03</strong> folder from GitHub:<p class="source-code">X = pd.read_csv('../data/HCV_feats.csv')</p><p class="source-code">y = pd.read_csv('../data/HCV_target.csv')</p></li>
				<li>Print the number of examples in the dataset, the number of features available, and the possible values for the class labels.</li>
				<li>Scale the data using the <strong class="source-inline">StandardScalar</strong> function from <strong class="source-inline">sklearn.preprocessing</strong> and split the dataset into the training set and test set with an <strong class="source-inline">80:20</strong> ratio. Then, print the number of examples in each set after splitting.</li>
				<li>Implement a shallow neural network with one hidden layer of size 3 and a <strong class="source-inline">tanh</strong> activation function to perform the classification. Compile the model with the following values for the hyperparameters: <strong class="source-inline">optimizer = 'sgd', loss = 'binary_crossentropy', metrics = ['accuracy']</strong></li>
				<li>Fit the model with the following hyperparameters and store the values for training error rate and test error rate during the training process: <strong class="source-inline">batch_size = 20</strong>, <strong class="source-inline">epochs = 100</strong>, <strong class="source-inline">validation_split=0.1</strong>, and <strong class="source-inline">shuffle=False</strong>.</li>
				<li>Plot the training error rate and test error rate for every epoch of training. Use the plot to determine at which epoch the network is starting to overfit to the dataset. Also, print the values of the best accuracy that were reached on the training set and on the test set, as well as the loss and accuracy that were evaluated on the test dataset.</li>
				<li>Repeat <em class="italic">steps 4</em> and <em class="italic">5</em> for a deep neural network with two hidden layers (the first layer of size 4 and the second layer of size 3) and a <strong class="source-inline">'tanh'</strong> activation function for both layers in order to perform the classification. <p class="callout-heading">Note</p><p class="callout">The solution for this activity can be found on page 374.</p></li>
			</ol>
			<p>Please note that both models were able to achieve better <strong class="source-inline">accuracy</strong> on the <strong class="source-inline">training</strong> or <strong class="source-inline">validation</strong> set compared to the <strong class="source-inline">test</strong> set, and the <strong class="source-inline">training error rate</strong> kept decreasing when it was trained for a significant number of epochs. However, the <strong class="source-inline">validation error rate</strong> decreased during training to a certain value, and after that, it started increasing, which is indicative of <strong class="source-inline">overfitting</strong> to the <strong class="source-inline">training</strong> data. The maximum validation accuracy corresponds to the point on the plots where the validation loss is at its lowest and is truly representative of how well the model will perform on independent examples later.</p>
			<p>It can be seen from the results that the model with one hidden layer is able to reach a lower validation and <strong class="source-inline">test error rate</strong> in comparison to the two-layer models. From this, we may conclude that this model is the best match for this particular problem. The model with one hidden layer shows a large amount of bias, indicated by the large gap between the training and validation errors, and both were still decreasing, indicating that the model can be trained for more epochs. Lastly, it can be determined from the plots that we should stop training around the region where the validation error rate starts increasing to prevent the model from <strong class="source-inline">overfitting</strong> to the data points.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor083"/>Summary</h1>
			<p>In this chapter, you extended your knowledge of deep learning, from understanding the common representations and terminology to implementing them in practice through exercises and activities. You learned how <strong class="source-inline">forward propagation</strong> in neural networks works and how it is used for predicting outputs, how the loss function works as a measure of model performance, and how backpropagation is used to compute the derivatives of loss functions with respect to model parameters.</p>
			<p>You also learned about gradient descent, which uses the gradients that are computed by <strong class="source-inline">backpropagation</strong> to gradually update the model parameters. In addition to basic theory and concepts, you implemented and trained both shallow and deep neural networks with Keras and utilized them to make predictions about the output of a given input.</p>
			<p>To evaluate your models appropriately, you split a dataset into a training set and a test set as an alternative approach to improving network evaluation and learned the reasons why evaluating a model on training examples can be misleading. This helped further your understanding of overfitting and underfitting that can happen when training a model. Finally, you utilized the training error rate and test error rate to detect overfitting and underfitting in a network and implemented early stopping in order to reduce overfitting in a network.</p>
			<p>In the next chapter, you will learn about the Keras wrapper with <strong class="source-inline">scikit-learn</strong> and how to use it to further improve model evaluation by using resampling methods such as cross-validation. By doing this, you will learn how to find the best set of hyperparameters for a deep neural network.</p>
		</div>
		<div>
			<div id="_idContainer095" class="Content">
			</div>
		</div>
	</body></html>