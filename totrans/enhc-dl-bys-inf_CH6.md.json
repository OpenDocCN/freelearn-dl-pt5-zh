["```py\n\nfrom tensorflow.keras import Sequential \nfrom tensorflow.keras.layers import Flatten, Conv2D, MaxPooling2D, Dropout, Dense \n\nmodel = Sequential([ \nConv2D(32, (3,3), activation=\"relu\", input_shape=(28, 28, 1)), \nMaxPooling2D((2,2)), \nDropout(0.2), \nConv2D(64, (3,3), activation=\"relu\"), \nMaxPooling2D((2,2)), \nDropout(0.5), \nFlatten(), \nDense(64, activation=\"relu\"), \nDropout(0.5), \nDense(10) \n]) \n\n```", "```py\n\ndef mc_dropout_inference( \nimgs: np.ndarray, \nnb_inference: int, \nmodel: Sequential \n) -*>* np.ndarray: \n\"\"\"\" \nRun inference nb_inference times with random dropout enabled \n(training=True) \n\"\"\" \ndivds = [] \nfor _ in range(nb_inference): \ndivds.append(model(imgs, training=True)) \nreturn tf.nn.softmax(divds, axis=-1).numpy() \n\nPredictions = mc_dropout_inference(test_images, 50, model)\n```", "```py\n\npredictive_mean = np.mean(predictions, axis=0) \npredictive_variance = np.var(predictions, axis=0)\n```", "```py\n\nimport tensorflow as tf \nimport numpy as np \nimport matplotlib.pyplot as plt \n\nENSEMBLE_MEMBERS = 3\n```", "```py\n\n# download data set \nfashion_mnist = tf.keras.datasets.fashion_mnist \n# split in train and test, images and labels \n(train_images, train_labels), (test_images, test_labels) = fashion_mnist.load_data() \n\n# set class names \nCLASS_NAMES = ['T-shirt', 'Trouser', 'Pullover', 'Dress', 'Coat', \n               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n```", "```py\n\ndef build_model(): \n# we build a forward neural network with tf.keras.Sequential \nmodel = tf.keras.Sequential([ \n# we define two convolutional layers followed by a max-pooling operation each \ntf.keras.layers.Conv2D(filters=32, kernel_size=(5,5), padding='same', \nactivation='relu', input_shape=(28, 28, 1)), \ntf.keras.layers.MaxPool2D(strides=2), \ntf.keras.layers.Conv2D(filters=48, kernel_size=(5,5), padding='valid', \nactivation='relu'), \ntf.keras.layers.MaxPool2D(strides=2), \n# we flatten the matrix output into a vector \ntf.keras.layers.Flatten(), \n# we apply three fully-connected layers \ntf.keras.layers.Dense(256, activation='relu'), \ntf.keras.layers.Dense(84, activation='relu'), \ntf.keras.layers.Dense(10) \n]) \n\nreturn model \n\n```", "```py\n\ndef compile_model(model): \nmodel.compile(optimizer='adam', \nloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \nmetrics=['accuracy']) \nreturn model \n\n```", "```py\n\ndeep_ensemble = [] \nfor ind in range(ENSEMBLE_MEMBERS): \nmodel = build_model() \nmodel = compile_model(model) \nprint(f\"Train model {ind:02}\") \nmodel.fit(train_images, train_labels, epochs=10) \n    deep_ensemble.append(model)\n```", "```py\n\n# get logit predictions for all three models for images in the test split \nensemble_logit_predictions = [model(test_images) for model in deep_ensemble] \n# convert logit predictions to softmax \nensemble_softmax_predictions = [ \ntf.nn.softmax(logits, axis=-1) for logits in ensemble_logit_predictions] \n\n# take mean across models, this will result in one prediction vector per image \nensemble_predictions = tf.reduce_mean(ensemble_softmax_predictions, axis=0)\n```", "```py\n\n# calculate variance across model predictions \nensemble_std = tf.reduce_mean( \ntf.math.reduce_variance(ensemble_softmax_predictions, axis=0), \naxis=1) \n# find index of test image with highest variance across predictions \nind_disagreement = np.argmax(ensemble_std) \n\n# get predictions per model for test image with highest variance \nensemble_disagreement = [] \nfor ind in range(ENSEMBLE_MEMBERS): \nmodel_prediction = np.argmax(ensemble_softmax_predictions[ind][ind_disagreement]) \nensemble_disagreement.append(model_prediction) \n# get class predictions \npredicted_classes = [CLASS_NAMES[ind] for ind in ensemble_disagreement] \n\n# define image caption \nimage_caption = \\ \nf\"Network 1: {predicted_classes[0]}\\n\" + \\ \nf\"Network 2: {predicted_classes[1]}\\n\" + \\ \nf\"Network 3: {predicted_classes[2]}\\n\" \n\n# visualise image and predictions \nplt.figure() \nplt.title(f\"Correct class: {CLASS_NAMES[test_labels[ind_disagreement]]}\") \nplt.imshow(test_images[ind_disagreement], cmap=plt.cm.binary) \nplt.xlabel(image_caption) \nplt.show()\n```", "```py\n\nfrom tensorflow.keras import Model, Sequential, layers, optimizers, metrics, losses \nimport tensorflow as tf \nimport tensorflow_probability as tfp \nfrom sklearn.datasets import load_boston \nfrom sklearn.model_selection import train_test_split \nfrom sklearn.preprocessing import StandardScaler \nfrom sklearn.metrics import mean_squared_error \nimport pandas as pd \nimport numpy as np \n\nseed = 213 \nnp.random.seed(seed) \ntf.random.set_seed(seed) \ndtype = tf.float32 \n\nboston = load_boston() \ndata = boston.data \ntargets = boston.target \n\nX_train, X_test, y_train, y_test = train_test_split(data, targets, test_size=0.2) \n\n# Scale our inputs \nscaler = StandardScaler() \nX_train = scaler.fit_transform(X_train) \nX_test = scaler.transform(X_test) \n\nmodel = Sequential() \nmodel.add(layers.Dense(20, input_dim=13, activation='relu', name='layer_1')) \nmodel.add(layers.Dense(8, activation='relu', name='layer_2')) \nmodel.add(layers.Dense(1, activation='relu', name='layer_3')) \n\nmodel.compile(optimizer=optimizers.Adam(), \nloss=losses.MeanSquaredError(), \nmetrics=[metrics.RootMeanSquaredError()],) \n\nnum_epochs = 200 \nmodel.fit(X_train, y_train, epochs=num_epochs) \nmse, rmse = model.evaluate(X_test, y_test)\n```", "```py\n\nbasis_func = Model(inputs=self.model.input, \n                           outputs=self.model.get_layer('layer_2').output)\n```", "```py\n\nlayer_2_output = basis_func.predict(X_test)\n```", "```py\n\nclass BayesianLastLayer(): \n\ndef __init__(self, \nmodel, \nbasis_layer, \nn_samples=1e4, \nn_burnin=5e3, \nstep_size=1e-4, \nn_leapfrog=10, \nadaptive=False): \n# Setting up our model \nself.model = model \nself.basis_layer = basis_layer \nself.initialize_basis_function() \n# HMC Settings \n# number of hmc samples \nself.n_samples = int(n_samples) \n# number of burn-in steps \nself.n_burnin = int(n_burnin) \n# HMC step size \nself.step_size = step_size \n# HMC leapfrog steps \nself.n_leapfrog = n_leapfrog \n# whether to be adaptive or not \n        self.adaptive = adaptive\n```", "```py\n\ndef initialize_basis_function(self): \nself.basis_func = Model(inputs=self.model.input, \noutputs=self.model.get_layer(self.basis_layer).output) \n\ndef get_basis(self, X): \n        return self.basis_func.predict(X)\n```", "```py\n\ndef fit(self, X, y): \nX = tf.convert_to_tensor(self.get_basis(X), dtype=dtype) \ny = tf.convert_to_tensor(y, dtype=dtype) \ny = tf.reshape(y, (-1, 1)) \nD = X.shape[1] \n\n# Define our joint distribution \ndistribution = tfp.distributions.JointDistributionNamedAutoBatched( \ndict( \nsigma=tfp.distributions.HalfNormal(scale=tf.ones([1])), \nalpha=tfp.distributions.Normal( \nloc=tf.zeros([1]), \nscale=tf.ones([1]), \n), \nbeta=tfp.distributions.Normal( \nloc=tf.zeros([D,1]), \nscale=tf.ones([D,1]), \n), \ny=lambda beta, alpha, sigma: \ntfp.distributions.Normal( \nloc=tf.linalg.matmul(X, beta) + alpha, \nscale=sigma \n) \n) \n) \n. . .\n```", "```py\n\n. . . \n# Define the log probability function \ndef target_log_prob_fn(beta, alpha, sigma): \nreturn distribution.log_prob(beta=beta, alpha=alpha, sigma=sigma, y=y) \n\n# Define the HMC kernel we'll be using for sampling \nhmc_kernel  = tfp.mcmc.HamiltonianMonteCarlo( \ntarget_log_prob_fn=target_log_prob_fn, \nstep_size=self.step_size, \nnum_leapfrog_steps=self.n_leapfrog \n) \n\n# We can use adaptive HMC to automatically adjust the kernel step size \nif self.adaptive: \nadaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation( \ninner_kernel = hmc_kernel, \nnum_adaptation_steps=int(self.n_burnin * 0.8) \n) \n. . .\n```", "```py\n\n. . . \n# If we define a function, we can extend this to multiple chains. \n@tf.function \ndef run_chain(): \nstates, kernel_results = tfp.mcmc.sample_chain( \nnum_results=self.n_samples, \nnum_burnin_steps=self.n_burnin, \ncurrent_state=[ \ntf.zeros((X.shape[1],1), name='init_model_coeffs'), \ntf.zeros((1), name='init_bias'), \ntf.ones((1), name='init_noise'), \n], \nkernel=hmc_kernel \n) \nreturn states, kernel_results \n\nprint(f'Running HMC with {self.n_samples} samples.') \nstates, kernel_results = run_chain() \n\nprint('Completed HMC sampling.') \ncoeffs, bias, noise_std = states \naccepted_samples = kernel_results.is_accepted[self.n_burnin:] \nacceptance_rate = 100*np.mean(accepted_samples) \n# Print the acceptance rate - if this is low, we need to check our \n# HMC parameters \n        print('Acceptance rate: %0.1f%%' % (acceptance_rate))\n```", "```py\n\n# Obtain the post-burnin samples \nself.model_coeffs = coeffs[self.n_burnin:,:,0] \nself.bias = bias[self.n_burnin:] \n        self.noise_std = noise_std[self.n_burnin:]\n```", "```py\n\ndef get_divd_dist(self, X): \npredictions = (tf.matmul(X, tf.transpose(self.model_coeffs)) + \nself.bias[:,0]) \nnoise = (self.noise_std[:,0] * \ntf.random.normal([self.noise_std.shape[0]])) \nreturn predictions + noise \n\ndef predict(self, X): \nX = tf.convert_to_tensor(self.get_basis(X), dtype=dtype) \ndivd_dist = np.zeros((X.shape[0], self.model_coeffs.shape[0])) \nX = tf.reshape(X, (-1, 1, X.shape[1])) \nfor i in range(X.shape[0]): \ndivd_dist[i,:] = self.get_divd_dist(X[i,:]) \n\ny_divd = np.mean(divd_dist, axis=1) \ny_std = np.std(divd_dist, axis=1) \n        return y_divd, y_std\n```", "```py\n\nbll = BayesianLastLayer(model, 'layer_2') \n\nbll.fit(X_train, y_train) \n\ny_divd, y_std = bll.predict(X_test)\n```", "```py\n\nbasis_func = Model(inputs=model.input, \n                   outputs=model.get_layer('layer_2').output)\n```", "```py\n\nll_dropout = Sequential() \nll_dropout.add(layers.Dropout(0.25)) \nll_dropout.add(layers.Dense(1, input_dim=8, activation='relu', name='dropout_layer'))\n```", "```py\n\nll_dropout.compile(optimizer=optimizers.Adam(), \nloss=losses.MeanSquaredError(), \nmetrics=[metrics.RootMeanSquaredError()],) \nnum_epochs = 50 \nll_dropout.fit(basis_func.predict(X_train), y_train, epochs=num_epochs)\n```", "```py\n\ndef predict_ll_dropout(X, basis_func, ll_dropout, nb_inference): \nbasis_feats = basis_func(X) \nll_divd = [ll_dropout(basis_feats, training=True) for _ in range(nb_inference)] \nll_divd = np.stack(ll_divd) \n    return ll_divd.mean(axis=0), ll_divd.std(axis=0)\n```", "```py\n\ny_divd, y_std = predict_ll_dropout(X_test, basis_func, ll_dropout, 50)\n```"]