["```py\nIn: (0, 0, 0, 0, 1, 0, 1, 0, 1, 0)\nOut: 3\n```", "```py\nimport numpy as np\n# The first dimension represents the mini-batch\nx = np.array([[0, 0, 0, 0, 1, 0, 1, 0, 1, 0]])\ny = np.array([3])\n```", "```py\ndef step(s_t, x_t, U, W):\n    return x_t * U + s_t * W\n```", "```py\ndef forward(x, U, W):\n    # Number of samples in the mini-batch\n    number_of_samples = len(x)\n    # Length of each sample\n    sequence_length = len(x[0])\n    # Initialize the state activation for each sample along the sequence\n    s = np.zeros((number_of_samples, sequence_length + 1))\n    # Update the states over the sequence\n    for t in range(0, sequence_length):\n        s[:, t + 1] = step(s[:, t], x[:, t], U, W)  # step function\n    return s\n```", "```py\n    def backward(x, s, y, W):\n        sequence_length = len(x[0])\n        # The network output is just the last activation of sequence\n        s_t = s[:, -1]\n        # Compute the gradient of the output w.r.t. MSE cost function at final state\n        gS = 2 * (s_t - y)\n        # Set the gradient accumulations to 0\n        gU, gW = 0, 0\n        # Accumulate gradients backwards\n        for k in range(sequence_length, 0, -1):\n            # Compute the parameter gradients and accumulate the results.\n            gU += np.sum(gS * x[:, k - 1])\n            gW += np.sum(gS * s[:, k - 1])\n            # Compute the gradient at the output of the previous layer\n            gS = gS * W\n        return gU, gW\n    ```", "```py\n    def train(x, y, epochs, learning_rate=0.0005):\n        # Set initial parameters\n        weights = (-2, 0)  # (U, W)\n        # Accumulate the losses and their respective weights\n        losses, gradients_u, gradients_w = list(), list(), list()\n        # Perform iterative gradient descent\n        for i in range(epochs):\n            # Perform forward and backward pass to get the gradients\n            s = forward(x, weights[0], weights[1])\n            # Compute the loss\n            loss = (y[0] - s[-1, -1]) ** 2\n            # Store the loss and weights values for later display\n            losses.append(loss)\n            gradients = backward(x, s, y, weights[1])\n            gradients_u.append(gradients[0])\n            gradients_w.append(gradients[1])\n            # Update each parameter `p` by p = p - (gradient * learning_rate).\n            # `gp` is the gradient of parameter `p`\n            weights = tuple((p - gp * learning_rate) for p, gp in zip(weights, gradients))\n        return np.array(losses), np.array(gradients_u), np.array(gradients_w)\n    ```", "```py\n    losses, gradients_u, gradients_w = train(x, y,\n         epochs=150)\n    ```", "```py\nx = np.array([[0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0]])\ny = np.array([12])\nlosses, gradients_u, gradients_w = train(x, y, epochs=150)\nplot_training(losses, gradients_u, gradients_w)\n```", "```py\nRuntimeWarning: overflow encountered in multiply\n  return x * U + s * W\nRuntimeWarning: invalid value encountered in multiply\n  gU += np.sum(gS * x[:, k - 1])\nRuntimeWarning: invalid value encountered in multiply\n  gW += np.sum(gS * s[:, k - 1])\n```", "```py\n    import torch\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    ```", "```py\n    from torchtext.data.utils import get_tokenizer\n    tokenizer = get_tokenizer('basic_english')\n    ```", "```py\n    from torchtext.datasets import IMDB\n    from torchtext.vocab import build_vocab_from_iterator\n    def yield_tokens(data_iter):\n        for _, text in data_iter:\n            yield tokenizer(text)\n    vocabulary = build_vocab_from_iterator(\n        yield_tokens(IMDB(split='train')),\n        specials=[\"<unk>\"])\n    vocabulary.set_default_index(vocabulary[\"<unk>\"])\n    ```", "```py\n    def collate_batch(batch):\n        labels, samples, offsets = [], [], [0]\n        for (_label, _sample) in batch:\n            labels.append(int(_label) - 1)\n            processed_text = torch.tensor(\n                vocabulary(tokenizer(_sample)),\n                dtype=torch.int64)\n            samples.append(processed_text)\n            offsets.append(processed_text.size(0))\n        labels = torch.tensor(\n            labels,\n            dtype=torch.int64)\n        offsets = torch.tensor(\n            offsets[:-1]).cumsum(dim=0)\n        samples = torch.cat(samples)\n        return labels, samples, offsets\n    ```", "```py\n    class LSTMModel(torch.nn.Module):\n        def __init__(self, vocab_size, embedding_size, hidden_size,\n            num_classes):\n            super().__init__()\n            # Embedding field\n            self.embedding = torch.nn.EmbeddingBag(\n                num_embeddings=vocab_size,\n                embedding_dim=embedding_size)\n            # LSTM cell\n            self.rnn = torch.nn.LSTM(\n                input_size=embedding_size,\n                hidden_size=hidden_size)\n            # Fully connected output\n            self.fc = torch.nn.Linear(\n                hidden_size, num_classes)\n        def forward(self, text_sequence, offsets):\n            # Extract embedding vectors\n            embeddings = self.embedding(\n                text_sequence, offsets)\n            h_t, c_t = self.rnn(embeddings)\n            return self.fc(h_t)\n    ```", "```py\n    model = LSTMModel(\n        vocab_size=len(vocabulary),\n        embedding_size=64,\n        hidden_size=64,\n        num_classes=2)\n    cost_fn = torch.nn.CrossEntropyLoss()\n    optim = torch.optim.Adam(model.parameters())\n    ```", "```py\n    from torchtext.data.functional import to_map_style_dataset\n    train_iter, test_iter = IMDB()\n    train_dataset = to_map_style_dataset(train_iter)\n    test_dataset = to_map_style_dataset(test_iter)\n    from torch.utils.data import DataLoader\n    train_dataloader = DataLoader(\n        train_dataset, batch_size=64,\n        shuffle=True, collate_fn=collate_batch)\n    test_dataloader = DataLoader(\n        test_dataset, batch_size=64,\n        shuffle=True, collate_fn=collate_batch)\n    ```", "```py\n    for epoch in range(5):\n        print(f'Epoch: {epoch + 1}')\n        train_model(model, cost_fn, optim, train_dataloader)\n        test_model(model, cost_fn, test_dataloader)\n    ```"]