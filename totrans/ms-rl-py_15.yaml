- en: '*Chapter 12*: Meta-Reinforcement Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Humans learn new skills from much less data compared to a **reinforcement learning**
    (**RL**) agent. This is because first, we come with priors in our brains at birth;
    and second, we are able to transfer our knowledge from one skill to another quite
    efficiently. Meta-RL aims to achieve a similar capability. In this chapter, we
    will describe what meta-RL is, what approaches we use, and what the challenges
    are under the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to meta-RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meta-RL with recurrent policies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-based meta-RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meta-RL as partially observed RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Challenges in meta-RL
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to meta-RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will introduce meta-RL, which is actually a very intuitive
    concept but could be hard to wrap your mind around at first. To make things even
    clearer for you, we will also discuss the connection between meta-RL and other
    concepts we covered in the earlier chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Learning to learn
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s say you are trying to convince a friend to go on a trip that you really
    want to go on together. There are several arguments that come to your mind. You
    could talk about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The beauty of the nature at your destination.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How you are so burned out and really need this time away.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This could be the last chance for a trip together for a while because you will
    be busy at work.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well, you've known your friend for years now and know how they love nature,
    so you recognize that the first argument will be the most enticing one, because
    they love nature! If it was your mom, perhaps you could use the second one because
    she cares about you a lot and wants to support you. In either of these situations,
    you know how to achieve what you want because you have a common past with these
    individuals.
  prefs: []
  type: TYPE_NORMAL
- en: 'Have you ever left a store, say a car dealership, buying something more expensive
    than you had originally planned? How were you convinced? Perhaps the salesperson
    figured out how much you care about the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Your family, and convinced you to buy an SUV to make them more comfortable
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Your look, and convinced you to buy a sports car that will attract a lot of
    eyes
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The environment, and convinced you to buy an electric vehicle with no emissions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The salesperson does not know you, but through years of experience and training,
    they know how to learn about you very quickly and effectively. They ask you questions,
    understand your background, discover your interests, and figure out your budget.
    Then, you are presented with some options, and based on what you like and don't
    like, you end up with an offer package with a make and model, upgrades and options,
    and a payment plan, all customized for you.
  prefs: []
  type: TYPE_NORMAL
- en: Here, the former of these examples correspond to RL, where an agent learns a
    good policy for a particular environment and a task to maximize its reward. The
    latter example corresponds to meta-RL, where an agent learns a good **procedure**
    to quickly adapt to a new environment and task to maximize its reward.
  prefs: []
  type: TYPE_NORMAL
- en: After this example, let's formally define meta-RL.
  prefs: []
  type: TYPE_NORMAL
- en: Defining meta-RL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In meta-RL, in each episode, the agent faces a task, ![](img/Formula_12_001.png),
    that comes from a distribution, ![](img/Formula_12_002.png), ![](img/Formula_12_003.png).
    A task, ![](img/Formula_12_004.png), is a **Markov** **decision** **process**
    (**MDP**) with possibly different transition and reward dynamics, described as
    ![](img/Formula_12_005.png), where we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_12_006.png) is the state space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_12_007.png) is the action space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_12_008.png) is the transition distribution for task ![](img/Formula_12_009.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_12_010.png) is the reward function for task ![](img/Formula_12_011.png).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, during the training and test time, we expect the tasks to come from the
    same distribution, but we don''t expect them to be the same, which is the setup
    in a typical machine learning problem. In meta-RL, at the test time, we expect
    the agent to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Effectively explore to understand the task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Adapt to the task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Meta-learning is embedded in animal learning. Let's explore this connection
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Relation to animal learning and the Harlow experiment
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Artificial neural networks notoriously require a lot of data to be trained.
    On the other hand, our brains learn much more efficiently from small data. There
    are two major factors contributing to this:'
  prefs: []
  type: TYPE_NORMAL
- en: Unlike an untrained artificial neural network, our brains are pre-trained and
    come with **priors** embedded in for vision, audio, and motor skill tasks. Some
    especially impressive examples of pre-trained beings are **precocial** animals,
    such as ducks, whose ducklings take to water within 2 hours of hatching.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When we learn new tasks, we learn at two time scales: in a **fast loop**, we
    learn about the specific task we are dealing with and, as we will see more examples,
    in a **slow loop**, we learn **abstractions** that help us generalize our knowledge
    to new examples very fast. Let''s say you learn about a particular cat breed,
    such as the American Curl, and all the examples you have seen are in white and
    yellow tones. When you see a black cat of this breed, it won''t be difficult for
    you to recognize it. That is because the abstraction you developed will help you
    recognize this breed from its peculiar ears that curl back toward the back of
    the skull, not from its color.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the big challenges in machine learning is to enable learning from small
    data similar to previously. To mimic *step 1*, we fine-tune trained models for
    new tasks. For example, a language model that is trained on generic corpora (Wikipedia
    pages, news articles, and so on) can be fine-tuned on a specialized corpus (maritime
    law) where the amount of data available is limited. *Step 2* is what meta-learning
    is about.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Empirically, fine-tuning trained models for new tasks doesn't work in RL as
    well as in image or language tasks. It turns out that the neural network representations
    of RL policies are not as hierarchical as, for example, in image recognition,
    where the first layers detect edges and the last layers detect complete objects.
  prefs: []
  type: TYPE_NORMAL
- en: 'To better understand meta-learning capabilities in animals, let''s take a look
    at a canonical example: the Harlow experiment.'
  prefs: []
  type: TYPE_NORMAL
- en: The Harlow experiment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Harlow experiment explored meta-learning in animals, and involved a monkey
    that was shown two objects at a time:'
  prefs: []
  type: TYPE_NORMAL
- en: One of these objects was associated with a food reward, which the monkey had
    to discover.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each step (in a total of six), the objects were randomly placed in left and
    right positions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The monkey had to learn which object gave it a reward independent of its position.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After the six steps were over, the objects were replaced with new ones that
    were unfamiliar to the monkey and with an unknown reward association.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The monkey learned a strategy to randomly pick an object in the first step,
    understand which object gave the reward, and choose that object in the remaining
    steps regardless of the position of the object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This experiment nicely expresses the meta-learning capabilities in animals
    as it involves the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An unfamiliar environment/task for the agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent's effective adaptation to the unfamiliar environment/task through
    a strategy that involves necessary exploration, developing a task-specific policy
    on the fly (making a choice based on the object associated with the reward but
    not its position), and then exploitation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal in meta-RL is along the same lines, as we will see shortly. For now,
    let's continue to explore meta-RL's relation to some other concepts we have covered.
  prefs: []
  type: TYPE_NORMAL
- en: Relation to partial observability and domain randomization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main goals in a meta-RL procedure is to uncover the underlying environment/task
    at the test time. By definition, this means the environment is partially observable,
    and meta-RL is a specific approach to deal with it.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous chapter, [*Chapter 11*](B14160_11_Final_SK_ePub.xhtml#_idTextAnchor239),
    *Generalization and Partial Observability*, we discussed that we need memory and
    domain randomization to deal with partial observability. So, how is meta-RL different?
    Well, memory is still one of the key tools leveraged in meta-RL. We also randomize
    the environments/tasks while training meta-RL agents, similar to domain randomization.
    At this point, they may seem indistinguishable to you. However, there is a key
    difference:'
  prefs: []
  type: TYPE_NORMAL
- en: In domain randomization, the goal of training the agent is to develop a robust
    policy for all variations of an environment over a set of parameter ranges. For
    example, a robot could be trained under a range of friction and torque values.
    At test time, based on a sequence of observations that carry information and torque,
    the agent takes actions using the trained policy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In meta-RL, the goal of training the agent is to develop an adaptation procedure
    for new environments/tasks, which will potentially lead to different policies
    at the test time after an exploration period.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The difference can still be subtle when it comes to memory-based meta-RL methods,
    and the training procedure may be identical in some cases. To better understand
    the difference, remember the Harlow experiment: the idea of domain randomization
    does not suit the experiment, since the objects shown to the agent are completely
    new in every episode. Therefore, the agent does not learn how to act over a range
    of objects in meta-RL. Instead, it learns how to discover the task and act accordingly
    when it is shown completely new objects.'
  prefs: []
  type: TYPE_NORMAL
- en: With that, now it is finally time to discuss several meta-RL approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: A pioneer in meta-learning is Professor Chelsea Finn of Stanford University,
    who worked with Professor Sergey Levine of UC Berkeley as her PhD advisor. Profesor
    Finn has an entire course on meta-learning, available at [https://cs330.stanford.edu/](https://cs330.stanford.edu/).
    In this chapter, we mostly follow the terminology and classification of meta-RL
    approaches used by Professor Finn and Professor Levine.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's start with meta-RL with recurrent policies.
  prefs: []
  type: TYPE_NORMAL
- en: Meta-RL with recurrent policies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will cover one of the more intuitive approaches in meta-RL
    that uses **recurrent** **neural** **networks** (**RNNs**) to keep a memory, also
    known as the RL­2 algorithm. Let's start with an example to understand this approach.
  prefs: []
  type: TYPE_NORMAL
- en: Grid world example
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Consider a grid world where the agent''s task is to reach a goal state, G,
    from a start state, S. These states are randomly placed for different tasks, so
    the agent has to learn exploring the world to discover where the goal state is,
    and is then given a big reward. When the same task is repeated, the agent is expected
    to quickly reach the goal state, which is to adapt to the environment, since there
    is a penalty incurred for each time step. This is illustrated in *Figure 12.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.1 – Grid world example for meta-RL. (a) a task, (b) the agent''s
    exploration of the task, (c) the agent''s exploitation of what it learned'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_12_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.1 – Grid world example for meta-RL. (a) a task, (b) the agent's exploration
    of the task, (c) the agent's exploitation of what it learned
  prefs: []
  type: TYPE_NORMAL
- en: 'In order to excel at the task, the agent must do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Explore the environment (at test time).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Remember and exploit what it learned earlier.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Now, since we would like the agent to remember its previous experiences, we
    need to introduce a memory mechanism, implying using an RNN to represent the policy.
    There are several points we need to pay attention to:'
  prefs: []
  type: TYPE_NORMAL
- en: Just remembering the past observations is not enough as the goal state changes
    from task to task. The agent also needs to remember the past actions and rewards
    so that it can associate which actions in which states led to which reward to
    be able to uncover the task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Just remembering the history within the current episode is not enough. Notice
    that once the agent reaches the goal state, the episode ends. If we don't carry
    the memory over to the next episode, the agent has no way of benefiting from the
    experience gained in the previous episode. Again, note that there is no training
    or updates to the weights of the policy network taking place here. This is all
    happening at the test time, in an unseen task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Handling the former is easy: we just need to feed actions and rewards along
    with the observations to the RNN. To handle the former, we make sure that we *do
    not reset the recurrent state between episodes* unless the task changes so that
    the memory is not discontinued.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, during training, why would the agent learn a policy that explicitly starts
    a new task with an exploration phase? That is because the exploration phase helps
    the agent discover the task and collect higher rewards later. If we reward the
    agent during training based on individual episodes, the agent would not learn
    this behavior. That is because exploration has some immediate costs, which are
    recouped only in future episodes and when that memory is carried across episodes
    for the same task. To this end, we form **meta-episodes**, or **trials**, which
    are ![](img/Formula_12_012.png) episodes of the same task that are concatenated
    together. Again, the recurrent state is not reset within each episode, and the
    reward is calculated over the meta-episode. This is illustrated in *Figure 12.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B14160_12_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 12.2 – Procedure of agent-environment interaction (source: Duan et al.,
    2017)'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's see how this can be implemented in RLlib.
  prefs: []
  type: TYPE_NORMAL
- en: RLlib implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Concerning what we mentioned previously, meta-episodes can be formed by modifying
    the environment, so that is not quite related to RLlib. For the rest, we modify
    the model dictionary inside the agent config:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we enable the **long short-term memory** (**LSTM**) model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We pass actions and rewards in addition to observations to the LSTM:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We make sure that the LSTM input sequence is long enough to cover the multiple
    episodes within a meta-episode. The default sequence length is `20`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: That is all you need! You can train your meta-RL agents with a few lines of
    code change!
  prefs: []
  type: TYPE_NORMAL
- en: Info
  prefs: []
  type: TYPE_NORMAL
- en: This procedure may not always converge, or when it does, it may converge to
    bad policies. Try training multiple times (with different seeds) and with different
    hyperparameter settings, and this may help you obtain a good policy, but this
    is not guaranteed.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we can proceed on to gradient-based approaches.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-based meta-RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Gradient-based meta-RL methods propose improving the policy by continuing the
    training at test time so that the policy adapts to the environment it is applied
    in. The key is that the policy parameters right before adaptation, ![](img/Formula_06_036.png),
    are set in such a way that the adaptation takes place in just a few shots.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Gradient-based meta-RL is based on the idea that some initializations of policy
    parameters enable learning from very little data during adaptation. The meta-training
    procedure aims to find this initialization.
  prefs: []
  type: TYPE_NORMAL
- en: A specific approach in this branch is called **model-agnostic meta-learning**
    (**MAML**), which is a general meta-learning method that can also be applied to
    RL. MAML trains the agent for a variety of tasks to figure out a good ![](img/Formula_12_014.png)
    value that facilitates adaptation and learning from a few shots.
  prefs: []
  type: TYPE_NORMAL
- en: Let's see how you can use RLlib for this.
  prefs: []
  type: TYPE_NORMAL
- en: RLlib implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MAML is one of the agents implemented in RLlib and can be easily used with
    Ray''s Tune:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Using MAML requires implementing a few additional methods within the environment.
    These are, namely, the `sample_tasks`, `set_task`, and `get_task` methods, which
    help with training over various tasks. An example implementation could be in a
    pendulum environment, which is implemented in RLlib as follows ([https://github.com/ray-project/ray/blob/releases/1.0.0/rllib/examples/env/pendulum_mass.py](https://github.com/ray-project/ray/blob/releases/1.0.0/rllib/examples/env/pendulum_mass.py)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'While training MAML, RLlib measures the agent''s performance before any adaptation
    to the environment it is in via `episode_reward_mean`. The performance after *N*
    gradient steps of adaptation is shown in `episode_reward_mean_adapt_N`. The number
    of these inner adaptation steps is a config of the agent that can be modified:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'During training, you can see these metrics displayed on TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 12.3 – TensorBoard statistics'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_12_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 12.3 – TensorBoard statistics
  prefs: []
  type: TYPE_NORMAL
- en: That's it! Now, let's introduce the last approach of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Meta-RL as partially observed RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Another approach in meta-RL is to focus on the partially observable nature
    of the tasks and explicitly estimate the state from the observations until that
    point in time:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_12_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, form a probability distribution over possible tasks based on the likelihood
    of them being active in that episode, or more precisely, some vector that contains
    the task information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_12_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, iteratively sample a task vector from this probability distribution and
    pass that to the policy, in addition to the state:'
  prefs: []
  type: TYPE_NORMAL
- en: Sample ![](img/Formula_12_017.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Take actions from a policy that receives the state and task vector as input,
    ![](img/Formula_12_018.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: With that, we conclude our discussion on the three main meta-RL methods. Before
    we wrap up the chapter, let's discuss some of the challenges in meta-RL.
  prefs: []
  type: TYPE_NORMAL
- en: Challenges in meta-RL
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The main challenges regarding meta-RL, following *Rakelly*, *2019*, are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Meta-RL requires a meta-training step over various tasks, which are usually
    hand-crafted. A challenge is to create an automated procedure to generate these
    tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The exploration phase that is supposed to be learned during meta-training is
    in practice not efficiently learned.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Meta-training involves sampling from an independent and identical distribution
    of tasks, which is not a realistic assumption. So, one goal is to make meta-RL
    more "online" by making it learn from a stream of tasks.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Well, congratulations on coming this far! We have just covered meta-RL, a concept
    that could be hard to absorb. Hopefully, this introduction gives you the courage
    to dive into the literature on this topic and further explore it for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we covered meta-RL, one of the most important research directions
    in RL as its promise is to train agents that can adapt to new environments very
    quickly. To this end, we covered three methods: recurrent policies, gradient-based,
    and partial observability-based. Currently, meta-RL is in its infancy and is not
    performing as well as the more established approaches, so we covered the challenges
    in this area.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will cover several advanced topics in a single chapter.
    So, stay tuned to further deepen your RL expertise.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Prefrontal cortex as a meta-reinforcement learning system*, Wang, JX., Kurth-Nelson,
    Z., et al: [https://www.nature.com/articles/s41593-018-0147-8](https://www.nature.com/articles/s41593-018-0147-8)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Meta-Reinforcement Learning*: [https://blog.floydhub.com/meta-rl/](https://blog.floydhub.com/meta-rl/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Prefrontal cortex as a meta-reinforcement learning system*, blog: [https://deepmind.com/blog/article/prefrontal-cortex-meta-reinforcement-learning-system](https://deepmind.com/blog/article/prefrontal-cortex-meta-reinforcement-learning-system)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*RL2, Fast Reinforcement Learning via Slow Reinforcement Learning*, Duan, Y.,
    Schulman, J., Chen, X., Bartlett, P. L., Sutskever, I., & Abbeel P.: [https://arxiv.org/abs/1611.02779](https://arxiv.org/abs/1611.02779)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Learning to reinforcement learn*, Wang, JX., Kurth-Nelson, Z., Tirumala, D.,
    Soyer, H., Leibo, J. Z., Munos, R., Blundell, C., Kumaran, D., & Botvinick, M.:
    [https://arxiv.org/abs/1611.05763](https://arxiv.org/abs/1611.05763)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Open-sourcing Psychlab*: [https://deepmind.com/blog/article/open-sourcing-psychlab](https://deepmind.com/blog/article/open-sourcing-psychlab)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Meta-Reinforcement Learning of Structured Exploration Strategies*: [https://papers.nips.cc/paper/2018/file/4de754248c196c85ee4fbdcee89179bd-Paper.pdf](https://papers.nips.cc/paper/2018/file/4de754248c196c85ee4fbdcee89179bd-Paper.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Precociality*: [https://en.wikipedia.org/wiki/Precociality](https://en.wikipedia.org/wiki/Precociality)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Meta-Learning: from Few-Shot Learning to Rapid Reinforcement Learning*: [https://icml.cc/media/Slides/icml/2019/halla(10-09-15)-10-13-00-4340-meta-learning.pdf](https://icml.cc/media/Slides/icml/2019/halla(10-09-15)-10-13-00-4340-meta-learning.pdf)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Workshop on Meta-Learning (MetaLearn 2020): [https://meta-learn.github.io/2020/](https://meta-learn.github.io/2020/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
