["```py\n    pip install gym[box2d]==0.15.6\n    ```", "```py\n    import argparse\n    import pprint\n    from ray import tune\n    import ray\n    ```", "```py\n    from ray.rllib.agents.pg.pg import (\n        DEFAULT_CONFIG,\n        PGTrainer as trainer)\n    ```", "```py\n    if __name__ == \"__main__\":\n        parser = argparse.ArgumentParser()\n        parser.add_argument('--env',\n                            help='Gym env name.')\n        args = parser.parse_args()\n    ```", "```py\n        config = DEFAULT_CONFIG.copy()\n        config_update = {\n                     \"env\": args.env,\n                     \"num_gpus\": 1,\n                     \"num_workers\": 50,\n                     \"evaluation_num_workers\": 10,\n                     \"evaluation_interval\": 1\n                }\n        config.update(config_update)\n        pp = pprint.PrettyPrinter(indent=4)\n        pp.pprint(config)\n    ```", "```py\n        ray.init()\n        tune.run(trainer,\n                 stop={\"timesteps_total\": 2000000},\n                 config=config\n                 )\n    ```", "```py\n    python pg_agent.py --env \"LunarLanderContinuous-v2\"\n    ```", "```py\n    tensorboard --logdir=~/ray_results\n    ```", "```py\nfrom ray.rllib.agents.a3c.a2c import (\n    A2C_DEFAULT_CONFIG as DEFAULT_CONFIG,\n    A2Ctrainer as trainer)\n```", "```py\nfrom ray.rllib.agents.a3c.a3c import (\n    DEFAULT_CONFIG,\n    A3CTrainer as trainer)\n```", "```py\nfrom ray.rllib.agents.ppo.ppo import (\n    DEFAULT_CONFIG,\n    PPOTrainer as trainer)\n```", "```py\nfrom ray.rllib.agents.ddpg.ddpg import(\n    DEFAULT_CONFIG,\n    DDPGTrainer as trainer)\n```", "```py\nfrom ray.rllib.agents.ddpg.apex import (\n    APEX_DDPG_DEFAULT_CONFIG as DEFAULT_CONFIG,\n    ApexDDPGTrainer as trainer) \n```", "```py\nfrom ray.rllib.agents.ddpg.td3 import (\n    TD3_DEFAULT_CONFIG as DEFAULT_CONFIG,\n    TD3Trainer as trainer)\n```", "```py\nfrom ray.rllib.agents.sac.sac import (\n    DEFAULT_CONFIG,\n    SACTrainer as trainer)\n```", "```py\nfrom ray.rllib.agents.impala.impala import (\n    DEFAULT_CONFIG,\n    ImpalaTrainer as trainer)\n```"]