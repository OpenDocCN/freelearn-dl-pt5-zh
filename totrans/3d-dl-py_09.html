<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer155">
<h1 class="chapter-number" id="_idParaDest-105" lang="en-GB"><a id="_idTextAnchor108"/>8</h1>
<h1 id="_idParaDest-106" lang="en-GB"><a id="_idTextAnchor109"/>Modeling the Human Body in 3D</h1>
<p lang="en-GB">In the previous chapters, we explored ideas for modeling a 3D scene and the objects in them. Most of the objects we modeled were static and unchanging, but many applications of computer vision in real life center around humans in their natural habitat. We want to model their interactions with other humans and objects in <span class="No-Break" lang="">the scene.</span></p>
<p lang="en-GB">There are several applications for this. Snapchat filters, FaceRig, virtual try-on, and motion capture technology in Hollywood all benefit from accurate 3D body modeling. Consider, for example, an automated checkout technology. Here, a retail store is equipped with several depth-sensing cameras. They might want to detect whenever a person retrieves an object and modify their checkout basket accordingly. Such an application and many more will require us to accurately model the <span class="No-Break" lang="">human body.</span></p>
<p lang="en-GB">Human pose estimation is a cornerstone problem of human body modeling. Such a model can predict the location of joints such as shoulders, hips, and elbows to create a skeleton of the person in an image. They are then used for several downstream applications such as action recognition and human-object interaction. However, modeling a human body as a set of joints has <span class="No-Break" lang="">its limitations:</span></p>
<ul>
<li lang="en-GB">Human joints are not visible and never interact with the physical world. So, we cannot rely on them to accurately model <span class="No-Break" lang="">human-object interactions.</span></li>
<li lang="en-GB">Joints do not model the topology, volume, and surface of the body. For certain applications, such as modeling how clothing fits, joints alone are <span class="No-Break" lang="">not useful.</span></li>
</ul>
<p lang="en-GB">We can come to an agreement that human pose models are functional for some applications but certainly not realistic. How can we realistically model the human body? Will that address these limitations? What other applications can this unlock? We answer these questions in this chapter. Concretely, we will cover the <span class="No-Break" lang="">following topics:</span></p>
<ul>
<li lang="en-GB">Formulating the 3D <span class="No-Break" lang="">modeling problem</span></li>
<li lang="en-GB">Understanding the Linear Blend <span class="No-Break" lang="">Skinning technique</span></li>
<li lang="en-GB">Understanding the <span class="No-Break" lang="">SMPL model</span></li>
<li lang="en-GB">Using the <span class="No-Break" lang="">SMPL model</span></li>
<li lang="en-GB">Estimating 3D human pose and shape <span class="No-Break" lang="">using SMPLify</span></li>
<li lang="en-GB"><span class="No-Break" lang="">Exploring SMPLify</span></li>
</ul>
<h1 id="_idParaDest-107" lang="en-GB"><a id="_idTextAnchor110"/>Technical requirements</h1>
<p lang="en-GB">The computation requirements for the code in this chapter are pretty low. However, running this in a Linux environment is recommended since it has better support for certain libraries. However, it is not impossible to run this in other environments. In the coding sections, we describe in detail how to set up the environment to successfully run the code. We will need the following technical requirements for <span class="No-Break" lang="">this chapter:</span></p>
<ul>
<li lang="en-GB"><span class="No-Break" lang="">Python 2.7</span></li>
<li lang="en-GB">Libraries such as opendr, matplotlib, opencv, <span class="No-Break" lang="">and numpy.</span></li>
</ul>
<p lang="en-GB">The code snippets for this chapter can be found <span class="No-Break" lang="">at </span><a href="https://github.com/PacktPublishing/3D-Deep-Learning-with-Python"><span class="No-Break" lang="">https://github.com/PacktPublishing/3D-Deep-Learning-with-Python</span></a><span class="No-Break" lang="">.</span></p>
<h1 id="_idParaDest-108" lang="en-GB"><a id="_idTextAnchor111"/>Formulating the 3D modeling problem</h1>
<p lang="en-GB"><em class="italic" lang="">“All models are wrong, but some are useful”</em> is a popular aphorism<a id="_idIndexMarker362"/> in statistics. It suggests that it is often hard to mathematically model all the tiny details of a problem. A model will always be an approximation of reality, but some models are more accurate and, therefore, more useful <span class="No-Break" lang="">than others.</span></p>
<p lang="en-GB">In the field of machine learning, modeling a problem generally involves the following <span class="No-Break" lang="">two components:</span></p>
<ul>
<li lang="en-GB">Mathematically formulating <span class="No-Break" lang="">the problem</span></li>
<li lang="en-GB">Building algorithms to solve that problem under the constraints and boundaries of <span class="No-Break" lang="">that formulation</span></li>
</ul>
<p lang="en-GB">Good algorithms used on badly formulated problems often result in sub-optimal models. However, less powerful algorithms applied to a well-formulated model can sometimes result in great solutions. This insight is especially true for building 3D human <span class="No-Break" lang="">body models.</span></p>
<p lang="en-GB">The goal of this modeling<a id="_idIndexMarker363"/> problem is to create realistic animated human bodies. More importantly, this should represent realistic body shapes and must deform naturally according to changes in body pose and capture soft tissue motions. Modeling the human body in 3D is a hard challenge. The human body has a mass of bones, organs, skin, muscles, and water and they interact with each other in complex ways. To exactly model the human body, we need to model the behavior of all these individual components and their interactions with each other. This is a hard challenge, and for some practical applications, this level of exactness is unnecessary. In this chapter, we will model the human body’s surface and shape in 3D as a proxy for modeling the entire human body. We do not need the model to be exact; we just need it to have a realistic external appearance. This makes the problem <span class="No-Break" lang="">more approachable.</span></p>
<h2 id="_idParaDest-109" lang="en-GB"><a id="_idTextAnchor112"/>Defining a good representation</h2>
<p lang="en-GB">The goal is to represent<a id="_idIndexMarker364"/> the human body accurately<a id="_idIndexMarker365"/> with a low-dimensional representation. Joint models are low-dimensional representations (typically 17 to 25 points in 3D space) but do not carry a lot of information about the shape and texture of the person. On another end, we can consider the voxel grid representation. This can model the 3D body shape and texture, but it is extremely highly dimensional and does not naturally lend itself to modeling body dynamics due to <span class="No-Break" lang="">pose changes.</span></p>
<p lang="en-GB">Therefore, we need a representation that can jointly represent body joints and surfaces, which contains information about body volume. There are several candidate representations for surfaces; one such representation is a mesh of vertices. The <strong class="bold" lang="">Skinned Multi-Person Linear</strong> (<strong class="bold" lang="">SMPL</strong>) model uses this representation. Once specified, this mesh of vertices will describe the 3D shape of a <span class="No-Break" lang="">human body.</span></p>
<p lang="en-GB">Because there is a lot of history to this problem, we will find that many artists in the character animation industry have worked on building good body meshes. The SMPL model uses such expert insights to build a good initial template of a body mesh. This is an important first step because certain parts of the body have high-frequency variations (such as the face and hands). Such high-frequency variations need more densely packed points to describe them, but body parts with lower frequency variations (such as thighs) need less dense points to accurately describe them. Such a hand-crafted initial mesh will help bring down the dimensionality<a id="_idIndexMarker366"/> of the problem while keeping the necessary<a id="_idIndexMarker367"/> information to accurately model it. This mesh in the SMPL model is gender-neutral, but you can build variations for men and <span class="No-Break" lang="">women separately.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer134">
<img alt="Figure 8.1 – The SMPL model template mesh in the resting pose " height="689" src="image/B18217_08_01.jpg" width="1294"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – The SMPL model template mesh in the resting pose</p>
<p lang="en-GB">More concretely, the initial template mesh consists of 6,890 points in 3D space to represent the human body surface. When this is vectorized, this template mesh has a vector length of 6,890 x 3 = 20,670. Any human body can be obtained by distorting this template mesh vector to fit the <span class="No-Break" lang="">body surface.</span></p>
<p lang="en-GB">It sounds like a remarkably simple concept on paper, but the number of configurations of a 20,670-dimensional vector is extremely high. The set of configurations that represents a real human body is an extremely tiny subset of all the possibilities. The problem then becomes defining a method to obtain a plausible configuration that represents a real <span class="No-Break" lang="">human body.</span></p>
<p lang="en-GB">Before we understand how the SMPL model is designed, we need to learn about skinning models. In the next section, we will look at one of the simplest skinning techniques: the Linear Blend Skinning technique. This is important<a id="_idIndexMarker368"/> because the SMPL model<a id="_idIndexMarker369"/> is built on top of <span class="No-Break" lang="">this technique.</span></p>
<h1 id="_idParaDest-110" lang="en-GB"><a id="_idTextAnchor113"/>Understanding the Linear Blend Skinning technique</h1>
<p lang="en-GB">Once we have a good representation<a id="_idIndexMarker370"/> of the 3D human body, we want to model how it looks in different poses. This is particularly important<a id="_idIndexMarker371"/> for character animation. The idea is that <strong class="bold" lang="">skinning</strong> involves enveloping an underlying skeleton with a skin (a surface) that conveys the appearance of the object being animated. This is a term used in the animation industry. Typically, this representation takes the form of vertices, which are then used to define connected polygons to form <span class="No-Break" lang="">the surface.</span></p>
<p lang="en-GB">The Linear Blend Skinning model is used to take a skin in the resting pose and transform it into a skin in any arbitrary pose using a simple linear model. This is so efficient to render that many game engines support this technique, and it is also used to render game characters in <span class="No-Break" lang="">real time.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer135">
<img alt="Figure 8.2 – Initial blend shape (left) and deformed mesh generated with blend skinning (right) " height="320" src="image/B18217_08_02.jpg" width="320"/>
</div>
</div>
<p class="IMG---Figure" lang="en-GB"> </p>
<div>
<div class="IMG---Figure" id="_idContainer136">
<img alt="Figure 8.2 – Initial blend shape (left) and deformed mesh generated with blend skinning (right) " height="583" src="image/B18217_08_03.jpg" width="583"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – Initial blend shape (left) and deformed mesh generated with blend skinning (right)</p>
<p lang="en-GB">Let us now understand what this technique involves. This technique is a model that uses the <span class="No-Break" lang="">following parameters:</span></p>
<ul>
<li lang="en-GB">A template mesh, <em class="italic" lang="">T,</em> with <em class="italic" lang="">N</em> vertices. In this case, N = <span class="No-Break" lang="">6,890.</span></li>
<li lang="en-GB">We have the <em class="italic" lang="">K</em> joint locations represented by the vector <em class="italic" lang="">J</em>. These joints correspond to joints in the human body (such as shoulders, wrists, and ankles). There are 23 of these joints (K = <span class="No-Break" lang="">23).</span></li>
<li lang="en-GB">Blend weights, <em class="italic" lang="">W</em>. This is typically a matrix of size <em class="italic" lang="">N</em> x <em class="italic" lang="">K</em> that captures the relationship between the <em class="italic" lang="">N</em> surface vertices and the <em class="italic" lang="">K</em> joints of <span class="No-Break" lang="">the body.</span></li>
<li lang="en-GB">The pose parameters, Ɵ. These are the rotation parameters for each of the K joints. There are 3K of these parameters. In this case, we have 72 of them. 69 of these parameters correspond to 23 joints and 3 correspond to the overall <span class="No-Break" lang="">body rotation.</span></li>
</ul>
<p lang="en-GB">The skinning function<a id="_idIndexMarker372"/> takes the resting pose mesh, the joint locations, the blend weights, and the pose parameters as input and produces the <span class="No-Break" lang="">output vertices:</span></p>
<p class="IMG---Figure" lang="en-GB"><img alt="" height="124" src="image/B18217_08_Formula.png" width="844"/></p>
<p lang="en-GB">In Linear Blend Skinning, the function takes the form of a simple linear function of the transformed template vertices as described in the <span class="No-Break" lang="">following equation:</span></p>
<p class="IMG---Figure" lang="en-GB"><img alt="" height="133" src="image/Formula_08_002.png" width="521"/></p>
<p lang="en-GB">The meaning of these terms is <span class="No-Break" lang="">the following:</span></p>
<ul>
<li lang="en-GB"><em class="italic" lang="">t_i</em> represents the vertices in the original mesh in the <span class="No-Break" lang="">resting pose.</span></li>
<li lang="en-GB"><em class="italic" lang="">G</em>(Ɵ, J) is the matrix that transforms the joint k from the resting pose to the <span class="No-Break" lang="">target pose.</span></li>
<li lang="en-GB"><em class="italic" lang="">w_k, i </em> are elements of the blend weights, <em class="italic" lang="">W</em>. They represent the amount of influence the joint k has on the <span class="No-Break" lang="">vertex i.</span></li>
</ul>
<p lang="en-GB">While this is an easy-to-use linear model and is very well used in the animation industry, it does not explicitly preserve volume. Therefore, transformations can <span class="No-Break" lang="">look unnatural.</span></p>
<p lang="en-GB">In order to fix this problem, artists tweak the template mesh so that when the skinning model is applied, the outcome looks natural and realistic. Such linear deformations applied to the template mesh to obtain realistic-looking transformed mesh are called blend shapes. These blend shapes<a id="_idIndexMarker373"/> are artist-designed for all of the different poses the animated character can have. This is a very <span class="No-Break" lang="">time-consuming process.</span></p>
<p lang="en-GB">As we will see later, the SMPL model<a id="_idIndexMarker374"/> automatically calculates the blend shapes before applying the skinning model to them. In the next section, you will learn about how the SMPL model creates such pose-dependent blend shapes and how data is used to <span class="No-Break" lang="">guide it.</span></p>
<h1 id="_idParaDest-111" lang="en-GB"><a id="_idTextAnchor114"/>Understanding the SMPL model</h1>
<p lang="en-GB"><strong class="bold" lang="">As the acronym of SMPL</strong> suggests, this is a learned linear model<a id="_idIndexMarker375"/> trained on data from thousands of people. This model is built upon concepts from the Linear Blend Skinning model. It is an unsupervised and generative model that generates a 20,670-dimensional vector using the provided input parameters that we can control. This model calculates the blend shapes required to produce the correct deformations for varying input parameters. We need these input parameters to have the following <span class="No-Break" lang="">important properties:</span></p>
<ul>
<li lang="en-GB">It should correspond to a real tangible attribute of the <span class="No-Break" lang="">human body.</span></li>
<li lang="en-GB">The features must be low-dimensional in nature. This will enable us to easily control the <span class="No-Break" lang="">generative process.</span></li>
<li lang="en-GB">The features must be disentangled and controllable in a predictable manner. That is, varying one parameter should not change the output characteristics attributed to <span class="No-Break" lang="">other parameters.</span></li>
</ul>
<p lang="en-GB">Keeping these requirements in mind, the creators of the SMPL model came up with the two most important input attributes: some notion of body identity and body pose. The SMPL model decomposes the final 3D body<a id="_idIndexMarker376"/> mesh into an identity-based shape and pose-based shape (identity-based shape is also referred to as shape-based shape because the body<a id="_idIndexMarker377"/> shape is tied to a person’s identity). This gives the model the desired property of feature disentanglement. There are some other important factors such as breathing and soft tissue dynamics (when the body is in motion) that we do not explain in this chapter but are part of the <span class="No-Break" lang="">SMPL model.</span></p>
<p lang="en-GB">Most importantly, the SMPL model is an additive model of deformations. That is, the desired output body shape vector is obtained by adding deformations to the original template body vector. This additive property makes this model very intuitive to understand<a id="_idIndexMarker378"/> <span class="No-Break" lang="">and optimize.</span></p>
<h2 id="_idParaDest-112" lang="en-GB"><a id="_idTextAnchor115"/>Defining the SMPL model</h2>
<p lang="en-GB">The SMPL model builds<a id="_idIndexMarker379"/> on top of the standard skinning models. It makes the following changes <span class="No-Break" lang="">to it:</span></p>
<ul>
<li lang="en-GB">Rather than using the standard resting pose template, it uses a template mesh that is a function of the body shape <span class="No-Break" lang="">and poses</span></li>
<li lang="en-GB">Joint locations are a function of the <span class="No-Break" lang="">body shape</span></li>
</ul>
<p lang="en-GB">The function specified by the SMPL model takes the <span class="No-Break" lang="">following form:</span></p>
<p class="IMG---Figure" lang="en-GB"><img alt="" height="72" src="image/Formula_08_003.png" width="641"/></p>
<p lang="en-GB">The following is the meaning of the terms in the <span class="No-Break" lang="">preceding definitions:</span></p>
<ul>
<li lang="en-GB">β is a vector representing<a id="_idIndexMarker380"/> the identity (also called the shape) of the body. We will later learn more about what <span class="No-Break" lang="">it represents.</span></li>
<li lang="en-GB">Ɵ is the pose parameter corresponding to the <span class="No-Break" lang="">target pose.</span></li>
<li lang="en-GB">W is the blend weight from the Linear Blend <span class="No-Break" lang="">Skinning model.</span></li>
</ul>
<p lang="en-GB">This function looks very similar to the Linear Blend Skinning model. In this function, the template mesh is a function of shape and pose parameters, and the joint’s location is a function of shape parameters. This is not the case in the Linear Blend <span class="No-Break" lang="">Skinning model.</span></p>
<h3 lang="en-GB">Shape and pose-dependent template mesh</h3>
<p lang="en-GB">The redefined template mesh<a id="_idIndexMarker381"/> is an additive linear deformation of the standard <span class="No-Break" lang="">template mesh:</span></p>
<p class="IMG---Figure" lang="en-GB"><img alt="" height="72" src="image/Formula_08_004.png" width="584"/></p>
<p lang="en-GB">Here, we see <span class="No-Break" lang="">the following:</span></p>
<ul>
<li lang="en-GB"><img alt="" height="32" src="image/Image93832.png" width="37"/> is an additive deformation from the body shape parameters β. It is a learned deformation modeled as the first few principal components of the shape displacements. These principal components are obtained from the training data involving different people in the same <span class="No-Break" lang="">resting pose.</span></li>
<li lang="en-GB"><img alt="" height="51" src="image/Formula_08_006.png" width="74"/> is an additive pose<a id="_idIndexMarker382"/> deformation term. This is parametrized by Ɵ. This is also a linear function learned from a multi-pose dataset of people in <span class="No-Break" lang="">different poses.</span></li>
</ul>
<h3 lang="en-GB">Shape-dependent joints</h3>
<p lang="en-GB">Since joint locations depend<a id="_idIndexMarker383"/> on the body shape, they are redefined as a function of body shape. The SMPL model defines this in the <span class="No-Break" lang="">following manner:</span></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><img alt="" height="79" src="image/Formula_08_007.png" width="682"/></p>
<p lang="en-GB">Here, <img alt="" height="50" src="image/Formula_08_008.png" width="81"/> is the additive deformation from the body shape parameters β, and J is a matrix that transforms the rest template vertices to the rest template poses. The parameters of J are also learned <span class="No-Break" lang="">from data.</span></p>
<h1 id="_idParaDest-113" lang="en-GB"><a id="_idTextAnchor116"/>Using the SMPL model</h1>
<p lang="en-GB">Now that you have a high-level understanding<a id="_idIndexMarker384"/> of the SMPL model, we will look at how to use it to create 3D models of humans. In this section, we will start with a few basic functions; this will help you explore the SMPL model. We will load the SMP<a id="_idTextAnchor117"/>L model, and edit the shape and pose parameters to create a new 3D body. We will then save this as an object file and <span class="No-Break" lang="">visualize it.</span></p>
<p lang="en-GB">This code was originally created by the authors of SMPL for <strong class="source-inline" lang="">python2</strong>. Therefore, we need to create a separate <strong class="source-inline" lang="">python2</strong> environment. The following are the instructions <span class="No-Break" lang="">for this:</span></p>
<pre class="source-code" lang="en-GB">cd chap8</pre>
<pre class="source-code" lang="en-GB">conda create -n python2 python=2.7 anaconda</pre>
<pre class="source-code" lang="en-GB">source activate python2</pre>
<pre class="source-code" lang="en-GB">pip install –r requirements.txt</pre>
<p lang="en-GB">This creates and activates<a id="_idIndexMarker385"/> the Python 2.7 conda environment and installs the required modules. Python 2.7 is being deprecated, so you might see warning messages when you try to use it. In order to create a 3D human body with random shape and pose parameters, run the <span class="No-Break" lang="">following command.</span></p>
<p class="source-code" lang="en-GB">$ python render_smpl.py</p>
<p lang="en-GB">This will pop up a window that shows the rendering of a human body in 3D. Our output will likely be different since <strong class="source-inline" lang="">render_smpl.py</strong> creates a human body with random pose and <span class="No-Break" lang="">shape parameters.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer145">
<img alt="Figure 8.3 – An example rendering of the hello_smpl.obj created by explore_smpl.py " height="417" src="image/B18217_08_04.jpg" width="765"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – An example rendering of the hello_smpl.obj created by explore_smpl.py</p>
<p lang="en-GB">Now that we have run the code and obtained a visual output, let us explore what is happening in the <span class="No-Break" lang=""><strong class="source-inline" lang="">render_smpl.py</strong></span><span class="No-Break" lang=""> file:</span></p>
<ol>
<li lang="en-GB">Import all of the <span class="No-Break" lang="">required modules:</span><p class="source-code" lang="en-GB"><strong class="bold" lang="">import cv2</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">import numpy as np</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">from opendr.renderer import ColoredRenderer</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">from opendr.lighting import LambertianPointLight</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">from opendr.camera import ProjectPoints</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">from smpl.serialization import load_model</strong></p></li>
<li lang="en-GB">Load the model weights<a id="_idIndexMarker386"/> of the basic SMPL model. Here, we load the neural <span class="No-Break" lang="">body model:</span><p class="source-code" lang="en-GB"><strong class="bold" lang="">m = load_model('../smplify/code/models/basicModel_neutral_lbs_10_207_0_v1.0.0.pkl')</strong></p></li>
<li lang="en-GB">Next, we assign random pose and shape parameters. The following pose and shape parameters dictate how the 3D body mesh looks in <span class="No-Break" lang="">the end:</span><p class="source-code" lang="en-GB"><strong class="bold" lang="">m.pose[:] = np.random.rand(m.pose.size) * .2</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">m.betas[:] = np.random.rand(m.betas.size) * .03</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">m.pose[0] = np.pi</strong></p></li>
<li lang="en-GB">We now create a renderer and assign attributes to it, and we construct the light source. By default, we use the OpenDR renderer, but you can switch this to use the PyTorch3D renderer<a id="_idIndexMarker387"/> and light source. Before doing that, make sure to address any Python <span class="No-Break" lang="">incompatibility issues.</span><p class="source-code" lang="en-GB"><strong class="bold" lang="">rn = ColoredRenderer()</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">w, h = (640, 480)</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">rn.camera = ProjectPoints(v=m, rt=np.zeros(3), t=np.array([0, 0, 2.]), f=np.array([w,w])/2., c=np.array([w,h])/2., k=np.zeros(5))</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">rn.frustum = {'near': 1., 'far': 10., 'width': w, 'height': h}</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">rn.set(v=m, f=m.f, bgcolor=np.zeros(3))</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">rn.vc = LambertianPointLight(f=m.f, v=rn.v, num_verts=len(m), light_pos=np.array([-1000,-1000,-2000]), vc=np.ones_like(m)*.9, light_color=np.array([1., 1., 1.]))</strong></p></li>
<li lang="en-GB">We can now render the mesh and display it in an <span class="No-Break" lang="">OpenCV window:</span><p class="source-code" lang="en-GB"><strong class="bold" lang="">cv2.imshow('render_SMPL', rn.r)</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">cv2.waitKey(0)</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">cv2.destroyAllWindows()</strong></p></li>
</ol>
<p lang="en-GB">We have now used the SMPL model<a id="_idIndexMarker388"/> to generate a random 3D human body. In reality, we might be interested in generating 3D shapes that are more controllable. We will look at how to do this in the <span class="No-Break" lang="">next section.</span></p>
<h1 id="_idParaDest-114" lang="en-GB"><a id="_idTextAnchor118"/>Estimating 3D human pose and shape using SMPLify</h1>
<p lang="en-GB">In the previous<a id="_idIndexMarker389"/> section, you explored<a id="_idIndexMarker390"/> the SMPL model<a id="_idIndexMarker391"/> and used it to generate<a id="_idIndexMarker392"/> a 3D human body with a random shape and pose. It is natural to wonder whether it is possible to use the SMPL model to fit a 3D human body onto a person in a 2D image. This has multiple practical applications, such as understanding human actions or creating animations from 2D videos. This is indeed possible, and in this chapter, we are going to explore this idea in <span class="No-Break" lang="">more detail.</span></p>
<p lang="en-GB">Imagine that you are given a single RGB image of a person without any information about body pose, camera parameters, or shape parameters. Our goal is to deduce the 3D shape and pose from just this single image. Estimating the 3D shape from a 2D image is not always error-free. It is a challenging problem because of the complexity of the human body, articulation, occlusion, clothing, lighting, and the inherent ambiguity in inferring 3D from 2D (because multiple 3D poses can have the same 2D pose when projected). We also need an automatic way of estimating this without much manual intervention. It also needs to work on complex poses in natural images with a variety of backgrounds, lighting conditions, and <span class="No-Break" lang="">camera parameters.</span></p>
<p lang="en-GB">One of the best methods of doing this was invented by researchers from the Max Planck Institute of Intelligent Systems (where the SMPL model was invented), Microsoft, the University of Maryland, and the University of Tübingen. This approach is called SMPLify. Let us explore this approach in <span class="No-Break" lang="">more detail.</span></p>
<p lang="en-GB">The SMPLify approach<a id="_idIndexMarker393"/> consists of the following <span class="No-Break" lang="">two stages:</span></p>
<ol>
<li lang="en-GB" value="1">Automatically detect 2D joints using established pose detection models such as OpenPose or DeepCut. Any 2D joint detectors can be used in their place as long as they are predicting the <span class="No-Break" lang="">same joints.</span></li>
<li lang="en-GB">Use the SMPL model to generate the 3D shape. Directly optimize the parameters of the SMPL<a id="_idIndexMarker394"/> model so that the model joints of the SMPL model project to the 2D joints predicted in the <span class="No-Break" lang="">previous stage.</span></li>
</ol>
<p lang="en-GB">We know that SMPL<a id="_idIndexMarker395"/> captures shapes from just the joints. With the SMPL model, we can therefore capture<a id="_idIndexMarker396"/> information about body <a id="_idIndexMarker397"/>shape just from the joints. In the SMPL model, the body shape parameters are characterized by β. They are the coefficients of the principal components in the PCA shape model. The pose is parametrized by the relative rotation and theta of the 23 joints in the kinematic tree. We need to fit these parameters, β and theta, so that we minimize an <span class="No-Break" lang="">objective function.</span></p>
<h2 id="_idParaDest-115" lang="en-GB"><a id="_idTextAnchor119"/>Defining the optimization objective function</h2>
<p lang="en-GB">Any objective function <a id="_idIndexMarker398"/>must capture our intention to minimize some notion of error. The more accurate this error calculation is, the more accurate the output of the optimization step will be. We will first look at the entire objective function, then look at each of the individual components of that function and explain why each of them <span class="No-Break" lang="">is necessary:</span></p>
<p lang="en-GB"><img alt="" height="63" src="image/Formula_08_009.png" width="1135"/></p>
<ul>
<li lang="en-GB">We want to minimize this objective function by optimizing parameters <em class="italic" lang="">β</em> and Ɵ. It consists of four terms and corresponding coefficients, <em class="italic" lang="">λ</em><span class="subscript" lang="">Ɵ</span>, <em class="italic" lang="">λ</em><span class="subscript" lang="">a</span>, <em class="italic" lang="">λ</em><span class="subscript" lang="">sp</span>, and <em class="italic" lang="">λ</em><span class="subscript" lang="">β</span>, which are hyperparameters of the optimization process. The following is what each of the individual <span class="No-Break" lang="">terms means:</span><ul><li lang="en-GB"><img alt="" height="84" src="image/Formula_08_010.png" width="597"/>is the joint-based term that penalizes the distance between the 2D projected joint of the SMPL model and the predicted joint location from the 2D joint detector (such as DeepCut or OpenPose). <em class="italic" lang="">w_i</em> is the confidence score of each of the joints provided by the 2D joint detection model. When a joint is occluded, the confidence score for that joint will be low. Naturally, we should not place a lot of importance on such <span class="No-Break" lang="">occluded joints.</span></li><li lang="en-GB"><img alt="" height="80" src="image/Formula_08_011.png" width="240"/>is the pose that penalizes large angles between joints. For example, it ensures that elbows and knees do not <span class="No-Break" lang="">bend unnaturally.</span></li><li lang="en-GB"><img alt="" height="34" src="image/Formula_08_012.png" width="73"/>is a Gaussian mixture model fit on natural<a id="_idIndexMarker399"/> poses obtained from a very large dataset. This dataset is called the CMU Graphics Lab Motion Capture Database, consisting of nearly one million data points. This data-driven term in the optimization function ensures that the pose parameters are close to what we observe <span class="No-Break" lang="">in reality.</span></li></ul></li>
</ul>
<ol>
<li lang="en-GB" value="1"><img alt="" height="54" src="image/Formula_08_013.png" width="137"/> is the self-penetration error. When the authors optimized the objective function without this error term, they saw unnatural self-penetrations, such as elbows and hands twisted and penetrating through the stomach. This is physically impossible. However, after adding this error term, they found naturally qualitative results. This error term consists of body parts that are approximated as a set of spheres. They define incompatible spheres and penalize the intersection of these <span class="No-Break" lang="">incompatible spheres.</span></li>
<li lang="en-GB"><img alt="" height="70" src="image/Formula_08_014.png" width="234"/>is the shape obtained from the SMPL model. Note here that the principal component matrix is part of the SMPL model, which was obtained by training on the SMPL <span class="No-Break" lang="">training dataset.</span></li>
</ol>
<p lang="en-GB">In summary, the objective function consists of five components that, together, ensure that the solution to this objective function is a set of pose and shape parameters (theta and beta) that ensure that the 2D join projection distances are minimized while simultaneously ensuring that there are no large joint angles, no unnatural self-penetrations, and that the pose and shape<a id="_idIndexMarker400"/> parameters adhere to a prior distribution we see in a large dataset consisting of natural body poses <span class="No-Break" lang="">and shapes.</span></p>
<h1 id="_idParaDest-116" lang="en-GB"><a id="_idTextAnchor120"/>Exploring SMPLify</h1>
<p lang="en-GB">Now that we have a broad overview<a id="_idIndexMarker401"/> of how to estimate the 3D body shape of a person in a 2D RGB image, let us get a hands-on experience with code. Concretely, we are going to fit a 3D body shape onto two 2D images from the <strong class="bold" lang="">Leeds Sports Pose </strong>(<strong class="bold" lang="">LSP</strong>) dataset. This dataset contains 2,000 pose-annotated images of mostly sportspeople gathered from Flickr. We will first run through the code and generate these fitted body shapes before we dig deeper into the code. All the code used in this section was adapted from the implementation of the paper titled <em class="italic" lang="">Keep it SMPL: Automatic Estimation of 3D Human Pose and Shape from a Single Image</em>. We have only adapted it in a way that helps you, the learner, to quickly run the code and visualize the <span class="No-Break" lang="">outputs yourself.</span></p>
<p lang="en-GB">This code was originally created by the authors of SMPLify for <strong class="source-inline" lang="">python2</strong>. Therefore, we need to use the same <strong class="source-inline" lang="">python2</strong> environment we used while exploring the SMPL model. Before we run any code, let us quickly get an overview of how the code <span class="No-Break" lang="">is structured:</span></p>
<p class="source-code" lang="en-GB">chap8</p>
<p class="source-code" lang="en-GB">  -- smplify</p>
<p class="source-code" lang="en-GB">    -- code</p>
<p class="source-code" lang="en-GB">      -- fit3d_utils.py</p>
<p class="source-code" lang="en-GB">      -- run_fit3d.py</p>
<p class="source-code" lang="en-GB">      -- render_model.py</p>
<p class="source-code" lang="en-GB">      -- lib</p>
<p class="source-code" lang="en-GB">      -- models</p>
<p class="source-code" lang="en-GB">    -- images</p>
<p class="source-code" lang="en-GB">    -- results</p>
<h2 id="_idParaDest-117" lang="en-GB"><a id="_idTextAnchor121"/>Running the code</h2>
<p lang="en-GB">The main file you<a id="_idIndexMarker402"/> will be directly interacting with is <strong class="source-inline" lang="">run_fir3d.py</strong>. The folder named <strong class="source-inline" lang="">images</strong> will have some example images from the LSP dataset. However, before we run the code, make sure that <strong class="source-inline" lang="">PYTHONPATH</strong> is set correctly. This should point to the location of the <strong class="source-inline" lang="">chap8</strong> folder. You can run the following code <span class="No-Break" lang="">for it:</span></p>
<p class="source-code" lang="en-GB">export PYTHONPATH=$PYTHONPATH:&lt;user-specific-path&gt;/3D-Deep-Learning-with-Python/chap8/</p>
<p lang="en-GB">Now, go to the <span class="No-Break" lang="">right folder:</span></p>
<p class="source-code" lang="en-GB">cd smplify/code</p>
<p lang="en-GB">You can now run the following command to fit a 3D body onto images in the <span class="No-Break" lang=""><strong class="source-inline" lang="">images</strong></span><span class="No-Break" lang=""> folder:</span></p>
<p class="source-code" lang="en-GB">python run_fit3d.py --base_dir ../ --out_dir .</p>
<p lang="en-GB">This run will not use any interpenetration error since it will be faster to go through the optimization iterations. In the end, we will fit a body-neutral shape. You will be able to visualize the projected pose of the 3D body as it fits the person in the picture. Once the optimization is complete, you will see the following <span class="No-Break" lang="">two images:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer152">
<img alt="Figure 8.4 – Image in the LSP dataset of a person running (left) and the 3D body shape fitting this image (right) " height="687" src="image/B18217_08_05.jpg" width="1139"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Image in the LSP dataset of a person running (left) and the 3D body shape fitting this image (right)</p>
<p lang="en-GB">Another<a id="_idIndexMarker403"/> output is <span class="No-Break" lang="">as follows:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer153">
<img alt="Figure 8.5 – Image in the LSP dataset of a soccer player in action (left) and the 3D body shape fitting this image (right) " height="471" src="image/B18217_08_06.jpg" width="1062"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – Image in the LSP dataset of a soccer player in action (left) and the 3D body shape fitting this image (right)</p>
<h2 id="_idParaDest-118" lang="en-GB"><a id="_idTextAnchor122"/>Exploring the code</h2>
<p lang="en-GB">Now that you have run<a id="_idIndexMarker404"/> the code to fit humans in 2D images, let us look at the code in more detail to understand some of the main components needed to achieve this. You will find all the components in <strong class="source-inline" lang="">run_fit3d.py</strong>. You need to perform the <span class="No-Break" lang="">following steps:</span></p>
<ol>
<li lang="en-GB" value="1">Let us first import all of the modules we <span class="No-Break" lang="">will need:</span><p class="source-code" lang="en-GB"><strong class="bold" lang="">from os.path import join, exists, abspath, dirname</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">from os import makedirs</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">import cPickle as pickle</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">from glob import glob</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">import cv2</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">import numpy as np</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">import matplotlib.pyplot as plt</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">import argparse</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">from smpl.serialization import load_model</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">from smplify.code.fit3d_utils import run_single_fit</strong></p></li>
<li lang="en-GB">Let us now define<a id="_idIndexMarker405"/> where our SMPL model is located. This is done through <span class="No-Break" lang="">the following:</span><p class="source-code" lang="en-GB"><strong class="bold" lang="">MODEL_DIR = join(abspath(dirname(__file__)), 'models')</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">MODEL_NEUTRAL_PATH = join(</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">MODEL_DIR, 'basicModel_neutral_lbs_10_207_0_v1.0.0.pkl')</strong></p></li>
<li lang="en-GB">Let us set some parameters required for the optimization method and define the directories where our images and results are located. The results folder will have joint estimates for all the images in the dataset. <strong class="source-inline" lang="">viz</strong> is set to <strong class="source-inline" lang="">True</strong> to enable visualization. We are using an SMPL model with 10 parameters (that is, it uses 10 principal components to model the body shape). <strong class="source-inline" lang="">flength</strong> refers to the focal length of the camera; this is kept fixed during optimization. <strong class="source-inline" lang="">pix_thsh</strong> refers to the threshold (in pixels). If the distance between shoulder joints in 2D is lower than <strong class="source-inline" lang="">pix_thsh</strong>, the body orientation is ambiguous. This could happen when a person is standing perpendicular to the camera. As a consequence, it is hard to say whether they are facing left or right. So, a fit is run on both the estimated one and <span class="No-Break" lang="">its flip:</span><p class="source-code" lang="en-GB"><strong class="bold" lang="">viz = True</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">n_betas = 10</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">flength = 5000.0</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">pix_thsh = 25.0</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">img_dir = join(abspath(base_dir), 'images/lsp')</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">data_dir = join(abspath(base_dir), 'results/lsp')</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">if not exists(out_dir):</strong></p><p class="source-code" lang="en-GB"><strong class="bold" lang="">    makedirs(out_dir)</strong></p></li>
<li lang="en-GB">We should then load this gender-neutral SMPL model <span class="No-Break" lang="">into memory:</span><p class="source-code" lang="en-GB">model = load_model(MODEL_NEUTRAL_PATH)</p></li>
<li lang="en-GB">We then need to load<a id="_idIndexMarker406"/> the joint estimates for the images in the LSP dataset. The LSP dataset itself contains joint estimates and corresponding joint confidence scores for all the images in the dataset. We are going to just use that directly. You can also provide your own joint estimates or use good joint estimators, such as OpenPose or DeepCut, to get the <span class="No-Break" lang="">joint estimates:</span><p class="source-code" lang="en-GB">est = np.load(join(data_dir, 'est_joints.npz'))['est_joints']</p></li>
<li lang="en-GB">Next, we need to load images in the dataset and get the corresponding joint estimates and <span class="No-Break" lang="">confidence scores:</span><p class="source-code" lang="en-GB">img_paths = sorted(glob(join(img_dir, '*[0-9].jpg')))</p><p class="source-code" lang="en-GB">for ind, img_path in enumerate(img_paths):</p><p class="source-code" lang="en-GB">    img = cv2.imread(img_path)</p><p class="source-code" lang="en-GB">    joints = est[:2, :, ind].T</p><p class="source-code" lang="en-GB">    conf = est[2, :, ind]</p></li>
<li lang="en-GB">For each of the images in the dataset, use the <strong class="source-inline" lang="">run_single_fit</strong> function to fit the parameters beta and theta. The following function returns these parameters after running the optimization on an objective function similar to the SMPLify objective function we discussed in the <span class="No-Break" lang="">previous section:</span><p class="source-code" lang="en-GB">params, vis = run_single_fit(img, joints, conf, model, regs=sph_regs, n_betas=n_betas, flength=flength, pix_thsh=pix_thsh, scale_factor=2, viz=viz, do_degrees=do_degrees)</p></li>
</ol>
<p lang="en-GB">While the objective function<a id="_idIndexMarker407"/> is being optimized, this function creates a <strong class="source-inline" lang="">matplotlib</strong> window where the green circles are the 2D joint estimates from a 2D joint detection model (which are provided to you). The red circles are the projected joints of the SMPL 3D model that is being fitted onto the <span class="No-Break" lang="">2D image:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer154">
<img alt="Figure 8.6 – Visualization of the provided 2D joints (green) and the projected joints (red) of the SMPL model being fit to the 2D image " height="448" src="image/B18217_08_07.jpg" width="817"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – Visualization of the provided 2D joints (green) and the projected joints (red) of the SMPL model being fit to the 2D image</p>
<ol>
<li lang="en-GB" value="8">Next, we want to visualize the fitted 3D human body alongside the 2D RGB image. We use matplotlib<a id="_idIndexMarker408"/> for this. The following opens up an interactive window where you can save images <span class="No-Break" lang="">to disk:</span><p class="source-code" lang="en-GB">if viz:</p><p class="source-code" lang="en-GB">    import matplotlib.pyplot as plt</p><p class="source-code" lang="en-GB">    plt.ion()</p><p class="source-code" lang="en-GB">    plt.show()</p><p class="source-code" lang="en-GB">    plt.subplot(121)</p><p class="source-code" lang="en-GB">    plt.imshow(img[:, :, ::-1])</p><p class="source-code" lang="en-GB">    for di, deg in enumerate(do_degrees):</p><p class="source-code" lang="en-GB">        plt.subplot(122)</p><p class="source-code" lang="en-GB">        plt.cla()</p><p class="source-code" lang="en-GB">        plt.imshow(vis[di])</p><p class="source-code" lang="en-GB">        plt.draw()</p><p class="source-code" lang="en-GB">        plt.title('%d deg' % deg)</p><p class="source-code" lang="en-GB">        plt.pause(1)</p><p class="source-code" lang="en-GB">        raw_input('Press any key to continue...')</p></li>
<li lang="en-GB">We then want to save these parameters and visualization to disk with the <span class="No-Break" lang="">following code:</span><p class="source-code" lang="en-GB">    with open(out_path, 'w') as outf:</p><p class="source-code" lang="en-GB">        pickle.dump(params, outf)</p><p class="source-code" lang="en-GB">    if do_degrees is not None:</p><p class="source-code" lang="en-GB">        cv2.imwrite(out_path.replace('.pkl', '.png'), vis[0])</p></li>
</ol>
<p lang="en-GB">In the preceding code, the most important function is <strong class="source-inline" lang="">run_single_fit</strong>. You can explore this function in more detail <span class="No-Break" lang="">in </span><span class="No-Break" lang=""><strong class="source-inline" lang="">smplify.code.fit3d_utils.py</strong></span><span class="No-Break" lang="">.</span></p>
<p lang="en-GB">It is important to note<a id="_idIndexMarker409"/> here that the accuracy of the fitted 3D body is dependent on the accuracy of the 2D joints. Since the 2D joints are predicted using a joint detection model (such as OpenPose or DeepCut), the accuracy of such joint prediction models becomes very important and relevant to this problem. Estimating 2D joints is especially error-prone in the <span class="No-Break" lang="">following scenarios:</span></p>
<ul>
<li lang="en-GB">Joints that are not completely visible are hard to predict. This could happen due to a variety of reasons including self-occlusion, occlusion by other objects, unusual clothing, and <span class="No-Break" lang="">so on.</span></li>
<li lang="en-GB">It is easy to confuse between left and right joints (for example, left wrist versus right wrist). This is especially true when the person is facing the <span class="No-Break" lang="">camera sideways.</span></li>
<li lang="en-GB">Detecting joints in unusual poses is hard if the model is not trained with those poses. This depends on the diversity in the dataset used to train the <span class="No-Break" lang="">joint detector.</span></li>
</ul>
<p lang="en-GB">More broadly, a system<a id="_idIndexMarker410"/> consisting of multiple machine learning models interacting with each other sequentially (that is, when the output of one model becomes the input of another model) will suffer from cascading errors. Small errors in one component will result in large errors in outputs from downstream components. Such a problem is typically solved by training a system end to end. However, this strategy cannot be used here at the moment since there is no ground-truth data in the research community that directly maps a 2D input image to a <span class="No-Break" lang="">3D model.</span></p>
<h1 id="_idParaDest-119" lang="en-GB"><a id="_idTextAnchor123"/>Summary</h1>
<p lang="en-GB">In this chapter, we got an overview of the mathematical formulation of modeling human bodies in 3D. We understood the power of good representation and used simple methods such as Linear Blend Skinning on a powerful representation to obtain realistic outputs. We then got a high-level overview of the SMPL model and used it to create a random 3D human body. Afterward, we went over the code used to generate it. Next, we looked at how SMPLify can be used to fit a 3D human body shape onto a person in a 2D RGB image. We learned about how this uses the SMPL model in the background. Moreover, we fit human body shapes to two images in the LSP dataset and understood the code we used to do this. With this, we got a high-level overview of modeling the human body <span class="No-Break" lang="">in 3D.</span></p>
<p lang="en-GB">In the next chapter, we will explore the SynSin model, which is typically used for 3D reconstruction. The goal of the next chapter is to understand how to reconstruct an image from a different view, given just a <span class="No-Break" lang="">single image.</span></p>
</div>
</div></body></html>