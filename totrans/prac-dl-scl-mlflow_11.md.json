["```py\nlogged_model = f'runs:/{run_id}/model'\nmodel = mlflow.pytorch.load_model(logged_model)\n```", "```py\n    mlflow run . --experiment-name dl_model_chapter07 -P pipeline_steps=download_data,fine_tuning_model\n    ```", "```py\nruns:/1290f813d8e74a249c86eeab9f6ed24e/model\n```", "```py\n    class InferencePipeline(mlflow.pyfunc.PythonModel):\n        def __init__(self, finetuned_model_uri):\n            self.finetuned_model_uri = finetuned_model_uri\n        def sentiment_classifier(self, row):\n            pred_label = self.finetuned_text_classifier.predict({row[0]})\n            return pred_label\n        def load_context(self, context):\n            self.finetuned_text_classifier = mlflow.pytorch.load_model(self.finetuned_model_uri)\n        def predict(self, context, model_input):\n            results = model_input.apply(\n                        self.sentiment_classifier, axis=1,\n                        result_type='broadcast')\n            return results\n    ```", "```py\n    input = json.dumps([{'name': 'text', 'type': 'string'}])\n    output = json.dumps([{'name': 'text', 'type': 'string'}])\n    signature = ModelSignature.from_dict({'inputs': input, 'outputs': output})\n    ```", "```py\n    MODEL_ARTIFACT_PATH = 'inference_pipeline_model'\n    with mlflow.start_run() as dl_model_tracking_run:\n        finetuned_model_uri = 'runs:/1290f813d8e74a249c86eeab9f6ed24e/model'\n        inference_pipeline_uri = f'runs:/{dl_model_tracking_run.info.run_id}/{MODEL_ARTIFACT_PATH}'\n        mlflow.pyfunc.log_model(\n          artifact_path=MODEL_ARTIFACT_PATH,\n          conda_env=CONDA_ENV,\n          python_model=InferencePipeline(\n            finetuned_model_uri),\n          signature=signature)\n    ```", "```py\n    input = {\"text\":[\"what a disappointing movie\",\"Great movie\"]}\n    input_df = pd.DataFrame(input)\n    with mlflow.start_run():\n        loaded_model = \\\n        mlflow.pyfunc.load_model(inference_pipeline_uri)\n        results = loaded_model.predict(input_df)\n    ```", "```py\n'runs:/6edf6013d2454f7f8a303431105f25f2/inference_pipeline_model'\n```", "```py\n    import gcld3\n    self.detector = gcld3.NNetLanguageIdentifier(\n        min_num_bytes=0,\n        max_num_bytes=1000)\n    ```", "```py\n    def preprocessing_step_lang_detect(self, row):\n        language_detected = \\\n        self.detector.FindLanguage(text=row[0])\n        if language_detected.language != 'en':\n            print(\"found Non-English Language text!\")\n        return language_detected.language\n    ```", "```py\n    language_detected = self.preprocessing_step_lang_detect(row)\n    ```", "```py\n    from cachetools import LRUCache\n    self.cache = LRUCache(100)\n    ```", "```py\n    def preprocessing_step_cache(self, row):\n        if row[0] in self.cache:\n            print(\"found cached result\")\n            return self.cache[row[0]]\n    ```", "```py\n        cached_response = self.preprocessing_step_cache(row)\n        if cached_response is not None:\n            return cached_response\n    ```", "```py\n    self.cache[row[0]]=response\n    ```", "```py\n    def __init__(self, \n                 finetuned_model_uri,\n                 inference_pipeline_uri=None):\n        self.cache = LRUCache(100)\n        self.finetuned_model_uri = finetuned_model_uri\n        self.inference_pipeline_uri = inference_pipeline_uri\n    ```", "```py\n    response = json.dumps({\n                    'response': {\n                        'prediction_label': pred_label\n                    },\n                    'metadata': {\n                        'language_detected': language_detected,\n                    },\n                    'model_metadata': {\n                        'finetuned_model_uri': self.finetuned_model_uri,\n                        'inference_pipeline_model_uri': self.inference_pipeline_uri\n                    },\n                })                    \n    ```", "```py\n    mlflow.pyfunc.log_model(\n      artifact_path=MODEL_ARTIFACT_PATH,\n      conda_env=CONDA_ENV,\n      python_model=InferencePipeline(finetuned_model_uri,\n      inference_pipeline_uri),\n      signature=signature)\n    ```", "```py\nloaded_model = mlflow.pyfunc.load_model(inference_pipeline_uri)\n```", "```py\ninput = {\"text\":[\"what a disappointing movie\", \"Great movie\", \"Great movie\", \"很好看的电影\"]}\ninput_df = pd.DataFrame(input)\n```", "```py\nfound cached result \nfound Non-English language text.\n```", "```py\nfor i in range(results.size):\n    print(results['text'][i])\n```", "```py\ninference_pipeline_model:\n    parameters:\n      finetuned_model_run_id: { type: str, default: None }\n    command: \"python pipeline/inference_pipeline_model.py --finetuned_model_run_id {finetuned_model_run_id}\"\n```", "```py\nmlflow run . -e inference_pipeline_model  --experiment-name dl_model_chapter07 -P finetuned_model_run_id=07b900a96af04037a956c74ef691396e\n```", "```py\nhttp://localhost/#/models/inference_pipeline_model/\n```", "```py\nmlflow run . --experiment-name dl_model_chapter07\n```", "```py\n    def task(finetuned_model_run_id, pipeline_run_name):\n    ```", "```py\n    mlflow.pyfunc.log_model(\n        artifact_path=MODEL_ARTIFACT_PATH,\n        conda_env=CONDA_ENV,          \n        python_model=InferencePipeline(\n            finetuned_model_uri, \n            inference_pipeline_uri),\n        signature=signature,\n        registered_model_name=MODEL_ARTIFACT_PATH)\n    ```"]