<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Setting Up R for Deep Learning</h1>
                </header>
            
            <article>
                
<p>In this book, we will primarily use the following libraries for deep learning: <strong>H2O</strong>, <strong>MXNet</strong>, and <strong>Keras</strong>. We will also use the <span><strong>Restricted Boltzmann Machine</strong> (<strong>RBM</strong>)</span> package specifically for RBMs and <strong>deep belief networks</strong> (<strong>DBNs</strong>). In addition, we will conclude the book by using the <kbd>ReinforcementLearning</kbd> package.</p>
<p>In this chapter, we will install all of the previously listed packages. Each package can be used to train deep learning models in R. However, each has its particular strengths and weaknesses. We will explore the underlying architecture for each of these packages, which will help us to understand how they execute code. The packages have been created to allow R programmers to perform deep learning, with the exception of <kbd>RBM</kbd> and <kbd>ReinforcementLearning</kbd>, which are not written natively in R. This does have important implications for us to consider, starting with ensuring that we have all the necessary dependencies in order to install the packages.</p>
<p>This chapter will cover the following main topics:</p>
<ul>
<li>Installing the packages</li>
<li>Preparing a sample dataset</li>
<li>Exploring Keras</li>
<li>Exploring H2O</li>
<li>Reinforcement learning and RBM</li>
<li>Deep learning library comparison</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>You can find the code files used in this chapter at <a href="https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R">https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing the packages</h1>
                </header>
            
            <article>
                
<p>Some packages can be <span>directly</span><span> </span><span>installed from CRAN or GitHub, while</span> <kbd>H2O</kbd><span> and</span> <kbd>MXNet</kbd> <span>are a bit more complex. We will start with the packages that are the most straightforward to install and then move on to those that are more complex.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing ReinforcementLearning</h1>
                </header>
            
            <article>
                
<p>You can install <kbd>ReinforcementLearning</kbd> by using <kbd>install.packages</kbd>, since there is a CRAN release for this package, with the following line of code:</p>
<pre>install.packages("ReinforcementLearning")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing RBM</h1>
                </header>
            
            <article>
                
<p>The <kbd>RBM</kbd> package is only available on GitHub and is not available on CRAN, so the installation here is slightly different. First, you will need to install the <kbd>devtools</kbd> package if you do not already have it installed on your system. Next, use the <kbd>install_github()</kbd> function from the <kbd>devtools</kbd> package in place of <kbd>install.packages</kbd> in order to install the <kbd>RBM</kbd> package, as in the following code:</p>
<pre>install.packages("devtools")<br/>library(devtools)<br/>install_github("TimoMatzen/RBM")</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Keras</h1>
                </header>
            
            <article>
                
<p>Installing Keras is similar to the way that we installed <kbd>RBM</kbd>, with one subtle, but important, difference. <span>After running <kbd>install_github()</kbd> function to download and install the package, you run <kbd>install_keras()</kbd> to complete the installation.</span><span><span> </span></span><span>From the Keras documentation, calling <kbd>install_keras()</kbd> function is not required if you would prefer to install Keras yourself. </span></p>
<p><span>If you choose this route, the R package will find the version that you have installed. For this book, we will use <kbd>install_keras()</kbd> to complete installation, as in the following code:</span></p>
<pre>devtools::install_github("rstudio/keras")<br/>library(keras)<br/><br/>install_keras()</pre>
<p class="mce-root">If you would prefer to install the GPU version, then just make one change when calling the function, as follows:</p>
<pre>## for the gpu version :<br/>install_keras(gpu=TRUE)</pre>
<p>Running <kbd>install_keras()</kbd> will install Keras and TensorFlow in a virtual environment by default, except on a Windows machine where<span>—</span>at the time of writing<span>—</span><span>this is not supported,</span><span> in which case a</span> <kbd>conda</kbd> <span>environment will be used and Anaconda will need to be installed on the Windows machine beforehand. By default, the CPU version of TensorFlow and the latest release of Keras will be installed; an optional argument can be added in order to install the GPU version, as in the preceding code. For this book, we will accept the default values and run</span> <kbd>install_keras</kbd><span>.</span></p>
<p>If you have a machine with multiple versions of Python, you may discover some issues. If the Python instance that you wish to use is not declared, then R will attempt to find Python by first looking in common locations such as <kbd>usr/bin</kbd> and <kbd>usr/local/bin</kbd>.</p>
<p>When using Keras, you will likely want to point to the Python instance in your TensorFlow virtual environment. By default, the virtual environment will be called <kbd>r-tensorflow</kbd>. You can tell R the version of Python you would like to use by using the <kbd>use_python()</kbd> function from the <kbd>reticulate</kbd> package. Within the function, simply note the path to the instance of Python in the virtual environment. On my machine, this looks as follows:</p>
<pre>use_python('/Users/pawlus/.virtualenvs/r-tensorflow/bin/python') </pre>
<p>This should look similar on your machine.</p>
<p>Once R has the path to the right instance of Python, the code that we will introduce later in this chapter should work. However, if you are not referencing the correct version of Python, you will encounter errors and the code will not run.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing H2O</h1>
                </header>
            
            <article>
                
<p>For H2O, we will use the installation instructions from the H2O website. Using this approach, we will first search for any previous installations of H2O and remove those. Next, <kbd>RCurl</kbd> and <kbd>jsonlite</kbd> are installed and, after this, H2O is installed from the AWS S3 bucket containing the latest release. This is accomplished by simply making a change to the location of the repository when obtaining the package files, which, by default, is a CRAN server. We install H2O by running the following code:</p>
<pre>if ("package:h2o" %in% search()) { detach("package:h2o", unload=TRUE) }<br/>if ("h2o" %in% rownames(installed.packages())) { remove.packages("h2o") }<br/><br/>pkgs &lt;- c("RCurl","jsonlite")<br/>for (pkg in pkgs) {<br/>  if (! (pkg %in% rownames(installed.packages()))) { install.packages(pkg) }<br/>}<br/><br/>install.packages("h2o", type="source", repos=(c("http://h2o-release.s3.amazonaws.com/h2o/latest_stable_R")))</pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing MXNet</h1>
                </header>
            
            <article>
                
<p>There are multiple ways to install MXNet. The following code is the easiest installation instruction for setting up the CPU version of MXNet:</p>
<pre><span class="n">cran</span> <span class="o">&lt;-</span> <span class="nf">getOption</span><span class="p">(</span><span class="s">"repos"</span><span class="p">)</span>
<span class="n">cran[</span><span class="s">"dmlc"</span><span class="n">]</span> <span class="o">&lt;-</span> <span class="s">"https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/CRAN/"</span>
<span class="nf">options</span><span class="p">(</span><span class="n">repos</span> <span class="o">=</span> <span class="n">cran</span><span class="p">)</span>
<span class="nf">install.packages</span><span class="p">(</span><span class="s">"mxnet"</span><span class="p">)</span></pre>
<p>For the GPU support, use the following installation code:</p>
<pre>  <span class="n">cran</span> <span class="o">&lt;-</span> <span class="nf">getOption</span><span class="p">(</span><span class="s">"repos"</span><span class="p">)</span>
  <span class="n">cran[</span><span class="s">"dmlc"</span><span class="n">]</span> <span class="o">&lt;-</span> <span class="s">"https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/R/CRAN/GPU/cu92"</span>
  <span class="nf">options</span><span class="p">(</span><span class="n">repos</span> <span class="o">=</span> <span class="n">cran</span><span class="p">)</span>
  <span class="nf">install.packages</span><span class="p">(</span><span class="s">"mxnet"</span><span class="p">)</span></pre>
<p>OpenCV and OpenBLAS are required to use MXNet. If you need to install these, you can do so through one of the following options.</p>
<p>For macOS X, Homebrew can be used to install these libraries:</p>
<ol>
<li>If Homebrew is not already installed, installation instructions can be found at <a href="https://brew.sh/">https://brew.sh/</a>.</li>
<li>With Homebrew available, open a Terminal window and install the libraries using the following commands:</li>
</ol>
<pre style="padding-left: 60px"><strong>brew install opencv
brew install openblas</strong></pre>
<ol start="3">
<li>Finally, as shown here, create a symbolic link to ensure that the latest version of OpenBLAS is being used:</li>
</ol>
<pre style="padding-left: 60px"><strong>ln -sf /usr/local/opt/openblas/lib/libopenblas.dylib /usr/local/opt/openblas/lib/libopenblasp-r0.3.1.dylib</strong></pre>
<p>For Windows, the process is slightly more involved and so the details will not be noted in this book:</p>
<ul>
<li>To install OpenCV, follow the instructions found at <a href="https://docs.opencv.org/3.4.3/d3/d52/tutorial_windows_install.html">https://docs.opencv.org/3.4.3/d3/d52/tutorial_windows_install.html</a>.</li>
<li>To install OpenBLAS, follow the instructions found at <a href="https://github.com/xianyi/OpenBLAS/wiki/How-to-use-OpenBLAS-in-Microsoft-Visual-Studio">https://github.com/xianyi/OpenBLAS/wiki/How-to-use-OpenBLAS-in-Microsoft-Visual-Studio</a>.</li>
</ul>
<p>With OpenCV and OpenBLAS installed, the preceding lines should work to download and install the MXNet package. However, if you run into errors while trying to load the library, then it may be necessary to build the MXNet package and then create the R package. The instructions to complete this are very clear and detailed, but they are too long to include in this book:</p>
<ul>
<li><strong>For macOS X</strong>: <a href="https://mxnet.incubator.apache.org/versions/master/install/osx_setup.html">https://mxnet.incubator.apache.org/versions/master/install/osx_setup.html</a></li>
<li><strong>For Windows</strong>: <a href="https://mxnet.incubator.apache.org/versions/master/install/windows_setup.html">https://mxnet.incubator.apache.org/versions/master/install/windows_setup.html</a></li>
</ul>
<p>If there are still issues when downloading and installing the package after following the steps needed to build the MXNet library and the R bindings, this could be due to a number of possible reasons, many of which have been documented. Unfortunately, trying to work through all the possible installation scenarios and issues is outside the scope of this book. However, to use MXNet for learning purposes, it is possible to use a kernel from the Kaggle website, where MXNet is available.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing a sample dataset</h1>
                </header>
            
            <article>
                
<p>For Keras, H2O, and MXNet, we will use the adult census dataset, which uses U.S. Census data to predict whether someone makes more or less than USD50,000 a year. We will perform the data preparation for the Keras and MXNet examples here, so we are not repeating the same code in both examples:</p>
<ol>
<li>In the following code, we will load the data and label the two datasets to prepare for combining them:</li>
</ol>
<pre style="padding-left: 60px">library(tidyverse)<br/>library(caret)<br/><br/>train &lt;- read.csv("adult_processed_train.csv")<br/>train &lt;- train %&gt;% dplyr::mutate(dataset = "train")<br/>test &lt;- read.csv("adult_processed_test.csv")<br/>test &lt;- test %&gt;% dplyr::mutate(dataset = "test")</pre>
<p style="padding-left: 60px">As a result of running the preceding code, we will now have our libraries loaded and ready to use. We also have the <kbd>train</kbd> and <kbd>test</kbd> data loaded, which can now be seen in the <kbd>Environment</kbd> pane.</p>
<ol start="2">
<li>Next, we will combine the datasets so we can make some changes to all the data at the same time. We will use the <kbd>complete.cases</kbd> function to remove rows with <kbd>NA</kbd> for the sake of simplicity for these examples. We will also remove white space around the character terms so that terms such as <kbd>Male</kbd> and <kbd>Male </kbd> are both treated as the same term. Let's have a look at the following code:</li>
</ol>
<pre style="padding-left: 60px">all &lt;- rbind(train,test)<br/><br/>all &lt;- all[complete.cases(all),]<br/><br/>all &lt;- all %&gt;%<br/>  mutate_if(~is.factor(.),~trimws(.))</pre>
<ol start="3">
<li>We will now perform some extra pre-processing steps on the <kbd>train</kbd> dataset. First, we use the <kbd>filter()</kbd> function to extract the <kbd>train</kbd> data from the combined data frame labeled <kbd>all</kbd>. After that, we will extract the <kbd>target</kbd> column as a vector and then remove the <kbd>target</kbd> and <kbd>label</kbd> columns. We isolate the <kbd>train</kbd> data and <kbd>train</kbd> target variables using the following code:</li>
</ol>
<pre style="padding-left: 60px">train &lt;- all %&gt;% filter(dataset == "train")<br/>train_target &lt;- as.numeric(factor(train$target))<br/>train &lt;- train %&gt;% select(-target, -dataset)</pre>
<ol start="4">
<li>We will now separate the columns with numeric values and those with character values so that we can encode the character value column to prepare for a completely numeric matrix. We separate the numeric and character columns using the following code:</li>
</ol>
<pre style="padding-left: 60px">train_chars &lt;- train %&gt;%<br/>  select_if(is.character)<br/><br/>train_ints &lt;- train %&gt;%<br/>  select_if(is.integer)</pre>
<ol start="5">
<li>Next, we will use the <kbd>dummyVars()</kbd> function from <kbd>caret</kbd> to pivot the character values within a column into separate columns and we will indicate whether a character string is present in the row by assigning <kbd>1</kbd> to the row. If the character string is not present, then the column will contain <kbd>0</kbd> for that row. We perform this one-hot encoding step by running the following code:</li>
</ol>
<pre style="padding-left: 60px">ohe &lt;- caret::dummyVars(" ~ .", data = train_chars)<br/>train_ohe &lt;- data.frame(predict(ohe, newdata = train_chars))</pre>
<ol start="6">
<li>After transforming the data, we will now bind the two datasets back together with the following line of code:</li>
</ol>
<pre style="padding-left: 60px">train &lt;- cbind(train_ints,train_ohe)</pre>
<ol start="7">
<li>We will then repeat the same steps for the <kbd>test</kbd> dataset by running the following code:</li>
</ol>
<pre style="padding-left: 60px">test &lt;- all %&gt;% filter(dataset == "test")<br/>test_target &lt;- as.numeric(factor(test$target))<br/>test &lt;- test %&gt;% select(-target, -dataset)<br/><br/>test_chars &lt;- test %&gt;%<br/>  select_if(is.character)<br/><br/>test_ints &lt;- test %&gt;%<br/>  select_if(is.integer)<br/><br/>ohe &lt;- caret::dummyVars(" ~ .", data = test_chars)<br/>test_ohe &lt;- data.frame(predict(ohe, newdata = test_chars))<br/><br/>test &lt;- cbind(test_ints,test_ohe)</pre>
<ol start="8">
<li>When we created our target vector, it converted the factor values to <kbd>1</kbd> and <kbd>2</kbd>. However, we would like this to be <kbd>1</kbd> and <kbd>0</kbd>, so we will subtract <kbd>1</kbd> from the vector, as in the following code:</li>
</ol>
<pre style="padding-left: 60px">train_target &lt;- train_target-1<br/>test_target &lt;- test_target-1</pre>
<ol start="9">
<li>The last step is to clean up one column from the <kbd>train</kbd> dataset because it is not present in the <kbd>test</kbd> dataset. We remove this particular column by running the following line of code:</li>
</ol>
<pre style="padding-left: 60px">train &lt;- train %&gt;% select(-native.countryHoland.Netherlands)</pre>
<p>Now that we have loaded and prepared this dataset, we can use it in the next step to demonstrate some preliminary examples using all the packages that we have installed. At this point, our goal is to take a look at the syntax and ensure that the code can run and that the libraries are installed correctly. In later chapters, we will go into more depth about each package.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring Keras</h1>
                </header>
            
            <article>
                
<p>Keras was created and is maintained by Francois Chollet. Keras lays claim to being designed for humans, so common use cases are simple to execute and the syntax is clear and comprehensible. Keras is made to work with a number of lower-level deep learning languages and, in this book, Keras will be the interface that we use to utilize a number of popular deep learning backends, including TensorFlow.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Available functions</h1>
                </header>
            
            <article>
                
<p>Keras offers support for a broad array of deep learning methods, including the following:</p>
<ul>
<li><strong>Recurrent neural networks</strong> (<strong>RNNs</strong>)</li>
<li><strong>Long short-term memory</strong> (<strong>LSTM</strong>) networks<strong> </strong></li>
<li><strong>Convolutional neural networks</strong> (<strong>CNNs</strong>)</li>
<li><strong>Multilayer perceptrons</strong> (<strong>MLPs</strong>)</li>
<li><strong>Variable autoencoders</strong></li>
</ul>
<p>This is not an exhaustive list and further support is available for additional methods. However, these are what will be covered in the later chapters of this book.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">A Keras example</h1>
                </header>
            
            <article>
                
<p>In this example, we will train a multilayer perceptron on the adult census dataset that we just prepared. This example is included to introduce the syntax of the package and to show that a basic exercise can be completed without an excessive amount of code:</p>
<div class="packt_tip">If you have multiple versions of Python installed on your system, then this can become an issue. Use the <kbd>reticulate</kbd> package and the <kbd>use_python()</kbd> function to define the path to the Python instance that you would like to use; for example, <kbd>use_python(usr/local/bin/python3)</kbd>. You can also use <kbd>RETICULATE_PYTHON</kbd> within your <kbd>.Rprofile</kbd> file to set the path to the Python instance that R should use.</div>
<ol>
<li>To begin, we will load the <kbd>tensorflow</kbd> and <kbd>keras</kbd> libraries, as shown:</li>
</ol>
<pre style="padding-left: 60px">library(tensorflow)<br/>library(keras)</pre>
<ol start="2">
<li>Next, we will convert our datasets into matrices, as shown:</li>
</ol>
<pre style="padding-left: 60px">train &lt;- as.matrix(train)<br/>test &lt;- as.matrix(test)</pre>
<ol start="3">
<li>Now, we can create a sequential model, which will move through each layer in succession. We will have one layer and then we will compile the results. We define our model by running the following code:</li>
</ol>
<pre style="padding-left: 60px">model &lt;- keras_model_sequential()<br/><br/>model %&gt;%<br/>  layer_dense(units=35, activation = 'relu')<br/><br/>model %&gt;% keras::compile(loss='binary_crossentropy',<br/>                         optimizer='adam',<br/>                         metrics='accuracy')</pre>
<ol start="4">
<li>In the previous step, we defined our model and now, in the following code, we will fit this model to our training dataset:</li>
</ol>
<pre style="padding-left: 60px">history &lt;- model%&gt;%<br/>  fit(train, <br/>      train_target,<br/>      epoch=10,<br/>     batch=16,<br/>      validation_split = 0.15)</pre>
<ol start="5">
<li>Finally, we can evaluate our model by comparing our model results to the <kbd>test</kbd> target values. We evaluate model performance by running the following code:</li>
</ol>
<pre style="padding-left: 60px">model%&gt;%<br/>  keras::evaluate(test,test_target)</pre>
<p>This is the general syntax for <kbd>keras</kbd>. As we have shown, it is compatible with piping and has a syntax that will be familiar to R programmers. Next, we will look at an example using the MXNet package.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring MXNet</h1>
                </header>
            
            <article>
                
<p>MXNet is a deep learning library designed by the Apache Software Foundation. It supports both imperative and symbolic programming. It is designed for speed by serializing functions with dependencies while running functions with no dependencies in parallel. It provides compatibility with CPU and GPU processors.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Available functions</h1>
                </header>
            
            <article>
                
<p>MXNet provides the means to run a very extensive list of deep learning methods, including the following:</p>
<ul>
<li>CNN</li>
<li>RNN</li>
<li>GAN</li>
<li>LSTM</li>
<li>Autoencoders</li>
<li>RBM/DBN</li>
<li>Reinforcement learning</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Getting started with MXNet</h1>
                </header>
            
            <article>
                
<p>For MXNet, we will use the same prepared adult census dataset. We will also use a multilayer perceptron as our model. Fitting a model using MXNet will be quite familiar to you if you are familiar with fitting a model using a number of other common machine learning packages:</p>
<ol>
<li>First, we will load the MXNet package with the following line of code:</li>
</ol>
<pre style="padding-left: 60px">library(mxnet)</pre>
<ol start="2">
<li>Then, we will define our multilayer perceptron. A <kbd>seed</kbd> value is set for reproducibility purposes. Afterward, the training data is converted to a data matrix and passed as an argument <span>to the model</span>, along with the training target values, as shown:</li>
</ol>
<pre style="padding-left: 60px"><span class="p">mx.set.seed(0)<br/><br/>model &lt;- mx.mlp(data.matrix(train), train_target, hidden_node=10, out_node=2, out_activation="softmax",<br/>                num.round=10, array.batch.size=20, learning.rate=0.05, momentum=0.8,<br/>                eval.metric=mx.metric.accuracy)</span></pre>
<ol start="3">
<li>Next, we will make our predictions by applying the model to a data matrix version of the <kbd>test</kbd> data, as in the following code:</li>
</ol>
<pre style="padding-left: 60px"><span class="p">preds = predict(model, data.matrix(test))</span></pre>
<ol start="4">
<li>We can then use a confusion matrix to evaluate performance, with the adjusted target class on the <em>y</em>-axis and the predicted results on the <em>x</em>-axis, as shown:</li>
</ol>
<pre style="padding-left: 60px"><span class="p">pred.label = max.col(t(preds))-1<br/>table(pred.label, test_target)</span></pre>
<p>The syntax for MXNet should look familiar to those with experience with doing machine learning coding in R. <span>The function to train the model takes in descriptive and target data, as well as captures values for a number of options, just like using RandomForest or XGBoost.</span></p>
<p>The options are slightly different and we will cover how to best assign values to these arguments in a later chapter. However, the syntax is quite similar to the syntax used for other machine learning libraries in R. Next, we will write the code to train a minimal model using H2O.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring H2O</h1>
                </header>
            
            <article>
                
<p>H2O has been around longer than Keras and MXNet and is still used widely. It makes use of Java and MapReduce in-memory compression to handle big datasets. H2O is used for many machine learning tasks and also supports deep learning. In particular, H2O provides native support for feedforward artificial neural networks (multilayer perceptrons). H2O performs automatic data preparation and missing value handling. Loading data requires the use of a special data type: <kbd>H2OFrame</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Available functions</h1>
                </header>
            
            <article>
                
<p>H2O only natively supports feedforward neural networks. Compared with the other main packages for deep learning, this creates an obvious limitation for this library. However, this is a very common deep learning implementation. In addition, H2O allows for large objects to be stored outside memory in the H2O cluster. For these reasons, H2O is still a valuable library to know about when studying deep learning.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An H2O example</h1>
                </header>
            
            <article>
                
<p>For this example, we will again use the adult census dataset to predict income. As with our Keras example, this will be kept extremely minimal and we will cover just enough to illustrate the syntax for working with H2O, as well as the design nuances that differ from other packages:</p>
<ol>
<li>The first major difference when working with H2O is that we must explicitly initialize our H2O session, which will generate a Java Virtual Machine instance and connect it with R. This is accomplished with the following lines of code:</li>
</ol>
<pre style="padding-left: 60px"># load H2O package<br/>library(h2o)<br/><br/># start H2O<br/>h2o::h2o.init()</pre>
<ol start="2">
<li>Loading data to use with H2O requires converting the data to <kbd>H2OFrame</kbd>. <kbd>H2OFrame</kbd> is very similar to data frames, with the major distinction having to do with where the object is stored. While data frames are held in memory, <kbd>H2OFrame</kbd> is stored on the H2O cluster. This feature can be an advantage with very large datasets. In the following example, we will convert the data into the proper format using a two-step process. First, we load the data by reading <kbd>csv</kbd> in the usual way. Second, we will convert the data frames to <kbd>H2OFrame</kbd>. We convert our data into the proper format using the following code:</li>
</ol>
<pre style="padding-left: 60px">## load data <br/>train &lt;- read.csv("adult_processed_train.csv")<br/>test &lt;- read.csv("adult_processed_test.csv")<br/><br/># load data on H2o<br/>train &lt;- as.h2o(train)<br/>test &lt;- as.h2o(test)</pre>
<ol start="3">
<li>For this example, we will perform some imputation as the sole pre-processing step. In this step, we will replace all missing values and we will use <kbd>mean</kbd> for numeric data and <kbd>mode</kbd> for factor data. In H2O, setting <kbd>column = 0</kbd> will apply the function to the entire frame. Of note is that the function is called on the data; however, it is not necessary to assign the results to a new object as the imputations will be directly reflected in the data passed through as an argument to the function. It is also worth highlighting that in H2O, we can pass a vector to the method argument and it will be used for every variable in this case by first checking whether the first method can be used and, if not, moving on to the second method. Pre-processing this data is accomplished by running the following lines of code:</li>
</ol>
<pre style="padding-left: 60px">## pre-process<br/>h2o.impute(train, column = 0, method = c("mean", "mode"))<br/>h2o.impute(test, column = 0, method = c("mean", "mode"))</pre>
<ol start="4">
<li>In addition, in this step, we will define the <kbd>dependent</kbd> and <kbd>independent</kbd> variables. The <kbd>dependent</kbd> variable is held in the <kbd>target</kbd> column, while all the remaining columns contain the <kbd>independent</kbd> variables, which will be used for predicting the <kbd>target</kbd> variable during this task:</li>
</ol>
<pre style="padding-left: 60px">#set dependent and independent variables<br/>target &lt;- "target"<br/>predictors &lt;- colnames(train)[1:14]</pre>
<ol start="5">
<li>With all of the preparation steps complete, we can now create a minimal model. The H2O <kbd>deeplearning</kbd> function will create a feedforward artificial neural network. In this example, just the minimum required to run the model will be included. However, this function can accept 80 to 90 arguments and we will cover many of these in the later chapters. In the following code, we provide a name for our model, identify the training data, set a seed for reproducibility through replicating pseudo-random numbers involved in the model, define the <kbd>dependent</kbd> and <kbd>independent</kbd> variables, and note the number of times the model should be run and how the data should be cut for each round:</li>
</ol>
<pre style="padding-left: 60px">#train the model - without hidden layer<br/>model &lt;- h2o.deeplearning(model_id = "h2o_dl_example"<br/>                          ,training_frame = train<br/>                          ,seed = 321<br/>                          ,y = target<br/>                          ,x = predictors<br/>                          ,epochs = 10<br/>                          ,nfolds = 5)</pre>
<ol start="6">
<li>After running the model, the performance can be evaluated on the out-of-fold samples using the following line of code:</li>
</ol>
<pre style="padding-left: 60px">h2o.performance(model, xval = TRUE)</pre>
<ol start="7">
<li>Finally, when our model is complete, the cluster must be explicitly shut down just as it was initialized. The following function will close the current <kbd>h2o</kbd> instance:</li>
</ol>
<pre style="padding-left: 60px">h2o::h2o.shutdown()</pre>
<p>We can observe the following in this example:</p>
<ul>
<li>The syntax for H2O varies quite a bit from other machine learning libraries.</li>
<li>First, we need to initiate the Java Virtual Machine and we need to store our data in special data containers with this package.</li>
<li>In addition, we can see that imputation happens by running the function on a data object without assigning the changes back to an object.</li>
<li>We can see that we also need to include all the independent variable column names, which is slightly different from other models.</li>
<li>All of this is to say that H2O may feel a little unfamiliar as you use it. It is also limited in terms of the algorithms available. However, the ability to work with larger datasets is a definite advantage to this package.</li>
</ul>
<p>Now that we have looked at the comprehensive deep learning packages, we will focus on packages written with R that perform a specific modeling task or a limited set of tasks.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Exploring ReinforcementLearning and RBM</h1>
                </header>
            
            <article>
                
<p>The <kbd>ReinforcementLearning</kbd> and <kbd>RBM</kbd> packages differ from the libraries already covered in two important ways: first, they are specialized packages that have functions for only one specific deep learning task instead of attempting to support myriad deep learning options, and second, they are completely written in R and have no additional language dependencies. This can be an advantage as the complexity of the previous libraries means that the packages can break when changes happen outside the package. The support pages for these libraries are full of examples of installation FAQs and troubleshooting instructions, as well as some cases where a given package may suddenly stop working or become deprecated. In these cases, we encourage you to continue searching CRAN and other sites as the R community is well known for its dynamic, evolving, and robust support and development.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reinforcement learning example</h1>
                </header>
            
            <article>
                
<p>In this example, we will create a sample environment for reinforcement learning. The concept of reinforcement learning will be explored in more detail in a later chapter. For this example, we will generate a series of states and actions, along with the reward for taking those actions, that is, whether taking the action led to the desired result or a negative consequence. Afterward, we will define how our agent should respond or learn from actions. Once all of this has been defined, we will run the program and the agent will navigate through the environment to learn to solve the task. We will define and run a minimal reinforcement learning example by running the following code:</p>
<pre>library(ReinforcementLearning)<br/><br/>data &lt;- sampleGridSequence(N = 1000)<br/><br/>control &lt;- list(alpha = 0.1, gamma = 0.1, epsilon = 0.1)<br/><br/>model &lt;- ReinforcementLearning(data, s = "State", a = "Action", r = "Reward", s_new = "NextState", control = control)<br/><br/>print(model)</pre>
<p>We can see the following in this example:</p>
<ul>
<li>The syntax is very familiar and similar to many other R packages that we might use. In addition, we can see that we can complete a simple reinforcement learning task using a minimal amount of code.</li>
<li>In the GitHub repository for the package, all of the functions are written in R, which provides the convenience to explore the possible reasons for issues if they arrive. This also alleviates any concerns over dependencies on additional languages present in the more complex packages noted previously.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">An RBM example</h1>
                </header>
            
            <article>
                
<p>Here is a simple example using the <kbd>RBM</kbd> package. RBMs can be created using the <kbd>MXNet</kbd> library. However, we include this package in this book to note when it makes the most sense to train an <kbd>RBM</kbd> model using <kbd>MXNet</kbd> and when a standalone implementation of the algorithm may be a better fit.</p>
<p>In the following example, we assign the <kbd>train</kbd> <span>Fashion MNIST</span><span> </span><span>dataset to an object, create an RBM model on this data, and then make a prediction using the modeled results. A detailed exploration of how the RBM algorithm achieves this outcome and suggested applications are included in a future chapter. We will see how simply we can train this model and use it for prediction with a familiar syntax by running the following code:</span></p>
<pre>library(RBM)<br/><br/>data(Fashion)<br/><br/>train &lt;- Fashion$trainX<br/>train_label &lt;- Fashion$trainY<br/><br/>rbmModel &lt;- RBM(x = t(train), y = train_label, n.iter = 500, n.hidden = 200, size.minibatch = 10)<br/><br/>test &lt;- Fashion$testX<br/>test_label &lt;- Fashion$testY<br/><br/>PredictRBM(test = t(test), labels = test_label, model = rbmModel)</pre>
<p>As with the <kbd>ReinforcementLearning</kbd> package, the following applies:</p>
<ul>
<li>RBM is written entirely in R, so exploring the code in the repository is an excellent way to better understand how this particular technique works.</li>
<li>Also, as noted before, if you only need to train a model using RBM, then using a standalone package can be a great way to avoid having to load too many unnecessary functions, as is the case when using a library such as MXNet.</li>
<li>Both comprehensive packages and standalone packages have their place in a deep learning workflow, so this book will highlight the advantages and disadvantages of each of them.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing the deep learning libraries</h1>
                </header>
            
            <article>
                
<p>When comparing the three comprehensive machine learning libraries highlighted in this chapter (Keras, H2O, and MXNet), there are three primary differences: external language dependencies, functions, and syntax (ease of use and cognitive load). We will now cover each of these main differences in turn.</p>
<p>The first major difference between the three packages is the external language dependencies for each. As mentioned earlier, none of these packages are written in R. What this means is that you will need additional languages installed on your machine in order for these packages to work. It also means that you cannot easily look at the source documentation to see how a particular function works or why you are receiving a certain error (unless you know one of the languages, of course). The packages are written using the following languages: Keras in Python, H2O in Java, and MXNet in C#.</p>
<p>The next major difference has to do with the types of models that can be implemented from each package. You can use all three packages to train a feedforward model, such as a multilayer perceptron, where all hidden layers are fully connected layers. Keras and MXNet allow you to train deep learning models that include different types of hidden layers, as well as feedback loops between layers. These include RNNs, LSTMs, and CNNs. MXNet offers support for additional algorithms, including GANs, RBMs/DBNs, and reinforcement learning.</p>
<p>The last major difference has to do with the syntax for the model. Some use a very familiar syntax, which, in turn, makes them easier to learn and use. There is less for you to remember when the code resembles other R code that you use for other purposes. To this end, Keras has a very familiar syntax. It is modular, which means every function performs a discrete step within the overall model and all the functions can be piped together.</p>
<p>This closely resembles the way that <kbd>tidyverse</kbd> functions are chained together for data preparation. MXNet follows a syntax similar to other machine learning packages, where a dataset and target variable vector are passed to the function to train the model, along with numerous additional arguments that control how the model is created. The syntax for H2O deviates the furthest from common R programming conventions. It requires a cluster to be initialized ahead of any modeling. Data must also be stored in specific data objects and some functions operate on a data object just by calling a function on that object without assigning the results to a new object, as in typical R programming.</p>
<p>Aside from these differences, Keras also provides a means for using TensorFlow, while H2O allows larger objects to be stored out of memory. As noted, MXNet has the most robust offering of deep learning algorithms. As we can see, each package has its advantages and, throughout this book, we will explore them in depth and note the most suitable use cases and applications along the way.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>Having completed this chapter, you should now have all of the libraries that will be used in this book installed. In addition, you should be familiar with the syntax for each of them, and you should have seen a preliminary example of how to train a model using each one. We also explored some of the differences between the deep learning libraries, noting their strengths as well as their limitations. The three <span>main</span><span> </span><span>packages (Keras, MXNet, and H2O) are widely used for deep learning in industry and academia, and an understanding of these will enable you to tackle a number of deep learning problems. We are now ready to explore them all in more depth. However, before we do, we will review neural networks—the building block for all deep learning.</span></p>
<p>In the following chapter,<span> you will learn about artificial neural networks, which comprise the base building block for all deep learning. We will not use these deep learning libraries yet in the next chapter; however, the basics of how neural networks are coded will be critical as we move forward. Everything we cover in the next chapter will carry forward and be useful as we code examples of deep learning models. All deep learning cases are variations of the basic neural network, which we will learn how to create in the next chapter.</span></p>


            </article>

            
        </section>
    </body></html>