- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Solving Classification Problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we learned how to set up and run MXNet, how to work
    with Gluon and DataLoader, and how to visualize datasets for regression, classification,
    image, and text problems. We also discussed the different learning methodologies.
    In this chapter, we are going to focus on supervised learning with classification
    problems. We will learn why these problems are suitable for deep learning models
    with an overview of the equations that define these problems. We will learn how
    to create suitable models for them and how to train them, emphasizing the choice
    of hyperparameters. We will end each section by evaluating the models according
    to our data, as expected in supervised learning, and we will look at the different
    evaluation criteria for classification problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, we will cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding math for classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Defining loss functions and evaluation metrics for classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training for classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating classification models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    technical requirements apply:'
  prefs: []
  type: TYPE_NORMAL
- en: Ensure that you have completed the first recipe, *Installing MXNet, Gluon, GluonCV
    and GluonNLP*, from [*Chapter 1*](B16591_01.xhtml#_idTextAnchor016), *Up and Running*
    *with MXNet*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Ensure that you have completed the second recipe, *Toy dataset for classification
    – Loading, Managing, and Visualizing Iris Dataset*, from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon* *and DataLoader.*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Most of the concepts for the model, the loss and evaluation functions, and the
    training were introduced in [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving
    Regression Problems*. Furthermore, as we will see in this chapter, classification
    can be seen as a special case of regression. Therefore, it is strongly recommended
    to complete [*Chapter* *3*](B16591_03.xhtml#_idTextAnchor052) first.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch04](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch04).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, you can access each recipe directly from Google Colab; for example,
    for the first recipe of this chapter, visit the following link: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch04/4_1_Understanding_Maths_for_Classification_Models.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch04/4_1_Understanding_Maths_for_Classification_Models.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding math for classification models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As we saw in the previous chapter, **classification** problems are **supervised
    learning** problems whose output is a class from a set of classes (categorical
    assignments) – for example, the *iris* class of a flower.
  prefs: []
  type: TYPE_NORMAL
- en: As we will see throughout this recipe, classification models can be seen as
    individual cases of regression models. We will start by exploring a binary classification
    model. This is a model that will output one of two classes. We will label these
    classes `[0, 1]` for simplicity.
  prefs: []
  type: TYPE_NORMAL
- en: The simplest model we can use for such a binary classification problem is a
    **linear regression** model. This model will output a number; therefore, to modify
    the output to satisfy our new classification criteria, we will modify the activation
    function to a more suitable one.
  prefs: []
  type: TYPE_NORMAL
- en: 'As in the previous recipes, we will use a neural network as our model, and
    we will solve the iris dataset prediction problem we introduced in the second
    recipe, *Toy dataset for classification: Load, Manage and Visualize Iris Dataset*,
    in [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029), *Working with MXNet and Visualizing
    Datasets: Gluon* *and DataLoader.*'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be building upon the knowledge we gained in the previous recipes; therefore,
    it is highly recommended to read them. Moreover, as mentioned in the previous
    chapter, before jumping to understand our model, for the math part of this recipe,
    we will be using a little bit of matrix operations and linear algebra, but it
    will not be hard at all.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will be looking at the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Defining a binary classification model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining a multi-label classification model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining the features
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Initializing the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Defining a binary classification model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This is the perceptron model we introduced in the previous recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Perceptron](img/B16591_04_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – Perceptron
  prefs: []
  type: TYPE_NORMAL
- en: This model could be mathematically described as *y = f(WX + b)*, where *W* is
    the weight vector *[W*1*, W*2*, …. W*n*]*, (*n* is the number of features), *X*
    is the feature vector *[X*1*, X*2*, …. X*n*]*, *b* is the bias term, and *f()*
    is the activation function.
  prefs: []
  type: TYPE_NORMAL
- en: In the regression use case, we chose the activation function as the identity
    function, which provided an output equal to the input; therefore, we had *y =
    WX +* *b*.
  prefs: []
  type: TYPE_NORMAL
- en: For our binary classification use case, we want to have an output that will
    help us classify our input data points into two classes (`0` and `1`). In the
    original Perceptron paper in 1958, Rosenblatt, who studied a binary classification
    problem, chose the step function, which provided 0 and 1 as its only possible
    output.
  prefs: []
  type: TYPE_NORMAL
- en: If we recall from [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving
    Regression Problems*, in the third recipe, *Loss functions and evaluation metrics
    for regression*, we imposed certain properties on those functions. The fourth
    property, differentiability, was required due to the computations required by
    gradient descent. The same property applies to activation functions, and the step
    function Rosenblatt used does not comply with it.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, if we could find a function that is also continuous between 0 and
    1, we could assess that number as the probability or confidence of the output
    being `1`, as assessed by the model. This approach has advantages that we will
    explore later in the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, as the step function does not fulfill our properties, we need a
    new activation function. The most common activation function as the output of
    binary classification models is the **sigmoid** function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Sigmoid activation function](img/B16591_04_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.2 – Sigmoid activation function
  prefs: []
  type: TYPE_NORMAL
- en: The sigmoid function complies with all the required properties and the output
    quickly becomes `0` or `1`, which allows us to identify the output class suggested
    by the model.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a multi-label classification model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: What happens when we have multiple (let’s call this number *k*) classes instead
    of having just two classes to classify as we just saw? In this case, we need a
    different network architecture for our model. On the one hand, one output will
    not suffice now, as we need to have *k* different outputs. On the other hand,
    although we could use the sigmoid function as the activation function for each
    of the outputs, it is very useful if each output can be assessed as probabilities
    of each class, as we saw for the binary case. Using sigmoid will not enforce the
    condition for probabilities that the sum of all of them must be 1 (which means
    that each of the inputs must correspond to one of the classes).
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, a function very similar to the sigmoid function that satisfies
    the conditions described is the softmax function:'
  prefs: []
  type: TYPE_NORMAL
- en: σ(x j) =  ⅇ x j _ Σ i ⅇ x i
  prefs: []
  type: TYPE_NORMAL
- en: 'The class that will be selected is the one that will output the maximum value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – Multi-label classification network](img/B16591_04_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – Multi-label classification network
  prefs: []
  type: TYPE_NORMAL
- en: 'The full definition of our perceptron is *Y = f(WX + B)*, where *W* is now
    a weight matrix (with *n* x *k* shape: *n* being the number of features and *k*
    the number of outputs), *X* is the feature vector (*n* components), *B* is the
    bias vector (*k* components), and *f()* is the softmax activation function. *Y*
    is now a vector of *k* outputs, where the class with the maximum value will be
    the class assigned to the input.'
  prefs: []
  type: TYPE_NORMAL
- en: Defining the features
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have defined our model and its behavior theoretically; we did not
    use our problem framing or our dataset to define it. In this section, we will
    start working at a more practical level.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step to continue defining our model is to decide on which features
    (inputs) we are going to work with. We will continue using the Iris dataset we
    already know from the second recipe, *Toy dataset for classification: loading,
    managing, and visualizing the House Sales dataset* in [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029),
    *Working with MXNet and Visualizing Datasets: Gluon and DataLoader*. This dataset
    contained data for 150 flowers, including the class and 4 input features:'
  prefs: []
  type: TYPE_NORMAL
- en: Sepal length (in cm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sepal width (in cm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal length (in cm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petal width (in cm)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we show the first five flowers, we will see the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Features of flowers (Iris dataset)](img/B16591_04_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Features of flowers (Iris dataset)
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, for the Iris dataset, we have different output classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Setosa (0)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Versicolor (1)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Virginica (2)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Initializing the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that we have defined the input dimension (number of features) and output
    dimensions, we can initialize our model using random initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: If we compare these values to the values we obtained in the first recipe, *Understanding
    math for regression models*, from [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052),
    *Solving Regression Problems*, we can see how the weights are now represented
    as a matrix as we have several outputs, not just one (as it was in the regression
    case), and the bias is a vector instead of a number, for the same reason.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now that our model is initialized, we can use it to estimate the class of the
    first flower, which can be seen in *Figure 3**.24* to be *Setosa* (`0`). Here
    it is with our current model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Nicely done! Unfortunately, this was pure chance as the model was randomly initialized.
  prefs: []
  type: TYPE_NORMAL
- en: In the next recipe, we will see how to properly evaluate our classification
    models.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Like regression, classification models can have as many layers (depth) as needed,
    stacking as many of them as the problem’s solution requires.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we described the modifications from the perceptron described
    in the first recipe, *Understanding maths for regression models*, in [*Chapter
    3*](B16591_03.xhtml#_idTextAnchor052), *Solving Regression Problems*. There were
    two main modifications. The first one is that, as in this case, we want to categorize
    each input into a set of classes, we need one output per class. Moreover, in order
    to be able to understand the outputs of our model as probabilities, we needed
    a new activation function, softmax.
  prefs: []
  type: TYPE_NORMAL
- en: To finalize, we learned how to initialize our model, the effect initialization
    has on the weights and bias, and how we can use our data to evaluate it. We will
    develop all these topics further in the later recipes.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'At the beginning of the recipe, we walked through the activation function changes
    from Rosenblatt (step function) to regression (linear) to classification (sigmoid).
    One of the details we discussed was the non-differentiability of the step function.
    A deeper analysis can be found at the following link: [https://en.wikibooks.org/wiki/Signals_and_Systems/Engineering_Functions#Derivative](https://en.wikibooks.org/wiki/Signals_and_Systems/Engineering_Functions#Derivative)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using multi-layer architectures and/or sigmoid (or other activation functions)
    provides neural networks with the capability to approximate any function, which
    is known as the **Universal Approximation Theorem**. More details can be found
    here: [https://en.wikipedia.org/wiki/Universal_approximation_theorem](https://en.wikipedia.org/wiki/Universal_approximation_theorem)'
  prefs: []
  type: TYPE_NORMAL
- en: Defining loss functions and evaluation metrics for classification
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous recipe, we defined our input features, described our model,
    and initialized it. At that point, we passed a features vector of a flower to
    predict its iris species, calculated the output, and compared it against the expected
    class.
  prefs: []
  type: TYPE_NORMAL
- en: We also showed how those preliminary results did not represent a proper evaluation.
    In this recipe, we will explore the topic of evaluating our classification models.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we will also understand which loss functions fit best for the binary
    and multi-label classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Loss functions and evaluation functions need to satisfy the same properties
    that are described in [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving
    Regression Problems*, in the second recipe, *Defining Loss functions and evaluation
    metrics for regression*; therefore, I recommend reading that chapter first for
    a more thorough understanding.
  prefs: []
  type: TYPE_NORMAL
- en: We will start developing our topics by analyzing the binary classification approach
    (two output classes), and we will generalize afterward to the multiple-label classification
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s discuss some evaluation and loss functions and analyze their advantages
    and disadvantages. The functions we will describe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-entropy loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation – confusion matrix
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluation – metrics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-entropy loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As we discussed in our previous recipe, once the model has output a *probability*
    for each of our classes, we want to select the class that has the maximum probability
    as the output of our model.
  prefs: []
  type: TYPE_NORMAL
- en: When optimizing our model parameters, what we want is to find out which model
    parameters provide a maximum probability (`1`) for our desired class and a minimum
    probability for the rest (`0`). The derivation of the equation is out of the scope
    of the book, but you can find more information in *There’s more...* section at
    the end of the recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'For the case of two output classes, we have the binary cross-entropy loss (for
    *N* samples):'
  prefs: []
  type: TYPE_NORMAL
- en: BCE = −  1 _ N  ∑ i=0 N y i . log( ˆ y  i) + (1 − y i) . log(1 −  ˆ y  i)
  prefs: []
  type: TYPE_NORMAL
- en: 'We can plot this function for one sample, and assume the expected output to
    be *1 (yi =* *1)*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Binary cross-entropy loss graph (yi = 1)](img/B16591_04_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 – Binary cross-entropy loss graph (yi = 1)
  prefs: []
  type: TYPE_NORMAL
- en: 'For the general case of multiple labels, the multi-label or categorical cross-entropy
    loss for *M* classes becomes the following:'
  prefs: []
  type: TYPE_NORMAL
- en: L (y, ŷ) = − ∑ j=0 M ∑ i=0  M ( y ij * log ( ŷ ij))
  prefs: []
  type: TYPE_NORMAL
- en: This equation yields the same graph as *Figure 4**.5* when comparing each pair
    of classes.
  prefs: []
  type: TYPE_NORMAL
- en: We will use this function in combination with an optimizer during the training
    loop in the next recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation – confusion matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The confusion matrix helps us measure the performance of our model, comparing
    the results between the expected values (ground truth) and the actual values our
    model will provide. For a binary classification problem (where we defined the
    classes as `Positive` and `Negative`), we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Binary confusion matrix](img/B16591_04_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Binary confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 4**.6*, for each combination of predicted and actual class, we have
    the following terms (only valid in binary classification):'
  prefs: []
  type: TYPE_NORMAL
- en: '**TP**: True positives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FP**: False positives. It is also known as a *type* *I* error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**FN**: False negatives. It is also known as a *type* *II* error.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**TN**: True negatives.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ideally, we want TP and TN to be as close to 100% as possible, and FP and FN
    as close to 0% as possible.
  prefs: []
  type: TYPE_NORMAL
- en: 'When we have a multi-label classification problem, in the confusion matrix,
    we will have one row and one column per class. For *K* classes, we will have a
    *KxK* matrix where, in the matrix main diagonal, we are looking for 100% probabilities
    and 0% in the rest of the values. For example, using our Iris dataset (three output
    classes), we can compute the confusion matrix (*3x3*) for a model just randomly
    initialized, similar to the one we computed in the previous recipe. In this case,
    we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – Multi-label confusion matrix](img/B16591_04_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – Multi-label confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: As expected, the results shown are quite bad. Notably, not a single versicolor
    flower was correctly classified; however, this example helps us visualize a multi-label
    confusion matrix.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation – accuracy, precision, recall, specificity, and F1-score metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To characterize the performance of a model for a given binary classification
    problem, there are several interesting metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: Precision =  TP _ TP + FP
  prefs: []
  type: TYPE_NORMAL
- en: Recall =  TP _ TP + FN
  prefs: []
  type: TYPE_NORMAL
- en: F1 =  2 × Precision × Recall  _______________  Precision + Recall
  prefs: []
  type: TYPE_NORMAL
- en: Accuracy =  TP + TN ______________  TP + FN + TN + FP
  prefs: []
  type: TYPE_NORMAL
- en: Specificity =  TN _ TN + FP
  prefs: []
  type: TYPE_NORMAL
- en: 'Each of these metrics serves the purpose of helping us understand the performance
    of our models:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: From all the values, which ones were correctly classified (for
    both classes)?'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Precision**: This is the rate of positive predictions that were correctly
    classified. However, it does not provide any information regarding negative predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Recall**: This is the rate of positive labels that were correctly classified
    by the model. This figure does not include any information regarding negative
    labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specificity**: Similar to recall but for negative labels. This is the rate
    of negative labels that were correctly classified by the model. This figure does
    not include any information regarding positive labels. This metric is seldom used.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**F1 score**: This is the harmonic mean of precision and recall. By combining
    both metrics, this metric provides a better assessment of the model considering
    positive and negative classes. To achieve a high F1 score, the model needs to
    have a high precision and a high recall. A low F1 score will indicate that either
    precision, recall, or both are low.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These metrics can be computed for the multi-label scenario. For example, for
    our randomly initialized model and the Iris datasets, these were the figures computed
    (except specificity, which does not have a metrics function in **scikit-learn**):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The values are very close to average results, as expected from a randomly initialized
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluation – Area Under the Curve (AUC)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For binary classification problems, the output we want to provide is a class
    (`Positive` or `Negative`); however, the output of our model is a number (the
    probability of a positive result). To transform this result into a class, we need
    to apply a threshold.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, if we define our threshold as `0.5`, every probability larger
    than 0.5 will get assigned the `Positive` class. By decreasing our threshold,
    more values will be considered positive, increasing the number of TPs (**True
    Positive Rate**, or **TPR**) and the number of FPs (**False Positive Rate**, or
    **FPR**). If the threshold is increased, the effect is the contrary: fewer values
    will be considered positive, hence a smaller TPR and FPR.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we modify the value of the threshold, we will have different TPR and FPR
    values. If we plot those values in a graph, we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – AUC](img/B16591_04_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – AUC
  prefs: []
  type: TYPE_NORMAL
- en: If we calculate the area covered between the curve, the *x* axis, the *y = 0*
    axis, and the *y = 1* axis, we obtain a parameter that is not dependent on the
    threshold value; it defines the performance of our model for the given data.
  prefs: []
  type: TYPE_NORMAL
- en: 'TPR and FPR are only defined for binary classification cases. For the multi-label
    classification case, we can emulate binary classification cases. There are two
    possible approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: One versus one
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One versus rest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you are interested, you can find more information in the *There’s more...*
    section of this recipe. These curves are also known as **receiver operating**
    **characteristic** curves.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After understanding the differences between a regression model and a classification
    model, including the activation functions, in this recipe, we focused on the loss
    functions (useful for training) and the metrics (useful for evaluation). We explored
    both cases: binary classification and multi-label classification.'
  prefs: []
  type: TYPE_NORMAL
- en: We computed the most common loss function for classification, the binary/categorical
    cross-entropy loss function, and we defined several evaluation metrics such as
    accuracy, precision, recall, and F1 score. Furthermore, we learned about the confusion
    matrix as an easy way to look at the per-class performance of our models.
  prefs: []
  type: TYPE_NORMAL
- en: We ended the recipe by taking a look at the AUC, which provides a visualization
    agnostic of thresholds.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The mathematical formulas for cross-entropy loss were not derived. In these
    links, you can find more information:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Binary Cross-Entropy Loss: [https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross-Entropy-Loss-with-Sigmoid](https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Categorical Cross-Entropy Loss: [https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross-Entropy-Loss-with-Softmax](https://mxnet.apache.org/versions/1.7/api/python/docs/tutorials/packages/gluon/loss/loss.html#Cross-Entropy-Loss-with-Softmax)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a better understanding of how to compute multi-label classification metrics,
    I recommend the following link: [https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2](https://towardsdatascience.com/multi-class-metrics-made-simple-part-i-precision-and-recall-9250280bddc2).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To conclude, reading this AUC explanation can provide further insight: [https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc](https://developers.google.com/machine-learning/crash-course/classification/roc-and-auc).'
  prefs: []
  type: TYPE_NORMAL
- en: 'For the multi-label case, these examples can help understand the one versus
    one/one versus rest approaches: [https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html](https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc.html).'
  prefs: []
  type: TYPE_NORMAL
- en: Training for classification models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will visit the basic concepts of training a model to solve
    a classification problem. We will apply them to optimize the classification model
    we previously defined in this chapter, combined with the usage of the loss functions
    and evaluation metrics we discussed.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will predict the iris class of flowers using the dataset seen in the second
    recipe, *Toy dataset for classification – load, manage, and visualize Iris dataset*,
    from [*Chapter 2*](B16591_02.xhtml#_idTextAnchor029), *Working with MXNet and
    Visualizing Datasets: Gluon* *and DataLoader.*'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we will follow a similar pattern as we did in [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052),
    *Solving Regression Problems*, in the third recipe, *Training for regression models*,
    so it will be interesting to revisit the concepts of the loss function, optimizer,
    dataset split, epochs, and batch size.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will create our own training loop and we will evaluate how
    each hyperparameter influences the training. To achieve this, we will follow these
    steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Improve the model.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the loss function and optimizer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split our dataset and analyze fairness and diversity.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Put everything together for a training loop.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Improving the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To solve this problem, given the limited amount of data the dataset contains
    (150 samples), we will define a **Multi-Layer Perceptron** (**MLP**) network architecture,
    as we saw in [*Chapter 3*](B16591_03.xhtml#_idTextAnchor052), *Solving Regression
    Problems*, in the third recipe, *Training for regression models*. This will have
    2 hidden layers fully connected (dense) and **ReLU** activation function with
    10 neurons in each, and an output layer with the corresponding 3 outputs (1 per
    class). The last layer is left without an activation function, although softmax
    was expected. In the next section, we will understand why. For this network, the
    necessary code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'We have also applied scaling to our input features. The number of parameters
    for the model is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The number of trainable parameters is ~200\. For our (small) dataset, we have
    4 features for each of the 150 rows; therefore, our dataset is ~3 times the number
    of parameters of the model. Typically, this is the minimum for a successful model
    and, ideally, we would like to work with a dataset size of ~10 times the number
    of the parameters of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Even though the comparison between the data points available and the number
    of parameters of the model is a very useful one, different architectures have
    different requirements in terms of data. As usual, experimentation (trial and
    error) is the key to finding the right balance.
  prefs: []
  type: TYPE_NORMAL
- en: Defining the loss function and optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: As discussed in the previous recipe, we will compute the **Categorial Cross-Entropy**
    (**CCE**) loss function. However, there is an optimization detail when computing
    the CCE loss with the softmax activation function; therefore, the computation
    for the softmax function during training is included in the loss function. For
    **inference**, we need to add it externally. Similar to what we did for regression
    problems, we will focus our analysis on the **Stochastic Gradient Descent** (**SGD**)
    and Adam optimizers.
  prefs: []
  type: TYPE_NORMAL
- en: Splitting our dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: One of the strongest disadvantages of the Iris dataset is its size; with 150
    samples, it is a small dataset. For this reason, we will apply a 50/40/10 split
    for the training, validation, and test sets.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we analyze the splits to verify fairness and diversity, we obtain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Distribution for a training set (left), a validation set (middle
    ), and a test set (right)](img/B16591_04_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Distribution for a training set (left), a validation set (middle
    ), and a test set (right)
  prefs: []
  type: TYPE_NORMAL
- en: We can see that each of the classes is well represented across all features,
    the small size being the only reason for the discrepancies.
  prefs: []
  type: TYPE_NORMAL
- en: Putting it all together for a training loop
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The training loop for the classification case is very similar to the regression
    case, and we will follow a similar analysis as done earlier in this chapter: we
    will compare each of the hyperparameters, keeping the others constant (unless
    otherwise noted).'
  prefs: []
  type: TYPE_NORMAL
- en: Optimizer and learning rate
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: As discussed earlier, the chosen optimizer for the training loop and the learning
    rate are related because, for some optimizers (such as SGD), the learning rate
    is kept constant, whereas for others (such as Adam), it varies from a starting
    point.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The best optimizer depends on several factors, and nothing trumps trial and
    error; I strongly suggest trying a few to see which one fits best. In my experience,
    typically, SGD and Adam are the ones that work best, including in this problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s analyze how the training loss and validation loss vary for the SGD optimizer
    when we change the **learning rate** (**LR**), keeping the other parameters constant:
    epochs = 100, batch size = 64, and loss function = softmax cross-entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Loss for SGD optimizer with different LRs](img/B16591_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Loss for SGD optimizer with different LRs
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 4**.10*, we can conclude that for the SGD optimizer, an LR value
    between 1.0 and 3.0 is optimal. Furthermore, we can see that for very large values
    of LR (> 2.0), the algorithm still converges, whereas for the regression case,
    SGD and very large LRs made the model diverge.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s analyze how the training loss and validation loss vary for the Adam optimizer
    when we change the LR, keeping the other parameters constant: epochs = 100, batch
    size = 64, and loss function = softmax cross-entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Loss for the Adam optimizer with different LRs](img/B16591_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – Loss for the Adam optimizer with different LRs
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 4**.11*, we can conclude that for the Adam optimizer, an LR value
    between 10-2 and 10-1 is optimal.
  prefs: []
  type: TYPE_NORMAL
- en: Although, in this case, SGD with an LR of 3.0 is yielding the best results (smallest
    loss), the evolution of the optimization process is much noisier than for Adam,
    possibly due to the limited amount of data available (the batch size did not influence
    this). A smooth optimization process is also an indication of how well a model
    can generalize; hence, we will choose Adam as our optimizer for the rest of our
    tests.
  prefs: []
  type: TYPE_NORMAL
- en: Batch size
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s analyze how the training loss and validation loss vary for the Adam optimizer
    by changing the batch size, keeping the other parameters constant: epochs = 100,
    LR = 10-2, and loss function = softmax cross-entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Loss for the Adam optimizer by varying the batch size](img/B16591_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Loss for the Adam optimizer by varying the batch size
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 4**.12*, we can conclude that for the Adam optimizer, a batch size
    value between 32 and 64 provides the best results.
  prefs: []
  type: TYPE_NORMAL
- en: Epochs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let’s analyze how the training loss and validation loss vary for the Adam optimizer
    by varying the epochs, keeping the other parameters constant: LR = 10-2, batch
    size = 32, and loss function = softmax cross-entropy:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Loss for the Adam optimizer by varying the number of epochs](img/B16591_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Loss for the Adam optimizer by varying the number of epochs
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 4**.13*, we can conclude that a range of 200–300 epochs is good
    for our problem. With these values, it is very likely the best result will be
    achieved earlier than that.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: On our path to solving our classification problem, in this recipe, we learned
    how to update our model hyperparameters optimally. We revisited the role that
    each hyperparameter plays in the training loop and we performed some ablation
    studies for each individual hyperparameter. This helped us understand how our
    training and validation losses behaved when we modified each hyperparameter individually.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our current problem and the chosen model, we verified that the best set
    of hyperparameters was as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Optimizer: Adam'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'LR: 10-2'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Batch size: 32'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Number of epochs: 300'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: At the end of the training loop, these hyperparameters gave us a training loss
    of `0.01` and a validation loss of `0.1`.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we mostly put together concepts we have been learning about
    in the previous recipes and chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'We did pass through how, in our model definition, we did not explicitly use
    the softmax activation function. This is due to how cross-entropy loss and the
    softmax activation function work together during training (its joint derivative).
    A good reference to understand this point is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://peterroelants.github.io/posts/cross-entropy-softmax/](https://peterroelants.github.io/posts/cross-entropy-softmax/)'
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating classification models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the previous recipe, we learned how to choose our training hyperparameters
    to optimize our training. We also verified how those choices affected the training
    and validation losses. In this recipe, we are going to explore how those choices
    affect our actual evaluation in the real world. You will have noticed that we
    split the dataset into three different sets: training, validation, and test sets.
    However, during our training, we only used the training set and the validation
    set. In this recipe, we will emulate real-world behavior by using the unseen data
    from our model, the test set.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When evaluating a model, we can perform qualitative evaluation and quantitative
    evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: '**Qualitative evaluation** is the selection of one or more random (or not so
    random, depending on what we are looking for) samples and analyzing the result,
    verifying whether it matches our expectations.'
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will compute the evaluation metrics we defined in the second
    recipe, *Defining loss functions and evaluation metrics for classification models*,
    in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Furthermore, we are going to take a look at how training can have a large influence
    on the evaluation.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before jumping into model evaluation, we will discuss how we can measure our
    model training performance. Therefore, the steps of this recipe are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Measuring training performance – losses and accuracy
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Qualitative evaluation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Quantitative evaluation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Measuring training performance – losses and accuracy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As we saw for regression, a good way to prevent overfitting was early stopping.
    When training our classification model in the previous recipe, we stored the training
    loss, validation loss, and validation accuracy. Let’s see how the training loss,
    validation loss, and validation accuracy evolve as the training progresses:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 4.14 – Losses \uFEFFand accuracy versus epochs (Adam)](img/B16591_04_14.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Losses and accuracy versus epochs (Adam)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 4**.14*, at around epoch 50, the validation loss starts
    increasing although the training loss continues to decrease. Moreover, the validation
    accuracy also seems to plateau (close to 1.0/100%) around that epoch. We saved
    the model for the best accuracy and those are the values reported during training
    in the third recipe, *Training for classification models*, in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: 'As a reminder, if you want to use early stopping as is, MXNet provides a callback
    for it: [https://mxnet.apache.org/versions/1.6/api/r/docs/api/mx.callback.early.stop.html](https://mxnet.apache.org/versions/1.6/api/r/docs/api/mx.callback.early.stop.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'An important thing to mention is that, in the previous recipe, we mentioned
    that SGD did not provide very smooth training. If we plot the values, we obtain
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Losses versus epochs (SGD)](img/B16591_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Losses versus epochs (SGD)
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the training is not stable at all.
  prefs: []
  type: TYPE_NORMAL
- en: Qualitative evaluation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To verify that our model is behaving similarly to what we expect (yielding
    a high accuracy when predicting the iris species of a flower), a simple qualitative
    approach is to run our model for a random input from the test set (unseen data).
    In our case, this is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: For this example, as this was just one random input, accuracy can be either
    100% or 0% (accurate or not), and we got the right class.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Although I tried to keep the code as reproducible as possible (including setting
    the seeds for all random processes), there might be some sources of randomness.
    This means that your results might be different, but typically, the order of magnitude
    of errors will be similar.
  prefs: []
  type: TYPE_NORMAL
- en: Quantitative evaluation – confusion matrix
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For the stored results, the confusion matrix obtained is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – Confusion matrix](img/B16591_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – Confusion matrix
  prefs: []
  type: TYPE_NORMAL
- en: These results are excellent, as there are only values different from zero in
    the main diagonal. This means our model yielded perfect results for the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Quantitative evaluation – accuracy, precision, recall, and F1 score
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'During our previous recipes, we defined these metrics and worked with them
    to optimize the training. These evaluations were done for the training set and
    the validation set. For the test set, we obtained the following values:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Accuracy: 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Precision: 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Recall: 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'F1 score: 1.0'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we explored how to evaluate our classification model. To properly
    do this, we revisited the right decision of splitting our full dataset into a
    training set, a validation set, and a test set.
  prefs: []
  type: TYPE_NORMAL
- en: During training, we used the training set to calculate the gradients to update
    our model parameters, and the validation set to confirm the real-world behavior.
    Afterward, to evaluate our model performance, we used the test set, which was
    the only remaining set of unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: We discovered the value of qualitatively describing our model behavior by calculating
    the output of random samples, and of quantitatively describing our model performance
    by exploring several numbers and graphs including the confusion matrix, accuracy,
    precision, recall, and F1 score.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we computed the most important evaluation metrics for balanced
    classification datasets. However, when the dataset is imbalanced, we need to be
    careful. This is a good tutorial I like about this topic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/](https://machinelearningmastery.com/probability-metrics-for-imbalanced-classification/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'At this stage, we have completed our path through a complete classification
    problem: we explored our classification dataset, decided on our evaluation metrics,
    and defined and initialized our model. We understood the best hyperparameter combination
    of optimizer, learning rate, batch size, and epochs, and trained it with early
    stopping. We ended by evaluating it qualitatively and quantitatively.'
  prefs: []
  type: TYPE_NORMAL
