["```py\n    logged_model = 'runs:/37b5b4dd7bc04213a35db646520ec404/inference_pipeline_model'\n    ```", "```py\nlogged_model = 'models:/inference_pipeline_model/6'\n```", "```py\n    loaded_model = mlflow.pyfunc.spark_udf(spark, model_uri=logged_model, result_type=StringType())\n    ```", "```py\n    df = df.withColumn('predictions', loaded_model())\n    ```", "```py\n    python batch_inference.py\n    ```", "```py\nmlflow models serve -m models:/inference_pipeline_model/6\n```", "```py\n2022/03/06 21:50:19 INFO mlflow.models.cli: Selected backend for flavor 'python_function'\n2022/03/06 21:50:21 INFO mlflow.utils.conda: === Creating conda environment mlflow-a0968092d20d039088e2875ad04bbaa0f3a75206 ===\n± |main U:1 ?:8 X| done\nSolving environment: done\n```", "```py\n2022/03/06 21:52:11 INFO mlflow.pyfunc.backend: === Running command 'source /Users/yongliu/opt/miniconda3/bin/../etc/profile.d/conda.sh && conda activate mlflow-a0968092d20d039088e2875ad04bbaa0f3a75206 1>&2 && gunicorn --timeout=60 -b 127.0.0.1:5000 -w 1 ${GUNICORN_CMD_ARGS} -- mlflow.pyfunc.scoring_server.wsgi:app'\n[2022-03-06 21:52:12 -0800] [97554] [INFO] Starting gunicorn 20.1.0\n[2022-03-06 21:52:12 -0800] [97554] [INFO] Listening at: http://127.0.0.1:5000 (97554)\n[2022-03-06 21:52:12 -0800] [97554] [INFO] Using worker: sync\n[2022-03-06 21:52:12 -0800] [97561] [INFO] Booting worker with pid: 97561\n```", "```py\ncurl http://127.0.0.1:5000/invocations -H 'Content-Type: application/json' -d '{\n    \"columns\": [\"text\"],\n    \"data\": [[\"This is the best movie we saw.\"], [\"What a movie!\"]]\n}'\n```", "```py\n[{\"text\": \"{\\\"response\\\": {\\\"prediction_label\\\": [\\\"positive\\\"]}, \\\"metadata\\\": {\\\"language_detected\\\": \\\"en\\\"}, \\\"model_metadata\\\": {\\\"finetuned_model_uri\\\": \\\"runs:/07b900a96af04037a956c74ef691396e/model\\\", \\\"inference_pipeline_model_uri\\\": \\\"runs:/37b5b4dd7bc04213a35db646520ec404/inference_pipeline_model\\\"}}\"}, {\"text\": \"{\\\"response\\\": {\\\"prediction_label\\\": [\\\"positive\\\"]}, \\\"metadata\\\": {\\\"language_detected\\\": \\\"en\\\"}, \\\"model_metadata\\\": {\\\"finetuned_model_uri\\\": \\\"runs:/07b900a96af04037a956c74ef691396e/model\\\", \\\"inference_pipeline_model_uri\\\": \\\"runs:/37b5b4dd7bc04213a35db646520ec404/inference_pipeline_model\\\"}}\"}]\n```", "```py\npip install mlflow-ray-serve\n```", "```py\nray start --head\nserve start\n```", "```py\nmlflow deployments create -t ray-serve -m runs:/63f101fb3700472ca58975488636f4ae/inference_pipeline_model --name dl-inference-model-on-ray -C num_replicas=1\n```", "```py\n2022-03-20 20:16:46,564    INFO worker.py:842 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379\n2022-03-20 20:16:46,717    INFO api.py:242 -- Updating deployment 'dl-inference-model-on-ray'. component=serve deployment=dl-inference-model-on-ray\n(ServeController pid=78159) 2022-03-20 20:16:46,784    INFO deployment_state.py:912 -- Adding 1 replicas to deployment 'dl-inference-model-on-ray'. component=serve deployment=dl-inference-model-on-ray\n2022-03-20 20:17:10,309    INFO api.py:249 -- Deployment 'dl-inference-model-on-ray' is ready at `http://127.0.0.1:8000/dl-inference-model-on-ray`. component=serve deployment=dl-inference-model-on-ray\npython_function deployment dl-inference-model-on-ray is created\n```", "```py\npython ray_serve/query_ray_serve_endpoint.py\n```", "```py\n2022-03-20 21:16:45,125    INFO worker.py:842 -- Connecting to existing Ray cluster at address: 127.0.0.1:6379\n[{'name': 'dl-inference-model-on-ray', 'info': Deployment(name=dl-inference-model-on-ray,version=None,route_prefix=/dl-inference-model-on-ray)}]\n{\n    \"columns\": [\n        \"text\"\n    ],\n    \"index\": [\n        0,\n        1\n    ],\n    \"data\": [\n        [\n            \"{\\\"response\\\": {\\\"prediction_label\\\": [\\\"negative\\\"]}, \\\"metadata\\\": {\\\"language_detected\\\": \\\"en\\\"}, \\\"model_metadata\\\": {\\\"finetuned_model_uri\\\": \\\"runs:/be2fb13fe647481eafa071b79dde81de/model\\\", \\\"inference_pipeline_model_uri\\\": \\\"runs:/63f101fb3700472ca58975488636f4ae/inference_pipeline_model\\\"}}\"\n        ],\n        [\n            \"{\\\"response\\\": {\\\"prediction_label\\\": [\\\"positive\\\"]}, \\\"metadata\\\": {\\\"language_detected\\\": \\\"en\\\"}, \\\"model_metadata\\\": {\\\"finetuned_model_uri\\\": \\\"runs:/be2fb13fe647481eafa071b79dde81de/model\\\", \\\"inference_pipeline_model_uri\\\": \\\"runs:/63f101fb3700472ca58975488636f4ae/inference_pipeline_model\\\"}}\"\n        ]\n    ]\n}\n```", "```py\nray stop\n```", "```py\nmlflow sagemaker build-and-push-container --build --no-push -c mlflow-dl-inference\n```", "```py\n#15 exporting to image\n#15 sha256:e8c613e07b0b7ff33893b694f7759a10 d42e180f2b4dc349fb57dc6b71dcab00\n#15 exporting layers\n#15 exporting layers 8.7s done\n#15 writing image sha256:95bc539b021179e5e87087 012353ebb43c71410be535ef368d1121b550c57bd4 done\n#15 naming to docker.io/library/mlflow-dl-inference done\n#15 DONE 8.7s\n```", "```py\ndocker images | grep mlflow-dl-inference\n```", "```py\nmlflow-dl-inference          latest                  95bc539b0211   6 minutes ago   2GB\n```", "```py\nFROM mlflow-dl-inference\n```", "```py\nADD mlruns/1/meta.yaml  /opt/mlflow/mlruns/1/meta.yaml\n```", "```py\nADD mlruns/1/d01fc81e11e842f5b9556ae04136c0d3/ /opt/mlflow/mlruns/1/d01fc81e11e842f5b9556ae04136c0d3/\n```", "```py\nADD tmp/opt/mlflow/hf/cache/dl_model_chapter08/csv/ /opt/mlflow/tmp/opt/mlflow/hf/cache/dl_model_chapter08/csv/\n```", "```py\ndocker build . -t mlflow-dl-inference-w-finetuned-model\n```", "```py\n[+] Building 0.2s (9/9) FINISHED\n => [internal] load build definition from Dockerfile                                                      0.0s\n…………\n=> => naming to docker.io/library/mlflow-dl-inference-w-finetuned-model\n```", "```py\nmlflow sagemaker run-local -m runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model -p 5555 -i mlflow-dl-inference-w-finetuned-model\n```", "```py\n[2022-03-18 01:47:20 +0000] [552] [INFO] Starting gunicorn 20.1.0\n[2022-03-18 01:47:20 +0000] [552] [INFO] Listening at: http://127.0.0.1:8000 (552)\n[2022-03-18 01:47:20 +0000] [552] [INFO] Using worker: gevent\n[2022-03-18 01:47:20 +0000] [565] [INFO] Booting worker with pid: 565\n[2022-03-18 01:47:20 +0000] [566] [INFO] Booting worker with pid: 566\n[2022-03-18 01:47:20 +0000] [567] [INFO] Booting worker with pid: 567\n[2022-03-18 01:47:20 +0000] [568] [INFO] Booting worker with pid: 568\n[2022-03-18 01:47:20 +0000] [569] [INFO] Booting worker with pid: 569\n[2022-03-18 01:47:20 +0000] [570] [INFO] Booting worker with pid: 570\n```", "```py\ncurl http://127.0.0.1:5555/invocations -H 'Content-Type: application/json' -d '{\n    \"columns\": [\"text\"],\n    \"data\": [[\"This is the best movie we saw.\"], [\"What a movie!\"]]\n}'\n```", "```py\n[{\"text\": \"{\\\"response\\\": {\\\"prediction_label\\\": [\\\"positive\\\"]}, \\\"metadata\\\": {\\\"language_detected\\\": \\\"en\\\"}, \\\"model_metadata\\\": {\\\"finetuned_model_uri\\\": \\\"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\\\", \\\"inference_pipeline_model_uri\\\": \\\"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\\\"}}\"}, {\"text\": \"{\\\"response\\\": {\\\"prediction_label\\\": [\\\"negative\\\"]}, \\\"metadata\\\": {\\\"language_detected\\\": \\\"en\\\"}, \\\"model_metadata\\\": {\\\"finetuned_model_uri\\\": \\\"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\\\", \\\"inference_pipeline_model_uri\\\": \\\"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\\\"}}\"}]\n```", "```py\naws configure\n```", "```py\nmlflow sagemaker build-and-push-container --no-build --push -c mlflow-dl-inference-w-finetuned-model\n```", "```py\n2022/03/18 17:36:05 INFO mlflow.sagemaker: Pushing image to ECR\n2022/03/18 17:36:06 INFO mlflow.sagemaker: Pushing docker image mlflow-dl-inference-w-finetuned-model to xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1\nCreated new ECR repository: mlflow-dl-inference-w-finetuned-model\n2022/03/18 17:36:06 INFO mlflow.sagemaker: Executing: aws ecr get-login-password | docker login  --username AWS  --password-stdin xxxxx.dkr.ecr.us-west-2.amazonaws.com;\ndocker tag mlflow-dl-inference-w-finetuned-model xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1;\ndocker push xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1\nLogin Succeeded\nThe push refers to repository [xxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model]\n447db5970ca5: Pushed\n9d6787a516e7: Pushed\n1.23.1: digest: sha256:f49f85741bc2b82388e85c79f6621f4 d7834e19bdf178b70c1a6c78c572b4d10 size: 3271\n```", "```py\nxxxxx.dkr.ecr.us-west-2.amazonaws.com/mlflow-dl-inference-w-finetuned-model:1.23.1\n```", "```py\nsudo python sagemaker/deploy_to_sagemaker.py\n```", "```py\nmlflow.sagemaker.deploy(\n    mode='create',\n    app_name=endpoint_name,\n    model_uri=model_uri,\n    image_url=image_uri,\n    execution_role_arn=role,\n    instance_type='ml.m5.xlarge',\n    bucket = bucket_for_sagemaker_deployment,\n    instance_count=1,\n    region_name=region\n)\n```", "```py\n2022/03/18 19:30:47 INFO mlflow.sagemaker: Using the python_function flavor for deployment!\n2022/03/18 19:30:47 INFO mlflow.sagemaker: tag response: {'ResponseMetadata': {'RequestId': 'QMAQRCTJT36TXD2H', 'HostId': 'DNG57U3DJrhLcsBxa39zsjulUH9VB56FmGkxAiMYN+2fhc/rRukWe8P3qmBmvRYbMj0sW3B2iGg=', 'HTTPStatusCode': 200, 'HTTPHeaders': {'x-amz-id-2': 'DNG57U3DJrhLcsBxa39zsjulUH9VB56FmGkxAiMYN+2fhc/rRukWe8P3qmBmvRYbMj0sW3B2iGg=', 'x-amz-request-id': 'QMAQRCTJT36TXD2H', 'date': 'Sat, 19 Mar 2022 02:30:48 GMT', 'server': 'AmazonS3', 'content-length': '0'}, 'RetryAttempts': 0}}\n2022/03/18 19:30:47 INFO mlflow.sagemaker: Creating new endpoint with name: dl-sentiment-model ...\n2022/03/18 19:30:47 INFO mlflow.sagemaker: Created model with arn: arn:aws:sagemaker:us-west-2:xxxxx:model/dl-sentiment-model-model-qbca2radrxitkujn3ezubq\n2022/03/18 19:30:47 INFO mlflow.sagemaker: Created endpoint configuration with arn: arn:aws:sagemaker:us-west-2:xxxxx:endpoint-config/dl-sentiment-model-config-r9ax3wlhrfisxkacyycj8a\n2022/03/18 19:30:48 INFO mlflow.sagemaker: Created endpoint with arn: arn:aws:sagemaker:us-west-2:xxxxx:endpoint/dl-sentiment-model\n2022/03/18 19:30:48 INFO mlflow.sagemaker: Waiting for the deployment operation to complete...\n2022/03/18 19:30:48 INFO mlflow.sagemaker: Waiting for endpoint to reach the \"InService\" state. Current endpoint status: \"Creating\"\n```", "```py\naws sagemaker-runtime invoke-endpoint --endpoint-name 'dl-sentiment-model' --content-type 'application/json; format=pandas-split' --body '{\"columns\":[\"text\"], \"data\": [[\"This is the best movie we saw.\"], [\"What a movie!\"]]}' response.json\n```", "```py\n{\n    \"ContentType\": \"application/json\",\n    \"InvokedProductionVariant\": \"dl-sentiment-model-model-qbca2radrxitkujn3ezubq\"\n}\n```", "```py\ncat response.json\n```", "```py\n[{\"text\": \"{\\\"response\\\": {\\\"prediction_label\\\": [\\\"positive\\\"]}, \\\"metadata\\\": {\\\"language_detected\\\": \\\"en\\\"}, \\\"model_metadata\\\": {\\\"finetuned_model_uri\\\": \\\"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\\\", \\\"inference_pipeline_model_uri\\\": \\\"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\\\"}}\"}, {\"text\": \"{\\\"response\\\": {\\\"prediction_label\\\": [\\\"negative\\\"]}, \\\"metadata\\\": {\\\"language_detected\\\": \\\"en\\\"}, \\\"model_metadata\\\": {\\\"finetuned_model_uri\\\": \\\"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\\\", \\\"inference_pipeline_model_uri\\\": \\\"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\\\"}}\"}]\n```", "```py\nclient = boto3.client('sagemaker-runtime') \n```", "```py\nresponse = client.invoke_endpoint(\n```", "```py\n    EndpointName=app_name, \n```", "```py\n    ContentType=content_type,\n```", "```py\n    Accept=accept,\n```", "```py\n    Body=payload\n```", "```py\n    )\n```", "```py\npython sagemaker/query_sagemaker_endpoint.py\n```", "```py\nApplication status is: InService\n[{\"text\": \"{\\\"response\\\": {\\\"prediction_label\\\": [\\\"positive\\\"]}, \\\"metadata\\\": {\\\"language_detected\\\": \\\"en\\\"}, \\\"model_metadata\\\": {\\\"finetuned_model_uri\\\": \\\"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\\\", \\\"inference_pipeline_model_uri\\\": \\\"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\\\"}}\"}, {\"text\": \"{\\\"response\\\": {\\\"prediction_label\\\": [\\\"negative\\\"]}, \\\"metadata\\\": {\\\"language_detected\\\": \\\"en\\\"}, \\\"model_metadata\\\": {\\\"finetuned_model_uri\\\": \\\"runs:/d01fc81e11e842f5b9556ae04136c0d3/model\\\", \\\"inference_pipeline_model_uri\\\": \\\"runs:/dc5f670efa1a4eac95683633ffcfdd79/inference_pipeline_model\\\"}}\"}]\n```"]