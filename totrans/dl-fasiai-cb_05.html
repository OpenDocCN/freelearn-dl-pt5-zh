<html><head></head><body>
		<div id="_idContainer159">
			<h1 id="_idParaDest-128"><em class="italic"><a id="_idTextAnchor134"/>Chapter 5</em>: Training Recommender Systems</h1>
			<p>In this book, so far we have worked through recipes to train deep learning with fastai for a variety of datasets. In this chapter, we will go through recipes that take advantage of fastai's support for <strong class="bold">recommender systems</strong>, also known as <strong class="bold">collaborative filtering systems</strong>. Recommender systems combine the characteristics of tabular data models introduced in <a href="B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083"><em class="italic">Chapter 3</em></a>, <em class="italic">Training Models with Tabular Data</em>, with characteristics of text data models introduced in <a href="B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Models with Text Data</em>. </p>
			<p>Recommender systems cover a narrow, but well-established, use case: given a set of users and their ratings of a set of items, a recommender system predicts the rating that a user will give for an item that the user has not rated yet. For example, given a set of books and a set of readers' assessments of these books, recommender systems can make predictions about a given reader's assessment of a book they haven't read yet.</p>
			<p>In this chapter, you will learn how to use fastai's built-in support for recommender systems by working through a series of recipes that train models on a variety of recommender system datasets. You will see fastai features that will be familiar to you from previous chapters, as well as some new features that are unique to recommender systems. By the time you have completed this chapter, you will be ready to use the fastai high-level API to create recommender systems on your own datasets.</p>
			<p>Here are the recipes that will be covered in this chapter:</p>
			<ul>
				<li>Training a recommender system on a small curated dataset</li>
				<li>Training a recommender system on a large curated dataset</li>
				<li>Training a recommender system on a standalone dataset</li>
				<li>Test your knowledge</li>
			</ul>
			<h1 id="_idParaDest-129"><a id="_idTextAnchor135"/>Technical requirements</h1>
			<p>Ensure that you have completed the setup sections from <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with fastai</em>, and have a working <strong class="bold">Gradient</strong> instance or <strong class="bold">Colab</strong> setup. The recipes described in this chapter assume that you are using Gradient. Ensure that you have cloned the repository for the book, <a href="https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook">https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook</a>, and have access to the <strong class="source-inline">ch5</strong> folder. This folder contains the code samples described in this chapter.</p>
			<h1 id="_idParaDest-130"><a id="_idTextAnchor136"/>Training a recommender system on a small curated dataset</h1>
			<p>You <a id="_idIndexMarker336"/>may<a id="_idIndexMarker337"/> recall that <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with fastai</em>, described<a id="_idIndexMarker338"/> applications <a id="_idIndexMarker339"/>supported by fastai to<a id="_idIndexMarker340"/> cover <a id="_idIndexMarker341"/>four types of datasets: <strong class="bold">tabular</strong>, <strong class="bold">text</strong>, <strong class="bold">recommender systems</strong>, and <strong class="bold">images</strong>. In <a href="B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057"><em class="italic">Chapter 2</em></a>, <em class="italic">Exploring and Cleaning Up Data with fastai</em>, you saw sections on examining tabular datasets, text datasets, and image datasets. </p>
			<p>You may have wondered why there wasn't a section on examining recommender system datasets. The reason is that the data ingestion process for recommender systems in fastai is identical to the process for tabular datasets, as you will see in this section. While the ingestion process for recommender systems in fastai is identical to the ingestion process for tabular datasets, fastai does provide model training details that are specifically intended for recommender systems. </p>
			<p>In this section, we will go through the process of training a recommender system on a curated dataset to learn how to train recommender systems with fastai. </p>
			<p>In this section, you will train your recommender system using a small curated dataset – <strong class="source-inline">ML_SAMPLE</strong>. This dataset is a subset of the MovieLens dataset (<a href="https://grouplens.org/datasets/movielens">https://grouplens.org/datasets/movielens</a>) that contains user scores for movies. In the <em class="italic">Training a recommender system on a large curated dataset</em> section, we will train a recommender system on a much larger subset of the MovieLens dataset – <strong class="source-inline">ML_100k</strong>.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor137"/>Getting ready</h2>
			<p>Confirm that you can open the <strong class="source-inline">training_recommender_systems.ipynb</strong> notebook in the <strong class="source-inline">ch5</strong> directory of your repository.</p>
			<p>The dataset used in this section and <em class="italic">Training a recommender system on a large curated dataset</em> section are from the MovieLens Datasets. I gratefully acknowledge the opportunity to use this dataset to illustrate the recommender system capabilities of fastai. </p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. (2011). <em class="italic">Learning Word Vectors for Sentiment Analysis </em>(<a href="http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf">http://ai.stanford.edu/~amaas/papers/wvSent_acl2011.pdf</a>). The 49th Annual Meeting of the Association for Computational Linguistics (ACL 2011) <a href="http://www.aclweb.org/anthology/P11-1015">http://www.aclweb.org/anthology/P11-1015</a></p>
			<h2 id="_idParaDest-132"><a id="_idTextAnchor138"/>How to do it…</h2>
			<p>In this section, you will be running through the <strong class="source-inline">training_recommender_systems.ipynb</strong> notebook. Once you have the notebook open in your fastai environment, complete the following steps:</p>
			<ol>
				<li>Run the cells in the notebook up to the <strong class="source-inline">Ingest the dataset</strong> cell to import the required libraries and set up your notebook.</li>
				<li>Run the<a id="_idIndexMarker342"/> following<a id="_idIndexMarker343"/> cell to define the <strong class="source-inline">path</strong> object for this dataset:<p class="source-code">path = untar_data(URLs.ML_SAMPLE)</p></li>
				<li>Run the following cell to examine the directory structure of the dataset:<p class="source-code">path.ls()</p><p>The following output shows the directory structure of the dataset:</p><div id="_idContainer127" class="IMG---Figure"><img src="image/B16216_5_1.jpg" alt="Figure 5.1 – Output of path.ls()&#13;&#10;"/></div><p class="figure-caption">Figure 5.1 – Output of path.ls()</p></li>
				<li>Run the following cell to load the dataset into the <strong class="source-inline">df</strong> DataFrame:<p class="source-code">df = pd.read_csv(path/'ratings.csv')</p></li>
				<li>Run the following cell to see some records from the dataset:<p class="source-code">df.head()</p><p>The output, as shown in <em class="italic">Figure 5.2</em>, lists records from the dataset. Each record represents a user's rating for a movie. The <strong class="source-inline">userId</strong> column has the ID for the user. The <strong class="source-inline">movieId</strong> column has the ID for the movie. The values in the <strong class="source-inline">rating</strong> column are the ratings given by the users for the movies. For example, user <strong class="source-inline">73</strong> gives movie <strong class="source-inline">1097</strong> a rating of 4.0:</p><div id="_idContainer128" class="IMG---Figure"><img src="image/B16216_5_2.jpg" alt="Figure 5.2 – Output of df.head()&#13;&#10;"/></div><p class="figure-caption">Figure 5.2 – Output of df.head()</p></li>
				<li>Run the <a id="_idIndexMarker344"/>following<a id="_idIndexMarker345"/> cell to define a <strong class="source-inline">CollabDataLoaders</strong> object for the dataset: <p class="source-code">dls=CollabDataLoaders.from_df(df,bs= 64)</p><p>The definition of the <strong class="source-inline">CollabDataLoaders</strong> object uses the following arguments:</p><p>a) <strong class="source-inline">df</strong>: The DataFrame that you defined earlier in this notebook</p><p>b) <strong class="source-inline">bs</strong>: The batch size for the model training process</p></li>
				<li>Run the following cell to see a batch from the <strong class="source-inline">CollabDataLoaders</strong> object that you defined in the previous step:<p class="source-code">dls.show_batch()</p><p>The output displays the contents of the batch, as shown in the following screenshot:</p><div id="_idContainer129" class="IMG---Figure"><img src="image/B16216_5_3.jpg" alt="Figure 5.3 – Output of show_batch()&#13;&#10;"/></div><p class="figure-caption">Figure 5.3 – Output of show_batch()</p></li>
				<li>Run the<a id="_idIndexMarker346"/> following<a id="_idIndexMarker347"/> cell to define the <strong class="source-inline">collab_learner</strong> object for the recommender system model:<p class="source-code">learn=collab_learner(dls, y_range= [ 0 , 5.0 ] )</p><p>Here are the arguments for the definition of the <strong class="source-inline">collab_learner</strong> object:</p><p>a) <strong class="source-inline">dls</strong>: The <strong class="source-inline">CollabDataLoaders</strong> object that you defined in a previous step</p><p>b) <strong class="source-inline">y_range</strong>: The range of values in the <strong class="source-inline">rating</strong> column</p></li>
				<li>Run the following cell to train the collaborative filtering model:<p class="source-code">learn.fit_one_cycle( 5 )</p><p>Here is the argument for the model definition:</p><p>a) <strong class="source-inline">5</strong>: The number of epochs in the training run for the model</p><p>The output shows the training and validation loss, as shown in the following screenshot. You get an indication of the training improving as the validation loss decreases through the epochs:</p><div id="_idContainer130" class="IMG---Figure"><img src="image/B16216_5_4.jpg" alt="Figure 5.4 – Output of training the collaborative filtering model&#13;&#10;"/></div><p class="figure-caption">Figure 5.4 – Output of training the collaborative filtering model</p></li>
				<li>Now that we have trained a recommender system, we want to test it out by getting it <a id="_idIndexMarker348"/>to predict <a id="_idIndexMarker349"/>the ratings that some users will give for some movies. To exercise the recommender system, we first have to create some test data. To create some test data, run the following cell to define a DataFrame that includes a couple of test entries:<p class="source-code">scoring_columns = ['userId','movieId']</p><p class="source-code">test_df = pd.DataFrame(columns=scoring_columns)</p><p class="source-code">test_df.at[0,'userId'] = 388</p><p class="source-code">test_df.at[0,'movieId'] = 153</p><p class="source-code">test_df.at[1,'userId'] = 607</p><p class="source-code">test_df.at[1,'movieId'] = 1210</p><p class="source-code">test_df.head()</p><p>Here are the key elements of this cell:</p><p>a) <strong class="source-inline">scoring_columns</strong>: A list of the column names of the DataFrame. Note that these column names are the same column names that you saw in the output of <strong class="source-inline">show_batch()</strong>.</p><p>b) <strong class="source-inline">test_df</strong>: The DataFrame for containing the test entries with column names specified in the <strong class="source-inline">scoring_columns</strong> list.</p><p>The output of <strong class="source-inline">test_df.head()</strong> shows the contents of the completed DataFrame, as shown in the following screenshot:</p><div id="_idContainer131" class="IMG---Figure"><img src="image/B16216_5_5.jpg" alt="Figure 5.5 – Contents of the test_df DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 5.5 – Contents of the test_df DataFrame</p><p>What do <a id="_idIndexMarker350"/>these entries<a id="_idIndexMarker351"/> in the <strong class="source-inline">test_df</strong> DataFrame mean? We will use this DataFrame to see what the recommender system will predict for the following combinations:</p><p>a) The rating that <strong class="source-inline">userId</strong> <strong class="source-inline">388</strong> will give for <strong class="source-inline">movieId</strong> <strong class="source-inline">153</strong>.</p><p>b) The rating that <strong class="source-inline">userId</strong> <strong class="source-inline">607</strong> will give for <strong class="source-inline">movieId</strong> <strong class="source-inline">1210</strong>.</p></li>
				<li>Now that you have defined the <strong class="source-inline">test_df</strong> DataFrame to contain the entries you want to test the recommender system with, you can use it to exercise the trained recommender system. Run the following cell to get rating predictions from the recommender system for the entries in <strong class="source-inline">test_df</strong>:<p class="source-code">dl = learn.dls.test_dl(test_df)</p><p class="source-code">learn.get_preds(dl=dl)</p><p>The output of this cell shows the results of the recommender system, as shown in the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B16216_5_6.jpg" alt="Figure 5.6 – Results of the recommender system&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Results of the recommender system</p>
			<p>What do these results mean? The recommender system is predicting the following:</p>
			<p>a) <strong class="source-inline">userId</strong> <strong class="source-inline">388</strong> will give a rating of <strong class="source-inline">2.4156</strong> for <strong class="source-inline">movieId</strong> <strong class="source-inline">153</strong>.</p>
			<p>b) <strong class="source-inline">userId</strong> <strong class="source-inline">607</strong> will give a rating of <strong class="source-inline">3.6090</strong> for <strong class="source-inline">movieId</strong> <strong class="source-inline">1210</strong>.</p>
			<p>Congratulations! You have trained and exercised a recommender system in fastai using one of the curated datasets.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor139"/>How it works…</h2>
			<p>In this section, you worked through a recipe to train a very basic recommender system. Here's a<a id="_idIndexMarker352"/> summary of the key <a id="_idIndexMarker353"/>fastai objects you created in this recipe:</p>
			<ul>
				<li>A <strong class="source-inline">path</strong> object associated with the <strong class="source-inline">URLs</strong> object for the curated dataset:<p class="source-code">path = untar_data(URLs.ML_SAMPLE)</p></li>
				<li>A <strong class="source-inline">DataLoaders</strong> object:<p class="source-code">dls=CollabDataLoaders.from_df(df,bs= 64)</p></li>
				<li>A <strong class="source-inline">learner</strong> object:<p class="source-code">learn=collab_learner(dls, y_range= [ 0 , 5.0 ]</p></li>
			</ul>
			<p>These ingredients should look familiar to you; they are the same core ingredients that you used in the recipes in <a href="B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083"><em class="italic">Chapter 3</em></a>, <em class="italic">Training Models with Tabular Data</em>, and <a href="B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Models with Text Data</em>. One of the strengths of the high-level fastai API is that it uses the same building blocks to create and train deep learning models for a broad variety of datasets, including the datasets you have seen in previous chapters and in this section. </p>
			<p>There is one part of this section that is different from most of the recipes that you have seen so far – exercising the trained deep learning model on a set of test samples. Recall how you exercised the language model in <a href="B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Models with Text Data</em>. Here is the cell from the <strong class="source-inline">text_model_training.ipynb</strong> notebook that exercises the language model:</p>
			<p class="source-code">learn.predict("what comes next", n_words=20)</p>
			<p>With a simple call to the <strong class="source-inline">predict()</strong> function, you get the results from the trained language <a id="_idIndexMarker354"/>model, as shown in<a id="_idIndexMarker355"/> the following screenshot:</p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B16216_5_7.jpg" alt="Figure 5.7 – Prediction of a language model trained on a standalone text dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – Prediction of a language model trained on a standalone text dataset</p>
			<p>We will contrast the way that you exercised the language model in <a href="B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Models with Text Data</em>, with what you needed to do in this recipe to train a recommender system as follows:</p>
			<ol>
				<li value="1">Build a <strong class="source-inline">test_df</strong> DataFrame to contain the test samples, as shown in the following screenshot:<div id="_idContainer134" class="IMG---Figure"><img src="image/B16216_5_8.jpg" alt="Figure 5.8 – Contents of the test_df DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 5.8 – Contents of the test_df DataFrame</p></li>
				<li>Call <strong class="source-inline">test_dl</strong> using the <strong class="source-inline">test_df</strong> DataFrame that you just created as an argument. You can think of <strong class="source-inline">test_dl</strong> applying the same pipeline to the <strong class="source-inline">test_df</strong> DataFrame that was applied to the <strong class="source-inline">CollabDataLoaders</strong> object, <strong class="source-inline">dls</strong>:<p class="source-code">dl = learn.dls.test_dl(test_df)</p><p>The output of this call is the test data loader object, <strong class="source-inline">dl</strong>. With this call to <strong class="source-inline">test_dl</strong>, you have transformed the DataFrame into a format to which the trained model (the recommender system) can be applied.</p></li>
				<li>Call <strong class="source-inline">get_preds</strong> on the <strong class="source-inline">learn</strong> object to apply the trained recommender system to the data loader object, <strong class="source-inline">dl</strong>:<p class="source-code">learn.get_preds(dl=dl)</p></li>
			</ol>
			<p>Why did you need the extra steps to exercise the recommender system that you didn't need when you were exercising the language model? The answer is that the text phrase that is the input to the language model does not need to go through a sample-specific pipeline before it can be applied to the model. </p>
			<p>Behind the scenes, fastai uses the vocabulary associated with the <strong class="source-inline">TextDataLoaders</strong> object to convert the input string into tokens (by default, words) and then convert the tokens into <a id="_idIndexMarker356"/>numeric IDs. You don't<a id="_idIndexMarker357"/> have to explicitly invoke this pipeline – it gets invoked implicitly for you when you call <strong class="source-inline">predict()</strong> on the <strong class="source-inline">learn</strong> object for the language model.</p>
			<p>Unlike the text string that feeds into the language model, the input to the recommender system has structure, and that's why you need to define the input sample for the recommender system as a DataFrame. Once you have defined the DataFrame, you need to call <strong class="source-inline">test_dl</strong> to apply the transformation pipeline (that was implicitly defined when you defined the <strong class="source-inline">CollabDataLoaders</strong> object) on the input sample. The output of <strong class="source-inline">test_dl</strong> can then be used to get a prediction from the recommender system.</p>
			<p>Before moving on to the next recipe, it's worth taking a closer look at the model from this recipe. A deeply detailed description of the model is beyond the scope of this book, so we will just focus on some highlights here. </p>
			<p>As shown in the recipe, the model is defined as a <strong class="source-inline">collab_learner</strong> object (documentation here: <a href="https://docs.fast.ai/collab.html#collab_learner">https://docs.fast.ai/collab.html#collab_learner</a>). This object is a specialization of the fastai <strong class="source-inline">learner</strong> object which you first saw in section <em class="italic">Understanding the world in four applications: tables, text, recommender systems</em>, and images of <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a><em class="italic">, Getting Started with fastai</em>. </p>
			<p>The output of <strong class="source-inline">learn.model</strong> shows that this model is an <strong class="source-inline">EmbeddingDotBias</strong> model (documentation here:  <a href="https://docs.fast.ai/collab.html#EmbeddingDotBias">https://docs.fast.ai/collab.html#EmbeddingDotBias</a>).</p>
			<h1 id="_idParaDest-134"><a id="_idTextAnchor140"/>Training a recommender system on a large curated dataset</h1>
			<p>In the <em class="italic">Training a recommender system on a small curated dataset</em> section, we saw the basics <a id="_idIndexMarker358"/>of how to create a<a id="_idIndexMarker359"/> recommender system model. The resulting system left something to be desired because the dataset only included user IDs and movie IDs, so it wasn't possible to determine what movies were actually being rated by users and having their ratings predicted by the model. </p>
			<p>In this section, we are going to create a recommender system that addresses this gap in the previous recommender system because it is trained on a dataset that includes movie titles. Like the <strong class="source-inline">ML_SAMPLE</strong> dataset, the dataset we'll use in this section, <strong class="source-inline">ML_100k</strong>, is also derived from the MovieLens dataset, but it includes a much larger set of records and a much richer set of features. By creating a recommender system using this dataset, we will encounter additional features in fastai for ingesting and working with recommender system datasets and get a trained recommender system that is more interesting to use.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor141"/>Getting ready</h2>
			<p>Confirm that you can open the <strong class="source-inline">training_large_recommender_systems.ipynb</strong> notebook in the <strong class="source-inline">ch5</strong> directory of your repository.</p>
			<p>In the recipe in this section, you will use the <strong class="source-inline">tree</strong> command to examine the directory that contains the dataset. If you have not already installed the <strong class="source-inline">tree</strong> command in your Gradient instance, follow these steps to install it:</p>
			<ol>
				<li value="1">Run the following command in a Gradient terminal to prepare to install the <strong class="source-inline">tree</strong> command:<p class="source-code"><strong class="bold">apt update</strong></p></li>
				<li>Run the following command in a Gradient terminal to install the <strong class="source-inline">tree</strong> command:<p class="source-code"><strong class="bold">apt install tree</strong></p></li>
			</ol>
			<p>Now that you <a id="_idIndexMarker360"/>have the <strong class="source-inline">tree</strong> command <a id="_idIndexMarker361"/>available to use in your Gradient environment, you are all ready to work through the recipe in this section.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor142"/>How to do it…</h2>
			<p>In this section, you will be running through the <strong class="source-inline">training_large_recommender_systems.ipynb</strong> notebook. Once you have the notebook open in your fastai environment, complete the following steps:</p>
			<ol>
				<li value="1">Run the cells in the notebook up to the <strong class="source-inline">Ingest the dataset</strong> cell to import the required libraries and set up your notebook.</li>
				<li>Run the following cell to define the <strong class="source-inline">path</strong> object for this dataset:<p class="source-code">path = untar_data(URLs.ML_100k)</p></li>
				<li>Run the following cell to examine the directory structure of the dataset:<p class="source-code">path.ls()</p><p>The output shows the directory structure of the dataset, as shown in <em class="italic">Figure 5.9</em>. Note that the dataset has a more complex structure than <strong class="source-inline">ML_SAMPLE</strong>:</p><div id="_idContainer135" class="IMG---Figure"><img src="image/B16216_5_9.jpg" alt="Figure 5.9 – Output of path.ls() for the ML_100k dataset&#13;&#10;"/></div><p class="figure-caption">Figure 5.9 – Output of path.ls() for the ML_100k dataset</p></li>
				<li>You can get an idea of the files that make up the dataset by examining the path directly. You can do this by running the <strong class="source-inline">tree</strong> command. First, run the following command in a Gradient terminal to make the root directory of the dataset your current directory: <p class="source-code"><strong class="bold">cd /storage/data/ml-100k</strong></p></li>
				<li>Next, run <a id="_idIndexMarker362"/>the following <a id="_idIndexMarker363"/>command in the Gradient terminal to list the contents of the directory:<p class="source-code"><strong class="bold">tree</strong></p><p>The output of the <strong class="source-inline">tree</strong> command lists the contents of the directory:</p><p class="source-code">├── README</p><p class="source-code">├── allbut.pl</p><p class="source-code">├── mku.sh</p><p class="source-code">├── u.data</p><p class="source-code">├── u.genre</p><p class="source-code">├── u.info</p><p class="source-code">├── u.item</p><p class="source-code">├── u.occupation</p><p class="source-code">├── u.user</p><p class="source-code">├── u1.base</p><p class="source-code">├── u1.test</p><p class="source-code">├── u2.base</p><p class="source-code">├── u2.test</p><p class="source-code">├── u3.base</p><p class="source-code">├── u3.test</p><p class="source-code">├── u4.base</p><p class="source-code">├── u4.test</p><p class="source-code">├── u5.base</p><p class="source-code">├── u5.test</p><p class="source-code">├── ua.base</p><p class="source-code">├── ua.test</p><p class="source-code">├── ub.base</p><p class="source-code">└── ub.test</p><p>We <a id="_idIndexMarker364"/>want to focus on<a id="_idIndexMarker365"/> the contents of two files from this set: <strong class="source-inline">u.data</strong>, which lists the ratings provided by users for movie IDs, and <strong class="source-inline">u.item</strong>, which lists details about the movies, including their titles.</p></li>
				<li>Run the following cell to define the <strong class="source-inline">df_data</strong> DataFrame to contain the contents of the <strong class="source-inline">u.data</strong> file. This DataFrame will contain the ratings provided by users for movies:<p class="source-code">df_data = pd.read_csv(path/'u.data', delimiter = '\t',</p><p class="source-code">header = None,\</p><p class="source-code">names = ['userId','movieId','rating','timestamp'])</p><p>This DataFrame definition is the most complex one that we have seen so far. Let's go through the arguments:</p><p>a) <strong class="source-inline">path/'u.data'</strong>: Specifies the source for this DataFrame, the <strong class="source-inline">u.data</strong> file</p><p>b) <strong class="source-inline">delimiter = '\t'</strong>: Specifies that tabs are the delimiter that separates columns in this file</p><p>c) <strong class="source-inline">header = None</strong>: Specifies that the <strong class="source-inline">u.data</strong> file does not include column names in the first row</p><p>d) <strong class="source-inline">names = ['userId','movieId','rating','timestamp']</strong>: Assigns names to the columns of the DataFrame</p></li>
				<li>Run the following cell to define the <strong class="source-inline">df_item</strong> DataFrame to contain the contents of the <strong class="source-inline">u.item</strong> file. This DataFrame will contain details about movies, including their titles:<p class="source-code">df_item = pd.read_csv(path/'u.item', delimiter = '|',header = None,encoding = "ISO-8859-1")</p><p>Here<a id="_idIndexMarker366"/> are the arguments<a id="_idIndexMarker367"/> for the definition of <strong class="source-inline">df_item</strong>:</p><p>a) <strong class="source-inline">path/'u.item'</strong>: Specifies the source for this DataFrame, the <strong class="source-inline">u.item</strong> file</p><p>b) <strong class="source-inline">delimiter = '|'</strong>: Specifies that the pipe character, '<strong class="source-inline">|</strong>', is the delimiter that separates columns in this file</p><p>c) <strong class="source-inline">header = None</strong>: Specifies that this file does not include column names in the first row</p><p>d) <strong class="source-inline">encoding = "ISO-8859-1"</strong>: Specifies the encoding used to read the file. You will get an error if you do not specify this encoding</p></li>
				<li>Run the following cell to see a sample of the contents of the <strong class="source-inline">df_data</strong> DataFrame that you created:<p class="source-code">df_data.head()</p><p>The output of this command shows the first few rows of the DataFrame:</p><div id="_idContainer136" class="IMG---Figure"><img src="image/B16216_5_10.jpg" alt="Figure 5.10 – The first few rows of the df_data DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 5.10 – The first few rows of the df_data DataFrame</p></li>
				<li>Run the following cell to see the dimensions of the <strong class="source-inline">df_data</strong> DataFrame:<p class="source-code">df_data.shape</p><p>The <a id="_idIndexMarker368"/>output of this <a id="_idIndexMarker369"/>command shows the number of rows and columns in the <strong class="source-inline">df_data</strong> DataFrame:</p><div id="_idContainer137" class="IMG---Figure"><img src="image/B16216_5_11.jpg" alt="Figure 5.11 – Shape of the df_data DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 5.11 – Shape of the df_data DataFrame</p></li>
				<li>Run the following cell to see a sample of the contents of the <strong class="source-inline">df_item</strong> DataFrame that you created:<p class="source-code"><strong class="bold">df_item.head()</strong></p><p>The output of this command shows the first few rows of the DataFrame:</p><div id="_idContainer138" class="IMG---Figure"><img src="image/B16216_5_12.jpg" alt=""/></div><p class="figure-caption">Figure 5.12 – The first few rows of the df_item DataFrame</p></li>
				<li>Run<a id="_idIndexMarker370"/> the following<a id="_idIndexMarker371"/> cell to see the dimensions of the <strong class="source-inline">df_item</strong> DataFrame:<p class="source-code"><strong class="bold">df_item.shape</strong></p><p>The output of this command shows the number of rows and columns in the <strong class="source-inline">df_item</strong> DataFrame:</p><div id="_idContainer139" class="IMG---Figure"><img src="image/B16216_5_13.jpg" alt="Figure 5.13 – The shape of the df_item DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 5.13 – The shape of the df_item DataFrame</p></li>
				<li>Run the following cell to prepare the <strong class="source-inline">df_item</strong> DataFrame by removing most of the columns and adding column names for the remaining columns:<p class="source-code"><strong class="bold">df_item = df_item.iloc[:,0:2]</strong></p><p class="source-code"><strong class="bold">df_item.columns = ['movieId','movieName']</strong></p><p class="source-code"><strong class="bold">df_item.head()</strong></p><p>Following are the actions accomplished by each command in this cell:</p><p>a) <strong class="source-inline">df_item = df_item.iloc[:,0:2]</strong>: Removes all the columns in the DataFrame except for the first two columns that contain the movie ID and the movie title</p><p>b) <strong class="source-inline">df_item.columns = ['movieId','movieName']</strong>: Assigns names to the columns in the remaining columns in the DataFrame</p><p>c) <strong class="source-inline">df_item.head()</strong>: Displays the first few rows of the transformed DataFrame</p><p>This <a id="_idIndexMarker372"/>cell produces<a id="_idIndexMarker373"/> as output the first few rows of the transformed DataFrame, as shown in <em class="italic">Figure 5.14</em>:</p><div id="_idContainer140" class="IMG---Figure"><img src="image/B16216_5_14.jpg" alt="Figure 5.14 – The first few rows of the updated df_item DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 5.14 – The first few rows of the updated df_item DataFrame</p></li>
				<li>Run the following cell to combine the <strong class="source-inline">df_data</strong> and <strong class="source-inline">df_item</strong> DataFrames into a single new DataFrame, <strong class="source-inline">df</strong>, that combines the columns from the original DataFrames:<p class="source-code">df =\</p><p class="source-code">pd.merge(df_data,df_item,on=['movieId'],how='left')</p><p class="source-code">df.head()</p><p>Here are the arguments for <strong class="source-inline">merge</strong> to combine the DataFrames:</p><p>a) <strong class="source-inline">df_data</strong>: The DataFrame containing the user ID, movie ID, and rating.</p><p>b) <strong class="source-inline">df_item</strong>: The DataFrame containing the movie ID and movie title.</p><p>c) <strong class="source-inline">on=['movieId']</strong>: Specifies that the two DataFrames will be joined on the <strong class="source-inline">movieID</strong> column.</p><p>d) <strong class="source-inline">how='left'</strong>: Specifies that the rows from the <strong class="source-inline">df_data</strong> DataFrame will form the basis for the new DataFrame that is the result of the merge. That is, the new DataFrame will have the same number of rows as <strong class="source-inline">df_data</strong>, with each row including all the columns from <strong class="source-inline">df_data</strong> plus the <strong class="source-inline">movieName</strong> column from <strong class="source-inline">df_item</strong>.</p><p>This<a id="_idIndexMarker374"/> cell produces<a id="_idIndexMarker375"/> as output the first few rows of the new <strong class="source-inline">df</strong> DataFrame, as shown in <em class="italic">Figure 5.15</em>. Compared to <strong class="source-inline">df_data</strong>, you can see that <strong class="source-inline">df</strong> has one additional column, <strong class="source-inline">movieName</strong>:</p><div id="_idContainer141" class="IMG---Figure"><img src="image/B16216_5_15.jpg" alt="Figure 5.15 – The first few rows of df&#13;&#10;"/></div><p class="figure-caption">Figure 5.15 – The first few rows of df</p></li>
				<li>Run the following cell to see the dimensions of the <strong class="source-inline">df</strong> DataFrame:<p class="source-code"><strong class="bold">df.shape</strong></p><p>The output of this command shows the number of rows and columns in the <strong class="source-inline">df</strong> DataFrame. Note that <strong class="source-inline">df</strong> has the same number of rows as <strong class="source-inline">df_data</strong>:</p><div id="_idContainer142" class="IMG---Figure"><img src="image/B16216_5_16.jpg" alt="Figure 5.16 – The shape of the df DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 5.16 – The shape of the df DataFrame</p></li>
				<li>Run the following cell to see the number of unique values in each column of the <strong class="source-inline">df</strong> DataFrame:<p class="source-code"><strong class="bold">df.nunique()</strong></p><p>The output of this command lists all the columns of the DataFrame with the count of unique items in each column:</p><div id="_idContainer143" class="IMG---Figure"><img src="image/B16216_5_17.jpg" alt="Figure 5.17 – Count of unique values in each column of the df DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 5.17 – Count of unique values in each column of the df DataFrame</p></li>
				<li>Run<a id="_idIndexMarker376"/> the following<a id="_idIndexMarker377"/> cell to see whether there are any missing values in any of the columns of <strong class="source-inline">df</strong>. If there are any missing values, we will need to deal with them prior to training the recommender system:<p class="source-code"><strong class="bold">df.isnull().sum()</strong></p><p>The output of this command lists the number of missing values for each column. Since there are no missing values, we can proceed with the steps to train the recommender system:</p><div id="_idContainer144" class="IMG---Figure"><img src="image/B16216_5_18.jpg" alt="Figure 5.18 – Count of missing values in each column of the df DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 5.18 – Count of missing values in each column of the df DataFrame</p></li>
				<li>Run the following cell to define a <strong class="source-inline">CollabDataLoaders</strong> object for the recommender system:<p class="source-code">dls=/</p><p class="source-code">CollabDataLoaders.from_df(df,item_name='movieName',bs= 64)</p><p>Here <a id="_idIndexMarker378"/>are the arguments<a id="_idIndexMarker379"/> for the definition of the <strong class="source-inline">CollabDataLoaders</strong> object:</p><p>a) <strong class="source-inline">df</strong>: The DataFrame used to create the <strong class="source-inline">CollabDataLoaders</strong> object</p><p>b) <strong class="source-inline">item_name='movieName'</strong>: Specifies the column that contains the name of the item that is the subject of the recommender system, in this case, <strong class="source-inline">movieName</strong></p><p>c) <strong class="source-inline">bs= 64</strong>: Sets the batch size (the number of items on which the average loss is calculated) at 64</p></li>
				<li>Run the following cell to see a batch from the <strong class="source-inline">CollabDataLoaders</strong> object that you defined in the previous step:<p class="source-code">dls.show_batch()</p><p>The output displays the contents of the batch, as shown in <em class="italic">Figure 5.19</em>:</p><div id="_idContainer145" class="IMG---Figure"><img src="image/B16216_5_19.jpg" alt="Figure 5.19 – Output of show_batch&#13;&#10;"/></div><p class="figure-caption">Figure 5.19 – Output of show_batch</p></li>
				<li>Run<a id="_idIndexMarker380"/> the following<a id="_idIndexMarker381"/> cell to define the model for the recommender system by defining a <strong class="source-inline">collab_learner</strong> object:<p class="source-code">learn=collab_learner(dls,y_range= [ 1 , 5 ] )</p><p>Here are the arguments for the definition of the <strong class="source-inline">collab_learner</strong> object:</p><p>a) <strong class="source-inline">dls</strong>: The <strong class="source-inline">CollabDataLoaders</strong> object that you defined for the dataset.</p><p>b) <strong class="source-inline">y_range= [ 1 , 5 ]</strong>: Specifies the range of the values being predicted by the recommender system. In our case, this is the range of values for movie ratings in the <strong class="source-inline">rating</strong> column of the dataset.</p></li>
				<li>Run the following cell to train the model for the recommender system:<p class="source-code">learn.fit_one_cycle( 5 )</p><p>Here is the argument for the model definition:</p><p>a) <strong class="source-inline">5</strong>: The number of epochs in the training run for the model</p><p>The output displays the training and validation loss for each epoch, as shown in <em class="italic">Figure 5.20</em>. You get an indication of the training improving as the validation loss decreases through the epochs:</p><div id="_idContainer146" class="IMG---Figure"><img src="image/B16216_5_20.jpg" alt="Figure 5.20 – Output of training the collaborative filtering model&#13;&#10;"/></div><p class="figure-caption">Figure 5.20 – Output of training the collaborative filtering model</p></li>
				<li>Before<a id="_idIndexMarker382"/> we exercise <a id="_idIndexMarker383"/>the recommender system, let's get an idea of what ratings users give to a movie that has a reputation for not being very good. Run the following cell to see the subset of the <strong class="source-inline">df</strong> DataFrame for the movie <strong class="source-inline">Showgirls</strong>:<p class="source-code">df_one_movie = df[df.movieName=='Showgirls (1995)']</p><p class="source-code">df_one_movie.head()</p><p>The output shows the first few rows in the <strong class="source-inline">df</strong> DataFrame that contain ratings for <em class="italic">Showgirls</em>, as shown in <em class="italic">Figure 5.21</em>:</p><div id="_idContainer147" class="IMG---Figure"><img src="image/B16216_5_21.jpg" alt="Figure 5.21 – Subset of the df DataFrame for a single movie&#13;&#10;"/></div><p class="figure-caption">Figure 5.21 – Subset of the df DataFrame for a single movie</p></li>
				<li>From the output of the previous cell, it seems like there is a range of ratings for this movie. Let's see what the average rating is for this movie for all users. Run the following cell to see the average rating for this movie:<p class="source-code">df_one_movie['rating'].mean()</p><p>The output is shown in <em class="italic">Figure 5.22</em>. As we suspected, the average rating for this movie is indeed low:</p><div id="_idContainer148" class="IMG---Figure"><img src="image/B16216_5_22.jpg" alt="Figure 5.22 – Average rating for Showgirls&#13;&#10;"/></div><p class="figure-caption">Figure 5.22 – Average rating for Showgirls</p></li>
				<li>To exercise<a id="_idIndexMarker384"/> the recommender <a id="_idIndexMarker385"/>system, we want to get rating predictions for one user for a set of movies. Run the following cell to define a test DataFrame to exercise the trained recommender system:<p class="source-code">scoring_columns = ['userId','movieId','movieName']</p><p class="source-code">test_df = pd.DataFrame(columns=scoring_columns)</p><p class="source-code">test_df.at[0,'userId'] = 607</p><p class="source-code">test_df.at[0,'movieId'] = 242</p><p class="source-code">test_df.at[0,'movieName'] = 'Kolya (1996)'</p><p class="source-code">test_df.at[1,'userId'] = 607</p><p class="source-code">test_df.at[1,'movieId'] = 302</p><p class="source-code">test_df.at[1,'movieName'] = 'L.A. Confidential (1997)'</p><p class="source-code">test_df.at[2,'userId'] = 607</p><p class="source-code">test_df.at[2,'movieId'] = 375</p><p class="source-code">test_df.at[2,'movieName'] = 'Showgirls (1995)'</p><p class="source-code">test_df.head()</p><p>Here are the key elements of this cell:</p><p>a) <strong class="source-inline">scoring_columns</strong>: A list of the column names of the DataFrame</p><p>b) <strong class="source-inline">test_df</strong>: The DataFrame for containing the test entries with column names specified in the <strong class="source-inline">scoring_columns</strong> list</p><p>Note <a id="_idIndexMarker386"/>that the value <a id="_idIndexMarker387"/>for <strong class="source-inline">userId</strong> is the same for every row in this DataFrame. When we apply this DataFrame to our recommender system, we will be getting predicted ratings for one user for three movies.</p><p>The output of <strong class="source-inline">test_df.head()</strong> shows the contents of the completed DataFrame, as shown in <em class="italic">Figure 5.23</em>. For each row in this DataFrame, we will get the recommender system to predict the rating that <strong class="source-inline">userID</strong> <strong class="source-inline">607</strong> would give for the movie in that row:</p><div id="_idContainer149" class="IMG---Figure"><img src="image/B16216_5_23.jpg" alt="Figure 5.23 – Contents of test_df DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 5.23 – Contents of test_df DataFrame</p></li>
				<li>Run the following cell to get rating predictions from the recommender system for the entries in <strong class="source-inline">test_df</strong>:<p class="source-code">dl = learn.dls.test_dl(test_df)</p><p class="source-code">learn.get_preds(dl=dl)</p><p>The output of this cell shows the results of the recommender system, as shown in the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer150" class="IMG---Figure">
					<img src="image/B16216_5_24.jpg" alt="Figure 5.24 – Results of the recommender system&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.24 – Results of the recommender system</p>
			<p>What do these results mean? The recommender system is predicting that <strong class="source-inline">userID</strong> <strong class="source-inline">607</strong> will give the following ratings:</p>
			<p>a) <strong class="source-inline">4.2557</strong> for <strong class="source-inline">Kolya</strong></p>
			<p>b) <strong class="source-inline">4.3637</strong> for <strong class="source-inline">L.A. Confidential</strong></p>
			<p>c) <strong class="source-inline">2.2996</strong> for <strong class="source-inline">Showgirls</strong></p>
			<p>This is not<a id="_idIndexMarker388"/> an exhaustive test of the <a id="_idIndexMarker389"/>recommender system, but the ratings seem plausible because a high rating is predicted for <strong class="source-inline">L.A. Confidential</strong>, a well-regarded movie, and a low rating is predicted for <strong class="source-inline">Showgirls</strong>, a movie that received very poor reviews.</p>
			<p>Congratulations! You have trained a recommender system in fastai using a large curated dataset that includes a broad variety of rating combinations.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor143"/>How it works…</h2>
			<p>In this section, you worked through a recipe to train a recommender system on a large dataset. It's worth summarizing how this recipe differed from the recipe in <em class="italic">Training a recommender system on a small curated dataset</em> section. Here are the key differences:</p>
			<ul>
				<li><strong class="bold">Structure of the datasets</strong>: The small <a id="_idIndexMarker390"/>dataset consisted of a single file: <strong class="source-inline">ratings.csv</strong>. The large dataset is made up of over 20 files, although we only use two for the recipe in this section: <strong class="source-inline">u.data</strong> (which contains user IDs, movie IDs, and the user's ratings for the movies) and <strong class="source-inline">u.item</strong> (which contains<a id="_idIndexMarker391"/> additional information about movies, including their titles).</li>
				<li><strong class="bold">Size of the datasets</strong>: The small <a id="_idIndexMarker392"/>dataset has a little over 6,000 records. The large dataset has 100 k records.</li>
				<li><strong class="bold">Test DataFrame</strong>: For the <a id="_idIndexMarker393"/>recommender system trained on a small dataset, the test DataFrame contained the user IDs and movie IDs for which we wanted to get predicted ratings. For the recommender system trained on a large dataset, the test DataFrame contained user IDs, movie IDs, and movie titles for which we wanted to get predicted ratings.</li>
			</ul>
			<p>The facilities<a id="_idIndexMarker394"/> of fastai make it easy<a id="_idIndexMarker395"/> to create recommender systems on both the small and large curated datasets. In the next section, we'll explore how to use fastai to create a recommender system on a standalone dataset.</p>
			<h1 id="_idParaDest-138"><a id="_idTextAnchor144"/>Training a recommender system on a standalone dataset</h1>
			<p>In<a id="_idIndexMarker396"/> the <em class="italic">Training a recommender system on a small curated dataset</em> and <em class="italic">Training a recommender system on a large curated dataset</em> recipes, we used two curated datasets adapted from the MovieLens dataset that contains user IDs, movie IDs, and user ratings for these movies. Using these two curated datasets, we created recommender systems that predicted what rating a user would give to a particular movie. </p>
			<p>In this section, we will explore a dataset that is not a part of fastai's set of curated datasets. The dataset we will use in this section is the Amazon product dataset (<a href="https://www.kaggle.com/saurav9786/amazon-product-reviews">https://www.kaggle.com/saurav9786/amazon-product-reviews</a>). This dataset contains user ratings for a large range of products available on Amazon. </p>
			<p>In this section, we will be working with a subset of the dataset, the subset related to electronic goods. In the recipe in this section, we will ingest this standalone dataset and then use it to train a recommender system that can predict a user's rating for an item.</p>
			<h2 id="_idParaDest-139"><a id="_idTextAnchor145"/>Getting ready</h2>
			<p>Confirm that you can open the <strong class="source-inline">training_recommender_systems_on_standalone_dataset.ipynb</strong> notebook in the <strong class="source-inline">ch5</strong> directory of your repository.</p>
			<p>Ensure that you have uploaded the file for the Amazon product dataset to your Gradient environment by following these steps:</p>
			<ol>
				<li value="1">Download the <strong class="source-inline">archive.zip</strong> file from <a href="https://www.kaggle.com/saurav9786/amazon-product-reviews">https://www.kaggle.com/saurav9786/amazon-product-reviews</a>.</li>
				<li>Unzip <a id="_idIndexMarker397"/>the downloaded <strong class="source-inline">archive.zip</strong> file <a id="_idIndexMarker398"/>to extract <strong class="source-inline">ratings_Electronics (1).csv</strong>. Rename this file to <strong class="source-inline">ratings_Electronics.csv</strong>.</li>
				<li>From the terminal in your Gradient environment, make <strong class="source-inline">/storage/archive</strong> your current directory:<p class="source-code"><strong class="bold">cd /storage/archive</strong></p></li>
				<li>Create the <strong class="source-inline">/storage/archive/amazon_reviews</strong> directory:<p class="source-code"><strong class="bold">mkdir amazon_reviews</strong></p></li>
				<li>Make <strong class="source-inline">/storage/archive/amazon_reviews</strong> your current directory:<p class="source-code"><strong class="bold">cd /storage/archive/amazon_reviews</strong></p></li>
				<li>Upload the files you extracted in <em class="italic">step 2</em> (<strong class="source-inline">ratings_Electronics.csv</strong>) to <strong class="source-inline">/storage/archive/amazon_reviews</strong>. You can use the upload button in JupyterLab in Gradient to do the upload by following these steps:<p>a) From the terminal in your Gradient environment, make <strong class="source-inline">/notebooks</strong> your current directory:</p><p class="source-code"><strong class="bold">cd /notebooks</strong></p><p>b) If you have not already created a <strong class="source-inline">notebooks/temp</strong> directory, make a new <strong class="source-inline">/notebooks/temp</strong> directory:</p><p class="source-code"><strong class="bold">mkdir temp</strong></p><p>c) In the JupyterLab file browser, make <strong class="source-inline">temp</strong> your current folder, select the upload <a id="_idIndexMarker399"/>button (see <em class="italic">Figure 5.25</em>), and <a id="_idIndexMarker400"/>then select the <strong class="source-inline">ratings_Electronics.csv</strong> file from your local system folder where you extracted it in <em class="italic">step 2</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer151" class="IMG---Figure">
					<img src="image/B16216_5_25.jpg" alt="Figure 5.25 – The upload button in JupyterLab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.25 – The upload button in JupyterLab</p>
			<p>d) From the terminal in your Gradient environment, copy the <strong class="source-inline">ratings_Electronics.csv</strong> file into the <strong class="source-inline">/storage/archive/amazon_reviews</strong> directory:</p>
			<p class="source-code"><strong class="bold">cp /notebooks/temp/ratings_Electronics.csv /storage/archive/amazon_reviews/ratings_Electronics.csv</strong></p>
			<p>I gratefully acknowledge the opportunity to use the Amazon product dataset to illustrate the recommender system capabilities of fastai on non-curated datasets.</p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">J. McAuley, C. Targett, J. Shi, A. van den Hengel (2015). <em class="italic">Image-based recommendations on styles and substitutes</em> (<a href="http://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf">http://cseweb.ucsd.edu/~jmcauley/pdfs/sigir15.pdf</a>). Special Interest Group on Information Retrieval (SIGIR 2015) </p>
			<p>Now that you have copied the dataset file into your Gradient environment, you are ready to go through the recipe.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor146"/>How to do it…</h2>
			<p>In this section, you will be running through the <strong class="source-inline">training_recommender_systems_on_standalone_dataset.ipynb</strong> notebook. Once you have the notebook open in your fastai environment, complete the following steps:</p>
			<ol>
				<li value="1">Run the cells in the notebook up to the <strong class="source-inline">Ingest the dataset</strong> cell to import the required libraries and set up your notebook.</li>
				<li>Run the following cell to define the path object for this dataset:<p class="source-code">path = URLs.path('amazon_reviews')</p><p>The argument <strong class="source-inline">'amazon_reviews'</strong> indicates that this path object is being defined to point to the <strong class="source-inline">/storage/archive/amazon_reviews</strong> directory that you created in the <em class="italic">Getting ready</em> section.</p></li>
				<li>Run <a id="_idIndexMarker401"/>the following cell to<a id="_idIndexMarker402"/> examine the directory structure of the dataset:<p class="source-code">path.ls()</p><p>The output shows the directory structure of the dataset, as shown in <em class="italic">Figure 5.26</em>: </p><div id="_idContainer152" class="IMG---Figure"><img src="image/B16216_5_26.jpg" alt="Figure 5.26 – The output of path.ls() for the Amazon product dataset&#13;&#10;"/></div><p class="figure-caption">Figure 5.26 – The output of path.ls() for the Amazon product dataset</p></li>
				<li>Run the following cell to define the <strong class="source-inline">df</strong> DataFrame to contain the contents of the <strong class="source-inline">ratings_Electronics.csv</strong> file:<p class="source-code">df = pd.read_csv(path/'ratings_Electronics.csv',header = None)</p><p>Here are the arguments for the definition of the <strong class="source-inline">df</strong> DataFrame:</p><p>a) <strong class="source-inline">path/'ratings_Electronics.csv'</strong>: Specifies the source for this DataFrame, the <strong class="source-inline">ratings_Electronics.csv</strong> file</p><p>b) <strong class="source-inline">header = None</strong>: Specifies that this file does not include column names in the first row</p></li>
				<li>Run the following cell to define names for the columns of the <strong class="source-inline">df</strong> DataFrame. These column names come from the description of the dataset at <a href="https://www.kaggle.com/saurav9786/amazon-product-reviews">https://www.kaggle.com/saurav9786/amazon-product-reviews</a>:<p class="source-code">df.columns = ['userID','productID','rating','timestamp']</p></li>
				<li>Run the following cell to examine the <strong class="source-inline">df</strong> DataFrame:<p class="source-code">df.head()</p><p>The <a id="_idIndexMarker403"/>output, as shown <a id="_idIndexMarker404"/>in <em class="italic">Figure 5.26</em>, lists records from the dataset. Each record represents a user's rating for a product. The <strong class="source-inline">userID</strong> column has the ID for the user. The <strong class="source-inline">productID</strong> column has the ID for the product. The <strong class="source-inline">rating</strong> column is the rating given by the user for the product:</p><div id="_idContainer153" class="IMG---Figure"><img src="image/B16216_5_27.jpg" alt="Figure 5.27 – Output of df.head()&#13;&#10;"/></div><p class="figure-caption">Figure 5.27 – Output of df.head()</p></li>
				<li>Run the following cell to get the dimensions of the <strong class="source-inline">df</strong> DataFrame: <p class="source-code">df.shape</p><p>The output, as shown in <em class="italic">Figure 5.27</em>, shows that the DataFrame has over 7 million rows:</p><div id="_idContainer154" class="IMG---Figure"><img src="image/B16216_5_28.jpg" alt="Figure 5.28 – Shape of DataFrame df&#13;&#10;"/></div><p class="figure-caption">Figure 5.28 – Shape of DataFrame df</p></li>
				<li>Run the following cell to define the <strong class="source-inline">CollabDataLoaders</strong> object, <strong class="source-inline">dls</strong>:<p class="source-code">dls=CollabDataLoaders.from_df(df,bs= 64)</p><p>Here are the arguments to the definition of the <strong class="source-inline">CollabDataLoaders</strong> object, <strong class="source-inline">dls</strong>:</p><p>a) <strong class="source-inline">df</strong>: Specifies that the DataFrame you created for the dataset, <strong class="source-inline">df</strong>, is used to create the <strong class="source-inline">CollabDataLoaders</strong> object</p><p>b) <strong class="source-inline">bs = 64</strong>: Specifies that the batch size is <strong class="source-inline">64</strong></p></li>
				<li>Run the <a id="_idIndexMarker405"/>following cell to see <a id="_idIndexMarker406"/>a batch from the <strong class="source-inline">CollabDataLoaders</strong> object that you defined in the previous step:<p class="source-code">dls.show_batch()</p><p>The output displays the contents of the batch, as shown in <em class="italic">Figure 5.28</em>:</p><div id="_idContainer155" class="IMG---Figure"><img src="image/B16216_5_29.jpg" alt="Figure 5.29 – Output of show_batch()&#13;&#10;"/></div><p class="figure-caption">Figure 5.29 – Output of show_batch()</p></li>
				<li>Run the following cell to define the <strong class="source-inline">collab_learner</strong> object for the recommender system model:<p class="source-code">learn=collab_learner(dls,y_range= [ 0 , 5.0 ] )</p><p>Here are the arguments for the definition of the <strong class="source-inline">collab_learner</strong> object:</p><p>a) <strong class="source-inline">dls</strong>: The <strong class="source-inline">CollabDataLoaders</strong> object that you defined in a previous step</p><p>b) <strong class="source-inline">y_range</strong>: The range of values in the <strong class="source-inline">rating</strong> column</p></li>
				<li>Run the following cell to train the recommender system model:<p class="source-code">learn.fit_one_cycle( 1 )</p><p>Here<a id="_idIndexMarker407"/> is the argument <a id="_idIndexMarker408"/>for the model definition:</p><p>a) <strong class="source-inline">1</strong>: The number of epochs in the training run for the model.</p><p class="callout-heading">Note</p><p class="callout">The dataset you are using in this recipe is big. Recall that when you ran the cell to get the dimensions of the <strong class="source-inline">df</strong> DataFrame, you found that this DataFrame has over 7 million rows. What this means is that training the model will take some time. For example, it took me over 3 hours to train a recommender system model on this dataset in a Gradient environment. </p><p>The output shows the training and validation loss, as shown in the following screenshot:</p><div id="_idContainer156" class="IMG---Figure"><img src="image/B16216_5_30.jpg" alt="Figure 5.30 – Output of training the collaborative filtering model&#13;&#10;"/></div><p class="figure-caption">Figure 5.30 – Output of training the collaborative filtering model</p></li>
				<li>Run the following cell to define the <strong class="source-inline">test_df</strong> DataFrame, which includes a couple of test entries that you can use to exercise the trained recommender system:<p class="source-code">scoring_columns = ['userID','productID']</p><p class="source-code">test_df = pd.DataFrame(columns=scoring_columns)</p><p class="source-code">test_df.at[0,'userID'] = 'A<a id="_idTextAnchor147"/>2NYK9KWFMJV4Y'</p><p class="source-code">test_df.at[0,'productID'] = 'B008ABOJKS'</p><p class="source-code">test_df.at[1,'userID'] = 'A29ZTEO6EKSRDV'</p><p class="source-code">test_df.at[1,'productID'] = 'B006202R44'</p><p class="source-code">test_df.head()</p><p>Here<a id="_idIndexMarker409"/> are the key elements<a id="_idIndexMarker410"/> of this cell:</p><p>a) <strong class="source-inline">scoring_columns</strong>: A list of the column names of the DataFrame. Note that these column names are the same column names that you saw in the output of <strong class="source-inline">show_batch()</strong> in <em class="italic">Figure 5.29</em>.</p><p>b) <strong class="source-inline">test_df</strong>: The DataFrame for containing the test entries with column names specified in the <strong class="source-inline">scoring_columns</strong> list.</p><p>The output of <strong class="source-inline">test_df.head()</strong> shows the contents of the completed <strong class="source-inline">test_df</strong> DataFrame, as shown in the following screenshot:</p><div id="_idContainer157" class="IMG---Figure"><img src="image/B16216_5_31.jpg" alt="Figure 5.31 – Contents of the test_df DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 5.31 – Contents of the test_df DataFrame</p></li>
				<li>Now that you have defined the <strong class="source-inline">test_df</strong> DataFrame, you can use it to exercise the trained recommender system. Run the following cell to get rating predictions from the recommender system for the entries in <strong class="source-inline">test_df</strong>:<p class="source-code">dl = learn.dls.test_dl(test_df)</p><p class="source-code">learn.get_preds(dl=dl)</p><p>The <a id="_idIndexMarker411"/>output of this cell shows <a id="_idIndexMarker412"/>the results of the recommender system, as shown in the following screenshot:</p></li>
			</ol>
			<div>
				<div id="_idContainer158" class="IMG---Figure">
					<img src="image/B16216_5_32.jpg" alt="Figure 5.32 – Results of the recommender system&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.32 – Results of the recommender system</p>
			<p>What do these results mean? </p>
			<p>a) The first entry in the output, <strong class="source-inline">4.4364</strong>, is the recommender system's prediction for the first entry in <strong class="source-inline">test_df</strong>. The recommender system is predicting that the user with a user ID of <strong class="source-inline">A2NYK9KWFMJV4Y</strong> will give a rating of <strong class="source-inline">4.4364</strong> for the product with a product ID of <strong class="source-inline">B008ABOJKS</strong>.</p>
			<p>b) The second entry in the output, <strong class="source-inline">2.5531</strong>, is the recommender system's prediction for the second entry in <strong class="source-inline">test_df</strong>. The recommender system is predicting that the user with a user ID of <strong class="source-inline">A29ZTEO6EKSRDV</strong> will give a rating of <strong class="source-inline">2.5531</strong> for the product with a product ID of <strong class="source-inline">B006202R44</strong>.</p>
			<p>Congratulations! You have trained a recommender system in fastai using a standalone dataset.</p>
			<h2 id="_idParaDest-141"><a id="_idTextAnchor148"/>How it works…</h2>
			<p>In this section, you worked through a recipe for training a recommender system on a standalone dataset. Like the standalone datasets that you encountered in <a href="B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083"><em class="italic">Chapter 3</em></a>, <em class="italic">Training Models with Tabular Data</em>, and <a href="B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Models with Text Data</em>, the standalone dataset that you used in this section required you to take some additional steps compared to the fastai curated datasets that you used in the <em class="italic">Training a recommender system on a small curated dataset</em> and <em class="italic">Training a recommender system on a large curated dataset</em> recipes. </p>
			<p>Here is a summary of the additional steps required for a standalone dataset:</p>
			<ol>
				<li value="1"><strong class="bold">Find a dataset</strong>. This may sound simplistic, but once you move beyond the world of the fastai curated datasets and try to learn more by using standalone datasets, it can be a challenge to find the right dataset. The Kaggle site, <a href="https://www.kaggle.com/">https://www.kaggle.com/</a>, is a great place to start. You want a dataset that's big enough to give a fighting chance when it comes to training a deep learning model. <p>My experience has been that if I'm not using transfer learning (that is, starting off with a<a id="_idIndexMarker413"/> pre-existing <a id="_idIndexMarker414"/>model that has been trained on a general dataset that applies to the use case I want to train my deep learning model on), then I need to have a dataset that has at least several tens of thousands of items in it. . </p><p>You want a dataset that's not too big. As you saw in the recipe in this section and in the language models you trained in <a href="B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109"><em class="italic">Chapter 4</em></a>, <em class="italic">Training Models with Text Data</em>, training a model with a big dataset can take hours. If you're using a Gradient instance with a cost (as opposed to Colab or a free Gradient instance), you can end up spending several dollars for a single training run. Costs can add up if you need to do multiple training runs.</p></li>
				<li><strong class="bold">Bring the dataset into your fastai environment</strong>. As described in the <em class="italic">Getting ready</em> section of this recipe, after you have downloaded the dataset to your local system, you have to create a directory in your fastai environment to hold the dataset files and then upload the files to that directory.</li>
			</ol>
			<p>One of the advantages of fastai is the large number of curated datasets. However, for dataset types such as recommender systems, where fastai only offers one or two curated datasets, it is advantageous for you to feel comfortable with finding standalone datasets with<a id="_idIndexMarker415"/> different characteristics, so<a id="_idIndexMarker416"/> you can perform a wider variety of experiments. By following the recommendations in this section, you will be able to expand your world of datasets beyond the fastai curated datasets when you're ready to learn more.</p>
			<h1 id="_idParaDest-142"><a id="_idTextAnchor149"/>Test your knowledge</h1>
			<p>Now that you<a id="_idIndexMarker417"/> have worked through some extended examples of training fastai recommender systems, you can try some variations to exercise what you've learned. </p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor150"/>Getting ready</h2>
			<p>Ensure that you have followed the <em class="italic">Training a recommender system on a standalone dataset</em> recipe. In this section, you will be adapting the notebook worked through in that recipe to create a recommender system for a new standalone dataset.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor151"/>How to do it…</h2>
			<p>You can follow the steps in this section to try some variations on the recommender system that you trained with the Amazon product dataset in the <em class="italic">Training a recommender system on a standalone dataset</em> recipe:</p>
			<ol>
				<li value="1">Make a copy of the <strong class="source-inline">training_recommender_systems_on_standalone_dataset.ipynb</strong> notebook that you worked through in the <em class="italic">Training a recommender system on a standalone dataset</em> recipe. Give your new copy of the notebook the following name: <strong class="source-inline">training_recommender_systems_on_new_standalone_dataset.ipynb</strong>.</li>
				<li>Review the description of the whole Amazon product dataset at <a href="http://jmcauley.ucsd.edu/data/amazon/">http://jmcauley.ucsd.edu/data/amazon/</a>. From the list of <strong class="screen-inline">small subsets for</strong><em class="italic"> </em><strong class="screen-inline">experimentation</strong>, select a category other than <strong class="screen-inline">Electronics</strong>. You have already trained a <a id="_idIndexMarker418"/>recommender system with the <strong class="screen-inline">Electronics</strong> dataset in the <em class="italic">Training a recommender system on a standalone dataset</em> recipe, so you will want to pick another category. <p>For example, you could pick <strong class="screen-inline">Office Products</strong> or <strong class="screen-inline">Automotive</strong>. I suggest avoiding the <strong class="screen-inline">Books</strong> category because its file is three times larger than the <strong class="screen-inline">Electronics</strong> dataset, so you could be facing a whole day training a recommender system on the <em class="italic">Books</em> dataset.</p></li>
				<li>Adapt the steps in the <em class="italic">Getting ready</em> section of the <em class="italic">Training a recommender system on a standalone dataset</em> recipe to get the <em class="italic">ratings only</em> dataset for the category you chose in <em class="italic">Step 2</em> in your fastai environment.</li>
				<li>Update your <strong class="source-inline">training_recommender_systems_on_new_standalone_dataset.ipynb</strong> notebook to ingest the dataset you brought into your fastai environment in <em class="italic">Step 3</em>.</li>
				<li>Update your notebook to load the dataset into a DataFrame and then use the techniques you have seen in this chapter (including <strong class="source-inline">head()</strong> and <strong class="source-inline">shape</strong>) to examine the structure of the dataset.</li>
				<li>Using what<a id="_idIndexMarker419"/> you learned in <em class="italic">Step 5</em>, update the rest of your <strong class="source-inline">training_recommender_systems_on_new_standalone_dataset.ipynb</strong> notebook to create, train, and test a recommender system for the dataset you selected in <em class="italic">Step 2</em>.</li>
			</ol>
			<p>Congratulations! You have completed a review of training recommender systems using fastai.</p>
		</div>
	</body></html>