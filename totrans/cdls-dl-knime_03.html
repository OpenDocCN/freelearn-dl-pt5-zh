<html><head></head><body>
		<div id="_idContainer046">
			<h1 id="_idParaDest-30"><em class="italic"><a id="_idTextAnchor051"/>Chapter 2: </em>Data Access and Preprocessing with KNIME Analytics Platform</h1>
			<p>Before deep-diving into neural networks and deep learning architectures, it might be a good idea to get familiar with KNIME Analytics Platform and its most important functions.</p>
			<p>In this chapter, we will cover a few basic operations within KNIME Analytics Platform. Since every project needs data, we will first go through the basics of how to access data: from files or databases. In KNIME Analytics Platform, you can also access data from REST services, cloud repositories, specific industry formats, and more. We will leave the exploration of these other options to you.</p>
			<p>Data comes in a number of shapes and types. In the <em class="italic">Data Types and Conversions</em> section, we will briefly investigate the tabular nature of the KNIME data representation, the basic types of data in a data table, and how to convert from one type to another.</p>
			<p>At this point, after we have imported the data into a KNIME workflow, we will show some basic data operations, such as filtering, joining, concatenating, aggregating, and other commonly used data transformations.</p>
			<p>The parameterization of a static workflow will conclude this very quick overview of the basic operations you can perform with KNIME Analytics Platform on your data.</p>
			<p>This chapter will take you through the following topics:</p>
			<ul>
				<li>Accessing Data</li>
				<li>Data Types and Conversions</li>
				<li>Transforming Data</li>
				<li>Parameterizing the Workflow</li>
			</ul>
			<p>Let's start with how to import data into a KNIME workflow.</p>
			<h1 id="_idParaDest-31"><a id="_idTextAnchor052"/>Accessing Data</h1>
			<p>Before<a id="_idIndexMarker080"/> starting with examples of how to access and import data into a KNIME workflow, let's create the workflow:</p>
			<ol>
				<li>Click on the <strong class="bold">File</strong> item in the top menu or right-click on a folder, such as <strong class="bold">LOCAL</strong>, for example, in <strong class="bold">KNIME Explorer</strong>.</li>
				<li>Then, select the <strong class="bold">New KNIME Workflow</strong> option.</li>
				<li>Give it a name – for example, <strong class="source-inline">Ch2_Workflow_Examples</strong> – and a destination.<p>An empty canvas will open in the central part of the KNIME workbench: the workflow editor.</p></li>
			</ol>
			<p>For this chapter, we will use toy data already available at installation. A set of workflows is installed together with the core KNIME Analytics Platform. You can find them in the <strong class="source-inline">Example Workflows</strong> folder (<em class="italic">Figure 2.1</em>) in the <strong class="bold">KNIME Explorer</strong> panel. Its <strong class="source-inline">TheData</strong> sub-folder contains some free toy datasets:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B16391_02_001.jpg" alt="Figure 2.1 – Structure of the Example Workflows folder in the KNIME Explorer panel"/>
				</div>
			</div>
			<p class="figure-caption"><a id="_idTextAnchor053"/>Figure 2.1 – Structure of the Example Workflows folder in the KNIME Explorer panel</p>
			<p>We <a id="_idIndexMarker081"/>will mainly use the datasets in the <strong class="source-inline">Misc</strong> sub-folder.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">In order to upload data to the <strong class="bold">KNIME Explorer</strong> panel, just copy it into a folder within the current workspace folder on your machine. The folder and its contents will then appear in <strong class="bold">KNIME Explorer</strong> in the list of workflows, servers, KNIME Hub spaces, and data available.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor054"/>Reading Data from Files</h2>
			<p>Let's <a id="_idIndexMarker082"/>start with <a id="_idIndexMarker083"/>a classic: reading a <strong class="bold">CSV-formatted</strong> text file. To<a id="_idIndexMarker084"/> read a CSV-formatted text file, you need <a id="_idIndexMarker085"/>the <strong class="bold">File Reader</strong> node or its simplified version, the <strong class="bold">CSV Reader</strong> node. Let's focus on the File<a id="_idIndexMarker086"/> Reader node, which, though more complex, is also more powerful and flexible. There are now two ways to create and configure a File Reader node.</p>
			<p>In the long way, you search for the File Reader node in the Node Repository; drag and drop it into the <a id="_idIndexMarker087"/>workflow editor; double-click it to open its configuration window, or alternatively, right-click it and then select <strong class="bold">Configure</strong>; and set the required settings, which at the very least require the file path via the <strong class="bold">Browse</strong> button (<em class="italic">Figure 2.2</em>).</p>
			<p>In the short way, you just drag and drop your CSV-formatted file from the File Explorer panel into the workflow editor. This way automatically creates a File Reader node, fills up most of its configuration settings, including the file path, and keeps the configuration window open for further adjustments (<em class="italic">Figure 2.2</em>).</p>
			<p>Under the file path, there are some basic settings: whether to read the first row as column headers and/or the first column as <strong class="source-inline">RowID</strong>, the column delimiter for general text files, and how to deal with spaces and tabs.</p>
			<p>Notice two <a id="_idIndexMarker088"/>more things in this configuration window of the File <a id="_idIndexMarker089"/>Reader node: the data preview and the <strong class="bold">Advanced</strong> button. The data preview in the lower part of the window allows you to see whether the dataset is being read properly. The <strong class="bold">Advanced</strong> button takes you to more advanced settings, such as enabling shorter lines, character encoding, quotes, and other similar preferences.</p>
			<p>When using the short way to create and configure a <strong class="bold">File Reader</strong> node, in the preview panel in the node configuration window (<em class="italic">Figure 2.2</em>), you can see whether the automatic settings were sufficient or whether additional customization is neces<a id="_idTextAnchor055"/>sary:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B16391_02_002.jpg" alt="Figure 2.2 – The File Reader node and its configuration window"/>
				</div>
			</div>
			<p class="figure-caption">Fig<a id="_idTextAnchor056"/>ure 2.2 – The File Reader node and its configuration window</p>
			<p>We drag and drop the <strong class="source-inline">Demographics.csv</strong> file from <strong class="source-inline">Example Workflows/TheData/Misc</strong> into the workflow editor. In the configuration window of the<a id="_idIndexMarker090"/> File Reader node, we see that the <strong class="source-inline">CustomerKey</strong> column<a id="_idIndexMarker091"/> is interpreted as the row ID of the data rows, rather than its own column. We need to disable the <strong class="bold">read Row IDs</strong> option to read the data properly. After the configuration is complete, we click <strong class="bold">OK</strong>; the node state moves to yellow and the node can now be executed.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The automatic creation of the node and the configuration of its settings by file drag and drop works only for specific file extensions: <strong class="source-inline">.csv</strong> for a File Reader node, <strong class="source-inline">.table</strong> for a Table Reader node, <strong class="source-inline">.xls</strong> and .<strong class="source-inline">xlsx</strong> for an Excel Reader node, and so on.</p>
			<p>Similarly, if<a id="_idIndexMarker092"/> we drag and drop the <strong class="source-inline">ProductData2.xls</strong> file from the<a id="_idIndexMarker093"/> KNIME Explorer panel to the <a id="_idIndexMarker094"/>workflow editor, an <strong class="bold">Excel Reader</strong> (<strong class="bold">XLS</strong>) node is created and automatically configured (<em class="italic">Fig<a id="_idTextAnchor057"/>ure 2.3</em>):</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B16391_02_003.jpg" alt="Figure 2.3 – The Excel Reader (XLS) node and its configuration window"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – The Excel Reader (XLS) node and its configuration window</p>
			<p>The <a id="_idIndexMarker095"/>configuration window (<em class="italic">Figure 2.3</em>) is similar to the one <a id="_idIndexMarker096"/>of the <strong class="bold">File Reader</strong> node, but, of course, customized to deal with Excel files. Three items especially are different:</p>
			<ul>
				<li>The preview part is activated by a <strong class="bold">refresh</strong> button. You need to click on <strong class="bold">refresh</strong> to update the preview.</li>
				<li>Column headers and row IDs are extracted from spreadsheet cells, identified with an alphabet letter (the column with the row IDs) and a row number (the row with the column headers), according to the Excel standards.</li>
				<li>On top of the URL path, there is a menu with a default choice, <strong class="bold">Custom URL</strong>. This menu allows you to express the file path as an absolute path (<strong class="bold">local file system</strong>), as a path relative to a mountpoint (<strong class="bold">Mountpoint</strong>), as a path relative to one of the current locations (data, workflow, or mountpoint), or as a custom path (<strong class="bold">Custom URL</strong>). This feature will be soon extended to other reader nodes.</li>
			</ul>
			<p>In our case, the<a id="_idIndexMarker097"/> automated configuration process does not include<a id="_idIndexMarker098"/> the column headers. We can see this from the preview segment. So, because we have the column headers in the first row, we adjust the <strong class="bold">Table contains column names in row number</strong> setting to <strong class="source-inline">1</strong>, refresh the preview, and click <strong class="bold">OK</strong> to save the changes and close the window.</p>
			<p>Next, let's read the <strong class="source-inline">SentimentAnalysis.table</strong> file. <strong class="source-inline">.table</strong> files contain binary content in a KNIME proprietary format optimized for speed and size. These files are read by the Table Reader node. Since all the information about the file is already included in the file itself, the configuration window of the <strong class="bold">Table Reader</strong> node just <a id="_idIndexMarker099"/>requires the file path and a few additional settings to limit the content to import. Again, dragging and dropping the <strong class="source-inline">SentimentAnalysis.table</strong> file automatically generates a Table Reader node with a pre-configured URL.</p>
			<p>To conclude this section, let's read the last files, <strong class="source-inline">SentimentRating.csv</strong> and <strong class="source-inline">WebDataOldSystem.csv</strong>, with two more File Reader nodes; then, let's add the name of the file in the comment under each node. Then, finally, let's group all these reader nodes inside an annotation explaining <strong class="bold">Reading data from files</strong> (<em class="italic">Figure 2.9</em>).</p>
			<p><strong class="source-inline">Demographics.csv</strong> contains the demographics of a number of customers, such as age and gender. Each customer is identified via a <strong class="source-inline">CustomerKey</strong> value. <strong class="source-inline">ProductData2.xls</strong> contains the products purchased by each customer, again identified via the <strong class="source-inline">CustomerKey</strong> value. <strong class="source-inline">SentimentAnalysis.table</strong> contains the sentiment expressed as text by the customer toward the company and the product, again identified via the <strong class="source-inline">CustomerKey</strong> value. <strong class="source-inline">SentimentRating.csv</strong> contains the <a id="_idIndexMarker100"/>mapping between the sentiment rating and the sentiment text. Finally, <strong class="source-inline">WebdataOldSystem.csv</strong> contains the old activity index by each customer, as classified in the old web <a id="_idIndexMarker101"/>system, before migration.</p>
			<p>Of course, if there is a dataset from before migration, we must have a newer dataset with data from the system after migration. This can be found in a database table in the <strong class="source-inline">WebActivity.sqlite</strong> SQLite database. </p>
			<p>This leads us to the next section, where we will learn how to read data from a database.</p>
			<h3>Reading Data from Databases</h3>
			<p>In the <a id="_idIndexMarker102"/>Node Repository, there is a category named <strong class="bold">DB</strong>, dedicated to <strong class="bold">database</strong> operations. All database operations are performed according to the same sequence (<em class="italic">Figure 2.4</em>): connect to database, select the table to work on, build a SQL query, and import data according to the SQL query.</p>
			<p>There are nodes for each of these steps, as show<a id="_idTextAnchor058"/>n in <em class="italic">Figure 2.4</em>:</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B16391_02_004.jpg" alt="Figure 2.4 – Importing data from databases: connect, select, build SQL query, and import"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Importing data from databases: connect, select, build SQL query, and import</p>
			<p>Let's check these nodes one by one:</p>
			<ul>
				<li><strong class="bold">Connectors</strong>: Connector nodes<a id="_idIndexMarker103"/> connect KNIME Analytics Platform to a database. The only <a id="_idIndexMarker104"/>generic connector is the <strong class="bold">DB Connector</strong> node. Besides that, there are many dedicated connectors. There is a SQLite connector, a MySQL connector, a Microsoft Access connector, and many other dedicated connectors. The SQLite connector is what we need to connect to the database contained in the <strong class="source-inline">WebActivity.sqlite</strong> file. The configuration window only requires the database file path since SQLite is a file-based database. All other settings have been preset in the node. Indeed, it is common to have some preset settings in dedicated connectors, and therefore dedicated connectors need fewer settings than the generic DB Connector node. A drag and drop of the <strong class="source-inline">.sqlite</strong> file automatically generates<a id="_idIndexMarker105"/> the <strong class="bold">SQLite DB Connector</strong> node with preloaded configuration settings.</li>
				<li><strong class="bold">Selecting the Table</strong>: The <strong class="bold">DB Table Selector</strong> node<a id="_idIndexMarker106"/> allows you to select the table from the connected database to work on. If you are a SQL expert, the <strong class="bold">Custom Query</strong> flag allows you to create <a id="_idIndexMarker107"/>your own query for the subset of data to extract.</li>
				<li><strong class="bold">Build SQL Query</strong>: If you are not a SQL expert, you can still build your SQL query to extract the subset of data. The DB nodes in the <strong class="bold">DB/Query</strong> category take a SQL query as input and add one more SQL queries on top of it. The node GUI is completely codeless and therefore there is no need to know any SQL code. So, for example, the configuration window of the <strong class="bold">DB Row Filter</strong> node presents<a id="_idIndexMarker108"/> a graphical editor on the right to build a row-filtering SQL query. </li>
			</ul>
			<p>In the following screenshot (<em class="italic">Figure 2.5</em>), record(s) of <strong class="bold">CustomerKey = 1117<a id="_idTextAnchor059"/>7</strong> have been excluded:</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B16391_02_005.jpg" alt="Figure 2.5 – The GUI of the DB Row Filter node. This node builds a SQL query to filter out records without using any SQL script"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – The GUI of the DB Row Filter node. This node builds a SQL query to filter out records without using any SQL script</p>
			<ul>
				<li><strong class="bold">Import Data</strong>: Finally, the <strong class="bold">DB Reader</strong> node imports<a id="_idIndexMarker109"/> the data from the database connection according to the input SQL query. The DB Reader<a id="_idIndexMarker110"/> node has no configuration window since all the required SQL settings to import the data are contained in the SQL query at its input port. There are many other nodes, besides the DB Reader node, to import data from a database at the end of such a sequence. They are all in the <strong class="bold">DB/Read/Write</strong> category<a id="_idIndexMarker111"/> in the Node Configuration panel.<p class="callout-heading">Important note</p><p class="callout">Did you notice the node ports in <em class="italic">Figure 2.4</em>? We passed from the black triangle (data) to the red square (connection) to the brown square (SQL query). Only ports of the same type, transporting data of the same type, can be connected!</p></li>
			</ul>
			<p>In order to inspect the results, after the successful execution of the DB Reader node, you can right-click the last node in the sequence – the one with a black triangle (data) port, in this case, the DB Reader node – and select the last item in the menu. This shows the output data table.</p>
			<p>The database nodes only <a id="_idIndexMarker112"/>produce a SQL query. At the output port, you can still inspect the results of the query by right-clicking the node, selecting the last item in the menu, then clicking on the <strong class="bold">Cache no of Rows</strong> button in the <strong class="bold">Table Preview</strong> tab to temporarily visualize just the top rows in the selected number.</p>
			<p>At this point, we have also imported the last dataset, including customer web activity after migration to the new web system.</p>
			<p>Let's spend a bit of time now on the data structure and data types.</p>
			<h1 id="_idParaDest-33"><a id="_idTextAnchor060"/>Data Types and Conversions</h1>
			<p>If you <a id="_idIndexMarker113"/>inspect any of the <a id="_idIndexMarker114"/>output data tables from any of the nodes described previously, you will see a table-like representation of the data. Here, each value is identified via <strong class="source-inline">RowID</strong>, the identification number for the record, and <a id="_idIndexMarker115"/>via a <strong class="bold">column header</strong>, the name of the attribute (<em class="italic">Figure 2.6</em>). So, the gender of <strong class="source-inline">CustomerKey 11000</strong> is <strong class="source-inline">M</strong>, as identified via the <strong class="source-inline">Gender</strong> column header, and the row ID is <strong class="source-inline">Row0</strong>. In a reader node, the row ID and column header can be generated automatically or assigned from the values in a column or a row in the data.</p>
			<p>The following is a screenshot of the data table outp<a id="_idTextAnchor061"/>ut by the File Reader node:</p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B16391_02_006.jpg" alt="Figure 2.6 – A KNIME data table. Here, a cell is identified via its RowID value and column header"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6 – A KNIME data table. Here, a cell is identified via its RowID value and column header</p>
			<p>Each data <a id="_idIndexMarker116"/>column also has <a id="_idIndexMarker117"/>a data type, as you can see in <em class="italic">Figure 2.6</em> from the icons in the column headers. Basic data types are <strong class="bold">Integer</strong>, <strong class="bold">Double</strong>, <strong class="bold">Boolean</strong> (<strong class="source-inline">true/false</strong>), and <strong class="bold">String</strong>. However, more complex data types are also available, such as <strong class="bold">Date&amp;Time</strong>, <strong class="bold">Document</strong>, <strong class="bold">Image</strong>, <strong class="bold">Network</strong>, and more. We will see some of these data types in the upcoming chapters.</p>
			<p>Of course, a data column is not condemned to stay with that data type forever. If the condition exists, it can move to another data type. Some nodes are dedicated to conversions and can be found in the Node Repository under <strong class="bold">Manipulation/Column/Convert &amp; Replace</strong>.</p>
			<p>In the data that we have read, <strong class="source-inline">CustomerKey</strong> has been imported as a five-digit integer. However, it might be convenient to move from an integer type representation to a string type representation. For that, we use<a id="_idIndexMarker118"/> the <strong class="bold">Number to String</strong> node. The configuration window consists of an include/exclude framework to select those columns whose type needs changing. The opposite transformation is <a id="_idIndexMarker119"/>obtained with the <strong class="bold">String to Number</strong> node. The <strong class="bold">Double to Int</strong> node might also be useful for a<a id="_idIndexMarker120"/> transformation from double to integer.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The <strong class="bold">String Manipulation</strong> and <strong class="bold">Math Formula</strong> nodes, even<a id="_idIndexMarker121"/> though their primary task <a id="_idIndexMarker122"/>is data transformation, also present some conversion functionality.</p>
			<p>We <a id="_idIndexMarker123"/>would like to draw your attention to the <strong class="bold">Category To Number</strong> node. This node <a id="_idIndexMarker124"/>comes in handy to discretize nominal classes and transform them into numbers, as neural<a id="_idIndexMarker125"/> networks only accept numbers as target classes.</p>
			<p>Special data types, such as <strong class="bold">Image</strong> or <strong class="bold">Date&amp;Time</strong>, offer their own conversion nodes. A very helpful node for that is the <strong class="bold">String to Date&amp;Time</strong> node. <strong class="bold">Date</strong> or <strong class="bold">Time</strong> objects are <a id="_idIndexMarker126"/>often read as <strong class="bold">String</strong>, and this node converts them into the appropriate type object.</p>
			<p>In the next section, we want to consolidate all this customer information, starting with the web activity before and after the migration. In these two datasets, the columns describing web activity have different names: <strong class="source-inline">First_WebActivity_</strong> and <strong class="source-inline">First(WebActivity)</strong>. Let's standardize them to the same name: <strong class="source-inline">First_WebActivity_</strong>.</p>
			<p>This is <a id="_idIndexMarker127"/>what the <strong class="bold">Column Rename</strong> node does: </p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B16391_02_007.jpg" alt="Figure 2.7 – The Column Rename node and its configuration window"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7 – The Column Rename node and its configuration window</p>
			<p>The<a id="_idIndexMarker128"/> configuration window<a id="_idIndexMarker129"/> of the <strong class="bold">Column Rename</strong> node lists all the columns from the input data <a id="_idIndexMarker130"/>table on the left. Double-clicking on a column opens a frame on the right showing the current column name and requiring the new name and/or new type. All the nodes we have introduced in this section can be seen in the workflow in <em class="italic">Figure 2.13</em>.</p>
			<p>Now, we are ready to concatenate the two web activity datasets and join all the other datasets by their <strong class="source-inline">CustomerKey</strong> values.</p>
			<h1 id="_idParaDest-34"><a id="_idTextAnchor062"/>Transforming Data</h1>
			<p>We have <a id="_idIndexMarker131"/>read the data from files and databases. In this section, we will perform some operations to consolidate, filter, aggregate, and transform them. We will start with consolidation operations.</p>
			<h3>Joining and Concatenating</h3>
			<p>The web activity <a id="_idIndexMarker132"/>dataset from the old system comes from a CSV file and, after column renaming, consists of two data columns: <strong class="source-inline">CustomerKey</strong> and <strong class="source-inline">First_WebActivity_</strong>. <strong class="source-inline">First_WebActivity_</strong> ranks how active a customer is on the company's web site: <strong class="source-inline">0</strong> means <strong class="bold">not active all</strong> and <strong class="source-inline">3</strong> means <strong class="bold">very active</strong>.</p>
			<p>The web activity dataset from the new web system comes from the SQLite database and consists of three columns: <strong class="source-inline">CustomerKey</strong>, <strong class="source-inline">First_WebActivity_</strong>, and <strong class="source-inline">Count</strong>. <strong class="source-inline">Count</strong> is just a progressive number associated with the data rows. It is not important for the upcoming analysis. We can decide later whether to remove it or keep it.</p>
			<p>It would be nice to have both rankings for the web activity, from the old and the new system, together in one single data table. For this, we <a id="_idIndexMarker133"/>use the <strong class="bold">Concatenate</strong> node. Two input data tables are placed together in the same output data table. Data<a id="_idIndexMarker134"/> cells belonging to columns with the same name are placed in the same output column. Data columns existing in only one of the tables can be retained (union of columns) or removed (intersection of columns), as set in the node configuration window. The node configuration window also offers a few strategies to deal with rows with the same row IDs existing in both input tables.</p>
			<p>We concatenated the two web activity data tables and kept the union of data columns in the output data table.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The Concatenate node icon shows three dots in its lower-left corner. Clicking these three dots gives you the chance to add more input ports and therefore to concatenate more input data tables.</p>
			<p>Let's now move on to the sentiment analysis data. <strong class="source-inline">SentimentAnalysis.table</strong> produced a data table with <strong class="source-inline">CustomerKey</strong> and <strong class="source-inline">SentimentAnalysis</strong> columns. <strong class="source-inline">SentimentAnalysis</strong> includes the customer's sentiment toward the company and product, expressed as text. <strong class="source-inline">SentimentRating.csv</strong> produced a data table with two columns: <strong class="source-inline">SentimentAnalysis</strong> and <strong class="source-inline">SentimentRating</strong>. Both columns express the customer sentiment: one in text and one in ranking ordinals. This is a mapping data table, translating text into ranking sentiment and vice versa. Depending on the kind of analysis we will run, we might need the text expression or the ranking expression. So, to be on the safe side, let's join these two data tables together to have them all, <strong class="source-inline">CustomerKey</strong>, <strong class="source-inline">SentimentAnalysis</strong> (text), and <strong class="source-inline">SentimentRating</strong> (ordinals), in one data table only. This is obtained with the <strong class="bold">Joiner</strong> node.</p>
			<p>The Joiner node<a id="_idIndexMarker135"/> joins data cells from two input data tables together into the same data row, according to a key value. In our case, the key values are provided by the <strong class="source-inline">SentimentAnalysis</strong> columns present in both input data tables. So, each customer (<strong class="source-inline">CustomerKey</strong>) will have the <strong class="source-inline">SentimentAnalysis</strong> text value and the corresponding <strong class="source-inline">SentimentRating</strong> value. The Joiner node<a id="_idIndexMarker136"/> offers four different join modes: <strong class="bold">inner join</strong> (intersection of key values in the two tables), <strong class="bold">left outer join</strong> (all key values from the left/top table), <strong class="bold">right outer join</strong> (all key values from the right/bottom table), and <strong class="bold">full outer join</strong> (all key values from both tables).</p>
			<p>In <em class="italic">Figure 2.8</em>, you can find the two tabs o<a id="_idTextAnchor063"/>f the configuration window of the Joiner node:</p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B16391_02_008.jpg" alt="Figure 2.8 – Configuration window of the Joiner node: the Joiner Settings and Column Selection tabs"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8 – Configuration window of the Joiner node: the Joiner Settings and Column Selection tabs</p>
			<p>The configuration window of the Joiner node includes two tabs: <strong class="bold">Joiner Settings</strong> and <strong class="bold">Column Selection</strong>. The <strong class="bold">Joiner Settings</strong> tab exposes for selection the joiner mode and the data columns containing the key values for both input tables. The <strong class="bold">Column Selection</strong> tab sets the <a id="_idIndexMarker137"/>columns from both input tables to retain when building the final joint data rows. A few additional options are available to deal with columns with the same names in the two tables and to set what to do with the key columns after the joining is performed.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">There can be more than one level of key columns for the join. Just select the <strong class="bold">+</strong> button in the <strong class="bold">Joiner Settings</strong> tab to add more key columns. If you have more than one level of key columns, you can decide whether a join is performed if all key values match (<strong class="bold">Match all of the following</strong>) or if just one key value matches (<strong class="bold">Match any of the following</strong>), as set in the top radio buttons (<em class="italic">Figure 2.8</em> on the left).</p>
			<p>We joined the<a id="_idIndexMarker138"/> two sentiment tables using <strong class="source-inline">SentimentAnalysis</strong> as the key column in both tables and using a left outer join. The left outer join includes all key values from the left (upper) table (the customer table) and therefore makes sure that all sentiment values for all customers are retained in the output data table.</p>
			<p>After joining <strong class="source-inline">CustomerKey</strong> with all the sentiment expressions, we will perform other similar join operations, multiple times, in cascade, using <strong class="source-inline">CustomerKey</strong> as the key column, to collect together the different pieces of data for the same customers in one single table (<em class="italic">Figure 2.13</em>).</p>
			<p>If we inspect the output produced by the <strong class="bold">File Reader</strong> node on the <strong class="source-inline">Demographics.csv</strong> file, we notice two data columns that are also provided by other files: <strong class="source-inline">WebActivity</strong> and <strong class="source-inline">SentimentRating</strong>. They are old columns and should be substituted with the same columns from the <strong class="source-inline">SentimentAnalysis.table</strong> file and the web activity files. We could remove these two columns in the <strong class="bold">Column Selection</strong> tab of the <strong class="bold">Joiner</strong> node. Alternatively, we can just filter those two columns out with a dedicated node.</p>
			<p>Let's see how to filter columns and rows out of a data table.</p>
			<h3>Column and Row Filtering</h3>
			<p>The <strong class="bold">Column Filter</strong> node is dedicated to<a id="_idIndexMarker139"/> filtering columns from the input data table. We can do that as follows:</p>
			<ul>
				<li>Manually select which columns to keep and which to exclude (<strong class="bold">Manual Selection</strong>).</li>
				<li>Use a wildcard or a Regex expression to match the names of the columns to exclude or to keep (<strong class="bold">Wildcard/Regex Selection</strong>).</li>
				<li>Define one or more data types for the columns to include or exclude (<strong class="bold">Type Selection</strong>).</li>
			</ul>
			<p>All these options are available at the top of the configuration window of the <strong class="bold">Column Filter</strong> node. Selecting <a id="_idIndexMarker140"/>one of them changes the configuration window according to the required settings for that option. Here are the options.</p>
			<ul>
				<li><strong class="bold">Manual Selection</strong>: Provides an include/exclude framework to move columns from one frame to the other to include or exclude input columns from the output data table (<em class="italic">Figure 2.9</em>). </li>
				<li><strong class="bold">Wildcard/Regex Selection</strong>: This option provides a textbox to enter the expression to match the column names. Wildcard expressions use <strong class="source-inline">*</strong> for joker characters; for example, <strong class="source-inline">R*</strong> indicates all words starting with <strong class="source-inline">R</strong>, <strong class="source-inline">R*a</strong> indicates all words starting with <strong class="source-inline">R</strong> and ending with <strong class="source-inline">a</strong>, and so on. Regex refers to regular expressions. </li>
				<li><strong class="bold">Type Selection</strong>: This option provides a multiple choice for the data types of the columns to include.</li>
			</ul>
			<p>The configuration window <a id="_idTextAnchor064"/>of the Column Filter node is shown in <em class="italic">Figure 2.9</em>:</p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B16391_02_009.jpg" alt="Figure 2.9 – The Column Filter node and its configuration window"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.9 – The Column Filter node and its configuration window</p>
			<p>So far, we have been filtering data by columns. The other flavor for data filtering is by rows. In this case, we want to remove or keep just some of the data rows in the table. For example, still working on the data from the <strong class="source-inline">Demographics.csv</strong> file, we might want to keep only the men in the dataset or remove all records with <strong class="bold">CustomerKey 11177</strong>. For this kind of filtering operation, there are many different nodes: <strong class="bold">Row Filter</strong>, <strong class="bold">Row Filter (Labs)</strong>, <strong class="bold">Rule-based Row Filter</strong>, <strong class="bold">Reference Row Filter</strong>, <strong class="bold">Date&amp;Time based Row Filter</strong>, and more:</p>
			<ul>
				<li>The <strong class="bold">Row Filter</strong> node<a id="_idIndexMarker141"/> is very simple and very powerful: on the right, the filtering condition and on the left the filtering mode.</li>
				<li><strong class="bold">Filtering Condition</strong> matches the content of cells in a data column with a condition. The input data column to match is selected at the top. The condition can<a id="_idIndexMarker142"/> consist of <strong class="bold">pattern matching</strong>, including wildcards and regex in the <a id="_idIndexMarker143"/>pattern expression; <strong class="bold">range checking</strong>, which is useful for numerical<a id="_idIndexMarker144"/> columns; and <strong class="bold">missing value matching</strong>. <p><strong class="bold">Filtering Mode</strong> on the left sets whether to include or exclude the matching rows<a id="_idTextAnchor065"/>, matching by attribute value, row number, or <strong class="bold">RowID</strong>:</p></li>
			</ul>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B16391_02_010.jpg" alt="Figure 2.10 – The Row Filter node and its configuration window"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.10 – The Row Filter node and its configuration window</p>
			<p>Here, we filter out, using the <strong class="bold">Exclude</strong> option on the left, all data rows where the <strong class="source-inline">CustomerKey</strong> attribute has a value of <strong class="source-inline">11177</strong>.</p>
			<ul>
				<li>A similar result could have been obtained using a <strong class="bold">Reference Row Filter</strong> node. The Reference Row Filter node<a id="_idIndexMarker145"/> has two input ports. It matches the rows in the top table with the rows in the lower table according to the cell content in the set columns. Matching rows will be excluded or included according to the node configuration settings. In the workflow in <em class="italic">Figure 2.13</em>, we feed the value <strong class="source-inline">11177</strong> into the lower port of the <strong class="bold">Reference Row Filter</strong> node from a <strong class="bold">Table Creator</strong> node.</li>
				<li>The <strong class="bold">Table Creator</strong> node<a id="_idIndexMarker146"/> is an interesting node for temporary small data. It covers the role of an internal spreadsheet, which is where to store a few lines of data.</li>
			</ul>
			<p>Another group of very important nodes is the ones performing aggregations.</p>
			<h3>Aggregations</h3>
			<p>Aggregations are a <a id="_idIndexMarker147"/>very important part of any data preparation. Whether for dashboard or machine learning algorithms, some aggregation operations are usually necessary. There <a id="_idIndexMarker148"/>are two commonly used nodes for aggregations: the <strong class="bold">GroupBy</strong> node and the <strong class="bold">Pivoting</strong> node.</p>
			<p>In <em class="italic">Figure 2.11</em>, you can see<a id="_idIndexMarker149"/> the t<a id="_idTextAnchor066"/>wo tabs in the configuration window of the GroupBy node:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B16391_02_011.jpg" alt="Figure 2.11 – The two tabs of the GroupBy node's configuration window: &#13;&#10;Groups and Manual Aggregation"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.11 – The two tabs of the GroupBy node's configuration window: Groups and Manual Aggregation</p>
			<p>The <strong class="bold">GroupBy</strong> node<a id="_idIndexMarker150"/> isolates groups of data and on these groups calculates some measures, such as simple count, average, variance, percentages, and others. Identification of the groups happens in the tab named <strong class="bold">Groups</strong> of the configuration window; measure setting happens in one of the other tabs (<em class="italic">Figure 2.11</em>).</p>
			<p>In the <strong class="bold">Groups</strong> tab, you select the data <a id="_idIndexMarker151"/>columns whose value combinations define the different groups of data. The node then creates one row for each group. For example, selecting the <strong class="bold">Gender</strong> column as the group column with distinct values of <strong class="bold">male</strong> and <strong class="bold">female</strong> means to identify those groups of data with <strong class="bold">Gender</strong> as <strong class="bold">male</strong> or <strong class="bold">female</strong>. Selecting the <strong class="bold">Gender</strong> (<strong class="bold">male</strong>/<strong class="bold">female</strong>) and <strong class="bold">MaritalStatus</strong> (<strong class="bold">single</strong>/<strong class="bold">married</strong>) columns as group columns means to identify the <strong class="bold">single-female</strong>, <strong class="bold">single-male</strong>, <strong class="bold">married-female</strong>, and <strong class="bold">married-male</strong> groups.</p>
			<p>Then, we need to select the measures we want to provide for these groups. Here we can proceed by doing the following:</p>
			<ul>
				<li>Manually selecting the columns and the measures to apply one by one (<strong class="bold">Manual Aggregation</strong>)</li>
				<li>Selecting the columns based on a pattern, including wildcard or Regex expressions, and the measures to apply to each set of columns (<strong class="bold">Pattern Based Aggregation</strong>)</li>
				<li>Selecting the columns by type and the measures to apply to each set of columns (<strong class="bold">Type Based Aggregation</strong>)</li>
			</ul>
			<p>Each measure setting mode has its own tab in the configuration window (<em class="italic">Figure 2.11</em>). In the <strong class="bold">Manual Aggregation</strong> tab, we set the simple <strong class="bold">Count</strong> measure on the <strong class="source-inline">CustomerKey</strong> column and the <strong class="bold">Mean</strong> measure on the <strong class="source-inline">Age</strong> column. For <strong class="source-inline">Gender</strong> as the group column, we then get the number and the average age of women and men in the input table.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">The <strong class="bold">GroupBy</strong> node offers a large number of measures. We have seen <strong class="bold">Count</strong> and <strong class="bold">Mean</strong>. However, we could have also used percentage, median, variance, number of missing values, sum, mode, minimum, maximum, first, last, kurtosis, concatenation of (distinct) values, correlation, and more. It is worth taking some time to investigate all the measurement methods available within the <strong class="bold">GroupBy</strong> node.</p>
			<p>Like the <strong class="bold">GroupBy</strong> node, we have the Pivoting node. The <strong class="bold">Pivoting</strong> node also identifies groups<a id="_idIndexMarker152"/> in the data and provides some aggregation measures on selected data columns for each group. The difference with the <strong class="bold">GroupBy</strong> node is in the shape of the result. Let's take the example of the <strong class="source-inline">Gender</strong> (<strong class="bold">Group</strong>) and <strong class="source-inline">MaritalStatus</strong> (<strong class="bold">Pivot</strong>) groups and the <strong class="bold">Count</strong> measure applied to the <strong class="source-inline">CustomerKey</strong> data column. The final result is a table with <strong class="bold">male</strong>/<strong class="bold">female</strong> as the row IDs, <strong class="bold">married</strong>/<strong class="bold">single</strong> as the column headers, and the count of occurrences of each combination as the cell content.</p>
			<p>This means that the distinct values in the group columns generate rows and the distinct values in the pivoting columns generate columns.</p>
			<p>The<a id="_idIndexMarker153"/> configuration window of the <strong class="bold">Pivoting</strong> node then has three tabs: <strong class="bold">Groups</strong> to select the group columns, <strong class="bold">Pivots</strong> to select the pivoting columns, and <strong class="bold">Manual Aggregation</strong> to manually select data columns and the measures to calculate on them. If more than one manual aggregation is used, the resulting pivoting table has one column for each combination of aggregation method and pivot value.</p>
			<p>In addition, the node returns the total aggregation based on only the group columns on the second output port and the total aggregation based on only the pivoted columns at the third output port.</p>
			<p>Let's move on now to a few more very flexible and very powerful nodes to perform data transformation.</p>
			<h3>Math Formula and String Manipulation nodes</h3>
			<p>KNIME Analytics Platform offers many nodes for data transformation. We cannot describe all of them here. So, while we leave the enjoyment of their discovery to you, we will describe two very <a id="_idIndexMarker154"/>powerful nodes here: the <strong class="bold">String Manipulation</strong> and <strong class="bold">Math Formula</strong> nodes.</p>
			<p>The <strong class="bold">String Manipulation</strong> node<a id="_idIndexMarker155"/> applies transformations on string values in data cells. The transformations are listed in the <strong class="bold">Function</strong> panel in the node configuration window (<em class="italic">Figure 2.12</em>). There, you can see the function and its possible syntaxes. If you select a function in the list, in the panel on the right, named <strong class="bold">Description</strong>, a full description of the function task and syntax appears. The transformation, however, is implemented in the <strong class="bold">Expression</strong> editor at the bottom of the window.</p>
			<p>First, you select (double-click) a transformation from the <strong class="bold">Function</strong> list, then you populate it with the appropriate arguments in the <strong class="bold">Expression</strong> editor. Arguments of a function can be constant strings, thus enclosed in <strong class="source-inline">""</strong>, or values from other columns in the input data table. Values from columns are inserted automatically with the right syntax with a double-click on the column name in the <strong class="bold">Column List</strong> panel on the left.</p>
			<p>Let's take an example:</p>
			<ol>
				<li value="1">In the data table resulting from the <strong class="bold">GroupBy</strong> node, we got two data rows: one for male (<strong class="source-inline">M</strong>) and one for female (<strong class="source-inline">F</strong>), containing the number of occurrences and the average age for each group (<strong class="source-inline">M/F</strong>). Let's change <strong class="source-inline">"M"</strong> to <strong class="source-inline">"Male"</strong> and <strong class="source-inline">"F"</strong> to <strong class="source-inline">Female"</strong>.</li>
				<li>Then, we would use the <strong class="source-inline">replace(str, search, replace)</strong> function, where <strong class="source-inline">str</strong> indicates the column to work on, <strong class="source-inline">search</strong> the string to search in the cell value, and <strong class="source-inline">replace</strong> the string to use as a replacement. Double-clicking on the <strong class="bold">Gender</strong> column in the <strong class="bold">Column List</strong> panel and completing the expression by hand, we end up with the following expression:<p class="source-code">replace($Gender$, "M", "Male")</p></li>
				<li>We get the <a id="_idIndexMarker156"/>following in a subsequent node:<p class="source-code">replace($Gender$, "F", "Female")</p><p>The <a id="_idIndexMarker157"/>String Manipulat<a id="_idTextAnchor067"/>ion node and its configuration window are shown in <em class="italic">Figure 2.12</em>:</p><div id="_idContainer043" class="IMG---Figure"><img src="image/B16391_02_012.jpg" alt="Figure 2.12 – The String Manipulation node and its configuration window"/></div><p class="figure-caption">Figure 2.12 – The String Manipulation node and its configuration window</p><p>It is also possible to nest transformation functions. If, for example, we want to standardize all cells to make sure to include the <strong class="source-inline">"M"</strong> or <strong class="source-inline">"F"</strong> capital letters before applying the <strong class="source-inline">replace()</strong> transformation, we would nest the <strong class="source-inline">uppercase(str)</strong> function in it and end up with the following expression:</p><p class="source-code">replace(upperCase($Gender$), "M", "Male")</p><p>We would get a similar expression for <strong class="source-inline">"F"</strong> and <strong class="source-inline">"Female"</strong>.</p></li>
				<li>Finally, we set to replace the original values in the <strong class="source-inline">Gender</strong> column with the new values, using the <strong class="bold">Replace Column</strong> option in the lower part of the configuration window.</li>
			</ol>
			<p>It is also possible to apply the same transformation to more than one input data column, with<a id="_idIndexMarker158"/> the <strong class="bold">String Manipulation (Multi Column)</strong> node. This node essentially works like the <strong class="bold">String Manipulation</strong> node. It just applies the set expression to all selected data columns. The lower part of its configuration window is the same as for the <strong class="bold">String Manipulation</strong> node. In the upper part, though, you can select all columns on which to apply the expression.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">In the <strong class="bold">String Manipulation (Multi Column)</strong> node, if we want to apply the same expression to all<a id="_idIndexMarker159"/> columns selected in the upper part of the configuration window, we need to use the <strong class="source-inline">$$CURRENTCOLUMN$$</strong> general column name in the <strong class="bold">Expression</strong> editor. The very large number of string transformations in the <strong class="bold">Function</strong> list makes this node extremely powerful.</p>
			<p>A node very similar to the String Manipulation node, even though working on a different task, is the <strong class="bold">Math Formula</strong> node. The Math Formula node implements a mathematical expression<a id="_idIndexMarker160"/> on the input data. Besides that, it works exactly the same as the String Manipulation node. In the configuration window, the available math functions are listed in the central <strong class="bold">Function</strong> panel. If a function from the list is selected, the description appears in the <strong class="bold">Description</strong> panel. The final expression is crafted in the <strong class="bold">Expression</strong> editor at the bottom. Insertion of column names in the <strong class="bold">Expression</strong> editor happens by double-clicking the column name in the <strong class="bold">Column List</strong> panel on the left. Nested mathematical functions are possible.</p>
			<p>The <strong class="bold">Math Formula (Multi Column)</strong> node extends the <strong class="bold">Math Formula</strong> node to apply the same formula onto many<a id="_idIndexMarker161"/> selected columns.</p>
			<p><em class="italic">Figure 2.13</em> shows the final workflow containing all the operations described in this chapter, which is also available on the KNIME Hub: <a href="https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%202/">https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%202/</a>:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B16391_02_013.jpg" alt="Figure 2.13 – Workflow that summarizes some data access, data conversion, and &#13;&#10;data transformation nodes available in KNIME Analytics Platform"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.13 – Workflow that summarizes some data access, data conversion, and data transformation nodes available in KNIME Analytics Platform</p>
			<p>So far, we have seen static transformations on data. What about having a different transformation for different conditions? Let's take the <strong class="bold">Row Filter</strong> node. Today, I might want to filter out the female occurrences from the data table, while tomorrow the male ones. How can I do that without having to change the configuration settings for all involved nodes at every run? The time has come to introduce you to <strong class="bold">Flow Variables</strong>.</p>
			<h1 id="_idParaDest-35"><a id="_idTextAnchor068"/>Parameterizing the Workflow</h1>
			<p>Let's <a id="_idIndexMarker162"/>consider a simple workflow: read the <strong class="source-inline">Demographics.csv</strong> file, filter all data rows with <strong class="source-inline">Gender = M or F</strong>, and replace <strong class="source-inline">M</strong> or <strong class="source-inline">F</strong> with <strong class="source-inline">Male</strong> or <strong class="source-inline">Female</strong>, respectively. Once we have decided whether to work on <strong class="source-inline">M</strong> or <strong class="source-inline">F</strong>, the workflow becomes quite simple and includes a <strong class="bold">File Reader</strong>, <strong class="bold">Row Filter</strong>, and <strong class="bold">String Manipulation</strong> node with the <strong class="source-inline">replace()</strong> function:</p>
			<ol>
				<li value="1">Let's add one node that allows us to choose whether to work on <strong class="source-inline">M</strong> or <strong class="source-inline">F</strong> records: the <strong class="bold">String Configuration</strong> node. This node generates a flow variable. A flow variable<a id="_idIndexMarker163"/> is a<a id="_idIndexMarker164"/> parameter that travels with the data flow along the workflow branch and it can be used to overwrite settings in other nodes.</li>
				<li>As far as we are concerned, for now, two settings are important in the configuration window of this node: the default value and the variable name. Let's use default value <strong class="source-inline">M</strong> for now, to work with <strong class="source-inline">Gender = M</strong> records, and let's name the flow variable <strong class="source-inline">gender_variable</strong>.</li>
				<li>Executing <a id="_idTextAnchor069"/>the node creates a Flow Variable named <strong class="source-inline">gender_variable</strong> with value <strong class="source-inline">M</strong>:<div id="_idContainer045" class="IMG---Figure"><img src="image/B16391_02_014.jpg" alt="Figure 2.14 – This workflow shows how to use flow variables"/></div><p class="figure-caption">Figure 2.14 – This workflow shows how to use flow variables</p></li>
				<li>Now, let's use the value of the <strong class="bold">Flow Variable</strong> to overwrite the filter setting in the <strong class="bold">Row Filter</strong> node. In the configuration window of the <strong class="bold">Row Filter</strong> node (<em class="italic">Figure 2.10</em>), on the right of the setting for <strong class="bold">Use pattern matching</strong> there is a button with a <strong class="bold">V</strong> on it. Through this button, we can overwrite the setting with the value contained in one of the available flow variables.<p>Alternatively, if the<a id="_idIndexMarker165"/> configuration setting does not display this button, you can overwrite the setting via the <strong class="bold">Flow Variables</strong> tab at the top of the configuration window. In this tab, search for your setting, and in the corresponding empty space, select the Flow Variable to use. In our case, we overwrote the <strong class="bold">Use pattern matching</strong> setting with the Flow Variable <strong class="source-inline">gender_variable</strong> via the <strong class="bold">V</strong> button.</p><p>Did you notice the red connection between the <strong class="bold">String Configuration</strong> node and the <strong class="bold">Row Filter</strong> node? This is a <strong class="bold">Flow Variable</strong> connection. Flow variables are injected into nodes and branches via these connections.</p><p class="callout-heading">Important note</p><p class="callout">All nodes have hidden red circle ports for the input and output of flow variables. Clicking on the flow variable port of a node and releasing on another node brings out the hidden flow variable port and connects the nodes. Alternatively, in the context menu of each node, the <strong class="bold">Show Flow Variable Ports</strong> option makes them visible. </p></li>
				<li>After that, we create a small table with two rows, [<strong class="source-inline">M, Male</strong>] and [<strong class="source-inline">F, Female</strong>]. We select the row corresponding to the value in the <strong class="source-inline">gender_variable</strong> flow variable, and we aim to replace the <strong class="source-inline">M</strong> or <strong class="source-inline">F</strong> character with the text. For this last part, we need to replace the hardcoded strings in the <strong class="bold">String Manipulation</strong> node with the current text values. We already have the <strong class="source-inline">M</strong> or <strong class="source-inline">F</strong> character as a <strong class="bold">Flow Variable</strong>.</li>
				<li>Now, we transform the <strong class="source-inline">Male</strong>/<strong class="source-inline">Female</strong> text into a new flow variable. We do this via the <strong class="bold">Table Column To Variable</strong> node. This node converts the values from a table column <a id="_idIndexMarker166"/>into flow variables with the row IDs as their variable name and the values in the selected column as the variable values.<p>At this<a id="_idIndexMarker167"/> point, the <strong class="bold">String Manipulation</strong> node sees both <strong class="bold">Flow Variables</strong>: <strong class="source-inline">gender_variable</strong>, generated by the <strong class="bold">String Configuration</strong> node, and <strong class="source-inline">Row0</strong>, generated by the <strong class="bold">Table Column to Variable</strong> node. So, we can use the following syntax to perform the replacement operation alternatively with [<strong class="source-inline">M, Male</strong>] or [<strong class="source-inline">F, Female</strong>], depending on what has been selected in the <strong class="bold">String Configuration</strong> node:</p><p class="source-code">replace($Gender$, $${Sgender_variable}$$, $${SRow0}$$)</p><p>Notice the difference in syntax in the expression when dealing with columns (<strong class="source-inline">$Gender$</strong>) and flow variables (<strong class="source-inline">$${Sgender_variable}$$</strong>). Also, flow variables can be inserted automatically and with the right syntax in the <strong class="bold">Expression</strong> editor, by double-clicking on the flow variable name in the <strong class="bold">Flow Variable List</strong> panel on the left of the String Manipulation node's configuration window (<em class="italic">Figure 2.12</em>).</p></li>
			</ol>
			<p>The benefit of <a id="_idIndexMarker168"/>using flow variables is clear. When we decide to use <strong class="source-inline">F</strong> instead of <strong class="source-inline">M</strong>, we just change the setting in the <strong class="bold">String Configuration</strong> node instead of checking and changing the setting in every single node.</p>
			<p>We have shown only a small fraction of the nodes dealing with flow variables. You can explore more of these nodes in the <strong class="bold">Workflow Control/Variables</strong> category in the Node Repository panel.</p>
			<h1 id="_idParaDest-36"><a id="_idTextAnchor070"/>Summary</h1>
			<p>We do not have space in this book to describe more of the many nodes available in KNIME Analytics Platform. We will leave this exploratory task to you.</p>
			<p>KNIME Analytics Platform includes more than 2,000 nodes and covers a large variety of functionalities. However, the factotum nodes that work in most situations are much fewer in number, such as, for example, File Reader, Row Filter, GroupBy, Join, Concatenation, Math Formula, String Manipulation, Rule Engine, and more. We have described most of them in this chapter to give you a solid basis to build more complex workflows for deep learning, which we will do in the next chapter.</p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor071"/>Questions and Exercises</h1>
			<p>Check your level of understanding of the concepts presented in this chapter by answering the foll<a id="_idTextAnchor072"/>owing questions:</p>
			<ol>
				<li value="1">How can I read a text file with lines of variable length?<p>a) By using the CSV Reader node</p><p>b) By using the File Reader node</p><p>c) By using the File Reader node and the allow short lines enabled option</p><p>d) By using the File Reader node and the Limit Rows enabled option</p></li>
				<li>How can I filter records to the <strong class="source-inline">Age &gt; 42</strong> column?<p>a) By using the Column Filter node and selecting the <strong class="source-inline">Age</strong> column</p><p>b) By using the Row Filter node and pattern matching <strong class="source-inline">=42</strong> with the <strong class="bold">Include</strong> option on the right</p><p>c) By using the Row Filter node and range checking on, lower boundary <em class="italic">42</em>, with the <strong class="bold">Include</strong> option on the right</p><p>d) By using the Row Filter node and range checking on, lower boundary <em class="italic">42</em>, with the <strong class="bold">Exclude</strong> option on the right</p></li>
				<li>How can I find the average sentiment rating for single women?<p>a) By using a GroupBy node with <strong class="source-inline">Gender</strong> and <strong class="source-inline">MaritalStatus</strong> as group columns and the mean operation on the <strong class="source-inline">SentimentRating</strong> column</p><p>b) By using a GroupBy node with <strong class="source-inline">Gender</strong> as the group column and a count operation on the <strong class="source-inline">CustomerKey</strong> column</p><p>c) By using a GroupBy node with <strong class="source-inline">CustomerKey</strong> as the group column and a concatenate operation on the <strong class="source-inline">SentimentAnalysis</strong> column </p><p>d) By using a GroupBy node with <strong class="source-inline">MaritalStatus</strong> as the group column and a percent operation on the <strong class="source-inline">SentimentRating</strong> column </p></li>
				<li>Why do we need flow variables?<p>a) To generate new values</p><p>b) To feed the necessary red connections</p><p>c) To populate the flow variables list in configuration windows</p><p>d) To parameterize the workflow</p></li>
			</ol>
		</div>
	</body></html>