["```py\npip install -U numpy sklearn matplotlib\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import fetch_california_housing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression\n```", "```py\n    # Load the California housing dataset\n    ```", "```py\n    X, y = fetch_california_housing(return_X_y=True)\n    ```", "```py\n    X = np.concatenate([X, X*X], axis=1)\n    ```", "```py\n    # Split the data\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        X, y, test_size=0.2, random_state=0)\n    ```", "```py\n    # Rescale the data\n    ```", "```py\n    scaler = StandardScaler()\n    ```", "```py\n    X_train = scaler.fit_transform(X_train)\n    ```", "```py\n    X_test = scaler.transform(X_test)\n    ```", "```py\n    # Fit the linear regression model\n    ```", "```py\n    lr = LinearRegression()\n    ```", "```py\n    lr.fit(X_train, y_train)\n    ```", "```py\n    # Print the R2-score on train and test\n    ```", "```py\n    print('R2-score on train set:', lr.score(X_train, y_train))\n    ```", "```py\n    print('R2-score on test set:', lr.score(X_test, y_test))\n    ```", "```py\nR2-score on train set: 0.6323843381852894 R2-score on test set: -1.2472000127402643\n```", "```py\nprint('w values:', lr.coef_)\nprint('b value:', lr.intercept_)\n```", "```py\nw values: [ 1.12882772e+00 -6.48931138e-02 -4.04087026e-01  4.87937619e-01  -1.69895164e-03 -4.09553062e-01 -3.72826365e+00 -8.38728583e+00  -2.67065542e-01  2.04856554e-01  2.46387700e-01 -3.19674747e-01   2.58750270e-03  3.91054062e-01  2.82040287e+00 -7.50771410e+00] b value: 2.072498958939411\n```", "```py\nimport matplotlib.pyplot as plt\nplt.bar(np.arange(len(lr.coef_)), lr.coef_)\nplt.xlabel('feature index')\nplt.ylabel('weight value')\nplt.show()\n```", "```py\n    from sklearn.linear_model import Ridge\n    ```", "```py\n    # Fit the Ridge model ridge = Ridge(alpha=5000)\n    ```", "```py\n    ridge.fit(X_train, y_train)\n    ```", "```py\n    Ridge(alpha=5000)\n    ```", "```py\n    # Print the R2-score on train and test\n    ```", "```py\n    print('R2-score on train set:', ridge.score(X_train, y_train))\n    ```", "```py\n    print('R2-score on test set:', ridge.score(X_test, y_test))\n    ```", "```py\nR2-score on train set: 0.5398290317808138 R2-score on test set: 0.5034148460338739\n```", "```py\nprint('theta values:', ridge.coef_)\nprint('b value:', ridge.intercept_)\n```", "```py\ntheta values: [ 0.43456599  0.06311698  0.00463607  0.00963748  0.00896739 -0.05894055  -0.17177956 -0.15109744  0.22933247  0.08516982  0.01842825 -0.01049763  -0.00358684  0.03935491 -0.17562536  0.1507696 ] b value: 2.07249895893891\n```", "```py\nplt.bar(np.arange(len(ridge.coef_)), ridge.coef_)\nplt.xlabel('feature index') plt.ylabel('weight value')\nplt.show()\n```", "```py\n    from sklearn.linear_model import Lasso\n    ```", "```py\n    # Fit the Lasso model lasso = Lasso(alpha=0.02)\n    ```", "```py\n    lasso.fit(X_train, y_train)\n    ```", "```py\n    # Print the R2-score on train and test\n    ```", "```py\n    print('R2-score on train set:', lasso.score(X_train, y_train))\n    ```", "```py\n    print('R2-score on test set:', lasso.score(X_test, y_test))\n    ```", "```py\nR2-score on train set: 0.5949103710772492\nR2-score on test set: 0.57350350155955\n```", "```py\nplt.bar(np.arange(len(lasso.coef_)), lasso.coef_)\nplt.xlabel('feature index')\nplt.ylabel('weight value')\nplt.show()\n```", "```py\n    from sklearn.linear_model import ElasticNet\n    ```", "```py\n    # Fit the LASSO model\n    ```", "```py\n    Elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\n    ```", "```py\n    elastic.fit(X_train, y_train)\n    ```", "```py\n    ElasticNet(alpha=0.1)\n    ```", "```py\n    # Print the R2-score on train and test\n    ```", "```py\n    print('R2-score on train set:', elastic.score(\n    ```", "```py\n        X_train, y_train))\n    ```", "```py\n    print('R2-score on test set:', elastic.score(\n    ```", "```py\n        X_test, y_test))\n    ```", "```py\nR2-score on train set: 0.539957010948829\nR2-score on test set: 0.5134203748307193\n```", "```py\n    from sklearn.datasets import load_breast_cancer\n    ```", "```py\n    from sklearn.linear_model import LogisticRegression\n    ```", "```py\n    # Load the dataset\n    ```", "```py\n    X, y = load_breast_cancer(return_X_y=True)\n    ```", "```py\n    # Split the data\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        X, y, test_size=0.2, random_state=42)\n    ```", "```py\n    # Rescale the data\n    ```", "```py\n    scaler = StandardScaler()\n    ```", "```py\n    X_train = scaler.fit_transform(X_train)\n    ```", "```py\n    X_test = scaler.transform(X_test)\n    ```", "```py\n    # Fit the logistic regression model with no regularization\n    ```", "```py\n    Lr = LogisticRegression(penalty='none')\n    ```", "```py\n    lr.fit(X_train, y_train)\n    ```", "```py\n    LogisticRegression(penalty='none')\n    ```", "```py\n    # Print the accuracy score on train and test\n    ```", "```py\n    print('Accuracy on train set:', lr.score(X_train, y_train))\n    ```", "```py\n    print('Accuracy on test set:', lr.score(X_test, y_test))\n    ```", "```py\nAccuracy on train set: 1.0 Accuracy on test set: 0.9385964912280702\n```", "```py\n    lr = LogisticRegression(penalty='l2', C=0.1)\n    ```", "```py\n    lr.fit(X_train, y_train)\n    ```", "```py\n    LogisticRegression(C=0.1)\n    ```", "```py\n    # Print the accuracy score on train and test\n    ```", "```py\n    print('Accuracy on train set:', lr.score(\n    ```", "```py\n        X_train, y_train))\n    ```", "```py\n    print('Accuracy on test set:', lr.score(\n    ```", "```py\n        X_test, y_test))\n    ```", "```py\n    Accuracy on train set: 0.9802197802197802\n    ```", "```py\n    Accuracy on test set: 0.9824561403508771\n    ```", "```py\naccuracy_train = []\naccuracy_test = []\nc_values = [0.001,0.003,0.01,0.03,0.1,0.3,1,3,10,30]\nfor c in c_values:\n    lr = LogisticRegression(penalty='l2', C=c)\n    lr.fit(X_train, y_train)\n    accuracy_train.append(lr.score(X_train, y_train))\n    accuracy_test.append(lr.score(X_test, y_test))\nplt.plot(c_values, accuracy_train, label='train')\nplt.plot(c_values, accuracy_test, label='test')\nplt.legend()\nplt.xlabel('C: inverse of regularization strength')\nplt.ylabel('Accuracy')\nplt.xscale('log')\nplt.show()\n```", "```py\n    from sklearn.model_selection import GridSearchCV\n    ```", "```py\n    # Define the hyperparameters we want to test param_grid = {\n    ```", "```py\n        'penalty': ['l1', 'l2'],\n    ```", "```py\n        'C': [0.01, 0.03, 0.06, 0.1, 0.3, 0.6] }\n    ```", "```py\n        # Instantiate the grid search\n        ```", "```py\n        object grid = GridSearchCV(\n        ```", "```py\n            LogisticRegression(solver='liblinear'),\n        ```", "```py\n            param_grid,\n        ```", "```py\n            scoring='accuracy',\n        ```", "```py\n            cv=5 )\n        ```", "```py\n    # Fit and wait\n    ```", "```py\n    grid.fit(X_train, y_train)\n    ```", "```py\n    # Print the best set of hyperparameters\n    ```", "```py\n    print('best hyperparameters:', grid.best_params_)\n    ```", "```py\n    best hyperparameters: {'C': 0.06, 'penalty': 'l2'}\n    ```", "```py\n    # Print the accuracy score on train and test\n    ```", "```py\n    print('Accuracy on train set:', grid.score(\n    ```", "```py\n        X_train, y_train))\n    ```", "```py\n    print('Accuracy on test set:', grid.score(\n    ```", "```py\n        X_test, y_test))\n    ```", "```py\n    Accuracy on train set: 0.9824175824175824\n    ```", "```py\n    Accuracy on test set: 0.9912280701754386\n    ```"]