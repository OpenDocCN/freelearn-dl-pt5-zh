<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch006.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="chapter-1-bayesian-inference-in-the-age-of-deep-learning" class="level1 chapterHead" data-number="6">
<h1 class="chapterHead" data-number="6"><span class="titlemark">Chapter 1</span><br/>
<span id="x1-150001"></span>Bayesian Inference in the Age of Deep Learning</h1>
<p>Over the last fifteen years, <strong>machine learning</strong> (<strong>ML</strong>) has <span id="dx1-15001"></span>gone from a relatively little-known field to a buzzword in the tech community. This is due in no small part to the <span id="dx1-15002"></span>impressive feats of <strong>neural networks</strong> (<strong>NNs</strong>). Once a niche underdog in the field, <strong>deep learning</strong>’s accomplishments in almost every conceivable application have resulted in a near-meteoric rise in its popularity. Its success has been so pervasive that, rather than being impressed by features afforded by deep learning, we’ve come to <em>expect</em> them. From applying filters in social networking apps, through to relying on Google Translate when on vacation abroad, it’s undeniable that deep learning is now well and truly embedded in the technology landscape.</p>
<p>But, despite all of its impressive accomplishments, and the variety of products and features it’s afforded us, deep learning has not yet surmounted its final hurdle. As sophisticated neural networks are increasingly applied in mission-critical and safety-critical applications, the questions around their robustness become more and more pertinent. The black-box nature of many deep learning algorithms makes them daunting candidates for safety-savvy solutions architects - so much so that many would prefer sub-standard performance over the potential risks of an opaque system.</p>
<p>So, how can we conquer the apprehension surrounding deep learning and ensure that we create more robust, trustworthy models? While some of the answers to this lie <span id="dx1-15003"></span>down the path of <strong>explainable artificial intelligence</strong> (<strong>XAI</strong>), an important building block <span id="dx1-15004"></span>lies in the field of <strong>Bayesian deep</strong> <strong>learning</strong> (<strong>BDL</strong>). Through this book, you will discover the fundamental principles behind BDL through practical examples, allowing you to develop a strong understanding of the field, and equipping you with the knowledge and tools you need to build your own BDL models.</p>
<p>But, before we get started, let’s delve deeper into the justifications of BDL, and why typical deep learning methods may not be as robust as we’d like. In this chapter, we’ll learn about some of the key successes and failures of deep learning, and how BDL can help us to avoid the potentially tragic consequences of standard deep models. We’ll then outline the core topics of the rest of the book, before introducing you to the libraries and data that we’ll be using in practical examples.</p>
<p>These topics will be covered in the following sections:</p>
<ul>
<li><p>Wonders of the deep learning age</p></li>
<li><p>Understanding the limitations of deep learning</p></li>
<li><p>Core topics</p></li>
<li><p>Setting up the work environment</p></li>
</ul>
<p><span id="x1-15005r1"></span></p>
<section id="technical-requirements" class="level2 sectionHead" data-number="6.1">
<h2 class="sectionHead" data-number="6.1" id="sigil_toc_id_14"><span class="titlemark">1.1 </span> <span id="x1-160001"></span>Technical requirements</h2>
<p>All of the code for this book can be found on the GitHub repository for the book: <a href="https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference" class="url"><span class="No-Break">https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference</span></a>. <span id="x1-16001r16"></span></p>
</section>
<section id="wonders-of-the-deep-learning-age" class="level2 sectionHead" data-number="6.2">
<h2 class="sectionHead" data-number="6.2" id="sigil_toc_id_15"><span class="titlemark">1.2 </span> <span id="x1-170002"></span>Wonders of the deep learning age</h2>
<p>Over the last 10 to 15 years, we’ve seen a <span id="dx1-17001"></span>dramatic shift in the landscape of ML thanks to the enormous success of deep learning. Perhaps one of the most impressive feats of the universal impact of deep learning is that it has affected fields from medical imaging and manufacturing all the way through to tools for translation and content creation.</p>
<p>While deep learning has only seen great success over recent years, many of its core principles are already well established. Researchers have been working with neural networks for some time – in fact, one could argue that the first neural network was introduced by Frank Rosenblatt as early as 1957! This, of course, wasn’t as sophisticated as the models we have today, but it was an important component of these models: the perceptron, as shown in <em>Figure</em> <em>1.1</em>.</p>
<div class="IMG---Figure">
<img src="../media/perceptron_diagram.JPG" class="graphics" alt="PIC"/> <span id="x1-17002r1"></span> <span id="x1-17003"></span></div>
<p class="IMG---Caption">Figure 1.1: Diagram of a single perceptron 
</p>
<p>The 1980s saw the introduction of many now-familiar concepts, with the introduction of <strong>convolutional neural networks</strong> (<strong>CNNs</strong>) by <span id="dx1-17004"></span>Kunihiko Fukushima in 1980, and the development of the <strong>recurrent neural network</strong> (<strong>RNN</strong>) by John <span id="dx1-17005"></span>Hopfield in 1982. The 1980s and 1990s saw further maturation of these technologies: Yann LeCun famously applied back-propagation to create a CNN capable of recognizing hand-written digits in 1989, and the crucial concept of long short-term memory RNNs was introduced by Hochreiter and Schmidhuber in 1997.</p>
<p>But, while we had the foundation of today’s <span id="dx1-17006"></span>powerful models before the turn of the century, it wasn’t until the introduction of modern GPUs that the field really took off. With the introduction of accelerated training and inference afforded by GPUs, it became possible to develop networks with dozens (or even hundreds) of layers. This opened the door to incredibly sophisticated neural network architectures capable of learning compact feature representations of complex, high-dimensional data.</p>
<div class="IMG---Figure">
<img src="../media/file4.png" alt="PIC"/> <span id="x1-17007r2"></span> <span id="x1-17008"></span></div>
<p class="IMG---Caption">Figure 1.2: Diagram of AlexNet 
</p>
<p>One of the first highly influential network architectures was AlexNet. This network, developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, comprised 11 layers and was capable of classifying images into one of 1,000 possible classes. It achieved unprecedented performance on the ImageNet Large Scale Visual Recognition Challenge in 2012, illustrating the power of deep networks. AlexNet was the first of an array of influential neural network architectures, and the following years saw the introduction of many now-familiar architectures, including VGG Net, the Inception architectures, ResNet, EfficientNet, YOLO... The list goes on!</p>
<p>But NNs weren’t just successful in computer vision applications. In 2014, work by Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio demonstrated that end-to-end NN models could be used to obtain state-of-the-art results in machine translation. This was a watershed moment for the field, and large-scale machine translation services quickly adopted these end-to-end networks, spurring further advancements in natural language processing. Fast-forward to today, and these concepts have matured to <span id="dx1-17009"></span>produce the <strong>transformer</strong> architecture – an architecture that has had a dramatic effect on deep learning through its ability to learn rich feature embeddings through self-supervised learning.</p>
<p>With the impressive flexibility granted to them by the wide variety of architectures, neural networks have now achieved state-of-the-art performance in applications across almost every conceivable field, and they’re now a familiar part of our daily lives. Whether it’s the facial recognition we use on our mobile devices, translation services such as Google Translate, or speech recognition in our smart devices, it’s clear that these networks are not just competitive in image classification challenges, they’re now an important part of the technologies we’re developing, and they’re even capable of <em>outperforming</em> <em>humans</em>.</p>
<p>While reports of deep learning models outperforming human experts are becoming more and more frequent, the most profound examples are perhaps those in medical imaging. In 2020, a network developed by researchers at Imperial College London and Google Health outperformed six radiologists when detecting breast cancer from mammograms. A few months later, a study from February 2021 demonstrated that a deep learning model was able to outperform two human experts in diagnosing gallbladder disorders. Another study published later that year showed that a CNN outperformed 157 dermatologists in detecting melanoma from images of skin abnormalities.</p>
<p>All of the applications we’ve discussed so far have been supervised applications of ML, in which models have been trained for classification or regression problems. However, some of the most impressive feats of deep learning are found in other applications, including generative modeling and reinforcement learning. Perhaps one of the <span id="dx1-17010"></span>most famous examples of the latter is <strong>AlphaGo</strong>, a reinforcement learning model developed by DeepMind. The algorithm, as <span id="dx1-17011"></span>indicated in its name, was trained to play the game Go via reinforcement learning. Unlike some games, such as chess, which can be solved via fairly straightforward artificial intelligence methods, Go is far more challenging from a computational standpoint. This is due to the sophisticated nature of the game – the many possible combinations of moves are difficult for more traditional approaches. Thus, when AlphaGo successfully beat Go champions Fan Hui and Lee Sedol, in 2015 and 2016 respectively, this was big news.</p>
<p>DeepMind went on to further refine AlphaGo by creating a version which learned by playing games against itself – AlphaGo Zero. This model was superior to any previous model, achieving superhuman performance in Go. The algorithm at the core of its success, AlphaZero, went on to achieve superhuman performance in a range of other games, proving the algorithm’s ability to generalize to other applications.</p>
<p>Another significant milestone for deep learning over the last decade was the introduction of <strong>Generative Adversarial Networks</strong>, or <strong>GANs</strong>. GANs work by employing two networks. The goal of the first network is to generate data with the <span id="dx1-17012"></span>same statistical qualities as a training set. The goal of the second network is to classify the output of the first network, using what it has learned from the dataset. Because the first network is not trained directly on the data, it does not learn to simply replicate data – instead, it effectively learns to deceive the second network. This is why the term <em>adversarial</em> is used. Through this process, the first network is able to learn which kinds of outputs successfully deceive the second network, and thus is able to generate content that matches the data distribution.</p>
<p>GANs can produce particularly impressive outputs. For example, the following image was generated by the StyleGAN2 model:</p>
<div class="IMG---Figure">
<img src="../media/file5.jpg" alt="PIC"/> <span id="x1-17013r3"></span> <span id="x1-17014"></span></div>
<p class="IMG---Caption">Figure 1.3: Face generated by StyleGAN2 from thispersondoesnotexist.com. 
</p>
<p>But GANs aren’t just useful for generating realistic faces; they have practical applications in many other fields, such as suggesting molecular combinations for drug discovery. They are also a powerful tool for improving other ML methods through data augmentation – using the GAN-generated data to augment datasets.</p>
<p>All these successes may make deep <span id="dx1-17015"></span>learning seem infallible. While its achievements are impressive, they don’t tell the whole story. In the next section, we’ll learn about some of deep learning’s failings, and start to understand how Bayesian approaches may help to avoid these in the future. <span id="x1-17016r17"></span></p>
</section>
<section id="understanding-the-limitations-of-deep-learning" class="level2 sectionHead" data-number="6.3">
<h2 class="sectionHead" data-number="6.3" id="sigil_toc_id_16"><span class="titlemark">1.3 </span> <span id="x1-180003"></span>Understanding the limitations of deep learning</h2>
<p>As we’ve seen, deep learning has achieved some remarkable feats, and it’s undeniable that it’s revolutionizing the way that we deal with data and predictive modeling. But deep learning’s short history also comprises darker tales: stories that bring with them crucial lessons for developing systems that are more robust, and, crucially, safer.</p>
<p>In this section, we’ll introduce a couple of key cases in which deep learning failed, and we will discuss how a Bayesian perspective could have helped to produce a better outcome. <span id="x1-18001r1"></span></p>
<section id="bias-in-deep-learning-systems" class="level3 subsectionHead" data-number="6.3.1">
<h3 class="subsectionHead" data-number="6.3.1" id="sigil_toc_id_17"><span class="titlemark">1.3.1 </span> <span id="x1-190001"></span>Bias in deep learning systems</h3>
<p>We’ll start with a textbook example of <strong>bias</strong>, a crucial <span id="dx1-19001"></span>problem faced by data-driven methods. This example centers around Amazon. Now a household name, the e-commerce company started out by revolutionizing the world of book retail, before becoming literally <em>the</em> one-stop shop for just about anything: from garden furniture to a new laptop, or even a home security system, if you can imagine it, you can probably purchase it on Amazon. The company has also been responsible for significant strides technologically, often as a means of improving its infrastructure in order to enable its expansion. From hardware infrastructure to theoretical and technological leaps in optimization methods, what started out as an e-commerce organization has now become one of the key figures in technology.</p>
<p>While these technological leaps often set the standard for the industry, this example did the opposite: demonstrating a key weakness of data-driven methods. The case we’re referring to is that of Amazon’s AI recruiting software. With automation playing such a key role in so much of Amazon’s success, it made sense to expand this automation to reviewing resumes. In 2014, Amazon’s ML engineers deployed a tool to do just that. Trained on the previous 10 year’s worth of applicants, the tool was designed to learn to identify favorable traits from the company’s enormous pool of applicants. However, in 2015 it became clear that it had latched onto certain features that resulted in deeply undesirable behavior.</p>
<p>The issue was largely due to the underlying data: because of the nature of the tech industry at the time, Amazon’s dataset of resumes was dominated by male applicants. This resulted in tremendous inequity in the model’s predictions: it effectively learned to favor men, becoming hugely biased against female applicants. The discriminatory behavior of the model resulted in the project being abandoned by Amazon, and it now serves as a key example of bias for the AI community.</p>
<p>An important factor to consider in the <span id="dx1-19002"></span>problem presented here is that this bias isn’t just driven by <em>explicit</em> information, such as a person’s name (which could be a clue as to their gender): algorithms learn latent information, which can then drive bias. This means the problem can’t simply be solved by anonymizing people – it’s up to the engineers and scientists to ensure that bias is evaluated comprehensively so that the algorithms we deploy are fair. While Bayesian methods can’t make bias disappear, they present us with a range of <span id="dx1-19003"></span>tools that can help with these problems. As we’ll see later in the book, Bayesian methods give us the ability to determine whether data is in-distribution or <strong>out-of-distribution</strong> (<strong>OOD</strong>). In this case, Amazon could have used this capability of Bayesian methods: separating the OOD data and analyzing it to understand why it was OOD. Was it picking up on things that were relevant, such as applicants with the wrong kind of experience? Or was it picking up on something irrelevant and discriminatory, such as the applicant’s gender? This could have helped Amazon’s ML team to spot the undesirable behavior early, allowing them to develop an unbiased solution. <span id="x1-19004r22"></span></p>
</section>
<section id="the-danger-of-over-confident-predictions" class="level3 subsectionHead" data-number="6.3.2">
<h3 class="subsectionHead" data-number="6.3.2" id="sigil_toc_id_18"><span class="titlemark">1.3.2 </span> <span id="x1-200002"></span>The danger of over-confident predictions</h3>
<p>Another widely referenced example of a deep <span id="dx1-20001"></span>learning failure is illustrated in the paper <em>Robust Physical-World Attacks on Deep Learning Visual Classification</em> by Kevin Eykholt <em>et al.</em> ( <a href="https://arxiv.org/abs/1707.08945" class="url"><span class="No-Break">https://arxiv.org/abs/1707.08945</span></a>). This paper played an important role in highlighting the issue of <strong>adversarial attacks</strong> on deep <span id="dx1-20002"></span>learning models: slightly modifying input data so that the model produces an incorrect prediction. In one of the key examples from their paper, they stick white and black stickers to a stop sign. While the modifications to the sign were subtle, the computer vision model interpreted the modified sign as a Speed Limit 45 sign.</p>
<div class="IMG---Figure">
<img src="../media/adversarial_illustration.PNG" class="graphics" alt="PIC"/> <span id="x1-20003r4"></span> <span id="x1-20004"></span></div>
<p class="IMG---Caption">Figure 1.4: Illustration of the effect of a simple adversarial attack on a model interpreting a stop sign. 
</p>
<p>At first, this may seem inconsequential, but if we take a step back and consider the amount of work that Tesla, Uber, and others have dedicated towards self-driving cars, it’s easy to see how this sort of adversarial perturbation could lead to catastrophic consequences. In the case of this sign, this misclassification could lead to a self-driving car bypassing a stop sign, hurtling into traffic at an intersection. This would obviously not be good for the passengers or other road users. In fact, an incident not too dissimilar to what we’re describing here happened in 2016 when a Tesla Model S collided with a truck in northern Florida ( <a href="https://www.reuters.com/article/us-tesla-crash-idUSKBN19A2XC" class="url"><span class="No-Break">https://www.reuters.com/article/us-tesla-crash-idUSKBN19A2XC</span></a>). According to Tesla, the trailer being pulled by the truck wasn’t detected by Tesla’s autopilot as it couldn’t distinguish it from the backdrop of the bright sky behind the trailer. The driver also didn’t notice the trailer, ultimately resulting in a fatal collision. But what if the decision processes used by the autopilot were more sophisticated? One of the key themes throughout this book is that of making <em>robust</em> decisions with our ML systems, particularly in mission-critical or safety-critical applications.</p>
<p>While this traffic sign example provides an intuitive illustration of the dangers associated with misclassifications, this applies to a vast range of <span id="dx1-20005"></span>other scenarios, from robotics equipment used for manufacturing through to automated surgical procedures.</p>
<p>Having some idea of confidence (or uncertainty) is an important step towards improving the robustness of these systems and ensuring consistently safe behavior. In the case of the stop sign, simply having a model that ”knows when it doesn’t know” can prevent potentially tragic outcomes. As we’ll see later in the book, BDL methods allow us to detect adversarial inputs through their uncertainty estimates. In our self-driving car example, this could be incorporated in the logic so that, if the model is uncertain, the car safely comes to a stop, switching to manual mode to allow the driver to safely navigate the situation. This is the <em>wisdom</em> that comes with uncertainty-aware models: allowing us to design models that know their limitations, and thus are more robust in unexpected scenarios. <span id="x1-20006r23"></span></p>
</section>
<section id="shifting-trends" class="level3 subsectionHead" data-number="6.3.3">
<h3 class="subsectionHead" data-number="6.3.3" id="sigil_toc_id_19"><span class="titlemark">1.3.3 </span> <span id="x1-210003"></span>Shifting trends</h3>
<p>Our last examples look at the challenge of dealing with data that changes over time – a common problem in real-world applications. The first <span id="dx1-21001"></span>problem we’ll consider, typically referred to as <strong>dataset shift</strong> or <strong>covariate shift</strong>, occurs when the <span id="dx1-21002"></span>data encountered by a model at inference time changes relative to the data the model was trained on. This is often due to the dynamic nature of real-world problems and the fact that training sets – even very large training sets – rarely represent the total variation present in the phenomena they represent. An important example of this can be found in the paper <em>Systematic Review of Approaches to Preserve Machine Learning</em> <em>Performance in the Presence of Temporal Dataset Shift in Clinical Medicine</em>, in which Lin Lawrence Guo <em>et al.</em> highlight concerns around dataset shift ( <a href="https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8410238/" class="url"><span class="No-Break">https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8410238/</span></a>). Their work shows that there is relatively little literature on tackling issues related to dataset shift in ML models applied in clinical settings. This is problematic because clinical data is dynamic. Let’s consider an example.</p>
<p>In our example, we have a model that’s <span id="dx1-21003"></span>trained to automatically prescribe medication for a patient given their symptoms. A patient complains to a physician about respiratory symptoms, and the physician uses the model to prescribe medication. Because of the data presented to the model, it prescribes antibiotics. This works for many patients for a while, but over time something changes: a new disease becomes prevalent in the population. The new disease happens to have very similar symptoms to the bacterial infection that was going around previously, but this is caused by a virus. Because the model isn’t capable of adapting to dataset shift, it continues recommending antibiotics. Not only will they not help the patients, but it could contribute to antibiotic resistance within the local population.</p>
<p>In order to be robust to these shifts in real-world data, models need to be sensitive to dataset shift. One way to do this is through the use of Bayesian methods, which provide uncertainty estimates. Applying this to our automatic prescriber example, the model becomes sensitive to small changes in the data when capable of producing uncertainty estimates. For example, there may be subtle differences in symptoms, such as a different type of cough, associated with our new viral infection. This will cause the uncertainty associated with the model predictions to rise, indicating that the model needs to be updated with new data.</p>
<p>A related issue, referred to as <strong>catastrophic forgetting</strong>, is caused by <span id="dx1-21004"></span>models adapting to changes in data. Given our example, this sounds like a good thing: if models are adapting to changes in the data, then they’re always up to date, right? Unfortunately, it’s not quite so simple. Catastrophic forgetting occurs when models learn from new data, but ”forget” about past data in the process.</p>
<p>For example, say an ML algorithm is developed to identify fraudulent documents. It may work very well at first, but fraudsters will quickly notice that methods that used to fool automated document verification no longer work, so they develop new methods. While a few of these methods get through, the model – using its uncertainty estimates – notices that it needs to adapt to the new data. The model updates its dataset, focusing on the current popular attack methods, and runs a few more training iterations. Once again, it successfully thwarts the fraudsters, but, much to the surprise of the model’s designers, the model has started letting through older, less sophisticated attacks: attacks that used to be easy for it to identify.</p>
<p>In training on the new data, the model’s parameters have changed. Because there wasn’t sufficient support for the old data in the updated dataset, the model has lost information about old associations between the inputs (documents) and their classification (whether or not they’re fraudulent).</p>
<p>While this example used uncertainty estimates to tackle the issue of dataset shift, it could have further leveraged them to <span id="dx1-21005"></span>ensure that its dataset was balanced. This can be done using methods such as <strong>uncertainty sampling</strong>, which look to sample <span id="dx1-21006"></span>from uncertain regions, ensuring that the dataset used to train the model captures all available information from current and past data. <span id="x1-21007r21"></span></p>
</section>
</section>
<section id="core-topics" class="level2 sectionHead" data-number="6.4">
<h2 class="sectionHead" data-number="6.4" id="sigil_toc_id_20"><span class="titlemark">1.4 </span> <span id="x1-220004"></span>Core topics</h2>
<p>The aim of this book is to provide you with the tools and knowledge you need to develop your own BDL solutions. To this end, while we assume some familiarity with concepts of statistical learning and deep learning, we will still provide a refresher of these fundamental concepts.</p>
<p>In <a href="CH2.xhtml#x1-250002"><em>Chapter 2</em></a>, <a href="CH2.xhtml#x1-250002"><em>Fundamentals of Bayesian Inference</em></a>, we’ll go over some of the key concepts from Bayesian inference, including probabilities and model uncertainty estimates. In <a href="CH3.xhtml#x1-350003"><em>Chapter 3</em></a>, <a href="CH3.xhtml#x1-350003"><em>Fundamentals of Deep Learning</em></a>, we’ll cover important key aspects of deep learning, including learning via backpropagation, and popular varieties of NNs. With these fundamentals covered, we’ll start to explore BDL in <a href="CH4.xhtml#x1-490004"><em>Chapter 4</em></a>, <a href="CH4.xhtml#x1-490004"><em>Introducing Bayesian Deep Learning</em></a>. In <em>Chapters 5</em> and <em>6</em> we’ll delve deeper into BDL; we’ll first learn about principled methods, before going on to understand more practical methods for approximating Bayesian neural networks.</p>
<p>In <a href="CH7.xhtml#x1-1130007"><em>Chapter 7</em></a>, <a href="CH7.xhtml#x1-1130007"><em>Practical Considerations for Bayesian Deep Learning</em></a>, we’ll explore some practical considerations for BDL, helping us to understand how best to apply these methods to real-world problems. By <a href="CH8.xhtml#x1-1320008"><em>Chapter 8</em></a>, <a href="CH8.xhtml#x1-1320008"><em>Applying Bayesian</em> <em>Deep Learning</em></a>, we should have a strong understanding of the core BDL methods, and we’ll cement this with a number of practical examples. Finally, <a href="CH9.xhtml#x1-1780009"><em>Chapter 9</em></a>, <a href="CH9.xhtml#x1-1780009"><em>Next Steps in Bayesian Deep Learning</em></a> will provide an overview of the current challenges within the field of BDL and give you an idea of where the technology is headed.</p>
<p>Throughout most of the book, the theory will be accompanied by hands-on examples, allowing you to develop a strong understanding by implementing these methods yourself. In order to follow these coding examples, you will need to have a Python environment set up with the necessary prerequisites. We’ll go over these in the next section. <span id="x1-22001r26"></span></p>
</section>
<section id="setting-up-the-work-environment" class="level2 sectionHead" data-number="6.5">
<h2 class="sectionHead" data-number="6.5" id="sigil_toc_id_21"><span class="titlemark">1.5 </span> <span id="x1-230005"></span>Setting up the work environment</h2>
<p>To complete the practical elements of the book, you’ll need a Python 3.9 environment with the necessary prerequisites. We recommend using <span class="obeylines-h"><span class="verb"><code>conda</code></span></span>, a Python package manager specifically designed for scientific computing applications. To install <span class="obeylines-h"><span class="verb"><code>conda</code></span></span>, simply head to <a href="https://conda.io/projects/conda/en/latest/user-guide/install/index.html" class="url"><span class="No-Break">https://conda.io/projects/conda/en/latest/user-guide/install/index.html</span></a> and follow the instructions for your operating system.</p>
<p>With <span class="obeylines-h"><span class="verb"><code>conda</code></span></span> installed, you can set up the <span class="obeylines-h"><span class="verb"><code>conda</code></span></span> environment that you’ll use for the book:</p>
<pre id="fancyvrb3" class="fancyvrb"><span id="x1-23003r1"></span> 
<code><span>conda</span><span> create</span><span> -n</span><span> bdl</span><span> </span><span id="textcolor27"><span>python</span></span><span id="textcolor28"><span>=</span></span><span id="textcolor29"><span>3</span></span><span>.9</span></code></pre>
<p>When you hit <em>Enter</em> to execute this command, you’ll be asked if you wish to continue installing the required packages; simply type <code>y</code> and hit <strong>Enter</strong>. <span class="obeylines-h"><span class="verb"><code>conda</code></span></span> will now proceed to install the core packages.</p>
<p>You can now activate your environment by typing the following:</p>
<pre id="fancyvrb4" class="fancyvrb"><span id="x1-23007r1"></span> 
<code><span>conda</span><span> activate</span><span> bdl</span></code></pre>
<p>You’ll now see that your shell prompt contains <code>bdl</code>, indicating that your <span class="obeylines-h"><span class="verb"><code>conda</code></span></span> environment is active. Now you’re ready to install the prerequisites for the book. The key libraries required for the book are as follows:</p>
<ul>
<li><p><strong>NumPy</strong>: Numerical Python, or NumPy, is the core package for numerical programming in Python. You’re likely very familiar with this already.</p></li>
<li><p><strong>SciPy</strong>: SciPy, or Scientific Python, provides the fundamental packages for scientific computing applications. The full scientific computing stack comprising SciPy, matplotlib, NumPy, and other libraries, is often referred to as the SciPy stack.</p></li>
<li><p><strong>scikit-learn</strong>: This is the core Python machine learning library. Built on the SciPy stack, it provides easy-to-use implementations of many popular ML methods. It also provides a substantial number of helper classes and functions for data loading and processing, which we’ll use throughout the book.</p></li>
<li><p><strong>TensorFlow</strong>: TensorFlow, along with PyTorch and JAX, is one of the popular Python deep learning frameworks. It provides the tools necessary for developing deep learning models, and it will provide the foundation for many of the programming examples throughout the book.</p></li>
<li><p><strong>TensorFlow Probability</strong>: Built on TensorFlow, TensorFlow Probability provides the tools necessary for working with probabilistic neural networks. We’ll be using this along with TensorFlow for many of the Bayesian neural network examples.</p></li>
</ul>
<p>To install the full list of dependencies required for the book, with your <span class="obeylines-h"><span class="verb"><code>conda</code></span></span> environment activated, enter the following:</p>
<pre id="fancyvrb5" class="fancyvrb"><span id="x1-23012r1"></span> 
<code><span>conda</span><span> install</span><span> -c</span><span> conda-forge</span><span> scipy</span><span> sklearn</span><span> matplotlib</span><span> seaborn</span> <span id="x1-23014r2"></span> </code>
<code><span>tensorflow</span><span> tensorflow-probability</span></code></pre>
<p>Let’s summarize what we have learned. <span id="x1-23015r27"></span></p>
</section>
<section id="summary" class="level2 sectionHead" data-number="6.6">
<h2 class="sectionHead" data-number="6.6" id="sigil_toc_id_22"><span class="titlemark">1.6 </span> <span id="x1-240006"></span>Summary</h2>
<p>In this chapter, we’ve revisited the successes of deep learning, renewing our understanding of its enormous potential, and its ubiquity within today’s technology. We’ve also explored some key examples of its shortcomings: scenarios in which deep learning has failed us, demonstrating the potential for catastrophic consequences. While BDL can’t eliminate these risks, it can allow us to build more robust ML systems that incorporate both the flexibility of deep learning and the caution of Bayesian inference.</p>
<p>In the next chapter, we’ll dive deeper into the latter as we cover some of the core concepts of Bayesian inference and probability, in preparation for our foray into BDL. <span id="x1-24001r15"></span></p>
</section>
</section>
</body>
</html>