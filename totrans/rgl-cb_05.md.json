["```py\nfrom hashlib import md5\nprint('hashing of \"regularization\" ->',\n    md5(b'regularization').hexdigest())\nprint('hashing of \"regularized\" ->',\n    md5(b'regularized').hexdigest())\nprint('hashing of \"machine learning\" ->',\n    md5(b'machine learning').hexdigest())\n```", "```py\nhashing of \"regularization\" -> 04ef847b5e35b165c190ced9d91f65da\nhashing of \"regularized\" -> bb02c45d3c38892065ff71198e8d2f89\nhashing of \"machine learning\" -> e04d1bcee667afb8622501b9a4b4654d\n```", "```py\n    pip install kaggle\n    ```", "```py\n    mkdir ~/.kaggle && mv kaggle.json ~/.kaggle/.\n    ```", "```py\n    kaggle datasets download -d reddynitin/aug-train\n    ```", "```py\n    pip install category_encoders pandas scikit-learn.\n    ```", "```py\n    Import numpy as np\n    ```", "```py\n    import pandas as pd\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    from sklearn.preprocessing import StandardScaler, OneHotEncoder\n    ```", "```py\n    from category_encoders.hashing import HashingEncoder\n    ```", "```py\n    from sklearn.linear_model import LogisticRegression\n    ```", "```py\n    df = pd.read_csv('aug-train.zip')\n    ```", "```py\n    print('number of unique values for the feature city',\n    ```", "```py\n        df['city'].nunique())\n    ```", "```py\nnumber of unique values for the feature city 123\n```", "```py\n    df = df.drop(columns=['gender', 'major_discipline',\n    ```", "```py\n        'company_size', 'company_type'])\n    ```", "```py\n    df = df.dropna()\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        df.drop(columns=['target']), df['target'],\n    ```", "```py\n        stratify=df['target'], test_size=0.2,\n    ```", "```py\n        random_state=0\n    ```", "```py\n    )\n    ```", "```py\n    quanti_feats = ['city_development_index', 'training_hours']\n    ```", "```py\n    # Instantiate the scaler\n    ```", "```py\n    scaler = StandardScaler()\n    ```", "```py\n    # Select quantitative features\n    ```", "```py\n    X_train_quanti = X_train[quanti_feats]\n    ```", "```py\n    X_test_quanti = X_test[quanti_feats]\n    ```", "```py\n    # Rescale quantitative features\n    ```", "```py\n    X_train_quanti = scaler.fit_transform(X_train_quanti)\n    ```", "```py\n    X_test_quanti = scaler.transform(X_test_quanti)\n    ```", "```py\n    quali_feats = ['relevent_experience',\n    ```", "```py\n        'enrolled_university', 'education_level',\n    ```", "```py\n        'experience', 'last_new_job']\n    ```", "```py\n    quali_feats = ['last_new_job']\n    ```", "```py\n    # Instantiate the one hot encoder\n    ```", "```py\n    encoder = OneHotEncoder()\n    ```", "```py\n    # Select qualitative features to one hot encode\n    ```", "```py\n    X_train_quali = X_train[quali_feats]\n    ```", "```py\n    X_test_quali = X_test[quali_feats]\n    ```", "```py\n    # Encode those features\n    ```", "```py\n    X_train_quali = encoder.fit_transform(\n    ```", "```py\n        X_train_quali).toarray()\n    ```", "```py\n    X_test_quali = encoder.transform(\n    ```", "```py\n        X_test_quali).toarray()\n    ```", "```py\n    high_cardinality_feature = ['city']\n    ```", "```py\n    # Instantiate the hashing encoder\n    ```", "```py\n    hasher = HashingEncoder(n_components=7)\n    ```", "```py\n    # Encode the city feature with hashing\n    ```", "```py\n    X_train_hash = hasher.fit_transform(\n    ```", "```py\n        X_train[high_cardinality_feature])\n    ```", "```py\n    X_test_hash = hasher.fit_transform(\n    ```", "```py\n        X_test[high_cardinality_feature])\n    ```", "```py\n    # Display the result on the training set\n    ```", "```py\n    X_train_hash.head()\n    ```", "```py\n  col_0    col_1    col_2    col_3    col_4    col_5    col_6\n18031     1       0         0       0       0         0              0\n16295     0       0         0       1        0         0              0\n7679      0       0         0        0       0         1              0\n18154     0       0         1        0        0        0              0\n10843     0       0         0        0        0        1              0\n```", "```py\n    X_train = np.concatenate([X_train_quali,\n    ```", "```py\n        X_train_quanti, X_train_hash], 1)\n    ```", "```py\n    X_test = np.concatenate([X_test_quali,\n    ```", "```py\n        X_test_quanti, X_test_hash], 1)\n    ```", "```py\n    lr = LogisticRegression()\n    ```", "```py\n    lr.fit(X_train, y_train)\n    ```", "```py\n    print('Accuracy train set:', lr.score(X_train,\n    ```", "```py\n        y_train))\n    ```", "```py\n    print('Accuracy test set:', lr.score(X_test, y_test))\n    ```", "```py\nAccuracy train set: 0.7812087988342239\nAccuracy test set: 0.7826810990840966\n```", "```py\nkaggle datasets download -d reddynitin/aug-train\n```", "```py\npip install pandas scikit-learn.\n```", "```py\n    Import numpy as np\n    ```", "```py\n    import pandas as pd\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    from sklearn.preprocessing import OneHotEncoder, StandardScaler\n    ```", "```py\n    from sklearn.linear_model import LogisticRegression\n    ```", "```py\n    df = pd.read_csv('aug-train.zip')\n    ```", "```py\n    df = df.drop(columns=['gender', 'major_discipline',\n    ```", "```py\n        'company_size', 'company_type'])\n    ```", "```py\n    df = df.dropna()\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        df.drop(columns=['target']), df['target'],\n    ```", "```py\n        stratify=df['target'], test_size=0.2,\n    ```", "```py\n        random_state=0\n    ```", "```py\n    )\n    ```", "```py\n    quanti_feats = ['city_development_index',\n    ```", "```py\n        'training_hours']\n    ```", "```py\n    scaler = StandardScaler()\n    ```", "```py\n    X_train_quanti = X_train[quanti_feats]\n    ```", "```py\n    X_test_quanti = X_test[quanti_feats]\n    ```", "```py\n    X_train_quanti = scaler.fit_transform(X_train_quanti)\n    ```", "```py\n    X_test_quanti = scaler.transform(X_test_quanti)\n    ```", "```py\n    # Get only cities above threshold\n    ```", "```py\n    threshold = 0.1\n    ```", "```py\n    kept_cities = X_train['city'].value_counts(\n    ```", "```py\n        normalize=True)[X_train['city'].value_counts(\n    ```", "```py\n        normalize=True) > threshold].index\n    ```", "```py\n    # Update all cities below threshold as 'other'\n    ```", "```py\n    X_train.loc[~X_train['city'].isin(kept_cities),\n    ```", "```py\n        'city'] = 'other'\n    ```", "```py\n    X_test.loc[~X_test['city'].isin(kept_cities),\n    ```", "```py\n        'city'] = 'other'\n    ```", "```py\n    # Get qualitative features\n    ```", "```py\n    quali_feats = ['city', 'relevent_experience',\n    ```", "```py\n        'enrolled_university', 'education_level',\n    ```", "```py\n        'experience', 'last_new_job']\n    ```", "```py\n    X_train_quali = X_train[quali_feats]\n    ```", "```py\n    X_test_quali = X_test[quali_feats]\n    ```", "```py\n    # Instantiate the one hot encoder\n    ```", "```py\n    encoder = OneHotEncoder()\n    ```", "```py\n    # Apply one hot encoding\n    ```", "```py\n    X_train_quali = encoder.fit_transform(\n    ```", "```py\n        X_train_quali).toarray()\n    ```", "```py\n    X_test_quali = encoder.transform(\n    ```", "```py\n        X_test_quali).toarray()\n    ```", "```py\n    X_train = np.concatenate([X_train_quali,\n    ```", "```py\n        X_train_quanti], 1)\n    ```", "```py\n    X_test = np.concatenate([X_test_quali, X_test_quanti], 1)\n    ```", "```py\n    lr = LogisticRegression()\n    ```", "```py\n    lr.fit(X_train, y_train)\n    ```", "```py\n    print('Accuracy train set:', lr.score(X_train, y_train))\n    ```", "```py\n    print('Accuracy test set:', lr.score(X_test, y_test))\n    ```", "```py\nAccuracy train set: 0.7805842759003538\nAccuracy test set: 0.774909797391063\n```", "```py\ndf['city'].value_counts(normalize=True)\n```", "```py\ncity_103         0.232819\ncity_21           0.136227\ncity_16           0.081659\ncity_114         0.069613\ncity_160         0.045354\n                                  ...\ncity_111         0.000167\ncity_129         0.000111\ncity_8              0.000111\ncity_140         0.000056\ncity_171         0.000056\nName: city, Length: 123, dtype: float64\n```", "```py\ndf['city'].value_counts(normalize=True) > 0.05\n```", "```py\ncity_103           True\ncity_21              True\ncity_16              True\ncity_114           True\ncity_160         False\n                             ...\ncity_111         False\ncity_129         False\ncity_8              False\ncity_140         False\ncity_171         False\nName: city, Length: 123, dtype: bool\n```", "```py\nkept_cities = df['city'].value_counts(normalize=True)[\n    df['city'].value_counts(normalize=True) > 0.05].index\nkept_cities\n```", "```py\nIndex(['city_103', 'city_21', 'city_16', 'city_114'], dtype='object')\n```", "```py\nkaggle datasets download -d mlg-ulb/creditcardfraud\n```", "```py\npip install pandas scikit-learn matplotlib imbalanced-learn\n```", "```py\n        import pandas as pd\n        ```", "```py\n        import matplotlib.pyplot as plt\n        ```", "```py\n        from sklearn.model_selection import train_test_split\n        ```", "```py\n        from sklearn.preprocessing import StandardScaler\n        ```", "```py\n        from imblearn.under_sampling import RandomUnderSampler\n        ```", "```py\n        from sklearn.linear_model import LogisticRegression\n        ```", "```py\n        from sklearn.metrics import roc_auc_score\n        ```", "```py\n    df = pd.read_csv('creditcardfraud.zip')\n    ```", "```py\n    df['Class'].value_counts(normalize=True)\n    ```", "```py\n0         0.998273\n1         0.001727\nName: Class, dtype: float64\n```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        df.drop(columns=['Class']), df['Class'],\n    ```", "```py\n        test_size=0.2, random_state=0,\n    ```", "```py\n        stratify=df['Class'])\n    ```", "```py\n    # Instantiate the object with a 10% strategy\n    ```", "```py\n    rus = RandomUnderSampler(sampling_strategy=0.1,\n    ```", "```py\n        random_state=0)\n    ```", "```py\n    # Undersample the train dataset\n    ```", "```py\n    X_train, y_train = rus.fit_resample(X_train, y_train)\n    ```", "```py\n    # Check the balance\n    ```", "```py\n    y_train.value_counts()\n    ```", "```py\n0         3940\n1           394\nName: Class, dtype: int64\n```", "```py\n    # Scale the data\n    ```", "```py\n    scaler = StandardScaler()\n    ```", "```py\n    X_train = scaler.fit_transform(X_train)\n    ```", "```py\n    X_test = scaler.transform(X_test)\n    ```", "```py\n    lr = LogisticRegression()\n    ```", "```py\n    lr.fit(X_train, y_train)\n    ```", "```py\n    # Get the probas\n    ```", "```py\n    y_train_proba = lr.predict_proba(X_train)[:, 1]\n    ```", "```py\n    y_test_proba = lr.predict_proba(X_test)[:, 1]\n    ```", "```py\n    # Display the ROC AUC\n    ```", "```py\n    print('ROC AUC training set:', roc_auc_score(y_train,\n    ```", "```py\n        y_train_proba))\n    ```", "```py\n    print('ROC AUC test set:', roc_auc_score(y_test,\n    ```", "```py\n        y_test_proba))\n    ```", "```py\nROC AUC training set: 0.9875041871730784\nROC AUC test set: 0.9731067071595099\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\n# Display the ROC curve\nfpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\nfpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)\nplt.plot(fpr_test, tpr_test, label='test')\nplt.plot(fpr_train, tpr_train, label='train')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend()\nplt.show()\n```", "```py\nkaggle datasets download -d mlg-ulb/creditcardfraud\n```", "```py\npip install pandas scikit-learn matplotlib imbalanced-learn.\n```", "```py\n        import pandas as pd\n        ```", "```py\n        import matplotlib.pyplot as plt\n        ```", "```py\n        from sklearn.model_selection import train_test_split\n        ```", "```py\n        from sklearn.preprocessing import StandardScaler\n        ```", "```py\n        from imblearn.over_sampling import RandomOverSampler\n        ```", "```py\n        from sklearn.linear_model import LogisticRegression\n        ```", "```py\n        from sklearn.metrics import roc_auc_score\n        ```", "```py\n    df = pd.read_csv('creditcardfraud.zip')\n    ```", "```py\n    df['Class'].value_counts(normalize=True)\n    ```", "```py\n0         0.998273\n1         0.001727\nName: Class, dtype: float64\n```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        df.drop(columns=['Class']), df['Class'],\n    ```", "```py\n        test_size=0.2, random_state=0,\n    ```", "```py\n        stratify=df['Class'])\n    ```", "```py\n    # Instantiate the oversampler with a 10% strategy\n    ```", "```py\n    ros = RandomOverSampler(sampling_strategy=0.1,\n    ```", "```py\n        random_state=0)\n    ```", "```py\n    # Overersample the train dataset\n    ```", "```py\n    X_train, y_train = ros.fit_resample(X_train, y_train)\n    ```", "```py\n    # Check the balance\n    ```", "```py\n    y_train.value_counts()\n    ```", "```py\n0         227451\n1           22745\nName: Class, dtype: int64\n```", "```py\n    # Scale the data\n    ```", "```py\n    scaler = StandardScaler()\n    ```", "```py\n    X_train = scaler.fit_transform(X_train)\n    ```", "```py\n    X_test = scaler.transform(X_test)\n    ```", "```py\n    lr = LogisticRegression()\n    ```", "```py\n    lr.fit(X_train, y_train)\n    ```", "```py\n    # Get the probas\n    ```", "```py\n    y_train_proba = lr.predict_proba(X_train)[:, 1]\n    ```", "```py\n    y_test_proba = lr.predict_proba(X_test)[:, 1]\n    ```", "```py\n    # Display the ROC AUC\n    ```", "```py\n    print('ROC AUC training set:', roc_auc_score(y_train,\n    ```", "```py\n         y_train_proba))\n    ```", "```py\n    print('ROC AUC test set:', roc_auc_score(y_test,\n    ```", "```py\n        y_test_proba))\n    ```", "```py\nROC AUC training set: 0.9884952360756659\nROC AUC test set: 0.9721115830969416\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\n# Display the ROC curve\nfpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\nfpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)\nplt.plot(fpr_test, tpr_test, label='test')\nplt.plot(fpr_train, tpr_train, label='train')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend()\nplt.show()\n```", "```py\nkaggle datasets download -d mlg-ulb/creditcardfraud\n```", "```py\npip install pandas scikit-learn matplotlib imbalanced-learn.\n```", "```py\n        import pandas as pd\n        ```", "```py\n        import matplotlib.pyplot as plt\n        ```", "```py\n        from sklearn.model_selection import train_test_split\n        ```", "```py\n        from sklearn.preprocessing import StandardScaler\n        ```", "```py\n        from imblearn.over_sampling import SMOTE\n        ```", "```py\n        from sklearn.linear_model import LogisticRegression\n        ```", "```py\n        from sklearn.metrics import roc_auc_score\n        ```", "```py\n    df = pd.read_csv('creditcardfraud.zip')\n    ```", "```py\n    df['Class'].value_counts(normalize=True)\n    ```", "```py\n0         0.998273\n1         0.001727\nName: Class, dtype: float64\n```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        df.drop(columns=['Class']), df['Class'],\n    ```", "```py\n        test_size=0.2, random_state=0,\n    ```", "```py\n        stratify=df['Class'])\n    ```", "```py\n    # Instantiate the SLOT with a 10% strategy\n    ```", "```py\n    smote = SMOTE(sampling_strategy=0.1, random_state=0)\n    ```", "```py\n    # Overersample the train dataset\n    ```", "```py\n    X_train, y_train = smote.fit_resample(X_train,\n    ```", "```py\n        y_train)\n    ```", "```py\n    # Check the balance\n    ```", "```py\n    y_train.value_counts()\n    ```", "```py\n0         227451\n1           22745\nName: Class, dtype: int64\n```", "```py\n    # Scale the data\n    ```", "```py\n    scaler = StandardScaler()\n    ```", "```py\n    X_train = scaler.fit_transform(X_train)\n    ```", "```py\n    X_test = scaler.transform(X_test)\n    ```", "```py\n    lr = LogisticRegression()\n    ```", "```py\n    lr.fit(X_train, y_train)\n    ```", "```py\n    # Get the probas\n    ```", "```py\n    y_train_proba = lr.predict_proba(X_train)[:, 1]\n    ```", "```py\n    y_test_proba = lr.predict_proba(X_test)[:, 1]\n    ```", "```py\n    # Display the ROC AUC\n    ```", "```py\n    print('ROC AUC training set:', roc_auc_score(y_train,\n    ```", "```py\n        y_train_proba))\n    ```", "```py\n    print('ROC AUC test set:', roc_auc_score(y_test,\n    ```", "```py\n        y_test_proba))\n    ```", "```py\nROC AUC training set: 0.9968657635906649\nROC AUC test set: 0.9711737923925902\n```", "```py\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_curve\n# Display the ROC curve\nfpr_test, tpr_test, _ = roc_curve(y_test, y_test_proba)\nfpr_train, tpr_train, _ = roc_curve(y_train, y_train_proba)\nplt.plot(fpr_test, tpr_test, label='test')\nplt.plot(fpr_train, tpr_train, label='train')\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.legend()\nplt.show()\n```"]