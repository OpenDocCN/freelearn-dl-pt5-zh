<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Deep Autoencoders
                </header>
            
            <article>
                
<p class="mce-root">This chapter introduces the concept of deep belief networks and the significance of this type of deep unsupervised learning. It explains such concepts by introducing deep autoencoders along with two regularization techniques that can help create robust models. These regularization techniques, batch normalization and dropout, have been known to facilitate the learning of deep models and have been widely adopted. We will demonstrate the power of a deep autoencoder on MNIST and on a much harder dataset known as CIFAR-10, which contains color images.</p>
<p>By the end of this chapter, you will appreciate the benefits of making deep belief networks by observing the ease of modeling and quality of the output that they provide. You will be able to implement your own deep autoencoder and prove to yourself that deeper models are better than shallow models for most tasks. You will become familiar with batch normalization and dropout strategies for optimizing models and maximizing performance.</p>
<p>This chapter is organized as follows:</p>
<ul>
<li>Introducing deep belief networks</li>
<li>Making deep autoencoders </li>
<li>Exploring latent spaces with deep autoencoders</li>
</ul>
<h1 id="uuid-3d43aa1f-7d2b-4bb9-bb56-44e5748fe426">Introducing deep belief networks</h1>
<p>In machine learning, there is a field that is often discussed when talking about <strong>deep learning</strong> (<strong>DL</strong>), called <strong>deep belief networks</strong> (<strong>DBNs</strong>) (<span>Sutskever, I., and Hinton, G. E. (2008))</span>. Generally speaking, this term is used also for a type of machine learning model based on graphs, such as the well-known <strong>Restricted Boltzmann Machine</strong>. However, DBNs are usually regarded as part of the DL family, with deep autoencoders as one of the most notable members of that family. </p>
<p class="mce-root"/>
<p>Deep autoencoders are considered DBNs in the sense that there are latent variables that are only visible to single layers in the forward direction. These layers are usually many in number compared to autoencoders with a single pair of layers. One of the main tenets of DL and DBNs in general is that during the learning process, there is different knowledge represented across different sets of layers. This knowledge representation is learned by <em>feature learning</em> without a bias toward a specific class or label. Furthermore, it has been demonstrated that such knowledge appears to be hierarchical. Consider images, for example; usually, layers closer to the input layer learn features that are of low order (that is, edges), while deeper layers learn higher-order features, that is, well-defined shapes, patterns, or objects (<span>Sainath, T. N., et.al. (2012))</span>. </p>
<p>In DBNs, as in most DL models, the interpretability of the feature space can be difficult. Usually, looking at the weights of the first layer can offer information about the features learned and or the looks of the feature maps; however, due to high non-linearities in the deeper layers, interpretability of the feature maps has been a problem and careful considerations need to be made (<span>Wu, K., </span>et.al.<span> (2016))</span>. Nonetheless, despite this, DBNs are showing excellent results in feature learning. In the next few sections, we will cover deeper versions of autoencoders on highly complex datasets. We will be introducing a couple of new types of layers into the mix to demonstrate how deep a model can be.</p>
<h1 id="uuid-30315061-3136-47d3-b4c9-99058f78cfaf">Making deep autoencoders</h1>
<p>An autoencoder can be called <em>deep</em> so long as it has more than one pair of layers (an encoding one and a decoding one). Stacking layers on top of each other in an autoencoder is a good strategy to improve its power for feature learning in finding unique latent spaces that can be highly discriminatory in classification or regression applications. However, in <a href="480521d9-845c-4c0a-b82b-be5f15da0171.xhtml">Chapter 7</a>, <em>Autoencoders</em>, we covered how to stack layers onto an autoencoder, and we will do that again, but this time we will use a couple of new types of layers that are beyond the dense layers we have been using. These are the <strong>batch normalization</strong> and <strong>dropout </strong>layers. </p>
<p>There are no neurons in these layers; however, they act as mechanisms that have very specific purposes during the learning process that can lead to more successful outcomes by means of preventing overfitting or reducing numerical instabilities. Let's talk about each of these and then we will continue to experiment with both of these on a couple of important datasets.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<h2 id="uuid-48fd4d7f-7e15-4125-a333-71df1e1c69c5">Batch normalization</h2>
<p>Batch normalization has been an integral part of DL since it was introduced in 2015 (Ioffe, S., and Szegedy, C. (2015)). It has been a major game-changer because it has a couple of nice properties:</p>
<ul>
<li>It can prevent the problem known as <strong>vanishing gradient</strong> or <strong>exploding gradient</strong>, which is very common in recurrent networks (Hochreiter, S. (1998)).</li>
<li>It can lead to faster training by acting as a regularizer to the learning model (Van Laarhoven, T. (2017)).</li>
</ul>
<p>A summary of these properties and the block image we will use to denote batch normalization are shown in <em>Figure 8.1</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/37f526d5-f4cd-42bf-b81a-c1f884b173bb.png" style="width:29.58em;height:8.00em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.1 – Batch normalization layer main properties</div>
<p>The authors of <em>batch norm</em>, as it is often called by data scientists, introduced this simple mechanism to accelerate training or model convergence by providing stability to the calculation of gradients and how they affect the update of the weights across different layers of neurons. This is because they can prevent gradient vanishing or explosion, which is a natural consequence of gradient-based optimization operating on DL models. That is, the deeper the model is, the way the gradient affects the layers and individual units in deeper layers can have the effect of large updates or very small updates that can lead to variable overflow or numerical-zero values.</p>
<p class="mce-root"/>
<p>As illustrated at the top of <em>Figure 8.2</em>, batch normalization has the ability to regulate the boundaries of the input data by normalizing the data that goes in so that the output follows a normal distribution. The bottom of the figure illustrates where batch normalization is applied, that is, within the neuron right before sending the output to the next layer:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a4f3dd44-a3c2-4505-8287-996983c125dc.png" style="width:35.67em;height:28.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.2 – Batch normalization on a simple autoencoder</div>
<p>Consider having a (mini-)batch of data, <img class="fm-editor-equation" src="assets/4e0de339-6188-4267-a7d6-f8310f64141f.png" style="width:0.67em;height:0.75em;"/>, of size <img class="fm-editor-equation" src="assets/f575e97f-0ad0-4d83-b281-0f35baff870a.png" style="width:0.67em;height:0.75em;"/>, which allows us to define the following equations. First, the mean of the batch, at layer <img class="fm-editor-equation" src="assets/e5cd3a66-efd6-4e6f-9fce-a8fdd940b0bd.png" style="width:0.42em;height:1.08em;"/>, can be calculated as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/30413e7c-0c75-4ebe-9c23-f8005cae11eb.png" style="width:8.33em;height:3.92em;"/></p>
<p>The corresponding standard deviation is calculated as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/cdb6c81f-6cb2-4c00-9901-f0389a4bc6c1.png" style="width:11.67em;height:3.50em;"/>.</p>
<p class="mce-root">Then, we can normalize every unit <img class="fm-editor-equation" src="assets/76f8290e-3700-4064-9e45-9d2ee62a754e.png" style="width:0.50em;height:1.25em;"/> in layer <img style="font-size: 1em;width:0.42em;height:1.08em;" class="fm-editor-equation" src="assets/cb840e84-cf93-45a2-a7b2-29bb2eae0783.png"/><span> as follows:</span></p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/a0fa2623-dd01-4b71-88d1-5d93f35557dc.png" style="width:10.67em;height:5.33em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/1227909d-af4a-4fac-b9ec-397992525707.png" style="width:4.58em;height:0.92em;"/> is a constant introduced merely for numerical stability, but can be altered as needed. Finally, the normalized neural output of <span>every unit </span><img class="fm-editor-equation" src="assets/90c9d851-81ed-4cda-8c3e-7f34a7552b23.png" style="width:0.50em;height:1.25em;"/><span> in layer </span><img class="fm-editor-equation" src="assets/474e8170-88cb-4656-9737-d964b318a040.png" style="width:0.42em;height:1.08em;"/>,<span> <img src="assets/db17d453-c78e-44b6-9c54-265c14cb0674.png" style="width:1.92em;height:1.58em;"/></span>, can be calculated before it goes into the activation function as follows:</p>
<p class="mce-root CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/3443d3f0-ccf0-422c-81a3-f8af404ca4c8.png" style="width:13.50em;height:1.75em;"/></p>
<p>Here, <img class="fm-editor-equation" src="assets/4cb094b3-7c28-451f-bce4-1cd64bdd67e1.png" style="width:0.58em;height:0.92em;"/> and <img class="fm-editor-equation" src="assets/1ac6ba44-eb6b-43d4-aac8-e42df3047cea.png" style="width:0.50em;height:0.92em;"/> are parameters that need to be learned for each neural unit. After this, any choice of activation function at <span>unit </span><img class="fm-editor-equation" src="assets/899c2cb8-2ba1-4548-9d22-37a526fb0f50.png" style="width:0.50em;height:1.25em;"/><span> in layer </span><img class="fm-editor-equation" src="assets/6e5c045c-0614-444b-acd3-7110b61ada78.png" style="width:0.42em;height:1.08em;"/> will receive the normalized input, <sub><img class="fm-editor-equation" src="assets/c8199dba-4ba0-4160-99f1-f26f0912be69.png" style="width:3.75em;height:1.33em;"/></sub>, and produce an output that is optimally normalized to minimize the loss function.</p>
<p><span>One easy way to look at the benefits is to imagine the normalization process: although it occurs at each unit, the learning process itself determines the best normalization that is required to maximize the performance of the model (loss minimization). Therefore, it has the capability to nullify the effects of the normalization if it is not necessary for some feature or latent spaces, or it can also use the normalization effects. The important point to remember is that, when batch normalization is used, the learning algorithm will learn to use normalization optimally.</span></p>
<p>We can use <kbd>tensorflow.keras.layers.BatchNormalization</kbd> to create a batch normalization layer as follows:</p>
<pre>from tensorflow.keras.layers import BatchNormalization<br/>...<br/>bn_layer = BatchNormalization()(prev_layer)<br/>...</pre>
<p>This is obviously done using the functional paradigm. Consider the following example of a dataset corresponding to movie reviews, called <em>IMDb</em> (Maas, A. L., et al. (2011)), which we will explain in more detail in <a href="a6e892c5-e890-4c0a-ad92-c5442328a64a.xhtml">Chapter 13</a>, <em>Recurrent Neural Networks</em>. In this example, we are simply trying to prove the effects of adding a batch normalization layer as opposed to not having one. Take a close look at the following code fragment:</p>
<pre>from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import Dense, Activation, Input<br/>from tensorflow.keras.layers import <strong>BatchNormalization</strong><br/>from keras.datasets import imdb<br/>from keras.preprocessing import sequence<br/>import numpy as np<br/><br/>inpt_dim = 512    #input dimensions<br/>ltnt_dim = 256    #latent dimensions<br/><br/># -- the explanation for this will come later --<br/>(x_train, y_train), (x_test, y_test) = imdb.load_data()<br/>x_train = sequence.pad_sequences(x_train, maxlen=inpt_dim)<br/>x_test = sequence.pad_sequences(x_test, maxlen=inpt_dim)<br/># ----------------------------------------------</pre>
<p class="mce-root">And we proceed with building the model:</p>
<pre>x_train = x_train.astype('float32') <br/>x_test = x_test.astype('float32')<br/><br/># model with batch norm<br/>inpt_vec = Input(shape=(inpt_dim,))<br/>el1 = Dense(ltnt_dim)(inpt_vec)        #dense layer followed by<br/>el2 = <strong>BatchNormalization</strong>()(el1)        #batch norm<br/>encoder = Activation('sigmoid')(el2)<br/>decoder = Dense(inpt_dim, activation='sigmoid') (encoder)<br/>autoencoder = Model(inpt_vec, decoder)<br/><br/># compile and train model with bn<br/>autoencoder.compile(loss='binary_crossentropy', optimizer='adam')<br/>autoencoder.fit(x_train, x_train, epochs=20, batch_size=64, <br/>                shuffle=True, validation_data=(x_test, x_test))</pre>
<p class="mce-root">In this code fragment, batch normalization is placed right before the activation layer. This will, therefore, normalize the input to the activation function, which in this case is a <kbd>sigmoid.</kbd> Similarly, we can build the same model without a batch normalization layer as follows:</p>
<pre># model without batch normalization<br/>inpt_vec = Input(shape=(inpt_dim,))<br/>el1 = Dense(ltnt_dim)(inpt_vec) #no batch norm after this<br/>encoder = Activation('sigmoid')(el1)<br/>latent_ncdr = Model(inpt_vec, encoder)<br/>decoder = Dense(inpt_dim, activation='sigmoid') (encoder)<br/>autoencoder = Model(inpt_vec, decoder)<br/><br/># compile and train model with bn<br/>autoencoder.compile(loss='binary_crossentropy', optimizer='adam')<br/>autoencoder.fit(x_train, x_train, epochs=20, batch_size=64, <br/>                shuffle=True, validation_data=(x_test, x_test))</pre>
<p class="mce-root"/>
<p>If we train both models and plot their performance as they minimize the loss function, we will notice quickly that having batch normalization pays off, as shown in <em>Figure 8.3</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/12072ed9-ec0b-4784-a47d-b726124dc480.png" style="width:29.92em;height:17.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.3 – Comparison of learning progress with and without batch normalization</div>
<p>The figure indicates that having batch normalization has the effect of reducing the loss function both in training and in validation sets of data. These results are consistent with many other experiments that you can try on your own! However, as we said before, it is not necessarily a guarantee that this will happen all the time. This is a relatively modern technique that has proven to function properly so far, but this does not mean that it works for everything that we know of.</p>
<div class="packt_tip">We highly recommend that in all your models, you first try to solve the problem with a model that has no batch normalization, and then once you feel comfortable with the performance you have, come back and use batch normalization to see if you can get a slight boost in <strong>performance</strong> and <strong>training speed</strong>.</div>
<p class="mce-root"/>
<p>Let's say that you tried batch normalization and you were rewarded with a boost in performance, speed, or both, but you have <span>now </span>discovered that your model was overfitting all this time. Fear not! There is another interesting and novel technique, known as <strong>dropout</strong>. This can offer a model an alternative to reduce overfitting, as we will discuss in the following section.</p>
<h2 id="uuid-4e9b8362-071a-4bef-9aea-f072e8f94133">Dropout</h2>
<p>Dropout is a technique published in 2014 that became popular shortly after that year (Srivastava, N., Hinton, G., et.al. (2014)). It came as an alternative to combat overfitting, which is one of its major properties, and can be summarized as follows:</p>
<ul>
<li>It can reduce the chances of overfitting.</li>
<li>It can lead to better generalization.</li>
<li>It can reduce the effect of dominating neurons.</li>
<li>It can promote neuron diversity.</li>
<li>It can promote better neuron teamwork.</li>
</ul>
<p>The block image we will use for dropout, along with its main properties, is shown in <em>Figure 8.4</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/20c83967-c1c8-4cbc-a56d-0702da606ca5.png" style="width:34.17em;height:7.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.4 – Dropout layer properties</div>
<p>A dropout strategy works because it enables the network to search for an alternative hypothesis to solve the problem by disconnecting a particular number of neurons that represent certain hypotheses (or models) within a network itself. One easy way to look at this strategy is by thinking about the following: Imagine that you have a number of experts that are tasked with passing judgment on whether an image contains a cat or a chair. There might be a large number of experts that moderately believe that there is a chair in the image, but it only takes one expert to be particularly loud and fully convinced that there is a cat to persuade the decision-maker into listening to this particularly loud expert and ignoring the rest. In this analogy, experts are neurons. </p>
<p class="mce-root"/>
<p>There might be some neurons that are particularly convinced (sometimes incorrectly, due to overfitting on irrelevant features) of a certain fact about the information, and their output values are particularly high compared to the rest of the neurons in that layer, so much so that the deeper layers learn to listen more to that particular layer, thus perpetuating overfitting on deeper layers. <strong>Dropout</strong> is the mechanism that will select a number of neurons in a layer and completely disconnect them from the layer so that no input flows into those neurons nor is there output coming out of those neurons, as shown in <em>Figure 8.5</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/7b2a57b2-b790-49fc-9060-b8f35c0a5ced.png" style="width:37.75em;height:29.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.5 – Dropout mechanism over the first hidden layer. Dropout here disconnects one neuron from the layer</div>
<p>In the preceding diagram, the first hidden layer has a dropout rate of one third. This means that, completely at random, one third of the neurons will be disconnected. <em>Figure 8.5</em> shows an example of when the second neuron in the first hidden layer is disconnected: no input from the input layer goes in, and no output comes out of it. The model is completely oblivious to its existence; for all practical purposes, this is a different neural network! </p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>However, the neurons that are disconnected are only disconnected for one training step: their weights are unchanged for one training step, while all other weights are updated. This has a few interesting implications:</p>
<ul>
<li>Due to the random selection of neurons, those <em>troublemakers</em> that tend to dominate (overfit) on particular features are bound to be selected out at some point, and the rest of the neurons will learn to process feature spaces without those <em>troublemakers</em>. This leads to the prevention and reduction of overfitting, while promoting collaboration among diverse neurons that are experts in different things.</li>
<li>Due to the constant ignorance/disconnection of neurons, the network has the potential of being fundamentally different – it is almost as if we are training multiple neural networks in every single step without actually having to make many different models. It all happens because of dropout.</li>
</ul>
<p>It is usually recommended to use dropout in deeper networks to ameliorate the traditional problem of overfitting that is common in DL.</p>
<p>To show the difference in performance when using dropout, we will use the exact same dataset as in the previous section, but we will add an additional layer in the autoencoder as follows:</p>
<pre>from tensorflow.keras.layers import <strong>Dropout</strong><br/>...<br/># encoder with dropout<br/>inpt_vec = Input(shape=(inpt_dim,))<br/>el1 = Dense(inpt_dim/2)(inpt_vec)</pre>
<p class="mce-root">In this code, the dropout rate is 10%, meaning that 10% of the neurons in the dense layer <kbd>e14</kbd> are disconnected multiple times at random during training.</p>
<pre><br/>el2 = Activation('relu')(el1)<br/><strong>el3 = Dropout(0.1)(el2)</strong><br/>el4 = Dense(ltnt_dim)(el3)<br/>encoder = Activation('relu')(el4)</pre>
<p>The decoder is left exactly the same as before, and the baseline model simply does not contain a dropout layer:</p>
<pre># without dropout<br/>inpt_vec = Input(shape=(inpt_dim,))<br/>el1 = Dense(inpt_dim/2)(inpt_vec)<br/>el2 = Activation('relu')(el1)<br/>el3 = Dense(ltnt_dim)(el2)<br/>encoder = Activation('relu')(el3)</pre>
<p>If we choose <kbd>'adagrad'</kbd> and we perform training over 100 epochs and compare the performance results, we can obtain the performance shown in <em>Figure 8.6</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/e582e979-2bb1-45a8-989d-454a99610665.png" style="width:36.50em;height:22.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.6 – Autoencoder reconstruction loss comparing models with dropout and without</div>
<p>Here is the full code:</p>
<pre>from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import Dense, Activation, Input<br/>from tensorflow.keras.layers import <strong>Dropout</strong><br/>from keras.datasets import imdb<br/>from keras.preprocessing import sequence<br/>import numpy as np<br/>import matplotlib.pyplot as plt<br/><br/>inpt_dim = 512<br/>ltnt_dim = 128<br/><br/>(x_train, y_train), (x_test, y_test) = imdb.load_data()<br/>x_train = sequence.pad_sequences(x_train, maxlen=inpt_dim)<br/>x_test = sequence.pad_sequences(x_test, maxlen=inpt_dim)<br/><br/>x_train = x_train.astype('float32') <br/>x_test = x_test.astype('float32')</pre>
<p>Then we define the model with dropout like so:</p>
<pre><strong># with dropout</strong><br/>inpt_vec = Input(shape=(inpt_dim,))<br/>el1 = Dense(inpt_dim/2)(inpt_vec)<br/>el2 = Activation('relu')(el1)<br/><strong>el3 = Dropout(0.1)(el2)</strong><br/>el4 = Dense(ltnt_dim)(el3)<br/>encoder = Activation('relu')(el4)<br/><br/># model that takes input and encodes it into the latent space<br/>latent_ncdr = Model(inpt_vec, encoder)<br/><br/>decoder = Dense(inpt_dim, activation='relu') (encoder)<br/><br/># model that takes input, encodes it, and decodes it<br/>autoencoder = Model(inpt_vec, decoder)</pre>
<p>Then we compile it, train it, store the training history, and clear the variables to re-use them as follows:</p>
<pre>autoencoder.compile(loss='binary_crossentropy', optimizer='<strong>adagrad</strong>')<br/><br/>hist = autoencoder.fit(x_train, x_train, epochs=100, batch_size=64, <br/>                       shuffle=True, validation_data=(x_test, x_test))<br/><br/>bn_loss = hist.history['loss']<br/>bn_val_loss = hist.history['val_loss']<br/><br/>del autoencoder<br/>del hist</pre>
<p>And then we do the same for a model without dropout:</p>
<pre><strong># now without dropout</strong><br/>inpt_vec = Input(shape=(inpt_dim,))<br/>el1 = Dense(inpt_dim/2)(inpt_vec)<br/>el2 = Activation('relu')(el1)<br/>el3 = Dense(ltnt_dim)(el2)<br/>encoder = Activation('relu')(el3)<br/><br/># model that takes input and encodes it into the latent space<br/>latent_ncdr = Model(inpt_vec, encoder)<br/><br/>decoder = Dense(inpt_dim, activation='relu') (encoder)<br/><br/># model that takes input, encodes it, and decodes it<br/>autoencoder = Model(inpt_vec, decoder)<br/><br/>autoencoder.compile(loss='binary_crossentropy', optimizer='<strong>adagrad</strong>')<br/><br/>hist = autoencoder.fit(x_train, x_train, epochs=100, batch_size=64, <br/>                       shuffle=True, validation_data=(x_test, x_test))</pre>
<p>Next we gather the training data and plot it like so:</p>
<pre>loss = hist.history['loss']<br/>val_loss = hist.history['val_loss']<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.plot(bn_loss, color='#785ef0')<br/>plt.plot(bn_val_loss, color='#dc267f')<br/>plt.plot(loss, '--', color='#648fff')<br/>plt.plot(val_loss, '--', color='#fe6100')<br/>plt.title('Model reconstruction loss')<br/>plt.ylabel('Binary Cross-Entropy Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['With Drop Out - Training', <br/>            'With Drop Out - Validation', <br/>            'Without Drop Out - Training', <br/>            'Without Drop Out - Validation'], loc='upper right')<br/>plt.show()</pre>
<p>From <em>Figure 8.6</em> we can see that the performance of the model with dropout is superior than without. This suggests that training without dropout has a higher chance of overfitting, the reason being that the learning curve is worse on the validation set when dropout is not used. </p>
<p>As mentioned earlier, the <kbd>adagrad</kbd> optimizer has been chosen for this particular task. We made this decision because it is important for you to learn more optimizers, one at a time. Adagrad is an adaptive algorithm; it performs updates with respect to the frequency of features (Duchi, J., et al. (2011)). If features occur frequently, the updates are small, while larger updates are done for features that are out of the ordinary. </p>
<div class="packt_tip">It is recommended to use Adagrad when the <strong>dataset is sparse</strong>. For example, in word embedding cases such as the one in this example, frequent words will cause small updates, while rare words will require larger updates.</div>
<p class="mce-root"/>
<p>Finally, it is important to mention that <kbd>Dropout(rate)</kbd> belongs to the <kbd>tf.keras.layers.Dropout</kbd> class. The rate that is taken as a parameter corresponds to the rate at which neurons will be disconnected at random at every single training step for the particular layer on which dropout is used.</p>
<div class="packt_tip">It is recommended that you use a dropout rate between <strong>0.1 and 0.5</strong> to achieve significant changes to your network's performance. And it is recommended to use dropout <strong>only in deep networks</strong>. However, these are empirical findings and your own experimentation is necessary.</div>
<p>Now that we have explained these two relatively new concepts, dropout and batch normalization, we will create a deep autoencoder network that is relatively simple and yet powerful in finding latent representations that are not biased toward particular labels.</p>
<h1 id="uuid-70ad4673-ce72-4dc1-bc4c-28d3cb1004f5">Exploring latent spaces with deep autoencoders</h1>
<p>Latent spaces, as we defined them in <a href="480521d9-845c-4c0a-b82b-be5f15da0171.xhtml">Chapter 7</a>, <em>Autoencoders</em>, are very important in DL because they can lead to powerful decision-making systems that are based on assumed rich latent representations. And, once again, what makes the latent spaces produced by autoencoders (and other unsupervised models) rich in their representations is that they are not biased toward particular labels.</p>
<p>In <a href="480521d9-845c-4c0a-b82b-be5f15da0171.xhtml">Chapter 7</a>, <em>Autoencoders</em>, we explored the MNIST dataset, which is a standard dataset in DL, and showed that we can easily find very good latent representations with as few as four dense layers in the encoder and eight layers for the entire autoencoder model. In the next section, we will take on a much more difficult dataset known as CIFAR-10, and then we will come back to explore the latent representation of the <kbd>IMDB</kbd> dataset, which we have already explored briefly in the previous sections of this chapter.</p>
<h2 id="uuid-89392767-457c-41e6-a01e-8c666ff9f982">CIFAR-10</h2>
<p>In 2009, the <em>Canadian Institute for Advanced Research (CIFAR)</em> released a very large collection of images that can be used to train DL models to recognize a variety of objects. The one we will use in this example is widely known as CIFAR-10, since it has only 10 classes and a total of 60,000 images; <em>Figure 8.7</em> depicts samples of each class:</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/4e66fc36-252d-4f4b-b881-690e8d57122c.png" style="width:33.83em;height:16.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.7 – Sample images from the CIFAR-10 dataset. The number indicates the numeric value assigned to each class for convenience</div>
<p>Every image in the dataset is 32 by 32 pixels using 3 dimensions to keep track of the color details. As can be seen from the figure, these small images contain other objects beyond those labeled, such as text, background, structures, landscapes, and other partially occluded objects, while preserving the main object of interest in the foreground. This makes it more challenging than MNIST, where the background is always black, images are grayscale, and there is only one number in every image. If you have never worked in computer vision applications, you may not know that it is exponentially more complicated to deal with CIFAR-10 compared to MNIST. Therefore, our models need to be more robust and deep in comparison to MNIST ones.</p>
<p>In TensorFlow and Keras, we can easily load and prepare our dataset with the following code:</p>
<pre>import numpy as np<br/>from tensorflow.keras.datasets import <strong>cifar10</strong><br/><br/>(x_train, y_train), (x_test, y_test) = cifar10.load_data()<br/>x_train = x_train.astype('float32') / 255.<br/>x_test = x_test.astype('float32') / 255.<br/>x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))<br/>x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))<br/><br/>print('x_train shape is:', x_train.shape)<br/>print('x_test shape is:', x_test.shape)</pre>
<p>The preceding code outputs the following:</p>
<pre>x_train shape is: (50000, 3072)<br/>x_test shape is: (10000, 3072)</pre>
<p class="mce-root"/>
<p>This says that we have one-sixth of the dataset (~16%) separated for test purposes, while the rest is used for training. The 3,072 dimensions come from the number of pixels and channels: <img class="fm-editor-equation" src="assets/13725260-8e91-4e8f-bb67-204f0f8917c4.png" style="width:9.67em;height:1.00em;"/>. The preceding code also normalizes the data from the range [0, 255] down to [0.0, 1.0] in floating-point numbers.</p>
<p>To move on with our example, we will propose a deep autoencoder with the architecture shown in <em>Figure 8.8</em>, which will take a 3,072-dimensional input and will encode it down to 64 dimensions:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a5b1d547-5beb-43a1-8ce4-eda0b318876d.png" style="width:42.00em;height:24.58em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.8 – Architecture of a deep autoencoder on the CIFAR-10 dataset</div>
<p>This architecture uses 17 layers in the encoder and 15 layers in the decoder. Dense layers in the diagram have the number of neurons written in their corresponding block. As can be seen, this model implements a series of strategic batch normalization and dropout strategies throughout the process of encoding the input data. In this example, all dropout layers have a 20% dropout rate.</p>
<p>If we train the model for 200 epochs using the standard <kbd>adam</kbd> optimizer and the standard binary cross-entropy loss, we could obtain the training performance shown in <em>Figure 8.9</em>:</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/39b4e755-a820-4fe6-9432-eaf516ec6a13.png" style="width:32.92em;height:20.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.9 – Reconstruction of the loss of the deep autoencoder model on CIFAR-10</div>
<p>Here is the full code:</p>
<pre>from tensorflow import keras<br/>from tensorflow.keras.datasets import cifar10<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import Dense, <strong>Dropout</strong>, Activation, Input<br/>from tensorflow.keras.layers import <strong>BatchNormalization<br/></strong>import matplotlib.pyplot as plt<br/>import numpy as np<br/><br/>inpt_dim = 32*32*3<br/><strong>ltnt_dim = 64</strong><br/><br/># The data, split between train and test sets:<br/>(x_train, y_train), (x_test, y_test) = cifar10.load_data()<br/>x_train = x_train.astype('float32') / 255.<br/>x_test = x_test.astype('float32') / 255.<br/>x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))<br/>x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))<br/>print('x_train shape:', x_train.shape)<br/>print('x_test shape:', x_test.shape)</pre>
<p>We define the model as follows:</p>
<pre>inpt_vec = Input(shape=(inpt_dim,))<br/>el1 = Dense(2048)(inpt_vec)<br/>el2 = Activation('relu')(el1)<br/>el3 = Dense(1024)(el2)<br/>el4 = <strong>BatchNormalization</strong>()(el3)<br/>el5 = Activation('relu')(el4)<br/>el6 = <strong>Dropout</strong>(0.2)(el5)<br/><br/>el7 = Dense(512)(el6)<br/>el8 = Activation('relu')(el7)<br/>el9 = Dense(256)(el8)<br/>el10 = <strong>BatchNormalization</strong>()(el9)<br/>el11 = Activation('relu')(el10)<br/>el12 = <strong>Dropout</strong>(0.2)(el11)<br/><br/>el13 = Dense(128)(el12)<br/>el14 = Activation('relu')(el13)<br/>el15 = <strong>Dropout</strong>(0.2)(el14)<br/>el16 = Dense(ltnt_dim)(el15)<br/>el17 = <strong>BatchNormalization</strong>()(el16)<br/>encoder = Activation('tanh')(el17)<br/><br/># model that takes input and encodes it into the latent space<br/>latent_ncdr = Model(inpt_vec, encoder)</pre>
<p>Next we define the decoder portion of the model like this:</p>
<pre>dl1 = Dense(128)(encoder)<br/>dl2 = <strong>BatchNormalization</strong>()(dl1)<br/>dl3 = Activation('relu')(dl2)<br/><br/>dl4 = <strong>Dropout</strong>(0.2)(dl3)<br/>dl5 = Dense(256)(dl4)<br/>dl6 = Activation('relu')(dl5)<br/>dl7 = Dense(512)(dl6)<br/>dl8 = <strong>BatchNormalization</strong>()(dl7)<br/>dl9 = Activation('relu')(dl8)<br/><br/>dl10 = <strong>Dropout</strong>(0.2)(dl9)<br/>dl11 = Dense(1024)(dl10)<br/>dl12 = Activation('relu')(dl11)<br/>dl13 = Dense(2048)(dl12)<br/>dl14 = <strong>BatchNormalization</strong>()(dl13)<br/>dl15 = Activation('relu')(dl14)<br/>decoder = Dense(inpt_dim, activation='sigmoid') (dl15)</pre>
<p>We put it together in an autoencoder model, compile it and train it like so:</p>
<pre># model that takes input, encodes it, and decodes it<br/>autoencoder = Model(inpt_vec, decoder)<br/><br/># setup RMSprop optimizer<br/>opt = keras.optimizers.<strong>RMSprop</strong>(learning_rate=0.0001, decay=1e-6, )<br/><br/>autoencoder.compile(loss='binary_crossentropy', optimizer=opt)<br/><br/>hist = autoencoder.fit(x_train, x_train, epochs=200, batch_size=10000, <br/>                       shuffle=True, validation_data=(x_test, x_test))<br/><br/># and now se visualize the results<br/>fig = plt.figure(figsize=(10,6))<br/>plt.plot(hist.history['loss'], color='#785ef0')<br/>plt.plot(hist.history['val_loss'], color='#dc267f')<br/>plt.title('Model reconstruction loss')<br/>plt.ylabel('Binary Cross-Entropy Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training Set', 'Test Set'], loc='upper right')<br/>plt.show()</pre>
<p>The model performance shown in <em>Figure 8.9</em> converges nicely and loss decays both on the training and test sets, which implies that the model is not overfitting and continues to adjust the weights properly over time. To visualize the model's performance on unseen data (the test set), we can simply pick samples from the test set at random, such as the ones in <em>Figure 8.10</em>, which produce the output shown in <em>Figure 8.11</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a502db01-f2ff-4c8f-987d-d8877f7c0e70.png" style="width:36.75em;height:19.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.10 – Sample input from the test set of CIFAR-10</div>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/5befa42e-5531-46a3-89c2-6fedf3830087.png" style="width:38.00em;height:19.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.11 – Sample output (reconstructions) from the samples given in Figure 8.10</div>
<p>You can see from <em>Figure 8.11</em> that the reconstructions correctly address the color spectrum of the input data. However, it is clear that the problem is much harder in reconstruction terms than MNIST. Shapes are blurry although they seem to be in the correct spatial position. A level of detail is evidently missing in the reconstructions. We can make the autoencoder deeper, or train for longer, but the problem might not be properly solved. We can justify this performance with the fact that we deliberately chose to find a latent representation of size 64, which is smaller than a tiny 5 by 5 image: <img class="fm-editor-equation" src="assets/861e2ca8-20e9-4276-94b4-bb24d061553c.png" style="width:7.33em;height:1.00em;"/>. If you think about it and reflect on this, then it is clear that it is nearly impossible, as 3,072 to 64 represents a 2.08% compression! </p>
<p>A solution to this would be not to make the model larger, but to acknowledge that the latent representation size might not be large enough to capture relevant details of the input to have a good reconstruction. The current model might be too aggressive in reducing the dimensionality of the feature space. If we use UMAP to visualize the 64-dimensional latent vectors in 2 dimensions, we will obtain the plot shown in <em>Figure 8.12</em>:</p>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/c9f94d24-4eb7-457f-9ae4-4d0d8dbbefe2.png" style="width:36.08em;height:31.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.12 – UMAP two-dimensional representation of the latent vectors in the test set</div>
<p>We have not spoken about UMAP before, but we will briefly state that this a ground-breaking data visualization tool that has been proposed recently and is starting to gain attention (McInnes, L., et al. (2018)). In our case, we simply used UMAP to visualize the data distribution since we are not using the autoencoder to encode all the way down to two dimensions. <em>Figure 8.12</em> indicates that the distribution of the classes is not sufficiently clearly defined so as to enable us to observe separation or well-defined clusters. This confirms that the deep autoencoder has not captured sufficient information for class separation; however, there are still clearly defined groups in some parts of the latent space, such as the clusters on the bottom middle and left, one of which is associated with a group of airplane images, for example. This <strong>deep belief network </strong>has acquired knowledge about the input space well enough to make out some different aspects of the input; for example, it knows that airplanes are quite different from frogs, or at least that they might appear in different conditions, that is, a frog will appear against green backgrounds while an airplane is likely to have blue skies in the background.</p>
<p class="mce-root"/>
<div class="packt_infobox"><strong>Convolutional neural networks</strong> (<strong>CNNs</strong>)<strong> </strong>are a much better alternative for most computer vision and image analysis problems such as this one. We will get there in due time in <a href="c36bdee9-51f3-4283-8f15-6dd603d071a1.xhtml">Chapter 12</a>, <em>Convolutional Neural Networks</em>. Be patient for now as we gently introduce different models one by one. You will see how we can make a convolutional autoencoder that can achieve much better performance than an autoencoder that uses fully connected layers. For now, we will continue with autoencoders for a little longer.</div>
<p>The model introduced in <em>Figure 8.8</em> can be produced using the functional approach; the encoder can be defined as follows:</p>
<pre>from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import Dense, Dropout, Activation, Input<br/>from tensorflow.keras.layers import BatchNormalization, MaxPooling1D<br/>import numpy as np<br/><br/>inpt_dim = 32*32*3<br/>ltnt_dim = 64<br/><br/>inpt_vec = Input(shape=(inpt_dim,))<br/>el1 = Dense(2048)(inpt_vec)<br/>el2 = Activation('relu')(el1)<br/>el3 = Dense(1024)(el2)<br/>el4 = <strong>BatchNormalization</strong>()(el3)<br/>el5 = Activation('relu')(el4)<br/>el6 = <strong>Dropout</strong>(0.2)(el5)<br/><br/>el7 = Dense(512)(el6)<br/>el8 = Activation('relu')(el7)<br/>el9 = Dense(256)(el8)<br/>el10 = <strong>BatchNormalization</strong>()(el9)<br/>el11 = Activation('relu')(el10)<br/>el12 = <strong>Dropout</strong>(0.2)(el11)<br/><br/>el13 = Dense(128)(el12)<br/>el14 = Activation('relu')(el13)<br/>el15 = <strong>Dropout</strong>(0.2)(el14)<br/>el16 = Dense(ltnt_dim)(el15)<br/>el17 = <strong>BatchNormalization</strong>()(el16)<br/>encoder = Activation('tanh')(el17)<br/><br/># model that takes input and encodes it into the latent space<br/>latent_ncdr = Model(inpt_vec, encoder)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Notice that all the dropout layers have a 20% rate every time. The 17 layers go from mapping an input of <kbd>inpt_dim=3072</kbd> dimensions down to <kbd>ltnt_dim = 64</kbd> dimensions. The last activation function of the encoder is the hyperbolic tangent <kbd>tanh</kbd>, which provides an output in the range [-1,1]; this choice is only made for convenience in visualizing the latent space.</p>
<p>Next, the definition of the decoder is as follows:</p>
<pre>dl1 = Dense(128)(encoder)<br/>dl2 = <strong>BatchNormalization</strong>()(dl1)<br/>dl3 = Activation('relu')(dl2)<br/><br/>dl4 = <strong>Dropout</strong>(0.2)(dl3)<br/>dl5 = Dense(256)(dl4)<br/>dl6 = Activation('relu')(dl5)<br/>dl7 = Dense(512)(dl6)<br/>dl8 = <strong>BatchNormalization</strong>()(dl7)<br/>dl9 = Activation('relu')(dl8)<br/><br/>dl10 = <strong>Dropout</strong>(0.2)(dl9)<br/>dl11 = Dense(1024)(dl10)<br/>dl12 = Activation('relu')(dl11)<br/>dl13 = Dense(2048)(dl12)<br/>dl14 = <strong>BatchNormalization</strong>()(dl13)<br/>dl15 = Activation('relu')(dl14)<br/>decoder = Dense(inpt_dim, activation='sigmoid') (dl15)<br/><br/># model that takes input, encodes it, and decodes it<br/>autoencoder = Model(inpt_vec, decoder)</pre>
<p>The last layer of the decoder has a <kbd>sigmoid</kbd> activation function that maps back to the input space range, that is, [0.0, 1.0]. Finally, we can train the <kbd>autoencoder</kbd> model as previously defined using the <kbd>binary_crossentropy</kbd> loss and <kbd>adam</kbd> optimizer for 200 epochs like so:</p>
<pre>autoencoder.compile(loss='binary_crossentropy', optimizer='adam')<br/><br/>hist = autoencoder.fit(x_train, x_train, epochs=200, batch_size=5000, <br/>                       shuffle=True, validation_data=(x_test, x_test))</pre>
<p>The results have been previously shown in <em>Figures 8.9</em> to 8.<em>11</em>. However, it is interesting to revisit MNIST, but this time using a deep autoencoder, as we will discuss next.</p>
<p class="mce-root"/>
<h2 id="uuid-aa4bf8ed-378a-4af5-b927-a9cf544bbad6">MNIST </h2>
<p>The <kbd>MNIST</kbd> dataset is a good example of a dataset that is less complex than CIFAR-10, and that can be approached with a deep autoencoder. Previously, in <a href="480521d9-845c-4c0a-b82b-be5f15da0171.xhtml">Chapter 7</a>, <em>Autoencoders</em>, we discussed shallow autoencoders and showed that adding layers was beneficial. In this section, we go a step further to show that a deep autoencoder with dropout and batch normalization layers can perform better at producing rich latent representations. <em>Figure 8.13</em> shows the proposed architecture:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/45e95b72-b8c6-4ea4-9c14-742b2d699963.png" style="width:29.00em;height:17.00em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.13 – Deep autoencoder for MNIST</div>
<p>The number of layers and the sequence of layers is the same as in <em>Figure 8.8</em>; however, the number of neurons in the dense layers and the latent representation dimensions have changed. The compression rate is from 784 to 2, or 0.25%:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a6763900-9f9b-405e-a06e-928342690342.png" style="width:20.58em;height:10.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 8.14 – MNIST original sample digits of the test set</div>
<p>And yet, the reconstructions are very good, as sho<span>wn in <em>Figure 8.14</em> and in <em>Figure 8.15</em>:</span></p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/6842fab6-7196-442d-9768-48f4f29ab489.png" style="width:19.92em;height:10.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 8.15 – Reconstructed MNIST digits from the original test set shown in <em>Figure 8.14</em></span></div>
<p>The reconstructions shown in the figure show a level of detail that is very good, although it seems blurry around the edges. The general shape of the digits seems to be captured well by the model. The corresponding latent representations of the test set are shown in <em>Figure 8.16</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a3ad8424-e567-49ed-808e-d340eac53c77.png" style="width:28.50em;height:24.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><span>Figure 8.16 – Latent representation of MNIST digits in the test set, partially shown in <em>Figure 8.14</em></span></div>
<p class="mce-root"/>
<p>From the preceding plot, we can see that there are well-defined clusters; however, it is important to point out that the autoencoder knows nothing about labels and that these clusters have been learned from the data alone. This is the power of autoencoders at their best. If the encoder model is taken apart and re-trained with labels, the model is likely to perform even better. However, for now, we will leave it here and continue with a type of <em>generative</em> model in <a href="c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml">Chapter 9</a>, <em>Variational Autoencoders</em>, which comes next.</p>
<h1 id="uuid-3ede9249-acac-46ba-a12a-0561c0d2350a">Summary </h1>
<p>This intermediate chapter showed the power of deep autoencoders when combined with regularization strategies such as dropout and batch normalization. We implemented an autoencoder that has more than 30 layers! That's <em>deep</em>! We saw that in difficult problems a deep autoencoder can offer an unbiased latent representation of highly complex data, as most deep belief networks do. We looked at how dropout can reduce the risk of overfitting by ignoring (disconnecting) a fraction of the neurons at random in every learning step. Furthermore, we learned that batch normalization can offer stability to the learning algorithm by gradually adjusting the response of some neurons so that activation functions and other connected neurons don't saturate or overflow numerically. </p>
<p>At this point, you should feel confident applying batch normalization and dropout strategies in a deep autoencoder model. You should be able to create your own deep autoencoders and apply them to different tasks where a rich latent representation is required for data visualization purposes, data compression or dimensionality reduction problems, and other types of data embeddings where a low-dimensional representation is required.</p>
<p><a href="c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml">Chapter 9</a>,<span> </span><em>Variational Autoencoders,</em><span> will continue with autoencoders but from a <em>generative</em> <em>modeling</em> perspective. Generative models have the ability to generate data by sampling a probability density function, which is quite interesting. We will specifically discuss the variational autoencoder model as a better alternative to a deep autoencoder in the presence of noisy data. </span></p>
<p class="mce-root"/>
<h1 id="uuid-60d2ea9e-5716-4f84-ae0f-5a5ce267dffe">Questions and answers</h1>
<ol>
<li><strong>Which regularization strategy discussed in this chapter alleviates overfitting in deep models?</strong></li>
</ol>
<p style="padding-left: 60px">Dropout.</p>
<ol start="2">
<li><strong>Does adding a batch normalization layer make the learning algorithm have to learn more parameters? </strong></li>
</ol>
<p style="padding-left: 60px">Actually, no. For every layer in which dropout is used, there will be only two parameters for every neuron to learn: <img src="assets/5db1f077-f826-4c6a-8b08-ca701527c5c0.png" style="width:2.50em;height:1.25em;"/>. If you do the math, the addition of new parameters is rather small.</p>
<ol start="3">
<li><strong>What other deep belief networks are out there?</strong></li>
</ol>
<p style="padding-left: 60px">Restricted Boltzmann machines, for example, are another very popular example of deep belief networks. <a href="https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=33&amp;action=edit">Chapter 10</a>, <em>Restricted Boltzmann Machines</em>, will cover these in more detail.</p>
<ol start="4">
<li><strong>How come deep autoencoders perform better on MNIST than on CIFAR-10?</strong></li>
</ol>
<p style="padding-left: 60px">Actually, we do not have an objective way of saying that deep autoencoders are better on these datasets. We are biased in thinking about it in terms of clustering and data labels. Our bias in thinking about the latent representations in <em>Figure 8.12</em> and <em>Figure 8.16</em> in terms of labels is precluding us from thinking about other possibilities. Consider the following for CIFAR-10: what if the autoencoder is learning to represent data according to textures? Or color palettes? Or geometric properties? Answering these questions is key to understanding what is going on inside the autoencoder and why it is learning to represent the data in the way it does, but requires more advanced skills and time. In summary, we don't know for sure whether it is underperforming or not until we answer these questions; otherwise, if we put on our lenses of classes, groups, and labels, then it might just seem that way.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<h1 id="uuid-3663e959-56a2-49fe-8d2c-2dd35a74c0b5">References</h1>
<ul>
<li>Sutskever, I., &amp; Hinton, G. E. (2008). Deep, narrow sigmoid belief networks are universal approximators. <em>Neural computation</em>, 20(11), 2629-2636.</li>
<li><span>Sainath, T. N., Kingsbury, B., &amp; Ramabhadran, B. (2012, March). Auto-encoder bottleneck features using deep belief networks. In 2012 <em>IEEE international conference on acoustics, speech and signal processing (ICASSP)</em> (pp. 4153-4156). IEEE.</span></li>
<li>Wu, K., &amp; Magdon-Ismail, M. (2016). Node-by-node greedy deep learning for interpretable features. <em>arXiv preprint</em> arXiv:1602.06183.</li>
<li>Ioffe, S., &amp; Szegedy, C. (2015, June). Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift. In <em>International Conference on Machine Learning (ICML)</em> (pp. 448-456).</li>
<li>Srivastava, N., Hinton, G., Krizhevsky, A., Sutskever, I., &amp; Salakhutdinov, R. (2014). Dropout: a simple way to prevent neural networks from overfitting. <em>The journal of machine learning research</em>, 15(1), 1929-1958.</li>
<li>Duchi, J., Hazan, E., &amp; Singer, Y. (2011). Adaptive subgradient methods for online learning and stochastic optimization. <em>Journal of machine learning research</em>, 12(Jul), 2121-2159.</li>
<li>McInnes, L., Healy, J., &amp; Umap, J. M. (2018). Uniform manifold approximation and projection for dimension reduction. <em>arXiv preprint</em> arXiv:1802.03426.</li>
<li>Maas, A. L., Daly, R. E., Pham, P. T., Huang, D., Ng, A. Y., &amp; Potts, C. (2011, June). Learning word vectors for sentiment analysis. In <em>Proceedings of the 49th annual meeting of the association for computational linguistics</em>: <em>Human language technologies</em>-volume 1 (pp. 142-150). Association for Computational Linguistics.</li>
<li>Hochreiter, S. (1998). The vanishing gradient problem during learning recurrent neural nets and problem solutions. International Journal of Uncertainty, <em>Fuzziness and Knowledge-Based Systems</em>, 6(02), 107-116.</li>
<li>Van Laarhoven, T. (2017). L2 regularization versus batch and weight normalization. <em>arXiv preprint</em> arXiv:1706.05350.</li>
</ul>


            </article>

            
        </section>
    </body></html>