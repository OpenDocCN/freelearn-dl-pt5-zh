- en: Machine Learning Basics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习基础
- en: Welcome to *Hands-On Deep Learning with R*! This book will take you through
    all of the steps that are necessary to code deep learning models using the R statistical
    programming language. It begins with simple examples as the first step for those
    just getting started, along with a review of the foundational elements of deep
    learning for those with more experience. As you progress through this book, you
    will learn how to code increasingly complex deep learning solutions for a wide
    variety of tasks. However, regardless of the complexity, each chapter will carefully
    detail each step. This is so that all topics and concepts can be fully comprehended
    and the reason for every line of code is completely explained.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 欢迎来到*R语言深度学习实战*！本书将带领你完成使用R统计编程语言编写深度学习模型的所有必要步骤。它从简单的示例开始，为刚刚入门的人提供第一步，并回顾了深度学习的基础元素，以便有更多经验的人复习。在你逐步深入本书时，你将学习如何编写越来越复杂的深度学习解决方案，适用于各种任务。无论任务复杂度如何，每一章都将详细说明每一步。这样所有的主题和概念都能被充分理解，并且每一行代码的原因都能得到完全解释。
- en: In this chapter, we will go through a quick overview of the machine learning
    process as it will form a base for the subsequent chapters of this book. We will
    look at processing a dataset to review techniques such as handling outliers and
    missing values. We will learn how to model data to brush up on the process of
    predicting an outcome and evaluating the results, and we will also review the
    most suitable metrics for various problems. We will look at improving a model
    using parameter tuning, feature engineering, and ensembling, and we will learn when
    to use different machine learning algorithms based on the task to solve.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将快速概述机器学习过程，因为它将为本书后续章节奠定基础。我们将学习如何处理数据集，以复习处理离群值和缺失值等技术。我们将学习如何对数据进行建模，以回顾预测结果的过程并评估结果，还将复习针对各种问题最合适的评估指标。我们将探讨如何通过参数调整、特征工程和集成方法来改进模型，并学习如何根据任务选择不同的机器学习算法。
- en: 'This chapter will cover the following topics:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将涵盖以下主题：
- en: An overview of machine learning
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习概述
- en: Preparing data for modeling
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备数据以进行建模
- en: Training a model on prepared data
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在准备好的数据上训练模型
- en: Evaluating model results
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型结果
- en: Improving model results
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改进模型结果
- en: Reviewing different algorithms
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾不同的算法
- en: An overview of machine learning
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习概述
- en: All deep learning is machine learning, but not all machine learning is deep
    learning. Throughout this book, we will focus on processes and techniques that
    are specific to deep learning in R. However, all the core principles of machine
    learning are essential to understand before we can move on to explore deep learning.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有的深度学习都是机器学习，但并非所有机器学习都是深度学习。本书将重点介绍与R中深度学习相关的过程和技术。然而，理解机器学习的所有核心原理是非常重要的，只有这样我们才能继续探索深度学习。
- en: Deep learning is marked as a special subset of machine learning based on the
    use of neural networks that mimic brain activity behavior. The learning is referred
    to as being deep because, during the modeling process, the data is manipulated
    by a number of hidden layers. In this type of modeling, specific information is
    gathered from each layer. For example, one layer may find the edges of images
    while another finds particular hues.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习被视为机器学习的一个特殊子集，基于使用模拟大脑活动的神经网络。学习被称为“深度”是因为在建模过程中，数据通过多个隐藏层进行处理。在这种建模方式中，每个层都会收集特定的信息。例如，一个层可能会找到图像的边缘，而另一个层则可能会找到特定的色调。
- en: 'Notable applications for this type of machine learning include the following:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 该类型机器学习的显著应用包括以下几个方面：
- en: Image recognition (including facial recognition)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像识别（包括人脸识别）
- en: Signal detection
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号检测
- en: Recommendation systems
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 推荐系统
- en: Document summarization
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档摘要
- en: Topic modeling
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主题建模
- en: Forecasting
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测
- en: Solving games
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决游戏
- en: Moving an object through space, for example, self-driving cars
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在空间中移动物体，例如自动驾驶汽车
- en: All of these topics will be covered throughout the course of this book. All
    of these topics implement deep learning and neural networks, which are primarily
    used for classification and regression.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的整个过程中将涵盖所有这些主题。所有这些主题都实现了深度学习和神经网络，主要用于分类和回归。
- en: Preparing data for modeling
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据以进行建模
- en: 'One of the benefits of deep learning is that it largely removes the need for
    feature engineering, which you may be used to with machine learning. That being
    said, the data still needs to be prepared before we begin modeling. Let''s review
    the following goals to prepare data for modeling:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的一个好处是，它大大减少了对特征工程的需求，这可能是你在机器学习中常见的。不过，数据在建模之前仍然需要准备。让我们回顾以下目标，以便为建模做好数据准备：
- en: Remove no-information and extremely low-information variables
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除无信息和极低信息变量
- en: Identify dates and extract date parts
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别日期并提取日期部分
- en: Handle missing values
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: Handle outliers
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理离群值
- en: In this chapter, we will be investigating air quality data using data provided
    by the London Air Quality Network. Specifically, we will look at readings for
    nitrogen dioxide in the area of Tower Hamlets (Mile End Road) during 2018\. This
    is a very small dataset with only a few features and approximately 35,000 observations.
    We are using a limited dataset here so that all of our code, even our modeling,
    runs quickly. That said, the dataset fits well for the process that we will explore.
    It requires some, but not an inordinate amount of, initial cleaning and preparation.
    In addition to this, it is suitable to use for decision tree-based modeling, which
    will be a useful form of machine learning to review as we start to apply deep
    learning models in future chapters.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将使用伦敦空气质量网络提供的数据，研究空气质量数据。具体来说，我们将查看2018年Tower Hamlets（Mile End Road）地区的二氧化氮读数。这是一个非常小的数据集，只有几个特征和大约35,000个观测值。我们使用一个有限的数据集，以确保我们的所有代码，甚至包括建模，都能快速运行。也就是说，这个数据集非常适合我们要探索的过程。它需要一些初步的清理和准备，但不需要过多的工作。除此之外，这个数据集适合用于基于决策树的建模，这也是我们在未来章节中开始应用深度学习模型时，审查的一个有用的机器学习方法。
- en: 'Our first step will be to do some cursory data exploration to see what data
    cleaning and preparation steps will be necessary. R has some really helpful convenience
    packages for this type of exploratory data analysis. Let''s do a quick review
    by looking at some important areas of exploratory data analysis using the following
    series of code blocks:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步将是进行一些初步的数据探索，以便了解哪些数据清理和准备步骤是必要的。R有一些非常有用的便捷包，可以用于这种类型的探索性数据分析。让我们通过查看以下代码块中的一些重要领域，快速回顾一下探索性数据分析：
- en: 'We will start by loading our data and libraries. To do this, we will use the
    base R `library()` function to load all of the libraries that we will need. If
    there are any libraries listed that you do not have installed, use the `install.packages()`
    function to install these libraries. We will also use the `read_csv()` function
    from the `readr` package to load in the data. We load libraries and data using
    the following code:'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将从加载数据和库开始。为此，我们将使用基础R中的`library()`函数加载我们所需的所有库。如果列表中有你没有安装的库，可以使用`install.packages()`函数来安装它们。我们还将使用`readr`包中的`read_csv()`函数来加载数据。我们将使用以下代码来加载库和数据：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Packages are included before the functions are called; therefore, this gives
    us an understanding of where and why each package is being used. As shown in the
    preceding code block, the following packages will be used in this chapter:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 包会在调用函数之前包含；因此，这能帮助我们理解每个包的使用位置和原因。如前面的代码块所示，本章将使用以下包：
- en: '`tidyverse`: This suite of packages will be used extensively. In this case,
    the `dplyr` package is used in this chapter for data wrangling, for instance,
    looking at aggregate values or adding and removing columns and rows.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`tidyverse`：这一系列的包将会广泛使用。在本章中，`dplyr`包用于数据清理，例如，查看汇总值或添加和删除列和行。'
- en: '`lubridate`: This will be used to easily extract details from a column holding
    values with a date data type.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`lubridate`：这个包将用于轻松提取包含日期数据类型的列中的详细信息。'
- en: '`xgboost`: This will be the model that we will use for our data.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xgboost`：这是我们将用于数据建模的模型。'
- en: '`Metrics`: This will be used to evaluate our model.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Metrics`：这个包将用于评估我们的模型。'
- en: '`DataExplorer`: This will be used for generating exploratory data analysis
    plots.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DataExplorer`：这个包将用于生成探索性数据分析的图表。'
- en: '`caret`: This will be used when tuning our model as it provides a convenient
    method for performing a grid search of hyperparameters.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`caret`：当我们调整模型时，将使用此包，它提供了一种方便的网格搜索超参数的方法。'
- en: 'Next, we will view the structure of the data using the `str` function, which
    provides details on the data object class and dimensions and column-specific details
    on the data type, along with some sample values, as shown in the following code:'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`str`函数查看数据的结构，该函数提供有关数据对象类别和维度的详细信息，以及每列数据类型的详细信息，并显示一些示例值，如下代码所示：
- en: '[PRE1]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After running the code, we will see the following printed to our console:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们将看到控制台输出如下内容：
- en: '![](img/2e73256f-5d6a-4543-a8ae-7865d1cd07be.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](img/2e73256f-5d6a-4543-a8ae-7865d1cd07be.png)'
- en: 'All of this data is from the same site, with readings for the same species
    of pollutants, and we can conclude that the unit of measure will likely be consistent
    throughout as well. If this is the case, we can remove these columns as they provide
    no informational value. Even if we did not know the first variable would always
    be the same, we can start to see this pattern from the results of the structure
    (`str`) function. We can confirm that this is the case, though, by running the
    following code, using the `group_by` and `summarise` functions from the `dplyr`
    package:'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有这些数据来自同一站点，且污染物的物种读数相同，我们可以得出单位度量在整个数据中可能保持一致的结论。如果是这种情况，我们可以删除这些列，因为它们没有提供任何信息价值。即使我们不知道第一个变量总是相同的，通过`str`函数的结果，我们也可以开始看出这个模式。我们可以通过运行以下代码来确认这一点，使用`dplyr`包中的`group_by`和`summarise`函数：
- en: '[PRE2]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After running the preceding code, we will see the following printed to our
    console:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，我们将看到控制台输出如下内容：
- en: '![](img/7e62e5f7-96fc-4110-adbf-c4835663da13.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](img/7e62e5f7-96fc-4110-adbf-c4835663da13.png)'
- en: We have confirmed that the `Site`, `Species`, and `Units` values are always
    the same, so we can remove them from the data as they will provide no information.
    We can also see that the actual reading values are stored as character strings
    and we have dates stored that way as well. In its current form, the `date` field
    exhibits a characteristic known as high cardinality, which is to say that there
    are a large number of unique values. When we see this, we will usually want to
    act on these types of columns so that they have fewer distinct values. In this
    case, the technique is clear because we know that this should be a date value.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经确认`Site`、`Species`和`Units`的值始终相同，因此可以将它们从数据中删除，因为它们不会提供任何有用信息。我们还可以看到，实际的读数值是以字符字符串的形式存储的，日期也以相同的方式存储。当前形式下，`date`字段表现出一种特征，称为高基数性，即存在大量唯一的值。当我们看到这种情况时，通常会希望对这些类型的列进行处理，以减少其独特值的数量。在这种情况下，方法是显而易见的，因为我们知道这应该是一个日期值。
- en: 'In the code that follows, we will use the `dplyr` `select` function to remove
    the columns that we don''t want to keep. We will use the `dplyr` `mutate` function
    along with functions from the `lubridate` package to transform the variables we
    have identified. After transforming the data, we can remove the old character
    string date column and the full date column as we will use the atomized date values
    going forward. We remove the columns we don''t need and break the converted date
    field into its component parts using the following code:'
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在接下来的代码中，我们将使用`dplyr`的`select`函数删除不需要保留的列。我们将使用`dplyr`的`mutate`函数，并结合`lubridate`包中的函数来转换我们已识别的变量。转换数据后，我们可以删除旧的字符字符串日期列和完整日期列，因为我们将使用原子化的日期值。我们删除不需要的列，并使用以下代码将转换后的日期字段拆分为其组成部分：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Before running the preceding code, we should note that the dataframe in our
    Environment pane looks like the following screenshot:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行上述代码之前，我们应该注意到我们的环境面板中的`dataframe`如下所示：
- en: '![](img/90893f34-2a4b-47f5-ac2c-b07650f18196.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](img/90893f34-2a4b-47f5-ac2c-b07650f18196.png)'
- en: 'After running the code, we can note the differences to the dataframe, which
    should now look like the following screenshot:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 运行代码后，我们可以注意到`dataframe`的变化，现在应该如下所示：
- en: '![](img/37ede6ab-693c-42d1-827a-39365609dfb9.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](img/37ede6ab-693c-42d1-827a-39365609dfb9.png)'
- en: As you can see, the columns that contained only one value have been removed,
    and the data column now occupies five columns with one for each of the date and
    time parts.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，只包含一个值的列已被删除，数据列现在占据了五个列，每个列对应一个日期和时间部分。
- en: 'Next, we will use the `DataExplorer` package to explore any missing values.
    There are numerous ways in which we summarize the number and proportion of missing
    values in a given data object. Of these, the `plot_missing()` function offers
    a count and percentage of missing values along with a visualization—all from one
    function call. We plot missing values using the following line of code:'
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将使用`DataExplorer`包来检查是否存在缺失值。有多种方法可以总结数据对象中缺失值的数量和比例。其中，`plot_missing()`函数可以一次性提供缺失值的计数、百分比及其可视化效果。我们通过以下代码行来绘制缺失值：
- en: '[PRE4]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'After running this code, a plot is produced. In your **Viewer** pane, you should
    see the following plot:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，会生成一个图表。在你的**查看器**面板中，你应该能看到如下图表：
- en: '![](img/bbf2f7c5-bd39-40b0-862a-bd86a8823db1.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](img/bbf2f7c5-bd39-40b0-862a-bd86a8823db1.png)'
- en: As you can see, there are no missing values among the independent variables.
    However, among the target variable class, there are around 1,500 missing values,
    accounting for 3.89% of the column.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，独立变量中没有缺失值。然而，在目标变量类别中，约有1,500个缺失值，占该列的3.89%。
- en: 'Since we have missing values, we should consider whether any action should
    be taken. In this case, we will simply remove these rows as there are not many
    of them, and the portion of the data with no missing values will still be representative
    of the entire dataset. While, in this case, the values are simply removed, there
    are a number of options available to handle missing values. A summary of possible
    actions is presented later on in this chapter. To remove the rows with missing
    values, we run the following line of code:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于存在缺失值，我们应该考虑是否需要采取任何措施。在这种情况下，我们将简单地删除这些行，因为它们并不多，并且没有缺失值的数据部分仍然能够代表整个数据集。虽然在此案例中我们仅删除了缺失值，但处理缺失值的方式有很多种选择。本章稍后将介绍可能的处理方法。为了删除含有缺失值的行，我们运行以下代码：
- en: '[PRE5]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We will also run a check on our discrete variable as well just to see the distribution
    of values among the categories present in this column. Again, the `DataExplorer`
    package offers a convenient function for this, which will provide a plot for the
    discrete values present noting the frequency of each. We generate this plot with
    the `plot_bar` function using the following line of code:'
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们还将检查我们的离散变量，以查看该列中各类别的值分布。同样，`DataExplorer`包提供了一个方便的函数，可以为离散值生成图表，显示每个值的频率。我们通过以下代码行使用`plot_bar`函数生成此图表：
- en: '[PRE6]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After running the preceding function, we are able to view the following visualization,
    which clearly shows that there are more **Ratified** results than **Provisional**
    results. From reading the documentation for this dataset, the **Ratified** results
    are verified and can be trusted to be accurate, while the **Provisional** results
    may not be as accurate. Let''s take a look at the output:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述函数后，我们能够看到以下可视化结果，清晰地显示了**批准**结果比**临时**结果更多。从查阅该数据集的文档中，**批准**结果是经过验证的，可以信赖其准确性，而**临时**结果可能不如其准确。让我们来看一下输出：
- en: '![](img/19b0d38a-2dac-4413-bb72-2d78982c2ec2.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](img/19b0d38a-2dac-4413-bb72-2d78982c2ec2.png)'
- en: 'We created a plot in order to quickly see the distribution of values among
    the discrete terms in the **Provisional** or **Ratified** column. We can also
    create a table using the following code to get more specific details:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建了一个图表，以便快速查看**临时**或**批准**列中离散项的值分布。我们还可以使用以下代码创建一个表格，以获取更具体的详细信息：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The preceding code uses the `group_by` function to group rows based on the
    values present in the `Provisional or Ratified` column. It then uses the `summarise`
    function, along with setting the `count` argument equal to `n()`, to calculate
    the number of rows containing each of the discrete values from the `Provisional
    or Ratified` column. Running the preceding code will print the output to your
    console, as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码使用`group_by`函数按`临时或批准`列中的值对行进行分组。接着，它使用`summarise`函数，并将`count`参数设置为`n()`，以计算包含每个离散值的行数。运行上述代码后，输出将显示在控制台中，如下所示：
- en: '![](img/dc87a317-c018-4d13-a281-87aa35e51c16.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![](img/dc87a317-c018-4d13-a281-87aa35e51c16.png)'
- en: 'Since there are very few values marked as `Provisional`, we will just remove
    these rows. To do so, we will first use the `filter` function, which is used to
    remove rows based on a particular condition. In this case, we will filter the
    data so that only rows with an `''R''` value in the `Provisional or Ratified`
    column will remain, as shown here:'
  id: totrans-71
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于标记为`Provisional`的值非常少，我们将删除这些行。为此，我们将首先使用`filter`函数，它用于根据特定条件删除行。在这种情况下，我们将过滤数据，使得`Provisional
    or Ratified`列中只有值为`'R'`的行保留下来，如下所示：
- en: '[PRE8]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next, we will remove the `Provisional or Ratified` column since it only holds
    one unique value. To do this, we will use the `select()` function, which is used
    to remove columns in a similar way to how a filter is used to remove rows. Here,
    the `select()` function is called and the argument passed to the function is ``
    -`Provisional or Ratified` ``, which will remove this column. Alternatively, all
    of the other column names, aside from `Provisional or Ratified`, could be passed
    in as an argument. The `select()` function works by either using the columns to
    include or the columns to exclude. In this case, it is faster to note the column
    to exclude, which is why this choice was made. Please refer to the following code:'
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们将删除`Provisional or Ratified`列，因为它只包含一个唯一值。为此，我们将使用`select()`函数，删除列的方式与删除行的过滤器类似。在这里，调用`select()`函数，传递给函数的参数是`-Provisional
    or Ratified`，这样就会删除这一列。或者，可以将所有其他列名（除了`Provisional or Ratified`）作为参数传递给它。`select()`函数通过指定要包含或排除的列来工作。在这种情况下，指定要排除的列更为高效，这就是为什么做出这个选择的原因。请参考以下代码：
- en: '[PRE9]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Earlier, we noted that all of our data is from the year 2018\. Now that our
    date data is broken up into component parts, we should only see one value in the
    `reading_year` column. One way to test whether this is the case is to use the
    `range()` function. We check for the minimum and maximum values in the `reading_year`
    column by running the following code:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 早些时候，我们提到所有数据都来自2018年。现在我们的日期数据已经被拆分成了各个组成部分，我们应该只在`reading_year`列中看到一个值。检验这一点的一个方法是使用`range()`函数。我们通过运行以下代码来检查`reading_year`列中的最小值和最大值：
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Running the preceding code will result in values being printed to our console.
    Your console should look like this:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码将导致值被打印到控制台。您的控制台应该如下所示：
- en: '![](img/066e67dd-a241-4afe-9579-76d2c8b912aa.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](img/066e67dd-a241-4afe-9579-76d2c8b912aa.png)'
- en: 'We can see from the results of our call to the `range()` function that, in
    fact, the `reading_year` column only includes one value. With this being the case,
    we can use the `select()` function to remove the `reading_year` column with the
    help of the following code:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们调用`range()`函数的结果中，我们可以看到，实际上`reading_year`列只包含一个值。既然如此，我们可以使用`select()`函数在以下代码的帮助下删除`reading_year`列：
- en: '[PRE11]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we can do a histogram check of continuous variables to look for outliers.
    To accomplish this, we will once again use the `DataExplorer` package. This time,
    we will use the `plot_histogram` function to visualize the continuous values for
    all of the columns with these types of values, as shown in the following code
    block:'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们可以对连续变量进行直方图检查，以寻找异常值。为了实现这一点，我们将再次使用`DataExplorer`包。这次，我们将使用`plot_histogram`函数来可视化所有具有这些类型值的列中的连续值，如下所示的代码块：
- en: '[PRE12]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'After running the preceding code, five plots are generated displaying the frequency
    for continuous values among the five columns that have these types of values.
    Your **Viewer** pane should look like the following:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 运行上述代码后，会生成五个图表，显示这五个包含连续值的列的频率。您的**查看器**面板应该如下所示：
- en: '![](img/918b3b3b-8119-4bc4-8bd0-7e88e503861b.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](img/918b3b3b-8119-4bc4-8bd0-7e88e503861b.png)'
- en: In the preceding output, we do see that our independent variable is slightly
    right-skewed, and, as a result, we could take some action on the outlier values.
    If there was a more dramatic skew, then we could apply a log transformation. As
    there are not many, we could also remove these values if we thought they were
    noisy. However, for now, we will just leave them in the data. If, in the end,
    our model is performing poorly, then it would be worthwhile to perform some outlier
    treatment to see whether this improves performance.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在上述输出中，我们确实看到我们的自变量稍微偏向右侧，因此，我们可以对异常值采取一些处理措施。如果存在更剧烈的偏斜，我们可以应用对数变换。由于异常值不多，我们也可以删除这些值，如果我们认为它们是噪声的话。然而，目前我们将这些值保留在数据中。如果最终我们的模型表现不佳，那么进行一些异常值处理，看看是否能改善性能，将是值得的。
- en: 'Lastly, let''s do a correlation check. We will again use the `DataExplorer`
    package, and this time we will use the `plot_correlation()` function. To generate
    a correlation plot, we run the following line of code:'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，让我们进行一次相关性检查。我们将再次使用`DataExplorer`包，这次我们将使用`plot_correlation()`函数。为了生成相关图，我们运行以下代码：
- en: '[PRE13]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Running the preceding line of code will generate a plot. The plot uses blue
    and red colors to denote negative and positive correlations, respectively. These
    colors will not be present in the diagram in this book; however, you can still
    see the correlation values. After running the preceding code, you will see a plot
    in your Viewer pane, which looks like the following:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 运行前面一行代码将生成一个图表。该图表使用蓝色和红色分别表示负相关和正相关。这些颜色在本书中的图表中不会出现；然而，你仍然可以看到相关值。在运行前面的代码后，你将在查看器面板中看到一个图表，如下所示：
- en: '![](img/f41a9729-df2e-493a-961b-f943ad256b53.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![](img/f41a9729-df2e-493a-961b-f943ad256b53.png)'
- en: From this plot, we can see that the pollutant value does have some correlation
    with the `reading_hour` feature, and less correlation with the `reading_day` and
    `reading_month` features, suggesting that there is a higher trend throughout the
    day for when this pollutant is being produced than in the week, month, or year.
    The main objective of this plot is to look for independent variables that are
    highly correlated as it might suggest that these variables are conveying the same
    information, and, in that case, we may want to remove one or combine them in some
    way.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个图表中，我们可以看到污染物值确实与`reading_hour`特征有一定的相关性，而与`reading_day`和`reading_month`特征的相关性较小，这表明该污染物的产生在一天内有较高的趋势，而不是在一周、一个月或一年内。这个图表的主要目的是寻找高度相关的独立变量，因为这可能意味着这些变量传达了相同的信息，在这种情况下，我们可能希望删除其中一个或以某种方式将它们合并。
- en: We now have a dataset that is properly preprocessed and ready for modeling.
    The correlation plot shows that the variables are not significantly correlated.
    Of course, this is not surprising as the remaining variables simply describe discrete
    moments in time.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个已经适当预处理并准备好建模的数据集。相关图显示这些变量之间没有显著的相关性。当然，这并不令人惊讶，因为剩余的变量只是描述时间中的离散时刻。
- en: The date value was converted from a string that provided no informational value
    to a date value further split into date parts represented as numeric data. All
    columns that contained only one value were removed as they provided no information.
    The few rows marked as provisional values were removed since there were not many
    of these, and, in the description of the data, there were warnings about the validity
    of pollutant measures marked this way. Lastly, rows containing null values for
    the predictor variable were removed. This was done because there were not many
    of these; however, there are other tactics we could have taken in this situation.
    We have noted them in the following section.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 日期值已从没有信息价值的字符串转换为日期值，进一步分割为以数字数据表示的日期部分。所有仅包含一个值的列都已被删除，因为它们没有提供任何信息。标记为临时值的少数行被删除，因为这些行不多，而且在数据描述中有关于这些污染物测量有效性的警告。最后，包含预测变量为空值的行被删除。之所以这样做，是因为这种行的数量不多；然而，我们在这种情况下本可以采取其他策略，我们将在下节中提到它们。
- en: Handling missing values
  id: totrans-93
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理缺失值
- en: 'In the preprocessing work that we just completed, we decided to remove missing
    values. This is an option when there are very few cases that contain missing values
    and, in this example, this was true. However, other situations may require different
    approaches to handling missing values. Here are some common options in addition
    to deleting rows and columns:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们刚刚完成的预处理工作中，我们决定删除缺失值。当缺失值的情况非常少时，这是一个可行的选择，在这个例子中确实如此。然而，其他情况下可能需要不同的方法来处理缺失值。除了删除行和列外，以下是一些常见的其他选项：
- en: '**Imputation with a measure of centrality** (**mean**/**median**/**mode**):
    Use one of the measures of centrality to fill in the missing values. This can
    work well if you have normally distributed numeric data. Modal imputation can
    also be used on non-numeric data by selecting the most frequent value to replace
    the missing values.'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用集中趋势度量进行插补**（**均值**/**中位数**/**众数**）：使用一种集中趋势度量来填充缺失值。如果你的数据是正态分布的数值数据，这种方法效果较好。对于非数值数据，也可以使用众数插补，通过选择最频繁的值来替换缺失值。'
- en: '**Tweak for the missing values**: You can use the known values to impute the
    missing values. Examples of this approach include using regression with linear
    data or the **k-nearest neighbor** (**KNN**) algorithm to assign a value based
    on similarity to known values in the feature space.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**调整缺失值**：你可以使用已知值来填补缺失值。此方法的例子包括使用回归分析处理线性数据或使用**k近邻**（**KNN**）算法，根据特征空间中与已知值的相似度来分配一个值。'
- en: '**Replace it with a constant value**: The missing value can also be replaced
    with a constant value outside the range of values present or not already present
    in the categorical data. The advantage here is that it will become clear later
    on whether these missing values have any informational value, as they will be
    clearly set to the side. This is in contrast to imputing with a measure of centrality
    where the final result will be some missing values now containing the imputed
    value, while some equal values will have actually already been present in the
    data. In this case, it becomes difficult to know which values were missing values
    and which were the values already present in the data.'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**用常数值替换**：缺失值也可以用一个常数值替换，该常数值不在已有数据的值范围内，也未出现在分类数据中。这样做的好处是，后来会更清楚这些缺失值是否具有信息价值，因为它们会被明确地放置在一边。与用集中趋势度量填补缺失值的方式不同，集中趋势填补后的最终结果会使一些缺失值包含填补后的值，而有些相同的值实际上已经存在于数据中。在这种情况下，便难以区分哪些是缺失值，哪些是原本已存在的数据值。'
- en: Training a model on prepared data
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练一个模型来拟合准备好的数据
- en: Now that the data is ready, we will split it into train and test sets and run
    a simple model. The objective at this point is not to try to achieve the best
    performance, but rather to get some type of a benchmark result to use in the future
    as we try to improve our model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据准备好了，我们将其划分为训练集和测试集，并运行一个简单的模型。此时的目标不是尽力取得最佳性能，而是得到一种基准结果，以便未来在尝试提升模型时使用。
- en: Train and test data
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练数据和测试数据
- en: 'When we build predictive models, we need to create two separate sets of data
    with the help of the following segments. One is used by the model to learn the
    task and the other is used to test how well the model learned the task. Here are
    the types of data that we will look at:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建预测模型时，我们需要通过以下几个部分创建两个独立的数据集。一个用来让模型学习任务，另一个用来测试模型是否学会了这个任务。以下是我们将要查看的数据类型：
- en: '**Train data**: The segment of the data used to fit the model. The model has
    access to the explainer variables or independent variables, which are the selected columns,
    to describe a record in your data, as well as the target variable or dependent
    variable. That is the value we are trying to predict during the training process
    using this dataset. This segment should usually be between 50% and 80% of your
    total data.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练数据**：用于拟合模型的数据部分。模型可以访问解释变量或独立变量（即被选择的列，用来描述数据记录），以及目标变量或依赖变量。我们在训练过程中尝试预测的就是这个目标变量。这个数据部分通常应该占你总数据的50%到80%。'
- en: '**Test data**: The segment of the data used to evaluate the model results.
    The model should never have access to this data during the learning process and
    should never see the target variable. This dataset is used to test what the model
    has learned about the dependent variables. After fitting our model during the
    training phase, we now use this model to predict values on the test set. During
    this phase, only we have the correct answers; the independent variable, that is,
    the model, never has access to these values. After the model makes its predictions,
    we can evaluate how well the model performed by comparing the predicted values
    to the actual correct values.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试数据**：用于评估模型结果的数据部分。在学习过程中，模型永远不能接触到这些数据，也永远不应看到目标变量。这个数据集用于测试模型在依赖变量上的学习情况。在训练阶段拟合模型后，我们现在使用这个模型来预测测试集中的值。在这一阶段，只有我们拥有正确答案；而独立变量，即模型，永远无法接触到这些值。模型做出预测后，我们可以通过将预测值与实际正确值进行比较来评估模型的表现。'
- en: '**Validation data**: Validation data is a portion of the training dataset that
    the model uses to refine hyperparameters. As the varying values are selected for
    the hyperparameters, the model makes checks against the validation set and uses
    the results gathered during this process to select the values for the hyperparameters
    that produce the best performing models.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证数据**：验证数据是训练数据集的一部分，模型利用它来调整超参数。随着超参数的不同值被选定，模型会对验证集进行检查，并使用在此过程中收集的结果来选择产生最佳性能模型的超参数值。'
- en: '**C****ross-validation**: One potential issue that can arise when we use only
    one train and test set is that the model will learn about specific descriptive
    features that are particular to this segment of the data. What the model learns
    may not generalize well when applied to other data in the future. This is known
    as overfitting. To mitigate this problem, we can use a process known as cross-validation.
    In a simple example, we can do an 80/20 split of the data where 20% is held for
    test data and we can model and test on this split. We can then create a separate
    80/20 split and do the same modeling and testing. We can repeat this process 5
    times with 5 different test sets—each composed of a different fifth of the data.
    This exact type of cross-validation is known as 5-fold cross-validation. After
    all of the iterations are completed, we can check whether the results are consistent
    for each. And, if so, we can feel more confident that our model is not overfitting
    and use this to generalize on more data.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**交叉验证**：当我们只使用一个训练集和测试集时，可能会出现一个潜在问题，即模型会学习到一些特定的描述性特征，这些特征是该数据段特有的。模型所学到的内容在未来应用于其他数据时可能无法很好地推广。这种情况被称为过拟合。为了减轻这个问题，我们可以使用一种叫做交叉验证的过程。在一个简单的例子中，我们可以将数据按80/20的比例划分，其中20%作为测试数据，模型在这个划分上进行建模和测试。然后，我们可以创建一个独立的80/20划分，进行相同的建模和测试。我们可以重复这个过程5次，使用5个不同的测试集——每个测试集由数据的五分之一组成。这种精确的交叉验证方法被称为5折交叉验证。在所有迭代完成后，我们可以检查每次迭代的结果是否一致。如果一致，我们可以更有信心地认为我们的模型没有过拟合，并可以利用这个模型在更多数据上进行推广。'
- en: Choosing an algorithm
  id: totrans-106
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 选择算法
- en: For this task, we will use `xgboost`, which is a very popular implementation
    of the gradient tree boosting algorithm. The reason this works so well is that
    each model iteration learns from the results of the previous model. This model
    uses boosting for iterative learning in contrast to bagging. Both of these ensembling
    techniques can be used to compensate for a known weakness in tree-based learners,
    which has to do with overfitting to the training data.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们将使用`xgboost`，这是一个非常流行的梯度树提升算法的实现。它之所以效果如此好，是因为每次模型迭代都会从前一个模型的结果中学习。与bagging相比，该模型使用提升方法进行迭代学习。这两种集成技术都可以用来弥补基于树的学习者在训练数据上过拟合的已知弱点。
- en: One simple difference between bagging and boosting is that, with bagging, full
    trees are grown and then the results are averaged, while, with boosting, each
    iteration of the tree model learns from the model before it. This is an important
    concept, as this idea of an algorithm that incorporates an additive function with
    information gained after modeling on the residuals of the previous model will
    be used in deep learning as we move forward.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: bagging和boosting之间的一个简单区别是，在bagging中，会生长完整的树，然后将结果进行平均，而在boosting中，每次树模型的迭代都会从前一个模型中学习。这是一个重要的概念，因为这个结合了加法函数的算法思想，在对前一个模型残差建模后获得信息，将在深度学习中得到应用。
- en: 'Here, we will explore the power of this type of machine learning algorithm
    on this simple example. Additionally, we will pay particular attention to how
    what we learn here is relevant for more complex examples in the subsequent chapters.
    We will use the following code to train an `xgboost` model:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将探索这种类型的机器学习算法在这个简单例子中的强大功能。此外，我们还将特别关注我们在这里学到的内容如何与后续章节中更复杂的例子相关。我们将使用以下代码来训练一个`xgboost`模型：
- en: 'We start the process of fitting a model to our data by partitioning our data
    into train and test sets, using the following code:'
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过以下代码开始将模型拟合到数据的过程，首先将数据划分为训练集和测试集：
- en: '[PRE14]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: In the preceding code, we started with the `set.seed()` function. This is because
    aspects of this modeling process involve pseudorandomness, and setting the seed
    ensures that the same values are used for these elements every time, so we can
    consistently produce the same results.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: We split the data into training data, which we will model on, and a test set
    to check whether our predictions are accurate. We do this by getting a random
    sample of row index values, which we store in the vector labeled `partition`,
    and use them to subset our data. In addition, after partitioning, we split out
    the target variable and store this in a vector.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Then, we convert our data into a dense matrix so that it is in the proper format
    to be passed to the `xgboost` algorithm.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: 'Following this, we will create a list of parameters. Some of these values are
    required for the analysis that we will be doing and others are just starting values
    chosen arbitrarily. For those values, later on, we will look at ways to more scientifically
    choose them. We prepare our initial parameter list using the following code:'
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'In the preceding code, the required values in this list include the following:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: '`objective = "reg:linear"`: This is used to define the task objective as linear
    regression. Here, we are conducting a regression task seeking the value of nitrogen
    dioxide.'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`booster = "gbtree"`: This tells us that we will use gradient tree boosting
    to choose the best model for predicting results.'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`eval_metric = "rmse"`: This tells us that we will use the **Root Mean Squared
    Error** (**RMSE**) to evaluate the success of our model. Later on, we will look
    at why this is the most appropriate choice, along with some of the other options
    we can use here for other tasks.'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Next, these are the variables where we are arbitrarily choosing starting values:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '`eta=0.1`: This is used to define the learning rate. To begin, it makes sense
    to use a larger number; however, as we move forward, we will want to use a smaller
    learning rate and additional rounds to improve performance.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`subsample=0.8`: This tells us that, for each tree, we will use 80% of the
    rows from the data.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`col_subsample=0.75`: This tells us that, for each tree, we will use 75% of
    the columns from the data.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These are the parameters that do not impact the model and only impact how we
    review the results of the model:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '`print_every_n = 10`: This is used to state that the evaluation scores should
    be printed after every 10 rounds.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`verbose = TRUE`: This is used to denote that the evaluation scores should
    be printed to the console so that they can be seen by the end user during the
    model run.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have the parameters defined, we will run the model using the following
    code:'
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: When we run the model, we bring in the list of parameters that we previously
    defined, that is, the model should be run against the training dataset and we
    choose to run the model for 100 rounds. This means that we will grow 100 trees.
    This is, again, an arbitrary value. Later, we will look at ways to discover the
    optimal number of rounds. Then, we predict the test dataset using the model that
    we just defined. For our train dataset, the algorithm knows the correct value
    and uses this to adjust the model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we will apply the model to the test data where the model
    no longer has access to the correct values, and, without this knowledge, the model
    uses the independent variables to make predictions for the target variable. Please
    refer to the following code block:'
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: When we run the preceding code, we take the data from the dense matrix, labeled
    `dtest`, and run it through our model labeled `xgb`. The model takes the tree
    splits that were calculated during training and applies them to the new data to
    make predictions. The predictions are stored in a vector called `pred`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we will use the RMSE function to check model performance. For this
    evaluation metric, the closer it is to zero, the better, as this is a measure
    of the difference between true values and predicted values. To evaluate the performance
    of our model, we run the following line of code:'
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'After running the preceding code, we will see a value printed to our console.
    Your console should look like the following output:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/18404f69-a22b-4d99-b795-01f4b55398fb.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
- en: From our quick and simple model, we have achieved an RMSE score of 0.054\. Soon,
    we will make some changes to the model parameters to try to improve our score.
    Before that, let's take a quick look at all of the different evaluation metrics
    that we can use in addition to RMSE while also taking a deep dive into explaining
    how RMSE works.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating model results
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We only know whether a model is successful if we can measure it, and it is worthwhile
    taking a moment to remember which metrics to use in which scenarios. Take, for
    example, a credit card fraud dataset where there is a large imbalance in the target
    variable because there will only be a, relatively, few cases of fraud among many
    non-fraudulent cases.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: If we use a metric that just measures the percentage of the target variable
    that we predict successfully, then we will not be evaluating our model in a very
    helpful way. In this case, to keep the math simple, let's imagine we have 10,000
    cases and only 10 of them are fraudulent accounts. If we predict that all cases
    are not fraudulent, then we will have 99.9% accuracy. This is very accurate, but
    it is not very helpful. Here is a review of the different metrics and when to
    use them.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Machine learning metrics
  id: totrans-142
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Choosing the wrong metric will make it very difficult to evaluate performance
    and, as a result, improve our model. Therefore, it is very important to choose
    the right metric. Let''s take a look at the following machine learning metrics:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '**Accuracy**: The simplest evaluation metric is accuracy. Accuracy measures
    the difference between the predicted value and the actual value. This metric is
    easy to interpret and communicate; however, as we mentioned earlier, it doesn''t
    measure performance well when used to evaluate a highly unbalanced target variable,
    for example.'
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confusion Matrix**: The confusion matrix provides a convenient way to display
    classification accuracy along with Type I and Type II errors. The combined view
    of these four related metrics can be especially informative in deciding where
    to focus our efforts during the tuning process. It can also help mark cases where
    other metrics may be more helpful. When there are too many values in the majority
    class, then a metric that is designed for use with class imbalances, such as log-loss,
    should be employed.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Mean Absolute Error** (**MAE**): This metric takes the difference between
    the predicted value and the actual value and calculates the mean value of these
    errors. This metric is simple to interpret and is useful when there is no need
    to apply an additional penalty to large errors. If an error that is three times
    larger than another error is three times as bad, then this is a good metric to
    use. However, there are many cases where an error that is three times larger than
    another error is much more than three times as bad, and this results in adding
    an additional penalty, which can be accomplished with the next metric.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RMSE**: This metric takes the square of the error for every prediction, the
    difference between the predicted value and the actual value, sums these squared
    errors, and then takes the square root of the sums. In this case, if the squaring
    has even a few highly inaccurate predictions, it will result in a sizable penalty
    and a higher value on this error metric. We can see how this would help in our
    preceding example and why we have chosen to use RMSE. This metric is used for
    regression.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Area Under the Curve** (**AUC**): The AUC refers to the *Area under the Receiver-Operator
    Curve*. In this model, your target variable needs to be a value expressing the
    confidence or probability that a row belongs to the positive or negative target
    condition. To make this more concrete, AUC can be used when your task is to predict
    how likely someone is to make a given purchase. Clearly, from this explanation,
    we can see that AUC is a metric for classification.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logarithmic Loss** (**Log-Loss**): The log-loss evaluation metric rewards
    confident predictions more than AUC and penalizes neutral predictions. This is
    important when we have an imbalanced target dataset and finding the minority class
    is critical. Having an extra penalty on incorrect guesses helps us get to the
    model that better predicts these minority class members correctly. Log-loss is
    also better for multiclass models where the target variable is not binary.'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Improving model results
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Since we have a regression problem, we now know why we chose RMSE, and we have
    a baseline metric of performance, we can begin to work on improving our model.
    Every model will have its own different way of improving results; however, we
    can generalize slightly. Feature engineering helps to improve model performance;
    however, since this type of work is less important with deep learning, we will
    not focus on that here. Also, we have already used feature engineering to generate
    our date and time parts. In addition, we can run our model for longer at a slower
    learning rate and we can tune hyperparameters. In order to find the best values
    using this type of model improvement method, we will use a technique called **grid
    search** to look at a range of values for a number of different fields.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Let's search for the optimal number of rounds. Using the cross-validation version
    of `xgboost` through the R interface, we can train our model again on our default
    hyperparameter settings. This time, instead of choosing 100 rounds, we will use
    the functionality within `xgboost` to determine the optimal number of trees to
    grow. Using cross-validation, the model can evaluate the error rate at the end
    of every round, and we will use the `early_stopping_rounds` feature so that the
    model stops growing additional trees after a given number of attempts, when it
    no longer continues to decrease the error rate.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a look at the following code, which determines the number of rounds
    or number of trees that produces the lowest error rate given the default setting:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'After we run the preceding code, we will see model performance metrics printing
    to the console as the model runs. The beginning of this report on the console
    should look like the following screenshot:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3f1eb80-0aef-4aca-ab6c-b7fe03e2b5dc.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that our model performance is improving rapidly, and we have
    confirmation that our model will stop training when the model hasn't improved
    for 25 rounds.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: 'When your model reaches the optimal number of runs, your console should look
    like the following screenshot:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/405145eb-3455-465a-90b3-7f44a11164e9.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that the model performance is improving more slowly and that
    the model has stopped because it was no longer improving. The report that is printed
    out to the console also identifies the best performing round.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down everything that is happening in the preceding example, we again
    train an `xgboost` model on the same data with the same parameters as we did previously.
    However, we set the number of rounds to be much higher. In order to find the best
    iteration, we need enough rounds so that the model doesn't stop growing trees
    before finding the optimal number of trees. In this case, the best iteration occurs
    at around 3,205; so, if we had set the number of rounds at 1,000, for example,
    the modeling process would have completed after growing 1,000 trees. However,
    we still would not know the number of rounds that produces the lowest error rate,
    which is why we set the number of rounds to be so high.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'The following is the list of settings being used in the preceding code. Let''s
    take a look at the purpose of using each of these settings:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '`nfold`: This is the number of folds or how many partitions to make in the
    data for cross-validation. Here, we use `5`, which utilizes the alternating 80/20
    split referenced earlier.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`showsd`: This shows the standard deviation in order to note the variation
    among the results from the different combinations of folds. This is important
    to ensure the model works well on all sets of data and will generalize well when
    used on future data.'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`stratified`: This ensures that each fold of data contains the same proportion
    of the target class.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`print_every_n`: This tells us how often to print the results of the cross-validation
    to the console.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`early_stopping_rounds`: This is a value that decides when the model should
    stop growing trees. You can use this value to check whether performance has improved
    during the given number of rounds. The process will stop when the model no longer
    improves while growing trees to the limit of rounds set.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`maximize`: This notes whether the evaluation metric is one where improvement
    involves maximizing the score or minimizing the score.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, let''s do a grid search on select hyperparameters. As the name implies,
    a grid search will model for all defined hyperparameter value combinations. Using
    this technique, we can adjust a few settings that will control how the model grows
    trees and then evaluate which settings provide the best performance. A complete
    list of all of the tunable hyperparameters for `xgboost` is included with the
    package documentation. For this example, we will focus on the following three
    hyperparameters:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: '`max_depth`: This is the maximum depth of the tree. Since we only have 4 features,
    we will try depths of `2`, `3`, and `4`.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`gamma`: This is the minimum loss reduction needed to continue creating splits.
    Setting this level higher will create shallower trees as nodes that contribute
    less to reducing the error rate are not further divided. We will try values of
    `0`, `0.5`, and `1`.'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_child_weight`: This is the number of instances needed to grow a node.
    If there are fewer instances than the threshold, then the tree will discontinue
    partitioning from this node. The higher this number, the more shallow trees that
    will be grown. For this example, we will try values of `1`, `3`, and `5`.'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will now go through all the code required to perform a grid search to tune
    our parameters to the optimal values in order to improve model performance:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'Our first step will be to define our search grid by assigning the vector of
    values, mentioned previously, to their respective hyperparameter within the parameter
    grid. We define the values we will try for our hyperparameters by running the
    following code:'
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: As shown in the preceding code, for expediency, we will set the number of rounds
    to 500\. Though, you could use the rounds found by searching for the best iteration
    previously in a real-world situation.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: When including `eta` in your grid search, also remember to include `nrounds`
    as you will need more rounds as the learning rate decreases.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: 'After this, we will use the `trainControl` function within `caret` to define
    how we want to handle this parameter search. For this, we will list the code and
    then walk through the settings selected. There are many additional settings for
    `trainControl`; however, we are focusing on a select few for this chapter. We
    set how we will train our model using the following code:'
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The `method` and `number` parameters for this function simply define our cross-validation
    strategy, which will be 5-fold again, as used previously. We will use a grid search
    to go through all possible combinations among the parameter settings defined in
    the last section of code. The next two parameters are used to save the resample
    and prediction details for the best iteration after modeling on all combinations.
    Lastly, we set `verboseIter` to `TRUE` to print iteration details to the console,
    and `allowParallel` is set to `TRUE` to use parallel processing to increase computational
    speed.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: 'With the grid search values in place and the search strategy defined, we now
    run the model again using every possible hyperparameter combination. We then train
    our model while employing a grid search of our hyperparameters using the settings
    from the first two steps:'
  id: totrans-181
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After running this code, we will see a report printing to our console that
    is similar to when we ran our model the first time. Your console should look like
    the following screenshot:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0fd3e9e5-7cb7-4d1d-86a1-906cc08b8020.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
- en: The report shows the current fold and current parameter setting as the model
    goes through all combinations on all five different splits of the data. In the
    preceding screenshot, we see a test of all `min_child_weight` options holding
    everything else constant on the fifth split of the data. In the end, we see the
    best tuning parameters being selected.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: In this situation, the best depth is using all of the features, which is not
    surprising given the lack of features. The best minimum child weight is `1`, which
    means that even sparsely populated nodes still hold important information for
    our model. The best gamma value is `0`, which is the default value. As this value
    rises, it places a slight constraint on the error rate improvement needed by each
    node before splitting. In this case, after our grid search, we are largely left
    with the default values; yet, we can see the process by which we would choose
    alternatives to these defaults if they helped to improve model performance.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we know the best hyperparameter settings, we can plug them back into
    the model that we ran before and see whether there is any improvement. We train
    our model using the parameters and iteration count that we found optimizes performance
    by running the following code:'
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When we run the preceding code, we train our model and make predictions, like
    we did earlier, and we also run the line of code to calculate the RMSE value.
    When we run this line, we will see a value printed to our console. Your console
    should look like the following screenshot:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5d8f1590-72d6-450f-8c12-05069c79dbc6.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
- en: From the results of calculating the error rate, we can see that the score has
    changed from 0.054 to 0.022, which is an improvement from our first attempt. Using
    this data, with limited features, we may think that a time series model would
    have been a better choice; however, with only 1 year of data, a time series approach
    wouldn't catch any late seasonality effects that are not already present. This
    type of modeling creates a map for future years and shows that missing data can
    be predicted by simply using the date and time values for known data. This means
    that we can estimate NO2 values for future years. After collecting several years
    of data, a time series approach could then be used to make predictions that take
    into account year over year trends, in addition to the seasonality information
    captured here.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: We used `xgboost`, which is a popular tree boosting algorithm, to predict pollution
    levels in an area of London. Earlier, we walked through creating a simple model
    to establish a benchmark. We then looked at how we should measure performance.
    Then, we took steps that improved performance. While we selected `xgboost` for
    this task, there are other machine learning algorithms that we could choose from,
    which we will review next.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing different algorithms
  id: totrans-193
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have raced through machine learning relatively quickly, as we wanted to focus
    on the underlying concepts that will follow along with us as we head into deep
    learning. As such, we cannot offer a comprehensive explanation of all machine
    learning techniques; however, we will quickly review the different algorithm types
    here, as this will be helpful to remember going forward.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: 'We''ll do a quick review of the following machine learning algorithms:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
- en: '**Decision Trees**: A decision tree is a simple model that makes up the base
    learners of many more complex algorithms. A decision tree simply splits a dataset
    at a given variable and notes the proportion of the target class that exists in
    the splits. For example, if we were to predict who is more likely to enjoy playing
    with baby toys, then a split on age would likely show that the split of the data
    containing just those under the age of 3 has a high percentage of true results
    in the `target` variable, that is, a high proportion that does enjoy this type
    of activity, while those who are older would likely not enjoy this.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Random Forests**: Random forests are similar to `xgboost`, which was used
    in this brief machine learning overview. A notable distinction between random
    forests and `xgboost` is that random forests build full decision trees. This makes
    up the set of base learners. The results from these simple base learner models
    are then averaged together to arrive at predictions that are better than any base
    learner. This technique is known as bagging. In contrast, `xgboost` uses boosting,
    which includes what it has learned from building previous base learners as it
    applies additional decision trees to the data. While both are useful and powerful
    ways to ensemble results and improve performance, we have chosen to focus on `xgboost`
    in this example because this idea of carrying forward information learned from
    the previous iteration is also present in deep learning.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Logistic Regression and Support Vector Machines** (**SVM**):SVMs separate
    features in *n*-dimensional space with a line that is the farthest from the two
    closest points in that space. This boundary is then applied to the test data and
    points on one side are classified one way, while points on the other are classified
    as a member of the other class. This is similar to logistic regression, with the
    main difference being that logistic regression evaluates all data points, while
    SVM just includes the points nearest to the line used to split the data. In addition,
    logistic regression works better when there are fewer explainer variables, and
    SVM works better when the dataset contains a larger number of dimensions. SVM
    will seek to find a line that divides all features, while logistic regression
    will use a combination of best-fitting, but not perfect, lines to estimate the
    probability that a data point belongs to a particular member of the target variable.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**KNN** **and k-means**: These are two ways to create clusters within our data.
    KNN is a supervised learning technique. Using this method, the model plots the
    points on a *k*-dimensional feature space. When new points are introduced during
    the training process, the model identifies the class for the nearest neighbors
    to the new point and assigns this class to this record. By contrast, *k*-means
    is an unsupervised learning technique that finds centroids in the feature space
    such that *k* clusters can be created where each point is classified based on
    the minimum distance to a given centroid.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GBM and LightGBM**: Aside from `xgboost`, GBM and LightGBM also provide a
    means to generate predictions using a boosting mechanism for improving model performance
    between iterations. **Gradient Boosting Machines** (**GBM**) is the precursor
    to `xgboost` and LightGBM. It largely operates the same way by using a boosting
    ensemble technique on decision tree base learners; however, it is more primitive
    in its approach. GBM grows full trees using all features, while `xgboost` and
    LightGBM have different ways to reduce the number of splits that take place, which
    speed up how fast trees can be grown.'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The biggest difference between `xgboost` and LightGBM is that, where `xgboost`
    grows a new level for every tree after computing the feature splits, LightGBM
    will just grow the level below the most predictive leaf. This leaf-wise splitting
    offers superior speed advantages over the level-wise splitting used by `xgboost`.
    Also, with the focus on single leaf splits, the model can better find the values
    that minimize error compared with splitting into entire levels, which can lead
    to better performance. LightGBM may overtake `xgboost` as the go-to model for
    practitioners; however, for now, `xgboost` is still more widely used, which is
    why it was selected for this brief overview.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-202
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we referred to a raw dataset, explored the data, and took the
    necessary preprocessing steps to get the data ready for modeling. We performed
    data type transformations to convert numbers and dates being stored as character
    strings into numeric and date value columns, respectively. In addition, we performed
    some feature engineering by breaking up the date value into its component parts.
    After completing preprocessing, we modeled our data. We followed an approach that
    included creating a baseline model and then tuning hyperparameters to improve
    our initial score. We used early stopping rounds and grid searches to identify
    hyperparameter values that produced the best results. After modifying our model-based
    results from our tuning procedures, we noticed much better performance.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: All of the aspects of machine learning that were discussed in this chapter will
    be used in the subsequent chapters too. We will need to get our data ready for
    modeling, and we will need to know how we can improve model performance by adjusting
    its settings. In addition, we have been focusing on a decision tree ensembling
    model in `xgboost` because our work with neural networks in upcoming chapters
    will be similar. We will need to consider efficiency and performance just as we
    did with `xgboost`, by adjusting how trees are grown.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: This review of machine learning provides the foundation for stepping into deep
    learning. We begin, in the next chapter, with installing and exploring the packages
    used.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
