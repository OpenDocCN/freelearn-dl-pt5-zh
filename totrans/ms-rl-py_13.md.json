["```py\nclass MountainCar(gym.Env):\n    def __init__(self, env_config={}):\n        self.wrapped = gym.make(\"MountainCar-v0\")\n        self.action_space = self.wrapped.action_space\n        ...\n```", "```py\n$ virtualenv rlenv\n$ source rlenv/bin/activate\n$ pip install gym[box2d]\n$ pip install tensorflow==2.3.1\n$ pip install ray[rllib]==1.0.1\n```", "```py\nALL_STRATEGIES = [\n    \"default\",\n    \"with_dueling\",\n    \"custom_reward\",\n    ...\n]\nSTRATEGY = \"default\"\n```", "```py\nSTRATEGY = \"default\"\n```", "```py\nAverage episode length: 192.23\n```", "```py\n    def step(self, action):\n        self.t += 1\n        state, reward, done, info = self.wrapped.step(action)\n        if self.reward_fun == \"custom_reward\":\n            position, velocity = state\n            reward += (abs(position+0.5)**2) * (position>-0.5)\n        obs = self._get_obs()\n        if self.t >= 200:\n            done = True\n        return obs, reward, done, info\n```", "```py\nSTRATEGY = \"custom_reward\"\n```", "```py\nAverage episode length: 131.33\n```", "```py\n    def _get_init_conditions(self):\n        if self.lesson == 0:\n            low = 0.1\n            high = 0.4\n            velocity = self.wrapped.np_random.uniform(\n                low=0, high=self.wrapped.max_speed\n            )\n        ...\n```", "```py\nCURRICULUM_TRANS = 150\n...\ndef set_trainer_lesson(trainer, lesson):\n    trainer.evaluation_workers.foreach_worker(\n        lambda ev: ev.foreach_env(lambda env: env.set_lesson(lesson))\n    )\n    trainer.workers.foreach_worker(\n        lambda ev: ev.foreach_env(lambda env: env.set_lesson(lesson))\n    )\n    ...\ndef increase_lesson(lesson):\n    if lesson < CURRICULUM_MAX_LESSON:\n        lesson += 1\n    return lesson    if \"evaluation\" in results:\n        if results[\"evaluation\"][\"episode_len_mean\"] < CURRICULUM_TRANS:\n            lesson = increase_lesson(lesson)\n            set_trainer_lesson(trainer, lesson)\n```", "```py\n            if results[\"evaluation\"][\"episode_len_mean\"] < CURRICULUM_TRANS:\n                lesson = increase_lesson(lesson)\n                set_trainer_lesson(trainer, lesson)\n                print(f\"Lesson: {lesson}\")\n```", "```py\nSTRATEGY = \"curriculum\"\n```", "```py\nAverage episode length: 104.66\n```", "```py\n        ...\n        new_obs, r, done, info = env.step(a)\n        # Build the batch\n        batch_builder.add_values(\n            t=t,\n            eps_id=eps_id,\n            agent_index=0,\n            obs=prep.transform(obs),\n            actions=a,\n            action_prob=1.0,  # put the true action probability here\n            action_logp=0,\n            action_dist_inputs=None,\n            rewards=r,\n            prev_actions=prev_action,\n            prev_rewards=prev_reward,\n            dones=done,\n            infos=info,\n            new_obs=prep.transform(new_obs),\n        )\n        obs = new_obs\n        prev_action = a\n        prev_reward = r\n```", "```py\n    def update_avail_actions(self):\n        self.action_mask = np.array([1.0] * self.action_space.n)\n        pos, vel = self.wrapped.unwrapped.state\n        # 0: left, 1: no action, 2: right\n        if (pos < -0.3) and (pos > -0.8) and (vel < 0) and (vel > -0.05):\n            self.action_mask[1] = 0\n            self.action_mask[2] = 0\n```", "```py\nclass ParametricActionsModel(DistributionalQTFModel):\n    def __init__(\n        self,\n        obs_space,\n        action_space,\n        num_outputs,\n        model_config,\n        name,\n        true_obs_shape=(2,),\n        **kw\n    ):\n        super(ParametricActionsModel, self).__init__(\n            obs_space, action_space, num_outputs, model_config, name, **kw\n        )\n        self.action_value_model = FullyConnectedNetwork(\n            Box(-1, 1, shape=true_obs_shape),\n            action_space,\n            num_outputs,\n            model_config,\n            name + \"_action_values\",\n        )\n        self.register_variables(self.action_value_model.variables())\n    def forward(self, input_dict, state, seq_lens):\n        action_mask = input_dict[\"obs\"][\"action_mask\"]\n        action_values, _ = self.action_value_model(\n            {\"obs\": input_dict[\"obs\"][\"actual_obs\"]}\n        )\n        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)\n        return action_values + inf_mask, state\n```", "```py\n    if strategy == \"action_masking\":\n        config[\"hiddens\"] = []\n        config[\"dueling\"] = False\n        ModelCatalog.register_custom_model(\"pa_model\", ParametricActionsModel)\n        config[\"env_config\"] = {\"use_action_masking\": True}\n        config[\"model\"] = {\n            \"custom_model\": \"pa_model\",\n        }\n```", "```py\nSTRATEGY = \"action_masking\"\n```", "```py\nAverage episode length: 147.22\n```"]