- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Managing SageMaker Development Environment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In previous chapters, we learned about the fundamental components and capabilities
    of Amazon SageMaker. By now, you know how to build and deploy your first simple
    models on SageMaker. In many more complex cases, however, you will need to write,
    profile, and test your DL code before deploying it to SageMaker-managed training
    or hosting clusters. Being able to perform this action locally while mocking SageMaker
    runtime will shorten development cycles and will avoid any unnecessary costs associated
    with provisioning SageMaker resources for development.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore how to organize your development environment
    to effectively develop and test your DL models for SageMaker. This chapter includes
    considerations for choosing your IDE software for development and testing, as
    well as simulated SageMaker runtimes on your local machine. We will also provide
    an overview of available SDKs and APIs to manage your SageMaker resources.
  prefs: []
  type: TYPE_NORMAL
- en: 'These topics will be covered in the following sections:'
  prefs: []
  type: TYPE_NORMAL
- en: Selecting a development environment for SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging SageMaker code locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After reading this chapter, you will be able to set up an efficient development
    environment compatible with SageMaker, based on your specific use case requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you can use code walk-through samples, so you can develop
    practical skills. Full code examples are available here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter3/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter3/).
    To follow along with this code, you will need to have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An AWS account and be an IAM user with permission to manage Amazon SageMaker
    resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have Docker and Docker Compose installed on your local machine. If your development
    environment has a GPU device, you will need to install `nvidia-docker` ([https://github.com/NVIDIA/nvidia-docker](https://github.com/NVIDIA/nvidia-docker)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have Conda installed ([https://docs.conda.io/en/latest/](https://docs.conda.io/en/latest/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Selecting a development environment for SageMaker
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The choice of development environment and IDE is typically driven by personal
    preferences or corporate policies. SageMaker being a cloud platform doesn’t restrict
    you from using an IDE of your choice. You can run an IDE on your local machine
    or cloud machine (such as Amazon EC2). SageMaker also provides a set of SDKs and
    packages to simulate SageMaker runtime environments, so you can first test your
    code using local mocks before deploying anything to the cloud.
  prefs: []
  type: TYPE_NORMAL
- en: With advances in data science, and machine learning specifically, a new type
    of development runtime environment has evolved – **interactive notebooks**, namely
    **Jupyter Notebooks** and **JupyterLab** (the next generation of Jupyter Notebooks
    with additional development capabilities such as code debugging). While not fully
    replacing a classical IDE, notebooks have become popular because they allow you
    to explore and visualize data, and develop and share your code with others.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker provides several managed notebook environments:'
  prefs: []
  type: TYPE_NORMAL
- en: The **SageMaker Studio** service – a proprietary serverless notebook IDE for
    ML development
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SageMaker notebook instances** – a managed Jupyter Notebook/JupyterLab environment'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All three options – a classical IDE, SageMaker notebook instances, and SageMaker
    Studio – have certain benefits and may be optimal for a specific set of scenarios.
    In the following sections, we will review IDE options in detail with their pros
    and cons as they relate to DL development.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up a local environment for SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There are a number of benefits of doing your initial development locally, specifically
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You don’t incur any running costs for doing your development locally
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can choose your preferred IDE, which results in more efficient development
    cycles
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, local development runtime also has certain limitations. For instance,
    you cannot test and profile your code on different hardware devices. Getting the
    latest GPU devices designed for DL workloads can be impractical and not cost-efficient.
    That’s why, in many cases, you will do initial development and testing of your
    DL code using a CPU device to troubleshoot initial issues, and then do the final
    code profiling and tweaking on cloud instances with access to target GPU devices.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker provides a number of SDKs to allow integration between the local environment
    and the AWS cloud. Let’s do a practical example of how to configure your local
    environment to work with remote SageMaker resources.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a Python environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We start our configuration by setting up and configuring a Python environment
    with AWS integration. It’s recommended to use Conda environment management software
    to isolate your SageMaker local environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can start by installing Conda on your local machine using the appropriate
    installation method (it depends on your local OS). Once Conda is installed, you
    can create a new Python environment by running the following command in your terminal
    window:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that we are explicitly specifying which version of Python interpreter to
    use in this environment.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we switch to create an environment and install the AWS and SageMaker
    SDKs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let’s review the SDKs we just installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`awscli` is an AWS CLI toolkit that allows you to programmatically work with
    any AWS service. It also provides a mechanism to store and use AWS credentials
    locally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`boto3` is a Python SDK to manage your AWS resources. It uses credentials established
    by the AWS CLI toolkit to cryptographically sign any management requests and,
    thus, authenticate in AWS.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`sagemaker` – This Python SDK should be already familiar to you at this point
    of book, as we used it in previous chapters to interact with SageMaker resources
    such as training jobs or inference endpoints. Unlike `boto3`, the SageMaker SDK
    abstracts many aspects of the management of underlying resources and is generally
    recommended whenever you need to programmatically manage your SageMaker workloads.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before we proceed, we first need to configure AWS credentials. To do so, you
    will need to run the following command in your terminal and provide your AWS access
    and secret keys:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can read the details of how to set up AWS credentials here: [https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml](https://docs.aws.amazon.com/cli/latest/userguide/cli-configure-quickstart.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Configuring a Jupyter environment
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Once we have the basic Python environments configured and our AWS credentials
    established, we are ready to start the Jupyter server. In this example, we will
    use the JupyterLab environment. However, you are free to configure your own IDE
    for this purpose, as many IDEs, such as PyCharm and Visual Studio Code, support
    Jupyter notebooks via plugins or natively. The additional benefit of such an approach
    is that you can easily switch between your notebooks and training and inference
    scripts within the same IDE:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To install JupyterLab and create a kernel, run the following commands in your
    terminal:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we start the JupyterLab server on our machine:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Your JupyterLab server should be now available on `http://localhost:8888`.
  prefs: []
  type: TYPE_NORMAL
- en: Running model training on SageMaker
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In the JupyterLab instance, let’s run some tests to make sure that we can connect
    and manage SageMaker resources from our local machine:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The full notebook code and training script are in the `chapter3` directory
    of this book’s GitHub repository:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: SageMaker execution role
  prefs: []
  type: TYPE_NORMAL
- en: Please note that you will need to manually define your execution role. For SageMaker
    managed environments, such as SageMaker Studio or SageMaker notebook instances,
    you can use the `get_execution_role()` method to retrieve the execution role.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can configure and kick off our SageMaker training the same way as before:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the training job is done, you can explore locally training results and
    where output artifacts have been stored:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, having a local development environment provides you with the
    flexibility to choose your preferred IDE and avoid paying for SageMaker-managed
    development environments. At the same time, it requires you to carefully manage
    your development environment, which requires specific expertise and dedicated
    efforts. Another potential challenge is the synchronization of the development
    environment between team members.
  prefs: []
  type: TYPE_NORMAL
- en: Using SageMaker Notebook instances
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SageMaker notebook instances are managed by an AWS Jupyter environment running
    on top of EC2 instances. You can choose an instance type from a list of CPU and
    GPU-based instances. SageMaker provides a number of preconfigured Jupyter kernels
    with Python runtimes. It includes preconfigured runtimes with versions of PyTorch,
    TensorFlow, MXNet, and other popular DL and ML frameworks. You can also customize
    existing kernels (for instance, install new packages) or create fully custom kernels
    using Conda environment management.
  prefs: []
  type: TYPE_NORMAL
- en: Since a Jupyter environment runs directly on top of an EC2 instance, you can
    directly observe resource consumption during local training or inference (for
    example, by monitoring the `nvidia-smi` utility output). You can also run Docker
    operations such as building custom containers and testing them using SageMaker
    local mode, which we will discuss in detail in the *Debugging SageMaker code locally*
    section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are scenarios when using notebook instances can be beneficial, such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You need to have access to a specific type of hardware to test and debug your
    model (for example, finding max training throughput without running into OOM issues)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You want to baseline your model performance locally for a specific combination
    of hyperparameters and hardware before deploying to a remote environment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One downside of notebook instances is the lack of flexibility. You cannot change
    your instance type quickly if your hardware requirements have changed. That may
    lead to unnecessary costs when you have a combination of tasks with different
    resource requirements.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s consider a scenario where you want to locally preprocess training data
    and then debug your training script on this data. Typically, data processing is
    a CPU-bound process and doesn’t require any GPU devices. However, training DL
    morning will require a GPU device. So, you will have to provision an instance
    that satisfies the highest hardware requirements among your tasks. Alternatively,
    you will have to store your work between tasks and reprovision your notebook instance
    altogether.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker addressed this lack of elasticity in a newer product called SageMaker
    Studio notebooks. Let’s review it in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Using SageMaker Studio
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**SageMaker Studio** is a web-based interface that allows you to interact with
    various SageMaker capabilities, from visual data exploration, to Model Zoo and
    model training, to code development and endpoint monitoring. SageMaker Studio
    is intended to simplify and optimize all steps of ML development by providing
    a single environment to work and collaborate.'
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple capabilities within SageMaker Studio. Let’s review two specific
    capabilities relevant for DL development:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Studio notebooks** allow fast access to different compute instances and runtimes
    without the need to leave your JupyterLab application'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SageMaker JumpStart** is a collection of prebuilt solutions and Model Zoo
    that allows you to deploy your DL solutions in a couple of clicks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Next, let’s discuss these capabilities and use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Studio notebooks
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Studio notebooks provide a fully managed JupyterLab environment with the ability
    to quickly switch between different kernels and compute instances. During the
    switch, your work is persisted in a shared filesystem automatically. A shared
    filesystem is highly available and scales seamlessly as needed. Studio notebooks
    come with a set of prebuilt kernels similar to notebook instances, which can be
    further customized. You can also create a fully custom kernel image for Studio
    notebooks.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can choose a compute instance from a wide spectrum of EC2 instances, the
    latest CPU instances, and specialized GPU instances for training and inference
    tasks. Studio notebooks have access to two types of EC2 instances:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Fast instances**, which allows switching within 2 minutes.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regular instances**, for which you need to allow around 5 minutes to start.
    Note that this is approximate timing that may be impacted by resource availability
    in a given AWS region.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Collaborative Capabilities
  prefs: []
  type: TYPE_NORMAL
- en: Studio notebooks support a sharing capability that allows you to share your
    code, kernel, and instance configuration with teammates in just a few clicks.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker notebook kernels run within Docker images. As a result, there are
    several limitations:'
  prefs: []
  type: TYPE_NORMAL
- en: You cannot build or run containers in your Studio notebooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Studio notebooks don’t support local mode to debug containers before deployment
    on SageMaker
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AWS provides an **Image Build CLI** to circumvent this limitation and allow
    users to build custom containers while working in Studio notebooks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For most scenarios, Studio notebooks will be a convenient and cost-efficient
    alternative to running your own JupyterLab on an EC2 instance or using SageMaker
    notebook instances. However, you should be mindful of the constraints of Studio
    notebooks mentioned previously, and assess whether these are a dealbreaker for
    your particular use case or usage pattern. Additionally, Studio notebooks come
    as part of the SageMaker Studio platform, which provides additional benefits such
    as visual data exploration and processing, visual model monitoring, prebuilt solutions,
    UI conveniences for managing your feature stores, model building pipelines, endpoints,
    experiments, and more.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker JumpStart
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: SageMaker JumpStart is a library of prebuilt end-to-end ML and DL solutions,
    sample notebooks, and models that can be deployed on SageMaker in one click. JumpStart’s
    library of solutions and models is large and continuously growing.
  prefs: []
  type: TYPE_NORMAL
- en: '**JumpStart solutions** are designed for specific industry use cases, such
    as transaction fraud detection, document understanding, and predictive maintenance.
    Each solution includes multiple integrated components and once deployed can be
    immediately used by end users. Note that you will need to provide your own dataset
    to train JumpStart models.'
  prefs: []
  type: TYPE_NORMAL
- en: '**JumpStart models** provide access to the SOTA model zoo. Depending on your
    model architecture, you may choose to immediately deploy this model to inference,
    fine-tune, train from scratch, or resume incremental training on your own dataset.
    JumpStart allows users to fully customize user actions, such as defining a size
    and instance type of training cluster, a hyperparameter of a training job, and
    the location of data.'
  prefs: []
  type: TYPE_NORMAL
- en: Model Zoo includes models for CV and NLP tasks from TensorFlow Hub, PyTorch
    Hub, and Hugging Face models.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker JumpStart can come in handy in scenarios when your business problem
    can be addressed using generic solutions with your proprietary data. JumpStart
    can also be a friendly introduction to DL on SageMaker or for non-technical users
    who are looking to experiment with DL on their own.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we reviewed available development environment options for SageMaker.
    All three options come with their pros and cons, and a specific choice is largely
    driven by personal preferences and use case requirements. It’s generally a good
    idea to have both a local environment and SageMaker Studio notebooks or notebook
    instances available. This setup allows you to develop, test, and do initial debugging
    locally without paying for any cloud resources. Once your code is working locally,
    you can then easily run the same code on cloud hardware. Studio notebooks can
    be especially useful, as they allow you to easily switch between different CPU
    and GPU runtimes without leaving your Jupyter notebook, so you can experiment
    with your training config (for instance, tweak the batch size or gradient accumulation).
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will focus on how to efficiently debug your SageMaker
    code locally before moving your workload to SageMaker cloud resources.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging SageMaker code locally
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: To simplify code development and testing locally, SageMaker supports **local
    mode**. This mode allows you to run your training, inference, or data processing
    locally in SageMaker containers. This is particularly helpful when you want to
    troubleshoot your scripts before provisioning any SageMaker resources.
  prefs: []
  type: TYPE_NORMAL
- en: Local mode is supported for all SageMaker images as well as custom SageMaker-compatible
    images. It is implemented as part of the `sagemaker` Python SDK. When running
    your jobs in local mode, the SageMaker SDK under the hood creates a Docker Compose
    YAML file with your job parameters and starts a relevant container locally. The
    complexities of configuring a Docker runtime environment are abstracted from the
    user.
  prefs: []
  type: TYPE_NORMAL
- en: 'Local mode is supported for both CPU and GPU devices. You can run the following
    types of SageMaker jobs in local mode:'
  prefs: []
  type: TYPE_NORMAL
- en: Training job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Real-time endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Processing job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Batch transform job
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Limitations of local mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'There are several limitations when running your SageMaker jobs locally:'
  prefs: []
  type: TYPE_NORMAL
- en: Only one local endpoint is supported.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Distributed local training for a GPU is not supported. However, you can run
    distributed jobs on a CPU.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: EFS and FSx for Lustre are not supported as data sources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Gzip` compression, Pipe mode, or manifest files for input are not supported.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Running training and inference in local mode
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s train a simple model in local mode and then deploy an inference endpoint
    locally. The full notebook code and training script are in the `chapter3` directory
    of the book repository:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We start by installing all required dependencies for local mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then configure the SageMaker local runtime. Note that we are using the `LocalSession`
    class to let the SageMaker SDK know that we want to provision resources locally:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this notebook, we intend to use a public PyTorch image from the SageMaker
    ECR repository. For this, we need to store credentials so that the Docker daemon
    can pull the images. Run the following command in your notebook (you can also
    run it in your terminal window; just remove `!`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to decide whether we will use a GPU (if available) or CPU device
    (the default choice). The following code snippet determines whether a CUDA-compatible
    device is available (the `"local_gpu"` value) and, if not, defaults to a CPU device
    (the `"local"` value):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once we define which local device to use, we configure and run a SageMaker
    training job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The SageMaker Python SDK performs the following operations automatically:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pulls the appropriate PyTorch image from a public ECR repository
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Generates a `docker-compose.yml` file with appropriate volume mount points to
    access code and training data
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Starts a Docker container with the `train` command
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: SageMaker will output the Docker Compose command and the STDOUT/STDERR of the
    training container to a Jupyter cell.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging code inside a container
  prefs: []
  type: TYPE_NORMAL
- en: Many modern IDEs support debugging an application running inside a container.
    For instance, you can set a breakpoint in your training code. The code execution
    inside the container will stop so that you can inspect whether it’s executing
    correctly. Consult your IDE documentation on how to set it up.
  prefs: []
  type: TYPE_NORMAL
- en: After the training job has finished, let’s see how we can deploy a trained model
    to a local real-time endpoint. Note that, by default, we are training only for
    a single epoch, so don’t expect great results!
  prefs: []
  type: TYPE_NORMAL
- en: 'You can deploy an inference container locally just by running the `deploy()`
    method on your estimator:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once the endpoint is deployed, the SageMaker SDK will start sending the output
    of the model server to a Jupyter cell. You can also observe container logs in
    the Docker client UI or via the `docker logs CONTAINER_ID` terminal command.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can now send a test image and observe how our inference scripts handle an
    inference request in the Docker logs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the preceding code block, we did the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Formed an inference payload and serialized it to `bytes` objects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Formed `content-type` and `accept-type` HTTP headers to indicate to the inference
    server what type of content the client is sending and what it expects
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sent a request to the local SageMaker endpoint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Read the response output
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are any issues, you can log in to a running inference container to
    examine the runtime environment or set up a debugging session using your IDE capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we reviewed some available solutions and best practices on
    how to organize the development of DL code for Amazon SageMaker. Depending on
    your use case requirements and personal preferences, you can choose a DIY environment
    locally or use one of SageMaker’s notebook environments – notebook instances and
    Studio notebooks. You also learned how to test SageMaker DL containers locally
    to speed up your development efforts and avoid any additional testing costs.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will focus on data management and data processing for
    SageMaker. As many training datasets for DL problems are large and require pre-
    or post-processing, it’s crucial to understand an optimal storage solution. We
    also discuss aspects of data labeling and data processing using SageMaker capabilities,
    as well as the best practices for accessing your training data.
  prefs: []
  type: TYPE_NORMAL
