<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer260">
<h1 class="chapter-number" id="_idParaDest-206"><a id="_idTextAnchor206"/>8</h1>
<h1 id="_idParaDest-207"><a id="_idTextAnchor207"/>Regularization with Recurrent Neural Networks</h1>
<p>In this chapter, we will work with <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>). As we will see, they are well suited for <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) tasks, even if they also apply well to time series tasks. After learning how to train RNNs, we will apply several regularization methods, such as using dropout and the sequence maximum length. This will allow you to gain foundational knowledge that can be applied to NLP or time series-related tasks. This will also give you the necessary knowledge to understand more advanced techniques covered in the <span class="No-Break">next chapter.</span></p>
<p>In this chapter, we’ll cover the <span class="No-Break">following recipes:</span></p>
<ul>
<li>Training <span class="No-Break">an RNN</span></li>
<li>Training a <strong class="bold">Gated Recurrent </strong><span class="No-Break"><strong class="bold">Unit</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">GRU</strong></span><span class="No-Break">)</span></li>
<li>Regularizing <span class="No-Break">with dropout</span></li>
<li>Regularizing with a maximum <span class="No-Break">sequence length</span></li>
</ul>
<h1 id="_idParaDest-208"><a id="_idTextAnchor208"/>Technical requirements</h1>
<p>In this chapter, we will train RNNs on various tasks using the <span class="No-Break">following libraries:</span></p>
<ul>
<li><span class="No-Break">NumPy</span></li>
<li><span class="No-Break">pandas</span></li>
<li><span class="No-Break">scikit-learn</span></li>
<li><span class="No-Break">Matplotlib</span></li>
<li><span class="No-Break">PyTorch</span></li>
<li><span class="No-Break">Transformers</span></li>
</ul>
<h1 id="_idParaDest-209"><a id="_idTextAnchor209"/>Training an RNN</h1>
<p>In NLP, input data is<a id="_idIndexMarker456"/> commonly textual data. Since a text is usually nothing but a sequence of words, using RNNs is sometimes a good solution. Indeed, RNNs, unlike fully connected networks, consider data’s <span class="No-Break">sequential information.</span></p>
<p>In this recipe, we will train an RNN on tweets to predict whether they are positive, negative, <span class="No-Break">or neutral.</span></p>
<h2 id="_idParaDest-210"><a id="_idTextAnchor210"/>Getting started</h2>
<p>In NLP, we usually manipulate textual data, which is unstructured. To handle it properly, this is usually a multi-step process – first, convert the text into numbers, and then only train a model on <span class="No-Break">those numbers.</span></p>
<p>There are several ways to convert text into numbers. In this recipe, we will use a simple approach called <strong class="bold">tokenization</strong>. Tokenization is<a id="_idIndexMarker457"/> just converting a sentence into tokens. A token can be as simple as a word, so that a sentence like “<em class="italic">The dog is out</em>” would be tokenized as <strong class="source-inline">['the', 'dog', 'is', 'out']</strong>. There is usually one more step in the tokenization process – once the sentence is converted into a list of words, it must be converted to a number. Each word is assigned to a number so that the sentence “<em class="italic">The dog is out</em>” could be tokenized as <strong class="source-inline">[3, 198, </strong><span class="No-Break"><strong class="source-inline">50, 3027]</strong></span><span class="No-Break">.</span></p>
<p class="callout-heading">Tip</p>
<p class="callout">This is a quite simplistic explanation of tokenization. Check the <em class="italic">See also</em> subsection for <span class="No-Break">more resources.</span></p>
<p>In this recipe, we will train an RNN on tweets for a multiclass classification task. However, how does RNN work? An RNN takes as input a sequence of features, as represented in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer208">
<img alt="Figure 8.1 – A representation of an RNN. At the bottom level is the input features, in the middle is the hidden layers, and at the top level is the output layer" height="620" src="image/B19629_08_01.jpg" width="1056"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.1 – A representation of an RNN. At the bottom level is the input features, in the middle is the hidden layers, and at the top level is the output layer</p>
<p>In <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.1</em>, the hidden<a id="_idIndexMarker458"/> layer of an RNN has two inputs and <span class="No-Break">two outputs:</span></p>
<ul>
<li><strong class="bold">Inputs</strong>: The features at the current step, <img alt="" height="18" src="image/Formula_08_001.png" width="19"/>, and the hidden state of the previous <span class="No-Break">step, <img alt="" height="23" src="image/Formula_08_002.png" width="39"/></span></li>
<li><strong class="bold">Outputs</strong>: The hidden state, <img alt="" height="23" src="image/Formula_08_003.png" width="20"/> (fed to the next step), and this step’s activation output <img alt="" height="18" src="image/Formula_08_004.png" width="20"/> </li>
</ul>
<p>In the case of a <a id="_idIndexMarker459"/>one-layer RNN, the activation function is simply the <span class="No-Break">output <img alt="" height="24" src="image/Formula_08_005.png" width="20"/>.</span></p>
<p>Going back to our example, the input features are the tokens. So, at each sequence step, one or more layers of neural networks take as input both the features that are at this sequence step and the hidden state of the <span class="No-Break">previous step.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">RNNs can be used in other contexts, such as forecasting, where the input features can be both quantitative and <span class="No-Break">qualitative features.</span></p>
<p>RNNs also have several sets of weights. As represented in <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.2</em>, there are three sets <span class="No-Break">of weights:</span></p>
<ul>
<li><img alt="" height="25" src="image/Formula_08_006.png" width="41"/>: The weights <a id="_idIndexMarker460"/>applied to the hidden state of the previous step, for the current hidden <span class="No-Break">state computation</span></li>
<li><img alt="" height="23" src="image/Formula_08_007.png" width="38"/>: The weights applied to the input features, for the current hidden <span class="No-Break">state computation</span></li>
<li><img alt="" height="25" src="image/Formula_08_008.png" width="38"/>: The weights applied to the current hidden state, for the <span class="No-Break">current output</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer217">
<img alt="Figure 8.2 – A representation of an RNN with different sets of weights" height="620" src="image/B19629_08_02.jpg" width="1085"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.2 – A representation of an RNN with different sets of weights</p>
<p>Overall, considering <a id="_idIndexMarker461"/>all of this, the computation of the hidden state and the activation output can be computed <span class="No-Break">as follows:</span></p>
<p class="IMG---Figure"><img alt="" height="25" src="image/Formula_08_009.png" width="292"/></p>
<p class="IMG---Figure"><img alt="" height="27" src="image/Formula_08_010.png" width="257"/></p>
<p>Here, <em class="italic">g</em> is the activation function, and <img alt="" height="24" src="image/Formula_08_011.png" width="23"/> and <img alt="" height="25" src="image/Formula_08_012.png" width="21"/> are biases. We use <em class="italic">softmax</em> here for the output computation, assuming it is a multiclass classification, but any activation function is possible depending on <span class="No-Break">the task.</span></p>
<p>Finally, the loss can be computed easily, as for any other machine-learning task (for example, for a classification task, a cross-entropy loss can be computed between the ground truth and the neural network’s output). Backpropagation on such neural networks, called <strong class="bold">backpropagation through time</strong>, is <a id="_idIndexMarker462"/>beyond the scope of <span class="No-Break">this book.</span></p>
<p>On a practical side, for this recipe, we will need a Kaggle dataset. To get this dataset, once the Kaggle API has been set, the following command lines can be used to get the dataset in the current <span class="No-Break">working directory:</span></p>
<pre class="source-code">
kaggle datasets download -d crowdflower/twitter-airline-sentiment --unzip</pre>
<p>This line should download a <strong class="source-inline">.zip</strong> file and unzip its content, and then a file named <strong class="source-inline">Tweets.csv</strong> should be available. You can move or copy this file to the current <span class="No-Break">working directory.</span></p>
<p>Finally, the following libraries must be installed: <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">matplotlib</strong>, <strong class="source-inline">torch</strong>, and <strong class="source-inline">transformers</strong>. They can be installed with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install pandas numpy scikit-learn matplotlib torch transformers</pre>
<h2 id="_idParaDest-211"><a id="_idTextAnchor211"/>How to do it…</h2>
<p>In this recipe, we<a id="_idIndexMarker463"/> will use an RNN to perform the classification of tweets into three classes – negative, neutral, and positive. As explained in the previous section, this will be a multi-step process – first, a tokenization of the tweet’s texts, and then just <span class="No-Break">model training:</span></p>
<ol>
<li>Import the <span class="No-Break">required libraries:</span><ul><li><strong class="source-inline">torch</strong> and some related modules and classes for the <span class="No-Break">neural network</span></li><li><strong class="source-inline">train_test_split</strong> and <strong class="source-inline">LabelEncoder</strong> from scikit-learn <span class="No-Break">for preprocessing</span></li><li><strong class="source-inline">AutoTokenizer</strong> from Transformers to tokenize <span class="No-Break">the tweets</span></li><li><strong class="source-inline">pandas</strong> to load <span class="No-Break">the dataset</span></li><li><strong class="source-inline">matplotlib</strong> <span class="No-Break">for visualization:</span><pre class="source-code">
import torch import torch.nn as nn</pre><pre class="source-code">
import torch.optim as optim from torch.utils.data</pre><pre class="source-code">
import DataLoader, Dataset from sklearn.model_selection</pre><pre class="source-code">
import train_test_split from sklearn.preprocessing</pre><pre class="source-code">
import LabelEncoder from transformers</pre><pre class="source-code">
import AutoTokenizer</pre><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre></li></ul></li>
<li>Load the data from the <strong class="source-inline">.csv</strong> file <span class="No-Break">with pandas:</span><pre class="source-code">
# Load data</pre><pre class="source-code">
data = pd.read_csv('Tweets.csv')</pre><pre class="source-code">
data[['airline_sentiment', 'text']].head()</pre></li>
</ol>
<p>The output will be <span class="No-Break">the following:</span></p>
<table class="T---Table _idGenTablePara-1" id="table001-3">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="T---Table">
<td class="T---Table T---Body T---Body"/>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="source-inline">airline_sentiment</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="source-inline">Text</strong></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break">Neutral</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><strong class="source-inline">@VirginAmerica What @</strong><span class="No-Break"><strong class="source-inline">dhepburn said.</strong></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break">Positive</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><strong class="source-inline">@VirginAmerica plus you've added </strong><span class="No-Break"><strong class="source-inline">commercials t...</strong></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>2</p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break">Neutral</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><strong class="source-inline">@VirginAmerica I didn't today... Must mean </strong><span class="No-Break"><strong class="source-inline">I n...</strong></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>3</p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break">Negative</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><strong class="source-inline">@VirginAmerica it's really aggressive </strong><span class="No-Break"><strong class="source-inline">to blast...</strong></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>4</p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break">Negative</span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><strong class="source-inline">@VirginAmerica and it's a really big </strong><span class="No-Break"><strong class="source-inline">bad thing...</strong></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 8.1 – Output with the data classified</p>
<p>The data <a id="_idIndexMarker464"/>we will use is made of labels from the <strong class="source-inline">airline_sentiment</strong> column (either negative, neutral, or positive) and their associated raw tweets texts from the <span class="No-Break"><strong class="source-inline">text</strong></span><span class="No-Break"> column.</span></p>
<p>3.	Split the data into train and test sets, using the <strong class="source-inline">train_test_split</strong> function, with a test size of 20% and a specified random state <span class="No-Break">for reproducibility:</span></p>
<pre class="source-code">
# Split data into train and test sets
train_data, test_data = train_test_split(data,
    test_size=0.2, random_state=0)</pre>
<p>4.	Implement the <strong class="source-inline">TextClassificationDataset</strong> dataset class, handling the data. At instance creation, this class will so <span class="No-Break">the following:</span></p>
<ul>
<li>Instantiate <strong class="source-inline">AutoTokenizer</strong> <span class="No-Break">from Transformers</span></li>
<li>Tokenize the tweets with that previously instantiated tokenizer and store <span class="No-Break">the results</span></li>
<li>Encode the labels and <span class="No-Break">store them:</span><pre class="source-code">
# Define dataset class class TextClassificationDataset(Dataset):</pre><pre class="source-code">
def __init__(self, data, max_length):</pre><pre class="source-code">
    self.data = data</pre><pre class="source-code">
    self.tokenizer = AutoTokenizer.from_pretrained(</pre><pre class="source-code">
        'bert-base-uncased')</pre><pre class="source-code">
    self.tokens = self.tokenizer(</pre><pre class="source-code">
        data['text'].to_list(), padding=True,</pre><pre class="source-code">
        truncation=True, max_length=max_length,</pre><pre class="source-code">
        return_tensors='pt')['input_ids']</pre><pre class="source-code">
    le = LabelEncoder()</pre><pre class="source-code">
    self.labels = torch.tensor(le.fit_transform(</pre><pre class="source-code">
        data['airline_sentiment']))</pre><pre class="source-code">
def __len__(self):</pre><pre class="source-code">
    return len(self.data)</pre><pre class="source-code">
def __getitem__(self, index):</pre><pre class="source-code">
    return self.tokens[index], self.labels[index]</pre></li>
</ul>
<p>Several <a id="_idIndexMarker465"/>options are specified with <span class="No-Break">this tokenizer:</span></p>
<ul>
<li>It is instantiated with the <strong class="source-inline">'bert-base-uncased'</strong> tokenizer, a tokenizer used for <span class="No-Break">BERT models</span></li>
<li>The tokenization is made, with a maximum length provided as a <span class="No-Break">constructor argument</span></li>
<li>The padding is set to <strong class="source-inline">True</strong>, meaning that if a tweet has less than the maximum length, it will be filled with zeros to match <span class="No-Break">that length</span></li>
<li>The truncation is set to <strong class="source-inline">True</strong>, meaning that if a tweet has more than the maximum length, the remaining tokens will <span class="No-Break">be ignored</span></li>
<li>The return tensor is specified as <strong class="source-inline">'pt'</strong> so that it returns a <span class="No-Break">PyTorch tensor</span></li>
</ul>
<p class="callout-heading">Tip</p>
<p class="callout">See the <em class="italic">There’s more…</em> subsection for more details about what the <span class="No-Break">tokenizer does.</span></p>
<p>5.	Instantiate the <strong class="source-inline">TextClassificationDataset</strong> objects for the train and test sets, as well as the related data loaders. We specify here a maximum number of words of <strong class="source-inline">24</strong> and a batch size of <strong class="source-inline">64</strong>. This means that each tweet will be converted into a sequence of exactly <span class="No-Break">24 tokens:</span></p>
<pre class="source-code">
batch_size = 64 max_length = 24
# Initialize datasets and dataloaders
train_dataset = TextClassificationDataset(train_data,
    max_length)
test_dataset = TextClassificationDataset(test_data,
    max_length)
train_dataloader = DataLoader(train_dataset,
    batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset,
    batch_size=batch_size, shuffle=True)</pre>
<p>6.	Implement<a id="_idIndexMarker466"/> the <span class="No-Break">RNN model:</span></p>
<pre class="source-code">
# Define RNN model
class RNNClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim,
        hidden_size, output_size, num_layers=3):
            super(RNNClassifier, self).__init__()
            self.num_layers = num_layers
            self.hidden_size = hidden_size
            self.embedding = nn.Embedding(
                num_embeddings=vocab_size,
                embedding_dim=embedding_dim)
            self.rnn = nn.RNN(
                input_size=embedding_dim,
                hidden_size=hidden_size,
                num_layers=num_layers,
                nonlinearity='relu',
                batch_first=True)
            self.fc = nn.Linear(hidden_size, output_size)
    def forward(self, inputs):
        batch_size = inputs.size(0)
        zero_hidden = torch.zeros(self.num_layers,
            batch_size, self.hidden_size)
        embedded = self.embedding(inputs)
        output, hidden = self.rnn(embedded, zero_hidden)
        output = torch.softmax(self.fc(output[:, -1]),
            dim=1)
        return output</pre>
<p>The RNN model defined here can be described in <span class="No-Break">several steps:</span></p>
<ul>
<li>An embedding that takes the tokens as input, with the input the size of the vocabulary and the output the size of the given <span class="No-Break">embedding dimension</span></li>
<li>Three layers of RNN that take as input the embedding output, with the given number of layers, a hidden size, and a ReLU <span class="No-Break">activation function</span></li>
<li>Finally, an embedding that takes the tokens as input, with the input the size of the vocabulary and the output the size of the given embedding dimension; note that the output is computed only for the last sequence step (that is, <strong class="source-inline">output[:, -1]</strong>), and a softmax activation function <span class="No-Break">is applied</span></li>
</ul>
<p class="callout-heading">Important note</p>
<p class="callout">The output is not necessarily computed only for the last sequence step. Depending on the task, it can be useful to output a value at each step alike (for example, forecasting) or only a final value (for example, <span class="No-Break">classification task).</span></p>
<p>7.	Instantiate <a id="_idIndexMarker467"/>and test the model. The vocabulary size is given by the tokenizer and the output size of three is defined by the task; there are three classes (negative, neutral, positive). The other arguments are hyperparameters; here, the following values <span class="No-Break">are chosen:</span></p>
<ul>
<li>An embedding dimension <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">64</strong></span></li>
<li>A hidden dimension <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">64</strong></span></li>
</ul>
<p>Other values can, of course, <span class="No-Break">be tested:</span></p>
<pre class="source-code">
vocab_size = train_dataset.tokenizer.vocab_size
embedding_dim = 64
hidden_dim = 64
output_size = 3
model = RNNClassifier(
    vocab_size=vocab_size,
    embedding_dim=embedding_dim,
    hidden_size=hidden_dim,
    output_size=output_size, )
random_data = torch.randint(0, vocab_size,
    size=(batch_size, max_length))
result = model(random_data)
print('Resulting output tensor:', result.shape) print('Sum of the output tensor:', result.sum())</pre>
<p>The code will output <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">Resulting output tensor: torch.Size([64, 3]) Sum of the output tensor: tensor(64.0000, grad_fn=&lt;SumBackward0&gt;)</strong></pre>
<p>8.	Instantiate the optimizer; here, we will use an Adam optimizer, with a learning rate of <strong class="source-inline">0.001</strong>. The loss is the cross-entropy loss, since this is a multiclass <span class="No-Break">classification task:</span></p>
<pre class="source-code">
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.CrossEntropyLoss()</pre>
<p>9.	Let’s define two helper functions to train <span class="No-Break">the model.</span></p>
<p><strong class="source-inline">epoch_step_tweet</strong> will compute the loss and accuracy for one epoch, as well as update <a id="_idIndexMarker468"/>the weights for the <span class="No-Break">training set:</span></p>
<pre class="source-code">
def epoch_step_tweet(model, dataloader,
    training_set: bool):
        running_loss = 0
        correct = 0.
    for i, data in enumerate(dataloader, 0):
        # Get the inputs: data is a list of [inputs, labels]
        inputs, labels = data
        if training_set:
            # Zero the parameter gradients
            optimizer.zero_grad()
        # Forward + backward + optimize
        outputs = model(inputs)
        loss = criterion(outputs, labels) .long()
        if training_set:
            loss.backward()
            optimizer.step()
        # Add correct predictions for this batch
        correct += (outputs.argmax(
            dim=1) == labels).float().sum()
        # Compute loss for this batch
        running_loss += loss.item()
    return running_loss, correct</pre>
<p><strong class="source-inline">train_tweet_classification</strong> will use loop over the epochs and use <strong class="source-inline">epoch_step_tweet</strong> to compute <a id="_idIndexMarker469"/>and store the loss <span class="No-Break">and accuracy:</span></p>
<pre class="source-code">
def train_tweet_classification(model,
    train_dataloader, test_dataloader, criterion,
    epochs: int = 20):
        # Train the model
        train_losses = []
        test_losses = []
        train_accuracy = []
        test_accuracy = []
    for epoch in range(20):
        running_train_loss = 0.
        correct = 0.
        model.train()
        running_train_loss,
        correct = epoch_step_tweet(model,
            dataloader=train_dataloader,
            training_set=True)
        # Compute and store loss and accuracy for this epoch
        train_epoch_loss = running_train_loss / len(
            train_dataloader)
        train_losses.append(train_epoch_loss)
        train_epoch_accuracy = correct / len(
            train_dataset)
        train_accuracy.append(train_epoch_accuracy)
        ## Evaluate the model on the test set
        running_test_loss = 0.
        correct = 0.
        model.eval()
        with torch.no_grad():
            running_test_loss,
            correct = epoch_step_tweet(model,
                dataloader=test_dataloader,
                training_set=False)
            test_epoch_loss = running_test_loss / len(
                test_dataloader)
            test_losses.append(test_epoch_loss)
            test_epoch_accuracy = correct / len(
            test_dataset)
            test_accuracy.append(test_epoch_accuracy)
        # Print stats
        print(f'[epoch {epoch + 1}] Training: loss={train_epoch_loss:.3f} accuracy={train_epoch_accuracy:.3f} |\
    \t Test: loss={test_epoch_loss:.3f} accuracy={test_epoch_accuracy:.3f}')
    return train_losses, test_losses, train_accuracy,
    test_accuracy</pre>
<p>10.	Reusing the helper functions, we can now train the model on 20 epochs. Here, we will compute and store the accuracy and the loss for both the train and test sets at each epoch, to plot <span class="No-Break">them afterward:</span></p>
<pre class="source-code">
train_losses, test_losses, train_accuracy, test_accuracy = train_tweet_classification(model,
    train_dataloader, test_dataloader, criterion,
    epochs=20)</pre>
<p>After 20 epochs, the<a id="_idIndexMarker470"/> output should be something like <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">[epoch 20] Training: loss=0.727 accuracy=0.824 |  Test: loss=0.810 accuracy=0.738</strong></pre>
<p>11.	Plot the loss as a function of the epoch number, for both the train and <span class="No-Break">test sets:</span></p>
<pre class="source-code">
plt.plot(train_losses, label='train')
plt.plot(test_losses, label='test')
plt.xlabel('epoch') plt.ylabel('loss (CE)')
plt.legend() plt.show()</pre>
<p>Here is the <span class="No-Break">resulting graph:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer222">
<img alt="Figure 8.3 – Cross-entropy loss as a function of the epoch" height="408" src="image/B19629_08_03.jpg" width="554"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.3 – Cross-entropy loss as a function of the epoch</p>
<p>We can see some overfitting as early as the fifth epoch, since the train loss keeps decreasing while the test loss reaches <span class="No-Break">a plateau.</span></p>
<p>12.	Similarly, plot the <a id="_idIndexMarker471"/>accuracy as a function of the epoch number of both the train and <span class="No-Break">test sets:</span></p>
<pre class="source-code">
plt.plot(train_accuracy, label='train')
plt.plot(test_accuracy, label='test')
plt.xlabel('epoch') plt.ylabel('Accuracy')
plt.legend() plt.show()</pre>
<p>We then get <span class="No-Break">this graph:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer223">
<img alt="Figure 8.4 – Accuracy as a function of the epoch" height="408" src="image/B19629_08_04.jpg" width="553"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.4 – Accuracy as a function of the epoch</p>
<p>After 20 epochs, the accuracy of the train set is about 82%, but it is only about 74% on the test set, meaning<a id="_idIndexMarker472"/> there might be room for improvement with <span class="No-Break">proper regularization.</span></p>
<h2 id="_idParaDest-212"><a id="_idTextAnchor212"/>There’s more…</h2>
<p>In this recipe, we<a id="_idIndexMarker473"/> used the <strong class="source-inline">HuggingFace</strong> tokenizer, but what does it actually do? Let’s have a look at a text example to fully understand what <span class="No-Break">it is.</span></p>
<p>First, let’s define a brand-new tokenizer with the <strong class="source-inline">AutoTokenizer</strong> class, specifying the <span class="No-Break">BERT tokenizer:</span></p>
<pre class="source-code">
from transformers import AutoTokenizer
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')</pre>
<p class="callout-heading">Important note</p>
<p class="callout">There are many tokenizers that have different methods and, thus, different outputs for the same given text. <strong class="source-inline">'bert-base-uncased'</strong> is quite a common one, but many others can <span class="No-Break">be used.</span></p>
<p>Let’s now apply this tokenizer to a given text, using the <strong class="source-inline">tokenize</strong> method, to see what the <span class="No-Break">output is:</span></p>
<pre class="source-code">
tokenizer.tokenize("Let's use regularization in ML. Regularization should help to improve model robustness")</pre>
<p>The code output is <span class="No-Break">the following:</span></p>
<pre class="source-code">
['let',  "'",  's',  'use',  'regular',  '##ization',  'in',
  'ml',  '.',  'regular',  '##ization',  'should',  'help',
  'to',  'improve',  'model',  'robust',  '##ness']</pre>
<p>So, the tokenization can be described as splitting a sentence into smaller chunks. Other tokenizers can have different chunks (or tokens) at the end, but the process remains essentially <span class="No-Break">the same.</span></p>
<p>Now, if we just apply the tokenization in this same sentence, we can get the token numbers <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">'input_ids'</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
tokenizer("Let's use regularization in ML. Regularization should help to improve model robustness")['input_ids']</pre>
<p>The code output is now <span class="No-Break">the following:</span></p>
<pre class="source-code">
[101,  2292,  1005,  1055,  2224,  3180,  3989,  1999,  19875,
  1012,  3180,  3989,  2323,  2393,  2000,  5335,  2944,  15873,
  2791,  102]</pre>
<p class="callout-heading">Important note</p>
<p class="callout">Note that the <strong class="source-inline">3180</strong> and <strong class="source-inline">3989</strong> tokens are present twice. Indeed, the word <strong class="source-inline">regularization</strong> (tokenized as two separate tokens) is <span class="No-Break">present twice.</span></p>
<p>For a given<a id="_idIndexMarker474"/> tokenizer, the vocabulary size is just the number of existing tokens. This is stored in the <strong class="source-inline">vocab_size</strong> attribute. In this case, the vocabulary size <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">30522</strong></span><span class="No-Break">.</span></p>
<p class="callout-heading">Tip</p>
<p class="callout">If you’re curious, you can also directly have a look at the whole vocabulary, stored in the <strong class="source-inline">.vocab</strong> attribute as <span class="No-Break">a dictionary.</span></p>
<h2 id="_idParaDest-213"><a id="_idTextAnchor213"/>See also</h2>
<ul>
<li>This is great content about <a id="_idIndexMarker475"/>tokenizers by <span class="No-Break">HuggingFace: </span><a href="https://huggingface.co/docs/transformers/tokenizer_summary%0D"><span class="No-Break">https://huggingface.co/docs/transformers/tokenizer_summary</span></a></li>
<li>The official <a id="_idIndexMarker476"/>documentation about <span class="No-Break"><strong class="source-inline">AutoTokenizer</strong></span><span class="No-Break">: </span><a href="https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/auto#transformers.AutoTokenizer%0D"><span class="No-Break">https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/auto#transformers.AutoTokenizer</span></a></li>
<li>The official<a id="_idIndexMarker477"/> documentation about <span class="No-Break">RNNs: </span><a href="https://pytorch.org/docs/stable/generated/torch.nn.RNN.xhtml%0D"><span class="No-Break">https://pytorch.org/docs/stable/generated/torch.nn.RNN.xhtml</span></a></li>
</ul>
<h1 id="_idParaDest-214"><a id="_idTextAnchor214"/>Training a GRU</h1>
<p>In this recipe, we will<a id="_idIndexMarker478"/> keep exploring RNNs with the<strong class="bold"> GRU </strong>– what it is, how it works, and how to train such <span class="No-Break">a model.</span></p>
<h2 id="_idParaDest-215"><a id="_idTextAnchor215"/>Getting started</h2>
<p>One of the main limitations of RNNs is the memory of the network throughout their steps. GRUs try to overcome this limit by adding a <span class="No-Break">memory gate.</span></p>
<p>If we take a step back and describe an RNN cell with a simple diagram, it could look like <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.5</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer224">
<img alt="Figure 8.5 – A diagram of an RNN cell" height="787" src="image/B19629_08_05.jpg" width="734"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.5 – A diagram of an RNN cell</p>
<p>So basically, at each step <em class="italic">t</em>, there are both a hidden state <img alt="" height="23" src="image/Formula_08_013.png" width="39"/> and a set of features <img alt="" height="18" src="image/Formula_08_014.png" width="20"/>. They are concatenated, then weights are applied and an activation function g resulting in a new hidden<a id="_idIndexMarker479"/> state <img alt="" height="23" src="image/Formula_08_015.png" width="21"/>. Optionally, an output <img alt="" height="24" src="image/Formula_08_016.png" width="21"/> is computed from this hidden state, and <span class="No-Break">so on.</span></p>
<p>But what if this step of features <img alt="" height="19" src="image/Formula_08_017.png" width="21"/> is not relevant? Or what if it would be useful for the network to just remember fully this hidden state <img alt="" height="23" src="image/Formula_08_018.png" width="39"/> from time to time? This is exactly what a GRU does, by adding a new set of parameters through what is called <span class="No-Break">a gate.</span></p>
<p>A <strong class="bold">gate</strong> is learned<a id="_idIndexMarker480"/> through backpropagation too, using a new set of weights, and allows a network to learn more complex patterns, as well as remember relevant <span class="No-Break">past information.</span></p>
<p>A GRU is made up of <span class="No-Break">two gates:</span></p>
<ul>
<li><img alt="" height="23" src="image/Formula_08_019.png" width="20"/>: the update<a id="_idIndexMarker481"/> gate, responsible for learning whether to update the <span class="No-Break">hidden state</span></li>
<li><img alt="" height="22" src="image/Formula_08_020.png" width="18"/>: the relevance<a id="_idIndexMarker482"/> gate, responsible for learning how relevant the hidden <span class="No-Break">state is</span></li>
</ul>
<p>In the end, a simplified diagram of the GRU unit looks like the one shown in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.6</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer233">
<img alt="Figure 8.6 – A diagram of a GRU cell. The relevance gate is omitted for clarity" height="797" src="image/B19629_08_06.jpg" width="713"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.6 – A diagram of a GRU cell. The relevance gate is omitted for clarity</p>
<p>The forward computation<a id="_idIndexMarker483"/> is now slightly more complicated than for a simple RNN cell, and it can be described with the following set <span class="No-Break">of formulas:</span></p>
<p class="IMG---Figure"><img alt="" height="25" src="image/Formula_08_021.png" width="295"/></p>
<p class="IMG---Figure"><img alt="" height="27" src="image/Formula_08_022.png" width="258"/></p>
<p>These equations can be simply described in a few words. Compared to a simple RNN, there are three <span class="No-Break">major differences:</span></p>
<ul>
<li>Two gates, <img alt="" height="23" src="image/Formula_08_023.png" width="20"/> and <img alt="" height="22" src="image/Formula_08_024.png" width="19"/>, are computed with <span class="No-Break">associated weights</span></li>
<li>The relevance gate <img alt="" height="23" src="image/Formula_08_025.png" width="19"/> is used to compute the intermediate hidden <span class="No-Break">state <img alt="" height="26" src="image/Formula_08_026.png" width="21"/></span></li>
<li>The final hidden state <img alt="" height="22" src="image/Formula_08_027.png" width="20"/> is a linear combination of the previous hidden state and the current intermediate hidden state, with the update gate <img alt="" height="22" src="image/Formula_08_028.png" width="19"/> as <span class="No-Break">the weight</span></li>
</ul>
<p>The major trick here is the use of the update gate, which can be interpreted in extreme cases <span class="No-Break">as follows:</span></p>
<ul>
<li>If <img alt="" height="23" src="image/Formula_08_029.png" width="19"/> is only made of ones, the previous hidden state <span class="No-Break">is forgotten</span></li>
<li>If <img alt="" height="22" src="image/Formula_08_030.png" width="19"/>is only made of zeros, the new hidden state is not taken <span class="No-Break">into account</span></li>
</ul>
<p>Although the concepts can be quite complex at first, the GRU is fortunately super easy to use with PyTorch, as we will see in <span class="No-Break">this recipe.</span></p>
<p>To run the code in this recipe, we will use the IMDb dataset – a dataset containing movie reviews, and positive or negative labels. The task is to guess the polarity of the review (positive or negative) based on the text. It can be downloaded with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-moviereviews --unzip</pre>
<p>We will also need <a id="_idIndexMarker484"/>the following libraries: <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">matplotlib</strong>, <strong class="source-inline">torch</strong>, and <strong class="source-inline">transformers</strong>. They can be installed with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install pandas numpy scikit-learn matplotlib torch transformers</pre>
<h2 id="_idParaDest-216"><a id="_idTextAnchor216"/>How to do it…</h2>
<p>In this recipe, we will train a GRU on the same IMDb dataset for a binary classification task. As we will see, the code to train a GRU is almost the same as that to train a <span class="No-Break">simple RNN:</span></p>
<ol>
<li>Import the <span class="No-Break">required libraries:</span><ul><li><strong class="source-inline">torch</strong> and some related modules and classes for the <span class="No-Break">neural network</span></li><li><strong class="source-inline">train_test_split</strong> and <strong class="source-inline">LabelEncoder</strong> from scikit-learn <span class="No-Break">for preprocessing</span></li><li><strong class="source-inline">AutoTokenizer</strong> from Transformers to tokenize <span class="No-Break">the </span><span class="No-Break">reviews</span></li><li><strong class="source-inline">pandas</strong> to load <span class="No-Break">the dataset</span></li><li><strong class="source-inline">matplotlib</strong> <span class="No-Break">for visualization:</span><pre class="source-code">
import torch</pre><pre class="source-code">
import torch.nn as nn</pre><pre class="source-code">
import torch.optim as optim from torch.utils.data</pre><pre class="source-code">
import DataLoader,Dataset from sklearn.model_selection</pre><pre class="source-code">
import train_test_split from sklearn.preprocessing</pre><pre class="source-code">
import LabelEncoder from transformers</pre><pre class="source-code">
import AutoTokenizer</pre><pre class="source-code">
import pandas as pd</pre><pre class="source-code">
import numpy as np</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre></li></ul></li>
<li>Load the data from the <strong class="source-inline">.csv</strong> file with pandas. This is a 50,000-row dataset, with textual reviews <span class="No-Break">and labels:</span><pre class="source-code">
# Load data</pre><pre class="source-code">
data = pd.read_csv('IMDB Dataset.csv')</pre><pre class="source-code">
data.head()</pre></li>
</ol>
<p>The code output is <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">                                   review    sentiment 0  One of the other reviewers has mentioned that ...       positive</strong>
<strong class="bold">1  A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...  positive</strong>
<strong class="bold">2  I thought this was a wonderful way to spend ti...     positive</strong>
<strong class="bold">3  Basically there's a family where a little boy ...       negative</strong>
<strong class="bold">4  Petter Mattei's "Love in the Time of Money" is...     positive</strong></pre>
<ol>
<li value="3">Split the data<a id="_idIndexMarker485"/> into train and test sets, using the <strong class="source-inline">train_test_split</strong> function, with a test size of 20% and a specified random state <span class="No-Break">for reproducibility:</span><pre class="source-code">
# Split data into train and test sets</pre><pre class="source-code">
Train_data, test_data = train_test_split(data,</pre><pre class="source-code">
    test_size=0.2, random_state=0)</pre></li>
<li>Implement the <strong class="source-inline">TextClassificationDataset</strong> dataset class, handling the data. At instance creation, this class will do <span class="No-Break">the following:</span><ul><li>Instantiate <strong class="source-inline">AutoTokenizer</strong> from the transformers library, using the <span class="No-Break"><strong class="source-inline">bert-base-uncased</strong></span><span class="No-Break"> tokenizer</span></li><li>Tokenize the tweets with the previously instantiated tokenizer, along with the provided maximum length, padding, <span class="No-Break">and truncation</span></li><li>Encode the labels and <span class="No-Break">store them:</span><pre class="source-code">
# Define dataset class</pre><pre class="source-code">
class TextClassificationDataset(Dataset):</pre><pre class="source-code">
    def __init__(self, data, max_length):</pre><pre class="source-code">
        self.data = data</pre><pre class="source-code">
        self.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')</pre><pre class="source-code">
        self.tokens = self.tokenizer(</pre><pre class="source-code">
            data['review'].to_list(), padding=True,</pre><pre class="source-code">
            truncation=True, max_length=max_length,</pre><pre class="source-code">
            return_tensors='pt')['input_ids']</pre><pre class="source-code">
        le = LabelEncoder()</pre><pre class="source-code">
        self.labels = torch.tensor(le.fit_transform(</pre><pre class="source-code">
            data['sentiment']).astype(np.float32))</pre><pre class="source-code">
    def __len__(self):</pre><pre class="source-code">
        return len(self.data)</pre><pre class="source-code">
    def __getitem__(self, index):</pre><pre class="source-code">
        return self.tokens[index],self.labels[index]</pre></li></ul></li>
<li>Instantiate the <strong class="source-inline">TextClassificationDataset</strong> objects for the train and test sets, as<a id="_idIndexMarker486"/> well as the related data loaders, with a maximum number of words of 64 and a batch size of 64. This means that each movie review will be converted as a sequence of exactly <span class="No-Break">64 tokens:</span><pre class="source-code">
batch_size = 64</pre><pre class="source-code">
max_words = 64</pre><pre class="source-code">
# Initialize datasets and dataloaders</pre><pre class="source-code">
train_dataset = TextClassificationDataset(train_data,</pre><pre class="source-code">
    max_words)</pre><pre class="source-code">
test_dataset = TextClassificationDataset(test_data,</pre><pre class="source-code">
    max_words)</pre><pre class="source-code">
train_dataloader = DataLoader(train_dataset,</pre><pre class="source-code">
    batch_size=batch_size, shuffle=True)</pre><pre class="source-code">
test_dataloader = DataLoader(test_dataset,</pre><pre class="source-code">
    batch_size=batch_size, shuffle=True)</pre></li>
<li>Implement the GRU classifier model. It is made up of the <span class="No-Break">following elements:</span><ul><li>An embedding layer (taking a zero vector as the <span class="No-Break">first input)</span></li><li>Three layers <span class="No-Break">of GRU</span></li><li>A fully connected layer on the last sequence step, with a sigmoid activation function, since it’s a <span class="No-Break">binary classification:</span><pre class="source-code">
# Define GRU model</pre><pre class="source-code">
class GRUClassifier(nn.Module):</pre><pre class="source-code">
    def __init__(self, vocab_size, embedding_dim,</pre><pre class="source-code">
        hidden_size, output_size, num_layers=3):</pre><pre class="source-code">
            super(GRUClassifier, self).__init__()</pre><pre class="source-code">
            self.num_layers = num_layers</pre><pre class="source-code">
            self.hidden_size = hidden_size</pre><pre class="source-code">
            self.embedding = nn.Embedding(</pre><pre class="source-code">
                num_embeddings=vocab_size,</pre><pre class="source-code">
                embedding_dim=embedding_dim)</pre><pre class="source-code">
            self.gru = nn.GRU(</pre><pre class="source-code">
                input_size=embedding_dim,</pre><pre class="source-code">
                hidden_size=hidden_size,</pre><pre class="source-code">
                num_layers=num_layers,</pre><pre class="source-code">
                batch_first=True)</pre><pre class="source-code">
            self.fc = nn.Linear(hidden_size,</pre><pre class="source-code">
                output_size)</pre><pre class="source-code">
    def forward(self, inputs):</pre><pre class="source-code">
        batch_size = inputs.size(0)</pre><pre class="source-code">
        zero_hidden = torch.zeros(</pre><pre class="source-code">
            self.num_layers, batch_size,</pre><pre class="source-code">
            self.hidden_size).to(device)</pre><pre class="source-code">
        embedded = self.embedding(inputs)</pre><pre class="source-code">
        output, hidden = self.gru(embedded,</pre><pre class="source-code">
            zero_hidden)</pre><pre class="source-code">
        output = torch.sigmoid(self.fc(output[:, -1]))</pre><pre class="source-code">
        return output</pre></li></ul></li>
<li>Instantiate the<a id="_idIndexMarker487"/> GRU model, with an embedding dimension and a hidden dimension <span class="No-Break">of 32:</span><pre class="source-code">
vocab_size = train_dataset.tokenizer.vocab_size</pre><pre class="source-code">
embedding_dim = 32</pre><pre class="source-code">
hidden_dim = 32</pre><pre class="source-code">
output_size = 1</pre><pre class="source-code">
# Optionally, set the device to GPU if you have one device = torch.device(</pre><pre class="source-code">
    'cuda' if torch.cuda.is_available() else 'cpu')</pre><pre class="source-code">
model = GRUClassifier(</pre><pre class="source-code">
    vocab_size=vocab_size,</pre><pre class="source-code">
    embedding_dim=embedding_dim,</pre><pre class="source-code">
    hidden_size=hidden_dim,</pre><pre class="source-code">
    output_size=output_size,</pre><pre class="source-code">
).to(device)</pre><pre class="source-code">
random_data = torch.randint(0, vocab_size,</pre><pre class="source-code">
    size=(batch_size, max_words)).to(device)</pre><pre class="source-code">
result = model(random_data)</pre><pre class="source-code">
print('Resulting output tensor:', result.shape)</pre><pre class="source-code">
print('Sum of the output tensor:', result.sum())</pre></li>
</ol>
<p>The code output is <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">Resulting output tensor: torch.Size([64, 1]) Sum of the output tensor: tensor(31.0246, device='cuda:0', grad_fn=&lt;SumBackward0&gt;)</strong></pre>
<ol>
<li value="8">Instantiate the optimizer as an Adam optimizer, with a learning rate of <strong class="source-inline">0.001</strong>. The loss is defined as the binary cross-entropy loss because this is a binary <span class="No-Break">classification task:</span><pre class="source-code">
optimizer = optim.Adam(model.parameters(), lr=0.001)</pre><pre class="source-code">
criterion = nn.BCELoss()</pre></li>
<li>Let’s now implement two <span class="No-Break">helper functions.</span></li>
</ol>
<p><strong class="source-inline">epoch_step_IMDB</strong> updates the weights on the train set and computes the binary cross-entropy<a id="_idIndexMarker488"/> loss and accuracy for a <span class="No-Break">given epoch:</span></p>
<pre class="source-code">
def epoch_step_IMDB(model, dataloader, device,
    training_set: bool):
        running_loss = 0.
        correct = 0.
        for i, data in enumerate(dataloader, 0):
  # Get the inputs: data is a list of [inputs, labels]
            inputs, labels = data
            inputs = inputs.to(device)
            labels = labels.unsqueeze(1).to(device)
            if training_set:
                # Zero the parameter gradients
                optimizer.zero_grad()
                # Forward + backward + optimize
                outputs = model(inputs)
                loss = criterion(outputs, labels)
            if training_set:
                loss.backward()
                optimizer.step()
             # Add correct predictions for this batch
                correct += (
             (outputs &gt; 0.5) == labels).float().sum()
                # Compute loss for this batch
                running_loss += loss.item()
    return running_loss, correct</pre>
<p><strong class="source-inline">train_IMDB_classification</strong> loops over the epochs, trains a model, and stores the accuracy <a id="_idIndexMarker489"/>and loss for the train and <span class="No-Break">test sets:</span></p>
<pre class="source-code">
def train_IMDB_classification(model, train_dataloader,
    test_dataloader, criterion, device,
    epochs: int = 20):
        # Train the model
        train_losses = []
        test_losses = []
        train_accuracy = []
        test_accuracy = []
    for epoch in range(20):
        running_train_loss = 0.
        correct = 0.
        model.train()
        running_train_loss, correct = epoch_step_IMDB(
            model, train_dataloader, device,
                training_set=True
        )
        # Compute and store loss and accuracy for this epoch
        train_epoch_loss = running_train_loss / len(
            train_dataloader)
        train_losses.append(train_epoch_loss)
        train_epoch_accuracy = correct / len(
            train_dataset)
        train_accuracy.append(
            train_epoch_accuracy.cpu().numpy())
        ## Evaluate the model on the test set
        running_test_loss = 0.
        correct = 0.
        model.eval()
        with torch.no_grad():
            running_test_loss,
            correct = epoch_step_IMDB(
                model, test_dataloader, device,
                training_set=False
            )
            test_epoch_loss = running_test_loss / len(
                test_dataloader)
            test_losses.append(test_epoch_loss)
            test_epoch_accuracy = correct / len(
                test_dataset)
            test_accuracy.append(
                test_epoch_accuracy.cpu().numpy())
        # Print stats
        print(f'[epoch {epoch + 1}] Training: loss={train_epoch_loss:.3f} accuracy={train_epoch_accuracy:.3f} |\
    \t Test: loss={test_epoch_loss:.3f} accuracy={test_epoch_accuracy:.3f}')
    return train_losses, test_losses, train_accuracy,
        test_accuracy</pre>
<ol>
<li value="10">Train the model <a id="_idIndexMarker490"/>over 20 epochs, reusing the functions we just implemented. Compute and store the accuracy and the loss for both the train and test sets at each epoch, for <span class="No-Break">visualization purposes:</span><pre class="source-code">
train_losses, test_losses, train_accuracy, test_accuracy = train_IMDB_classification(model,</pre><pre class="source-code">
    train_dataloader, test_dataloader, criterion,</pre><pre class="source-code">
    device, epochs=20)</pre></li>
</ol>
<p>After 20 epochs, the results should be close to the following <span class="No-Break">code output:</span></p>
<pre class="source-code">
<strong class="bold">[epoch 20] Training: loss=0.040 accuracy=0.991 |  Test: loss=1.155 accuracy=0.751</strong></pre>
<ol>
<li value="11">Plot the loss as a function of the epoch number, for both the train and <span class="No-Break">test sets:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.xlabel('epoch') plt.ylabel('loss (BCE)')</pre><pre class="source-code">
plt.legend() plt.show()</pre></li>
</ol>
<p>We then get <span class="No-Break">this graph:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer244">
<img alt="Figure 8.7 – A binary cross-entropy loss as a function of the epoch" height="415" src="image/B19629_08_07.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.7 – A binary cross-entropy loss as a function of the epoch</p>
<p>As we can see, the loss is clearly diverging for the test set, meaning after only a few epochs, there is <span class="No-Break">already overfitting.</span></p>
<ol>
<li value="12">Similarly, plot<a id="_idIndexMarker491"/> the accuracy as a function of the epoch number of both the train and <span class="No-Break">test sets:</span><pre class="source-code">
plt.plot(train_accuracy, label='train')</pre><pre class="source-code">
plt.plot(test_accuracy, label='test')</pre><pre class="source-code">
plt.xlabel('epoch') plt.ylabel('Accuracy')</pre><pre class="source-code">
plt.legend() plt.show()</pre></li>
</ol>
<p>This is the <span class="No-Break">graph obtained:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer245">
<img alt="Figure 8.8 – Accuracy as a function of the epoch" height="413" src="image/B19629_08_08.jpg" width="551"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.8 – Accuracy as a function of the epoch</p>
<p>As with the loss, we can see we face overfitting with an accuracy close to 100% on the train set, but it is only about a maximum of 77% on the <span class="No-Break">test set.</span></p>
<p>On a side note, if you try this recipe and the previous one yourself, you may find the GRU much more<a id="_idIndexMarker492"/> stable in the results, while the RNN in the previous recipe may sometimes have a hard time <span class="No-Break">properly converging.</span></p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor217"/>There’s more…</h2>
<p>When working with sequential data such as text, time series, and audio, RNNs are commonly used. While simple RNNs are not so frequently used because of their limitations, GRUs are usually a better choice. Besides simple RNNs and GRUs, another type of cell is frequently<a id="_idIndexMarker493"/> used – <strong class="bold">long short-term memory</strong> cells, better known <span class="No-Break">as </span><span class="No-Break"><strong class="bold">LSTMs</strong></span><span class="No-Break">.</span></p>
<p>The cell of an LSTM is even more complex than the one of a GRU. While a GRU cell has a hidden state and two gates, an LSTM cell has two types of hidden states (the hidden state and the cell state) and three gates. Let’s now have a <span class="No-Break">quick look.</span></p>
<p>The cell state of an LSTM is described in <span class="No-Break"><em class="italic">Figure 8</em></span><span class="No-Break"><em class="italic">.9</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer246">
<img alt="Figure 8.9 – A diagram of an LSTM cell, assuming the LSTM activation function is a tanh and the output layer activation function is a softmax" height="877" src="image/B19629_08_09.jpg" width="1006"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.9 – A diagram of an LSTM cell, assuming the LSTM activation function is a tanh and the output layer activation function is a softmax</p>
<p>Without getting into all the computational details of the LSTM, from <span class="No-Break"><em class="italic">Figure 8</em></span><em class="italic">.9</em> we can see there are three gates, computed with their own set of weights, based on both the previous hidden<a id="_idIndexMarker494"/> state <img alt="" height="22" src="image/Formula_08_031.png" width="38"/> and the current features <img alt="" height="18" src="image/Formula_08_032.png" width="19"/>, just like for a GRU, with a sigmoid <span class="No-Break">activation function:</span></p>
<ul>
<li>The forget <span class="No-Break">gate <img alt="" height="25" src="image/Formula_08_033.png" width="18"/></span></li>
<li>The update <span class="No-Break">gate <img alt="" height="22" src="image/Formula_08_034.png" width="21"/></span></li>
<li>The output <span class="No-Break">gate <img alt="" height="22" src="image/Formula_08_035.png" width="19"/></span></li>
</ul>
<p>There are also two states, computed at <span class="No-Break">each step:</span></p>
<ul>
<li>A cell <span class="No-Break">state </span><span class="No-Break"><img alt="" height="26" src="image/Formula_08_036.png" width="161"/></span></li>
<li>A hidden <span class="No-Break">state <img alt="" height="27" src="image/Formula_08_037.png" width="86"/></span></li>
</ul>
<p>Here, the intermediary state <img alt="" height="23" src="image/Formula_08_038.png" width="18"/> is computed with its own set of weights, just like a gate, with a free <span class="No-Break">activation function.</span></p>
<p>Having more gates and states, LSTMs have more parameters than GRUs and, thus, usually need more data to be properly trained. However, they are proven to be very effective with long sequences, such as <span class="No-Break">long texts.</span></p>
<p>Using PyTorch, the code for training an LSTM is not much different from the code to train a GRU. In this recipe, the only piece of code that needs to be changed would be the model implementation, replaced, for example, with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim,
        hidden_size, output_size, num_layers=3):
            super(LSTMClassifier, self).__init__()
            self.hidden_size = hidden_size
            self.num_layers = num_layers
            self.embedding = nn.Embedding(
                num_embeddings=vocab_size,
                embedding_dim=embedding_dim)
                self.lstm = nn.LSTM(
                    input_size=embedding_dim,
                    hidden_size=hidden_size,
                    num_layers=num_layers,
                    batch_first=True)
            self.fc = nn.Linear(hidden_size, output_size)
    def forward(self, inputs):
        batch_size = inputs.size(0)
        h_0 = torch.zeros(self.num_layers, batch_size,
            self.hidden_size)
        c_0 = torch.zeros(self.num_layers, batch_size,
            self.hidden_size)
        embedded = self.embedding(inputs)
        output,
        (final_hidden_state, final_cell_state) = self.lstm(
            embedded, (h_0, c_0))
        output = torch.softmax(self.fc(output[:, -1]),
            dim=1)
        return output</pre>
<p>The main differences <a id="_idIndexMarker495"/>with <strong class="source-inline">GRUClassifier</strong> implemented earlier are <span class="No-Break">the following:</span></p>
<ul>
<li>In <strong class="source-inline">init</strong>: Of course, using <strong class="source-inline">nn.LSTM</strong> instead of <strong class="source-inline">nn.GRU</strong>, since we now want an <span class="No-Break">LSTM-based classifier</span></li>
<li>In <strong class="source-inline">forward</strong>: We now initialize two zero vectors, <strong class="source-inline">h0</strong> and <strong class="source-inline">c0</strong>, which are fed to <span class="No-Break">the LSTM</span></li>
<li>The output of the LSTM is now made of the output and both the hidden and <span class="No-Break">cell states</span></li>
</ul>
<p>Besides that, it can be trained the same way as a GRU, with the <span class="No-Break">same code.</span></p>
<p>On a comparative note, let’s compute the number of parameters in this LSTM, and let’s compare it to the number of parameters in an “equivalent” RNN and GRU (that is, the same hidden dimension, the same number of layers, and <span class="No-Break">so on).</span></p>
<p>The number of parameters in this LSTM can be computed with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
sum(p.numel() for p in list(
    model.parameters())[1:] if p.requires_grad)</pre>
<p class="callout-heading">Important note</p>
<p class="callout">Note that we do not take into account the embedding part, since we omit the <span class="No-Break">first layer.</span></p>
<p>Here is <a id="_idIndexMarker496"/>the <a id="_idIndexMarker497"/>number<a id="_idIndexMarker498"/> of parameters for each type <span class="No-Break">of model:</span></p>
<ul>
<li><span class="No-Break"><strong class="bold">RNN</strong></span><span class="No-Break">: 6,369</span></li>
<li><span class="No-Break"><strong class="bold">GRU</strong></span><span class="No-Break">: 19,041</span></li>
<li><span class="No-Break"><strong class="bold">LSTM</strong></span><span class="No-Break">: 25,377</span></li>
</ul>
<p>A rule of thumb to explain this is the number of gates. Compared to a simple RNN, a GRU has two additional gates requiring their own weights, hence a total number of parameters multiplied <a id="_idIndexMarker499"/>by 3. The same logic applies to the LSTM having <span class="No-Break">three gates.</span></p>
<p>Overall, the more parameters a model contains, the more data it needs to be trained robustly, which is why a GRU is a good trade-off and usually a good <span class="No-Break">first choice.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Up to now, we only assumed GRUs (and RNNs in general) can go from left to right – from the start of a sentence to the end of a sentence. Just because that’s what we humans usually do when we read, it doesn’t mean it’s necessarily the most optimal way for a neural network to learn. It is possible to use RNNs in both directions, known as <strong class="bold">bidirectional RNNs</strong>, and <a id="_idIndexMarker500"/>this can apply to not only simple RNNs but also GRUs and LSTMs. Implementing such a model cannot be easier than with PyTorch, since the only thing to change is to add <strong class="source-inline">bidirectional=True</strong> to the model definition, such <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">nn.GRU(bidirectional=True)</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-218"><a id="_idTextAnchor218"/>See also</h2>
<ul>
<li>The official <a id="_idIndexMarker501"/>documentation about <span class="No-Break">GRUs: </span><a href="https://pytorch.org/docs/stable/generated/torch.nn.GRU.xhtml%0D"><span class="No-Break">https://pytorch.org/docs/stable/generated/torch.nn.GRU.xhtml</span></a></li>
<li>The official <a id="_idIndexMarker502"/>documentation about <span class="No-Break">LSTMs: </span><a href="https://pytorch.org/docs/stable/generated/torch.nn.LSTM.xhtml%0D"><span class="No-Break">https://pytorch.org/docs/stable/generated/torch.nn.LSTM.xhtml</span></a></li>
<li>A somewhat out-of-date but great post about LSTMs’ effectiveness by Andrej <span class="No-Break">Karpathy: </span><a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/%0D"><span class="No-Break">https://karpathy.github.io/2015/05/21/rnn-effectiveness/</span></a></li>
</ul>
<h1 id="_idParaDest-219"><a id="_idTextAnchor219"/>Regularizing with dropout</h1>
<p>In this recipe, we <a id="_idIndexMarker503"/>will add dropout to a GRU to add regularization to the IMDb <span class="No-Break">classification dataset.</span></p>
<h2 id="_idParaDest-220"><a id="_idTextAnchor220"/>Getting ready</h2>
<p>Just like fully connected neural networks, recurrent neural networks such as GRUs and LSTMs can be trained with dropout. As a reminder, dropout is just randomly setting some unit’s activation to zero during training. As a result, it allows a network to have less information at once and to hopefully <span class="No-Break">generalize better.</span></p>
<p>We will improve upon the results of the GRU training recipe, by using dropout on the same task – the IMDb dataset <span class="No-Break">binary classification.</span></p>
<p>If not already done, the dataset can be downloaded using the Kaggle API with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-moviereviews --unzip</pre>
<p>The required libraries can be installed with <span class="No-Break">the following:</span></p>
<pre class="source-code">
pip install pandas numpy scikit-learn matplotlib torch transformers</pre>
<h2 id="_idParaDest-221"><a id="_idTextAnchor221"/>How to do it…</h2>
<p>Here are the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li>We will train a GRU on the IMDb dataset, just like in the <em class="italic">Training a GRU</em> recipe. Since the five first steps of <em class="italic">Training a GRU</em> (from the imports to the <strong class="source-inline">DataLoaders</strong> instantiation) are common to this recipe, let’s just assume they have been run and start directly with the model class implementation. Implement the GRU classifier model. It is made of the <span class="No-Break">following elements:</span><ul><li>An embedding layer (taking a zero vector as the first input), on which dropout is applied in <span class="No-Break">the forward</span></li><li>Three layers of GRU, with dropout directly provided as an argument to the <span class="No-Break">GRU constructor</span></li><li>A fully connected <a id="_idIndexMarker504"/>layer on the last sequence step, with a sigmoid activation function, with <span class="No-Break">no dropout:</span><pre class="source-code">
# Define GRU model</pre><pre class="source-code">
class GRUClassifier(nn.Module):</pre><pre class="source-code">
    def __init__(self, vocab_size, embedding_dim,</pre><pre class="source-code">
        hidden_size, output_size, num_layers=3,</pre><pre class="source-code">
        dropout=0.25):</pre><pre class="source-code">
            super(GRUClassifier, self).__init__()</pre><pre class="source-code">
            self.num_layers = num_layers</pre><pre class="source-code">
            self.hidden_size = hidden_size</pre><pre class="source-code">
            self.embedding = nn.Embedding(</pre><pre class="source-code">
                num_embeddings=vocab_size,</pre><pre class="source-code">
                embedding_dim=embedding_dim)</pre><pre class="source-code">
            self.dropout = nn.Dropout(dropout)</pre><pre class="source-code">
            self.gru = nn.GRU(</pre><pre class="source-code">
                input_size=embedding_dim,</pre><pre class="source-code">
                hidden_size=hidden_size,</pre><pre class="source-code">
                num_layers=num_layers,</pre><pre class="source-code">
                batch_first=True, dropout=dropout)</pre><pre class="source-code">
            self.fc = nn.Linear(hidden_size,</pre><pre class="source-code">
                output_size)</pre><pre class="source-code">
    def forward(self, inputs):</pre><pre class="source-code">
        batch_size = inputs.size(0)</pre><pre class="source-code">
        zero_hidden = torch.zeros(self.num_layers,</pre><pre class="source-code">
            batch_size, self.hidden_size).to(device)</pre><pre class="source-code">
        embedded = self.dropout(</pre><pre class="source-code">
            self.embedding(inputs))</pre><pre class="source-code">
        output, hidden = self.gru(embedded,</pre><pre class="source-code">
            zero_hidden)</pre><pre class="source-code">
        output = torch.sigmoid(self.fc(output[:, -1]))</pre><pre class="source-code">
        return output</pre></li></ul></li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">It is not mandatory to apply dropout to the embedding, nor is it always useful. In this case, since the embedding is a large part of the model, applying dropout only to the GRU layers won’t have a significant impact <span class="No-Break">on performance.</span></p>
<ol>
<li value="2">Instantiate the GRU model, with an embedding dimension and a hidden dimension <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">32</strong></span><span class="No-Break">:</span><pre class="source-code">
vocab_size = train_dataset.tokenizer.vocab_size</pre><pre class="source-code">
embedding_dim = 32 hidden_dim = 32 output_size = 1</pre><pre class="source-code">
# Optionally, set the device to GPU if you have one</pre><pre class="source-code">
device = torch.device(</pre><pre class="source-code">
    'cuda' if torch.cuda.is_available() else 'cpu')</pre><pre class="source-code">
model = GRUClassifier(</pre><pre class="source-code">
    vocab_size=vocab_size,</pre><pre class="source-code">
    embedding_dim=embedding_dim,</pre><pre class="source-code">
    hidden_size=hidden_dim,</pre><pre class="source-code">
     output_size=output_size, ).to(device)</pre></li>
</ol>
<p>Instantiate the optimizer as an Adam optimizer, with a learning rate of <strong class="source-inline">0.001</strong>. The loss is defined as the binary cross-entropy loss, since this is a binary <span class="No-Break">classification task:</span></p>
<pre class="source-code">
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.BCELoss()</pre>
<ol>
<li value="3">Train the model <a id="_idIndexMarker505"/>over 20 epochs by reusing the helper function implemented in the previous recipe. For each epoch, we compute and store the accuracy and the loss for both the train and <span class="No-Break">test sets:</span><pre class="source-code">
train_losses, test_losses, train_accuracy, </pre><pre class="source-code">
test_accuracy = train_IMDB_classification(model,</pre><pre class="source-code">
    train_dataloader, test_dataloader, criterion,</pre><pre class="source-code">
    device, epochs=20)</pre></li>
</ol>
<p>The last epoch output should look <span class="No-Break">like this:</span></p>
<pre class="source-code">
<strong class="bold">[epoch 20] Training: loss=0.248 accuracy=0.896 |  Test: loss=0.550 accuracy=0.785</strong></pre>
<ol>
<li value="4">Plot the loss as a function of the epoch number, for both the train and <span class="No-Break">test sets:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.xlabel('epoch') plt.ylabel('loss (BCE)')</pre><pre class="source-code">
plt.legend() plt.show()</pre></li>
</ol>
<p>Here is <span class="No-Break">the output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer255">
<img alt="Figure 8.10 – Binary cross-entropy loss as a function of the epoch" height="414" src="image/B19629_08_10.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.10 – Binary cross-entropy loss as a function of the epoch</p>
<p>We can see that <a id="_idIndexMarker506"/>we are still overfitting, but it’s a bit less dramatic than <span class="No-Break">without dropout.</span></p>
<ol>
<li value="5">Finally, plot the accuracy as a function of the epoch number of both the train and <span class="No-Break">test sets:</span><pre class="source-code">
plt.plot(train_accuracy, label='train')</pre><pre class="source-code">
plt.plot(test_accuracy, label='test')</pre><pre class="source-code">
plt.xlabel('epoch') plt.ylabel('Accuracy')</pre><pre class="source-code">
plt.legend() plt.show()</pre></li>
</ol>
<p>This is <span class="No-Break">the output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer256">
<img alt="Figure 8.11 – Accuracy as a function of the epoch" height="413" src="image/B19629_08_11.jpg" width="551"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.11 – Accuracy as a function of the epoch</p>
<p>Note the effect of dropout on the <span class="No-Break">train accuracy.</span></p>
<p>While the accuracy did not spectacularly improve, it has increased from 77% to 79% with dropout. Also, the difference between the train and test losses is smaller than it was without <a id="_idIndexMarker507"/>dropout, allowing us to <span class="No-Break">improve generalization.</span></p>
<h2 id="_idParaDest-222"><a id="_idTextAnchor222"/>There’s more…</h2>
<p>Unlike dropout, other methods that are useful to regularize fully connected neural networks can be used with GRUs and other <span class="No-Break">RNN-based architectures.</span></p>
<p>For example, since we have rather substantial overfitting here, with a train loss having a steep decrease, it might be interesting to test smaller architectures, with fewer parameters to <span class="No-Break">be learned.</span></p>
<h1 id="_idParaDest-223"><a id="_idTextAnchor223"/>Regularizing with the maximum sequence length</h1>
<p>In this recipe, we <a id="_idIndexMarker508"/>will regularize by playing with the maximum sequence length, on the IMDB dataset, using a GRU-based <span class="No-Break">neural network.</span></p>
<h2 id="_idParaDest-224"><a id="_idTextAnchor224"/>Getting ready</h2>
<p>Up to now, we have not played much with the maximum length of the sequence, but it is sometimes one of the most important hyperparameters <span class="No-Break">to tune.</span></p>
<p>Indeed, depending on the input dataset, the optimal maximum length can be <span class="No-Break">quite different:</span></p>
<ul>
<li>A tweet is short, so having a maximum number of tokens of hundreds does not make sense most of <span class="No-Break">the time</span></li>
<li>A product or movie review can be significantly longer, and sometimes, the reviewer writes a lot of pros and cons about the product/movie, before getting to the final conclusion – in such cases, a larger maximum length <span class="No-Break">may help</span></li>
</ul>
<p>In this recipe, we will train a GRU on the IMDb dataset, containing movie reviews and associated labels (either positive or negative); this dataset contains some very lengthy texts. So, we will<a id="_idIndexMarker509"/> significantly increase the maximum number of words, and see how it impacts the <span class="No-Break">final accuracy.</span></p>
<p>If not already done, you can download the dataset, assuming you have a Kaggle API installed, running the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
kaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-moviereviews --unzip</pre>
<p>The following libraries are needed: <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">matplotlib</strong>, <strong class="source-inline">torch</strong>, and <strong class="source-inline">transformers</strong>. They can be installed with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install pandas numpy scikit-learn matplotlib torch transformers</pre>
<h2 id="_idParaDest-225"><a id="_idTextAnchor225"/>How to do it…</h2>
<p>Here are the steps to perform <span class="No-Break">this recipe:</span></p>
<ol>
<li>This recipe will be mostly the same as the <em class="italic">Training a GRU</em> recipe on the IMDb dataset; it will only change the maximum length of the sequence. Since the most significant differences are the sequence length value and the results, we will assume the four first steps of <em class="italic">Training a GRU</em> (from the imports to the dataset implementation) have been run, and we will reuse some of the code. Instantiate the <strong class="source-inline">TextClassificationDataset</strong> objects for the train and test sets (reusing the class implemented in <em class="italic">Training a GRU</em>), as well as the related <span class="No-Break">data loaders.</span></li>
</ol>
<p>This time, we chose a maximum number of words of <strong class="source-inline">256</strong>, significantly higher than the earlier value of <strong class="source-inline">64</strong>. We will keep a batch size <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">64</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
batch_size = 64 max_words = 256
# Initialize datasets and dataloaders
Train_dataset = TextClassificationDataset(train_data,
    max_words)
test_dataset = TextClassificationDataset(test_data,
    max_words)
train_dataloader = DataLoader(train_dataset,
    batch_size=batch_size, shuffle=True)
test_dataloader = DataLoader(test_dataset,
    batch_size=batch_size, shuffle=True)</pre>
<ol>
<li value="2">Instantiate the GRU <a id="_idIndexMarker510"/>model by reusing the <strong class="source-inline">GRUClassifier</strong> class implemented in <em class="italic">Training a GRU</em>, with an embedding dimension and a hidden dimension <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">32</strong></span><span class="No-Break">:</span><pre class="source-code">
vocab_size = train_dataset.tokenizer.vocab_size</pre><pre class="source-code">
embedding_dim = 32</pre><pre class="source-code">
hidden_dim = 32</pre><pre class="source-code">
output_size = 1</pre><pre class="source-code">
# Optionally, set the device to GPU if you have one</pre><pre class="source-code">
device = torch.device(</pre><pre class="source-code">
    'cuda' if torch.cuda.is_available() else 'cpu')</pre><pre class="source-code">
model = GRUClassifier(</pre><pre class="source-code">
    vocab_size=vocab_size,</pre><pre class="source-code">
    embedding_dim=embedding_dim,</pre><pre class="source-code">
    hidden_size=hidden_dim,</pre><pre class="source-code">
    output_size=output_size, ).to(device)</pre></li>
</ol>
<p>Instantiate the optimizer as an Adam optimizer, with a learning rate of <strong class="source-inline">0.001</strong>. The loss is defined as the binary cross-entropy loss, since this is a binary <span class="No-Break">classification task:</span></p>
<pre class="source-code">
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.BCELoss()</pre>
<ol>
<li value="3">Train the model over 20 epochs reusing the <strong class="source-inline">train_IMDB_classification</strong> helper function implemented in the <em class="italic">Training a GRU</em> recipe; store the accuracy and the loss for both the train and test sets for <span class="No-Break">each epoch:</span><pre class="source-code">
train_losses, test_losses, train_accuracy, </pre><pre class="source-code">
test_accuracy = train_IMDB_classification(model,</pre><pre class="source-code">
    train_dataloader, test_dataloader, criterion,</pre><pre class="source-code">
    device, epochs=20)</pre></li>
</ol>
<p>After 20 epochs, the<a id="_idIndexMarker511"/> output looks <span class="No-Break">like this:</span></p>
<pre class="source-code">
<strong class="bold">[epoch 20] Training: loss=0.022 accuracy=0.995 |  Test: loss=0.640 accuracy=0.859</strong></pre>
<ol>
<li value="4">Plot the loss as a function of the epoch number for both the train and <span class="No-Break">test sets:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.xlabel('epoch') plt.ylabel('loss (BCE)')</pre><pre class="source-code">
plt.legend() plt.show()</pre></li>
</ol>
<p>Here is <span class="No-Break">the output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer257">
<img alt="Figure 8.12 – Binary cross-entropy loss as a function of the epoch" height="413" src="image/B19629_08_12.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.12 – Binary cross-entropy loss as a function of the epoch</p>
<p>We can see that there is overfitting after only a <span class="No-Break">few epochs.</span></p>
<ol>
<li value="5">Finally, plot <a id="_idIndexMarker512"/>the accuracy as a function of the epoch number of both the train and <span class="No-Break">test sets:</span><pre class="source-code">
plt.plot(train_accuracy, label='train')</pre><pre class="source-code">
plt.plot(test_accuracy, label='test')</pre><pre class="source-code">
plt.xlabel('epoch') plt.ylabel('Accuracy')</pre><pre class="source-code">
plt.legend() plt.show()</pre></li>
</ol>
<p>This is what <span class="No-Break">we get:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer258">
<img alt="Figure 8.13 – Accuracy as a function of the epoch" height="412" src="image/B19629_08_13.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.13 – Accuracy as a function of the epoch</p>
<p>The test accuracy reaches a maximum after a few epochs and then <span class="No-Break">slowly decreases.</span></p>
<p>Although there is still a large overfitting effect, compared to the training with a maximum length of 64, the test accuracy went from a maximum of 77% to a maximum of 87%, a <span class="No-Break">significant improvement.</span></p>
<h2 id="_idParaDest-226"><a id="_idTextAnchor226"/>There’s more…</h2>
<p>Instead of blindly choosing the maximum number of tokens, it might be interesting to first quickly analyze the <span class="No-Break">length distribution.</span></p>
<p>For relatively small datasets, it’s easy to compute the length of all samples; let’s do it with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
review_lengths = [len(tokens) for tokens in tokenizer(
    train_data['review'].to_list())['input_ids']]</pre>
<p>We can now plot<a id="_idIndexMarker513"/> the distribution of the review lengths with a histogram, using a <span class="No-Break">log scale:</span></p>
<pre class="source-code">
plt.hist(review_lengths, bins=50, log=True)
plt.xlabel('Review length (#tokens)') plt.show()</pre>
<p>This is <span class="No-Break">the output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer259">
<img alt="Figure 8.14 – A histogram of the review length of the IMDb dataset, in a log scale" height="925" src="image/B19629_08_14.jpg" width="1182"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 8.14 – A histogram of the review length of the IMDb dataset, in a log scale</p>
<p>We can see a peak of around 300 tokens in length, and almost no review has more than 1,500 tokens. As we can see from the histogram, most of the reviews seem to have a length of a couple of hundred tokens. We can also compute the average and <span class="No-Break">median length:</span></p>
<pre class="source-code">
print('Average length:', np.mean(review_lengths))
print('Median length:', np.median(review_lengths))</pre>
<p>The computed average and median values are <span class="No-Break">the following:</span></p>
<pre class="source-code">
Average length: 309.757075 Median length: 231.0</pre>
<p>As a result, the<a id="_idIndexMarker514"/> average length is about 309, and the median length is 231. According to this information, if the computational power allows it and depending on the task, choosing a maximum length of 256 seems to be a good <span class="No-Break">first choice.</span></p>
</div>
</div></body></html>