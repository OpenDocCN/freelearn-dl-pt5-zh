["```py\nkaggle datasets download -d lakshmi25npathi/imdb-dataset-of-50k-moviereviews --unzip\n```", "```py\npip install pandas numpy scikit-learn matplotlib torch gensim nltk\n```", "```py\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader, Dataset\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport gensim.downloader\nimport nltk\n# If running for the first time nltk.download('punkt')\n```", "```py\n    # Will take a while the first time, need to download about 1.6GB of the model\n    ```", "```py\n    word2vec_model = gensim.downloader.load('\n    ```", "```py\n        word2vec-google-news-300')\n    ```", "```py\n    # Load data data = pd.read_csv('IMDB Dataset.csv')\n    ```", "```py\n    # Split data into train and test sets train_data, \n    ```", "```py\n        test_data = train_test_split(data, test_size=0.2,\n    ```", "```py\n            random_state=0)\n    ```", "```py\n    # Define dataset class\n    ```", "```py\n    class TextClassificationDataset(Dataset):\n    ```", "```py\n        def __init__(self, data, word2vec_model,\n    ```", "```py\n            max_words):\n    ```", "```py\n            self.data = data\n    ```", "```py\n            self.word2vec_model = word2vec_model\n    ```", "```py\n            self.max_words = max_words\n    ```", "```py\n            self.embeddings = data['review'].apply(\n    ```", "```py\n                self.embed)\n    ```", "```py\n            le = LabelEncoder()\n    ```", "```py\n            self.labels = torch.tensor(le.fit_transform(\n    ```", "```py\n                data['sentiment']).astype(np.float32))\n    ```", "```py\n        def __len__(self):\n    ```", "```py\n            return len(self.data)\n    ```", "```py\n        def __getitem__(self, index):\n    ```", "```py\n            return self.embeddings.iloc[index],\n    ```", "```py\n                self.labels[index]\n    ```", "```py\n        def embed(self, text):\n    ```", "```py\n            tokens = nltk.word_tokenize(text)\n    ```", "```py\n            return self.tokens_to_embeddings(tokens)\n    ```", "```py\n        def tokens_to_embeddings(self, tokens):\n    ```", "```py\n            embeddings = []\n    ```", "```py\n            for i, token in enumerate(tokens):\n    ```", "```py\n                if i >= self.max_words:\n    ```", "```py\n                    break\n    ```", "```py\n                if token not in self.word2vec_model:\n    ```", "```py\n                    continue\n    ```", "```py\n                embeddings.append(\n    ```", "```py\n                    self.word2vec_model[token])\n    ```", "```py\n            while len(embeddings) < self.max_words:\n    ```", "```py\n                embeddings.append(np.zeros((300, )))\n    ```", "```py\n            return np.array(embeddings, dtype=np.float32)\n    ```", "```py\n    batch_size = 64 max_words = 64\n    ```", "```py\n    # Initialize datasets and dataloaders\n    ```", "```py\n    Train_dataset = TextClassificationDataset(train_data,\n    ```", "```py\n        word2vec_model, max_words)\n    ```", "```py\n    test_dataset = TextClassificationDataset(test_data,\n    ```", "```py\n        word2vec_model, max_words)\n    ```", "```py\n    train_dataloader = DataLoader(train_dataset,\n    ```", "```py\n        batch_size=batch_size, shuffle=True)\n    ```", "```py\n    test_dataloader = DataLoader(test_dataset,\n    ```", "```py\n        batch_size=batch_size, shuffle=True)\n    ```", "```py\n    # Define RNN model\n    ```", "```py\n    class GRUClassifier(nn.Module):\n    ```", "```py\n        def __init__(self, embedding_dim, hidden_size,\n    ```", "```py\n            output_size, num_layers=3):\n    ```", "```py\n                super(GRUClassifier, self).__init__()\n    ```", "```py\n                self.hidden_size = hidden_size\n    ```", "```py\n                self.num_layers = num_layers\n    ```", "```py\n                self.gru = nn.GRU(\n    ```", "```py\n                    input_size=embedding_dim,\n    ```", "```py\n                    hidden_size=hidden_size,\n    ```", "```py\n                    num_layers=num_layers,\n    ```", "```py\n                    batch_first=True)\n    ```", "```py\n            self.fc = nn.Linear(hidden_size, output_size)\n    ```", "```py\n        def forward(self, inputs):\n    ```", "```py\n            batch_size = inputs.size(0)\n    ```", "```py\n            zero_hidden = torch.zeros(self.num_layers,\n    ```", "```py\n                batch_size, self.hidden_size).to(device)\n    ```", "```py\n            output, hidden = self.gru(inputs, zero_hidden)\n    ```", "```py\n            output = torch.sigmoid(self.fc(output[:, -1]))\n    ```", "```py\n            return output\n    ```", "```py\n    embedding_dim = 300\n    ```", "```py\n    hidden_dim = 32\n    ```", "```py\n    output_size = 1\n    ```", "```py\n    # Optionally, set the device to GPU if you have one device = torch.device(\n    ```", "```py\n        'cuda' if torch.cuda.is_available() else 'cpu')\n    ```", "```py\n    model = GRUClassifier(\n    ```", "```py\n        embedding_dim=ebedding_dim,\n    ```", "```py\n        hidden_siz=hidden_dim,\n    ```", "```py\n        output_size=output_size, ).to(device)\n    ```", "```py\n    optimizer = optim.Adam(model.parameters(), lr=0.001)\n    ```", "```py\n    criterion = nn.BCELoss()\n    ```", "```py\n    train_losses, test_losses, train_accuracy, \n    ```", "```py\n    test_accuracy = train_model(\n    ```", "```py\n        model, train_dataloader, test_dataloader,\n    ```", "```py\n        criterion, optimizer, device, epochs=20)\n    ```", "```py\n[epoch 20] Training: loss=0.207 accuracy=0.917 |  Test: loss=0.533 accuracy=0.790\n```", "```py\n    plt.plot(train_losses, label='train')\n    ```", "```py\n    plt.plot(testlosse, label=''test'')\n    ```", "```py\n    plt.xlabel('epoch') plt.ylabel('loss (BCE)')\n    ```", "```py\n    plt.legend() plt.show()\n    ```", "```py\n    plt.plot(train_accuracy, label='train')\n    ```", "```py\n    plt.plot(testaccurcy, label=''test'')\n    ```", "```py\n    plt.xlabel('epoch') plt.ylabel('Accuracy')\n    ```", "```py\n    plt.legend() plt.show()\n    ```", "```py\n    import numpy as np\n    ```", "```py\n    import gensim.downloader\n    ```", "```py\n    # Load the Word2Vec model\n    ```", "```py\n    word2vec_model = gensim.downloader.load(\n    ```", "```py\n        'word2vec-google-news-300')\n    ```", "```py\n    def replace_words_with_similar(text, model,\n    ```", "```py\n        sim_threshold: float = 0.5,\n    ```", "```py\n        probability: float = 0.5,\n    ```", "```py\n        top_similar: int = 3,\n    ```", "```py\n        stop_words: list[str] = []):\n    ```", "```py\n        # Split in words\n    ```", "```py\n        words = text.split()\n    ```", "```py\n        # Create an empty list of the output words\n    ```", "```py\n        new_words = []\n    ```", "```py\n        # Loop over the words\n    ```", "```py\n        for word in words:\n    ```", "```py\n            added = False\n    ```", "```py\n            # If the word is in the vocab, not in stop words, and above probability, then...\n    ```", "```py\n            if word in model and word not in stop_words and np.random.uniform(0, 1) > probability:\n    ```", "```py\n                # Get the top_similar most similar words\n    ```", "```py\n                similar_words = model.most_similar(word,\n    ```", "```py\n                    topn=top_similar)\n    ```", "```py\n                # Randomly pick one of those words\n    ```", "```py\n                idx = np.random.randint(len(similar_words))\n    ```", "```py\n                # Get the similar word and similarity score\n    ```", "```py\n                sim_word, sim_score = similar_words[idx]\n    ```", "```py\n                # If the similary score is above threshold, add the word\n    ```", "```py\n                if sim_score > sim_threshold:\n    ```", "```py\n                    new_words.append(sim_word)\n    ```", "```py\n                    added = True\n    ```", "```py\n            if not added:\n    ```", "```py\n                # If no similar word is added, add the original word\n    ```", "```py\n                new_words.append(word)\n    ```", "```py\n        # Return the list as a string\n    ```", "```py\n        return ' '.join(new_words)\n    ```", "```py\n    original_text = \"The quick brown fox jumps over the lazy dog\"\n    ```", "```py\n    generated_text = replace_words_with_similar(\n    ```", "```py\n        original_text, word2vec_model, top_words=['the'])\n    ```", "```py\n    print(\"\"Original text: {}\"\".format(original_text))\n    ```", "```py\n    print(\"New text: {}\".format(generated_text))\n    ```", "```py\nOriginal text: The quick brown fox jumps over the lazy dog New text: This quick brown squirrel jumps Over the lazy puppy\n```", "```py\nclass TextClassificationDatasetGeneration(Dataset):\n    def __init__(self, data, max_length):\n        self.data = data\n        self.max_length = max_length\n        self.tokenizer = AutoTokenizer.from_pretrained(\n            'bert-base-uncased')\n        self.tokens = self.tokenizer(\n            data['review'].to_list(), padding=True,\n            truncation=True, max_length=max_length,\n            return_tensors='pt')['input_ids']\n        le = LabelEncoder()\n        self.labels = torch.tensor(le.fit_transform(\n            data['sentiment']).astype(np.float32))\n    def __len__(self):\n        return len(self.data)\n    def __getitem__(self, index):\n        # Generate a new text\n        text = replace_words_with_similar(\n            self.data['review'].iloc[index])\n        # Tokenize it\n        tokens = self.tokenizer(text, padding=True,\n            truncation=True, max_length=self.max_length,\n            return_tensors='pt')['input_ids']\n        return self.tokens[index], self.labels[index]\n```", "```py\nkaggle datasets download -d crowdflower/twitter-airline-sentiment --unzip\n```", "```py\npip install pandas scikit-learn transformers\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom transformers import pipeline\n```", "```py\n    # Load dat\n    ```", "```py\n    Data = pd.read_csv(''Tweets.csv'')\n    ```", "```py\n    data[['airline_sentiment', 'text']].head()\n    ```", "```py\n    # Split data into train and test sets\n    ```", "```py\n    Train_data, test_data = train_test_split(data,\n    ```", "```py\n        test_size=0.2, random_state=0)\n    ```", "```py\n# Taking a long time first time for downloading odel...\nClassifier = pipeline(task=\"\"zero-shot-classification\"\",\n    model=\"facebook/bart-large-mnli\")\n```", "```py\n    candidate_labels = data['airline_sentiment'].unique()\n    ```", "```py\n    # Create an empty list to store the predictions\n    ```", "```py\n    preds = [] # Loop over the data\n    ```", "```py\n    for i in range(len(test_data)):\n    ```", "```py\n        # Compute the classifier results\n    ```", "```py\n        res = classifier(\n    ```", "```py\n            test_data['text'].iloc[i],\n    ```", "```py\n            candidate_labels=candidate_labels,\n    ```", "```py\n        )\n    ```", "```py\n        # Apply softmax to the results to get the predicted class\n    ```", "```py\n        pred = np.array(res['scores']).argmax()\n    ```", "```py\n        labels = res['labels']\n    ```", "```py\n        # Store the results in the list\n    ```", "```py\n        preds.append(labels[pred])\n    ```", "```py\n    print(accuracy_score(test_data['airline_sentiment'], preds))\n    ```", "```py\n0.7452725250278087\n```", "```py\nres = classifier(\n    'I love to learn about regularization',\n    candidate_labels=['positive', 'negative', 'neutral'], )\nprint(res)\n```", "```py\n{'sequence': 'I love to learn about regularization',  'labels': ['positive', 'neutral', 'negative'],  'scores': [0.6277033686637878, 0.27620458602905273, 0.09609206020832062]}\n```", "```py\nres['labels'][np.array(res['scores']).argmax()]\n```", "```py\nkaggle datasets download -d crowdflower/twitter-airline-sentiment --unzip\n```", "```py\nimport torch\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom transformers import BertConfig, BertModel, BertTokenizer import pandas as pd\n```", "```py\n    # Load data data = pd.read_csv('Tweets.csv')\n    ```", "```py\n    # Split data into train and test sets train_data, \n    ```", "```py\n        test_data = train_test_split(data, test_size=0.2,\n    ```", "```py\n            random_state=0)\n    ```", "```py\n# Instantiate the tokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n# Initializing a BERT configuration\nconfiguration = BertConfig()\n# Initializing a BERT model with random weights\nbert = BertModel(configuration)\n# Loading pre-trained weights\nbert = bert.from_pretrained('bert-base-uncased')\n# Load the model on the GPU\nif any device = torch.device(\n    \"cuda\" if torch.cuda.is_available() else \"cpu\")\nbert.to(device)\n# Set the model to eval mode\nbert.eval()\n```", "```py\nmax_length = 24\n# Compute the embeddings for the train set \ntrain_tokens = tokenizer(\n    train_data['text'].values.tolist(),\n    add_special_tokens=True,\n    padding='max_length',\n    truncation=True,\n    max_length=max_length,\n    return_tensors='pt')\n    train_tokens = {k: v.to(device) for k,\n        v in train_tokens.items()}\nwith torch.no_gad():\n    train_embeddings = bert(\n        **train_tokens)..pooler_output\n# Compute the embeddings for the test set\ntest_tokens = tokenizer(\n    test_data['text'].values.tolist(),\n    add_special_tokens=True, padding='max_length',\n    truncation=True, max_length=max_length,\n    return_tensors='pt')\ntest_tokens = {k: v.to(device) for k,\n    v in test_tokens.items()}\nwith torch.no_grad():\n    test_embeddings = bert(\n        **test_tokens).pooler_output\n```", "```py\n    lr = LogisticRegression(C=0.5, max_iter=10000)\n    ```", "```py\n    lr.fit(train_embeddings.cpu(),\n    ```", "```py\n        train_data['airline_sentiment'])\n    ```", "```py\n    print('train accuracy:',\n    ```", "```py\n        lr.score(train_embeddings.cpu(),\n    ```", "```py\n        train_data['airline_sentiment']))\n    ```", "```py\n    print('test accuracy:',\n    ```", "```py\n        lr.score(test_embeddings.cpu(),\n    ```", "```py\n        test_data['airline_sentiment']))\n    ```", "```py\ntrain accuracy: 0.8035348360655737 test accuracy: 0.7882513661202186\n```", "```py\ntokens = tokenizer('What is a tokenizer?', add_special_tokens=True,\n    padding='max_length', truncation=True, max_length=max_length,\n    return_tensors='pt')\nprint(tokens)\n```", "```py\n{'input_ids': tensor([[  101,  2054,  2003,  1037, 19204, \n17629,  1029,  2023,  2003,  1037,           2204,  3160,  1012,\n   102,     0,     0,     0,     0,     0,     0,              0,\n     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0, \n0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])}\n```", "```py\nimport openai\n# Give your\nopenai.api_key = 'xx-xxx'\n# Query the API\ninput_text = 'This is a test sentence'\nmodel = 'text-embedding-ada-002'\nembeddings = openai.Embedding.create(input = [input_text],\n    model=model)['data'][0]['embedding']\n```", "```py\n    import openai\n    ```", "```py\n    openai.api_key = 'xxxxx'\n    ```", "```py\n    positive_examples = openai.ChatCompletion.create(\n    ```", "```py\n        model=\"gpt-3.5-turbo\",\n    ```", "```py\n        messages=[\n    ```", "```py\n            {\"role\": \"system\", \n    ```", "```py\n                \"content\": \"You watched a movie you loved.\"},\n    ```", "```py\n            {\"role\": \"user\", \"content\": \"Write a short,\n    ```", "```py\n                100-words review about this movie\"},\n    ```", "```py\n        ],\n    ```", "```py\n        max_tokens=128,\n    ```", "```py\n        temperature=0.5,\n    ```", "```py\n        n=3, )\n    ```", "```py\n    for i in range(len(positive_examples['choices'])):\n    ```", "```py\n        print(f'\\n\\nGenerated sentence {i+1}: \\n')\n    ```", "```py\n        print(positive_examples['choices'][i]['message']['content'])\n    ```", "```py\n    The following is the output of the three positive reviews generated by GPT-3.5:\n    ```", "```py\n    Generated sentence 1:   I recently watched the movie \"Inception\" and was blown away by its intricate plot and stunning visuals. The film follows a team of skilled thieves who enter people's dreams to steal their secrets. The concept of dream-sharing is fascinating and the execution of the idea is flawless. The cast, led by Leonardo DiCaprio, delivers outstanding performances that add depth to the characters. The action scenes are thrilling and the special effects are mind-bending. The film's score by Hans Zimmer is also noteworthy, adding to the overall immersive experience. \"Inception\" is a masterpiece that will leave you pondering its themes long after the credits roll.   Generated sentence 2:   I recently watched the movie \"The Shawshank Redemption\" and absolutely loved it. The story follows the life of a man named Andy Dufresne, who is wrongfully convicted of murder and sent to Shawshank prison. The movie beautifully portrays the struggles and hardships faced by prisoners, and the importance of hope and friendship in such a harsh environment. The acting by Tim Robbins and Morgan Freeman is outstanding, and the plot twists keep you engaged throughout the movie. Overall, \"The Shawshank Redemption\" is a must-watch for anyone who loves a good drama and a heartwarming story about the power of the human spirit.   Generated sentence 3:   I recently watched the movie \"Parasite\" and it blew me away. The story revolves around a poor family who slowly infiltrates the lives of a wealthy family, but things take a dark turn. The movie is a masterclass in storytelling, with each scene building tension and adding layers to the plot. The acting is superb, with standout performances from the entire cast. The cinematography is also stunning, with each shot expertly crafted to enhance the mood and atmosphere of the film. \"Parasite\" is a must-watch for anyone who loves a good thriller with a twist.\n    ```", "```py\n    # Generate the generated examples\n    ```", "```py\n    ngative_examples = openai.ChatCompletion.create(\n    ```", "```py\n        model=\"gpt-3.5-turbo\",\n    ```", "```py\n        messages=[\n    ```", "```py\n            {\"role\": \"system\",\n    ```", "```py\n             \"content\": \"You watched a movie you hated.\"},\n    ```", "```py\n            {\"role\": \"user\",\n    ```", "```py\n             \"content\": \"Write a short,\n    ```", "```py\n                100-wordsreview about this movie\"},\n    ```", "```py\n        ],\n    ```", "```py\n        max_tokens=128,\n    ```", "```py\n        temperature=0.5,\n    ```", "```py\n        n=3, )\n    ```", "```py\n    # Display the generated examples\n    ```", "```py\n    for i in range(len(\n    ```", "```py\n        negative_examples['choices'])):\n    ```", "```py\n        print(f'\\n\\nGenerated sentence {i+1}: \\n')\n    ```", "```py\n        print(negative_examples[\n    ```", "```py\n            'choices'][i]['message']['content'])\n    ```", "```py\nGenerated sentence 1:   I recently watched a movie that left me feeling disappointed and frustrated. The plot was weak and predictable, and the characters were one-dimensional and unrelatable. The acting was subpar, with wooden performances and lackluster chemistry between the cast. The special effects were underwhelming and failed to add any excitement or visual interest to the film. Overall, I found myself checking the time and counting down the minutes until the end. I wouldn't recommend this movie to anyone looking for a compelling and engaging cinematic experience.   Generated sentence 2:   I recently watched a movie that left me feeling disappointed and underwhelmed. The plot was predictable and lacked any real depth or complexity. The characters were one-dimensional and unrelatable, making it hard to invest in their stories. The pacing was slow and dragged on unnecessarily, making the already dull plot even more tedious to sit through. The acting was subpar, with even the most talented actors failing to bring any life to their roles. Overall, I found this movie to be a complete waste of time and would not recommend it to anyone looking for an engaging and entertaining film experience.   Generated sentence 3:   I recently watched a movie that I absolutely hated - \"The Roommate\". The plot was predictable and the acting was subpar at best. The characters were one-dimensional and lacked any depth or development throughout the film. The dialogue was cringe-worthy and the attempts at suspense fell flat. Overall, I found the movie to be a waste of time and would not recommend it to anyone. If you're looking for a thrilling and well-crafted thriller, \"The Roommate\" is definitely not the movie for you.\n```", "```py\n# Import the pipeline from transformers\nfrom transformers import pipeline\n# Instantiate a text-generation based on GPT-2 model\nGenerator = pipeline(''text-generation'', model=''gpt2'')\n#generator = pipeline('text2text-generation')#,\n# model = 'facebook/mbart-large-50')\n```", "```py\n# Generate the positive examples\npositive_example = generator(\n    \"I loved everything about this movie, \",\n    max_length = 512,\n    num_return_sequences=3, )\n# Display the generated examples\nfor i in range(len(positive_example)):\n    print(f'\\n\\nGenerated sentence {i+1}: \\n')\n    print(positive_example[i]['generated_text'])\n```", "```py\nGenerated sentence 1:   I loved everything about this movie,  the movie itself was great and I didn't want to stop after the first movie because I was so happy for it. But I did like the first movie and I loved the second one and so much of what I did with it and other things with how I handled the other films. What were your thoughts as fans? After the opening credits, I kept thinking, \"This will be my big, special moment, I can't do this alone\". I've watched the original 2 episodes, they do have a big difference in the ending. But really, it's just my dream to put in movies you love, and I'm always looking over every minute on how good they were and looking forward. I think because everyone is talking about it, and so much of the movie is just it of my dream, the movie that I am to get this, the movie that I want to watch or maybe I'm doing my dreams. Also, it's more to a big than your dream. I think if I get enough reviews, people will start to see it. There will even be a few who will see it in the summer. I know I did it in a way when I was in high school. I never read the reviews again because I'm not a gamer and I never read the reviews because I love that. But by watching the movies, I feel that I am better than that. So, my dream is going to be to give up on the game I started out with in high school and focus on the movies I like. Not just the one I played, but the one you all enjoy watching! I really hope you guys give it more attention and like for this one, just keep going or stay up for the next movie for when all the things you said can be true. Thanks in advance, and happy movie watching!   Generated sentence 2:   I loved everything about this movie,  It was a surprise to see. I want to say thank to the cast of the film, but don't call me the original star. I love that I have to keep myself on top of the world in other things. (laughs) I was excited about the ending and I was thinking about how much fun it would be to watch that ending. At the end of the day it was all for me. The movie was a shock to watch. It was all about the fact that he and her father can all die. It was so exciting. Says a fan, \"I've been waiting for this movie since childhood, and this is the first time I've seen it.\"   Generated sentence 3:   I loved everything about this movie,  so I made the only mistake I have ever made because for once it felt like this movie was happening. It's always exciting to see a feature that gives the fans something to feel. It's a truly beautiful world in which life isn't a game; life is a process. But it's fun to be forced to watch something that tells you some great things about our environment, even when only one person actually is there, who cares about it. This film was not just another film, it was a true movie. And while I'm still looking forward to seeing more amazing, unique movies from the history of cinema, I can guarantee you that there's more we'll be hearing about from our friends at AMC and others who care about our history, the history of film making, and the history of art-design in general...\n```"]