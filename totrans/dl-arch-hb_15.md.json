["```py\n    pip install git+ https://github.com/ELS-RD/transformer-deploy/tree/6b88e24ade6ce199e825adc0477b28a07f51f17d\n    ```", "```py\n    git clone https://github.com/ELS-RD/transformer-deploy\n    git checkout 6b88e24ade6ce199e825adc0477b28a07f51f17d\n    transformer-deploy uses the tracing mode from PyTorch to convert a PyTorch model into ONNX. This involves sending some example PyTorch tensor data to the PyTorch helper tool, which will then trace the data shapes in the model through a forward pass to form the graph in ONNX. The following code shows a snippet of this tool as a pytorch library method:\n\n    ```", "```py\n\n    ```", "```py\n\n    Here, `network_def` contains the ONNX graph definition of the model, and `config` specifies all the optimization strategies where the TensorRT default will be used.\n    ```", "```py\n    roneneldan/TinyStories-3M. Additionally, one aspect that we need to take care of when using any graph optimizations and conversions to another framework is to make sure it maintains a satisfactory level of accuracy and doesn’t degrade too much from the model when it is run in the original base framework. A natural way to do this is to check the validation performance where the model is trained. In this case, we don’t know what dataset the Hugging Face model is trained from, so a workaround can be to take any relevant text generation dataset and validate on both the base framework and target framework setup. The transformer-deploy tool performs a simple predicted values deviation check to make sure the predicted values that are generated using the target framework don’t deviate too far from the values generated using the base framework. The atol parameter controls the leniency of this deviation check.\n    ```", "```py\n    [Pytorch (FP32)] mean=6.05ms, sd=0.21ms, min=5.91ms, max=8.12ms, median=6.01ms, 95p=6.27ms, 99p=7.16ms\n    [Pytorch (FP16)] mean=7.76ms, sd=0.25ms, min=7.52ms, max=10.16ms, median=7.72ms, 95p=7.98ms, 99p=9.20ms\n    [TensorRT (FP16)] mean=0.61ms, sd=0.06ms, min=0.56ms, max=0.86ms, median=0.57ms, 95p=0.70ms, 99p=0.71ms\n    [ONNX Runtime (FP32)] mean=1.89ms, sd=0.06ms, min=1.81ms, max=2.48ms, median=1.88ms, 95p=1.93ms, 99p=2.21ms\n    [ONNX Runtime (FP16)] mean=2.07ms, sd=0.09ms, min=1.99ms, max=3.11ms, median=2.06ms, 95p=2.11ms, 99p=2.62ms\n    Each inference engine output is within 5.0 tolerance compared to Pytorch output\n    triton_models/model.plan\n    ```", "```py\n    models/transformer_tensorrt_text_generation/1/model.plan\n    ```", "```py\n    from typing import List\n    import numpy as np\n    import tensorrt as trt\n    import torch\n    import triton_python_backend_utils as pb_utils\n    from transformers import AutoTokenizer, TensorType\n    from transformer_deploy.backends.trt_utils import load_engine\n    ```", "```py\n    model = \"roneneldan/TinyStories-3M\"\n    tensorrt_path = \"/models/transformer_tensorrt_text_generation/1/model.plan\"\n    ```", "```py\n    class TritonPythonModel:\n        def initialize(self, args):\n            self.tokenizer = AutoTokenizer.from_pretrained(model)\n            self.model_input_names = self.tokenizer.model_input_names\n            trt_logger = trt.Logger(trt.Logger.VERBOSE)\n            runtime = trt.Runtime(trt_logger)\n            self.model = load_engine(\n                runtime=runtime, engine_file_path=tensorrt_path\n            )\n    ```", "```py\n    def execute(self, requests):\n            responses = []\n            for request in requests:\n                query = [t.decode(\"UTF-8\") for t in pb_utils.get_input_tensor_by_name(request, \"TEXT\").as_numpy().tolist()]\n                tokens = self.tokenizer(\n                    text=query, return_tensors=TensorType.PYTORCH, return_attention_mask=False\n                )\n                input_ids = tokens.input_ids.type(dtype=torch.int32)\n                input_ids = input_ids.to(\"cuda\")\n                output_seq: torch.Tensor = self.model({\"input_ids\": input_ids})['output'].cpu().argmax(2)\n                decoded_texts: List[str] = [self.tokenizer.decode(seq, skip_special_tokens=True) for seq in output_seq]\n                tensor_output = [pb_utils.Tensor(\"OUTPUT_TEXT\", np.array(t, dtype=object)) for t in decoded_texts]\n                responses.append(pb_utils.InferenceResponse(tensor_output))\n            return responses\n    ```", "```py\n    name: \"transformer_tensorrt_text_generation\"\n    max_batch_size: 0\n    backend: \"python\"\n    input [\n    {\n          name: \"TEXT\"\n          data_type: TYPE_STRING\n          dims: [ -1 ]\n    }\n    ]\n    output [\n    {\n          name: \"OUTPUT_TEXT\"\n          data_type: TYPE_STRING\n          dims: [ -1 ]\n    }\n    ]\n    instance_group [\n    {\n         count: 1\n         kind: KIND_GPU\n    }\n    ]\n    ```", "```py\n    models/transformer_tensorrt_text_generation/config.pbtxt\n    ```", "```py\n    triton_client.py, which will define the code that’s needed to query the hosted model to obtain a generated text and print it out on the command line. The first step is to import the necessary libraries, which in this case will only be the HTTP client from the tritonclient library:\n\n    ```", "```py\n\n    ```", "```py\n    MODEL_NAME = \"transformer_tensorrt_text_generation\"\n    MODEL_VERSION = \"1\"\n    ```", "```py\n    def main():\n        client = httpclient.InferenceServerClient(url=\"localhost:8000\"\n        input_text = np.array([\"Tell me a joke.\"], dtype=object)\n        input_tensors = [\n            httpclient.InferInput(\"TEXT\", (1,), datatype=\"BYTES\")\n        ]\n        input_tensors[0].set_data_from_numpy(input_text)\n        outputs = [\n           httpclient.InferRequestedOutput(\"OUTPUT_TEXT\")\n        ]\n        query_response = client.infer(\n            model_name=MODEL_NAME,\n            model_version=MODEL_VERSION,\n            inputs=input_tensors,\n            outputs=outputs\n          )\n          output_text = query_response.as_numpy(\"OUTPUT_TEXT\")\n          print(output_text)\n        if __name__ == '__main__':\n            main()\n    ```", "```py\n    b' you are big!\"\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n'\n    ```"]