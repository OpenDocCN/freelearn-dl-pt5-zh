<html><head></head><body>
		<div id="_idContainer031">
			<h1 id="_idParaDest-17"><em class="italic"><a id="_idTextAnchor016"/>Chapter 1:</em> Introduction to Deep Learning with KNIME Analytics Platform</h1>
			<p>We'll start our journey of exploring <strong class="bold">Deep Learning</strong> (<strong class="bold">DL</strong>) paradigms by looking at KNIME Analytics Platform. If you have always been drawn to neural networks and deep learning architectures and have always thought that the coding part would be an obstacle to you developing a quick learning curve, then this is the book for you.</p>
			<p>Deep learning can be quite complex, and we must make sure that the journey is worth the result. Thus, we'll start this chapter by stating, once again, the relevance of deep learning techniques when it comes to successfully implementing applications for data science.</p>
			<p>We will continue by providing a quick overview of the tool of choice for this book – KNIME Software – and focus on how it complements both KNIME Analytics Platform and KNIME Server.</p>
			<p>The work we'll be doing throughout this book will be implemented in KNIME Analytics Platform, which is open source and available for free. We will dedicate a full section to how to download, install, and use KNIME Analytics Platform, even though more details will be provided in the chapters to follow.</p>
			<p>Among the benefits of KNIME Analytics Platform is, of course, its codeless Deep Learning - Keras Integration extension, which we will be making extensive use of throughout this book. In this chapter, we will just focus on the basic concepts and requirements for this KNIME extension.</p>
			<p>Finally, we will conclude this chapter by stating the goal and structure of this book. We wanted to give it a practical flavor, so most of the chapters will revolve around a practical case study that includes real-world data. In each chapter, we will take the chance to dig deeper into the required neural architecture, data preparation, deployment, and other aspects necessary to make the case study at hand a success.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>The Importance of Deep Learning</li>
				<li>Exploring KNIME Software</li>
				<li>Exploring KNIME Analytics Platform</li>
				<li>Installing KNIME Deep Learning – Keras Integration</li>
				<li>Goals and Structure of this Book</li>
			</ul>
			<p>We'll start by stating the importance of deep learning when it comes to successful data science applications.</p>
			<h1 id="_idParaDest-18"><a id="_idTextAnchor017"/>The Importance of Deep Learning</h1>
			<p>If you have been working <a id="_idIndexMarker000"/>in the field of <strong class="bold">data science</strong> – or <strong class="bold">Artificial Intelligence</strong> (<strong class="bold">AI</strong>), as it is called<a id="_idIndexMarker001"/> nowadays – for a few years, you might have noticed the recent sudden explosion of scholarly and practitioner articles about successful solutions based on deep learning techniques.</p>
			<p>The big breakthrough<a id="_idIndexMarker002"/> happened in 2012 when the deep learning-based AlexNet network won the ImageNet challenge by an unprecedented margin. This victory kicked off a surge in the usage of deep learning networks. Since then, these have expanded to many different domains and tasks.</p>
			<p>So, what are we referring to exactly when we talk about deep learning? Deep learning covers a subset of <strong class="bold">Machine Learning</strong> (<strong class="bold">ML</strong>) algorithms, most of which stem from neural networks. Deep learning <a id="_idIndexMarker003"/>is indeed the modern evolution of traditional neural networks. Apart from the classic feedforward, fully connected, backpropagation-trained, and multilayer perceptron architectures, <em class="italic">deeper</em> architectures have been added. Deeper indicates<a id="_idIndexMarker004"/> more hidden layers and a <a id="_idIndexMarker005"/>few new <a id="_idIndexMarker006"/>additional neural paradigms, including <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>), <strong class="bold">Long-Short Term Memory</strong> (<strong class="bold">LSTM</strong>), <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>), <strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>), and <a id="_idIndexMarker007"/>more.</p>
			<p>The recent success of these new types of neural networks is due to several reasons. First, the increased computational power in modern machines has favored the introduction and development of new paradigms and more complex neural architectures. Training a complex neural network in minutes leaves space for more experimentation compared to training the same network for hours or days. Another reason is due to their flexibility. Neural networks are universal function approximators, which means that they can approximate almost anything, provided that their architecture is sufficiently complex. </p>
			<p>Having mathematical knowledge of these algorithms, experience with the most effective paradigms and architectures, and domain wisdom are all basic, important, and necessary ingredients for the success of any data science project. However, there are other, more contingent factors – such as ease of learning, speed of prototyping, options for debugging and testing to ensure the correctness of the solution, flexibility to experiment, availability of help<a id="_idIndexMarker008"/> from external experts, and automation and security capabilities – that also influence the final result of the project.</p>
			<p>In this book, we'll present deep learning solutions that can be implemented with the open source, visual programming-based, free-to-use tool known as KNIME Analytics Platform. The deployment phases for some of these solutions also use a few features provided by KNIME Server.</p>
			<p>Next, we will learn about how KNIME Analytics Platform and KNIME Server complement each other, as well as which tasks both should be used for.</p>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor018"/>Exploring KNIME Software</h1>
			<p>We will mainly be<a id="_idIndexMarker009"/> working with two KNIME products: KNIME Analytics Platform and KNIME Server. KNIME Analytics Platform includes ML and deep learning algorithms and data operations needed for data science projects. KNIME Server, on the other hand, provides the IT infrastructure for easy and secure deployment, as well as model monitoring over time.</p>
			<p>We'll concentrate on KNIME Analytics Platform first and provide an overview of what it can accomplish.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor019"/>KNIME Analytics Platform</h2>
			<p><strong class="bold">KNIME Analytics Platform</strong> is <a id="_idIndexMarker010"/>an open source piece of software for all your data needs. It is free to download from the KNIME <a id="_idIndexMarker011"/>website (<a href="https://www.knime.com/downloads">https://www.knime.com/downloads</a>) and free to use. It covers all the main data wrangling and machine learning techniques available at the time of writing, and it is based on visual programming.</p>
			<p><strong class="bold">Visual programming</strong> is a <a id="_idIndexMarker012"/>key feature of KNIME Analytics Platform for quick prototyping. It makes the tool very easy to use. In visual programming, a <strong class="bold">Graphical User Interface</strong> (<strong class="bold">GUI</strong>) guides you through all the necessary steps for building a pipeline (workflow) of dedicated<a id="_idIndexMarker013"/> blocks (nodes). Each node implements a given task; each workflow of nodes takes your data from the beginning to the end of the designed journey. A workflow substitutes a script; a node substitutes one or more script lines.</p>
			<p>Without extensive coverage when it comes to commonly used data wrangling techniques, machine learning algorithms, and data types and formats, and without integration with most common database software, data sources, reporting tools, external scripts, and programming languages, the software's ease of use would be limited. For this reason, KNIME Analytics Platform has been designed to be open to different data formats, data types, data sources, and data platforms, as well as external tools such as Python and R.</p>
			<p>We'll start by looking at a few ML algorithms. KNIME Analytics Platform<a id="_idIndexMarker014"/> covers most machine learning algorithms: from decision trees to random forest and gradient boosted trees, from recommendation engines to a number of clustering techniques, from Naïve Bayes to linear and logistic regression, from neural networks to deep learning. Most of these algorithms are native to KNIME Analytics Platform, though some can be integrated from other open source tools such as Python and R.</p>
			<p>To train different deep learning architectures, such as RNNs, autoencoders, and CNNs, KNIME Analytics Platform has integrated the <strong class="bold">Keras</strong> deep <a id="_idIndexMarker015"/>learning library through the <strong class="bold">KNIME Deep Learning - Keras Integration</strong> extension (<a href="https://www.knime.com/deeplearning/keras">https://www.knime.com/deeplearning/keras</a>). Through this extension, it is possible to drag and drop nodes to define complex neural architectures and train the final network without necessarily writing any code.</p>
			<p>However, defining the network is just one of the many steps that must be taken. Ensuring the data is in the right form to train the network is another crucial step. For this, a very large number of nodes are available so that we can implement a myriad of <strong class="bold">Data Wrangling</strong> techniques. By combining nodes dedicated to small tasks, you can implement very complex data transformation operations.</p>
			<p>KNIME Analytics Platform also connects to most of the required data sources: from databases to cloud repositories, from big data platforms to files.</p>
			<p>But what if all of this is not enough? What if you<a id="_idIndexMarker016"/> need a specific procedure for a specific domain? What if you need a specific network manipulation function from Python? Where KNIME Analytics Platform and its extensions cannot reach, you can integrate with other scripting and programming languages, such as <strong class="bold">Python</strong>, <strong class="bold">R</strong>, <strong class="bold">Java</strong>, and <strong class="bold">Javascript</strong>, just to mention a few. In addition, KNIME Analytics Platform has seamless integration with BIRT, a business intelligence and reporting tool. Integrations with other reporting platforms such as Tableau, QlickView, PowerBI, and Spotfire are also available.</p>
			<p>Several JavaScript-based nodes are dedicated to implementing data visualization plots and charts: from a simple scatter plot to a more complex sunburst chart, from a simple histogram to a parallel coordinate plot, and more. These nodes seem simple but are potentially quite powerful. If you combine them within a <strong class="bold">component</strong>, you can interactively select data points across multiple charts. By doing this, the component inherits and combines all the views from the contained nodes and connects them in a way that, if the points are selected and visualized in one chart, they can also be selected and visualized in the other charts of the component's composite view.</p>
			<p><em class="italic">Figure 1.1</em> shows an example of a co<a id="_idTextAnchor020"/>mposite view:</p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B16391_01_001.jpg" alt="Figure 1.1 – Composite view of a component containing a scatter plot, a bar chart, and a parallel coordinate plot"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1 – Composite view of a component containing a scatter plot, a bar chart, and a parallel coordinate plot</p>
			<p><em class="italic">Figure 1.1</em> shows the<a id="_idIndexMarker017"/> composite view of a component containing a scatter plot, a bar chart, and a parallel coordinate plot. The three plots visualize the same data and are connected in a way that, by selecting data in the bar chart, it selects and optionally visualizes the data that's been selected in the other two charts.</p>
			<p>When it comes to creating a data science solution, KNIME Analytics Platform provides everything you need. However, KNIME Server offers a few additional features to ease your job when it comes to moving the solution to production.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor021"/>KNIME Server for the Enterprise</h2>
			<p>The last step in any data science <a id="_idIndexMarker018"/>cycle is to deploy the solution to production – and in the case of an enterprise, providing an easy, comfortable, and secure deployment. </p>
			<p>This process of moving the application into the real world is called <em class="italic">moving into production</em>. The process of including the trained model in this<a id="_idIndexMarker019"/> final application is called <strong class="bold">deployment</strong>. Both phases are deeply connected and can be quite problematic since all the errors that occurred in the application design show up at this stage.</p>
			<p>It is possible, though limited, to move an application into production using KNIME Analytics Platform. If you, as a lone data scientist or a data science student, do not regularly deploy applications and models, KNIME Analytics Platform is probably enough for your needs. However, if you are just a bit more involved in an enterprise environment, where scheduling, versioning, access rights, disaster recovery, web applications and REST services, and all the other typical functions of a production server are needed, then just using KNIME Analytics Platform for production can be cumbersome.</p>
			<p>In this case, <strong class="bold">KNIME Server</strong>, which comes with an annual license fee, can make your life easier. First of all, it is going to fit the governance of the enterprise's IT environment better. It also offers a protected collaboration environment for your group and the entire data science lab. And of course, its main advantage consists of making model deployment and moving it into production easier and safer since it uses the <em class="italic">integrated deployment</em> feature and allows you to use <em class="italic">one-click deployment</em> into production. End users can then run the application from a KNIME Analytics Platform client or – even better – from a web browser.</p>
			<p>Remember those composite views that offer interactive interconnected views of selected points? These become fully formed web pages when the application is executed on a web browser via <strong class="bold">KNIME Server's WebPortal</strong>.</p>
			<p>Using the components as <a id="_idIndexMarker020"/>touchpoints within the workflow, we get<a href="https://www.knime.com/blog/principles-of-guided-analytics"> a <strong class="bold">Guided Analytics</strong> (</a><a href="https://www.knime.com/blog/principles-of-guided-analytics"/>) application within the web browser. Guided analytics inserts touchpoints to be consumed by the end user from a web browser within the flow of the application. The end user can take advantage of these touchpoints to insert knowledge or preferences and to steer the analysis in the desired direction.</p>
			<p>Now, let's download KNIME Analytics Platform and give it a try!</p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor022"/>Exploring KNIME Analytics Platform </h1>
			<p>To<a id="_idIndexMarker021"/> install KNIME Analytics Platform, foll<a href="http://www.knime.com/downloads">ow these steps:</a></p>
			<ol>
				<li><a href="http://www.knime.com/downloads">Go to </a><a href="http://www.knime.com/downloads"/>.</li>
				<li>Provide some details about yourself (step <strong class="bold">1</strong> in <em class="italic">Figure 1.2</em>).</li>
				<li>Download the version that's suitable for your operating system (step <strong class="bold">2</strong> in <em class="italic">Figure 1.2</em>).</li>
				<li>While you're waiting for the appropriate version to download, browse through the different steps to get started (step <strong class="bold">3</strong><a id="_idTextAnchor023"/> in <em class="italic">Figure 1.2</em>): </li>
			</ol>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B16391_01_002.jpg" alt="Figure 1.2 – Steps for downloading the KNIME Analytics Platform package"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2 – Steps for downloading the KNIME Analytics Platform package</p>
			<p>Once you've downloaded the package, locate it, start it, and follow the instructions that appear onscreen to install it in any directory that you have write permissions for.</p>
			<p>Once it's been installed, locate your instance of KNIME Analytics Platform – from the appropriate folder, desktop link, application, or link in the start menu – and start it.</p>
			<p>When the splash screen appears, a window will ask for the location of your workspace (<em class="italic">Figure 1.3</em>). This workspace is a folder on your machine that will host all your work. The default workspace folder <a id="_idIndexMarker022"/>is called<a id="_idTextAnchor024"/> <strong class="source-inline">knime-workspace</strong>:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B16391_01_003.jpg" alt="Figure 1.3 – The KNIME Analytics Platform Launcher window asking for the workspace folder"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3 – The KNIME Analytics Platform Launcher window asking for the workspace folder</p>
			<p>After<a id="_idIndexMarker023"/> clicking <strong class="bold">Launch</strong>, the workbench for <strong class="bold">KNIME Analytics Platform</strong> will open.</p>
			<p>The workbench of KNIME Analytics Platform is organized as depicte<a id="_idTextAnchor025"/>d in <em class="italic">Figure 1.4</em>: </p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B16391_01_004.jpg" alt="Figure 1.4 – The KNIME Analytics Platform workbench"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.4 – The KNIME Analytics Platform workbench</p>
			<p>The KNIME workbench consists of different panels that can be resized, removed by clicking the <strong class="bold">X</strong> on their tab, or reinserted via the <strong class="screen-inline">View</strong> menu. Let's take a look at these panels:</p>
			<ul>
				<li><strong class="bold">KNIME Explorer</strong>: The <strong class="bold">KNIME Explorer</strong> panel in the upper-left corner displays all the workflows<a id="_idIndexMarker024"/> in the selected (<strong class="bold">LOCAL</strong>) workspace, possible connections to mounted KNIME servers, a connection to the <strong class="bold">EXAMPLES</strong> server, and a connection to the <strong class="bold">My-KNIME-Hub</strong> space.<p>The <strong class="bold">LOCAL</strong> workspace displays all workflows, saved in the workspace folder that were selected when KNIME Analytics Platform was started. The very first time the platform is opened, the LOCAL workspace only contains workflows and data in the <em class="italic">Example Workflows</em> folder. These are example applications to be used as starting points for your projects.</p><p>The <strong class="bold">EXAMPLES</strong> server is a read-only KNIME hosted server that contains many more example workflows, organized into categories. Just double-click it to be automatically logged in with read-only mode. Once you've done this, you can browse, open, explore, and <a id="_idIndexMarker025"/>download all available example workflows. Once you have located a workflow, double-click it to explore it or drag and drop it into <strong class="bold">LOCAL</strong> to create a local editable copy.</p><p><strong class="bold">My-KNIME-Hub</strong> provides access to the KNIME community shared repository (<strong class="bold">KNIME Hub</strong>), either in public or private mode. You can use <strong class="bold">My-KNIME-Hub/Public</strong> to share your work with the KNIME community or <strong class="bold">My-KNIME-Hub/Private</strong> as a space for your current work.</p></li>
				<li><strong class="bold">Workflow Coach</strong>: <strong class="bold">Workflow Coach</strong> is <a id="_idIndexMarker026"/>a node recommendation engine that aids you when you're building workflows. Based on worldwide user statistics or your own private statistics, it will give you suggestions on which nodes you should use to complete your workflow.</li>
				<li><strong class="bold">Node Repository</strong>: The <strong class="bold">Node Repository</strong> contains all the KNIME nodes you have currently installed, organized <a id="_idIndexMarker027"/>into categories. To help you with orientation, a search box is located at the top of the <strong class="bold">Node Repository</strong> panel. The magnifier lens on its left switches between the exact match and the fuzzy search option.</li>
				<li><strong class="bold">Workflow Editor</strong>: The <strong class="bold">Workflow Editor</strong> is<a id="_idIndexMarker028"/> the canvas at the center of the page and is where you assemble workflows, configure and execute nodes, inspect results, and explore data. Nodes are added from the <strong class="bold">Node Repository</strong> panel to the workflow editor by drag and drop or double-click. Upon starting KNIME Analytics Platform, the Workflow Editor will open on the <strong class="bold">Welcome Page</strong> panel, which includes a number of useful tips on where to find help, courses, events, and the latest news about the software.</li>
				<li><strong class="bold">Outline</strong>: The <strong class="bold">Outline</strong> view displays<a id="_idIndexMarker029"/> the entire workflow, even if only a small part is visible in the workflow editor. This part is marked in gray in the <strong class="bold">Outline</strong> view. Moving the gray rectangle in the <strong class="bold">Outline</strong> view changes the portion of the workflow that's visible in the Workflow Editor.</li>
				<li><strong class="bold">Console</strong> and <strong class="bold">Node Monitor</strong>: The <strong class="bold">Console</strong> and the <strong class="bold">Node Monitor</strong> share one panel with two tabs. The <strong class="bold">Console</strong> tab<a id="_idIndexMarker030"/> prints out possible error and warning messages. The same information is written to a log file, located in the workspace directory. The <strong class="bold">Node Monitor</strong> tab shows <a id="_idIndexMarker031"/>you the data that's available at the output ports of the selected executed node in the Workflow Editor. If a node has multiple output ports, you can select the data of interest from a dropdown menu. By default, the data at the top output port is sh<a href="http://www.hub.knime.com">own.</a></li>
				<li><a href="http://www.hub.knime.com"><strong class="bold">KNIME Hub</strong>: The <strong class="bold">KNI</strong></a><strong class="bold">ME Hub</strong> (<a href="https://hub.knime.com/">https://hub.knime.com/</a>) is an external space where KNIME users <a id="_idIndexMarker032"/>can share their work. This panel <a id="_idIndexMarker033"/>allows you to search for workflows, nodes, and components shared by members of the KNIME community.</li>
				<li><strong class="bold">Description</strong>: The <strong class="bold">Description</strong> panel displays information about the selected node or category. In <a id="_idIndexMarker034"/>particular, for nodes, it explains the node's task, the algorithm behind it (if any), the dialog options, the available views, the expected input data, and the resulting output data. For categories, it displays all contained nodes.</li>
			</ul>
			<p>Finally, at the very top, you can find the <strong class="bold">Top Menu</strong>, which includes menus for file management and preference settings, workflow editing options, additional views, node commands, and help documentation.</p>
			<p>Besides the core software, KNIME Analytics Platform benefits from external <strong class="bold">extensions </strong>provided by the KNIME community. The <strong class="bold">install KNIME extensions</strong> and <strong class="bold">update KNIME</strong> commands, available in the <strong class="bold">File</strong> menu, allow you to expand your current instance with external extensions or update it to a newer version.</p>
			<p>Under the top menu, a <strong class="bold">toolbar</strong> is available. When a workflow is open, the toolbar offers commands for workflow editing, node execution, and customization.</p>
			<p>A <strong class="bold">workflow</strong> can be <a id="_idIndexMarker035"/>built by dragging and dropping nodes from the <strong class="bold">Node Repository</strong> panel onto the <strong class="bold">Workflow Editor</strong> window or by just double-clicking them. Nodes are the basic processing units of any workflow. Each <strong class="bold">node</strong> has several input and/or output ports. <strong class="bold">Data</strong> flows over a connection from an <strong class="bold">output port</strong> to the <strong class="bold">input port</strong>(s) of other nodes. Two nodes are connected – and the data flow is established – by clicking the mouse at the output port of the first node and releasing the mouse at the input port of the next node. A pipeline of such nodes makes a workflow.</p>
			<p>In <em class="italic">Figure 1.5</em>, under each node, you will see a <strong class="bold">status <a id="_idTextAnchor026"/>light</strong>: red, yellow, or green:</p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B16391_01_005.jpg" alt="Figure 1.5 – Node structure and status lights"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.5 – Node structure and status lights</p>
			<p>When a new node is created, the status light is usually red, which means that the node's settings still need to be configured for the node to be able to execute its task.</p>
			<p>To configure a node, right-click it and select <strong class="bold">Configure</strong> or just double-click it. Then, adjust the necessary settings in the node's dialog. When the dialog is closed by pressing the <strong class="bold">OK</strong> button, the node is configured, and the status light changes to yellow; this means that the node is ready to be executed. Right-clicking on the node again shows an enabled <strong class="bold">Execute</strong> option; pressing it will execute the node.</p>
			<p>The ports on the left are input ports, where the data from the outport of the predecessor node is fed into the node. Ports on the right are outgoing ports. The result of the node's operation on the data is provided by the output port of the successor nodes. When you hover over the port, a tooltip will provide information about the output dimension of the node.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Only ports of the same type can be connected!</p>
			<p><strong class="bold">Data ports</strong> (black triangles) are the <a id="_idIndexMarker036"/>most common type of node ports and transfer flat data tables from node to node. <strong class="bold">Database ports</strong> (brown squares) transfer SQL queries from node to node. Many more <a id="_idIndexMarker037"/>node ports exist and transfer different objects from one node to the next.</p>
			<p>After successful execution, the status light of the node turns green, indicating that the processed data is now available on the outports. The result(s) can be inspected by exploring the outport view(s): the last entries in the context menu open them.</p>
			<p>With that, we have completed our quick tour of the workbench in KNIME Analytics Platform.</p>
			<p>Now, let's take a look at where we can find starting examples and help.</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor027"/>Useful Links and Materials</h2>
			<p>At this point, we have already looked at the <strong class="bold">KNIME Hub</strong> already. The KNIME Hub<a id="_idIndexMarker038"/> (<a href="https://hub.knime.com/">https://hub.knime.com/</a>) is a very useful public repository for applications, extensions, examples, and tutorials provided by the KNIME community. Here, you can share your workflows and download workflows that have been created by other KNIME users. Just type in some keywords and you will get a list of related workflows, components, extensions, and more. For example, just type in <strong class="source-inline">read file</strong> and you will get a list of example workflows illustrating how to read <strong class="source-inline">CSV</strong> files, <strong class="source-inline">.table</strong> files, <strong class="source-inline">Excel</strong> f<a id="_idTextAnchor028"/>iles, and so on. (<em class="italic">Figure 1.6</em>):</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B16391_01_006.jpg" alt="Figure 1.6 – Resulting list of workflows from searching for &quot;read file&quot; on the KNIME Hub"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.6 – Resulting list of workflows from searching for "read file" on the KNIME Hub</p>
			<p>All workflows described in this book are also available on the KNIME Hub for you: <a href="https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/">https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/</a>.</p>
			<p>Once you've isolated the workflow you are interested in, click on it to open its page, and then download it or open it in KNIME Analytics Platform to customize it to your own needs.</p>
			<p>On the other hand, to share your work on the KNIME Hub, just copy your workflows from your local workspace into the <em class="italic">My-KNIME-Hub/Public</em> folder in the <strong class="bold">KNIME Explorer</strong> panel within the KNIME workbench. It will be automatically available to all members of the KNIME community.</p>
			<p>The KNIME community is also very active, with tips and tricks available<a id="_idIndexMarker039"/> on the <strong class="bold">KNIME Forum</strong> (<a href="https://forum.knime.com/">https://forum.knime.com/</a>). Here, you can ask questions or search for answers.</p>
			<p>Finally, contributions from the community are available as<a id="_idIndexMarker040"/> posts on the <strong class="bold">KNIME Blog</strong> (<a href="https://www.knime.com/blog">https://www.knime.com/blog</a>), as books<a id="_idIndexMarker041"/> via <strong class="bold">KNIME Press</strong> (<a href="https://www.knime.com/knimepress">https://www.knime.com/knimepress</a><a href="https://www.youtube.com/user/KNIMETV"/><strong class="bold">E TV</strong> (<a href="https://www.youtube.com/user/KNIMETV"/>) channel<a id="_idIndexMarker042"/> on YouTube.</p>
			<p>The two books <em class="italic">KNIME Beginner's Luck</em> and <em class="italic">KNIME Advanced Luck</em> provide tutorials for those users who are starting out in data science with KNIME Analytics Platform.</p>
			<p>Now, let's build our first workflow, shall we?</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor029"/>Build and Execute Your First Workflow</h2>
			<p>In this section, we'll<a id="_idIndexMarker043"/> build our first, simple, small workflow. We'll start with something basic: reading <a id="_idIndexMarker044"/>data from an ASCII file, performing some filtering, and displaying the results in a bar chart.</p>
			<p>In KNIME Explorer, do the following:</p>
			<ol>
				<li value="1">Create a new empty folder by doing the following:<p>a) Right-click <strong class="bold">LOCAL</strong> (or anywhere you want your folder to be).</p><p>b) Select <strong class="bold">New Workflow Group</strong>... (as shown in <em class="italic">Figure 1.7</em>), and, in the window that opens, name it <strong class="source-inline">Chapter 1</strong>.</p></li>
				<li>Click <strong class="bold">Finish</strong>. You should then see a new folder with that name in the <strong class="bold">KNIME Explorer</strong> panel.<p class="callout-heading">Important note</p><p class="callout">Folders in KNIME Explorer are called <strong class="bold">Workflow Groups</strong>.</p></li>
			</ol>
			<p>Similarly, you can create a new workflow, as follows:</p>
			<ol>
				<li value="1">Create a new workflow by doing the following:<p>a) Right-click the <strong class="source-inline">Chapter 1</strong> folder (or anywhere you want your workflow to be).</p><p>b) Select <strong class="bold">New KNIME Workflow</strong> (as shown in <em class="italic">Figure 1.7</em>) and, in the window that opens, name it <strong class="source-inline">My_first_workflow</strong>.</p></li>
				<li>Click <strong class="bold">Finish</strong>. You should then see a new workflow with that name in the <strong class="bold">KNIME Explorer</strong> panel.</li>
			</ol>
			<p>After clicking <strong class="bold">Finish</strong>, the Workflow <a id="_idIndexMarker045"/>Editor will open the canvas for the empty <a id="_idIndexMarker046"/>workflow.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">By default, the canvas for a new workflow opens with the grid on; to turn it off, click the <strong class="bold">Open the settings dialog for the workflow editor</strong> button (the button before the last one) in the toolbar. This button opens a window where you can customize the workflow's appearance (for example, allowing curved connections) and perform editing (turn the grid on/off).</p>
			<p><em class="italic">Figure 1.7</em> shows the <strong class="bold">New Workflow Group</strong>... option in the KNIME Explorer's context menu. It allo<a id="_idTextAnchor030"/>ws you to create a new, empty folder:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B16391_01_007.jpg" alt="Figure 1.7 – Context menu for creating a new folder and a new workflow in KNIME Explorer"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.7 – Context menu for creating a new folder and a new workflow in KNIME Explorer</p>
			<p>The first thing we need to <a id="_idIndexMarker047"/>do in our workflow is read an ASCII file with the data. Let's<a id="_idIndexMarker048"/> read the <em class="italic">adult.csv</em> file that comes with the installation of KNIME Analytics Platform. This can be found under <strong class="bold">Example Workflows/The Data/Basics</strong>. adult.csv is a US Census public file that describes 30K people by age, gender, origin, and professional and private life.</p>
			<p>Let's <strong class="bold">create</strong> the node so that we can read the <em class="italic">adult.csv</em> ASCII file:</p>
			<p>a) In the <strong class="screen-inline">Node Repository</strong>, search for the <strong class="bold">File Reader</strong> node (it is actually located in the <strong class="bold">IO/Read</strong> category).</p>
			<p>b) Drag and drop the <strong class="screen-inline">File Reader</strong> node onto the <strong class="bold">Workflow Editor</strong> panel.</p>
			<p>c) Alternatively, just double-click the <strong class="screen-inline">File Reader</strong> node in the <strong class="screen-inline">Node Repository</strong>; this will automatically create it in the <strong class="bold">Workflow Editor</strong> panel.</p>
			<p>In <em class="italic">Figure 1.8</em>, see the <strong class="screen-inline">File Reade<a id="_idTextAnchor031"/>r</strong> node located in the <strong class="screen-inline">Node Repository</strong>:</p>
			<div>
				<div id="_idContainer017" class="IMG---Figure">
					<img src="image/B16391_01_008.jpg" alt="Figure 1.8 – The File Reader node under IO/Read in the Node Repository"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.8 – The File Reader node under IO/Read in the Node Repository</p>
			<p>Now, let's <strong class="bold">configure</strong> the node <a id="_idIndexMarker049"/>so that it reads the <em class="italic">adult.csv</em> file. Double-click the newly created<a id="_idIndexMarker050"/> <strong class="screen-inline">File Reader</strong> node in the Workflow Editor and manually configure it with the file path to the <em class="italic">adult.csv</em> file. Alternatively, just drag and drop the <em class="italic">adult.csv</em> file from the <strong class="bold">KNIME Explorer</strong> panel (or from anywhere on your machine) onto the <strong class="bold">Workflow Editor</strong> window. <a id="_idTextAnchor032"/>You can see this action in <em class="italic">Figure 1.9</em>: </p>
			<div>
				<div id="_idContainer018" class="IMG---Figure">
					<img src="image/B16391_01_009.jpg" alt="Figure 1.9 – Dragging and dropping the adult.csv file onto the Workflow Editor panel."/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.9 – Dragging and dropping the adult.csv file onto the Workflow Editor panel.</p>
			<p>This<a id="_idIndexMarker051"/> automatically generates a File Reader node that contains most of the correct<a id="_idIndexMarker052"/> configuration settings for reading the file. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The <strong class="bold">Advanced</strong> button in the File Reader configuration window leads you to additional advanced settings: reading files with special characters, such as quotes; allowing lines with different lengths; using different encodings; and so on.</p>
			<p>To execute this node, just right-click it and from the context menu, select <strong class="bold">Execute</strong>; alternatively, click on the <strong class="bold">Execute</strong> buttons (single and double white arrows on a green background) that are available in the toolbar.</p>
			<p>To inspect the output data table that's produced by this node's execution, right-click on the node and select the last option available in the context menu. This opens the data table that appears as a result of reading the <em class="italic">adult.csv</em> file. You will notice columns such as <strong class="bold">Age</strong>, <strong class="bold">Workclass</strong>, and so on.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Data in KNIME Analytics Platform is organized into tables. Each cell is uniquely identified via the <strong class="bold">column header</strong> and the <strong class="bold">row ID</strong>. Therefore, column headers and row IDs need to have unique values.</p>
			<p><strong class="screen-inline">fnlwgt</strong> is one column for which we were <a id="_idIndexMarker053"/>never sure of what it meant. So, let's remove it from further analysis by using the <strong class="bold">Column Filter</strong> node.</p>
			<p>To do this, search<a id="_idIndexMarker054"/> for <strong class="bold">Column Filter</strong> in the search box above the Node Repository, then drag and drop it onto the Workflow Editor and connect the output of the <strong class="screen-inline">File Reader</strong> node to the input of the <strong class="screen-inline">Column Filter</strong> node. Alternatively, we can select the <strong class="screen-inline">File Reader</strong> node in the <strong class="bold">Workflow Editor</strong> panel and then double-click the <strong class="screen-inline">Column Filter</strong> node in the Node Repository. This automatically creates a node and its connections in the Workflow Editor. </p>
			<p>The <strong class="screen-inline">Column Filter</strong> node and its con<a id="_idTextAnchor033"/>figuration window are shown in <em class="italic">Figure 1.10</em>:</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B16391_01_010.jpg" alt="Figure 1.10 – Configuring the Column Filter node to remove the column named fnlwgt from the input data table"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.10 – Configuring the Column Filter node to remove the column named fnlwgt from the input data table</p>
			<p>Again, double-click or right-click the node and then select <strong class="bold">Configure</strong> to configure it. This configuration window contains three options that can be selected via three radio buttons: <strong class="bold">Manual Selection</strong>, <strong class="bold">Wildcard/Regex Selection</strong>, and <strong class="bold">Type Selection</strong>. Let's take a look at these in more detail:</p>
			<ul>
				<li><strong class="bold">Manual Selection</strong> offers an Include/Exclude framework so that you can manually transfer columns from the <strong class="bold">Include</strong> set into the <strong class="bold">Exclude</strong> set and vice versa.</li>
				<li><strong class="bold">Wildcard/Regex Selection</strong> extracts the columns you wish to keep, based on a wildcard (using <strong class="bold">*</strong> as the wildcard) or regex expression.</li>
				<li><strong class="bold">Type Selection</strong> keeps the columns based on the data types they carry.</li>
			</ul>
			<p>Since this is our first <a id="_idIndexMarker055"/>workflow, we'll go for the easiest approach; that is, Manual Selection. Go to the <strong class="bold">Manual Selection</strong> tab and transfer the <strong class="screen-inline">fnlwgt</strong> column to the <strong class="bold">Exclude</strong> set via<a id="_idIndexMarker056"/> the buttons in-between the two frames (these can be seen in <em class="italic">Figure 1.10</em>).</p>
			<p>After executing the Column Filter node, if we inspect the output data table (right-click and select the last option in the context menu), we'll see a table that doesn't contain the <strong class="screen-inline">fnlwgt</strong> column.</p>
			<p>Now, let's extract all the records of people who work more than 20 hours/week. <strong class="screen-inline">hours-per-week</strong> is the column that contains the data of interest.</p>
			<p>For this, we need to create a Row Filter node and implement the required condition:</p>
			<p class="source-code">IF hours-per-week  &gt; 20     THEN Keep data row.</p>
			<p>Again, let's locate the <strong class="bold">Row Filter</strong> node in the Node Repository panel, drag and drop it (or double-click it) into the Workflow Editor, connect the output port of the <strong class="screen-inline">Column Filter</strong> node to its input port, and open its configuration window.</p>
			<p>In the configuration window of the <strong class="screen-inline">Row Filter</strong> node (<em class="italic">Figure 1.11</em>), we'll find three default filtering criteria: <strong class="bold">use pattern matching</strong>, <strong class="bold">use range checking</strong>, and <strong class="bold">only missing values match</strong>. Let's take a look at what they do:</p>
			<ul>
				<li><strong class="bold">use pattern matching</strong> matches the given pattern to the content of the selected column in the <strong class="bold">Column to test</strong> field and keeps the matching rows.</li>
				<li><strong class="bold">use range checking</strong> keeps only those data rows whose value in the <strong class="bold">Column to test</strong> columns falls between the <strong class="bold">lower bound</strong> and <strong class="bold">upper bound</strong> values.</li>
				<li><strong class="bold">only missing values match</strong> only keeps the data rows where a missing value is present in the selected column.</li>
			</ul>
			<p>The default<a id="_idIndexMarker057"/> behavior is to include the matching data rows in the output data<a id="_idIndexMarker058"/> table. However, this can be changed by enabling <strong class="bold">Exclude rows by attribute value</strong> via the radio buttons on the left-hand side of the configuration window.</p>
			<p>Alternative filtering criteria can be done by row number or by row ID. This can also be enabled via the radio buttons on th<a id="_idTextAnchor034"/>e left-hand side of the configuration window:</p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B16391_01_011.jpg" alt="Figure 1.11 – Configuring the Row Filter node to keep only rows with hours-per-week &gt; 20 in the input data table"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.11 – Configuring the Row Filter node to keep only rows with hours-per-week &gt; 20 in the input data table</p>
			<p>After execution, upon opening the output data table (<em class="italic">Figure 1.12</em>), no data rows<a id="_idTextAnchor035"/> with <em class="italic">hours-per-week &lt; 20 </em>should be present:</p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B16391_01_012.jpg" alt="Figure 1.12 – Right-clicking a successfully executed node and selecting the last option shows the data table that was produced by the node"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.12 – Right-clicking a successfully executed node and selecting the last option shows the data table that was produced by the node</p>
			<p>Now, let's look at some very<a id="_idIndexMarker059"/> basic visualization. Let's visualize the number of men versus women <a id="_idIndexMarker060"/>in this dataset, which conta<a id="_idTextAnchor036"/>ins people who work more than 20 hours/week: </p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B16391_01_013.jpg" alt="Figure 1.13 – The Bar Chart node and its configuration window"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.13 – The Bar Chart node and its configuration window</p>
			<p>To do this, locate the <strong class="bold">Bar Chart</strong> node in the Node Repository, create an instance in the workflow, connect it to receive input from the <strong class="screen-inline">Row Filter</strong> node, and open its configuration<a id="_idIndexMarker061"/> window (<em class="italic">Figure 1.13</em>). Here, there are four tabs we can use<a id="_idIndexMarker062"/> for configuration purposes. <strong class="bold">Options</strong> covers all data settings, <strong class="bold">General Plot Options</strong> covers all plot settings, <strong class="bold">Control Options</strong> covers all control options, and <strong class="bold">Interactivity</strong> covers all subscription events when it comes to interacting with other plots, views, and charts when they've been assembled to create a component. Again, since this is just a beginner's workflow, we'll adopt all the default settings and just set the following:</p>
			<ul>
				<li>From the <strong class="bold">Options</strong> tab, set <strong class="bold">Category Column</strong> to <strong class="source-inline">sex</strong>, ensuring it appears on the <em class="italic">x</em> axis. Then, select <strong class="bold">Occurrence Count</strong> in order to count the number of rows by sex.</li>
				<li>From the <strong class="bold">General Plot Options</strong> tab, set a title, a subtitle, and the axis labels.</li>
			</ul>
			<p>This node does not produce data, but rather a view of the bar chart. So, to inspect the results produced by this node after its execution, right-click it and select the central option; that is, <strong class="bold">In<a id="_idTextAnchor037"/>teractive View: Group Bar Chart</strong> (<em class="italic">Figure 1.14</em>):</p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B16391_01_014.jpg" alt="Figure 1.14 – Right-clicking a successfully executed visualization node and selecting the Interactive View: Grouped Bar Chart option to see the chart/plot that has been produced"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.14 – Right-clicking a successfully executed visualization node and selecting the Interactive View: Grouped Bar Chart option to see the chart/plot that has been produced</p>
			<p>Notice the three <a id="_idIndexMarker063"/>buttons in the top-right corner of the view on the right of <em class="italic">Figure 1.14</em>. These<a id="_idIndexMarker064"/> three buttons enable <strong class="bold">zooming</strong>, <strong class="bold">toggling to full screen</strong>, and <strong class="bold">node settings</strong>, respectively. From the view itself, you can explore how the chart would look if different settings were to be selected, such as a different category column or a different title.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Most data visualization nodes produce a view and not a data table. To see the respective view, right-click the successfully executed node and select the <strong class="bold">Interactive View: …</strong> option.</p>
			<p>The second lower input port of the <strong class="bold">Bar Chart</strong> node is optional (a white triangle) and is used to read a color map so that you can color the bars in the bar chart.</p>
			<p class="callout-heading">Important note</p>
			<p class="callout">Note that a number of different data visualization nodes are available in the Node Repository: JavaScript, Local(Swing), Plotly, and so on. <strong class="bold">JavaScript</strong> and <strong class="bold">Plotly</strong> nodes offer the highest level of interactivity and the most polished graphics. We used the <strong class="screen-inline">Bar Chart</strong> node from the JavaScript category in the <strong class="bold">Node Repository</strong> panel here.</p>
			<p>Now, we'll add a few<a id="_idIndexMarker065"/> comments to document the workflow. You can add comments at the node level or at the general workflow level. </p>
			<p>Each node in the <a id="_idIndexMarker066"/>workflow is created with a default label of <em class="italic">Node xx</em> under it. Upon double-clicking it, the node label editor appears. This allows you to customize the text, the font, the color, the background, and other similar properties of the node (<em class="italic">Figure 1.15</em>). We need to write a little comment under each node t<a id="_idTextAnchor038"/>o make it clear what tasks they are implementing:</p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B16391_01_015.jpg" alt="Figure 1.15 – Editor for customizing the labels under each node"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.15 – Editor for customizing the labels under each node</p>
			<p>You can also write annotations at the workflow level. Just right-click anywhere in the Workflow Editor and select <strong class="bold">New Workflow Annotation</strong>. A yellow frame will appear in editing mode. Here, you can add text and customize it, as well as its frame. To close the annotation editor, just click anywhere else in the Workflow Editor. To reopen the annotation editor, double-click in the <a id="_idTextAnchor039"/>top-left corner of the annotation (<em class="italic">Figure 1.16</em>):</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/B16391_01_016.jpg" alt="Figure 1.16 – Creating and editing workflow annotations"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.16 – Creating and editing workflow annotations</p>
			<p>Congratulations! You have just<a id="_idIndexMarker067"/> built your first workflow with KNIME Analytics Platform. It <a id="_idIndexMarker068"/>s<a id="_idTextAnchor040"/>hould look something like the one in <em class="italic">Figure 1.17</em>:</p>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B16391_01_017.jpg" alt="Figure 1.17 – My_first_Workflow"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.17 – My_first_Workflow</p>
			<p>That was a quick introduction to how to use KNIME Analytics Platform. </p>
			<p>Now, let's make sure we have KNIME Deep Learning – Keras Integration installed and functioning.</p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor041"/>Installing KNIME Deep Learning – Keras Integration</h1>
			<p>In this section, you will learn how to install and set up <strong class="bold">KNIME Deep Learning - Keras Integration</strong> in order <a id="_idIndexMarker069"/>to train neural networks in KNIME Analytics Platform.</p>
			<p>KNIME Analytics Platform consists of a software core and several provided extensions and integrations. Such extensions and integrations are provided by the KNIME community and extend the original software core through a variety of data science functionalities, including advanced algorithms for AI.</p>
			<p>The KNIME extension of interest here is called <strong class="bold">KNIME Deep Learning – Keras Integration</strong>. It offers a codeless GUI-based integration of the Keras library, while using TensorFlow as its backend. This means that a number of functions from Keras libraries have been wrapped into KNIME nodes, within KNIME's classic, easy-to-use visual dialog window. Due to this integration, you can read, write, create, train, and execute deep learning networks without writing code.</p>
			<p>Another deep learning integration that's available is called <strong class="bold">KNIME Deep Learning - TensorFlow Integration</strong>. This extension <a id="_idIndexMarker070"/>allows you to convert <strong class="bold">Keras</strong> models into <strong class="bold">TensorFlow</strong> models, as well as read, execute, and write TensorFlow models.</p>
			<p>TensorFlow is an open source library provided by Google that includes a number of deep learning paradigms. TensorFlow functions can run on single devices, as well as on multiple CPUs and multiple GPUs. This parallel calculation feature is the key to speeding up the computationally intensive training that's required for deep learning networks.</p>
			<p>However, using the TensorFlow library within Python can prove quite complicated, even for an expert Python programmer or a deep learning pro. Thus, a number of simplified interfaces have been developed on top of TensorFlow that expose a subset of its functions and parameters. The most successful of such TensorFlow-based libraries is Keras. However, even Keras still requires some programming skills. The KNIME Deep Learning – Keras Integration puts the KNIME GUI on top of the Keras libraries that are available, mostly eliminating the need to code.</p>
			<p>To make the <a id="_idIndexMarker071"/>KNIME Deep Learning – Keras Integration work, a few pieces of the puzzle need to be installed:</p>
			<ul>
				<li>The Keras and TensorFlow nodes</li>
				<li>The Python environment</li>
			</ul>
			<p>Let's start with the first piece: installing the Keras and TensorFlow nodes.</p>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor042"/>Installing the Keras and TensorFlow Nodes</h2>
			<p>To add nodes to the<a id="_idIndexMarker072"/> Node Repository, you must install a few extensions and<a id="_idIndexMarker073"/> integrations.</p>
			<p>You can install them from within KNIME Analytics Platform by clicking on <strong class="bold">File</strong> from the top menu and selecting <strong class="bold">Install KNI<a id="_idTextAnchor043"/>ME Extension…</strong>. This opens the dialog shown in <em class="italic">Figure 1.18</em>:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/B16391_01_018.jpg" alt="Figure 1.18 – Dialog for installing extensions"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.18 – Dialog for installing extensions</p>
			<p>From this new dialog, you can select the extensions and integrations you want to install. Using the search bar at the top is helpful for filtering the available extensions and integrations.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Another way you can install extensions is by dragging and dropping them from the KNIME Hub.</p>
			<p>To install the Keras and<a id="_idIndexMarker074"/> TensorFlow nodes that will be used in the case studies <a id="_idIndexMarker075"/>described in this book, you need to select the following:</p>
			<ul>
				<li><strong class="bold">KNIME Deep Learning – Keras Integration</strong></li>
				<li><strong class="bold">KNIME Deep Learning – TensorFlow Integration</strong></li>
			</ul>
			<p>Then, press the <strong class="bold">Next</strong> button, accept the terms and conditions, and click <strong class="bold">Finish</strong>. Once the installation is done, you need to restart KNIME Analytics Platform.</p>
			<p>At this point, you should have the Keras and T<a id="_idTextAnchor044"/>ensorFlow nodes in your Node Repository (<em class="italic">Figure 1.19</em>):</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/B16391_01_019.jpg" alt="Figure 1.19 – Installed deep learning nodes in the Node Repository"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.19 – Installed deep learning nodes in the Node Repository</p>
			<p>A large number of<a id="_idIndexMarker076"/> nodes implement neural layers: the nodes for input and<a id="_idIndexMarker077"/> dropout layers can be found in the <strong class="bold">Core</strong> sub-category, the nodes for LSTM layers can be found in <strong class="bold">Recurrent</strong>, and the nodes for embedding layers can be found in <strong class="bold">Embedding</strong>. Then, there are the Learner, Reader, and Writer nodes, which can be used to train, load, and store a network, respectively. All these nodes have a configuration window and don't require any coding. The Python deep learning nodes allow you to define, train, execute, and edit networks using Python code. The last subcategory contains TensorFlow-based nodes.</p>
			<p>Next, we need to set up the Python environment.</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor045"/>Setting up the Python Environment</h2>
			<p>The KNIME Keras Integration <a id="_idIndexMarker078"/>and the KNIME TensorFlow Integration depend on an existing <strong class="bold">Python</strong> installation, which requires certain Python dependencies to be installed. </p>
			<p>Similar to the KNIME Python Integration, the KNIME Deep Learning Integration uses <strong class="bold">Anaconda</strong> to manage Python environments. If you have already installed Anaconda for, for example, the KNIME Python Integration, you can skip the first step. </p>
			<p>Let's get started:</p>
			<ol>
				<li value="1">First, get and install the latest Anaconda version (Anaconda ≥ 2019.03, conda ≥ 4.6.2) from <a href="https://www.anaconda.com/products/individual">https://www.anaconda.com/products/individual</a>. On the Anaconda download page, you can choose between Anaconda with Python 3.x or Python 2.x. Either one should work (if you're not sure, we suggest selecting Python 3).</li>
				<li>Next, we need to create an environment with the correct libraries installed. To do so, from within KNIME Analytics Platform, open the Python Deep Learning preferences. From here, do the following:</li>
				<li>First, select <strong class="bold">File -&gt; Preferences</strong> from the top menu. This will open a new dialog with a list on the left. </li>
				<li>From the dialog, select <strong class="bold">KNIME</strong> <strong class="bold">-&gt; Python Deep Lea<a id="_idTextAnchor046"/>rning</strong>.<p>You should now see a dialog like that in <em class="italic">Figure 1.20</em>:</p><div id="_idContainer029" class="IMG---Figure"><img src="image/B16391_01_020.jpg" alt="Figure 1.20 – Python Deep Learning preference page"/></div><p class="figure-caption">Figure 1.20 – Python Deep Learning preference page</p><p>From this page, create some Conda environments with the correct packages installed for Keras or TensorFlow 2. For the case studies in this book, it will be sufficient to set up an environment for Keras.</p></li>
				<li>To create and <a id="_idIndexMarker079"/>set up a new environment, enable <strong class="bold">Use special Deep Learning configuration</strong> and set <strong class="bold">Keras</strong> to <strong class="bold">Library used for DL Python</strong>. Next, enable <strong class="bold">Conda</strong> and provide the path to your Conda installation directory.</li>
				<li>In addition, to create a new environment for Keras, click on the <strong class="bold">New environment…</strong> button in the Keras framework.<p>This opens a new dialog, as in <em class="italic">Fi<a id="_idTextAnchor047"/>gure 1.21</em>, where you can set the new environment's name:</p><div id="_idContainer030" class="IMG---Figure"><img src="image/B16391_01_021.jpg" alt="Figure 1.21 – Dialog for setting the new environment's name"/></div><p class="figure-caption">Figure 1.21 – Dialog for setting the new environment's name</p></li>
				<li>Click on the <strong class="bold">Create new CPU environment</strong> or <strong class="bold">Create new GPU environment</strong> button to create a new environment for using either a CPU or GPU, if available.</li>
			</ol>
			<p>Now, you can get started. In this section, you were introduced to the most convenient way of setting up a Python environment. Other options can be found in the KNIME documentation: <a href="https://docs.knime.com/2019-06/deep_learning_installation_guide/index.html#keras-integration">https://docs.knime.com/2019-06/deep_learning_installation_guide/index.html#keras-integration</a>.</p>
			<h1 id="_idParaDest-28"><a id="_idTextAnchor048"/>Goal and Structure of this Book</h1>
			<p>In this book, our aim is to provide you with a strong theoretical basis about deep learning architectures and training paradigms, as well as some detailed codeless experience of their implementations for solving practical case studies based on real-world data.</p>
			<p>For this journey, we have adopted the codeless tool, KNIME Analytics Platform. KNIME Analytics Platform is based on visual programming and exploits a user-friendly GUI to make data analytics a more affordable task without the barrier of coding. As with many other external extensions, KNIME Analytics Platform has integrated the Keras libraries under this same GUI, thus including deep learning as part of its list of codeless extensions. From within KNIME Analytics Platform, you can build, train, and test a deep learning architecture with just a few drag and drops and a few clicks of the mouse. We provided a little introduction to the tool in this chapter, but we will provide more detailed information about it in <a href="B16391_02_Final_SK_ePUB.xhtml#_idTextAnchor051"><em class="italic">Chapter 2</em></a>, <em class="italic">Data Access and Preprocessing with KNIME Analytics Platform</em>.</p>
			<p>After that, in <a href="B16391_03_Final_PG_ePUB.xhtml#_idTextAnchor073"><em class="italic">Chapter 3</em></a>, <em class="italic">Getting Started with Neural Networks</em>, we will provide a quick overview of the basic concepts behind neural networks and deep learning. This chapter will by no means provide complete coverage of all the architectures and paradigms involved in neural networks and deep learning. Instead, it will provide a quick overview of them to help you familiarize yourself with the concept, either for the first time or again, before you continue implementing them. Please refer to more specialized literature if you want to know more about the mathematical background of deep learning.</p>
			<p>As we stated previously, we decided to talk about deep learning techniques in a very practical way; that is, always with reference to real case studies where a particular deep learning technique had been successfully implemented. We'll start this trend in <a href="B16391_04_Final_NM_ePUB.xhtml#_idTextAnchor101"><em class="italic">Chapter 4</em></a>, <em class="italic">Building and Training a Feedforward Network</em>, where we'll describe a few basic example applications we can use to train and apply the basic concepts surrounding deep learning networks that we explored in <a href="B16391_03_Final_PG_ePUB.xhtml#_idTextAnchor073"><em class="italic">Chapter 3</em></a>, <em class="italic">Getting Started with Neural Networks</em>. Although these are simple toy examples, they are still useful for illustrating how to apply the <a id="_idTextAnchor049"/>theoretical concepts we described in the previous chapter.</p>
			<p>With <a href="B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152"><em class="italic">Chapter 5</em></a>, <em class="italic">Autoencoder for Fraud Detection</em>, we'll start looking at real case studies. The first case study we'll describe in this chapter aims to prevent fraud detection in credit card transactions by firing an alarm every time a suspicious transaction is detected. To implement this subspecies of anomaly detection, we'll use an approach based on the autoencoder architecture, as well as the calculated distance between the output and the input values of the network.</p>
			<p>With <a href="B16391_05_Final_NM_ePUB.xhtml#_idTextAnchor152"><em class="italic">Chapter 5</em></a>, <em class="italic">Autoencoder for Fraud Detection</em>, we are still in the realm of classic neural networks, including feedforward networks and those trained with backpropagation, albeit with an original architecture. In <a href="B16391_06_Final_VK_ePUB.xhtml#_idTextAnchor181"><em class="italic">Chapter 6</em></a>, <em class="italic">Recurrent Neural Networks for Demand Prediction</em>, we'll enter the realm of deep learning network with RNNs – specifically, with LSTMs. Here, the dynamic character of such networks and their capability to capture the time evolution of a signal will be exploited to solve a classic time series analysis problem: demand prediction.</p>
			<p>Upon introducing RNNs, we will learn how to use them for <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>) case studies. <a href="B16391_07_Final_NM_ePUB.xhtml#_idTextAnchor230"><em class="italic">Chapter 7</em></a>, <em class="italic">Implementing NLP Applications</em>, covers a few such NLP use cases: sentiment analysis, free text generation, and product name generation, to name a few. All such use cases are similar in the sense that they analyze streams of text. All of them are also slightly different in that they find a solution to a different problem: classification for sentiment analysis for the former case, and unconstrained generation of sequences of words or characters for the other two use cases. Nevertheless, data preparation techniques and RNN architectures are similar for all case studies, which is why they have been placed into one single chapter.</p>
			<p><a href="B16391_08_Final_SK_ePUB.xhtml#_idTextAnchor290"><em class="italic">Chapter 8</em></a>, <em class="italic">Neural Machine Translation</em>, describes a spin-off case of free text generation with RNNs. Here, a sequence of words will be generated at the output of the network as a response to a corresponding sequence of words in the input layer. The output sequence will be generated in the target language, while the input sequence will be provided in the source language.</p>
			<p>Deep learning does not just come in the form of RNNs and text mining. Actually, the first examples of deep learning networks came from the field of image processing. <a href="B16391_09_Final_NM_ePUB.xhtml#_idTextAnchor316"><em class="italic">Chapter 9</em></a>, <em class="italic">Convolutional Neural Networks for Image Classification</em>, is dedicated to describing a case study where histopathology slide images must be classified as one of three different types of cancer. To do that, we will introduce CNNs. Training networks for image analysis is not a simple task in terms of time, the amount of data, and computational resources. Often, to train a neural network so that it recognizes images, we must rely on the benefits of transfer learning, as described in <a href="B16391_09_Final_NM_ePUB.xhtml#_idTextAnchor316"><em class="italic">Chapter 9</em></a>, <em class="italic">Convolutional Neural Networks for Image Classification</em>, as well.</p>
			<p><a href="B16391_09_Final_NM_ePUB.xhtml#_idTextAnchor316"><em class="italic">Chapter 9</em></a>, <em class="italic">Convolutional Neural Networks for Image Classification</em>, concludes our in-depth look into how deep learning techniques can be implemented for real case studies. We are aware of the fact that other deep learning paradigms have been used to produce solutions for other data science problems. However, here, we decided to only report the common paradigms in which we had real-life experiences.</p>
			<p>After training a network, the deployment phase must take place. Deployment is often conveniently forgotten since this is the phase where all problems are put to the test. This includes errors in the application's design, in training the network, in accessing and preparing the data: all of them will show up here, during deployment. Due to this, the last two chapters of this book are dedicated to the deployment phase of trained deep learning networks.</p>
			<p><a href="B16391_10_Final_VK_ePUB.xhtml#_idTextAnchor367"><em class="italic">Chapter 10</em></a>, <em class="italic">Deploying a Deep Learning Network</em>, will show you how to build a deployment application, while <a href="B16391_11_Final_NM_ePUB.xhtml#_idTextAnchor386"><em class="italic">Chapter 11</em></a>, <em class="italic">Best Practices and Other Deployment Options</em>, will show you all the deployment options that are available (a web application or a REST service). It will also provide you with a few tips and tricks from our own experience.</p>
			<p>Each chapter comes with its own set of questions so that you can test your understanding of the material that's been provided.</p>
			<p>With that, please read on to discover the various deep learning architectures that can be applied to real use cases using KNIME Analytics Platform.</p>
			<h1 id="_idParaDest-29"><a id="_idTextAnchor050"/>Summary</h1>
			<p>This first chapter aimed to prepare you for the content provided in this book.</p>
			<p>Thus, we started this chapter by reminding you of the importance of deep learning, as well as the surge in popularity it garnered following the first deep learning success stories. Such a surge in popularity is probably what brought you here, with the desire to learn more about practical implementations of deep learning networks for real use cases.</p>
			<p>Nowadays, the main barrier that we come across when learning about deep learning is the coding skills that are required. Here, we adopted KNIME software, and in particular the open source KNIME Analytics Platform, so that we can look at the case studies that will be proposed throughout this book. To do this, we described KNIME software and KNIME Analytics Platform in detail.</p>
			<p>KNIME Analytics Platform also benefits from an extension known as KNIME Deep Learning – Keras Integration, which helps with integrating Keras deep learning libraries. It does this by wrapping Python-based libraries into the codeless KNIME GUI. We dedicated a full section to installing it.</p>
			<p>Finally, we concluded this chapter by providing an overview of what the remaining chapters in this book will cover.</p>
			<p>Before we dive into the math and applications of deep learning networks, we will use the next chapter to familiarize ourselves with the basic features of KNIME Analytics Platform.</p>
		</div>
	</body></html>