<html><head></head><body>
<div id="_idContainer513">
<h1 class="chapter-number" id="_idParaDest-93" lang="en-GB"><a id="_idTextAnchor146"/><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-94" lang="en-GB"><a id="_idTextAnchor147"/><span class="koboSpan" id="kobo.2.1">Advanced Computer Vision Applications</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.3.1">In </span><a href="B19627_04.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.4.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.5.1">, we introduced </span><strong class="bold"><span class="koboSpan" id="kobo.6.1">convolutional networks</span></strong><span class="koboSpan" id="kobo.7.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.8.1">CNNs</span></strong><span class="koboSpan" id="kobo.9.1">) for computer vision and some of the most popular and best-performing CNN models. </span><span class="koboSpan" id="kobo.9.2">In this chapter, we’ll continue with more of the same, but at a more advanced level. </span><span class="koboSpan" id="kobo.9.3">Our </span><em class="italic"><span class="koboSpan" id="kobo.10.1">modus operandi</span></em><span class="koboSpan" id="kobo.11.1"> so far has been to provide simple classification examples to support your theoretical knowledge of </span><strong class="bold"><span class="koboSpan" id="kobo.12.1">neural networks</span></strong><span class="koboSpan" id="kobo.13.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.14.1">NNs</span></strong><span class="koboSpan" id="kobo.15.1">). </span><span class="koboSpan" id="kobo.15.2">In the universe of computer vision tasks, classification is fairly straightforward as it assigns a single label to an image. </span><span class="koboSpan" id="kobo.15.3">This also makes it possible to manually create large, labeled training datasets. </span><span class="koboSpan" id="kobo.15.4">In this chapter, we’ll introduce </span><strong class="bold"><span class="koboSpan" id="kobo.16.1">transfer learning</span></strong><span class="koboSpan" id="kobo.17.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.18.1">TL</span></strong><span class="koboSpan" id="kobo.19.1">), a technique that will allow us to transfer the knowledge of pre-trained NNs to a new and unrelated task. </span><span class="koboSpan" id="kobo.19.2">We’ll also see how TL makes it possible to solve two interesting computer vision tasks – object detection and semantic segmentation. </span><span class="koboSpan" id="kobo.19.3">We can say that these tasks are more complex compared to classification because the model has to obtain a more comprehensive understanding of the image. </span><span class="koboSpan" id="kobo.19.4">It has to be able to detect different objects as well as their positions in the image. </span><span class="koboSpan" id="kobo.19.5">At the same time, the task’s complexity allows for more </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">creative solutions.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.21.1">Finally, we’ll introduce a new class of algorithms called generative models, which will help us generate </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">new images.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.23.1">This chapter will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">following topics:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.25.1">Transfer </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.26.1">learning</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.27.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.28.1">TL</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.29.1">)</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.30.1">Object detection</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.31.1">Semantic segmentation</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.32.1">Image generation with </span><span class="No-Break"><span class="koboSpan" id="kobo.33.1">diffusion models</span></span></li>
</ul>
<h1 id="_idParaDest-95" lang="en-GB"><a id="_idTextAnchor148"/><span class="koboSpan" id="kobo.34.1">Technical requirements</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.35.1">We’ll implement the example in this chapter using Python, PyTorch, Keras, and Ultralytics YOLOv8 (</span><a href="https://github.com/ultralytics/ultralytics"><span class="koboSpan" id="kobo.36.1">https://github.com/ultralytics/ultralytics</span></a><span class="koboSpan" id="kobo.37.1">). </span><span class="koboSpan" id="kobo.37.2">If you don’t have an environment set up with these tools, fret not – the example is available as a Jupyter notebook on Google Colab. </span><span class="koboSpan" id="kobo.37.3">You can find the code examples in this book’s GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter05"><span class="No-Break"><span class="koboSpan" id="kobo.39.1">https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter05</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.40.1">.</span></span></p>
<h1 id="_idParaDest-96" lang="en-GB"><a id="_idTextAnchor149"/><span class="koboSpan" id="kobo.41.1">Transfer learning (TL)</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.42.1">So far, we’ve trained small </span><a id="_idIndexMarker614"/><span class="koboSpan" id="kobo.43.1">models on toy datasets, where the training took no more than an hour. </span><span class="koboSpan" id="kobo.43.2">But if we want to work with large datasets, such as ImageNet, we will need a much bigger network that trains for a lot longer. </span><span class="koboSpan" id="kobo.43.3">More importantly, large datasets are not always available for the tasks we’re interested in. </span><span class="koboSpan" id="kobo.43.4">Keep in mind that besides obtaining the images, they have to be labeled, and this could be expensive and time-consuming. </span><span class="koboSpan" id="kobo.43.5">So, what does a humble engineer do when they want to solve a real ML problem with limited resources? </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">Enter TL.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.45.1">TL is the process of applying an existing trained ML model to a new, but related, problem. </span><span class="koboSpan" id="kobo.45.2">For example, we can take a network trained on ImageNet and repurpose it to classify grocery store items. </span><span class="koboSpan" id="kobo.45.3">Alternatively, we could use a driving simulator game to train an NN to drive a simulated car, and then use the network to drive a real car (but don’t try this at home!). </span><span class="koboSpan" id="kobo.45.4">TL is a general ML concept that applies to all ML algorithms – we’ll also use TL in </span><a href="B19627_08.xhtml#_idTextAnchor220"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.46.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.47.1">. </span><span class="koboSpan" id="kobo.47.2">But in this chapter, we’ll talk about TL in CNNs. </span><span class="koboSpan" id="kobo.47.3">Here’s how </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">it works.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.49.1">We start with an existing pre-trained net. </span><span class="koboSpan" id="kobo.49.2">The most common scenario is to take a network pre-trained with ImageNet, but it could be any dataset. </span><span class="koboSpan" id="kobo.49.3">PyTorch, </span><strong class="bold"><span class="koboSpan" id="kobo.50.1">TensorFlow</span></strong><span class="koboSpan" id="kobo.51.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.52.1">TF</span></strong><span class="koboSpan" id="kobo.53.1">), and Keras all have popular ImageNet pre-trained neural architectures that we can use. </span><span class="koboSpan" id="kobo.53.2">Alternatively, we can train our network with a dataset of </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">our choice.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.55.1">In </span><a href="B19627_04.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.56.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.57.1">, we mentioned</span><a id="_idIndexMarker615"/><span class="koboSpan" id="kobo.58.1"> how the </span><strong class="bold"><span class="koboSpan" id="kobo.59.1">fully connected</span></strong><span class="koboSpan" id="kobo.60.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.61.1">FC</span></strong><span class="koboSpan" id="kobo.62.1">) layers at the end of a CNN act as translators between the network’s language (the abstract feature representations learned during training) and our language, which is the class of each sample. </span><span class="koboSpan" id="kobo.62.2">You can think of TL as a translation to another language. </span><span class="koboSpan" id="kobo.62.3">We start with the network’s features, which is the output of the last convolutional or pooling layer. </span><span class="koboSpan" id="kobo.62.4">Then, we translate them to a different set of classes for the new task. </span><span class="koboSpan" id="kobo.62.5">We can do this by removing the last layers of an existing pre-trained network and replacing them with a different set of layers, which represents the classes of the new problem. </span><span class="koboSpan" id="kobo.62.6">Here is a diagram of the </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">TL scenario:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer416">
<span class="koboSpan" id="kobo.64.1"><img alt="Figure 5.1 – A TL scenario, where we replace the last layer(s) of a  pre-trained network and repurpose it for a new problem" src="image/B19627_05_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.65.1">Figure 5.1 – A TL scenario, where we replace the last layer(s) of a </span><br/><span class="koboSpan" id="kobo.66.1">pre-trained network and repurpose it for a new problem</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.67.1">However, we cannot do this mechanically and expect the new network to work because we still have to train the new layer with data related to the new task. </span><span class="koboSpan" id="kobo.67.2">We have two options to </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">do this:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.69.1">Use the original part of the network as a feature extractor and only train the new layer(s)</span></strong><span class="koboSpan" id="kobo.70.1">: First, we feed the network a training batch of the new data and propagate it</span><a id="_idIndexMarker616"/><span class="koboSpan" id="kobo.71.1"> forward and backward to see the network’s output and error gradients. </span><span class="koboSpan" id="kobo.71.2">This part works just like regular training would. </span><span class="koboSpan" id="kobo.71.3">But during the weight updates phase, we lock the weights of the original network and only update the weights of the new layers. </span><span class="koboSpan" id="kobo.71.4">This is the recommended approach when we have limited training data for the new problem. </span><span class="koboSpan" id="kobo.71.5">By locking most of the network weights, we prevent overfitting on the </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">new data.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.73.1">Fine-tune the whole network</span></strong><span class="koboSpan" id="kobo.74.1">: We train the whole network and not just the newly added layers at the end. </span><span class="koboSpan" id="kobo.74.2">It is possible to update all the network weights, but we can also lock some of the weights in the first layers. </span><span class="koboSpan" id="kobo.74.3">The idea here is that the initial layers detect general features – not related to a specific task – and it makes sense to reuse them. </span><span class="koboSpan" id="kobo.74.4">On the other hand, the deeper layers may detect task-specific features and it would be better to update them. </span><span class="koboSpan" id="kobo.74.5">We can use this method when we have more training data and don’t need to worry </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">about overfitting.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.76.1">Before we continue, let’s note that TL is not limited to classification-to-classification problems. </span><span class="koboSpan" id="kobo.76.2">As we’ll see later in this chapter, we can use pre-trained CNN as a backbone NN for object detection </span><a id="_idIndexMarker617"/><span class="koboSpan" id="kobo.77.1">and semantic segmentation tasks. </span><span class="koboSpan" id="kobo.77.2">With that, let’s see how to implement TL </span><span class="No-Break"><span class="koboSpan" id="kobo.78.1">in practice.</span></span></p>
<h2 id="_idParaDest-97" lang="en-GB"><a id="_idTextAnchor150"/><span class="koboSpan" id="kobo.79.1">Transfer learning with PyTorch</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.80.1">In this section, we’ll apply an</span><a id="_idIndexMarker618"/><span class="koboSpan" id="kobo.81.1"> advanced ImageNet pre-trained network on the CIFAR-10 images. </span><span class="koboSpan" id="kobo.81.2">We’ll implement both types of TL. </span><span class="koboSpan" id="kobo.81.3">It’s preferable to run </span><a id="_idIndexMarker619"/><span class="koboSpan" id="kobo.82.1">this example </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">on GPU:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.84.1">To define the training dataset, we have to consider a </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">few things:</span></span><a id="_idTextAnchor151"/><ul><li lang="en-GB"><span class="koboSpan" id="kobo.86.1">Use mini-batch with </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">size 50.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.88.1">The CIFAR-10 images are 32×32, while the ImageNet network expects 224×224 input. </span><span class="koboSpan" id="kobo.88.2">As we are using an ImageNet-based network, we’ll upsample the 32×32 CIFAR images to 224×224 </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">using </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.90.1">transforms.</span></strong></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.91.1">Resize</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.93.1">Standardize the CIFAR-10 data using the ImageNet mean and standard deviation, because this is what the </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">network expects.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.95.1">Add minor data </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">augmentation (flip).</span></span></li></ul><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.97.1">We can do all this with the </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">following code:</span></span></p><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.99.1">
import torch
from torch.utils.data import DataLoader
from torchvision import datasets
from torchvision import transforms
batch_size = 50
# training data
train_data_transform = transforms.Compose([
    transforms.Resize(224),
    transforms.RandomHorizontalFlip(),
    transforms.RandomVerticalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(
        [0.485, 0.456, 0.406],
        [0.229, 0.224, 0.225])
])
train_set = datasets.CIFAR10(
    root='data',
    train=True,
    download=True,
    transform=train_data_transform)
train_loader = DataLoader(
    dataset=train_set,
    batch_size=batch_size,
    shuffle=True,
    num_workers=2)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.100.1">Follow the same steps </span><a id="_idIndexMarker620"/><span class="koboSpan" id="kobo.101.1">with the validation data (except</span><a id="_idIndexMarker621"/><span class="koboSpan" id="kobo.102.1"> for the </span><span class="No-Break"><span class="koboSpan" id="kobo.103.1">data augmentation):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.104.1">
val_data_transform = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize(
        [0.485, 0.456, 0.406],
        [0.229, 0.224, 0.225])
])
val_set = datasets.CIFAR10(
    root='data',
    train=False,
    download=True,
    transform=val_data_transform)
val_order = DataLoader(
    dataset=val_set,
    batch_size=batch_size,
    shuffle=False,
    num_workers=2)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.105.1">Choose a device – preferably</span><a id="_idIndexMarker622"/><span class="koboSpan" id="kobo.106.1"> a GPU with </span><a id="_idIndexMarker623"/><span class="koboSpan" id="kobo.107.1">a fallback </span><span class="No-Break"><span class="koboSpan" id="kobo.108.1">on CPU:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.109.1">
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.110.1">To train and validate the model, we’ll use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.111.1">train_model(model, loss_function, optimizer, data_loader)</span></strong><span class="koboSpan" id="kobo.112.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.113.1">test_model(model, loss_function, data_loader)</span></strong><span class="koboSpan" id="kobo.114.1"> functions. </span><span class="koboSpan" id="kobo.114.2">We first implemented them in </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.115.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.116.1">, so we will not repeat the implementation here (it is available in the source code example </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">on GitHub).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.118.1">Define the first TL scenario, where we use the pre-trained network as a </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">feature extractor:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.120.1">We’ll use a popular</span><a id="_idIndexMarker624"/><span class="koboSpan" id="kobo.121.1"> network, </span><strong class="bold"><span class="koboSpan" id="kobo.122.1">MobileNetV3</span></strong><span class="koboSpan" id="kobo.123.1"> (we discussed it in </span><a href="B19627_04.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.124.1">Chapter 4</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.125.1">).</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.126.1">PyTorch will automatically download the pre-trained </span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">ImageNet weights.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.128.1">Replace the last network layer with a new layer with 10 outputs, one for each </span><span class="No-Break"><span class="koboSpan" id="kobo.129.1">CIFAR-10 class.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.130.1">Exclude the existing network layers from the weight updates phase, and only pass the newly added FC layer to the </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">Adam optimizer.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.132.1">Run the training for </span><strong class="source-inline"><span class="koboSpan" id="kobo.133.1">epochs</span></strong><span class="koboSpan" id="kobo.134.1"> and evaluate the network accuracy after </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">each epoch.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.136.1">Use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.137.1">plot_accuracy</span></strong><span class="koboSpan" id="kobo.138.1"> accuracy </span><a id="_idIndexMarker625"/><span class="koboSpan" id="kobo.139.1">function, which plots the validation accuracy on a </span><strong class="source-inline"><span class="koboSpan" id="kobo.140.1">matplotlib</span></strong><span class="koboSpan" id="kobo.141.1"> graph. </span><span class="koboSpan" id="kobo.141.2">We won’t include</span><a id="_idIndexMarker626"/><span class="koboSpan" id="kobo.142.1"> the full implementation here, but it is available </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">on GitHub.</span></span></li></ul><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.144.1">The following is the </span><strong class="source-inline"><span class="koboSpan" id="kobo.145.1">tl_feature_extractor</span></strong><span class="koboSpan" id="kobo.146.1"> function, which implements </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">all this:</span></span></p><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.148.1">
import torch.nn as nn
import torch.optim as optim
from torchvision.models import MobileNet_V3_Small_Weights, mobilenet_v3_small
def tl_feature_extractor(epochs=5):
    # load the pre-trained model
    model = mobilenet_v3_small(
        weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)
    # exclude existing parameters from backward pass
    # for performance
    for param in model.parameters():
        param.requires_grad = False
    # newly constructed layers have requires_grad=True by default
    num_features = model.classifier[0].in_features
    model.classifier = nn.Linear(num_features, 10)
    # transfer to GPU (if available)
    model = model.to(device)
    loss_function = nn.CrossEntropyLoss()
    # only parameters of the final layer are being optimized
    optimizer = optim.Adam(model.classifier.parameters())
    # train
    test_acc = list()  # collect accuracy for plotting
    for epoch in range(epochs):
        print('Epoch {}/{}'.format(epoch + 1,
            epochs))
        train_model(model, loss_function,
        optimizer, train_loader)
        _, acc = test_model(model, loss_function,
            val_order)
        test_acc.append(acc.cpu())
    plot_accuracy(test_acc)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.149.1">Implement the</span><a id="_idIndexMarker627"/><span class="koboSpan" id="kobo.150.1"> fine-tuning approach with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.151.1">tl_fine_tuning</span></strong><span class="koboSpan" id="kobo.152.1"> function. </span><span class="koboSpan" id="kobo.152.2">This</span><a id="_idIndexMarker628"/><span class="koboSpan" id="kobo.153.1"> function is similar to </span><strong class="source-inline"><span class="koboSpan" id="kobo.154.1">tl_feature_extractor</span></strong><span class="koboSpan" id="kobo.155.1">, but now, we’ll train the </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">whole network:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.157.1">
def tl_fine_tuning(epochs=5):
    # load the pre-trained model
    model = mobilenet_v3_small(
        weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1)
    # replace the last layer
    num_features = model.classifier[0].in_features
    model.classifier = nn.Linear(num_features, 10)
    # transfer the model to the GPU
    model = model.to(device)
    # loss function
    loss_function = nn.CrossEntropyLoss()
    # We'll optimize all parameters
    optimizer = optim.Adam(model.parameters())
    # train
    test_acc = list()  # collect accuracy for plotting
    for epoch in range(epochs):
        print('Epoch {}/{}'.format(epoch + 1,
            epochs))
        train_model(model, loss_function,
        optimizer, train_loader)
        _, acc = test_model(model, loss_function,
            val_order)
        test_acc.append(acc.cpu())
    plot_accuracy(test_acc)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.158.1">We</span><a id="_idIndexMarker629"/><span class="koboSpan" id="kobo.159.1"> can run</span><a id="_idIndexMarker630"/><span class="koboSpan" id="kobo.160.1"> the whole thing in one of </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">two ways:</span></span><ol><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.162.1">Call </span><strong class="source-inline"><span class="koboSpan" id="kobo.163.1">tl_fine_tuning(epochs=5)</span></strong><span class="koboSpan" id="kobo.164.1"> to use the fine-tuning approach for </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">five epochs.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.166.1">Call </span><strong class="source-inline"><span class="koboSpan" id="kobo.167.1">tl_feature_extractor(epochs=5)</span></strong><span class="koboSpan" id="kobo.168.1"> to train the network with the feature extractor approach for </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">five epochs.</span></span></li></ol></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.170.1">With a network as a feature</span><a id="_idIndexMarker631"/><span class="koboSpan" id="kobo.171.1"> extractor, we’ll get about 81% accuracy, while with fine-tuning, we’ll get 89%. </span><span class="koboSpan" id="kobo.171.2">But if we run the fine-tuning for more</span><a id="_idIndexMarker632"/><span class="koboSpan" id="kobo.172.1"> epochs, the network will start overfitting. </span><span class="koboSpan" id="kobo.172.2">Next, let’s see the same example but </span><span class="No-Break"><span class="koboSpan" id="kobo.173.1">with Keras.</span></span></p>
<h2 id="_idParaDest-98" lang="en-GB"><a id="_idTextAnchor152"/><span class="koboSpan" id="kobo.174.1">Transfer learning with Keras</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.175.1">In this section, we’ll implement the</span><a id="_idIndexMarker633"/><span class="koboSpan" id="kobo.176.1"> two TL scenarios again, but this time</span><a id="_idIndexMarker634"/><span class="koboSpan" id="kobo.177.1"> using Keras and TF. </span><span class="koboSpan" id="kobo.177.2">In this way, we can compare the two libraries. </span><span class="koboSpan" id="kobo.177.3">Again, we’ll use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">MobileNetV3Small</span></strong><span class="koboSpan" id="kobo.179.1"> architecture. </span><span class="koboSpan" id="kobo.179.2">In addition to Keras, this example also requires the TF Datasets package (</span><a href="https://www.tensorflow.org/datasets"><span class="koboSpan" id="kobo.180.1">https://www.tensorflow.org/datasets</span></a><span class="koboSpan" id="kobo.181.1">), a collection of various popular ML datasets. </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">Let’s start:</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.183.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.184.1">This example is partially based </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">on </span></span><a href="https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.186.1">https://github.com/tensorflow/docs/blob/master/site/en/tutorials/images/transfer_learning.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.187.1">.</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.188.1">Define the mini-batch and input image sizes (the image size is determined by the </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">network architecture):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.190.1">
IMG_SIZE = 224
BATCH_SIZE = 50</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.191.1">Load the CIFAR-10 dataset with the help </span><a id="_idTextAnchor153"/><span class="koboSpan" id="kobo.192.1">of TF datasets. </span><span class="koboSpan" id="kobo.192.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.193.1">repeat()</span></strong><span class="koboSpan" id="kobo.194.1"> method allows us to reuse the dataset for </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">multiple epochs:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.196.1">
import tensorflow as tf
import tensorflow_datasets as tfds
data, metadata = tfds.load('cifar10', with_info=True,
    as_supervised=True)
raw_train, raw</span><a id="_idTextAnchor154"/><span class="koboSpan" id="kobo.197.1">_test = data['train'].repeat(</span><a id="_idTextAnchor155"/><span class="koboSpan" id="kobo.198.1">),
    data['test'].repeat()</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.199.1">Define the </span><strong class="source-inline"><span class="koboSpan" id="kobo.200.1">train_format_sample</span></strong><span class="koboSpan" id="kobo.201.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.202.1">test_format_sample</span></strong><span class="koboSpan" id="kobo.203.1"> functions, which will transform the initial images into suitable CNN inputs. </span><span class="koboSpan" id="kobo.203.2">These functions play the same roles that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.204.1">transforms.Compose</span></strong><span class="koboSpan" id="kobo.205.1"> object plays, which we defined in the </span><em class="italic"><span class="koboSpan" id="kobo.206.1">Implementing transfer learning with PyTorch</span></em><span class="koboSpan" id="kobo.207.1"> section. </span><span class="koboSpan" id="kobo.207.2">The input is</span><a id="_idIndexMarker635"/><span class="koboSpan" id="kobo.208.1"> transformed </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">as follows:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.210.1">The images are resized</span><a id="_idIndexMarker636"/><span class="koboSpan" id="kobo.211.1"> to 224×224, which is the expected network </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">input size</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.213.1">Each image is standardized by transforming its values so that it’s in the (-1; </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">1) interval</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.215.1">The labels are converted into </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">one-hot encodings</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.217.1">The training images are randomly flipped horizontally </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">and vertically</span></span></li></ul><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.219.1">Let’s look at the </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">actual implementation:</span></span></p><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.221.1">
def train_format_sample(image, label):
    """Transform data for training"""
    image = tf.cast(image, tf.float32)
    image = tf.image.resize(image, (IMG_SIZE,
        IMG_SIZE))
    image = tf.image.random_flip_left_right(image)
    image = tf.image.random_flip_up_down(image)
    label = tf.one_hot(label,
        metadata.features['label'].num_classes)
    return image, label
def test_format_sample(image, label):
    """Transform data for testing"""
    image = tf.cast(image, tf.float32)
    image = tf.image.resize(image, (IMG_SIZE,
        IMG_SIZE))
    label = tf.one_hot(label,
        metadata.features['label'].num_classes)
    return image, label</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.222.1">Next is some </span><a id="_idIndexMarker637"/><span class="koboSpan" id="kobo.223.1">boilerplate code that assigns these </span><a id="_idIndexMarker638"/><span class="koboSpan" id="kobo.224.1">transformers to the train/test datase</span><a id="_idTextAnchor156"/><span class="koboSpan" id="kobo.225.1">ts and splits them </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">into mini-batches:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.227.1">
# assign transformers to raw data
train_data = raw_train.map(train_format_sample)
test_data = raw_test.map(test_format_sample)
# extract batches from the training set
train_batches =</span><a id="_idTextAnchor157"/><span class="koboSpan" id="kobo.228.1"> train_data.shuffle(1000).batch(BATCH_SIZE)
test_batches = test_data.batch(BATCH_SIZE)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.229.1">Define the feature </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">extraction model:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.231.1">Use Keras for the pre-trained network and model definition since it is an integral part </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">of TF</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.233.1">Load the </span><strong class="source-inline"><span class="koboSpan" id="kobo.234.1">MobileNetV3Small</span></strong><span class="koboSpan" id="kobo.235.1"> pre-trained net, excluding the final </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">FC layers</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.237.1">Call </span><strong class="source-inline"><span class="koboSpan" id="kobo.238.1">base_model.trainable = False</span></strong><span class="koboSpan" id="kobo.239.1">, which freezes all the network weights and prevents them </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">from training</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.241.1">Add a </span><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">GlobalAveragePooling2D</span></strong><span class="koboSpan" id="kobo.243.1"> operation, followed by a new and trainable FC trainable layer at</span><a id="_idIndexMarker639"/><span class="koboSpan" id="kobo.244.1"> the end of </span><span class="No-Break"><span class="koboSpan" id="kobo.245.1">the network</span></span></li></ul><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.246.1">The following code</span><a id="_idIndexMarker640"/> <span class="No-Break"><span class="koboSpan" id="kobo.247.1">implements this:</span></span></p><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.248.1">
def build_fe_model():
    """"Create feature extraction model from the pre-trained model ResNet50V2"""
    # create the pre-trained part of the network, excluding FC layers
    base_model = tf.keras.applications.MobileNetV3Small(
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
        include_top=False,
        classes=10,
        weights='imagenet',
        include_preprocessing=True)
    # exclude all model layers from training
    base_model.trainable = False
    # create new model as a combination of the pre-trained net
    # and one fully connected layer at the top
    return tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(
            metadata.features['label'].num_classes,
            activation='softmax')
    ])</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.249.1">Define the fine-tuning model. </span><span class="koboSpan" id="kobo.249.2">The only difference it has from the feature extraction is that we only freeze</span><a id="_idIndexMarker641"/><span class="koboSpan" id="kobo.250.1"> some of the bottom pre-trained network layers (as opposed to all of them). </span><span class="koboSpan" id="kobo.250.2">The following is</span><a id="_idIndexMarker642"/> <span class="No-Break"><span class="koboSpan" id="kobo.251.1">the implementation:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.252.1">
def build_ft_model():
    """"Create fine tuning model from the pre-trained model MobileNetV3Small"""
    # create the pre-trained part of the network, excluding FC layers
    base_model = tf.keras.applications.MobileNetV3Small(
        input_shape=(IMG_SIZE, IMG_SIZE, 3),
        include_top=False,
        weights='imagenet',
        include_preprocessing=True
    )
    # Fine tune from this layer onwards
    fine_tune_at = 100
    # Freeze all the layers before the `fine_tune_at` layer
    for layer in base_model.layers[:fine_tune_at]:
        layer.trainable = False
    # create new model as a combination of the pre-trained net
    # and one fully connected layer at the top
    return tf.keras.Sequential([
        base_model,
        tf.keras.layers.GlobalAveragePooling2D(),
        tf.keras.layers.Dense(
           metadata.features['label'].num_classes,
           activation='softmax')
    ])</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.253.1">Implement the </span><strong class="source-inline"><span class="koboSpan" id="kobo.254.1">train_model</span></strong><span class="koboSpan" id="kobo.255.1"> function, which trains and evaluates the models that are created</span><a id="_idIndexMarker643"/><span class="koboSpan" id="kobo.256.1"> by either the </span><strong class="source-inline"><span class="koboSpan" id="kobo.257.1">build_fe_model</span></strong><span class="koboSpan" id="kobo.258.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.259.1">build_ft_model</span></strong><span class="koboSpan" id="kobo.260.1"> function. </span><span class="koboSpan" id="kobo.260.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.261.1">plot_accuracy</span></strong><span class="koboSpan" id="kobo.262.1"> function is not implemented </span><a id="_idIndexMarker644"/><span class="koboSpan" id="kobo.263.1">here but is available </span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">on GitHub:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.265.1">
def train_model(model, epochs=5):
    """Train the model. </span><span class="koboSpan" id="kobo.265.2">This function is shared for both FE and FT modes"""
    # configure the model for training
    model.compile(
        optimizer=tf.keras.optimizers.Adam(
             learning_rate=0.0001),
        loss='categorical_crossentropy',
        metrics=['accuracy'])
    # train the model
    history = model.fit(
        train_batches,
        epochs=epochs,
        steps_per_epoch=metadata.splits['train'].num_examples /
            BATCH_SIZE,
        validation_data=test_batches,
        validation_steps=metadata.splits['test'].num_examples /
            BATCH_SIZE,
        workers=4)
    # plot accuracy
    plot_accuracy(history.history['val_accuracy'])</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.266.1">We can run either the</span><a id="_idIndexMarker645"/><span class="koboSpan" id="kobo.267.1"> feature extraction or fine-tuning TL using the </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">following code:</span></span><ul><li lang="en-GB"><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.269.1">train_model(build_ft_model())</span></strong></span></li><li lang="en-GB"><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.270.1">train_model(build_fe_model())</span></strong></span></li></ul></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.271.1">With a network as a feature</span><a id="_idIndexMarker646"/><span class="koboSpan" id="kobo.272.1"> extractor, we’ll get about 82% accuracy, while with fine-tuning, we’ll get 89% accuracy. </span><span class="koboSpan" id="kobo.272.2">The results are similar to the </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">PyTorch example.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.274.1">Next, let’s turn our attention to object detection – a task we can solve with the help </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">of TL.</span></span></p>
<h1 id="_idParaDest-99" lang="en-GB"><a id="_idTextAnchor158"/><span class="koboSpan" id="kobo.276.1">Object detection</span></h1>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.277.1">Object detection</span></strong><span class="koboSpan" id="kobo.278.1"> is the process of finding object instances of a certain class, such as people, cars, and trees, in images or videos. </span><span class="koboSpan" id="kobo.278.2">Unlike classification, object detection can detect multiple objects as </span><a id="_idIndexMarker647"/><span class="koboSpan" id="kobo.279.1">well as their location in </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">the image.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.281.1">An object detector would return a list of detected objects with the following information for </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">each object:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.283.1">The class of the object (person, car, tree, and </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">so on).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.285.1">A probability (or objectness score) in the [0, 1] range, which conveys how confident the detector is that the object exists in that location. </span><span class="koboSpan" id="kobo.285.2">This is similar to the output of a regular </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">binary classifier.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.287.1">The coordinates of the rectangular region of the image where the object is located. </span><span class="koboSpan" id="kobo.287.2">This rectangle</span><a id="_idIndexMarker648"/><span class="koboSpan" id="kobo.288.1"> is called a </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.289.1">bounding box</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.291.1">We can see the typical output of an object-detection algorithm in the following figure. </span><span class="koboSpan" id="kobo.291.2">The object type and objectness score are above each </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">bounding box:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer417">
<span class="koboSpan" id="kobo.293.1"><img alt="Figure 5.2 – The output of an object detector. Source: https://en.wikipedia.org/wiki/File:2011_FIA_GT1_Silverstone_2.jpg" src="image/B19627_05_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.294.1">Figure 5.2 – The output of an object detector. </span><span class="koboSpan" id="kobo.294.2">Source: </span><a href="https://en.wikipedia.org/wiki/File:2011_FIA_GT1_Silverstone_2.jpg"><span class="koboSpan" id="kobo.295.1">https://en.wikipedia.org/wiki/File:2011_FIA_GT1_Silverstone_2.jpg</span></a></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.296.1">Next, let’s outline</span><a id="_idIndexMarker649"/><span class="koboSpan" id="kobo.297.1"> the different approaches to solving an object </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">detection task.</span></span></p>
<h2 id="_idParaDest-100" lang="en-GB"><a id="_idTextAnchor159"/><span class="koboSpan" id="kobo.299.1">Approaches to object detection</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.300.1">In this section, we’ll outline </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">three </span></span><span class="No-Break"><a id="_idIndexMarker650"/></span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">approaches:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.303.1">Classic sliding window</span></strong><span class="koboSpan" id="kobo.304.1">: Here, we’ll use a</span><a id="_idIndexMarker651"/><span class="koboSpan" id="kobo.305.1"> regular classification network (classifier). </span><span class="koboSpan" id="kobo.305.2">This approach can work with any type of classification algorithm, but it’s relatively slow </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">and error-prone:</span></span><ul><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.307.1">Build an image pyramid</span></strong><span class="koboSpan" id="kobo.308.1">: This is a combination of different scales of the same image (see the following figure). </span><span class="koboSpan" id="kobo.308.2">For example, each scaled image can be two times smaller than the previous one. </span><span class="koboSpan" id="kobo.308.3">In this way, we’ll be able to detect objects regardless of their size in the </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">original image.</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.310.1">Slide the classifier across the whole image</span></strong><span class="koboSpan" id="kobo.311.1">: We’ll use each location of the image as an input to the classifier, and the result will determine the type of object that is in the location. </span><span class="koboSpan" id="kobo.311.2">The bounding box of the location is just the image region that </span><a id="_idIndexMarker652"/><span class="koboSpan" id="kobo.312.1">we used </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">as input.</span></span></li><li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.314.1">Multiple overlapping bounding boxes for each object</span></strong><span class="koboSpan" id="kobo.315.1">: We’ll use some heuristics to</span><a id="_idIndexMarker653"/><span class="koboSpan" id="kobo.316.1"> combine them into a </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">single prediction.</span></span></li></ul></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.318.1">Here is a figure showing the sliding </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">window approach:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer418">
<span class="koboSpan" id="kobo.320.1"><img alt="Figure 5.3 – Sliding window plus image pyramid object detection" src="image/B19627_05_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.321.1">Figure 5.3 – Sliding window plus image pyramid object detection</span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.322.1">Two-stage detection methods</span></strong><span class="koboSpan" id="kobo.323.1">: These </span><a id="_idIndexMarker654"/><span class="koboSpan" id="kobo.324.1">methods are very accurate but relatively slow. </span><span class="koboSpan" id="kobo.324.2">As its name suggests, this involves </span><span class="No-Break"><span class="koboSpan" id="kobo.325.1">two steps:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.326.1">A special type of CNN, called a </span><strong class="bold"><span class="koboSpan" id="kobo.327.1">Region Proposal Network</span></strong><span class="koboSpan" id="kobo.328.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.329.1">RPN</span></strong><span class="koboSpan" id="kobo.330.1">), scans the image and proposes </span><a id="_idIndexMarker655"/><span class="koboSpan" id="kobo.331.1">several possible bounding </span><a id="_idIndexMarker656"/><span class="koboSpan" id="kobo.332.1">boxes, or </span><strong class="bold"><span class="koboSpan" id="kobo.333.1">regions of interest</span></strong><span class="koboSpan" id="kobo.334.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.335.1">RoI</span></strong><span class="koboSpan" id="kobo.336.1">), where objects might be located. </span><span class="koboSpan" id="kobo.336.2">However, this network doesn’t detect the type of object, but only whether an object is present in </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">the region.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.338.1">The RoI is sent to the second stage for object classification, which determines the actual object in each </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">bounding box.</span></span></li></ul></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.340.1">One-stage (or one-shot) detection methods</span></strong><span class="koboSpan" id="kobo.341.1">: Here, a single CNN produces both the object</span><a id="_idIndexMarker657"/><span class="koboSpan" id="kobo.342.1"> type and the bounding box. </span><span class="koboSpan" id="kobo.342.2">These approaches are usually faster but less accurate </span><a id="_idIndexMarker658"/><span class="koboSpan" id="kobo.343.1">than the </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">two-stage methods.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.345.1">In the next section, we’ll introduce </span><strong class="bold"><span class="koboSpan" id="kobo.346.1">YOLO</span></strong><span class="koboSpan" id="kobo.347.1"> – an accurate and efficient one-stage </span><span class="No-Break"><span class="koboSpan" id="kobo.348.1">detection algorithm.</span></span></p>
<h2 id="_idParaDest-101" lang="en-GB"><a id="_idTextAnchor160"/><span class="koboSpan" id="kobo.349.1">Object detection with YOLO</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.350.1">YOLO is one of the most</span><a id="_idIndexMarker659"/><span class="koboSpan" id="kobo.351.1"> popular one-stage detection algorithms. </span><span class="koboSpan" id="kobo.351.2">The name is an acronym for the popular motto “You only live once”, which reflects the one-stage</span><a id="_idIndexMarker660"/><span class="koboSpan" id="kobo.352.1"> nature of the algorithm. </span><span class="koboSpan" id="kobo.352.2">Since its original release, there have been multiple YOLO versions, with different authors. </span><span class="koboSpan" id="kobo.352.3">For the sake of clarity, we’ll list all </span><span class="No-Break"><span class="koboSpan" id="kobo.353.1">versions here:</span></span></p>
<ul>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.354.1">You Only Look Once: Unified, Real-Time Object Detection</span></em><span class="koboSpan" id="kobo.355.1"> (</span><a href="https://arxiv.org/abs/1506.02640"><span class="koboSpan" id="kobo.356.1">https://arxiv.org/abs/1506.02640</span></a><span class="koboSpan" id="kobo.357.1">), by Joseph Redmon, Santosh Divvala, Ross Girshick, and </span><span class="No-Break"><span class="koboSpan" id="kobo.358.1">Ali Farhadi.</span></span></li>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.359.1">YOLO9000: Better, Faster, Stronger</span></em><span class="koboSpan" id="kobo.360.1"> (</span><a href="https://arxiv.org/abs/1612.08242"><span class="koboSpan" id="kobo.361.1">https://arxiv.org/abs/1612.08242</span></a><span class="koboSpan" id="kobo.362.1">), by Joseph Redmon and </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">Ali Farhadi.</span></span></li>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.364.1">YOLOv3: An Incremental Improvement</span></em><span class="koboSpan" id="kobo.365.1"> (</span><a href="https://arxiv.org/abs/1804.02767"><span class="koboSpan" id="kobo.366.1">https://arxiv.org/abs/1804.02767</span></a><span class="koboSpan" id="kobo.367.1">, </span><a href="https://github.com/pjreddie/darknet"><span class="koboSpan" id="kobo.368.1">https://github.com/pjreddie/darknet</span></a><span class="koboSpan" id="kobo.369.1">), by Joseph Redmon and </span><span class="No-Break"><span class="koboSpan" id="kobo.370.1">Ali Farhadi.</span></span></li>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.371.1">YOLOv4: Optimal Speed and Accuracy of Object Detection</span></em><span class="koboSpan" id="kobo.372.1"> (</span><a href="https://arxiv.org/abs/2004.10934"><span class="koboSpan" id="kobo.373.1">https://arxiv.org/abs/2004.10934</span></a><span class="koboSpan" id="kobo.374.1">, </span><a href="https://github.com/AlexeyAB/darknet"><span class="koboSpan" id="kobo.375.1">https://github.com/AlexeyAB/darknet</span></a><span class="koboSpan" id="kobo.376.1">), by Alexey Bochkovskiy, Chien-Yao </span><a id="_idIndexMarker661"/><span class="koboSpan" id="kobo.377.1">Wang, and Hong-Yuan </span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">Mark Liao.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.379.1">YOLOv5</span></strong><span class="koboSpan" id="kobo.380.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.381.1">YOLOv8</span></strong><span class="koboSpan" id="kobo.382.1"> (</span><a href="https://github.com/ultralytics/yolov5"><span class="koboSpan" id="kobo.383.1">https://github.com/ultralytics/yolov5</span></a><span class="koboSpan" id="kobo.384.1">, </span><a href="https://github.com/ultralytics/ultralytics"><span class="koboSpan" id="kobo.385.1">https://github.com/ultralytics/ultralytics</span></a><span class="koboSpan" id="kobo.386.1">), by</span><a id="_idIndexMarker662"/><span class="koboSpan" id="kobo.387.1"> Ultralitics (</span><a href="https://ultralytics.com/"><span class="koboSpan" id="kobo.388.1">https://ultralytics.com/</span></a><span class="koboSpan" id="kobo.389.1">). </span><span class="koboSpan" id="kobo.389.2">V5 and </span><a id="_idIndexMarker663"/><span class="koboSpan" id="kobo.390.1">v8 have no</span><a id="_idIndexMarker664"/> <span class="No-Break"><span class="koboSpan" id="kobo.391.1">official paper.</span></span></li>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.392.1">YOLOv6 v3.0: A Full-Scale Reloading</span></em><span class="koboSpan" id="kobo.393.1"> (</span><a href="https://arxiv.org/abs/2301.05586"><span class="koboSpan" id="kobo.394.1">https://arxiv.org/abs/2301.05586</span></a><span class="koboSpan" id="kobo.395.1">, </span><a href="https://github.com/meituan/YOLOv6"><span class="koboSpan" id="kobo.396.1">https://github.com/meituan/YOLOv6</span></a><span class="koboSpan" id="kobo.397.1">), by Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">Xiangxiang Chu.</span></span></li>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.399.1">YOLOv7: Trainable bag-of-freebies sets new state-of-the-art for real-time object detectors</span></em><span class="koboSpan" id="kobo.400.1"> (</span><a href="https://arxiv.org/abs/2207.02696"><span class="koboSpan" id="kobo.401.1">https://arxiv.org/abs/2207.02696</span></a><span class="koboSpan" id="kobo.402.1">, Mark L</span><a href="https://github.com/WongKinYiu/yolov7"><span class="koboSpan" id="kobo.403.1">https://github.com/WongKinYiu/yolov7</span></a><span class="koboSpan" id="kobo.404.1">), by Chien-Yao Wang, Alexey Bochkovskiy, and </span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">Hong-Yuan iao.</span></span></li>
</ul>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.406.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.407.1">v3 is the last version, released </span><a id="_idIndexMarker665"/><span class="koboSpan" id="kobo.408.1">by the original authors of the algorithm. </span><span class="koboSpan" id="kobo.408.2">v4 is a fork of v3 and was endorsed by the main author of v1-v3, Joseph Redmon (</span><a href="https://twitter.com/pjreddie/status/1253891078182199296"><span class="koboSpan" id="kobo.409.1">https://twitter.com/pjreddie/status/1253891078182199296</span></a><span class="koboSpan" id="kobo.410.1">). </span><span class="koboSpan" id="kobo.410.2">On the other hand, v5 is an independent implementation, inspired by YOLO. </span><span class="koboSpan" id="kobo.410.3">This sparked a controversy regarding the name of v5. </span><span class="koboSpan" id="kobo.410.4">You can follow some of the discussion at https://github.com/AlexeyAB/darknet/issues/5920, where Alexey Bochkovskiy, the author of v4, has also posted. </span><span class="koboSpan" id="kobo.410.5">The authors of v5 have also addressed the controversy here: </span><a href="https://blog.roboflow.com/yolov4-versus-yolov5/"><span class="koboSpan" id="kobo.411.1">https://blog.roboflow.com/yolov4-versus-yolov5/</span></a><span class="koboSpan" id="kobo.412.1">. </span><span class="koboSpan" id="kobo.412.2">Regardless of this discussion, v5 and v8 have proven to work and are popular detection algorithms in their </span><span class="No-Break"><span class="koboSpan" id="kobo.413.1">own right.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.414.1">We’ll discuss the YOLO properties shared among all versions and we’ll point out some of </span><span class="No-Break"><span class="koboSpan" id="kobo.415.1">the differences.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.416.1">Let’s start with the </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">YOLO architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer419">
<span class="koboSpan" id="kobo.418.1"><img alt="Figure 5.4 – The YOLO architecture" src="image/B19627_05_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.419.1">Figure 5.4 – The YOLO architecture</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.420.1">It contains the </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">following components:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.422.1">Backbone</span></strong><span class="koboSpan" id="kobo.423.1">: This is a </span><a id="_idIndexMarker666"/><span class="koboSpan" id="kobo.424.1">CNN model that’s responsible for extracting </span><a id="_idIndexMarker667"/><span class="koboSpan" id="kobo.425.1">features from the input image. </span><span class="koboSpan" id="kobo.425.2">These features are then passed to the next components for object detection. </span><span class="koboSpan" id="kobo.425.3">Usually, the backbone is an ImageNet pre-trained CNN, similar to the advanced </span><a id="_idIndexMarker668"/><span class="koboSpan" id="kobo.426.1">models we discussed in </span><a href="B19627_04.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.427.1">Chapter 4</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.428.1">.</span></span><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.429.1">The backbone is an example of</span><a id="_idIndexMarker669"/><span class="koboSpan" id="kobo.430.1"> TL – we take a CNN trained for classification and repurpose it for object detection. </span><span class="koboSpan" id="kobo.430.2">The different YOLO versions use different backbones. </span><span class="koboSpan" id="kobo.430.3">For example, v3 uses a special fully convolutional CNN called DarkNet-53 with</span><a id="_idIndexMarker670"/><span class="koboSpan" id="kobo.431.1"> 53 layers. </span><span class="koboSpan" id="kobo.431.2">Subsequent YOLO versions introduce various improvements to this architecture, while others use their own </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">unique backbone.</span></span></p></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.433.1">Neck</span></strong><span class="koboSpan" id="kobo.434.1">: This is an intermediate</span><a id="_idIndexMarker671"/><span class="koboSpan" id="kobo.435.1"> part of the model that connects the backbone to the head. </span><span class="koboSpan" id="kobo.435.2">It concatenates the output at different stages of the backbone feature maps before sending the combined result to the next component (the head). </span><span class="koboSpan" id="kobo.435.3">This is an alternative to the standard approach, where we would just send the output of the last backbone convolution for further processing. </span><span class="koboSpan" id="kobo.435.4">To understand the need for the neck, let’s recall that our goal is to create a precise bounding box around the edges of the detected object. </span><span class="koboSpan" id="kobo.435.5">The object itself might be big or small, relative to the image. </span><span class="koboSpan" id="kobo.435.6">However, the receptive field of the deeper layers of the backbone is large because it aggregates the receptive fields of all preceding layers. </span><span class="koboSpan" id="kobo.435.7">Hence, the features detected at the deeper layers encompass large parts of the input image. </span><span class="koboSpan" id="kobo.435.8">This runs contrary to our goal of fine-grained object detection, regardless of the object’s size. </span><span class="koboSpan" id="kobo.435.9">To solve</span><a id="_idIndexMarker672"/><span class="koboSpan" id="kobo.436.1"> this, the neck combines the feature maps at different backbone stages, which makes it possible to detect objects at different </span><a id="_idIndexMarker673"/><span class="koboSpan" id="kobo.437.1">scales. </span><span class="koboSpan" id="kobo.437.2">However, the feature maps at each backbone stage have different dimensions and cannot be combined directly. </span><span class="koboSpan" id="kobo.437.3">The neck applies different techniques, such as upsampling or downsampling, to equalize these dimensions, so that they can </span><span class="No-Break"><span class="koboSpan" id="kobo.438.1">be concatenated.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.439.1">Head</span></strong><span class="koboSpan" id="kobo.440.1">: This is the final </span><a id="_idIndexMarker674"/><span class="koboSpan" id="kobo.441.1">component of the model, which outputs the detected objects. </span><span class="koboSpan" id="kobo.441.2">Each detected object is represented by its bounding box coordinates and </span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">its class.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.443.1">With that, we have gained a</span><a id="_idIndexMarker675"/><span class="koboSpan" id="kobo.444.1"> bird’s-eye view of the YOLO architecture. </span><span class="koboSpan" id="kobo.444.2">But it doesn’t answer some inconvenient (yet intriguing) questions, such as how the model detects multiple objects on the same image, or what happens if two or more objects overlap and one is only partially visible. </span><span class="koboSpan" id="kobo.444.3">To find the answers to these questions, let’s introduce the following diagram, which consists of two </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">overlapping objects:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer420">
<span class="koboSpan" id="kobo.446.1"><img alt="Figure 5.5 – An object detection YOLO example with two overlapping objects and their bounding boxes" src="image/B19627_05_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.447.1">Figure 5.5 – An object detection YOLO example with two overlapping objects and their bounding boxes</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.448.1">These are the steps that YOLO implements to </span><span class="No-Break"><span class="koboSpan" id="kobo.449.1">detect them:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.450.1">Split the input image into a grid of </span><em class="italic"><span class="koboSpan" id="kobo.451.1">S×S</span></em><span class="koboSpan" id="kobo.452.1"> cells (the preceding diagram uses a </span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">3×3 grid):</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.454.1">The center of a cell represents the center of a region where an object might </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">be located.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.456.1">The model can detect both objects that span multiple cells and ones that lie entirely within the cell. </span><span class="koboSpan" id="kobo.456.2">Each object is associated with a single cell, even if it covers multiple cells. </span><span class="koboSpan" id="kobo.456.3">In this case, we’ll associate the object with the cell, where the </span><a id="_idIndexMarker676"/><span class="koboSpan" id="kobo.457.1">center of its bounding box lies. </span><span class="koboSpan" id="kobo.457.2">For example, the two objects in the diagram span multiple cells, but they are both assigned to the central cell because their centers lie </span><span class="No-Break"><span class="koboSpan" id="kobo.458.1">in it.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.459.1">A cell can contain multiple objects (</span><em class="italic"><span class="koboSpan" id="kobo.460.1">1-to-n</span></em><span class="koboSpan" id="kobo.461.1"> relationship) or no objects at all. </span><span class="koboSpan" id="kobo.461.2">We’re only interested in the cells </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">with objects.</span></span></li></ul></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.463.1">The model outputs multiple possible detected objects for each grid cell. </span><span class="koboSpan" id="kobo.463.2">Each detected object is </span><a id="_idIndexMarker677"/><span class="koboSpan" id="kobo.464.1">represented by the following array of values: </span><span class="koboSpan" id="kobo.465.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/320.png" style="vertical-align:-0.533em;height:1.294em;width:9.453em"/></span><span class="koboSpan" id="kobo.466.1">. </span><span class="koboSpan" id="kobo.466.2">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">discuss them:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.468.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/321.png" style="vertical-align:-0.533em;height:1.294em;width:4.772em"/></span><span class="koboSpan" id="kobo.469.1"> describes the object bounding box. </span><span class="koboSpan" id="kobo.470.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/322.png" style="vertical-align:-0.533em;height:1.294em;width:2.504em"/></span><span class="koboSpan" id="kobo.471.1"> are the coordinates of the center of the box concerning the whole image. </span><span class="koboSpan" id="kobo.471.2">They are normalized in the [0, 1] range. </span><span class="koboSpan" id="kobo.471.3">For example, if the image size is 100×100 and the center of the bounding box is located at [40, 70], then </span><br/><span class="koboSpan" id="kobo.472.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0.4,0.7&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/323.png" style="vertical-align:-0.533em;height:1.294em;width:7.867em"/></span><span class="koboSpan" id="kobo.473.1">. </span><span class="koboSpan" id="kobo.474.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/324.png" style="vertical-align:-0.390em;height:1.151em;width:2.956em"/></span><span class="koboSpan" id="kobo.475.1"> represent the normalized bounding box height and width concerning the whole image. </span><span class="koboSpan" id="kobo.475.2">If the bounding box’s size is 80×50, then </span><span class="koboSpan" id="kobo.476.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0.8,0.5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/325.png" style="vertical-align:-0.390em;height:1.151em;width:7.575em"/></span><span class="koboSpan" id="kobo.477.1"> for the same 100×100 image. </span><span class="koboSpan" id="kobo.477.2">In practice, a YOLO implementation usually includes helper methods, which will allow us to obtain the absolute coordinates of the </span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">bounding boxes.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.479.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/326.png" style="vertical-align:-0.340em;height:0.858em;width:0.675em"/></span><span class="koboSpan" id="kobo.480.1"> is an objectness score, which represents the confidence of the model (in the [0, 1] range) that an object is present in the cell. </span><span class="koboSpan" id="kobo.480.2">If </span><span class="koboSpan" id="kobo.481.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/327.png" style="vertical-align:-0.340em;height:0.858em;width:0.697em"/></span><span class="koboSpan" id="kobo.482.1"> is closer to 1, then the model is confident that an object is present and </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">vice versa.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.484.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/328.png" style="vertical-align:-0.390em;height:0.888em;width:3.989em"/></span><span class="koboSpan" id="kobo.485.1"> is a one-hot encoding of the class of the detected object. </span><span class="koboSpan" id="kobo.485.2">For example, if we have bicycle, flower, person, and fish classes, and the current object is a person, its encoding will be [0, 0, </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">1, 0].</span></span></li></ul></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.487.1">So far, we’ve demonstrated that the model can detect multiple objects on the same image. </span><span class="koboSpan" id="kobo.487.2">Next, let’s focus on the trickier case with multiple objects in the same cell. </span><span class="koboSpan" id="kobo.487.3">YOLO has an elegant solution to this problem in the form of </span><strong class="bold"><span class="koboSpan" id="kobo.488.1">anchor boxes</span></strong><span class="koboSpan" id="kobo.489.1"> (also known as </span><strong class="bold"><span class="koboSpan" id="kobo.490.1">priors</span></strong><span class="koboSpan" id="kobo.491.1">). </span><span class="koboSpan" id="kobo.491.2">To </span><a id="_idIndexMarker678"/><span class="koboSpan" id="kobo.492.1">understand this concept, we’ll start with the following diagram, which shows </span><a id="_idIndexMarker679"/><span class="koboSpan" id="kobo.493.1">the grid cell (square, uninterrupted line) and two anchor boxes – vertical and horizontal (</span><span class="No-Break"><span class="koboSpan" id="kobo.494.1">dashed lines):</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer430">
<span class="koboSpan" id="kobo.495.1"><img alt="Figure 5.6 – A grid cell (a square, uninterrupted line) with two anchor boxes (dashed lines)" src="image/B19627_05_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.496.1">Figure 5.6 – A grid cell (a square, uninterrupted line) with two anchor boxes (dashed lines)</span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.497.1">For each cell, we’ll have multiple candidate anchor boxes with different scales and aspect ratios. </span><span class="koboSpan" id="kobo.497.2">If we have multiple objects in the same cell, we’ll associate each object with a single anchor box. </span><span class="koboSpan" id="kobo.497.3">If an anchor box doesn’t have an associated object, it will have an objectness score of zero (</span><span class="koboSpan" id="kobo.498.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/329.png" style="vertical-align:-0.340em;height:0.974em;width:2.712em"/></span><span class="koboSpan" id="kobo.499.1">). </span><span class="koboSpan" id="kobo.499.2">We can detect as many objects as there are anchor boxes per cell. </span><span class="koboSpan" id="kobo.499.3">For example, our example 3×3 grid</span><a id="_idIndexMarker680"/><span class="koboSpan" id="kobo.500.1"> with two anchor boxes per cell can detect a total of 3*3*2 = 18 objects. </span><span class="koboSpan" id="kobo.500.2">Because we have a fixed number of cells (</span><em class="italic"><span class="koboSpan" id="kobo.501.1">S×S</span></em><span class="koboSpan" id="kobo.502.1">) and a fixed number of anchor boxes per cell, the size of the network output doesn’t change with the number of detected objects. </span><span class="koboSpan" id="kobo.502.2">Instead, we’ll output results for all possible anchor boxes, but we’ll only consider the ones with an objectness score </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">of </span></span><span class="No-Break"><span class="koboSpan" id="kobo.504.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∼&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/330.png" style="vertical-align:-0.340em;height:0.974em;width:2.800em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">.</span></span></p>
<ol>
<li lang="en-GB" value="4"><span class="koboSpan" id="kobo.506.1">The YOLO algorithm </span><a id="_idIndexMarker681"/><span class="koboSpan" id="kobo.507.1">uses the </span><strong class="bold"><span class="koboSpan" id="kobo.508.1">Intersection over Union</span></strong><span class="koboSpan" id="kobo.509.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.510.1">IoU</span></strong><span class="koboSpan" id="kobo.511.1">) technique both during training and inference to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">its performance:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer433">
<span class="koboSpan" id="kobo.513.1"><img alt="Figure 5.7 – Intersection over Union (IoU)" src="image/B19627_05_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.514.1">Figure 5.7 – Intersection over Union (IoU)</span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.515.1">IoU is the ratio between the area of the intersection and the area of the union of the detected object bounding box and the ground truth (or another object’s) </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">bounding box.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.517.1">During training, we can</span><a id="_idIndexMarker682"/><span class="koboSpan" id="kobo.518.1"> compute the IoU between the anchor boxes and ground truth bounding boxes. </span><span class="koboSpan" id="kobo.518.2">Then, we can assign each ground truth object to its highest overlapping anchor box to generate labeled training data. </span><span class="koboSpan" id="kobo.518.3">In addition, we can compute the IoU between the detected bounding box and the ground truth (label) box. </span><span class="koboSpan" id="kobo.518.4">The higher value of IoU indicates a better overlap between ground truth and prediction. </span><span class="koboSpan" id="kobo.518.5">This can help us evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">the detector.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.520.1">During inference, the output of the model includes all possible anchor boxes for each cell, regardless of whether an object is </span><a id="_idIndexMarker683"/><span class="koboSpan" id="kobo.521.1">present in them. </span><span class="koboSpan" id="kobo.521.2">Many of the boxes will overlap and predict the same object. </span><span class="koboSpan" id="kobo.521.3">We can filter the overlapping objects with the </span><a id="_idIndexMarker684"/><span class="koboSpan" id="kobo.522.1">help of IoU and </span><strong class="bold"><span class="koboSpan" id="kobo.523.1">non-maximum suppression</span></strong><span class="koboSpan" id="kobo.524.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.525.1">NMS</span></strong><span class="koboSpan" id="kobo.526.1">). </span><span class="koboSpan" id="kobo.526.2">Here’s how </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">it works:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.528.1">Discard all bounding boxes with an objectness score </span><span class="No-Break"><span class="koboSpan" id="kobo.529.1">of </span></span><span class="No-Break"><span class="koboSpan" id="kobo.530.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;&lt;&lt;/mml:mo&gt;&lt;mml:mn&gt;0.6&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/331.png" style="vertical-align:-0.340em;height:0.980em;width:3.375em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.531.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.532.1">Pick the box with the highest objectness score, </span><span class="koboSpan" id="kobo.533.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/326.png" style="vertical-align:-0.340em;height:0.858em;width:0.675em"/></span><span class="koboSpan" id="kobo.534.1">, from the </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">remaining boxes.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.536.1">Discard all boxes with IoU &gt;= 0.5 with the box we selected in the </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">previous step.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.538.1">Now that we are (hopefully) familiar with YOLO, let’s learn how to use it </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">in practice.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.540.1">Using Ultralytics YOLOv8</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.541.1">In this section, we’ll demonstrate</span><a id="_idIndexMarker685"/><span class="koboSpan" id="kobo.542.1"> how to use the YOLOv8 algorithm, developed by Ultralytics. </span><span class="koboSpan" id="kobo.542.2">For this example, you’ll need to install the </span><strong class="source-inline"><span class="koboSpan" id="kobo.543.1">ultralytics</span></strong><span class="koboSpan" id="kobo.544.1"> Python package. </span><span class="No-Break"><span class="koboSpan" id="kobo.545.1">Let’s start:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.546.1">Import the YOLO module. </span><span class="koboSpan" id="kobo.546.2">We’ll load a pre-trained </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">YOLOv8 model:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.548.1">
from ultralytics import YOLO
model = YOLO("yolov8n.pt")</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.549.1">Use </span><strong class="source-inline"><span class="koboSpan" id="kobo.550.1">model</span></strong><span class="koboSpan" id="kobo.551.1"> to detect the objects on a </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">Wikipedia image:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.553.1">
results = model.predict('https://raw.githubusercontent.com/ivan-vasilev/Python-Deep-Learning-3rd-Edition/m</span><a id="_idTextAnchor161"/><span class="koboSpan" id="kobo.554.1">ain/Chapter05/wikipedia-2011_FIA_GT1_Silverstone_2.jpg')</span></pre><p class="list-inset" lang="en-GB"><strong class="source-inline"><span class="koboSpan" id="kobo.555.1">results</span></strong><span class="koboSpan" id="kobo.556.1"> is a list, composed of a single instance of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.557.1">ultralytics.yolo.engine.results.Results</span></strong><span class="koboSpan" id="kobo.558.1"> class. </span><span class="koboSpan" id="kobo.558.2">The instance contains the list of detected objects: their bounding boxes, classes, and </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">objectness </span><a id="_idTextAnchor162"/><span class="koboSpan" id="kobo.560.1">scores.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.561.1">We can display the</span><a id="_idIndexMarker686"/><span class="koboSpan" id="kobo.562.1"> results with the help of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.563.1">results[0].plot()</span></strong><span class="koboSpan" id="kobo.564.1"> method, which overlays the detected object on the input image. </span><span class="koboSpan" id="kobo.564.2">The result of this operation is the first image, we introduced at the start of the </span><em class="italic"><span class="koboSpan" id="kobo.565.1">Introduction to object </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.566.1">detection</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.567.1"> section:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.568.1">
from PIL import Image
Image.fromarray</span><a id="_idTextAnchor163"/><span class="koboSpan" id="kobo.569.1">(results[0].plot()).show()</span></pre></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.570.1">This concludes our introduction to the YOLO family of single-shot object detection models. </span><span class="koboSpan" id="kobo.570.2">Next, we’ll focus on a popular example of a two-shot </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">detection algorithm.</span></span></p>
<h2 id="_idParaDest-102" lang="en-GB"><a id="_idTextAnchor164"/><span class="koboSpan" id="kobo.572.1">Object detection with Faster R-CNN</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.573.1">In this section, we’ll </span><a id="_idIndexMarker687"/><span class="koboSpan" id="kobo.574.1">discuss the </span><strong class="bold"><span class="koboSpan" id="kobo.575.1">Faster R-CNN</span></strong><span class="koboSpan" id="kobo.576.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.577.1">Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks</span></em><span class="koboSpan" id="kobo.578.1">, </span><a href="https://arxiv.org/abs/1506.01497"><span class="koboSpan" id="kobo.579.1">https://arxiv.org/abs/1506.01497</span></a><span class="koboSpan" id="kobo.580.1">) two-stage object </span><a id="_idIndexMarker688"/><span class="koboSpan" id="kobo.581.1">detection algorithm. </span><span class="koboSpan" id="kobo.581.2">It is an evolution of the earlier two-stage </span><a id="_idIndexMarker689"/><span class="koboSpan" id="kobo.582.1">detectors, </span><strong class="bold"><span class="koboSpan" id="kobo.583.1">Fast R-CNN</span></strong><span class="koboSpan" id="kobo.584.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.585.1">Fast R-CNN</span></em><span class="koboSpan" id="kobo.586.1">, </span><a href="https://arxiv.org/abs/1504.08083"><span class="koboSpan" id="kobo.587.1">https://arxiv.org/abs/1504.08083</span></a><span class="koboSpan" id="kobo.588.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.589.1">R-CNN</span></strong><span class="koboSpan" id="kobo.590.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.591.1">Rich feature hierarchies for accurate object detection and semantic </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.592.1">segmentation</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">, </span></span><a href="https://arxiv.org/abs/1311.2524"><span class="No-Break"><span class="koboSpan" id="kobo.594.1">https://arxiv.org/abs/1311.2524</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.595.1">).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.596.1">The general structure of the Faster R-CNN model is outlined in the </span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer436">
<span class="koboSpan" id="kobo.598.1"><img alt="Figure 5.8 – The structure of Faster R-CNN. Source: https://arxiv.org/abs/1506.01497" src="image/B19627_05_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.599.1">Figure 5.8 – The structure of Faster R-CNN. </span><span class="koboSpan" id="kobo.599.2">Source: </span><a href="https://arxiv.org/abs/1506.01497"><span class="koboSpan" id="kobo.600.1">https://arxiv.org/abs/1506.01497</span></a></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.601.1">Let’s keep this figure in</span><a id="_idIndexMarker690"/><span class="koboSpan" id="kobo.602.1"> mind while we explain the algorithm. </span><span class="koboSpan" id="kobo.602.2">Like YOLO, Faster R-CNN starts with a backbone classification network trained</span><a id="_idIndexMarker691"/><span class="koboSpan" id="kobo.603.1"> on ImageNet, which serves as a base for the different </span><a id="_idIndexMarker692"/><span class="koboSpan" id="kobo.604.1">modules of the model. </span><span class="koboSpan" id="kobo.604.2">Originally, the authors of the paper experimented with classic backbone </span><a id="_idIndexMarker693"/><span class="koboSpan" id="kobo.605.1">architectures, such as </span><strong class="bold"><span class="koboSpan" id="kobo.606.1">VGG-16</span></strong><span class="koboSpan" id="kobo.607.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.608.1">Very Deep Convolutional Networks for Large-Scale Image Recognition</span></em><span class="koboSpan" id="kobo.609.1">, </span><a href="https://arxiv.org/abs/1409.1556"><span class="koboSpan" id="kobo.610.1">https://arxiv.org/abs/1409.1556</span></a><span class="koboSpan" id="kobo.611.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.612.1">ZFNet</span></strong><span class="koboSpan" id="kobo.613.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.614.1">Visualizing and Understanding Convolutional Networks</span></em><span class="koboSpan" id="kobo.615.1">, </span><a href="https://arxiv.org/abs/1311.2901"><span class="koboSpan" id="kobo.616.1">https://arxiv.org/abs/1311.2901</span></a><span class="koboSpan" id="kobo.617.1">). </span><span class="koboSpan" id="kobo.617.2">Today, the</span><a id="_idIndexMarker694"/><span class="koboSpan" id="kobo.618.1"> model is available with more </span><a id="_idIndexMarker695"/><span class="koboSpan" id="kobo.619.1">contemporary backbones, such as ResNet </span><span class="No-Break"><span class="koboSpan" id="kobo.620.1">and MobileNet.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.621.1">Unlike YOLO, Faster R-CNN doesn’t have a neck module and only uses the feature maps of the last backbone convolutional layer as input to the next components of the algorithm. </span><span class="koboSpan" id="kobo.621.2">More specifically, the backbone</span><a id="_idIndexMarker696"/><span class="koboSpan" id="kobo.622.1"> serves as a backbone (get it?) to the two other components of the model (hence</span><a id="_idIndexMarker697"/><span class="koboSpan" id="kobo.623.1"> two-stage) – the </span><strong class="bold"><span class="koboSpan" id="kobo.624.1">region proposal network</span></strong><span class="koboSpan" id="kobo.625.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.626.1">RPN</span></strong><span class="koboSpan" id="kobo.627.1">) and the detection network. </span><span class="koboSpan" id="kobo.627.2">Let’s discuss the </span><span class="No-Break"><span class="koboSpan" id="kobo.628.1">RPN first.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.629.1">The region proposal network</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.630.1">In the first stage, the RPN takes an image (of any size) as input and outputs a set of rectangular RoI, where an object might be located. </span><span class="koboSpan" id="kobo.630.2">The RoI is equivalent to the bounding box in YOLO. </span><span class="koboSpan" id="kobo.630.3">The RPN itself is created by taking the first </span><em class="italic"><span class="koboSpan" id="kobo.631.1">p</span></em><span class="koboSpan" id="kobo.632.1"> convolutional layers of the backbone model (see the preceding diagram). </span><span class="koboSpan" id="kobo.632.2">Once the input image is propagated to the last shared convolutional layer, the algorithm takes the feature map of that layer and slides another small network over each location of the feature map. </span><span class="koboSpan" id="kobo.632.3">The small network outputs</span><a id="_idIndexMarker698"/><span class="koboSpan" id="kobo.633.1"> whether an object is present at any of the </span><em class="italic"><span class="koboSpan" id="kobo.634.1">k</span></em><span class="koboSpan" id="kobo.635.1"> anchor boxes (the concept of anchor box is the same as in YOLO), as well as the coordinates of its potential bounding box. </span><span class="koboSpan" id="kobo.635.2">This is illustrated on the left-hand side image</span><a id="_idIndexMarker699"/><span class="koboSpan" id="kobo.636.1"> of the following diagram, which shows a single location of the RPN sliding over a single feature map of the last </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">convolutional layer:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer437">
<span class="koboSpan" id="kobo.638.1"><img alt="Figure 5.9 – ﻿RPN proposals over a single location﻿. Source: https://arxiv.org/abs/1506.01497" src="image/B19627_05_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.639.1">Figure 5.9 – RPN proposals over a single location. </span><span class="koboSpan" id="kobo.639.2">Source: https://arxiv.org/abs/1506.01497</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.640.1">The small network takes an </span><em class="italic"><span class="koboSpan" id="kobo.641.1">n×n</span></em><span class="koboSpan" id="kobo.642.1"> region at the same location across all input feature maps as input </span><br/><span class="koboSpan" id="kobo.643.1">(</span><em class="italic"><span class="koboSpan" id="kobo.644.1">n = 3</span></em><span class="koboSpan" id="kobo.645.1"> according to the paper). </span><span class="koboSpan" id="kobo.645.2">For example, if the final convolutional layer has 512 feature maps, the small network’s input size at one location is 512*3*3 = 4608. </span><span class="koboSpan" id="kobo.645.3">The 512 3×3 feature maps are flattened to a 4,608-dimensional vector. </span><span class="koboSpan" id="kobo.645.4">It serves as input to a fully connected layer, which maps it to a lower dimensional (usually 512) vector. </span><span class="koboSpan" id="kobo.645.5">This vector itself serves as input to the following two parallel fully </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">connected layers:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.647.1">A classification layer with </span><em class="italic"><span class="koboSpan" id="kobo.648.1">2k</span></em><span class="koboSpan" id="kobo.649.1"> units organized into </span><em class="italic"><span class="koboSpan" id="kobo.650.1">k</span></em><span class="koboSpan" id="kobo.651.1"> 2-unit binary softmax outputs. </span><span class="koboSpan" id="kobo.651.2">Like YOLO, the output of each softmax represents the objectness score (in the [0, 1] range) of whether an object exists in each of the </span><em class="italic"><span class="koboSpan" id="kobo.652.1">k</span></em><span class="koboSpan" id="kobo.653.1"> anchor boxes. </span><span class="koboSpan" id="kobo.653.2">During training, an object is assigned to an anchor box based on the IoU formula in the same way as </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">in YOLO.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.655.1">A regression layer with </span><em class="italic"><span class="koboSpan" id="kobo.656.1">4k</span></em><span class="koboSpan" id="kobo.657.1"> units organized into </span><em class="italic"><span class="koboSpan" id="kobo.658.1">k</span></em><span class="koboSpan" id="kobo.659.1"> 4-unit RoI arrays. </span><span class="koboSpan" id="kobo.659.2">Like YOLO, the first array elements represent the coordinates of the RoI center in the [0:1] range relative to the whole image. </span><span class="koboSpan" id="kobo.659.3">The other</span><a id="_idIndexMarker700"/><span class="koboSpan" id="kobo.660.1"> two elements represent the height and width of the region, relative to the whole image (again, similar </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">to YOLO).</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.662.1">The authors of the paper experimented with three scales and three aspect ratios, resulting in nine possible anchor boxes over each location. </span><span class="koboSpan" id="kobo.662.2">The typical </span><em class="italic"><span class="koboSpan" id="kobo.663.1">H×W</span></em><span class="koboSpan" id="kobo.664.1"> size </span><a id="_idIndexMarker701"/><span class="koboSpan" id="kobo.665.1">of the final feature map is around 2,400, which results in 2,400*9 = 21,600 </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">anchor boxes.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.667.1">RPN as a cross-channel convolution</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.668.1">In theory, we slide the</span><a id="_idIndexMarker702"/><span class="koboSpan" id="kobo.669.1"> small network over the feature map of the last convolutional layer. </span><span class="koboSpan" id="kobo.669.2">However, the small network weights are shared along all locations. </span><span class="koboSpan" id="kobo.669.3">Because of this, the sliding can be implemented as a cross-channel convolution. </span><span class="koboSpan" id="kobo.669.4">Therefore, the network can produce output for all anchor boxes in a single image pass. </span><span class="koboSpan" id="kobo.669.5">This is an improvement over Fast R-CNN, which requires a separate network pass for each </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">anchor box.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.671.1">The RPN is trained with backpropagation and stochastic gradient descent (what a surprise!). </span><span class="koboSpan" id="kobo.671.2">The weights of the shared convolutional layers are initialized with the pre-trained weights of </span><a id="_idTextAnchor165"/><span class="koboSpan" id="kobo.672.1">the backbone network and the rest are initialized r</span><a id="_idTextAnchor166"/><span class="koboSpan" id="kobo.673.1">andomly. </span><span class="koboSpan" id="kobo.673.2">The samples of each mini-batch are extracted from a single image. </span><span class="koboSpan" id="kobo.673.3">Each mini-batch contains an equal number of positive (objects) and negative (background) anchor boxes. </span><span class="koboSpan" id="kobo.673.4">There are two kinds of anchors with positive labels: the anchor/anchors with the highest IoU overlap with a ground truth box and an anchor that has an IoU overlap of higher than 0.7 with any ground truth box. </span><span class="koboSpan" id="kobo.673.5">If the IoU ratio of an anchor is lower than 0.3, the box is assigned a negative label. </span><span class="koboSpan" id="kobo.673.6">Anchors that are neither positive nor negative do not participate in </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">the training.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.675.1">As the RPN has two output layers (classification and regression), the training uses the following composite cost</span><a id="_idIndexMarker703"/><span class="koboSpan" id="kobo.676.1"> function with classification (</span><span class="koboSpan" id="kobo.677.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/333.png" style="vertical-align:-0.340em;height:0.988em;width:1.061em"/></span><span class="koboSpan" id="kobo.678.1">) and regression (</span><span class="koboSpan" id="kobo.679.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/334.png" style="vertical-align:-0.483em;height:1.131em;width:1.155em"/></span><span class="koboSpan" id="kobo.680.1">) </span><span class="No-Break"><span class="koboSpan" id="kobo.681.1">parts:</span></span><a id="_idTextAnchor167"/></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.682.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfenced open=&quot;{&quot; close=&quot;}&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;{&quot; close=&quot;}&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:munder underaccent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;λ&lt;/mml:mi&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;*&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;_&lt;/mml:mo&gt;&lt;/mml:munder&gt;&lt;/mml:math&gt;" src="image/335.png" style="vertical-align:-0.959em;height:2.000em;width:20.679em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.683.1">Let’s discuss </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">its components:</span></span></p>
<ul>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.685.1">i</span></em><span class="koboSpan" id="kobo.686.1">: The index of the anchor in </span><span class="No-Break"><span class="koboSpan" id="kobo.687.1">the mini-batch.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.688.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/336.png" style="vertical-align:-0.340em;height:0.858em;width:0.648em"/></span><span class="koboSpan" id="kobo.689.1">: The classification output, which represents the predicted objectness score of an anchor, </span><em class="italic"><span class="koboSpan" id="kobo.690.1">i</span></em><span class="koboSpan" id="kobo.691.1">, being an object or background. </span><span class="koboSpan" id="kobo.692.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;*&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/337.png" style="vertical-align:-0.340em;height:1.060em;width:0.799em"/></span><span class="koboSpan" id="kobo.693.1"> is the target data for the same (0 </span><span class="No-Break"><span class="koboSpan" id="kobo.694.1">or 1).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.695.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/338.png" style="vertical-align:-0.340em;height:0.932em;width:0.548em"/></span><span class="koboSpan" id="kobo.696.1">: The regression output</span><a id="_idIndexMarker704"/><span class="koboSpan" id="kobo.697.1"> vector with size 4, which represents the </span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">RoI parameters.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.699.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;*&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/339.png" style="vertical-align:-0.340em;height:1.060em;width:0.624em"/></span><span class="koboSpan" id="kobo.700.1">: The target vector for </span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">the same.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.702.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/333.png" style="vertical-align:-0.340em;height:0.988em;width:1.061em"/></span><span class="koboSpan" id="kobo.703.1">: A cross-entropy loss for the </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">classification layer.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.705.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/341.png" style="vertical-align:-0.340em;height:0.988em;width:1.234em"/></span><span class="koboSpan" id="kobo.706.1">: A normalization term, equal to the </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">mini-batch size.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.708.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/342.png" style="vertical-align:-0.483em;height:1.131em;width:1.203em"/></span><span class="koboSpan" id="kobo.709.1">: The regression loss, </span><span class="koboSpan" id="kobo.710.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;L&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;R&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;*&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/343.png" style="vertical-align:-0.483em;height:1.253em;width:6.717em"/></span><span class="koboSpan" id="kobo.711.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.712.1">R</span></em><span class="koboSpan" id="kobo.713.1"> is the mean absolute </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">error (</span></span><a href="https://en.wikipedia.org/wiki/Mean_absolute_error"><span class="No-Break"><span class="koboSpan" id="kobo.715.1">https://en.wikipedia.org/wiki/Mean_absolute_error</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.716.1">).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.717.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/344.png" style="vertical-align:-0.483em;height:1.131em;width:1.328em"/></span><span class="koboSpan" id="kobo.718.1">: A normalization term equal to the total number of anchor locations (</span><span class="No-Break"><span class="koboSpan" id="kobo.719.1">around 2400).</span></span></li>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.720.1">λ</span></em><span class="koboSpan" id="kobo.721.1">: This helps combine the classification and regression components of the cost function. </span><span class="koboSpan" id="kobo.721.2">Since </span><span class="koboSpan" id="kobo.722.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;mml:mi&gt;e&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∼&lt;/mml:mo&gt;&lt;mml:mn&gt;2400&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/345.png" style="vertical-align:-0.483em;height:1.131em;width:4.893em"/></span><span class="koboSpan" id="kobo.723.1"> and </span><span class="koboSpan" id="kobo.724.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;256&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/346.png" style="vertical-align:-0.340em;height:0.988em;width:4.214em"/></span><span class="koboSpan" id="kobo.725.1">, </span><em class="italic"><span class="koboSpan" id="kobo.726.1">λ</span></em><span class="koboSpan" id="kobo.727.1"> is set to 10 to preserve the balance between the </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">two losses.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.729.1">Now that we’ve discussed the RPN, let’s focus on the </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">detection network.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.731.1">Detection network</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.732.1">Let’s go back to the diagram that was shown at the beginning of the </span><em class="italic"><span class="koboSpan" id="kobo.733.1">Object detection with Faster R-CNN</span></em><span class="koboSpan" id="kobo.734.1"> section. </span><span class="koboSpan" id="kobo.734.2">Recall that</span><a id="_idIndexMarker705"/><span class="koboSpan" id="kobo.735.1"> in the first stage, the RPN has already generated the RoI coordinates and their objectness scores. </span><span class="koboSpan" id="kobo.735.2">The detection network is a regular classifier, which determines the class of objects in the current RoI. </span><span class="koboSpan" id="kobo.735.3">Both the RPN and the detection network share their first convolutional layers, borrowed from the backbone network. </span><span class="koboSpan" id="kobo.735.4">In addition, the detection network incorporates the proposed regions from the RPN, along with the feature maps of the last </span><span class="No-Break"><span class="koboSpan" id="kobo.736.1">shared layer.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.737.1">But how do we combine the backbone feature maps and the proposed regions in a unified input format? </span><span class="koboSpan" id="kobo.737.2">We can</span><a id="_idIndexMarker706"/><span class="koboSpan" id="kobo.738.1"> do this with the help of </span><strong class="bold"><span class="koboSpan" id="kobo.739.1">RoI pooling</span></strong><span class="koboSpan" id="kobo.740.1">, which is the first layer of the second part of the </span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">detection network:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer452">
<span class="koboSpan" id="kobo.742.1"><img alt="Figure 5.10 – An example of 2×2 RoI pooling with a 10×7 feature map and a 5×5 RoI (bold rectangle)" src="image/B19627_05_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.743.1">Figure 5.10 – An example of 2×2 RoI pooling with a 10×7 feature map and a 5×5 RoI (bold rectangle)</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.744.1">To understand how RoI pooling works, let’s assume that we have a single 10×7 feature map and a single RoI. </span><span class="koboSpan" id="kobo.744.2">As we learned in the </span><em class="italic"><span class="koboSpan" id="kobo.745.1">Region proposal network</span></em><span class="koboSpan" id="kobo.746.1"> section, a RoI is defined by its center coordinates, width, and height. </span><span class="koboSpan" id="kobo.746.2">The RoI pooling first converts these parameters into actual coordinates on the feature map. </span><span class="koboSpan" id="kobo.746.3">In this example, the region size is </span><em class="italic"><span class="koboSpan" id="kobo.747.1">h×w = 5×5</span></em><span class="koboSpan" id="kobo.748.1">. </span><span class="koboSpan" id="kobo.748.2">The RoI pooling is further defined by its output height and width, </span><em class="italic"><span class="koboSpan" id="kobo.749.1">H</span></em><span class="koboSpan" id="kobo.750.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.751.1">W</span></em><span class="koboSpan" id="kobo.752.1">. </span><span class="koboSpan" id="kobo.752.2">In this example, </span><em class="italic"><span class="koboSpan" id="kobo.753.1">H×W = 2×2</span></em><span class="koboSpan" id="kobo.754.1">, but in practice, the values could be larger, such as 7×7. </span><span class="koboSpan" id="kobo.754.2">The operation splits the </span><em class="italic"><span class="koboSpan" id="kobo.755.1">h×w</span></em><span class="koboSpan" id="kobo.756.1"> RoI into a grid of subregions with different sizes (displayed in the figure with different background colors). </span><span class="koboSpan" id="kobo.756.2">Once this is done, each subregion is downsampled to a single output cell by taking the maximum value of that region. </span><span class="koboSpan" id="kobo.756.3">In other words, RoI pooling can transform inputs with arbitrary sizes into a fixed-size output window. </span><span class="koboSpan" id="kobo.756.4">In this way, the transformed data can propagate through the network in a </span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">consistent format.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.758.1">As we mentioned in the </span><em class="italic"><span class="koboSpan" id="kobo.759.1">Object detection with Faster R-CNN</span></em><span class="koboSpan" id="kobo.760.1"> section, the RPN and the detection network share their initial layers. </span><span class="koboSpan" id="kobo.760.2">However, they start their lives as separate networks. </span><span class="koboSpan" id="kobo.760.3">The training alternates between the two in a </span><span class="No-Break"><span class="koboSpan" id="kobo.761.1">four-step process:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.762.1">Train the RPN, which is initialized with the ImageNet weights of </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">the backbone.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.764.1">Train the detection network, using the proposals from the freshly trained RPN from </span><em class="italic"><span class="koboSpan" id="kobo.765.1">step 1</span></em><span class="koboSpan" id="kobo.766.1">. </span><span class="koboSpan" id="kobo.766.2">The training also starts with the weights of the ImageNet backbone. </span><span class="koboSpan" id="kobo.766.3">At this point, the two networks don’t </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">share weights.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.768.1">Use the detection network shared layers to initialize the weights of the RPN. </span><span class="koboSpan" id="kobo.768.2">Then, train the RPN again, but freeze the shared layers and fine-tune the RPN-specific layers only. </span><span class="koboSpan" id="kobo.768.3">The two networks share their </span><span class="No-Break"><span class="koboSpan" id="kobo.769.1">weights now.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.770.1">Train the detection </span><a id="_idIndexMarker707"/><span class="koboSpan" id="kobo.771.1">network by freezing the shared layers and fine-tuning the detection-net-specific </span><span class="No-Break"><span class="koboSpan" id="kobo.772.1">layers only.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.773.1">Now that we’ve introduced Faster R-CNN, let’s discuss how to use it in practice with the help of a pre-trained </span><span class="No-Break"><span class="koboSpan" id="kobo.774.1">PyTorch model.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.775.1">Using Faster R-CNN with PyTorch</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.776.1">In this section, we’ll use a pre-trained</span><a id="_idIndexMarker708"/><span class="koboSpan" id="kobo.777.1"> PyTorch Faster R-CNN model with a ResNet50 backbone for </span><a id="_idIndexMarker709"/><span class="koboSpan" id="kobo.778.1">object detection. </span><span class="koboSpan" id="kobo.778.2">PyTorch has out-of-the-box support for Faster R-CNN, which makes it easy for us to use. </span><span class="koboSpan" id="kobo.778.3">This example is implemented with PyTorch. </span><span class="koboSpan" id="kobo.778.4">In addition, it uses the </span><strong class="source-inline"><span class="koboSpan" id="kobo.779.1">torchvision</span></strong><span class="koboSpan" id="kobo.780.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.781.1">opencv-python</span></strong><span class="koboSpan" id="kobo.782.1"> packages. </span><span class="koboSpan" id="kobo.782.2">We</span><a id="_idIndexMarker710"/><span class="koboSpan" id="kobo.783.1"> will only include the relevant parts of the code, but you can find the full version in this book’s GitHub repository. </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">Let’s start:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.785.1">Load the pre-trained model with the latest available weights. </span><span class="koboSpan" id="kobo.785.2">Ensure this by using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.786.1">DEFAULT</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.787.1"> option:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.788.1">
from torchvision.models.detection import \
FasterRCNN_ResNet50_FPN_V2_Weights, \
fasterrcnn_resnet50_fpn_v2
model = fasterrcnn_resnet50_fpn_v2(
    weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)</span><a id="_idTextAnchor168"/></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.789.1">We are going to use the model for inference and not for training, so we’ll enable the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.790.1">eval()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.791.1"> mode:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.792.1">
model.eval()</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.793.1">Use </span><strong class="source-inline"><span class="koboSpan" id="kobo.794.1">opencv-python</span></strong><span class="koboSpan" id="kobo.795.1"> to read the RGB image located at </span><strong class="source-inline"><span class="koboSpan" id="kobo.796.1">image_file_path</span></strong><span class="koboSpan" id="kobo.797.1">. </span><span class="koboSpan" id="kobo.797.2">We’ll omit the code, which downloads the image from this book’s repository if it doesn’t already </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">exist locally:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.799.1">
import cv2
img = cv2.imread(image_file_path)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.800.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.801.1">img</span></strong><span class="koboSpan" id="kobo.802.1"> is a three-dimensional </span><strong class="source-inline"><span class="koboSpan" id="kobo.803.1">numpy</span></strong><span class="koboSpan" id="kobo.804.1"> array </span><span class="No-Break"><span class="koboSpan" id="kobo.805.1">of integers.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.806.1">Implement the</span><a id="_idIndexMarker711"/><span class="koboSpan" id="kobo.807.1"> single-step image pre-processing pipeline. </span><span class="koboSpan" id="kobo.807.2">It</span><a id="_idIndexMarker712"/><span class="koboSpan" id="kobo.808.1"> transforms the </span><strong class="source-inline"><span class="koboSpan" id="kobo.809.1">img</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.810.1">numpy</span></strong><span class="koboSpan" id="kobo.811.1"> array into </span><strong class="source-inline"><span class="koboSpan" id="kobo.812.1">torch.Tensor</span></strong><span class="koboSpan" id="kobo.813.1">, which will serve as input to </span><span class="No-Break"><span class="koboSpan" id="kobo.814.1">the model:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.815.1">
import torchvision.transforms as transforms
transform = transforms.ToTensor()</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.816.1">Run the </span><span class="No-Break"><span class="koboSpan" id="kobo.817.1">detection </span></span><span class="No-Break"><a id="_idIndexMarker713"/></span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">mod</span><a id="_idTextAnchor169"/><span class="koboSpan" id="kobo.819.1">el:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.820.1">
nn_input = transform(img)
detected_objects = model([nn_input])</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.821.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.822.1">detected_objects</span></strong><span class="koboSpan" id="kobo.823.1"> is a dictionary with </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">three items:</span></span></p><ul><li lang="en-GB"><strong class="source-inline"><span class="koboSpan" id="kobo.825.1">boxes</span></strong><span class="koboSpan" id="kobo.826.1">: A list of bounding boxes, represented by their top-left and bottom-right </span><span class="No-Break"><span class="koboSpan" id="kobo.827.1">pixel coordinates</span></span></li><li lang="en-GB"><strong class="source-inline"><span class="koboSpan" id="kobo.828.1">labels</span></strong><span class="koboSpan" id="kobo.829.1">: A list of labels for each </span><span class="No-Break"><span class="koboSpan" id="kobo.830.1">detected object</span></span></li><li lang="en-GB"><strong class="source-inline"><span class="koboSpan" id="kobo.831.1">scores</span></strong><span class="koboSpan" id="kobo.832.1">: A list of objectness scores for each </span><span class="No-Break"><span class="koboSpan" id="kobo.833.1">detected object</span></span></li></ul></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.834.1">Use the initial </span><strong class="source-inline"><span class="koboSpan" id="kobo.835.1">img</span></strong><span class="koboSpan" id="kobo.836.1"> array and </span><strong class="source-inline"><span class="koboSpan" id="kobo.837.1">detected_objects</span></strong><span class="koboSpan" id="kobo.838.1"> as parameters for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.839.1">draw_bboxes</span></strong><span class="koboSpan" id="kobo.840.1"> function, which overlays the bounding boxes and their labels on the original input image (the implementation of </span><strong class="source-inline"><span class="koboSpan" id="kobo.841.1">draw_bboxes</span></strong><span class="koboSpan" id="kobo.842.1"> is available in the </span><span class="No-Break"><span class="koboSpan" id="kobo.843.1">full example):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.844.1">
draw_bboxes(img, detected_objects)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.845.1">Display the result </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.847.1">opencv-python</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.848.1">:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.849.1">
cv2.imshow("Object detection", img)
cv2.waitKey()</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.850.1">The output image</span><a id="_idIndexMarker714"/><span class="koboSpan" id="kobo.851.1"> looks </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">like this:</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer453">
<span class="koboSpan" id="kobo.853.1"><img alt="Figure 5.11 – Object detection with Faster R-CNN" src="image/B19627_05_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.854.1">Figure 5.11 – Object detection with Faster R-CNN</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.855.1">We’re now familiar</span><a id="_idIndexMarker715"/><span class="koboSpan" id="kobo.856.1"> with two of the most popular object detection </span><a id="_idIndexMarker716"/><span class="koboSpan" id="kobo.857.1">algorithms. </span><span class="koboSpan" id="kobo.857.2">In the next section, we’ll focus on the next major computer vision task, called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.858.1">image segmentation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.859.1">.</span></span></p>
<h1 id="_idParaDest-103" lang="en-GB"><a id="_idTextAnchor170"/><span class="koboSpan" id="kobo.860.1">Introducing image segmentation</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.861.1">Image segmentation is the process</span><a id="_idIndexMarker717"/><span class="koboSpan" id="kobo.862.1"> of assigning a class label (such as person, bicycle, or animal) to each pixel of an image. </span><span class="koboSpan" id="kobo.862.2">You can think of it as classification but on a pixel level – instead of classifying the entire image under one label, we’ll classify each pixel separately. </span><span class="koboSpan" id="kobo.862.3">The output of an image segmentation operation is known as a </span><strong class="bold"><span class="koboSpan" id="kobo.863.1">segmentation mask</span></strong><span class="koboSpan" id="kobo.864.1">. </span><span class="koboSpan" id="kobo.864.2">It is a</span><a id="_idIndexMarker718"/><span class="koboSpan" id="kobo.865.1"> tensor with the same dimensions as the original input image, but instead of color, each pixel is represented by the class of object, to which it belongs. </span><span class="koboSpan" id="kobo.865.2">There are two types </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">of segmentation:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.867.1">Semantic segmentation</span></strong><span class="koboSpan" id="kobo.868.1">: This assigns a class to each pixel but doesn’t differentiate between object instances. </span><span class="koboSpan" id="kobo.868.2">For </span><a id="_idIndexMarker719"/><span class="koboSpan" id="kobo.869.1">example, the middle image in the following figure shows a semantic segmentation mask, where the pixels of each separate vehicle have the same value. </span><span class="koboSpan" id="kobo.869.2">Semantic segmentation can tell us that a pixel is part of a vehicle but cannot make a distinction between </span><span class="No-Break"><span class="koboSpan" id="kobo.870.1">two vehicles.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.871.1">Instance segmentation</span></strong><span class="koboSpan" id="kobo.872.1">: This assigns a class </span><a id="_idIndexMarker720"/><span class="koboSpan" id="kobo.873.1">to each pixel and differentiates between object instances. </span><span class="koboSpan" id="kobo.873.2">For </span><a id="_idIndexMarker721"/><span class="koboSpan" id="kobo.874.1">example, the image on the right in the following figure shows an instance segmentation mask, where each vehicle is segmented as a </span><span class="No-Break"><span class="koboSpan" id="kobo.875.1">separate object.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.876.1">The following figure shows an example of semantic and </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">instance segmentation:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer454">
<span class="koboSpan" id="kobo.878.1"><img alt="Figure 5.12 – Left: input image; middle: semantic segmentation mask; right: instance segmentation mask. Source: http://sceneparsing.csail.mit.edu/" src="image/B19627_05_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.879.1">Figure 5.12 – Left: input image; middle: semantic segmentation mask; right: instance segmentation mask. </span><span class="koboSpan" id="kobo.879.2">Source: http://sceneparsing.csail.mit.edu/</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.880.1">To train a segmentation algorithm, we’ll need a special type of ground truth data, where the labels of each image are the segmented version of </span><span class="No-Break"><span class="koboSpan" id="kobo.881.1">the image.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.882.1">The easiest way to segment an image is by using the familiar sliding-window technique, which we described in the </span><em class="italic"><span class="koboSpan" id="kobo.883.1">Approaches to object detection</span></em><span class="koboSpan" id="kobo.884.1"> section – that is, we’ll use a regular classifier, and we’ll slide it in either direction with stride 1. </span><span class="koboSpan" id="kobo.884.2">After we get the prediction for a location, we’ll take the pixel that lies in the middle of the input region, and we’ll assign it to the predicted class. </span><span class="koboSpan" id="kobo.884.3">Predictably, this approach is very slow because of the large number</span><a id="_idIndexMarker722"/><span class="koboSpan" id="kobo.885.1"> of pixels in an image (even a 1,024×1,024 image has more than 1 million pixels). </span><span class="koboSpan" id="kobo.885.2">Thankfully, there are faster and more accurate algorithms, which we’ll discuss in the </span><span class="No-Break"><span class="koboSpan" id="kobo.886.1">following sections.</span></span></p>
<h2 id="_idParaDest-104" lang="en-GB"><a id="_idTextAnchor171"/><span class="koboSpan" id="kobo.887.1">Semantic segmentation with U-Net</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.888.1">The first approach to </span><a id="_idIndexMarker723"/><span class="koboSpan" id="kobo.889.1">segmentation we’ll discuss is</span><a id="_idIndexMarker724"/><span class="koboSpan" id="kobo.890.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.891.1">U-Net</span></strong><span class="koboSpan" id="kobo.892.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.893.1">U-Net: Convolutional Networks for Biomedical Image Segmentation</span></em><span class="koboSpan" id="kobo.894.1">, </span><a href="https://arxiv.org/abs/1505.04597"><span class="koboSpan" id="kobo.895.1">https://arxiv.org/abs/1505.04597</span></a><span class="koboSpan" id="kobo.896.1">). </span><span class="koboSpan" id="kobo.896.2">The name comes from the visualization of the </span><span class="No-Break"><span class="koboSpan" id="kobo.897.1">network architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer455">
<span class="koboSpan" id="kobo.898.1"><img alt="Figure 5.13 – The U-Net architecture. Source: https://arxiv.org/abs/1505.04597" src="image/B19627_05_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.899.1">Figure 5.13 – The U-Net architecture. </span><span class="koboSpan" id="kobo.899.2">Source: https://arxiv.org/abs/1505.04597</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.900.1">U-Net is a type of </span><strong class="bold"><span class="koboSpan" id="kobo.901.1">fully convolutional network</span></strong><span class="koboSpan" id="kobo.902.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.903.1">FCN</span></strong><span class="koboSpan" id="kobo.904.1">), called so</span><a id="_idIndexMarker725"/><span class="koboSpan" id="kobo.905.1"> because it contains only convolutional layers and doesn’t use any fully connected layers at its output. </span><span class="koboSpan" id="kobo.905.2">An FCN takes the whole image </span><a id="_idIndexMarker726"/><span class="koboSpan" id="kobo.906.1">as input and outputs its segmentation map in a single pass. </span><span class="koboSpan" id="kobo.906.2">To better understand </span><a id="_idIndexMarker727"/><span class="koboSpan" id="kobo.907.1">this architecture, let’s clarify the figure </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">notations first:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.909.1">The horizontal dark blue arrows correspond to 3×3 cross-channel convolutions with ReLU activation. </span><span class="koboSpan" id="kobo.909.2">The single light blue arrow at the end of the model represents a 1×1 bottleneck convolution to reduce the number </span><span class="No-Break"><span class="koboSpan" id="kobo.910.1">of channels.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.911.1">All feature maps are denoted with blue boxes. </span><span class="koboSpan" id="kobo.911.2">The number of feature maps is on top of the box, and the feature map’s size is at the lower-left edge of </span><span class="No-Break"><span class="koboSpan" id="kobo.912.1">the box.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.913.1">The horizontal gray arrows represent copy and crop operation (more on </span><span class="No-Break"><span class="koboSpan" id="kobo.914.1">that later).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.915.1">The red vertical arrows represent 2×2 max </span><span class="No-Break"><span class="koboSpan" id="kobo.916.1">pooling operations.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.917.1">The vertical green arrows </span><a id="_idIndexMarker728"/><span class="koboSpan" id="kobo.918.1">represent 2×2 up-convolutions (or transposed convolutions; see </span><a href="B19627_04.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.919.1">Chapter 4</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.920.1">).</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.921.1">We can separate the </span><a id="_idIndexMarker729"/><span class="koboSpan" id="kobo.922.1">U-Net model into two virtual components (in reality, this is just a </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">single network):</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.924.1">Encoder</span></strong><span class="koboSpan" id="kobo.925.1">: The first part of the network (the left part of the </span><em class="italic"><span class="koboSpan" id="kobo.926.1">U</span></em><span class="koboSpan" id="kobo.927.1">) is similar to a regular CNN but without the </span><a id="_idIndexMarker730"/><span class="koboSpan" id="kobo.928.1">fully connected layers at the end. </span><span class="koboSpan" id="kobo.928.2">Its role is to learn highly abstract representations of the input image (nothing new here). </span><span class="koboSpan" id="kobo.928.3">The input image itself can be an arbitrary size, so long as the input feature maps of every max pooling operation have even (and not odd) dimensions. </span><span class="koboSpan" id="kobo.928.4">Otherwise, the output segmentation mask will be distorted. </span><span class="koboSpan" id="kobo.928.5">By default, the input size is 572×572. </span><span class="koboSpan" id="kobo.928.6">From there, it continues like a regular CNN with alternating convolutional and max pooling layers. </span><span class="koboSpan" id="kobo.928.7">The encoder consists of four identical blocks of two consecutive valid (unpadded) </span><br/><span class="koboSpan" id="kobo.929.1">cross-channel 3×3 convolutions with stride 1, optional batch normalization, ReLU activations, and a 2×2 max pooling layer. </span><span class="koboSpan" id="kobo.929.2">Each downsampling step doubles the number of feature maps. </span><span class="koboSpan" id="kobo.929.3">The final encoder convolution ends with 1,024 28×28 </span><span class="No-Break"><span class="koboSpan" id="kobo.930.1">feature maps.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.931.1">Decoder</span></strong><span class="koboSpan" id="kobo.932.1">: The second part of the </span><a id="_idIndexMarker731"/><span class="koboSpan" id="kobo.933.1">network (the right part of the </span><em class="italic"><span class="koboSpan" id="kobo.934.1">U</span></em><span class="koboSpan" id="kobo.935.1">) is symmetrical to the encoder. </span><span class="koboSpan" id="kobo.935.2">The decoder takes the innermost 28×28 encoder feature maps and simultaneously upsamples and converts them into a 388×388 segmentation map. </span><span class="koboSpan" id="kobo.935.3">It contains four identical </span><span class="No-Break"><span class="koboSpan" id="kobo.936.1">upsampling blocks:</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.937.1">The upsampling works with 2×2 transposed cross-channel convolutions with </span><span class="No-Break"><span class="koboSpan" id="kobo.938.1">stride 2.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.939.1">The output of each upsampling step is concatenated with the cropped high-resolution feature maps of the corresponding encoder step (gray horizontal arrows). </span><span class="koboSpan" id="kobo.939.2">The cropping is necessary because of the loss of border pixels in every unpadded encoder and </span><span class="No-Break"><span class="koboSpan" id="kobo.940.1">decoder convolution.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.941.1">Each transposed convolution is followed by two regular convolutions to smooth the </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">expanded representation.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.943.1">The upsampling steps halve the number of feature maps. </span><span class="koboSpan" id="kobo.943.2">The final output uses a 1×1 bottleneck convolution to map the 64-component feature map tensor to the desired number of classes (light blue arrow). </span><span class="koboSpan" id="kobo.943.3">The authors of the paper have demonstrated the binary segmentation of medical images </span><span class="No-Break"><span class="koboSpan" id="kobo.944.1">of cells.</span></span></li></ul></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.945.1">The network’s output is a softmax over each pixel of the segmentation mask – that is, the output contains as many independent softmax operations as the number of pixels. </span><span class="koboSpan" id="kobo.945.2">The softmax output for one pixel determines the pixel class. </span><span class="koboSpan" id="kobo.945.3">U-Net is trained like a regular classification network. </span><span class="koboSpan" id="kobo.945.4">However, the </span><a id="_idIndexMarker732"/><span class="koboSpan" id="kobo.946.1">cost function is a combination of the cross-entropy losses of the softmax outputs over </span><span class="No-Break"><span class="koboSpan" id="kobo.947.1">all pixels.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.948.1">We can see that because </span><a id="_idIndexMarker733"/><span class="koboSpan" id="kobo.949.1">of the unpadded convolutions of the network, the output segmentation map is smaller than the input image (388 versus 572). </span><span class="koboSpan" id="kobo.949.2">However, the output map is not a rescaled version of the input image. </span><span class="koboSpan" id="kobo.949.3">Instead, it has a one-to-one scale compared to the input, but only covers the central part of the input tile. </span><span class="koboSpan" id="kobo.949.4">This is illustrated in the </span><span class="No-Break"><span class="koboSpan" id="kobo.950.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer456">
<span class="koboSpan" id="kobo.951.1"><img alt="Figure 5.14 – An overlap-tile strategy for segmenting large images. Source: https://arxiv.org/abs/1505.04597" src="image/B19627_05_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.952.1">Figure 5.14 – An overlap-tile strategy for segmenting large images. </span><span class="koboSpan" id="kobo.952.2">Source: https://arxiv.org/abs/1505.04597</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.953.1">The unpadded convolutions are necessary so that the network doesn’t produce noisy artifacts at the borders of the segmentation map. </span><span class="koboSpan" id="kobo.953.2">This makes it possible to segment images with arbitrary large sizes</span><a id="_idIndexMarker734"/><span class="koboSpan" id="kobo.954.1"> using the so-called overlap-tile strategy. </span><span class="koboSpan" id="kobo.954.2">The input image is split into overlapping</span><a id="_idIndexMarker735"/><span class="koboSpan" id="kobo.955.1"> input tiles, like the one shown on the left of the preceding figure. </span><span class="koboSpan" id="kobo.955.2">The segmentation </span><a id="_idIndexMarker736"/><span class="koboSpan" id="kobo.956.1">map of the small light area in the image on the right requires the large light area (one tile) on the left image </span><span class="No-Break"><span class="koboSpan" id="kobo.957.1">as input.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.958.1">The next input tile overlaps with the previous one in such a way that their segmentation maps cover adjacent areas of the image. </span><span class="koboSpan" id="kobo.958.2">To predict the pixels in the border region of the image, the missing context is extrapolated by mirroring the </span><span class="No-Break"><span class="koboSpan" id="kobo.959.1">input image.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.960.1">We’re not going to implement a code example with U-Net, but you can check out </span><a href="https://github.com/mateuszbuda/brain-segmentation-pytorch"><span class="koboSpan" id="kobo.961.1">https://github.com/mateuszbuda/brain-segmentation-pytorch</span></a><span class="koboSpan" id="kobo.962.1"> for U-Net brain MRI </span><span class="No-Break"><span class="koboSpan" id="kobo.963.1">image segmentation.</span></span></p>
<h2 id="_idParaDest-105" lang="en-GB"><a id="_idTextAnchor172"/><span class="koboSpan" id="kobo.964.1">Instance segmentation with Mask R-CNN</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.965.1">Mask R-CNN (</span><a href="https://arxiv.org/abs/1703.06870"><span class="koboSpan" id="kobo.966.1">https://arxiv.org/abs/1703.06870</span></a><span class="koboSpan" id="kobo.967.1">) is an extension of Faster R-CNN for instance segmentation. </span><span class="koboSpan" id="kobo.967.2">Faster</span><a id="_idIndexMarker737"/><span class="koboSpan" id="kobo.968.1"> R-CNN has two outputs for each candidate object: bounding box parameters and class labels. </span><span class="koboSpan" id="kobo.968.2">In addition to</span><a id="_idIndexMarker738"/><span class="koboSpan" id="kobo.969.1"> these, Mask R-CNN adds a third output – an FCN that produces a binary segmentation mask for each RoI. </span><span class="koboSpan" id="kobo.969.2">The following diagram shows the structure of </span><span class="No-Break"><span class="koboSpan" id="kobo.970.1">Mask R-CNN:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer457">
<span class="koboSpan" id="kobo.971.1"><img alt="Figure 5.15 – Mask R-CNN structure" src="image/B19627_05_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.972.1">Figure 5.15 – Mask R-CNN structure</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.973.1">The segmentation and detection paths both use the RoI predictions of the RPN but are otherwise independent and </span><em class="italic"><span class="koboSpan" id="kobo.974.1">parallel</span></em><span class="koboSpan" id="kobo.975.1"> to each other. </span><span class="koboSpan" id="kobo.975.2">The segmentation path produces </span><em class="italic"><span class="koboSpan" id="kobo.976.1">I</span></em> <em class="italic"><span class="koboSpan" id="kobo.977.1">m×m</span></em><span class="koboSpan" id="kobo.978.1"> segmentation masks, one for each of the </span><em class="italic"><span class="koboSpan" id="kobo.979.1">I</span></em><span class="koboSpan" id="kobo.980.1"> RoIs. </span><span class="koboSpan" id="kobo.980.2">Since the detection path handles the classification of the object, the segmentation mask is </span><em class="italic"><span class="koboSpan" id="kobo.981.1">binary</span></em><span class="koboSpan" id="kobo.982.1"> and independent of the object class. </span><span class="koboSpan" id="kobo.982.2">The segmented pixels are automatically assigned to the class produced by the detection path. </span><span class="koboSpan" id="kobo.982.3">T</span><a id="_idTextAnchor173"/><span class="koboSpan" id="kobo.983.1">his is opposed to other algorithms, such as U-Net, where the segmentation is combined with classification and an individual softmax is applied at each pixel. </span><span class="koboSpan" id="kobo.983.2">At training or inference, only the mask related to the predicted object of the classification path is considered; the rest </span><span class="No-Break"><span class="koboSpan" id="kobo.984.1">are discarded.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.985.1">Mask R-CNN replaces the RoI max </span><a id="_idIndexMarker739"/><span class="koboSpan" id="kobo.986.1">pooling operation with a more accurate RoI align layer. </span><span class="koboSpan" id="kobo.986.2">The RPN outputs </span><a id="_idIndexMarker740"/><span class="koboSpan" id="kobo.987.1">the anchor box center and its height and width as four floating-point numbers. </span><span class="koboSpan" id="kobo.987.2">Then, the RoI pooling layer translates them into integer feature map cell coordinates (quantization). </span><span class="koboSpan" id="kobo.987.3">Additionally, the division of the RoI to </span><em class="italic"><span class="koboSpan" id="kobo.988.1">H×W</span></em><span class="koboSpan" id="kobo.989.1"> bins (the same size as the RoI pooling regions) also involves quantization. </span><span class="koboSpan" id="kobo.989.2">The RoI example from the </span><em class="italic"><span class="koboSpan" id="kobo.990.1">Object detection with Faster R-CNN</span></em><span class="koboSpan" id="kobo.991.1"> section shows that the bins have different sizes (3×3, 3×2, 2×3, 2×2). </span><span class="koboSpan" id="kobo.991.2">These two quantization levels can introduce misalignment between the RoI and the extracted features. </span><span class="koboSpan" id="kobo.991.3">The following diagram shows how RoI alignment solves </span><span class="No-Break"><span class="koboSpan" id="kobo.992.1">this problem:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer458">
<span class="koboSpan" id="kobo.993.1"><img alt="Figure 5.16 – RoI align example. Source: https://arxiv.org/abs/1703.06870" src="image/B19627_05_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.994.1">Figure 5.16 – RoI align example. </span><span class="koboSpan" id="kobo.994.2">Source: https://arxiv.org/abs/1703.06870</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.995.1">The dashed lines represent the feature map cells. </span><span class="koboSpan" id="kobo.995.2">The region with solid lines in the middle is a 2×2 RoI overlaid on the feature map. </span><span class="koboSpan" id="kobo.995.3">Note that it doesn’t match the cells exactly. </span><span class="koboSpan" id="kobo.995.4">Instead, it is located according to the RPN prediction without quantization. </span><span class="koboSpan" id="kobo.995.5">In the same way, a cell of the RoI (the black dots) doesn’t match one particular cell of the feature map. </span><span class="koboSpan" id="kobo.995.6">The </span><strong class="bold"><span class="koboSpan" id="kobo.996.1">RoI align</span></strong><span class="koboSpan" id="kobo.997.1"> operation </span><a id="_idIndexMarker741"/><span class="koboSpan" id="kobo.998.1">computes the value of a RoI cell with a bilinear interpolation of its adjacent cells. </span><span class="koboSpan" id="kobo.998.2">In this way, RoI align is more accurate than </span><span class="No-Break"><span class="koboSpan" id="kobo.999.1">RoI pooling.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1000.1">At training, a RoI is assigned a positive label if it has IoU with a ground truth box of at least 0.5, and negative otherwise. </span><span class="koboSpan" id="kobo.1000.2">The mask target is the intersection between a RoI and its associated ground truth mask. </span><span class="koboSpan" id="kobo.1000.3">Only the positive RoIs participate in the segmentation </span><span class="No-Break"><span class="koboSpan" id="kobo.1001.1">path training.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.1002.1">Using Mask R-CNN with PyTorch</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.1003.1">In this section, we’ll use a pre-trained PyTorch Mask R-CNN model with a ResNet50 backbone for instance</span><a id="_idIndexMarker742"/><span class="koboSpan" id="kobo.1004.1"> segmentation. </span><span class="koboSpan" id="kobo.1004.2">Like Faster R-CNN, PyTorch has out-of-the-box support for Mask R-CNN. </span><span class="koboSpan" id="kobo.1004.3">The program structure and the requirements</span><a id="_idIndexMarker743"/><span class="koboSpan" id="kobo.1005.1"> are the same as the ones in the </span><em class="italic"><span class="koboSpan" id="kobo.1006.1">Using Faster R-CNN with PyTorch</span></em><span class="koboSpan" id="kobo.1007.1"> section. </span><span class="koboSpan" id="kobo.1007.2">We will only include the relevant parts of the code, but you can find the full version in this book’s GitHub repository. </span><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">Let’s start:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1009.1">Load the pre-trained model with the latest available weights, which you can ensure by using the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1010.1">DEFAULT</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1"> option:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1012.1">
from torchvision.models.detection import \
maskrcnn_resnet50_fpn_v2, \
MaskRCNN_ResNet50_FPN_V2_Weights
model = maskrcnn_resnet50_fpn_v2(
    weights=MaskRCNN_ResNe</span><a id="_idTextAnchor174"/><span class="koboSpan" id="kobo.1013.1">t50_FPN_V2_Weights.DEFAULT)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1014.1">We are going to use the model for inference and not for training, so we’ll enable the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1015.1">eval()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1016.1"> mode:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1017.1">
model.eval()</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1018.1">Use </span><strong class="source-inline"><span class="koboSpan" id="kobo.1019.1">opencv-python</span></strong><span class="koboSpan" id="kobo.1020.1"> to read the RGB image located at </span><strong class="source-inline"><span class="koboSpan" id="kobo.1021.1">image_file_path</span></strong><span class="koboSpan" id="kobo.1022.1">. </span><span class="koboSpan" id="kobo.1022.2">We’ll omit the code, which downloads the image from this book’s repository if it doesn’t already </span><span class="No-Break"><span class="koboSpan" id="kobo.1023.1">exist locally:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1024.1">
import cv2
img = cv2.imread(image_file_path)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1025.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1026.1">img</span></strong><span class="koboSpan" id="kobo.1027.1"> is a three-dimensional </span><strong class="source-inline"><span class="koboSpan" id="kobo.1028.1">numpy</span></strong><span class="koboSpan" id="kobo.1029.1"> array </span><span class="No-Break"><span class="koboSpan" id="kobo.1030.1">of integers.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1031.1">Implement the single-step image pre-processing pipeline. </span><span class="koboSpan" id="kobo.1031.2">It transforms the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1032.1">img</span></strong> <strong class="source-inline"><span class="koboSpan" id="kobo.1033.1">numpy</span></strong><span class="koboSpan" id="kobo.1034.1"> array into </span><strong class="source-inline"><span class="koboSpan" id="kobo.1035.1">torch.Tensor</span></strong><span class="koboSpan" id="kobo.1036.1">, which will serve as input to </span><span class="No-Break"><span class="koboSpan" id="kobo.1037.1">the model:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1038.1">
import torchvision.transforms as transforms
transform = transforms.ToTensor()</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1039.1">R</span><a id="_idTextAnchor175"/><span class="koboSpan" id="kobo.1040.1">un the </span><span class="No-Break"><span class="koboSpan" id="kobo.1041.1">detection model:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1042.1">
nn_input = transform(image)
segmented_objects = model([nn_input])</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1043.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1044.1">segmented_objects</span></strong><span class="koboSpan" id="kobo.1045.1"> is a dict</span><a id="_idTextAnchor176"/><span class="koboSpan" id="kobo.1046.1">ionary with four items: </span><strong class="source-inline"><span class="koboSpan" id="kobo.1047.1">boxes</span></strong><span class="koboSpan" id="kobo.1048.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1049.1">labels</span></strong><span class="koboSpan" id="kobo.1050.1">,  </span><strong class="source-inline"><span class="koboSpan" id="kobo.1051.1">scores</span></strong><span class="koboSpan" id="kobo.1052.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1053.1">masks</span></strong><span class="koboSpan" id="kobo.1054.1">. </span><span class="koboSpan" id="kobo.1054.2">The first three are the same as in Faster R-CNN. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1055.1">masks</span></strong><span class="koboSpan" id="kobo.1056.1"> is a tensor with a shape of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1057.1">[number_of_detected_objects, 1, image_height, image_width]</span></strong><span class="koboSpan" id="kobo.1058.1">. </span><span class="koboSpan" id="kobo.1058.2">We have </span><a id="_idIndexMarker744"/><span class="koboSpan" id="kobo.1059.1">one binary segmentation mask that covers the entire image for each</span><a id="_idIndexMarker745"/><span class="koboSpan" id="kobo.1060.1"> detected object. </span><span class="koboSpan" id="kobo.1060.2">Each such mask has zeroes at all pixels, except the pixels where the object is detected with a value </span><span class="No-Break"><span class="koboSpan" id="kobo.1061.1">of 1.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1062.1">Use the initial </span><strong class="source-inline"><span class="koboSpan" id="kobo.1063.1">img</span></strong><span class="koboSpan" id="kobo.1064.1"> array and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1065.1">segmented_objects</span></strong><span class="koboSpan" id="kobo.1066.1"> as parameters for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1067.1">draw_segmentation_masks</span></strong><span class="koboSpan" id="kobo.1068.1"> function. </span><span class="koboSpan" id="kobo.1068.2">It overlays the bounding boxes, the segmentation masks</span><a id="_idTextAnchor177"/><span class="koboSpan" id="kobo.1069.1">, and the labels of the detected objects on the original input image (the implementation of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1070.1">draw_segmentation_masks</span></strong><span class="koboSpan" id="kobo.1071.1"> is available in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">full example):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1073.1">
draw_segmentation_masks(image, segmented_objects)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1074.1">Display the result </span><span class="No-Break"><span class="koboSpan" id="kobo.1075.1">with </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1076.1">opencv</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1077.1">:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1078.1">
cv2.imshow("Object detection", img)
cv2.waitKey()</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1079.1">The output image looks </span><span class="No-Break"><span class="koboSpan" id="kobo.1080.1">like this:</span></span></p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer459">
<span class="koboSpan" id="kobo.1081.1"><img alt="Figure 5.17 – Instance segmentation with Mask R-CNN" src="image/B19627_05_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1082.1">Figure 5.17 – Instance segmentation with Mask R-CNN</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1083.1">We’ve now</span><a id="_idIndexMarker746"/><span class="koboSpan" id="kobo.1084.1"> discussed object detection and semantic segmentation. </span><span class="koboSpan" id="kobo.1084.2">In</span><a id="_idIndexMarker747"/><span class="koboSpan" id="kobo.1085.1"> the next section, we’ll discuss how to use CNNs to generate new images, instead of simply processing </span><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">existing ones.</span></span></p>
<h1 id="_idParaDest-106" lang="en-GB"><a id="_idTextAnchor178"/><span class="koboSpan" id="kobo.1087.1">Image generation with diffusion models</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1088.1">So far, we’ve used NNs as </span><strong class="bold"><span class="koboSpan" id="kobo.1089.1">discriminative models</span></strong><span class="koboSpan" id="kobo.1090.1">. </span><span class="koboSpan" id="kobo.1090.2">This simply </span><a id="_idIndexMarker748"/><span class="koboSpan" id="kobo.1091.1">means that, given input data, a discriminative model will </span><strong class="bold"><span class="koboSpan" id="kobo.1092.1">map</span></strong><span class="koboSpan" id="kobo.1093.1"> it to a certain label (in other words, a classification). </span><span class="koboSpan" id="kobo.1093.2">A typical </span><a id="_idIndexMarker749"/><span class="koboSpan" id="kobo.1094.1">example is the classification of MNIST images</span><a id="_idIndexMarker750"/><span class="koboSpan" id="kobo.1095.1"> in one of ten digit classes, where the NN maps input data features (pixel intensities) to the digit label. </span><span class="koboSpan" id="kobo.1095.2">We can also say this in another way: a discriminative model gives us the probability of </span><em class="italic"><span class="koboSpan" id="kobo.1096.1">y</span></em><span class="koboSpan" id="kobo.1097.1"> (class), given </span><em class="italic"><span class="koboSpan" id="kobo.1098.1">x</span></em><span class="koboSpan" id="kobo.1099.1"> (input). </span><span class="koboSpan" id="kobo.1099.2">In the case of MNIST, this is the probability of the digit when given the pixel intensities of the image. </span><span class="koboSpan" id="kobo.1099.3">In the next section, we’ll introduce NNs as </span><span class="No-Break"><span class="koboSpan" id="kobo.1100.1">generative models.</span></span></p>
<h2 id="_idParaDest-107" lang="en-GB"><a id="_idTextAnchor179"/><span class="koboSpan" id="kobo.1101.1">Introducing generative models</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1102.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.1103.1">generative model</span></strong><span class="koboSpan" id="kobo.1104.1"> learns </span><a id="_idIndexMarker751"/><span class="koboSpan" id="kobo.1105.1">the distribution of data. </span><span class="koboSpan" id="kobo.1105.2">In a way, it is the opposite of the discriminative model we just described. </span><span class="koboSpan" id="kobo.1105.3">It predicts the probability of the input sample, given its class, </span><em class="italic"><span class="koboSpan" id="kobo.1106.1">y</span></em><span class="koboSpan" id="kobo.1107.1"> – </span><span class="koboSpan" id="kobo.1108.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;Y&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/347.png" style="vertical-align:-0.307em;height:1.105em;width:4.703em"/></span><span class="koboSpan" id="kobo.1109.1">. </span><br/><span class="koboSpan" id="kobo.1110.1">For example, a generative model will be able to create an image based on textual description. </span><span class="koboSpan" id="kobo.1110.2">Most often, </span><em class="italic"><span class="koboSpan" id="kobo.1111.1">y</span></em><span class="koboSpan" id="kobo.1112.1"> is tensor, rather than scalar. </span><span class="koboSpan" id="kobo.1112.2">This tensor </span><a id="_idIndexMarker752"/><span class="koboSpan" id="kobo.1113.1">exists in the so-called </span><strong class="bold"><span class="koboSpan" id="kobo.1114.1">latent space</span></strong><span class="koboSpan" id="kobo.1115.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.1116.1">latent feature space</span></strong><span class="koboSpan" id="kobo.1117.1">), and we’ll refer to</span><a id="_idIndexMarker753"/><span class="koboSpan" id="kobo.1118.1"> it as the </span><strong class="bold"><span class="koboSpan" id="kobo.1119.1">latent representation</span></strong><span class="koboSpan" id="kobo.1120.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.1121.1">latent space representation</span></strong><span class="koboSpan" id="kobo.1122.1">) of the original data, which itself exists in its own </span><strong class="bold"><span class="koboSpan" id="kobo.1123.1">feature space</span></strong><span class="koboSpan" id="kobo.1124.1">. </span><span class="koboSpan" id="kobo.1124.2">We can</span><a id="_idIndexMarker754"/><span class="koboSpan" id="kobo.1125.1"> think of the latent representation as a </span><strong class="bold"><span class="koboSpan" id="kobo.1126.1">compressed</span></strong><span class="koboSpan" id="kobo.1127.1"> (or simplified) version of the original feature space. </span><span class="koboSpan" id="kobo.1127.2">The digit-to-class case serves as an extreme example of this paradigm – after all, we’re compressing an entire image into a single digit. </span><span class="koboSpan" id="kobo.1127.3">For the latent representation to work, it will have to capture the most important hidden properties of the original data and discard </span><span class="No-Break"><span class="koboSpan" id="kobo.1128.1">the noise.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1129.1">Because of its relative simplicity, we can reasonably expect that we have some knowledge of the structure and properties of the latent space. </span><span class="koboSpan" id="kobo.1129.2">This is opposed to the feature space, which is complex beyond our comprehension. </span><span class="koboSpan" id="kobo.1129.3">Therefore, if we know the reverse mapping from the latent space to the feature space, we could generate different feature space representations (that is, images) based on different latent representations. </span><span class="koboSpan" id="kobo.1129.4">More importantly, we can influence the output image properties by modifying (in a conscious way) the initial </span><span class="No-Break"><span class="koboSpan" id="kobo.1130.1">latent representation.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1131.1">To illustrate this, let’s imagine that we’ve managed to create a reverse mapping between latent vectors with </span><em class="italic"><span class="koboSpan" id="kobo.1132.1">n=3</span></em><span class="koboSpan" id="kobo.1133.1"> elements and full-fledged images of vehicles. </span><span class="koboSpan" id="kobo.1133.2">Each vector element represents one vehicle property, such as length, height, and width (as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1134.1">following diagram):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer461">
<span class="koboSpan" id="kobo.1135.1"><img alt="Figure 5.18 – An example of feature space-latent space and latent space-feature space mapping" src="image/B19627_05_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1136.1">Figure 5.18 – An example of feature space-latent space and latent space-feature space mapping</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1137.1">Say that the average vehicle length is four meters. </span><span class="koboSpan" id="kobo.1137.2">Instead of a discrete value, we can represent this property as a </span><strong class="bold"><span class="koboSpan" id="kobo.1138.1">normal</span></strong><span class="koboSpan" id="kobo.1139.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1140.1">Gaussian</span></strong><span class="koboSpan" id="kobo.1141.1">)  </span><strong class="bold"><span class="koboSpan" id="kobo.1142.1">distribution</span></strong><span class="koboSpan" id="kobo.1143.1"> (</span><a href="https://en.wikipedia.org/wiki/Normal_distribution"><span class="koboSpan" id="kobo.1144.1">https://en.wikipedia.org/wiki/Normal_distribution</span></a><span class="koboSpan" id="kobo.1145.1">) with a mean of 4, making the latent space continuous (the same applies to the other properties). </span><span class="koboSpan" id="kobo.1145.2">Then, we can choose to sample new values</span><a id="_idIndexMarker755"/><span class="koboSpan" id="kobo.1146.1"> for each element from the ranges of their distributions. </span><span class="koboSpan" id="kobo.1146.2">They will form a</span><a id="_idIndexMarker756"/><span class="koboSpan" id="kobo.1147.1"> new latent vector (in this case, a </span><strong class="bold"><span class="koboSpan" id="kobo.1148.1">latent variable</span></strong><span class="koboSpan" id="kobo.1149.1">), which we can use as a seed to generate new images. </span><span class="koboSpan" id="kobo.1149.2">For example, we can create longer and lower vehicles </span><br/><span class="koboSpan" id="kobo.1150.1">(as </span><span class="No-Break"><span class="koboSpan" id="kobo.1151.1">illustrated previously).</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1152.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1153.1">The second edition of this book included a whole chapter on NN-based generative models, where we discussed two particular </span><a id="_idIndexMarker757"/><span class="koboSpan" id="kobo.1154.1">architectures: </span><strong class="bold"><span class="koboSpan" id="kobo.1155.1">variational autoencoders</span></strong><span class="koboSpan" id="kobo.1156.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1157.1">VAE</span></strong><span class="koboSpan" id="kobo.1158.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1159.1">Auto-Encoding Variational Bayes</span></em><span class="koboSpan" id="kobo.1160.1">, </span><a href="https://arxiv.org/abs/1312.6114"><span class="koboSpan" id="kobo.1161.1">https:/</span><span id="_idTextAnchor180"/><span class="koboSpan" id="kobo.1162.1">/arxiv.org/abs/1312.6114</span></a><span class="koboSpan" id="kobo.1163.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.1164.1">generative adversarial networks</span></strong><span class="koboSpan" id="kobo.1165.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1166.1">GAN</span></strong><span class="koboSpan" id="kobo.1167.1">, </span><a href="https://arxiv.org/abs/1406.2661"><span class="koboSpan" id="kobo.1168.1">https://arxiv.org/abs/1406.2661</span></a><span class="koboSpan" id="kobo.1169.1">). </span><span class="koboSpan" id="kobo.1169.2">At the time, these were the state-of-the-art generative</span><a id="_idIndexMarker758"/><span class="koboSpan" id="kobo.1170.1"> models for images. </span><span class="koboSpan" id="kobo.1170.2">Since then, they’ve been surpassed by a new class of algorithms called </span><strong class="bold"><span class="koboSpan" id="kobo.1171.1">diffusion models</span></strong><span class="koboSpan" id="kobo.1172.1">. </span><span class="koboSpan" id="kobo.1172.2">As</span><a id="_idIndexMarker759"/><span class="koboSpan" id="kobo.1173.1"> we have to move with the times, in this edition, we’ll omit VAEs and GANs, and we’ll focus on diffusion </span><span class="No-Break"><span class="koboSpan" id="kobo.1174.1">models instead.</span></span></p>
<h2 id="_idParaDest-108" lang="en-GB"><a id="_idTextAnchor181"/><span class="koboSpan" id="kobo.1175.1">Denoising Diffusion Probabilistic Models</span></h2>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1176.1">Diffusion models</span></strong><span class="koboSpan" id="kobo.1177.1"> are a particular class</span><a id="_idIndexMarker760"/><span class="koboSpan" id="kobo.1178.1"> of generative models, first introduced in 2015 (</span><em class="italic"><span class="koboSpan" id="kobo.1179.1">Deep Unsupervised Learning using Nonequilibrium Thermodynamics</span></em><span class="koboSpan" id="kobo.1180.1">, </span><a href="https://arxiv.org/abs/1503.03585"><span class="koboSpan" id="kobo.1181.1">https://arxiv.org/abs/1503.03585</span></a><span class="koboSpan" id="kobo.1182.1">). </span><span class="koboSpan" id="kobo.1182.2">In this section, we’ll </span><a id="_idIndexMarker761"/><span class="koboSpan" id="kobo.1183.1">focus on </span><strong class="bold"><span class="koboSpan" id="kobo.1184.1">Denoising Diffusion Probabilistic Models</span></strong><span class="koboSpan" id="kobo.1185.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1186.1">DDPM</span></strong><span class="koboSpan" id="kobo.1187.1">, </span><a href="https://arxiv.org/abs/2006.11239"><span class="koboSpan" id="kobo.1188.1">https://arxiv.org/abs/2006.11239</span></a><span class="koboSpan" id="kobo.1189.1">), which form the foundation of some of the most impressive generative tools</span><a id="_idIndexMarker762"/><span class="koboSpan" id="kobo.1190.1"> such as </span><strong class="bold"><span class="koboSpan" id="kobo.1191.1">Stable </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1192.1">Diffusion</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1193.1"> (</span></span><a href="https://github.com/CompVis/stable-diffusion"><span class="No-Break"><span class="koboSpan" id="kobo.1194.1">https://github.com/CompVis/stable-diffusion</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1195.1">).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1196.1">DDPM follows a similar pattern to the generative models we’ve already discussed: it starts with a latent variable and uses it to generate a full-fledged image. </span><span class="koboSpan" id="kobo.1196.2">The DDPM training algorithm is split into </span><span class="No-Break"><span class="koboSpan" id="kobo.1197.1">two parts:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1198.1">Forward diffusion</span></strong><span class="koboSpan" id="kobo.1199.1">: This starts</span><a id="_idIndexMarker763"/><span class="koboSpan" id="kobo.1200.1"> with an initial image and then gradually adds random </span><strong class="bold"><span class="koboSpan" id="kobo.1201.1">Gaussian noise</span></strong><span class="koboSpan" id="kobo.1202.1"> (</span><a href="https://en.wikipedia.org/wiki/Gaussian_noise"><span class="koboSpan" id="kobo.1203.1">https://en.wikipedia.org/wiki/Gaussian_noise</span></a><span class="koboSpan" id="kobo.1204.1">) to it through a series of small steps </span><a id="_idIndexMarker764"/><span class="koboSpan" id="kobo.1205.1">until the final (latent) representation is </span><span class="No-Break"><span class="koboSpan" id="kobo.1206.1">pure noise.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1207.1">Reverse diffusion</span></strong><span class="koboSpan" id="kobo.1208.1">: This is the </span><a id="_idIndexMarker765"/><span class="koboSpan" id="kobo.1209.1">opposite of the forward process. </span><span class="koboSpan" id="kobo.1209.2">It starts with pure noise and gradually tries</span><a id="_idIndexMarker766"/><span class="koboSpan" id="kobo.1210.1"> to restore the </span><span class="No-Break"><span class="koboSpan" id="kobo.1211.1">original image.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1212.1">The following diagram illustrates the forward (top) and reverse (bottom) </span><span class="No-Break"><span class="koboSpan" id="kobo.1213.1">diffusion processes:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer462">
<span class="koboSpan" id="kobo.1214.1"><img alt="Figure 5.19 – The forward (bottom) and reverse (top) diffusion processes. Source: https://arxiv.org/abs/2006.11239" src="image/B19627_05_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1215.1">Figure 5.19 – The forward (bottom) and reverse (top) diffusion processes. </span><span class="koboSpan" id="kobo.1215.2">Source: https://arxiv.org/abs/2006.11239</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1216.1">Let’s discuss it </span><span class="No-Break"><span class="koboSpan" id="kobo.1217.1">in detail:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1218.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/348.png" style="vertical-align:-0.340em;height:0.781em;width:0.811em"/></span><span class="koboSpan" id="kobo.1219.1">: The initial image from the original feature space, represented as </span><span class="No-Break"><span class="koboSpan" id="kobo.1220.1">a tensor.</span></span></li>
<li lang="en-GB"><em class="italic"><span class="koboSpan" id="kobo.1221.1">T</span></em><span class="koboSpan" id="kobo.1222.1">: The number of steps in the forward and reverse processes. </span><span class="koboSpan" id="kobo.1222.2">Originally, the authors used </span><em class="italic"><span class="koboSpan" id="kobo.1223.1">T=1000</span></em><span class="koboSpan" id="kobo.1224.1">. </span><span class="koboSpan" id="kobo.1224.2">More recently, </span><em class="italic"><span class="koboSpan" id="kobo.1225.1">T=4000</span></em><span class="koboSpan" id="kobo.1226.1"> has been proposed (</span><em class="italic"><span class="koboSpan" id="kobo.1227.1">Improved Denoising Diffusion Probabilistic Models</span></em><span class="koboSpan" id="kobo.1228.1">). </span><span class="koboSpan" id="kobo.1228.2">Each forward or reverse step adds or removes small amounts </span><span class="No-Break"><span class="koboSpan" id="kobo.1229.1">of noise.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1230.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/349.png" style="vertical-align:-0.333em;height:0.774em;width:0.892em"/></span><span class="koboSpan" id="kobo.1231.1">: The final result of the forward diffusion, which represents pure noise. </span><span class="koboSpan" id="kobo.1231.2">We can think of </span><span class="koboSpan" id="kobo.1232.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/349.png" style="vertical-align:-0.333em;height:0.774em;width:0.890em"/></span><span class="koboSpan" id="kobo.1233.1"> as a peculiar latent representation of </span><span class="koboSpan" id="kobo.1234.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/351.png" style="vertical-align:-0.340em;height:0.781em;width:0.804em"/></span><span class="koboSpan" id="kobo.1235.1">. </span><span class="koboSpan" id="kobo.1235.2">The two tensors have the same dimensions, unlike the example we discussed in the </span><em class="italic"><span class="koboSpan" id="kobo.1236.1">Introducing generative </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1237.1">models</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1238.1"> section.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1239.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/352.png" style="vertical-align:-0.340em;height:0.781em;width:0.710em"/></span><span class="koboSpan" id="kobo.1240.1"> (note the lowercase </span><em class="italic"><span class="koboSpan" id="kobo.1241.1">t</span></em><span class="koboSpan" id="kobo.1242.1">): The noise-augmented tensor at an intermediate step, </span><em class="italic"><span class="koboSpan" id="kobo.1243.1">t</span></em><span class="koboSpan" id="kobo.1244.1">. </span><span class="koboSpan" id="kobo.1244.2">Again, it has the same dimensions as </span><span class="koboSpan" id="kobo.1245.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/348.png" style="vertical-align:-0.340em;height:0.781em;width:0.811em"/></span> <span class="No-Break"><span class="koboSpan" id="kobo.1246.1">and </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1247.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/349.png" style="vertical-align:-0.333em;height:0.774em;width:0.883em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1248.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1249.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/355.png" style="vertical-align:-0.390em;height:1.189em;width:3.806em"/></span><span class="koboSpan" id="kobo.1250.1">: This is the </span><strong class="bold"><span class="koboSpan" id="kobo.1251.1">probability density function</span></strong><span class="koboSpan" id="kobo.1252.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1253.1">PDF</span></strong><span class="koboSpan" id="kobo.1254.1">) of the forward diffusion process at an intermediate</span><a id="_idIndexMarker767"/><span class="koboSpan" id="kobo.1255.1"> step, </span><em class="italic"><span class="koboSpan" id="kobo.1256.1">t</span></em><span class="koboSpan" id="kobo.1257.1">. </span><span class="koboSpan" id="kobo.1257.2">PDF sounds scary, but it isn’t. </span><span class="koboSpan" id="kobo.1257.3">It simply means that we add small amounts of Gaussian noise to the already noisy tensor, </span><span class="koboSpan" id="kobo.1258.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/356.png" style="vertical-align:-0.340em;height:0.781em;width:1.364em"/></span><span class="koboSpan" id="kobo.1259.1">, to produce a new, noisier, tensor, </span><span class="koboSpan" id="kobo.1260.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/357.png" style="vertical-align:-0.340em;height:0.781em;width:0.738em"/></span><span class="koboSpan" id="kobo.1261.1"> (</span><span class="koboSpan" id="kobo.1262.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/357.png" style="vertical-align:-0.340em;height:0.781em;width:0.738em"/></span><span class="koboSpan" id="kobo.1263.1"> is conditioned on </span><span class="koboSpan" id="kobo.1264.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/359.png" style="vertical-align:-0.340em;height:0.781em;width:1.387em"/></span><span class="koboSpan" id="kobo.1265.1">). </span><span class="koboSpan" id="kobo.1265.2">The forward diffusion doesn’t involve ML or NNs and has no learnable parameters. </span><span class="koboSpan" id="kobo.1265.3">We just add noise and that’s it. </span><span class="koboSpan" id="kobo.1265.4">Still, it represents a mapping from the original feature space to the latent </span><span class="No-Break"><span class="koboSpan" id="kobo.1266.1">representation space.</span></span><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1267.1">Note that we</span><a id="_idIndexMarker768"/><span class="koboSpan" id="kobo.1268.1"> need to know </span><span class="koboSpan" id="kobo.1269.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/360.png" style="vertical-align:-0.340em;height:0.781em;width:1.298em"/></span><span class="koboSpan" id="kobo.1270.1"> to produce </span><span class="koboSpan" id="kobo.1271.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/361.png" style="vertical-align:-0.340em;height:0.781em;width:0.692em"/></span><span class="koboSpan" id="kobo.1272.1">, </span><span class="koboSpan" id="kobo.1273.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/362.png" style="vertical-align:-0.340em;height:0.781em;width:1.298em"/></span><span class="koboSpan" id="kobo.1274.1"> for </span><span class="koboSpan" id="kobo.1275.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/360.png" style="vertical-align:-0.340em;height:0.781em;width:1.298em"/></span><span class="koboSpan" id="kobo.1276.1"> and so on – that is, we need all tensors </span><span class="koboSpan" id="kobo.1277.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/364.png" style="vertical-align:-0.340em;height:0.781em;width:3.134em"/></span><span class="koboSpan" id="kobo.1278.1"> to produce </span><span class="koboSpan" id="kobo.1279.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/365.png" style="vertical-align:-0.340em;height:0.781em;width:0.721em"/></span><span class="koboSpan" id="kobo.1280.1">. </span><span class="koboSpan" id="kobo.1280.2">Thankfully, the authors have proposed an optimization that allows us to derive the value of any </span><span class="koboSpan" id="kobo.1281.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/352.png" style="vertical-align:-0.340em;height:0.781em;width:0.710em"/></span><span class="koboSpan" id="kobo.1282.1"> using only the initial </span><span class="No-Break"><span class="koboSpan" id="kobo.1283.1">tensor, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1284.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/348.png" style="vertical-align:-0.340em;height:0.781em;width:0.811em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1285.1">:</span></span></p></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1286.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msqrt&gt;&lt;mml:mover accent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;¯&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:msqrt&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msqrt&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mover accent=&quot;false&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mo&gt;¯&lt;/mml:mo&gt;&lt;/mml:mover&gt;&lt;/mml:msqrt&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;ϵ&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt; &lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;ϵ&lt;/mml:mi&gt;&lt;mml:mo&gt;∼&lt;/mml:mo&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/368.png" style="vertical-align:-0.380em;height:1.184em;width:14.281em"/></span><span class="koboSpan" id="kobo.1287.1">(</span><span class="No-Break"><span class="koboSpan" id="kobo.1288.1">1)</span></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1289.1">Here, </span><span class="koboSpan" id="kobo.1290.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:mfenced open=&quot;[&quot; close=&quot;]&quot; separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;mml:mo&gt;:&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/369.png" style="vertical-align:-0.340em;height:1.024em;width:4.035em"/></span><span class="koboSpan" id="kobo.1291.1"> is a coefficient, which changes on a pre-defined schedule, but generally increases with </span><em class="italic"><span class="koboSpan" id="kobo.1292.1">t</span></em><span class="koboSpan" id="kobo.1293.1">. </span><strong class="bold"><span class="koboSpan" id="kobo.1294.1">ϵ</span></strong><span class="koboSpan" id="kobo.1295.1"> is the Gaussian random noise tensor with the same size as </span><span class="koboSpan" id="kobo.1296.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/352.png" style="vertical-align:-0.340em;height:0.781em;width:0.704em"/></span><span class="koboSpan" id="kobo.1297.1">. </span><span class="koboSpan" id="kobo.1297.2">The square root ensures that the new </span><span class="koboSpan" id="kobo.1298.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/357.png" style="vertical-align:-0.340em;height:0.781em;width:0.739em"/></span><span class="koboSpan" id="kobo.1299.1"> will still follow a Gaussian distribution. </span><span class="koboSpan" id="kobo.1299.2">We can see that </span><span class="koboSpan" id="kobo.1300.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/357.png" style="vertical-align:-0.340em;height:0.781em;width:0.740em"/></span><span class="koboSpan" id="kobo.1301.1"> is a mixture of </span><span class="koboSpan" id="kobo.1302.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/373.png" style="vertical-align:-0.340em;height:0.781em;width:0.832em"/></span><span class="koboSpan" id="kobo.1303.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1304.1">ϵ</span></strong><span class="koboSpan" id="kobo.1305.1"> and </span><span class="koboSpan" id="kobo.1306.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/374.png" style="vertical-align:-0.340em;height:0.788em;width:0.649em"/></span><span class="koboSpan" id="kobo.1307.1"> determines the balance between the two. </span><span class="koboSpan" id="kobo.1307.2">If </span><span class="koboSpan" id="kobo.1308.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;mn&gt;0&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/375.png" style="vertical-align:-0.028em;height:0.662em;width:2.592em"/></span><span class="koboSpan" id="kobo.1309.1">, then </span><span class="koboSpan" id="kobo.1310.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/376.png" style="vertical-align:-0.340em;height:0.781em;width:0.802em"/></span><span class="koboSpan" id="kobo.1311.1"> will have more weight. </span><span class="koboSpan" id="kobo.1311.2">The more </span><span class="koboSpan" id="kobo.1312.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mo&gt;→&lt;/mo&gt;&lt;mi&gt;T&lt;/mi&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/377.png" style="vertical-align:-0.028em;height:0.701em;width:2.819em"/></span><span class="koboSpan" id="kobo.1313.1">, the more the noise, </span><strong class="bold"><span class="koboSpan" id="kobo.1314.1">ϵ</span></strong><span class="koboSpan" id="kobo.1315.1">, will prevail. </span><span class="koboSpan" id="kobo.1315.2">Because of this optimization, we don’t have a real multi-step forward diffusion process. </span><span class="koboSpan" id="kobo.1315.3">Instead, we generate the desired noisy representation at step </span><em class="italic"><span class="koboSpan" id="kobo.1316.1">t</span></em><span class="koboSpan" id="kobo.1317.1">, </span><span class="koboSpan" id="kobo.1318.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/352.png" style="vertical-align:-0.340em;height:0.781em;width:0.710em"/></span><span class="koboSpan" id="kobo.1319.1">, in a </span><span class="No-Break"><span class="koboSpan" id="kobo.1320.1">single operation.</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1321.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/379.png" style="vertical-align:-0.390em;height:1.189em;width:4.074em"/></span><span class="koboSpan" id="kobo.1322.1">: This is the PDF of the reverse diffusion process at an intermediate step, </span><em class="italic"><span class="koboSpan" id="kobo.1323.1">t-1</span></em><span class="koboSpan" id="kobo.1324.1">. </span><span class="koboSpan" id="kobo.1324.2">This is the opposite function of </span><span class="koboSpan" id="kobo.1325.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;q&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/380.png" style="vertical-align:-0.390em;height:1.189em;width:3.765em"/></span><span class="koboSpan" id="kobo.1326.1">. </span><span class="koboSpan" id="kobo.1326.2">It is a mapping from the latent space to the original feature space – that is, we start from the pure noise tensor, </span><span class="koboSpan" id="kobo.1327.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/381.png" style="vertical-align:-0.333em;height:0.774em;width:0.860em"/></span><span class="koboSpan" id="kobo.1328.1">, and we gradually try to remove the noise until we reach the original image in </span><em class="italic"><span class="koboSpan" id="kobo.1329.1">T</span></em><span class="koboSpan" id="kobo.1330.1"> steps. </span><span class="koboSpan" id="kobo.1330.2">The reverse diffusion is a lot more challenging compared to simply adding noise to an image, as in the forward phase. </span><span class="koboSpan" id="kobo.1330.3">This is the primary reason to split the denoising process into multiple steps with small amounts of noise in the first place. </span><span class="koboSpan" id="kobo.1330.4">Our best chance is to train an NN with the hope that it will learn a reasonable approximation of the actual mapping between the latent and the original feature spaces. </span><span class="koboSpan" id="kobo.1330.5">Therefore, </span><span class="koboSpan" id="kobo.1331.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;p&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/382.png" style="vertical-align:-0.340em;height:0.858em;width:0.783em"/></span><span class="koboSpan" id="kobo.1332.1"> is an NN, where the </span><em class="italic"><span class="koboSpan" id="kobo.1333.1">θ</span></em><span class="koboSpan" id="kobo.1334.1"> index indicates its weights. </span><span class="koboSpan" id="kobo.1334.2">The authors have proposed a </span><strong class="bold"><span class="koboSpan" id="kobo.1335.1">U-Net</span></strong><span class="koboSpan" id="kobo.1336.1"> type of network. </span><span class="koboSpan" id="kobo.1336.2">It takes the noisy tensor, </span><span class="koboSpan" id="kobo.1337.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/365.png" style="vertical-align:-0.340em;height:0.781em;width:0.721em"/></span><span class="koboSpan" id="kobo.1338.1">, as input and outputs its approximation of the noise (that is, only the noise and not the image itself) that was added to the original image, </span><span class="koboSpan" id="kobo.1339.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;ϵ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/384.png" style="vertical-align:-0.340em;height:0.812em;width:0.772em"/></span><span class="koboSpan" id="kobo.1340.1">. </span><span class="koboSpan" id="kobo.1340.2">The input and output tensors have the same dimensions. </span><span class="koboSpan" id="kobo.1340.3">DDPM was released later than the original U-Net, so their NN architecture uses some improvements that were</span><a id="_idIndexMarker769"/><span class="koboSpan" id="kobo.1341.1"> introduced in the meantime. </span><span class="koboSpan" id="kobo.1341.2">These</span><a id="_idIndexMarker770"/><span class="koboSpan" id="kobo.1342.1"> include residual blocks, </span><strong class="bold"><span class="koboSpan" id="kobo.1343.1">group normalization</span></strong><span class="koboSpan" id="kobo.1344.1"> (an alternative to batch normalization), and </span><strong class="bold"><span class="koboSpan" id="kobo.1345.1">attention</span></strong><span class="koboSpan" id="kobo.1346.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.1347.1">Attention Is All You Need</span></em><span class="koboSpan" id="kobo.1348.1">, </span><a href="https://arxiv.org/abs/1706.03762"><span class="koboSpan" id="kobo.1349.1">https://arxiv.org/abs/1706.03762</span></a><span class="koboSpan" id="kobo.1350.1">), </span><a href="https://arxiv.org/abs/1803.08494"><span class="koboSpan" id="kobo.1351.1">https://arxiv.org/abs/1803.08494</span></a><span class="koboSpan" id="kobo.1352.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.1353.1">attention</span></strong><span class="koboSpan" id="kobo.1354.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.1355.1">Attention Is All You </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1356.1">Need</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1357.1">, </span></span><a href="https://arxiv.org/abs/1706.03762"><span class="No-Break"><span class="koboSpan" id="kobo.1358.1">https://arxiv.org/abs/1706.03762</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1359.1">).</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1360.1">Next, let’s focus on the </span><a id="_idIndexMarker771"/><span class="koboSpan" id="kobo.1361.1">DDPM training, which is displayed on the left in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1362.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer500">
<span class="koboSpan" id="kobo.1363.1"><img alt="Figure 5.20 – DDPM training (left); DDPM sampling (right). Source: https://arxiv.org/abs/2006.11239" src="image/B19627_05_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1364.1">Figure 5.20 – DDPM training (left); DDPM sampling (right). </span><span class="koboSpan" id="kobo.1364.2">Source: https://arxiv.org/abs/2006.11239</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1365.1">A single training episode involves the following steps (</span><span class="No-Break"><span class="koboSpan" id="kobo.1366.1">li</span><a id="_idTextAnchor182"/><span class="koboSpan" id="kobo.1367.1">ne 1):</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1368.1">Start with a random sample (image), </span><span class="koboSpan" id="kobo.1369.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/348.png" style="vertical-align:-0.340em;height:0.781em;width:0.811em"/></span><span class="koboSpan" id="kobo.1370.1">, from the training set (</span><span class="No-Break"><span class="koboSpan" id="kobo.1371.1">line 2).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1372.1">Sample the random noise step, </span><em class="italic"><span class="koboSpan" id="kobo.1373.1">t</span></em><span class="koboSpan" id="kobo.1374.1">, in the range [1:</span><em class="italic"><span class="koboSpan" id="kobo.1375.1">T</span></em><span class="koboSpan" id="kobo.1376.1">] (</span><span class="No-Break"><span class="koboSpan" id="kobo.1377.1">line 3).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1378.1">Sample the random noise tensor, </span><strong class="bold"><span class="koboSpan" id="kobo.1379.1">ϵ</span></strong><span class="koboSpan" id="kobo.1380.1">, from a Gaussian distribution (line 4). </span><span class="koboSpan" id="kobo.1380.2">Within the NN itself, the step, </span><em class="italic"><span class="koboSpan" id="kobo.1381.1">t</span></em><span class="koboSpan" id="kobo.1382.1">, is embedded in the values of </span><strong class="bold"><span class="koboSpan" id="kobo.1383.1">ϵ</span></strong><span class="koboSpan" id="kobo.1384.1"> using </span><strong class="bold"><span class="koboSpan" id="kobo.1385.1">sinusoidal position embeddings</span></strong><span class="koboSpan" id="kobo.1386.1">. </span><span class="koboSpan" id="kobo.1386.2">Don’t worry if you </span><a id="_idIndexMarker772"/><span class="koboSpan" id="kobo.1387.1">don’t understand the concept of positional embeddings. </span><span class="koboSpan" id="kobo.1387.2">We’ll discuss it in detail in </span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1388.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.1389.1">, as it was first introduced in that context. </span><span class="koboSpan" id="kobo.1389.2">All we need to know now is that the step number, </span><em class="italic"><span class="koboSpan" id="kobo.1390.1">t</span></em><span class="koboSpan" id="kobo.1391.1">, is implicitly encoded in the elements of </span><strong class="bold"><span class="koboSpan" id="kobo.1392.1">ϵ</span></strong><span class="koboSpan" id="kobo.1393.1">, in a way that allows the model to use this information. </span><span class="koboSpan" id="kobo.1393.2">The step-adjusted noise is denoted with </span><span class="koboSpan" id="kobo.1394.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;ϵ&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/386.png" style="vertical-align:-0.117em;height:0.706em;width:1.103em"/></span><span class="koboSpan" id="kobo.1395.1"> in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1396.1">preceding diagram.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1397.1">Produce a corrupt image tensor, </span><span class="koboSpan" id="kobo.1398.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/387.png" style="vertical-align:-0.340em;height:0.781em;width:0.768em"/></span><span class="koboSpan" id="kobo.1399.1">, conditioned on the initial image, </span><span class="koboSpan" id="kobo.1400.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/388.png" style="vertical-align:-0.340em;height:0.781em;width:0.872em"/></span><span class="koboSpan" id="kobo.1401.1">, and based on the sampled noise step, </span><em class="italic"><span class="koboSpan" id="kobo.1402.1">t</span></em><span class="koboSpan" id="kobo.1403.1">, and the random noise, </span><span class="koboSpan" id="kobo.1404.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;ϵ&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/389.png" style="vertical-align:-0.117em;height:0.706em;width:1.121em"/></span><span class="koboSpan" id="kobo.1405.1">. </span><span class="koboSpan" id="kobo.1405.2">To do this, we’ll use formula (1), which we introduced earlier in this section. </span><span class="koboSpan" id="kobo.1405.3">Thanks to it, this single step constitutes the entire forward diffusion phase (</span><span class="No-Break"><span class="koboSpan" id="kobo.1406.1">line 5).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1407.1">Perform a single gradient descent step and weight update. </span><span class="koboSpan" id="kobo.1407.2">The training uses the </span><strong class="bold"><span class="koboSpan" id="kobo.1408.1">mean squared error</span></strong><span class="koboSpan" id="kobo.1409.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1410.1">MSE</span></strong><span class="koboSpan" id="kobo.1411.1">). </span><span class="koboSpan" id="kobo.1411.2">It measures</span><a id="_idIndexMarker773"/><span class="koboSpan" id="kobo.1412.1"> the difference between the sampled noise, </span><strong class="bold"><span class="koboSpan" id="kobo.1413.1">ϵ</span></strong><span class="koboSpan" id="kobo.1414.1"> (line 4), and the noise predicted by the model, </span><span class="koboSpan" id="kobo.1415.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;ϵ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/390.png" style="vertical-align:-0.340em;height:0.812em;width:0.803em"/></span><span class="koboSpan" id="kobo.1416.1"> (line 5). </span><span class="koboSpan" id="kobo.1416.2">The loss equation seems deceptively simple. </span><span class="koboSpan" id="kobo.1416.3">The paper’s authors made a long chain of transformations</span><a id="_idIndexMarker774"/><span class="koboSpan" id="kobo.1417.1"> and assumptions to reach this simple result. </span><span class="koboSpan" id="kobo.1417.2">This is one of the main contributions of </span><span class="No-Break"><span class="koboSpan" id="kobo.1418.1">the paper.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1419.1">Once the model has been trained, we can use it to sample new images based on random initial tensors, </span><span class="koboSpan" id="kobo.1420.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/349.png" style="vertical-align:-0.333em;height:0.774em;width:0.883em"/></span><span class="koboSpan" id="kobo.1421.1">. </span><span class="koboSpan" id="kobo.1421.2">We can do this with the following procedure (preceding </span><span class="No-Break"><span class="koboSpan" id="kobo.1422.1">diagram, right):</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1423.1">Sample the initial random latent tensor, </span><span class="koboSpan" id="kobo.1424.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;T&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/349.png" style="vertical-align:-0.333em;height:0.774em;width:0.883em"/></span><span class="koboSpan" id="kobo.1425.1">, from a Gaussian distribution (</span><span class="No-Break"><span class="koboSpan" id="kobo.1426.1">line 1).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1427.1">Repeat the next steps </span><em class="italic"><span class="koboSpan" id="kobo.1428.1">T</span></em><span class="koboSpan" id="kobo.1429.1"> times (</span><span class="No-Break"><span class="koboSpan" id="kobo.1430.1">line 2):</span></span><ul><li lang="en-GB"><span class="koboSpan" id="kobo.1431.1">Sample random noise tensor, </span><strong class="bold"><span class="koboSpan" id="kobo.1432.1">z</span></strong><span class="koboSpan" id="kobo.1433.1">, from a Gaussian distribution (line 3). </span><span class="koboSpan" id="kobo.1433.2">We do this for all reverse steps, except for the </span><span class="No-Break"><span class="koboSpan" id="kobo.1434.1">final one.</span></span></li><li lang="en-GB"><span class="koboSpan" id="kobo.1435.1">Use the trained U-Net model to predict the noise, </span><span class="koboSpan" id="kobo.1436.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;ϵ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;θ&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/384.png" style="vertical-align:-0.340em;height:0.812em;width:0.772em"/></span><span class="koboSpan" id="kobo.1437.1">, at step </span><em class="italic"><span class="koboSpan" id="kobo.1438.1">t</span></em><span class="koboSpan" id="kobo.1439.1">. </span><span class="koboSpan" id="kobo.1439.2">Subtract this noise from the current sample, </span><span class="koboSpan" id="kobo.1440.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/357.png" style="vertical-align:-0.340em;height:0.781em;width:0.724em"/></span><span class="koboSpan" id="kobo.1441.1">, to produce the new, less noisy, </span><span class="koboSpan" id="kobo.1442.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/395.png" style="vertical-align:-0.340em;height:0.781em;width:1.348em"/></span><span class="koboSpan" id="kobo.1443.1"> (line 4). </span><span class="koboSpan" id="kobo.1443.2">The scheduling </span><a id="_idIndexMarker775"/><span class="koboSpan" id="kobo.1444.1">coefficient, </span><span class="koboSpan" id="kobo.1445.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/396.png" style="vertical-align:-0.340em;height:0.788em;width:0.646em"/></span><span class="koboSpan" id="kobo.1446.1">, </span><br/><span class="koboSpan" id="kobo.1447.1">also takes part in this formula, as it did in the forward phase. </span><span class="koboSpan" id="kobo.1447.2">The formula also preserves the mean and the variance of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1448.1">original distribution.</span></span></li></ul></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1449.1">The final denoising step produces the </span><span class="No-Break"><span class="koboSpan" id="kobo.1450.1">generated image.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1451.1">This concludes our introduction to DDPMs for now. </span><span class="koboSpan" id="kobo.1451.2">However, we’ll revisit them in </span><a href="B19627_09.xhtml#_idTextAnchor236"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1452.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.1453.1">, but in the context of </span><span class="No-Break"><span class="koboSpan" id="kobo.1454.1">Stable Diffusion.</span></span></p>
<h1 id="_idParaDest-109" lang="en-GB"><a id="_idTextAnchor183"/><span class="koboSpan" id="kobo.1455.1">Summary</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1456.1">In this chapter, we discussed some advanced computer vision tasks. </span><span class="koboSpan" id="kobo.1456.2">We started with TL, a technique that makes it possible to bootstrap our experiments with the help of pre-trained models. </span><span class="koboSpan" id="kobo.1456.3">We also introduced object detection and semantic segmentation models, which benefit from TL. </span><span class="koboSpan" id="kobo.1456.4">Finally, we focused on generative models and DDPM </span><span class="No-Break"><span class="koboSpan" id="kobo.1457.1">in particular.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1458.1">In the next chapter, we’ll introduce language modeling and </span><span class="No-Break"><span class="koboSpan" id="kobo.1459.1">recurrent networks.</span></span></p>
</div>


<div class="Content" id="_idContainer514">
<h1 id="_idParaDest-110" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor184"/><span class="koboSpan" id="kobo.1.1">Part 3: </span><br/><span class="koboSpan" id="kobo.2.1">Natural Language Processing and Transformers</span></h1>
</div>
<div id="_idContainer515">
<p lang="en-GB"><span class="koboSpan" id="kobo.3.1">We’ll start this part with an introduction to natural language processing, which will serve as a backdrop for our discussion on recurrent networks and transformers. </span><span class="koboSpan" id="kobo.3.2">Transformers will be the main focus of this section because they represent one of the most significant deep learning advances in recent years. </span><span class="koboSpan" id="kobo.3.3">They are the foundation of </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">large language models</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">LLM</span></strong><span class="koboSpan" id="kobo.7.1">), such as ChatGPT. </span><span class="koboSpan" id="kobo.7.2">We’ll discuss their architecture and their core element – the attention mechanism. </span><span class="koboSpan" id="kobo.7.3">Then, we’ll discuss the properties of LLMs. </span><span class="koboSpan" id="kobo.7.4">Finally, we’ll focus on some advanced LLM applications, such as text and image generation, and learn how to build </span><span class="No-Break"><span class="koboSpan" id="kobo.8.1">LLM-centered applications.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.9.1">This part has the </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">following chapters:</span></span></p>
<ul>
<li lang="en-GB"><a href="B19627_06.xhtml#_idTextAnchor185"><em class="italic"><span class="koboSpan" id="kobo.11.1">Chapter 6</span></em></a><span class="koboSpan" id="kobo.12.1">, </span><em class="italic"><span class="koboSpan" id="kobo.13.1">Natural Language Processing and Recurrent Neural Networks</span></em></li>
<li lang="en-GB"><a href="B19627_07.xhtml#_idTextAnchor202"><em class="italic"><span class="koboSpan" id="kobo.14.1">Chapter 7</span></em></a><span class="koboSpan" id="kobo.15.1">, </span><em class="italic"><span class="koboSpan" id="kobo.16.1">The Attention Mechanism and Transformers</span></em></li>
<li lang="en-GB"><a href="B19627_08.xhtml#_idTextAnchor220"><em class="italic"><span class="koboSpan" id="kobo.17.1">Chapter 8</span></em></a><span class="koboSpan" id="kobo.18.1">, </span><em class="italic"><span class="koboSpan" id="kobo.19.1">Exploring Large Language Models in Depth</span></em></li>
<li lang="en-GB"><a href="B19627_09.xhtml#_idTextAnchor236"><em class="italic"><span class="koboSpan" id="kobo.20.1">Chapter 9</span></em></a><span class="koboSpan" id="kobo.21.1">, </span><em class="italic"><span class="koboSpan" id="kobo.22.1">Advanced Applications of Large Language Models</span></em></li>
</ul>
</div>
<div>
<div id="_idContainer516">
</div>
</div>
<div>
<div class="Basic-Graphics-Frame" id="_idContainer517">
</div>
</div>
</body></html>