- en: Generative Adversarial Networks
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
- en: Reading about making sushi is easy; actually cooking a new kind of sushi is
    harder than we might think. In deep learning, the creative process is harder,
    but not impossible. We have seen how to build models that can classify numbers,
    using dense, convolutional, or recurrent networks, and today we will see how to
    build a model that can create numbers. This chapter introduces a learning approach
    known as generative adversarial networks, which belong to the family of adversarial
    learning and generative models. The chapter explains the concepts of generators
    and discriminators and why having good approximations of the distribution of the
    training data can lead to the success of the model in other areas such as *data
    augmentation*. By the end of the chapter, you will know why adversarial training
    is important; you will be able to code the necessary mechanisms for training a
    generator and a discriminator on questionable data; and you will code a **Generative
    Adversarial Network** (**GAN**) to generate images from a learned latent space.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is organized as follows:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: Introducing adversarial learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a GAN
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing GANs and VAEs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Thinking about the ethical implications of generative models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing adversarial learning
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Recently, there has been interest in adversarial training using adversarial
    neural networks (Abadi, M., et al. (2016)). This is due to adversarial neural
    networks that can be trained to protect the model itself from AI-based adversaries.
    We could categorize adversarial learning into two major branches:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
- en: '**Black box**: In this category, a machine learning model exists as a black
    box, and the adversary can only learn to attack the black box to make it fail.
    The adversary arbitrarily (within some bounds) creates fake input to make the
    black box model fail, but it has no access to the model it is attacking (Ilyas,
    A., et al. (2018)).'
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Insider**: This type of adversarial learning is meant to be part of the training
    process of the model it aims to attack. The adversary has an influence on the
    outcome of a model that is trained *not* to be fooled by such an adversary (Goodfellow,
    I., et al. (2014)).'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are pros and cons to each of these:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
- en: '| **Black box pros** | **Black box cons** | **Insider pros** | **Insider cons**
    |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
- en: '| It gives the ability to explore more generative approaches. | Does not have
    a way to influence or change the black box model. | The model that is trained
    adversarially can be more robust to specific black box attacks. | The options
    for generating attacks are currently limited. |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
- en: '| It is usually fast and likely to find a way to break a model. | The generator
    usually focuses only on perturbing existing data. | The generator can be used
    to *augment* datasets. | It is usually slower. |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
- en: '|  | The generator may not be usable in *augmenting* datasets. |  |  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
- en: 'Since this book is for beginners, we will focus on one of the simplest models:
    an insider model known as a GAN. We will look at its parts and discuss the batch
    training of it.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
- en: GANs have historically been used to generate realistic images (Goodfellow, I., et
    al. (2014)), generally solving multi-agent problems (Sukhbaatar, S., *et al.*
    (2016)), and even cryptography (Rivas, P., et al. (2020)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
- en: Let's briefly discuss adversarial learning and GANs.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
- en: Learning using an adversary
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A machine learning model can learn traditionally to do classification or regression
    and other tasks, among which there may be a model trying to learn to distinguish
    whether the input is legitimate or fake. In this scenario, an machine learning
    model can be created to be an adversary that produces fake inputs, as shown in
    *Figure 14.1*:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/91180858-c5ad-465e-a34a-2a50c08f263e.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 - Adversarial learning
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: In this paradigm, a machine learning model needs to learn to distinguish between
    true inputs and fake ones. When it makes a mistake, it needs to *learn* to adjust
    itself to make sure it properly recognizes true input. On the other hand, the
    adversary will need to keep producing fake inputs with the aim of making the model
    fail.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is what success looks like for each model:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
- en: The **machine learning main model** is successful if it can successfully distinguish
    fake from real input.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The **Adversary model** is successful if it can fool the machine learning main
    model into passing fake data as real.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see, they are competing against each other. One's success is the
    failure of the other, and vice versa.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: During the learning process, the machine learning main model will continuously
    call for batches of real and fake data to learn, adjust, and repeat until we are
    satisfied with the performance, or some other stopping criteria have been met.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: In general in adversarial learning, there is no specific requirement on the
    adversary, other than to produce fake data.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: '**Adversarial robustness** is a new term that is used to certify that certain
    models are robust against adversarial attacks. These certificates are usually
    designated for particular types of adversaries. See Cohen, J. M., *et al.* (2019)
    for further details.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: A popular type of adversarial learning takes place within a GAN, which we will
    discuss next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: GANs
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A GAN is one of the simplest neural-based models that implements adversarial
    learning, and was initially conceived in a bar in Montreal by Ian Goodfellow and
    collaborators (Goodfellow, I., et al. (2014)). It is based on a min-max optimization
    problem that can be posed as follows:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0b573800-6bdf-4433-8909-35c9a245825c.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
- en: 'There are several parts to this equation that require an explanation, so here
    we go:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/530da91e-fd4b-408a-9c71-b41264bb9b22.png): In a GAN, this is the discriminator,
    which is a neural network that takes input data ![](img/cc202992-bba0-4e9f-8a96-2438a06b6f27.png) and
    determines whether it is fake or real, as shown in *Figure 14.2*.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/b29516df-1c42-4df7-b399-1cb96d67038b.png): In a GAN, this is the generator,
    which is also a neural network, but its input is random noise, ![](img/7e4747cc-6204-46b0-a126-eeaab1448a2b.png),
    with the probability ![](img/3db1149e-bb25-4f0c-8c17-d75a8d6624f7.png):'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/09cb1a8f-dc00-4c81-86ca-0e958b50a8ec.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 - GAN main paradigm
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we want to maximize the correct predictions of the discriminator ![](img/612c1e2c-f7da-4500-9b2c-56397fee42b0.png),
    while, at the same time, we want to minimize the error of the generator, ![](img/90d6dec2-992d-4841-9a0b-27f5b6b5610a.png), producing
    a sample that does not fool the discriminator, which is expressed as ![](img/12e9c0b3-a3e2-45a1-aac7-1bdff68ccaf0.png).
    The formulation of expectations and logarithms comes from the standard cross-entropy
    loss function.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: To recap, in a GAN, the generator and the discriminator are neural networks.
    The generator draws random noise from a random distribution, and uses that noise
    to generate *fake* input to fool the discriminator.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: With this in mind, let's proceed and code a simple GAN.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: Training a GAN
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will begin our implementation with a simple MLP-based model, that is, our
    generator and discriminator will be dense, fully connected, networks. Then, we
    will move on to implementing a convolutional GAN.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: An MLP model
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will now focus in creating the model shown in *Figure 14.3*. The model has
    a generator and discriminator that are distinct in terms of their numbers of layers
    and total parameters. It is usually the case that the generator takes more resources
    to build than the discriminator. This is intuitive if you think about it: the
    creative process is usually more complex than the process of recognition. In life,
    it might be easy to recognize a painting from Pablo Picasso if you see all of
    his paintings repeatedly.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: 'However, it might be much harder, in comparison, to actually paint like Picasso:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5abff9d3-4f0b-4129-a969-40b777d0c526.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
- en: Figure 14.3 - MLP-based GAN architecture
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: This figure depicts an icon that simply represents the fact that the discriminator
    will be taking both fake and valid data and learning from both worlds. One thing
    that you must always remember about GANs is that they **generate **data from **random
    noise**. Just think about that for a minute and you will realize that this is
    very cool.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: So, the architecture in *Figure 14.3* does not have any new items we have not
    discovered before. However, the design itself is what is original. Also, the way
    to create it in Keras is quite the task. So, we will show the whole code, with
    as many comments as possible to make things clear.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the full code:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we define the generator as follows:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Next, we can define the discriminator as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The next step is to put things together as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next, we will make the training happen inside a loop that will run for as many
    epochs as we want:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This produces output similar to the following:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This might look different in your system because this is all based on **random**
    noise. This randomness aspect will most likely take your model in a different
    direction. However, what you will see is that your generator's loss should decrease
    gradually, and if the generator is working properly, the accuracy should be getting
    closer to random change, that is, close to 50%. If your discriminator is always
    100%, then your generator is not good enough, and if your discriminator is around
    50% accuracy, then your generator might be too good or your discriminator too
    weak.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's plot a couple of things; the learning curves (losses and accuracy),
    and the samples generated across epochs.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will plot the learning curves:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This generates the plot shown in the following diagram:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54e97377-4afd-4ae0-bec1-940d89686a15.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 - Loss of generator and discriminator across epochs. Accuracy across
    epochs for an MLP-based GAN
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: As the plot indicates, the loss of the discriminator is initially low, as also
    indicated by the accuracy. However, as epochs advance, the generator gets better
    (loss decreases) and accuracy slowly decreases.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14.5* shows a couple of images at every sampled epoch that were produced
    from random noise:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0692c795-14c9-40d4-a63e-9c561043e645.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
- en: Figure 14.5 - GAN-generated images across epochs
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, the initial images look noisy, while the later images have
    more detail and familiar shapes. This would confirm the decrease in the discriminator
    accuracy since these images can easily pass as real. *Figure 14.5* was produced
    using the following code:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Let''s consider a few takeaways from this model:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: The model, as it has been presented, has room for improvements if we make the
    model larger where needed.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If what we need is a good generator, we can extend the generator, or change
    it into a convolutional one (next section).
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we want, we could save the `discriminator` and retrain it (fine-tune it)
    for the classification of digits.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we want, we could use the `generator` to augment the dataset with as many
    images as we want.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In spite of the *decent* quality of the MLP-based GAN, we can appreciate that
    the shapes might not be as well defined as original samples. However, convolutional
    GANs can help.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: Let's proceed and change the MLP-based model into a convolutional one.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
- en: A convolutional model
  id: totrans-84
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The convolutional approach to a GAN was made popular by Radford, A., *et al.*
    (2015). The proposed model was called **Deep Convolutional GAN** (**DCGAN**).
    The primary goal is to make a series of convolutional layers learn feature representations
    to produce *fake* images or to *distinguish* between valid or fake images.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'Moving forward, we will be **intentionally** using a different name for the
    discriminator network, which we will call **critic**. Both terms are used in the
    literature. However, there is a new trend to use the term *critic* and the old
    term may go away at some point. Regardless, you should know that both terms refer
    to the same thing: a network that is tasked with determining whether input is
    valid (from the original dataset) or fake (from an adversarial generator).'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be implementing the model depicted in the following diagram:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0ef4b620-6eac-4a91-9886-7b8fa5c1bb55.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
- en: Figure 14.6 - CNN-based GAN architecture
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
- en: This model has something new never before seen in this book: `Conv2DTranspose`.
    This type of layer is exactly like the traditional convolutional layer, `Conv2D`,
    except that it works in the exact opposite direction. While a `Conv2D` layer learns
    filters (feature maps) that split the input into filtered information, a `Conv2DTranspose` layer
    takes filtered information and joins it together.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: Some people refer to `Conv2DTranspose` as *deconvolution*. However, I personally
    think it is incorrect to do so since *deconvolution* is a mathematical operation
    significantly different from what `Conv2DTranspose` does. Either way, you need
    to remember that if you read *deconvolution* in the context of CNNs, it means `Conv2DTranspose`.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
- en: 'The remainder of the elements in the model are things that we have already
    discussed previously. The full code, which omits comments, is the following:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Next we define the generator as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Then we define the critic networks as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Next we put things together and set the parameters of the model as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Then we train using the following cycle:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'About 70% of the preceding code is the same as before. However, the convolutional
    network design was new. The code would print summaries for both the generator
    and critic. Here is the summary for the generator:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Here is the summary for the critic:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'A sample output for the training steps would look like the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'From the training output, we can see that the convolutional network is able
    to reduce the loss of the generator faster than its MLP counterpart. It appears
    that for the remainder of the epochs, the critic learns slowly to be more robust
    against the generator of fake input. This can be more clearly observed by plotting
    the results using the following code:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The code produces the plot shown in *Figure 14.7*. From the diagram, we can
    appreciate the claims made on faster convergence to small losses and slow recovery
    of the critic''s accuracy:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0c37a8ce-3502-4c4a-8f69-986f32a2abaf.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: Figure 14.7 - Learning curves for CNN-based GANs
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also display the samples generated as the convolutional GAN was being
    trained. The results are shown in *Figure 14.8*. These results are consistent
    with a poor-quality generator trained under 2,000 epochs. After 5,000 epochs,
    the generator is able to produce well-defined numerals that can easily pass as
    valid:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4b8424b6-2de1-4879-a650-d6d4318e39dd.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
- en: Figure 14.8 - Samples generated during training
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: For reference, we can compare *Figure 14.5* and *Figure 14.8* for the MLP-based
    and convolutional-based approach, respectively. Such a comparison can offer insights
    on the fundamental differences between a general-purpose GAN (MLP-based) or a
    GAN specialized in spatial relationships (CNN-based).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Now, we would like to discuss briefly the generative abilities that **Variational
    Autoencoders** (**VAEs**) and GANs bring to the table.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: Comparing GANs and VAEs
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational Autoencoders*,
    we discussed VAEs as a mechanism for dimensionality reduction that aims to learn
    the parameters of the distribution of the input space, and effect reconstruction
    based on random draws from the latent space using the learned parameters. This
    offered a number of advantages we already discussed in [Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational
    Autoencoders*, such as the following:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: The ability to reduce the effect of noisy inputs, since it learns the distribution
    of the input, not the input itself
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ability to generate samples by simply querying the latent space
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'On the other hand, GANs can also be used to generate samples, like the VAE.
    However, the learning of both is quite different. In GANs, we can think of the
    model as having two major parts: a critic and a generator. In VAEs, we also have
    two networks: an encoder and a decoder.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
- en: If we were to make any connection between the two, it would be that the decoder
    and generator play a very similar role in VAEs and GANs, respectively. However,
    an encoder and a critic have very different goals. An encoder will learn to find
    a rich latent representation, usually with very few dimensions compared to the
    input space. Meanwhile, a critic does not aim to find any representations, but
    to solve a growing complex binary classification problem.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: We could make a case that the critic is certainly learning features from the
    input space; however, the claim that features in the deepest layers are similar
    in both the critic and encoder requires more evidence.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: One thing we can do to make a comparison is to take the deep VAE model shown
    in [Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational Autoencoders*,
    *Figure 14.7*, train it, and draw some random samples from the generator in the
    VAE, and do the same for the convolutional GAN.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'We can start by displaying the samples from the convolutional GAN and executing
    the following code immediately after the last piece of code in the previous section,
    which contains the trained GAN. Here is the code:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This code will produce 400 numerals from random noise! The plot is shown in
    *Figure 14.9*:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b787b620-e438-4ddd-b25c-8715396e7cee.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
- en: Figure 14.9 - 400 numerals produced by a convolutional GAN
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Recall that these numerals were produced after 12,000 epochs. The quality seems
    relatively good. Most of these numerals could actually fool a human being into
    thinking they were really written by a human.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，这些数字是在经过 12,000 次训练后生成的。质量似乎相对较好。这些数字大多数可能会欺骗人类，让他们以为它们真的是人类写的。
- en: Now, we want to take a look at the quality of the numerals generated with a
    VAE. For this, you will need to go to [Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational
    Autoencoders*, and use the code provided to implement the deep VAE and train it
    for, say, 5,000 epochs. After training it, you can use the decoder to generate
    samples from random noise by choosing random parameters.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们想看看使用 VAE 生成的数字质量。为此，你需要参考[第9章](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml)，*变分自编码器*，并使用提供的代码实现深度
    VAE，并训练大约 5,000 次迭代。训练完成后，你可以使用解码器通过选择随机参数从随机噪声中生成样本。
- en: 'Here is the code you should use *once* the training of the VAE is complete:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 VAE 的训练完成，以下是你应该使用的代码：
- en: '[PRE18]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'A couple of visible differences is that the VAE assumes that the parameters
    of the latent space follow a normal distribution; also, the output needs to be
    reshaped to 28x28, as opposed to the GAN, which gives the output already in its
    correct shape thanks to the 2D convolutional output layer. The output of this
    code is shown in *Figure 14.10*:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 几个明显的区别是，VAE 假设潜在空间的参数遵循正态分布；此外，输出需要被重塑为 28x28，而 GAN 则通过 2D 卷积输出层直接生成正确形状的输出。这段代码的输出如*图
    14.10*所示：
- en: '![](img/84984cdc-549e-4db3-ad8c-921d787020f6.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](img/84984cdc-549e-4db3-ad8c-921d787020f6.png)'
- en: Figure 14.10 - 400 samples of numerals generated by the decoder of a VAE using
    random noise
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.10 - 由 VAE 解码器使用随机噪声生成的 400 个数字样本
- en: As you can see from the diagram, some of these numerals look very good; some
    might say too good. They look smooth, well-rounded, and perhaps we can say noise-free.
    The numerals produced by the VAE lack the distinctive quality of looking noisy
    compared to the ones produced by the GAN. However, this can be a good thing or
    a bad thing depending on what you want to do.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，这些数字中的一些看起来非常好；有些人可能会说看起来太好。它们看起来平滑、圆润，或许我们可以说没有噪点。与由 GAN 生成的数字相比，VAE
    生成的数字缺少那种带有噪音感的独特特征。然而，这可以是好事，也可以是坏事，取决于你想做什么。
- en: Say that you want to have clean-looking samples that might be easily identified
    as *fake,* then a VAE is the best choice. Now, say that we want samples that can
    easily fool a human into thinking they are not produced by a machine; here, perhaps
    a GAN fits better.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要一些干净的样本，容易被识别为*伪造的*，那么 VAE 是最好的选择。现在，假设我们希望生成的样本能轻易地让人类相信它们不是机器生成的；在这种情况下，可能
    GAN 更合适。
- en: Regardless of these differences, both can be used to augment your datasets if
    you need to have more data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有这些区别，两者都可以用来扩充你的数据集，如果你需要更多的数据。
- en: Thinking about the ethical implications of GANs
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 思考 GAN 的伦理影响
- en: Some ethical thoughts about generative models have already been provided in
    [Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational Autoencoders*.
    However, a second round of thoughts is in order given the adversarial nature of
    GANs. That is, there is an implicit demand from a GAN to *trick *a critic in a
    min-max game where the generator needs to come out victorious (or the critic as
    well). This concept generalized to adversarial learning provides the means to *attack*
    existing machine learning models.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 关于生成模型的一些伦理思考已经在[第9章](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml)，*变分自编码器*中提到。然而，鉴于
    GAN 的对抗性质，第二轮的思考是必要的。也就是说，GAN 隐含地要求在最小-最大博弈中“欺骗”一个判别器，生成器需要取得胜利（或者判别器也可以）。这一概念推广到对抗学习，为攻击现有的机器学习模型提供了手段。
- en: Very successful computer vision models such as VGG16 (a CNN model) have been
    attacked by models that perform adversarial attacks. There are *patches* that
    you can print, put on a t-shirt, cap, or any object, and as soon as the patch
    is present in the input to the models being attacked, they are fooled into thinking
    that the existing object is a completely different one (Brown, T. B., et al. (2017)).
    Here is an example of an adversarial patch that tricks a model into thinking that
    a banana is a toaster: [https://youtu.be/i1sp4X57TL4](https://youtu.be/i1sp4X57TL4).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 很成功的计算机视觉模型，如VGG16（一个CNN模型），已经遭受过执行对抗攻击的模型攻击。有一些*补丁*可以打印出来，放在T恤、帽子或任何物体上，当这些补丁出现在输入到被攻击的模型中时，模型会被欺骗，认为原本存在的物体是完全不同的（Brown,
    T. B., 等人（2017））。这里有一个示例，通过对抗性补丁将香蕉欺骗成烤面包机：[https://youtu.be/i1sp4X57TL4](https://youtu.be/i1sp4X57TL4)。
- en: Now that these types of adversarial attacks are known to exist, researchers
    have found vulnerabilities in their current systems. Therefore, it has become
    almost an obligation for us, the deep learning practitioners, to make sure our
    models are robust against adversarial attacks. This is particularly important
    for systems that involve sensitive information, or systems that make decisions
    that affect human life.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 既然这些类型的对抗攻击已经被确认存在，研究人员也发现了当前系统中的漏洞。因此，作为深度学习从业者，我们几乎有责任确保我们的模型在面对对抗攻击时具有鲁棒性。这对于涉及敏感信息的系统，或做出可能影响人类生命的决策的系统尤为重要。
- en: For example, a deep learning model that is deployed in an airport to assist
    security efforts needs to be tested so as to avoid a person wearing a t-shirt
    with a printed adversarial patch aiming to avoid being recognized as a person
    in a restricted area. This is critical for the security of the population. However,
    a deep learning system to automatically tune the audio of a person singing might
    not be particularly critical.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，部署在机场以协助安全工作的深度学习模型需要进行测试，以避免某人穿着印有对抗性补丁的T恤，意图避免被识别为受限区域内的人员。这对人口安全至关重要。然而，用于自动调节某人唱歌音频的深度学习系统可能并不是特别关键。
- en: What is required from you is to look into testing your models for adversarial
    attacks. There are several resources online being updated frequently that you
    can easily find if you search for them. If you come to find a vulnerability in
    a deep learning model, you should report it to the creators immediately, for the
    well-being of our society.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要做的是测试你的模型是否受到对抗性攻击的影响。网上有多个资源，并且经常更新，只需搜索就可以轻松找到。如果你发现深度学习模型中存在漏洞，你应该立即向开发者报告，以确保我们社会的福祉。
- en: Summary
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: This advanced chapter showed you how to create GAN networks. You learned the
    major components of GANs, a generator and a critic, and their role in the learning
    process. You learned about adversarial learning in the context of breaking models
    and making them robust against attacks. You coded an MLP-based and a convolutional-based
    GAN on the same dataset and observed the differences. At this point, you should
    feel confident explaining why adversarial training is important. You should be
    able to code the necessary mechanisms to train a generator and a discriminator
    of a GAN. You should feel confident about coding a GAN and comparing it to a VAE
    to generate images from a learned latent space. You should be able to design generative
    models, considering the societal implications and the responsibilities that come
    with using generative models.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 本章高级内容向你展示了如何创建GAN网络。你了解了GAN的主要组件：生成器和判别器，以及它们在学习过程中的作用。你了解了对抗性学习的概念，尤其是在打破模型并使其对攻击具备鲁棒性方面。你基于同一数据集编写了基于MLP和基于卷积的GAN，并观察了它们的差异。到此为止，你应该能够自信地解释为什么对抗训练如此重要。你应该能够编写必要的机制来训练GAN的生成器和判别器。你应该对编写GAN代码并与VAE进行对比，从已学习的潜在空间生成图像充满信心。你应该能够设计生成模型，考虑到其社会影响及使用生成模型时应承担的责任。
- en: GANs are very interesting and have yielded amazing research and applications.
    They have also exposed the vulnerabilities of other systems. The present state
    of deep learning involves combinations of AEs, GANs, CNNs, and RNNs, using specific
    components of each, and gradually increasing the potential of applications of
    deep learning across different fields. The world of deep learning is exciting
    right now, and you are now ready to embrace it and dive deeper into whatever area
    you feel you like. [Chapter 15](216a275e-ae7e-451c-a8c6-f31eac314d3f.xhtml), *Final
    Remarks on the Future of Deep Learning*, will present brief comments on how we
    see the future of deep learning. It attempts to use some kind of *prophetic* voice
    about the things to come. But before you go, quiz yourself with the following
    questions.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Who is the adversary in a GAN?**'
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The generator. It acts as a model whose sole purpose is to make the critic fail;
    it is the critic's adversary.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: '**Why is the generator model bigger than the critic? **'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This is not always the case. The models discussed here were more interesting
    as generators of data. However, we could use the critic and retrain it for classification,
    in which case, the critic model might be bigger.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: '**What is adversarial robustness?**'
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It is a new field in deep learning tasked with researching ways to certify that
    deep learning models are robust against adversarial attacks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
- en: '**Which is better – a GAN or a VAE?**'
  id: totrans-157
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This depends on the application. GANs tend to produce more "interesting" results
    than VAEs, but VAEs are more stable. Also, it is often faster to train a GAN than
    a VAE.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '**Are there any risks associated with GANs?**'
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes. There is a known problem called *mode collapse*, which refers to the inability
    of a GAN to produce novel, different, results across epochs. It seems like the
    network gets stuck on a few samples that can cause sufficient confusion in the
    critic so as to produce a low loss, while having no diversity of generated data.
    This is still an open problem with no universal solution. A lack of diversity
    in a GAN's generator is an indication that it has collapsed. To find out more
    about mode collapse, read Srivastava, A., et al. (2017).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-161
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Abadi, M., and Andersen, D. G. (2016). *Learning to protect communications with
    adversarial neural cryptography*. *arXiv preprint* arXiv:1610.06918.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ilyas, A., Engstrom, L., Athalye, A., and Lin, J. (2018). *Black box adversarial
    attacks with limited queries and information*. *arXiv preprint* arXiv:1804.08598.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair,
    S., and Bengio, Y. (2014). *Generative adversarial nets*. In *Advances in neural
    information processing systems* (pp. 2672-2680).
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sukhbaatar, S., and Fergus, R. (2016). *Learning multi-agent communication with
    backpropagation*. In *Advances in neural information processing systems* (pp.
    2244-2252).
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rivas, P., and Banerjee, P. (2020). *Neural-Based Adversarial Encryption of
    Images in ECB Mode with 16-bit Blocks*. In *International Conference on Artificial
    Intelligence*.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cohen, J. M., Rosenfeld, E., and Kolter, J. Z. (2019). *Certified adversarial
    robustness via randomized smoothing*. *arXiv preprint* arXiv:1902.02918.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford, A., Metz, L., and Chintala, S. (2015). *Unsupervised representation
    learning with deep convolutional generative adversarial networks*. *arXiv preprint*
    arXiv:1511.06434.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Brown, T. B., Mané, D., Roy, A., Abadi, M., and Gilmer, J. (2017). *Adversarial
    patch*. *arXiv preprint* arXiv:1712.09665.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Srivastava, A., Valkov, L., Russell, C., Gutmann, M. U., and Sutton, C. (2017).
    *Veegan: Reducing mode collapse in GANs using implicit variational learning*.
    In *Advances in Neural Information Processing Systems* (pp. 3308-3318).'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
