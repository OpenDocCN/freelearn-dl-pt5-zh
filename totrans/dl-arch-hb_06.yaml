- en: '6'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Neural Network Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Not to be confused with the electrical devices that are also called transformers,
    neural network transformers are the jack-of-all-trades variant of NNs. Transformers
    are capable of processing and capturing patterns from data of any modality, including
    sequential data such as text data and time-series data, image data, audio data,
    and video data.
  prefs: []
  type: TYPE_NORMAL
- en: The transformer architecture was introduced in 2017 with the motive of replacing
    RNN-based sequence-to-sequence architectures and primarily focusing on the machine
    translation use case of converting text data from one language to another language.
    The results performed better than the baseline RNN-based model and proved that
    we don’t need inherent inductive biases on the sequential nature of the data that
    the RNNs employ. Transformers then became the root of a family of neural network
    architectures and branched off to model variants that are capable of capturing
    patterns in other data modalities while continually raising the bar of the performance
    of the original text sequence data-based tasks. This showed that we don’t really
    need to have inherent inductive bias built into a model in general, regardless
    of the data modality, and instead, we can allow the model to *learn* these inherent
    structures and patterns. Do you have sequential data such as video, text, or audio?
    Let the neural network learn its sequential nature. Do you have image data? Let
    the neural network learn the spatial and depth relationships between the pixels.
    You get the picture.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we explore a more in-depth overview of transformers, take a breather
    to check out the topics that will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring neural network transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decoding the original transformer architecture holistically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncovering transformer improvements using only the encoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Uncovering transformer improvements using only the decoder
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring neural network transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Figure 6**.1* provides an overview of the impact transformers have had, thanks
    to the plethora of transformer model variants.'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.1 – Transformers’ different modality and model branches](img/B18187_06_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.1 – Transformers’ different modality and model branches
  prefs: []
  type: TYPE_NORMAL
- en: The transformer does not have inherent inductive bias structurally designed
    into its architecture. Inductive bias refers to the pre-assumptions made by a
    learning algorithm on the data. This bias can be built into the model architecture
    or the learning process, and it helps to guide the model toward learning specific
    patterns or structures in the data. Traditional models, such as RNNs, incorporate
    inductive bias through their design, for instance, by assuming that data has a
    sequential structure and that the order of elements is important. Another example
    is CNN models, which are specifically designed for processing grid-like data,
    such as images, by incorporating inductive bias in the form of local connectivity
    and translation invariance through the use of convolutional layers and pooling
    layers. In this context, the model architecture itself enforces certain constraints
    on the patterns that can be learned.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers were designed with the idea that we should allow the model to decide
    how and where to focus, based on all the input data provided. This is executed
    technically using an aggregate of multiple mechanisms that each decide where to
    focus in different ways. Thus, transformers depend entirely on the information
    provided by the input data to determine any form of inductive bias your data has,
    if any. These focusing components are formally called attention layers. The improvements
    on top of transformers that form the branches you see in *Figure 6**.1* do not
    deviate far from the base structure of transformers. Usually, the improvements
    to add different modalities are done by data setup variations to adapt the input
    data structure to the structure of the transformer along with the different prediction
    output details specific to variations in the target task application.
  prefs: []
  type: TYPE_NORMAL
- en: At the moment, transformers are the architecture family with the biggest capacity
    for learning and identifying highly complex patterns that exist in the real world.
    To add to that, for a period of almost a year from mid-2022 to 2023, transformers
    demonstrated that their informational capacity is bounded only by the hardware
    resource capacity. Currently, one of the largest transformer models, scaled up
    many times from the base transformer, with an astronomical *540 billion* parameters,
    is a model variant called **PaLM** provided by Google. Additionally, it is rumored
    that the GPT-4 multimodal text generation model by OpenAI, which is not available
    as an open sourced model but as a service, consists of either multiple models
    or a single model that adds up more than a trillion parameters. These huge models
    usually take months of training to allow the model to achieve peak performance,
    along with the need to have highly performant and state-of-the-art GPUs.
  prefs: []
  type: TYPE_NORMAL
- en: You might wonder why we would want to train such a big model and wonder whether
    the value it provides is worth it. Let’s evaluate one of the transformer models,
    called **GPT-3**, developed by OpenAI. GPT-3 is a type of language model; it takes
    in input text and outputs text-based predictions based on what it thinks is the
    most appropriate and useful response. Now, this spans many different tasks in
    the NLP space that conventionally would have been accomplished by individual models
    for each task, making it a **task-agnostic model**. The tasks that can be accomplished
    are machine language translation, reading comprehension, reasoning, arithmetic
    processing, and in general, demonstrating a wide variety of language understanding
    capabilities, providing results in different formats, depending on the query input
    text. For example, end user applications include generating code in any specified
    languages capable of achieving the described objectives, writing fiction with
    a specified theme, obtaining any requested information on any publicly known person,
    summarizing customer feedback, in general or with a focus on certain topics, such
    as “what’s frustrating the customer,” and adding realism to the conversations
    of characters in a virtual world. To date, there are more than 300 applications
    of GPT-3 being used in a variety of categories and industries, a number that will
    seem small but will pave the way for more innovation and adoption of the immense
    capabilities of NNs.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers have gone through multiple years of gradual improvements through
    rigorous research but have not deviated a lot from the base architecture. This
    means that understanding the original architecture is key to understanding all
    the latest and greatest improved transformers such as GPT-3\. With that, let’s
    dive into the original transformer architecture from 2017 and discuss which components
    have been changed or adapted in the past five years of research that gave birth
    to new model architectures.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding the original transformer architecture holistically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we look into the structure of the model, let’s talk about the basic intent
    of transformers.
  prefs: []
  type: TYPE_NORMAL
- en: As we covered in the previous chapter, transformers are also a family of architectures
    that utilize the concept of encoder and decoder. The encoder encodes data into
    what is known as the code and the decoder decodes the code into a data format
    that looks similar to raw, unprocessed data. The very first transformer used both
    the encoder and decoder concepts to build the entire architecture and demonstrated
    its application in text generation. The subsequent adaptations and improvements
    either used only the encoder or only the decoder to achieve different tasks. In
    a transformer, however, the encoder’s goal is not to compress the data to achieve
    a smaller and more compact representation of the data, but instead mainly to serve
    as a feature extractor. Additionally, the decoder’s goal for transformers is not
    to reproduce the same input.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to build an actual autoencoder structure with the transformer
    component instead of using CNN or RNN components, but this will not be covered
    in this book.
  prefs: []
  type: TYPE_NORMAL
- en: The original transformer fixed the data/weight dimensions in the encoder and
    decoder to a unified single size so that residual connections could be made, using
    a dimension of 512.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers also utilize logical block structures to define their novelties,
    which allows you to scale their size easily. The core operation of the transformer
    is the mechanism to identify which part of the entire input data to focus on for
    each input data unit in an input data sample. The focusing mechanism is achieved
    technically by a type of neural network layer called the **attention layer**.
    There are many types of attention variants that will not be covered in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Transformers utilize the simplest variant of attention that utilizes the `softmax`
    activation to achieve `softmax` forces values to add up to `1.0`, there is usually
    one strongest focal point for each layer. Take this as a form of gate similar
    to the gates in RNN, for which you can refer to [*Chapter 4*](B18187_04.xhtml#_idTextAnchor068),
    *Understanding Recurrent Neural Networks*, again for the full context on what
    an RNN is and how it functions. Transformers use multiple attention layers where
    each input will be focused on in multiple ways, and the aggregated name to simplify
    referencing and scaling is called the **multi-head** **attention** layer.
  prefs: []
  type: TYPE_NORMAL
- en: The inputs and outputs of transformers are in the form of tokens. Tokens are
    a way to refer to the individual units of input data that can be passed into and
    out of a transformer. These tokens can be sequential or non-sequential, grouped
    or non-grouped, although the structure of transformers is not explicitly designed
    with these assumptions. Through research and advancements since they were devised,
    tokens for text can be words or sub-words, tokens for images can be image patches,
    tokens for audio can be sequential time windows of audio data, for example, one-second
    windows, and tokens for videos can be single image frames or a group of image
    frames. As the model does not have an inherent inductive bias toward any type
    of data modalities, the positional or sequential identification of the data is
    encoded into the data explicitly by tokens. These can range from incremental discrete
    integer positions (1, 2, 3 ...) to continuous floating-point positions in between
    discrete integer values (1.2, 1.4556, 2.42325 ...), from absolute positions to
    relative positions, and finally, single-valued positions for each token or embedding
    that can be learned. The original transformer used a mapping function that maps
    absolute integer positions to relative floating-point positions that consider
    the data dimensions of the tokens and the intended unified data dimension size,
    but the state-of-the-art models commonly adopt instead embeddings that can be
    learned. Embeddings are a look-up table to encode any discrete categorical data
    into a higher-dimensional and more complex representation of the original category
    that can accurately discern categories among each other. Note that the actual
    input data token itself is also usually applied with embeddings depending on the
    input data (image frames or patches usually don’t use embeddings as they are already
    high in dimensionality, but text tokens are categorical and use embeddings).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 6**.2* shows a high-level overview of the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.2 – Transformers viewed with input and output visualizations with
    both the encoder and decoder of the task to translate an English sentence to its
    Mandarin counterpart](img/B18187_06_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.2 – Transformers viewed with input and output visualizations with both
    the encoder and decoder of the task to translate an English sentence to its Mandarin
    counterpart
  prefs: []
  type: TYPE_NORMAL
- en: The encoder and decoder have somewhat similar structures, with the exception
    that the decoder has an extra middle `softmax` activation function.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The decoder also takes in other data, aside from the encoder outputs.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve data generation without a constraint on the number of outputs, the
    transformer was designed into an autoregressive model. An autoregressive model
    uses the result of the first prediction as input for the second prediction and
    is subsequently used as the input for the next predictions. The following figure
    shows an example of this prediction process with text-based data that aims to
    accomplish machine translation from English to Mandarin.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.3 – Autoregressive workflow for transformers](img/B18187_06_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.3 – Autoregressive workflow for transformers
  prefs: []
  type: TYPE_NORMAL
- en: Each forward pass of the transformer will predict a single output. An end-of-sentence
    token prediction is factored in at the transformer output to signal that the prediction
    is done. During the training process, however, it is not necessary to perform
    this autoregressive loop; instead, masks are generated at random positions to
    nullify all the future tokens from the target tokens to prevent the model from
    copying the single target token directly and taking in future token information.
    Plainly speaking, the masking mechanism is not used during the prediction or inference
    stage.
  prefs: []
  type: TYPE_NORMAL
- en: The multi-head attention layer has a few other key operations besides the actual
    attention mechanism. *Figure 6**.4* shows the unraveled multi-head attention layer
    along with the previously mentioned attention mechanism in the scaled dot-product
    attention layer.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.4 – Multi-head attention layer with the scaled dot-product attention
    structure](img/B18187_06_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.4 – Multi-head attention layer with the scaled dot-product attention
    structure
  prefs: []
  type: TYPE_NORMAL
- en: Take the multiple attention layer as multiple humans contributing to different
    focusing patterns on the same data. `softmax` from focusing on unimportant areas.
    The attention mechanism is meant to find the degree of relevance of each token
    to the entire set of input tokens, in other words, to find out what tokens interacted
    with what tokens and figure out how much they depend on each other. Recall that
    the model’s dimension has to stay a fixed sized throughout the architecture to
    ensure values can be added together without additional processing. Since the output
    of the multiple attention heads will be concatenated instead of added, the dimensions
    of the linear layer have to be evenly distributed among the heads. With 8 heads,
    for example, and a data/weight dimension of 512, the linear layer dimension for
    each query, key, and value for each head would then be 512/8=64 neurons. *Figure
    6**.5* shows a simple example of the outputs and their shapes in the workflow
    with the number of fixed data/weight dimensions per token being 3, using the same
    input text data as *Figure 6**.4*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.5 – Part operation of the multi-head attention before concatenating
    using the query of the word “I” in the context of “I love you” with a linear layer
    with an output dimension of 3](img/B18187_06_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.5 – Part operation of the multi-head attention before concatenating
    using the query of the word “I” in the context of “I love you” with a linear layer
    with an output dimension of 3
  prefs: []
  type: TYPE_NORMAL
- en: The same operation will be done for the other two query words, “love” and “you.”
    An operation from a single head focuses on the input data in one way while the
    other heads focus on the input data in a different way. In the encoder-decoder
    layer, the query comes from the decoder layer while the keys and value come from
    the encoder layer. This allows the decoder to choose where to focus in the given
    input text to produce the next output. *Figure 6**.6* shows a good way to think
    about the output of multi-head attention with a four-headed multi-head attention
    focusing on different parts of the text.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.6 – Multi-head attention example output with four heads](img/B18187_06_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.6 – Multi-head attention example output with four heads
  prefs: []
  type: TYPE_NORMAL
- en: The regularization mechanisms part of the transformer has the same purpose as
    described in other previous architectures such as CNNs, RNNs, and MLPs, and won’t
    be discussed again here. This sums up all the components of the transformer. This
    base architecture allows us to achieve better data generation results compared
    to older architectures such as sequence-to-sequence or autoencoders. Although
    the original architecture focused on applications for text data, the concept can
    and has been adapted to handle other types of data modalities such as image, video,
    and audio data. Recall that the improvements and adaptations that came after the
    original model used only the encoder or only the decoder. In the next two topics,
    we will dive into the two different concepts separately.
  prefs: []
  type: TYPE_NORMAL
- en: Uncovering transformer improvements using only the encoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first type of architectural advancements based on transformers we will discuss
    are transformers that utilize only the encoder part of the original transformer
    using the same multi-head attention layer. The encoder-only line of transformers
    is adopted generally because there is no masked multi-head attention layer since
    the next token prediction training setup is not used. In this line of improvements,
    training goals and setups vary across different data modalities and vary slightly
    for sequential improvements under the same data modality. However, one concept
    that stays pretty much constant across different data modalities is the fact that
    a semi-supervised learning method is used. In the case of transformers, this means
    that a form of unsupervised learning is executed first and then the straightforward
    supervised learning method is executed next. Unsupervised learning offers transformers
    a way to initialize their state based on a wider understanding of the nature of
    the data. This process is also known as **pre-training** a model, which is just
    one form of unsupervised learning. Unsupervised learning will be discussed more
    extensively in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125), *Exploring Unsupervised*
    *Deep Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Some improvements are specific to the nature of the data modality, which mostly
    consists of text-specific improvements and involves a change in the type of task
    to optimize. Other improvements are general transformer architectural improvements
    that can generalize to different data modalities. We will start by focusing on
    the general architectural improvements before diving into the improvements specifically
    crafted for text to date, which might or might not be adaptable to other data
    modalities. To start off, we will go through the base architecture of an encoder-only
    structure called BERT.
  prefs: []
  type: TYPE_NORMAL
- en: The `softmax` layer. This specific task was proven to be highly effective as
    a pretraining method that allows downstream subsequent supervised tasks to achieve
    better performance. Since the tokens are masked out in different positions from
    the original sequence, the architecture is said to be bidirectional, as information
    from the future tokens can be attended to predict tokens in the past in addition
    to the past tokens. 15% of tokens were masked, for example, and from that percentage,
    10% were randomly replaced with random tokens to act as noise, making the model
    similar to a denoising autoencoder and allowing it to be robust to noise. Standard
    autoregressive structures are naturally uni- and forward-directional and can’t
    take advantage of future tokens. BERT also introduced ways to encode multiple
    sentences in a single input representation. This is achieved by adding a special
    separator token that has its own learned embeddings to signify a separation between
    text along with an extra segment embedding that signals to the transformer which
    sentence number the token is part of. This allows another task called `softmax`.
    The three embeddings (positional embeddings, segment embeddings, and token embeddings)
    are added together before passing the result into the transformer. *Figure 6**.7*
    depicts this architecture.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.7 – BERT architecture](img/B18187_06_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.7 – BERT architecture
  prefs: []
  type: TYPE_NORMAL
- en: The architecture focused on ways to represent extra input and methods to increase
    model understanding of the nature of the data modality through multiple-task learning
    and exceeded prior performance on multiple downstream text-based tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now that we have introduced the base encoder-only model, we are ready to explore
    the different categories of advancements specifically for an encoder-only transformer.
    Improvements will be explored in terms of advancement categories instead of by
    model this time, as the different models combine many advancements, which makes
    it hard to compare actual advancements. The three types of advancements are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Improved data-modality-based tasks used for pre-training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Architectural improvements in terms of compactness, speed, and efficiency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Core/functional architectural improvements
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will skip the details of data-based improvements such as using multilingual
    data to build a multilingual BERT or the fact that sub-word-based tokens are used
    to reduce vocabulary and increase vocabulary reuse between words.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the encoder only pre-training tasks and objectives
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recall that MLM and NSP are used in the base BERT model and that MLM was mentioned
    to be a robust task for the understanding of text language but can be easily adapted
    to tokens from other data modalities. NSP, however, has been proven to be unstable
    and doesn’t concretely help models improve performance in every case. One improvement
    in this direction is to use **sentence order prediction** (**SOP**). SOP demonstrated
    higher consistency in improving the downstream supervised task performance compared
    to simply utilizing the inverted sequence of positive consecutive sentences from
    the same document as a negative sentence. Conceptually, it learns the cohesion
    between sentences instead of trying to simultaneously predict whether two sentences
    are from the same topic or not. This method was introduced in the **ALBERT** model
    in 2020, which mainly focused on transformer efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Another notable improvement is called **replaced token detection** (**RTD**).
    RTD, from the **ELECTRA** model, predicts whether a token is replaced by a random
    token or not and requires that all token positions are learned from, compared
    to the MLM process of only learning from the masked token positions. This was
    introduced as an improvement on the MLM objective.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s dive into architectural improvements in terms of compactness, speed,
    and efficiency.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the encoder-only transformer’s architectural compactness and efficiency
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformers have proved themselves to hold an immense structural capacity to
    take in information that grows along with the model. Some research on transformers
    has been about increasing and scaling the transformer model to the level of hundreds
    of billions of parameters in both decoder-only and encoder-only models. The results
    achieved using these huge models are astounding, but they are unsuitable for practical
    use due to the need to have state-of-the-art GPUs and machines that are not readily
    available or affordable for most people in the world. This is where the line of
    architectural compactness and efficiency improvements can help to level the field.
    Since we are mostly limited by the hardware resource that we can use, if the model
    can be efficiently reduced in size architecturally while maintaining the same
    performance, this would allow a more performant model when you scale it up to
    the limits of your hardware resource.
  prefs: []
  type: TYPE_NORMAL
- en: One of the notable improvements in this line comes again from the ALBERT model,
    a lite BERT model. A key improvement made here was to factorize the embeddings
    layer in the same way as convolutional layers were factorized to make depthwise
    convolutions. A smaller embedding dimension is used with a fully connected layer
    that maps the small dimensional embedding to the same desired dimensions. As embeddings
    are essential weights and contribute to the number of parameters, factorizing
    the embeddings layer allows for a transformer model with fewer parameters. This
    allows BERT to be more performant at a lower number of parameters. Note that some
    improvements add model parallelization and use clever memory management, but these
    will not be covered extensively here. However, since there are some methods that
    speed up the model in general for both inference and training stages, they will
    be discussed and covered in [*Chapter 15*](B18187_15.xhtml#_idTextAnchor217),
    *Deploying Deep Learning Models in Production*, which is all about deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the encoder-only transformers’ core functional architecture
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The most notable improvement made in this line is from the model called **Decoding-enhanced
    BERT with Disentangled Attention** (**DeBERTa**), introduced in 2021, which is
    considered to be the current architectural SOTA. The main idea here is that two
    separate positional encoded embeddings are used, which are relative positions
    and absolute positions. These positional embeddings, however, are not added along
    with the input token embeddings but instead are treated as separate queries and
    keys to be fed into the layers of the transformer, with their own fully connected
    layers. This allows three explicit attention maps to be obtained, specifically,
    content-to-position attention, content-to-content attention, and position-to-content
    attention. The different attention maps are then summed together. The relative
    positions are specifically used by each intermediate multi-head attention layer’s
    internal scaled dot-product attention layer by acting either as the key or query,
    depending on whether it is being attended to by the content or attending to the
    content.
  prefs: []
  type: TYPE_NORMAL
- en: This allows the raw relative positional information to be explicitly considered
    in every attention layer instead of possibly being forgotten after going through
    many layers. We are essentially applying a form of skip connections for relative
    position data. Relative encoding allows the model to learn more generalizable
    representations by conceptually allowing pattern identifiers to be reused at each
    relative position. This solution is similar to how the same convolutional filters
    are used at different partitions of a single-image data instead of using one fully
    connected layer on an entire image, which doesn’t allow the reuse of pattern identifiers
    across the entire input space. As for the usage of absolute positions, it was
    only added due to the nature of text data where the absolute position is needed
    to discern the absolute importance of different things, such as police holding
    more power than most people and trucks being bigger than motorcycles. Absolute
    positions, however, are only added to the last few layers once, using a mechanism
    called **enhanced mask decoder** (**EMD**). EMD applies absolute positional embeddings
    as the query component of the scaled dot-product attention. There are no concrete
    reasons why it is only applied in the final few layers and it can probably be
    extended to be applied in every layer.
  prefs: []
  type: TYPE_NORMAL
- en: Bucketing
  prefs: []
  type: TYPE_NORMAL
- en: A bucketing mechanism to group multiple positions into a single number is used
    to reduce the number of embeddings and thus the number of parameters. Consider
    this as grouped position information that again increases the compactness of the
    model.
  prefs: []
  type: TYPE_NORMAL
- en: Uncovering encoder-only transformers’ adaptations to other data modalities
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Transformers are capable of handling other data modalities. First, the input
    data just needs to be structured properly in a sequence format. After that, since
    pre-training is at the core of a transformer, suitable task objectives need to
    be crafted for the encoder-only transformers to achieve a pre-trained state that
    captures an understanding of the specific data modality. Even though the introduction
    to transformers using text is focused on an unsupervised pre-training method,
    it really can be either supervised, unsupervised, or semi-supervised, where the
    key is to set the weights of the model to be in a state that the subsequent supervised
    downstream task can leverage.
  prefs: []
  type: TYPE_NORMAL
- en: For images, **Vision Transformer** (**ViT**), introduced in 2021, forms the
    base of an image-based transformer and has proved that transformers can achieve
    competitive performance for image-based tasks when compared to convolutional-based
    models. One of the most problematic issues for transformers to handle images is
    that images are big in size and attending to the entire large input space bloats
    up the model size of transformers very quickly. ViT solves this by splitting images
    into systematic patches where each patch would subsequently be fed into a single
    fully connected layer that reduces the dimension of each image patch substantially.
    The patch data with reduced dimensions will be the token embeddings for the transformer.
    The patch conversion is an essential process to represent an image input in a
    format that a transformer can process. Additionally, this patch-based mechanism
    introduces some form of image domain inductive bias, but the rest of the architecture
    remains practically free of inductive bias for image data. As transformers are
    almost always coupled with a pre-training method for performance improvement reasons,
    ViT adopted a similar style of pre-training as the CNN model families, using supervised
    classification pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: For audio, **wav2vec 2.0**, created in 2020, utilized similar concepts to its
    predecessor, **wav2vec**, but instead utilized a decoder-only transformer as part
    of the architecture. A brief overview is that wav2vec 2.0 first encodes audio
    of a specific time window length using 1D convolutions into a lower-dimensional
    space as a feature vector. The encoder feature vectors for each window are then
    fed into the decoder transformer and act as tokens. The unsupervised pre-training
    method is applied here, similarly to MLM from BERT, where a certain portion of
    the input encoded feature vectors is masked, and the task is to predict the masked
    encoded feature vector. After pre-training, the model is then fine-tuned on downstream
    tasks, using a linear layer right after the transformer outputs for tasks such
    as speech recognition. wav2vec 2.0 achieved a new SOTA on a downstream speech
    recognition dataset and proved that audio works extremely well with transformers.
  prefs: []
  type: TYPE_NORMAL
- en: '**VideoBERT**, created in 2019 by Google, not only demonstrates methods to
    model video data for transformers but also the capability of the transformer model
    to learn joint representations in a multimodal way using both video and text.
    Note that a video by itself is also inherently multimodal since video data contains
    multiple image frames as well as an audio data component. In VideoBERT, a cooking
    video sentence pair dataset with clear annotations for the starting and ending
    time stamps of each cooking action is used to pre-train the model. The model takes
    in the sequence of image frames by encoding a preset number of frames into a feature
    vector using a pre-trained 3D convolutional model and treats the feature vector
    as a token. These encoded video tokens are then paired with the text data and
    use a similar objective to MLM to predict masked tokens of videos and text tokens.
    The model then can either be used as a featurizer for video and text-related data
    or similarly be fine-tuned with an additional linear layer on the outputs of the
    transformer model.'
  prefs: []
  type: TYPE_NORMAL
- en: The preceding summaries of different approaches to different data modalities
    are meant to serve as a reminder that transformers can be applied to more than
    just text data and not as an introduction to the current state of the art for
    these modalities. A key takeaway here is that the different data modalities need
    to be arranged into a sequence of token formats with reduced dimensions and suitable
    task objectives need to be crafted for the encoder-only transformers to achieve
    a pre-trained state that captures an understanding of the main nature of the data
    modality. Moreover, the same pre-training method from BERT can be easily adapted
    to other data modalities. So instead of naming it “masked language modeling,”
    maybe it would be better to name it “masked token modeling.” Next, let’s uncover
    the transformer’s improvements using only the decoder.
  prefs: []
  type: TYPE_NORMAL
- en: Uncovering transformer improvements using only the decoder
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Recall that the decoder block of the transformer focuses on an autoregressive
    structure. For the decoder-only transformer line of models, the task of predicting
    tokens autoregressively remains the same. With the removal of the encoder, the
    architecture has to adapt its input to accept more than one sentence, similar
    to what BERT does. Starting, ending, and separator tokens are used to encode input
    data sequentially. Masking is still performed to prevent the model from depending
    on the current token to predict future tokens from the input data during predictions,
    which is similar to the original transformer along with positional embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: Diving into the GPT model family
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: All these architectural concepts were introduced by the GPT model in 2018, which
    is short for **generative pre-training**. As the name suggests, GPT also adopts
    unsupervised pre-training as the initial stage and subsequently moves into the
    supervised fine-tuning stage for any downstream task. GPT is focused on text data,
    but the same concepts can be applied to other data modalities. GPT uses the basic
    language modeling task for pre-training to predict the next token given the prior
    tokens as context with a fixed window to limit the number of context tokens taken
    as input. After the weights are pre-trained, it is subsequently fine-tuned to
    the objectives defined by any of the supervised task types. GPT-1 utilized the
    same language modeling task objective as an auxiliary side objective along with
    the main supervised task as an attempt to boost the performance on downstream
    tasks and showed that it boosts performance only on larger datasets. Furthermore,
    the model showcases first-hand the generalizability of using language modeling
    as a pre-training task to other tasks without actually fine-tuning it and still
    achieving good scores. This line of behavior is termed **zero-shot** and will
    be discussed more extensively in [*Chapter 9*](B18187_09.xhtml#_idTextAnchor149),
    *Exploring Unsupervised Deep Learning*. The model was pre-trained on 7,000 unpublished
    books and had 117 million parameters with 12 repeated decoder blocks.
  prefs: []
  type: TYPE_NORMAL
- en: It’s important to clarify that the GPT model’s ability to perform without fine-tuning
    (few-shot or zero-shot learning) doesn’t negate the benefits that fine-tuning
    can provide in domain-specific tasks. While the GPT line of models has shown impressive
    capabilities in various tasks, fine-tuning may still be advantageous for specific
    applications, leading to improved performance and better adaptation to the unique
    requirements of a given task.
  prefs: []
  type: TYPE_NORMAL
- en: Proceeding to the next advancement, GPT-2 had no concrete architectural changes
    as the main idea but emphasized the capability of zero-shot learning with the
    concept of task conditioning along with scaling the GPT-1 model by 10 times in
    the number of layers, embedding dimensions, the context window size, and vocabulary
    size. Task conditioning is the idea that you tell the model what kind of outputs
    to generate instead of having a fixed, single, known prediction output that is
    determined by the type of task you train it for. To allow true flexibility similar
    to humans, they didn’t add any new architecture to accommodate the task conditioning
    idea but instead decided that the textual input for the transformer could be directly
    used as the task conditioning method. For example, free-form text model input
    such as “I love you = ![](img/01.png), I love deep learning=” naturally directs
    the transformer to perform a translation from English to Mandarin. This can also
    be done differently with an example that specifies both the task and the context
    such as “(translate to mandarin), I love deep learning =”. GPT-2 achieves this
    by pre-training with the language modeling objective on a wide variety of textual
    domains such as news articles, fiction books, Wikipedia, and blogs, essentially
    a much larger dataset compared to the training dataset in GPT-1\. This work exemplifies
    the huge potential of true AI generalization in general even though at the moment
    only text and image data have been utilized. For image data, this is done in a
    model called **DALL-E**, which is capable of generating images according to the
    provided input text.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the GPT-3 model was introduced in 2020\. It is similar to the previous
    advancement, with the main changes being increasing the size of the model in all
    components and characterizing different types of text examples with different
    ways of specifying the task conditions. We won’t go into this too much here. Although
    GPT-2 performed well on zero-shot settings without fine-tuning for the downstream
    tasks, only by fine-tuning could the model exceed previous benchmarks on multiple
    text language datasets. The sheer size of GPT-3 completely removed the need for
    fine-tuning by surpassing previous benchmarks without any form of fine-tuning.
    This model, however, requires specialized machines to train and is not readily
    accessible to individuals or small organizations. It is currently part of OpenAI’s
    API offering. GPT-3 demonstrates a wide range of applicability to many different
    tasks without any fine-tuning, where some of the most notable uses are code generation
    for any programming language, storytelling, and producing amazingly human-seeming
    chatbots. The three improvements pushed the performance limitations on many datasets
    with language-based tasks to higher levels, and serve as an example of the generalizability
    component of machine learning, as well as the immense learning potential of transformers,
    limited only by the amount of hardware resources you have.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discuss a slightly different form of a decoder model that is worth
    mentioning, called the **XLNet** model.
  prefs: []
  type: TYPE_NORMAL
- en: Diving into the XLNet model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: If you and a friend were given the prompt “_ _ is a city,” would you both separately
    predict “New York” to fill in the blanks? This is one of the flaws of MLM objectives
    of the encoder-only transformers as they learn by predicting on multiple tokens
    at once. Standard autoregressive models, however, are not susceptible to this
    issue due to their autoregressive nature of only predicting the future and only
    one token at a time. Encoder-only transformers, however, have a bidirectional
    sequence support that leads to improved performance. Mainly, XLNet works on the
    idea of making an autoregressive model attend to data in a bidirectional way like
    encoder-only models, while maintaining the benefits of an autoregressive model
    of predicting a single output.
  prefs: []
  type: TYPE_NORMAL
- en: The way XLNet achieves the best of both worlds is by conditioning the decoder-only
    transformer during MLM pre-training. The idea is to pre-train the model on all
    permutations of the token’s sequence order by using the masking mechanism. Standard
    autoregressive models use the order of “1-2-3-4” sequentially to predict the fifth
    token during pre-training, but in XLNet, the order can be changed into “3-4-1-5”
    to predict the second token. XLNet also introduced another extra query linear
    layer hidden state along with an additional single fixed learnable embedding that
    will be traversed through the attention layer along with each token embedding,
    as an extra stream along with the original token-based content stream. The extra
    path is called the query stream. The entire process is called the masked two-stream
    attention layer, depicted in *Figure 6**.8*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 6.8 – XLNet workflow](img/B18187_06_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 6.8 – XLNet workflow
  prefs: []
  type: TYPE_NORMAL
- en: The forward propagation mechanism of the content-based stream for any given
    position of a sequence ordering has the capability to attend to the content of
    its own position and the positions before it while the query-based stream for
    any given position of a sequence ordering only has the capability to attend to
    the content of positions before it. *Figure 6**.8* (**a**) shows how the attention
    operation for the content stream for the actual position 1 in the model can attend
    to all tokens in other positions due to the ordering of “3-2-4-1” where position
    1 is permuted to be the last token. *Figure 6**.8* (**b**) shows how the token
    content at position 1 is not used to update the additional query stream path data
    denoted as **g** and that the **g** component acts as the query to obtain the
    next **g** as the output of the attention layer. If the fourth position of the
    model needs to be updated, only the second and third position content will be
    attended to in the content stream, as the first position is the future token according
    to the permuted sequence order. The masking mechanisms are applied dynamically
    based on this concept to prevent each position of **h** and **g** from attending
    to the content positions before it is defined from the permuted sequence order.
    Note the **w** component in *Figure 6**.8* (**c**) denotes the single-weight embedding
    shared across all positions for the query stream. During pre-training, the query
    stream acts as the path for the final classification output for the MLM objective
    where the final **h** components are ignored. When passed on to the fine-tuning
    stage, however, the query stream components and query-based linear layer hidden
    states are discarded or ignored completely.
  prefs: []
  type: TYPE_NORMAL
- en: The novel way of enabling the bidirectional context of the data while leveraging
    the benefits of a single-word prediction-based language modeling objective during
    pre-training led to improved performance on downstream supervised learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Discussing additional advancements for a decoder-only transformer model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Since the advent of GPT, there have not been any highly impactful changes to
    the base decoder-only architecture. Most of the work focuses on three things:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying transformers to different problem types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scaling up the model size to hundreds of billions of parameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Focusing on engineering solutions so that a huge model is feasible to be trained
    on the available hardware resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One of the most notable works utilizing decoder-only transformers for other
    problem types is the DALL-E model from the OpenAI team. Transformers were used
    in both the first and second versions of DALL-E to autoregressively predict a
    compact image embedding given a text input with some other implementation-specific
    inputs. These image embeddings are then transformed into actual image embeddings
    through other mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: For model scaling, GPT-3 showed that the model capacity can still be increased
    along with the performance. First to mention is **Megatron**, which focused on
    making the transformer architecture more efficient with engineering strategies
    made to achieve parallelism and capable of being trained with 512 GPUs, making
    it 5.6 times larger than GPT-2\. GPT-3 outsized Megatron, but another notable
    model called **BLOOM** scaled it to 176B using Megatron as a base to match the
    GPT-3 model size of 175B. What’s most notable is not that it is bigger than GPT-3
    but the fact that BLOOM gives birth to a new paradigm – open source – which means
    that the collective work and contributions of the community of researchers and
    institutions can also train highly useful and performant models on a massive scale.
    These huge models in the past have always been exclusive to big corporations due
    to their immense hardware resources. Other than that, another model, subsequently
    released in 2022, that scaled up GPT to 540 billion parameters is **PaLM**, which
    exceeded the performance of every other smaller GPT variant model. **PaLM 2**
    outperforms its predecessor by offering enhanced performance across various tasks,
    including English and multilingual understanding, reasoning, and code generation.
    It achieves this by optimizing computing scaling laws, utilizing diverse multilingual
    datasets, and implementing architectural improvements. PaLM 2 excels in multilingual
    proficiency, classification, question-answering, and translation tasks, demonstrating
    its broad applicability. The model also incorporates control tokens to mitigate
    toxicity and provides guidelines for responsible development and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: When selecting a transformer model for a specific task, it’s crucial to take
    a balanced approach, considering several factors in addition to performance metrics.
    Model size and resource requirements play a significant role in determining the
    feasibility and applicability of the chosen model. While larger models such as
    GPT-3 have shown impressive capabilities, their resource demands may not be suitable
    for all situations, especially for individuals or small organizations with limited
    computational resources.
  prefs: []
  type: TYPE_NORMAL
- en: It’s essential to weigh the trade-offs between model size, performance, and
    resource requirements when choosing a transformer for a particular task. In some
    cases, smaller models may provide adequate performance while being more efficient
    and environmentally friendly. Additionally, fine-tuning can often improve the
    performance of a model in domain-specific tasks, even if it is not as large as
    some of the most prominent models such as GPT-3\. Ultimately, the best choice
    depends on the unique requirements and constraints of the specific task, as well
    as the resources available for model training and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Transformers are versatile NNs capable of capturing relationships of any data
    modality without explicit data-specific biases in the architecture. Instead of
    a neural network architecture capable of ingesting different data modalities directly,
    careful considerations of the data input structure along with crafting proper
    training task objectives are needed to successfully build a performant transformer.
    The benefits of pre-training still hold true even for the current SOTA architecture.
    The act of pre-training is part of a concept called transfer learning, which will
    be covered more extensively in the supervised and unsupervised learning chapters.
    Transformers can currently perform both data generation and supervised learning
    tasks in general with more and more research experimenting with using transformers
    in unexplored niche tasks and data modalities. Look forward to more deep learning
    innovations in the coming years with transformers being at the forefront of the
    advancement.
  prefs: []
  type: TYPE_NORMAL
- en: 'By now, you have gained the knowledge needed to appropriately choose and design
    a neural network architecture according to your data and requirements. Most of
    the architectures and concepts presented in *Chapters 2* to *6* are tricky to
    implement from scratch but with the help of open source work such as [https://github.com/rwightman/pytorch-image-models](https://github.com/rwightman/pytorch-image-models)
    and the help of many deep learning frameworks, using a model is a matter of importing
    libraries and adding a few lines of code. Truthfully, understanding what goes
    into each of the architectures under the hood is not actually needed to utilize
    these models, due to the ease of adopting publicly available work complete with
    pre-trained weights on big datasets. More often than not, understanding architectural
    concepts such as CNNs at a high level and knowing some bits and pieces about training
    a model is really all it takes today to benefit practically from these mostly
    ready-off-the-shelf architectures/models. However, understanding the implementation
    details of these architectures is essential and will prove to be beneficial when
    it comes to the following cases:'
  prefs: []
  type: TYPE_NORMAL
- en: When things fail or don’t work as expected (the model does not converge to an
    optimum solution or diverges, model errors with the prepared input with unexpected
    data shapes, and so on).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When it’s necessary to choose a more appropriate architecture based on your
    dataset, runtime requirements, or performance requirements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you are inventing a shiny new neural network layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When you want to decode what the neural network actually learns and when you
    want to understand why the neural network makes its predictions. This will be
    explored in more depth in [*Chapter 11*](B18187_11.xhtml#_idTextAnchor172), *Explaining
    Neural Network Predictions*, and *Chapter 12*, *Interpreting* *Neural Networks*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To adopt concepts from one architecture domain to another domain. For example,
    skip connections from DenseNet and ResNet can be easily transferable to MLPs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since you’ve read to the end of this chapter, which completes the neural network
    architecture specific content, give yourself a pat on the back. You now have knowledge
    about models that deal with images (CNNs), time-series or sequence data (RNNs),
    models that deal with tabular data (MLPs), and models that are a jack of all trades
    (transformers). The performance of architectures, however, is closely coupled
    with the data type, the data structure, the data preprocessing method, the weights
    learning and optimizing method, the loss function, and the optimization task.
    More details about these components will be decoded in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125),
    *Exploring Supervised Deep Learning*, and [*Chapter 9*](B18187_09.xhtml#_idTextAnchor149),
    *Exploring Unsupervised* *Deep Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore an emerging method to design neural network
    models in an automated way called neural architecture search.
  prefs: []
  type: TYPE_NORMAL
