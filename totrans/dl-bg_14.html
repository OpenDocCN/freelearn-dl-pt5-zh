<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Deep and Wide Neural Networks
                </header>
            
            <article>
                
<p class="mce-root">So far, we have covered a variety of unsupervised deep learning methodologies that can lead to many interesting applications, such as feature extraction, information compression, and data augmentation. However, as we move toward supervised deep learning methodologies that can perform classification or regression, for example, we have to begin by addressing an important question related to neural networks that might be in your mind already: <em>what is the difference between wide and deep neural networks?</em> </p>
<p class="mce-root">In this chapter, you will implement deep and wide neural networks to see the difference in the performance and complexities of both. As a bonus, we will cover the concepts of dense networks and sparse networks in terms of the connections between neurons. We will also optimize the dropout rates in our networks to maximize the generalization ability of the network, which is a critical skill to have today.</p>
<p><span>This chapter is organized as follows:</span></p>
<ul>
<li><span>Wide neural networks</span></li>
<li><span>Dense deep neural networks</span></li>
<li><span>Sparse deep neural networks</span></li>
<li><span>Hyperparameter optimization</span></li>
</ul>
<h1 id="uuid-48da11ac-7959-44c5-8ea6-66673dccee52">Wide neural networks</h1>
<p>Before we discuss the types of neural networks covered in this chapter, it might be appropriate to revisit the definition of deep learning and then continue addressing all these types. </p>
<h2 id="uuid-5e0a7351-50b2-4e48-bad9-20e79d94fb0f">Deep learning revisited</h2>
<p>Recently, on February 9, 2020, Turing Award winner Yann LeCun gave an interesting talk at the AAAI-20 conference in New York City. In his talk, he provided clarity with respect to what deep learning is, and before we give this definition here, let me remind you that LeCun (along with J. Bengio, and G. Hinton) is considered one of the fathers of deep learning, and received the Turing Award for precisely his achievements in the area. Therefore, what he has to say is important. Secondly, throughout this book, we have not given a strong definition of what deep learning is; people might be thinking that it refers to deep neural networks, but that is not factually correct – it is much more than that, so let's set the record straight once and for all.</p>
<div class="packt_quote">"It is not just supervised learning, it is not just neural networks, <strong>Deep Learning</strong> is the idea of building a system by assembling parametrized modules into a (possibly dynamic) computation graph, and training it to perform a task by optimizing the parameters using a gradient-based method." - Yann LeCun</div>
<p>Most of the models we have covered so far fit this definition, with the exception of the simple introductory models that we used to explain the more complex ones. The only reason why those introductory models are not included as deep learning is that they are not necessarily part of a computation graph; we are referring specifically to the perceptron (Rosenblatt, F. (1958)<em>)</em>, and the corresponding <strong>Perceptron Learning Algorithm</strong> (<strong>PLA</strong>) (Muselli, M. (1997)<em>)</em>. However, from the <strong>Multilayer Perceptron</strong> (<strong>MLP</strong>) and forward, all algorithms presented so far are, in fact, deep learning algorithms.</p>
<p>This is an important distinction to make at this point since this is a deep learning book, and you are <em>learning</em> about deep learning. We are about to learn some of the most interesting topics in deep learning and we need to keep a focus on what deep learning is. We will talk about deep networks and wide networks; however, both are deep learning. In fact, all the models we will be discussing here are deep learning models.</p>
<p>With this clarification in mind, let's define what a wide network is.</p>
<h2 id="uuid-0904fe71-5be7-487e-bd14-da87f558e504">Wide layers</h2>
<p>What makes a neural network <strong>wide</strong> is a relatively large number of neurons in a relatively small number of hidden layers. Recent developments in deep learning have even made possible the computational treatment of wide networks with an infinite amount of neural units (Novak, R., et al. (2019)<em>)</em>. Although this is a very nice advance in the field, we will limit our layers to have a reasonable number of units. To make our comparison with a <em>less wide</em> network, we will create a wide network for the CIFAR-10 dataset. We will create the architecture shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/556ac356-0a15-48e3-91b0-68750f573abf.png" style="width:33.00em;height:25.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.1 – Network architecture of a wide network for CIFAR-10</div>
<p>One important aspect of neural networks that we will consider from now on is the number of <strong>parameters</strong>.</p>
<div class="packt_infobox">In deep learning, the number of <strong>parameters</strong> is defined as the number of variables that the learning algorithm needs to estimate through gradient descent techniques in order to minimize the loss function. The great majority of parameters are, usually, the weights of a network; however, other parameters might include biases, mean and standard deviation for <em>batch normalization</em>, filters for convolutional networks, memory vectors for recurrent networks, and many others.</div>
<p>Knowing the number of parameters has been of particular importance because, in an ideal world, you want to have more data samples than variables you want to learn. In other words, an ideal learning scenario includes more data than parameters. If you think about it, it is intuitive; imagine having a matrix with two rows and three columns. The three columns describe the color representation in red, green, and blue of a fruit. The two rows correspond to one sample for an orange and another one for an apple. If you want to build a linear regression system to determine the probability of the data being from an orange, you certainly would like to have a lot more data! Especially since there are many apples that may have a color that is close to the color of an orange. More data is better! But if you have more parameters, like in linear regression where you have as many parameters as columns, then your problem is usually described as an <em>ill-posed </em>problem. In deep learning, this phenomenon is known as <strong>over-parametrization</strong>. </p>
<p>Only in deep learning do over-parametrized models work really well. There is research that has shown that in the particular case of neural networks, given the redundancy of data as it flows in non-linear relationships, the loss functions can produce smooth landscapes (Soltanolkotabi, M., et al. (2018)). This is particularly interesting because then we could prove that over-parametrized deep learning models will converge to very good solutions using gradient descent (<span>Du, S. S., et al. (2018))</span>.</p>
<h3 id="uuid-9c383df5-2850-4ec8-98a8-bcfc0a63ed12">Summaries</h3>
<p>In Keras, there is a function called <kbd>summary()</kbd> that, called from a <kbd>Model</kbd> object, can give the total number of parameters to be estimated. For example, let's create the wide network in <em>Figure 11.1</em>:</p>
<pre>from tensorflow.keras.layers import Input, Dense, Dropout<br/>from tensorflow.keras.models import <strong>Model</strong><br/><br/>inpt_dim = 32*32*3    # this corresponds to the dataset<br/>                      # to be explained shortly<br/>inpt_vec = Input(shape=(inpt_dim,), <strong>name</strong>='inpt_vec')<br/>dl = Dropout(0.5, <strong>name</strong>='d1')(inpt_vec)<br/>l1 = Dense(inpt_dim, activation='relu', <strong>name</strong>='l1')(dl)<br/>d2 = Dropout(0.2, <strong>name</strong>='d2')(l1)<br/>l2 = Dense(inpt_dim, activation='relu', <strong>name</strong>='l2') (d2)<br/>output = Dense(10, activation='sigmoid', <strong>name</strong>='output') (l2)<br/><br/>widenet = Model(inpt_vec, output, <strong>name</strong>='widenet')<br/><br/>widenet.compile(loss='binary_crossentropy', optimizer='adam')<br/>widenet.<strong>summary()</strong></pre>
<p class="mce-root"/>
<p>This code produces the following output:</p>
<pre>Model: "widenet"<br/>_________________________________________________________________<br/>Layer (type)           Output Shape    Param # <br/>=================================================================<br/>inpt_vec (InputLayer)  [(None, 3072)]  0 <br/>_________________________________________________________________<br/>d1 (Dropout)           (None, 3072)    0 <br/>_________________________________________________________________<br/>l1 (Dense)             (None, 3072)    9440256 <br/>_________________________________________________________________<br/>d2 (Dropout)           (None, 3072)    0 <br/>_________________________________________________________________<br/>l2 (Dense)             (None, 3072)    9440256 <br/>_________________________________________________________________<br/>output (Dense)         (None, 10)      30730 <br/>=================================================================<br/>Total params: <strong>18,911,242</strong><br/>Trainable params: 18,911,242<br/>Non-trainable params: 0</pre>
<p>The summary produced here indicates that the total number of parameters in the model is 18,911,242. This is to show that a simple wide network can have nearly 19 million parameters for a problem with 3,072 features. This is clearly an over-parametrized model on which we will perform gradient descent to learn those parameters; in other words, this is a deep learning model.</p>
<h3 id="uuid-f19ec4a6-5713-4a55-a44d-bb453a8f8362">Names</h3>
<p>Another new thing that we will introduce in this chapter is the use of <strong>names</strong> for individual pieces of the Keras models. You should have noticed that in the preceding code, the script contains a new argument with a string assigned to it; for example, <kbd>Dropout(0.5, <strong>name</strong>='d1')</kbd>. This is used internally to keep track of the names of pieces in the model. This can be good practice; however, it is not required. If you do not provide names, Keras will automatically assign generic names to each individual piece. Assigning names to elements can be helpful when saving or restoring models (we will do that soon enough – be patient), or can be useful when printing summaries, like the preceding one.</p>
<p>Now, let's look at the dataset that we will load. Precisely, the data mentioned earlier that has 3,072 dimensions, called CIFAR-10.</p>
<p class="mce-root"/>
<h2 id="uuid-b7585742-0f87-45f5-86b6-5d3c84fb22e7">The CIFAR-10 dataset</h2>
<p>The dataset we will work with in this chapter is called <strong>CIFAR-10</strong>. It comes from the acronym <strong>Canadian Institute For Advanced Research</strong> (<strong>CIFAR</strong>). The number 10 comes from the number of classes with which the dataset is organized. It is a dataset of color images that also has an alternative database with 100 different objects, known as CIFAR-100; however, we will focus on CIFAR-10 for now. Each color image is <img class="fm-editor-equation" src="assets/4de98ef3-ecc3-425b-8729-23683bac81a5.png" style="width:3.42em;height:0.92em;"/> pixels. Its total dimensions, considering the color channels, is <img class="fm-editor-equation" src="assets/7cc49236-3995-41c3-a6b9-2a8a255a67c9.png" style="width:8.83em;height:0.92em;"/>.</p>
<p>The diagram in <em>Figure 11.1</em> has one image sample and <em>Figure 11.2</em> has an example for each class within the test set:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/3fa9732f-d25a-4a1b-b7f4-81561e948eaf.png" style="width:39.17em;height:16.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.2 – Sample color images for each class in the CIFAR-10 dataset</div>
<p>This dataset can be loaded by executing the following commands:</p>
<pre>from tensorflow.keras.datasets import <strong>cifar10</strong><br/>from tensorflow.keras.utils import to_categorical<br/>import NumPy as np<br/><br/>(x_train, y_train), (x_test, y_test) = <strong>cifar10.load_data()</strong><br/><br/># Makes images floats between [0,1]<br/>x_train = x_train.astype('float32') / 255.<br/>x_test = x_test.astype('float32') / 255.<br/><br/># Reshapes images to make them vectors of 3072-dimensions<br/>x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))<br/>x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))<br/><br/># Converts list of numbers into one-hot encoded vectors of 10-dim<br/>y_train = to_categorical(y_train, 10)<br/>y_test = to_categorical(y_test, 10)<br/><br/>print('x_train shape:', x_train.shape)<br/>print('x_test shape:', x_test.shape)</pre>
<p>This downloads the data automatically and produces the following output:</p>
<pre>x_train shape: (50000, 3072)<br/>x_test shape: (10000, 3072)</pre>
<p>These things are nothing new except for the dataset. For more information about how this dataset was prepared, please check <a href="8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml">Chapter 3</a>, <em>Preparing Data</em>, where we go over how to convert data into usable data by normalizing it and converting targets to one-hot encoding.</p>
<p>The output we receive by printing the shape of the dataset, using the <kbd>.shape</kbd> attribute of NumPy arrays, tells us that we have 50,000 samples to train with, and 10,000 samples on which to test our training performance. This is the standard split in the deep learning community and helps the comparison among methodologies.</p>
<h2 id="uuid-7b0acd32-5e50-4cb8-9ae2-ab86c482d0c7">New training tools</h2>
<p>With the code we have so far, we can easily begin the training process by invoking the <kbd>fit()</kbd> method as follows:</p>
<pre>widenet.<strong>fit</strong>(x_train, y_train, epochs=100, batch_size=1000, <br/>            shuffle=True, validation_data=(x_test, y_test))</pre>
<p>This is nothing new; we covered all these details in <a href="c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml">Chapter 9</a>, <em>Variational Autoencoders</em>. However, we want to introduce new important tools into the mix that will help us train a better model, much more efficiently, and preserve our best-trained model.</p>
<h3 id="uuid-2492a35a-5c95-4d99-a3d6-19db6a7b10d6">Saving or loading models</h3>
<p>Saving our trained model is important if we want to sell a product, or distribute a working architecture, or to control versions of models. These models can be saved by calling on either of the following methods:</p>
<ul>
<li><kbd>save()</kbd>, used to save the whole model, including optimizer states such as the gradient descent algorithm, the number of epochs, the learning rate, and others.</li>
<li><kbd>save_weights()</kbd>, used to save only the parameters of the model.</li>
</ul>
<p class="mce-root"/>
<p>For example, we can save our model's weights as follows:</p>
<pre>widenet.save_weights("widenet.hdf5")</pre>
<p>This will create a file in your local disk called <kbd>widenet.hdf5</kbd>. This type of file extension is for a standard file format called <strong>Hierarchical Data Format</strong> (<strong>HDF</strong>), which enables consistency across common platforms, and, therefore, the easy sharing of data.</p>
<p>You can re-load a saved model later on by simply executing the following:</p>
<pre>widenet.load_weights("widenet.hdf5")</pre>
<p>Note that doing this relies on you building the model first, that is, creating <strong>exactly</strong> all the layers of the model in the exact same order and with the exact same names. An alternative, to save all the effort of reconstructing the model exactly, is to use the <kbd>save()</kbd> method.</p>
<div>
<pre><span>widenet.save(</span><span>"widenet.hdf5"</span><span>)</span></pre></div>
<p>However, the downside of using the <kbd>save()</kbd> method is that to load the model you will need to import an additional library, as follows:</p>
<pre>from tensorflow.keras.models import <strong>load_model</strong><br/><br/>widenet = <strong><span>load_model</span></strong>("widenet.hdf5")</pre>
<p>This essentially removes the need to re-create the model. Throughout this chapter, we will be saving our model weights simply so that you get used to it. Now let's take a look at how to use <strong>callbacks</strong>, which are interesting ways to monitor the learning process. We will start with a <strong>callback </strong>for reducing the learning rate.</p>
<h3 id="uuid-5ea707c9-ed58-4e50-aac8-9088378b5eea">Reducing the learning rate on the fly</h3>
<p>Keras has a superclass for <strong>callbacks</strong>, found in <span><kbd>tensorflow.keras.callbacks</kbd>, </span>where we have, among other nice things, a class for reducing the learning rate of the learning algorithm. If you don't remember what the <strong>learning rate</strong> is, feel free to go back to <a href="a6dd89cc-54bd-454d-8bea-7dd4518e85b0.xhtml">Chapter 6</a>, <em>Training Multiple Layers of Neurons</em>, to review the concept. But, as a quick recap, the learning rate controls how big the steps that are taken to update the parameters of the model in the direction of the gradient are.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The problem is that, many times, you will encounter that certain types of deep learning models <em>get stuck</em> in the learning process. By <em>getting stuck</em> we mean that there is no progress being made toward reducing the loss function either on the training or validation set. The technical term the <em>professionals </em>use is that the learning looks like a <strong>plateau</strong>. It is a problem that is evident when you look at how the loss function is minimized across epochs because it looks like a <em>plateau</em>, that is, a flat line. Ideally, we want to see the loss decreasing at every epoch, and it is usually the case for the first few epochs, but there can be a time when reducing the learning rate can help the learning algorithm to <em>focus</em> by making small changes to the existing acquired knowledge, that is, the learned parameters.</p>
<p>The class we are discussing here is called <kbd>ReduceLROnPlateau</kbd>. You can load it as follows:</p>
<pre>from tensorflow.keras.callbacks import ReduceLROnPlateau</pre>
<p>To use this library, you will have to use the <kbd>callbacks</kbd> argument in the <kbd>fit()</kbd> function after defining it like this:</p>
<pre><strong>reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=20)</strong><br/><br/>widenet.<strong>fit</strong>(x_train, y_train, batch_size=128, epochs=100, <br/>            <strong>callbacks=reduce_lr</strong>, shuffle=True, <br/>            validation_data=(x_test, y_test))</pre>
<p>In this code fragment, we call <kbd>ReduceLROnPlateau</kbd> with the following arguments:</p>
<ul>
<li><kbd>monitor='val_loss'</kbd>, this is the default value, but you can change it to look for a plateau in the <kbd>'loss'</kbd> curve.</li>
<li><kbd>factor=0.1</kbd>, this is the default value, and it is the rate by which the learning rate will be reduced. For example, the default learning rate for the Adam optimizer is 0.001, but when a plateau is detected, it will be multiplied by 0.1, leading to a new updated learning rate of 0.0001.</li>
<li><kbd>patience=20</kbd>, the default value is 10, and is the number of epochs with no improvement in the monitored loss, which will be considered a plateau.</li>
</ul>
<p>There are other arguments that you can use in this method, but these are the most popular, in my opinion. </p>
<p>Next, let's look at another important callback: <em>early stopping</em>.</p>
<p class="mce-root"/>
<h3 id="uuid-18960a73-1504-4b88-aa04-541fb43436e2">Stopping the learning process early</h3>
<p>This next callback is interesting because it allows you to stop the training if there is no progress being made and <strong>it allows you to keep the best version of the model </strong>during the learning process. It is found in the same class as the preceding one and is called <kbd>EarlyStopping()</kbd>, and you can load it as follows:</p>
<pre>from tensorflow.keras.callbacks import EarlyStopping</pre>
<p>The early stopping callback essentially lets you stop the training process if there has been no progress in the last few epochs, as specified in the <kbd>patience</kbd> parameter. You can define and use the early stopping callback as follows:</p>
<pre><strong>stop_alg = EarlyStopping(monitor='val_loss', patience=100, restore_best_weights=True)</strong><br/><br/>widenet.fit(x_train, y_train, batch_size=128, <strong>epochs=1000</strong>, <br/>            callbacks=<strong>stop_alg</strong>, shuffle=True, <br/>            validation_data=(x_test, y_test))</pre>
<p>Here is a short explanation of each of the arguments used in <kbd>EarlyStopping()</kbd>:</p>
<ul>
<li><kbd>monitor='val_loss'</kbd>, <span>this is the default value, but you can change it to look for changes in the </span><kbd>'loss'</kbd><span> curve.</span></li>
<li><kbd>patience=100</kbd>, <span>the default value is 10 and is the number of epochs with no improvement in the monitored loss. I personally like to set this to a larger number than the patience in <kbd>ReduceLROnPlateau</kbd>, because I like to let the learning rate produce an improvement in the learning process (hopefully) before I terminate the learning process because there was no improvement.</span></li>
<li><kbd>restore_best_weights=True</kbd>, the default value is <kbd>False</kbd>. If <kbd>False</kbd>, the model weights obtained at the last epoch are preserved. However, if set to <kbd>True</kbd>, it will preserve and return the best weights at the end of the learning process. </li>
</ul>
<p>This last argument is my personal favorite because I can set the number of epochs to a large number, within reason, and let the training go for as long as it needs. In the preceding example, if we set the number of epochs to 1,000, it does not necessarily mean that the learning process will go for 1,000 epochs, but if there is no progress within 50 epochs, the process can stop early. If the process gets to a point at which it has learned good parameters, it can get to a point at which there is no progress, then stop after 50 epochs and still return the best model that was ever recorded during the learning process.</p>
<p>We can combine all the preceding callbacks and the saving methodology as follows:</p>
<pre>from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping<br/><br/>reduce_lr = <strong>ReduceLROnPlateau</strong>(monitor='val_loss', factor=0.5, patience=20)<br/>stop_alg = <strong>EarlyStopping</strong>(monitor='val_loss', patience=100, <br/>                         restore_best_weights=True)<br/><br/><strong>hist</strong> = widenet.fit(x_train, y_train, batch_size=1000, epochs=1000, <br/>                   callbacks=<strong>[stop_alg, reduce_lr]</strong>, shuffle=True, <br/>                   validation_data=(x_test, y_test))<br/><br/>widenet.<strong>save_weights</strong>("widenet.hdf5")</pre>
<p>Notice that the callbacks have been combined into a list of callbacks that will monitor the learning process, looking for plateaus to decrease the learning rate, or looking to stop the process if there has been no improvement in a few epochs. Also, notice that we created a new variable, <kbd>hist</kbd>. This variable contains a dictionary with logs of the learning process, such as the losses across epochs. We can plot such losses to see how the training takes place as follows:</p>
<pre>import matplotlib.pyplot as plt<br/><br/>plt.plot(<strong>hist.history['loss']</strong>, color='#785ef0')<br/>plt.plot(<strong>hist.history['val_loss']</strong>, color='#dc267f')<br/>plt.title('Model reconstruction loss')<br/>plt.ylabel('Binary Cross-Entropy Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training Set', 'Test Set'], loc='upper right')<br/>plt.show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>This produces the curves in <em>Figure 11.3</em>:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/fe6ae368-567e-4d3c-9a03-630d7eedfdaf.png" style="width:41.58em;height:26.08em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 11.3 – Model loss of widenet across epochs using callbacks</div>
<p>From the figure, we can clearly see the evidence of the learning rate reduction around epoch 85 where the learning is adjusted after the plateau in the validation loss (that is, the loss over the test set); however, this has little effect on the validation loss, and therefore training is terminated early around epoch 190 since there was no improvement in the validation loss.</p>
<p>In the next section, we will analyze the performance of the <kbd>widenet</kbd> model in a quantitative manner that will allow comparison later on.</p>
<p class="mce-root"/>
<h2 id="uuid-f49007f3-2b6a-4b46-8977-955fe113ccbe">Results</h2>
<p>Here, we want to simply explain the performance of the network in terms that are easy to understand and to communicate to others. We will be focusing on analyzing the confusion matrix of the model, precision, recall, F1-score, accuracy, and balanced error rate. If you do not recall what these terms mean, please go back and quickly review <a href="7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml">Chapter 4</a>, <em>Learning from Data</em>.</p>
<p>One of the nice things about scikit-learn is that it has a nice automated process for calculating a report for classification performance that includes most of the terms mentioned above. It is simply called a <strong>classification report</strong>. This and the other libraries that we will need can be found in the <kbd>sklearn.metrics</kbd> class and can be imported as follows:</p>
<pre>from sklearn.metrics import <strong>classification_report</strong><br/>from sklearn.metrics import <strong>confusion_matrix</strong><br/>from sklearn.metrics import <strong>balanced_accuracy_score</strong></pre>
<p>These three libraries operate in a similar way – they take the ground truth and the predictions to evaluate performance:</p>
<pre>from sklearn.metrics import classification_report<br/>from sklearn.metrics import confusion_matrix<br/>from sklearn.metrics import balanced_accuracy_score<br/>import NumPy as np<br/><br/><strong>y_hat = widenet.predict(x_test)    </strong># we take the neuron with maximum<br/>y_pred = np.argmax(y_hat, axis=1)  # output as our prediction<br/><br/>y_true = np.argmax(y_test, axis=1)   # this is the ground truth<br/>labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]<br/><br/>print(<strong>classification_report</strong>(y_true, y_pred, labels=labels))<br/><br/>cm = <strong>confusion_matrix</strong>(y_true, y_pred, labels=labels)<br/>print(cm)<br/><br/>ber = 1- <strong>balanced_accuracy_score</strong>(y_true, y_pred)<br/>print('BER:', ber)</pre>
<p class="mce-root"/>
<p>This code outputs something like this:</p>
<pre>    precision  recall  f1-score  support<br/>0   0.65       0.59    0.61      1000<br/>1   0.65       0.68    0.67      1000<br/>2   0.42       0.47    0.44      1000<br/>3   0.39       0.37    0.38      1000<br/>4   0.45       0.44    0.44      1000<br/>5   0.53       0.35    0.42      1000<br/>6   0.50       0.66    0.57      1000<br/>7   0.66       0.58    0.62      1000<br/>8   0.62       0.71    0.67      1000<br/>9   0.60       0.57    0.58      1000<br/>              <br/>accuracy               0.54      10000<br/><br/><br/>[[587  26  86  20  39   7  26  20 147  42]<br/> [ 23 683  10  21  11  10  22  17  68 135]<br/> [ 63  21 472  71 141  34 115  41  24  18]<br/> [ 19  22  90 370  71 143 160  43  30  52]<br/> [ 38  15 173  50 442  36 136  66  32  12]<br/> [ 18  10 102 224  66 352 120  58  29  21]<br/> [  2  21  90  65  99  21 661   9  14  18]<br/> [ 36  15  73  67  90  45  42 582  13  37]<br/> [ 77  70  18  24  17   3  20   9 713  49]<br/> [ 46 167  20  28  14  14  30  36  74 571]]<br/><br/><br/>BER: 0.4567</pre>
<p>The part on the top indicates the output of <kbd>classification_report()</kbd>. It gives the precision, recall, f1-score, and accuracy of the model. Ideally, we want to have all of those numbers as close to 1.0 as possible. Intuitively, the accuracy needs to be 100% (or 1.0); however, the rest of the numbers require careful study. From this report, we can observe that the total accuracy is 54%. From the rest of the report, we can determine that the classes that are more accurately classified are 1 and 8, corresponding to <em>automobile</em> and <em>ship</em>. Similarly, we can see that the two classes most poorly classified are 3 and 5, corresponding to <em>cats </em>and <em>dogs</em>, respectively.</p>
<p>While these numbers are informative, we could look into what is the source of the confusion, by looking at the confusion matrix, which is the group of numbers produced by <kbd>confusion_matrix()</kbd>. If we inspect the confusion matrix on row number four (corresponding to label 3, <em>cats</em>), we see that it correctly classifies 370 cats as cats, but 143 cats were classified as dogs, and 160 cats were classified as frogs, just to name the most serious areas of confusions. Another way to look at it is visually, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/4316c4aa-3eac-45dd-84a5-b7025c5926b2.png" style="width:29.83em;height:25.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.4 – Confusion matrix visualization for the widenet model</div>
<p>Ideally, we want to see a confusion matrix that is diagonal; however, in this case, we don't see that effect. After visual inspection, from <em>Figure 11.4</em>, we can observe which classes have the lowest correct predictions and confirm, visually, where the confusion is. </p>
<p>Finally, it is important to note that while <strong>classification accuracy</strong> (<strong>ACC</strong>) is 54%, we still need to verify the <strong>balanced error rate</strong> (<strong>BER</strong>) to complement what we know about the accuracy. This is particularly important when the classes are not evenly distributed, that is, when there are more samples for some classes compared to others. As explained in <span><a href="7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml">Chapter 4</a>, </span><em>Learning from Data</em>, we can simply calculate the balanced accuracy and subtract it from one. This reveals that the BER is 0.4567, or 45.67%. In an ideal world, we want to lower the BER to zero, and definitely stay away from a BER of 50%, which would imply that the model is no better than random chance. </p>
<p>In this case, the accuracy of the model is not impressive; however, this is a very challenging classification problem for fully connected networks, thus, this performance is not surprising. Now, we will try to do a similar experiment, changing from a relatively wide network, to a deep network, and compare the results.</p>
<p class="mceNonEditable"/>
<h1 id="uuid-1a1220b0-e4a3-4dd9-821a-e36ee892a2f3">Dense deep neural networks</h1>
<p>It is widely known that deeper networks can offer good performance in classification tasks (<span>Liao, Q., et al. (2018))</span>. In this section, we want to build a deep dense neural network and see how it performs in the CIFAR-10 dataset. We will be building the model shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/2063cb67-f41e-4a7b-b26d-9cab8a459240.png" style="width:37.92em;height:36.25em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.5 – Network architecture of a deep dense network for CIFAR-10</div>
<p>One of the aims of this model is to have the same number of neural units as the model in <em>Figure 11.1</em>, for the wide network. This model has a bottleneck architecture, where the number of neurons decreases as the network gets deeper. This can be coded programmatically using the Keras functional approach, as we discuss next.</p>
<h2 id="uuid-9e5bf5a3-16b7-4209-8c18-7bade2c7d000">Building and training the model</h2>
<p>One interesting fact about Keras' functional approach is that we can <strong>recycle</strong> variable names as we build the model and that we can even build a model using a loop. For example, let's say that I would like to create dense layers with dropout rates that exponentially decrease along with the number of neurons by a factor of 1.5 and 2, respectively.</p>
<p>We could achieve this by having a cycle that uses an initial dropout rate, <kbd>dr</kbd>, and an initial number of neural units, <kbd>units</kbd>, and decreases both by a factor of 1.5 and 2, respectively, every time, as long as the number of neural units is always greater than 10; we stop at 10 because the last layer will contain 10 neurons, one for each class. It looks something like this:</p>
<pre class="mce-root">while units &gt; 10: <br/>  dl = Dropout(dr)(dl)<br/>  dl = Dense(units, activation='relu')(dl)<br/>  units = units//2<br/>  dr = dr/1.5</pre>
<p>The preceding code snippet illustrates that we can reuse variables without confusing Python since TensorFlow operates over a computational graph that has no problem in resolving parts of the graph in the correct sequence. The code also shows that we can create a bottleneck-type of network very easily with an exponentially decaying number of units and dropout rate. </p>
<p>The full code to build this model looks like this:</p>
<pre class="mce-root"># Dimensionality of input for CIFAR-10<br/>inpt_dim = 32*32*3<br/><br/>inpt_vec = Input(shape=(inpt_dim,))<br/><br/>units = inpt_dim    # Initial number of neurons <br/>dr = 0.5    # Initial drop out rate   <br/><br/>dl = Dropout(dr)(inpt_vec)<br/>dl = Dense(units, activation='relu')(dl)<br/><br/># Iterative creation of bottleneck layers<br/><strong>units = units//2</strong><br/><strong>dr = dr/2</strong><br/><strong>while units&gt;10: </strong><br/><strong>  dl = Dropout(dr)(dl)</strong><br/><strong>  dl = Dense(units, activation='relu')(dl)</strong><br/><strong>  units = units//2</strong><br/><strong>  dr = dr/1.5</strong><br/><br/># Output layer<br/>output = Dense(10, activation='sigmoid')(dl)<br/><br/>deepnet = Model(inpt_vec, output)</pre>
<p>Compiling and training the model goes like this:</p>
<pre class="mce-root">deepnet.compile(loss='binary_crossentropy', optimizer='adam')<br/><strong>deepnet.summary()</strong><br/><br/>reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, <br/>                              min_delta=1e-4, mode='min')<br/>stop_alg = EarlyStopping(monitor='val_loss', patience=100, <br/>                         restore_best_weights=True)<br/>hist = deepnet.<strong>fit</strong>(x_train, y_train, batch_size=1000, epochs=1000, <br/>                   callbacks=[stop_alg, reduce_lr], shuffle=True, <br/>                   validation_data=(x_test, y_test))<br/><br/>deepnet.save_weights("deepnet.hdf5")</pre>
<p>This produces the following output, caused by <kbd>deepnet.summary()</kbd> in the preceding code:</p>
<pre>Model: "model"<br/>_________________________________________________________________<br/>Layer (type)          Output Shape    Param # <br/>=================================================================<br/>input_1 (InputLayer)  [(None, 3072)]  0 <br/>_________________________________________________________________<br/>dropout (Dropout)     (None, 3072)    0 <br/>_________________________________________________________________<br/>dense (Dense)         (None, 3072)    9440256 <br/>_________________________________________________________________<br/>.<br/>.<br/>.<br/>_________________________________________________________________<br/>dense_8 (Dense)       (None, 12)      300 <br/>_________________________________________________________________<br/>dense_9 (Dense)       (None, 10)      130 <br/>=================================================================<br/>Total params: <strong>15,734,806</strong><br/>Trainable params: 15,734,806<br/>Non-trainable params: 0</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>As shown in the preceding summary, and also in <em>Figure 11.5</em>, the total number of parameters of this model is <strong>15,734,806</strong>. This confirms that this is an over-parametrized model. The printed summary also depicts how each part of the model is named when no specific name is provided; that is, they all receive a generic name based on the name of the class and a consecutive number.</p>
<p>The <kbd>fit()</kbd> method trains the deep model and when we plot the training logged in the <kbd>hist</kbd> variable, as we did earlier for <em>Figure 11.3</em>, we obtain the following figure:</p>
<div class="CDPAlignCenter packt_figref CDPAlign"><img src="assets/719dd863-8eec-447e-91f8-a95203ded62a.png" style="width:31.50em;height:19.75em;"/></div>
<div class="CDPAlignCenter packt_figref CDPAlign">Figure 11.6 – Model loss of deepnet across epochs using callbacks</div>
<p>From <em>Figure 11.6</em>, we see that the deep network stops training after about 200 epochs and the training and test sets cross paths around epoch 70, after which, the model begins to overfit the training set. If we compare this result to the one in <em>Figure 11.3</em> for the wide network, we can see that the model starts overfitting around epoch 55.</p>
<p>Let's now discuss the quantitative results of this model.</p>
<p class="mce-root"/>
<h2 id="uuid-256afa34-6347-4138-ab89-66822ecb2e6b">Results</h2>
<p>If we generate a classification report in the same manner as we did for the wide network, we obtain the results shown here:</p>
<pre>     precision  recall  f1-score  support<br/><br/> 0   0.58       0.63    0.60      1000<br/> 1   0.66       0.68    0.67      1000<br/> 2   0.41       0.42    0.41      1000<br/> 3   0.38       0.35    0.36      1000<br/> 4   0.41       0.50    0.45      1000<br/> 5   0.51       0.36    0.42      1000<br/> 6   0.50       0.63    0.56      1000<br/> 7   0.67       0.56    0.61      1000<br/> 8   0.65       0.67    0.66      1000<br/> 9   0.62       0.56    0.59      1000<br/><br/> accuracy               0.53      10000<br/><br/><br/>[[627  22  62  19  45  10  25  18 132  40]<br/> [ 38 677  18  36  13  10  20  13  55 120]<br/> [ 85  12 418  82 182  45  99  38  23  16]<br/> [ 34  14 105 347  89 147 161  50  17  36]<br/> [ 58  12 158  34 496  29 126  55  23   9]<br/> [ 25   7 108 213  91 358 100  54  23  21]<br/> [ 9   15  84  68 124  26 631   7  11  25]<br/> [ 42  23  48  58 114  57  61 555  10  32]<br/> [110  75  16  22  30  11   8   5 671  52]<br/> [ 51 171  14  34  16   9  36  36  69 564]]<br/><br/>BER <strong>0.4656</strong></pre>
<p>This suggests comparable results to the wide model, in which we obtained a 0.4567 BER, which represents a difference of 0.0089 in favor of the wide model, which does not represent a significant difference in this case. We can verify that the models are also comparable with respect to their classification performance on particular classes by looking at the preceding results or at the confusion matrix shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/f5ce5c8d-8be0-496f-a076-8fd401f0ea24.png" style="width:29.33em;height:25.17em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.7 – <span>Confusion matrix visualization for the </span>deepnet<span> model</span></div>
<p>From the preceding results, we can confirm that the toughest class to classify is number 3, <em>cats</em>, which are often confused with dogs. Similarly, the easiest to classify is number 1, <em>ship</em><em>s</em>, which are often confused with airplanes. But once again, this is consistent with the results from the wide network.</p>
<p>One more type of deep network that we can experiment with is one that promotes sparsity among the weights of the network, which we'll discuss next.</p>
<h1 id="uuid-94e6ff0e-3bea-49ef-bc47-e51c863b21ea">Sparse deep neural networks</h1>
<p>A sparse network can be defined as <em>sparse</em> in different aspects of its architecture (Gripon, V., and Berrou, C., 2011). However, the specific type of sparseness we'll look into in this section is the sparseness obtained with respect to the weights of the network, that is, its parameters. We will be looking at each specific parameter to see if it is relatively close to zero (computationally speaking).</p>
<p class="mce-root"/>
<p>Currently, there are three ways of imposing weight sparseness in Keras over Tensorflow, and they are related to the concept of a vector norm. If we look at the Manhattan norm, <img class="fm-editor-equation" src="assets/0393ae08-e2cd-4212-bdd7-09a6d5ebb397.png" style="width:1.00em;height:1.17em;"/>, or the Euclidean norm, <img class="fm-editor-equation" src="assets/17601231-bf41-41e7-8008-03f7ad3924e4.png" style="width:1.00em;height:1.17em;"/>, they are defined as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/1566d40d-fd4b-423a-b31e-1ef070ce041c.png" style="width:10.58em;height:4.50em;"/>,</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/d216abc0-69b5-487b-97f4-b6265cc2c2e6.png" style="width:9.92em;height:4.50em;"/></p>
<p>Here, <em>n</em> is the number of elements in the vector <img class="fm-editor-equation" src="assets/3b4165c3-c661-408a-b9f9-0f0d5a43b31d.png" style="width:1.08em;height:0.83em;"/>. As you can see, in simple terms, the <img class="fm-editor-equation" src="assets/14b8cca1-5449-4bdb-9f76-d7839f733dc0.png" style="width:1.00em;height:1.17em;"/>-norm adds up all elements in terms of their absolute value, while the <img class="fm-editor-equation" src="assets/0a7be022-f165-4047-b381-1a597f96a9ed.png" style="width:1.00em;height:1.17em;"/>-norm does it in terms of their squared values. It is evident that if both norms are close to zero, <img class="fm-editor-equation" src="assets/51bcc6df-b0d4-421a-963b-a3d2127e4284.png" style="width:4.25em;height:1.08em;"/>, the chances are that most of its elements are zero or close to zero. As a matter of personal choice here, we will use the <img class="fm-editor-equation" src="assets/6cc9b5e1-9ac6-4f3a-8fd4-59b862fb22e4.png" style="width:0.92em;height:1.08em;"/>-norm because, as opposed to <img class="fm-editor-equation" src="assets/ffda2abd-d60c-46db-8190-2425c9dec01d.png" style="width:0.92em;height:1.08em;"/>, very large vectors are quadratically penalized so as to avoid specific neurons dominating specific terms.</p>
<p>Keras contains these tools in the <kbd>regularizers</kbd> class: <kbd>tf.keras.regularizers</kbd>. We can import them as follows:</p>
<ul>
<li><img class="fm-editor-equation" src="assets/c66b2062-de8b-428f-93fc-60a69e3c6d59.png" style="width:1.00em;height:1.17em;"/><span>-norm: <kbd>tf.keras.regularizers.l1(l=0.01)</kbd></span></li>
<li><img class="fm-editor-equation" src="assets/0bbc9e82-8105-4829-9d6a-5f98b9c233d3.png" style="width:1.00em;height:1.17em;"/><span>-norm: <kbd>tf.keras.regularizers.l2(l=0.01)</kbd></span></li>
</ul>
<p>These regularizers are applied to the loss function of the network in order to minimize the norm of the weights.  </p>
<div class="packt_infobox">A <strong>regularizer</strong> is a term that is used in machine learning to denote a term or function that provides elements to an objective (loss) function, or to a general optimization problem (such as gradient descent), in order to provide numerical stability or promote the feasibility of the problem. In this case, the regularizer promotes the stability of the weights by preventing the explosion of some weight values, while at the same time promoting general sparsity.</div>
<p><span>The parameter <kbd>l=0.01</kbd></span> is a penalty factor that directly determines the importance of minimizing weight norms. In other words, the penalty is applied as follows:</p>
<p class="CDPAlignCenter CDPAlign"><img class="fm-editor-equation" src="assets/c469f7b0-3678-4ec0-ba10-72f886eaafcf.png" style="width:8.42em;height:4.50em;"/></p>
<p>Therefore, using a very small value, such as <kbd>l=0.0000001</kbd> will pay little attention to the norm, and <kbd>l=0.01</kbd> will pay a lot of attention to the norm during the minimization of the loss function. Here's the catch: this parameter needs to be tuned up because if the network is too big, there might be several millions of parameters, which can make the norm look very large, and so a small penalty is in order; whereas if the network is relatively small, a larger penalty is recommended. Since this exercise is on a very deep network with 15+ million parameters, we will use a value of <kbd>l=0.0001</kbd>.</p>
<p>Let's go ahead and build a sparse network.</p>
<h2 id="uuid-1b9fee96-1fe5-4dd4-a28a-85d11f3e3602">Building a sparse network and training it</h2>
<p>To build this network, we will use the exact same architecture shown in <em>Figure 11.5</em>, except that the declaration of each individual dense layer will contain a specification that we want to consider the minimization of the norm of the weights associated with that layer. Please look at the code of the previous section and compare it to the following code, where we highlight the differences:</p>
<pre class="mce-root"># Dimensionality of input for CIFAR-10<br/>inpt_dim = 32*32*3<br/><br/>inpt_vec = Input(shape=(inpt_dim,))<br/><br/>units = inpt_dim    # Initial number of neurons <br/>dr = 0.5    # Initial drop out rate   <br/><br/>dl = Dropout(dr)(inpt_vec)<br/>dl = Dense(units, activation='relu', <br/>           <strong>kernel_regularizer=regularizers.l2(0.0001)</strong>)(dl)<br/><br/># Iterative creation of bottleneck layers<br/>units = units//2<br/>dr = dr/2<br/>while units&gt;10: <br/>  dl = Dropout(dr)(dl)<br/>  dl = Dense(units, activation='relu', <br/>             <strong>kernel_regularizer=regularizers.l2(0.0001)</strong>)(dl)<br/>  units = units//2<br/>  dr = dr/1.5<br/><br/># Output layer<br/>output = Dense(10, activation='sigmoid', <br/>               <strong>kernel_regularizer=regularizers.l2(0.0001)</strong>)(dl)<br/><br/>sparsenet = Model(inpt_vec, output)</pre>
<p>Compiling and training the model goes the same, like this:</p>
<pre class="mce-root">sparsenet.compile(loss='binary_crossentropy', optimizer='adam')<br/>sparsenet.summary()<br/><br/>reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20, <br/>                              min_delta=1e-4, mode='min')<br/>stop_alg = EarlyStopping(monitor='val_loss', patience=100, <br/>                         restore_best_weights=True)<br/>hist = sparsenet.fit(x_train, y_train, batch_size=1000, epochs=1000, <br/>                     callbacks=[stop_alg, reduce_lr], shuffle=True, <br/>                     validation_data=(x_test, y_test))<br/><br/>sparsenet.save_weights("sparsenet.hdf5")</pre>
<p>The output of <kbd>sparsenet.summary()</kbd> is identical to the one shown in the previous section for <kbd>deepnet.summary()</kbd>, so we will not repeat it here. However, we can look at the training curve as the loss is minimized – see the following figure: </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a48a28cd-af02-4fcb-bc98-03cf53e4c9df.png" style="width:27.58em;height:17.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.8 – Loss function optimization across epochs for the sparsenet model</div>
<p>From the figure, we can see that both curves, the training and test set, are minimized closely together up to around epoch 120, where both start to digress, and the model begins to overfit after that. In comparison to previous models in <em>Figure 11.3</em> and <em>Figure 11.6</em>, we can see that this model can be trained a bit more slowly and still achieve relative convergence. Note, however, that while the loss function still remains the binary cross-entropy, the model is also minimizing the <img style="font-size: 1em;width:1.33em;height:1.58em;" class="fm-editor-equation" src="assets/c35d10cc-5624-4614-9a2e-457897f8f7cb.png"/>-<span>norm, making this particular loss not directly comparable to the previous ones.</span></p>
<p>Let's now discuss the quantitative results of this model.</p>
<h2 id="uuid-e0c0d18b-0563-4214-bd73-bc0f2f5ac862">Results</h2>
<p>When we look at a quantitative analysis of performance, we can tell that the model is comparable to the previous models. There is a slight gain in terms of a BER; however, it is not enough to declare victory and the problem solved by any means – see the following analysis:</p>
<pre>     precision recall f1-score support<br/><br/> 0   0.63      0.64   0.64     1000<br/> 1   0.71      0.66   0.68     1000<br/> 2   0.39      0.43   0.41     1000<br/> 3   0.37      0.23   0.29     1000<br/> 4   0.46      0.45   0.45     1000<br/> 5   0.47      0.50   0.49     1000<br/> 6   0.49      0.71   0.58     1000<br/> 7   0.70      0.61   0.65     1000<br/> 8   0.63      0.76   0.69     1000<br/> 9   0.69      0.54   0.60     1000<br/><br/> accuracy             0.55     10000<br/><br/>[[638  17  99   7  27  13  27  10 137  25]<br/> [ 40 658  11  32  11   7  21  12 110  98]<br/> [ 78  11 431  34 169  93 126  31  19   8]<br/> [ 18  15  96 233  52 282 220  46  14  24]<br/> [ 47   3 191  23 448  36 162  57  28   5]<br/> [ 17   6 124 138  38 502 101  47  16  11]<br/> [  0   9  59  51 111  28 715   8  13   6]<br/> [ 40   1  66  50  85  68  42 608  12  28]<br/> [ 76  45  18  16  25   8  22   5 755  30]<br/> [ 51 165  12  38   6  23  29  43  98 535]]<br/><br/>BER <strong>0.4477</strong></pre>
<p>What we can clearly conclude is that the model is not worse in terms of performance when compared to other models discussed in this chapter. In fact, close inspection of the confusion matrix shown in the following figure indicates that similar errors are made with this network as well in terms of objects that are similar in nature:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a4731f7e-bd2c-482c-ae01-db80b399522c.png" style="width:30.75em;height:26.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.9 – Confusion matrix for the sparsenet model</div>
<p>Now, since it is difficult to appreciate the differences between the models we have discussed so far – wide, deep, and sparse – we can calculate and plot the norm of the weights of each trained model, as shown in the following figure:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/4bc9b510-5899-4937-ac67-37f1a60f9456.png" style="width:29.83em;height:18.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11.10 – Cumulative norm weights of the trained models</div>
<p>This figure shows the calculation in terms of the <img class="fm-editor-equation" src="assets/2ab2673c-164f-45b9-89d9-d6fc42d48046.png" style="width:0.92em;height:1.08em;"/>-norm so as to have values close enough to appreciate them; on the horizontal axis we have the number of layers, and on the vertical axis, we have the cumulative norm as we progress in the layers of the networks. This is where we can appreciate how different the networks are with respect to their parameters. In a sparse network, the cumulative norm is much smaller (about four to five times) in comparison to the other networks. This can be an interesting and important characteristic for those networks that might be implemented on a chip or other applications in which zero-weights can lead to efficient computations in production (Wang, P., et al. 2018).</p>
<p>While the level at which the network weights are affected by the norm can be determined experimentally through hyperparameter optimization techniques, it is often more common to determine other parameters such as the dropout rate, the number of neural units, and others we discuss in the next section.</p>
<h1 id="uuid-9917848c-69ba-4639-82be-a4f690834475">Hyperparameter optimization</h1>
<p>There are a few methodologies out there for optimizing parameters; for example, some are gradient-based (Rivas, P., et al. 2014; Maclaurin, D., et al. 2015<em>)</em>, others are Bayesian (Feurer, M., et al. 2015<em>)</em>. However, it has been difficult to have a generalized method that works extremely well and that is efficient at the same time – usually, you get one or the other. You can read more about other algorithms here (Bergstra, J. S., et al. 2011<em>)</em>.</p>
<p class="mce-root"/>
<p>For any beginner in this field, it might be better to get started with something simple and easy to remember, such as random search (Bergstra, J., &amp; Bengio, Y. 2012<em>)</em> or grid search. These two methods are very similar and while we will focus here on <strong>grid search</strong>, the implementations of both are very similar.</p>
<h2 id="uuid-8add1db6-a9fa-450f-a48b-f4e4d92ec7d6">Libraries and parameters</h2>
<p>We will need to use two major libraries that we have not covered before: <kbd>GridSearchCV</kbd>, for executing the grid search with cross-validation, and <kbd>KerasClassifier</kbd>, to create a Keras classifier that can communicate with scikit-learn.</p>
<p>Both libraries can be imported as follows:</p>
<pre>from sklearn.model_selection import <strong>GridSearchCV</strong><br/>from tensorflow.keras.wrappers.scikit_learn import <strong>KerasClassifier</strong></pre>
<p>The hyperparameters that we will optimize (and their possible values) are the following:</p>
<ul>
<li><strong>Dropout rate</strong>: <kbd>0.2</kbd>, <kbd>0.5</kbd></li>
<li><strong>Optimizer</strong>: <kbd>rmsprop</kbd>, <kbd>adam</kbd></li>
<li><strong>Learning rate</strong>: <kbd>0.01</kbd>, <kbd>0.0001</kbd></li>
<li><strong>Neurons in hidden layers</strong>: <kbd>1024</kbd>, <kbd>512</kbd>, <kbd>256</kbd></li>
</ul>
<p>In total, the possible combination of hyperparameters is 2x2x2x3=24. This is the total number of options in the four-dimensional grid. The number of alternatives can be much larger and more comprehensive but remember: we want to keep things simple for this example. Furthermore, since we will be applying cross-validation, you will multiply the possible combinations by the number of splits in cross-validation and that would be how many complete, end-to-end training sessions will be executed to determine the best combination of hyperparameters. </p>
<div class="packt_tip">Be mindful of the number of options you will try in the grid search, since all of them will be tested, and this can take a lot of time for larger networks and for larger datasets. When you gain more experience, you will be able to choose a smaller set of parameters just by thinking about the architecture that you define.</div>
<p>The full implementation is discussed next.</p>
<h2 id="uuid-4180390d-5f25-477f-97c2-8fa65949344a">Implementation and results</h2>
<p>The complete code for the grid search is shown here, but consider that most of these things are repetitive since this is modeled on the wide network model discussed earlier in this chapter:</p>
<pre>from sklearn.model_selection import GridSearchCV<br/>from tensorflow.keras.wrappers.scikit_learn import KerasClassifier<br/>from tensorflow.keras.layers import Input, Dense, Dropout<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras.optimizers import Adam, RMSprop<br/>from tensorflow.keras.datasets import cifar10<br/>from tensorflow.keras.utils import to_categorical<br/>from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping<br/>import NumPy as np<br/><br/># load and prepare data (same as before)<br/>(x_train, y_train), (x_test, y_test) = cifar10.load_data()<br/>x_train = x_train.astype('float32') / 255.0<br/>x_test = x_test.astype('float32') / 255.0<br/>x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))<br/>x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))<br/>y_train = to_categorical(y_train, 10)<br/>y_test = to_categorical(y_test, 10)</pre>
<p>We declare a method to build a model and return it like so:</p>
<pre># A KerasClassifier will use this to create a model on the fly<br/>def <strong>make_widenet</strong>(dr=0.0, optimizer='adam', lr=0.001, units=128):<br/>  # This is a wide architecture<br/>  inpt_dim = 32*32*3<br/>  inpt_vec = Input(shape=(inpt_dim,))<br/>  dl = Dropout(dr)(inpt_vec)<br/>  l1 = Dense(units, activation='relu')(dl)<br/>  dl = Dropout(dr)(l1)<br/>  l2 = Dense(units, activation='relu') (dl)<br/>  output = Dense(10, activation='sigmoid') (l2)<br/><br/>  widenet = Model(inpt_vec, output)<br/><br/>  # Our loss and lr depends on the choice<br/>  if optimizer == 'adam':<br/>    optmzr = Adam(learning_rate=lr)<br/>  else:<br/>    optmzr = RMSprop(learning_rate=lr)<br/> <br/>  widenet.compile(loss='binary_crossentropy', optimizer=optmzr, <br/>                  metrics=['accuracy'])<br/> <br/>  return widenet</pre>
<p>Then we put the pieces together, searching for parameters, and training as follows:</p>
<pre># This defines the model architecture<br/><strong>kc</strong> = <strong>KerasClassifier</strong>(build_fn=<strong>make_widenet</strong>, epochs=100, batch_size=1000, <br/>                     verbose=0)<br/><br/># This sets the grid search parameters<br/><strong>grid_space</strong> = dict(dr=[0.2, 0.5],      # Dropout rates<br/>                  optimizer=['adam', 'rmsprop'], <br/>                  lr=[0.01, 0.0001],  # Learning rates<br/>                  units=[1024, 512, 256])<br/><br/>gscv = GridSearchCV(estimator=<strong>kc</strong>, param_grid=<strong>grid_space</strong>, n_jobs=1, cv=3, verbose=2)<br/><strong>gscv_res</strong> = gscv.fit(x_train, y_train, validation_split=0.3,<br/>                    callbacks=[EarlyStopping(monitor='val_loss', <br/>                                             patience=20, <br/>                                             restore_best_weights=True),<br/>                               ReduceLROnPlateau(monitor='val_loss', <br/>                                                 factor=0.5, patience=10)])<br/><br/># Print the dictionary with the best parameters found:<br/>print(<strong>gscv_res.best_params_</strong>)</pre>
<p>This will print out several lines, one for each time the cross-validation runs. We will omit a lot of the output here, just to show you what it looks like, but you can tune the level of verbosity manually if you want:</p>
<pre>Fitting 3 folds for each of 24 candidates, totalling 72 fits<br/>[CV] dr=0.2, lr=0.01, optimizer=adam, units=1024 .....................<br/>[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.<br/>[CV] ...... dr=0.2, lr=0.01, optimizer=adam, units=1024, total= 21.1s<br/>[CV] dr=0.2, lr=0.01, optimizer=adam, units=1024 .....................<br/>[Parallel(n_jobs=1)]: Done 1 out of 1 | elapsed: 21.1s remaining: 0.0s<br/>[CV] ...... dr=0.2, lr=0.01, optimizer=adam, units=1024, total= 21.8s<br/>[CV] dr=0.2, lr=0.01, optimizer=adam, units=1024 .....................<br/>[CV] ...... dr=0.2, lr=0.01, optimizer=adam, units=1024, total= 12.6s<br/>[CV] dr=0.2, lr=0.01, optimizer=adam, units=512 ......................<br/>[CV] ....... dr=0.2, lr=0.01, optimizer=adam, units=512, total= 25.4s<br/>.<br/>.<br/>.<br/>[CV] .. dr=0.5, lr=0.0001, optimizer=rmsprop, units=256, total= 9.4s<br/>[CV] dr=0.5, lr=0.0001, optimizer=rmsprop, units=256 .................<br/>[CV] .. dr=0.5, lr=0.0001, optimizer=rmsprop, units=256, total= 27.2s<br/>[Parallel(n_jobs=1)]: Done 72 out of 72 | elapsed: 28.0min finished<br/><br/><strong>{'dr': 0.2, 'lr': 0.0001, 'optimizer': 'adam', 'units': 1024}</strong></pre>
<p>This last line is the most precious information you need since it is the best combination of parameters that give the best results. Now you can go ahead and change your original implementation of the wide network with these <strong>optimized </strong>parameters and see how the performance changes. You should receive a boost in the average accuracy of around 5%, which is not bad! </p>
<p>Alternatively, you can try out a larger set of parameters or increase the number of splits for cross-validation. The possibilities are endless. You should always try to optimize the number of parameters in your models for the following reasons:</p>
<ul>
<li>It gives you confidence in your model.</li>
<li>It gives your clients confidence in you.</li>
<li>It tells the world that you are a professional.</li>
</ul>
<p>Good work! It is time to wrap up.</p>
<h1 id="uuid-56eb9dbb-ef4d-460c-9717-9207060ec78d">Summary </h1>
<p>This chapter discussed different implementations of neural networks, namely, wide, deep, and sparse implementations. After reading this chapter, you should appreciate the differences in design and how they may affect performance or training time. At this point, you should be able to appreciate the simplicity of these architectures and how they present new alternatives to other things we've discussed so far. In this chapter, you also learned to optimize the hyperparameters of your models, for example, the dropout rates, aiming to maximize the generalization ability of the network.</p>
<p>I am sure you noticed that these models achieved accuracies beyond random chance, that is, &gt; 50%; however, the problem we discussed is a very difficult problem to solve, and you might not be surprised that a general neural architecture, like the ones we studied here, does not perform extraordinarily well. In order to achieve better performance, we can use a more specialized type of architecture designed to solve problems with a high spatial correlation of the input, such as image processing. One type of specialized architecture is known as a <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>).</p>
<p>Our next station,<span> <a href="c36bdee9-51f3-4283-8f15-6dd603d071a1.xhtml">Chapter 12</a>, <em>Convolutional Neural Networks,</em> will discuss precisely that</span><em>.</em><span> You will be able to see how much a difference can make when you move from a general-purpose model to a more field-specific model. </span><span>You cannot miss this upcoming chapter. But before you go, please try to quiz yourself with the following questions. </span></p>
<h1 id="uuid-c5a446f2-1693-4ebc-92ba-1b6ca905f383">Questions and answers</h1>
<ol>
<li class="mce-root"><strong>Was there a significant difference in performance between a wide or deep network?</strong></li>
</ol>
<p style="padding-left: 60px">Not much in the case, we studied here. However, one thing you must remember is that both networks learned fundamentally different things or aspects of the input. Therefore, in other applications, the performance might vary.</p>
<ol start="2">
<li><strong>Is deep learning the same as a deep neural network? </strong></li>
</ol>
<p style="padding-left: 60px">No. Deep learning is the area of machine learning focused on all algorithms that train over-parametrized models using novel gradient descent techniques. Deep neural networks are networks with many hidden layers. Therefore, a deep network is deep learning. But deep learning is not uniquely specific to deep networks.</p>
<ol start="3">
<li class="mce-root"><strong>Could you give an example of when sparse networks are desired?</strong></li>
</ol>
<p style="padding-left: 60px">Let's think about robotics. In this field, most things run on microchips that have memory constraints and storage constraints and computational power constraints; finding neural architectures whose weights are mostly zero would mean you do not have to calculate those products. This implies having weights that can be stored in less space, loaded quickly, and computed faster. Other possibilities include IoT devices, smartphones, smart vehicles, smart cities, law enforcement, and so on. </p>
<ol start="4">
<li><strong>How can we make these models perform better?</strong></li>
</ol>
<p style="padding-left: 60px">We can further optimize the hyperparameters by including more options. We can use autoencoders to preprocess the input. But the most effective thing would be to switch to CNNs to solve this problem since CNNs are particularly good at the classification of images. See the next chapter.</p>
<h1 id="uuid-9e5d18c1-185d-46c6-93c3-c3234575c217">References</h1>
<ul>
<li><span>Rosenblatt, F. (1958). The perceptron: a probabilistic model for information storage and organization in the brain. </span><em>Psychological review</em><span>, </span><span>65</span><span>(6), 386.</span></li>
<li>Muselli, M. (1997). On convergence properties of the pocket algorithm.<span> </span><em>IEEE Transactions on Neural Networks</em>, 8(3), 623-629.</li>
<li>Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A. A., Sohl-Dickstein, J., &amp; Schoenholz, S. S. (2019). Neural Tangents: Fast and Easy Infinite Neural Networks in Python. <em>arXiv preprint</em> arXiv:1912.02803.</li>
<li><span>Soltanolkotabi, M., Javanmard, A., &amp; Lee, J. D. (2018). Theoretical insights into the optimization landscape of over-parameterized shallow neural networks. </span><em>IEEE Transactions on Information Theory</em><span>, </span>65<span>(2), 742-769.</span></li>
<li><span>Du, S. S., Zhai, X., Poczos, B., &amp; Singh, A. (2018). Gradient descent provably optimizes over-parameterized neural networks. </span><em>arXiv preprint</em> arXiv:1810.02054<span>.</span></li>
<li><span>Liao, Q., Miranda, B., Banburski, A., Hidary, J., &amp; Poggio, T. (2018). A surprising linear relationship predicts test performance in deep networks. </span><em>arXiv preprint</em> arXiv:1807.09659<span>.</span></li>
<li>Gripon, V., &amp; Berrou, C. (2011). Sparse neural networks with large learning diversity. <em>IEEE transactions on neural networks</em>, 22(7), 1087-1096.</li>
<li>Wang, P., Ji, Y., Hong, C., Lyu, Y., Wang, D., &amp; Xie, Y. (2018, June). SNrram: an efficient sparse neural network computation architecture based on resistive random-access memory. In <em>2018 55th ACM/ESDA/IEEE Design Automation Conference</em> (DAC) (pp. 1-6). IEEE.</li>
<li>Rivas-Perea, P., Cota-Ruiz, J., &amp; Rosiles, J. G. (2014). A nonlinear least squares quasi-newton strategy for lp-svr hyper-parameters selection. <em>International Journal of Machine Learning and Cybernetics</em>, 5(4), 579-597.</li>
<li>Maclaurin, D., Duvenaud, D., &amp; Adams, R. (2015, June). Gradient-based hyperparameter optimization through reversible learning. In <em>International Conference on Machine Learning</em> (pp. 2113-2122).</li>
<li>Feurer, M., Springenberg, J. T., &amp; Hutter, F. (2015, February). Initializing Bayesian hyperparameter optimization via meta-learning. In <em>Twenty-Ninth AAAI Conference on Artificial Intelligence</em>.</li>
<li>Bergstra, J., &amp; Bengio, Y. (2012). Random search for hyper-parameter optimization. The <em>Journal of Machine Learning Research</em>, 13(1), 281-305.</li>
<li>Bergstra, J. S., Bardenet, R., Bengio, Y., &amp; Kégl, B. (2011). Algorithms for hyper-parameter optimization. In <em>Advances in neural information processing systems</em> (pp. 2546-2554).</li>
</ul>


            </article>

            
        </section>
    </body></html>