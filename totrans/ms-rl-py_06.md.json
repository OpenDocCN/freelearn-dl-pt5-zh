["```py\n    import numpy as np\n    import gym\n    ```", "```py\n    class FoodTruck(gym.Env):\n        def __init__(self):\n            self.v_demand = [100, 200, 300, 400]\n            self.p_demand = [0.3, 0.4, 0.2, 0.1]\n            self.capacity = self.v_demand[-1]\n            self.days = ['Mon', 'Tue', 'Wed', \n                         'Thu', 'Fri', \"Weekend\"]\n            self.unit_cost = 4\n            self.net_revenue = 7\n            self.action_space = [0, 100, 200, 300, 400]\n            self.state_space = [(\"Mon\", 0)] \\\n                             + [(d, i) for d in self.days[1:] \n                                 for i in [0, 100, 200, 300]]\n    ```", "```py\n        def get_next_state_reward(self, state, action, demand):\n            day, inventory = state\n            result = {}\n            result['next_day'] = self.days[self.days.index(day) \\\n                                           + 1]\n            result['starting_inventory'] = min(self.capacity, \n                                               inventory \n                                               + action)\n            result['cost'] = self.unit_cost * action \n            result['sales'] = min(result['starting_inventory'], \n                                  demand)\n            result['revenue'] = self.net_revenue * result['sales']\n            result['next_inventory'] \\\n                = result['starting_inventory'] - result['sales']\n            result['reward'] = result['revenue'] - result['cost']\n            return result\n    ```", "```py\n        def get_transition_prob(self, state, action):\n            next_s_r_prob = {}\n            for ix, demand in enumerate(self.v_demand):\n                result = self.get_next_state_reward(state, \n                                                    action, \n                                                    demand)\n                next_s = (result['next_day'],\n                          result['next_inventory'])\n                reward = result['reward']\n                prob = self.p_demand[ix]\n                if (next_s, reward) not in next_s_r_prob:\n                    next_s_r_prob[next_s, reward] = prob\n                else:\n                    next_s_r_prob[next_s, reward] += prob\n            return next_s_r_prob\n    ```", "```py\n    def base_policy(states):\n        policy = {}\n        for s in states:\n            day, inventory = s\n            prob_a = {} \n            if inventory >= 300:\n                prob_a[0] = 1\n            else:\n                prob_a[200 - inventory] = 0.5\n                prob_a[300 - inventory] = 0.5\n            policy[s] = prob_a\n        return policy\n    ```", "```py\n    def expected_update(env, v, s, prob_a, gamma):\n        expected_value = 0\n        for a in prob_a:\n            prob_next_s_r = env.get_transition_prob(s, a)\n            for next_s, r in prob_next_s_r:\n                expected_value += prob_a[a] \\\n                                * prob_next_s_r[next_s, r] \\\n                                * (r + gamma * v[next_s])\n        return expected_value\n    ```", "```py\n    def policy_evaluation(env, policy, max_iter=100, \n                          v = None, eps=0.1, gamma=1):\n        if not v:\n            v = {s: 0 for s in env.state_space}\n        k = 0\n        while True:\n            max_delta = 0\n            for s in v:\n                if not env.is_terminal(s):\n                    v_old = v[s]\n                    prob_a = policy[s]\n                    v[s] = expected_update(env, v, \n                                           s, prob_a, \n                                           gamma)\n                    max_delta = max(max_delta, \n                                    abs(v[s] - v_old))\n            k += 1\n            if max_delta < eps:\n                print(\"Converged in\", k, \"iterations.\")\n                break\n            elif k == max_iter:\n                print(\"Terminating after\", k, \"iterations.\")\n                break\n        return v\n    ```", "```py\n    foodtruck = FoodTruck()\n    ```", "```py\n    policy = base_policy(foodtruck.state_space)\n    ```", "```py\n    v = policy_evaluation(foodtruck, policy)\n    print(\"Expected weekly profit:\", v[\"Mon\", 0])\n    ```", "```py\n    Converged in 6 iterations.\n    Expected weekly profit: 2515.0\n    ```", "```py\n        def reset(self):\n            self.day = \"Mon\"\n            self.inventory = 0\n            state = (self.day, self.inventory)\n            return state\n    ```", "```py\n        def is_terminal(self, state):\n            day, inventory = state\n            if day == \"Weekend\":\n                return True\n            else:\n                return False\n    ```", "```py\n        def step(self, action):\n            demand = np.random.choice(self.v_demand, \n                                      p=self.p_demand)\n            result = self.get_next_state_reward((self.day, \n                                                 self.inventory), \n                                           action, \n                                           demand)\n            self.day = result['next_day']\n            self.inventory = result['next_inventory']\n            state = (self.day, self.inventory)\n            reward = result['reward']\n            done = self.is_terminal(state)\n            info = {'demand': demand, 'sales': result['sales']}\n            return state, reward, done, info\n    ```", "```py\n    def choose_action(state, policy):\n        prob_a = policy[state]\n        action = np.random.choice(a=list(prob_a.keys()), \n                                  p=list(prob_a.values()))\n        return action\n    ```", "```py\n    def simulate_policy(policy, n_episodes):\n        np.random.seed(0)\n        foodtruck = FoodTruck()\n        rewards = []\n        for i_episode in range(n_episodes):\n            state = foodtruck.reset()\n            done = False\n            ep_reward = 0\n            while not done:\n                action = choose_action(state, policy)\n                state, reward, done, info = foodtruck.step(action) \n                ep_reward += reward\n            rewards.append(ep_reward)\n        print(\"Expected weekly profit:\", np.mean(rewards))\n    ```", "```py\n    simulate_policy(policy, 1000)\n    ```", "```py\n    Expected weekly profit: 2518.1\n    ```", "```py\n    def policy_improvement(env, v, s, actions, gamma):\n        prob_a = {}\n        if not env.is_terminal(s):\n            max_q = np.NINF\n            best_a = None\n            for a in actions:\n                q_sa = expected_update(env, v, s, {a: 1}, gamma)\n                if q_sa >= max_q:\n                    max_q = q_sa\n                    best_a = a\n            prob_a[best_a] = 1\n        else:\n            max_q = 0\n        return prob_a, max_q\n    ```", "```py\n    def policy_iteration(env,  eps=0.1, gamma=1):\n        np.random.seed(1)\n        states = env.state_space\n        actions = env.action_space\n        policy = {s: {np.random.choice(actions): 1}\n                 for s in states}\n        v = {s: 0 for s in states}\n        while True:\n            v = policy_evaluation(env, policy, v=v, \n                              eps=eps, gamma=gamma)\n            old_policy = policy\n            policy = {}\n            for s in states:\n                policy[s], _ = policy_improvement(env, v, s, \n                                        actions, gamma)\n            if old_policy == policy:\n                break\n        print(\"Optimal policy found!\")\n        return policy, v\n    ```", "```py\n    policy, v = policy_iteration(foodtruck)\n    print(\"Expected weekly profit:\", v[\"Mon\", 0])\n    ```", "```py\n    Converged in 6 iterations.\n    Converged in 6 iterations.\n    Converged in 5 iterations.\n    Optimal policy found!\n    Expected weekly profit: 2880.0\n    ```", "```py\n    def value_iteration(env, max_iter=100, eps=0.1, gamma=1):\n        states = env.state_space\n        actions = env.action_space\n        v = {s: 0 for s in states}\n        policy = {}\n        k = 0\n        while True:\n            max_delta = 0\n            for s in states:\n                old_v = v[s]\n                policy[s], v[s] = policy_improvement(env, \n                                                     v, \n                                                     s, \n                                                     actions, \n                                                     gamma)\n                max_delta = max(max_delta, abs(v[s] - old_v))\n            k += 1\n            if max_delta < eps:\n                print(\"Converged in\", k, \"iterations.\")\n                break\n            elif k == max_iter:\n                print(\"Terminating after\", k, \"iterations.\")\n                break\n        return policy, v\n    ```", "```py\n    policy, v = value_iteration(foodtruck)\n    print(\"Expected weekly profit:\", v[\"Mon\", 0])\n    ```", "```py\n    Converged in 6 iterations.\n    Expected weekly profit: 2880.0\n    ```", "```py\n    def first_visit_return(returns, trajectory, gamma):\n        G = 0\n        T = len(trajectory) - 1\n        for t, sar in enumerate(reversed(trajectory)):\n            s, a, r = sar\n            G = r + gamma * G\n            first_visit = True\n            for j in range(T - t):\n                if s == trajectory[j][0]:\n                    first_visit = False\n            if first_visit:\n                if s in returns:\n                    returns[s].append(G)\n                else:\n                    returns[s] = [G]\n        return returns\n    ```", "```py\n    def get_trajectory(env, policy):\n        trajectory = []\n        state = env.reset()\n        done = False\n        sar = [state]\n        while not done:\n            action = choose_action(state, policy)\n            state, reward, done, info = env.step(action)\n            sar.append(action)\n            sar.append(reward)\n            trajectory.append(sar)\n            sar = [state]\n        return trajectory\n    ```", "```py\n    def first_visit_mc(env, policy, gamma, n_trajectories):\n        np.random.seed(0)\n        returns = {}\n        v = {}\n        for i in range(n_trajectories):\n            trajectory = get_trajectory(env, policy)\n            returns = first_visit_return(returns, \n                                         trajectory, \n                                         gamma)\n        for s in env.state_space:\n            if s in returns:\n                v[s] = np.round(np.mean(returns[s]), 1)\n        return v\n    ```", "```py\n    foodtruck = FoodTruck()\n    policy = base_policy(foodtruck.state_space)\n    ```", "```py\n    v_est = first_visit_mc(foodtruck, policy, 1, 1000)\n    ```", "```py\n    {('Mon', 0): 2515.9,\n     ('Tue', 0): 1959.1,\n     ('Tue', 100): 2362.2,\n     ('Tue', 200): 2765.2,\n    ...\n    ```", "```py\n    v_true = policy_evaluation(foodtruck, policy)\n    ```", "```py\n    {('Mon', 0): 2515.0,\n     ('Tue', 0): 1960.0,\n     ('Tue', 100): 2360.0,\n     ('Tue', 200): 2760.0,\n    ...\n    ```", "```py\ndef get_eps_greedy(actions, eps, a_best):\n    prob_a = {}\n    n_a = len(actions)\n    for a in actions:\n        if a == a_best:\n            prob_a[a] = 1 - eps + eps/n_a\n        else:\n            prob_a[a] = eps/n_a\n    return prob_a\n```", "```py\n    def get_random_policy(states, actions):\n        policy = {}\n        n_a = len(actions)\n        for s in states:\n            policy[s] = {a: 1/n_a for a in actions}\n        return policy\n    ```", "```py\n    import operator\n    def on_policy_first_visit_mc(env, n_iter, eps, gamma):\n        np.random.seed(0)\n        states =  env.state_space\n        actions = env.action_space\n        policy =  get_random_policy(states, actions)\n        Q = {s: {a: 0 for a in actions} for s in states}\n        Q_n = {s: {a: 0 for a in actions} for s in states}\n        for i in range(n_iter):\n            if i % 10000 == 0:\n                print(\"Iteration:\", i)\n            trajectory = get_trajectory(env, policy)\n            G = 0\n            T = len(trajectory) - 1\n            for t, sar in enumerate(reversed(trajectory)):\n                s, a, r = sar\n                G = r + gamma * G\n                first_visit = True\n                for j in range(T - t):\n                    s_j = trajectory[j][0]\n                    a_j = trajectory[j][1]\n                    if (s, a) == (s_j, a_j):\n                        first_visit = False\n                if first_visit:\n                    Q[s][a] = Q_n[s][a] * Q[s][a] + G\n                    Q_n[s][a] += 1\n                    Q[s][a] /= Q_n[s][a]\n                    a_best = max(Q[s].items(), \n                                 key=operator.itemgetter(1))[0]\n                    policy[s] = get_eps_greedy(actions, \n                                               eps, \n                                               a_best)\n        return policy, Q, Q_n\n    ```", "```py\n    policy, Q, Q_n = on_policy_first_visit_mc(foodtruck, \n                                              300000, \n                                              0.05, \n                                              1)\n    ```", "```py\n    {('Mon', 0):{0:0.01, 100:0.01, 200:0.01, 300:0.01, 400:0.96},\n    …\n    ```", "```py\n    def off_policy_mc(env, n_iter, eps, gamma):\n        np.random.seed(0)\n        states =  env.state_space\n        actions = env.action_space\n        Q = {s: {a: 0 for a in actions} for s in states}\n        C = {s: {a: 0 for a in actions} for s in states}\n        target_policy = {}\n        behavior_policy = get_random_policy(states, \n                                            actions)\n        for i in range(n_iter):\n            if i % 10000 == 0:\n                print(\"Iteration:\", i)\n            trajectory = get_trajectory(env, \n                                        behavior_policy)\n            G = 0\n            W = 1\n            T = len(trajectory) - 1\n            for t, sar in enumerate(reversed(trajectory)):\n                s, a, r = sar\n                G = r + gamma * G\n                C[s][a] += W\n                Q[s][a] += (W/C[s][a]) * (G - Q[s][a])\n                a_best = max(Q[s].items(), \n                             key=operator.itemgetter(1))[0]\n                target_policy[s] = a_best\n                behavior_policy[s] = get_eps_greedy(actions, \n                                                    eps, \n                                                    a_best)\n                if a != target_policy[s]:\n                    break\n                W = W / behavior_policy[s][a]\n        target_policy = {s: target_policy[s] for s in states}\n        return target_policy, Q\n    ```", "```py\n    policy, Q = off_policy_mc(foodtruck, 300000, 0.05, 1)\n    ```", "```py\n    {('Mon', 0): 400,\n     ('Tue', 0): 400,\n     ('Tue', 100): 300,\n     ('Tue', 200): 200,\n     ('Tue', 300): 100,\n     ('Wed', 0): 400,\n    ...\n    ```", "```py\n    def one_step_td_prediction(env, policy, gamma, alpha, n_iter):\n        np.random.seed(0)\n        states = env.state_space\n        v = {s: 0 for s in states}\n        s = env.reset()\n        for i in range(n_iter):\n            a = choose_action(s, policy)\n            s_next, reward, done, info = env.step(a)\n            v[s] += alpha * (reward + gamma * v[s_next] - v[s])\n            if done:\n                s = env.reset()\n            else:\n                s = s_next\n        return v\n    ```", "```py\n    policy = base_policy(foodtruck.state_space)\n    ```", "```py\n    one_step_td_prediction(foodtruck, policy, 1, 0.01, 100000)\n    ```", "```py\n    {('Mon', 0): 2507.0, \n     ('Tue', 0): 1956.0\n    ...\n    ```", "```py\n    def sarsa(env, gamma, eps, alpha, n_iter):\n        np.random.seed(0)\n        states = env.state_space\n        actions = env.action_space\n        Q = {s: {a: 0 for a in actions} for s in states}\n        policy = get_random_policy(states, actions)\n        s = env.reset()\n        a = choose_action(s, policy)\n    ```", "```py\n        for i in range(n_iter):\n            if i % 100000 == 0:\n                print(\"Iteration:\", i)\n            s_next, reward, done, info = env.step(a)\n            a_best = max(Q[s_next].items(), \n                         key=operator.itemgetter(1))[0]\n            policy[s_next] = get_eps_greedy(actions, eps, a_best)\n            a_next = choose_action(s_next, policy)\n            Q[s][a] += alpha * (reward \n                                + gamma * Q[s_next][a_next] \n                                - Q[s][a])\n            if done:\n                s = env.reset()\n                a_best = max(Q[s].items(), \n                             key=operator.itemgetter(1))[0]\n                policy[s] = get_eps_greedy(actions, eps, a_best)\n                a = choose_action(s, policy)\n            else:\n                s = s_next\n                a = a_next\n        return policy, Q\n    ```", "```py\n    policy, Q = sarsa(foodtruck, 1, 0.1, 0.05, 1000000)\n    ```", "```py\n    {('Mon', 0): {0: 0.02, 100: 0.02, 200: 0.02, 300: 0.02, 400: 0.92},\n     ('Tue', 0): {0: 0.02, 100: 0.02, 200: 0.02, 300: 0.92, 400: 0.02},\n     ('Tue', 100): {0: 0.02, 100: 0.02, 200: 0.02, 300: 0.92, 400: 0.02},\n    ...\n    ```", "```py\n    {0: 2049.95351191411,\n     100: 2353.5460655683123,\n     200: 2556.736260693101,\n     300: 2558.210868908282,\n     400: 2593.7601273913133}\n    ```", "```py\n    def q_learning(env, gamma, eps, alpha, n_iter):\n        np.random.seed(0)\n        states = env.state_space\n        actions = env.action_space\n        Q = {s: {a: 0 for a in actions} for s in states}\n        policy = get_random_policy(states, actions)\n        s = env.reset()\n    ```", "```py\n         for i in range(n_iter):\n            if i % 100000 == 0:\n                print(\"Iteration:\", i)\n            a_best = max(Q[s].items(), \n                         key=operator.itemgetter(1))[0]\n            policy[s] = get_eps_greedy(actions, eps, a_best)\n            a = choose_action(s, policy)\n            s_next, reward, done, info = env.step(a)\n            Q[s][a] += alpha * (reward \n                                + gamma * max(Q[s_next].values()) \n                                - Q[s][a])\n            if done:\n                s = env.reset()\n            else:\n                s = s_next\n    ```", "```py\n        policy = {s: {max(policy[s].items(), \n                     key=operator.itemgetter(1))[0]: 1}\n                     for s in states}\n        return policy, Q\n    ```", "```py\n    policy, Q = q_learning(foodtruck, 1, 0.1, 0.01, 1000000)\n    ```", "```py\n    {('Mon', 0): {400: 1},\n     ('Tue', 0): {400: 1},\n     ('Tue', 100): {300: 1},\n     ('Tue', 200): {200: 1},\n     ('Tue', 300): {100: 1},\n    ...\n    ```"]