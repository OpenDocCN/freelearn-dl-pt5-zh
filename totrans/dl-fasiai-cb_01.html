<html><head></head><body>
		<div id="_idContainer053">
			<h1 id="_idParaDest-20"><em class="italic"><a id="_idTextAnchor019"/>Chapter 1</em>: Getting Started with fastai</h1>
			<p>Over the last decade, deep learning has revolutionized swathes of technology, from image recognition to machine translation. Until recently, only those with extensive training and access to specialized hardware have been able to unlock the benefits of deep learning. The fastai framework is an effort to democratize deep learning by making it accessible to non-specialists. One of the key ways that fastai opens up deep learning to the masses is by making it easy to get started. </p>
			<p>In this chapter, we will show you what you need to get started with fastai, starting with how to set up an environment for fastai. By the end of this chapter, you will be able to do the following: set up a cloud environment in which to run <strong class="source-inline">fastai</strong> examples; exercise a basic fastai example; explain the relationship between fastai and PyTorch (the underlying deep learning library for fastai); and contrast fastai with Keras, the other high-level library for deep learning.</p>
			<p>Here are the recipes that will be covered in this chapter:</p>
			<ul>
				<li>Setting up a fastai environment in Paperspace Gradient</li>
				<li>Setting up a fastai environment in Google Colaboratory (Google Colab)</li>
				<li>Setting up JupyterLab environment in Paperspace Gradient</li>
				<li>"Hello world" for fastai—creating a model for the <strong class="bold">Modified National Institute of Science and Technology</strong> (<strong class="bold">MNIST) dataset</strong> </li>
				<li>Understanding the world in four applications: tables, text, recommender systems, and images </li>
				<li>Working with PyTorch tensors</li>
				<li>Contrasting fastai with Keras</li>
				<li>Test your knowledge</li>
			</ul>
			<h1 id="_idParaDest-21"><a id="_idTextAnchor020"/>Technical requirements</h1>
			<p>For this chapter, you will be using the following technologies:</p>
			<ul>
				<li>Paperspace Gradient: <a href="https://gradient.paperspace.com/">https://gradient.paperspace.com/</a></li>
				<li>Google Colab: <a href="https://colab.research.google.com/notebooks/intro.ipynb">https://colab.research.google.com/notebooks/intro.ipynb</a></li>
				<li>Google Drive: <a href="https://drive.google.com">https://drive.google.com</a></li>
				<li>Keras: <a href="https://keras.io/">https://keras.io/</a></li>
			</ul>
			<p>You can find the code referred to in this chapter at the following link:</p>
			<p><a href="https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook/tree/main/ch1">https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook/tree/main/ch1</a></p>
			<h1 id="_idParaDest-22"><a id="_idTextAnchor021"/>Setting up a fastai environment in Paperspace Gradient</h1>
			<p>There are two free cloud environments that<a id="_idIndexMarker000"/> you can use to explore fastai: <strong class="bold">Paperspace Gradient</strong> and <strong class="bold">Google Colab</strong>. In this section, we'll go through the steps to set up Paperspace Gradient with a fastai notebook <a id="_idIndexMarker001"/>environment, and in the next section, we'll go through the setup steps for Colab. It's your choice, so pick <a id="_idIndexMarker002"/>the environment that works best for you. </p>
			<p>Gradient is simpler to use because you have access to a standard filesystem for storage. With Colab, you need to use Google Drive for storage and, unlike Gradient, you don't have convenient access to the terminal for command-line interactions. </p>
			<p>On the other hand, Colab gives you direct access to a wider set of libraries beyond those needed for fastai—for example, you can run the Keras MNIST example in Colab but it won't work off the shelf in a Gradient fastai instance. To get the most out of the examples in the book, it's best to set up both environments so that you can choose which one works best for you as you go along. We'll start with Gradient, since it is the simplest to get started with.</p>
			<h2 id="_idParaDest-23"><a id="_idTextAnchor022"/>Getting ready</h2>
			<p>Prior to setting up Gradient for fastai, you need to create a Paperspace account. You can do this by going to <a href="https://console.paperspace.com/signup?gradient=true">https://console.paperspace.com/signup?gradient=true</a>.</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor023"/>How to do it…</h2>
			<p>Once you have a Paperspace account, you can create a free fastai notebook in Gradient by following these steps to create a fastai notebook instance in Gradient. Once created, this will be a complete Jupyter Notebook environment with all the libraries that you need (including fastai, PyTorch, and related libraries).</p>
			<ol>
				<li> Go to the Paperspace site and sign in using the account you created in the <em class="italic">Getting ready</em> section. </li>
				<li>From the pulldown at the top of the page, select <strong class="bold">Gradient</strong>:<div id="_idContainer005" class="IMG---Figure"><img src="image/B16216_01_01.jpg" alt="Figure 1.1. – Selecting gradient from the pulldown&#13;&#10;"/></div><p class="figure-caption">Figure 1.1. – Selecting gradient from the pulldown</p></li>
				<li>Select the <strong class="bold">Notebooks</strong> tab:<div id="_idContainer006" class="IMG---Figure"><img src="image/B16216_01_02.jpg" alt="Figure 1.2 – Selecting the Notebooks tab&#13;&#10;"/></div><p class="figure-caption">Figure 1.2 – Select the Notebooks tab</p></li>
				<li>Select the <strong class="bold">CREATE</strong> button.<div id="_idContainer007" class="IMG---Figure"><img src="image/B16216_01_03.jpg" alt="Figure 1.3 – CREATE button&#13;&#10;"/></div><p class="figure-caption">Figure 1.3 – CREATE button</p></li>
				<li>Enter a name for your notebook in the <strong class="bold">Name</strong> field.</li>
				<li>In the <strong class="bold">Select a runtime </strong> section, select <strong class="bold">fastai</strong>.</li>
				<li>In the <strong class="bold">Select a machine </strong> section, select <strong class="bold">Free-GPU</strong> or <strong class="bold">Free-P5000</strong>. Note that you may receive a message indicating out of capacity for the machine type you selected. If this happens, you can either choose another GPU-enabled machine type or wait a few minutes and try again with your original machine type. Also note that after your notebook is created, you can change the machine type—for example, if you find that the <a id="_idIndexMarker003"/>free instance is not <a id="_idIndexMarker004"/>meeting your needs, you can switch your notebook to a paid machine. You can also define multiple notebooks for different applications and configure auto-shutdown (how many hours your instance will run before shutting itself down) if you opt for a paid subscription. For details, see <a href="https://console.paperspace.com/teim6pi2i/upgrade">https://console.paperspace.com/teim6pi2i/upgrade</a>.</li>
				<li>Select the <strong class="bold">START NOTEBOOK</strong> button to launch the process of creating a new fastai instance for you in Gradient.<div id="_idContainer008" class="IMG---Figure"><img src="image/B16216_01_04.jpg" alt="Figure 1.4 – START NOTEBOOK button&#13;&#10;"/></div><p class="figure-caption">Figure 1.4 – START NOTEBOOK button</p></li>
				<li>Your notebook will take a minute or so to be created. When it is ready, you will see a <strong class="bold">Running</strong> message at the bottom of the screen:<div id="_idContainer009" class="IMG---Figure"><img src="image/B16216_01_05.jpg" alt="Figure 1.5 – Running message&#13;&#10;"/></div><p class="figure-caption">Figure 1.5 – Running message</p></li>
				<li>Next, you should see a Jupyter button appear in the navigation panel on the left, as highlighted in <em class="italic">Figure 1.6</em>:<div id="_idContainer010" class="IMG---Figure"><img src="image/B16216_01_06.jpg" alt="Figure 1.6 – Jupyter icon in the navigation panel&#13;&#10;"/></div><p class="figure-caption">Figure 1.6 – Jupyter icon in the navigation panel</p></li>
				<li>Select the Jupyter button to start your new notebook environment. You should now see a Jupyter files view, as shown in <em class="italic">Figure 1.7</em>:</li>
			</ol>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B16216_01_07.jpg" alt="Figure 1.7 – Jupyter file view in Gradient&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.7 – Jupyter file view in Gradient</p>
			<p>Now that your notebook has started, you need to validate that it was set up correctly by running a short notebook that checks the fastai version available to your notebook and confirms that your notebook has access to <strong class="bold">graphics processing units</strong> (<strong class="bold">GPUs</strong>), the specialized hardware required to efficiently run subsequent examples in this book:</p>
			<ol>
				<li value="1">Open a terminal in the root directory of your Gradient notebook environment:<div id="_idContainer012" class="IMG---Figure"><img src="image/B16216_01_08.jpg" alt="Figure 1.8– Pulldown to open a terminal in Jupyter notebook&#13;&#10;"/></div><p class="figure-caption">Figure 1.8– Pulldown to open a terminal in Jupyter notebook</p></li>
				<li>In the terminal, create <a id="_idIndexMarker005"/>a new directory, <strong class="source-inline">fastai_cookbook</strong>, in the<a id="_idIndexMarker006"/> root level of your notebook: <p class="source-code"><strong class="bold">mkdir fastai_cookbook</strong></p></li>
				<li>In the terminal, make this new directory your current directory:<p class="source-code"><strong class="bold">cd fastai_cookbook</strong></p></li>
				<li>Initialize <strong class="source-inline">git</strong> in this new directory:<p class="source-code"><strong class="bold">git init</strong></p></li>
				<li>Clone the repository for the book:<p class="source-code"><strong class="bold">git clone https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook.git</strong></p></li>
				<li>Once the repository has been cloned, go to the <strong class="source-inline">ch1</strong> directory and open the <strong class="source-inline">validate_gradient_setup.ipynb</strong> notebook:<div id="_idContainer013" class="IMG---Figure"><img src="image/B16216_01_09.jpg" alt="Figure 1.9 – validate_gradient_setup.ipynb notebook in the Files view&#13;&#10;"/></div><p class="figure-caption">Figure 1.9 – validate_gradient_setup.ipynb notebook in the Files view</p></li>
				<li>Run the entire notebook (<strong class="bold">Cell -&gt; Run all</strong>) and check the output. </li>
				<li>For the first code cell, you<a id="_idIndexMarker007"/> should see something<a id="_idIndexMarker008"/> like the following if your notebook has access to the <strong class="source-inline">fastai</strong> library. Don't worry about the exact level of fastai—the key point is that you are able to import the library and get back a valid version without errors:<div id="_idContainer014" class="IMG---Figure"><img src="image/B16216_01_10.jpg" alt="Figure 1.10 – Getting the fastai version&#13;&#10;"/></div><p class="figure-caption">Figure 1.10 – Getting the fastai version</p></li>
				<li>For the second code cell, you should see something like the table shown next if your notebook has access to a GPU. A GPU is specialized hardware for deep learning that you will need in order to efficiently run subsequent examples. Don't worry about the specific type of GPU listed; just confirm that you get a table like this as output of this cell:</li>
			</ol>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B16216_01_11.jpg" alt="Figure 1.11 – Output of the nvidia-smi command&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.11 – Output of the nvidia-smi command</p>
			<p>If you get the<a id="_idIndexMarker009"/> following kind of output from this <a id="_idIndexMarker010"/>cell, then your notebook was not set up correctly with access to a GPU: </p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B16216_01_12.jpg" alt="Figure 1.12 – Error from the nvidia-smi command&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.12 – Error from the nvidia-smi command</p>
			<p>Congratulations! You have<a id="_idIndexMarker011"/> set up a Gradient environment<a id="_idIndexMarker012"/> that is ready to explore fastai. </p>
			<h2 id="_idParaDest-25"><a id="_idTextAnchor024"/>How it works…</h2>
			<p>Now that you have a working Gradient instance, you will be able to run fastai examples. Gradient includes PyTorch, fastai, and other libraries that you need to run the examples in this book, along with access to the GPU hardware that you need to run these examples efficiently.</p>
			<p>Some of the aspects of Gradient notebooks that you need to be aware of are listed here:</p>
			<ul>
				<li>By default, your free instance will run for 6 hours and then shut itself down. If you want to have longer, uninterrupted sessions, you will need to change to a paid subscription.</li>
				<li>Generally<a id="_idIndexMarker013"/> speaking, restarting a Gradient instance takes between 3 and 10 minutes, so it's a good idea to go to the <strong class="bold">Notebook</strong> section of the Paperspace console and click on <strong class="bold">START</strong> for your notebook a few minutes before you're ready to actually get working. I am in the<a id="_idIndexMarker014"/> habit of starting my notebook and then completing some other task (such as sending an email or making a cup of tea) so that I'm not waiting too long for the notebook to start.</li>
				<li>If you are a bit rusty about how to use Jupyter notebooks, the tutorial available at <a href="https://www.dataquest.io/blog/jupyter-notebook-tutorial/">https://www.dataquest.io/blog/jupyter-notebook-tutorial/</a> gives a good review of the key points.</li>
			</ul>
			<h2 id="_idParaDest-26"><a id="_idTextAnchor025"/>There's more…</h2>
			<p>If you have completed all the steps in this section and have a working Gradient environment, the next section is not strictly required. I recommend that you set up both Gradient and Colab, but it's not mandatory to have both environments in order to complete most of the examples in this book. However, if you want the best of both worlds, you can also set up Colab for fastai—it's also free, and it offers some advantages over Gradient, such as supporting Keras applications.</p>
			<h1 id="_idParaDest-27"><a id="_idTextAnchor026"/>Setting up a fastai environment in Google Colab</h1>
			<p>If you are already familiar with the <strong class="bold">Google Colab</strong> environment or want to take advantage of Google's overall <a id="_idIndexMarker015"/>machine learning <a id="_idIndexMarker016"/>ecosystem, Colab may be the right environment for you to use to explore fastai. In this section, we'll go through the steps to get set up with Colab and validate that it's ready for you to use with fastai.</p>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor027"/>Getting ready</h2>
			<p>To use Colab, you will need a Google ID and access to Google Drive. If you don't already have a Google ID, follow the instructions here to create one: <a href="https://support.google.com/accounts/answer/27441?hl=en">https://support.google.com/accounts/answer/27441?hl=en</a>.</p>
			<p>Once you have a Google ID, you <a id="_idIndexMarker017"/>need to confirm that you have <a id="_idIndexMarker018"/>access to Google Drive. You need access to Drive because it acts as the storage system for Colab. You save your notebooks and data in Drive when you are working in Colab. Follow the instructions here to get access to Drive: <a href="https://support.google.com/drive/answer/2424384?co=GENIE.Platform%3DDesktop&amp;hl=en">https://support.google.com/drive/answer/2424384?co=GENIE.Platform%3DDesktop&amp;hl=en</a>.</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor028"/>How to do it…</h2>
			<p>Once you have a Google ID with access to Drive, you can set up Colab to work with fastai by completing the following steps. First, we'll get access to Drive from within a Colab notebook, then clone the repository for this book, and finally run the validation notebook to confirm the setup worked.</p>
			<ol>
				<li value="1">Open Colab (<a href="https://colab.research.google.com/">https://colab.research.google.com/</a>).</li>
				<li>Open a new, blank notebook by selecting <strong class="bold">File -&gt; New notebook</strong>.</li>
				<li>In the new notebook, paste the following statement into an empty cell:<p class="source-code">print("hello world")</p><p>Then, select the <strong class="bold">Run</strong> button:</p><div id="_idContainer017" class="IMG---Figure"><img src="image/B16216_01_13.jpg" alt="Figure 1.13 – Colab run button&#13;&#10;"/></div><p class="figure-caption">Figure 1.13 – Colab run button</p></li>
				<li>Confirm that you get the expected output:<div id="_idContainer018" class="IMG---Figure"><img src="image/B16216_01_14.jpg" alt="Figure 1.14 – Expected output of &quot;hello world&quot; in Colab&#13;&#10;"/></div><p class="figure-caption">Figure 1.14 – Expected output of "hello world" in Colab</p></li>
				<li>Go to<a id="_idIndexMarker019"/> Drive and create a new folder <a id="_idIndexMarker020"/>called <strong class="source-inline">fastai_cookbook</strong> in your root folder in Drive.</li>
				<li>Go into this new folder and right-click, and select <strong class="bold">Google Colaboratory</strong>:<div id="_idContainer019" class="IMG---Figure"><img src="image/B16216_01_15.jpg" alt="Figure 1.15 – Selecting Google Colaboratory in your new directory in Drive&#13;&#10;"/></div><p class="figure-caption">Figure 1.15 – Selecting Google Colaboratory in your new directory in Drive</p></li>
				<li>Colab will<a id="_idIndexMarker021"/> open a new notebook. In this<a id="_idIndexMarker022"/> notebook, select <strong class="bold">Connect</strong> -&gt; <strong class="bold">Connect to hosted runtime</strong>:<div id="_idContainer020" class="IMG---Figure"><img src="image/B16216_01_16.jpg" alt="Figure 1.16 – Selecting Connect to hosted runtime&#13;&#10;"/></div><p class="figure-caption">Figure 1.16 – Selecting Connect to hosted runtime</p></li>
				<li>In a new <a id="_idIndexMarker023"/>cell in this notebook, paste the<a id="_idIndexMarker024"/> following code and run the cell (for example, by clicking the arrow):<p class="source-code">from google.colab import drive</p><p class="source-code">drive.mount('/content/drive')</p></li>
				<li>In the response that comes back, click on the link that is provided:<div id="_idContainer021" class="IMG---Figure"><img src="image/B16216_01_17.jpg" alt="Figure 1.17 – Prompt to mount Google Drive in your notebook&#13;&#10;"/></div><p class="figure-caption">Figure 1.17 – Prompt to mount Google Drive in your notebook</p></li>
				<li>Select an account:<div id="_idContainer022" class="IMG---Figure"><img src="image/B16216_01_18.jpg" alt="Figure 1.18 – Dialog to select your Google account&#13;&#10;"/></div><p class="figure-caption">Figure 1.18 – Dialog to select your Google account</p></li>
				<li>On the <a id="_idIndexMarker025"/>screen for <strong class="bold">Google Drive File Stream</strong> access, select<a id="_idIndexMarker026"/> on <strong class="bold">Allow</strong>:<div id="_idContainer023" class="IMG---Figure"><img src="image/B16216_01_19.jpg" alt="Figure 1.19 – Google Drive File Stream dialog&#13;&#10;"/></div><p class="figure-caption">Figure 1.19 – Google Drive File Stream dialog</p></li>
				<li>On<a id="_idIndexMarker027"/> the <strong class="bold">Sign in</strong> screen, select the <strong class="bold">copy</strong> icon to <a id="_idIndexMarker028"/>copy your access code:<div id="_idContainer024" class="IMG---Figure"><img src="image/B16216_01_20.jpg" alt="Figure 1.20 – Dialog to get access code&#13;&#10;"/></div><p class="figure-caption">Figure 1.20 – Dialog to get access code</p></li>
				<li>Now, return to<a id="_idIndexMarker029"/> the notebook in Colab and<a id="_idIndexMarker030"/> paste the access code in the authorization code field, and then press <em class="italic">Enter</em>:<div id="_idContainer025" class="IMG---Figure"><img src="image/B16216_01_21.jpg" alt="Figure 1.21 – Access code entered to mount Google Drive&#13;&#10;"/></div><p class="figure-caption">Figure 1.21 – Access code entered to mount Google Drive</p></li>
				<li>The cell will run and produce the following mounted message to confirm that your Google Drive has been mounted and is available for your Colab notebook:</li>
			</ol>
			<div>
				<div id="_idContainer026" class="IMG---Figure">
					<img src="image/B16216_01_22.jpg" alt="Figure 1.22 – Message confirming that Google Drive has been mounted&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.22 – Message confirming that Google Drive has been mounted</p>
			<p>Now that<a id="_idIndexMarker031"/> Drive is mounted in Colab, the next step is to clone the <a id="_idIndexMarker032"/>book's repository:</p>
			<ol>
				<li value="1">Make the <strong class="source-inline">fastai_cookbook</strong> new directory folder in Drive your current directory by running a cell in the notebook with the following command:<p class="source-code"><strong class="bold">%cd /content/drive/MyDrive/fastai_cookbook</strong></p></li>
				<li>Run the following command in a new cell to list the contents of this directory:<p class="source-code"><strong class="bold">%ls</strong></p></li>
				<li>Run the following code in a new cell in your notebook to clone the book's repository:<p class="source-code"><strong class="bold">!git clone https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook.git </strong></p></li>
				<li>Run the cell to list the directory contents again, and you should see the directory created now for the repository. You can confirm in Drive that the repository has been cloned.</li>
			</ol>
			<p>Now that you have cloned the repository, you can run the validation notebook to confirm that you have access to the <strong class="source-inline">fastai</strong> library and GPUs:</p>
			<ol>
				<li value="1">In Drive, navigate to the <strong class="source-inline">fastai_cookbook/Deep-Learning-with-fastai-Cookbook/ch1</strong> folder, right-click on the <strong class="source-inline">validate_gradient_setup.ipynb</strong> notebook, and select <strong class="bold">Open With</strong> | <strong class="bold">Google Colaboratory</strong>.</li>
				<li>The notebook opens up in Colab. Select <strong class="bold">Runtime</strong> | <strong class="bold">Change Runtime Type</strong>. In the <strong class="bold">Notebook settings</strong> dialog that comes up, select <strong class="bold">GPU</strong> in the <strong class="bold">Hardware accelerator</strong> field, and select <strong class="bold">SAVE</strong>:<div id="_idContainer027" class="IMG---Figure"><img src="image/B16216_01_23.jpg" alt="Figure 1.23 – Selecting GPU as the hardware accelerator in the Notebook settings dialog&#13;&#10;"/></div><p class="figure-caption">Figure 1.23 – Selecting GPU as the hardware accelerator in the Notebook settings dialog</p></li>
				<li>Run the <a id="_idIndexMarker033"/>notebook by selecting <strong class="bold">Runtime</strong> | <strong class="bold">Run all</strong>.</li>
				<li>Confirm that you get output like the<a id="_idIndexMarker034"/> following, with no errors, for the first code cell in the notebook:<div id="_idContainer028" class="IMG---Figure"><img src="image/B16216_01_24.jpg" alt="Figure 1.24 – Confirmation of the fastai version&#13;&#10;"/></div><p class="figure-caption">Figure 1.24 – Confirmation of the fastai version</p></li>
				<li>Confirm that you <a id="_idIndexMarker035"/>get output like the following <a id="_idIndexMarker036"/>for the second code cell in the notebook. Don't worry about the specific GPU type listed—this will vary depending on what's available. If you did not specify <strong class="bold">GPU</strong> as the hardware accelerator in <em class="italic">Step 2</em>, then you won't get this output:</li>
			</ol>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/B16216_01_25.jpg" alt="Figure 1.25 – Output of nvidia-smi confirming access to a GPU&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.25 – Output of nvidia-smi confirming access to a GPU</p>
			<p>Congratulations! You have set up a Colab environment that is ready to explore fastai. </p>
			<h2 id="_idParaDest-30"><a id="_idTextAnchor029"/>How it works…</h2>
			<p>Now that you have <a id="_idIndexMarker037"/>a working Colab environment, you will be able to run fastai examples in it. Colab incorporates PyTorch, fastai, and other libraries that you need to run the examples in this book. Note that, unlike Gradient, every<a id="_idIndexMarker038"/> time you start up a new Colab session, you will need to follow the steps to mount Drive and will also need to specify that you want a GPU. By default, Drive is not mounted and your Colab notebooks don't have access to GPUs until you explicitly change the hardware accelerator type.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor030"/>There's more…</h2>
			<p>If you have set up both Gradient and Colab environments, I recommend that you use Gradient to exercise the examples in this book by default. Gradient gives you direct access to a terminal, which is handy for entering command-line commands, and does not require you to mount a filesystem or request a GPU every time you start a new session. Colab does have some advantages, including not shutting down after 6 hours, but overall you will have a smoother experience with Gradient.</p>
			<h1 id="_idParaDest-32"><a id="_idTextAnchor031"/>Setting up JupyterLab environment in Gradient</h1>
			<p>Earlier in this chapter, we <a id="_idIndexMarker039"/>went through the steps to set up Gradient as an environment to explore fastai. With this set up, you get the standard Jupyter notebook<a id="_idIndexMarker040"/> environment that features a filesystem view and the ability to update notebooks, launch terminal windows, and perform basic operations such as uploading and downloading files from your local system. If you want a richer development environment, you can set up Gradient to use JupyterLab. </p>
			<p>In addition to allowing you to maintain multiple views (for example, a terminal view along with several notebooks) within the same browser tab, JupyterLab also lets you take advantage of visual debuggers in the context of a notebook. In this section, we will go through the steps to set up Gradient so that you can use JupyterLab. Note that this recipe is optional—any example in this book that you can run in Gradient with JupyterLab will also work in vanilla Jupyter.</p>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor032"/>Getting ready</h2>
			<p>Before you attempt to set up <a id="_idIndexMarker041"/>Gradient with JupyterLab, ensure that you have successfully completed the steps in the <em class="italic">Setting up a fastai environment in Paperspace Gradient</em> section. Once you have set up JupyterLab Gradient, you will be able to switch back and forth between the vanilla Jupyter view and JupyterLab at any time.</p>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor033"/>How to do it…</h2>
			<p>To get JupyterLab set up, you begin<a id="_idIndexMarker042"/> by starting up your Gradient instance, run a command to install JupyterLab, and then restart the instance to see the result. Here are the steps to do this:</p>
			<ol>
				<li value="1">Start your Gradient fastai instance to bring up the vanilla Jupyter <strong class="bold">Files</strong> view:<div id="_idContainer030" class="IMG---Figure"><img src="image/B16216_01_26.jpg" alt="Figure 1.29 – Vanilla Jupyter Files view&#13;&#10;"/></div><p class="figure-caption">Figure 1.26 – Vanilla Jupyter Files view</p></li>
				<li>Once you are in the filesystem view for your instance, select <strong class="bold">New</strong> | <strong class="bold">Terminal</strong>:<div id="_idContainer031" class="IMG---Figure"><img src="image/B16216_01_27.jpg" alt="Figure 1.27 – Pulldown to open up a terminal in Jupyter&#13;&#10;"/></div><p class="figure-caption">Figure 1.27 – Pulldown to open up a terminal in Jupyter</p></li>
				<li>This will <a id="_idIndexMarker043"/>open a <a id="_idIndexMarker044"/>terminal window:<div id="_idContainer032" class="IMG---Figure"><img src="image/B16216_01_28.jpg" alt="Figure 1.28 – Jupyter terminal window&#13;&#10;"/></div><p class="figure-caption">Figure 1.28 – Jupyter terminal window</p></li>
				<li>In the terminal window, enter this command to install JupyterLab:<p class="source-code"><strong class="bold">pip install jupyterlab</strong></p></li>
				<li>Once the install has completed, exit Jupyter, stop your Gradient instance in the Paperspace console, and restart it.</li>
				<li>When you get to the vanilla Jupyter <strong class="bold">Files</strong> view, update<a id="_idIndexMarker045"/> the <strong class="bold">Uniform Resource Locator</strong> (<strong class="bold">URL</strong>) to replace <strong class="source-inline">tree</strong> at the end of the URL with <strong class="source-inline">lab</strong>, and hit <em class="italic">Enter</em>. You should <a id="_idIndexMarker046"/>now see the JupyterLab view<a id="_idIndexMarker047"/> instead of the vanilla Jupyter view:</li>
			</ol>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B16216_01_29.jpg" alt="Figure 1.29 – JupyterLab environment in Gradient&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.29 – JupyterLab environment in Gradient</p>
			<p>Congratulations! You have set up Gradient so that you can use the JupyterLab view. </p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor034"/>How it works…</h2>
			<p>You can go back and forth between the vanilla Jupyter view and JupyterLab any time you like by simply modifying the URL so that the end is <strong class="source-inline">tree</strong> (for Jupyter) or <strong class="source-inline">lab</strong> (for JupyterLab).</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor035"/>There's more…</h2>
			<p>If you want more details on the<a id="_idIndexMarker048"/> benefits of JupyterLab, this tutorial explains the features and how to use them: <a href="https://dzone.com/articles/getting-started-with-jupyterlab">https://dzone.com/articles/getting-started-with-jupyterlab</a>.</p>
			<p>I mentioned<a id="_idIndexMarker049"/> earlier that one of the benefits of JupyterLab is that it<a id="_idIndexMarker050"/> supports a visual Python debugger you can use in notebooks. For more details on this debugger and how to set it up, see <strong class="source-inline">https://medium.com/@cristiansaavedra/visual-jupyter-debugger-for-python-e96fdd4f6f68</strong> and <a href="https://blog.jupyter.org/a-visual-debugger-for-jupyter-914e61716559">https://blog.jupyter.org/a-visual-debugger-for-jupyter-914e61716559</a>.</p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor036"/>"Hello world" for fastai – creating a model for MNIST</h1>
			<p>Now that you have set up<a id="_idIndexMarker051"/> your environment for fastai, it's time to run through an example. In this section, you will go through the process of creating a simple deep learning model trained on the MNIST dataset. This dataset consists of images of handwritten digits. The goal of the trained model is to predict the digit given an image. For example, we want the trained model to predict that the following digits are <strong class="source-inline">6,</strong> <strong class="source-inline">3,</strong> <strong class="source-inline">9,</strong> and <strong class="source-inline">6</strong>:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B16216_01_30.jpg" alt="Figure 1.30 – Sample handwritten digits from the MNIST dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.30 – Sample handwritten digits from the MNIST dataset</p>
			<p>We won't be covering every detail of the fastai solution for MNIST in this section, but we will be running a complete example that demonstrates one of the key values of fastai—getting a <a id="_idIndexMarker052"/>powerful deep learning result with only a few lines of code. This example should also whet your appetite for the more advanced fastai examples that are coming in subsequent chapters.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor037"/>Getting ready…</h2>
			<p>Ensure that you have followed the steps to set up fastai in Gradient and confirm that you can open the <strong class="source-inline">MNIST</strong> <strong class="source-inline">hello_world</strong> notebook (<strong class="source-inline">mnist_hello_world.ipynb</strong>) in the <strong class="source-inline">ch1</strong> directory. If you choose to use Colab, ensure that you have selected <strong class="bold">Runtime</strong> | <strong class="bold">Change runtime type</strong> and have selected <strong class="bold">GPU</strong> as the hardware accelerator.</p>
			<p>The dataset used in this section is the classic dataset of deep learning, MNIST (<a href="http://yann.lecun.com/exdb/mnist/">http://yann.lecun.com/exdb/mnist/</a>). I gratefully acknowledge the opportunity to use this dataset to provide an initial illustration of the capabilities of fastai.</p>
			<p class="callout-heading">Dataset citation</p>
			<p class="callout">Y. LeCun, L. Bottou, Y. Bengio and P. Haffner.(1998) Gradient-Based Learning Applied to Document Recognition  (http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf). Proceedings of the IEEE, 86(11):2278-2324, November 1998 </p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor038"/>How to do it…</h2>
			<p>You will begin by running the notebook all the way through. By running all the cells in this notebook you will be executing code that trains an image classification deep learning model that predicts the class (that is, which digit from 0 to 9) a given image of a hand-written digit belongs to. </p>
			<p>First, you will make the MNIST dataset, which consists of a set of images of handwritten digits organized into directories (one for each digit from 0 to 9), available to the Python code in the notebook. Next, you will define a <strong class="source-inline">dataloaders</strong> object that specifies the training subset of the dataset (that is, the images that will be used to train the model) and the validation subset of the dataset (that is, the images that will be used to assess the performance of the model as it is trained). Next, you will define the deep learning model itself using a pre-defined architecture (that is, an organization of layers that make up the model) made available by fastai. </p>
			<p>Next, you will train the model, that is iteratively apply the training set to update the weights in the model to optimize the performance of the model for the specified metric (in the case of this model, accuracy). Next, you will examine batches of images in the training and validation sets. You will then look at images where the model does the worst job of classification. Finally, you will apply the trained deep learning model to example hand-written images to see whether the model predicts the correct digit for these images. In the following steps you will run the code in the entire notebook and then go through the cells in the notebook one-by-one to review what the code is doing:</p>
			<ol>
				<li value="1">Open the MNIST <strong class="source-inline">hello_world</strong> notebook <strong class="source-inline">mnist_hello_world.ipynb</strong> in the <strong class="source-inline">ch1</strong> directory.</li>
				<li>Run the entire notebook by selecting the appropriate choice for your environment:<p>a) <strong class="bold">Cell</strong>|<strong class="bold">Run all</strong> (Jupyter)</p><p>b) <strong class="bold">Run</strong>|<strong class="bold">Run all</strong> (JupyterLab)</p><p>c) <strong class="bold">Runtime</strong>|<strong class="bold">Run all</strong> (Colab)</p></li>
				<li>Confirm that the notebook runs correctly to the end. You should see the following output from the last cell. Don't worry if you see a different digit output, as long as you get output with no errors in this cell:</li>
			</ol>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B16216_01_31.jpg" alt="Figure 1.31 – Example MNIST digit prediction&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.31 – Example MNIST digit prediction</p>
			<p>Congratulations! You have<a id="_idIndexMarker053"/> just successfully trained your first deep learning model with fastai and used the trained model to predict the digits depicted in a set of handwritten digits from the MNIST dataset. Now, let's go through the notebook cell by cell to review what this example tells us about fastai:</p>
			<ol>
				<li value="1">The first code cell imports the libraries that the notebook needs:<p class="source-code">!pip install -Uqq fastbook</p><p class="source-code">import fastbook</p><p class="source-code">from fastbook import *</p><p class="source-code">from fastai.vision.all import *</p></li>
				<li>The second cell calls the <strong class="source-inline">fastai</strong> function to prepare a notebook to run a fastai application. In Colab, for example, this function triggers the steps to mount Drive so that it's accessible within the notebook:<p class="source-code">fastbook.setup_book()</p></li>
				<li>The third cell defines the location of the dataset that will be used to train the model. fastai provides a set of oven-ready datasets (including several varieties of the MNIST dataset) that you can ingest into your notebook with a single call using the <strong class="source-inline">untar_data()</strong> function. We'll dig into more details about these datasets in <a href="B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057"><em class="italic">Chapter 2</em></a><em class="italic">, Exploring and Cleaning Up Data with fastai</em>:<p class="source-code">path = untar_data(URLs.MNIST)</p></li>
				<li><a id="_idTextAnchor039"/>The fifth cell is the heart of the notebook and demonstrates the power of fastai. Here are three lines of code that completely define and train a deep learning model:<p>a) The first line creates a <strong class="source-inline">dataloaders</strong> object from the <strong class="source-inline">path</strong> object created in the previous <a id="_idIndexMarker054"/>cell and identifies the subdirectories that contain the training and validation datasets. See the fastai documentation (<a href="https://docs.fast.ai/vision.data.html#ImageDataLoaders">https://docs.fast.ai/vision.data.html#ImageDataLoaders</a>) for more details on <strong class="source-inline">ImageDataLoaders</strong>, the specific kind of <strong class="source-inline">dataloaders</strong> object used for image problems:</p><p class="source-code">dls = ImageDataLoaders.from_folder(path, train='training', valid='testing')</p><p>b) The second line defines the structure of the deep learning model, including its architecture (based on the famous <strong class="bold">residual neural network</strong> (<strong class="bold">ResNet</strong>) architecture - for more details see the documentation (<a href="https://pytorch.org/vision/stable/models.html">https://pytorch.org/vision/stable/models.html</a>), its loss function (in this case, a loss function that is appropriate for a multi-class classification problem), and the metric that will be optimized (in this case, validation accuracy):</p><p class="source-code">learn = cnn_learner(dls, resnet18, </p><p class="source-code">pretrained=False,                                                                            </p><p class="source-code">sloss_func=LabelSmoothingCrossEntropy(), metrics=accuracy)</p><p>c) The third line trains the model, specifying that the training will be for <strong class="source-inline">1</strong> epoch (that is, one iteration through the entire training set) and the learning rate will be <strong class="source-inline">0.1</strong>:</p><p class="source-code">learn.fit_one_cycle(1, 0.1)</p></li>
				<li>The output of this cell shows the results of the training process, including the training and validation loss, the validation accuracy, and the time taken to complete the training. Note that the accuracy is very high:<div id="_idContainer036" class="IMG---Figure"><img src="image/B16216_01_32.jpg" alt="Figure 1.32 – Output of training the MNIST model&#13;&#10;"/></div><p class="figure-caption">Figure 1.32 – Output of training the MNIST model</p></li>
				<li>The next two cells <a id="_idIndexMarker055"/>display examples of the training and validation <strong class="bold">test</strong> datasets:<div id="_idContainer037" class="IMG---Figure"><img src="image/B16216_01_33.jpg" alt="Figure 1.33 – Examples from the MNIST train and test datasets&#13;&#10;"/></div><p class="figure-caption">Figure 1.33 – Examples from the MNIST train and test datasets</p></li>
				<li>The next cell shows examples of digits that the model got the most wrong. Note how these digits are not easy for us humans to identify, so it's not surprising that the model got<a id="_idIndexMarker056"/> them wrong:<div id="_idContainer038" class="IMG---Figure"><img src="image/B16216_01_34.jpg" alt="Figure 1.34 – Digits for which the MNIST model made the worst predictions&#13;&#10;"/></div><p class="figure-caption">Figure 1.34 – Digits for which the MNIST model made the worst predictions</p></li>
				<li>The next cell displays a summary of information about the model, including the layers that make it up, how many parameters it has, and the optimizer and loss function used:<p class="source-code">learn.summary()</p></li>
				<li>Finally, we have a set of cells at the end of the notebook that display digit images from the validation set, and then apply the trained model to get a prediction of which digit is shown in the image. In the following example, the model correctly identifies from the validation set an image of a zero as a zero:</li>
			</ol>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B16216_01_35.jpg" alt="Figure 1.35 – Example prediction by the MNIST model&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.35 – Example prediction by the MNIST model</p>
			<p>That's it—a complete, self-contained deep learning model that solves a famous computer vision problem (predicting a digit in a handwritten image) with remarkable accuracy. With fastai, you<a id="_idIndexMarker057"/> can accomplish this with just a few lines of code. This notebook contains some additional code to validate the model and investigate the dataset, but all you really need are the first five cells, which together contain barely 10 lines of code. </p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor040"/>How it works…</h2>
			<p>You may be asking yourself about the details of the code that we just went through. How exactly does the data ingestion work? What's a dataloader? How does the model know to include all the layers that are shown by the <strong class="source-inline">summary()</strong> function? We'll be answering these questions in subsequent chapters. In <a href="B16216_02_Final_VK_ePub.xhtml#_idTextAnchor057"><em class="italic">Chapter 2</em></a><em class="italic">, Exploring and Cleaning Up Data with fastai</em>, we'll dig into the data ingestion story for fastai, and in subsequent chapters we'll take a detailed tour through fastai's solutions for a set of common deep learning applications, including tabular data, text data, recommender systems, and computer vision.</p>
			<p>One of the beauties of fastai is that you can abstract away much of the complexity of deep learning (if you want), and get a working and useful model with just a few lines of code, as with the MNIST model we just saw. However, fastai doesn't keep the details hidden or limit your flexibility. In addition to providing an elegant way to create deep learning models with very little code, fastai incorporates a set of layers, each of which reveals more flexibility and detail. This means that as you learn more about fastai, you can continue to dig deeper and<a id="_idIndexMarker058"/> customize your solution to meet your exact needs.</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor041"/>There's more…</h2>
			<p>This section makes use of some standard machine learning terminology, including <strong class="bold">loss function</strong>, <strong class="bold">optimizer</strong>, <strong class="bold">accuracy</strong>, and <strong class="bold">multi-class classification</strong>. If you need a refresher<a id="_idIndexMarker059"/> on these<a id="_idIndexMarker060"/> and other <a id="_idIndexMarker061"/>fundamental machine learning concepts, I recommend the series of<a id="_idIndexMarker062"/> tutorials here: <a href="https://machinelearningmastery.com/">https://machinelearningmastery.com/</a>. This site includes clear descriptions of the major concepts of machine learning, along with Python code samples that illustrate how to apply the concepts.</p>
			<h1 id="_idParaDest-42"><a id="_idTextAnchor042"/>Understanding the world in four applications: tables, text, recommender systems, and images</h1>
			<p>In their seminal paper describing fastai, Howard and Gugger (<a href="https://arxiv.org/pdf/2002.04688.pdf">https://arxiv.org/pdf/2002.04688.pdf</a>) describe the four application areas that fastai supports <em class="italic">out of the box</em>. In this section, we will go through these four <a id="_idIndexMarker063"/>applications of deep<a id="_idIndexMarker064"/> learning that fastai directly <a id="_idIndexMarker065"/>supports: tabular data, text data, recommender<a id="_idIndexMarker066"/> systems, and computer vision. The MNIST example that you saw in the previous section is an example of a computer vision application. The MNIST example included the following:</p>
			<ul>
				<li>Curated dataset: MNIST. You can find<a id="_idIndexMarker067"/> an overall list of curated datasets here: <p><a href="https://course.fast.ai/datasets">https://course.fast.ai/datasets</a></p></li>
				<li>Easy ingestion of the curated dataset via <strong class="source-inline">untar_data()</strong></li>
				<li>Image-specific handling of the dataset via a data loader object</li>
				<li>Definition of an image-specific model structure via a <strong class="source-inline">Learner</strong> object</li>
				<li>Utilities to examine the dataset</li>
			</ul>
			<p>Similarly, fastai also provides components specifically aimed at the other three application areas: tabular data, text data, and recommender systems. In this section, we'll examine each of these application areas and learn about how fastai provides support for them.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor043"/>Getting ready</h2>
			<p>Have the MNIST example that you <a id="_idIndexMarker068"/>went through in the last section open because we will be referring to is as we go through the four application areas. Before we get into the description of the <a id="_idIndexMarker069"/>four application areas, it's important to get some definitions:</p>
			<ul>
				<li><strong class="source-inline">DataLoader</strong>: A structure<a id="_idIndexMarker070"/> that allows you to access batches of <em class="italic">x</em> (independent) and <em class="italic">y</em> (dependent) values. The <em class="italic">x</em> values are the data you <a id="_idIndexMarker071"/>use to train the model, and the <em class="italic">y</em> values are what you are trying to predict with the model.</li>
				<li><strong class="source-inline">DataLoaders</strong>: A structure that contains training and validation <strong class="source-inline">DataLoader</strong> objects.</li>
				<li><strong class="source-inline">Learner</strong>: An object that combines <strong class="source-inline">DataLoaders</strong>, architecture, and other characteristics (including loss function and optimizer) to define a model. To contrast <strong class="source-inline">Learner</strong> objects with models in Keras, <strong class="source-inline">Learner</strong> objects fully incorporate the data used to train the model, whereas with Keras models, the various facets of the dataset (such as training independent values, training dependent values, and much more) are arguments to the model that need to be addressed separately from the model itself.</li>
			</ul>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor044"/>How to do it…</h2>
			<p>Let's go through each of the four application areas and examine the support that fastai provides for them.</p>
			<ol>
				<li value="1"><strong class="bold">Text data</strong>, also called <a id="_idIndexMarker072"/>free-form<a id="_idIndexMarker073"/> text: fastai provides support for <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>):<p>a) Curated datasets, including: <strong class="source-inline">AG_NEWS</strong> (~ 0.5 M categorized news articles), and <strong class="source-inline">DBPedia</strong> (training/testing samples from a knowledge base (<a href="https://wiki.dbpedia.org/about">https://wiki.dbpedia.org/about</a>) containing structured content from Wikimedia projects), <strong class="source-inline">YELP_REVIEWS</strong> (~1.5 M Yelp reviews along with corresponding star scores)</p><p>b) Text-specific <strong class="source-inline">DataLoaders</strong> object: <strong class="source-inline">TextDataLoaders</strong> <a href="https://docs.fast.ai/text.data.html#TextDataLoaders">https://docs.fast.ai/text.data.html#TextDataLoaders</a></p><p>c) Text-specific learner <a id="_idIndexMarker074"/>object: <strong class="source-inline">TextLearner</strong> <a href="https://docs.fast.ai/text.learner.html#TextLearner">https://docs.fast.ai/text.learner.html#TextLearner</a></p></li>
				<li><strong class="bold">Tabular data</strong>, also called<a id="_idIndexMarker075"/> structured <a id="_idIndexMarker076"/>data, is data arranged in rows and columns, such as you would find in a <strong class="bold">comma-separated values</strong> (<strong class="bold">CSV</strong>) file or a <a id="_idIndexMarker077"/>database table. fastai provides custom support for deep learning with tabular data, including the following features:<p>a) Tabular data-specific <strong class="source-inline">DataLoaders</strong> object: <strong class="source-inline">TabularDataLoaders </strong><a href="https://docs.fast.ai/tabular.data.html#TabularDataLoaders">https://docs.fast.ai/tabular.data.html#TabularDataLoaders</a></p><p>b) Tabular data-specific <strong class="source-inline">Learner</strong> object: <strong class="source-inline">TabularDataLearner</strong> <a href="https://docs.fast.ai/tabular.learner.html#TabularLearner">https://docs.fast.ai/tabular.learner.html#TabularLearner</a></p><p>c) Utilities to examine the dataset: <strong class="source-inline">TabularPandas </strong><a href="https://docs.fast.ai/tabular.core.html#TabularPandas">https://docs.fast.ai/tabular.core.html#TabularPandas</a></p></li>
				<li><strong class="bold">Recommender systems</strong>, also called <a id="_idIndexMarker078"/>collaborative filtering systems, combine aspects of text and tabular data as well as<a id="_idIndexMarker079"/> combining supervised and unsupervised learning to make predictions about a user's reaction to a given artifact. <p>For example, recommender systems can be used to predict whether viewers will like movies they haven't seen yet or whether readers will like books they haven't read yet. fastai supports deep learning with recommender systems with a variety of features, including the following:</p><p>a) Curated datasets, including <strong class="source-inline">ML_SAMPLE</strong> and <strong class="source-inline">ML_100k</strong> (rankings of thousands of movies by thousands of users) </p><p>b) Recommender system-specific <strong class="source-inline">DataLoaders</strong> object: <strong class="source-inline">CollabDataLoaders</strong> <a href="https://docs.fast.ai/collab.html#CollabDataLoaders">https://docs.fast.ai/collab.html#CollabDataLoaders</a></p><p>c) Recommender <a id="_idIndexMarker080"/>system-specific <strong class="source-inline">Learner</strong> object: <strong class="source-inline">collab_learner</strong> <a href="https://docs.fast.ai/collab.html#Create-a-Learner">https://docs.fast.ai/collab.html#Create-a-Learner</a></p></li>
				<li><strong class="bold">Image data</strong>, also called computer vision<a id="_idIndexMarker081"/> applications. You have already seen fastai in action in this<a id="_idIndexMarker082"/> application area in the MNIST example. Of the four application areas, computer vision gets the most focus from fastai. The following list is just a subset of the many features in the fastai framework that make it easy to create deep learning solutions for image data problems:<p>a) Curated datasets, including: MNIST (handwritten digits) and CARS (15,000+ images of cars, categorized into types) image classification datasets; <strong class="source-inline">BIWI_HEAD_POSE</strong> (images of people, along with descriptions of their positions), <strong class="source-inline">PASCAL_2007</strong>, and <strong class="source-inline">PASCAL_2012</strong> (images, along with corresponding segmentation maps for each image) image localization datasets</p><p>b) Image-specific <strong class="source-inline">DataLoaders</strong> object: <strong class="source-inline">ImageDataLoaders</strong> <a href="https://docs.fast.ai/vision.data.html#ImageDataLoaders">https://docs.fast.ai/vision.data.html#ImageDataLoaders</a></p><p>c) Image-specific <strong class="source-inline">Learner</strong> object: <strong class="source-inline">cnn_learner </strong><a href="https://docs.fast.ai/vision.learner.html#cnn_learner">https://docs.fast.ai/vision.learner.html#cnn_learner</a></p><p>d) Utilities to examine the dataset: many convenient functions that make it easy to render individual images and categories of images </p></li>
			</ol>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>How it works…</h2>
			<p>There is a lot to digest in this section, but don't worry. Each application-specific aspect of fastai gets its own dedicated chapter in which we'll cover the details of the application-specific features (datasets, <strong class="source-inline">DataLoaders</strong>, <strong class="source-inline">Learners</strong>, and others) and show you how to harness these features to create deep learning solutions for each application area. The important thing to note is that fastai provides these application-specific features to make it easy for you to create applications across all four of the areas: <strong class="bold">tabular data</strong>, <strong class="bold">text data</strong>, <strong class="bold">recommender systems</strong>, and <strong class="bold">computer vision</strong>.</p>
			<h1 id="_idParaDest-46"><a id="_idTextAnchor046"/>Working with PyTorch tensors</h1>
			<p>Throughout most <a id="_idIndexMarker083"/>of this book, the focus will be on the features provided by the fastai framework. However, some of the solutions that we'll review also exploit general Python libraries (such as the <strong class="source-inline">pandas</strong> library for deep learning applications with tabular data) as well as aspects of PyTorch, the low-level deep learning framework upon which fastai is built. To give you a small taste of PyTorch, in this section we'll go through some basic examples of using tensors, the PyTorch structure for multidimensional matrices. </p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor047"/>Getting ready</h2>
			<p>If you are already familiar with NumPy arrays, then you will have a good basis for examining PyTorch tensors because tensors play much the same role for PyTorch as NumPy arrays do for general-purpose Python applications. If you are not familiar with NumPy arrays or it's been a while since you have had a chance to use them, take a bit of time to review them—for example, by going through this tutorial: <a href="https://numpy.org/doc/stable/user/quickstart.html">https://numpy.org/doc/stable/user/quickstart.html</a>.</p>
			<p>Once you have completed your NumPy array review, ensure that you have followed the steps to set up fastai in Gradient and confirm that you can open the PyTorch tensor walkthrough notebook (<strong class="source-inline">pytorch_tensor_walkthrough.ipynb</strong>) in the <strong class="source-inline">ch1</strong> directory. If you choose to use Colab, ensure that you have selected <strong class="bold">Runtime</strong> | <strong class="bold">Change runtime type</strong> and have selected <strong class="bold">GPU</strong> as the hardware accelerator.</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor048"/>How to do it…</h2>
			<p>In this section, we'll go through some basic operations with PyTorch tensors:</p>
			<ol>
				<li value="1">Open the <strong class="source-inline">pytorch_tensor_walkthrough.ipynb</strong> PyTorch tensor walkthrough notebook in the <strong class="source-inline">ch1</strong> directory.</li>
				<li>Run the first four cells of the notebook to import the necessary libraries and define three tensors.<p>a) Note that since this notebook only makes use of PyTorch and doesn't need any fastai libraries, we only need one <strong class="source-inline">import</strong> statement:</p><p class="source-code">import torch</p><p>b) Define <strong class="source-inline">a</strong>: a two-dimensional 5x7 tensor with value <strong class="source-inline">1</strong> in every position:</p><div id="_idContainer040" class="IMG---Figure"><img src="image/B16216_01_36.jpg" alt="Figure 1.36 – Defining a 5x7 tensor&#13;&#10;"/></div><p class="figure-caption">Figure 1.36 – Defining a 5x7 tensor</p><p>c) Define <strong class="source-inline">b</strong>: a two-dimensional 5x7 tensor <a id="_idIndexMarker084"/>with <strong class="source-inline">0</strong>s in every position except a diagonal of <strong class="source-inline">1</strong>s:</p><div id="_idContainer041" class="IMG---Figure"><img src="image/B16216_01_37.jpg" alt="Figure 1.37 – Defining a 5x7 tensor with 1s on the diagonal&#13;&#10;"/></div><p class="figure-caption">Figure 1.37 – Defining a 5x7 tensor with 1s on the diagonal</p><p>d) Define <strong class="source-inline">c</strong>: a two-dimensional 5x5 identity tensor:</p><div id="_idContainer042" class="IMG---Figure"><img src="image/B16216_01_38.jpg" alt="Figure 1.38 – Defining a 5x5 identity tensor&#13;&#10;"/></div><p class="figure-caption">Figure 1.38 – Defining a 5x5 identity tensor</p></li>
				<li>Now, run the cells in the <strong class="bold">Examine tensor elements</strong> section of the notebook to look at<a id="_idIndexMarker085"/> parts of one of the tensors.<p>a) Get the <strong class="source-inline">0</strong>th row of tensor <strong class="source-inline">b</strong>:</p><div id="_idContainer043" class="IMG---Figure"><img src="image/B16216_01_39.jpg" alt="Figure 1.39 – The 0th row of tensor b&#13;&#10;"/></div><p class="figure-caption">Figure 1.39 – The 0th row of tensor b</p><p>b) Get the <strong class="source-inline">0</strong>th element of the <strong class="source-inline">0</strong>th row:</p><div id="_idContainer044" class="IMG---Figure"><img src="image/B16216_01_40.jpg" alt="Figure 1.40 – Element [0,0] of tensor b&#13;&#10;"/></div><p class="figure-caption">Figure 1.40 – Element [0,0] of tensor b</p><p>c) Get rows starting at row 2 to the end:</p><div id="_idContainer045" class="IMG---Figure"><img src="image/B16216_01_41.jpg" alt="Figure 1.41 – Rows of tensor b from row to the end&#13;&#10;"/></div><p class="figure-caption">Figure 1.41 – Rows of tensor b from row to the end</p></li>
				<li>Run the cells in the <strong class="bold">Do operations on the tensors</strong> section of the notebook to see how you can <a id="_idIndexMarker086"/>apply basic matrix arithmetic operations to tensors.<p>a) Add tensors <strong class="source-inline">a</strong> and <strong class="source-inline">b</strong>:</p></li>
			</ol>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B16216_01_42.jpg" alt="Figure 1.42 – Adding two tensors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.42 – Adding two tensors</p>
			<p>b) Attempt to multiply tensors <strong class="source-inline">a</strong> and <strong class="source-inline">c</strong>—note that you get an error because the tensors do not have compatible dimensions. To multiply two two-dimensional tensors, the second dimension of the first tensor has to be identical to the first dimension of the second vector:</p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B16216_01_43.jpg" alt="Figure 1.43 – Attempt to multiply two incompatible tensors generates an error&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.43 – Attempt to multiply two incompatible tensors generates an error</p>
			<p>c) Define a 7x7 identity <a id="_idIndexMarker087"/>tensor:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B16216_01_44.jpg" alt="Figure 1.44 – Defining a 7x7 identity tensor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.44 – Defining a 7x7 identity tensor</p>
			<p>d) Now, multiply tensors <strong class="source-inline">a</strong> and <strong class="source-inline">d</strong>—this time, there is no error because the tensors' dimensions are compatible:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B16216_01_45.jpg" alt="Figure 1.45 – Multiplying two compatible tensors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.45 – Multiplying two compatible tensors</p>
			<p>e) Create a new <a id="_idIndexMarker088"/>tensor that is the transpose of tensor <strong class="source-inline">a</strong> (that is, the columns of tensor <strong class="source-inline">a</strong> become the rows of the new tensor):</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B16216_01_46.jpg" alt="Figure 1.46 – Transposing a tensor&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.46 – Transposing a tensor</p>
			<p>f) Multiply the transpose of tensor <strong class="source-inline">a</strong> with tensor <strong class="source-inline">c </strong>— while tensor <strong class="source-inline">a</strong> multiplied by tensor <strong class="source-inline">c</strong> caused an error, there will be no error this time because the tensors' dimensions are compatible:</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B16216_01_47.jpg" alt="Figure 1.47 – Multiplying two compatible tensors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.47 – Multiplying two compatible tensors</p>
			<p>Congratulations! You have <a id="_idIndexMarker089"/>had your first direct taste of PyTorch, the framework that underlies fastai. </p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>How it works…</h2>
			<p>In this section you got a taste of tensors, one of the building blocks of PyTorch. If you are familiar with the relationship between Keras and TensorFlow, you can think of the relationship between fastai and PyTorch being similar. Similar to the way that Keras is a high-level <strong class="bold">application programming interface</strong> (<strong class="bold">API</strong>) for TensorFlow, fastai is built on top of PyTorch and abstracts away <a id="_idIndexMarker090"/>some of the complexity of PyTorch (for example, by making reasonable assumptions about defaults). With fastai, you can focus on creating deep learning applications without having to worry about all the details. </p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor050"/>There's more…</h2>
			<p>If you are curious <a id="_idIndexMarker091"/>and want to get an overview of PyTorch now, you can check out this introductory tutorial: <a href="https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html">https://pytorch.org/tutorials/beginner/nlp/pytorch_tutorial.html</a>. </p>
			<h1 id="_idParaDest-51"><a id="_idTextAnchor051"/>Contrasting fastai with Keras</h1>
			<p>In this section, we'll cover some of the similarities and differences between fastai and Keras. While both <a id="_idIndexMarker092"/>frameworks provide high-level APIs for deep learning, there are some significant differences between them in terms of their architecture and approach to the problem, as well as differences between the communities using each. By <a id="_idIndexMarker093"/>contrasting these two frameworks, you will get a clearer idea of the strengths of fastai and be better prepared for the detailed examinations of fastai applications that are coming in subsequent chapters.</p>
			<h2 id="_idParaDest-52"><a id="_idTextAnchor052"/>Getting ready</h2>
			<p>If you have used Keras recently, then you'll be in good shape to benefit from this section. If you haven't used Keras before, or it's been a while since you've used it, I recommend that you take a brief look at this tutorial so that you have a fresh overview of Keras: <a href="https://keras.io/getting_started/intro_to_keras_for_engineers/">https://keras.io/getting_started/intro_to_keras_for_engineers/</a>.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor053"/>How to do it…</h2>
			<p>In this section, we will compare a Keras approach to the MNIST problem with the fastai MNIST solution that we reviewed earlier in this chapter. You can see the Keras approach in the <strong class="source-inline">ch1</strong> directory of the repository, in <strong class="source-inline">keras_sequential_api_hello_world.ipynb</strong>. </p>
			<p>Note that, by default, you will not be able to execute this Keras notebook in your fastai Gradient instance because the required TensorFlow and Keras libraries are not installed in that instance. You will be able to run the Keras MNIST notebook in Colab, if you have that set up.</p>
			<ol>
				<li value="1">Compare the library <strong class="source-inline">import</strong> statements. Both MNIST examples require a similar number of <strong class="source-inline">import</strong> statements:<p>a) Keras:</p><p class="source-code">import tensorflow as tf</p><p class="source-code">import pydotplus</p><p class="source-code">from tensorflow.keras.utils import plot_model</p><p>b) fastai:</p><p class="source-code">import fastbook</p><p class="source-code">from fastbook import *</p><p class="source-code">from fastai.vision.all import *</p></li>
				<li>Compare the <a id="_idIndexMarker094"/>setup and definition of the dataset:<p>a) Keras—The MNIST <a id="_idIndexMarker095"/>dataset is <em class="italic">oven ready</em> with Keras. Keras offers seven such datasets—for details, see <a href="https://keras.io/api/datasets/">https://keras.io/api/datasets/</a>. By comparison, fastai has over 25 such datasets. For details, see <a href="https://course.fast.ai/datasets">https://course.fast.ai/datasets</a>:</p><p class="source-code">mnist = tf.keras.datasets.mnist</p><p class="source-code">(x_train, y_train), (x_test, y_test) = mnist.load_data()</p><p class="source-code">x_train, x_test = x_train / 255.0, x_test / 255.0</p><p>b) fastai—This requires a <strong class="source-inline">setup</strong> statement that Keras doesn't need (although this <strong class="source-inline">setup</strong> statement saves a step in the Drive mounting process when you are using Colab) but only requires two statements to define the dataset, versus three statements for Keras:</p><p class="source-code">fastbook.setup_book()</p><p class="source-code">path = untar_data(URLs.MNIST)</p><p class="source-code">dls = ImageDataLoaders.from_folder(path, train='training', valid='testing')</p></li>
				<li>Compare the model definition statements:<p>a) Keras—Every layer in the model needs to be explicitly spelled out: </p><p class="source-code"># define layers for the hello world model</p><p class="source-code">hello_world_model = tf.keras.models.Sequential([ </p><p class="source-code">        tf.keras.layers.Flatten(input_shape=(28, 28)), </p><p class="source-code">        tf.keras.layers.Dense(128, activation='relu'), </p><p class="source-code">        tf.keras.layers.Dropout(0.15), </p><p class="source-code">        tf.keras.layers.Dense(10) </p><p class="source-code">])</p><p class="source-code"># compile the hello world model, including specifying the </p><p class="source-code">loss # function, optimizer, and metrics</p><p class="source-code">hello_world_model.compile(optimizer='adam',                                                    </p><p class="source-code">sloss=tf.keras.losses.SparseCategoricalCrossentropy(from</p><p class="source-code">_logits=True), metrics=['accuracy'])</p><p>b) fastai—A single <a id="_idIndexMarker096"/>statement defines the model. The ability to <a id="_idIndexMarker097"/>specify the architecture (in this case, <strong class="source-inline">resnet18</strong>) with a single parameter streamlines the model definition. Note that the architecture specified for the fastai model is not identical to the architecture for the Keras model. For example, if you compared the layers listed in the output of the <strong class="source-inline">learn.summary()</strong> cell in this notebook with the layers specified in the definition of the Keras model, you can see that the fastai model has many more layers than the Keras model. In sum, the contrast between the fastai and Keras solutions for MNIST is not strictly <em class="italic">apples to apples</em>:</p><p class="source-code">learn = cnn_learner(dls, resnet18, pretrained=False,                                                                            </p><p class="source-code">sloss_func=LabelSmoothingCrossEntropy(), metrics=accuracy)</p></li>
				<li>Compare the <strong class="source-inline">fit</strong> statements:<p>a) Keras:</p><p class="source-code">history = hello_world_model.fit(x_train, y_train,</p><p class="source-code">                                           batch_size=64,</p><p class="source-code">                                           epochs=10,</p><p class="source-code">                                           validation</p><p class="source-code">_split=0.15)</p><p>b) fastai:</p><p class="source-code">learn.fit_one_cycle(1, 0.1)</p></li>
				<li>Compare the performance of the Keras model and the fastai model. Again, note that because of differences between the models (including the architecture and details of the fitting process), it's not possible to draw a general conclusion from the differences in performance between the two models:<p>a) Keras:</p><p class="source-code">Loss for test dataset: 0.07588852692145155</p><p class="source-code">Accuracy for test dataset: 0.9775</p><p>b) fastai:</p></li>
			</ol>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B16216_01_48.jpg" alt="Figure 1.48 – Results of training the MNIST model in fastai&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.48 – Results of training the MNIST model in fastai</p>
			<p>You have now seen <a id="_idIndexMarker098"/>a quick comparison of fastai and Keras for the same MNIST<a id="_idIndexMarker099"/> problem.</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor054"/>How it works…</h2>
			<p>What does this comparison between a Keras solution for MNIST and a fastai solution for MNIST tell us?</p>
			<ul>
				<li>Keras offers far fewer <em class="italic">oven-ready</em> datasets than fastai, and the fastai statements for defining such datasets are simpler. This is a critical benefit for fastai, particularly for beginners. It really helps in the process of learning about deep learning to have a wide variety of datasets that can be ingested easily. fastai really delivers on this count thanks to the big and varied set of <em class="italic">oven-ready</em> datasets available with fastai. We'll spend some time in the next chapter taking a closer look at these datasets.</li>
				<li>Other than the model definition, there isn't that much difference in the number of lines of code between Keras and fastai for each of the steps in the solution. This means that for the MNIST problem, Keras isn't far behind fastai's standard of delivering a complete solution with a handful of lines of code. </li>
				<li>The model definition is more complex in Keras, primarily because fastai lets us define the layers that make up the model with a single architecture parameter, whereas we have to explicitly define the layers in Keras. A mitigating factor for the complexity of the<a id="_idIndexMarker100"/> model definition in Keras is readability. In Keras, the<a id="_idIndexMarker101"/> layers are explicitly listed. By comparison, in the high-level fastai API, the layers are not listed.</li>
				<li>fastai offers better usability than Keras by making it possible for users to use the high-level fastai API without having to worry about all the explicit details.</li>
				<li>The statement for fitting the model is simpler in fastai. In addition, fastai incorporates best practices in default settings that often result in faster fitting times and better performance. </li>
			</ul>
			<p>Keras benefits from greater transparency because the layers are explicitly listed. fastai has superior usability and out-of-the box performance thanks to carefully selected defaults for many settings. We are not going to do additional Keras versus fastai bakeoffs in this book, but I expect that, based on my experience using both Keras and fastai, fastai's benefits would stand out even more in complex applications. In addition, fastai has a big advantage because of its large set of curated, <em class="italic">oven-ready</em> datasets.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor055"/>Test your knowledge</h1>
			<p>Now that you have completed the recipes in this chapter, you can follow the next steps to exercise that you have learned:</p>
			<ol>
				<li value="1">Make a copy of the <strong class="source-inline">mnist_hello_world.ipynb</strong> notebook—call it <strong class="source-inline">mnist_hello_world_variations.ipynb</strong>.</li>
				<li>Update your new copy of the notebook to ingest a variation of the MNIST dataset, called <strong class="source-inline">MNIST_SAMPLE</strong>. Which statement will you need to update to ingest this dataset rather than the full-blown MNIST curated dataset?</li>
				<li>Use the <strong class="source-inline">path.ls()</strong> statement to examine the directory structure of the <strong class="source-inline">MNIST_SAMPLE</strong> dataset. How is the output of this statement different from its output for the full-blown MNIST dataset? </li>
				<li>Keeping in mind the difference in the directory structure of the <strong class="source-inline">MNIST_SAMPLE</strong> dataset, update the values of the <strong class="source-inline">train</strong> and <strong class="source-inline">valid</strong> parameters in the following statement so that it will work with this dataset:<p class="source-code">dls = ImageDataLoaders.from_folder(path, train='training', valid='testing')</p></li>
				<li>Again keeping the directory structure in mind, update the following statement so that it will work with the <strong class="source-inline">MNIST_SAMPLE</strong> dataset:<p class="source-code">img_files = get_image_files(path/"testing")</p></li>
				<li>The <strong class="source-inline">MNIST_SAMPLE</strong> dataset is smaller than the full-blown MNIST dataset. Keeping this in mind, update the following statements so that they will work with the smaller dataset:<p class="source-code">img = PILImage.create(img_files[7000])</p><p class="source-code">img = PILImage.create(img_files[2030])</p><p class="source-code">img = PILImage.create(img_files[5800])</p></li>
				<li>Now that you have updated the notebook to work with the <strong class="source-inline">MNIST_SAMPLE</strong> dataset, run the whole notebook to confirm that it can run to the end with no errors.</li>
			</ol>
			<p>Congratulations! If you have completed this section, then you have succ<a id="_idTextAnchor056"/>essfully adapted a recipe to work with another curated dataset.</p>
		</div>
	</body></html>