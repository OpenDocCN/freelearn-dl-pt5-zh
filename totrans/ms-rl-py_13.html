<html><head></head><body>
		<div id="_idContainer1427">
			<h1 id="_idParaDest-214"><em class="italic"><a id="_idTextAnchor220"/>Chapter 10</em>: Machine Teaching</h1>
			<p>The great excitement about <strong class="bold">reinforcement learning</strong> (<strong class="bold">RL</strong>) is, to a significant extent, due to its similarities to human learning: an RL agent learns from experience. This is also why many consider it as the path to artificial general intelligence. On the other hand, if you think about it, reducing human learning to just trial and error would be a gross underestimation. We don't discover everything we know, in science, art, engineering, and so on, from scratch when we are born! Instead, we build on knowledge and intuition that have been accumulated over thousands of years! We transfer this knowledge among us through different, structured or unstructured, forms of <strong class="bold">teaching</strong>. This capability makes it possible for us to gain skills relatively quickly and advance common knowledge.</p>
			<p>When we think about it from this perspective, what we are doing with machine learning seems quite inefficient: we dump a bunch of raw data into algorithms, or expose them to an environment, in the case of RL, and train them with virtually no guidance. This is partly why machine learning requires so much data and fails at times. </p>
			<p><strong class="bold">Machine teaching</strong> (<strong class="bold">MT</strong>) is an emerging approach that shifts the focus to extracting knowledge from a teacher, rather than raw data, which guides the process of training machine learning algorithms. In turn, learning new skills and mappings is achieved more efficiently and with less data, time, and compute. In this chapter, we will introduce the components of MT for RL and some of its most important methods, such as reward function engineering, curriculum learning, demonstration learning, and action masking. At the end, we will also discuss the downsides and the future of MT. More concretely, we will cover the following topics in this chapter:</p>
			<ul>
				<li>Introduction to MT</li>
				<li>Engineering the reward function</li>
				<li>Curriculum learning</li>
				<li>Warm starts and demonstration learning</li>
				<li>Action masking</li>
				<li>Concept networks</li>
				<li>Downsides and the promises of MT</li>
			</ul>
			<h1 id="_idParaDest-215"><a id="_idTextAnchor221"/>Technical requirements</h1>
			<p>All the code for the chapter can be found at the following GitHub URL:</p>
			<p><a href="https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python">https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python</a></p>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor222"/>Introduction to MT</h1>
			<p>MT is the name of <a id="_idIndexMarker933"/>a general approach and collection of methods to efficiently transfer knowledge from a teacher – a subject matter expert – to a machine learning algorithm. With that, we aim to make the training much more efficient, and even feasible for tasks that would be impossible to achieve otherwise. Let's talk about what MT is in more detail, why we need it, and what its components are.</p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor223"/>Understanding the need for MT</h2>
			<p>Did you know that<a id="_idIndexMarker934"/> the United States is expected to spend about 1.25 trillion dollars, around 5% of its gross domestic product, on education in 2021? This should speak to the existential significance of education to our society and civilization (and many would argue that we should spend more). We humans have built such a giant education system, which we expect people to spend many years in, because we don't expect ourselves to be able to decipher the alphabet or math on our own. Not just that, we continuously learn from teachers around us, about how to use software, how to drive, how to cook, and so on. These teachers don't have to be human teachers: books, blog posts, manuals, and course materials all distill valuable information for us so that we can learn, not just in school but throughout our lives. </p>
			<p>I hope this convinces you of the importance of teaching. But if you found this example too populist and perhaps a bit irrelevant to RL, let's discuss how MT could specifically help in RL. </p>
			<h3>Feasibility of learning</h3>
			<p>Have you ever<a id="_idIndexMarker935"/> felt overwhelmed when trying to learn something on your own, without a (good) teacher? This is akin to an RL agent not figuring out a good policy for the problem at hand due to an overwhelming number of possible policies. One of the main obstacles in this process is the lack of proper feedback about their quality. You can also think of <strong class="bold">hard exploration problems</strong> with sparse rewards in the same context, a serious challenge in RL. </p>
			<p>Consider the following example: an RL agent is trying to learn chess against a competitive opponent with a reward of +1 for winning, 0 for draw, and -1 for losing at the end of the game. The RL agent needs to stumble upon tens of "good moves," one after another, and among many alternative moves at each step, to be able to get its first 0 or +1 reward. Since this is a low likelihood, the training is likely to fail without a huge exploration budget. A teacher, on the other hand, might guide the exploration so that the RL agent knows at least a few ways to succeed, from which it can gradually improve upon the winning strategies. </p>
			<p class="callout-heading">Info</p>
			<p class="callout">When DeepMind created its AlphaStar agent to play StarCraft II, they used supervised learning to train the agent on past human game logs before going into RL-based training. Human players in some sense were the first teachers of the agent, and without them, the training would be impractical/too costly. To support this argument, you can take the example of the OpenAI Five agent trained to play Dota 2. It took almost a year to train the agent.</p>
			<p>The following figure shows the agent in action:</p>
			<p class="figure-caption"><a id="_idTextAnchor224"/></p>
			<div>
				<div id="_idContainer1386" class="IMG---Figure">
					<img src="image/B14160_10_1.jpg" alt="Figure 10.1 – DeepMind's AlphaStar agent in action (source: The AlphaStar Team, 2019)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.1 – DeepMind's AlphaStar agent in action (source: The AlphaStar Team, 2019)</p>
			<p>In summary, having <a id="_idIndexMarker936"/>access to a teacher could make the learning feasible in a reasonable amount of time.</p>
			<h3>Time, data, and compute efficiency</h3>
			<p>Let's say you<a id="_idIndexMarker937"/> have enough compute resources and<a id="_idIndexMarker938"/> can afford to try an enormous <a id="_idIndexMarker939"/>number of sequences of moves for the RL agent to discover winning strategies in an environment. Just because you can, doesn't mean that you should do it and waste all those resources. A teacher could help you to greatly reduce the training time, data, and compute. You can use the resources you saved to iterate on your ideas and come up with better agents.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Do you have mentors in your life to help you with your career, education, marriage, and so on? Or do you read books about these topics? What is your motivation? You don't want to repeat the mistakes of others or reinvent what others already know just to waste your time, energy, and opportunities, do you? MT similarly helps your agent jumpstart its task.</p>
			<p>The benefit of<a id="_idIndexMarker940"/> MT goes beyond the feasibility of<a id="_idIndexMarker941"/> learning or its efficiency. Next, let's talk about another aspect: the safety of your agent.</p>
			<h3>Ensuring the safety of the agent</h3>
			<p>A teacher is a <a id="_idIndexMarker942"/>subject matter expert on a topic. Therefore, a teacher usually has a pretty good idea about what actions under which conditions can get the agent in trouble. The teacher can inform the agent about these conditions by limiting the actions it could take to ensure its safety. For example, while training an RL agent for a self-driving car, it is natural to limit the speed of the car depending on the conditions of the road. This is especially needed if the training happens in the real world so that the agent does not blindly explore crazy actions to discover how to drive. Even when the training happens in a simulation, imposing these limitations will help with the efficient use of the exploration budget, related to the tip in the previous section.</p>
			<h3>Democratizing machine learning</h3>
			<p>When teachers<a id="_idIndexMarker943"/> train students, they do not worry about the details of the biological mechanisms of learning, such as which chemicals are transferred between which brain cells. Those details are abstracted away from the teacher; neuroscientists and experts who study the brain put out research about effective teaching and learning techniques.</p>
			<p>Just like how teachers don't have to be neuroscientists, subject matter experts don't have to be machine learning experts to train machine learning algorithms. The MT paradigm suggests abstracting the low-level details of machine learning away from the machine teacher by developing effective and intuitive teaching methods. With that, it would be much easier for subject matter experts to infuse their knowledge into machines. Eventually, this would lead to the democratization of machine learning and its much greater use in many applications.</p>
			<p>Data science, in general, requires combining business insights and expertise with mathematical tools and software to create value. When you want to apply RL to business problems, the situation is the same. This often requires either the data scientist to learn about the business or the subject matter expert to learn about data science, or people from both fields working together in a team. This poses a high bar for the adoption of (advanced) machine learning techniques in many settings, because it is rare for these two types of people to exist at the same time in the same place. </p>
			<p class="callout-heading">Info</p>
			<p class="callout">A McKinsey study shows that lack of analytical talent is a major barrier to unlocking the value in data and analytics. MT, its specific tools aside, is a paradigm to overcome these barriers by lowering the bar for entry for non-machine learning experts through the creation of intuitive tools to this end. To check out the study, visit <a href="https://mck.co/2J3TFEj">https://mck.co/2J3TFEj</a>.</p>
			<p>This vision we <a id="_idIndexMarker944"/>have just mentioned is aimed more for the long term as it requires a lot of research and abstractions on the machine learning side. The methods we will cover in this section will be pretty technical. For example, we will discuss the <strong class="bold">action masking</strong> method to limit the available actions for the agent depending on the state it is in, which will require coding and modifying the neural network outcomes. However, you can imagine an advanced MT tool listening to the teacher saying "don't go over 40 miles per hour within the city limits," parsing that command, and implementing action masking under the hood for a self-driving car agent:</p>
			<div>
				<div id="_idContainer1387" class="IMG---Figure">
					<img src="image/B14160_10_2.jpg" alt="Figure 10.2 – The future of MT?&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.2 – The future of MT?</p>
			<p>Before closing this section and diving into the details of MT, let me put out a necessary disclaimer.</p>
			<p class="callout-heading">Disclaimer</p>
			<p class="callout">One of the most vocal proponents of the MT approach is Microsoft and its Autonomous Systems division. As of the time of writing this book, I am an employee of the Autonomous Systems organization of Microsoft, working toward the mission of using MT to create intelligent systems. However, my goal here is not to promote any Microsoft product or discourse, but to tell you about this emerging topic that I find important. In addition, I do not officially represent Microsoft in any capacity and my views on the topic may not necessarily align with the company's. If you are curious about Microsoft's view on MT, check out the blog post at <a href="https://blogs.microsoft.com/ai/machine-teaching/">https://blogs.microsoft.com/ai/machine-teaching/</a> and the Autonomous Systems website at <a href="https://www.microsoft.com/en-us/ai/autonomous-systems">https://www.microsoft.com/en-us/ai/autonomous-systems</a>.</p>
			<p>Now, it is time <a id="_idIndexMarker945"/>to make the discussion more concrete. In the next section, let's look at the elements of MT.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor225"/>Exploring the elements of MT</h2>
			<p>As MT is an <a id="_idIndexMarker946"/>emerging field, it is hard to formally define its elements. Still, let's look into some of the common components and themes used in it. We have already discussed who the machine teacher is, but let's start with that for the sake of completeness. Then, we will look into concepts, lessons, curriculum, training data, and feedback. </p>
			<h3>Machine teacher</h3>
			<p>The <strong class="bold">machine teacher</strong>, or<a id="_idIndexMarker947"/> simply the <strong class="bold">teacher</strong>, is<a id="_idIndexMarker948"/> the subject matter expert of the problem at hand. In the absence of abstractions that decouple machine learning from teaching, this will be the data scientist – you – but this time with the explicit concern of guiding the training using your knowledge of the problem domain.</p>
			<h3>Concept</h3>
			<p>The <strong class="bold">concept</strong> is a <a id="_idIndexMarker949"/>specific part of the skillset that is needed to solve the problem. Think about training a basketball player in real life. Training does not consist of only practice games but is divided into mastering individual skills as well. Some of these skills are as follows:</p>
			<ul>
				<li>Shooting</li>
				<li>Passing</li>
				<li>Dribbling</li>
				<li>Stopping and landing</li>
			</ul>
			<p>Conventional training of an RL agent playing basketball would be through playing entire games, with which we would expect the agent to pick up these individual skills. MT suggests breaking the problem down into smaller concepts to learn, such as the skills we listed previously. This has several benefits:</p>
			<ul>
				<li>A monolithic task often comes with sparse rewards, which is challenging for an RL agent to learn from. For example, winning the basketball game would be +1 and losing would be -1. However, the machine teacher would know that winning a game would be possible through mastering individual skills. To train the agent on individual skills and concepts, there will be rewards assigned to them. This is helpful to get around the sparse reward issue and provide more frequent feedback to the agent in a manner that facilitates learning.</li>
				<li>The credit assignment problem is a serious challenge in RL, which is about the difficulty of attributing the reward in later stages to individual actions in the earlier ones. When the training is broken down into concepts, it is easier to see the concepts that <a id="_idIndexMarker950"/>the agent is not good at. To be specific, this does not solve the credit assignment problem in itself. It is still the teacher that determines whether mastering a particular concept is important. But once these concepts are defined by the teacher, it is easier to isolate what the agent is and isn't good at.</li>
				<li>As a corollary to the preceding point, the teacher can allocate more of the training budget to concepts that need more training and/or are difficult to learn. This results in more efficient use of time and compute resources.</li>
			</ul>
			<p>For all these reasons, a task that is impractical or costly to solve monolithically can be efficiently<a id="_idIndexMarker951"/> solved by breaking it down into concepts.</p>
			<h3>Lessons and curriculum</h3>
			<p>Another<a id="_idIndexMarker952"/> important element in MT is called <strong class="bold">curriculum learning</strong>. While <a id="_idIndexMarker953"/>training <a id="_idIndexMarker954"/>the agent on a concept, exposing it to an expert-level difficulty may derail the training. Instead, what makes more sense is to start with some easy settings and increase the difficulty gradually. Each of these difficulty levels makes a separate <strong class="bold">lesson</strong>, and they, together with the success thresholds that define the transition criteria from one lesson to the next, comprise a <strong class="bold">curriculum</strong>. </p>
			<p>Curriculum learning is one of the most important research areas in RL and we will elaborate more on it later. A<a id="_idIndexMarker955"/> curriculum may be designed by hand by the teacher, or an <strong class="bold">auto-curriculum</strong> algorithm can be used. </p>
			<h3>Training material/data</h3>
			<p>Related to the<a id="_idIndexMarker956"/> previous point, another aspect of MT is to engineer the data that the agent will learn from. For example, the machine teacher could seed the training with data that includes successful episodes while using off-policy methods, which can overcome hard exploration tasks. This data could be obtained from an existing non-RL controller or the teacher's actions. This approach<a id="_idIndexMarker957"/> is also called <strong class="bold">demonstration learning</strong>.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">Demonstration learning is a popular method to train RL agents, especially in robotics. An ICRA paper by Nair et al. shows robots how to pick and place objects to seed the training of an RL agent. Check out the video at <a href="https://youtu.be/bb_acE1yzDo">https://youtu.be/bb_acE1yzDo</a>. </p>
			<p>Conversely, the teacher could steer the agent away from bad actions. An effective way of achieving this is through <strong class="bold">action masking</strong>, which limits the available action space given for observation to a desirable set of actions.</p>
			<p>Another way of engineering the training data that the agent consumes is to monitor the performance of the agent, identify the parts of the state space that it needs more training in, and <a id="_idIndexMarker958"/>expose the agent to these states to improve the performance.</p>
			<h3>Feedback</h3>
			<p>RL agents learn <a id="_idIndexMarker959"/>through feedback in the form of rewards. Engineering the reward function to make the learning easy – and even feasible in some cases that would have been infeasible otherwise – is one of the most important tasks of the machine teacher. This is usually an iterative process. It is common to revise the reward function many times during the course of a project to get the agent to learn the desired behavior. A futuristic MT tool could involve interacting with the agent through natural language to provide this feedback, which shapes the reward function used under the hood.</p>
			<p>With this, we have introduced you to MT and its elements. Next, we will look into specific methods. Rather than going over an entire MT strategy for a sample problem, we will next focus on individual methods. You can use them as building blocks of your MT strategy depending on what your problem needs. We will start with the most common one, reward function engineering, which you might have already used before.</p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor226"/>Engineering the reward function</h1>
			<p>Reward function<a id="_idIndexMarker960"/> engineering means crafting the reward dynamics of the environment in an RL problem so that it reflects the exact objective you have in mind for your agent. How you define your reward function might make the training easy, difficult, or even impossible for the agent. Therefore, in most RL projects, significant effort is dedicated to designing the reward. In this section, we will cover some specific cases where you will need to do it and how, then provide a specific example, and finally, discuss the challenges that come with engineering the reward function.</p>
			<h2 id="_idParaDest-220"><a id="_idTextAnchor227"/>When to engineer the reward function </h2>
			<p>Multiple times in<a id="_idIndexMarker961"/> this book, including in the previous section when we discussed concepts, we have mentioned how sparse rewards pose a problem for learning. One way of dealing with this is to <strong class="bold">shape the reward</strong> to make it non-sparse. The sparse reward case, therefore, is a common reason why we may want to do reward function engineering. Yet, it is not the only one. Not all environments/problems have a predefined reward for you like in an Atari game. In addition, in some cases, there are multiple objectives that you want your agent to achieve. For all<a id="_idIndexMarker962"/> these reasons, many real-life tasks require the machine teacher to specify the reward function based on their expertise. Let's look into these cases next.</p>
			<h3>Sparse rewards</h3>
			<p>When the <a id="_idIndexMarker963"/>reward is sparse, meaning that the agent sees a change in the reward (from a constant 0 to positive/negative, from a constant negative to positive, and so on) with an unlikely sequence of random actions, the learning gets difficult. That is because the agent needs to stumble upon this sequence through random trial and error, which makes the problem exploration hard.</p>
			<p>Learning chess against a competitive player where the reward is +1 for winning, 0 for draw, and -1 for losing at the very end is a good example of an environment with sparse rewards. A classic example used in RL benchmarks is Montezuma's Revenge, an Atari game in which the player needs to collect equipment (keys, torch, and so on), open doors, and so on to be able to make any progress, which is very unlikely just by taking random actions:</p>
			<div>
				<div id="_idContainer1388" class="IMG---Figure">
					<img src="image/B14160_10_3.jpg" alt="Figure 10.3 – Montezuma's Revenge&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.3 – Montezuma's Revenge</p>
			<p>In such hard exploration problems, a <a id="_idIndexMarker964"/>common strategy is <strong class="bold">reward shaping</strong>, which is to modify the reward to steer the agent toward high rewards. For example, a reward shaping strategy could be to give -0.1 reward to the agent learning chess if it loses the queen, and smaller penalties when other pieces are lost. With that, the machine teacher conveys their knowledge about the queen being an important piece in the game to the agent, although <a id="_idIndexMarker965"/>not losing the queen or any other pieces (except the king) in itself is not the game's objective.</p>
			<p>We will talk more about reward shaping in detail later.</p>
			<h3>Qualitative objectives</h3>
			<p>Let's say you are <a id="_idIndexMarker966"/>trying to teach a humanoid robot how to walk. Well, what is walking? How can you define it? How can you define it mathematically? What kind of walking gets a high reward? Is it just about moving forward or are there some elements of aesthetics? As you can see, it is not easy to put what you have in your mind about walking into mathematical expressions.</p>
			<p>In their famous work, researchers at DeepMind used the following reward function for the humanoid robot they trained to walk:</p>
			<p class="figure-caption"><img src="image/Formula_10_001.png" alt=""/></p>
			<p>Here, <img src="image/Formula_10_002.png" alt=""/> and <img src="image/Formula_10_003.png" alt=""/> are velocities along the <img src="image/Formula_10_004.png" alt=""/> and <img src="image/Formula_10_005.png" alt=""/> axes, <img src="image/Formula_10_006.png" alt=""/> is the position on the <img src="image/Formula_10_007.png" alt=""/> axis, <img src="image/Formula_10_008.png" alt=""/> is a cutoff for the velocity reward, and <img src="image/Formula_05_281.png" alt=""/> is the control applied on the joints. As you can see, there are many arbitrary coefficients that are likely to differ for other kinds of robots. In fact, the paper uses three separate functions for three separate robot bodies. </p>
			<p class="callout-heading">Info</p>
			<p class="callout">If you are curious about how the robot walks after being trained with this reward, watch this video: <a href="https://youtu.be/gn4nRCC9TwQ">https://youtu.be/gn4nRCC9TwQ</a>. The way the robot walks is, how can I put it, a bit weird…</p>
			<p>So, in short, qualitative<a id="_idIndexMarker967"/> objectives require crafting a reward function to obtain the behavior intended.</p>
			<h3>Multi-objective tasks</h3>
			<p>A common <a id="_idIndexMarker968"/>situation in RL is to have multi-objective tasks. On the other hand, conventionally, RL algorithms optimize a scalar reward. As a result, when there are multiple objectives, they need to be reconciled into a single reward. This often results in mixing apples and oranges, and appropriately weighing them in the reward could be quite painful.</p>
			<p>When the task objective is qualitative, it is often also multi-objective. For example, the task of driving a car includes elements of speed, safety, fuel efficiency, equipment wear and tear, comfort, and so on. You can guess that it is not easy to express what comfort means mathematically. But there are also many tasks in which multiple quantitative objectives need to be optimized concurrently. An example of this is to control an HVAC system to keep the room temperature as close to the specified setpoint as possible while minimizing the cost of energy. In this problem, it is the machine teacher's duty to balance these trade-offs.</p>
			<p>It is very common for an RL task to involve one or more of the preceding situations. Then, engineering the reward function becomes a major challenge.</p>
			<p>After this much discussion, let's focus on reward shaping a little bit more.</p>
			<h2 id="_idParaDest-221"><a id="_idTextAnchor228"/>Reward shaping</h2>
			<p>The idea behind <a id="_idIndexMarker969"/>reward shaping is to incentivize the agent to move toward success states and discourage it from reaching failure states using positive and negative rewards that are relatively smaller in magnitude with respect to the actual reward (and punishment). This will usually shorten the training time as the agent will not spend as much time trying to discover how to reach a success state. Here is a simple example to make our discussion more concrete.</p>
			<h3>Simple robot example</h3>
			<p>Suppose that <a id="_idIndexMarker970"/>a robot is moving on a horizontal axis with 0.01 step sizes. The goal is to reach +1 and avoid -1, which are the terminal states, as shown here:</p>
			<div>
				<div id="_idContainer1398" class="IMG---Figure">
					<img src="image/B14160_10_4.jpg" alt="Figure 10.4 – Simple robot example with sparse rewards&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.4 – Simple robot example with sparse rewards</p>
			<p>As you can imagine, it is very difficult for the robot to discover the trophy when we use sparse rewards, such as giving +1 for reaching the trophy and -1 for reaching the failure state. If there is a timeout for the task, let's say after 200 steps, the episode is likely to end with that. </p>
			<p>In this example, we can guide the robot by giving a reward that increases as it moves toward the trophy. A simple choice could be setting <img src="image/Formula_10_010.png" alt=""/>, where <img src="image/Formula_10_011.png" alt=""/> is the position on the axis. </p>
			<p>There are two potential problems with this reward function:</p>
			<ul>
				<li>As the robot moves right, the incremental relative benefit of moving even further right gets smaller. For example, going from <img src="image/Formula_10_012.png" alt=""/> to <img src="image/Formula_10_013.png" alt=""/> increases the step reward by 10% but going from <img src="image/Formula_10_014.png" alt=""/> to <img src="image/Formula_10_015.png" alt=""/> only increases it by 1.1%. </li>
				<li>Since the goal of the agent is to maximize the total cumulative reward, it is not in the best interest of the agent to reach the trophy since that will terminate the episode. Instead, the agent might choose to hang out at 0.99 forever (or until the time limit is reached).</li>
			</ul>
			<p>We can address the<a id="_idIndexMarker971"/> first issue by shaping the reward in such a way that the agent gets increasing additional rewards for moving toward the success state. For example, we can set the reward to be <img src="image/Formula_10_016.png" alt=""/> within the <img src="image/Formula_10_017.png" alt=""/> range:</p>
			<div>
				<div id="_idContainer1407" class="IMG---Figure">
					<img src="image/B14160_10_5.jpg" alt="Figure 10.5 – A sample reward shaping where &#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.5 – A sample reward shaping where <img src="image/Formula_10_018.png" alt=""/></p>
			<p>With this, the amount of incentive accelerates as the robot gets closer to the trophy, encouraging the robot even further to go right. The situation is similar for the punishment for going to left.</p>
			<p>To address the latter, we should encourage the agent to finish the episode as soon as possible. We need to do that by punishing the agent for every time step it spends in the environment.</p>
			<p>This example shows two things: designing the reward function can get tricky even in such simple <a id="_idIndexMarker972"/>problems, and we need to consider the design of the reward function together with the terminal conditions we set.</p>
			<p>Before going into specific suggestions for reward shaping, let's also discuss how terminal conditions play a role in agent behavior.</p>
			<h3>Terminal conditions</h3>
			<p>Since the goal of the<a id="_idIndexMarker973"/> agent is to maximize the expected cumulative reward over an episode, how the episode ends will directly affect the agent's behavior. So, we can and should leverage a good set of terminal conditions to guide the agent.</p>
			<p>We can talk about <a id="_idIndexMarker974"/>several types of terminal conditions:</p>
			<ul>
				<li><strong class="bold">Positive terminal</strong> indicates the agent has accomplished the task (or part of it, depending on <a id="_idIndexMarker975"/>how you define success). This terminal condition comes with a significant positive reward to encourage the agent to reach it.</li>
				<li><strong class="bold">Negative terminal</strong> indicates <a id="_idIndexMarker976"/>a failure state and yields a significant negative reward. The agent will try to avoid these conditions.</li>
				<li><strong class="bold">Neutral terminal</strong> is<a id="_idIndexMarker977"/> neither success nor failure in itself, but it indicates that the agent has no path to success, and the episode is terminated with a zero reward in the last step. The machine teacher doesn't want the agent to spend any time after that point in the environment but to reset back to the initial conditions. Although this does not directly <a id="_idIndexMarker978"/>punish the agent, it prevents it from collecting additional rewards (or penalties). So, it is implicit feedback to the agent.</li>
				<li><strong class="bold">Time limit</strong> bounds<a id="_idIndexMarker979"/> the number of time steps spent in the environment. It encourages the agent to seek high rewards within this budget, rather than wandering around forever. It works as feedback about what sequences of actions are rewarding in a reasonable amount of time and which ones are not.</li>
			</ul>
			<p>In some environments, terminal conditions are preset; but in most cases, the machine teacher has<a id="_idIndexMarker980"/> the flexibility to set them.</p>
			<p>Now that we have all the components described, let's discuss some practical tips for reward shaping.</p>
			<h3>Practical tips for reward shaping</h3>
			<p>Here are some <a id="_idIndexMarker981"/>general guidelines you should keep in mind while designing the reward function:</p>
			<ul>
				<li>Keep the step reward between <img src="image/Formula_10_019.png" alt=""/> and <img src="image/Formula_10_020.png" alt=""/> for numerical stability, whenever possible.</li>
				<li>Express your reward (and state) with terms that are generalizable to other versions of your problem. For example, rather than rewarding the agent for reaching a point, <img src="image/Formula_10_021.png" alt=""/>, you can incentivize reducing the distance to the target, based on <img src="image/Formula_10_022.png" alt=""/>.</li>
				<li>Having a smooth reward function will provide the agent with feedback that is easy to follow. </li>
				<li>The agent should be able to correlate the reward with its observations. In other words, observations must contain some information about what is leading to high or low rewards. Otherwise, there won't be much for the agent to base its decisions on.</li>
				<li>The total incentive for getting close to the target states should not outweigh the actual reward of reaching the target state. Otherwise, the agent will prefer to focus on<a id="_idIndexMarker982"/> accumulating the incentives, rather than achieving the actual goal. </li>
				<li>If you would like your agent to complete a task as soon as possible, assign a negative reward to each time step. The agent will try to finish the episode to avoid accumulating negative rewards.</li>
				<li>If the agent can collect more positive rewards by not reaching a terminal state, it will try to collect them before reaching a terminal condition. If the agent is likely to collect only negative rewards by staying inside an episode (such as when there is a penalty per time step), it will try to reach a terminal condition. The latter can result in suicidal behavior if the life is too painful for the agent, meaning that the agent can seek any terminal state, including natural ones or failure states, to avoid incurring excessive penalties by staying alive.</li>
			</ul>
			<p>Now, we will look at an example of reward shaping using OpenAI.</p>
			<h2 id="_idParaDest-222"><a id="_idTextAnchor229"/>Example – reward shaping for a mountain car</h2>
			<p>In OpenAI's<a id="_idIndexMarker983"/> mountain car environment, the goal of the car is to reach the goal point on top of one of the hills, which is illustrated in <em class="italic">Figure 10.6</em>. The action space is to push the car to the left, push to the right, or apply no force:</p>
			<div>
				<div id="_idContainer1413" class="IMG---Figure">
					<img src="image/B14160_10_6.jpg" alt="Figure 10.6 – Mountain car environment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 10.6 – Mountain car <a id="_idTextAnchor230"/>environment</p>
			<p>Since the force we apply is not enough to climb the hill and reach the goal, the car needs to gradually<a id="_idIndexMarker984"/> accumulate potential energy by climbing in opposite directions. Figuring this out is non-trivial, because the car does not know what the goal is until it reaches it, which can be achieved after 100+ steps of correct actions. The only reward in the default environment is -1 per time step to encourage the car to reach the goal as quickly as possible to avoid accumulating negative rewards. The episode terminates after 200 steps. </p>
			<p>We will use various MT techniques to train our agent throughout the chapter. To that end, we will have a custom training flow and a customized environment with which we can experiment with these methods. Let's get things set up first.</p>
			<h3>Setting up the environment</h3>
			<p>Our<a id="_idIndexMarker985"/> custom <strong class="source-inline">MountainCar</strong> environment wraps OpenAI's <strong class="source-inline">MountainCar-v0</strong>, which looks like the following:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Chapter10/custom_mcar.py</p>
			<p class="source-code">class MountainCar(gym.Env):</p>
			<p class="source-code">    def __init__(self, env_config={}):</p>
			<p class="source-code">        self.wrapped = gym.make("MountainCar-v0")</p>
			<p class="source-code">        self.action_space = self.wrapped.action_space</p>
			<p class="source-code">        ...</p>
			<p>If you visit that file now, it may look complicated since it includes some add-ons we are yet to cover. For now, just know that this is the environment we will use.</p>
			<p>We will use <a id="_idIndexMarker986"/>Ray/RLlib's Ape-X DQN throughout the chapter to train our agents. Make sure that you have them installed, preferably within a virtual environment:</p>
			<p class="source-code">$ virtualenv rlenv</p>
			<p class="source-code">$ source rlenv/bin/activate</p>
			<p class="source-code">$ pip install gym[box2d]</p>
			<p class="source-code">$ pip install tensorflow==2.3.1</p>
			<p class="source-code">$ pip install ray[rllib]==1.0.1</p>
			<p>With that, next, let's get a baseline performance by training an agent without any MT.</p>
			<h3>Getting a baseline performance</h3>
			<p>We will use <a id="_idIndexMarker987"/>a single script for all of the training. We define a <strong class="source-inline">STRATEGY</strong> constant at the top of the script, which will control the strategy to be used in the training:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Chapter10/mcar_train.py</p>
			<p class="source-code">ALL_STRATEGIES = [</p>
			<p class="source-code">    "default",</p>
			<p class="source-code">    "with_dueling",</p>
			<p class="source-code">    "custom_reward",</p>
			<p class="source-code">    ...</p>
			<p class="source-code">]</p>
			<p class="source-code">STRATEGY = "default"</p>
			<p>For each strategy, we will kick off five different training sessions of 2 million time steps each, so we set <strong class="source-inline">NUM_TRIALS = 5</strong> and <strong class="source-inline">MAX_STEPS = 2e6</strong>. At the end of each training session, we will evaluate the trained agent over <strong class="source-inline">NUM_FINAL_EVAL_EPS = 20</strong> episodes. Therefore, the result for each strategy will reflect the average length of 100 test episodes, where a lower number indicates better performance.</p>
			<p>For most of the strategies, you will see that we have two variants: with and without dueling networks enabled. When the dueling network is enabled, the agent achieves a near-optimal result (around 100 steps to reach the goal), so it becomes uninteresting for our case. Moreover, when we implement action masking later in the chapter, we won't use dueling <a id="_idIndexMarker988"/>networks to avoid complexities in RLlib. Therefore, we will focus on the no-dueling network case in our example. Finally, note that the results of the experiments will be written to <strong class="source-inline">results.csv</strong>.</p>
			<p>With that, let's train our first agents. When I used no MT, in my case, I obtained the following average episode lengths:</p>
			<p class="source-code">STRATEGY = "default"</p>
			<p>The outcome is as follows:</p>
			<p class="source-code"><strong class="bold">Average episode length: 192.23</strong></p>
			<p>Let's see next whether reward shaping helps us here.</p>
			<h3>Solving the problem with a shaped reward</h3>
			<p>Anyone looking at <a id="_idIndexMarker989"/>the mountain car problem could tell that we should encourage the car to go right, at least eventually. In this section, that is what we'll do. The dip position of the car corresponds to an <img src="image/Formula_10_023.png" alt=""/> position of -0.5. We modify the reward function to give a quadratically increasing reward to the agent for going beyond this position toward the right. This happens inside the custom <strong class="source-inline">MountainCar</strong> environment:</p>
			<p class="source-code">    def step(self, action):</p>
			<p class="source-code">        self.t += 1</p>
			<p class="source-code">        state, reward, done, info = self.wrapped.step(action)</p>
			<p class="source-code">        if self.reward_fun == "custom_reward":</p>
			<p class="source-code">            position, velocity = state</p>
			<p class="source-code">            reward += (abs(position+0.5)**2) * (position&gt;-0.5)</p>
			<p class="source-code">        obs = self._get_obs()</p>
			<p class="source-code">        if self.t &gt;= 200:</p>
			<p class="source-code">            done = True</p>
			<p class="source-code">        return obs, reward, done, info</p>
			<p>Of course, feel <a id="_idIndexMarker990"/>free to try your own reward shaping here to gain a better idea.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Applying a constant reward, such as -1, is an example of sparse reward, although the step reward is not 0. That is because the agent does not get any feedback until the very end of the episode; none of its actions change the default reward for a long time.</p>
			<p>We can enable the custom (shaped) reward strategy with the following flag: </p>
			<p class="source-code">STRATEGY = "custom_reward"</p>
			<p>The outcome we get is something like the following:</p>
			<p class="source-code"><strong class="bold">Average episode length: 131.33</strong></p>
			<p>This is obviously a significant gain, so kudos to our shaped reward function! Admittedly, though, it took me several iterations to figure out something that brings in significant improvement. This is because the behavior we encourage is more complex than just going right: we want the agent to go between left and right to speed up. This is a bit hard to capture in a reward function, and it could easily give you headaches.</p>
			<p>Reward function engineering in general can get quite tricky and time-consuming – so much so that<a id="_idIndexMarker991"/> this topic deserves a dedicated section to discuss, which we will turn to next. </p>
			<h2 id="_idParaDest-223"><a id="_idTextAnchor231"/>Challenges with engineering the reward function</h2>
			<p>The objective of <a id="_idIndexMarker992"/>RL is to find a policy that maximizes the expected cumulative reward the agent collects. We design and use very sophisticated algorithms to overcome this optimization challenge. In some problems, we use billions of training samples to this end and try to squeeze out a little extra reward. After all this hassle, it is not uncommon to observe that your agent obtains a great reward, but the behavior it exhibits is not exactly what you intended. In other words, the agent learns something different than what you want it to learn. If you run into such a situation, don't get too mad. That is because the agent's sole purpose is to maximize the reward you specified. If that reward does not exactly reflect the objective you had in mind, which is much more challenging than you may think, neither will the behavior of the agent. </p>
			<p class="callout-heading">Info</p>
			<p class="callout">A famous example of a misbehaving agent due to an incorrectly specified reward is OpenAI's CoastRunners agent. In the game, the agent is expected to finish the boat race as quickly as possible while collecting rewards along the way. After training, the agent figured out a<a id="_idIndexMarker993"/> way of collecting higher rewards without having to finish the race, defeating the original purpose. You can read more about it on OpenAI's blog: <a href="https://openai.com/blog/faulty-reward-functions/">https://openai.com/blog/faulty-reward-functions/</a>.</p>
			<p>As a result, it is of tremendous importance that you specify a good reward function for your task, especially when it includes qualitative and/or complex objectives. Unfortunately, designing a good reward function is more of an art than science, and you will gain intuition through practice and trial and error. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Alex Irpan, a machine learning researcher at Google, beautifully expresses how important and challenging designing the reward function is: "I've taken to imagining deep RL as a demon that's deliberately misinterpreting your reward and actively searching for the laziest possible local optima. It's a bit ridiculous, but I've found it's actually a productive mindset to have." (Irpan, 2018). François Chollet, the author of Keras, says "loss function engineering is probably going to be a job title in the future." (<a href="https://youtu.be/Bo8MY4JpiXE?t=5270">https://youtu.be/Bo8MY4JpiXE?t=5270</a>).</p>
			<p>Despite these challenges, the tricks we have just covered should give you a running start. The rest will come with experience.</p>
			<p>With that, let's conclude our discussion on reward function engineering. This was a long yet necessary <a id="_idIndexMarker994"/>one. In the next section, we will discuss another topic, curriculum learning, which is important not just in MT but in RL as a whole.</p>
			<h1 id="_idParaDest-224"><a id="_idTextAnchor232"/>Curriculum learning</h1>
			<p>When we learn a <a id="_idIndexMarker995"/>new skill, we start with the basics. Bouncing and dribbling are the first steps when learning basketball. Doing alley-oops is not something to try to teach in the first lesson. You need to gradually proceed to advanced lessons after getting comfortable with the earlier ones. This idea of following a curriculum, from the basics to advanced levels, is the basis of the whole education system. The question is whether machine learning models can benefit from the same approach. It turns out that they can!</p>
			<p>In the context of RL, when we create a curriculum, we similarly start with "easy" environment configurations for the agent. This way, the agent can get an idea about what success means early on, rather than spending a lot of time blindly exploring the environment with the hope of stumbling upon success. We then gradually increase the difficulty if we observe that the agent is exceeding a certain reward threshold. Each of these difficulty levels is considered a <strong class="bold">lesson</strong>. Curriculum learning has been shown to increase training efficiency and makes tasks that are infeasible to achieve feasible for the agent.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Designing lessons and transition criteria is a non-trivial undertaking. It requires significant thought and subject matter expertise. Although we follow manual curriculum design in this chapter, when we revisit the topic in <a href="B14160_11_Final_SK_ePub.xhtml#_idTextAnchor239"><em class="italic">Chapter 11</em></a>, <em class="italic">Generalization and Partial Observability</em>, and <a href="B14160_14_Final_SK_ePub.xhtml#_idTextAnchor306"><em class="italic">Chapter 14</em></a>, <em class="italic">Robot Learning</em>, we will discuss automatic curriculum generation methods.</p>
			<p>In our mountain car example, we create lessons by modifying the initial conditions of the environment. Normally, as the episode beginnings, the environment randomizes the car position around the valley dip (<img src="image/Formula_10_024.png" alt=""/>) and sets the velocity (<img src="image/Formula_08_050.png" alt=""/>) to 0. In our curriculum, the car will start in the first lesson close to the goal and with a high velocity toward the right. With that, it will easily reach the goal. We will make things gradually closer <a id="_idIndexMarker996"/>to the original difficulty as the curriculum progresses.</p>
			<p>Specifically, here is how we define the lessons:</p>
			<ul>
				<li><strong class="bold">Lesson 0</strong>: <img src="image/Formula_10_026.png" alt=""/>, <img src="image/Formula_10_027.png" alt=""/></li>
				<li><strong class="bold">Lesson 1</strong>: <img src="image/Formula_10_028.png" alt=""/>, <img src="image/Formula_10_029.png" alt=""/></li>
				<li><strong class="bold">Lesson 2</strong>: <img src="image/Formula_10_030.png" alt=""/>, <img src="image/Formula_10_031.png" alt=""/></li>
				<li><strong class="bold">Lesson 3</strong>: <img src="image/Formula_10_032.png" alt=""/>, <img src="image/Formula_10_033.png" alt=""/></li>
				<li><strong class="bold">Lesson 4 (final / original)</strong>: <img src="image/Formula_10_034.png" alt=""/>, <img src="image/Formula_10_035.png" alt=""/></li>
			</ul>
			<p>This is how it is set inside the environment:</p>
			<p class="source-code">    def _get_init_conditions(self):</p>
			<p class="source-code">        if self.lesson == 0:</p>
			<p class="source-code">            low = 0.1</p>
			<p class="source-code">            high = 0.4</p>
			<p class="source-code">            velocity = self.wrapped.np_random.uniform(</p>
			<p class="source-code">                low=0, high=self.wrapped.max_speed</p>
			<p class="source-code">            )</p>
			<p class="source-code">        ...</p>
			<p>We will let the <a id="_idIndexMarker997"/>agent proceed to the next lesson once it has been successful enough in the current one. We define this threshold as having an average episode length of less than 150 over 10 evaluation episodes. We set the lessons in training and the evaluation workers with the following functions:</p>
			<p class="source-code">CURRICULUM_TRANS = 150</p>
			<p class="source-code">...</p>
			<p class="source-code">def set_trainer_lesson(trainer, lesson):</p>
			<p class="source-code">    trainer.evaluation_workers.foreach_worker(</p>
			<p class="source-code">        lambda ev: ev.foreach_env(lambda env: env.set_lesson(lesson))</p>
			<p class="source-code">    )</p>
			<p class="source-code">    trainer.workers.foreach_worker(</p>
			<p class="source-code">        lambda ev: ev.foreach_env(lambda env: env.set_lesson(lesson))</p>
			<p class="source-code">    )</p>
			<p class="source-code">    ...</p>
			<p class="source-code">def increase_lesson(lesson):</p>
			<p class="source-code">    if lesson &lt; CURRICULUM_MAX_LESSON:</p>
			<p class="source-code">        lesson += 1</p>
			<p class="source-code">    return lesson    if "evaluation" in results:</p>
			<p class="source-code">        if results["evaluation"]["episode_len_mean"] &lt; CURRICULUM_TRANS:</p>
			<p class="source-code">            lesson = increase_lesson(lesson)</p>
			<p class="source-code">            set_trainer_lesson(trainer, lesson)</p>
			<p>These are then <a id="_idIndexMarker998"/>used inside the training flow:</p>
			<p class="source-code">            if results["evaluation"]["episode_len_mean"] &lt; CURRICULUM_TRANS:</p>
			<p class="source-code">                lesson = increase_lesson(lesson)</p>
			<p class="source-code">                set_trainer_lesson(trainer, lesson)</p>
			<p class="source-code">                print(f"Lesson: {lesson}")</p>
			<p>So, this is how we implement a manual curriculum. Say you train the agent with this:</p>
			<p class="source-code">STRATEGY = "curriculum"</p>
			<p>You will see that the performance we get is near optimal!</p>
			<p class="source-code"><strong class="bold">Average episode length: 104.66</strong></p>
			<p>You just taught a machine something using a curriculum! Pretty cool, isn't it? Now it should feel like MT!</p>
			<p>Next, we will look at another interesting approach: MT using demonstrations.</p>
			<h1 id="_idParaDest-225"><a id="_idTextAnchor233"/>Warm starts and demonstration learning</h1>
			<p>A popular<a id="_idIndexMarker999"/> technique to demonstrate to the agent a way to success is to train it on data that is coming from a reasonably successful controller, such as humans. In RLlib, this can be done by saving the human play data from the mountain car environment:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Chapter10/mcar_demo.py</p>
			<p class="source-code">        ...</p>
			<p class="source-code">        new_obs, r, done, info = env.step(a)</p>
			<p class="source-code">        # Build the batch</p>
			<p class="source-code">        batch_builder.add_values(</p>
			<p class="source-code">            t=t,</p>
			<p class="source-code">            eps_id=eps_id,</p>
			<p class="source-code">            agent_index=0,</p>
			<p class="source-code">            obs=prep.transform(obs),</p>
			<p class="source-code">            actions=a,</p>
			<p class="source-code">            action_prob=1.0,  # put the true action probability here</p>
			<p class="source-code">            action_logp=0,</p>
			<p class="source-code">            action_dist_inputs=None,</p>
			<p class="source-code">            rewards=r,</p>
			<p class="source-code">            prev_actions=prev_action,</p>
			<p class="source-code">            prev_rewards=prev_reward,</p>
			<p class="source-code">            dones=done,</p>
			<p class="source-code">            infos=info,</p>
			<p class="source-code">            new_obs=prep.transform(new_obs),</p>
			<p class="source-code">        )</p>
			<p class="source-code">        obs = new_obs</p>
			<p class="source-code">        prev_action = a</p>
			<p class="source-code">        prev_reward = r</p>
			<p>This data can then be fed to the training, which is implemented in <strong class="source-inline">Chapter10/mcar_train.py</strong>. When I tried it, RLlib got stuck with NaNs in multiple attempts when the training was seeded using this method. So, now that you know about it, we <a id="_idIndexMarker1000"/>will leave the details of this to RLlib's documentation at <a href="https://docs.ray.io/en/releases-1.0.1/rllib-offline.html">https://docs.ray.io/en/releases-1.0.1/rllib-offline.html</a> and not focus on it here.</p>
			<h1 id="_idParaDest-226"><a id="_idTextAnchor234"/>Action masking</h1>
			<p>One final MT <a id="_idIndexMarker1001"/>approach we will use is action masking. With that, we can prevent the agent from taking certain actions in certain steps based on conditions we define. For the mountain car, assume that we have this intuition of building momentum before trying to climb the hill. So, we want the agent to apply force to the left if the car is already moving left around the valley. So, for these conditions, we will mask all the actions except left:</p>
			<p class="source-code">    def update_avail_actions(self):</p>
			<p class="source-code">        self.action_mask = np.array([1.0] * self.action_space.n)</p>
			<p class="source-code">        pos, vel = self.wrapped.unwrapped.state</p>
			<p class="source-code">        # 0: left, 1: no action, 2: right</p>
			<p class="source-code">        if (pos &lt; -0.3) and (pos &gt; -0.8) and (vel &lt; 0) and (vel &gt; -0.05):</p>
			<p class="source-code">            self.action_mask[1] = 0</p>
			<p class="source-code">            self.action_mask[2] = 0</p>
			<p>In order to be able to use this masking, we need to build a custom model. For the masked actions, we push down all the logits to negative infinity:</p>
			<p class="source-code">class ParametricActionsModel(DistributionalQTFModel):</p>
			<p class="source-code">    def __init__(</p>
			<p class="source-code">        self,</p>
			<p class="source-code">        obs_space,</p>
			<p class="source-code">        action_space,</p>
			<p class="source-code">        num_outputs,</p>
			<p class="source-code">        model_config,</p>
			<p class="source-code">        name,</p>
			<p class="source-code">        true_obs_shape=(2,),</p>
			<p class="source-code">        **kw</p>
			<p class="source-code">    ):</p>
			<p class="source-code">        super(ParametricActionsModel, self).__init__(</p>
			<p class="source-code">            obs_space, action_space, num_outputs, model_config, name, **kw</p>
			<p class="source-code">        )</p>
			<p class="source-code">        self.action_value_model = FullyConnectedNetwork(</p>
			<p class="source-code">            Box(-1, 1, shape=true_obs_shape),</p>
			<p class="source-code">            action_space,</p>
			<p class="source-code">            num_outputs,</p>
			<p class="source-code">            model_config,</p>
			<p class="source-code">            name + "_action_values",</p>
			<p class="source-code">        )</p>
			<p class="source-code">        self.register_variables(self.action_value_model.variables())</p>
			<p class="source-code">    def forward(self, input_dict, state, seq_lens):</p>
			<p class="source-code">        action_mask = input_dict["obs"]["action_mask"]</p>
			<p class="source-code">        action_values, _ = self.action_value_model(</p>
			<p class="source-code">            {"obs": input_dict["obs"]["actual_obs"]}</p>
			<p class="source-code">        )</p>
			<p class="source-code">        inf_mask = tf.maximum(tf.math.log(action_mask), tf.float32.min)</p>
			<p class="source-code">        return action_values + inf_mask, state</p>
			<p>Finally, when using this model, we turn off dueling to avoid an overly complicated implementation. Also, we register our custom model:</p>
			<p class="source-code">    if strategy == "action_masking":</p>
			<p class="source-code">        config["hiddens"] = []</p>
			<p class="source-code">        config["dueling"] = False</p>
			<p class="source-code">        ModelCatalog.register_custom_model("pa_model", ParametricActionsModel)</p>
			<p class="source-code">        config["env_config"] = {"use_action_masking": True}</p>
			<p class="source-code">        config["model"] = {</p>
			<p class="source-code">            "custom_model": "pa_model",</p>
			<p class="source-code">        }</p>
			<p>In order to <a id="_idIndexMarker1002"/>train your agent with this strategy, set the following:</p>
			<p class="source-code">STRATEGY = "action_masking"</p>
			<p>The performance will be as follows:</p>
			<p class="source-code"><strong class="bold">Average episode length: 147.22</strong></p>
			<p>This is definitely an improvement over the default case, yet it is behind the reward shaping and curriculum learning approaches. Having smarter masking conditions and adding dueling networks can further help with the performance.</p>
			<p>This is the end of the MT techniques we use for the mountain car problem. Before we wrap up, let's check one more important topic in MT.</p>
			<h1 id="_idParaDest-227"><a id="_idTextAnchor235"/>Concept networks</h1>
			<p>An important part<a id="_idIndexMarker1003"/> of the MT approach is to divide the problem into concepts that correspond to different skills to facilitate learning. For example, for an autonomous car, training separate agents for cruising on a highway and passing a car could help with performance. In some problems, divisions between concepts are even more clear. In those cases, training a single agent for the entire problem will often result in better performance.</p>
			<p>Before closing this chapter, let's talk about some of the potential downsides of the MT approach.</p>
			<h1 id="_idParaDest-228"><a id="_idTextAnchor236"/>Downsides and the promises of MT</h1>
			<p>There are<a id="_idIndexMarker1004"/> two potential downsides of the<a id="_idIndexMarker1005"/> MT approach.</p>
			<p>First, it is usually non-trivial to come up with good reward shaping, a good curriculum, a set of action masking conditions, and so on. This also in some ways defeats the purposes of learning from experience and not having to do feature engineering. On the other hand, whenever we are able to do so, feature engineering and MT could be immensely helpful for the agent to learn and increase its data efficiency.</p>
			<p>Second, when we adopt a MT approach, it is possible to inject the bias of the teacher to the agent, which could prevent it from learning better strategies. The machine teacher needs to avoid such biases whenever possible.</p>
			<p>Awesome job! We have reached the end of an exciting chapter here. Let's summarize what we have covered next.</p>
			<h1 id="_idParaDest-229"><a id="_idTextAnchor237"/>Summary</h1>
			<p>In this chapter, we covered an emerging paradigm in artificial intelligence, MT, which is about effectively conveying the expertise of a subject matter expert (teacher) to machine learning algorithms. We discussed how this is similar to how humans are educated: by building on others' knowledge, usually without reinventing it. The advantage of this approach is that it greatly increases data efficiency in machine learning, and, in some cases, makes learning possible that would have been impossible without a teacher. We discussed various methods in this paradigm, including reward function engineering, curriculum learning, demonstration learning, action masking, and concept networks. We observed how some of these methods have improved vanilla use of Ape-X DQN significantly. At the end, we also introduced potential downsides of this paradigm, namely the difficulty of designing the teaching process and tools, and possible bias introduced into learning. Despite these downsides, MT will become a standard part of an RL scientist's toolbox in the near future.</p>
			<p>In the next chapter, we will discuss generalization and partial observability, a key topic in RL. In doing so, we will visit curriculum learning again and see how it helps in creating robust agents.</p>
			<p>See you on the other side!</p>
			<h1 id="_idParaDest-230"><a id="_idTextAnchor238"/>References</h1>
			<ul>
				<li>Bonsai. (2017). <em class="italic">Deep Reinforcement Learning Models: Tips &amp; Tricks for Writing Reward Functions</em>. Medium. URL: <a href="https://bit.ly/33eTjBv">https://bit.ly/33eTjBv</a></li>
				<li>Weng, L. (2020). <em class="italic">Curriculum for Reinforcement Learning</em>. Lil'Log. URL: <a href="https://bit.ly/39foJvE">https://bit.ly/39foJvE</a></li>
				<li>OpenAI. (2016). <em class="italic">Faulty Reward Functions in the Wild</em>. OpenAI blog. URL: <a href="https://openai.com/blog/faulty-reward-functions/">https://openai.com/blog/faulty-reward-functions/</a> </li>
				<li>Irpan, A. (2018). <em class="italic">Deep Reinforcement Learning Doesn't Work Yet</em>. Sorta Insightful. URL: <a href="https://www.alexirpan.com/2018/02/14/rl-hard.html">https://www.alexirpan.com/2018/02/14/rl-hard.html</a> </li>
				<li>Heess, N. et al. (2017). <em class="italic">Emergence of Locomotion Behaviours in Rich Environments</em>. arXiv.org. URL: <a href="http://arxiv.org/abs/1707.02286">http://arxiv.org/abs/1707.02286</a></li>
				<li>Bonsai. (2017). <em class="italic">Writing Great Reward Functions</em> – Bonsai. YouTube. URL: <a href="https://youtu.be/0R3PnJEisqk">https://youtu.be/0R3PnJEisqk</a> </li>
				<li>Badnava, B. &amp; Mozayani, N. (2019). <em class="italic">A New Potential-Based Reward Shaping for Reinforcement Learning Agent</em>. arXiv.org. URL: <a href="http://arxiv.org/abs/1902.06239">http://arxiv.org/abs/1902.06239</a></li>
				<li>Microsoft Research. (2019). <em class="italic">Reward Machines: Structuring Reward Function Specifications and Reducing Sample Complexity</em>. YouTube. URL: <a href="https://youtu.be/0wYeJAAnGl8">https://youtu.be/0wYeJAAnGl8</a></li>
				<li>US Government Finances. (2020). URL: <a href="https://www.usgovernmentspending.com/">https://www.usgovernmentspending.com/</a></li>
				<li>The AlphaStar team. (2019). <em class="italic">AlphaStar: Mastering the Real-Time Strategy Game StarCraft II</em>. DeepMind blog. URL: <a href="https://bit.ly/39fpDIy">https://bit.ly/39fpDIy</a> </li>
			</ul>
		</div>
	</body></html>