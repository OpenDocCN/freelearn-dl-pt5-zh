<html><head></head><body>
		<div id="_idContainer106">
			<h1 id="_idParaDest-171" class="chapter-number"><a id="_idTextAnchor193"/>11</h1>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor194"/>Machine Learning Part 3 – Transformers and Large Language Models</h1>
			<p>In this chapter, we will cover the currently best-performing techniques in <strong class="bold">natural language processing</strong> (<strong class="bold">NLP</strong>) – <strong class="bold">transformers</strong> and <strong class="bold">pretrained models</strong>. We will discuss the concepts behind transformers and include examples of using transformers and <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) for text classification. The code for this chapter will be based on the TensorFlow/Keras Python libraries and the cloud services provided <span class="No-Break">by OpenAI.</span></p>
			<p>The topics covered in this chapter are important because although transformers and LLMs are only a few years old, they have become state-of-the-art for many different types of NLP applications. In fact, LLM systems such as ChatGPT have been widely covered in the press and you have undoubtedly encountered references to them. You have probably even used their online interfaces. In this chapter, you will learn how to work with the technology behind these systems, which should be part of the toolkit of every <span class="No-Break">NLP developer.</span></p>
			<p>In this chapter, we will cover the <span class="No-Break">following topics:</span></p>
			<ul>
				<li>Overview of transformers and large <span class="No-Break">language models</span></li>
				<li><strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>) and <span class="No-Break">its variants</span></li>
				<li>Using BERT – a <span class="No-Break">classification example</span></li>
				<li><span class="No-Break">Cloud-based LLMs</span></li>
			</ul>
			<p>We’ll start by listing the technical resources that we’ll use to run the examples in <span class="No-Break">this chapter.</span></p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor195"/>Technical requirements</h1>
			<p>The code that we will go over in this chapter makes use of a number of open source software libraries and resources. We have used many of these in earlier chapters, but we will list them here <span class="No-Break">for convenience:</span></p>
			<ul>
				<li>The Tensorflow machine learning libraries: <strong class="source-inline">hub</strong>, <strong class="source-inline">text</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">tf-models</strong></span></li>
				<li>The Python numerical <span class="No-Break">package, NumPy</span></li>
				<li>The Matplotlib plotting and <span class="No-Break">graphical package</span></li>
				<li>The IMDb movie <span class="No-Break">reviews dataset</span></li>
				<li>scikit-learn’s <strong class="source-inline">sklearn.model_selection</strong> to do the training, validation, and <span class="No-Break">test split</span></li>
				<li>A BERT model from TensorFlow Hub: we’re using this one –<strong class="source-inline">'small_bert/bert_en_uncased_L-4_H-512_A-8'</strong> – but you can use any other BERT model you like, bearing in mind that larger models might take a long time <span class="No-Break">to train</span></li>
			</ul>
			<p>Note that we have kept the models relatively small here so that they don’t require an especially powerful computer. The examples in this chapter were tested on a Windows 10 machine with an Intel 3.4 GHz CPU and 16 GB of RAM, without a separate GPU. Of course, more computing resources will speed up your training runs and enable you to use <span class="No-Break">larger models.</span></p>
			<p>The next section provides a brief description of the transformer and LLM technology that we’ll <span class="No-Break">be using.</span></p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor196"/>Overview of transformers and LLMs</h1>
			<p>Transformers and LLMs are <a id="_idIndexMarker844"/>currently the best-performing technologies <a id="_idIndexMarker845"/>for <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>). This does not mean that the approaches covered in earlier chapters are obsolete. Depending <a id="_idIndexMarker846"/>on the requirements of a specific NLP project, some of the simpler approaches may be more practical or cost-effective. In this chapter, you will get information about the more recent approaches that you can use to make <span class="No-Break">that decision.</span></p>
			<p>There is a great deal of information about the theoretical aspects of these techniques available on the internet, but here we will focus on applications and explore how these technologies can be applied to solving practical <span class="No-Break">NLU problems.</span></p>
			<p>As we saw in <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>) have been a very effective approach in NLP because they don’t assume that <a id="_idIndexMarker847"/>the elements of input, specifically words, are independent, and so are able to take into account sequences of input elements such as the order of words in sentences. As we have seen, RNNs keep the memory of earlier inputs by using previous outputs as inputs to later layers. However, with RNNs, the effect of earlier inputs on the current input diminishes quickly as processing proceeds through <span class="No-Break">the sequence.</span></p>
			<p>When longer documents are processed, because of the context-dependent nature of natural language, even <a id="_idIndexMarker848"/>very distant parts of the text can have a strong effect <a id="_idIndexMarker849"/>on the current input. In fact, in some cases, distant inputs can be more important than more recent parts of the input. But when the data is a long sequence, processing with an RNN means that the earlier information will not be able to have much impact on the later processing. Some initial attempts to <a id="_idIndexMarker850"/>address this issue include <strong class="bold">long short-term memory</strong> (<strong class="bold">LSTM</strong>), which allows the processor to maintain the state and includes forget <a id="_idIndexMarker851"/>gates, and <strong class="bold">gated recurrent units</strong> (<strong class="bold">GRUs</strong>), a new and relatively fast type of LSTM, which we will not cover in this book. Instead, we will focus on more recent approaches such as attention <span class="No-Break">and transformers.</span></p>
			<h2 id="_idParaDest-175"><a id="_idTextAnchor197"/>Introducing attention</h2>
			<p><strong class="bold">Attention</strong> is a technique that <a id="_idIndexMarker852"/>allows a network to learn where to pay attention to <span class="No-Break">the input.</span></p>
			<p>Initially, attention was used primarily in machine translation. The processing was based on an encoder-decoder architecture where a sentence was first encoded into a vector and then decoded into the translation. In the original encoder-decoder idea, each input sentence was encoded into a fixed-length vector. It turned out that it was difficult to encode all of the information in a sentence into a fixed-length vector, especially a long sentence. This is because more distant words that were outside of the scope of the fixed-length vector were not able to influence <span class="No-Break">the result.</span></p>
			<p>Encoding the sentence into a <em class="italic">set</em> of vectors, one per word, removed <span class="No-Break">this limitation.</span></p>
			<p>As one of the early papers on attention states, “<em class="italic">The most important distinguishing feature of this approach from the basic encoder-decoder is that it does not attempt to encode a whole input sentence into a single fixed-length vector. Instead, it encodes the input sentence into a sequence of vectors and chooses a subset of these vectors adaptively while decoding the translation. This frees a neural translation model from having to squash all the information of a source sentence, regardless of its length, into a fixed-length vector.</em>” (Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). <em class="italic">Neural machine translation by jointly learning to align and translate</em>. arXiv <span class="No-Break">preprint arXiv:1409.0473.)</span></p>
			<p>For machine translation applications, it is necessary both to encode the input text and to decode the results into the new language in order to produce the translated text. In this chapter, we will <a id="_idIndexMarker853"/>simplify this task by using a classification example that uses just the encoding part of the <span class="No-Break">attention architecture.</span></p>
			<p>A more recent technical development has been to demonstrate that one component of the attention <a id="_idIndexMarker854"/>architecture, RNNs, was not needed in order to get good results. This new development is called <strong class="bold">transformers</strong>, which we will briefly mention in the next section, and then illustrate with an <span class="No-Break">in-depth example.</span></p>
			<h2 id="_idParaDest-176"><a id="_idTextAnchor198"/>Applying attention in transformers</h2>
			<p>Transformers <a id="_idIndexMarker855"/>are a development of the attention <a id="_idIndexMarker856"/>approach that dispenses with the RNN part of the original attention systems. Transformers were introduced in the 2017 paper <em class="italic">Attention is all you need</em> (Ashish Vaswani, et al., 2017. <em class="italic">Attention is all you need</em>. In the Proceedings of the 31st International Conference on Neural Information Processing Systems (NIPS’17). Curran Associates Inc., Red Hook, NY, USA, 6000-6010). The paper showed that good results can be achieved just with attention. Nearly all research on NLP learning models is now based <span class="No-Break">on transformers.</span></p>
			<p>A second important technical component of the recent dramatic increases in NLP performance is the idea of pretraining models based on large amounts of existing data and making them available to NLP developers. The next section talks about the advantages of <span class="No-Break">this approach.</span></p>
			<h2 id="_idParaDest-177"><a id="_idTextAnchor199"/>Leveraging existing data – LLMs or pre-trained models</h2>
			<p>So far, in this book, we’ve created our own text representations (vectors) from training data. In our examples so far, all of the information that the model has about the language is contained in the training data, which is a very small sample of the full language. But if models start out with general knowledge of a language, they can take advantage of vast amounts of training data that would be impractical for a single project. This is called <a id="_idIndexMarker857"/>the <strong class="bold">pretraining</strong> of a model. These pretrained models can be reused for many <a id="_idIndexMarker858"/>projects because they capture general information about a language. Once a pretrained model is available, it can be fine-tuned to specific applications by supplying <span class="No-Break">additional data</span></p>
			<p>The next section will introduce one of the best-known and most important pretrained transformer <span class="No-Break">models, BERT.</span></p>
			<h1 id="_idParaDest-178"><a id="_idTextAnchor200"/>BERT and its variants</h1>
			<p>As an example of an LLM technology based on transformers, we will demonstrate the use of BERT, a widely <a id="_idIndexMarker859"/>used state-of-the-art system. BERT is an open source NLP approach developed by Google that is the foundation of today’s state-of-the-art NLP systems. The source code for BERT is available <span class="No-Break">at </span><a href="https://github.com/google-research/bert"><span class="No-Break">https://github.com/google-research/bert</span></a><span class="No-Break">.</span></p>
			<p>BERT’s key technical innovation is that the training is bidirectional, that is, taking both previous and later words in input into account. A second innovation is that BERT’s pretraining uses a masked language model, where the system masks out a word in the training data and attempts to <span class="No-Break">predict it.</span></p>
			<p>BERT also uses only the encoder part of the encoder-decoder architecture because, unlike machine translation systems, it focuses only on understanding; it doesn’t <span class="No-Break">produce language.</span></p>
			<p>Another advantage of BERT, unlike the systems we’ve discussed earlier in this book, is that the training <a id="_idIndexMarker860"/>process is unsupervised. That is, the text that it is trained on does not need to be annotated or assigned any meaning by a human. Because it is unsupervised, the training process can take advantage of the enormous quantities of text available on the web, without needing to go through the expensive process of having humans review it and decide what <span class="No-Break">it means.</span></p>
			<p>The initial BERT system was published in 2018. Since then, the ideas behind BERT have been explored and expanded into many different variants. The different variants have various features that <a id="_idIndexMarker861"/>make them appropriate for addressing different requirements. Some of these features include faster training times, smaller models, or higher accuracy. <em class="italic">Table 11.1</em> shows a few of the common BERT variants and their specific features. Our example will use the original BERT system since it is the basis of all the other <span class="No-Break">BERT versions:</span></p>
			<table id="table001-5" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Acronym</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Name</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Date</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Features</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">BERT</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Bidirectional Encoder Representations <span class="No-Break">from Transformer</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">2018</span></p>
						</td>
						<td class="No-Table-Style">
							<p>The original <span class="No-Break">BERT system.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">BERT-Base</span></p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p>A number of models released by the original <span class="No-Break">BERT authors.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">RoBERTa</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Robustly Optimized BERT <span class="No-Break">pre-training approach</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">2019</span></p>
						</td>
						<td class="No-Table-Style">
							<p>In this approach, different parts of the sentences are masked in different epochs, which makes it more robust to variations in the <span class="No-Break">training data.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">ALBERT</span></p>
						</td>
						<td class="No-Table-Style">
							<p>A <span class="No-Break">Lite BERT</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">2019</span></p>
						</td>
						<td class="No-Table-Style">
							<p>A version of BERT that shares parameters between layers in order to reduce the size <span class="No-Break">of models.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">DistilBERT</span></p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break">2020</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Smaller and faster than BERT with <span class="No-Break">good performance</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">TinyBERT</span></p>
						</td>
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><span class="No-Break">2019</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Smaller and faster than BERT-Base with good performance; good for <span class="No-Break">resource-restricted devices.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 11.1 – BERT variations</p>
			<p>The next section will go through a hands-on example of a <span class="No-Break">BERT application.</span></p>
			<h1 id="_idParaDest-179"><a id="_idTextAnchor201"/>Using BERT – a classification example</h1>
			<p>In this example, we’ll use BERT for classification, using the movie review dataset we saw in earlier chapters. We <a id="_idIndexMarker862"/>will start with a pretrained BERT model and <em class="italic">fine-tune</em> it to classify movie reviews. This is a process that you can follow if you want to apply BERT to your <span class="No-Break">own data.</span></p>
			<p>Using BERT for specific applications starts with one of the pretrained models available from TensorFlow Hub (<a href="https://tfhub.dev/tensorflow">https://tfhub.dev/tensorflow</a>) and then fine-tuning it with training data that is <a id="_idIndexMarker863"/>specific to the application. It is recommended to start with one of the small BERT models, which have the same architecture as BERT but are faster to train. Generally, the smaller models are less accurate, but if their accuracy is adequate for the application, it isn’t necessary to take the extra time and computer resources that would be needed to use a larger model. There are many models of various sizes that can be downloaded from <span class="No-Break">TensorFlow Hub.</span></p>
			<p>BERT models can also be cased or uncased, depending on whether they take the case of text into account. Uncased models will typically provide better results unless the application is one where <a id="_idIndexMarker864"/>the case of the text is informative, such as <strong class="bold">named entity recognition</strong> (<strong class="bold">NER</strong>), where proper names <span class="No-Break">are important.</span></p>
			<p>In this example, we will work with the <strong class="source-inline">small_bert/bert_en_uncased_L-4_H-512_A-8/1</strong> model. It has the following properties, which are encoded in <span class="No-Break">its name:</span></p>
			<ul>
				<li><span class="No-Break">Small BERT</span></li>
				<li><span class="No-Break">Uncased</span></li>
				<li>4 hidden <span class="No-Break">layers (L-4)</span></li>
				<li>A hidden size <span class="No-Break">of 512</span></li>
				<li>8 attention <span class="No-Break">heads (A-8)</span></li>
			</ul>
			<p>This model was trained on Wikipedia and BooksCorpus. This is a very large amount of text, but there are many pretrained models that were trained on much larger amounts of text, which we will discuss later in the chapter. Indeed, an important trend in NLP is developing and publishing models trained on larger and larger amounts <span class="No-Break">of text.</span></p>
			<p>The example that will be <a id="_idIndexMarker865"/>reviewed here is adapted from the TensorFlow tutorial for text classification with BERT. The full tutorial can be <span class="No-Break">found here:</span></p>
			<p><a href="https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=EqL7ihkN_862&#13;"><span class="No-Break">https://colab.research.google.com/github/tensorflow/text/blob/master/docs/tutorials/classify_text_with_bert.ipynb#scrollTo=EqL7ihkN_862</span></a></p>
			<p>We’ll start by installing and loading some basic libraries. We will be using a Jupyter notebook (you will recall that the process of setting up a Jupyter notebook was covered in detail in <a href="B19005_04.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic">Chapter 4</em></span></a>, and you can refer to <a href="B19005_04.xhtml#_idTextAnchor085"><span class="No-Break"><em class="italic">Chapter 4</em></span></a> for additional details <span class="No-Break">if necessary):</span></p>
			<pre class="source-code">
!pip install -q -U "tensorflow-text==2.8.*"
!pip install -q tf-models-official==2.7.0
!pip install numpy==1.21
import os
import shutil
import tensorflow as tf
import tensorflow_hub as hub
import tensorflow_text as text
from official.nlp import optimization  # to create AdamW optimizer
import matplotlib.pyplot as plt #for plotting results
tf.get_logger().setLevel('ERROR')</pre>
			<p>Our BERT fine-tuned model will be developed through the <span class="No-Break">following steps:</span></p>
			<ol>
				<li><span class="No-Break">Installing data.</span></li>
				<li>Splitting the data into training, validation, and <span class="No-Break">testing subsets.</span></li>
				<li>Loading a BERT model from <span class="No-Break">TensorFlow Hub.</span></li>
				<li>Building a model by combining BERT with <span class="No-Break">a classifier.</span></li>
				<li>Fine-tuning BERT to create <span class="No-Break">a model.</span></li>
				<li>Defining the loss function <span class="No-Break">and metrics.</span></li>
				<li>Defining the optimizer and number of <span class="No-Break">training epochs.</span></li>
				<li>Compiling <span class="No-Break">the model.</span></li>
				<li>Training <span class="No-Break">the model.</span></li>
				<li>Plotting the results of the training steps over the <span class="No-Break">training epochs.</span></li>
				<li>Evaluating the model with the <span class="No-Break">test data.</span></li>
				<li>Saving the model and using it to <span class="No-Break">classify texts.</span></li>
			</ol>
			<p>The following sections will go over each of these steps <span class="No-Break">in detail.</span></p>
			<h2 id="_idParaDest-180"><a id="_idTextAnchor202"/>Installing the data</h2>
			<p>The first step <a id="_idIndexMarker866"/>is to install the data. We will use the NLTK movie review dataset that we installed in <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>. We will use the <strong class="source-inline">tf.keras.utils.text_dataset_from_directory</strong> utility to make a TensorFlow dataset from the movie <span class="No-Break">review directory:</span></p>
			<pre class="source-code">
batch_size = 32
import matplotlib.pyplot as plt
tf.get_logger().setLevel('ERROR')
AUTOTUNE = tf.data.AUTOTUNE
raw_ds = tf.keras.utils.text_dataset_from_directory(
    './movie_reviews',
class_names = raw_ds.class_names
print(class_names)</pre>
			<p>There are 2,000 files in the dataset, divided into two classes, <strong class="source-inline">neg</strong> and <strong class="source-inline">pos</strong>. We print the class names in the final step as a check to make sure the class names are as expected. These steps can <a id="_idIndexMarker867"/>be used for any dataset that is contained in a directory structure with examples of the different classes contained in different directories with the class names as <span class="No-Break">directory names.</span></p>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor203"/>Splitting the data into training, validation, and testing sets</h2>
			<p>The next step is to split the dataset into training, validation, and testing sets. As you will recall from <a id="_idIndexMarker868"/>earlier chapters, the training set is <a id="_idIndexMarker869"/>used to develop the model. The validation <a id="_idIndexMarker870"/>set, which is kept separate from the training set, is used to look at the performance of the system on data that it hasn’t been trained on during the training process. In our example, we will use a common split of 80% training 10% for validation, and 10% for testing. The validation set can be used at the end of every training epoch, to see how training is progressing. The testing set is only used once, as a <span class="No-Break">final evaluation:</span></p>
			<pre class="source-code">
from sklearn.model_selection import train_test_split
def partition_dataset_tf(dataset, ds_size, train_split=0.8, val_split=0.1, test_split=0.1, shuffle=True, shuffle_size=1000):
    assert (train_split + test_split + val_split) == 1
    if shuffle:
        # Specify seed maintain the same split distribution between runs for reproducibilty
        dataset = dataset.shuffle(shuffle_size, seed=42)
    train_size = int(train_split * ds_size)
    val_size = int(val_split * ds_size)
    train_ds = dataset.take(train_size)
    val_ds = dataset.skip(train_size).take(val_size)
    test_ds = dataset.skip(train_size).skip(val_size)
    return train_ds, val_ds, test_ds
train_ds,val_ds,test_ds = partition_dataset_tf(
    raw_ds,len(raw_ds))</pre>
			<h2 id="_idParaDest-182"><a id="_idTextAnchor204"/>Loading the BERT model</h2>
			<p>The next step is to load the BERT model we will fine-tune in this example, as shown in the following code <a id="_idIndexMarker871"/>block. As discussed previously, there are many BERT models to select from, but this model is a good choice to <span class="No-Break">start with.</span></p>
			<p>We will also need to provide a preprocessor to transform the text inputs into numeric token IDs before their input to BERT. We can use the matching preprocessor provided by TensorFlow for <span class="No-Break">this model:</span></p>
			<pre class="source-code">
bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8'
map_name_to_handle = {
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',
}
map_model_to_preprocess = {
    'small_bert/bert_en_uncased_L-4_H-512_A-8':
        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',
}
tfhub_handle_encoder = map_name_to_handle[bert_model_name]
tfhub_handle_preprocess = map_model_to_preprocess[
    bert_model_name]
bert_preprocess_model = hub.KerasLayer(
    tfhub_handle_preprocess)</pre>
			<p>The code here <a id="_idIndexMarker872"/>specifies the model we’ll use and defines some convenience variables to simplify reference to the model, the encoder, and <span class="No-Break">the preprocessor.</span></p>
			<h2 id="_idParaDest-183"><a id="_idTextAnchor205"/>Defining the model for fine-tuning</h2>
			<p>The following <a id="_idIndexMarker873"/>code defines the model we will use. We can increase the size of the parameter to the <strong class="source-inline">Dropout</strong> layer if desired to make the model robust to variations in the <span class="No-Break">training data:</span></p>
			<pre class="source-code">
def build_classifier_model():
    text_input = tf.keras.layers.Input(shape=(),
        dtype=tf.string, name='text')
    preprocessing_layer = hub.KerasLayer(
        tfhub_handle_preprocess, name='preprocessing')
    encoder_inputs = preprocessing_layer(text_input)
    encoder = hub.KerasLayer(tfhub_handle_encoder,
        trainable = True, name='BERT_encoder')
    outputs = encoder(encoder_inputs)
    net = outputs['pooled_output']
    net = tf.keras.layers.Dropout(0.1)(net)
    net = tf.keras.layers.Dense(1, activation=None,
        name='classifier')(net)
    return tf.keras.Model(text_input, net)
# plot the model's structure as a check
tf.keras.utils.plot_model(classifier_model)</pre>
			<p>In <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.1</em>, we can see a visualization of the model’s layers, including the text input layer, the preprocessing layer, the BERT layer, the dropout layer, and the final classifier layer. The visualization was produced by the last line in the code block. This structure corresponds to the structure we defined in the <span class="No-Break">preceding code:</span></p>
			<div>
				<div id="_idContainer103" class="IMG---Figure">
					<img src="image/B19005_11_01.jpg" alt="Figure 11.1 – Visualizing the model structure"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – Visualizing the model structure</p>
			<p>Sanity checks such <a id="_idIndexMarker874"/>as this visualization are useful because, with larger datasets and models, the training process can be very lengthy, and if the structure of the model is not what was intended, a lot of time can be wasted in trying to train an <span class="No-Break">incorrect model.</span></p>
			<h2 id="_idParaDest-184"><a id="_idTextAnchor206"/>Defining the loss function and metrics</h2>
			<p>We will use a <a id="_idIndexMarker875"/>cross-entropy function for the loss function. <strong class="bold">Cross-entropy</strong> estimates <a id="_idIndexMarker876"/>the loss by scoring the average difference <a id="_idIndexMarker877"/>between the actual and predicted probability distributions for all classes. Since this is a binary classification problem (that is, there are only two outcomes, <em class="italic">positive</em> and <em class="italic">negative</em>), we’ll use the <strong class="source-inline">losses.BinaryCrossEntropy</strong> <span class="No-Break">loss function:</span></p>
			<pre class="source-code">
loss = tf.keras.losses.BinaryCrossentropy(from_logits=True)
metrics = tf.metrics.BinaryAccuracy()</pre>
			<p>A classification application with several possible outcomes, such as an intent identification <a id="_idIndexMarker878"/>problem where we have to decide which of 10 intents to <a id="_idIndexMarker879"/>assign to an input, would use categorical cross-entropy. Similarly, since this is a binary classification problem, the metric should be <strong class="source-inline">binary accuracy</strong>, rather than simply <strong class="source-inline">accuracy</strong>, which would be appropriate for a multi-class <span class="No-Break">classification problem.</span></p>
			<h2 id="_idParaDest-185"><a id="_idTextAnchor207"/>Defining the optimizer and the number of epochs</h2>
			<p>The optimizer improves <a id="_idIndexMarker880"/>the efficiency of the learning <a id="_idIndexMarker881"/>process. We’re using the popular <strong class="source-inline">Adam</strong> optimizer here, and starting it off with a very small learning rate (<strong class="source-inline">3e-5</strong>), which is recommended for BERT. The optimizer will dynamically adjust the learning rate <span class="No-Break">during training:</span></p>
			<pre class="source-code">
epochs = 15
steps_per_epoch = tf.data.experimental.cardinality(
    train_ds).numpy()
print(steps_per_epoch)
num_train_steps = steps_per_epoch * epochs
# a linear warmup phase over the first 10%
num_warmup_steps = int(0.1*num_train_steps)
init_lr = 3e-5
optimizer = optimization.create_optimizer(
        init_lr=init_lr, num_train_steps = num_train_steps,
        num_warmup_steps=num_warmup_steps,
        optimizer_type='adamw')</pre>
			<p>Note that we have selected 15 epochs of training. For the first training run, we’ll try to balance the goals of training on enough epochs to get an accurate model and wasting time training on <a id="_idIndexMarker882"/>more epochs than needed. Once we get our results <a id="_idIndexMarker883"/>from the first training run, we can adjust the number of epochs to balance <span class="No-Break">these goals.</span></p>
			<h2 id="_idParaDest-186"><a id="_idTextAnchor208"/>Compiling the model</h2>
			<p>Using the classifier model in the call to <strong class="source-inline">def build_classifier_model()</strong>, we can compile the <a id="_idIndexMarker884"/>model with the loss, metrics, and optimizer, and take a look at the summary. It’s a good idea to check the model before starting a lengthy training process to make sure the model looks <span class="No-Break">as expected:</span></p>
			<pre class="source-code">
classifier_model.compile(optimizer=optimizer,
                         loss=loss,
                         metrics=metrics)
classifier_model.summary()</pre>
			<p>The summary of the model will look something like the following (we will only show a few lines because it is <span class="No-Break">fairly long):</span></p>
			<pre class="source-code">
Model: model
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to
==================================================================================================
 text (InputLayer)              [(None,)]            0           []
 preprocessing (KerasLayer)     {'input_mask': (Non  0           ['text[0][0]']
                                e, 128),
                                 'input_type_ids':
                                (None, 128),
                                 'input_word_ids':
                                (None, 128)}</pre>
			<p>The output here <a id="_idIndexMarker885"/>just summarizes the first two layers – input <span class="No-Break">and preprocessing.</span></p>
			<p>The next step is training <span class="No-Break">the model.</span></p>
			<h2 id="_idParaDest-187"><a id="_idTextAnchor209"/>Training the model</h2>
			<p>In the following <a id="_idIndexMarker886"/>code, we start the training process with a call to <strong class="source-inline">classifier_model.fit(_)</strong>. We supply this method with parameters for the training data, the validation data, the verbosity level, and the number of epochs (which we set earlier), as shown in <span class="No-Break">this code:</span></p>
			<pre class="source-code">
print(f'Training model with {tfhub_handle_encoder}')
history = classifier_model.fit(x=train_ds,
                               validation_data=val_ds,
                               verbose = 2,
                               epochs=epochs)
Training model with https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1
Epoch 1/15
50/50 - 189s - loss: 0.7015 - binary_accuracy: 0.5429 - val_loss: 0.6651 - val_binary_accuracy: 0.5365 - 189s/epoch - 4s/step</pre>
			<p>Note that the <strong class="source-inline">classifier_model.fit()</strong> method returns a <strong class="source-inline">history</strong> object, which will include information about the progress of the complete training process. We will use the <strong class="source-inline">history</strong> object to create plots of the training process. These will provide quite a bit of insight into what happened during training, and we will use this information to guide our next steps. We will see these plots in the <span class="No-Break">next section.</span></p>
			<p>Training times for transformer models can be quite lengthy. The time taken depends on the size of the dataset, the number of epochs, and the size of the model, but this example should probably not take more than an hour to train on a modern CPU. If running this example takes significantly longer than that, you may want to try testing with a higher verbosity level (2 is the maximum) so that you can get more information about what is going on in the <span class="No-Break">training process.</span></p>
			<p>At the end of this <a id="_idIndexMarker887"/>code block, we also see the results of processing the first epoch of training. We can see that the first epoch of training took <strong class="source-inline">189</strong> seconds. The loss was <strong class="source-inline">0.7</strong> and the accuracy was <strong class="source-inline">0.54</strong>. The loss and accuracy after one epoch of training are not very good, but they will improve dramatically as training proceeds. In the next section, we will see how to show the training <span class="No-Break">progress graphically.</span></p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor210"/>Plotting the training process</h2>
			<p>After training is <a id="_idIndexMarker888"/>complete, we will want to see how the system’s performance changes over training epochs. We can see this with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
import matplotlib.pyplot as plt
!matplotlib inline
history_dict = history.history
print(history_dict.keys())
acc = history_dict['binary_accuracy']
val_acc = history_dict['val_binary_accuracy']
loss = history_dict['loss']
val_loss = history_dict['val_loss']
epochs = range(1, len(acc) + 1)</pre>
			<p>The preceding code defines some variables and gets the relevant metrics (<strong class="source-inline">binary_accuracy</strong> and <strong class="source-inline">loss</strong> for the training and validation data) from the model’s <strong class="source-inline">history</strong> object. We <a id="_idIndexMarker889"/>are now ready to plot the progress of the training process. As usual, we will use Matplotlib to create <span class="No-Break">our plots:</span></p>
			<pre class="source-code">
fig = plt.figure(figsize=(10, 6))
fig.tight_layout()
plt.subplot(2, 1, 1)
# r is for "solid red line"
plt.plot(epochs, loss, 'r', label='Training loss')
# b is for "solid blue line"
plt.plot(epochs, val_loss, 'b', label='Validation loss')
plt.title('Training and validation loss')
# plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.subplot(2, 1, 2)
plt.plot(epochs, acc, 'r', label='Training acc')
plt.plot(epochs, val_acc, 'b', label='Validation acc')
plt.title('Training and validation accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend(loc='lower right')
plt.show()
dict_keys(['loss', 'binary_accuracy', 'val_loss',
    'val_binary_accuracy'])</pre>
			<p>In <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.2</em>, we see the plot of the decreasing loss and increasing accuracy over time as the model <a id="_idIndexMarker890"/>is trained. The dashed lines represent the training loss and accuracy, and the solid lines represent the validation loss <span class="No-Break">and accuracy:</span></p>
			<div>
				<div id="_idContainer104" class="IMG---Figure">
					<img src="image/B19005_11_02.jpg" alt="Figure 11.2 – Accuracy and loss during the training process"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – Accuracy and loss during the training process</p>
			<p>It is most typical for the validation accuracy to be less than the training accuracy, and for the validation loss to be greater than the training loss, but this will not necessarily be the case, depending on how the data is split between validation and training subsets. In this example, the validation loss is uniformly lower than the training loss and the validation accuracy is uniformly higher than the training accuracy. We can see from this plot that the system isn’t changing after the first fourteen epochs. In fact, its performance is <span class="No-Break">almost perfect.</span></p>
			<p>Consequently, it is clear that there isn’t any reason to train the system after this point. In comparison, look at the plots around epoch <strong class="source-inline">4</strong>. We can see that it would not be a good idea to stop training after four epochs because loss is still decreasing and accuracy is still increasing. Another interesting observation that we can see in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.2</em> around epoch <strong class="source-inline">7</strong> is that the <a id="_idIndexMarker891"/>accuracy seems to decrease a bit. If we had stopped training at epoch <strong class="source-inline">7</strong>, we couldn’t tell that accuracy would start to increase again at epoch <strong class="source-inline">8</strong>. For that reason, it’s a good idea to keep training until we either see the metrics level off or start to get <span class="No-Break">consistently worse.</span></p>
			<p>Now we have a trained model, and we’d like to see how it performs on previously unseen data. This unseen data is the test data that we set aside during the training, validation, and <span class="No-Break">testing split.</span></p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor211"/>Evaluating the model on the test data</h2>
			<p>After the training is complete, we can see how the model performs on the test data. This can be <a id="_idIndexMarker892"/>seen in the following output, where we can see that the system is doing very well. The accuracy is nearly 100% and the loss is <span class="No-Break">near zero:</span></p>
			<pre class="source-code">
loss, accuracy = classifier_model.evaluate(test_ds)
print(f'Loss: {loss}')
print(f'Binary Accuracy: {accuracy}')
1/7 [===&gt;..........................] - ETA: 9s - loss: 0.0239 - binary_accuracy: 0.9688
2/7 [=======&gt;......................] - ETA: 5s - loss: 0.0189 - binary_accuracy: 0.9844
3/7 [===========&gt;..................] - ETA: 4s - loss: 0.0163 - binary_accuracy: 0.9896
4/7 [================&gt;.............] - ETA: 3s - loss: 0.0140 - binary_accuracy: 0.9922
5/7 [====================&gt;.........] - ETA: 2s - loss: 0.0135 - binary_accuracy: 0.9937
6/7 [========================&gt;.....] - ETA: 1s - loss: 0.0134 - binary_accuracy: 0.9948
7/7 [==============================] - ETA: 0s - loss: 0.0127 - binary_accuracy: 0.9955
7/7 [==============================] - 8s 1s/step - loss: 0.0127 - binary_accuracy: 0.9955
Loss: 0.012707981280982494
Accuracy: 0.9955357313156128</pre>
			<p>This is consistent <a id="_idIndexMarker893"/>with the system performance during training that we saw in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
			<p>It looks like we have a very accurate model. If we want to use it later on, we can <span class="No-Break">save it.</span></p>
			<h2 id="_idParaDest-190"><a id="_idTextAnchor212"/>Saving the model for inference</h2>
			<p>The final step is <a id="_idIndexMarker894"/>to save the fine-tuned model for later use – for example, if the model is to be used in a production system, or if we want to use it in further experiments. The code for saving the model can be <span class="No-Break">seen here:</span></p>
			<pre class="source-code">
dataset_name = 'movie_reviews'
saved_model_path = './{}_bert'.format(dataset_name.replace('/', '_'))
classifier_model.save(saved_model_path, include_optimizer=False)
reloaded_model = tf.saved_model.load(saved_model_path)
]</pre>
			<p>In the code here, we show both saving the model and then reloading it from the <span class="No-Break">saved location.</span></p>
			<p>As we saw in this section, BERT can be trained to achieve very good performance by fine-tuning it with a relatively small (2,000-item) dataset. This makes it a good choice for many practical problems. Looking back at the example of classification with the multi-layer perceptron in <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, we saw <a id="_idIndexMarker895"/>that the accuracy (as shown in <span class="No-Break"><em class="italic">Figure 10</em></span><em class="italic">.4</em>) was never better than about 80% for the validation data, even after 20 epochs of training. Clearly, BERT does much better <span class="No-Break">than that.</span></p>
			<p>Although BERT is an excellent system, it has recently been surpassed by very large cloud-based pretrained LLMs. We will describe them in the <span class="No-Break">next section.</span></p>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor213"/>Cloud-based LLMs</h1>
			<p>Recently, there have been a number of cloud-based pretrained large language models that have shown <a id="_idIndexMarker896"/>very impressive performance because they have been trained on very large amounts of data. In contrast to BERT, they are too large to be downloaded and used locally. In addition, some are closed and proprietary and can’t be downloaded for that reason. These newer models are based on the same principles as BERT, and they have shown a very impressive performance. This impressive performance is due to the fact that these models have been trained with much larger amounts of data than BERT. Because they cannot be downloaded, it is important to keep in mind that they aren’t appropriate for every application. Specifically, if there are any privacy or security concerns regarding the data, it may not be a good idea to send it to the cloud for processing. Some of these systems are GPT-2, GPT-3, GPT-4, ChatGPT, and OPT-175B, and new LLMs are being published on a <span class="No-Break">frequent basis.</span></p>
			<p>The recent dramatic advances in NLP represented by these systems are made possible by three related technical advances. One is the development of techniques such as attention, which are much more able to capture relationships among words in texts than previous approaches such as RNNs, and which scale much better than the rule-based approaches that we covered in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a>. The second factor is the availability of massive amounts of training data, primarily in the form of text data on the World Wide Web. The third factor is the tremendous increase in computer resources available for processing this data and <span class="No-Break">training LLMs.</span></p>
			<p>So far in the systems we’ve discussed, all of the knowledge of a language that goes into the creation of a model for a specific application is derived from the training data. The process starts without knowing anything about the language. LLMs, on the other hand, come with models that have been <em class="italic">pretrained</em> through processing very large amounts of more or less generic text, and as a consequence have a basic foundation of information about the language. Additional training data can be used for <em class="italic">fine-tuning</em> the model so that it can handle inputs that are specific to the application. An important aspect of fine-tuning a model for <a id="_idIndexMarker897"/>a specific application is to minimize the amount of new data that is needed for fine-tuning. This is a cutting-edge area in NLP research and you may find <a id="_idIndexMarker898"/>references to training approaches called <strong class="bold">few-shot learning</strong>, which is learning to <a id="_idIndexMarker899"/>recognize a new class with only a few examples, or even <strong class="bold">zero-shot learning</strong>, which enables a system to identify a class without having seen any examples of that class in the <span class="No-Break">training data.</span></p>
			<p>In the next section, we’ll take a look at one of the currently most popular <span class="No-Break">LLMs, ChatGPT.</span></p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor214"/>ChatGPT</h2>
			<p>ChatGPT (<a href="https://openai.com/blog/chatgpt/">https://openai.com/blog/chatgpt/</a>) is a system that can interact with users about generic information <a id="_idIndexMarker900"/>in a very capable way. Although at the time of writing, it is <a id="_idIndexMarker901"/>hard to customize ChatGPT for specific applications, it can be useful for other purposes than customized natural language applications. For example, it can very easily be used to generate training data for a conventional application. If we wanted to develop a banking application using some of the techniques discussed earlier in this book, we would need training data to provide the system with examples of how users might ask the system questions. Typically, this involves a process of collecting actual user input, which could be very time-consuming. ChatGPT could be used to generate training data instead, by simply asking it for examples. For example, for the prompt <em class="italic">give me 10 examples of how someone might ask for their checking balance</em>, ChatGPT responded with the sentences in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer105" class="IMG---Figure">
					<img src="image/B19005_11_03.jpg" alt="Figure 11.3 – GPT-3 generated training data for a banking application"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – GPT-3 generated training data for a banking application</p>
			<p>Most of these seem like pretty reasonable queries about a checking account, but some of them<a id="_idIndexMarker902"/> don’t seem very natural. For that reason, data generated in this <a id="_idIndexMarker903"/>way always needs to be reviewed. For example, a developer might decide not to include the second to the last example in a training set because it sounds stilted, but overall, this technique has the potential to save developers quite a bit <span class="No-Break">of time.</span></p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor215"/>Applying GPT-3</h2>
			<p>Another well-known LLM, GPT-3, can also be fine-tuned with application-specific data, which should result in<a id="_idIndexMarker904"/> better performance. To do this, you need <a id="_idIndexMarker905"/>an OpenAI key because using GPT-3 is a paid service. Both fine-tuning to prepare the model and using the fine-tuned model to process new data at inference time will incur a cost, so it is important to verify that the training process is performing as expected before training with a large dataset and incurring the <span class="No-Break">associated expense.</span></p>
			<p>OpenAI recommends the following steps to fine-tune a <span class="No-Break">GPT-3 model.</span></p>
			<ol>
				<li>Sign up for <a id="_idIndexMarker906"/>an account at <a href="https://openai.com/">https://openai.com/</a> and obtain an API key. The API key will be used to track your usage and charge your <span class="No-Break">account accordingly.</span></li>
				<li>Install the OpenAI <strong class="bold">command-line interface</strong> (<strong class="bold">CLI</strong>) with the <span class="No-Break">following command:</span><pre class="source-code">
! pip install --upgrade openai</pre></li>
			</ol>
			<p>This command <a id="_idIndexMarker907"/>can be used at a terminal prompt in Unix-like <a id="_idIndexMarker908"/>systems (some developers have reported problems with Windows or macOS). Alternatively, you can install GPT-3 to be used in a Jupyter notebook with the <span class="No-Break">following code:</span></p>
			<pre class="source-code">
!pip install --upgrade openai</pre>
			<p>All of the following examples assume that the code is running in a <span class="No-Break">Jupyter notebook:</span></p>
			<ol>
				<li>Set your <span class="No-Break">API key:</span><pre class="source-code">
api_key =&lt;your API key&gt;</pre><pre class="source-code">
openai.api_key = api_key</pre></li>
				<li>The next step is to specify the training data that you will use for fine-tuning GPT-3 for your application. This is very similar to the process of training any NLP system; however, GPT-3 has a specific format that must be used for training data. This format uses a syntax called JSONL, where every line is an independent JSON expression. For example, if we want to fine-tune GPT-3 to classify movie reviews, a couple of data items would look like the following (omitting some of the text <span class="No-Break">for clarity):</span><pre class="source-code">
{"prompt":"this film is extraordinarily horrendous and i'm not going to waste any more words on it . ","completion":" negative"}</pre><pre class="source-code">
{"prompt":"9 : its pathetic attempt at \" improving \" on a shakespeare classic . 8 : its just another piece of teen fluff . 7 : kids in high school are not that witty . … ","completion":" negative"}</pre><pre class="source-code">
{"prompt":"claire danes , giovanni ribisi , and omar epps make a likable trio of protagonists , …","completion":" negative"}</pre></li>
			</ol>
			<p>Each item consists of a JSON dict with two keys, <strong class="source-inline">prompt</strong> and  <strong class="source-inline">completion</strong>. <strong class="source-inline">prompt</strong> is the text to be classified, and <strong class="source-inline">completion</strong> is the correct classification. All three of these items are negative reviews, so the completions are all marked <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">negative</strong></span><span class="No-Break">.</span></p>
			<p>It might not always <a id="_idIndexMarker909"/>be convenient to get your data into this format if it <a id="_idIndexMarker910"/>is already in another format, but OpenAI provides a useful tool for converting other formats into JSONL. It accepts a wide range of input formats, such as CSV, TSV, XLSX, and JSON, with the only requirement for the input being that it contains two columns with <strong class="source-inline">prompt</strong> and <strong class="source-inline">completion</strong> headers. <em class="italic">Table 11.2</em> shows a few cells from an Excel spreadsheet with some movie reviews as <span class="No-Break">an example:</span></p>
			<table id="table002-3" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">prompt</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">completion</strong></span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>kolya is one of the richest films i’ve seen in some time . zdenek sverak plays a confirmed old bachelor ( who’s likely to remain so ) , who finds his life as a czech cellist increasingly impacted by the five-year old boy that he’s taking care <span class="No-Break">of …</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">positive</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>this three hour movie opens up with a view of singer/guitar player/musician/composer frank zappa rehearsing with his fellow band members . all the rest displays a compilation of footage , mostly from the concert at the palladium in new york city , halloween <span class="No-Break">1979 …</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">positive</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>`strange days’ chronicles the last two days of 1999 in los angeles . as the locals gear up for the new millenium , lenny nero ( ralph fiennes ) goes about his <span class="No-Break">business …</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">positive</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 11.2 – Movie review data for fine-tuning GPT-3</p>
			<p>To convert one of these alternative formats into JSONL, you can use the <strong class="source-inline">fine_tunes.prepare_data</strong> tool, as shown here, assuming that your data is contained in the <span class="No-Break"><strong class="source-inline">movies.csv</strong></span><span class="No-Break"> file:</span></p>
			<pre class="source-code">
!openai tools fine_tunes.prepare_data -f ./movies.csv -q</pre>
			<p>The <strong class="source-inline">fine_tunes.prepare_data</strong> utility will create a JSONL file of the data and will also provide some diagnostic information that can help improve the data. The most important diagnostic that it provides is whether or not the amount of data is sufficient. OpenAI recommends several hundred examples of good performance. Other diagnostics include various types of formatting information such as separators between the prompts and <span class="No-Break">the completions.</span></p>
			<p>After the data is correctly formatted, you can upload it to your OpenAI account and save <span class="No-Break">the filename:</span></p>
			<pre class="source-code">
file_name = "./movies_prepared.jsonl"
upload_response = openai.File.create(
  file=open(file_name, "rb"),
  purpose='fine-tune'
)
file_id = upload_response.id</pre>
			<p>The next step is to create and save a fine-tuned model. There are several different OpenAI models that can be <a id="_idIndexMarker911"/>used. The one we’re using here, <strong class="source-inline">ada</strong>, is the <a id="_idIndexMarker912"/>fastest and least expensive, and does a good job on many <span class="No-Break">classification tasks:</span></p>
			<pre class="source-code">
openai.FineTune.create(training_file=file_id, model="ada")
fine_tuned_model = fine_tune_response.fine_tuned_model</pre>
			<p>Finally, we can test the model with a <span class="No-Break">new prompt:</span></p>
			<pre class="source-code">
answer = openai.Completion.create(
  model = fine_tuned_model,
    engine = "ada",
  prompt = " I don't like this movie ",
  max_tokens = 10, # Change amount of tokens for longer completion
  temperature = 0
)
answer['choices'][0]['text']</pre>
			<p>In this example, since we <a id="_idIndexMarker913"/>are only using a few fine-tuning utterances, the <a id="_idIndexMarker914"/>results will not be very good. You are encouraged to experiment with larger amounts of <span class="No-Break">training data.</span></p>
			<h1 id="_idParaDest-194"><a id="_idTextAnchor216"/>Summary</h1>
			<p>This chapter covered the currently best-performing techniques in NLP – transformers and pretrained models. In addition, we have demonstrated how they can be applied to processing your own application-specific data, using both local pretrained models and <span class="No-Break">cloud-based models.</span></p>
			<p>Specifically, you learned about the basic concepts behind attention, transformers, and pretrained models, and then applied the BERT pretrained transformer system to a classification problem. Finally, we looked at using the cloud-based GPT-3 systems for generating data and for processing <span class="No-Break">application-specific data.</span></p>
			<p>In <a href="B19005_12.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>, we will turn to a different topic – unsupervised learning. Up to this point, all of our models have been <em class="italic">supervised</em>, which you will recall means that the data has been annotated with the correct processing result. Next, we will discuss applications of <em class="italic">unsupervised</em> learning. These applications include topic modeling and clustering. We will also talk about the value of unsupervised learning for exploratory applications and maximizing scarce data. It will also address types of partial supervision, including weak supervision and <span class="No-Break">distant supervision.</span></p>
		</div>
	</body></html>