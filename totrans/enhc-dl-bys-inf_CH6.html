<?xml version="1.0" encoding="UTF-8" standalone="no"?>
<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xmlns:epub="http://www.idpf.org/2007/ops">
<head>
  <meta charset="utf-8"/>
  <meta name="generator" content="pandoc"/>
  <title>ch011.xhtml</title>
  <link rel="stylesheet" type="text/css" href="../styles/stylesheet1.css"/>
</head>
<body epub:type="bodymatter">
<section id="chapter-6-using-the-standard-toolbox-for-bayesian-deep-learning" class="level1 chapterHead" data-number="11">
<h1 class="chapterHead" data-number="11"><span class="titlemark">ChapterÂ 6</span><br/>
<span id="x1-820006"></span>Using the Standard Toolbox for Bayesian Deep Learning</h1>
<p>As we saw in previous chapters, vanilla NNs often produce poor uncertainty estimates and tend to make overconfident predictions, and some arenâ€™t capable of producing uncertainty estimates at all. By contrast, probabilistic architectures offer principled means to obtain high-quality uncertainty estimates; however, they have a number of limitations when it comes to scaling and adaptability.</p>
<p>While both PBP and BBB can be implemented with popular ML frameworks (as shown in our previous TensorFlow examples), they are very complex. As we saw in the last chapter, implementing even a simple network isnâ€™t straightforward. This means that adapting them to new architectures is awkward and time-consuming (particularly for PBP, although it is possible â€“ see <em>Fully Bayesian Recurrent Neural Networks for Safe Reinforcement</em> <em>Learning</em>). For simple tasks, such as the examples from <em>Chapter 5, Principled</em> <em>Approaches for Bayesian Deep Learning</em>, this isnâ€™t an issue. But in many real-world tasks, such as</p>
<p>machine translation or object recognition, far more sophisticated network architectures are necessary.</p>
<p>While some academic institutions or large research organizations may have the time and resources required to adapt these complex probabilistic methods to a variety of sophisticated architectures, in many cases this simply is not viable. Additionally, more and more industry researchers and engineers are turning to transfer learning-based methods, using pre-trained networks as the backbone of their models. In these cases, itâ€™s impossible to simply add probabilistic machinery to predefined architectures.</p>
<p>To address this, in this chapter, we will explore how common paradigms in deep learning can be harnessed to develop probabilistic models. The methods introduced here show that, with relatively minor tweaks, you can easily adapt large, sophisticated architectures to produce high-quality uncertainty estimates. Weâ€™ll even introduce techniques that will enable you to get uncertainty estimates from networks youâ€™ve already trained!</p>
<p>The chapter will cover three key approaches for facilitating model uncertainty estimation easily with common deep learning frameworks. First, we will look at <strong>Monte Carlo Dropout</strong> (<strong>MC dropout</strong>), a method that induces variance across predictions by utilizing dropout at inference time. Second, we will introduce deep ensembles, whereby multiple neural networks are combined to facilitate both uncertainty estimation and improved model performance. Finally, we will explore various methods for adding a Bayesian layer to our model, allowing any model to produce uncertainty estimates.</p>
<p>These topics will be covered in the following sections:</p>
<ul>
<li><p>Introducing approximate Bayesian inference via dropout</p></li>
<li><p>Using ensembles for model uncertainty estimates</p></li>
<li><p>Exploring neural network augmentation with Bayesian last-layer methods</p></li>
</ul>
<p><span id="x1-82001r143"></span></p>
<section id="technical-requirements-4" class="level2 sectionHead" data-number="11.1">
<h2 class="sectionHead" data-number="11.1" id="sigil_toc_id_63"><span class="titlemark">6.1 </span> <span id="x1-830001"></span>Technical requirements</h2>
<p>To complete the practical tasks in this chapter, you will need a Python 3.8 environment with the SciPy stack and the following additional Python packages installed:</p>
<ul>
<li><p>TensorFlow 2.0</p></li>
<li><p>TensorFlow Probability</p></li>
</ul>
<p>All of the code for this book can be found on the GitHub repository for the book: <a href="https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference" class="url"><span class="No-Break">https://github.com/PacktPublishing/Enhancing-Deep-Learning-with-Bayesian-Inference</span></a>. <span id="x1-83001r145"></span></p>
</section>
<section id="introducing-approximate-bayesian-inference-via-dropout" class="level2 sectionHead" data-number="11.2">
<h2 class="sectionHead" data-number="11.2" id="sigil_toc_id_64"><span class="titlemark">6.2 </span> <span id="x1-840002"></span>Introducing approximate Bayesian inference via dropout</h2>
<p><strong>Dropout</strong> is traditionally used to <span id="dx1-84001"></span>prevent overfitting an NN. First introduced in 2012, it is now used in many common NN architectures and is one of the easiest and most widely used regularization methods. The idea of dropout is to randomly turn off (or drop) certain units of a neural network during training. Because of this, the model cannot solely rely on a particular small <span id="dx1-84002"></span>subset of neurons to solve the task it was given. Instead, the model is forced to find different ways to solve its task. This improves the robustness of the model and makes it less likely to overfit.</p>
<p>If we simplify a network to <em>y</em> = <em>Wx</em>, where <em>y</em> is the output of our network, <em>x</em> the input, and <em>W</em> our model weights, we can think of dropout as:</p>
<div class="math-display">
<img src="../media/file139.jpg" class="math-display" alt=" ( { wj, p wË†j = ( 0, otherwise "/>
</div>
<p>where <span class="accenthat"><em>w</em><sub><em>j</em></sub></span> is the new weights after applying dropout, <em>w</em><sub><em>j</em></sub> is our weights before applying dropout, and <em>p</em> is our probability of <em>not</em> applying dropout.</p>
<p>The original dropout paper recommends randomly dropping 50% of the units in a network and applying dropout to all layers. Input layers should not have the same dropout probability because this would mean that we throw away 50% of the input information for our network, which makes it more difficult for the model to converge. In practice, you can experiment with different <span id="dx1-84003"></span>dropout probabilities to find the dropout rate that works well for your specific dataset and model; that is another hyperparameter you can optimize. Dropout is typically available as a <span id="dx1-84004"></span>standalone layer in all standard neural network libraries you can find online. You typically add it after your activation function:</p>
<pre id="fancyvrb74" class="fancyvrb"><span id="x1-84023r1"></span> 
<code><span id="textcolor1833"><span>from</span></span><span>Â </span><span id="textcolor1834"><span>tensorflow.keras</span></span><span>Â </span><span id="textcolor1835"><span>import</span></span><span>Â Sequential</span> <span id="x1-84025r2"></span> </code>
<code><span id="textcolor1836"><span>from</span></span><span>Â </span><span id="textcolor1837"><span>tensorflow.keras.layers</span></span><span>Â </span><span id="textcolor1838"><span>import</span></span><span>Â Flatten,</span><span>Â Conv2D,</span><span>Â MaxPooling2D,</span><span>Â Dropout,</span><span>Â Dense</span> <span id="x1-84027r3"></span> </code>
<code><span id="x1-84029r4"></span></code>
<code><span id="x1-84031r5"></span></code>
<code><span>model</span><span>Â </span><span id="textcolor1839"><span>=</span></span><span>Â Sequential([</span> <span id="x1-84033r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â Conv2D(</span><span id="textcolor1840"><span>32</span></span><span>,</span><span>Â (</span><span id="textcolor1841"><span>3</span></span><span>,</span><span id="textcolor1842"><span>3</span></span><span>),</span><span>Â activation</span><span id="textcolor1843"><span>=</span></span><span id="textcolor1844"><span>"relu"</span></span><span>,</span><span>Â input_shape</span><span id="textcolor1845"><span>=</span></span><span>(</span><span id="textcolor1846"><span>28</span></span><span>,</span><span>Â </span><span id="textcolor1847"><span>28</span></span><span>,</span><span>Â </span><span id="textcolor1848"><span>1</span></span><span>)),</span> <span id="x1-84035r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â MaxPooling2D((</span><span id="textcolor1849"><span>2</span></span><span>,</span><span id="textcolor1850"><span>2</span></span><span>)),</span> <span id="x1-84037r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â Dropout(</span><span id="textcolor1851"><span>0.2</span></span><span>),</span> <span id="x1-84039r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â Conv2D(</span><span id="textcolor1852"><span>64</span></span><span>,</span><span>Â (</span><span id="textcolor1853"><span>3</span></span><span>,</span><span id="textcolor1854"><span>3</span></span><span>),</span><span>Â activation</span><span id="textcolor1855"><span>=</span></span><span id="textcolor1856"><span>"relu"</span></span><span>),</span> <span id="x1-84041r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â MaxPooling2D((</span><span id="textcolor1857"><span>2</span></span><span>,</span><span id="textcolor1858"><span>2</span></span><span>)),</span> <span id="x1-84043r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â Dropout(</span><span id="textcolor1859"><span>0.5</span></span><span>),</span> <span id="x1-84045r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â Flatten(),</span> <span id="x1-84047r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â Dense(</span><span id="textcolor1860"><span>64</span></span><span>,</span><span>Â activation</span><span id="textcolor1861"><span>=</span></span><span id="textcolor1862"><span>"relu"</span></span><span>),</span> <span id="x1-84049r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â Dropout(</span><span id="textcolor1863"><span>0.5</span></span><span>),</span> <span id="x1-84051r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â Dense(</span><span id="textcolor1864"><span>10</span></span><span>)</span> <span id="x1-84053r16"></span> </code>
<code><span>])</span> <span id="x1-84055r17"></span> </code>
<code></code></pre>
<p>Now that weâ€™ve been reminded of the vanilla application of dropout, letâ€™s look at how we can use it for Bayesian inference. <span id="x1-84056r104"></span></p>
<section id="using-dropout-for-approximate-bayesian-inference" class="level3 subsectionHead" data-number="11.2.1">
<h3 class="subsectionHead" data-number="11.2.1" id="sigil_toc_id_65"><span class="titlemark">6.2.1 </span> <span id="x1-850001"></span>Using dropout for approximate Bayesian inference</h3>
<p>Traditional <span id="dx1-85001"></span>dropout methods make the prediction of dropout networks deterministic at test time by turning off dropout during inference. However, we can also use the <span id="dx1-85002"></span>stochasticity of dropout to our advantage. This is called <strong>Monte</strong> <strong>Carlo (MC)</strong> dropout, and the <span id="dx1-85003"></span>idea is as follows:</p>
<ol>
<li><div id="x1-85005x1">
<p>We use dropout during test time.</p>
</div></li>
<li><div id="x1-85007x2">
<p>Instead of running inference once, we run it many times (for example, 30-100 times).</p>
</div></li>
<li><div id="x1-85009x3">
<p>We then average the predictions to get our uncertainty estimates.</p>
</div></li>
</ol>
<p>Why is this beneficial? As we said before, using dropout forces the model to learn different ways to solve its task. So, when we keep dropout enabled during inference, we use slightly different networks that all process the input via a slightly different path through the model. This diversity is helpful when we want a calibrated uncertainty score, as we will see in the next section, where we will discuss the concept of deep ensembles. Instead of predicting a point estimate (a single value) for each input, our network now produces a distribution of values (made up of multiple forward passes). We can use this distribution to compute a mean and variance associated with <span id="dx1-85010"></span>each input data point, as shown in <em>Figure</em> <a href="#x1-85011r1"><em>6.1</em></a>.</p>
<div class="IMG---Figure">
<img src="../media/file140.png" alt="PIC"/> <span id="x1-85011r1"></span> <span id="x1-85012"></span></div>
<p class="IMG---Caption">FigureÂ 6.1: Example of MC dropout 
</p>
<p>We can also interpret MC dropout in a Bayesian way. Using these slightly different networks with dropout can be seen as sampling from a distribution of all <span id="dx1-85013"></span>possible models: the posterior distribution over all of the parameters (or weights) of our network:</p>
<div class="math-display">
<img src="../media/file141.jpg" class="math-display" alt="ğœƒt âˆ¼ P (ğœƒ|D ) "/>
</div>
<p>Here, <em>ğœƒ</em><sub><em>t</em></sub> is a dropout configuration and <span class="cmsy-10x-x-109">âˆ¼ </span>a single sample from our posterior distribution <em>P</em>(<em>ğœƒ</em><span class="cmsy-10x-x-109">|</span><em>D</em>). This way, MC dropout is equivalent to a form of approximate Bayesian inference, similar to the methods we saw in <a href="CH5.xhtml#x1-600005"><em>ChapterÂ 5</em></a>, <a href="CH5.xhtml#x1-600005"><em>Principled Approaches for Bayesian Deep Learning</em></a>.</p>
<p>Now that we have an idea of how MC dropout works, letâ€™s implement it in TensorFlow. <span id="x1-85014r147"></span></p>
</section>
<section id="implementing-mc-dropout" class="level3 subsectionHead" data-number="11.2.2">
<h3 class="subsectionHead" data-number="11.2.2" id="sigil_toc_id_66"><span class="titlemark">6.2.2 </span> <span id="x1-860002"></span>Implementing MC dropout</h3>
<p>Letâ€™s assume we have trained a model with the <span id="dx1-86001"></span>convolutional architecture described in this chapterâ€™s first hands-on exercise. We can now use dropout at inference by setting <code>training=True</code>:</p>
<pre id="fancyvrb75" class="fancyvrb"><span id="x1-86019r1"></span> 
<code><span id="textcolor1865"><span>def</span></span><span>Â </span><span id="textcolor1866"><span>mc_dropout_inference</span></span><span>(</span> <span id="x1-86021r2"></span> </code>
<code><span>Â </span><span>Â imgs:</span><span>Â np</span><span id="textcolor1867"><span>.</span></span><span>ndarray,</span> <span id="x1-86023r3"></span> </code>
<code><span>Â </span><span>Â nb_inference:</span><span>Â </span><span id="textcolor1868"><span>int</span></span><span>,</span> <span id="x1-86025r4"></span> </code>
<code><span>Â </span><span>Â model:</span><span>Â Sequential</span> <span id="x1-86027r5"></span> </code>
<code><span>)</span><span>Â </span><span id="textcolor1869"><span>-</span><em>&gt;</em></span><span>Â np</span><span id="textcolor1870"><span>.</span></span><span>ndarray:</span> <span id="x1-86029r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1871"><span class="cmitt-10x-x-109">""""</span></span> <span id="x1-86031r7"></span> </code>
<code><span id="textcolor1872"><span class="cmitt-10x-x-109">Â </span><span class="cmitt-10x-x-109">Â </span><span class="cmitt-10x-x-109">Â </span><span class="cmitt-10x-x-109">Â Run</span><span class="cmitt-10x-x-109">Â inference</span><span class="cmitt-10x-x-109">Â nb_inference</span><span class="cmitt-10x-x-109">Â times</span><span class="cmitt-10x-x-109">Â with</span><span class="cmitt-10x-x-109">Â random</span><span class="cmitt-10x-x-109">Â dropout</span><span class="cmitt-10x-x-109">Â enabled</span></span> <span id="x1-86033r8"></span> </code>
<code><span id="textcolor1873"><span class="cmitt-10x-x-109">Â </span><span class="cmitt-10x-x-109">Â </span><span class="cmitt-10x-x-109">Â </span><span class="cmitt-10x-x-109">Â (training=True)</span></span> <span id="x1-86035r9"></span> </code>
<code><span id="textcolor1874"><span class="cmitt-10x-x-109">Â </span><span class="cmitt-10x-x-109">Â </span><span class="cmitt-10x-x-109">Â </span><span class="cmitt-10x-x-109">Â """</span></span> <span id="x1-86037r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â divds</span><span>Â </span><span id="textcolor1875"><span>=</span></span><span>Â []</span> <span id="x1-86039r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1876"><span>for</span></span><span>Â _</span><span>Â </span><span id="textcolor1877"><span>in</span></span><span>Â </span><span id="textcolor1878"><span>range</span></span><span>(nb_inference):</span> <span id="x1-86041r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â divds</span><span id="textcolor1879"><span>.</span></span><span>append(model(imgs,</span><span>Â training</span><span id="textcolor1880"><span>=</span></span><span id="textcolor1881"><span>True</span></span><span>))</span> <span id="x1-86043r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1882"><span>return</span></span><span>Â tf</span><span id="textcolor1883"><span>.</span></span><span>nn</span><span id="textcolor1884"><span>.</span></span><span>softmax(divds,</span><span>Â axis</span><span id="textcolor1885"><span>=-</span></span><span id="textcolor1886"><span>1</span></span><span>)</span><span id="textcolor1887"><span>.</span></span><span>numpy()</span> <span id="x1-86045r14"></span> </code>
<code><span id="x1-86047r15"></span></code>
<code><span id="x1-86049r16"></span></code>
<code><span>Predictions</span><span>Â </span><span id="textcolor1888"><span>=</span></span><span>Â mc_dropout_inference(test_images,</span><span>Â </span><span id="textcolor1889"><span>50</span></span><span>,</span><span>Â model)</span></code></pre>
<p>This allows us to get our mean and variance for every prediction of our model. Each row of our <code>Predictions</code> variable contains the predictions associated with each input, obtained from consecutive forward passes. From these predictions, we can compute the means and variances, as follows:</p>
<pre id="fancyvrb76" class="fancyvrb"><span id="x1-86054r1"></span> 
<code><span>predictive_mean</span><span>Â </span><span id="textcolor1890"><span>=</span></span><span>Â np</span><span id="textcolor1891"><span>.</span></span><span>mean(predictions,</span><span>Â axis</span><span id="textcolor1892"><span>=</span></span><span id="textcolor1893"><span>0</span></span><span>)</span> <span id="x1-86056r2"></span> </code>
<code><span>predictive_variance</span><span>Â </span><span id="textcolor1894"><span>=</span></span><span>Â np</span><span id="textcolor1895"><span>.</span></span><span>var(predictions,</span><span>Â axis</span><span id="textcolor1896"><span>=</span></span><span id="textcolor1897"><span>0</span></span><span>)</span></code></pre>
<p>As with all neural networks, Bayesian neural networks require some degree of fine-tuning via hyperparameters. The following <span id="dx1-86057"></span>three hyperparameters are <span id="dx1-86058"></span>particularly important for MC dropout:</p>
<ul>
<li><p><strong>Number of dropout layers</strong>: How many layers (in our <code>Sequential    </code> object) will use dropout, and which layers these will be.</p></li>
<li><p><strong>Dropout rate</strong>: The likelihood that nodes will be dropped.</p></li>
<li><p><strong>The number of MC dropout samples</strong>: A new hyperparameter specific to MC dropout. Shown here as <code>nb_inference    </code>, this defines the number of times to sample from the MC dropout network at inference time.</p></li>
</ul>
<p>Weâ€™ve now seen how MC dropout can be used in a new way, giving us an easy and intuitive method to compute Bayesian uncertainties using familiar tools. But this isnâ€™t the only method we have available to us. In the next section, weâ€™ll see how ensembling can be applied to neural networks; providing another straightforward approach for approximating BNNs. <span id="x1-86061r146"></span></p>
</section>
</section>
<section id="using-ensembles-for-model-uncertainty-estimates" class="level2 sectionHead" data-number="11.3">
<h2 class="sectionHead" data-number="11.3" id="sigil_toc_id_67"><span class="titlemark">6.3 </span> <span id="x1-870003"></span>Using ensembles for model uncertainty estimates</h2>
<p>This section will introduce you to <span id="dx1-87001"></span>deep ensembles: a popular method for obtaining Bayesian uncertainty estimates using an ensemble of deep networks. <span id="x1-87002r149"></span></p>
<section id="introducing-ensembling-methods" class="level3 subsectionHead" data-number="11.3.1">
<h3 class="subsectionHead" data-number="11.3.1" id="sigil_toc_id_68"><span class="titlemark">6.3.1 </span> <span id="x1-880001"></span>Introducing ensembling methods</h3>
<p>A common strategy in machine learning is to <span id="dx1-88001"></span>combine several single models into a committee of models. The process of learning such a combination of <span id="dx1-88002"></span>models is called <strong>ensemble learning</strong>, and the resulting committee of models is called an ensemble. Ensemble learning involves two main components: first, the different single models need to be trained. There are various strategies to obtain different models from the same training data: the models can be trained on different subsets of data, we can train different model types or models with different architectures, or we can initialize the same model types with different hyperparameters. Second, the outputs of the different single models need to be combined. Common strategies for combining the predictions of single models are simply taking their average or taking a majority vote among all members of the ensemble. More advanced strategies are taking a weighted average or, if more training data is available, learning an additional model to combine the different predictions of the ensemble members.</p>
<p>Ensembles are very popular in machine learning because they tend to improve predictive performance by minimizing the risk of accidentally picking a model with poor performance. In fact, ensembles are guaranteed to perform at least as well as any single model. Furthermore, ensembles will perform better than single models if there is enough diversity among the predictions of ensemble members. Diversity here means that different ensemble members make different mistakes on a given data sample. If, for example, some ensemble members misclassify the image of a dog as â€œcat,â€ but the majority of ensemble members make the correct prediction (â€œdogâ€), then the combined ensemble output will still be correct (â€œdogâ€). More generally, as long as every single model has an accuracy greater than 50% and the models make independent mistakes, then the predictive performance of the ensemble will approach 100% accuracy as we add more and more ensemble members.</p>
<p>In addition to improving predictive performance, we can leverage the degree of agreement (or disagreement) among ensemble members to obtain an uncertainty estimate along with the prediction of the ensemble. In the context of image classification, for example, if almost all ensemble members predict that the image shows a dog, then we can say that the ensemble predicted â€œdogâ€ with high confidence (or low uncertainty). Conversely, if there is significant disagreement among the predictions of different ensemble members, then we will observe high uncertainty in the form of significant variance across the outputs from the ensemble members, telling us that the prediction has low confidence.</p>
<p>Now that we are equipped with a basic understanding of ensembles, it is worth highlighting that MC dropout, which we explored in the previous section, may also be seen as an ensemble method. When we enable dropout during inference, we effectively run inference with a slightly different (sub-)network every time. The combination of these different sub-networks can be considered as a committee of different models, and therefore an ensemble. This observation led a team at Google to look into alternative ways of creating ensembles from DNNs, which led to the discovery of deep ensembles (Lakshminarayan et al, 2016) , which are introduced in the following section. <span id="x1-88003r151"></span></p>
</section>
<section id="introducing-deep-ensembles" class="level3 subsectionHead" data-number="11.3.2">
<h3 class="subsectionHead" data-number="11.3.2" id="sigil_toc_id_69"><span class="titlemark">6.3.2 </span> <span id="x1-890002"></span>Introducing deep ensembles</h3>
<p>The main idea behind <span id="dx1-89001"></span>deep ensembles is straightforward: train several different DNN models, then combine their predictions via averaging to improve model performance and leverage the agreement among the predictions of these models to obtain an estimate of predictive uncertainty.</p>
<p>More formally, assume that we have some training data <strong>X</strong>, where <em>X</em> <span class="cmsy-10x-x-109">âˆˆ</span><span class="msbm-10x-x-109">â„</span><sup><em>D</em></sup>, and corresponding target labels <strong>y</strong>. For example, in image classification the training data would be images, and the target labels would be integers that denote which object class is shown in the corresponding image, so <em>y</em> <span class="cmsy-10x-x-109">âˆˆ{</span>1<em>,...,K</em><span class="cmsy-10x-x-109">} </span>where <em>K</em> is the total number of classes. Training a single neural network means that we model the probabilistic predictive distribution <em>p</em><sub><em>ğœƒ</em></sub>(<em>y</em><span class="cmsy-10x-x-109">|</span><em>x</em>) over the labels and optimize <em>ğœƒ</em>, the parameters of the NN. For deep ensembles, we train <strong>M</strong> neural networks whose parameters can be described as <span class="cmsy-10x-x-109">{</span><em>ğœƒ</em><sub><em>m</em></sub><span class="cmsy-10x-x-109">}</span><sub><em>m</em><span class="cmr-8">=1</span></sub><sup><em>M</em></sup>, where each <em>ğœƒ</em><sub><em>m</em></sub> is optimized independently using <strong>X</strong> and <strong>y</strong> (meaning that we train each NN independently on the same data). The predictions of the deep ensemble members are combined via averaging, using <em>p</em>(<em>y</em><span class="cmsy-10x-x-109">|</span><em>x</em>) = <em>M</em><sup><span class="cmsy-8">âˆ’</span><span class="cmr-8">1</span></sup> <span class="cmex-10x-x-109">âˆ‘</span> <sub><em>m</em><span class="cmr-8">=1</span></sub><sup><em>M</em></sup><em>p</em><sub><em>ğœƒ</em><sub><span class="cmmi-6">m</span></sub></sub>(<em>y</em><span class="cmsy-10x-x-109">|</span><em>x,ğœƒ</em><sub><em>m</em></sub>).</p>
<p><em>Figure</em> <a href="#x1-89003r2"><em>6.2</em></a> illustrates the idea behind <span id="dx1-89002"></span>deep ensembles. Here, we have trained <em>M</em> = 3 different feed-forward NNs. Notice that each network has its own unique set of network weights, as illustrated by the varying thickness of the edges connecting the network notes. Each of the three networks will output its own prediction score, as illustrated by the green nodes, and we combine these scores via averaging.</p>
<div class="IMG---Figure">
<img src="../media/file142.png" alt="PIC"/> <span id="x1-89003r2"></span> <span id="x1-89004"></span></div>
<p class="IMG---Caption">FigureÂ 6.2: Example of a deep ensemble. Note that the three networks differ in their weights, as illustrated by edges with different thicknesses 
</p>
<p>How can we train several different neural network models if only one dataset is available for training? The strategy proposed in the original paper (and still the most commonly used strategy) is to start every training with a random initialization of the networkâ€™s weights. If every training starts with a different set of weights, the different training runs are likely to produce networks with different function approximations of the training data. This is because NNs tend to have many more weight parameters than there are samples in the training dataset. Therefore, the same observations in the training dataset can be approximated by many different weight parameter combinations. During training, the different NN models will each converge to their own parameter combination and will occupy different local optima on the loss landscape. Because of this, the different NNs will also often have differing perspectives on a given data sample, for example, the image of a dog. This also means that the different NNs will make different mistakes, for example, when classifying the data sample. The degree of consensus between the different networks in an ensemble provides information about how certain an ensemble is in its predictions for a given data point: the more the networks agree, the more confident we can be in the prediction.</p>
<p>Alternative ways to train different NN models with the same training data set are: to use a random ordering of mini-batches during training, to use different hyperparameters for every training run, or to use different network architecture for every model altogether. These strategies can also be combined, and understanding exactly which <span id="dx1-89005"></span>combination of strategies leads to the best outcomes, in terms predictive performance and predictive uncertainty, is an active field of research. <span id="x1-89006r152"></span></p>
</section>
<section id="implementing-a-deep-ensemble" class="level3 subsectionHead" data-number="11.3.3">
<h3 class="subsectionHead" data-number="11.3.3" id="sigil_toc_id_70"><span class="titlemark">6.3.3 </span> <span id="x1-900003"></span>Implementing a deep ensemble</h3>
<p>The following code example illustrates <span id="dx1-90001"></span>how to train a deep ensemble using the strategy of random weight initialization to obtain differing ensemble members.</p>
<section id="step-1-importing-libraries-1" class="level4 likesubsubsectionHead" data-number="11.3.3.1">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.3.3.1"><span id="x1-910003"></span>Step 1: Importing libraries</h4>
<p>We start by importing the relevant packages and <span id="dx1-91001"></span>setting the number of ensembles to <span class="obeylines-h"><span class="verb"><code>3</code></span></span> for this code example:</p>
<pre id="fancyvrb77" class="fancyvrb"><span id="x1-91009r1"></span> 
<code><span id="textcolor1898"><span>import</span></span><span>Â </span><span id="textcolor1899"><span>tensorflow</span></span><span>Â </span><span id="textcolor1900"><span>as</span></span><span>Â </span><span id="textcolor1901"><span>tf</span></span> <span id="x1-91011r2"></span> </code>
<code><span id="textcolor1902"><span>import</span></span><span>Â </span><span id="textcolor1903"><span>numpy</span></span><span>Â </span><span id="textcolor1904"><span>as</span></span><span>Â </span><span id="textcolor1905"><span>np</span></span> <span id="x1-91013r3"></span> </code>
<code><span id="textcolor1906"><span>import</span></span><span>Â </span><span id="textcolor1907"><span>matplotlib.pyplot</span></span><span>Â </span><span id="textcolor1908"><span>as</span></span><span>Â </span><span id="textcolor1909"><span>plt</span></span> <span id="x1-91015r4"></span> </code>
<code><span id="x1-91017r5"></span></code>
<code><span id="x1-91019r6"></span></code>
<code><span>ENSEMBLE_MEMBERS</span><span>Â </span><span id="textcolor1910"><span>=</span></span><span>Â </span><span id="textcolor1911"><span>3</span></span></code></pre>
</section>
<section id="step-2-obtaining-data" class="level4 likesubsubsectionHead" data-number="11.3.3.2">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.3.3.2"><span id="x1-920003"></span>Step 2: Obtaining data</h4>
<p>We then download the <span class="obeylines-h"><span class="verb"><code>MNIST</code><code>Â Fashion</code></span></span> dataset, which is a <span id="dx1-92001"></span>dataset that contains images of ten different clothing items:</p>
<pre id="fancyvrb78" class="fancyvrb"><span id="x1-92011r1"></span> 
<code><span id="textcolor1912"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â download</span><span class="cmitt-10x-x-109">Â data</span><span class="cmitt-10x-x-109">Â set</span></span> <span id="x1-92013r2"></span> </code>
<code><span>fashion_mnist</span><span>Â </span><span id="textcolor1913"><span>=</span></span><span>Â tf</span><span id="textcolor1914"><span>.</span></span><span>keras</span><span id="textcolor1915"><span>.</span></span><span>datasets</span><span id="textcolor1916"><span>.</span></span><span>fashion_mnist</span> <span id="x1-92015r3"></span> </code>
<code><span id="textcolor1917"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â split</span><span class="cmitt-10x-x-109">Â in</span><span class="cmitt-10x-x-109">Â train</span><span class="cmitt-10x-x-109">Â and</span><span class="cmitt-10x-x-109">Â test,</span><span class="cmitt-10x-x-109">Â images</span><span class="cmitt-10x-x-109">Â and</span><span class="cmitt-10x-x-109">Â labels</span></span> <span id="x1-92017r4"></span> </code>
<code><span>(train_images,</span><span>Â train_labels),</span><span>Â (test_images,</span><span>Â test_labels)</span><span>Â </span><span id="textcolor1918"><span>=</span></span><span>Â fashion_mnist</span><span id="textcolor1919"><span>.</span></span><span>load_data()</span> <span id="x1-92019r5"></span> </code>
<code><span id="x1-92021r6"></span></code>
<code><span id="textcolor1920"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â set</span><span class="cmitt-10x-x-109">Â class</span><span class="cmitt-10x-x-109">Â names</span></span> <span id="x1-92023r7"></span> </code>
<code><span>CLASS_NAMES</span><span>Â </span><span id="textcolor1921"><span>=</span></span><span>Â [</span><span id="textcolor1922"><span>'T-shirt'</span></span><span>,</span><span>Â </span><span id="textcolor1923"><span>'Trouser'</span></span><span>,</span><span>Â </span><span id="textcolor1924"><span>'Pullover'</span></span><span>,</span><span>Â </span><span id="textcolor1925"><span>'Dress'</span></span><span>,</span><span>Â </span><span id="textcolor1926"><span>'Coat'</span></span><span>,</span> <span id="x1-92025r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1927"><span>'Sandal'</span></span><span>,</span><span>Â </span><span id="textcolor1928"><span>'Shirt'</span></span><span>,</span><span>Â </span><span id="textcolor1929"><span>'Sneaker'</span></span><span>,</span><span>Â </span><span id="textcolor1930"><span>'Bag'</span></span><span>,</span><span>Â </span><span id="textcolor1931"><span>'Ankle</span><span>Â boot'</span></span><span>]</span></code></pre>
</section>
<section id="step-3-constructing-our-ensemble-model" class="level4 likesubsubsectionHead" data-number="11.3.3.3">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.3.3.3"><span id="x1-930003"></span>Step 3: Constructing our ensemble model</h4>
<p>Next, we create a helper function that <span id="dx1-93001"></span>defines our model. As you can see, we use a simple image classifier structure that consists of two convolutional layers, each followed by a max-pooling operation, and several fully connected layers:</p>
<pre id="fancyvrb79" class="fancyvrb"><span id="x1-93023r1"></span> 
<code><span id="textcolor1932"><span>def</span></span><span>Â </span><span id="textcolor1933"><span>build_model</span></span><span>():</span> <span id="x1-93025r2"></span> </code>
<code><span>Â </span><span>Â </span><span id="textcolor1934"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â we</span><span class="cmitt-10x-x-109">Â build</span><span class="cmitt-10x-x-109">Â a</span><span class="cmitt-10x-x-109">Â forward</span><span class="cmitt-10x-x-109">Â neural</span><span class="cmitt-10x-x-109">Â network</span><span class="cmitt-10x-x-109">Â with</span><span class="cmitt-10x-x-109">Â tf.keras.Sequential</span></span> <span id="x1-93027r3"></span> </code>
<code><span>Â </span><span>Â model</span><span>Â </span><span id="textcolor1935"><span>=</span></span><span>Â tf</span><span id="textcolor1936"><span>.</span></span><span>keras</span><span id="textcolor1937"><span>.</span></span><span>Sequential([</span> <span id="x1-93029r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1938"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â we</span><span class="cmitt-10x-x-109">Â define</span><span class="cmitt-10x-x-109">Â two</span><span class="cmitt-10x-x-109">Â convolutional</span><span class="cmitt-10x-x-109">Â layers</span><span class="cmitt-10x-x-109">Â followed</span><span class="cmitt-10x-x-109">Â by</span><span class="cmitt-10x-x-109">Â a</span><span class="cmitt-10x-x-109">Â max-pooling</span><span class="cmitt-10x-x-109">Â operation</span><span class="cmitt-10x-x-109">Â each</span></span> <span id="x1-93031r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1939"><span>.</span></span><span>keras</span><span id="textcolor1940"><span>.</span></span><span>layers</span><span id="textcolor1941"><span>.</span></span><span>Conv2D(filters</span><span id="textcolor1942"><span>=</span></span><span id="textcolor1943"><span>32</span></span><span>,</span><span>Â kernel_size</span><span id="textcolor1944"><span>=</span></span><span>(</span><span id="textcolor1945"><span>5</span></span><span>,</span><span id="textcolor1946"><span>5</span></span><span>),</span><span>Â padding</span><span id="textcolor1947"><span>=</span></span><span id="textcolor1948"><span>'same'</span></span><span>,</span> <span id="x1-93033r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â activation</span><span id="textcolor1949"><span>=</span></span><span id="textcolor1950"><span>'relu'</span></span><span>,</span><span>Â input_shape</span><span id="textcolor1951"><span>=</span></span><span>(</span><span id="textcolor1952"><span>28</span></span><span>,</span><span>Â </span><span id="textcolor1953"><span>28</span></span><span>,</span><span>Â </span><span id="textcolor1954"><span>1</span></span><span>)),</span> <span id="x1-93035r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1955"><span>.</span></span><span>keras</span><span id="textcolor1956"><span>.</span></span><span>layers</span><span id="textcolor1957"><span>.</span></span><span>MaxPool2D(strides</span><span id="textcolor1958"><span>=</span></span><span id="textcolor1959"><span>2</span></span><span>),</span> <span id="x1-93037r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1960"><span>.</span></span><span>keras</span><span id="textcolor1961"><span>.</span></span><span>layers</span><span id="textcolor1962"><span>.</span></span><span>Conv2D(filters</span><span id="textcolor1963"><span>=</span></span><span id="textcolor1964"><span>48</span></span><span>,</span><span>Â kernel_size</span><span id="textcolor1965"><span>=</span></span><span>(</span><span id="textcolor1966"><span>5</span></span><span>,</span><span id="textcolor1967"><span>5</span></span><span>),</span><span>Â padding</span><span id="textcolor1968"><span>=</span></span><span id="textcolor1969"><span>'valid'</span></span><span>,</span> <span id="x1-93039r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â activation</span><span id="textcolor1970"><span>=</span></span><span id="textcolor1971"><span>'relu'</span></span><span>),</span> <span id="x1-93041r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1972"><span>.</span></span><span>keras</span><span id="textcolor1973"><span>.</span></span><span>layers</span><span id="textcolor1974"><span>.</span></span><span>MaxPool2D(strides</span><span id="textcolor1975"><span>=</span></span><span id="textcolor1976"><span>2</span></span><span>),</span> <span id="x1-93043r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1977"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â we</span><span class="cmitt-10x-x-109">Â flatten</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â matrix</span><span class="cmitt-10x-x-109">Â output</span><span class="cmitt-10x-x-109">Â into</span><span class="cmitt-10x-x-109">Â a</span><span class="cmitt-10x-x-109">Â vector</span></span> <span id="x1-93045r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1978"><span>.</span></span><span>keras</span><span id="textcolor1979"><span>.</span></span><span>layers</span><span id="textcolor1980"><span>.</span></span><span>Flatten(),</span> <span id="x1-93047r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor1981"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â we</span><span class="cmitt-10x-x-109">Â apply</span><span class="cmitt-10x-x-109">Â three</span><span class="cmitt-10x-x-109">Â fully-connected</span><span class="cmitt-10x-x-109">Â layers</span></span> <span id="x1-93049r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1982"><span>.</span></span><span>keras</span><span id="textcolor1983"><span>.</span></span><span>layers</span><span id="textcolor1984"><span>.</span></span><span>Dense(</span><span id="textcolor1985"><span>256</span></span><span>,</span><span>Â activation</span><span id="textcolor1986"><span>=</span></span><span id="textcolor1987"><span>'relu'</span></span><span>),</span> <span id="x1-93051r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1988"><span>.</span></span><span>keras</span><span id="textcolor1989"><span>.</span></span><span>layers</span><span id="textcolor1990"><span>.</span></span><span>Dense(</span><span id="textcolor1991"><span>84</span></span><span>,</span><span>Â activation</span><span id="textcolor1992"><span>=</span></span><span id="textcolor1993"><span>'relu'</span></span><span>),</span> <span id="x1-93053r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor1994"><span>.</span></span><span>keras</span><span id="textcolor1995"><span>.</span></span><span>layers</span><span id="textcolor1996"><span>.</span></span><span>Dense(</span><span id="textcolor1997"><span>10</span></span><span>)</span> <span id="x1-93055r17"></span> </code>
<code><span>Â </span><span>Â ])</span> <span id="x1-93057r18"></span> </code>
<code><span id="x1-93059r19"></span></code>
<code><span>Â </span><span>Â </span><span id="textcolor1998"><span>return</span></span><span>Â model</span> <span id="x1-93061r20"></span> </code>
<code></code></pre>
<p>We also create another helper function that compiles the model for us, using <code>Adam</code> as our optimizer and a categorical cross-entropy loss:</p>
<pre id="fancyvrb80" class="fancyvrb"><span id="x1-93070r1"></span> 
<code><span id="textcolor1999"><span>def</span></span><span>Â </span><span id="textcolor2000"><span>compile_model</span></span><span>(model):</span> <span id="x1-93072r2"></span> </code>
<code><span>Â </span><span>Â model</span><span id="textcolor2001"><span>.</span></span><span>compile(optimizer</span><span id="textcolor2002"><span>=</span></span><span id="textcolor2003"><span>'adam'</span></span><span>,</span> <span id="x1-93074r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â loss</span><span id="textcolor2004"><span>=</span></span><span>tf</span><span id="textcolor2005"><span>.</span></span><span>keras</span><span id="textcolor2006"><span>.</span></span><span>losses</span><span id="textcolor2007"><span>.</span></span><span>SparseCategoricalCrossentropy(from_logits</span><span id="textcolor2008"><span>=</span></span><span id="textcolor2009"><span>True</span></span><span>),</span> <span id="x1-93076r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â metrics</span><span id="textcolor2010"><span>=</span></span><span>[</span><span id="textcolor2011"><span>'accuracy'</span></span><span>])</span> <span id="x1-93078r5"></span> </code>
<code><span>Â </span><span>Â </span><span id="textcolor2012"><span>return</span></span><span>Â model</span> <span id="x1-93080r6"></span> </code>
<code></code></pre>
</section>
<section id="step-4-training" class="level4 likesubsubsectionHead" data-number="11.3.3.4">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.3.3.4"><span id="x1-940003"></span>Step 4: Training</h4>
<p>We then train three different networks on the <span id="dx1-94001"></span>same dataset. Since the network weights are initialized at random, this will result in three different models. You will see that the training accuracy varies slightly between models:</p>
<pre id="fancyvrb81" class="fancyvrb"><span id="x1-94010r1"></span> 
<code><span>deep_ensemble</span><span>Â </span><span id="textcolor2013"><span>=</span></span><span>Â []</span> <span id="x1-94012r2"></span> </code>
<code><span id="textcolor2014"><span>for</span></span><span>Â ind</span><span>Â </span><span id="textcolor2015"><span>in</span></span><span>Â </span><span id="textcolor2016"><span>range</span></span><span>(ENSEMBLE_MEMBERS):</span> <span id="x1-94014r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â model</span><span>Â </span><span id="textcolor2017"><span>=</span></span><span>Â build_model()</span> <span id="x1-94016r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â model</span><span>Â </span><span id="textcolor2018"><span>=</span></span><span>Â compile_model(model)</span> <span id="x1-94018r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2019"><span>print</span></span><span>(</span><span id="textcolor2020"><span>f</span></span><span id="textcolor2021"><span>"Train</span><span>Â model</span><span>Â </span></span><span id="textcolor2022"><span>{</span></span><span>ind</span><span id="textcolor2023"><span>:</span></span><span id="textcolor2024"><span>02</span></span><span id="textcolor2025"><span>}</span></span><span id="textcolor2026"><span>"</span></span><span>)</span> <span id="x1-94020r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â model</span><span id="textcolor2027"><span>.</span></span><span>fit(train_images,</span><span>Â train_labels,</span><span>Â epochs</span><span id="textcolor2028"><span>=</span></span><span id="textcolor2029"><span>10</span></span><span>)</span> <span id="x1-94022r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â deep_ensemble</span><span id="textcolor2030"><span>.</span></span><span>append(model)</span></code></pre>
</section>
<section id="step-5-inference-1" class="level4 likesubsubsectionHead" data-number="11.3.3.5">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.3.3.5"><span id="x1-950003"></span>Step 5: Inference</h4>
<p>We can then perform inference and obtain the <span id="dx1-95001"></span>Predictions for each of the models for all images in the test split. We can also take the mean across the predictions of the three models, which will give us one prediction vector per image:</p>
<pre id="fancyvrb82" class="fancyvrb"><span id="x1-95011r1"></span> 
<code><span id="textcolor2031"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â get</span><span class="cmitt-10x-x-109">Â logit</span><span class="cmitt-10x-x-109">Â predictions</span><span class="cmitt-10x-x-109">Â for</span><span class="cmitt-10x-x-109">Â all</span><span class="cmitt-10x-x-109">Â three</span><span class="cmitt-10x-x-109">Â models</span><span class="cmitt-10x-x-109">Â for</span><span class="cmitt-10x-x-109">Â images</span><span class="cmitt-10x-x-109">Â in</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â test</span><span class="cmitt-10x-x-109">Â split</span></span> <span id="x1-95013r2"></span> </code>
<code><span>ensemble_logit_predictions</span><span>Â </span><span id="textcolor2032"><span>=</span></span><span>Â [model(test_images)</span><span>Â </span><span id="textcolor2033"><span>for</span></span><span>Â model</span><span>Â </span><span id="textcolor2034"><span>in</span></span><span>Â deep_ensemble]</span> <span id="x1-95015r3"></span> </code>
<code><span id="textcolor2035"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â convert</span><span class="cmitt-10x-x-109">Â logit</span><span class="cmitt-10x-x-109">Â predictions</span><span class="cmitt-10x-x-109">Â to</span><span class="cmitt-10x-x-109">Â softmax</span></span> <span id="x1-95017r4"></span> </code>
<code><span>ensemble_softmax_predictions</span><span>Â </span><span id="textcolor2036"><span>=</span></span><span>Â [</span> <span id="x1-95019r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor2037"><span>.</span></span><span>nn</span><span id="textcolor2038"><span>.</span></span><span>softmax(logits,</span><span>Â axis</span><span id="textcolor2039"><span>=-</span></span><span id="textcolor2040"><span>1</span></span><span>)</span><span>Â </span><span id="textcolor2041"><span>for</span></span><span>Â logits</span><span>Â </span><span id="textcolor2042"><span>in</span></span><span>Â ensemble_logit_predictions]</span> <span id="x1-95021r6"></span> </code>
<code><span id="x1-95023r7"></span></code>
<code><span id="textcolor2043"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â take</span><span class="cmitt-10x-x-109">Â mean</span><span class="cmitt-10x-x-109">Â across</span><span class="cmitt-10x-x-109">Â models,</span><span class="cmitt-10x-x-109">Â this</span><span class="cmitt-10x-x-109">Â will</span><span class="cmitt-10x-x-109">Â result</span><span class="cmitt-10x-x-109">Â in</span><span class="cmitt-10x-x-109">Â one</span><span class="cmitt-10x-x-109">Â prediction</span><span class="cmitt-10x-x-109">Â vector</span><span class="cmitt-10x-x-109">Â per</span><span class="cmitt-10x-x-109">Â image</span></span> <span id="x1-95025r8"></span> </code>
<code><span>ensemble_predictions</span><span>Â </span><span id="textcolor2044"><span>=</span></span><span>Â tf</span><span id="textcolor2045"><span>.</span></span><span>reduce_mean(ensemble_softmax_predictions,</span><span>Â axis</span><span id="textcolor2046"><span>=</span></span><span id="textcolor2047"><span>0</span></span><span>)</span></code></pre>
<p>Thatâ€™s it. We have trained an ensemble of networks and performed inference. Given that we have several predictions per image now, we can also look at images where the three models disagree.</p>
<p>Letâ€™s, for example, find the image with the highest disagreement and visualize it:</p>
<pre id="fancyvrb83" class="fancyvrb"><span id="x1-95054r1"></span> 
<code><span id="textcolor2048"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â calculate</span><span class="cmitt-10x-x-109">Â variance</span><span class="cmitt-10x-x-109">Â across</span><span class="cmitt-10x-x-109">Â model</span><span class="cmitt-10x-x-109">Â predictions</span></span> <span id="x1-95056r2"></span> </code>
<code><span>ensemble_std</span><span>Â </span><span id="textcolor2049"><span>=</span></span><span>Â tf</span><span id="textcolor2050"><span>.</span></span><span>reduce_mean(</span> <span id="x1-95058r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor2051"><span>.</span></span><span>math</span><span id="textcolor2052"><span>.</span></span><span>reduce_variance(ensemble_softmax_predictions,</span><span>Â axis</span><span id="textcolor2053"><span>=</span></span><span id="textcolor2054"><span>0</span></span><span>),</span> <span id="x1-95060r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â axis</span><span id="textcolor2055"><span>=</span></span><span id="textcolor2056"><span>1</span></span><span>)</span> <span id="x1-95062r5"></span> </code>
<code><span id="textcolor2057"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â find</span><span class="cmitt-10x-x-109">Â index</span><span class="cmitt-10x-x-109">Â of</span><span class="cmitt-10x-x-109">Â test</span><span class="cmitt-10x-x-109">Â image</span><span class="cmitt-10x-x-109">Â with</span><span class="cmitt-10x-x-109">Â highest</span><span class="cmitt-10x-x-109">Â variance</span><span class="cmitt-10x-x-109">Â across</span><span class="cmitt-10x-x-109">Â predictions</span></span> <span id="x1-95064r6"></span> </code>
<code><span>ind_disagreement</span><span>Â </span><span id="textcolor2058"><span>=</span></span><span>Â np</span><span id="textcolor2059"><span>.</span></span><span>argmax(ensemble_std)</span> <span id="x1-95066r7"></span> </code>
<code><span id="x1-95068r8"></span></code>
<code><span id="textcolor2060"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â get</span><span class="cmitt-10x-x-109">Â predictions</span><span class="cmitt-10x-x-109">Â per</span><span class="cmitt-10x-x-109">Â model</span><span class="cmitt-10x-x-109">Â for</span><span class="cmitt-10x-x-109">Â test</span><span class="cmitt-10x-x-109">Â image</span><span class="cmitt-10x-x-109">Â with</span><span class="cmitt-10x-x-109">Â highest</span><span class="cmitt-10x-x-109">Â variance</span></span> <span id="x1-95070r9"></span> </code>
<code><span>ensemble_disagreement</span><span>Â </span><span id="textcolor2061"><span>=</span></span><span>Â []</span> <span id="x1-95072r10"></span> </code>
<code><span id="textcolor2062"><span>for</span></span><span>Â ind</span><span>Â </span><span id="textcolor2063"><span>in</span></span><span>Â </span><span id="textcolor2064"><span>range</span></span><span>(ENSEMBLE_MEMBERS):</span> <span id="x1-95074r11"></span> </code>
<code><span>Â </span><span>Â model_prediction</span><span>Â </span><span id="textcolor2065"><span>=</span></span><span>Â np</span><span id="textcolor2066"><span>.</span></span><span>argmax(ensemble_softmax_predictions[ind][ind_disagreement])</span> <span id="x1-95076r12"></span> </code>
<code><span>Â </span><span>Â ensemble_disagreement</span><span id="textcolor2067"><span>.</span></span><span>append(model_prediction)</span> <span id="x1-95078r13"></span> </code>
<code><span id="textcolor2068"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â get</span><span class="cmitt-10x-x-109">Â class</span><span class="cmitt-10x-x-109">Â predictions</span></span> <span id="x1-95080r14"></span> </code>
<code><span>predicted_classes</span><span>Â </span><span id="textcolor2069"><span>=</span></span><span>Â [CLASS_NAMES[ind]</span><span>Â </span><span id="textcolor2070"><span>for</span></span><span>Â ind</span><span>Â </span><span id="textcolor2071"><span>in</span></span><span>Â ensemble_disagreement]</span> <span id="x1-95082r15"></span> </code>
<code><span id="x1-95084r16"></span></code>
<code><span id="textcolor2072"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â define</span><span class="cmitt-10x-x-109">Â image</span><span class="cmitt-10x-x-109">Â caption</span></span> <span id="x1-95086r17"></span> </code>
<code><span>image_caption</span><span>Â </span><span id="textcolor2073"><span>=</span></span><span>Â \</span> <span id="x1-95088r18"></span> </code>
<code><span>Â </span><span>Â </span><span id="textcolor2074"><span>f</span></span><span id="textcolor2075"><span>"Network</span><span>Â 1:</span><span>Â </span></span><span id="textcolor2076"><span>{</span></span><span>predicted_classes[</span><span id="textcolor2077"><span>0</span></span><span>]</span><span id="textcolor2078"><span>}</span></span><span id="textcolor2079"><span>\n</span></span><span id="textcolor2080"><span>"</span></span><span>Â </span><span id="textcolor2081"><span>+</span></span><span>Â \</span> <span id="x1-95090r19"></span> </code>
<code><span>Â </span><span>Â </span><span id="textcolor2082"><span>f</span></span><span id="textcolor2083"><span>"Network</span><span>Â 2:</span><span>Â </span></span><span id="textcolor2084"><span>{</span></span><span>predicted_classes[</span><span id="textcolor2085"><span>1</span></span><span>]</span><span id="textcolor2086"><span>}</span></span><span id="textcolor2087"><span>\n</span></span><span id="textcolor2088"><span>"</span></span><span>Â </span><span id="textcolor2089"><span>+</span></span><span>Â \</span> <span id="x1-95092r20"></span> </code>
<code><span>Â </span><span>Â </span><span id="textcolor2090"><span>f</span></span><span id="textcolor2091"><span>"Network</span><span>Â 3:</span><span>Â </span></span><span id="textcolor2092"><span>{</span></span><span>predicted_classes[</span><span id="textcolor2093"><span>2</span></span><span>]</span><span id="textcolor2094"><span>}</span></span><span id="textcolor2095"><span>\n</span></span><span id="textcolor2096"><span>"</span></span> <span id="x1-95094r21"></span> </code>
<code><span id="x1-95096r22"></span></code>
<code><span id="textcolor2097"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â visualise</span><span class="cmitt-10x-x-109">Â image</span><span class="cmitt-10x-x-109">Â and</span><span class="cmitt-10x-x-109">Â predictions</span></span> <span id="x1-95098r23"></span> </code>
<code><span>plt</span><span id="textcolor2098"><span>.</span></span><span>figure()</span> <span id="x1-95100r24"></span> </code>
<code><span>plt</span><span id="textcolor2099"><span>.</span></span><span>title(</span><span id="textcolor2100"><span>f</span></span><span id="textcolor2101"><span>"Correct</span><span>Â class:</span><span>Â </span></span><span id="textcolor2102"><span>{</span></span><span>CLASS_NAMES[test_labels[ind_disagreement]]</span><span id="textcolor2103"><span>}</span></span><span id="textcolor2104"><span>"</span></span><span>)</span> <span id="x1-95102r25"></span> </code>
<code><span>plt</span><span id="textcolor2105"><span>.</span></span><span>imshow(test_images[ind_disagreement],</span><span>Â cmap</span><span id="textcolor2106"><span>=</span></span><span>plt</span><span id="textcolor2107"><span>.</span></span><span>cm</span><span id="textcolor2108"><span>.</span></span><span>binary)</span> <span id="x1-95104r26"></span> </code>
<code><span>plt</span><span id="textcolor2109"><span>.</span></span><span>xlabel(image_caption)</span> <span id="x1-95106r27"></span> </code>
<code><span>plt</span><span id="textcolor2110"><span>.</span></span><span>show()</span></code></pre>
<p>Looking at the image in <em>Figure</em> <a href="#x1-95107r3"><em>6.3</em></a>, even for a human it is hard to tell whether there is a t-shirt, shirt, or bag in the image:</p>
<div class="IMG---Figure">
<img src="../media/file143.png" alt="PIC"/> <span id="x1-95107r3"></span> <span id="x1-95108"></span></div>
<p class="IMG---Caption">FigureÂ 6.3: Image with highest variance among ensemble predictions. The correct ground truth label is â€t-shirt,â€ but it is hard to tell, even for a human 
</p>
<p>While weâ€™ve seen that deep ensembles have several <span id="dx1-95109"></span>favorable qualities, they are not without limitations. In the next section, weâ€™ll explore what kinds of things we may wish to bear in mind when considering deep ensembles. <span id="x1-95110r154"></span></p>
</section>
</section>
<section id="practical-limitations-of-deep-ensembles" class="level3 subsectionHead" data-number="11.3.4">
<h3 class="subsectionHead" data-number="11.3.4" id="sigil_toc_id_71"><span class="titlemark">6.3.4 </span> <span id="x1-960004"></span>Practical limitations of deep ensembles</h3>
<p>Some practical limitations of ensembles become obvious when taking them from the research environment to <span id="dx1-96001"></span>production at scale. We know that, in theory, the predictive performance and the uncertainty estimate of an ensemble is expected to improve as we add more ensemble members. However, there is a cost of adding more ensemble members as the memory footprint and inference cost of ensembles increases linearly with the number of ensemble members. This can make deploying ensembles in a production setting a costly choice. For every NN that we add to the ensemble, we will need to store an extra set of network weights, which significantly increases memory requirements. Equally, for every network, we will also need to run an additional forward pass during inference. Even though the inferences of different networks can be run in parallel, and the impact on inference time can therefore be mitigated, such an approach will still require more compute resources than single models. As more compute resources tend to translate to higher costs, the decision of using an ensemble versus a single model will need to trade off the benefits of better performance and uncertainty estimation with the increase in cost.</p>
<p>Recent research has tried to address or mitigate these practical limitations. In an approach called <span id="dx1-96002"></span>BatchEnsembles (<span class="cite">[<strong>?</strong>]</span>), for example, all ensemble members share one underlying weight matrix. The final weight matrix for each ensemble member is obtained by element-wise multiplication of this shared weight matrix with a rank-one matrix that is unique to each ensemble member. This reduces the number of parameters that need to be stored for each additional ensemble member and thus reduces memory footprint. The ensembleâ€™s computational cost is also reduced because the BatchEnsembles can exploit vectorization, and the output for all <span id="dx1-96003"></span>ensemble members can be computed in a single forward pass. In a different approach, called <strong>multi-input/multi-output</strong> <strong>processing</strong> (<strong>MIMO</strong>; <span class="cite">[<strong>?</strong>]</span>), a single network is encouraged to learn several independent sub-networks. During training, multiple inputs are passed along with multiple, correspondingly labeled outputs. The network will, for example, be presented with three images: of a dog, a cat and a chicken. Corresponding output labels are passed and the network will need to learn to predict â€dogâ€ on its first output node, â€catâ€ on its second output node, and â€chickenâ€ on its third. During inference, one single image will be repeated three times and the MIMO ensemble will produce three different predictions (one on each output node). As a result, the memory footprint and computational cost of the MIMO approach is almost as little as that of a single neural network, while still providing all the benefits of an ensemble. <span id="x1-96004r150"></span></p>
</section>
</section>
<section id="exploring-neural-network-augmentation-with-bayesian-last-layer-methods" class="level2 sectionHead" data-number="11.4">
<h2 class="sectionHead" data-number="11.4" id="sigil_toc_id_72"><span class="titlemark">6.4 </span> <span id="x1-970004"></span>Exploring neural network augmentation with Bayesian last-layer methods</h2>
<p>Through the course of <a href="CH5.xhtml#x1-600005"><em>ChapterÂ 5</em></a>, <a href="CH5.xhtml#x1-600005"><em>Principled Approaches for Bayesian Deep</em> <em>Learning</em></a> and <a href="#x1-820006"><em>ChapterÂ 6</em></a>, <a href="#x1-820006"><em>Using the Standard Toolbox for Bayesian Deep</em> <em>Learning</em></a>, weâ€™ve <span id="dx1-97001"></span>explored a variety of methods for Bayesian inference with DNNs. These methods have incorporated some form of uncertainty information at every layer, whether through the use of <span id="dx1-97002"></span>explicitly probabilistic means or via ensemble-based or dropout-based approximations. These methods have certain advantages. Their consistent Bayesian (or, more accurately, approximately Bayesian) mechanics mean that they are consistent: the same principles are applied at each layer, both in terms of network architecture and update rules. This makes them easier to justify from a theoretical standpoint, as we know that any theoretical guarantees apply at each layer. In addition to this, it means that we have the benefit of being able to access uncertainties at every level: we can exploit embeddings in these networks just as we exploit embeddings in standard deep learning models, and weâ€™ll have access to uncertainties along with those embeddings.</p>
<p>However, these networks also come with some drawbacks. As weâ€™ve seen, methods such as PBP and BBB have more complicated mechanics, which makes them more difficult to apply to more sophisticated neural network architectures. The topics earlier in this chapter demonstrate that we can get around this by using MC dropout or deep ensembles, but they increase our overheads in terms of computation and/or memory footprint. This is where <strong>Bayesian Last-Layer</strong> (<strong>BLL</strong>) methods (see <em>Figure</em> <a href="#x1-97005r4"><em>6.4</em></a>) come in. This class of methods gives us both the flexibility of using any NN architecture, while also being more computationally and <span id="dx1-97003"></span>memory efficient than MC dropout or <span id="dx1-97004"></span>deep ensembles.</p>
<div class="IMG---Figure">
<img src="../media/file144.png" alt="PIC"/> <span id="x1-97005r4"></span> <span id="x1-97006"></span></div>
<p class="IMG---Caption">FigureÂ 6.4: Vanilla NN compared to a BLL network 
</p>
<p>As youâ€™ve probably guessed, the fundamental principle behind BLL methods is to estimate uncertainties only at the last-layer. But what you may not have guessed is why this is possible. Deep learningâ€™s success is due to the non-linear nature of NNs: the successive layers of non-linear transformations enable them to learn rich lower-dimensional representations of high-dimensional data. However, this non-linearity makes model uncertainty estimation difficult. Closed-form solutions for model uncertainty estimation are available for a variety of linear models, but unfortunately, this isnâ€™t the case for our highly non-linear DNNs. So, what can we do?</p>
<p>Well, fortunately for us, the representations learned by the DNNs can also serve as inputs to simpler linear models. In this way, we let the DNN do the heavy lifting: condensing our high-dimensional input space down to a task-specific low-dimensional representation. Because of this, the penultimate layer in the NN is far easier to deal with; after all, in most cases our output is simply some linear transformation of this layer. This means we can apply a linear model to this layer, which in turn means we can apply closed-form solutions for model uncertainty estimation.</p>
<p>We can make use of other last-layer approaches too; recent work has demonstrated that MC dropout is effective when applied only at the last layer. While this still requires multiple forward passes, these forward passes only need to be done for a single layer, making them much more computationally efficient, particularly for larger models. <span id="x1-97007r161"></span></p>
<section id="last-layer-methods-for-bayesian-inference" class="level3 subsectionHead" data-number="11.4.1">
<h3 class="subsectionHead" data-number="11.4.1" id="sigil_toc_id_73"><span class="titlemark">6.4.1 </span> <span id="x1-980001"></span>Last-layer methods for Bayesian inference</h3>
<p>The method proposed by Jasper Snoek et al. in their 2015 paper, <em>Scalable</em> <em>Bayesian Optimization Using Deep Neural Networks</em>, introduces the concept of using a post-hoc Bayesian linear regressor to obtain model uncertainties for DNNs. This method was devised as a way of achieving Gaussian Process-like high-quality uncertainties with improved scalability.</p>
<p>The method first involves <span id="dx1-98001"></span>training a NN on some data <em>X</em> and targets <strong>y</strong>. This training phase trains a linear output layer, <strong>z</strong><sub><em>i</em></sub>, resulting in a network that produces point estimates (typical of a standard DNN). We then take the penultimate layer (or the last hidden layer), <strong>z</strong><sub><em>i</em><span class="cmsy-8">âˆ’</span><span class="cmr-8">1</span></sub>, as our set of basis functions. From here, itâ€™s simply a case of replacing our final layer with a Bayesian linear regressor. Now, instead of our point estimates, our network will produce a predictive mean and variance. For further details on this method and adaptive basis regression, we point the reader to Jasper Snoek et al.â€™s paper, and to Christopher Bishopâ€™s <em>Pattern Recognition and Machine</em> <em>Learning</em>.</p>
<p>Now, letâ€™s see how we achieve this in code.</p>
<section id="step-1-creating-and-training-our-base-model" class="level4 likesubsubsectionHead" data-number="11.4.1.1">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.4.1.1"><span id="x1-990001"></span>Step 1: Creating and training our base model</h4>
<p>First, we set up <span id="dx1-99001"></span>and train our network:</p>
<pre id="fancyvrb84" class="fancyvrb"><span id="x1-99041r1"></span> 
<code><span id="textcolor2111"><span>from</span></span><span>Â </span><span id="textcolor2112"><span>tensorflow.keras</span></span><span>Â </span><span id="textcolor2113"><span>import</span></span><span>Â Model,</span><span>Â Sequential,</span><span>Â layers,</span><span>Â optimizers,</span><span>Â metrics,</span><span>Â losses</span> <span id="x1-99043r2"></span> </code>
<code><span id="textcolor2114"><span>import</span></span><span>Â </span><span id="textcolor2115"><span>tensorflow</span></span><span>Â </span><span id="textcolor2116"><span>as</span></span><span>Â </span><span id="textcolor2117"><span>tf</span></span> <span id="x1-99045r3"></span> </code>
<code><span id="textcolor2118"><span>import</span></span><span>Â </span><span id="textcolor2119"><span>tensorflow_probability</span></span><span>Â </span><span id="textcolor2120"><span>as</span></span><span>Â </span><span id="textcolor2121"><span>tfp</span></span> <span id="x1-99047r4"></span> </code>
<code><span id="textcolor2122"><span>from</span></span><span>Â </span><span id="textcolor2123"><span>sklearn.datasets</span></span><span>Â </span><span id="textcolor2124"><span>import</span></span><span>Â load_boston</span> <span id="x1-99049r5"></span> </code>
<code><span id="textcolor2125"><span>from</span></span><span>Â </span><span id="textcolor2126"><span>sklearn.model_selection</span></span><span>Â </span><span id="textcolor2127"><span>import</span></span><span>Â train_test_split</span> <span id="x1-99051r6"></span> </code>
<code><span id="textcolor2128"><span>from</span></span><span>Â </span><span id="textcolor2129"><span>sklearn.preprocessing</span></span><span>Â </span><span id="textcolor2130"><span>import</span></span><span>Â StandardScaler</span> <span id="x1-99053r7"></span> </code>
<code><span id="textcolor2131"><span>from</span></span><span>Â </span><span id="textcolor2132"><span>sklearn.metrics</span></span><span>Â </span><span id="textcolor2133"><span>import</span></span><span>Â mean_squared_error</span> <span id="x1-99055r8"></span> </code>
<code><span id="textcolor2134"><span>import</span></span><span>Â </span><span id="textcolor2135"><span>pandas</span></span><span>Â </span><span id="textcolor2136"><span>as</span></span><span>Â </span><span id="textcolor2137"><span>pd</span></span> <span id="x1-99057r9"></span> </code>
<code><span id="textcolor2138"><span>import</span></span><span>Â </span><span id="textcolor2139"><span>numpy</span></span><span>Â </span><span id="textcolor2140"><span>as</span></span><span>Â </span><span id="textcolor2141"><span>np</span></span> <span id="x1-99059r10"></span> </code>
<code><span id="x1-99061r11"></span></code>
<code><span>seed</span><span>Â </span><span id="textcolor2142"><span>=</span></span><span>Â </span><span id="textcolor2143"><span>213</span></span> <span id="x1-99063r12"></span> </code>
<code><span>np</span><span id="textcolor2144"><span>.</span></span><span>random</span><span id="textcolor2145"><span>.</span></span><span>seed(seed)</span> <span id="x1-99065r13"></span> </code>
<code><span>tf</span><span id="textcolor2146"><span>.</span></span><span>random</span><span id="textcolor2147"><span>.</span></span><span>set_seed(seed)</span> <span id="x1-99067r14"></span> </code>
<code><span>dtype</span><span>Â </span><span id="textcolor2148"><span>=</span></span><span>Â tf</span><span id="textcolor2149"><span>.</span></span><span>float32</span> <span id="x1-99069r15"></span> </code>
<code><span id="x1-99071r16"></span></code>
<code><span>boston</span><span>Â </span><span id="textcolor2150"><span>=</span></span><span>Â load_boston()</span> <span id="x1-99073r17"></span> </code>
<code><span>data</span><span>Â </span><span id="textcolor2151"><span>=</span></span><span>Â boston</span><span id="textcolor2152"><span>.</span></span><span>data</span> <span id="x1-99075r18"></span> </code>
<code><span>targets</span><span>Â </span><span id="textcolor2153"><span>=</span></span><span>Â boston</span><span id="textcolor2154"><span>.</span></span><span>target</span> <span id="x1-99077r19"></span> </code>
<code><span id="x1-99079r20"></span></code>
<code><span>X_train,</span><span>Â X_test,</span><span>Â y_train,</span><span>Â y_test</span><span>Â </span><span id="textcolor2155"><span>=</span></span><span>Â train_test_split(data,</span><span>Â targets,</span><span>Â test_size</span><span id="textcolor2156"><span>=</span></span><span id="textcolor2157"><span>0.2</span></span><span>)</span> <span id="x1-99081r21"></span> </code>
<code><span id="x1-99083r22"></span></code>
<code><span id="textcolor2158"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Scale</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â inputs</span></span> <span id="x1-99085r23"></span> </code>
<code><span>scaler</span><span>Â </span><span id="textcolor2159"><span>=</span></span><span>Â StandardScaler()</span> <span id="x1-99087r24"></span> </code>
<code><span>X_train</span><span>Â </span><span id="textcolor2160"><span>=</span></span><span>Â scaler</span><span id="textcolor2161"><span>.</span></span><span>fit_transform(X_train)</span> <span id="x1-99089r25"></span> </code>
<code><span>X_test</span><span>Â </span><span id="textcolor2162"><span>=</span></span><span>Â scaler</span><span id="textcolor2163"><span>.</span></span><span>transform(X_test)</span> <span id="x1-99091r26"></span> </code>
<code><span id="x1-99093r27"></span></code>
<code><span>model</span><span>Â </span><span id="textcolor2164"><span>=</span></span><span>Â Sequential()</span> <span id="x1-99095r28"></span> </code>
<code><span>model</span><span id="textcolor2165"><span>.</span></span><span>add(layers</span><span id="textcolor2166"><span>.</span></span><span>Dense(</span><span id="textcolor2167"><span>20</span></span><span>,</span><span>Â input_dim</span><span id="textcolor2168"><span>=</span></span><span id="textcolor2169"><span>13</span></span><span>,</span><span>Â activation</span><span id="textcolor2170"><span>=</span></span><span id="textcolor2171"><span>'relu'</span></span><span>,</span><span>Â name</span><span id="textcolor2172"><span>=</span></span><span id="textcolor2173"><span>'layer_1'</span></span><span>))</span> <span id="x1-99097r29"></span> </code>
<code><span>model</span><span id="textcolor2174"><span>.</span></span><span>add(layers</span><span id="textcolor2175"><span>.</span></span><span>Dense(</span><span id="textcolor2176"><span>8</span></span><span>,</span><span>Â activation</span><span id="textcolor2177"><span>=</span></span><span id="textcolor2178"><span>'relu'</span></span><span>,</span><span>Â name</span><span id="textcolor2179"><span>=</span></span><span id="textcolor2180"><span>'layer_2'</span></span><span>))</span> <span id="x1-99099r30"></span> </code>
<code><span>model</span><span id="textcolor2181"><span>.</span></span><span>add(layers</span><span id="textcolor2182"><span>.</span></span><span>Dense(</span><span id="textcolor2183"><span>1</span></span><span>,</span><span>Â activation</span><span id="textcolor2184"><span>=</span></span><span id="textcolor2185"><span>'relu'</span></span><span>,</span><span>Â name</span><span id="textcolor2186"><span>=</span></span><span id="textcolor2187"><span>'layer_3'</span></span><span>))</span> <span id="x1-99101r31"></span> </code>
<code><span id="x1-99103r32"></span></code>
<code><span>model</span><span id="textcolor2188"><span>.</span></span><span>compile(optimizer</span><span id="textcolor2189"><span>=</span></span><span>optimizers</span><span id="textcolor2190"><span>.</span></span><span>Adam(),</span> <span id="x1-99105r33"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â loss</span><span id="textcolor2191"><span>=</span></span><span>losses</span><span id="textcolor2192"><span>.</span></span><span>MeanSquaredError(),</span> <span id="x1-99107r34"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â metrics</span><span id="textcolor2193"><span>=</span></span><span>[metrics</span><span id="textcolor2194"><span>.</span></span><span>RootMeanSquaredError()],)</span> <span id="x1-99109r35"></span> </code>
<code><span id="x1-99111r36"></span></code>
<code><span>num_epochs</span><span>Â </span><span id="textcolor2195"><span>=</span></span><span>Â </span><span id="textcolor2196"><span>200</span></span> <span id="x1-99113r37"></span> </code>
<code><span>model</span><span id="textcolor2197"><span>.</span></span><span>fit(X_train,</span><span>Â y_train,</span><span>Â epochs</span><span id="textcolor2198"><span>=</span></span><span>num_epochs)</span> <span id="x1-99115r38"></span> </code>
<code><span>mse,</span><span>Â rmse</span><span>Â </span><span id="textcolor2199"><span>=</span></span><span>Â model</span><span id="textcolor2200"><span>.</span></span><span>evaluate(X_test,</span><span>Â y_test)</span></code></pre>
</section>
<section id="step-2-using-a-neural-network-layer-as-a-basis-function" class="level4 likesubsubsectionHead" data-number="11.4.1.2">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.4.1.2"><span id="x1-1000001"></span>Step 2: Using a neural network layer as a basis function</h4>
<p>Now that we have our base network, we just <span id="dx1-100001"></span>need to access the penultimate layer so that we can feed this as our basis function to our Bayesian regressor. This is easily done using TensorFlowâ€™s high-level API, for example:</p>
<pre id="fancyvrb85" class="fancyvrb"><span id="x1-100005r1"></span> 
<code><span>basis_func</span><span>Â </span><span id="textcolor2201"><span>=</span></span><span>Â Model(inputs</span><span id="textcolor2202"><span>=</span></span><span id="textcolor2203"><span>self</span></span><span id="textcolor2204"><span>.</span></span><span>model</span><span id="textcolor2205"><span>.</span></span><span>input,</span> <span id="x1-100007r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â outputs</span><span id="textcolor2206"><span>=</span></span><span id="textcolor2207"><span>self</span></span><span id="textcolor2208"><span>.</span></span><span>model</span><span id="textcolor2209"><span>.</span></span><span>get_layer(</span><span id="textcolor2210"><span>'layer_2'</span></span><span>)</span><span id="textcolor2211"><span>.</span></span><span>output)</span></code></pre>
<p>This will build a model that will allow us to obtain the output of the second hidden layer by simply calling its <code>predict</code> method:</p>
<pre id="fancyvrb86" class="fancyvrb"><span id="x1-100011r1"></span> 
<code><span>layer_2_output</span><span>Â </span><span id="textcolor2212"><span>=</span></span><span>Â basis_func</span><span id="textcolor2213"><span>.</span></span><span>predict(X_test)</span></code></pre>
<p>This is all thatâ€™s needed to prepare our basis function for passing to our Bayesian linear regressor.</p>
</section>
<section id="step-3-preparing-our-variables-for-bayesian-linear-regression" class="level4 likesubsubsectionHead" data-number="11.4.1.3">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.4.1.3"><span id="x1-1010001"></span>Step 3: Preparing our variables for Bayesian linear regression</h4>
<p>For the Bayesian regressor, we <span id="dx1-101001"></span>assume that our outputs, <em>y</em><sub><em>i</em></sub> <span class="cmsy-10x-x-109">âˆˆ</span> <strong>y</strong>, are conditionally normally distributed according to a linear relationship with our inputs, <strong>x</strong><sub><em>i</em></sub> <span class="cmsy-10x-x-109">âˆˆ </span><em>X</em>:</p>
<div class="math-display">
<img src="../media/file145.jpg" class="math-display" alt="yi = ğ’© (Î± + xâŠºiÎ², Ïƒ2) "/>
</div>
<p>Here, <em>Î±</em> is our bias term, <em>Î²</em> are our model coefficients, and <em>Ïƒ</em><sup><span class="cmr-8">2</span></sup> is the variance associated with our predictions. Weâ€™ll also make some prior assumptions about these parameters, namely:</p>
<div class="math-display">
<img src="../media/file146.jpg" class="math-display" alt="Î± â‰ˆ ğ’© (0,1) "/>
</div>
<div class="math-display">
<img src="../media/file147.jpg" class="math-display" alt="Î² â‰ˆ ğ’© (0,1) "/>
</div>
<div class="math-display">
<img src="../media/file148.jpg" class="math-display" alt="Ïƒ2 â‰ˆ |ğ’© (0,1)| "/>
</div>
<p>Note that equation 6.6 denotes the half-normal of a Gaussian distribution. To wrap up the Bayesian regressor in such a way that itâ€™s easy (and practical) to integrate it with our Keras model, weâ€™ll create a <code>BayesianLastLayer </code>class. This class will use the <span id="dx1-101002"></span>TensorFlow Probability library to allow us to implement the probability distributions and sampling functions weâ€™ll need for our Bayesian regressor. Letâ€™s walk through the various components of our class:</p>
<pre id="fancyvrb87" class="fancyvrb"><span id="x1-101030r1"></span> 
<code><span id="x1-101032r2"></span></code>
<code><span id="textcolor2214"><span>class</span></span><span>Â </span><span id="textcolor2215"><span>BayesianLastLayer</span></span><span>():</span> <span id="x1-101034r3"></span> </code>
<code><span id="x1-101036r4"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2216"><span>def</span></span><span>Â </span><span id="textcolor2217"><span>__init__</span></span><span>(</span><span id="textcolor2218"><span>self</span></span><span>,</span> <span id="x1-101038r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â model,</span> <span id="x1-101040r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â basis_layer,</span> <span id="x1-101042r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â n_samples</span><span id="textcolor2219"><span>=</span></span><span id="textcolor2220"><span>1e4</span></span><span>,</span> <span id="x1-101044r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â n_burnin</span><span id="textcolor2221"><span>=</span></span><span id="textcolor2222"><span>5e3</span></span><span>,</span> <span id="x1-101046r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â step_size</span><span id="textcolor2223"><span>=</span></span><span id="textcolor2224"><span>1e-4</span></span><span>,</span> <span id="x1-101048r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â n_leapfrog</span><span id="textcolor2225"><span>=</span></span><span id="textcolor2226"><span>10</span></span><span>,</span> <span id="x1-101050r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â adaptive</span><span id="textcolor2227"><span>=</span></span><span id="textcolor2228"><span>False</span></span><span>):</span> <span id="x1-101052r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2229"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Setting</span><span class="cmitt-10x-x-109">Â up</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â model</span></span> <span id="x1-101054r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2230"><span>self</span></span><span id="textcolor2231"><span>.</span></span><span>model</span><span>Â </span><span id="textcolor2232"><span>=</span></span><span>Â model</span> <span id="x1-101056r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2233"><span>self</span></span><span id="textcolor2234"><span>.</span></span><span>basis_layer</span><span>Â </span><span id="textcolor2235"><span>=</span></span><span>Â basis_layer</span> <span id="x1-101058r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2236"><span>self</span></span><span id="textcolor2237"><span>.</span></span><span>initialize_basis_function()</span> <span id="x1-101060r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2238"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â HMC</span><span class="cmitt-10x-x-109">Â Settings</span></span> <span id="x1-101062r17"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2239"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â number</span><span class="cmitt-10x-x-109">Â of</span><span class="cmitt-10x-x-109">Â hmc</span><span class="cmitt-10x-x-109">Â samples</span></span> <span id="x1-101064r18"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2240"><span>self</span></span><span id="textcolor2241"><span>.</span></span><span>n_samples</span><span>Â </span><span id="textcolor2242"><span>=</span></span><span>Â </span><span id="textcolor2243"><span>int</span></span><span>(n_samples)</span> <span id="x1-101066r19"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2244"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â number</span><span class="cmitt-10x-x-109">Â of</span><span class="cmitt-10x-x-109">Â burn-in</span><span class="cmitt-10x-x-109">Â steps</span></span> <span id="x1-101068r20"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2245"><span>self</span></span><span id="textcolor2246"><span>.</span></span><span>n_burnin</span><span>Â </span><span id="textcolor2247"><span>=</span></span><span>Â </span><span id="textcolor2248"><span>int</span></span><span>(n_burnin)</span> <span id="x1-101070r21"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2249"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â HMC</span><span class="cmitt-10x-x-109">Â step</span><span class="cmitt-10x-x-109">Â size</span></span> <span id="x1-101072r22"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2250"><span>self</span></span><span id="textcolor2251"><span>.</span></span><span>step_size</span><span>Â </span><span id="textcolor2252"><span>=</span></span><span>Â step_size</span> <span id="x1-101074r23"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2253"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â HMC</span><span class="cmitt-10x-x-109">Â leapfrog</span><span class="cmitt-10x-x-109">Â steps</span></span> <span id="x1-101076r24"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2254"><span>self</span></span><span id="textcolor2255"><span>.</span></span><span>n_leapfrog</span><span>Â </span><span id="textcolor2256"><span>=</span></span><span>Â n_leapfrog</span> <span id="x1-101078r25"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2257"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â whether</span><span class="cmitt-10x-x-109">Â to</span><span class="cmitt-10x-x-109">Â be</span><span class="cmitt-10x-x-109">Â adaptive</span><span class="cmitt-10x-x-109">Â or</span><span class="cmitt-10x-x-109">Â not</span></span> <span id="x1-101080r26"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2258"><span>self</span></span><span id="textcolor2259"><span>.</span></span><span>adaptive</span><span>Â </span><span id="textcolor2260"><span>=</span></span><span>Â adaptive</span></code></pre>
<p>As we see here, our class requires at least two arguments at instantiation: <code>model</code>, which is our Keras model, and <code>basis</code><code>_layer</code>, which is the layer output we wanted to feed to our Bayesian regressor. The following arguments are all parameters for the <strong>Hamiltonian Monte-Carlo</strong> (<strong>HMC</strong>) sampling for which we define some default values. These <span id="dx1-101081"></span>values may need to be changed <span id="dx1-101082"></span>depending on the input. For example, for a higher dimensional input (for instance, if youâ€™re using <code>layer</code><code>_1</code>), you may want to further reduce the step size and increase both the number of burn-in steps and the overall number of samples.</p>
</section>
<section id="step-4-connecting-our-basis-function-model" class="level4 likesubsubsectionHead" data-number="11.4.1.4">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.4.1.4"><span id="x1-1020001"></span>Step 4: Connecting our basis function model</h4>
<p>Next, we simply define a few <span id="dx1-102001"></span>functions for creating our basis function model and for obtaining its outputs:</p>
<pre id="fancyvrb88" class="fancyvrb"><span id="x1-102009r1"></span> 
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2261"><span>def</span></span><span>Â </span><span id="textcolor2262"><span>initialize_basis_function</span></span><span>(</span><span id="textcolor2263"><span>self</span></span><span>):</span> <span id="x1-102011r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2264"><span>self</span></span><span id="textcolor2265"><span>.</span></span><span>basis_func</span><span>Â </span><span id="textcolor2266"><span>=</span></span><span>Â Model(inputs</span><span id="textcolor2267"><span>=</span></span><span id="textcolor2268"><span>self</span></span><span id="textcolor2269"><span>.</span></span><span>model</span><span id="textcolor2270"><span>.</span></span><span>input,</span> <span id="x1-102013r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â outputs</span><span id="textcolor2271"><span>=</span></span><span id="textcolor2272"><span>self</span></span><span id="textcolor2273"><span>.</span></span><span>model</span><span id="textcolor2274"><span>.</span></span><span>get_layer(</span><span id="textcolor2275"><span>self</span></span><span id="textcolor2276"><span>.</span></span><span>basis_layer)</span><span id="textcolor2277"><span>.</span></span><span>output)</span> <span id="x1-102015r4"></span> </code>
<code><span id="x1-102017r5"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2278"><span>def</span></span><span>Â </span><span id="textcolor2279"><span>get_basis</span></span><span>(</span><span id="textcolor2280"><span>self</span></span><span>,</span><span>Â X):</span> <span id="x1-102019r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2281"><span>return</span></span><span>Â </span><span id="textcolor2282"><span>self</span></span><span id="textcolor2283"><span>.</span></span><span>basis_func</span><span id="textcolor2284"><span>.</span></span><span>predict(X)</span></code></pre>
</section>
<section id="step-5-creating-a-method-to-fit-our-bayesian-linear-regression-parameters" class="level4 likesubsubsectionHead" data-number="11.4.1.5">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.4.1.5"><span id="x1-1030001"></span>Step 5: Creating a method to fit our Bayesian linear regression parameters</h4>
<p>Now things get a little <span id="dx1-103001"></span>more complicated. We need to define the <code>fit() </code>method, which will use HMC sampling to find our model parameters <em>Î±</em>, <em>Î²</em>, and <em>Ïƒ</em><sup><span class="cmr-8">2</span></sup>. Weâ€™ll provide an overview of what the code is doing here, but for more (hands-on) information on sampling, we direct the reader to <em>Bayesian Analysis with Python</em> by Osvaldo Martin.</p>
<p>Firstly, we define a joint distribution using the priors described in equations 4.3-4.5. Thanks to TensorFlow Probabilityâ€™s <code>distributions </code>module, this is pretty straightforward:</p>
<pre id="fancyvrb89" class="fancyvrb"><span id="x1-103029r1"></span> 
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2285"><span>def</span></span><span>Â </span><span id="textcolor2286"><span>fit</span></span><span>(</span><span id="textcolor2287"><span>self</span></span><span>,</span><span>Â X,</span><span>Â y):</span> <span id="x1-103031r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â X</span><span>Â </span><span id="textcolor2288"><span>=</span></span><span>Â tf</span><span id="textcolor2289"><span>.</span></span><span>convert_to_tensor(</span><span id="textcolor2290"><span>self</span></span><span id="textcolor2291"><span>.</span></span><span>get_basis(X),</span><span>Â dtype</span><span id="textcolor2292"><span>=</span></span><span>dtype)</span> <span id="x1-103033r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â y</span><span>Â </span><span id="textcolor2293"><span>=</span></span><span>Â tf</span><span id="textcolor2294"><span>.</span></span><span>convert_to_tensor(y,</span><span>Â dtype</span><span id="textcolor2295"><span>=</span></span><span>dtype)</span> <span id="x1-103035r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â y</span><span>Â </span><span id="textcolor2296"><span>=</span></span><span>Â tf</span><span id="textcolor2297"><span>.</span></span><span>reshape(y,</span><span>Â (</span><span id="textcolor2298"><span>-</span></span><span id="textcolor2299"><span>1</span></span><span>,</span><span>Â </span><span id="textcolor2300"><span>1</span></span><span>))</span> <span id="x1-103037r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â D</span><span>Â </span><span id="textcolor2301"><span>=</span></span><span>Â X</span><span id="textcolor2302"><span>.</span></span><span>shape[</span><span id="textcolor2303"><span>1</span></span><span>]</span> <span id="x1-103039r6"></span> </code>
<code><span id="x1-103041r7"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2304"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Define</span><span class="cmitt-10x-x-109">Â our</span><span class="cmitt-10x-x-109">Â joint</span><span class="cmitt-10x-x-109">Â distribution</span></span> <span id="x1-103043r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â distribution</span><span>Â </span><span id="textcolor2305"><span>=</span></span><span>Â tfp</span><span id="textcolor2306"><span>.</span></span><span>distributions</span><span id="textcolor2307"><span>.</span></span><span>JointDistributionNamedAutoBatched(</span> <span id="x1-103045r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2308"><span>dict</span></span><span>(</span> <span id="x1-103047r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â sigma</span><span id="textcolor2309"><span>=</span></span><span>tfp</span><span id="textcolor2310"><span>.</span></span><span>distributions</span><span id="textcolor2311"><span>.</span></span><span>HalfNormal(scale</span><span id="textcolor2312"><span>=</span></span><span>tf</span><span id="textcolor2313"><span>.</span></span><span>ones([</span><span id="textcolor2314"><span>1</span></span><span>])),</span> <span id="x1-103049r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â alpha</span><span id="textcolor2315"><span>=</span></span><span>tfp</span><span id="textcolor2316"><span>.</span></span><span>distributions</span><span id="textcolor2317"><span>.</span></span><span>Normal(</span> <span id="x1-103051r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â loc</span><span id="textcolor2318"><span>=</span></span><span>tf</span><span id="textcolor2319"><span>.</span></span><span>zeros([</span><span id="textcolor2320"><span>1</span></span><span>]),</span> <span id="x1-103053r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â scale</span><span id="textcolor2321"><span>=</span></span><span>tf</span><span id="textcolor2322"><span>.</span></span><span>ones([</span><span id="textcolor2323"><span>1</span></span><span>]),</span> <span id="x1-103055r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â ),</span> <span id="x1-103057r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â beta</span><span id="textcolor2324"><span>=</span></span><span>tfp</span><span id="textcolor2325"><span>.</span></span><span>distributions</span><span id="textcolor2326"><span>.</span></span><span>Normal(</span> <span id="x1-103059r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â loc</span><span id="textcolor2327"><span>=</span></span><span>tf</span><span id="textcolor2328"><span>.</span></span><span>zeros([D,</span><span id="textcolor2329"><span>1</span></span><span>]),</span> <span id="x1-103061r17"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â scale</span><span id="textcolor2330"><span>=</span></span><span>tf</span><span id="textcolor2331"><span>.</span></span><span>ones([D,</span><span id="textcolor2332"><span>1</span></span><span>]),</span> <span id="x1-103063r18"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â ),</span> <span id="x1-103065r19"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â y</span><span id="textcolor2333"><span>=</span></span><span id="textcolor2334"><span>lambda</span></span><span>Â beta,</span><span>Â alpha,</span><span>Â sigma:</span> <span id="x1-103067r20"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tfp</span><span id="textcolor2335"><span>.</span></span><span>distributions</span><span id="textcolor2336"><span>.</span></span><span>Normal(</span> <span id="x1-103069r21"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â loc</span><span id="textcolor2337"><span>=</span></span><span>tf</span><span id="textcolor2338"><span>.</span></span><span>linalg</span><span id="textcolor2339"><span>.</span></span><span>matmul(X,</span><span>Â beta)</span><span>Â </span><span id="textcolor2340"><span>+</span></span><span>Â alpha,</span> <span id="x1-103071r22"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â scale</span><span id="textcolor2341"><span>=</span></span><span>sigma</span> <span id="x1-103073r23"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-103075r24"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-103077r25"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-103079r26"></span> </code>
<code><span id="textcolor2342"><span>.</span></span><span>Â </span><span id="textcolor2343"><span>.</span></span><span>Â </span><span id="textcolor2344"><span>.</span></span></code></pre>
<p>We then go on to set up our sampler using TensorFlow Probabilityâ€™s <code>HamiltonianMonteCarlo </code>sampler class. To do this, weâ€™ll need to define our <span id="dx1-103080"></span>target log probability function. The <code>distributions </code>module makes this fairly trivial, but we still need to define a function to feed our model parameters to the distribution objectâ€™s <code>log</code><code>_prob() </code>method (line 28). We can then pass this to the instantiation of <code>hmc</code><code>_kernel</code>:</p>
<pre id="fancyvrb90" class="fancyvrb"><span id="x1-103101r1"></span> 
<code><span id="textcolor2345"><span>.</span></span><span>Â </span><span id="textcolor2346"><span>.</span></span><span>Â </span><span id="textcolor2347"><span>.</span></span> <span id="x1-103103r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2348"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Define</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â log</span><span class="cmitt-10x-x-109">Â probability</span><span class="cmitt-10x-x-109">Â function</span></span> <span id="x1-103105r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2349"><span>def</span></span><span>Â </span><span id="textcolor2350"><span>target_log_prob_fn</span></span><span>(beta,</span><span>Â alpha,</span><span>Â sigma):</span> <span id="x1-103107r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2351"><span>return</span></span><span>Â distribution</span><span id="textcolor2352"><span>.</span></span><span>log_prob(beta</span><span id="textcolor2353"><span>=</span></span><span>beta,</span><span>Â alpha</span><span id="textcolor2354"><span>=</span></span><span>alpha,</span><span>Â sigma</span><span id="textcolor2355"><span>=</span></span><span>sigma,</span><span>Â y</span><span id="textcolor2356"><span>=</span></span><span>y)</span> <span id="x1-103109r5"></span> </code>
<code><span id="x1-103111r6"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2357"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Define</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â HMC</span><span class="cmitt-10x-x-109">Â kernel</span><span class="cmitt-10x-x-109">Â we'll</span><span class="cmitt-10x-x-109">Â be</span><span class="cmitt-10x-x-109">Â using</span><span class="cmitt-10x-x-109">Â for</span><span class="cmitt-10x-x-109">Â sampling</span></span> <span id="x1-103113r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â hmc_kernel</span><span>Â </span><span>Â </span><span id="textcolor2358"><span>=</span></span><span>Â tfp</span><span id="textcolor2359"><span>.</span></span><span>mcmc</span><span id="textcolor2360"><span>.</span></span><span>HamiltonianMonteCarlo(</span> <span id="x1-103115r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â target_log_prob_fn</span><span id="textcolor2361"><span>=</span></span><span>target_log_prob_fn,</span> <span id="x1-103117r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â step_size</span><span id="textcolor2362"><span>=</span></span><span id="textcolor2363"><span>self</span></span><span id="textcolor2364"><span>.</span></span><span>step_size,</span> <span id="x1-103119r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â num_leapfrog_steps</span><span id="textcolor2365"><span>=</span></span><span id="textcolor2366"><span>self</span></span><span id="textcolor2367"><span>.</span></span><span>n_leapfrog</span> <span id="x1-103121r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-103123r12"></span> </code>
<code><span id="x1-103125r13"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2368"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â We</span><span class="cmitt-10x-x-109">Â can</span><span class="cmitt-10x-x-109">Â use</span><span class="cmitt-10x-x-109">Â adaptive</span><span class="cmitt-10x-x-109">Â HMC</span><span class="cmitt-10x-x-109">Â to</span><span class="cmitt-10x-x-109">Â automatically</span><span class="cmitt-10x-x-109">Â adjust</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â kernel</span><span class="cmitt-10x-x-109">Â step</span><span class="cmitt-10x-x-109">Â size</span></span> <span id="x1-103127r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2369"><span>if</span></span><span>Â </span><span id="textcolor2370"><span>self</span></span><span id="textcolor2371"><span>.</span></span><span>adaptive:</span> <span id="x1-103129r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â adaptive_hmc</span><span>Â </span><span id="textcolor2372"><span>=</span></span><span>Â tfp</span><span id="textcolor2373"><span>.</span></span><span>mcmc</span><span id="textcolor2374"><span>.</span></span><span>SimpleStepSizeAdaptation(</span> <span id="x1-103131r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â inner_kernel</span><span>Â </span><span id="textcolor2375"><span>=</span></span><span>Â hmc_kernel,</span> <span id="x1-103133r17"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â num_adaptation_steps</span><span id="textcolor2376"><span>=</span></span><span id="textcolor2377"><span>int</span></span><span>(</span><span id="textcolor2378"><span>self</span></span><span id="textcolor2379"><span>.</span></span><span>n_burnin</span><span>Â </span><span id="textcolor2380"><span>*</span></span><span>Â </span><span id="textcolor2381"><span>0.8</span></span><span>)</span> <span id="x1-103135r18"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-103137r19"></span> </code>
<code><span id="textcolor2382"><span>.</span></span><span>Â </span><span id="textcolor2383"><span>.</span></span><span>Â </span><span id="textcolor2384"><span>.</span></span></code></pre>
<p>Now that things are set up, weâ€™re ready to run our sampler. To do this, we call the <code>mcmc.sample</code><code>_chain() </code>function, passing in our HMC parameters, an initial state for our model parameters, and our HMC sampler. We then run our sampling, which returns <code>states</code>, which comprises our parameter samples, and <code>kernel</code><code>_results</code>, which contains some information about the sampling process. The information we care about here is to do with the proportion of <span id="dx1-103138"></span>accepted samples. If our sampler has run successfully, then weâ€™ll have a good proportion of accepted samples (indicating a high acceptance rate). If it hasnâ€™t been successful, then our acceptance rate will be low (perhaps even 0%!) and we may need to tune our sampler parameters. We print this to the console so that we can keep an eye on the acceptance rate (we wrap the call to <code>sample</code><code>_chain() </code>in a <code>run</code><code>_chain() </code>function so that it can be extended to sampling with multiple chains):</p>
<pre id="fancyvrb91" class="fancyvrb"><span id="x1-103166r1"></span> 
<code><span id="textcolor2385"><span>.</span></span><span>Â </span><span id="textcolor2386"><span>.</span></span><span>Â </span><span id="textcolor2387"><span>.</span></span> <span id="x1-103168r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2388"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â If</span><span class="cmitt-10x-x-109">Â we</span><span class="cmitt-10x-x-109">Â define</span><span class="cmitt-10x-x-109">Â a</span><span class="cmitt-10x-x-109">Â function,</span><span class="cmitt-10x-x-109">Â we</span><span class="cmitt-10x-x-109">Â can</span><span class="cmitt-10x-x-109">Â extend</span><span class="cmitt-10x-x-109">Â this</span><span class="cmitt-10x-x-109">Â to</span><span class="cmitt-10x-x-109">Â multiple</span><span class="cmitt-10x-x-109">Â chains.</span></span> <span id="x1-103170r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2389"><span>@tf</span></span><span id="textcolor2390"><span>.</span></span><span>function</span> <span id="x1-103172r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2391"><span>def</span></span><span>Â </span><span id="textcolor2392"><span>run_chain</span></span><span>():</span> <span id="x1-103174r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â states,</span><span>Â kernel_results</span><span>Â </span><span id="textcolor2393"><span>=</span></span><span>Â tfp</span><span id="textcolor2394"><span>.</span></span><span>mcmc</span><span id="textcolor2395"><span>.</span></span><span>sample_chain(</span> <span id="x1-103176r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â num_results</span><span id="textcolor2396"><span>=</span></span><span id="textcolor2397"><span>self</span></span><span id="textcolor2398"><span>.</span></span><span>n_samples,</span> <span id="x1-103178r7"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â num_burnin_steps</span><span id="textcolor2399"><span>=</span></span><span id="textcolor2400"><span>self</span></span><span id="textcolor2401"><span>.</span></span><span>n_burnin,</span> <span id="x1-103180r8"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â current_state</span><span id="textcolor2402"><span>=</span></span><span>[</span> <span id="x1-103182r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor2403"><span>.</span></span><span>zeros((X</span><span id="textcolor2404"><span>.</span></span><span>shape[</span><span id="textcolor2405"><span>1</span></span><span>],</span><span id="textcolor2406"><span>1</span></span><span>),</span><span>Â name</span><span id="textcolor2407"><span>=</span></span><span id="textcolor2408"><span>'init_model_coeffs'</span></span><span>),</span> <span id="x1-103184r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor2409"><span>.</span></span><span>zeros((</span><span id="textcolor2410"><span>1</span></span><span>),</span><span>Â name</span><span id="textcolor2411"><span>=</span></span><span id="textcolor2412"><span>'init_bias'</span></span><span>),</span> <span id="x1-103186r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor2413"><span>.</span></span><span>ones((</span><span id="textcolor2414"><span>1</span></span><span>),</span><span>Â name</span><span id="textcolor2415"><span>=</span></span><span id="textcolor2416"><span>'init_noise'</span></span><span>),</span> <span id="x1-103188r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â ],</span> <span id="x1-103190r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â kernel</span><span id="textcolor2417"><span>=</span></span><span>hmc_kernel</span> <span id="x1-103192r14"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â )</span> <span id="x1-103194r15"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2418"><span>return</span></span><span>Â states,</span><span>Â kernel_results</span> <span id="x1-103196r16"></span> </code>
<code><span id="x1-103198r17"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2419"><span>print</span></span><span>(</span><span id="textcolor2420"><span>f</span></span><span id="textcolor2421"><span>'Running</span><span>Â HMC</span><span>Â with</span><span>Â </span></span><span id="textcolor2422"><span>{</span></span><span id="textcolor2423"><span>self</span></span><span id="textcolor2424"><span>.</span></span><span>n_samples</span><span id="textcolor2425"><span>}</span></span><span id="textcolor2426"><span>Â samples.'</span></span><span>)</span> <span id="x1-103200r18"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â states,</span><span>Â kernel_results</span><span>Â </span><span id="textcolor2427"><span>=</span></span><span>Â run_chain()</span> <span id="x1-103202r19"></span> </code>
<code><span id="x1-103204r20"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2428"><span>print</span></span><span>(</span><span id="textcolor2429"><span>'Completed</span><span>Â HMC</span><span>Â sampling.'</span></span><span>)</span> <span id="x1-103206r21"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â coeffs,</span><span>Â bias,</span><span>Â noise_std</span><span>Â </span><span id="textcolor2430"><span>=</span></span><span>Â states</span> <span id="x1-103208r22"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â accepted_samples</span><span>Â </span><span id="textcolor2431"><span>=</span></span><span>Â kernel_results</span><span id="textcolor2432"><span>.</span></span><span>is_accepted[</span><span id="textcolor2433"><span>self</span></span><span id="textcolor2434"><span>.</span></span><span>n_burnin:]</span> <span id="x1-103210r23"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â acceptance_rate</span><span>Â </span><span id="textcolor2435"><span>=</span></span><span>Â </span><span id="textcolor2436"><span>100</span></span><span id="textcolor2437"><span>*</span></span><span>np</span><span id="textcolor2438"><span>.</span></span><span>mean(accepted_samples)</span> <span id="x1-103212r24"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2439"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Print</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â acceptance</span><span class="cmitt-10x-x-109">Â rate</span><span class="cmitt-10x-x-109">Â -</span><span class="cmitt-10x-x-109">Â if</span><span class="cmitt-10x-x-109">Â this</span><span class="cmitt-10x-x-109">Â is</span><span class="cmitt-10x-x-109">Â low,</span><span class="cmitt-10x-x-109">Â we</span><span class="cmitt-10x-x-109">Â need</span><span class="cmitt-10x-x-109">Â to</span><span class="cmitt-10x-x-109">Â check</span><span class="cmitt-10x-x-109">Â our</span></span> <span id="x1-103214r25"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2440"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â HMC</span><span class="cmitt-10x-x-109">Â parameters</span></span> <span id="x1-103216r26"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2441"><span>print</span></span><span>(</span><span id="textcolor2442"><span>'Acceptance</span><span>Â rate:</span><span>Â </span></span><span id="textcolor2443"><span>%0.1f%%</span></span><span id="textcolor2444"><span>'</span></span><span>Â </span><span id="textcolor2445"><span>%</span></span><span>Â (acceptance_rate))</span></code></pre>
<p>Once weâ€™ve run our sampler, we <span id="dx1-103217"></span>can fetch our model parameters. We take them from the post-burn-in samples and assign them to class variables for later use in inference:</p>
<pre id="fancyvrb92" class="fancyvrb"><span id="x1-103223r1"></span> 
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2446"><span class="cmitt-10x-x-109">#</span><span class="cmitt-10x-x-109">Â Obtain</span><span class="cmitt-10x-x-109">Â the</span><span class="cmitt-10x-x-109">Â post-burnin</span><span class="cmitt-10x-x-109">Â samples</span></span> <span id="x1-103225r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2447"><span>self</span></span><span id="textcolor2448"><span>.</span></span><span>model_coeffs</span><span>Â </span><span id="textcolor2449"><span>=</span></span><span>Â coeffs[</span><span id="textcolor2450"><span>self</span></span><span id="textcolor2451"><span>.</span></span><span>n_burnin:,:,</span><span id="textcolor2452"><span>0</span></span><span>]</span> <span id="x1-103227r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2453"><span>self</span></span><span id="textcolor2454"><span>.</span></span><span>bias</span><span>Â </span><span id="textcolor2455"><span>=</span></span><span>Â bias[</span><span id="textcolor2456"><span>self</span></span><span id="textcolor2457"><span>.</span></span><span>n_burnin:]</span> <span id="x1-103229r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2458"><span>self</span></span><span id="textcolor2459"><span>.</span></span><span>noise_std</span><span>Â </span><span id="textcolor2460"><span>=</span></span><span>Â noise_std[</span><span id="textcolor2461"><span>self</span></span><span id="textcolor2462"><span>.</span></span><span>n_burnin:]</span></code></pre>
</section>
<section id="step-6-inference" class="level4 likesubsubsectionHead" data-number="11.4.1.6">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.4.1.6"><span id="x1-1040001"></span>Step 6: Inference</h4>
<p>The last thing we need to do is <span id="dx1-104001"></span>implement a function to make predictions using the learned parameters of our joint distribution. To do this, weâ€™ll define two functions: <code>get</code><code>_divd</code><code>_dist()</code>, which will obtain the posterior predictive distribution given our input, and <code>predict()</code>, which will call <code>get</code><code>_divd</code><code>_dist()</code> and compute our mean (<em>Î¼</em>) and standard deviation (<em>Ïƒ</em>) from our posterior distribution:</p>
<pre id="fancyvrb93" class="fancyvrb"><span id="x1-104020r1"></span> 
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2463"><span>def</span></span><span>Â </span><span id="textcolor2464"><span>get_divd_dist</span></span><span>(</span><span id="textcolor2465"><span>self</span></span><span>,</span><span>Â X):</span> <span id="x1-104022r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â predictions</span><span>Â </span><span id="textcolor2466"><span>=</span></span><span>Â (tf</span><span id="textcolor2467"><span>.</span></span><span>matmul(X,</span><span>Â tf</span><span id="textcolor2468"><span>.</span></span><span>transpose(</span><span id="textcolor2469"><span>self</span></span><span id="textcolor2470"><span>.</span></span><span>model_coeffs))</span><span>Â </span><span id="textcolor2471"><span>+</span></span> <span id="x1-104024r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2472"><span>self</span></span><span id="textcolor2473"><span>.</span></span><span>bias[:,</span><span id="textcolor2474"><span>0</span></span><span>])</span> <span id="x1-104026r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â noise</span><span>Â </span><span id="textcolor2475"><span>=</span></span><span>Â (</span><span id="textcolor2476"><span>self</span></span><span id="textcolor2477"><span>.</span></span><span>noise_std[:,</span><span id="textcolor2478"><span>0</span></span><span>]</span><span>Â </span><span id="textcolor2479"><span>*</span></span> <span id="x1-104028r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â tf</span><span id="textcolor2480"><span>.</span></span><span>random</span><span id="textcolor2481"><span>.</span></span><span>normal([</span><span id="textcolor2482"><span>self</span></span><span id="textcolor2483"><span>.</span></span><span>noise_std</span><span id="textcolor2484"><span>.</span></span><span>shape[</span><span id="textcolor2485"><span>0</span></span><span>]]))</span> <span id="x1-104030r6"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2486"><span>return</span></span><span>Â predictions</span><span>Â </span><span id="textcolor2487"><span>+</span></span><span>Â noise</span> <span id="x1-104032r7"></span> </code>
<code><span id="x1-104034r8"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2488"><span>def</span></span><span>Â </span><span id="textcolor2489"><span>predict</span></span><span>(</span><span id="textcolor2490"><span>self</span></span><span>,</span><span>Â X):</span> <span id="x1-104036r9"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â X</span><span>Â </span><span id="textcolor2491"><span>=</span></span><span>Â tf</span><span id="textcolor2492"><span>.</span></span><span>convert_to_tensor(</span><span id="textcolor2493"><span>self</span></span><span id="textcolor2494"><span>.</span></span><span>get_basis(X),</span><span>Â dtype</span><span id="textcolor2495"><span>=</span></span><span>dtype)</span> <span id="x1-104038r10"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â divd_dist</span><span>Â </span><span id="textcolor2496"><span>=</span></span><span>Â np</span><span id="textcolor2497"><span>.</span></span><span>zeros((X</span><span id="textcolor2498"><span>.</span></span><span>shape[</span><span id="textcolor2499"><span>0</span></span><span>],</span><span>Â </span><span id="textcolor2500"><span>self</span></span><span id="textcolor2501"><span>.</span></span><span>model_coeffs</span><span id="textcolor2502"><span>.</span></span><span>shape[</span><span id="textcolor2503"><span>0</span></span><span>]))</span> <span id="x1-104040r11"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â X</span><span>Â </span><span id="textcolor2504"><span>=</span></span><span>Â tf</span><span id="textcolor2505"><span>.</span></span><span>reshape(X,</span><span>Â (</span><span id="textcolor2506"><span>-</span></span><span id="textcolor2507"><span>1</span></span><span>,</span><span>Â </span><span id="textcolor2508"><span>1</span></span><span>,</span><span>Â X</span><span id="textcolor2509"><span>.</span></span><span>shape[</span><span id="textcolor2510"><span>1</span></span><span>]))</span> <span id="x1-104042r12"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2511"><span>for</span></span><span>Â i</span><span>Â </span><span id="textcolor2512"><span>in</span></span><span>Â </span><span id="textcolor2513"><span>range</span></span><span>(X</span><span id="textcolor2514"><span>.</span></span><span>shape[</span><span id="textcolor2515"><span>0</span></span><span>]):</span> <span id="x1-104044r13"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â divd_dist[i,:]</span><span>Â </span><span id="textcolor2516"><span>=</span></span><span>Â </span><span id="textcolor2517"><span>self</span></span><span id="textcolor2518"><span>.</span></span><span>get_divd_dist(X[i,:])</span> <span id="x1-104046r14"></span> </code>
<code><span id="x1-104048r15"></span></code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â y_divd</span><span>Â </span><span id="textcolor2519"><span>=</span></span><span>Â np</span><span id="textcolor2520"><span>.</span></span><span>mean(divd_dist,</span><span>Â axis</span><span id="textcolor2521"><span>=</span></span><span id="textcolor2522"><span>1</span></span><span>)</span> <span id="x1-104050r16"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â y_std</span><span>Â </span><span id="textcolor2523"><span>=</span></span><span>Â np</span><span id="textcolor2524"><span>.</span></span><span>std(divd_dist,</span><span>Â axis</span><span id="textcolor2525"><span>=</span></span><span id="textcolor2526"><span>1</span></span><span>)</span> <span id="x1-104052r17"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2527"><span>return</span></span><span>Â y_divd,</span><span>Â y_std</span></code></pre>
<p>And thatâ€™s it! We have our BLL implementation! With this class, we have a powerful and principled means of obtaining Bayesian uncertainty estimates by using penultimate NN layers as basis functions for Bayesian regression. Making use of it is as simple as <span id="dx1-104053"></span>passing our model and defining which layer we want to use as our basis function:</p>
<pre id="fancyvrb94" class="fancyvrb"><span id="x1-104060r1"></span> 
<code><span>bll</span><span>Â </span><span id="textcolor2528"><span>=</span></span><span>Â BayesianLastLayer(model,</span><span>Â </span><span id="textcolor2529"><span>'layer_2'</span></span><span>)</span> <span id="x1-104062r2"></span> </code>
<code><span id="x1-104064r3"></span></code>
<code><span>bll</span><span id="textcolor2530"><span>.</span></span><span>fit(X_train,</span><span>Â y_train)</span> <span id="x1-104066r4"></span> </code>
<code><span id="x1-104068r5"></span></code>
<code><span>y_divd,</span><span>Â y_std</span><span>Â </span><span id="textcolor2531"><span>=</span></span><span>Â bll</span><span id="textcolor2532"><span>.</span></span><span>predict(X_test)</span></code></pre>
<p>While this is a powerful tool, itâ€™s not always suited for the task at hand. You can experiment with this yourself: try creating a model with a larger embedding layer. As the size of the layer increases, you should start to see that the acceptance rate of the sampler drops. Once itâ€™s large enough, the acceptance rate may even fall to 0%. So, weâ€™ll need to modify the parameters of our sampler: reducing the step size, increasing the number of samples, and increasing the number of burn-in samples. As the dimensionality of the embedding grows, it becomes more and more difficult to obtain a representative set of samples for the distribution.</p>
<p>For some applications, this isnâ€™t an issue, but when dealing with complex, high-dimensional data, this can quickly become problematic. Applications in domains such as computer vision, speech processing, and molecular modeling all rely on high-dimensional embeddings. One solution here is to reduce these embeddings further, for example, via dimensionality reduction. But doing so can have an unpredictable effect on these encodings: in fact, by reducing the dimensionality, you could be unintentionally removing sources of uncertainty, resulting in poorer quality uncertainty estimates.</p>
<p>So, what can we do instead? Fortunately, there are a few other last-layer options we can employ. Next, weâ€™ll see how we can use last-layer dropout to approximate the Bayesian linear regression approach introduced here. <span id="x1-104069r164"></span></p>
</section>
</section>
<section id="last-layer-mc-dropout" class="level3 subsectionHead" data-number="11.4.2">
<h3 class="subsectionHead" data-number="11.4.2" id="sigil_toc_id_74"><span class="titlemark">6.4.2 </span> <span id="x1-1050002"></span>Last-layer MC dropout</h3>
<p>Earlier in the chapter, we saw how we can use dropout at <span id="dx1-105001"></span>test time to obtain a distribution over our model predictions. Here, weâ€™ll combine that concept with the concept of last-layer uncertainties: adding an MC dropout layer, but only as a single layer that we add to a pre-trained network.</p>
<section id="step-1-connecting-to-our-base-model" class="level4 likesubsubsectionHead" data-number="11.4.2.1">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.4.2.1"><span id="x1-1060002"></span>Step 1: Connecting to our base model</h4>
<p>Similarly to the Bayesian last-layer method, we first <span id="dx1-106001"></span>need to obtain the output from our modelâ€™s penultimate layer:</p>
<pre id="fancyvrb95" class="fancyvrb"><span id="x1-106005r1"></span> 
<code><span>basis_func</span><span>Â </span><span id="textcolor2533"><span>=</span></span><span>Â Model(inputs</span><span id="textcolor2534"><span>=</span></span><span>model</span><span id="textcolor2535"><span>.</span></span><span>input,</span> <span id="x1-106007r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â outputs</span><span id="textcolor2536"><span>=</span></span><span>model</span><span id="textcolor2537"><span>.</span></span><span>get_layer(</span><span id="textcolor2538"><span>'layer_2'</span></span><span>)</span><span id="textcolor2539"><span>.</span></span><span>output)</span></code></pre>
</section>
<section id="step-2-adding-an-mc-dropout-layer" class="level4 likesubsubsectionHead" data-number="11.4.2.2">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.4.2.2"><span id="x1-1070002"></span>Step 2: Adding an MC dropout layer</h4>
<p>Now, instead of implementing a Bayesian regressor, weâ€™ll <span id="dx1-107001"></span>simply instantiate a new output layer, which applies dropout to the penultimate layer:</p>
<pre id="fancyvrb96" class="fancyvrb"><span id="x1-107006r1"></span> 
<code><span>ll_dropout</span><span>Â </span><span id="textcolor2540"><span>=</span></span><span>Â Sequential()</span> <span id="x1-107008r2"></span> </code>
<code><span>ll_dropout</span><span id="textcolor2541"><span>.</span></span><span>add(layers</span><span id="textcolor2542"><span>.</span></span><span>Dropout(</span><span id="textcolor2543"><span>0.25</span></span><span>))</span> <span id="x1-107010r3"></span> </code>
<code><span>ll_dropout</span><span id="textcolor2544"><span>.</span></span><span>add(layers</span><span id="textcolor2545"><span>.</span></span><span>Dense(</span><span id="textcolor2546"><span>1</span></span><span>,</span><span>Â input_dim</span><span id="textcolor2547"><span>=</span></span><span id="textcolor2548"><span>8</span></span><span>,</span><span>Â activation</span><span id="textcolor2549"><span>=</span></span><span id="textcolor2550"><span>'relu'</span></span><span>,</span><span>Â name</span><span id="textcolor2551"><span>=</span></span><span id="textcolor2552"><span>'dropout_layer'</span></span><span>))</span></code></pre>
</section>
<section id="step-3-training-the-mc-dropout-last-layer" class="level4 likesubsubsectionHead" data-number="11.4.2.3">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.4.2.3"><span id="x1-1080002"></span>Step 3: Training the MC dropout last-layer</h4>
<p>Because weâ€™ve now added a new final layer, we <span id="dx1-108001"></span>need to run an additional step of training so that it can learn the mapping from our penultimate layer to the new output; but because our original model is doing all of the heavy lifting, this training is both computationally cheap and quick to run:</p>
<pre id="fancyvrb97" class="fancyvrb"><span id="x1-108008r1"></span> 
<code><span>ll_dropout</span><span id="textcolor2553"><span>.</span></span><span>compile(optimizer</span><span id="textcolor2554"><span>=</span></span><span>optimizers</span><span id="textcolor2555"><span>.</span></span><span>Adam(),</span> <span id="x1-108010r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â loss</span><span id="textcolor2556"><span>=</span></span><span>losses</span><span id="textcolor2557"><span>.</span></span><span>MeanSquaredError(),</span> <span id="x1-108012r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span>Â metrics</span><span id="textcolor2558"><span>=</span></span><span>[metrics</span><span id="textcolor2559"><span>.</span></span><span>RootMeanSquaredError()],)</span> <span id="x1-108014r4"></span> </code>
<code><span>num_epochs</span><span>Â </span><span id="textcolor2560"><span>=</span></span><span>Â </span><span id="textcolor2561"><span>50</span></span> <span id="x1-108016r5"></span> </code>
<code><span>ll_dropout</span><span id="textcolor2562"><span>.</span></span><span>fit(basis_func</span><span id="textcolor2563"><span>.</span></span><span>predict(X_train),</span><span>Â y_train,</span><span>Â epochs</span><span id="textcolor2564"><span>=</span></span><span>num_epochs)</span></code></pre>
</section>
<section id="step-4-obtaining-uncertainties" class="level4 likesubsubsectionHead" data-number="11.4.2.4">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.4.2.4"><span id="x1-1090002"></span>Step 4: Obtaining uncertainties</h4>
<p>Now that our last layer is trained, we can implement a <span id="dx1-109001"></span>function to obtain the mean and standard deviation for our predictions using multiple forward passes of our MC dropout layer; line 3 onwards should be familiar from earlier in the chapter, and line 2 simply obtains the output from our original modelâ€™s penultimate layer:</p>
<pre id="fancyvrb98" class="fancyvrb"><span id="x1-109008r1"></span> 
<code><span id="textcolor2565"><span>def</span></span><span>Â </span><span id="textcolor2566"><span>predict_ll_dropout</span></span><span>(X,</span><span>Â basis_func,</span><span>Â ll_dropout,</span><span>Â nb_inference):</span> <span id="x1-109010r2"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â basis_feats</span><span>Â </span><span id="textcolor2567"><span>=</span></span><span>Â basis_func(X)</span> <span id="x1-109012r3"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â ll_divd</span><span>Â </span><span id="textcolor2568"><span>=</span></span><span>Â [ll_dropout(basis_feats,</span><span>Â training</span><span id="textcolor2569"><span>=</span></span><span id="textcolor2570"><span>True</span></span><span>)</span><span>Â </span><span id="textcolor2571"><span>for</span></span><span>Â _</span><span>Â </span><span id="textcolor2572"><span>in</span></span><span>Â </span><span id="textcolor2573"><span>range</span></span><span>(nb_inference)]</span> <span id="x1-109014r4"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â ll_divd</span><span>Â </span><span id="textcolor2574"><span>=</span></span><span>Â np</span><span id="textcolor2575"><span>.</span></span><span>stack(ll_divd)</span> <span id="x1-109016r5"></span> </code>
<code><span>Â </span><span>Â </span><span>Â </span><span>Â </span><span id="textcolor2576"><span>return</span></span><span>Â ll_divd</span><span id="textcolor2577"><span>.</span></span><span>mean(axis</span><span id="textcolor2578"><span>=</span></span><span id="textcolor2579"><span>0</span></span><span>),</span><span>Â ll_divd</span><span id="textcolor2580"><span>.</span></span><span>std(axis</span><span id="textcolor2581"><span>=</span></span><span id="textcolor2582"><span>0</span></span><span>)</span></code></pre>
</section>
<section id="step-5-inference-2" class="level4 likesubsubsectionHead" data-number="11.4.2.5">
<h4 class="likesubsubsectionHead sigil_not_in_toc" data-number="11.4.2.5"><span id="x1-1100002"></span>Step 5: Inference</h4>
<p>All thatâ€™s left is to call this function and obtain our <span id="dx1-110001"></span>new model outputs, complete with uncertainty estimates:</p>
<pre id="fancyvrb99" class="fancyvrb"><span id="x1-110004r1"></span> 
<code><span>y_divd,</span><span>Â y_std</span><span>Â </span><span id="textcolor2583"><span>=</span></span><span>Â predict_ll_dropout(X_test,</span><span>Â basis_func,</span><span>Â ll_dropout,</span><span>Â </span><span id="textcolor2584"><span>50</span></span><span>)</span></code></pre>
<p>Last-layer MC dropout is by far one of the easiest ways to obtain uncertainty estimates from pre-trained networks. Unlike standard MC dropout, it doesnâ€™t require training a model from scratch, so you can apply this post-hoc to networks youâ€™ve already trained. Additionally, unlike the other last-layer methods, it can be implemented in just a few straightforward steps that never stray from TensorFlowâ€™s standard API. <span id="x1-110005r171"></span></p>
</section>
</section>
<section id="recap-of-last-layer-methods" class="level3 subsectionHead" data-number="11.4.3">
<h3 class="subsectionHead" data-number="11.4.3" id="sigil_toc_id_75"><span class="titlemark">6.4.3 </span> <span id="x1-1110003"></span>Recap of last-layer methods</h3>
<p>Last-layer methods are an excellent tool for when you <span id="dx1-111001"></span>need to obtain uncertainty estimates from a pre-trained network. Given how expensive and time-consuming neural network training can be, itâ€™s nice not to have to start from scratch just because you need some predictive uncertainties. Additionally, given that more and more machine learning practitioners are relying on state-of-the-art pre-trained models, these kinds of techniques are a practical way to incorporate model uncertainties after the fact.</p>
<p>But there are drawbacks to last-layer methods too. Unlike other methods, weâ€™re relying on a fairly limited source of variance: the penultimate layer of our model. This limits how much stochasticity we can induce over our model outputs, meaning weâ€™re at risk of over-confident predictions. Bear this in mind when using last-layer methods and, if you see the hallmark signs of over-confidence, consider using a more comprehensive method to obtain your predictive uncertainties. <span id="x1-111002r162"></span></p>
</section>
</section>
<section id="summary-5" class="level2 sectionHead" data-number="11.5">
<h2 class="sectionHead" data-number="11.5" id="sigil_toc_id_76"><span class="titlemark">6.5 </span> <span id="x1-1120005"></span>Summary</h2>
<p>In this chapter, weâ€™ve seen how familiar machine learning and deep learning concepts can be used to develop models with predictive uncertainties. Weâ€™ve also seen how, with relatively minor modifications, we can add uncertain estimates to pre-trained models. This means we can go beyond the point-estimate approach of standard NNs: using uncertainties to gain valuable insights into the performance of our models, and allowing us to develop more robust applications.</p>
<p>However, as with the methods introduced in <a href="CH5.xhtml#x1-600005"><em>ChapterÂ 5</em></a>, <a href="CH5.xhtml#x1-600005"><em>Principled Approaches</em> <em>for Bayesian Deep Learning</em></a>, all techniques have advantages and disadvantages. For example, last-layer methods may give us the flexibility to add uncertainties to any model, but theyâ€™re limited by the representation that the model has already learned. This could result in very low variance outputs, resulting in an overconfident model. Similarly, while ensemble methods allow us to capture variance across every layer of the network, they come at significant computational cost, requiring that we have multiple networks, rather than just a single network.</p>
<p>In the next chapter, we will examine the advantages and disadvantages in more detail, and learn how we can address some of the shortcomings of these methods.</p>
<p><span id="x1-112001r144"></span></p>
</section>
</section>
</body>
</html>