<html><head></head><body>
		<div id="_idContainer025">
			<h1 id="_idParaDest-52" class="chapter-number"><a id="_idTextAnchor059"/>3</h1>
			<h1 id="_idParaDest-53"><a id="_idTextAnchor060"/>Approaches to Natural Language Understanding – Rule-Based Systems, Machine Learning, and Deep Learning</h1>
			<p><a id="_idTextAnchor061"/><a id="_idTextAnchor062"/>This chapter will review the most common approaches to <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>) and discuss both the benefits and drawbacks of each approach, including rule-based techniques, statistical techniques, and deep learning. It will also discuss popular pre-trained models such as <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>) and its variants. We will learn that NLU is not a single technology; it includes a range of techniques, which are applicable to <span class="No-Break">different goals.</span></p>
			<p>In this chapter, we cover the following <span class="No-Break">main topics:</span></p>
			<ul>
				<li><span class="No-Break">Rule-based approaches</span></li>
				<li>Traditional <span class="No-Break">machine-learning approaches</span></li>
				<li>Deep <span class="No-Break">learning approaches</span></li>
				<li><span class="No-Break">Pre-trained models</span></li>
				<li>Considerations for <span class="No-Break">selecting technologies</span></li>
			</ul>
			<p><span class="No-Break">Let’s begin!</span></p>
			<h1 id="_idParaDest-54"><a id="_idTextAnchor063"/>Rule-based approaches</h1>
			<p>The basic idea behind <strong class="bold">rule-based approaches</strong> is that language obeys rules about how words are related to their <a id="_idIndexMarker155"/>meanings. For example, when we learn foreign languages, we typically learn specific rules about what words mean, how they’re ordered in sentences, and how prefixes and suffixes change the meanings of words. The rule-based approach to NLU operates on the premise that these kinds of rules can be provided to an NLU system so that the system can determine the meanings of sentences in the same way that a <span class="No-Break">person does.</span></p>
			<p>The rule-based approach was widely used in NLU from the mid-1950s through the mid-1990s until machine-learning-based approaches became popular. However, there are still NLU problems where rule-based approaches are useful, either on their own or when combined with <span class="No-Break">other techniques.</span></p>
			<p>We will begin by reviewing the rules and data that are relevant to various aspects <span class="No-Break">of language.</span></p>
			<h2 id="_idParaDest-55"><a id="_idTextAnchor064"/>Words and lexicons</h2>
			<p>Nearly everyone is familiar with the idea of words, which are usually defined as units of language that can be spoken<a id="_idIndexMarker156"/> individually. As we saw in <a href="B19005_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, in most, but not all, languages, words are separated by white space. The set of words in a language is referred to as the language’s <strong class="bold">lexicon</strong>. The idea of a lexicon<a id="_idIndexMarker157"/> corresponds to what we think of as a <a id="_idIndexMarker158"/>dictionary – a list of the words in a language. A computational lexicon also includes other information about each word. In particular, it includes its part or parts of speech. Depending on the language, it could also include information on whether the word has irregular forms (such as, for the irregular English verb “<em class="italic">eat</em>,” the past tense “<em class="italic">ate</em>” and the past participle “<em class="italic">eaten</em>” are irregular). Some lexicons also include semantic information such as words that are related in meaning to <span class="No-Break">each word.</span></p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor065"/>Part-of-speech tagging</h2>
			<p>Traditional parts of<a id="_idIndexMarker159"/> speech as taught in<a id="_idIndexMarker160"/> schools include categories such as “<em class="italic">noun</em>”, “<em class="italic">verb</em>”, “<em class="italic">adjective</em>”, <em class="italic">preposition</em>, and so on. The parts of speech used in computational lexicons are usually more detailed than these since they need to express more specific information than what could be captured by traditional categories. For example, the traditional <em class="italic">verb</em> category in English is usually broken down into several different parts of speech corresponding to the different forms of the verb, such as the past tense and past participle forms. A commonly used set of parts of speech for English is the parts of speech from the Penn Treebank (<a href="https://catalog.ldc.upenn.edu/LDC99T42">https://catalog.ldc.upenn.edu/LDC99T42</a>). Different languages will have different parts of speech categories in their <span class="No-Break">computational lexicons.</span></p>
			<p>A very useful task in <a id="_idIndexMarker161"/>processing natural language is to assign parts <a id="_idIndexMarker162"/>of speech to the words in a text. This is called <strong class="bold">part-of-speech tagging</strong> (<strong class="bold">POS tagging</strong>). <em class="italic">Table 3.1</em> shows an example of the Penn Treebank part-of-speech tags for the sentence “<em class="italic">We would like to book a flight from Boston </em><span class="No-Break"><em class="italic">to London</em></span><span class="No-Break">:”</span></p>
			<table id="table001-2" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Word</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Part </strong><span class="No-Break"><strong class="bold">of speech</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Meaning of part of </strong><span class="No-Break"><strong class="bold">speech label</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">we</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">PRP</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Personal pronoun</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">would</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">MD</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Modal verb</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">like</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">VB</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Verb, <span class="No-Break">base form</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">to</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">TO</span></p>
						</td>
						<td class="No-Table-Style">
							<p>To (this word has its own part <span class="No-Break">of speech)</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">book</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">VB</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Verb, <span class="No-Break">base form</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>a</p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">DT</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Determiner (article)</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">flight</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">NN</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Singular noun</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">from</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">IN</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Preposition</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Boston</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">NNP</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Proper noun</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">to</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">TO</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">To</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">London</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">NNP</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Proper noun</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor066"/>Table 3.1 – Part-of-speech tags for “We would like to book a flight from Boston to London”</p>
			<p>POS tagging is not just a matter of looking words up in a dictionary and labeling them with their parts of speech because many words have more than one part of speech. In our example, one of these is “<em class="italic">book</em>,” which is used as a verb in the example but is also commonly used as a noun. POS tagging algorithms have to not only look at the word itself but also consider its context in<a id="_idIndexMarker163"/> order to determine the correct part of speech. In this example, “<em class="italic">book</em>” follows “<em class="italic">to</em>,” which often indicates that the next word is <span class="No-Break">a verb.</span></p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor067"/>Grammar</h2>
			<p>Grammar rules are the rules that describe how words are ordered in sentences so that the sentences can be <a id="_idIndexMarker164"/>understood and also so that they can correctly convey<a id="_idIndexMarker165"/> the author’s meaning. They can be written in the form of rules describing part-whole relationships between sentences and their components. For example, a common grammar rule for English says that a sentence consists of a noun phrase followed by a verb phrase. A full computational grammar for any natural language usually consists of hundreds of rules and is very complex. It isn’t very common now to build grammar from scratch; rather, grammar is already included in commonly used Python <a id="_idIndexMarker166"/>NLP libraries such as <strong class="bold">natural language toolkit</strong> (<strong class="bold">NLTK</strong>) <span class="No-Break">and </span><span class="No-Break"><strong class="bold">spaCy</strong></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-58"><a id="_idTextAnchor068"/>Parsing</h2>
			<p>Finding the<a id="_idIndexMarker167"/> relationships between parts of a sentence is known as <strong class="bold">parsing</strong>. This involves <a id="_idIndexMarker168"/>applying the grammar rules to a specific sentence to show how the parts of the sentence are related to each other. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em> shows the parsing of the sentence “<em class="italic">We would like to book a flight</em>.” In this style of parsing, known as <strong class="bold">dependency parsing</strong>, the<a id="_idIndexMarker169"/> relationships between the words are shown as arcs between the words. For example, the fact that “<em class="italic">we</em>” is the subject of the verb “<em class="italic">like</em>” is shown by an arc labeled <strong class="bold">nsubj</strong> connecting “<em class="italic">like</em>” <span class="No-Break">to “</span><span class="No-Break"><em class="italic">we</em></span><span class="No-Break">”.</span></p>
			<div>
				<div id="_idContainer020" class="IMG---Figure">
					<img src="image/B19005_03_01.jpg" alt="Figure 3.1 – Parsing for “We would like to book a flight”"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.1 – Parsing for “We would like to book a flight”</p>
			<p>At this point, it isn’t necessary to worry about the details of parsing – we will discuss it in more detail in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor069"/>Semantic analysis</h2>
			<p>Parsing involves determining <a id="_idIndexMarker170"/>how words are structurally related to each other in sentences, but it doesn’t say anything about their meanings or how their meanings are related. This kind of processing is done through <strong class="bold">semantic analysis</strong>. There are many approaches to semantic<a id="_idIndexMarker171"/> analysis—this is an active research field—but one way to think of semantic analysis is that it starts with the main verb of the sentence and looks at the relationships between the verb and other parts of the sentence, such as the subject, direct object, and related prepositional phrases. For example, the subject of “<em class="italic">like</em>” in <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.1</em> is “<em class="italic">We</em>.” “<em class="italic">We</em>” could be described as the “<em class="italic">experiencer</em>” of “<em class="italic">like</em>” since it is described as experiencing “<em class="italic">liking.</em>” Similarly, the thing that is liked, “<em class="italic">to book a flight</em>,” could be described as the “<em class="italic">patient</em>” of “<em class="italic">like.</em>” Semantic analysis is most frequently done through the application of rules, but it can also be done with machine learning techniques, as described in the <span class="No-Break">next sections.</span></p>
			<p>Finding semantic relationships between the concepts denoted by words, independently of their roles in sentences, can also be useful. For example, we can think of a “<em class="italic">dog</em>” as a kind of “<em class="italic">animal</em>,” or we can think of “<em class="italic">eating</em>” as a kind of action. One helpful resource for finding these kinds of relationships is Wordnet (<a href="https://wordnet.princeton.edu/">https://wordnet.princeton.edu/</a>), which is a large, manually prepared database describing the relationships between thousands of English words. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.2</em> shows part of the Wordnet information for the word “<em class="italic">airplane</em>,” indicating that an airplane is a kind of “<em class="italic">heavier-than-aircraft</em>,” which is a kind of “<em class="italic">aircraft</em>,” and so on, going all the way up to the very general <span class="No-Break">category “</span><span class="No-Break"><em class="italic">enti<a id="_idTextAnchor070"/>ty</em></span><span class="No-Break">:”</span></p>
			<div>
				<div id="_idContainer021" class="IMG---Figure">
					<img src="image/B19005_03_02.jpg" alt="Figure 3.2 – Wordnet semantic hierarchy for the word “airplane”"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.2 – Wordnet semantic hierarchy for the word “airplane”</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor071"/>Pragmatic analysis</h2>
			<p><strong class="bold">Pragmatic analysis</strong> determines <a id="_idIndexMarker172"/>the meanings of words and phrases in context. For example, in long texts, different <a id="_idIndexMarker173"/>words can be used to refer to the same thing, or different things can be referred to with the same <a id="_idIndexMarker174"/>word. This is called <strong class="bold">coreference</strong>. For example, the sentence “<em class="italic">We want to book a flight from Boston to London</em>” could be followed by “<em class="italic">the flight needs to leave before 10 a.m.</em>” Pragmatic analysis determines that the flight that needs to leave before 10 a.m. is the same flight that we want <span class="No-Break">to book.</span></p>
			<p>A very important type of <a id="_idIndexMarker175"/>pragmatic analysis is <strong class="bold">named entity recognition</strong> (<strong class="bold">NER</strong>), which links up references that occur in texts to corresponding entities in the real world. <span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.3</em> shows NER for the sentence “<em class="italic">Book a flight to London on United for less than 1,000 dollars</em>.” “<em class="italic">London</em>” is a named entity, which is labeled as a geographical location, “<em class="italic">United</em>” is labeled as an organization, and “<em class="italic">less than 1,000 dollars</em>” is labeled as a <span class="No-Break">monetary<a id="_idTextAnchor072"/> amount:</span></p>
			<div>
				<div id="_idContainer022" class="IMG---Figure">
					<img src="image/B19005_03_03.jpg" alt="Figure 3.3 – NER for “Book a flight to London on United for less than 1,000 dollars”"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.3 – NER for “Book a flight to London on United for less than 1,000 dollars”</p>
			<h2 id="_idParaDest-61"><a id="_idTextAnchor073"/>Pipelines</h2>
			<p>In NLP applications, the<a id="_idIndexMarker176"/> steps we have just described are most often implemented as a <strong class="bold">pipeline</strong>; that is, a sequence <a id="_idIndexMarker177"/>of steps where the results of one step are the input to the next step. For example, a typical NLP pipeline might be <span class="No-Break">as follows:</span></p>
			<ul>
				<li><strong class="bold">Lexical lookup</strong>: Look up the words in the <span class="No-Break">application’s dictionary</span></li>
				<li><strong class="bold">POS tagging</strong>: Assign parts of speech to each word <span class="No-Break">in context</span></li>
				<li><strong class="bold">Parsing</strong>: Determine how the words are related to <span class="No-Break">each other</span></li>
				<li><strong class="bold">Semantic analysis</strong>: Determine the meanings of the words and the overall meaning of <span class="No-Break">the sentence</span></li>
				<li><strong class="bold">Pragmatic analysis</strong>: Determine aspects of the meaning that depend on a broader context, such as the interpretations <span class="No-Break">of pronouns</span></li>
			</ul>
			<p>One advantage of using pipelines is that each step can be implemented with different technologies, as long as the output of that step is in the format expected by the next step. So, pipelines are not only useful in rule-based approaches but also in the other techniques, which we will describe in the <span class="No-Break">next sections.</span></p>
			<p>More details on rule-based techniques will be provided in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a><span class="No-Break">.</span></p>
			<p>We will now turn to techniques that rely less on the rules of the language and more on machine learning with <span class="No-Break">existing data.</span></p>
			<h1 id="_idParaDest-62"><a id="_idTextAnchor074"/>Traditional machine learning approaches</h1>
			<p>While rule-based <a id="_idIndexMarker178"/>approaches provide very fine-grained and specific information about language, there are some drawbacks to these approaches, which has motivated the development of alternatives. There are two <span class="No-Break">major drawbacks:</span></p>
			<ul>
				<li>Developing the<a id="_idIndexMarker179"/> rules used in rule-based approaches can be a laborious process. Rule development can either be done by experts directly writing rules based on their knowledge of the language or, more commonly, the rules can be derived from examples of text that have been annotated with a correct analysis. Both of these approaches can be expensive <span class="No-Break">and time-consuming.</span></li>
				<li>Rules are not likely to be universally applicable to every text that the system encounters. The experts who developed the rules might have overlooked some cases, the annotated data might not have examples of every case, and speakers can make errors such as false starts, which need to be analyzed although they aren’t covered by any rule. Written language can include spelling errors, which results in words that aren’t in the lexicon. Finally, the language itself can change, resulting in new words and new phrases that aren’t covered by <span class="No-Break">existing rules.</span></li>
			</ul>
			<p>For these reasons, rule-based approaches are primarily used as part of NLU pipelines, supplementing <span class="No-Break">other techniques.</span></p>
			<p>Traditional machine learning approaches were motivated by problems in classification, where documents that are similar in meaning can be grouped. Two problems have to be solved <span class="No-Break">in classification:</span></p>
			<ul>
				<li>Representing the documents in a training set in such a way that documents in the same categories have <span class="No-Break">similar representations</span></li>
				<li>Deciding how new, previously unseen documents should be classified based on their similarity to the documents in the <span class="No-Break">training set</span></li>
			</ul>
			<h2 id="_idParaDest-63"><a id="_idTextAnchor075"/>Representing documents</h2>
			<p>Representations of <a id="_idIndexMarker180"/>documents are based on words. A very simple approach is to assume that the document should be represented<a id="_idIndexMarker181"/> simply as the set of words that it contains. This is called the <strong class="bold">bag of words</strong> (<strong class="bold">BoW</strong>) approach. The simplest way of using a BoW for document representation is to make a list of all the words in the corpus, and for each document and each word, state whether that word occurs in <span class="No-Break">that document.</span></p>
			<p>For example, suppose we have a corpus consisting of the three documents in <span class="No-Break"><em class="italic">Figure 3</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">:</span></p>
			<div>
				<div id="_idContainer023" class="IMG---Figure">
					<img src="image/B19005_03_04New.jpg" alt="Figure 3.4 – A small corpus of restaurant queries"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.4 – A small corpus of restaurant queries</p>
			<p>The entire vocabulary for this toy corpus is 29 words, so each document is associated with a list 29 items long<a id="_idIndexMarker182"/> that states, for each word, whether or not it appears in the document. The list represents <em class="italic">occurs</em> as <strong class="source-inline">1</strong> and <em class="italic">does not occur</em> as <strong class="source-inline">0</strong>, as shown in <span class="No-Break"><em class="italic">Table 3.2</em></span><span class="No-Break">:</span></p>
			<table id="table002-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><strong class="bold">a</strong></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">an</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">any</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">are</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">aren’t</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">away</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Chinese</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Eastern</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">…</strong></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">1</span></p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>…</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">2</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>…</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span lang="en-US" xml:lang="en-US">3</span></p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>0</p>
						</td>
						<td class="No-Table-Style">
							<p>1</p>
						</td>
						<td class="No-Table-Style">
							<p>…</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor076"/>Table 3.2 – BoW for a small corpus</p>
			<p><em class="italic">Table 3.2</em> shows the BoW lists for the first eight words in the vocabulary for these three documents. Each row in <em class="italic">Table 3.2</em> represents one document. For example, the word “<em class="italic">a</em>” occurs once in the first document, but the word “<em class="italic">an</em>” does not occur, so its entry is <em class="italic">0</em>. This representation is mathematically a <em class="italic">vector</em>. Vectors are a powerful tool in NLU, and we will discuss them later in much more detail. The BoW representation might seem very simplistic (for example, it doesn’t take into account any information about the order of words). However, other variations on this concept are more powerful, which will be discussed later on in <em class="italic">Chapters 9</em> <span class="No-Break">to </span><span class="No-Break"><em class="italic">12</em></span><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor077"/>Classification</h2>
			<p>The assumption behind the BoW approach is that the more words that two documents have in common, the <a id="_idIndexMarker183"/>more similar they are in meaning. This is not a hard-and-fast rule, but it turns out to be useful <span class="No-Break">in practice.</span></p>
			<p>For many applications, we want to group documents that are similar in meaning into different categories. This is the<a id="_idIndexMarker184"/> process of <strong class="bold">classification</strong>. If we want to classify a new document into one of these categories, we need to find out how similar its vector is to the vectors of the other documents in each category. For example, the sentiment analysis task discussed in <a href="B19005_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a> is the task of classifying documents into one of two categories – positive or negative sentiment regarding the topic of <span class="No-Break">the text.</span></p>
			<p>Many algorithms have been used <a id="_idIndexMarker185"/>for the classification of text documents. Naïve Bayes and <strong class="bold">support vector machines</strong> (<strong class="bold">SVMs</strong>), to be discussed in <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a>, are two of the most popular. Neural networks, especially <strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>), are also <a id="_idIndexMarker186"/>popular. Neural networks are briefly discussed in the next section and will be discussed in detail in <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">.</span></p>
			<p>In this section, we’ve summarized some approaches that are used in traditional machine learning. Now, we will turn our attention to the new approaches based on <span class="No-Break">deep learning.</span></p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor078"/>Deep learning approaches</h1>
			<p>Neural networks, and especially the large neural networks generally referred to as <strong class="bold">deep learning</strong>, have become<a id="_idIndexMarker187"/> very popular for NLU in the past few years because they significantly improve the accuracy of <span class="No-Break">earlier methods.</span></p>
			<p>The basic concept behind neural networks is that they consist of layers of connected units, called <strong class="bold">neurons</strong> in analogy to the<a id="_idIndexMarker188"/> neurons in animal nervous systems. Each neuron in a neural net is connected to other neurons in the neural net. If a neuron receives the appropriate inputs from other neurons, it will fire, or send input to another neuron, which will in turn fire or not fire depending on other inputs that it receives. During the training process, weights on the neurons are adjusted to maximize <span class="No-Break">classification accuracy.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 3</em></span><em class="italic">.5</em> shows an example of a four-layer neural net performing a sentiment analysis task. The neurons are circles connected by lines. The first layer, on the left, receives a text input. Two hidden<a id="_idIndexMarker189"/> layers of neurons then process the input, and the result (<em class="italic">positive</em>) is produced by the single neuron in the final <span class="No-Break">output lay<a id="_idTextAnchor079"/>er:</span></p>
			<div>
				<div id="_idContainer024" class="IMG---Figure">
					<img src="image/B19005_03_05.jpg" alt="Figure 3.5 – A four-layer neural network trained to perform sentiment analysis on product reviews"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 3.5 – A four-layer neural network trained to perform sentiment analysis on product reviews</p>
			<p>Although the concepts behind neural networks have existed for many years, the implementation of neural networks large enough to perform significant tasks has only been possible within the last few years because of the limitations of earlier computing resources. Their current popularity is largely due to the fact that they are often more accurate than earlier approaches, especially given sufficient training data. However, the process of training a neural net for large-scale tasks can be complex and time-consuming and can require the services of expert data scientists. In some cases, the additional accuracy that a neural net provides is not enough to justify the additional expense of developing <span class="No-Break">the system.</span></p>
			<p>Deep learning and neural networks will be discussed in detail in <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor080"/>Pre-trained models</h1>
			<p>The most recent approach to NLU is based on the idea that much of the information required to <a id="_idIndexMarker190"/>understand natural language can be made available to many different applications by processing generic text (such as internet text) to create a baseline model for the language. Some of these models are very large and are based on tremendous amounts of data. To apply these models to a specific application, the generic model is adapted to the application through the use of application-specific training data, through a <a id="_idIndexMarker191"/>process called <strong class="bold">fine-tuning</strong>. Because the baseline model already contains a vast amount of general information about the language, the amount of training data can be considerably less than the training data required for some of the traditional approaches. These popular technologies include BERT and<a id="_idIndexMarker192"/> its many variations and <strong class="bold">Generative Pre-trained Transformers</strong> (<strong class="bold">GPTs</strong>) and <span class="No-Break">their variations.</span></p>
			<p>Pre-trained models will be discussed in detail in <a href="B19005_11.xhtml#_idTextAnchor193"><span class="No-Break"><em class="italic">Chapter 11</em></span></a><span class="No-Break">.</span></p>
			<h1 id="_idParaDest-67"><a id="_idTextAnchor081"/>Considerations for selecting technologies</h1>
			<p>This chapter has<a id="_idIndexMarker193"/> introduced four classes of <span class="No-Break">NLU technologies:</span></p>
			<ul>
				<li><span class="No-Break">Rule-based</span></li>
				<li>Statistical <span class="No-Break">machine learning</span></li>
				<li>Deep learning and <span class="No-Break">neural networks</span></li>
				<li><span class="No-Break">Pre-trained models</span></li>
			</ul>
			<p>How should we decide which technology or technologies should be employed to solve a specific problem? The considerations are largely practical and have to do with the costs and effort required to create a working solution. Let’s look at the characteristics of <span class="No-Break">each approach.</span></p>
			<p><em class="italic">Table 3.3</em> lists the four approaches to NLU that we’ve reviewed in this chapter and how they compare with respect to developer expertise, the amount of data required, the training time, accuracy, and cost. As <em class="italic">Table 3.3</em> shows, every approach has advantages and disadvantages. For small or simple problems that don’t require large amounts of data, the rule-based, deep learning, or pre-trained approaches should be strongly considered, at least for part of the pipeline. While pre-trained models are accurate and have relatively low development costs, developers may prefer to avoid the costs of cloud services or the<a id="_idIndexMarker194"/> costs of managing large models on their local <span class="No-Break">computer resources:</span></p>
			<table id="table003" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style"/>
						<td class="No-Table-Style">
							<p><strong class="bold">Developer </strong><span class="No-Break"><strong class="bold">expertise</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><strong class="bold">Amount of </strong><span class="No-Break"><strong class="bold">data required</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Training time</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Accuracy</strong></span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break"><strong class="bold">Cost</strong></span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Rule-based</span></p>
						</td>
						<td class="No-Table-Style">
							<p>High (linguists or domain <span class="No-Break">experts)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Small amount of domain-specific <span class="No-Break">data</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Large amount of time for experts to <span class="No-Break">write rules</span></p>
						</td>
						<td class="No-Table-Style">
							<p>High if rules <span class="No-Break">are accurate</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Rule development may be costly; computer time costs <span class="No-Break">are low</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Statistical</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Medium – use standard tools; some NLP/data science expertise <span class="No-Break">required</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Medium amount of domain-specific <span class="No-Break">data</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Large amount of time <span class="No-Break">for annotation</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Medium</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Data annotation may be costly; computer time costs <span class="No-Break">are low</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Deep <span class="No-Break">learning</span></p>
						</td>
						<td class="No-Table-Style">
							<p>High (data <span class="No-Break">scientists)</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Large amount of domain-specific <span class="No-Break">data</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Large amount of time for annotation; additional computer time for <span class="No-Break">training models</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Medium-high</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Charges for some cloud services or local computer <span class="No-Break">resources</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break">Pre-trained</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Medium – use standard tools with some data science <span class="No-Break">expertise</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Small amount of domain-specific <span class="No-Break">data</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Medium amount of time to label data for fine-tuning <span class="No-Break">models</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">High</span></p>
						</td>
						<td class="No-Table-Style">
							<p>Charges for some cloud services or local computer <span class="No-Break">resources</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 3.3 – Comparison between general approaches to NLU</p>
			<p>The most important considerations are the problem that’s being addressed and what the acceptable costs<a id="_idIndexMarker195"/> are. It should also be kept in mind that choosing one or another technology isn’t a permanent commitment, especially for approaches that rely on annotated data, which can be used for more than <span class="No-Break">one approach.</span></p>
			<h1 id="_idParaDest-68"><a id="_idTextAnchor082"/>Summary</h1>
			<p>In this chapter, w<a id="_idTextAnchor083"/>e surveyed the various techniques that can be used in NLU applications and learned several <span class="No-Break">important skills.</span></p>
			<p>We learned about what rule-based approaches are and the major rule-based techniques, including topics such as POS tagging and parsing. We then learned about the important traditional machine learning techniques, especially the ways that text documents can be represented numerically. Next, we focused on the benefits and drawbacks of the more modern deep learning techniques and the advantages of <span class="No-Break">pre-trained models.</span><a id="_idTextAnchor084"/></p>
			<p>In the next chapter, we will review the basics of getting started with NLU – installing Python, using Jupyter Labs and GitHub, using NLU libraries such as NLTK and spaCy, and how to choose <span class="No-Break">between libraries.</span></p>
		</div>
	</body></html>