<html><head></head><body>
		<div id="_idContainer1381">
			<h1 id="_idParaDest-196"><em class="italic"><a id="_idTextAnchor200"/>Chapter 9</em>: Multi-Agent Reinforcement Learning</h1>
			<p>If there is something more exciting than training a <strong class="bold">reinforcement</strong> <strong class="bold">learning</strong> (<strong class="bold">RL</strong>) agent to exhibit intelligent behavior, it is to train multiple of them to collaborate or compete. <strong class="bold">Multi-agent</strong> <strong class="bold">RL</strong> (<strong class="bold">MARL</strong>) is where you will really feel the potential in artificial intelligence. Many famous RL stories, such as AlphaGo or OpenAI Five, stemmed from MARL, which we introduce you to in this chapter. Of course, there is no free lunch, and MARL comes with lots of challenges along with its opportunities, which we will also explore. At the end of the chapter, we will train a bunch of tic-tac-toe agents through competitive self-play. So, at the end, you will have some companions to play some game against. </p>
			<p>This will be a fun chapter, and specifically we will cover the following topics:</p>
			<ul>
				<li>Introducing multi-agent reinforcement learning,</li>
				<li>Exploring the challenges in multi-agent reinforcement learning,</li>
				<li>Training policies in multi-agent settings,</li>
				<li>Training tic-tac-toe agents through self-play.</li>
			</ul>
			<p>Let's get started!</p>
			<h1 id="_idParaDest-197"><a id="_idTextAnchor201"/>Introducing multi-agent reinforcement learning</h1>
			<p>All <a id="_idIndexMarker890"/>of the problems and algorithms we have covered in the book so far involved a single agent being trained in an environment. On the other hand, in many applications from games to autonomous vehicle fleets, there are multiple decision-makers, agents, which train concurrently, but execute local policies (i.e., without a central decision-maker). This leads us to MARL, which involves a much richer set of problems and challenges than single-agent RL does. In this section, we give an overview of MARL landscape.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor202"/>Collaboration and competition between MARL agents</h2>
			<p>MARL <a id="_idIndexMarker891"/>problems can be classified into three different groups with respect to the structure of <a id="_idIndexMarker892"/>collaboration and competition between agents. Let's look into what those groups are and what types of applications fit into each group.</p>
			<h3>Fully cooperative environments</h3>
			<p>In this setting, all <a id="_idIndexMarker893"/>of the agents in the environment work towards a common long-term goal. The agents are credited equally for the return the environment reveals, so there is no incentive for any of the agents to deviate from the common goal.</p>
			<p>Here are some examples to fully cooperative environments:</p>
			<ul>
				<li><strong class="bold">Autonomous vehicle / robot fleets</strong>: There are many applications where a fleet of autonomous vehicles / robots could work towards accomplishing a common mission. One<a id="_idIndexMarker894"/> example is disaster recovery / emergency response / rescue missions, where the fleet tries to achieve tasks such as delivering emergency supplies to first responders, shutting of gas valves, removing debris from roads etc. Similarly, transportation problems as in supply chains or in the form of transporting a big object using multiple robots are good examples in this category.</li>
				<li><strong class="bold">Manufacturing</strong>: The whole<a id="_idIndexMarker895"/> idea behind Industry 4.0 is to have interconnected equipment and cyber-physical systems to achieve efficient production and service. If you think of a single manufacturing floor, in which there are usually many decision-making equipment, MARL is a natural fit to model such control problems.</li>
				<li><strong class="bold">Smart grid</strong>: In the <a id="_idIndexMarker896"/>emerging field of smart grid, many problems can be modeled in this category. An example is the problem of cooling a data center that involves many cooling units. Similarly, control of traffic lights in an intersection is another good example in this area. In fact, in <a href="B14160_17_Final_SK_ePub.xhtml#_idTextAnchor365"><em class="italic">Chapter 17</em></a><em class="italic">, Smart City and Cybersecurity</em>, we will model and solve this problem using MARL.</li>
			</ul>
			<p>Before moving into discussing other types of MARL environments, while we are at MARL for autonomous vehicles, let's briefly mention a useful platform, MACAD-Gym, for you to experiment with.</p>
			<h4>MACAD-Gym for multi-agent connected autonomous driving</h4>
			<p>MACAD-Gym is<a id="_idIndexMarker897"/> a Gym-based library for connected and autonomous driving applications in multi-agent settings, built on top of the famous CARLA simula<a id="_idTextAnchor203"/><a id="_idTextAnchor204"/>tor.</p>
			<div>
				<div id="_idContainer1364" class="IMG---Figure">
					<img src="image/B14160_09_001.jpg" alt="Figure 9.1: MACAD-Gym platform (source: MACAD-Gym GitHub repo)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1: MACAD-Gym platform (source: MACAD-Gym GitHub repo)</p>
			<p>The platform <a id="_idIndexMarker898"/>provides a rich set of scenarios involving cars, pedestrians, traffic lights, bikes etc., depicted in Figure 9.1. In more detail, MACAD-Gym environments contain a variety of MARL configurations as in the following example:</p>
			<p class="source-code"><strong class="bold">Environment-ID: Short description</strong></p>
			<p class="source-code"><strong class="bold">{'HeteNcomIndePOIntrxMATLS1B2C1PTWN3-v0': </strong></p>
			<p class="source-code"><strong class="bold">'Heterogeneous, Non-communicating, '</strong></p>
			<p class="source-code"><strong class="bold">'Independent,Partially-Observable '</strong></p>
			<p class="source-code"><strong class="bold">'Intersection Multi-Agent scenario '</strong></p>
			<p class="source-code"><strong class="bold">'with Traffic-Light Signal, 1-Bike, '</strong></p>
			<p class="source-code"><strong class="bold">'2-Car,1-Pedestrian in Town3, '</strong></p>
			<p class="source-code"><strong class="bold">'version 0'}</strong></p>
			<p>To see what you can do with MACAD-Gym, check out its Github repo, developed and maintained by Praveen Palanisamy, at <a href="https://github.com/praveen-palanisamy/macad-gym">https://github.com/praveen-palanisamy/macad-gym</a>.</p>
			<p>After this short detour, let's proceed to fully competitive settings in MARL.</p>
			<h3>Fully competitive environments</h3>
			<p>In fully competitive environments, the <a id="_idIndexMarker899"/>success of one of the agents means failure for the others. Therefore, such settings are modelled as zero-sum games:</p>
			<div>
				<div id="_idContainer1365" class="IMG---Figure">
					<img src="image/Formula_09_001.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_09_002.png" alt=""/> is the reward for the <img src="image/Formula_09_003.png" alt=""/> agent. </p>
			<p>Some examples to fully competitive environments are the following:</p>
			<ul>
				<li><strong class="bold">Board games</strong>: This is the <a id="_idIndexMarker900"/>classic example for such environments, such as chess, Go, and tic-tac-toe. </li>
				<li><strong class="bold">Adversarial settings</strong>: In situations <a id="_idIndexMarker901"/>where we want to minimize the risk of failure for an agent in real-life, we might train it against adversarial agents. This creates a fully competitive environment.</li>
			</ul>
			<p>Finally, let's take a look at mixed cooperative-competitive environments.</p>
			<h3>Mixed cooperative-competitive environments</h3>
			<p>A third type of environments involves both collaboration and cooperation between agents. These environments are <a id="_idIndexMarker902"/>usually modelled as general-sum games:</p>
			<div>
				<div id="_idContainer1368" class="IMG---Figure">
					<img src="image/Formula_09_004.jpg" alt=""/>
				</div>
			</div>
			<p>Here <img src="image/Formula_09_005.png" alt=""/> is the reward for the <img src="image/Formula_09_006.png" alt=""/> agent and <img src="image/Formula_09_007.png" alt=""/> is some fixed total reward that can be collected by the agents.</p>
			<p>Here are some examples to mixed environments:</p>
			<ul>
				<li><strong class="bold">Team competitions</strong>: When<a id="_idIndexMarker903"/> there are teams of agents competing against each other, the agents within a team collaborate to defeat the other teams.</li>
				<li><strong class="bold">Economy</strong>: If you <a id="_idIndexMarker904"/>think about the economic activities we are involved in, it is a mix of competition and cooperation. A nice example to this is how tech companies such as Microsoft, Google, Facebook, and Amazon compete against each other for certain businesses while collaborating on some open-source projects to advance the software technology.<p class="callout-heading">Info</p><p class="callout">At this point, it is worth taking a pause to watch OpenAI's demo on agents playing hide-and-seek in teams. The agents develop very cool cooperation and competition strategies after playing against each other for a large number of episodes, inspiring us for the potential in RL towards artificial general intelligence. See Figure 9.2 for a quick snapshot and the link to the video.</p></li>
			</ul>
			<div>
				<div id="_idContainer1372" class="IMG---Figure">
					<img src="image/B14160_09_002.jpg" alt="Figure 9.2: OpenAI's agents playing hide-and-seek (Source: https://youtu.be/kopoLzvh5jY)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2: OpenAI's agents playing hide-and-seek (Source: <a href="https://youtu.be/kopoLzvh5jY">https://youtu.be/kopoLzvh5jY</a>)</p>
			<p>Now that we have covered the fundamentals, next, let's look into some of the challenges with MARL.</p>
			<h1 id="_idParaDest-199"><a id="_idTextAnchor205"/>Exploring the challenges in multi-agent reinforcement learning</h1>
			<p>In the earlier chapters in this<a id="_idIndexMarker905"/> book, we discussed many challenges in reinforcement learning. In particular, the dynamic programming methods we initially introduced are not able to scale to problems with complex and large observation and action spaces. Deep reinforcement learning approaches, on the other hand, although capable of handling complex problems, lack theoretical guarantees and therefore required many tricks to stabilize and converge. Now that we talk about problems in which there are more than one agent learning, interacting with each other, and affecting the environment; the challenges and complexities of single-agent RL are multiplied. For this reason, many results in MARL are empirical. </p>
			<p>In this section, we discuss what makes MARL specifically complex and challenging.</p>
			<h2 id="_idParaDest-200"><a id="_idTextAnchor206"/>Non-stationarity </h2>
			<p>The mathematical framework<a id="_idIndexMarker906"/> behind single-agent RL is the Markov decision process (MDP), which establishes that the environment dynamics depend on the state it is in and not the history. This suggests that the environment is stationary, on which many approaches rely for convergence. Now that there are multiple agents in the environment that are learning hence changing their behavior over time, that fundamental assumption falls apart and prevents us analyzing MARL the same way we analyze single-agent RL. </p>
			<p>As an example, consider off-policy methods such as Q-learning with replay buffer. In MARL, using such approaches is especially challenging because the experience that was collected a while ago might be wildly different than how the environment (in part, the other agents) responds to the actions of a single agent. </p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor207"/>Scalability </h2>
			<p>One possible solution to <a id="_idIndexMarker907"/>non-stationarity is to account for the actions of the other agents, such as using a joint action space. As the number of agents increases, this becomes increasingly intractable, which makes scalability an issue in MARL. </p>
			<p>Having said this, it is somewhat easier to analyze how the agent behavior could converge when there are only two agents in the environment. If you are familiar with game theory, a common way to look at such systems is to understand equilibrium points where neither of the agents benefit from changing their policies. </p>
			<p class="callout-heading">Info </p>
			<p class="callout">If you need a brief introduction to game theory and Nash equilibrium, check out this video at <a href="https://www.youtube.com/watch?v=0i7p9DNvtjk">https://www.youtube.com/watch?v=0i7p9DNvtjk</a> </p>
			<p>This analysis gets significantly harder when there are more than two agents in the environment, which makes large-scale MARL very difficult to understand. </p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor208"/>Unclear reinforcement learning objective </h2>
			<p>In single-agent RL, the<a id="_idIndexMarker908"/> objective is clear: To maximize the expected cumulative return. On the other hand, there is no such a unique objective defined MARL.  </p>
			<p>Think about a chess game in which we try to train a very good chess agent. To this end, we train many agents competing with each <a id="_idIndexMarker909"/>other using <strong class="bold">self-play</strong>. How would you set the objective of this problem? First thought could be to maximize the reward of the best agent. But this could result in making all the agents terrible players but one. This would certainly not what we would want. </p>
			<p>A popular objective in MARL is to achieve convergence to Nash equilibrium. This often works well, but it also has disadvantages when the agents are not fully rational. Moreover, Nash equilibrium naturally implies overfitting to the policies of the other agents, which is not necessarily desirable. </p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor209"/>Information sharing </h2>
			<p>Another important challenge in <a id="_idIndexMarker910"/>MARL is to design the information sharing structure between the agents. There are three alternative information structures we can consider: </p>
			<ul>
				<li><strong class="bold">Fully centralized</strong>: In this structure, all of the information collected by the agents are processed by a central mechanism and the local policies would leverage this centralized knowledge. The advantage of this structure is the full coordination between the agents. On the other hand, this could lead to an optimization problem that won't scale as the number of the agents grow. </li>
				<li><strong class="bold">Fully decentralized</strong>: In this structure, no information is exchanged between the agents and each agent would act based on their local observations. The obvious benefit here is that there is no burden of a centralized coordinator. On the flip side, the actions of the agents would be suboptimal due to their limited information about the environment. In addition, RL algorithms might have a hard time to converge when the training is fully independent due to high partial observability. </li>
				<li><strong class="bold">Decentralized but networked agents</strong>: This structure would allow information exchange between small groups of (neighbor) agents. In turn, this would help the information spread among them. The challenge here would be to create a robust communication structure that would work under different conditions of the<a id="_idIndexMarker911"/> environment. </li>
			</ul>
			<p>Depending on the objective of the RL problem as well as the availability of computational resources, different approaches could be preferred. Consider a cooperative environment in which a large swarm of robots are trying to achieve a common goal. In this problem, fully centralized or networked control might make sense. In a fully competitive environment, such as a strategy video game, a fully decentralized structure might be preferred since there would be no common goal between the agents. </p>
			<p>After this much theory, it is now time to go into practice! Soon, we will train a tic-tac-toe agent that you can play against during your meetings or classes. Let's first describe how we will do the training and then go into implementation.</p>
			<h1 id="_idParaDest-204"><a id="_idTextAnchor210"/>Training policies in multi-agent settings</h1>
			<p>There are many <a id="_idIndexMarker912"/>algorithms and approaches designed for MARL, which can be <a id="_idIndexMarker913"/>classified in the following two broad categories.</p>
			<ul>
				<li><strong class="bold">Independent learning</strong>: This approach suggests training agents individually while treating the other agents in the environment as part of the environment.</li>
				<li><strong class="bold">Centralized training and decentralized execution</strong>: In this approach, there is a centralized controller that uses information from multiple agents during training. At the time of execution (inference), the agents locally execute the policies, without relying on a central mechanism.</li>
			</ul>
			<p>Generally speaking, we can take any of the algorithms we covered in one of the previous chapters and use it in a multi-agent setting to train policies via independent learning, which, as it turns out, is a very competitive alternative to specialized MARL algorithms. So rather than dumping more theory and notation on you, in this chapter, we will skip discussing the technical details of any specific MARL algorithm and refer you to literature for that.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">An easy and highly recommended read on a comparison of deep MARL algorithms is by (Papoudakis et al. 2020). Just visit the references section to find the link to the paper.</p>
			<p>So, we will use<a id="_idIndexMarker914"/> independent learning. How does it work, though? Well, it requires us to:</p>
			<ul>
				<li>Have an environment with <a id="_idIndexMarker915"/>multiple agents,</li>
				<li>Maintain policies to support the agents,</li>
				<li>Appropriately assign the rewards coming out of the environment to the agents.</li>
			</ul>
			<p>It would be tricky for us to come up with a proper framework here to handle the points above. Fortunately, RLlib has a multi-agent environment to come to the rescue. Next, let's see how it works.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor211"/>RLlib multi-agent environment</h2>
			<p>RLlib's multi-agent <a id="_idIndexMarker916"/>environment flexibly allows us to hook up with one of the algorithms that you already know to use for MARL. In fact, RLlib documentation conveniently shows which algorithms are compatible with this environment type:</p>
			<div>
				<div id="_idContainer1373" class="IMG---Figure">
					<img src="image/B14160_09_003.jpg" alt="Figure 9.3: RLlib's algorithm list shows multi-agent compatibility (Source: https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.3: RLlib's algorithm list shows multi-agent compatibility (Source: <a href="https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html">https://docs.ray.io/en/releases-1.0.1/rllib-algorithms.html</a>)</p>
			<p>In that list, you will also see a separate section for MARL-specific algorithms. In this chapter, we will use PPO.</p>
			<p>Of course, the next step is to understand how to use an algorithm we selected with a multi-agent environment. At this point, we need to make a key differentiation: <em class="italic">Using RLlib, we will train policies, not the agents (at least directly). An agent will be mapped to one of the policies that are being trained to retrieve actions.</em> </p>
			<p>RLlib documentation<a id="_idIndexMarker917"/> illustrates this with the following figure:</p>
			<div>
				<div id="_idContainer1374" class="IMG---Figure">
					<img src="image/B14160_09_004.jpg" alt="Figure 9.4: Relationship between agents and policies in RLlib (source: https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical)&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4: Relationship between agents and policies in RLlib (source: <a href="https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical">https://docs.ray.io/en/releases-1.0.1/rllib-env.html#multi-agent-and-hierarchical</a>)</p>
			<p>In case you have not realized, this gives us a very powerful framework to model a MARL environment. For example, we can flexibly add agents to the environment, remove them, and train multiple policies for the same task. All is fine as far as we specify the mapping between the policies and agents.</p>
			<p>With that, let's look into what the training loop in RLlib requires to be used with a multi-agent environment:</p>
			<ol>
				<li>List of policies with corresponding ids. These are what will be trained.</li>
				<li>A function that maps a given agent id to a policy id, so that RLlib knows where the actions for a given agent will come from.</li>
			</ol>
			<p>Once this is set, the environment will use the Gym convention communicate with RLlib. The difference will be that observations, rewards, and terminal statements will be emitted for multiple agents in the environment. For example, a reset function will return a dictionary of observations like the following:</p>
			<p class="source-code">&gt; env.reset()</p>
			<p class="source-code">{"agent_1": [[...]], "agent_2":[[...]], "agent_4":[[...]],...</p>
			<p>Similarly, the actions by the policies will be passed to the agents from which we received an observation, similar to:</p>
			<p class="source-code">... = env.step(actions={"agent_1": ..., "agent_2":..., "agent_4":..., ...</p>
			<p>This means that if the<a id="_idIndexMarker918"/> environment returns an observation for an agent, it is asking an action back. </p>
			<p>So far so good! This should suffice to give you some idea about how it works. Things will be clearer when we go into the implementation!</p>
			<p>Soon, we will train tic-tac-toe policies, as we mentioned. The agents that will use these policies will compete against each other to learn how to play the game. This is called <strong class="bold">competitive self-play</strong>, which we discuss next.</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor212"/>Competitive self-play</h2>
			<p>Self-play is<a id="_idIndexMarker919"/> a great tool to train RL agents for competitive tasks, which include things like board games, multi-player video games, adversarial scenarios etc. Many of the famous RL agents you have heard about are trained this way, such as AlphaGo, OpenAI Five for Dota 2, and the <a id="_idIndexMarker920"/>StarCraft II agent of DeepMind.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">The story of OpenAI Five<a id="_idIndexMarker921"/> is a very interesting one, showing how the project started and evolved to what it is today. The blog posts on the project gives many beneficial information, from hyperparameters used in the models to how the OpenAI team overcame interesting challenges throughout the work. You can find the project page at <a href="https://openai.com/projects/five/">https://openai.com/projects/five/</a>. </p>
			<p>One of the drawbacks of vanilla self-play<a id="_idIndexMarker922"/> is that the agents, who only see the other agents trained the same way, tend to overfit to each other's strategies. To overcome this, it makes sense to train multiple policies and pit them against each other, which is also what we will do in this chapter.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">Overfitting is such a<a id="_idIndexMarker923"/> challenge in self-play that even training multiple policies and simply setting them against each other is not enough. DeepMind created a "League" of agents/policies, like a basketball league, to obtain a really competitive training environment, which led to the success of their StarCraft II agents. They explain their approach in a nice blog at <a href="https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning">https://deepmind.com/blog/article/AlphaStar-Grandmaster-level-in-StarCraft-II-using-multi-agent-reinforcement-learning</a></p>
			<p>Finally, it is time to experiment with multi-agent reinforcement learning!</p>
			<h1 id="_idParaDest-207"><a id="_idTextAnchor213"/>Training tic-tac-toe agents through self-play</h1>
			<p>In this section, we will <a id="_idIndexMarker924"/>provide you with some key explanations of the code in our Github repo to get a better grasp of MARL with RLlib while training tic-tac-toe agents on a 3x3 board. For the full code, you can refer to <a href="https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python">https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python</a>.</p>
			<div>
				<div id="_idContainer1375" class="IMG---Figure">
					<img src="image/B14160_09_005.jpg" alt="Figure 9.5: A 3x3 tic-tac-toe. For the image credit and to learn how it is played, see https://en.wikipedia.org/wiki/Tic-tac-toe&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5: A 3x3 tic-tac-toe. For the image credit and to learn how it is played, see <a href="https://en.wikipedia.org/wiki/Tic-tac-toe">https://en.wikipedia.org/wiki/Tic-tac-toe</a></p>
			<p>Let's started with designing the multi-agent environment.</p>
			<h2 id="_idParaDest-208"><a id="_idTextAnchor214"/>Designing the multi-agent tic-tac-toe environment</h2>
			<p>In the game, we have <a id="_idIndexMarker925"/>two agents, X and O, playing the game. We will train four policies for the agents to pull their actions from, and each policy can play either an X or O. We construct the environment class as follows:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Chapter09/tic_tac_toe.py</p>
			<p class="source-code">class TicTacToe(MultiAgentEnv):</p>
			<p class="source-code">    def __init__(self, config=None):</p>
			<p class="source-code">        self.s = 9</p>
			<p class="source-code">        self.action_space = Discrete(self.s)</p>
			<p class="source-code">        self.observation_space = MultiDiscrete([3] * self.s)</p>
			<p class="source-code">        self.agents = ["X", "O"]</p>
			<p class="source-code">        self.empty = " "</p>
			<p class="source-code">        self.t, self.state, self.rewards_to_send = \</p>
			<p class="source-code">                                    self._reset()</p>
			<p>Here, 9 refers to the number of squares on the board, each of which can be filled by either X, O, or none.</p>
			<p>We reset this environment as follows:</p>
			<p class="source-code">    def _next_agent(self, t):</p>
			<p class="source-code">        return self.agents[int(t % len(self.agents))]</p>
			<p class="source-code">    def _reset(self):</p>
			<p class="source-code">        t = 0</p>
			<p class="source-code">        agent = self._next_agent(t)</p>
			<p class="source-code">        state = {"turn": agent, </p>
			<p class="source-code">                 "board": [self.empty] * self.s}</p>
			<p class="source-code">        rews = {a: 0 for a in self.agents}</p>
			<p class="source-code">        return t, state, rews</p>
			<p>However, we don't pass this state directly to the policy as it is just full of characters. We process it so that, for the <a id="_idIndexMarker926"/>player that is about to play, its own marks are always represented as 1s and the other player's marks are always represented as 2s.</p>
			<p class="source-code">    def _agent_observation(self, agent):</p>
			<p class="source-code">        obs = np.array([0] * self.s)</p>
			<p class="source-code">        for i, e in enumerate(self.state["board"]):</p>
			<p class="source-code">            if e == agent:</p>
			<p class="source-code">                obs[i] = 1</p>
			<p class="source-code">            elif e == self.empty:</p>
			<p class="source-code">                pass</p>
			<p class="source-code">            else:</p>
			<p class="source-code">                obs[i] = 2</p>
			<p class="source-code">        return obs</p>
			<p>This processed observation is what is passed to the policy:</p>
			<p class="source-code">    def reset(self):</p>
			<p class="source-code">        self.t, self.state, self.rewards_to_send =\</p>
			<p class="source-code">                            self._reset()</p>
			<p class="source-code">        obs = {self.state["turn"]: \</p>
			<p class="source-code">               self._agent_observation(self.state["turn"])}</p>
			<p class="source-code">        return obs</p>
			<p>Finally, the <strong class="source-inline">step</strong> method processes the action for the player and proceeds the environment to the next step. For a win, the player gets a <img src="image/Formula_09_008.png" alt=""/>, and <img src="image/Formula_09_009.png" alt=""/> for a loss. Notice that the policies may suggest putting the mark on an already-occupied square, a behavior that is<a id="_idIndexMarker927"/> penalized by a <img src="image/Formula_09_010.png" alt=""/> points.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor215"/>Configuring the trainer</h2>
			<p>We create 4 <a id="_idIndexMarker928"/>policies to train, assign them some ids, and specify their observation and action spaces. Here is how we do it:</p>
			<p class="SC---Heading" lang="en-US" xml:lang="en-US">Chapter09/ttt_train.py</p>
			<p class="source-code">    env = TicTacToe()</p>
			<p class="source-code">    num_policies = 4</p>
			<p class="source-code">    policies = {</p>
			<p class="source-code">        "policy_{}".format(i): (None, </p>
			<p class="source-code">                                env.observation_space, </p>
			<p class="source-code">                                env.action_space, {})</p>
			<p class="source-code">        for i in range(num_policies)}</p>
			<p>While creating the config dictionary to pass to the trainer, we map the agents to the policies. To mitigate overfitting, rather than assigning a specific policy to a given agent, we randomly pick the policy to retrieve the action from for the agent that is about to play.</p>
			<p class="source-code">    policy_ids = list(policies.keys())</p>
			<p class="source-code">    config = {</p>
			<p class="source-code">        "multiagent": {</p>
			<p class="source-code">            "policies": policies,</p>
			<p class="source-code">            "policy_mapping_fn": (lambda agent_id: \</p>
			<p class="source-code">                           random.choice(policy_ids)),</p>
			<p class="source-code">        },</p>
			<p class="source-code">...</p>
			<p>During training, we<a id="_idIndexMarker929"/> save the models as they improve. Since there are multiple policies involved, as a proxy to measure the progress, we check if the episodes are getting longer with valid moves. Our hope is that as the agents get competitive, more and more games will result in draws, at which point the board is full of marks.</p>
			<p class="source-code">    trainer = PPOTrainer(env=TicTacToe, config=config)</p>
			<p class="source-code">    best_eps_len = 0</p>
			<p class="source-code">    mean_reward_thold = -1</p>
			<p class="source-code">    while True:</p>
			<p class="source-code">        results = trainer.train()</p>
			<p class="source-code">        if results["episode_reward_mean"] &gt; mean_reward_thold\</p>
			<p class="source-code">           and results["episode_len_mean"] &gt; best_eps_len:</p>
			<p class="source-code">            trainer.save("ttt_model")</p>
			<p class="source-code">            best_eps_len = results["episode_len_mean"]</p>
			<p class="source-code">        if results.get("timesteps_total") &gt; 10 ** 7:</p>
			<p class="source-code">            break</p>
			<p>That's it! Now the fun of watching the training!</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor216"/>Observing the results</h2>
			<p>Initially in the game, there <a id="_idIndexMarker930"/>will be lots of invalid moves, resulting in extended episode lengths and excessive penalties for the agents. Therefore, the mean agent reward plot will look like the following:</p>
			<div>
				<div id="_idContainer1379" class="IMG---Figure">
					<img src="image/B14160_09_006.jpg" alt="Figure 9.6: Average agent reward&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6: Average agent reward</p>
			<p>Notice how this starts<a id="_idIndexMarker931"/> in deep negatives to converge to zero, indicating draws as the common result. In the meantime, you should see the episode length converging to 9:</p>
			<div>
				<div id="_idContainer1380" class="IMG---Figure">
					<img src="image/B14160_09_007.jpg" alt="Fig 9.7: Episode length progress&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Fig 9.7: Episode length progress</p>
			<p>When you see the <a id="_idIndexMarker932"/>competition on fire, you can stop the training! What will be even more interesting is to play against the AI by running the script <strong class="source-inline">ttt_human_vs_ai.py</strong> or watch them compete by running <strong class="source-inline">ttt_ai_vs_ai.py</strong>.</p>
			<p>With that, we conclude this chapter. This was a fun one, wasn't it? Let's summarize the learnings from this chapter next.</p>
			<h1 id="_idParaDest-211"><a id="_idTextAnchor217"/>Summary</h1>
			<p>In this chapter, we covered multi-agent reinforcement learning. This branch of RL is more challenging than others due to multiple decision-makers influencing the environment and also evolving over time. After introducing some MARL concepts, we explored these challenges in detail. We then proceeded to train tic-tac-toe agents through competitive self-play using RLlib. And they were so competitive that they kept coming to a draw at the end of the training!</p>
			<p>In the next chapter, we switch gears to discuss an emerging approach in reinforcement learning, called Machine Teaching, which brings the subject matter expert, you, more actively into the process to guide the training. Hoping to see you there soon!</p>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor218"/>References</h1>
			<ol>
				<li value="1">Mosterman, P. J. et al. (2014). A heterogeneous fleet of vehicles for automated humanitarian missions. Computing in Science &amp; Engineering, vol. 16, issue 3, pg. 90-95. URL: <a href="http://msdl.cs.mcgill.ca/people/mosterman/papers/ifac14/review.pdf">http://msdl.cs.mcgill.ca/people/mosterman/papers/ifac14/review.pdf</a></li>
				<li>Papoudakis, Georgios, et al. (2020). Comparative Evaluation of Multi-Agent Deep Reinforcement Learning Algorithms. arXiv.org, <a href="http://arxiv.org/abs/2006.07869">http://arxiv.org/abs/2006.07869</a></li>
				<li>Palanisamy, Praveen. (2019). Multi-Agent Connected Autonomous Driving Using Deep Reinforcement Learning. arxiv.org, <a href="https://arxiv.org/abs/1911.04175v1">https://arxiv.org/abs/1911.04175v1</a></li>
			</ol>
		</div>
	</body></html>