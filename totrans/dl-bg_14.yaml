- en: Deep and Wide Neural Networks
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have covered a variety of unsupervised deep learning methodologies
    that can lead to many interesting applications, such as feature extraction, information
    compression, and data augmentation. However, as we move toward supervised deep
    learning methodologies that can perform classification or regression, for example,
    we have to begin by addressing an important question related to neural networks
    that might be in your mind already: *what is the difference between wide and deep
    neural networks?*
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, you will implement deep and wide neural networks to see the
    difference in the performance and complexities of both. As a bonus, we will cover
    the concepts of dense networks and sparse networks in terms of the connections
    between neurons. We will also optimize the dropout rates in our networks to maximize
    the generalization ability of the network, which is a critical skill to have today.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is organized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Wide neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense deep neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sparse deep neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wide neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before we discuss the types of neural networks covered in this chapter, it might
    be appropriate to revisit the definition of deep learning and then continue addressing
    all these types.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning revisited
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Recently, on February 9, 2020, Turing Award winner Yann LeCun gave an interesting
    talk at the AAAI-20 conference in New York City. In his talk, he provided clarity
    with respect to what deep learning is, and before we give this definition here,
    let me remind you that LeCun (along with J. Bengio, and G. Hinton) is considered
    one of the fathers of deep learning, and received the Turing Award for precisely
    his achievements in the area. Therefore, what he has to say is important. Secondly,
    throughout this book, we have not given a strong definition of what deep learning
    is; people might be thinking that it refers to deep neural networks, but that
    is not factually correct – it is much more than that, so let's set the record
    straight once and for all.
  prefs: []
  type: TYPE_NORMAL
- en: '"It is not just supervised learning, it is not just neural networks, **Deep
    Learning** is the idea of building a system by assembling parametrized modules
    into a (possibly dynamic) computation graph, and training it to perform a task
    by optimizing the parameters using a gradient-based method." - Yann LeCun'
  prefs: []
  type: TYPE_NORMAL
- en: Most of the models we have covered so far fit this definition, with the exception
    of the simple introductory models that we used to explain the more complex ones.
    The only reason why those introductory models are not included as deep learning
    is that they are not necessarily part of a computation graph; we are referring
    specifically to the perceptron (Rosenblatt, F. (1958)*)*, and the corresponding
    **Perceptron Learning Algorithm** (**PLA**) (Muselli, M. (1997)*)*. However, from
    the **Multilayer Perceptron** (**MLP**) and forward, all algorithms presented
    so far are, in fact, deep learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: This is an important distinction to make at this point since this is a deep
    learning book, and you are *learning* about deep learning. We are about to learn
    some of the most interesting topics in deep learning and we need to keep a focus
    on what deep learning is. We will talk about deep networks and wide networks;
    however, both are deep learning. In fact, all the models we will be discussing
    here are deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: With this clarification in mind, let's define what a wide network is.
  prefs: []
  type: TYPE_NORMAL
- en: Wide layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'What makes a neural network **wide** is a relatively large number of neurons
    in a relatively small number of hidden layers. Recent developments in deep learning
    have even made possible the computational treatment of wide networks with an infinite
    amount of neural units (Novak, R., et al. (2019)*)*. Although this is a very nice
    advance in the field, we will limit our layers to have a reasonable number of
    units. To make our comparison with a *less wide* network, we will create a wide
    network for the CIFAR-10 dataset. We will create the architecture shown in the
    following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/556ac356-0a15-48e3-91b0-68750f573abf.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – Network architecture of a wide network for CIFAR-10
  prefs: []
  type: TYPE_NORMAL
- en: One important aspect of neural networks that we will consider from now on is
    the number of **parameters**.
  prefs: []
  type: TYPE_NORMAL
- en: In deep learning, the number of **parameters** is defined as the number of variables
    that the learning algorithm needs to estimate through gradient descent techniques
    in order to minimize the loss function. The great majority of parameters are,
    usually, the weights of a network; however, other parameters might include biases,
    mean and standard deviation for *batch normalization*, filters for convolutional
    networks, memory vectors for recurrent networks, and many others.
  prefs: []
  type: TYPE_NORMAL
- en: Knowing the number of parameters has been of particular importance because,
    in an ideal world, you want to have more data samples than variables you want
    to learn. In other words, an ideal learning scenario includes more data than parameters.
    If you think about it, it is intuitive; imagine having a matrix with two rows
    and three columns. The three columns describe the color representation in red,
    green, and blue of a fruit. The two rows correspond to one sample for an orange
    and another one for an apple. If you want to build a linear regression system
    to determine the probability of the data being from an orange, you certainly would
    like to have a lot more data! Especially since there are many apples that may
    have a color that is close to the color of an orange. More data is better! But
    if you have more parameters, like in linear regression where you have as many
    parameters as columns, then your problem is usually described as an *ill-posed *problem.
    In deep learning, this phenomenon is known as **over-parametrization**.
  prefs: []
  type: TYPE_NORMAL
- en: Only in deep learning do over-parametrized models work really well. There is
    research that has shown that in the particular case of neural networks, given
    the redundancy of data as it flows in non-linear relationships, the loss functions
    can produce smooth landscapes (Soltanolkotabi, M., et al. (2018)). This is particularly
    interesting because then we could prove that over-parametrized deep learning models
    will converge to very good solutions using gradient descent (Du, S. S., et al.
    (2018)).
  prefs: []
  type: TYPE_NORMAL
- en: Summaries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In Keras, there is a function called `summary()` that, called from a `Model`
    object, can give the total number of parameters to be estimated. For example,
    let''s create the wide network in *Figure 11.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This code produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The summary produced here indicates that the total number of parameters in the
    model is 18,911,242\. This is to show that a simple wide network can have nearly
    19 million parameters for a problem with 3,072 features. This is clearly an over-parametrized
    model on which we will perform gradient descent to learn those parameters; in
    other words, this is a deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Names
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Another new thing that we will introduce in this chapter is the use of **names**
    for individual pieces of the Keras models. You should have noticed that in the
    preceding code, the script contains a new argument with a string assigned to it;
    for example, `Dropout(0.5, **name**='d1')`. This is used internally to keep track
    of the names of pieces in the model. This can be good practice; however, it is
    not required. If you do not provide names, Keras will automatically assign generic
    names to each individual piece. Assigning names to elements can be helpful when
    saving or restoring models (we will do that soon enough – be patient), or can
    be useful when printing summaries, like the preceding one.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at the dataset that we will load. Precisely, the data mentioned
    earlier that has 3,072 dimensions, called CIFAR-10.
  prefs: []
  type: TYPE_NORMAL
- en: The CIFAR-10 dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The dataset we will work with in this chapter is called **CIFAR-10**. It comes
    from the acronym **Canadian Institute For Advanced Research** (**CIFAR**). The
    number 10 comes from the number of classes with which the dataset is organized.
    It is a dataset of color images that also has an alternative database with 100
    different objects, known as CIFAR-100; however, we will focus on CIFAR-10 for
    now. Each color image is ![](img/4de98ef3-ecc3-425b-8729-23683bac81a5.png) pixels.
    Its total dimensions, considering the color channels, is ![](img/7cc49236-3995-41c3-a6b9-2a8a255a67c9.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The diagram in *Figure 11.1* has one image sample and *Figure 11.2* has an
    example for each class within the test set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3fa9732f-d25a-4a1b-b7f4-81561e948eaf.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – Sample color images for each class in the CIFAR-10 dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'This dataset can be loaded by executing the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This downloads the data automatically and produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: These things are nothing new except for the dataset. For more information about
    how this dataset was prepared, please check [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml), *Preparing
    Data*, where we go over how to convert data into usable data by normalizing it
    and converting targets to one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: The output we receive by printing the shape of the dataset, using the `.shape`
    attribute of NumPy arrays, tells us that we have 50,000 samples to train with,
    and 10,000 samples on which to test our training performance. This is the standard
    split in the deep learning community and helps the comparison among methodologies.
  prefs: []
  type: TYPE_NORMAL
- en: New training tools
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'With the code we have so far, we can easily begin the training process by invoking
    the `fit()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This is nothing new; we covered all these details in [Chapter 9](c7b8496e-70e6-47ab-8746-d5893a10493d.xhtml), *Variational
    Autoencoders*. However, we want to introduce new important tools into the mix
    that will help us train a better model, much more efficiently, and preserve our
    best-trained model.
  prefs: []
  type: TYPE_NORMAL
- en: Saving or loading models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Saving our trained model is important if we want to sell a product, or distribute
    a working architecture, or to control versions of models. These models can be
    saved by calling on either of the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`save()`, used to save the whole model, including optimizer states such as
    the gradient descent algorithm, the number of epochs, the learning rate, and others.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`save_weights()`, used to save only the parameters of the model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For example, we can save our model''s weights as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This will create a file in your local disk called `widenet.hdf5`. This type
    of file extension is for a standard file format called **Hierarchical Data Format**
    (**HDF**), which enables consistency across common platforms, and, therefore,
    the easy sharing of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can re-load a saved model later on by simply executing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Note that doing this relies on you building the model first, that is, creating **exactly**
    all the layers of the model in the exact same order and with the exact same names.
    An alternative, to save all the effort of reconstructing the model exactly, is
    to use the `save()` method.
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'However, the downside of using the `save()` method is that to load the model
    you will need to import an additional library, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: This essentially removes the need to re-create the model. Throughout this chapter,
    we will be saving our model weights simply so that you get used to it. Now let's
    take a look at how to use **callbacks**, which are interesting ways to monitor
    the learning process. We will start with a **callback **for reducing the learning
    rate.
  prefs: []
  type: TYPE_NORMAL
- en: Reducing the learning rate on the fly
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Keras has a superclass for **callbacks**, found in `tensorflow.keras.callbacks`, where
    we have, among other nice things, a class for reducing the learning rate of the
    learning algorithm. If you don't remember what the **learning rate** is, feel
    free to go back to [Chapter 6](a6dd89cc-54bd-454d-8bea-7dd4518e85b0.xhtml), *Training
    Multiple Layers of Neurons*, to review the concept. But, as a quick recap, the
    learning rate controls how big the steps that are taken to update the parameters
    of the model in the direction of the gradient are.
  prefs: []
  type: TYPE_NORMAL
- en: The problem is that, many times, you will encounter that certain types of deep
    learning models *get stuck* in the learning process. By *getting stuck* we mean
    that there is no progress being made toward reducing the loss function either
    on the training or validation set. The technical term the *professionals *use
    is that the learning looks like a **plateau**. It is a problem that is evident
    when you look at how the loss function is minimized across epochs because it looks
    like a *plateau*, that is, a flat line. Ideally, we want to see the loss decreasing
    at every epoch, and it is usually the case for the first few epochs, but there
    can be a time when reducing the learning rate can help the learning algorithm
    to *focus* by making small changes to the existing acquired knowledge, that is,
    the learned parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The class we are discussing here is called `ReduceLROnPlateau`. You can load
    it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'To use this library, you will have to use the `callbacks` argument in the `fit()`
    function after defining it like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'In this code fragment, we call `ReduceLROnPlateau` with the following arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '`monitor=''val_loss''`, this is the default value, but you can change it to
    look for a plateau in the `''loss''` curve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`factor=0.1`, this is the default value, and it is the rate by which the learning
    rate will be reduced. For example, the default learning rate for the Adam optimizer
    is 0.001, but when a plateau is detected, it will be multiplied by 0.1, leading
    to a new updated learning rate of 0.0001.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patience=20`, the default value is 10, and is the number of epochs with no
    improvement in the monitored loss, which will be considered a plateau.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are other arguments that you can use in this method, but these are the
    most popular, in my opinion.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at another important callback: *early stopping*.'
  prefs: []
  type: TYPE_NORMAL
- en: Stopping the learning process early
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'This next callback is interesting because it allows you to stop the training
    if there is no progress being made and **it allows you to keep the best version
    of the model **during the learning process. It is found in the same class as the
    preceding one and is called `EarlyStopping()`, and you can load it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The early stopping callback essentially lets you stop the training process
    if there has been no progress in the last few epochs, as specified in the `patience`
    parameter. You can define and use the early stopping callback as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a short explanation of each of the arguments used in `EarlyStopping()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`monitor=''val_loss''`, this is the default value, but you can change it to
    look for changes in the `''loss''` curve.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`patience=100`, the default value is 10 and is the number of epochs with no
    improvement in the monitored loss. I personally like to set this to a larger number
    than the patience in `ReduceLROnPlateau`, because I like to let the learning rate
    produce an improvement in the learning process (hopefully) before I terminate
    the learning process because there was no improvement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`restore_best_weights=True`, the default value is `False`. If `False`, the
    model weights obtained at the last epoch are preserved. However, if set to `True`,
    it will preserve and return the best weights at the end of the learning process.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This last argument is my personal favorite because I can set the number of epochs
    to a large number, within reason, and let the training go for as long as it needs.
    In the preceding example, if we set the number of epochs to 1,000, it does not
    necessarily mean that the learning process will go for 1,000 epochs, but if there
    is no progress within 50 epochs, the process can stop early. If the process gets
    to a point at which it has learned good parameters, it can get to a point at which
    there is no progress, then stop after 50 epochs and still return the best model
    that was ever recorded during the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can combine all the preceding callbacks and the saving methodology as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the callbacks have been combined into a list of callbacks that
    will monitor the learning process, looking for plateaus to decrease the learning
    rate, or looking to stop the process if there has been no improvement in a few
    epochs. Also, notice that we created a new variable, `hist`. This variable contains
    a dictionary with logs of the learning process, such as the losses across epochs.
    We can plot such losses to see how the training takes place as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the curves in *Figure 11.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/fe6ae368-567e-4d3c-9a03-630d7eedfdaf.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.3 – Model loss of widenet across epochs using callbacks
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, we can clearly see the evidence of the learning rate reduction
    around epoch 85 where the learning is adjusted after the plateau in the validation
    loss (that is, the loss over the test set); however, this has little effect on
    the validation loss, and therefore training is terminated early around epoch 190
    since there was no improvement in the validation loss.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will analyze the performance of the `widenet` model
    in a quantitative manner that will allow comparison later on.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Here, we want to simply explain the performance of the network in terms that
    are easy to understand and to communicate to others. We will be focusing on analyzing
    the confusion matrix of the model, precision, recall, F1-score, accuracy, and
    balanced error rate. If you do not recall what these terms mean, please go back
    and quickly review [Chapter 4](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml), *Learning
    from Data*.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the nice things about scikit-learn is that it has a nice automated process
    for calculating a report for classification performance that includes most of
    the terms mentioned above. It is simply called a **classification report**. This
    and the other libraries that we will need can be found in the `sklearn.metrics`
    class and can be imported as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'These three libraries operate in a similar way – they take the ground truth
    and the predictions to evaluate performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This code outputs something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: The part on the top indicates the output of `classification_report()`. It gives
    the precision, recall, f1-score, and accuracy of the model. Ideally, we want to
    have all of those numbers as close to 1.0 as possible. Intuitively, the accuracy
    needs to be 100% (or 1.0); however, the rest of the numbers require careful study.
    From this report, we can observe that the total accuracy is 54%. From the rest
    of the report, we can determine that the classes that are more accurately classified
    are 1 and 8, corresponding to *automobile* and *ship*. Similarly, we can see that
    the two classes most poorly classified are 3 and 5, corresponding to *cats *and *dogs*,
    respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'While these numbers are informative, we could look into what is the source
    of the confusion, by looking at the confusion matrix, which is the group of numbers
    produced by `confusion_matrix()`. If we inspect the confusion matrix on row number
    four (corresponding to label 3, *cats*), we see that it correctly classifies 370
    cats as cats, but 143 cats were classified as dogs, and 160 cats were classified
    as frogs, just to name the most serious areas of confusions. Another way to look
    at it is visually, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4316c4aa-3eac-45dd-84a5-b7025c5926b2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.4 – Confusion matrix visualization for the widenet model
  prefs: []
  type: TYPE_NORMAL
- en: Ideally, we want to see a confusion matrix that is diagonal; however, in this
    case, we don't see that effect. After visual inspection, from *Figure 11.4*, we
    can observe which classes have the lowest correct predictions and confirm, visually,
    where the confusion is.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, it is important to note that while **classification accuracy** (**ACC**)
    is 54%, we still need to verify the **balanced error rate** (**BER**) to complement
    what we know about the accuracy. This is particularly important when the classes
    are not evenly distributed, that is, when there are more samples for some classes
    compared to others. As explained in [Chapter 4](7f55e68e-2e9f-486f-9337-5b2ea7bdb504.xhtml), *Learning
    from Data*, we can simply calculate the balanced accuracy and subtract it from
    one. This reveals that the BER is 0.4567, or 45.67%. In an ideal world, we want
    to lower the BER to zero, and definitely stay away from a BER of 50%, which would
    imply that the model is no better than random chance.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the accuracy of the model is not impressive; however, this is
    a very challenging classification problem for fully connected networks, thus,
    this performance is not surprising. Now, we will try to do a similar experiment,
    changing from a relatively wide network, to a deep network, and compare the results.
  prefs: []
  type: TYPE_NORMAL
- en: Dense deep neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'It is widely known that deeper networks can offer good performance in classification
    tasks (Liao, Q., et al. (2018)). In this section, we want to build a deep dense
    neural network and see how it performs in the CIFAR-10 dataset. We will be building
    the model shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2063cb67-f41e-4a7b-b26d-9cab8a459240.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.5 – Network architecture of a deep dense network for CIFAR-10
  prefs: []
  type: TYPE_NORMAL
- en: One of the aims of this model is to have the same number of neural units as
    the model in *Figure 11.1*, for the wide network. This model has a bottleneck
    architecture, where the number of neurons decreases as the network gets deeper.
    This can be coded programmatically using the Keras functional approach, as we
    discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Building and training the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One interesting fact about Keras' functional approach is that we can **recycle** variable
    names as we build the model and that we can even build a model using a loop. For
    example, let's say that I would like to create dense layers with dropout rates
    that exponentially decrease along with the number of neurons by a factor of 1.5
    and 2, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could achieve this by having a cycle that uses an initial dropout rate,
    `dr`, and an initial number of neural units, `units`, and decreases both by a
    factor of 1.5 and 2, respectively, every time, as long as the number of neural
    units is always greater than 10; we stop at 10 because the last layer will contain
    10 neurons, one for each class. It looks something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code snippet illustrates that we can reuse variables without confusing
    Python since TensorFlow operates over a computational graph that has no problem
    in resolving parts of the graph in the correct sequence. The code also shows that
    we can create a bottleneck-type of network very easily with an exponentially decaying
    number of units and dropout rate.
  prefs: []
  type: TYPE_NORMAL
- en: 'The full code to build this model looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and training the model goes like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output, caused by `deepnet.summary()` in the preceding
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: As shown in the preceding summary, and also in *Figure 11.5*, the total number
    of parameters of this model is **15,734,806**. This confirms that this is an over-parametrized
    model. The printed summary also depicts how each part of the model is named when
    no specific name is provided; that is, they all receive a generic name based on
    the name of the class and a consecutive number.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fit()` method trains the deep model and when we plot the training logged
    in the `hist` variable, as we did earlier for *Figure 11.3*, we obtain the following
    figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/719dd863-8eec-447e-91f8-a95203ded62a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.6 – Model loss of deepnet across epochs using callbacks
  prefs: []
  type: TYPE_NORMAL
- en: From *Figure 11.6*, we see that the deep network stops training after about
    200 epochs and the training and test sets cross paths around epoch 70, after which,
    the model begins to overfit the training set. If we compare this result to the
    one in *Figure 11.3* for the wide network, we can see that the model starts overfitting
    around epoch 55.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now discuss the quantitative results of this model.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If we generate a classification report in the same manner as we did for the
    wide network, we obtain the results shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'This suggests comparable results to the wide model, in which we obtained a 0.4567
    BER, which represents a difference of 0.0089 in favor of the wide model, which
    does not represent a significant difference in this case. We can verify that the
    models are also comparable with respect to their classification performance on
    particular classes by looking at the preceding results or at the confusion matrix
    shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f5ce5c8d-8be0-496f-a076-8fd401f0ea24.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.7 – Confusion matrix visualization for the deepnet model
  prefs: []
  type: TYPE_NORMAL
- en: From the preceding results, we can confirm that the toughest class to classify
    is number 3, *cats*, which are often confused with dogs. Similarly, the easiest
    to classify is number 1, *ship**s*, which are often confused with airplanes. But
    once again, this is consistent with the results from the wide network.
  prefs: []
  type: TYPE_NORMAL
- en: One more type of deep network that we can experiment with is one that promotes
    sparsity among the weights of the network, which we'll discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse deep neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A sparse network can be defined as *sparse* in different aspects of its architecture
    (Gripon, V., and Berrou, C., 2011). However, the specific type of sparseness we'll
    look into in this section is the sparseness obtained with respect to the weights
    of the network, that is, its parameters. We will be looking at each specific parameter
    to see if it is relatively close to zero (computationally speaking).
  prefs: []
  type: TYPE_NORMAL
- en: 'Currently, there are three ways of imposing weight sparseness in Keras over
    Tensorflow, and they are related to the concept of a vector norm. If we look at
    the Manhattan norm, ![](img/0393ae08-e2cd-4212-bdd7-09a6d5ebb397.png), or the
    Euclidean norm, ![](img/17601231-bf41-41e7-8008-03f7ad3924e4.png), they are defined
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1566d40d-fd4b-423a-b31e-1ef070ce041c.png),'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d216abc0-69b5-487b-97f4-b6265cc2c2e6.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *n* is the number of elements in the vector ![](img/3b4165c3-c661-408a-b9f9-0f0d5a43b31d.png).
    As you can see, in simple terms, the ![](img/14b8cca1-5449-4bdb-9f76-d7839f733dc0.png)-norm
    adds up all elements in terms of their absolute value, while the ![](img/0a7be022-f165-4047-b381-1a597f96a9ed.png)-norm
    does it in terms of their squared values. It is evident that if both norms are
    close to zero, ![](img/51bcc6df-b0d4-421a-963b-a3d2127e4284.png), the chances
    are that most of its elements are zero or close to zero. As a matter of personal
    choice here, we will use the ![](img/6cc9b5e1-9ac6-4f3a-8fd4-59b862fb22e4.png)-norm
    because, as opposed to ![](img/ffda2abd-d60c-46db-8190-2425c9dec01d.png), very
    large vectors are quadratically penalized so as to avoid specific neurons dominating
    specific terms.
  prefs: []
  type: TYPE_NORMAL
- en: 'Keras contains these tools in the `regularizers` class: `tf.keras.regularizers`.
    We can import them as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c66b2062-de8b-428f-93fc-60a69e3c6d59.png)-norm: `tf.keras.regularizers.l1(l=0.01)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/0bbc9e82-8105-4829-9d6a-5f98b9c233d3.png)-norm: `tf.keras.regularizers.l2(l=0.01)`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These regularizers are applied to the loss function of the network in order
    to minimize the norm of the weights.
  prefs: []
  type: TYPE_NORMAL
- en: A **regularizer** is a term that is used in machine learning to denote a term
    or function that provides elements to an objective (loss) function, or to a general
    optimization problem (such as gradient descent), in order to provide numerical
    stability or promote the feasibility of the problem. In this case, the regularizer
    promotes the stability of the weights by preventing the explosion of some weight
    values, while at the same time promoting general sparsity.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameter `l=0.01` is a penalty factor that directly determines the importance
    of minimizing weight norms. In other words, the penalty is applied as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c469f7b0-3678-4ec0-ba10-72f886eaafcf.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Therefore, using a very small value, such as `l=0.0000001` will pay little
    attention to the norm, and `l=0.01` will pay a lot of attention to the norm during
    the minimization of the loss function. Here''s the catch: this parameter needs
    to be tuned up because if the network is too big, there might be several millions
    of parameters, which can make the norm look very large, and so a small penalty
    is in order; whereas if the network is relatively small, a larger penalty is recommended.
    Since this exercise is on a very deep network with 15+ million parameters, we
    will use a value of `l=0.0001`.'
  prefs: []
  type: TYPE_NORMAL
- en: Let's go ahead and build a sparse network.
  prefs: []
  type: TYPE_NORMAL
- en: Building a sparse network and training it
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To build this network, we will use the exact same architecture shown in *Figure
    11.5*, except that the declaration of each individual dense layer will contain
    a specification that we want to consider the minimization of the norm of the weights
    associated with that layer. Please look at the code of the previous section and
    compare it to the following code, where we highlight the differences:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Compiling and training the model goes the same, like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of `sparsenet.summary()` is identical to the one shown in the previous
    section for `deepnet.summary()`, so we will not repeat it here. However, we can
    look at the training curve as the loss is minimized – see the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a48a28cd-af02-4fcb-bc98-03cf53e4c9df.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.8 – Loss function optimization across epochs for the sparsenet model
  prefs: []
  type: TYPE_NORMAL
- en: From the figure, we can see that both curves, the training and test set, are
    minimized closely together up to around epoch 120, where both start to digress,
    and the model begins to overfit after that. In comparison to previous models in
    *Figure 11.3* and *Figure 11.6*, we can see that this model can be trained a bit
    more slowly and still achieve relative convergence. Note, however, that while
    the loss function still remains the binary cross-entropy, the model is also minimizing
    the ![](img/c35d10cc-5624-4614-9a2e-457897f8f7cb.png)-norm, making this particular
    loss not directly comparable to the previous ones.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now discuss the quantitative results of this model.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When we look at a quantitative analysis of performance, we can tell that the
    model is comparable to the previous models. There is a slight gain in terms of
    a BER; however, it is not enough to declare victory and the problem solved by
    any means – see the following analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'What we can clearly conclude is that the model is not worse in terms of performance
    when compared to other models discussed in this chapter. In fact, close inspection
    of the confusion matrix shown in the following figure indicates that similar errors
    are made with this network as well in terms of objects that are similar in nature:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a4731f7e-bd2c-482c-ae01-db80b399522c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.9 – Confusion matrix for the sparsenet model
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, since it is difficult to appreciate the differences between the models
    we have discussed so far – wide, deep, and sparse – we can calculate and plot
    the norm of the weights of each trained model, as shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4bc9b510-5899-4937-ac67-37f1a60f9456.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.10 – Cumulative norm weights of the trained models
  prefs: []
  type: TYPE_NORMAL
- en: This figure shows the calculation in terms of the ![](img/2ab2673c-164f-45b9-89d9-d6fc42d48046.png)-norm
    so as to have values close enough to appreciate them; on the horizontal axis we
    have the number of layers, and on the vertical axis, we have the cumulative norm
    as we progress in the layers of the networks. This is where we can appreciate
    how different the networks are with respect to their parameters. In a sparse network,
    the cumulative norm is much smaller (about four to five times) in comparison to
    the other networks. This can be an interesting and important characteristic for
    those networks that might be implemented on a chip or other applications in which
    zero-weights can lead to efficient computations in production (Wang, P., et al.
    2018).
  prefs: []
  type: TYPE_NORMAL
- en: While the level at which the network weights are affected by the norm can be
    determined experimentally through hyperparameter optimization techniques, it is
    often more common to determine other parameters such as the dropout rate, the
    number of neural units, and others we discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few methodologies out there for optimizing parameters; for example,
    some are gradient-based (Rivas, P., et al. 2014; Maclaurin, D., et al. 2015*)*,
    others are Bayesian (Feurer, M., et al. 2015*)*. However, it has been difficult
    to have a generalized method that works extremely well and that is efficient at
    the same time – usually, you get one or the other. You can read more about other
    algorithms here (Bergstra, J. S., et al. 2011*)*.
  prefs: []
  type: TYPE_NORMAL
- en: For any beginner in this field, it might be better to get started with something
    simple and easy to remember, such as random search (Bergstra, J., & Bengio, Y.
    2012*)* or grid search. These two methods are very similar and while we will focus
    here on **grid search**, the implementations of both are very similar.
  prefs: []
  type: TYPE_NORMAL
- en: Libraries and parameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will need to use two major libraries that we have not covered before: `GridSearchCV`,
    for executing the grid search with cross-validation, and `KerasClassifier`, to
    create a Keras classifier that can communicate with scikit-learn.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both libraries can be imported as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'The hyperparameters that we will optimize (and their possible values) are the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Dropout rate**: `0.2`, `0.5`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Optimizer**: `rmsprop`, `adam`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Learning rate**: `0.01`, `0.0001`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neurons in hidden layers**: `1024`, `512`, `256`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In total, the possible combination of hyperparameters is 2x2x2x3=24\. This
    is the total number of options in the four-dimensional grid. The number of alternatives
    can be much larger and more comprehensive but remember: we want to keep things
    simple for this example. Furthermore, since we will be applying cross-validation,
    you will multiply the possible combinations by the number of splits in cross-validation
    and that would be how many complete, end-to-end training sessions will be executed
    to determine the best combination of hyperparameters.'
  prefs: []
  type: TYPE_NORMAL
- en: Be mindful of the number of options you will try in the grid search, since all
    of them will be tested, and this can take a lot of time for larger networks and
    for larger datasets. When you gain more experience, you will be able to choose
    a smaller set of parameters just by thinking about the architecture that you define.
  prefs: []
  type: TYPE_NORMAL
- en: The full implementation is discussed next.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation and results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The complete code for the grid search is shown here, but consider that most
    of these things are repetitive since this is modeled on the wide network model
    discussed earlier in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'We declare a method to build a model and return it like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we put the pieces together, searching for parameters, and training as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'This will print out several lines, one for each time the cross-validation runs.
    We will omit a lot of the output here, just to show you what it looks like, but
    you can tune the level of verbosity manually if you want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: This last line is the most precious information you need since it is the best
    combination of parameters that give the best results. Now you can go ahead and
    change your original implementation of the wide network with these **optimized **parameters
    and see how the performance changes. You should receive a boost in the average
    accuracy of around 5%, which is not bad!
  prefs: []
  type: TYPE_NORMAL
- en: 'Alternatively, you can try out a larger set of parameters or increase the number
    of splits for cross-validation. The possibilities are endless. You should always
    try to optimize the number of parameters in your models for the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: It gives you confidence in your model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It gives your clients confidence in you.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It tells the world that you are a professional.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Good work! It is time to wrap up.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter discussed different implementations of neural networks, namely,
    wide, deep, and sparse implementations. After reading this chapter, you should
    appreciate the differences in design and how they may affect performance or training
    time. At this point, you should be able to appreciate the simplicity of these
    architectures and how they present new alternatives to other things we've discussed
    so far. In this chapter, you also learned to optimize the hyperparameters of your
    models, for example, the dropout rates, aiming to maximize the generalization
    ability of the network.
  prefs: []
  type: TYPE_NORMAL
- en: I am sure you noticed that these models achieved accuracies beyond random chance,
    that is, > 50%; however, the problem we discussed is a very difficult problem
    to solve, and you might not be surprised that a general neural architecture, like
    the ones we studied here, does not perform extraordinarily well. In order to achieve
    better performance, we can use a more specialized type of architecture designed
    to solve problems with a high spatial correlation of the input, such as image
    processing. One type of specialized architecture is known as a **Convolutional
    Neural Network** (**CNN**).
  prefs: []
  type: TYPE_NORMAL
- en: Our next station, [Chapter 12](c36bdee9-51f3-4283-8f15-6dd603d071a1.xhtml),
    *Convolutional Neural Networks,* will discuss precisely that*.* You will be able
    to see how much a difference can make when you move from a general-purpose model
    to a more field-specific model. You cannot miss this upcoming chapter. But before
    you go, please try to quiz yourself with the following questions.
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Was there a significant difference in performance between a wide or deep
    network?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Not much in the case, we studied here. However, one thing you must remember
    is that both networks learned fundamentally different things or aspects of the
    input. Therefore, in other applications, the performance might vary.
  prefs: []
  type: TYPE_NORMAL
- en: '**Is deep learning the same as a deep neural network? **'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No. Deep learning is the area of machine learning focused on all algorithms
    that train over-parametrized models using novel gradient descent techniques. Deep
    neural networks are networks with many hidden layers. Therefore, a deep network
    is deep learning. But deep learning is not uniquely specific to deep networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**Could you give an example of when sparse networks are desired?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let's think about robotics. In this field, most things run on microchips that
    have memory constraints and storage constraints and computational power constraints;
    finding neural architectures whose weights are mostly zero would mean you do not
    have to calculate those products. This implies having weights that can be stored
    in less space, loaded quickly, and computed faster. Other possibilities include
    IoT devices, smartphones, smart vehicles, smart cities, law enforcement, and so
    on.
  prefs: []
  type: TYPE_NORMAL
- en: '**How can we make these models perform better?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We can further optimize the hyperparameters by including more options. We can
    use autoencoders to preprocess the input. But the most effective thing would be
    to switch to CNNs to solve this problem since CNNs are particularly good at the
    classification of images. See the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rosenblatt, F. (1958). The perceptron: a probabilistic model for information
    storage and organization in the brain. *Psychological review*, 65(6), 386.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Muselli, M. (1997). On convergence properties of the pocket algorithm. *IEEE
    Transactions on Neural Networks*, 8(3), 623-629.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Novak, R., Xiao, L., Hron, J., Lee, J., Alemi, A. A., Sohl-Dickstein, J., &
    Schoenholz, S. S. (2019). Neural Tangents: Fast and Easy Infinite Neural Networks
    in Python. *arXiv preprint* arXiv:1912.02803.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Soltanolkotabi, M., Javanmard, A., & Lee, J. D. (2018). Theoretical insights
    into the optimization landscape of over-parameterized shallow neural networks. *IEEE
    Transactions on Information Theory*, 65(2), 742-769.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Du, S. S., Zhai, X., Poczos, B., & Singh, A. (2018). Gradient descent provably
    optimizes over-parameterized neural networks. *arXiv preprint* arXiv:1810.02054.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Liao, Q., Miranda, B., Banburski, A., Hidary, J., & Poggio, T. (2018). A surprising
    linear relationship predicts test performance in deep networks. *arXiv preprint*
    arXiv:1807.09659.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gripon, V., & Berrou, C. (2011). Sparse neural networks with large learning
    diversity. *IEEE transactions on neural networks*, 22(7), 1087-1096.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Wang, P., Ji, Y., Hong, C., Lyu, Y., Wang, D., & Xie, Y. (2018, June). SNrram:
    an efficient sparse neural network computation architecture based on resistive
    random-access memory. In *2018 55th ACM/ESDA/IEEE Design Automation Conference*
    (DAC) (pp. 1-6). IEEE.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rivas-Perea, P., Cota-Ruiz, J., & Rosiles, J. G. (2014). A nonlinear least squares
    quasi-newton strategy for lp-svr hyper-parameters selection. *International Journal
    of Machine Learning and Cybernetics*, 5(4), 579-597.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maclaurin, D., Duvenaud, D., & Adams, R. (2015, June). Gradient-based hyperparameter
    optimization through reversible learning. In *International Conference on Machine
    Learning* (pp. 2113-2122).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feurer, M., Springenberg, J. T., & Hutter, F. (2015, February). Initializing
    Bayesian hyperparameter optimization via meta-learning. In *Twenty-Ninth AAAI
    Conference on Artificial Intelligence*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bergstra, J., & Bengio, Y. (2012). Random search for hyper-parameter optimization.
    The *Journal of Machine Learning Research*, 13(1), 281-305.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bergstra, J. S., Bardenet, R., Bengio, Y., & Kégl, B. (2011). Algorithms for
    hyper-parameter optimization. In *Advances in neural information processing systems*
    (pp. 2546-2554).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
