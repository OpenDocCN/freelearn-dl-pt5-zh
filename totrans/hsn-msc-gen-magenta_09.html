<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Data Preparation for Training</h1>
                </header>
            
            <article>
                
<p class="mce-root">So far, we've used existing Magenta pre-trained models since they are quite powerful and easy to use. But training our own models is crucial since it allows us to generate music in a specific style or generate specific structures or instruments. Building and preparing a dataset is the first step before training our own model. To do that, we need to look at existing datasets and APIs that will help us to find meaningful data. Then, we need to build two datasets in MIDI for specific styles—dance and jazz. Finally, we will need to prepare the MIDI files for training using data transformations and pipelines.</p>
<p class="mce-root">The following topics will be covered in this chapter:</p>
<ul>
<li class="mce-root">Looking at existing datasets<span class="underline"><br/></span></li>
<li>Building a dance music dataset</li>
<li>Building a jazz dataset</li>
<li>Preparing the data using pipelines</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll use the following tools:</p>
<ul>
<li><strong>A command line</strong> or <strong>Bash</strong> to launch Magenta from the Terminal</li>
<li><strong>Python</strong> and its libraries</li>
<li>The Python <strong>multiprocessing</strong> module for multi-threaded data preparation</li>
<li><strong>Matplotlib</strong> to plot our data preparation results</li>
<li><strong>Magenta</strong> to launch data pipeline conversion</li>
<li><strong>MIDI</strong>, <strong>ABCNotation</strong>, and <strong>MusicXML</strong> as data formats</li>
<li>External APIs such as <strong>Last.fm</strong></li>
</ul>
<p>In Magenta, we'll make use of <strong>data pipelines</strong>. We will explain these in depth later in this chapter, but if you feel like you need more information, the pipeline README file in Magenta's source code (<a href="https://github.com/tensorflow/magenta/tree/master/magenta/pipelines">github.com/tensorflow/magenta/tree/master/magenta/pipelines</a>) is a good place to start. You can also take a look at Magenta's code, which is well documented. There's also additional content in the <em>Further reading</em> section.</p>
<p>The code for this chapter can be found in this book's GitHub repository, in the <kbd>Chapter06</kbd> folder, which is located at <a href="https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter06">github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter06</a>. For this chapter, you should use the <kbd>cd Chapter06</kbd> command before you start.</p>
<p>Check out the following video to see the Code in Action: <a href="http://bit.ly/3aXWLmC">http://bit.ly/3aXWLmC</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking at existing datasets</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll be preparing some data for trainin<span>g. Note that this will be</span><span> covered in more detail in <a href="6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml">Chapter 7</a>,</span> <em>Training Magenta Models</em><span>. Preparing data and training models are two different activities that are done in tandem—<span>first, we </span>prepare the data, then train the models, and finally go back to preparing the data to improve our model's performance.</span></p>
<p>First, we'll start by looking at symbolic representations other than MIDI, such as MusicXML and ABCNotation, since Magenta also handles them, even if the datasets we'll be working with in this chapter will be in MIDI only. Then, we'll provide an overview of existing datasets, including datasets from the Magenta team that were used to train some models we've already covered. This overview is by no means exhaustive but can serve as a starting point when it comes to finding training data.</p>
<p>The main dataset we'll be focusing on is the <strong>Lakh MIDI dataset</strong> (<strong>LMD</strong>), a recent and well crafted MIDI dataset that will serve as a basis for most of our examples. You can also use other datasets; the code we are providing here can be easily adapted to other content.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking at symbolic representations</h1>
                </header>
            
            <article>
                
<p>There are three main symbolic representations: MIDI, MusicXML, and ABCNotation. We've already covered MIDI in detail, but we haven't talked about MusicXML and ABCNotation yet. While we won't be using these two in this chapter, it is nice to know they exist and that Magenta can handle them as well as MIDI files.</p>
<p class="mce-root"/>
<p><strong>MusicXML</strong>, as its name suggests, is an XML-based musical representation format. It has the advantage of being text-based, meaning it doesn't require an external library such as PrettyMIDI to be processed and is supported in many sheet music editors, such as MuseScore. You can find the MusicXML specification on its official website: <a href="https://www.musicxml.com/">www.musicxml.com</a>.</p>
<p>Here's an example of a MusicXML file:</p>
<pre><span class="cp">&lt;?xml version="1.0" encoding="UTF-8" standalone="no"?&gt;</span>
<span class="cp">&lt;!DOCTYPE score-partwise PUBLIC</span>
<span class="cp">    "-//Recordare//DTD MusicXML 3.1 Partwise//EN"</span>
<span class="cp">    "http://www.musicxml.org/dtds/partwise.dtd"&gt;</span>
<span class="nt">&lt;score-partwise</span> <span class="na">version=</span><span class="s">"3.1"</span><span class="nt">&gt;</span>
 <span class="nt">&lt;part-list&gt;</span>
    <span class="nt">&lt;score-part</span> <span class="na">id=</span><span class="s">"P1"</span><span class="nt">&gt;</span>
      <span class="nt">&lt;part-name&gt;</span>Music<span class="nt">&lt;/part-name&gt;</span>
    <span class="nt">&lt;/score-part&gt;</span>
  <span class="nt">&lt;/part-list&gt;</span>
  <span class="nt">&lt;part</span> <span class="na">id=</span><span class="s">"P1"</span><span class="nt">&gt;</span>
    <span class="nt">&lt;measure</span> <span class="na">number=</span><span class="s">"1"</span><span class="nt">&gt;</span>
      <span class="nt">&lt;attributes&gt;</span>
...
      <span class="nt">&lt;/attributes&gt;</span>
      <span class="nt">&lt;note&gt;</span>
        <span class="nt">&lt;pitch&gt;</span>
          <span class="nt">&lt;step&gt;</span>C<span class="nt">&lt;/step&gt;</span>
          <span class="nt">&lt;octave&gt;</span>4<span class="nt">&lt;/octave&gt;</span>
        <span class="nt">&lt;/pitch&gt;</span>
        <span class="nt">&lt;duration&gt;</span>4<span class="nt">&lt;/duration&gt;</span>
        <span class="nt">&lt;type&gt;</span>whole<span class="nt">&lt;/type&gt;</span>
      <span class="nt">&lt;/note&gt;</span>
    <span class="nt">&lt;/measure&gt;</span>
  <span class="nt">&lt;/part&gt;</span>
<span class="nt">&lt;/score-partwise&gt;</span></pre>
<p><strong>ABCNotation</strong>, as its name suggests, is a text-based musical representation format based on the letter notation (A-G). The format is rather compact, with some header information concerning the whole file, followed by the content of the song using the letter notation. The notation is also well supported in sheet music software. Here's an example of an ABCNotation file:</p>
<pre>&lt;score lang="ABC"&gt;
X:1
T:The Legacy Jig
M:6/8
L:1/8
R:jig
K:G
GFG BAB | gfg gab | GFG BAB | d2A AFD |
GFG BAB | gfg gab | age edB |1 dBA AFD :|2 dBA ABd |:
efe edB | dBA ABd | efe edB | gdB ABd |
efe edB | d2d def | gfe edB |1 dBA ABd :|2 dBA AFD |]
&lt;/score&gt;</pre>
<p>We will provide some ABCNotation content in the <span><em>Looking at other datasets</em> section of this chapter.</span></p>
<p>The tools Magenta provides for preparing datasets for training handle MIDI, MusicXML, and ABCNotation in a single command, which is really handy. The <kbd>convert_dir_to_note_sequences</kbd> command will parse the content depending on its type and return <kbd>NoteSequence</kbd> regardless.</p>
<p>We'll look at these tools in more detail in <a href="6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml">Chapter 7</a>, <em>Training Magenta Models</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a dataset from the ground up</h1>
                </header>
            
            <article>
                
<p>Using an existing dataset and trimming it down is the easiest way to start building and preparing a dataset for training since it is a rather fast method of getting enough data for training.</p>
<p>Another option is to build the dataset from scratch, either by creating new data for it or by incorporating data from various sources. While requiring more work, this method might give better results during training since the resulting data is carefully selected. This is a process you should follow if you are a musician and have your own MIDI files ready.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the LMD for MIDI and audio files</h1>
                </header>
            
            <article>
                
<p>The LMD (<a href="https://colinraffel.com/projects/lmd/">colinraffel.com/projects/lmd</a>) is one of the most complete and easy-to-use MIDI datasets. If you don't have anything handy right now, we recommend using it. This chapter's code will use this dataset, but you can also follow the examples using another dataset since even if the information is different, most of the techniques we will use here can still be used.</p>
<p class="mce-root"/>
<p>The dataset has various distributions, but we'll be looking at the following ones in particular:</p>
<ul>
<li><strong>LMD-full</strong>: This is the full dataset, which contains 176,581 MIDI files.</li>
<li><strong>LMD-matched</strong>: The dataset is partially matched to another dataset, the <strong>million song dataset</strong> (<strong>MSD</strong>), which contains 45,129 MIDI files. This subset is useful because the matched files contain metadata such as artist and title.</li>
<li><strong>LMD-aligned</strong>: The LMD-matched dataset is aligned with an audio MP3 preview.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the MSD for metadata information</h1>
                </header>
            
            <article>
                
<p>The MSD (<a href="http://millionsongdataset.com/">millionsongdataset.com</a>) is a large scale dataset that has over a million entries containing audio features and metadata information. We won't be using the MSD dataset directly; instead, we'll be using the matched content included in the LMD-matched dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the MAESTRO dataset for performance music</h1>
                </header>
            
            <article>
                
<p>The <strong>MIDI and Audio Edited for Synchronous TRacks and Organization</strong> (<strong>MAESTRO</strong>) dataset (<a href="https://magenta.tensorflow.org/datasets/maestro">magenta.tensorflow.org/datasets/maestro</a>) is curated by the Magenta team and is based on live performances that have been recorded to both audio and MIDI, for over 200 hours of content. Since the recorded performances have been played by humans, the notes have expressive timing and dynamics (effects pedals are also represented).</p>
<p>The dataset's content comes from the International Piano-e-Competition (<a href="http://piano-e-competition.com/">piano-e-competition.com/</a>), which mainly contains classical music. This dataset has multiple usages, such as automatic audio to symbolic representation transcription, which has been used to train the Magenta Onsets and Frames model. It has also been used to train the Performance RNN model we covered in <a href="48023567-4100-492a-a28e-53b18a63e01e.xhtml">Chapter 3</a>, <em>Generating Polyphonic Melodies</em>.</p>
<p>Similar to MAESTRO but with less content, you also have the MusicNet and MAPS datasets available. We won't be using the MAESTRO dataset for our examples, but it is an important dataset you might want to look at. For more information, take a look at the <em>Further reading</em> section at the end of this chapter.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Groove MIDI Dataset for groovy drums</h1>
                </header>
            
            <article>
                
<p>The <strong>Groove MIDI Dataset</strong> (<a href="https://www.tensorflow.org/datasets/catalog/groove">www.tensorflow.org/datasets/catalog/groove</a>) is composed of 13.6 hours of aligned MIDI and (synthesized) audio of human-performed, tempo-aligned expressive drumming captured on an electronic drum. The dataset is also split into 2-bars and 4-bars MIDI segments and has been used to train the GrooVAE model, which we presented in <a href="838da33e-26a9-4701-bfd3-5014dfff4146.xhtml">Chapter 4</a>, <em>Latent Space Interpolation with MusicVAE</em>.</p>
<p>The GMD is an impressive dataset of recorded performances from professional drummers, who also improvised for the occasion, resulting in a diverse dataset. The performances are annotated with a genre, which can be used to filter and extract certain MIDI files.</p>
<p>While not quantized, the GMD can also be used to train quantized models, such as the drums RNN. The pipelines, which will be shown in the <em>Preparing the data using pipelines</em> section, can transform input data such as unquantized MIDI into quantized MIDI.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Bach Doodle Dataset</h1>
                </header>
            
            <article>
                
<p>Bach Doodle is a web application made by Google to celebrate the anniversary of the German composer and musician, Johann Sebastian Bach. Doodle allows the user to compose a 2-bars melody and ask the application to harmonize the input melody in Bach's style using Coconet and TensorFlow.js running in the browser.</p>
<p>The resulting Bach Doodle Dataset (<a href="https://magenta.tensorflow.org/datasets/bach-doodle">magenta.tensorflow.org/datasets/bach-doodle</a>) of harmonized composition is impressive: 21.6 million harmonizations for about 6 years of user-entered music. It contains the user input sequence and the output sequence from the network in note sequence format, as well as some metadata, such as the country of the user and the number of times it was played.</p>
<p>See the <em>Further reading</em> section for more information about data visualization regarding the dataset.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the NSynth dataset for audio content</h1>
                </header>
            
            <article>
                
<p>We covered the NSynth dataset (<a href="https://www.tensorflow.org/datasets/catalog/nsynth">www.tensorflow.org/datasets/catalog/nsynth</a>) in the previous chapter, <span><em>Audio Generation with NSynth and GANSynth</em>. We won't be coveri</span><span>ng audio training in this book, but this is a good dataset to use for training the GANSynth model.</span></p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using APIs to enrich existing data</h1>
                </header>
            
            <article>
                
<p>Using APIs is a good way of finding more information about MIDI tracks. We'll be showing an example of this in this chapter, where we'll query an API using the song artist and title to find genres and tags associated with the song.</p>
<p>There are multiple services you can use to find such information. We won't list all of them, but two good starting points are <strong>Last.fm</strong> (<a href="https://www.last.fm/">www.last.fm/</a>) and Spotify's <strong>Echo Nest</strong> (<a href="http://static.echonest.com/enspex/">static.echonest.com/enspex/</a>). The <strong>tagtraum</strong> dataset (<a href="http://www.tagtraum.com/msd_genre_datasets.html">www.tagtraum.com/msd_genre_datasets.html</a>) is another good dataset for genre, which is based on MSD.</p>
<p>In this chapter, we'll be using the <strong>Last.fm</strong> API to fetch song information, and we'll learn how to use its API in an upcoming section, <em>Building a jazz dataset</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking at other data sources</h1>
                </header>
            
            <article>
                
<p>Outside of curated datasets, there are tons of other sources for music files on the internet—mainly websites offering searchable databases. The downside of using such sources is that you'll have to download and verify each file by hand, which might be time-consuming but necessary if you want to build your own dataset from the ground up.</p>
<p>Websites such as <strong>MidiWorld</strong> (<a href="https://www.midiworld.com/">www.midiworld.com</a>) and <strong>MuseScore</strong> (<a href="https://musescore.com/">musescore.com</a>) contain a lot of MIDI files, often classified by style, composer, and instrument. There are also posts on Reddit that list pretty big MIDI datasets of varying quality.</p>
<p>For formats other than MIDI, you have the ABCNotation website (<a href="http://abcnotation.com">abcnotation.com</a>), which features over 600,000 files of mainly folk and traditional music, but with links to other sources.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a dance music dataset</h1>
                </header>
            
            <article>
                
<p>Now that we have datasets available so that we can build our own dataset, we'll look at different ways of using the information contained in a MIDI file. This section will serve as an introduction to the different tools that can be used for dataset creation using only MIDI files. In this section, we'll use the <strong>LMD-full</strong> distribution.</p>
<p>In the next section, <em>Building a jazz dataset</em>, we will delve deeper into using external information.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Threading the execution to handle large datasets faster</h1>
                </header>
            
            <article>
                
<p>When building datasets, we want our code to execute fast because of the amount of data we'll be handling. In Python, using threading and multiprocessing is one way to go. There are many ways of executing code in parallel in Python, but we'll be using the <kbd>multiprocessing</kbd> module because of its simple design, which is also similar to other popular techniques such as using the <kbd>joblib</kbd> module.</p>
<div class="packt_tip">You can find this code in the <kbd>multiprocessing_utils.py</kbd> file, in the source code of this chapter. There are more comments and content in the source code, so check it out.</div>
<p>For our examples, we'll be using the following code to start four threads that will execute in parallel:</p>
<pre><span>from </span>itertools <span>import </span>cycle<br/><span>from </span>multiprocessing <span>import </span>Manager<br/><span>from </span>multiprocessing.pool <span>import </span>Pool<span><br/><br/>with </span><strong>Pool(<span>4</span>)</strong> <span>as </span>pool:<br/>  <span># Add elements to process here<br/></span><span>  </span>elements = []<br/>  manager = Manager()<br/>  counter = AtomicCounter(manager, <span>len</span>(elements))<br/>  results = pool.starmap(<strong>process</strong>, <span>zip</span>(<strong>elements</strong>, cycle([counter])))<br/>  results = [result <span>for </span>result <span>in </span>results <span>if </span>result]</pre>
<p>Let's explain this code block in more detail:</p>
<ul>
<li>You can modify the number of threads in <kbd>Pool(4)</kbd> to fit your hardware. One thread per physical CPU is often a proper value for good performance.</li>
<li>We instantiate <kbd>Manager</kbd>, which is needed to share resources inside threads, and <kbd>AtomicCounter</kbd>, which is available in the <kbd>multiprocessing_utils</kbd> module from this book's code, to share a counter between the threads.</li>
<li>Finally, we use the <kbd>pool.starmap</kbd> method to launch the <kbd>process</kbd> method on the <kbd>elements</kbd> list (we'll be defining this method soon). The <kbd>starmap</kbd> method calls the <kbd>process</kbd> method with two parameters, the first one being <strong>one element</strong> of the <kbd>elements</kbd> list, with the second being <kbd>counter</kbd>.</li>
<li>The <kbd>process</kbd> method will handle one element at the time and will increment the counter at each call.</li>
<li>The <kbd>process</kbd> method should be able to return <kbd>None</kbd>, making it possible to filter out elements.</li>
<li>The thread's life cycle and the split of the elements list is handled by the <kbd>multiprocessing</kbd> module.</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting drum instruments from a MIDI file</h1>
                </header>
            
            <article>
                
<p>MIDI files can contain many instruments—multiple percussion instruments, pianos, guitars, and more. Extracting specific instruments and saving the result in a new MIDI file is a necessary step when building a dataset.</p>
<p>In this example, we'll be using <kbd>PrettyMIDI</kbd> to fetch the instruments information into a MIDI file, extract the drum instruments, merge them into a single drum instrument, and save the resulting instrument in a new MIDI file. Some MIDI files have multiple drum instruments to split the drum into multiple parts, such as bass drum, snare, and so on. For our example, we've chosen to merge them, but depending on what you are trying to do, you might want to keep them separate or keep only some parts.</p>
<div class="packt_tip">You can find this code in the <kbd>chapter_06_example_00.py</kbd> file, in the source code of this chapter. There are more comments and content in the source code, so check it out.</div>
<p>The <kbd>extract_drums</kbd> method takes <kbd>midi_path</kbd> and returns a single <kbd>PrettyMIDI</kbd> instance containing one merged drum instrument:</p>
<pre><span>import copy<br/>from typing import Optional<br/><br/>from pretty_midi import Instrument<br/>from pretty_midi import PrettyMIDI<br/><br/>def </span>extract_drums(midi_path: <span>str</span>) -&gt; Optional[PrettyMIDI]:<br/>  pm = PrettyMIDI(midi_path)<br/>  <strong>pm_drums = copy.deepcopy(pm)</strong><br/>  pm_drums.instruments = [instrument <span>for </span>instrument <span>in </span>pm_drums.instruments<br/>                          <span>if </span><strong>instrument.is_drum</strong>]<br/>  <span>if </span><span>len</span>(pm_drums.instruments) &gt; <span>1</span>:<br/>    <span># Some drum tracks are split, we can merge them<br/></span><span>    </span>drums = Instrument(<span>program</span>=<span>0</span>, <span>is_drum</span>=<span>True</span>)<br/>    <span>for </span>instrument <span>in </span>pm_drums.instruments:<br/>      <span>for </span>note <span>in </span>instrument.notes:<br/>        <strong>drums.notes.append(note)</strong><br/>    pm_drums.instruments = [drums]<br/>  <span>if </span><span>len</span>(pm_drums.instruments) != <span>1</span>:<br/>    <span>raise </span><span>Exception</span>(<span>f"Invalid number of drums </span><span>{</span>midi_path<span>}</span><span>: "<br/></span><span>                    f"</span><span>{</span><span>len</span>(pm_drums.instruments)<span>}</span><span>"</span>)<br/>  <span>return </span>pm_drums</pre>
<p>First, we use <kbd>copy.deepcopy</kbd> to copy the whole MIDI file content because we still want to keep the time signatures and tempo changes from the original file. That information is kept in the <kbd>PrettyMIDI</kbd> instance that's returned in the <kbd>copy.deepcopy</kbd> method from the <kbd>pm_drums</kbd> variable. Then, we filter the instruments using the <kbd>is_drum</kbd> property. If there are multiple drum instruments, we merge them together into a new <kbd>Instrument</kbd> by copying the notes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Detecting specific musical structures</h1>
                </header>
            
            <article>
                
<p>Now that we can extract specific instruments from the MIDI files, we can also find specific structures in the MIDI files to further refine our dataset. Dance and techno music, in most cases, have a bass drum on each beat, giving you that inescapable urge to dance. Let's see whether we can find that in our MIDI files.</p>
<p>The <kbd>get_bass_drums_on_beat</kbd> method returns the proportion of bass drums that fall directly on the beat:</p>
<pre><span>import math<br/>from pretty_midi import PrettyMIDI<br/><br/>def </span>get_bass_drums_on_beat(pm_drums: PrettyMIDI) -&gt; <span>float</span>:<br/>  beats = pm_drums.get_beats()<br/>  bass_drums = [note.start <span>for </span>note <span>in </span>pm_drums.instruments[<span>0</span>].notes<br/>                <strong><span>if </span>note.pitch == <span>35 </span><span>or </span>note.pitch == <span>36</span></strong>]<br/>  bass_drums_on_beat = []<br/>  <span>for </span>beat <span>in </span>beats:<br/>    beat_has_bass_drum = <span>False<br/></span><span>    for </span>bass_drum <span>in </span>bass_drums:<br/>      <span>if </span>math.isclose(beat, bass_drum):<br/>        beat_has_bass_drum = <span>True<br/></span><span>        break<br/></span><span>    </span><strong>bass_drums_on_beat.append(<span>True if </span>beat_has_bass_drum <span>else False</span>)</strong><br/>  num_bass_drums_on_beat = <span>len</span>([bd <span>for </span>bd <span>in </span>bass_drums_on_beat <span>if </span>bd])<br/>  <span>return </span><strong>num_bass_drums_on_beat / <span>len</span>(bass_drums_on_beat)</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p>The <kbd>get_beats</kbd> method from <kbd>PrettyMIDI</kbd> returns an array stating the start time of each beat. For example, on a 150 QPM file, we have the following array:</p>
<pre>[  0.    0.4   0.8   1.2   1.6   2.    2.4   2.8   3.2   3.6   4.    4.4<br/>   4.8   5.2   5.6   6.    6.4   6.8   7.2   7.6   8.    8.4   8.8   9.2<br/>...<br/> 201.6 202.  202.4 202.8 203.2 203.6 204.  204.4 204.8 205.2 205.6 206.<br/> 206.4 206.8 207.2 207.6]</pre>
<p>Then, we take only the bass drum pitches (35 or 36) in the given MIDI and make the assumption that we have exactly one instrument because our previous method, <kbd>extract_drums</kbd>, should have been called before. Then, we check whether, for each beat, a bass drum was played at that time and return the proportion as <kbd>float</kbd>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Analyzing the beats of our MIDI files</h1>
                </header>
            
            <article>
                
<p>Let's put everything together to check our results.</p>
<p>First, we'll add some arguments to our program using the <kbd>argparse</kbd> module:</p>
<pre><span>import argparse<br/><br/>parser = argparse.ArgumentParser()<br/>parser.add_argument("--<strong>path_output_dir</strong>", type=str, required=True)<br/>parser.add_argument("--<strong>bass_drums_on_beat_threshold</strong>", <br/>                    type=float, required=True, default=0)<br/>args = parser.parse_args()</span></pre>
<p>Here, we've declared two command-line arguments, <kbd>--path_output_dir</kbd> and <kbd>--bass_drums_on_beat_threshold</kbd>, using the <kbd>argparse</kbd> module. The output directory is useful if we wish to save the extracted MIDI files in a separate folder, while the threshold is useful for filtering more or less of the extracted MIDI sequences. </p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing the process method</h1>
                </header>
            
            <article>
                
<p>Now that we have our arguments ready, we can write the processing method.</p>
<p>Here is the <kbd>process</kbd> method that will be called by our multi-threaded code:</p>
<pre><span>import os<br/>from typing import Optional<br/>from multiprocessing_utils import AtomicCounter<br/><br/>def </span>process(midi_path: <span>str</span>, counter: AtomicCounter) -&gt; Optional[<span>dict</span>]:<br/>  <span>try</span>:<br/>    os.makedirs(args.path_output_dir, exist_ok=True)<br/>    pm_drums = <strong>extract_drums</strong>(midi_path)<br/>    bass_drums_on_beat = <strong>get_bass_drums_on_beat</strong>(pm_drums)<br/>    <span>if </span>bass_drums_on_beat &gt;= args.bass_drums_on_beat_threshold:<br/>      midi_filename = os.path.basename(midi_path)<br/>      <strong>pm_drums.write</strong>(os.path.join(args.path_output_dir, <span>f"</span><span>{</span>midi_filename<span>}</span><span>.mid"</span>))<br/>    <span>else</span>:<br/>      <span>raise </span><span>Exception</span>(<span>f"Not on beat </span><span>{</span>midi_path<span>}</span><span>: </span><span>{</span>bass_drums_on_beat<span>}</span><span>"</span>)<br/>    <span>return </span>{<span>"midi_path"</span>: midi_path,<br/>            <span>"pm_drums"</span>: pm_drums,<br/>            <span>"bass_drums_on_beat"</span>: bass_drums_on_beat}<br/>  <span>except </span><span>Exception </span><span>as </span>e:<br/>    <span>print</span>(<span>f"Exception during processing of </span><span>{</span>midi_path<span>}</span><span>: </span><span>{</span>e<span>}</span><span>"</span>)<br/>  <span>finally</span>:<br/>    counter.increment()</pre>
<p>The <kbd>process</kbd> method will create the output directory if it doesn't already exist. Then, it will call the <kbd>extract_drums</kbd> method with the given MIDI path, and then call the <kbd>get_bass_drums_on_beat</kbd> method using the returned <kbd>PrettyMIDI</kbd> drum, which returns the <kbd>bass_drums_on_beat</kbd> as a proportion. Then, if the value is over the threshold, we save that MIDI file on disk; otherize, we exit the method.</p>
<p>The return values of the <kbd>process</kbd> method are important <span>– </span>by returning the <kbd>PrettyMIDI</kbd> file and the proportion of bass drums on beat, we'll be able to make statistics about our dataset to make informed decisions about its size and content. The <kbd>process</kbd> method can also return an empty (<kbd>None</kbd>) value or raise an exception, which will make the caller drop that MIDI file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Calling the process method using threads</h1>
                </header>
            
            <article>
                
<p>Now that we have a processing method, we can use it to launch the execution.</p>
<p class="mce-root"/>
<p>Let's create an <kbd>app</kbd> method that calls the <kbd>process</kbd> method using threads:</p>
<pre><span>import shutil<br/>from itertools import cycle<br/>from multiprocessing import Manager<br/>from multiprocessing.pool import Pool<br/>from typing import List<br/>from multiprocessing_utils import AtomicCounter<br/><br/>def </span>app(midi_paths: List[<span>str</span>]):<span><br/></span><span>  </span><strong>shutil.rmtree</strong>(args.path_output_dir, <span>ignore_errors</span>=<span>True</span>)<br/><span><br/></span><span>  </span><span>with </span><strong>Pool(<span>4</span>)</strong> <span>as </span>pool:<br/>    manager = Manager()<br/>    counter = AtomicCounter(manager, <span>len</span>(midi_paths))<br/>    <strong>results</strong> = pool.starmap(<strong>process</strong>, <span>zip</span>(midi_paths, cycle([counter])))<br/>    results = [result <span>for </span>result <span>in </span>results <span>if </span>result]<br/>    results_percentage = <span>len</span>(results) / <span>len</span>(midi_paths) * <span>100<br/></span><span>    </span><span>print</span>(<span>f"Number of tracks: </span><span>{</span><span>len</span>(MIDI_PATHS)<span>}</span><span>, "<br/></span><span>          f"number of tracks in sample: </span><span>{</span><span>len</span>(midi_paths)<span>}</span><span>, "<br/></span><span>          f"number of results: </span><span>{</span><span>len</span>(results)<span>} </span><span>"<br/></span><span>          f"(</span><span>{</span>results_percentage<span>:</span><span>.2f</span><span>}</span><span>%)"</span>)</pre>
<p>The <kbd>app</kbd> method will be called with a list of MIDI paths when the program launches. First, we clean up the output directory for the process method so that we can write in it. Then, we start four threads using <kbd>Pool(4)</kbd> (refer to the previous section, <em>Threading the execution to handle large datasets</em>, for more information). Finally, we calculate how many results were returned for information purposes.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Plotting the results using Matplotlib</h1>
                </header>
            
            <article>
                
<p>Using the returned results, we can find statistics about our dataset. As an example, let's plot the drum length:</p>
<pre><span><strong>import matplotlib.pyplot as plt</strong><br/><br/></span>pm_drums = [result[<span>"pm_drums"</span>] <span>for </span>result <span>in </span>results]<br/>pm_drums_lengths = [pm.get_end_time() <span>for </span>pm <span>in </span>pm_drums]<br/><strong>plt.hist(pm_drums_lengths, <span>bins</span>=<span>100</span>)</strong><br/>plt.title(<span>'Drums lengths'</span>)<br/>plt.ylabel(<span>'length (sec)'</span>)<br/>plt.show()</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>We are using Matplotlib (<a href="https://matplotlib.org/">matplotlib.org/</a>), a popular and easy to use plotting library for Python. This will result in the following output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dd14f85c-4db9-49f8-a1f6-94e65d9fd4df.png" style="width:47.08em;height:37.67em;"/></p>
<p><span>Making plots of different statistics helps you visualize the content and size of your dataset.</span></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Processing a sample of the dataset</h1>
                </header>
            
            <article>
                
<p>Now that we have everything in place, we can call the <kbd>app</kbd> method using a subset of our dataset.</p>
<p class="mce-root"/>
<p>First, we'll use a smaller sample size to make sure our code works properly:</p>
<pre>parser.add_argument("--sample_size", type=int, default=1000)<br/>parser.add_argument("--path_dataset_dir", type=str, required=True)<br/><br/>MIDI_PATHS = glob.glob(os.path.join(args.path_dataset_dir, "**", "*.mid"),<br/>                       recursive=True)<br/><br/><span>if </span>__name__ == <span>"__main__"</span>:<br/>  <span>if </span>args.sample_size:<br/>    <span># Process a sample of it<br/></span><span>    </span>MIDI_PATHS_SAMPLE = random.sample(<span>list</span>(MIDI_PATHS), args.sample_size)<br/>  <span>else</span>:<br/>    <span># Process all the dataset<br/></span><span>    </span>MIDI_PATHS_SAMPLE = <span>list</span>(MIDI_PATHS)<br/>  app(MIDI_PATHS_SAMPLE)</pre>
<p>Let's explain the preceding code in more detail:</p>
<ul>
<li>We declare two new arguments, <kbd>--sample_size</kbd> and <kbd>--path_dataset_dir</kbd>. The first declares the size of the sample, while the second declares the location of the dataset.</li>
<li>Then, we use <kbd>glob.glob</kbd> on the root folder of the dataset, which will return a list of paths, with one element per MIDI file.</li>
<li>Because this operation takes a while on big datasets, you can also cache the result on disk (using the <kbd>pickle</kbd> module, for example) if you execute it often.</li>
<li>We use <kbd>random.sample</kbd> to take a subset of the MIDI paths and use the resulting MIDI paths to call the <kbd>app</kbd> method.</li>
</ul>
<p>You can use one of the distributions of LMD to launch the following code. You need to download it (for example, the <strong>LMD-full</strong> distribution) and extract the ZIP file. See the <em>Using the Lakh MIDI Dataset</em> section for the download links.</p>
<p>To launch this code with a small sample size, in a Terminal, use the following command (by replacing <kbd>PATH_DATASET</kbd> with the root folder of the extracted dataset and <kbd>PATH_OUTPUT</kbd>):</p>
<pre><strong>python chapter_06_example_00.py --sample_size=1000 --path_dataset_dir="PATH_DATASET" --path_output_dir="PATH_OUTPUT"</strong></pre>
<p>The extracted MIDI file will be in <kbd>PATH_OUTPUT</kbd>, resulting in the following statistics:</p>
<pre><strong>Number of tracks: 116189, number of tracks in sample: 116189, number of results: 12634 (10.87%)</strong><br/><strong>Time:  7197.6346254</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Here, we can see that approximately 10% of the MIDI file has a <kbd>--bass_drums_on_beat_threshold</kbd> over 0.75. This returns 12,634 results on the whole LMD dataset, which is more than enough to train our model later. We'll look at training in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Building a jazz dataset</h1>
                </header>
            
            <article>
                
<p>In the previous section, we introduced the tools that are necessary for building a dataset based on information contained in the MIDI files from the full LMD dataset. In this section, we'll delve deeper into building a custom dataset by using external APIs such as the Last.fm API.</p>
<p>In this section, we'll use the <strong>LMD-matched</strong> distribution since it is (partially) matched with the MSD containing metadata information that will be useful for us, such as artist and title. That metadata can then be used in conjunction with Last.fm to get the song's genre. We'll also be extracting drum and piano instruments, just like we did in the previous section.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The LMD extraction tools</h1>
                </header>
            
            <article>
                
<p>Before we start, we'll look at how to handle the LMD dataset. First, we need to download the following three elements from <a href="https://colinraffel.com/projects/lmd/">colinraffel.com/projects/lmd/</a>:</p>
<ul>
<li><strong>LMD-matched</strong>: A subset of LMD that is matched with MDS</li>
<li><strong>LMD-matched metadata</strong>: The H5 database containing the metadata information</li>
<li><strong>Match scores</strong>: The dictionary of match scores</li>
</ul>
<p>Once extracted in the same folder, you should have the following elements: the <kbd>lmd_matched</kbd> directory, the <kbd>lmd_matched_h5</kbd> directory, and the <kbd>match_scores.json</kbd> file.</p>
<div class="packt_tip">You can find this code in the <kbd>lakh_utils.py</kbd> file, in the source code of this chapter. There are more comments and content in the source code, so check it out.</div>
<p class="mce-root"/>
<p>In the <kbd>lakh_utils.py</kbd> file, you have the utilities to find metadata and MIDI file paths from a unique identifier, <kbd>MSD_ID</kbd>. Our starting point will be the <kbd>match_scores.json</kbd> file, a dictionary of matched files in the following format:</p>
<pre>{<br/>...<br/>  "<strong>TRRNARX128F4264AEB</strong>": {"<strong>cd3b9c8bb118575bcd712cffdba85fce</strong>": <strong>0.7040202098544246</strong>},<br/>  "TRWMHMP128EF34293F": {<br/>    "c3da6699f64da3db8e523cbbaa80f384": 0.7321245522741104,<br/>    "d8392424ea57a0fe6f65447680924d37": 0.7476196649194942<br/>  },<br/>...<br/>}</pre>
<p>As the key, we have the <kbd>MSD_ID</kbd>, and as the value, we have a dictionary of matches, each with a score. From an <kbd>MSD_ID</kbd>, we can get the highest score match using the <kbd>get_matched_midi_md5</kbd> method. From that MD5, we'll be able to load the corresponding MIDI file using the <kbd>get_midi_path</kbd> method.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fetching a song's genre using the Last.fm API</h1>
                </header>
            
            <article>
                
<p>The first part of our example uses the Last.fm API to fetch each song's genre. There are other APIs, such as Spotify's Echo Nest, that can be used to fetch such information. You can choose another service provider for this section if you feel like it.</p>
<p>The first step is to create an account on <a href="https://www.last.fm/api/">www.last.fm/api/</a>. Since we won't be making any changes using the API, once you have an account, you only need to keep the <strong>API key</strong>.</p>
<div class="packt_tip"><span>You can find this section's code in the </span><kbd>chapter_06_example_01.py</kbd><span> file, in the source code of this chapter. There are more comments and content in the source code, so check it out.</span></div>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Reading information from the MSD</h1>
                </header>
            
            <article>
                
<p>Before we call Last.fm to get the song's genre, we need to find the artist and title of each song. Because LMD is matched with MSD, finding that information is easy. Follow these steps to do so:</p>
<ol>
<li>First, let's define a <kbd>process</kbd> method, as we did in the previous chapter, that can be called using threads, and that fetches the artist's information from the H5 database:</li>
</ol>
<pre style="padding-left: 60px"><span>import </span>argparse<br/>import tables<br/><span>from </span>typing <span>import </span>List, Optional<br/><span>from </span>lakh_utils <span>import </span>msd_id_to_h5<br/><span>from </span>threading_utils <span>import </span>AtomicCounter<br/><br/>parser = argparse.ArgumentParser()<br/>parser.add_argument(<span>"--sample_size"</span>, <span>type</span>=<span>int</span>, <span>default</span>=<span>1000</span>)<br/>parser.add_argument(<span>"--path_dataset_dir"</span>, <span>type</span>=<span>str</span>, <span>required</span>=<span>True</span>)<br/>parser.add_argument(<span>"--path_match_scores_file"</span>, <span>type</span>=<span>str</span>, <span>required</span>=<span>True</span>)<br/>args = parser.parse_args()<br/><br/><span>def </span>process(msd_id: <span>str</span>, counter: AtomicCounter) -&gt; Optional[<span>dict</span>]:<br/>  <span>try</span>:<br/>    <span>with </span>tables.open_file(msd_id_to_h5(msd_id, args.path_dataset_dir)) <span>as </span>h5:<br/>      artist = h5.root.metadata.songs.cols.<strong>artist_name[<span>0</span>]</strong>.decode(<span>"utf-8"</span>)<br/>      title = h5.root.metadata.songs.cols.<strong>title[0]</strong>.decode("utf-8")<br/>      <span>return </span>{<span>"msd_id"</span>: msd_id, <span>"artist"</span>: artist, "title": title}<br/>  <span>except </span><span>Exception </span><span>as </span>e:<br/>    <span>print</span>(<span>f"Exception during processing of </span><span>{</span>msd_id<span>}</span><span>: </span><span>{</span>e<span>}</span><span>"</span>)<span><br/></span><span>  finally</span>:<br/>    counter.increment()</pre>
<p style="padding-left: 60px">This code looks just like the code we wrote in the previous section. Here, we use the <kbd>tables</kbd> module to open the H5 database. Then, we use the <kbd>msd_id_to_h5</kbd> method from our <kbd>lakh_utils</kbd> module to get the path to the H5 file. Finally, we fetch the artist and title in the H5 database before returning the result in a dictionary.</p>
<p class="mce-root"/>
<ol start="2">
<li>Now, we can call the <kbd>process</kbd> method, just like we did in the previous chapter. Before doing that, we need to load the score matches dictionary, which contains all the matches between LMD and MSD:</li>
</ol>
<pre style="padding-left: 60px"><span>from lakh_utils import get_msd_score_matches<br/><br/>MSD_SCORE_MATCHES = get_msd_score_matches(args.path_match_scores_file)<br/><br/>if </span>__name__ == <span>"__main__"</span>:<br/>  <span>if </span>args.sample_size:<br/>    <span># Process a sample of it<br/></span><span>    </span>MSD_IDS = random.sample(<span>list</span>(MSD_SCORE_MATCHES), args.sample_size)<br/>  <span>else</span>:<br/>    <span># Process all the dataset<br/></span><span>    </span>MSD_IDS = <span>list</span>(MSD_SCORE_MATCHES)<br/>  app(MSD_IDS)</pre>
<p style="padding-left: 60px">To do that, we need to use the <kbd>get_msd_score_matches</kbd> method, which loads the dictionary in memory. Then, we take a sample of the full dataset using our <kbd>app</kbd> method.</p>
<ol start="3">
<li>Finally, to launch this code with a small sample size, in a Terminal, use the following command (by replacing <kbd>PATH_DATASET</kbd> and <kbd>PATH_MATCH_SCORES</kbd>):</li>
</ol>
<pre style="padding-left: 60px"><strong>python chapter_06_example_01.py --sample_size=1000 --path_dataset_dir="PATH_DATASET" --path_match_score="PATH_MATCH_SCORES"</strong></pre>
<p>You should receive the following output:</p>
<pre><strong><span>Number of tracks: 31034, number of tracks in sample: 31034, number of results: 31034 (100.00%)</span><span><br/></span><span>Time:  21.0088559</span></strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Now, we can plot the 25 most common artists, which should result in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/dc57cefb-9c5d-4b6d-8a67-f89485f40645.png" style="width:45.33em;height:36.25em;"/></p>
<p>You could create a dataset based on one or multiple artists you like if you wanted to. You might end up with too few MIDI files for the training process, but it might be worth a shot.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using top tags to find genres</h1>
                </header>
            
            <article>
                
<p>Now that we know how to fetch information in the matched MSD database, we can call Last.fm to fetch the genre information for a track. </p>
<p class="mce-root"/>
<div class="packt_tip"><span>You can find this section's code in the </span><kbd>chapter_06_example_02.py</kbd><span> file, in the source code of this chapter. There are more comments and content in the source code, so check it out.</span></div>
<p>Let's get started:</p>
<ol>
<li>The easiest way to call the Last.fm API is to perform a simple <kbd>GET</kbd> request. We'll do this in a <kbd>get_tags</kbd> method that takes the H5 database as a parameter:</li>
</ol>
<pre style="padding-left: 60px"><span>import </span>argparse<br/><span>from </span>typing <span>import </span>List, Optional<br/><span>import </span>requests<br/><span>from </span>threading_utils <span>import </span>AtomicCounter<br/><br/>parser = argparse.ArgumentParser()<br/>parser.add_argument(<span>"--sample_size"</span>, <span>type</span>=<span>int</span>, <span>default</span>=<span>1000</span>)<br/>parser.add_argument(<span>"--path_dataset_dir"</span>, <span>type</span>=<span>str</span>, <span>required</span>=<span>True</span>)<br/>parser.add_argument(<span>"--path_match_scores_file"</span>, <span>type</span>=<span>str</span>, <span>required</span>=<span>True</span>)<br/>parser.add_argument(<span>"--last_fm_api_key"</span>, <span>type</span>=<span>str</span>, <span>required</span>=<span>True</span>)<br/>args = parser.parse_args()<br/><br/><span>def </span>get_tags(h5) -&gt; Optional[<span>list</span>]:<br/>  title = h5.root.metadata.songs.cols.title[<span>0</span>].decode(<span>"utf-8"</span>)<br/>  artist = h5.root.metadata.songs.cols.artist_name[<span>0</span>].decode(<span>"utf-8"</span>)<br/>  request = (<strong><span>f"https://ws.audioscrobbler.com/2.0/"<br/></span><span>             f"?method=track.gettoptags"<br/></span><span>             f"&amp;artist=</span><span>{</span>artist<span>}</span><span>"<br/></span><span>             f"&amp;track=</span><span>{</span>title<span>}</span><span>"<br/></span><span>             f"&amp;api_key=</span><span>{</span>args.last_fm_api_key<span>}</span><span>"<br/></span><span>             f"&amp;format=json"</span></strong>)<br/>  response = requests.get(request, <span>timeout</span>=<span>10</span>)<br/>  json = response.json()<br/>  <span>if </span><span>"error" </span><span>in </span>json:<br/>    <span>raise </span><span>Exception</span>(<span>f"Error in request for '</span><span>{</span>artist<span>}</span><span>' - '</span><span>{</span>title<span>}</span><span>': "<br/></span><span>                    f"'</span><span>{</span>json[<span>'message'</span>]<span>}</span><span>'"</span>)<br/>  <span>if </span><span>"toptags" </span><span>not in </span>json:<br/>    <span>raise </span><span>Exception</span>(<span>f"Error in request for '</span><span>{</span>artist<span>}</span><span>' - '</span><span>{</span>title<span>}</span><span>': "<br/></span><span>                    f"no top tags"</span>)<br/>  tags = [tag[<span>"name"</span>] <span>for </span>tag <span>in </span><strong>json[<span>"toptags"</span>][<span>"tag"</span>]</strong>]<br/>  tags = [tag.lower().strip() <span>for </span>tag <span>in </span>tags <span>if </span>tag]<br/>  <span>return </span>tags</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p style="padding-left: 60px">This code makes a request to the <kbd>track.gettoptags</kbd> API endpoint, which returns an ordered list of genres for the track, ordered from most tag count to less tag count, where the tag count is calculated from the user's submissions. The correct classification of those tags varies greatly from one artist to the other.</p>
<div class="packt_tip packt_infobox">You can find a lot of information on a track, artist, or release using APIs such s Last.fm or Echo Nest. Make sure you check out what information they provide when building your own dataset.</div>
<p style="padding-left: 60px">While a bit naive (we don't clean up the track name an artist name, or retry using another matching), most of the tracks (over 80%) are found on Last.fm, which is good enough for the purpose of our example.</p>
<ol start="2">
<li>Here's a simple process method that we can use to call our <kbd>get_tags</kbd> method:</li>
</ol>
<pre style="padding-left: 60px"><span>from typing import Optional<br/>import tables<br/>from threading_utils import AtomicCounter<br/>from lakh_utils import msd_id_to_h5<br/><br/>def </span>process(msd_id: <span>str</span>, counter: AtomicCounter) -&gt; Optional[<span>dict</span>]:<br/>  <span>try</span>:<br/>    <span>with </span>tables.open_file(msd_id_to_h5(msd_id, args.path_dataset_dir)) <span>as </span>h5:<br/>      tags = <strong>get_tags</strong>(h5)<br/>      <span>return </span>{<span>"msd_id"</span>: msd_id, <span>"tags"</span>: tags}<br/>  <span>except </span><span>Exception </span><span>as </span>e:<br/>    <span>print</span>(<span>f"Exception during processing of </span><span>{</span>msd_id<span>}</span><span>: </span><span>{</span>e<span>}</span><span>"</span>)<br/>  <span>finally</span>:<br/>    counter.increment()</pre>
<p class="mce-root"/>
<ol start="3">
<li>This example is based on jazz music, but you can use other genres for this example if you wish. You can plot the most popular common genres in LMD using the following code:</li>
</ol>
<pre style="padding-left: 60px"><span>from collections import Counter<br/></span><br/>tags = [result[<span>"tags"</span>][<span>0</span>] <span>for </span>result <span>in </span>results <span>if </span>result[<span>"tags"</span>]]<br/>most_common_tags_20 = Counter(tags).<strong>most_common(<span>20</span>)</strong><br/>plt.bar([tag <span>for </span>tag, _ <span>in </span>most_common_tags_20],<br/>        [count <span>for </span>_, count <span>in </span>most_common_tags_20])<br/>plt.title(<span>"Most common tags (20)"</span>)<br/>plt.xticks(<span>rotation</span>=<span>30</span>, <span>horizontalalignment</span>=<span>"right"</span>)<br/>plt.ylabel(<span>"count"</span>)<br/>plt.show()</pre>
<p>This should produce a plot that looks similar to the one shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><span class="underline"><img src="assets/f219f960-0db1-49cd-987f-d1c916694e02.png" style="width:42.67em;height:34.17em;"/></span></p>
<p>We'll be using the <strong>jazz</strong> genre, but you might want to combine multiple genres, such as <strong>jazz</strong> and <strong>blues</strong>, so that you have more content to work with or to create hybrid styles. We'll look at how much data you actually need for your models so that you can train them properly in the next chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Finding instrument classes using MIDI</h1>
                </header>
            
            <article>
                
<p>We'll be training on two instruments for our example, but you can do something else if you feel like it:</p>
<ul>
<li><strong>Percussion</strong>: We'll be extracting drum tracks from the MIDI file to train a Drums RNN model</li>
<li><strong>Piano</strong>: We'll be extracting piano tracks to train a Melody RNN model</li>
</ul>
<p>Here, the first step is finding the instruments that we have in our dataset. In the <kbd>PrettyMIDI</kbd> module, the <kbd>Instrument</kbd> class contains a <kbd>program</kbd> property that can be used to find such information. As a reminder, you can find more information about the various programs in the General MIDI 1 Sound Set specification (<a href="https://www.midi.org/specifications/item/gm-level-1-sound-set">www.midi.org/specifications/item/gm-level-1-sound-set</a>).</p>
<p>Each program corresponds to an instrument, and each instrument is classified in an instrument class. We'll be using this classification to find statistics about our dataset. Let's get started:</p>
<div class="packt_tip">You can find this section's code in the <kbd>chapter_06_example_04.py</kbd> file, in the source code of this chapter. There are more comments and content in the source code, so check it out.</div>
<ol>
<li>First, let's write the <kbd>get_instrument_classes</kbd> method for this purpose:</li>
</ol>
<pre style="padding-left: 60px"><span>from </span>typing <span>import </span>List, Optional<br/><span>from </span>pretty_midi <span>import </span>PrettyMIDI, program_to_instrument_class<br/>from lakh_utils import get_midi_path<br/>from lakh_utils import get_matched_midi_md5<br/><br/><span>def </span>get_instrument_classes(msd_id) -&gt; Optional[<span>list</span>]:<br/>  midi_md5 = get_matched_midi_md5(msd_id, MSD_SCORE_MATCHES)<br/>  midi_path = get_midi_path(msd_id, midi_md5, args.path_dataset_dir)<br/>  pm = PrettyMIDI(midi_path)<br/>  classes = [<strong>program_to_instrument_class(instrument.program)</strong><br/>             <span>for </span>instrument <span>in </span>pm.instruments<br/>             <span>if not </span>instrument.is_drum]<br/>  drums = [<span>"Drums" </span><span>for </span>instrument <span>in </span>pm.instruments <span>if </span>instrument.is_drum]<br/>  classes = classes + drums<br/>  <span>if not </span>classes:<br/>    <span>raise </span><span>Exception</span>(<span>f"No program classes for </span><span>{</span>msd_id<span>}</span><span>: "<br/></span><span>                    f"</span><span>{</span><span>len</span>(classes)<span>}</span><span>"</span>)<br/>  <span>return </span>classes</pre>
<p style="padding-left: 60px">First, we load a <kbd>PrettyMIDI</kbd> instance and then convert the <kbd>program</kbd> into its instrument class. Here, you can see that we are handling the drums separately since there is no <kbd>program</kbd> property for drums.</p>
<ol start="2">
<li>Now, we can write our <kbd>process</kbd> method as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>from typing import Optional<br/>import tables<br/>from threading_utils import AtomicCounter</span><span><br/>from lakh_utils import msd_id_to_h5<br/><br/>def </span>process(msd_id: <span>str</span>, counter: AtomicCounter) -&gt; Optional[<span>dict</span>]:<br/>  <span>try</span>:<br/>    <span>with </span>tables.open_file(msd_id_to_h5(msd_id, args.path_dataset_dir)) <span>as </span><span>h5</span>:<br/>      classes = <strong>get_instrument_classes</strong>(msd_id)<br/>      <span>return </span>{<span>"msd_id"</span>: msd_id, <span>"classes"</span>: classes}<br/>  <span>except </span><span>Exception </span><span>as </span>e:<br/>    <span>print</span>(<span>f"Exception during processing of </span><span>{</span>msd_id<span>}</span><span>: </span><span>{</span>e<span>}</span><span>"</span>)<br/>  <span>finally</span>:<br/>    counter.increment()</pre>
<ol start="3">
<li>The most common instrument classes can be found using the following code:</li>
</ol>
<pre style="padding-left: 60px">from collections import Counter<br/><br/>classes_list = [result[<span>"classes"</span>] <span>for </span>result <span>in </span>results]<br/>classes = [c <span>for </span>classes <span>in </span>classes_list <span>for </span>c <span>in </span>classes]<br/>most_common_classes = Counter(classes).<strong>most_common()</strong><br/>plt.bar([c <span>for </span>c, _ <span>in </span>most_common_classes],<br/>        [count <span>for </span>_, count <span>in </span>most_common_classes])<br/>plt.title(<span>'Instrument classes'</span>)<br/>plt.xticks(<span>rotation</span>=<span>30</span>, <span>horizontalalignment</span>=<span>"right"</span>)<br/>plt.ylabel(<span>'count'</span>)<br/>plt.show()</pre>
<p class="mce-root"/>
<p>You should have similar results to what can be seen in the following diagram on the LMD:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/22ac43bf-acfe-4473-8ad8-67bafcd5da12.png" style="width:49.17em;height:39.33em;"/></p>
<p>Here, we can see that the most used instrument classes in the LMD are <strong>Guitar</strong>, <strong>Drums</strong>, <strong>Ensemble</strong>, and P<strong>iano</strong>. We'll be using the <strong>Drums</strong> and <strong>Piano</strong> classes in the upcoming sections, but you can use another class if you feel like it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting jazz, drums, and piano tracks</h1>
                </header>
            
            <article>
                
<p>Now that we are able to find the instrument tracks we want, filter the song by genre, and export the resulting MIDI files, we can put everything together to create two jazz datasets, one containing percussion and the other containing piano.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting and merging jazz drums</h1>
                </header>
            
            <article>
                
<p>We've already implemented most of the code for extracting jazz drums, namely the <kbd>get_tags</kbd> method and the <kbd>extract_drums</kbd> method.</p>
<div class="packt_tip">You can find this section's code in the <kbd>chapter_06_example_07.py</kbd> file, in the source code of this chapter. There are more comments and content in the source code, so check it out.</div>
<p>The <kbd>process</kbd> method should call the <kbd>get_tags</kbd> and <kbd>extract_drums</kbd> methods like this:</p>
<pre><span>import argparse<br/>import ast<br/>from typing import Optional<br/>import tables<br/>from multiprocessing_utils import AtomicCounter<br/><br/>parser = argparse.ArgumentParser()<br/>parser.add_argument("--tags", type=str, required=True)<br/>args = parser.parse_args()<br/><br/><strong>TAGS</strong> = ast.literal_eval(args.tags)<br/><br/>def </span>process(msd_id: <span>str</span>, counter: AtomicCounter) -&gt; Optional[<span>dict</span>]:<br/>  <span>try</span>:<br/>    <span>with </span>tables.open_file(msd_id_to_h5(msd_id, args.path_dataset_dir)) <span>as </span>h5:<br/>      tags = <strong>get_tags</strong>(h5)<br/>      matching_tags = [tag <span>for </span>tag <span>in </span>tags <span>if </span>tag <span>in </span><strong>TAGS</strong>]<br/>      <span>if not </span>matching_tags:<br/>        <span>return<br/></span><span>      </span>pm_drums = <strong>extract_drums</strong>(msd_id)<br/>      <strong>pm_drums.write</strong>(os.path.join(args.path_output_dir, <span>f"</span><span>{</span>msd_id<span>}</span><span>.mid"</span>))<br/>      <span>return </span>{<span>"msd_id"</span>: msd_id,<br/>              <span>"pm_drums"</span>: pm_drums,<br/>              <span>"tags"</span>: matching_tags}<br/>  <span>except </span><span>Exception </span><span>as </span>e:<br/>    <span>print</span>(<span>f"Exception during processing of </span><span>{</span>msd_id<span>}</span><span>: </span><span>{</span>e<span>}</span><span>"</span>)<br/>  <span>finally</span>:<br/>    counter.increment()</pre>
<p>Here, we are using the <kbd>ast</kbd> module to parse the <kbd>tags</kbd> argument. This is useful because it allows us to use the Python list syntax for the value of a flag, that is, <kbd>--tags="['jazz', 'blues']"</kbd>. Then, we can check if the tags coming from Last.fm match with one of the required tags and write the resulting MIDI drums file to disk if so.</p>
<p class="mce-root"/>
<p>The drum's lengths and genre repartition can be seen in the following plots:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/58ac3d06-1bd5-4bd5-8757-67e39f5674d1.png"/></p>
<p>Here, we can see that we have around 2,000 MIDI files when combining both "<strong>jazz</strong>" and "<strong>blues</strong>".</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Extracting and splitting jazz pianos</h1>
                </header>
            
            <article>
                
<p>The last method we need to write is the piano extraction method. The <kbd>extract_pianos</kbd> method is similar to the previous <kbd>extract_drums</kbd> method, but instead of merging the tracks together, it splits them into separate piano tracks, potentially returning multiple tracks for each song. Let's get started:</p>
<div class="packt_tip">You can find this section's code in the <kbd>chapter_06_example_08.py</kbd> file, in the source code of this chapter. There are more comments and content in the source code, so check it out.</div>
<ol>
<li>First, we'll write the <kbd>extract_pianos</kbd> method, as follows:</li>
</ol>
<pre style="padding-left: 60px"><span>from typing import List<br/>from pretty_midi import Instrument<br/>from pretty_midi import PrettyMIDI<br/>from lakh_utils import get_matched_midi_md5<br/>from lakh_utils import get_midi_path<br/><br/>PIANO_PROGRAMS = list(range(0, 8))<br/><br/>def </span>extract_pianos(msd_id: <span>str</span>) -&gt; List[PrettyMIDI]:<br/>  os.makedirs(args.path_output_dir, <span>exist_ok</span>=<span>True</span>)<br/>  midi_md5 = get_matched_midi_md5(msd_id, MSD_SCORE_MATCHES)<br/>  midi_path = get_midi_path(msd_id, midi_md5, args.path_dataset_dir)<br/>  pm = PrettyMIDI(midi_path)<br/>  pm.instruments = [instrument <span>for </span>instrument <span>in </span>pm.instruments<br/>                    <strong><span>if </span>instrument.program <span>in </span>PIANO_PROGRAMS</strong><br/><strong>                    <span>and not </span>instrument.is_drum</strong>]<br/>  <strong>pm_pianos = []</strong><br/>  <span>if </span><span>len</span>(pm.instruments) &gt; <span>1</span>:<br/>    <span>for </span>piano_instrument <span>in </span>pm.instruments:<br/>      pm_piano = copy.deepcopy(pm)<br/>      pm_piano_instrument = Instrument(<span>program</span>=piano_instrument.program)<br/>      pm_piano.instruments = [pm_piano_instrument]<br/>      <span>for </span>note <span>in </span>piano_instrument.notes:<br/>        pm_piano_instrument.notes.append(note)<br/>      <strong>pm_pianos.append(pm_piano)</strong><br/>  <span>else</span>:<br/>    pm_pianos.append(pm)<br/>  <span>for </span>index, pm_piano <span>in </span><span>enumerate</span>(pm_pianos):<br/>    <span>if </span><span>len</span>(pm_piano.instruments) != <span>1</span>:<br/>      <span>raise </span><span>Exception</span>(<span>f"Invalid number of piano </span><span>{</span>msd_id<span>}</span><span>: "<br/></span><span>                      f"</span><span>{</span><span>len</span>(pm_piano.instruments)<span>}</span><span>"</span>)<br/>    <span>if </span>pm_piano.get_end_time() &gt; <span>1000</span>:<br/>      <span>raise </span><span>Exception</span>(<span>f"Piano track too long </span><span>{</span>msd_id<span>}</span><span>: "<br/></span><span>                      f"</span><span>{</span>pm_piano.get_end_time()<span>}</span><span>"</span>)<br/>  <span>return </span><strong>pm_pianos</strong></pre>
<p style="padding-left: 60px">We've already covered most of the code in this snippet. The difference here is that we are filtering the instrument on any of the piano programs, ranging from 0 to 8. We're also returning multiple piano MIDI files.</p>
<ol start="2">
<li>Now, we can call our method using the following <kbd>process</kbd> method:</li>
</ol>
<pre style="padding-left: 60px"><span>import argparse<br/>import ast<br/>from typing import Optional<br/>import tables<br/>from lakh_utils import msd_id_to_h5<br/>from multiprocessing_utils import AtomicCounter<br/><br/>parser = argparse.ArgumentParser()<br/>parser.add_argument("--tags", type=str, required=True)<br/>args = parser.parse_args()<br/><br/><strong>TAGS</strong> = ast.literal_eval(args.tags)<br/><br/>def </span>process(msd_id: <span>str</span>, counter: AtomicCounter) -&gt; Optional[<span>dict</span>]:<br/>  <span>try</span>:<br/>    <span>with </span>tables.open_file(msd_id_to_h5(msd_id, args.path_dataset_dir)) <span>as </span>h5:<br/>      tags = <strong>get_tags</strong>(h5)<br/>      matching_tags = [tag <span>for </span>tag <span>in </span>tags <span>if </span>tag <span>in </span><strong>TAGS</strong>]<br/>      <span>if not </span>matching_tags:<br/>        <span>return<br/></span><span>      </span>pm_pianos = <strong>extract_pianos</strong>(msd_id)<br/>      <span>for </span>index, pm_piano <span>in </span><span>enumerate</span>(pm_pianos):<br/>        <strong>pm_piano.write</strong>(os.path.join(args.path_output_dir,<br/>                                    <span>f"</span><span>{</span>msd_id<span>}</span><span>_</span><span>{</span>index<span>}</span><span>.mid"</span>))<br/>      <span>return </span>{<span>"msd_id"</span>: msd_id,<br/>              <span>"pm_pianos"</span>: pm_pianos,<br/>              <span>"tags"</span>: matching_tags}<br/>  <span>except </span><span>Exception </span><span>as </span>e:<br/>    <span>print</span>(<span>f"Exception during processing of </span><span>{</span>msd_id<span>}</span><span>: </span><span>{</span>e<span>}</span><span>"</span>)<br/>  <span>finally</span>:<br/>    counter.increment()</pre>
<p style="padding-left: 60px">This code was covered in the previous section on drums, except here, each piano file is written separately using an index. The piano's lengths and genres can be plotted like so:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a83ea8b1-eb95-4280-9dd2-1d7687090c27.png"/></p>
<p>Here, we've found just under 2,000 piano tracks for <strong>jazz</strong> and <strong>blues</strong>.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing the data using pipelines</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we looked at existing datasets and developed tools so that we can find and extract specific content. By doing so, we've effectively built a dataset we want to train our model on. But building the dataset isn't all <span>–</span> we also need to <strong>prepare</strong> it. By preparing, we mean the action of removing everything that isn't useful for training, cutting, and splitting tracks, and also automatically adding more content.</p>
<p>In this section, we'll be looking at some built-in utilities that we can use to transform the different data formats (MIDI, MusicXML, and ABCNotation) into a training-ready format. These utilities are called <kbd>pipelines</kbd> in Magenta, and are a sequence of operations that are executed on the input data.</p>
<p>An example of an operation that is already implemented in pipelines includes discarding melodies with too many pitches that are too long or too short. Another operation is transposing melodies, which consists of taking a melody and creating a new one by shifting the note's pitches up or down. This is a common practice in machine learning called <strong>dataset augmentation</strong>, which is useful if we wish to make the model train better by providing it with variations of the original data.</p>
<p>Let's take a look at what pipelines are and how they can be used. In this section, we'll be using the Melody RNN pipeline as an example, but each model has its own pipeline with its own specifics. For example, the Drums RNN pipeline does not transpose the drums sequences because it wouldn't make sense to do so.</p>
<p>Before we start talking about pipelines, we'll have a brief look at manually refining the dataset we built previously.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Refining the dataset manually</h1>
                </header>
            
            <article>
                
<p>This might sound obvious, but we'll stress it because it is also really important: verify the MIDI files you've extracted. The dataset's content should correspond to what you were looking for in terms of music.</p>
<p class="mce-root"/>
<p>The easiest way of verifying a track's content is by opening the content in MuseScore and listening to it:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/43a92975-da8c-4b3f-9f04-132c13d44b96.png"/></p>
<p>Let's have a look a the things we can verify for each of these files:</p>
<ul>
<li>The first thing to verify is whether the <strong>instruments</strong> we've extracted are present in the resulting MIDI files, meaning that, for our jazz piano example, we should have only one instrument track and that it should be any of the eight piano programs.</li>
<li>Another thing to verify is whether the <strong>genre</strong> of the tracks fits our requirements. Does the piano sound like jazz music? Does the music actually sound good to you?</li>
<li>Other problems to look out for include tracks that are <strong>incomplete</strong>, too short, too long, or full of silence. Some of those tracks will be filtered by the data preparation pipeline, but a manual pass on the data is also important.</li>
<li>If some of the tracks you like are filtered by the data preparation pipeline, for example, for being too short, you can manually fix this issue by copying and pasting parts of it to make it longer. You can also write a pipeline to automatically do that.</li>
</ul>
<p>If some of the tracks don't fit your requirements, remove them from the dataset.</p>
<p>Once you've validated the dataset's content and removed all the unwanted tracks, you can also cut and clean the content of each file. The easiest way of doing that is by going into MuseScore, listening to the track, removing the parts you don't like, and exporting the file back to MIDI.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking at the Melody RNN pipeline</h1>
                </header>
            
            <article>
                
<p>Once we've manually refined the dataset, we are ready to prepare it using a pipeline, resulting in data that has been prepared for training. As an example, we'll be looking at the Melody RNN pipeline.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Launching the data preparation stage on our dataset</h1>
                </header>
            
            <article>
                
<p>The first step when it comes to preparing the data is to call the <kbd>convert_dir_to_note_sequences</kbd> command, which is the same regardless of the model you will be using. This command will take a directory containing MIDI files, MusicXML files, or ABCNotation files as input and convert them into TensorFlow records of <kbd>NoteSequence</kbd>.</p>
<p>We recommend that you create another folder for your training data (separate from the dataset folder you created previously). Now, let's get started:</p>
<ol start="1">
<li>First, change directory to the folder you've created and call the <kbd>convert_dir_to_note_sequences</kbd> command using the following command (replace <kbd>PATH_OUTPUT_DIR</kbd> with the directory you used in the previous section):</li>
</ol>
<pre style="padding-left: 60px">convert_dir_to_note_sequences --input_dir="PATH_OUTPUT_DIR" --output_file="notesequences.tfrecord"</pre>
<p style="padding-left: 60px">This will output a bunch of "Converted MIDI" files and produce a <kbd>notesequences.tfrecord</kbd>. From now on, the data is in the same format, regardless of the symbolic representation we used when building the dataset.</p>
<ol start="2">
<li>Now, we can launch the pipeline on our data using the following code:</li>
</ol>
<pre style="padding-left: 60px">melody_rnn_create_dataset --config="attention_rnn" --input="notesequences.tfrecord" --output_dir="sequence_examples" --eval_ratio=0.10</pre>
<p style="padding-left: 60px">First, we need to give <kbd>--config</kbd> as an argument. This is necessary because the encoder and decoder are defined in the configuration (see <a href="48023567-4100-492a-a28e-53b18a63e01e.xhtml">Chapter 3</a>, <em>Generating Polyphonic Melodies</em>, for a refresher on how encoding and decoding works).</p>
<p style="padding-left: 60px">We also pass the <kbd>--eval_ratio</kbd> argument, which will give the pipeline the number of elements in the training and evaluation sets. When executed, the pipeline will output statistics and warnings about the files it encounters.</p>
<p class="mce-root"/>
<p style="padding-left: 60px">The statistics are printed on the console for each increment of 500 files that are processed, but only the last part (after the <strong>Completed.</strong> output) is of interest to us. The following is the output of the 500 samples of the jazz piano dataset:</p>
<pre style="padding-left: 60px"><strong>Processed 500 inputs total. Produced 122 outputs</strong>.<br/>DAGPipeline_MelodyExtractor_eval_melodies_discarded_too_few_pitches: 7<br/>DAGPipeline_MelodyExtractor_eval_melodies_discarded_too_long: 0<br/>DAGPipeline_MelodyExtractor_eval_melodies_discarded_too_short: 42<br/>DAGPipeline_MelodyExtractor_eval_melodies_truncated: 2<br/>DAGPipeline_MelodyExtractor_eval_melody_lengths_in_bars:<br/>  [7,8): 4<br/>  [8,10): 2<br/>  [10,20): 2<br/>  [30,40): 2<br/>DAGPipeline_MelodyExtractor_eval_polyphonic_tracks_discarded: 113<br/>DAGPipeline_MelodyExtractor_training_melodies_discarded_too_few_pitches: 45<br/>DAGPipeline_MelodyExtractor_training_melodies_discarded_too_long: 0<br/>DAGPipeline_MelodyExtractor_training_melodies_discarded_too_short: 439<br/>DAGPipeline_MelodyExtractor_training_melodies_truncated: 20<br/>DAGPipeline_MelodyExtractor_training_melody_lengths_in_bars:<br/><strong>  [7,8): 22</strong><br/><strong>  [8,10): 21</strong><br/><strong>  [10,20): 42</strong><br/><strong>  [20,30): 11</strong><br/><strong>  [30,40): 16</strong><br/>DAGPipeline_MelodyExtractor_training_polyphonic_tracks_discarded: 982<br/>DAGPipeline_RandomPartition_eval_melodies_count: 49<br/>DAGPipeline_RandomPartition_training_melodies_count: 451<br/>DAGPipeline_TranspositionPipeline_eval_skipped_due_to_range_exceeded: 0<br/>DAGPipeline_TranspositionPipeline_eval_transpositions_generated: 317<br/>DAGPipeline_TranspositionPipeline_training_skipped_due_to_range_exceeded: 0<br/>DAGPipeline_TranspositionPipeline_training_transpositions_generated: 2387</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p class="mceNonEditable"/>
<p style="padding-left: 60px">The statistics of interest here are as follows::</p>
<ul>
<li style="padding-left: 60px"><kbd>Processed 500 inputs total. <strong>Produced 122 outputs.</strong></kbd>: This gives us the input size or the number of provided MIDI files, as well as the number of resulting <kbd>SequenceExample</kbd> that will be used for training (122, counting both evaluation and training sets). This is the most important statistic.</li>
<li style="padding-left: 60px"><kbd>DAGPipeline_MelodyExtractor_MODE_melody_lengths_in_bars</kbd>: This gives you the length of the resulting <kbd>SequenceExample</kbd> elements for each "MODE".</li>
</ul>
<p>The <kbd>SequenceExample</kbd> encapsulates the data that will be fed to the network during training. Those statistics are useful because the quantity (as well as the quality) of the data is important for the model's training. If a model doesn't train properly on 122 outputs, we'll need to make sure we have more data for the next time we train.</p>
<p>In that sense, it is really important to look at the produced outputs, which tells us about the exact amount of data the network will receive. It doesn't matter whether we feed 100,000 MIDI files to the data preparation pipeline if a small amount of <kbd>SequenceExample</kbd> is produced because the input data isn't good. If a pipeline produces a small number of outputs for a big input, look at the statistics and find out which part of the processing step is removing the elements.</p>
<p>Now, let's have a look at how the pipeline is defined and executed for our example.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding a pipeline execution</h1>
                </header>
            
            <article>
                
<p>The statistics we provided in the previous section are a bit confusing because they are not shown in the order they are executed. Let's have a proper look at how this is really executed to understand what's going on:</p>
<ul>
<li><kbd>DagInput</kbd> initiates the pipeline's execution, taking each <kbd>NoteSequence</kbd> of the TensorFlow records as input (500 elements).</li>
<li><kbd>RandomPartition</kbd> randomly splits the elements into training and evaluation sets given the ratio provided in the command (450 elements in the training set and 50 elements in the evaluation set).</li>
<li><kbd>TimeChangeSplitter</kbd> splits the elements at each time change (doesn't output statistics).</li>
<li><kbd>Quantizer</kbd> quantizes the note sequence on the closest step defined by the <kbd>steps_per_quarter</kbd> attribute in the configuration and discards elements with multiple tempos and time signatures (doesn't output statistics).</li>
<li><kbd>TranspositionPipeline</kbd> transposes the note sequence into multiple pitches, adding new elements in the process (2,387 elements generated by transposition for the training set).</li>
<li><kbd>MelodyExtractor</kbd> extracts the melodies from the <kbd>NoteSequence</kbd>, returning a <kbd>Melody</kbd> and removing elements if needed, such as polyphonic tracks and tracks that are too short or too long (1,466 elements are removed for the training set). This part also outputs the lengths of the melodies in bars:
<ul>
<li>The minimum and maximum length of the melody are defined by <kbd>min_bars</kbd> and <kbd>max_steps</kbd>, respectively. See the next section to learn how to change them.</li>
<li><kbd>ignore_polyphonic_notes</kbd>, which is set <kbd>True</kbd>, discards polyphonic tracks.</li>
</ul>
</li>
<li><kbd>EncoderPipeline</kbd> encodes<kbd>Melody</kbd> into <kbd>SequenceExample</kbd> using <kbd>KeyMelodyEncoderDecoder</kbd> defined for the attention configuration (doesn't output statistics). The encoder pipeline receives the configuration passed as an argument; for example, <kbd>LookbackEventSequenceEncoderDecoder</kbd> for the <kbd>lookback_rnn</kbd> configuration.</li>
<li><kbd>DagOutput</kbd> finishes the execution.</li>
</ul>
<p>If you want to look at the implementation of the <kbd>Pipeline</kbd>, have a look at the <kbd>get_pipeline</kbd> method in the <kbd>melody_rnn_pipeline</kbd> module.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Writing your own pipeline</h1>
                </header>
            
            <article>
                
<p>As you might have noticed from the code in the <kbd>get_pipeline</kbd> method, most of the configurations cannot be changed. However, we can write our own pipeline and call it directly.</p>
<div class="packt_tip">You can find this section's code in the <kbd>melody_rnn_pipeline_example.py</kbd> file, in the source code of this chapter. There are more comments and content in the source code, so check it out.</div>
<p>For this example, we'll take the existing Melody RNN pipeline, copy it, and change the transposition and sequence length. Let's get started:</p>
<ol>
<li>First, copy the <kbd>get_pipeline</kbd> method and call it using the following Python code (replacing <kbd>INPUT_DIR</kbd> and <kbd>OUTPUT_DIR</kbd> with the proper values):</li>
</ol>
<pre style="padding-left: 60px">from magenta.pipelines import pipeline<br/><br/>pipeline_instance = <strong>get_pipeline</strong>("attention_rnn", <span>eval_ratio</span>=0.10)<br/>pipeline.run_pipeline_serial(<br/>    pipeline_instance,<br/>    pipeline.tf_record_iterator(INPUT_DIR, pipeline_instance.input_type),<br/>    OUTPUT_DIR)</pre>
<p style="padding-left: 60px">You should see the same output that we received previously when we used the pipeline method. By taking a small sample (500 pieces of data) of the piano jazz dataset, we received the following output:</p>
<pre style="padding-left: 60px"><strong>INFO:tensorflow:Processed 500 inputs total. Produced 115 outputs.</strong><br/><strong>...</strong><br/><strong>INFO:tensorflow:DAGPipeline_MelodyExtractor_training_melody_lengths_in_bars:</strong><br/><strong>  [7,8): 31</strong><br/><strong>  [8,10): 9</strong><br/><strong>  [10,20): 34</strong><br/><strong>  [20,30): 11</strong><br/><strong>  [30,40): 17</strong><br/><strong>  [50,100): 2</strong><br/><strong>...</strong><br/><strong>INFO:tensorflow:DAGPipeline_TranspositionPipeline_training_transpositions_generated: 2058</strong><br/><strong>...</strong></pre>
<ol start="2">
<li>Now, let's change some parameters to see how it works. In the following code, we've added some transpositions (the default transposition value is <kbd>(0,)</kbd>, which means no transposition shifts):</li>
</ol>
<pre style="padding-left: 60px">...<br/>transposition_pipeline = note_sequence_pipelines.TranspositionPipeline(<br/>  <strong>(<span>0</span>,12)</strong>, <span>name</span>=<span>'TranspositionPipeline_' </span>+ mode)<br/>...</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p style="padding-left: 60px">By using the transpositions <kbd>(0,12)</kbd>, we're telling the transposition pipeline to create, for each existing sequence, a sequence 12 pitches higher, corresponding to a full octave shift up. Keep the rest of the code as is.</p>
<div class="packt_infobox">Transposition values should follow musical intervals expressed in semitones (a pitch value in MIDI). The simplest interval is the perfect interval that we are using, which corresponds to an octave, or 12 semitones or MIDI pitches. Other intervals can be used, such as the Major third, which is used in the Polyphony RNN pipeline, with a transposition range of <kbd>(-4, 5)</kbd>.</div>
<p style="padding-left: 60px">Now, the output should look as follows:</p>
<pre style="padding-left: 60px"><strong>...</strong><br/><strong>INFO:tensorflow:Processed 500 inputs total. Produced 230 outputs.</strong><br/><strong>...</strong><br/><strong>INFO:tensorflow:DAGPipeline_MelodyExtractor_training_melody_lengths_in_bars:</strong><br/><strong>  [7,8): 66</strong><br/><strong>  [8,10): 14</strong><br/><strong>  [10,20): 64</strong><br/><strong>  [20,30): 22</strong><br/><strong>  [30,40): 34</strong><br/><strong>  [50,100): 4</strong><br/><strong>...</strong><br/><strong>INFO:tensorflow:DAGPipeline_TranspositionPipeline_training_transpositions_generated: 4297</strong><br/><strong>...</strong></pre>
<p style="padding-left: 60px">Notice how we now have approximately twice as much data to work with. Data augmentation is important for handling small datasets.</p>
<ol start="3">
<li>We can also change the minimum and maximum lengths of the sequences in the melody extractor, like so:</li>
</ol>
<pre style="padding-left: 60px">...<br/>melody_extractor = melody_pipelines.MelodyExtractor(<br/> <strong><span>min_bars</span>=<span>15</span></strong>, <strong><span>max_steps</span>=<span>1024</span></strong>, <span>min_unique_pitches</span>=<span>5</span>,<br/> <span>gap_bars</span>=<span>1.0</span>, <span>ignore_polyphonic_notes</span>=<span>False</span>,<br/> <span>name</span>=<span>'MelodyExtractor_' </span>+ mode)<br/>...</pre>
<p style="padding-left: 60px">The preceding code will output a total of 92 outputs (instead of our previous 230).</p>
<p class="mce-root"/>
<p style="padding-left: 60px">We can also write our own pipeline class. For example, we could automatically cut sequences that are too long or duplicate sequences that are too short, instead of discarding them. For a note sequence pipeline, we need to extend the <kbd>NoteSequencePipeline</kbd> class and implement the <kbd>transform</kbd> method, as shown in the following code:</p>
<pre style="padding-left: 60px"><span>from </span>magenta.pipelines.note_sequence_pipelines <span>import </span>NoteSequencePipeline<span><br/><br/>class </span>MyTransformationClass(<strong>NoteSequencePipeline</strong>):<span><br/></span><span>  </span><span>def </span><strong>transform</strong>(<span>self</span>, note_sequence):<br/>    # My transformation code here<br/>    <span>pass</span></pre>
<p>Take a look at the <kbd>sequences_lib</kbd> module in Magenta, which contains tons of utilities for handling note sequences. Each dataset needs to be prepared and the easiest way to prepare the data is by creating new pipelines. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking at MusicVAE data conversion</h1>
                </header>
            
            <article>
                
<p>The MusicVAE model doesn't use pipelines <span>–</span> actually, it doesn't even have a dataset creation script. Compared to our previous example with Melody RNN, it still uses similar transformations (such as data augmentation) and is more configurable since some of the transformations can be configured, instead of us needing to write a new pipeline.</p>
<p>Let's have a look at a simple MusicVAE configuration contained in the <kbd>configs</kbd> module of the <kbd>music_vae</kbd> module. Here, you can find the following <kbd>cat-mel_2bar_small</kbd> configuration:</p>
<pre><span># Melody<br/></span>CONFIG_MAP[<span>'<strong>cat-mel_2bar_small</strong>'</span>] = Config(<br/>    <span>model</span>=MusicVAE(lstm_models.BidirectionalLstmEncoder(),<br/>                   lstm_models.CategoricalLstmDecoder()),<br/>    <span>hparams</span>=merge_hparams(<br/>        lstm_models.get_default_hparams(),<br/>        HParams(<br/>            <span>batch_size</span>=<span>512</span>,<br/>            <span>max_seq_len</span>=<span>32</span>,  <span># 2 bars w/ 16 steps per bar<br/></span><span>            </span><span>z_size</span>=<span>256</span>,<br/>            <span>enc_rnn_size</span>=[<span>512</span>],<br/>            <span>dec_rnn_size</span>=[<span>256</span>, <span>256</span>],<br/>            <span>free_bits</span>=<span>0</span>,<br/>            <span>max_beta</span>=<span>0.2</span>,<br/>            <span>beta_rate</span>=<span>0.99999</span>,<br/>            <span>sampling_schedule</span>=<span>'inverse_sigmoid'</span>,<br/>            <span>sampling_rate</span>=<span>1000</span>,<br/>        )),<br/>    <strong><span>note_sequence_augmenter</span>=data.NoteSequenceAugmenter(<span>transpose_range</span>=(-<span>5</span>, <span>5</span>))</strong>,<br/>    <span>data_converter</span>=data.OneHotMelodyConverter(<br/>        <span>valid_programs</span>=data.MEL_PROGRAMS,<br/>        <span>skip_polyphony</span>=<span>False</span>,<br/>        <strong><span>max_bars</span>=<span>100</span></strong>,  <span># Truncate long melodies before slicing.<br/></span><span>        </span><strong><span>slice_bars</span>=<span>2</span></strong>,<br/>        <span>steps_per_quarter</span>=<span>4</span>),<br/>    <span>train_examples_path</span>=<span>None</span>,<br/>    <span>eval_examples_path</span>=<span>None</span>,<br/>)</pre>
<p>The following list further explains the code:</p>
<ul>
<li>By looking at the <kbd>NoteSequenceAugmenter</kbd> class, you can see that it takes note of sequence augmentation by using shifting (like in our custom pipeline) and stretching, another data augmentation technique.</li>
<li>It also limits the maximum length of the melody to <kbd>max_bars=100</kbd>, but remember that MusicVAE handles limited size samples because of its network type. In this example, each sample is sliced to a length of <kbd>slice_bars=2</kbd>.</li>
<li>The note sequence augmenter lets you decide a transposition range that it will randomly choose a value from.</li>
<li>Stretching isn't used for Melody RNN because most stretching ratios don't work for quantized sequences. Stretching can be used for Performance RNN, for example.</li>
</ul>
<p>We won't be looking at creating a new configuration just now. See <a href="6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml">Chapter 7</a>, <em>Training Magenta Models</em>, for more information on how to do that.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we looked at how to build and prepare a dataset that will be used for training. First, we looked at existing datasets and explained how some are more suitable than others for a specific use case. We then looked at the LMD and the MSD, which are useful for their size and completeness, and datasets from the Magenta team, such as the MAESTRO dataset and the GMD. We also looked at external APIs such as Last.fm, which can be used to enrich existing datasets.</p>
<p class="mce-root"/>
<p>Then, we built a dance music dataset and used information contained in MIDI files to detect specific structures and instruments. We learned how to compute our results using multiprocessing and how to plot statistics about the resulting MIDI files.</p>
<p>After, we built a jazz dataset by extracting information from the LMD and using the Last.fm API to find the genre of each song. We also looked at how to find and extract different instrument tracks in the MIDI files.</p>
<p>Finally, we prepared the data for training. By using pipelines, we were able to process the files we extracted, remove the files that weren't of the proper length, quantize them, and use data augmentation techniques to create a proper dataset, ready for training. By doing this, we saw how different models have different pipelines, depending on their network type.</p>
<p>In the next chapter, we'll use what we produced in this chapter to train some models on the datasets we've produced. You'll see that training is an empirical process that requires a lot of back and forth between preparing the data and training the model. During this process, you will likely come back to this chapter for more information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>What are the advantages and disadvantages of the different symbolic representations?</li>
<li>Write a piece of code that will extract cello instruments from MIDI files.</li>
<li>How many rock songs are present in LMD? How many match one of the "jazz", "blues", "country" tags?</li>
<li>Write a piece of code that will extend MIDI files that are too short for the Melody RNN pipeline.</li>
<li>Extract the jazz drums from GMD. Can we train a quantized model with this?</li>
<li>Why is data augmentation important?</li>
</ol>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>The MAESTRO Dataset and Wave2Midi2Wave:</strong> A Magenta team blog post on the MAESTRO dataset and its usage in the Wave2Midi2Wave method (<a href="https://magenta.tensorflow.org/maestro-wave2midi2wave">magenta.tensorflow.org/maestro-wave2midi2wave</a>)</li>
<li><strong>Enabling Factorized Piano Music Modeling and Generation with the MAESTRO Dataset:</strong> A paper (2019) about MAESTRO and Wave2Midi2Wave (<a href="https://arxiv.org/abs/1810.12247">arxiv.org/abs/1810.12247</a>)</li>
<li><strong>Celebrating Johann Sebastian Bach:</strong> The Bach Doodle, which gave us the Bach Doodle Dataset (<a href="https://www.google.com/doodles/celebrating-johann-sebastian-bach">www.google.com/doodles/celebrating-johann-sebastian-bach</a>)</li>
<li><strong>Visualizing the Bach Doodle Dataset:</strong> An amazing visualization of the Bach Doodle Dataset (<a href="https://magenta.tensorflow.org/bach-doodle-viz">magenta.tensorflow.org/bach-doodle-viz</a>)</li>
<li><strong>The Bach Doodle: Approachable music composition with machine learning at scale:</strong> A paper (2019) about the Bach Doodle dataset (<a href="https://arxiv.org/abs/1907.06637">arxiv.org/abs/1907.06637</a>)</li>
</ul>


            </article>

            
        </section>
    </body></html>