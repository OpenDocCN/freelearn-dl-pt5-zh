["```py\nfrom tensorflow.keras.datasets import imdb\n```", "```py\nvocab_size = 8000\n(X_train, y_train), (X_test, y_test) = imdb.load_data\\\n                                       (num_words=vocab_size)\n```", "```py\nprint(type(X_train))\nprint(type(X_train[5]))\nprint(X_train[5])\n```", "```py\n<class 'numpy.ndarray'>\n<class 'list'>\n[1, 778, 128, 74, 12, 630, 163, 15, 4, 1766, 7982, 1051, \n 2, 32, 85, 156, 45, 40, \n 148, 139, 121, 664, 665, 10, 10, 1361, 173, 4, 749, 2, 16, \n 3804, 8, 4, 226, 65,\n 12, 43, 127, 24, 2, 10, 10]\n```", "```py\nmaxlen = 200\n```", "```py\nfrom tensorflow.keras import preprocessing\nX_train = preprocessing.sequence.pad_sequences\\\n          (X_train, maxlen=maxlen)\nX_test = preprocessing.sequence.pad_sequences\\\n         (X_test, maxlen=maxlen)\n```", "```py\nprint(X_train[5])\n```", "```py\n    import numpy as np\n    import tensorflow as tf\n    np.random.seed(42)\n    tf.random.set_seed(42)\n    ```", "```py\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers \\\n    import SimpleRNN, Flatten, Dense, Embedding, \\\n    SpatialDropout1D, Dropout\n    model_rnn = Sequential()\n    ```", "```py\n    model_rnn.add(Embedding(vocab_size, output_dim=32))\n    model_rnn.add(SpatialDropout1D(0.4))\n    ```", "```py\n    model_rnn.add(SimpleRNN(32))\n    ```", "```py\n    model_rnn.add(Dropout(0.4))\n    ```", "```py\n    model_rnn.add(Dense(1, activation='sigmoid'))\n    ```", "```py\n    model_rnn.compile(loss='binary_crossentropy', \\\n                      optimizer='rmsprop', metrics=['accuracy'])\n    model_rnn.summary()\n    ```", "```py\n    history_rnn = model_rnn.fit(X_train, y_train, \\\n                                batch_size=128, \\\n                                validation_split=0.2, \\\n                                epochs = 10)\n    ```", "```py\n    y_test_pred = model_rnn.predict_classes(X_test)\n    from sklearn.metrics import accuracy_score\n    print(accuracy_score(y_test, y_test_pred))\n    ```", "```py\n    0.85128\n    ```", "```py\ninp_review = \"An excellent movie!\"\n```", "```py\nfrom tensorflow.keras.preprocessing.text \\\nimport text_to_word_sequence\n```", "```py\ntext_to_word_sequence(inp_review)\n```", "```py\n['an', 'excellent', 'movie']\n```", "```py\nword_map = imdb.get_word_index()\n```", "```py\nvocab_map = dict(sorted(word_map.items(), \\\n                 key=lambda x: x[1])[:vocab_size])\n```", "```py\ndef preprocess(review):\n    inp_tokens = text_to_word_sequence(review)\n    seq = []\n    for token in inp_tokens:\n        seq.append(vocab_map.get(token))\n    return seq\n```", "```py\npreprocess(inp_review)\n```", "```py\n[32, 318, 17]\n```", "```py\nmodel_rnn.predict_classes([preprocess(inp_review)])\n```", "```py\narray([[1]])\n```", "```py\ninp_review = \"Don't watch this movie\"\\\n             \" - poor acting, poor script, bad direction.\"\n```", "```py\nmodel_rnn.predict_classes([preprocess(inp_review)])\n```", "```py\narray([[0]])\n```", "```py\n    from tensorflow.keras.layers import LSTM\n    ```", "```py\n    model_lstm = Sequential()\n    model_lstm.add(Embedding(vocab_size, output_dim=32))\n    model_lstm.add(SpatialDropout1D(0.4))\n    ```", "```py\n    model_lstm.add(LSTM(32))\n    ```", "```py\n    model_lstm.add(Dropout(0.4))\n    model_lstm.add(Dense(1, activation='sigmoid'))\n    model_lstm.compile(loss='binary_crossentropy', \\\n                       optimizer='rmsprop', metrics=['accuracy'])\n    model_lstm.summary()\n    ```", "```py\n    history_lstm = model_lstm.fit(X_train, y_train, \\\n                                  batch_size=128, \\\n                                  validation_split=0.2, \\\n                                  epochs=5)\n    ```", "```py\n    y_test_pred = model_lstm.predict_classes(X_test)\n    print(accuracy_score(y_test, y_test_pred))\n    ```", "```py\n    0.87032\n    ```", "```py\n    from tensorflow.keras.layers import GRU\n    ```", "```py\n    model_gru = Sequential()\n    model_gru.add(Embedding(vocab_size, output_dim=32))\n    model_gru.add(SpatialDropout1D(0.4))\n    ```", "```py\n    model_gru.add(GRU(32, reset_after=False))\n    ```", "```py\n    model_gru.add(Dropout(0.4))\n    model_gru.add(Dense(1, activation='sigmoid'))\n    model_gru.compile(loss='binary_crossentropy', \\\n                      optimizer='rmsprop', metrics=['accuracy'])\n    model_gru.summary()\n    ```", "```py\n    history_gru = model_gru.fit(X_train, y_train, \\\n                                batch_size=128, \\\n                                validation_split=0.2, \\\n                                epochs = 4)\n    ```", "```py\n    y_test_pred = model_gru.predict_classes(X_test)\n    accuracy_score(y_test, y_test_pred)\n    ```", "```py\n    0.87156\n    ```", "```py\n    from tensorflow.keras.layers import Bidirectional\n    ```", "```py\n    model_bilstm = Sequential()\n    model_bilstm.add(Embedding(vocab_size, output_dim=32))\n    model_bilstm.add(SpatialDropout1D(0.4))\n    ```", "```py\n    model_bilstm.add(Bidirectional(LSTM(32)))\n    ```", "```py\n    model_bilstm.add(Dropout(0.4))\n    model_bilstm.add(Dense(1, activation='sigmoid'))\n    model_bilstm.compile(loss='binary_crossentropy', \\\n                         optimizer='rmsprop', metrics=['accuracy'])\n    model_bilstm.summary()\n    ```", "```py\n    history_bilstm = model_bilstm.fit(X_train, y_train, \\\n                                      batch_size=128, \\\n                                      validation_split=0.2, \\\n                                      epochs = 4)\n    ```", "```py\n    y_test_pred = model_bilstm.predict_classes(X_test)\n    accuracy_score(y_test, y_test_pred)\n    ```", "```py\n    0.877\n    ```", "```py\n    model_stack = Sequential()\n    model_stack.add(Embedding(vocab_size, output_dim=32))\n    model_stack.add(SpatialDropout1D(0.4))\n    ```", "```py\n    model_stack.add(LSTM(32, return_sequences=True))\n    ```", "```py\n    model_stack.add(LSTM(32, return_sequences=False))\n    ```", "```py\n    model_stack.add(Dropout(0.5))\n    model_stack.add(Dense(1, activation='sigmoid'))\n    model_stack.compile(loss='binary_crossentropy', \\\n                        optimizer='rmsprop', \\\n                        metrics=['accuracy'])\n    model_stack.summary()\n    ```", "```py\n    history_stack = model_stack.fit(X_train, y_train, \\\n                                    batch_size=128, \\\n                                    validation_split=0.2, \\\n                                    epochs = 4)\n    ```", "```py\n    y_test_pred = model_stack.predict_classes(X_test)\n    accuracy_score(y_test, y_test_pred)\n    ```", "```py\n    0.87572\n    ```", "```py\nsents = [\"life is good\", \"good life\", \"good\"]\n```", "```py\ntok = Tokenizer()\ntok.fit_on_texts(sents)\n```", "```py\ntok.texts_to_sequences(sents)\n```", "```py\n[[2, 3, 1], [1, 2], [1]]\n```"]