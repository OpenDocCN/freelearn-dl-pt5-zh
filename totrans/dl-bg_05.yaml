- en: Learning from Data
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation takes a great deal of time for complex datasets, as we saw
    in the previous chapter. However, time spent on data preparation is time well
    invested... this I can guarantee! In the same way, investing time in understanding
    the basic theory of learning from data is super important for any person that
    wants to join the field of deep learning. Understanding the fundamentals of learning
    theory will pay off whenever you read new algorithms or evaluate your own models.
    It will also make your life much easier when you get to the later chapters in
    this book.
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, this chapter introduces the most elementary concepts around
    the theory of deep learning, including measuring performance on regression and
    classification as well as the identification of overfitting. It also offers some
    warnings about the sensibility of—and the need to optimize—model hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The outline of this chapter is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Learning for a purpose
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring success and error
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Identifying overfitting and generalization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The art behind learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical implications of training deep learning algorithms
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learning for a purpose
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [Chapter 3](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=26&action=edit),
    *Preparing Data*, we discussed how to prepare data for two major types of problems: **regression**
    and **classification**. In this section, we will cover the technical differences
    between classification and regression in more detail. These differences are important
    because they will limit the type of machine learning algorithms you can use to
    solve your problem.
  prefs: []
  type: TYPE_NORMAL
- en: Classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'How do you know whether your problem is classification? The answer depends
    on two major factors: the **problem** you are trying to solve and the **data**
    you have to solve your problem. There might be other factors, for sure, but these
    two are by far the most significant.'
  prefs: []
  type: TYPE_NORMAL
- en: 'If your purpose is to make a model that, given some input, will determine whether
    the response or output of the model is to distinguish between two or more distinct
    categories, then you have a classification problem. Here is a non-exhaustive list
    of examples of classification problems:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Given an image, indicate what number it contains (distinguish between 10 categories:
    0-9 digits).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given an image, indicate whether it contains a cat or not (distinguish between
    two categories: yes or no).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given a sequence of readings about temperature, determine the season (distinguish
    between four categories: the four seasons).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given the text of a tweet, determine the sentiment (distinguish between two
    categories: positive or negative).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given an image of a person, determine the age group (distinguish between five
    categories: <18, 18-25, 26-35, 35-50, >50).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given an image of a dog, determine its breed (distinguish between 120 categories:
    those breeds that are internationally recognized).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given an entire document, determine whether it has been tampered with (distinguish between
    categories: authentic or altered).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Given satellite readings of a spectroradiometer, determine whether the geolocation
    matches the spectral signature of vegetation or not (distinguish between two categories:
    yes or no).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see from the examples in the list, there are different types of data
    for different types of problems. The data that we are seeing in these examples
    is known as **labeled data**.
  prefs: []
  type: TYPE_NORMAL
- en: Unlabeled data is very common but is rarely used for classification problems
    without some type of processing that allows the matching of data samples to a
    category. For example, unsupervised clustering can be used on unlabeled data to
    assign the data to specific clusters (such as groups or categories); at which
    point, the data technically becomes "labeled data."
  prefs: []
  type: TYPE_NORMAL
- en: 'The other important thing to notice from the list is that we can categorize
    the classification problems into two major groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary classification**: For classification between any two classes only'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-class classification**: For classification between more than just two
    classes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This distinction may seem arbitrary but it is not; in fact, the type of classification
    will limit the type of learning algorithm you can use and the performance you
    can expect. To understand this a little better, let's discuss each classification
    separately.
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: This type of classification is usually regarded as a much simpler problem than
    multiple classes. In fact, if we can solve the binary classification problem,
    we could, technically, solve the problem of multiple classes by deciding on a
    strategy to break down the problem into several binary classification problems
    (*Lorena, A. C.* et al., *2008*).
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the reasons why this is considered a simpler problem is because of the
    algorithmic and mathematical foundations behind binary classification learning
    algorithms. Let''s say that we have a binary classification problem, such as the
    Cleveland dataset explained in [Chapter 3](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=26&action=edit), *Preparing
    Data*. This dataset consists of 13 medical observations for each patient—we can
    call that ![](img/4ae6e047-6db7-4cea-89a7-d59b4577f989.png). For each of these
    patient records, there is an associated label that indicates whether the patient
    has some type of heart disease (+1) or not (-1)—we will call that ![](img/0f347d74-fa0b-4e7f-8523-3b84761dae43.png).
    So, an entire dataset, ![](img/fbdaf353-eb1c-4455-a3f5-cbebb2f0d6e7.png), with *N *samples
    can be defined as a set of data and labels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dff30c72-c9b3-4a6f-a20a-e3a3017c59e7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Then, as discussed in [Chapter 1](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=24&action=edit), *Introduction
    to Machine Learning*, the whole point of learning is to use an algorithm that
    will find a way to map input data, **x**, to label the *y* correctly for all samples
    in [![](img/fbdaf353-eb1c-4455-a3f5-cbebb2f0d6e7.png)] and to be able to further
    do so (hopefully) for samples outside of the known dataset, ![](img/fbdaf353-eb1c-4455-a3f5-cbebb2f0d6e7.png).
    Using a perceptron and a corresponding **Perceptron Learning Algorithm** (**PLA**),
    what we want is to find the parameters [![](img/72310c26-8d2e-4d7b-8b0e-2694ac960fbf.png)] that
    can satisfy the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a8cba13d-2fa4-48ee-907a-23d24d12ba66.png)'
  prefs: []
  type: TYPE_IMG
- en: For all samples, *i* = 1, 2, ..., *N.* However, as we discussed in [Chapter
    1](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=24&action=edit),
    *Introduction to Machine Learning*, the equation cannot be satisfied if the data
    is non-linearly separable. In that case, we can obtain an approximation, or a
    prediction, that is not necessarily the desired outcome; we will call such a prediction ![](img/ca21520f-7620-4ce7-8ba6-ef31faaed1e9.png).
  prefs: []
  type: TYPE_NORMAL
- en: The whole point of a learning algorithm, then, becomes to reduce the differences
    between the desired target label, ![](img/67f2f764-f8f9-4e62-abfd-5c9dddfe4114.png),
    and the prediction, ![](img/dda3ec0e-9026-4af0-abb5-71bf7486e83c.png). In an ideal
    world, we want ![](img/46139ee6-41db-420e-a2e9-95cc864cda3a.png) for all cases
    of *i* = 1, 2, ..., *N.* In cases of *i *where ![](img/7f1182f2-7155-4ec8-aca1-50a4033f4c85.png),
    the learning algorithm must make adjustments (that is, train itself) to avoid
    making such mistakes in the future by finding new parameters ![](img/2be5be4c-924d-462b-b31e-0dd3771de027.png) that
    are hopefully better.
  prefs: []
  type: TYPE_NORMAL
- en: 'The science behind such algorithms varies from model to model, but the ultimate
    goals are usually the same:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the number of errors, ![](img/dc0b01b2-c4f3-499b-8a4d-302e1d8fa01b.png),
    in every learning iteration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn the model parameters in as few iterations (steps) as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn the model parameters as fast as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since most datasets deal with non-separable problems, the PLA is disregarded
    in favor of other algorithms that will converge faster and in fewer iterations.
    Many learning algorithms like this learn to adjust the parameters ![](img/10030282-7398-4858-8911-a2f401ae5315.png) by
    taking specific steps to reduce the error, ![](img/fa1abe60-b158-4da2-b57c-73f2a10d40cf.png),
    based on derivatives with respect to the variability of the error and the choice
    of parameters. So, the most successful algorithms (in deep learning, at least)
    are those based on some type of gradient descent strategy (Hochreiter, S., et.al.
    2001).
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's go over the most basic iterative gradient strategy. Say that we want
    to learn the parameters ![](img/293a24a9-8122-473a-b0e9-3f3c1f5f62e0.png) given
    the dataset, ![](img/66b34a6d-fe6c-42c0-b839-75629dd44781.png). We will have to
    make a small adjustment to the problem formulation to make things a little easier.
    What we want is for ![](img/4e82d5bd-8068-4b93-8586-84592a77fc05.png) to be implied
    in the expression ![](img/4e4b289f-920a-4351-9622-14e68b8cacce.png). The only
    way this could work is if we set ![](img/f1ff6cd9-db78-495b-b8ed-23cf890c12cb.png)and ![](img/357ebb91-cfb5-4c22-b82d-49ddbb38b5f2.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'With this simplification, we can simply search for **w**, which implies a search
    for **b** as well. Gradient descent with a fixed *learning rate* is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Initialize the weights to zero ( ![](img/1f4e3960-c78b-45c9-90e2-3921c8452c3a.png)) and
    the iteration counter to zero (![](img/6ce01007-2b63-4bc4-8db8-4f1d0f7c33a1.png)).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'When ![](img/429a3278-fffb-4640-8730-c1c103a64135.png), do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate the gradient with respect to ![](img/23cda490-47ff-4f78-b5fc-195f6d6b2d0a.png) and
    store it in ![](img/99c1192e-4889-40ad-9e1b-34a8ad598d5f.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Update ![](img/8074e322-e8f8-4e22-828c-06357c3902b8.png) so that it looks like
    this: ![](img/1dfaa02c-ed8e-4bd1-aa7a-4ebd9003ccc8.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Increase the iteration counter and repeat.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'There are a couple of things that need to be explained here:'
  prefs: []
  type: TYPE_NORMAL
- en: The gradient calculation, ![](img/d6ee9c14-1d4a-4ce7-92eb-66e6f7639c98.png),
    is not trivial. For some specific machine learning models, it can be determined
    analytically; but in most cases, it must be determined numerically by using some
    of the latest algorithms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We still need to define how the error,![](img/497c3550-a1d6-4dae-bbf2-c38fce7e4fe8.png),
    is calculated; but this will be covered in the next section of this chapter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A learning rate, ![](img/1553a1e5-b84e-433e-8a44-39ec9c0a72ae.png), needs to
    be specified as well, which is a problem in itself.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One way of looking at this last issue is that in order to find the parameter, ![](img/de1605e4-2c5a-4802-919c-f70cf201ca77.png),
    that minimizes the error, we need parameter ![](img/68663c10-bdf9-4cd2-8f73-e5c381771b62.png).
    Now, we could, when applying gradient descent, think about finding the ![](img/10ee9ff7-c459-40a1-ae7e-e9f5d4fdfbe5.png) parameter, but
    we will then fall into an infinite cycle. We will not go into more detail about
    gradient descent and its learning rate since, nowadays, algorithms for gradient
    descent often include automatic calculations of it or adaptive ways of adjusting
    it (Ruder, S. 2016).
  prefs: []
  type: TYPE_NORMAL
- en: Multi-class classification
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classifying into multiple categories can have an important effect on the performance
    of learning algorithms. In a general sense, the performance of a model will decrease
    with the number of classes it is required to recognize. The exception is if you
    have plenty of data and access to lots of computing power because if you do, you
    can overcome the limitations of poor datasets that have class imbalance problems
    and you can estimate massive gradients and make large calculations and updates
    to the model. Computing power may not be a limitation in the future, but at the
    moment it is.
  prefs: []
  type: TYPE_NORMAL
- en: The multiple classes problem can be solved by using strategies such as **one
    versus one** or **one versus all**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In one versus all, you essentially have an expert binary classifier that is
    really good at recognizing one pattern from all the others and the implementation
    strategy is typically cascaded. An example is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is a graphical explanation of this strategy. Suppose we have two-dimensional
    data that tells us something about the four seasons of the year, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6ea29b49-ff32-4be0-a0cc-40d19d1896f6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 - Randomized two-dimensional data that could tell us something about
    the four seasons of the year
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case of randomized two-dimensional data, we have four categories corresponding
    to the seasons of the year. Binary classification will not work directly. However,
    we could train expert binary classifiers that specialize in *one* specific category
    *versus all* the rest. If we train one binary classifier to determine whether
    data points belong to the Summer category, using a simple perceptron, we could
    get the separating hyperplane shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ac4f8026-284f-43eb-89d9-1c25844ee256.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2: A PLA that is an expert in distinguishing from the Summer season
    data versus all the rest of the other seasons'
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, we can train the rest of the experts until we have enough to test
    our entire hypothesis; that is, until we are able to distinguish all the classes
    from each other.
  prefs: []
  type: TYPE_NORMAL
- en: Another alternative is to use classifiers that can handle multiple outputs;
    for example, decision trees or ensemble methods. But in the case of deep learning
    and neural networks, this refers to networks that can have multiple neurons in
    the output layer, such as the one depicted in *Figure 1.6* and *Figure 1.9* in [Chapter
    1](https://cdp.packtpub.com/deep_learning_for_beginners/wp-admin/post.php?post=24&action=edit), *Introduction
    to Machine Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical formulation of a multi-output neural network only changes
    slightly from a single-output one in that the output is no longer within a binary
    set of values, such as ![](img/0f347d74-fa0b-4e7f-8523-3b84761dae43.png), but
    is now a vector of one-hot encoded values, such as ![](img/3476b823-1083-4257-a7e0-71949ade683a.png).
    In this case, |*C*| denotes the size of the set *C*, which contains all the different
    class labels. For the previous example, *C *would contain the following: *C* =
    {''*Summer*'', ''*Fall*'', ''*Winter*'', ''*Spring*''}. Here is what each one-hot
    encoding would look like:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Summer**: ![](img/383c2835-581b-49ae-b2d7-745c43d5ed09.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fall**: ![](img/08a54e98-f381-4fd1-8834-8ae7c8c6a885.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Winter**: ![](img/1688ddb7-c3c5-48eb-9514-54e26474bb93.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spring**: ![](img/6f45ab24-f3c6-45f1-b841-636832c00cf0.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Every element in the target vector will correspond to the desired output of
    the four neurons. We should also point out that the dataset definition should now reflect
    that both the sample input data and the labels are vectors:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/556fdbd7-1f5b-4934-8441-d7a4d5a260d4.png)'
  prefs: []
  type: TYPE_IMG
- en: Another way of dealing with the problem of multiple-class classification is
    by using **regression**.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Previously, we specified that for binary classification, the target variable
    could take on a set of binary values; for example, ![](img/0f347d74-fa0b-4e7f-8523-3b84761dae43.png).
    We also said that for multiple classification, we could modify the target variable
    to be a vector whose size depends on the number of classes, ![](img/a5246959-0c32-4040-aa86-b99a914f8c09.png).
    Well, regression problems deal with cases where the target variable is any real
    value, ![](img/8dbf0dcb-9e08-48c6-888e-9e62c7d77f96.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The implications here are very interesting because with a regression model
    and algorithm, we could *technically* do binary classification since the set of
    real numbers contains any binary set of numbers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/aeaca13b-8358-4c7e-8bf1-e50fae3e3f72.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Further, if we change *C* = {''*Summer*'', ''*Fall*'', ''*Winter*'', ''*Spring*''}
    to a numerical representation instead, such as C = {0,1,2,3}, then *technically*,
    we would again use regression due to the same property:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c06b709f-7343-4fe6-9e3b-712fac9177ae.png).'
  prefs: []
  type: TYPE_NORMAL
- en: Although regression models can solve classification problems, it is recommended
    that you use models that are specialized in classification specifically and leave
    the regression models only for regression tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Even if regression models can be used for classification (Tan, X., et.al. 2012),
    they are ideal for when the target variable is a real number. Here is a sample
    list of regression problems:'
  prefs: []
  type: TYPE_NORMAL
- en: When given an image, indicate how many people are in it (the output can be any
    integer >=0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When given an image, indicate the probability of it containing a cat (the output
    can be any real number between 0 and 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When given a sequence of readings about temperature, determine what the temperature
    actually feels like (the output can be any integer whose range depends on the
    units).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When given the text of a tweet, determine the probability of it being offensive (the
    output can be any real number between 0 and 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When given an image of a person, determine their age (the output can be any
    positive integer, usually less than 100).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When given an entire document, determine the probable compression rate (the
    output can be any real number between 0 and 1).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When given satellite readings of a spectroradiometer, determine the corresponding
    infrared value (the output can be any real number).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When given the headlines of some major newspapers, determine the price of oil
    (the output can be any real number >=0).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As you can see from this list, there are many possibilities due to the fact
    that the range of real numbers encompasses all integers and all positive and negative
    numbers, and even if the range is too broad for specific applications, the regression
    model can be scaled up or down to meet the range specifications.
  prefs: []
  type: TYPE_NORMAL
- en: To explain the potential of regression models, let's start with a basic **linear
    regression** model and in later chapters, we will cover more complex regression
    models based on deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'The linear regression model tries to solve the following problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/970566f0-b7ac-4868-81dc-6b18e0b0fc2b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The problem is solved for *i* = 1, 2, ..., *N.* We could, however, use the
    same trick as before and include the calculation of *b *in the same equation.
    So, we can say that we are trying to solve the following problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c802c25e-40e3-46c3-92bc-22fc0113e952.png)'
  prefs: []
  type: TYPE_IMG
- en: Once again, we are trying to learn the parameters, ![](img/d59f4bf8-7872-4734-8050-abd4ec68ecab.png),
    that yield ![](img/562075f4-c6a5-4f29-a6e4-84f8c4f329c3.png) for all cases of *i.*
    In the case of linear regression, the prediction, ![](img/ca21520f-7620-4ce7-8ba6-ef31faaed1e9.png),
    should ideally be equal to the true target value, ![](img/2a654437-6c4b-4da0-a2ad-8d800b7847ac.png),
    if the input data, ![](img/5a5aa15c-23cf-45b3-801f-b4d8ca5449b8.png), somehow
    describes a perfect straight line. But because this is very unlikely, there has
    to be a way of learning the parameters, ![](img/d1d74b08-42ed-48ee-b896-c39e9f170034.png),
    even if ![](img/95a975fe-111d-454d-b981-b65ab5053683.png). To achieve this, the
    linear regression learning algorithm begins by describing a low penalty for small
    mistakes and a larger penalty for big mistakes. This does make sense, right? It
    is very intuitive.
  prefs: []
  type: TYPE_NORMAL
- en: 'A natural way of penalizing mistakes in proportion to their size is by squaring
    the difference between the prediction and the target. Here is an example of when
    the difference is small:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3f4c1827-d243-4724-b234-61abbeae95c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here is an example of when the difference is large:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/879e3b84-a279-419f-bb57-1d71b48c3f14.png)'
  prefs: []
  type: TYPE_IMG
- en: In both of these examples, the desired target value is `1`. In the first case,
    the predicted value of `0.98` is very close to the target and the squared difference
    is `0.0004`, which is small compared to the second case. The second prediction
    is off by `14.8`, which yields a squared difference of `219.4`. This seems reasonable
    and intuitive for building up a learning algorithm; that is, one that penalizes
    mistakes in proportion to how big or small they are.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can formally define the overall average error in function of the choice
    of parameters **w** as the averaged sum of all squared errors, which is also known
    as the **mean squared error (MSE)**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6b88425f-07b8-4cf1-8c8c-5e15ad59dcc4.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If we define the prediction in terms of the current choice of ![](img/1d7cd898-6ee9-4952-9d43-b441c2412c88.png) as ![](img/2aa011c3-c882-4e20-a5b0-99e710545353.png),
    then we can rewrite the error function as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b5a94efc-3f8c-47bc-8b0e-67bbf7851798.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be simplified in terms of the ![](img/3f0e57b4-ce39-452d-ad17-78002c082199.png)-norm
    (also known as the Euclidean norm, ![](img/da7143d1-c3d3-4a85-aef9-2dd8b3aff0b3.png))
    by first defining a matrix of data ![](img/15a5e1a8-a00b-478c-bd88-29ffbcd6374e.png),
    whose elements are data vector ![](img/a0383757-b717-41b7-8e94-64b4fc666f00.png),
    and a vector of corresponding targets, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bdd9cc28-d833-4f38-b123-f42e24649ef5.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'The simplification of the error is then as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b8f14b75-d5fd-42ed-8486-9a0c2d91df10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This can then be expanded into the following important equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/56c796c0-55f6-447a-b116-0a44e5b47bcf.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is important because it facilitates the calculation of the derivative
    of the error, ![](img/53799600-8705-4a59-ba5e-55275c4b2c2c.png), which is necessary
    for adjusting the parameters, ![](img/04db5ca2-e2be-4220-8c6d-e87905762461.png),
    in the direction of the derivative and in proportion to the error. Now, following
    the basic properties of linear algebra, we can say that the derivative of the
    error (which is called a gradient since it yields a matrix) is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ffea4ee4-25f6-4e04-be3f-e846f88d33f2.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Because we want to find the parameters that yield the smallest error, we can
    set the gradient to `0` and solve for ![](img/c50dc76b-19c9-43bb-818a-a8a9f7c6ee3d.png).By
    setting the gradient to `0` and ignoring constant values, we arrive at the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5eb81dc2-15b3-4822-a5ae-a533d265eb0c.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/7e5c5343-31fc-429b-8511-bc9a1c84fd1c.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'These are called **normal**** equations**(Krejn, S. G. E. 1982). Then, if we
    simply use the term ![](img/aa319bc1-8e1a-4349-8dc1-72a4206a1995.png), we arrive
    at the definition of a **pseudo-inverse** (Golub, G., and Kahan, W. 1965). The
    beauty of this is that we do not need to calculate the gradient iteratively to
    choose the best parameters, ![](img/0c2d89de-6794-45ef-9e8d-537d24518945.png). As
    a matter of fact, because the gradient is analytic and direct, we can calculate ![](img/b59769c0-1de8-4212-a587-0e18479a6b73.png)in
    one shot, as explained in this linear regression algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: From ![](img/f94f1b86-f92e-4f64-99e2-2b8cd4d9a7e0.png), construct the pair, ![](img/29abe296-f275-4635-a806-55a85636b84d.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Estimate the pseudo-inverse ![](img/aa319bc1-8e1a-4349-8dc1-72a4206a1995.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Calculate and return ![](img/1905d8ba-cdea-4768-af8c-e0528aec4962.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To show this graphically, let''s say that we have a system that sends a signal
    that follows a linear function; however, the signal, when it is transmitted, becomes
    contaminated with normal noise with a `0` mean and unit variance and we are only
    able to observe the noisy data, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0aaa35da-96e2-4872-9d97-880766124f97.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 - Data readings that are contaminated with random noise
  prefs: []
  type: TYPE_NORMAL
- en: 'If, say, a hacker reads this data and runs linear regression to attempt to
    determine the true function that produced this data before it was contaminated,
    then the data hacker would obtain the solution shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb38549f-33ef-47a1-a783-777e0249d647.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 - A linear regression solution to the problem of finding the true
    function given noisy data readings
  prefs: []
  type: TYPE_NORMAL
- en: 'Clearly, as the previous figure shows, the linear regression solution is very
    close to the true original linear function. In this particular example, a high
    degree of closeness can be observed since the data was contaminated with noise
    that follows a pattern of **white noise**; however, for different types of noise,
    the model may not perform as well as in this example. Furthermore, most regression
    problems are not linear at all; in fact, the most interesting regression problems
    are highly non-linear. Nonetheless, the basic learning principle is the same:'
  prefs: []
  type: TYPE_NORMAL
- en: Reduce the number of errors,![](img/53799600-8705-4a59-ba5e-55275c4b2c2c.png),
    in every learning iteration (or directly in one shot, such as in linear regression).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn the model parameters in as few iterations (steps) as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Learn the model parameters as fast as possible.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The other major component that guides the learning process is the way the success
    or error is calculated with respect to a choice of parameters, ![](img/53799600-8705-4a59-ba5e-55275c4b2c2c.png).
    In the case of the PLA, it simply found a mistake and adjusted with respect to
    it. For multiple classes, this was through a process of gradient descent over
    some measure of error and in linear regression, this was through direct gradient
    calculation using the MSE. But now, let's dive deeper into other types of error
    measures and successes that can be quantitative and qualitative.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring success and error
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There is a wide variety of performance metrics that people use in deep learning
    models, such as accuracy, balanced error rate, mean squared error, and many others.
    To keep things organized, we will divide them into three groups: for binary classification,
    for multiple classes, and for regression.'
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'There is one essential tool used when analyzing and measuring the success of
    our models. It is known as a **c****onfusion matrix**. A confusion matrix is not
    only helpful in visually displaying how a model makes predictions, but we can
    also retrieve other interesting information from it. The following diagram shows
    a template of a confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e700456f-66b7-4e67-91cc-b5cfa91d005d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.5 - A confusion matrix and the performance metrics derived from it
  prefs: []
  type: TYPE_NORMAL
- en: A confusion matrix and all the metrics derived from it are a very important
    way of conveying how good your models are. You should bookmark this page and come
    back to it whenever you need it.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding confusion matrix, you will notice that it has two columns in
    the vertical axis that indicate the true target values, while in the horizontal
    axis, it indicates the predicted value. The intersection of rows and columns indicates
    the relationship of what should have been predicted against what was actually
    predicted. Every entry in the matrix has a special meaning and can lead to other
    meaningful composite performance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the list of metrics and what they mean:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Acronym** | **Description** | **Interpretation** |'
  prefs: []
  type: TYPE_TB
- en: '| TP | *True Positive* | This is when a data point was of the positive class
    and was correctly predicted to be of the positive class. |'
  prefs: []
  type: TYPE_TB
- en: '| TN | *True Negative* | This is when a data point was of the negative class
    and was correctly predicted to be of the negative class. |'
  prefs: []
  type: TYPE_TB
- en: '| FP | *False Positive* | This is when a data point was of the negative class
    and was incorrectly predicted to be of the positive class. |'
  prefs: []
  type: TYPE_TB
- en: '| FN | *False Negative* | This is when a data point was of the positive class
    and was incorrectly predicted to be of the negative class. |'
  prefs: []
  type: TYPE_TB
- en: '| PPV | *Positive Predictive Value* or *Precision* | This is the proportion
    of positive values that are predicted correctly out of all the values predicted
    to be positive. |'
  prefs: []
  type: TYPE_TB
- en: '| NPV | *Negative Predictive Value* | This is the proportion of negative values
    that are predicted correctly out of all the values that are predicted to be negative.
    |'
  prefs: []
  type: TYPE_TB
- en: '| FDR | *False Discovery Rate* | This is the proportion of incorrect predictions
    as false positives out of all the values that are predicted to be positive. |'
  prefs: []
  type: TYPE_TB
- en: '| FOR | *False Omission Rate* | This is the proportion of incorrect predictions
    as false negatives out of all the values that are predicted to be negative. |'
  prefs: []
  type: TYPE_TB
- en: '| TPR | *True Positive Rate,* *Sensitivity*, *Recall*, *Hit Rate* | This is
    the proportion of predicted positives that are actually positives out of all that
    should be positives. |'
  prefs: []
  type: TYPE_TB
- en: '| FPR | *False Positive Rate *or *Fall-Out* | This is the proportion of predicted
    positives that are actually negatives out of all that should be negatives. |'
  prefs: []
  type: TYPE_TB
- en: '| TNR | **True Negative Rate**, *Specificity,* or *Selectivity* | This is the
    proportion of predicted negatives that are actually negatives out of all that
    should be negatives. |'
  prefs: []
  type: TYPE_TB
- en: '| FNR | **False Negative Rate**or *Miss Rate* | This is the proportion of predicted
    negatives that are actually positives out of all that should be positives. |'
  prefs: []
  type: TYPE_TB
- en: Some of these can be a little bit obscure to understand; however, you don't
    have to memorize them now, you can always come back to this table.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other metrics that are a little bit complicated to calculate, such
    as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Acronym** | **Description** | **Interpretation** |'
  prefs: []
  type: TYPE_TB
- en: '| ACC | *Accuracy* | This is the rate of correctly predicting the positives
    and the negatives out of all the samples. |'
  prefs: []
  type: TYPE_TB
- en: '| *F*[1] | *F*[1]*-Score* | This is the average of the precisionand sensitivity.
    |'
  prefs: []
  type: TYPE_TB
- en: '| MCC | *Matthews Correlation Coefficient* | This is the correlation between
    the desired and the predicted classes. |'
  prefs: []
  type: TYPE_TB
- en: '| BER | *Balanced Error Rate* | This is the average error rate for cases where
    there is a class imbalance. |'
  prefs: []
  type: TYPE_TB
- en: I included, in this list of *complicated*calculations, acronyms such as **ACC**
    and **BER**, which are acronyms that have a very intuitive meaning. The main issue
    is, however, that these will vary when we have multiple classes. So, their calculation
    will be slightly different in multiple classes. The rest of the metrics remain
    exclusive (as defined) to binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we discuss metrics for multiple classes, here are the formulas for calculating
    the previous metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5276ec41-4004-4150-8622-dcf51e6d52c6.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/40867f27-e549-47ff-95e9-55fe50bf85ad.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/29c0a645-a95a-43e3-9a0a-6e7b4f2d3152.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/17310bd8-85d1-4b80-8976-ec16f0bdd58b.png)'
  prefs: []
  type: TYPE_IMG
- en: In a general sense, you want **ACC**, **F[1]**, and **MCC** to be high and **BER**
    to be low.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple classes
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we go beyond simple binary classification, we often deal with multiple
    classes, such as *C* = {'*Summer*', '*Fall*', '*Winter*', '*Spring*'} or *C* =
    {0,1,2,3}. This can limit, to a certain point, the way we measure error or success.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the confusion matrix for multiple classes shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8b825d44-a56e-451e-8e4f-213c04487453.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 - A confusion matrix for multiple classes
  prefs: []
  type: TYPE_NORMAL
- en: 'From the following diagram, it is evident that the notion of true positive
    or negative has disappeared since we no longer have just positive and negative
    classes, but also sets of finite classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f9d6ecb6-50a5-4079-84ed-85028b1436ab.png)'
  prefs: []
  type: TYPE_IMG
- en: Individual classes, ![](img/16d50594-4373-4352-bfa9-e7c0c228c469.png), can be
    strings or numbers, as long as they follow the rules of sets. That is, the set
    of classes, ![](img/35755034-de0c-4834-9eb3-e8d7445dc5dd.png), must be finite
    and unique.
  prefs: []
  type: TYPE_NORMAL
- en: 'To measure ACC here, we will count all the elements in the main diagonal of
    the confusion matrix and divide it by the total number of samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cb518092-12fe-4555-986f-95df3fd0102d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this equation, ![](img/7e28c26a-52a3-4d61-ba59-d9c1bd3b9356.png) denotes
    the confusion matrix and ![](img/130d0fd7-ce83-47e6-b8c3-df493c208c66.png) denotes
    the trace operation; that is, the sum of the elements in the main diagonal of
    a square matrix. Consequently, the total error is `1-ACC`, but in the case of
    class imbalance, the error metric or plain accuracy may be deceiving. For this,
    we must use the BER metric, which for multiple classes can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e9878abc-5d61-4822-8244-c05f31a27b88.png)'
  prefs: []
  type: TYPE_IMG
- en: In this new formula for BER, ![](img/69426ee5-6e9a-46cf-a421-8d20e26857e7.png) refers
    to the element in the *j*th row and *i*th column of the confusion matrix, ![](img/308bc727-30e0-4f7f-b86d-9ca99ac39cc7.png).
  prefs: []
  type: TYPE_NORMAL
- en: Some machine learning schools of thought use the rows of the confusion matrix
    to denote true labels and the columns to denote the predicted labels. The theory
    behind the analysis is the same and the interpretation is, too. Don't be alarmed
    that `sklearn` uses the flipped approach; this is irrelevant and you should not
    have any problems with following any discussions about this.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, consider the dataset that was shown earlier in *Figure 4.1*.
    If we run a five-layered neural network classifier, we could obtain decision boundaries
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/170655fa-57cc-4652-bd30-d34d624664c8.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 - Classification regions for a sample two-dimensional dataset with
    a five-layer neural net
  prefs: []
  type: TYPE_NORMAL
- en: Clearly, the dataset is not perfectly separable by a non-linear hyperplane;
    there are some data points that cross the boundaries for each class. In the previous
    graph, we can see that only the *Summer* class has no points that are incorrectly
    classified based on the classification boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, this is more evident if we actually calculate and display the confusion
    matrix, shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc71ff7b-05ae-410b-9ec5-b66bf34c08b0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 - A confusion matrix obtained from training errors on the sample
    two-dimensional dataset
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the accuracy can be calculated as ACC=(25+23+22+24)/100, which
    yields an ACC of 0.94, which seems nice, and an error rate of 1-ACC = 0.06\. This
    particular example has a slight class imbalance. Here are the samples for each
    class:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Summer: 25'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Fall: 25'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Winter: 24'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Spring: 26'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Winter group has fewer examples than the rest and the Spring group has more
    examples than the rest. While this is a very small class imbalance, it can be
    enough to yield a deceivingly low error rate. We must now calculate the balanced
    error rate, BER.
  prefs: []
  type: TYPE_NORMAL
- en: 'BER can be calculated as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/585f5334-7957-47d0-b576-3aec44b83eac.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/366a761b-958b-4dea-bf40-3ca41bcaaa2a.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, the difference between the error rate and BER is a 0.01% under-estimation
    of the error. However, for classes that are highly imbalanced, the gap can be
    much larger and it is our responsibility to measure carefully and report the appropriate
    error measure, BER.
  prefs: []
  type: TYPE_NORMAL
- en: Another interesting fact about BER is that it intuitively is the counterpart
    of a balanced accuracy; this means that if we remove the `1–` term in the BER
    equation, we are left with the balanced accuracy. Further, if we examine the terms
    in the numerator, we can see that the fractions on it lead to class-specific accuracies;
    for example, the first class, Summer, has a 100% accuracy, the second, Fall, has
    a 92% accuracy, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, the `sklearn` library has a class that can determine the confusion
    matrix automatically, given the true and predicted labels. The class is called `confusion_matrix`
    and it belongs to the `metrics` super class and we can use it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'If `y` contains the true labels, and `y_pred` contains the predicted labels,
    then the preceding instructions will output something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'We can calculate BER by simply doing this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Alternatively, `sklearn` has a built-in function to calculate the balanced
    accuracy score in the same super class as the confusion matrix. The class is called `balanced_accuracy_score`
    and we can produce BER by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Let's now discuss the metrics for regression.
  prefs: []
  type: TYPE_NORMAL
- en: Regression
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The most popular metric is **MSE**, which we discussed earlier in this chapter
    when explaining how linear regression works. However, we explained it as a function
    of the choice of hyperparameters. Here, we will redefine it in a general sense
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ce4606dc-3c6f-42b1-9172-4094989cc388.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Another metric that is very similar to MSE is **mean absolute error** (**MAE**).
    While MSE penalizes big mistakes more (quadratically) and small errors much less,
    MAE penalizes everything in direct proportion to the absolute difference between
    what should be and what was predicted. This is a formal definition of MAE:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/240ba3a7-8194-473f-9e0f-5bda05e4c582.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Finally, out of the other measures for regression, the popular choice in deep
    learning is the ***R*² score**,also known as the **coefficient of determination**.
    This metric represents the proportion of variance, which is explained by the independent
    variables in the model. It measures how likely the model is to perform well on
    unseen data that follows the same statistical distribution as the training data.
    This is its definition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0f2c6d77-a573-4d0a-b3c6-b582c4f166c7.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The sample mean is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7de549fd-f078-464a-904d-b20a8675fead.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Scikit-learn has classes available for each one of these metrics, indicated
    in the following table:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Regression metric** | **Scikit-learn class** |'
  prefs: []
  type: TYPE_TB
- en: '| *R*² score | `sklearn.metrics.r2_score` |'
  prefs: []
  type: TYPE_TB
- en: '| MAE | `sklearn.metrics.mean_absolute_error` |'
  prefs: []
  type: TYPE_TB
- en: '| MSE | `sklearn.metrics.mean_squared_error` |'
  prefs: []
  type: TYPE_TB
- en: All of these classes take the true labels and predicted labels as input arguments.
  prefs: []
  type: TYPE_NORMAL
- en: 'As an example, if we take the data and linear regression model shown in *Figure
    4.3* and *Figure 4.4* as input, we can determine the three error metrics, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The output of the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The following graph shows the sample data used, along with the performance
    obtained. Clearly, the performance using the three performance metrics is good:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d22cb47d-a379-462d-bf76-0acbf7ddda8e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 - Error metrics over a linear regression model on data contaminated
    with white noise
  prefs: []
  type: TYPE_NORMAL
- en: In general, you always want to have a determination coefficient that is as close
    to `1` as possible and all your errors (MSE and MAE) as close to `0` as possible.
    But while all of these are good metrics to report on our models, we need to be
    careful to report these metrics over **unseen validation** or **test data**. This
    is so that we accurately measure the generalization ability of the model and identify
    overfitting in our models before it becomes a catastrophic error.
  prefs: []
  type: TYPE_NORMAL
- en: Identifying overfitting and generalization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Often, when we are in a controlled machine learning setting, we are given a
    dataset that we can use for training and a different set that we can use for testing.
    The idea is that you only run the learning algorithm on the **training** data,
    but when it comes to seeing how good your model is, you feed your model the **test**
    data and observe the output. It is typical for competitions and hackathons to
    give out the test data but withhold the labels associated with it because the
    winner will be selected based on how well the model performs on the test data
    and you don't want them to cheat by looking at the labels of the test data and
    making adjustments. If this is the case, we can use a **validation** dataset,
    which we can create by ourselves by separating a portion of the training data
    to be the validation data.
  prefs: []
  type: TYPE_NORMAL
- en: The whole point of having separate sets, namely a validation or test dataset,
    is to measure the performance on this data, knowing that our model was not trained
    with it. A model's ability to perform equally, or close to equally, well on unseen
    validation or test data is known as **generalization.**
  prefs: []
  type: TYPE_NORMAL
- en: Generalization is the ultimate goal of most learning algorithms; all of us professionals
    and practitioners of deep learning dream of achieving great generalization in
    all of our models. Similarly, our greatest nightmare is **overfitting**.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting is the opposite of generalization. It occurs when our models perform
    extremely well on the training data but when presented with validation or test
    data, the performance decreases significantly. This indicates that our model almost
    memorized the intricacies of the training data and missed the big picture generalities
    of the sample space that lead to good models.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this and further chapters, we will follow these rules with respect to data
    splits:'
  prefs: []
  type: TYPE_NORMAL
- en: If we are given test data (with labels), we will train on the training set and
    report the performance based on the test set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If we are not given test data (or if we have test data with no labels), we will
    split the training set, creating a validation set that we can report performance
    on using a cross-validation strategy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's discuss each scenario separately.
  prefs: []
  type: TYPE_NORMAL
- en: If we have test data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To begin this discussion, let''s say that we have a deep learning model with
    a set of hyper parameters, ![](img/fb401be2-c787-431a-ac8a-47d74004dcb3.png),
    which could be the weights of the model, the number of neurons, layers, the learning
    rate, the drop-out rate, and so on. Then, we can say that a model, ![](img/d790c8ca-2267-4437-b9df-2eb6ca27611c.png),
    (with parameters ![](img/cb15d854-ebcd-426f-9120-961cdcc51165.png)) that is trained
    with training data, ![](img/dff30c72-c9b3-4a6f-a20a-e3a3017c59e7.png), can have
    a training accuracy as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43cdd752-074f-4433-a75a-6c958eb9dbda.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is the training accuracy of a trained model on the training data. Consequently,
    if we are given labeled test data, ![](img/b2cc358b-36e4-4460-a96e-6f6234315997.png), with *M*
    data points, we can simply estimate the **test accuracy** by calculating the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b3b5f613-b3a4-40fb-9e26-a7097dd0ea7b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'One important property when reporting test accuracy usually holds true in most
    cases—all test accuracy is usually less than the training accuracy plus some noise
    caused by a poor selection of parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1e080952-7eb2-4e89-9893-848d25b5e9ae.png)'
  prefs: []
  type: TYPE_IMG
- en: This usually implies that if your test accuracy is significantly larger than
    your training accuracy, then there could be something wrong with the trained model.
    Also, we could consider the possibility that the test data is drastically different
    from the training data in terms of its statistical distribution and the multidimensional
    manifold that describes it.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, reporting performance on the test set is very important if we have
    test data that was properly chosen. Nonetheless, it would be completely normal
    for the performance to be less than it was in training. However, if it is significantly
    lower, there could be a problem of overfitting and if it is significantly greater,
    then there could be a problem with the code, the model, and even the choice of
    test data. The problem of overfitting can be solved by choosing better parameters, ![](img/cee96f36-9658-4621-9c99-9eadf7d7d500.png),
    or by choosing a different model, ![](img/d01418e4-a07a-41a4-83dd-3c65bde9d857.png),
    which is discussed in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's briefly discuss a case where we don't have test data or we have test
    data with no labels.
  prefs: []
  type: TYPE_NORMAL
- en: No test data? No problem – cross-validate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Cross-validation is a technique that allows us to split the training data, ![](img/dff30c72-c9b3-4a6f-a20a-e3a3017c59e7.png), into
    smaller groups for training purposes. The most important point to remember is
    that the splits are ideally made of an equal number of samples overall and that
    we want to rotate the choice of groups for training and validation sets.
  prefs: []
  type: TYPE_NORMAL
- en: Let's discuss the famous cross-validation strategy known as ***k*-fold cross-validation** (Kohavi,
    R. 1995). The idea here is to divide the training data into *k* groups, which
    are (ideally) equally large, then select *k*-1 groups for training the model and
    measure the performance of the group that was left out. Then, change the groups
    each time until all the groups have been selected for testing.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous sections, we discussed measuring performance using the standard
    accuracy, ACC, but we could use any performance metric. To show this, we will
    now calculate the MSE. This is how the *k*-fold cross-validation algorithm will
    look:'
  prefs: []
  type: TYPE_NORMAL
- en: Input the dataset, ![](img/e304304b-b22e-44a6-bb16-5f682710918c.png), the model, ![](img/17eb0626-a42f-4af0-a282-6e8b4763660f.png),
    the parameters, ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png), and the number
    of folds, ![](img/1d85f939-85aa-4c66-8a53-c5451f35ba70.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Divide the set of indices, ![](img/7976dcab-c59d-4f7d-adc1-46aa541730e0.png), into ![](img/7853dbb6-4144-4bdc-b6e9-f536bad0862d.png)groups
    (ideally equal in size), ![](img/1dab9af3-28c9-4288-b612-2871cba7b80e.png), such
    that ![](img/09103fa9-8eb1-4d3c-bda9-34629dee8f42.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'For each case of ![](img/3ae133ea-6268-4366-b1b1-8a00537970d7.png), do the
    following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the indices for training as [![](img/84dc0f45-f878-48b2-ade1-11e12122e33d.png)] and
    form the training set, [![](img/5bedbb94-b2d2-43fa-b526-2eda18f88551.png)].
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Select the indices for validation as ![](img/5e3df28e-3838-4ca1-b407-8f00962e9d85.png) and
    form the validation set, ![](img/107f4c15-a2f6-4f29-8a47-2393165addff.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Train the model with a choice of parameters over the training set: ![](img/9b069e01-510c-4bad-b188-0df9797af76a.png).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Compute the error of the model, [![](img/40685b40-6076-4fba-b045-11a6d39face0.png)], on
    the validation set : [![](img/cb27f873-4b41-48ee-b205-215589f73273.png)]'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Return ![](img/c8557d04-4efd-4275-845a-ca9183a4be72.png) for all cases of ![](img/355f6774-f073-4a73-9a57-c7df89a2cbbe.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With this, we can calculate the cross-validation error (MSE) given by the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f2e94af6-ec29-4bc0-9c9c-8cb59ba9ed2e.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can also calculate its corresponding standard deviation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/bba147e0-2d91-4e2c-bcb8-fb4db4c08f9a.png).'
  prefs: []
  type: TYPE_NORMAL
- en: It is usually a good idea to look at the standard deviation of our performance
    metric—regardless of the choice—since it gives an idea of how consistent our performance
    on the validation sets is. Ideally, we would like to have a cross-validated MSE
    of `0`, ![](img/5f2ba80e-d9bd-4d27-9965-a87fa2cb272a.png), and a standard deviation
    of `1`, ![](img/51bcebac-4507-44fd-bae4-111feca2dbd9.png).
  prefs: []
  type: TYPE_NORMAL
- en: To explain this, we can use the regression example of the sample data contaminated
    by white noise, shown in *Figure 4.3* and *Figure 4.4*. To keep things simple
    for this example, we will use a total of 100 samples, *N*=100, and we will use
    3 folds. We will use scikit-learn's `KFold` class inside the `model_selection`
    super class and we will obtain the cross-validated MSE and its standard deviation.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do this, we can use the following code and include other metrics as well:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of this code will return something as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'These results are cross-validated and give a clearer picture of the generalization
    abilities of the model. For comparison purposes, see the results shown in *Figure
    4.9*. You will notice that the results are very consistent between the performance
    measured before using the whole set in *Figure 4.9* and now, using only about
    66% of the data (since we split it into three groups) for training and about 33%
    for testing, as shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/352f369c-e46e-403f-80db-daee8d4fc3d2.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 - Cross-validated performance metrics with standard deviation in
    parenthesis
  prefs: []
  type: TYPE_NORMAL
- en: The previous graph shows the linear regression solution found for every split
    of the data as well as the true original function; you can see that the solutions
    found are fairly close to the true model, yielding a good performance, as measured
    by ***R*²**, **MAE**, and **MSE**.
  prefs: []
  type: TYPE_NORMAL
- en: '**Exercise**'
  prefs: []
  type: TYPE_NORMAL
- en: Go ahead and change the number of folds, progressively increasing it, and document
    your observations. What happens to the cross-validated performances? Do they stay
    the same, increase, or decrease? What happens to the standard deviations of the
    cross-validated performances? Do they stay the same, increase, or decrease? What
    do you think this means?
  prefs: []
  type: TYPE_NORMAL
- en: Usually, cross-validation is used on a dataset, ![](img/e304304b-b22e-44a6-bb16-5f682710918c.png),
    with a model, ![](img/17eb0626-a42f-4af0-a282-6e8b4763660f.png), trained on parameters, ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png).
    However, one of the greatest challenges in learning algorithms is finding the
    best set of parameters, ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png), that
    can yield the best (test or cross-validated) performance. Many machine learning
    scientists believe choosing the set of parameters can be **automated** with some
    algorithms and others believe this is an **art** (Bergstra, J. S., et.al. 2011).
  prefs: []
  type: TYPE_NORMAL
- en: The art behind learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: For those of us who have spent decades studying machine learning, experience
    informs the way we choose parameters for our learning algorithms. But for those
    who are new to it, this is a skill that needs to be developed and this skill comes
    after learning how learning algorithms work. Once you have finished this book,
    I believe you will have enough knowledge to choose your parameters wisely. In
    the meantime, we can discuss some ideas for finding parameters automatically using
    standard and novel algorithms here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we go any further, we need to make a distinction at this point and define
    two major sets of parameters that are important in learning algorithms. These
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Model parameters:** These are parameters that represent the solution that
    the model represents. For example, in perceptron and linear regression, this would
    be vector ![](img/5c1ca364-6e24-407a-8b15-dd412d3dda71.png)and scalar ![](img/f8b8288d-5510-4840-9753-7b375f040e5a.png),
    while for a deep neural network, this would be a matrix of weights, ![](img/74c8f74e-3b92-4f54-b1c7-85e2dcc4318e.png), and
    a vector of biases, ![](img/9fe4b0b9-c94e-4df7-bb9b-9efa507ee211.png). For a convolutional
    network, this would be filter sets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameters:** These are parameters needed by the model to guide the
    learning process to search for a solution (model parameters) and are usually represented
    as ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png). For example, in the PLA,
    a hyperparameter would be the maximum number of iterations; in a deep neural network,
    it would be the number of layers, the number of neurons, the activation function
    for the neurons, and the learning rate; and for a **convolutional neural network**
    (**CNN**), it would be the number of filters, the size of filters, the stride,
    the pooling size, and so on.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Put in other words, the model parameters are determined, in part, by the choice
    of hyperparameters. Usually, unless there is a numerical anomaly, all learning
    algorithms will consistently find solutions (model parameters) for the same set
    of hyperparameters. So, one of the main tasks when learning is finding the best
    set of hyperparameters that will give us the best solutions.
  prefs: []
  type: TYPE_NORMAL
- en: 'To observe the effects of altering the hyperparameters of a model, let''s once
    more consider the four-class classification problem of the seasons, shown earlier
    in *Figure 4.7*. We will assume that we are using a fully connected network, such
    as the one described in [*Chapter 1*](e3181710-1bb7-4069-825a-a235355bc116.xhtml),
    *Introduction to Machine Learning*, and the hyperparameter we want to determine
    is the best number of layers. Just for didactic purposes, let''s say that the
    number of neurons in each layer will increase exponentially in each layer, as
    shown:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Layer** | **Neurons in each layer** |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | (8) |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | (16, 8) |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | (32, 16, 8) |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | (64, 32, 16, 8) |'
  prefs: []
  type: TYPE_TB
- en: '| 5 | (128, 64, 32, 16, 8) |'
  prefs: []
  type: TYPE_TB
- en: '| 6 | (256, 128, 64, 32, 16, 8) |'
  prefs: []
  type: TYPE_TB
- en: '| 7 | (512, 256, 128, 64, 32, 16, 8) |'
  prefs: []
  type: TYPE_TB
- en: In the previous configuration, the first number in the brackets corresponds
    to the number of neurons closest to the input layer, while the last number in
    the brackets corresponds to the number of neurons closest to the output layer
    (which consists of 4 neurons, one per class).
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the number of layers represents ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png),
    in this example. If we loop through each configuration and determine the cross-validated
    BER, we can determine which architecture yields the best performance; that is,
    we are optimizing ![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png) for performance.
    The results obtained will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Layers – [![](img/37ac0424-1c5a-4d6a-9ce1-5e7a1eaff343.png)]** | **1**
    | **2** | **3** | **4** | **5** | **6** | **7** |'
  prefs: []
  type: TYPE_TB
- en: '| **BER** | 0.275 | 0.104 | 0.100 | 0.096 | 0.067 | 0.079 | 0.088 |'
  prefs: []
  type: TYPE_TB
- en: '| **Standard deviation** | 0.22 | 0.10 | 0.08 | 0.10 | 0.05 | 0.04 | 0.08 |'
  prefs: []
  type: TYPE_TB
- en: 'From the results, we can easily determine that the best architecture is one
    with five layers since it has the lowest BER and the second smallest standard
    deviation. We could, indeed, gather all the data at each split for each configuration
    and produce the box plot shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3c159f1a-3d99-48ee-8517-ac6a33da553e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 - A box plot of the cross-validated data optimizing the number of
    layers
  prefs: []
  type: TYPE_NORMAL
- en: This box plot illustrates a couple of important points. First, that there is
    a clear tendency of the model to reduce the BER as the number of layers increases
    up to `5`, then increases after that. This is very common in machine learning
    and it is known as the **overfitting curve**, which is usually a *u *shape (or
    *n *shape, for performance metrics that are better on higher values). The lowest
    point, in this case, would indicate the best set of hyperparameters (at `5`);
    anything to the left of that represents **underfitting** and anything to the right
    represents **overfitting**. The second thing that the box plot shows is that even
    if several models have a similar BER, we will choose the one that shows less variability
    and most consistency.
  prefs: []
  type: TYPE_NORMAL
- en: 'To illustrate the differences between underfitting, good fitting, and overfitting,
    we will show the decision boundaries produced by the worst underfit, the best
    fit, and the worst overfit. In this case, the worst underfit is one layer, the
    best fit is five layers, and the worst overfit is seven layers. Their respective
    decision boundaries are shown in *Figure 4.12*, *Figure 4.13*, and *Figure 4.14*, respectively:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7e730ac9-4d3a-4868-9313-a783b113408f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 - Classification boundaries for a one-hidden-layer network that
    is underfitting
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding graph, we can see that the underfit is clear since there are
    decision boundaries that prevent many datapoints from being classified correctly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5e2f7982-3a60-4d8d-941a-0dc2d14e0b42.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 - Classification boundaries for a five-hidden-layer network that
    has a relatively good fit
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, the previous graph shows the decision boundaries, but compared to
    *Figure 4.12*, these boundaries seem to provide a nicer separation of the data
    points for the different groups—a good fit:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/60fedb99-d5fc-4dfb-b89c-53f1318e7d43.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 - Classification boundaries for a seven-hidden-layer network that
    is overfitting
  prefs: []
  type: TYPE_NORMAL
- en: If you look closely, *Figure 4.12* shows that some regions are designated very
    poorly, while in *figure 4.14*, the network architecture is trying *too hard* to
    classify all the examples perfectly, to the point where the outlier in the *Fall* class
    (the yellow points) that goes into the region of the *Winter* class (the blue
    points) has its own little region, which may have negative effects down the road.
    The classes in *Figure 4.13* seem to be robust against some of the outliers and
    have well-defined regions, for the most part.
  prefs: []
  type: TYPE_NORMAL
- en: As we progress through this book, we will deal with more complex sets of hyperparameters.
    Here we just dealt with one, but the theory is the same. This method of looking
    at the best set of hyperparameters is known as an exhaustive search. However,
    there are other ways of looking at parameters, such as performing a **grid search.**
  prefs: []
  type: TYPE_NORMAL
- en: Suppose that you do not have a fixed way of knowing the number of neurons in
    each layer (as opposed to the earlier example); you only know that you would like
    to have something between `4` and `1024` neurons and something between `1` and
    `100` layers to allow deep or shallow models. In that case, you cannot do an exhaustive
    search; it would take too much time! Here, grid searchis used as a solution that
    will sample the search space in—usually—equally-spaced regions.
  prefs: []
  type: TYPE_NORMAL
- en: For example, grid search can look at a number of neurons in the `[4, 1024]` range on
    10 equally spaced values—`4`, `117`, `230`, `344`, `457`, `570`, `684`, `797`,
    `910`, and `1024`—and the number of layers that is in the `[1,100]` range on 10
    equally spaced values—`1`, `12`, `23`, `34`, `45`, `56`, `67`, `78`, `89`, and
    `100`. Rather than looking at 1020*100=102,000 searches, it will look at 10*10=100,
    instead.
  prefs: []
  type: TYPE_NORMAL
- en: In `sklearn`, there is a class, `GridSearchCV`, that can return the best models
    and hyperparameters in cross-validation; it is part of the `model_selection` super
    class. The same class group has another class, called `RandomizedSearchCV`, which
    contains a methodology based on randomly searching the space. This is called **random
    search.**
  prefs: []
  type: TYPE_NORMAL
- en: In **random search**, the premise is that it will look within the `[4, 1024]` range and
    the `[1,100]` range for neurons and layers, respectively, by randomly drawing
    numbers uniformly until it reaches a maximum limit of total iterations.
  prefs: []
  type: TYPE_NORMAL
- en: Typically, if you know the range and distribution of the parameter search space,
    try a **grid search** approach on the space you believe is likely to have a better
    cross-validated performance. However, if you know very little or nothing about
    the parameter search space, use a **random search** approach. In practice, both
    of these methods work well.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are other, more sophisticated methods that work well but whose implementation
    in Python is not yet standard, so we will not cover them in detail here. However,
    you should know about them:'
  prefs: []
  type: TYPE_NORMAL
- en: Bayesian hyperparameter optimization (Feurer, M., et.al. 2015)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evolution theory-based hyperparameter optimization (Loshchilov, I., et.al. 2016)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Gradient-based hyperparameter optimization (Maclaurin, D., et.al. 2015)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Least squares-based hyperparameter optimization (Rivas-Perea, P., et.al. 2014)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical implications of training deep learning algorithms
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a few things that can be said about the ethical implications of training
    deep learning models. There is potential harm whenever you are handling data that
    represents human perceptions. But also, data about humans and human interaction
    has to be rigorously protected and examined carefully before creating a model
    that will generalize based on such data. Such thoughts are organized in the following
    sections.
  prefs: []
  type: TYPE_NORMAL
- en: Reporting using the appropriate performance measures
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Avoid faking good performance by picking the one performance metric that makes
    your model look good. It is not uncommon to read articles and reports of multi-class
    classification models that are trained over clear, class-imbalanced datasets but
    report the standard accuracy. Most likely, these models will report a high standard
    of accuracy since the models will be biased toward the over-sampled class and
    against the under-sampled groups. So, these types of models must report the balanced
    accuracy or the balanced error rate.
  prefs: []
  type: TYPE_NORMAL
- en: Similarly, for other types of classification and regression problems, you must
    report the appropriate performance metric. When in doubt, report as many performance
    metrics as you can. Nobody has ever complained about someone reporting model performance
    using too many metrics.
  prefs: []
  type: TYPE_NORMAL
- en: The consequences of not reporting the appropriate metrics go from having biased
    models that go undetected and are deployed into production systems with disastrous
    consequences to having misleading information that can be detrimental to our understanding
    of specific problems and how models perform. We must recall that what we do may
    affect others and we need to be vigilant.
  prefs: []
  type: TYPE_NORMAL
- en: Being careful with outliers and verifying them
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Outliers are usually seen as bad things to work around during the learning process
    and I agree. Models should be robust against outliers, unless they are not really
    outliers. If we have some data and we don't know anything about it, it is a safe
    assumption to interpret outliers as anomalies.
  prefs: []
  type: TYPE_NORMAL
- en: However, if we know anything about the data (because we collected it, were given
    all the information about it, or know the sensors that produced it), then we can
    verify that outliers are really outliers. We must verify that they were the product
    of human error when typing data or produced by a faulty sensor, data conversion
    error, or some other artifact because if an outlier is not the product of any
    of these reasons, there is no reasonable basis for us to assume that it is an
    outlier. In fact, data like this gives us important information about situations
    that may not occur frequently but will eventually happen again and the model needs
    to respond properly.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the data shown in the following figure. If we arbitrarily decide to
    ignore outliers without verification (such as in the top diagram), it may be that
    they are in fact not really outliers and the model will create a narrow decision
    space that ignores the outliers. The consequence, in this example, is that one
    point will be incorrectly classified as belonging to another group, while another
    point might be left out of the majority group:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ad524a4e-52df-40bc-a71f-6c80e34dd76c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 - Differences in the learned space of my models. The top diagram
    shows the ignoring outliers outcome. The bottom diagram shows the including outliers
    outcome
  prefs: []
  type: TYPE_NORMAL
- en: However, if we verify the data and discover that the outliers are completely
    valid input, the models might learn a better decision space that could potentially
    include the outliers. Nonetheless, this can yield a secondary problem where a
    point is classified as belonging to two different groups with different degrees
    of membership. While this is a problem, it is a much smaller risk than incorrectly
    classifying something. It is better to have, say, 60% certainty that a point belongs
    to one class and 40% certainty that it belongs to the other class, rather than
    classifying it incorrectly with 100% certainty.
  prefs: []
  type: TYPE_NORMAL
- en: If you think about it, models that were built by ignoring outliers and then deployed
    into government systems can cause discrimination problems. They may show bias
    against minority or protected population groups. If deployed into incoming school
    student selection, it could lead to the rejection of exceptional students. If
    deployed into DNA classification systems, it could incorrectly ignore the similarity
    of two very close DNA groups. Therefore, always verify outliers if you can.
  prefs: []
  type: TYPE_NORMAL
- en: Weight classes with undersampled groups
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have a class imbalance, as in *Figure 4.15*, I recommend you try to
    balance the classes by getting more data rather than reducing it. If this is not
    an option, look into algorithms that allow you to weight some classes differently,
    so as to even out the imbalance. Here are a couple of the most common techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: On small datasets, use `sklearn` and the `class_weight` option. When training
    a model, it penalizes mistakes based on the provided weight for that class. There
    are a couple of automatic alternatives that you can look into that will also help,
    such as `class_weight="auto"` and `class_weight="balanced"`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: On large datasets where batch training is used, use Keras and the `BalancedBatchGenerator`
    class. This will prepare a selection of samples (batches) that is consistently
    balanced each time, thereby guiding the learning algorithm to consider all groups
    equally. The class is part of `imblearn.keras`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You should try to use these strategies every time you want to have a model that
    is not biased toward a majority group. The ethical implications of this are similar
    to the previous points already mentioned. But above all, we must protect life
    and treat people with respect; all people have an equal, infinite worth.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this basic-level chapter, we discussed the basics of learning algorithms
    and their purpose. Then, we studied the most basic way of measuring success and
    failure through performance analysis using accuracies, errors, and other statistical
    devices. We also studied the problem of overfitting and the super important concept
    of generalization, which is its counterpart. Then, we discussed the art behind
    the proper selection of hyperparameters and strategies for their automated search.
  prefs: []
  type: TYPE_NORMAL
- en: After reading this chapter, you are now able to explain the technical differences
    between classification and regression and how to calculate different performance
    metrics, such as ACC, BER, MSE, and others, as appropriate for different tasks.
    Now, you are capable of detecting overfitting by using train, validation, and
    test datasets under cross-validation strategies, you can experiment with and observe
    the effects of altering the hyperparameters of a learning model. You are also
    ready to think critically about the precautions and devices necessary to prevent
    human harm caused by deep learning algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter is [*Chapter 5*](4e4b45a6-1924-4918-b2cd-81f0448fb213.xhtml), *Training
    a Single Neuron,* which revises and expands the concept of a neuron, which was
    introduced in [*Chapter 1*](e3181710-1bb7-4069-825a-a235355bc116.xhtml), *Introduction
    to Machine Learning*, and shows its implementation in Python using different datasets
    to analyze the potential effects of different data; that is, linear and non-linearly
    separable data. However, before we go there, please try to quiz yourself using
    the following questions.
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**When you did the exercise on cross-validation, what happened to the standard
    deviation and what does that mean?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The standard deviation stabilizes and reduces on more folds. This means that
    the performance measurements are more reliable; it is an accurate measure of generalization
    or overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is the difference between hyperparameters and model parameters?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Model parameters are numerical solutions to a learning algorithm; hyperparameters
    are what the model needs to know in order to find a solution effectively.
  prefs: []
  type: TYPE_NORMAL
- en: '**Is a grid search faster than a randomized search for hyperparameters?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: It depends. If the choice of hyperparameters affects the computational complexity
    of the learning algorithm, then both could behave differently. However, in similar
    search spaces and in the amortized case, both should finish at about the same
    time.
  prefs: []
  type: TYPE_NORMAL
- en: '**Can I use a regression-based learning algorithm for a classification problem?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes, as long as the labels, categories, or groups are mapped to a number in
    the set of real numbers.
  prefs: []
  type: TYPE_NORMAL
- en: '**Can I use a classification-based learning algorithm for a regression problem?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: No.
  prefs: []
  type: TYPE_NORMAL
- en: '**Is the concept of a loss function the same as an error metric?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Yes and no. Yes, in the sense that a loss function will measure performance;
    however, the performance may not necessarily be with respect to the accuracy of
    classifying or regressing the data; it may be with respect to something else,
    such as the quality of groups or distances in information-theoretic spaces. For
    example, linear regression is based on the MSE algorithm as a loss function to
    minimize, while the loss function of the K-means algorithm is the sum of the squared
    distances of the data to their means, which it aims to minimize, but this does
    not necessarily mean it is an error. In the latter case, it is arguably meant
    as a cluster quality measure.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Lorena, A. C., De Carvalho, A. C., & Gama, J. M. (2008), A review on the combination
    of binary classifiers in multiclass problems, *Artificial Intelligence Review*,
    30(1-4), 19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Hochreiter, S., Younger, A. S., & Conwell, P. R. (2001, August), Learning to
    learn using gradient descent, in *International Conference on Artificial Neural
    Networks* (pp. 87-94), Springer: Berlin, Heidelberg'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ruder, S. (2016), An overview of gradient descent optimization algorithms, *arXiv*
    *preprint* arXiv:1609.04747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Tan, X., Zhang, Y., Tang, S., Shao, J., Wu, F., & Zhuang, Y. (2012, October),
    Logistic tensor regression for classification, in *International Conference on
    Intelligent Science and Intelligent Data Engineering* (pp. 573-581), Springer:
    Berlin, Heidelberg'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Krejn, S. G. E. (1982), *Linear Equations in Banach Spaces,* Birkhäuser: Boston'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Golub, G., & Kahan, W. (1965), Calculating the singular values and pseudo-inverse
    of a matrix, *Journal of the Society for Industrial and Applied Mathematics*,
    Series B: Numerical Analysis, 2(2), (pp. 205-224)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kohavi, R. (1995, August), A study of cross-validation and bootstrap for accuracy
    estimation and model selection, in *IJCAI*, 14(2), (pp. 1137-1145)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bergstra, J. S., Bardenet, R., Bengio, Y., & Kégl, B. (2011), Algorithms for
    hyper-parameter optimization, in *Advances in Neural Information Processing Systems,*
    (pp. 2546-2554)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Feurer, M., Springenberg, J. T., & Hutter, F. (2015, February), Initializing
    Bayesian hyperparameter optimization via meta-learning, in *Twenty-Ninth AAAI
    Conference on Artificial Intelligence*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loshchilov, I., & Hutter, F. (2016), CMA-ES for hyperparameter optimization
    of deep neural networks, *arXiv preprint* arXiv:1604.07269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Maclaurin, D., Duvenaud, D., & Adams, R. (2015, June), Gradient-based hyperparameter
    optimization through reversible learning, in *International Conference on Machine
    Learning* (pp. 2113-2122)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rivas-Perea, P., Cota-Ruiz, J., & Rosiles, J. G. (2014), A nonlinear least squares
    quasi-Newton strategy for LP-SVR hyper-parameters selection, *International Journal
    of Machine Learning and Cybernetics*, 5(4), (pp.579-597)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
