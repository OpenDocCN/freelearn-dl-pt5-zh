["```py\n\nimport tensorflow as tf \nfrom tensorflow.keras import datasets, layers, models \nimport numpy as np \nimport matplotlib.pyplot as plt\n```", "```py\n\n(train_images, train_labels), ( \ntest_images, \ntest_labels, \n) = datasets.mnist.load_data() \ntrain_images, test_images = train_images / 255.0, test_images / 255.0\n```", "```py\n\ndef get_model(): \nmodel = models.Sequential() \nmodel.add( \nlayers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)) \n) \nmodel.add(layers.MaxPooling2D((2, 2))) \nmodel.add(layers.Conv2D(64, (3, 3), activation=\"relu\")) \nmodel.add(layers.MaxPooling2D((2, 2))) \nmodel.add(layers.Conv2D(64, (3, 3), activation=\"relu\")) \nmodel.add(layers.Flatten()) \nmodel.add(layers.Dense(64, activation=\"relu\")) \nmodel.add(layers.Dense(10)) \nreturn model \n\nmodel = get_model()\n```", "```py\n\ndef fit_model(model): \nmodel.compile( \noptimizer=\"adam\", \nloss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), \nmetrics=[\"accuracy\"], \n) \n\nmodel.fit( \ntrain_images, \ntrain_labels, \nepochs=5, \nvalidation_data=(test_images, test_labels), \n) \nreturn model \n\nmodel = fit_model(model)\n```", "```py\n\ndef remove_signal(img: np.ndarray, num_lines: int) -*>* np.ndarray: \nimg = img.copy() \nimg[:num_lines] = 0 \n   return img\n```", "```py\n\nimgs = [] \nfor i in range(28): \nimg_perturbed = remove_signal(img, i) \nif np.array_equal(img, img_perturbed): \ncontinue \nimgs.append(img_perturbed) \nif img_perturbed.sum() == 0: \n     break\n```", "```py\n\nsoftmax_predictions = tf.nn.softmax(model(np.expand_dims(imgs, -1)), axis=1)\n```", "```py\n\nplt.figure(figsize=(10, 10)) \nbbox_dict = dict( \nfill=True, facecolor=\"white\", alpha=0.5, edgecolor=\"white\", linewidth=0 \n) \nfor i in range(len(imgs)): \nplt.subplot(5, 5, i + 1) \nplt.xticks([]) \nplt.yticks([]) \nplt.grid(False) \nplt.imshow(imgs[i], cmap=\"gray\") \nprediction = softmax_predictions[i].numpy().max() \nlabel = np.argmax(softmax_predictions[i]) \nplt.xlabel(f\"{label} - {prediction:.2%}\") \nplt.text(0, 3, f\" {i+1}\", bbox=bbox_dict) \nplt.show()\n```", "```py\n\ndef get_dropout_model(): \nmodel = models.Sequential() \nmodel.add( \nlayers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=(28, 28, 1)) \n) \nmodel.add(layers.Dropout(0.2)) \nmodel.add(layers.MaxPooling2D((2, 2))) \nmodel.add(layers.Conv2D(64, (3, 3), activation=\"relu\")) \nmodel.add(layers.MaxPooling2D((2, 2))) \nmodel.add(layers.Dropout(0.5)) \nmodel.add(layers.Conv2D(64, (3, 3), activation=\"relu\")) \nmodel.add(layers.Dropout(0.5)) \nmodel.add(layers.Flatten()) \nmodel.add(layers.Dense(64, activation=\"relu\")) \nmodel.add(layers.Dropout(0.5)) \nmodel.add(layers.Dense(10)) \n    return model\n```", "```py\n\ndropout_model = get_dropout_model() \ndropout_model = fit_model(dropout_model)\n```", "```py\n\nPredictions = np.array( \n[ \ntf.nn.softmax(dropout_model(imgs_np, training=True), axis=1) \nfor _ in range(100) \n] \n) \nPredictions_mean = np.mean(predictions, axis=0) \nplot_predictions(predictions_mean)\n```", "```py\n\nlogits = model.predict(images) \nlogits_scaled = logits / temperature \nsoftmax = tf.nn.softmax(logits, axis=1)\n```", "```py\n\nmean = np.mean(features_of_class, axis=0) \ncovariance = np.cov(features_of_class.T)\n```", "```py\n\ncovariance_inverse = np.linalg.pinv(covariance) \nx_minus_mu = features_of_class - mean \nmahalanobis = np.dot(x_minus_mu, covariance_inverse).dot(x_minus_mu.T) \nmahalanobis = np.sqrt(mahalanobis).diagonal()\n```", "```py\n\nfrom PIL import Image \nimport numpy as np \nimport imgaug.augmenters.imgcorruptlike as icl \n\nimage = np.asarray(Image.open(\"./kitty.png\").convert(\"RGB\")) \ncorruption_function = icl.ShotNoise \nimage_noise_level_01 = corruption_function(severity=1, seed=0)(image=image) \nimage_noise_level_05 = corruption_function(severity=5, seed=0)(image=image)\n```", "```py\n\nimport cv2 \nimport imgaug.augmenters as iaa \nimport imgaug.augmenters.imgcorruptlike as icl \nimport matplotlib.pyplot as plt \nimport numpy as np \nimport pandas as pd \nimport seaborn as sns \nimport tensorflow as tf \nimport tensorflow_probability as tfp \nfrom sklearn.metrics import accuracy_score\n```", "```py\n\ncifar = tf.keras.datasets.cifar10 \n(train_images, train_labels), (test_images, test_labels) = cifar.load_data() \n\nCLASS_NAMES = [ \n\"airplane\",\"automobile\", \"bird\", \"cat\", \"deer\", \n\"dog\", \"frog\", \"horse\", \"ship\", \"truck\" \n] \n\nNUM_TRAIN_EXAMPLES = train_images.shape[0]\n```", "```py\n\ndef cnn_building_block(num_filters): \nreturn tf.keras.Sequential( \n[ \ntf.keras.layers.Conv2D( \nfilters=num_filters, kernel_size=(3, 3), activation=\"relu\" \n), \ntf.keras.layers.MaxPool2D(strides=2), \n] \n    )\n```", "```py\n\ndef build_and_compile_model(): \nmodel = tf.keras.Sequential( \n[ \ntf.keras.layers.Rescaling(1.0 / 255, input_shape=(32, 32, 3)), \ncnn_building_block(16), \ncnn_building_block(32), \ncnn_building_block(64), \ntf.keras.layers.MaxPool2D(strides=2), \ntf.keras.layers.Flatten(), \ntf.keras.layers.Dense(64, activation=\"relu\"), \ntf.keras.layers.Dense(10, activation=\"softmax\"), \n] \n) \nmodel.compile( \noptimizer=\"adam\", \nloss=\"sparse_categorical_crossentropy\", \nmetrics=[\"accuracy\"], \n) \n    return model\n```", "```py\n\ndef cnn_building_block_bbb(num_filters, kl_divergence_function): \nreturn tf.keras.Sequential( \n[ \ntfp.layers.Convolution2DReparameterization( \nnum_filters, \nkernel_size=(3, 3), \nkernel_divergence_fn=kl_divergence_function, \nactivation=tf.nn.relu, \n), \ntf.keras.layers.MaxPool2D(strides=2), \n] \n    )\n```", "```py\n\ndef build_and_compile_model_bbb(): \n\nkl_divergence_function = lambda q, p, _: tfp.distributions.kl_divergence( \nq, p \n) / tf.cast(NUM_TRAIN_EXAMPLES, dtype=tf.float32) \n\nmodel = tf.keras.models.Sequential( \n[ \ntf.keras.layers.Rescaling(1.0 / 255, input_shape=(32, 32, 3)), \ncnn_building_block_bbb(16, kl_divergence_function), \ncnn_building_block_bbb(32, kl_divergence_function), \ncnn_building_block_bbb(64, kl_divergence_function), \ntf.keras.layers.Flatten(), \ntfp.layers.DenseReparameterization( \n64, \nkernel_divergence_fn=kl_divergence_function, \nactivation=tf.nn.relu, \n), \ntfp.layers.DenseReparameterization( \n10, \nkernel_divergence_fn=kl_divergence_function, \nactivation=tf.nn.softmax, \n), \n] \n) \n\nmodel.compile( \noptimizer=\"adam\", \nloss=\"sparse_categorical_crossentropy\", \nmetrics=[\"accuracy\"], \nexperimental_run_tf_function=False, \n) \n\nmodel.build(input_shape=[None, 32, 32, 3]) \n    return model\n```", "```py\n\nvanilla_model = build_and_compile_model() \nvanilla_model.fit(train_images, train_labels, epochs=10)\n```", "```py\n\nNUM_ENSEMBLE_MEMBERS = 5 \nensemble_model = [] \nfor ind in range(NUM_ENSEMBLE_MEMBERS): \nmember = build_and_compile_model() \nprint(f\"Train model {ind:02}\") \nmember.fit(train_images, train_labels, epochs=10) \n    ensemble_model.append(member)\n```", "```py\n\nbbb_model = build_and_compile_model_bbb() \nbbb_model.fit(train_images, train_labels, epochs=15)\n```", "```py\n\nNUM_SUBSET = 1000 \ntest_images_subset = test_images[:NUM_SUBSET] \ntest_labels_subset = test_labels[:NUM_SUBSET]\n```", "```py\n\ncorruption_functions = [ \nicl.GaussianNoise, \nicl.ShotNoise, \nicl.ImpulseNoise, \nicl.DefocusBlur, \nicl.GlassBlur, \nicl.MotionBlur, \nicl.ZoomBlur, \nicl.Snow, \nicl.Frost, \nicl.Fog, \nicl.Brightness, \nicl.Contrast, \nicl.ElasticTransform, \nicl.Pixelate, \nicl.JpegComdivssion, \n] \nNUM_TYPES = len(corruption_functions) \nNUM_LEVELS = 5\n```", "```py\n\ncorrupted_images = [] \n# loop over different corruption severities \nfor corruption_severity in range(1, NUM_LEVELS+1): \ncorruption_type_batch = [] \n# loop over different corruption types \nfor corruption_type in corruption_functions: \ncorrupted_image_batch = corruption_type( \nseverity=corruption_severity, seed=0 \n)(images=test_images_subset) \ncorruption_type_batch.append(corrupted_image_batch) \ncorruption_type_batch = np.stack(corruption_type_batch, axis=0) \ncorrupted_images.append(corruption_type_batch) \ncorrupted_images = np.stack(corrupted_images, axis=0)\n```", "```py\n\ncorrupted_images = corrupted_images.reshape((-1, 32, 32, 3))\n```", "```py\n\n# Get predictions on original images \nvanilla_predictions = vanilla_model.predict(test_images_subset) \n# Get predictions on corrupted images \nvanilla_predictions_on_corrupted = vanilla_model.predict(corrupted_images) \nvanilla_predictions_on_corrupted = vanilla_predictions_on_corrupted.reshape( \n(NUM_LEVELS, NUM_TYPES, NUM_SUBSET, -1) \n)\n```", "```py\n\ndef get_ensemble_predictions(images, num_inferences): \nensemble_predictions = tf.stack( \n[ \nensemble_model[ensemble_ind].predict(images) \nfor ensemble_ind in range(num_inferences) \n], \naxis=0, \n) \n    return np.mean(ensemble_predictions, axis=0)\n```", "```py\n\n# Get predictions on original images \nensemble_predictions = get_ensemble_predictions( \ntest_images_subset, NUM_ENSEMBLE_MEMBERS \n) \n# Get predictions on corrupted images \nensemble_predictions_on_corrupted = get_ensemble_predictions( \ncorrupted_images, NUM_ENSEMBLE_MEMBERS \n) \nensemble_predictions_on_corrupted = ensemble_predictions_on_corrupted.reshape( \n(NUM_LEVELS, NUM_TYPES, NUM_SUBSET, -1) \n)\n```", "```py\n\ndef get_bbb_predictions(images, num_inferences): \nbbb_predictions = tf.stack( \n[bbb_model.predict(images) for _ in range(num_inferences)], \naxis=0, \n) \n    return np.mean(bbb_predictions, axis=0)\n```", "```py\n\nNUM_INFERENCES_BBB = 20 \n# Get predictions on original images \nbbb_predictions = get_bbb_predictions( \ntest_images_subset, NUM_INFERENCES_BBB \n) \n# Get predictions on corrupted images \nbbb_predictions_on_corrupted = get_bbb_predictions( \ncorrupted_images, NUM_INFERENCES_BBB \n) \nbbb_predictions_on_corrupted = bbb_predictions_on_corrupted.reshape( \n(NUM_LEVELS, NUM_TYPES, NUM_SUBSET, -1) \n)\n```", "```py\n\ndef get_classes_and_scores(model_predictions): \nmodel_predicted_classes = np.argmax(model_predictions, axis=-1) \nmodel_scores = np.max(model_predictions, axis=-1) \n    return model_predicted_classes, model_scores\n```", "```py\n\n# Vanilla model \nvanilla_predicted_classes, vanilla_scores = get_classes_and_scores( \nvanilla_predictions \n) \n( \nvanilla_predicted_classes_on_corrupted, \nvanilla_scores_on_corrupted, \n) = get_classes_and_scores(vanilla_predictions_on_corrupted) \n\n# Ensemble model \n( \nensemble_predicted_classes, \nensemble_scores, \n) = get_classes_and_scores(ensemble_predictions) \n( \nensemble_predicted_classes_on_corrupted, \nensemble_scores_on_corrupted, \n) = get_classes_and_scores(ensemble_predictions_on_corrupted) \n\n# BBB model \n( \nbbb_predicted_classes, \nbbb_scores, \n) = get_classes_and_scores(bbb_predictions) \n( \nbbb_predicted_classes_on_corrupted, \nbbb_scores_on_corrupted, \n) = get_classes_and_scores(bbb_predictions_on_corrupted)\n```", "```py\n\nplot_images = corrupted_images.reshape( \n(NUM_LEVELS, NUM_TYPES, NUM_SUBSET, 32, 32, 3) \n)\n```", "```py\n\n# Index of the selected images \nind_image = 9 \n# Define figure \nfig, axes = plt.subplots(nrows=3, ncols=5, figsize=(16, 10)) \n# Loop over corruption levels \nfor ind_level in range(NUM_LEVELS): \n# Loop over corruption types \nfor ind_type in range(3): \n# Plot slightly upscaled image for easier inspection \nimage = plot_images[ind_level, ind_type, ind_image, ...] \nimage_upscaled = cv2.resize( \nimage, dsize=(150, 150), interpolation=cv2.INTER_CUBIC \n) \naxes[ind_type, ind_level].imshow(image_upscaled) \n# Get score and class predicted by vanilla model \nvanilla_score = vanilla_scores_on_corrupted[ \nind_level, ind_type, ind_image, ... \n] \nvanilla_prediction = vanilla_predicted_classes_on_corrupted[ \nind_level, ind_type, ind_image, ... \n] \n# Get score and class predicted by ensemble model \nensemble_score = ensemble_scores_on_corrupted[ \nind_level, ind_type, ind_image, ... \n] \nensemble_prediction = ensemble_predicted_classes_on_corrupted[ \nind_level, ind_type, ind_image, ... \n] \n# Get score and class predicted by BBB model \nbbb_score = bbb_scores_on_corrupted[ind_level, ind_type, ind_image, ...] \nbbb_prediction = bbb_predicted_classes_on_corrupted[ \nind_level, ind_type, ind_image, ... \n] \n# Plot prediction info in title \ntitle_text = ( \nf\"Vanilla: {vanilla_score:.3f} \" \n+ f\"[{CLASS_NAMES[vanilla_prediction]}] \\n\" \n+ f\"Ensemble: {ensemble_score:.3f} \" \n+ f\"[{CLASS_NAMES[ensemble_prediction]}] \\n\" \n+ f\"BBB: {bbb_score:.3f} \" \n+ f\"[{CLASS_NAMES[bbb_prediction]}]\" \n) \naxes[ind_type, ind_level].set_title(title_text, fontsize=14) \n# Remove axes ticks and labels \naxes[ind_type, ind_level].axis(\"off\") \nfig.tight_layout() \nplt.show()\n```", "```py\n\nvanilla_acc = accuracy_score( \ntest_labels_subset.flatten(), vanilla_predicted_classes \n) \nensemble_acc = accuracy_score( \ntest_labels_subset.flatten(), ensemble_predicted_classes \n) \nbbb_acc = accuracy_score( \ntest_labels_subset.flatten(), bbb_predicted_classes \n)\n```", "```py\n\naccuracies = [ \n{\"model_name\": \"vanilla\", \"type\": 0, \"level\": 0, \"accuracy\": vanilla_acc}, \n{\"model_name\": \"ensemble\", \"type\": 0, \"level\": 0, \"accuracy\": ensemble_acc}, \n{\"model_name\": \"bbb\", \"type\": 0, \"level\": 0, \"accuracy\": bbb_acc}, \n]\n```", "```py\n\nfor ind_type in range(NUM_TYPES): \nfor ind_level in range(NUM_LEVELS): \n# Calculate accuracy for vanilla model \nvanilla_acc_on_corrupted = accuracy_score( \ntest_labels_subset.flatten(), \nvanilla_predicted_classes_on_corrupted[ind_level, ind_type, :], \n) \naccuracies.append( \n{ \n\"model_name\": \"vanilla\", \n\"type\": ind_type + 1, \n\"level\": ind_level + 1, \n\"accuracy\": vanilla_acc_on_corrupted, \n} \n) \n\n# Calculate accuracy for ensemble model \nensemble_acc_on_corrupted = accuracy_score( \ntest_labels_subset.flatten(), \nensemble_predicted_classes_on_corrupted[ind_level, ind_type, :], \n) \naccuracies.append( \n{ \n\"model_name\": \"ensemble\", \n\"type\": ind_type + 1, \n\"level\": ind_level + 1, \n\"accuracy\": ensemble_acc_on_corrupted, \n} \n) \n\n# Calculate accuracy for BBB model \nbbb_acc_on_corrupted = accuracy_score( \ntest_labels_subset.flatten(), \nbbb_predicted_classes_on_corrupted[ind_level, ind_type, :], \n) \naccuracies.append( \n{ \n\"model_name\": \"bbb\", \n\"type\": ind_type + 1, \n\"level\": ind_level + 1, \n\"accuracy\": bbb_acc_on_corrupted, \n} \n        )\n```", "```py\n\ndf = pd.DataFrame(accuracies) \nplt.figure(dpi=100) \nsns.boxplot(data=df, x=\"level\", y=\"accuracy\", hue=\"model_name\") \nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5)) \nplt.tight_layout \nplt.show()\n```", "```py\n\ndef expected_calibration_error( \ndivd_correct, \ndivd_score, \nn_bins=5, \n): \n\"\"\"Compute expected calibration error. \n---------- \ndivd_correct : np.ndarray (n_samples,) \nWhether the prediction is correct or not \ndivd_score : np.ndarray (n_samples,) \nConfidence in the prediction \nn_bins : int, default=5 \nNumber of bins to discretize the [0, 1] interval. \n\"\"\" \n# Convert from bool to integer (makes counting easier) \ndivd_correct = divd_correct.astype(np.int32) \n\n# Create bins and assign prediction scores to bins \nbins = np.linspace(0.0, 1.0, n_bins + 1) \nbinids = np.searchsorted(bins[1:-1], divd_score) \n\n# Count number of samples and correct predictions per bin \nbin_true_counts = np.bincount( \nbinids, weights=divd_correct, minlength=len(bins) \n) \nbin_counts = np.bincount(binids, minlength=len(bins)) \n\n# Calculate sum of confidence scores per bin \nbin_probs = np.bincount(binids, weights=divd_score, minlength=len(bins)) \n\n# Identify bins that contain samples \nnonzero = bin_counts != 0 \n# Calculate accuracy for every bin \nbin_acc = bin_true_counts[nonzero] / bin_counts[nonzero] \n# Calculate average confidence scores per bin \nbin_conf = bin_probs[nonzero] / bin_counts[nonzero] \n\n    return np.average(np.abs(bin_acc - bin_conf), weights=bin_counts[nonzero])\n```", "```py\n\nNUM_BINS = 10 \n\nvanilla_cal = expected_calibration_error( \ntest_labels_subset.flatten() == vanilla_predicted_classes, \nvanilla_scores, \nn_bins=NUM_BINS, \n) \n\nensemble_cal = expected_calibration_error( \ntest_labels_subset.flatten() == ensemble_predicted_classes, \nensemble_scores, \nn_bins=NUM_BINS, \n) \n\nbbb_cal = expected_calibration_error( \ntest_labels_subset.flatten() == bbb_predicted_classes, \nbbb_scores, \nn_bins=NUM_BINS, \n)\n```", "```py\n\ncalibration = [ \n{ \n\"model_name\": \"vanilla\", \n\"type\": 0, \n\"level\": 0, \n\"calibration_error\": vanilla_cal, \n}, \n{ \n\"model_name\": \"ensemble\", \n\"type\": 0, \n\"level\": 0, \n\"calibration_error\": ensemble_cal, \n}, \n{ \n\"model_name\": \"bbb\", \n\"type\": 0, \n\"level\": 0, \n\"calibration_error\": bbb_cal, \n}, \n]\n```", "```py\n\nfor ind_type in range(NUM_TYPES): \nfor ind_level in range(NUM_LEVELS): \n# Calculate calibration error for vanilla model \nvanilla_cal_on_corrupted = expected_calibration_error( \ntest_labels_subset.flatten() \n== vanilla_predicted_classes_on_corrupted[ind_level, ind_type, :], \nvanilla_scores_on_corrupted[ind_level, ind_type, :], \n) \ncalibration.append( \n{ \n\"model_name\": \"vanilla\", \n\"type\": ind_type + 1, \n\"level\": ind_level + 1, \n\"calibration_error\": vanilla_cal_on_corrupted, \n} \n) \n\n# Calculate calibration error for ensemble model \nensemble_cal_on_corrupted = expected_calibration_error( \ntest_labels_subset.flatten() \n== ensemble_predicted_classes_on_corrupted[ind_level, ind_type, :], \nensemble_scores_on_corrupted[ind_level, ind_type, :], \n) \ncalibration.append( \n{ \n\"model_name\": \"ensemble\", \n\"type\": ind_type + 1, \n\"level\": ind_level + 1, \n\"calibration_error\": ensemble_cal_on_corrupted, \n} \n) \n\n# Calculate calibration error for BBB model \nbbb_cal_on_corrupted = expected_calibration_error( \ntest_labels_subset.flatten() \n== bbb_predicted_classes_on_corrupted[ind_level, ind_type, :], \nbbb_scores_on_corrupted[ind_level, ind_type, :], \n) \ncalibration.append( \n{ \n\"model_name\": \"bbb\", \n\"type\": ind_type + 1, \n\"level\": ind_level + 1, \n\"calibration_error\": bbb_cal_on_corrupted, \n} \n        )\n```", "```py\n\ndf = pd.DataFrame(calibration) \nplt.figure(dpi=100) \nsns.boxplot(data=df, x=\"level\", y=\"calibration_error\", hue=\"model_name\") \nplt.legend(loc=\"center left\", bbox_to_anchor=(1, 0.5)) \nplt.tight_layout \nplt.show()\n```", "```py\n\nimport dataclasses \nfrom pathlib import Path \nimport uuid \nfrom typing import Optional, Tuple \n\nimport numpy as np \nimport tensorflow as tf \nfrom sklearn.utils import shuffle\n```", "```py\n\n@dataclasses.dataclass \nclass Data: \nx_train: np.ndarray \ny_train: np.ndarray \nx_test: np.ndarray \ny_test: np.ndarray \nx_train_al: Optional[np.ndarray] = None \ny_train_al: Optional[np.ndarray] = None \n\ndef __repr__(self) -*>* str: \nrepr_str = \"\" \nfor field in dataclasses.fields(self): \nrepr_str += f\"{field.name}: {getattr(self, field.name).shape} \\n\" \n        return repr_str\n```", "```py\n\ndef get_data() -*>* Data: \nnum_classes = 10 \n(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data() \n# Scale images to the [0, 1] range \nx_train = x_train.astype(\"float32\") / 255 \nx_test = x_test.astype(\"float32\") / 255 \n# Make sure images have shape (28, 28, 1) \nx_train = np.expand_dims(x_train, -1) \nx_test = np.expand_dims(x_test, -1) \ny_train = tf.keras.utils.to_categorical(y_train, num_classes) \ny_test = tf.keras.utils.to_categorical(y_test, num_classes) \n    return Data(x_train, y_train, x_test, y_test)\n```", "```py\n\ndef get_random_balanced_indices( \ndata: Data, initial_n_samples: int \n) -*>* np.ndarray: \nlabels = np.argmax(data.y_train, axis=1) \nindices = [] \nlabel_list = np.unique(labels) \nfor label in label_list: \nindices_label = np.random.choice( \nnp.argwhere(labels == label).flatten(), \nsize=initial_n_samples // len(label_list), \nreplace=False \n) \nindices.extend(indices_label) \nindices = np.array(indices) \nnp.random.shuffle(indices) \n    return indices\n```", "```py\n\ndef get_initial_ds(data: Data, initial_n_samples: int) -*>* Data: \nindices = get_random_balanced_indices(data, initial_n_samples) \nx_train_al, y_train_al = data.x_train[indices], data.y_train[indices] \nx_train = np.delete(data.x_train, indices, axis=0) \ny_train = np.delete(data.y_train, indices, axis=0) \nreturn Data( \nx_train, y_train, data.x_test, data.y_test, x_train_al, y_train_al \n    )\n```", "```py\n\n@dataclasses.dataclass \nclass Config: \ninitial_n_samples: int \nn_total_samples: int \nn_epochs: int \nn_samples_per_iter: int \n# string representation of the acquisition function \nacquisition_type: str \n# number of mc_dropout iterations \n    n_iter: int\n```", "```py\n\ndef build_model(): \nmodel = tf.keras.models.Sequential([ \nInput(shape=(28, 28, 1)), \nlayers.Conv2D(32, kernel_size=(4, 4), activation=\"relu\"), \nlayers.Conv2D(32, kernel_size=(4, 4), activation=\"relu\"), \nlayers.MaxPooling2D(pool_size=(2, 2)), \nlayers.Dropout(0.25), \nlayers.Flatten(), \nlayers.Dense(128, activation=\"relu\"), \nlayers.Dropout(0.5), \nlayers.Dense(10, activation=\"softmax\"), \n]) \nmodel.compile( \ntf.keras.optimizers.Adam(), \nloss=\"categorical_crossentropy\", \nmetrics=[\"accuracy\"], \nexperimental_run_tf_function=False, \n) \n    return model\n```", "```py\n\ndef total_uncertainty( \ndivds: np.ndarray, epsilon: float = 1e-10 \n) -*>* np.ndarray: \nmean_divds = np.mean(divds, axis=1) \nlog_divds = -np.log(mean_divds + epsilon) \n    return np.sum(mean_divds * log_divds, axis=1)\n```", "```py\n\ndef data_uncertainty(divds: np.ndarray, epsilon: float = 1e-10) -*>* np.ndarray: \nlog_divds = -np.log(divds + epsilon) \n    return np.mean(np.sum(divds * log_divds, axis=2), axis=1)\n```", "```py\n\ndef knowledge_uncertainty( \ndivds: np.ndarray, epsilon: float = 1e-10 \n) -*>* np.ndarray: \n    return total_uncertainty(divds, epsilon) - data_uncertainty(divds, epsilon)\n```", "```py\n\nfrom typing import Callable \nfrom keras import Model \nfrom tqdm import tqdm \n\nimport numpy as np \n\ndef acquire_knowledge_uncertainty( \nx_train: np.ndarray, \nn_samples: int, \nmodel: Model, \nn_iter: int, \n*args, \n**kwargs \n): \ndivds = get_mc_predictions(model, n_iter, x_train) \nku = knowledge_uncertainty(divds) \n    return np.argsort(ku, axis=-1)[-n_samples:]\n```", "```py\n\ndef get_mc_predictions( \nmodel: Model, n_iter: int, x_train: np.ndarray \n) -*>* np.ndarray: \ndivds = [] \nfor _ in tqdm(range(n_iter)): \ndivds_iter = [ \nmodel(batch, training=True) \nfor batch in np.array_split(x_train, 6) \n] \ndivds.append(np.concatenate(divds_iter)) \n# format data such that we have n_images, n_predictions, n_classes \ndivds = np.moveaxis(np.stack(divds), 0, 1) \n    return divds\n```", "```py\n\ndef acquire_random(x_train: np.ndarray, n_samples: int, *args, **kwargs): \n    return np.random.randint(low=0, high=len(x_train), size=n_samples)\n```", "```py\n\ndef acquisition_factory(acquisition_type: str) -*>* Callable: \nif acquisition_type == \"knowledge_uncertainty\": \nreturn acquire_knowledge_uncertainty \nif acquisition_type == \"random\": \n        return acquire_random\n```", "```py\n\ncfg = Config( \ninitial_n_samples=20, \nn_total_samples=1000, \nn_epochs=50, \nn_samples_per_iteration=10, \nacquisition_type=\"knowledge_uncertainty\", \nn_iter=100, \n)\n```", "```py\n\ndata: Data = get_initial_ds(get_data(), cfg.initial_n_samples) \naccuracies = {} \nadded_indices = []\n```", "```py\n\nrun_uuid = str(uuid.uuid4()) \nmodel_dir = Path(\"./models\") / cfg.acquisition_type / run_uuid \nmodel_dir.mkdir(parents=True, exist_ok=True)\n```", "```py\n\n    for i in range(cfg.n_total_samples // cfg.n_samples_per_iter): \n    iter_dir = model_dir / str(i) \n    model = build_model() \n    model.fit( \n    x=data.x_train_al, \n    y=data.y_train_al, \n    validation_data=(data.x_test, data.y_test), \n    epochs=cfg.n_epochs, \n    callbacks=[get_callback(iter_dir)], \n    verbose=2, \n        )\n    ```", "```py\n\n    model = tf.keras.models.load_model(iter_dir) \n    indices_to_add = acquisition_factory(cfg.acquisition_type)( \n    data.x_train, \n    cfg.n_samples_per_iter, \n    n_iter=cfg.n_iter, \n    model=model, \n    ) \n    added_indices.append(indices_to_add) \n        data, (iter_x, iter_y) = update_ds(data, indices_to_add)\n    ```", "```py\n\n    save_images_and_labels_added(iter_dir, iter_x, iter_y) \n    divds = model(data.x_test) \n    accuracy = get_accuracy(data.y_test, divds) \n    accuracies[i] = accuracy \n        save_results(accuracies, added_indices, model_dir)\n    ```", "```py\n\ndef get_callback(model_dir: Path): \nmodel_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint( \nstr(model_dir), \nmonitor=\"val_accuracy\", \nverbose=0, \nsave_best_only=True, \n) \n    return model_checkpoint_callback\n```", "```py\n\ndef get_accuracy(y_test: np.ndarray, divds: np.ndarray) -*>* float: \nacc = tf.keras.metrics.CategoricalAccuracy() \nacc.update_state(divds, y_test) \n    return acc.result().numpy() * 100\n```", "```py\n\ndef save_images_and_labels_added( \noutput_path: Path, iter_x: np.ndarray, iter_y: np.ndarray \n): \ndf = pd.DataFrame() \ndf[\"label\"] = np.argmax(iter_y, axis=1) \niter_x_normalised = (np.squeeze(iter_x, axis=-1) * 255).astype(np.uint8) \ndf[\"image\"] = iter_x_normalised.reshape(10, 28*28).tolist() \ndf.to_parquet(output_path / \"added.parquet\", index=False) \n\ndef save_results( \naccuracies: Dict[int, float], added_indices: List[int], model_dir: Path \n): \ndf = pd.DataFrame(accuracies.items(), columns=[\"i\", \"accuracy\"]) \ndf[\"added\"] = added_indices \n    df.to_parquet(f\"{model_dir}/results.parquet\", index=False)\n```", "```py\n\nimport seaborn as sns \nimport matplotlib.pyplot as plt \nimport pandas as pd \nimport numpy as np \nsns.set_style(\"darkgrid\") \nsns.set_context(\"paper\")\n```", "```py\n\ndef plot(uuid: str, acquisition: str, ax=None): \nacq_name = acquisition.replace(\"_\", \" \") \ndf = pd.read_parquet(f\"./models/{acquisition}/{uuid}/results.parquet\")[:-1] \ndf = df.rename(columns={\"accuracy\": acq_name}) \ndf[\"n_samples\"] = df[\"i\"].apply(lambda x: x*10 + 20) \nreturn df.plot.line( \nx=\"n_samples\", y=acq_name, style='.-', figsize=(8,5), ax=ax \n    )\n```", "```py\n\nax = plot(\"bc1adec5-bc34-44a6-a0eb-fa7cb67854e4\", \"random\") \nax = plot( \n\"5c8d6001-a5fb-45d3-a7cb-2a8a46b93d18\", \"knowledge_uncertainty\", ax=ax \n) \nplt.xticks(np.arange(0, 1050, 50)) \nplt.yticks(np.arange(54, 102, 2)) \nplt.ylabel(\"Accuracy\") \nplt.xlabel(\"Number of acquired samples\") \nplt.show()\n```", "```py\n\ndef get_imgs_per_label(model_dirs) -*>* Dict[int, np.ndarray]: \nimgs_per_label = {i: [] for i in range(10)} \nfor model_dir in model_dirs: \ndf = pd.read_parquet(model_dir / \"images_added.parquet\") \ndf.image = df.image.apply( \nlambda x: x.reshape(28, 28).astype(np.uint8) \n) \nfor label in df.label.unique(): \ndff = df[df.label == label] \nif len(dff) == 0: \ncontinue \nimgs_per_label[label].append(np.hstack(dff.image)) \n    return imgs_per_label\n```", "```py\n\nfrom PIL import Image \nfrom pathlib import Path \n\ndef get_added_images( \nacquisition: str, uuid: str, n_iter: int = 5 \n) -*>* Image: \nbase_dir = Path(\"./models\") / acquisition / uuid \nmodel_dirs = filter(lambda x: x.is_dir(), base_dir.iterdir()) \nmodel_dirs = sorted(model_dirs, key=lambda x: int(x.stem)) \nimgs_per_label = get_imgs_per_label(model_dirs) \nimgs = [] \nfor i in range(10): \nlabel_img = np.hstack(imgs_per_label[i])[:, -(28 * n_iter):] \nimgs.append(label_img) \n    return Image.fromarray(np.vstack(imgs))\n```", "```py\n\nuuid = \"bc1adec5-bc34-44a6-a0eb-fa7cb67854e4\" \nimg_random = get_added_images(\"random\", uuid) \nuuid = \"5c8d6001-a5fb-45d3-a7cb-2a8a46b93d18\" \nimg_ku = get_added_images(\"knowledge_uncertainty\", uuid)\n```", "```py\n\nimport numpy as np \nimport tensorflow as tf \nfrom scipy.spatial.distance import euclidean \nfrom tensorflow.keras import ( \nModel, \nSequential, \nlayers, \noptimizers, \nmetrics, \nlosses, \n) \nimport pandas as pd \nfrom sklearn.preprocessing import StandardScaler \nimport copy \n\nclass Environment: \ndef __init__(self, env_size=8, max_steps=2000): \nself.env_size = env_size \nself.max_steps = max_steps \nself.agent_location = np.zeros(2) \nself.target_location = np.random.randint(0, self.env_size, 2) \nself.action_space = { \n0: np.array([0, 1]), \n1: np.array([0, -1]), \n2: np.array([1, 0]), \n3: np.array([-1, 0]), \n} \nself.delta = self.compute_distance() \nself.is_done = False \nself.total_steps = 0 \nself.ideal_steps = self.calculate_ideal_steps() \n    ...\n```", "```py\n\n... \n\ndef calculate_ideal_action(self, agent_location, target_location): \nmin_delta = 1e1000 \nideal_action = -1 \nfor k in self.action_space.keys(): \ndelta = euclidean( \nagent_location + self.action_space[k], target_location \n) \nif delta *<*= min_delta: \nmin_delta = delta \nideal_action = k \nreturn ideal_action, min_delta \n\ndef calculate_ideal_steps(self): \nagent_location = copy.deepcopy(self.agent_location) \ntarget_location = copy.deepcopy(self.target_location) \ndelta = 1e1000 \ni = 0 \nwhile delta *>* 0: \nideal_action, delta = self.calculate_ideal_action( \nagent_location, target_location \n) \nagent_location += self.action_space[ideal_action] \ni += 1 \nreturn i \n    ...\n```", "```py\n\n... \ndef update(self, action_int): \nself.agent_location = ( \nself.agent_location + self.action_space[action_int] \n) \n# prevent the agent from moving outside the bounds of the environment \nself.agent_location[self.agent_location *>* (self.env_size - 1)] = ( \nself.env_size - 1 \n) \nself.compute_reward() \nself.total_steps += 1 \nself.is_done = (self.delta == 0) or (self.total_steps *>*= self.max_steps) \nreturn self.reward \n    ...\n```", "```py\n\n... \ndef compute_reward(self): \nd1 = self.delta \nself.delta = self.compute_distance() \nif self.delta *<* d1: \nself.reward = 10 \nelse: \nself.reward = 1 \n    ...\n```", "```py\n\n... \ndef compute_distance(self): \nreturn euclidean(self.agent_location, self.target_location) \n    ...\n```", "```py\n\n... \ndef get_state(self): \nreturn np.concatenate([self.agent_location, self.target_location]) \n    ...\n```", "```py\n\nclass RLModel: \ndef __init__(self, state_size, n_actions, num_epochs=500): \nself.state_size = state_size \nself.n_actions = n_actions \nself.num_epochs = 200 \nself.model = Sequential() \nself.model.add( \nlayers.Dense( \n20, input_dim=self.state_size, activation=\"relu\", name=\"layer_1\" \n) \n) \nself.model.add(layers.Dense(8, activation=\"relu\", name=\"layer_2\")) \nself.model.add(layers.Dense(1, activation=\"relu\", name=\"layer_3\")) \nself.model.compile( \noptimizer=optimizers.Adam(), \nloss=losses.Huber(), \nmetrics=[metrics.RootMeanSquaredError()], \n) \n    ...\n```", "```py\n\n... \ndef fit(self, X_train, y_train, batch_size=16): \nself.scaler = StandardScaler() \nX_train = self.scaler.fit_transform(X_train) \nself.model.fit( \nX_train, \ny_train, \nepochs=self.num_epochs, \nverbose=0, \nbatch_size=batch_size, \n) \n\ndef predict(self, state): \nrewards = [] \nX = np.zeros((self.n_actions, self.state_size)) \nfor i in range(self.n_actions): \nX[i] = np.concatenate([state, [i]]) \nX = self.scaler.transform(X) \nrewards = self.model.predict(X) \n        return np.argmax(rewards)\n```", "```py\n\nenv_size = 8 \nstate_size = 5 \nn_actions = 4 \nepsilon = 1.0 \nhistory = {\"state\": [], \"reward\": []} \nn_samples = 1000 \nmax_steps = 500 \nregrets = [] \n\nmodel = RLModel(state_size, n_actions)\n```", "```py\n\nfor i in range(100): \nenv = Environment(env_size, max_steps=max_steps) \nwhile not env.is_done: \nstate = env.get_state() \nif np.random.rand() *<* epsilon: \naction = np.random.randint(n_actions) \nelse: \naction = model.predict(state) \nreward = env.update(action) \nhistory[\"state\"].append(np.concatenate([state, [action]])) \nhistory[\"reward\"].append(reward) \nprint( \nf\"Completed episode {i} in {env.total_steps} steps.\" \nf\"Ideal steps: {env.ideal_steps}.\" \nf\"Epsilon: {epsilon}\" \n) \nregrets.append(np.abs(env.total_steps-env.ideal_steps)) \nidxs = np.random.choice(len(history[\"state\"]), n_samples) \nmodel.fit( \nnp.array(history[\"state\"])[idxs], \nnp.array(history[\"reward\"])[idxs] \n) \n    epsilon-=epsilon/10\n```", "```py\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\ndf_plot = pd.DataFrame({\"regret\": regrets, \"episode\": np.arange(len(regrets))}) \nsns.lineplot(x=\"episode\", y=\"regret\", data=df_plot) \nfig = plt.gcf() \nfig.set_size_inches(5, 10) \nplt.show()\n```", "```py\n\ndef __init__(self, env_size=8, max_steps=2000, dynamic_obstacle=False, lambda_val=2): \nself.env_size = env_size \nself.max_steps = max_steps \nself.agent_location = np.zeros(2) \nself.dynamic_obstacle = dynamic_obstacle \nself.lambda_val = lambda_val \nself.target_location = np.random.randint(0, self.env_size, 2) \nwhile euclidean(self.agent_location, self.target_location) *<* 4: \nself.target_location = np.random.randint(0, self.env_size, 2) \nself.action_space = { \n0: np.array([0, 1]), \n1: np.array([0, -1]), \n2: np.array([1, 0]), \n3: np.array([-1, 0]), \n} \nself.delta = self.compute_distance() \nself.is_done = False \nself.total_steps = 0 \nself.obstacle_location = np.array( \n[self.env_size / 2, self.env_size / 2], dtype=int \n) \nself.ideal_steps = self.calculate_ideal_steps() \nself.collision = False \n\n```", "```py\n\ndef calculate_ideal_steps(self): \nagent_location = copy.deepcopy(self.agent_location) \ntarget_location = copy.deepcopy(self.target_location) \ndelta = 1e1000 \ni = 0 \nwhile delta *>* 0: \nideal_action, delta = self.calculate_ideal_action( \nagent_location, target_location \n) \nagent_location += self.action_space[ideal_action] \nif np.random.randint(0, 2) and self.dynamic_obstacle: \nself.obstacle_location = copy.deepcopy(agent_location) \ni += 1 \n        return i\n```", "```py\n\ndef get_obstacle_proximity(self): \nobstacle_action_dists = np.array( \n[ \neuclidean( \nself.agent_location + self.action_space[k], \nself.obstacle_location, \n) \nfor k in self.action_space.keys() \n] \n) \nreturn self.lambda_val * ( \nnp.array(obstacle_action_dists *<* 2.5, dtype=float) \n+ np.array(obstacle_action_dists *<* 3.5, dtype=float) \n+ np.array(obstacle_action_dists *<* 4.5, dtype=float) \n        )\n```", "```py\n\ndef compute_reward(self): \nd1 = self.delta \nself.delta = self.compute_distance() \nif euclidean(self.agent_location, self.obstacle_location) == 0: \nself.reward = 0 \nself.collision = True \nself.is_done = True \nelif self.delta *<* d1: \nself.reward = 10 \nelse: \n            self.reward = 1\n```", "```py\n\nclass RLModelDropout: \ndef __init__(self, state_size, n_actions, num_epochs=200, nb_inference=10): \nself.state_size = state_size \nself.n_actions = n_actions \nself.num_epochs = num_epochs \nself.nb_inference = nb_inference \nself.model = Sequential() \nself.model.add( \nlayers.Dense( \n10, input_dim=self.state_size, activation=\"relu\", name=\"layer_1\" \n) \n) \n# self.model.add(layers.Dropout(0.15)) \n# self.model.add(layers.Dense(8, activation='relu', name='layer_2')) \nself.model.add(layers.Dropout(0.15)) \nself.model.add(layers.Dense(1, activation=\"relu\", name=\"layer_2\")) \nself.model.compile( \noptimizer=optimizers.Adam(), \nloss=losses.Huber(), \nmetrics=[metrics.RootMeanSquaredError()], \n) \n\nself.proximity_dict = {\"proximity sensor value\": [], \"uncertainty\": []} \n    ...\n```", "```py\n\n... \ndef fit(self, X_train, y_train, batch_size=16): \nself.scaler = StandardScaler() \nX_train = self.scaler.fit_transform(X_train) \nself.model.fit( \nX_train, \ny_train, \nepochs=self.num_epochs, \nverbose=0, \nbatch_size=batch_size, \n) \n    ...\n```", "```py\n\n... \ndef predict(self, state, obstacle_proximity, dynamic_obstacle=False): \nrewards = [] \nX = np.zeros((self.n_actions, self.state_size)) \nfor i in range(self.n_actions): \nX[i] = np.concatenate([state, [i], [obstacle_proximity[i]]]) \nX = self.scaler.transform(X) \nrewards, y_std = self.predict_ll_dropout(X) \n# we subtract our standard deviations from our predicted reward values, \n# this way uncertain predictions are penalised \nrewards = rewards - (y_std * 2) \nbest_action = np.argmax(rewards) \nif dynamic_obstacle: \nself.proximity_dict[\"proximity sensor value\"].append( \nobstacle_proximity[best_action] \n) \nself.proximity_dict[\"uncertainty\"].append(y_std[best_action][0]) \nreturn best_action \n    ...\n```", "```py\n\n... \ndef predict_ll_dropout(self, X): \nll_divd = [ \nself.model(X, training=True) for _ in range(self.nb_inference) \n] \nll_divd = np.stack(ll_divd) \n        return ll_divd.mean(axis=0), ll_divd.std(axis=0)\n```", "```py\n\nclass RLModel: \ndef __init__(self, state_size, n_actions, num_epochs=500): \nself.state_size = state_size \nself.n_actions = n_actions \nself.num_epochs = 200 \nself.model = Sequential() \nself.model.add( \nlayers.Dense( \n20, input_dim=self.state_size, activation=\"relu\", name=\"layer_1\" \n) \n) \nself.model.add(layers.Dense(8, activation=\"relu\", name=\"layer_2\")) \nself.model.add(layers.Dense(1, activation=\"relu\", name=\"layer_3\")) \nself.model.compile( \noptimizer=optimizers.Adam(), \nloss=losses.Huber(), \nmetrics=[metrics.RootMeanSquaredError()], \n) \n\ndef fit(self, X_train, y_train, batch_size=16): \nself.scaler = StandardScaler() \nX_train = self.scaler.fit_transform(X_train) \nself.model.fit( \nX_train, \ny_train, \nepochs=self.num_epochs, \nverbose=0, \nbatch_size=batch_size, \n) \n\ndef predict(self, state, obstacle_proximity, obstacle=False): \nrewards = [] \nX = np.zeros((self.n_actions, self.state_size)) \nfor i in range(self.n_actions): \nX[i] = np.concatenate([state, [i], [obstacle_proximity[i]]]) \nX = self.scaler.transform(X) \nrewards = self.model.predict(X) \nreturn np.argmax(rewards) \n\n```", "```py\n\nenv_size = 8 \nstate_size = 6 \nn_actions = 4 \nepsilon = 1.0 \nhistory = {\"state\": [], \"reward\": []} \nmodel = RLModelDropout(state_size, n_actions, num_epochs=400) \nn_samples = 1000 \nmax_steps = 500 \nregrets = [] \ncollisions = 0 \nfailed = 0\n```", "```py\n\nfor i in range(100): \nif i *<* 50: \nenv = Environment(env_size, max_steps=max_steps) \ndynamic_obstacle = False \nelse: \ndynamic_obstacle = True \nepsilon = 0 \nenv = Environment( \nenv_size, max_steps=max_steps, dynamic_obstacle=True \n) \n    ...\n```", "```py\n\n... \nwhile not env.is_done: \nstate = env.get_state() \nobstacle_proximity = env.get_obstacle_proximity() \nif np.random.rand() *<* epsilon: \naction = np.random.randint(n_actions) \nelse: \naction = model.predict(state, obstacle_proximity, dynamic_obstacle) \nreward = env.update(action) \nhistory[\"state\"].append( \nnp.concatenate([state, [action], \n[obstacle_proximity[action]]]) \n) \nhistory[\"reward\"].append(reward) \n    ...\n```", "```py\n\nif env.total_steps == max_steps: \nprint(f\"Failed to find target for episode {i}. Epsilon: {epsilon}\") \nfailed += 1 \nelif env.total_steps *<* env.ideal_steps: \nprint(f\"Collided with obstacle during episode {i}. Epsilon: {epsilon}\") \ncollisions += 1 \nelse: \nprint( \nf\"Completed episode {i} in {env.total_steps} steps.\" \nf\"Ideal steps: {env.ideal_steps}.\" \nf\"Epsilon: {epsilon}\" \n) \nregrets.append(np.abs(env.total_steps-env.ideal_steps)) \nif not dynamic_obstacle: \nidxs = np.random.choice(len(history[\"state\"]), n_samples) \nmodel.fit( \nnp.array(history[\"state\"])[idxs], \nnp.array(history[\"reward\"])[idxs] \n) \n        epsilon-=epsilon/10\n```", "```py\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\ndf_plot = pd.DataFrame(model.proximity_dict) \nsns.boxplot(x=\"proximity sensor value\", y=\"uncertainty\", data=df_plot)\n```", "```py\n\ndef conv_block(filters): \nreturn [ \ntf.keras.layers.Conv2D( \nfilters, \n(3, 3), \nactivation=\"relu\", \nkernel_initializer=\"he_uniform\", \n), \ntf.keras.layers.MaxPooling2D((2, 2)), \ntf.keras.layers.Dropout(0.5), \n] \n\nmodel = tf.keras.models.Sequential( \n[ \ntf.keras.layers.Conv2D( \n32, \n(3, 3), \nactivation=\"relu\", \ninput_shape=(160, 160, 3), \nkernel_initializer=\"he_uniform\", \n), \ntf.keras.layers.MaxPooling2D((2, 2)), \ntf.keras.layers.Dropout(0.2), \n*conv_block(64), \n*conv_block(128), \n*conv_block(256), \n*conv_block(128), \ntf.keras.layers.Conv2D( \n64, \n(3, 3), \nactivation=\"relu\", \nkernel_initializer=\"he_uniform\", \n), \ntf.keras.layers.Flatten(), \ntf.keras.layers.Dense(64, activation=\"relu\"), \ntf.keras.layers.Dropout(0.5), \ntf.keras.layers.Dense(2), \n] \n) \n\n```", "```py\n\ntrain_dataset_divprocessed = train_dataset.map(lambda x, y: (x / 255., y)) \nval_dataset_divprocessed = validation_dataset.map(lambda x, y: (x / 255., y)) \n\nmodel.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \nloss=tf.keras.losses.CategoricalCrossentropy(from_logits=True), \nmetrics=['accuracy']) \nmodel.fit( \ntrain_dataset_divprocessed, \nepochs=200, \nvalidation_data=val_dataset_divprocessed, \n)\n```", "```py\n\nfrom cleverhans.tf2.attacks.fast_gradient_method import ( \nfast_gradient_method as fgsm, \n)\n```", "```py\n\nPredictions_standard, predictions_fgsm, labels = [], [], [] \nfor imgs, labels_batch in test_dataset: \nimgs /= 255\\. \npredictions_standard.extend(model.predict(imgs)) \nimgs_adv = fgsm(model, imgs, 0.01, np.inf) \npredictions_fgsm.extend(model.predict(imgs_adv)) \n  labels.extend(labels_batch)\n```", "```py\n\naccuracy_standard = CategoricalAccuracy()( \nlabels, predictions_standard \n).numpy() \naccuracy_fgsm = CategoricalAccuracy()( \nlabels, predictions_fgsm \n).numpy() \nprint(f\"{accuracy_standard=.2%}, {accuracy_fsgm=:.2%}\") \n# accuracy_standard=83.67%, accuracy_fsgm=30.70%\n```", "```py\n\nimport numpy as np \n\ndef mc_dropout(model, images, n_inference: int = 50): \nreturn np.swapaxes(np.stack([ \nmodel(images, training=True) for _ in range(n_inference) \n  ]), 0, 1)\n```", "```py\n\nPredictions_standard_mc, predictions_fgsm_mc, labels = [], [], [] \nfor imgs, labels_batch in test_dataset: \nimgs /= 255\\. \npredictions_standard_mc.extend( \nmc_dropout(model, imgs, 50) \n) \nimgs_adv = fgsm(model, imgs, 0.01, np.inf) \npredictions_fgsm_mc.extend( \nmc_dropout(model, imgs_adv, 50) \n) \n  labels.extend(labels_batch)\n```", "```py\n\naccuracy_standard_mc = CategoricalAccuracy()( \nlabels, np.stack(predictions_standard_mc).mean(axis=1) \n).numpy() \naccuracy_fgsm_mc = CategoricalAccuracy()( \nlabels, np.stack(predictions_fgsm_mc).mean(axis=1) \n).numpy() \nprint(f\"{accuracy_standard_mc=.2%}, {accuracy_fgsm_mc=:.2%}\") \n# accuracy_standard_mc=86.60%, accuracy_fgsm_mc=80.75%\n```", "```py\n\ndef get_mean_softmax_value(predictions) -*>* float: \nmean_softmax = tf.nn.softmax(predictions, axis=1) \nmax_softmax = np.max(mean_softmax, axis=1) \nmean_max_softmax = max_softmax.mean() \nreturn mean_max_softmax \n\ndef get_mean_softmax_value_mc(predictions) -*>* float: \npredictions_np = np.stack(predictions) \npredictions_np_mean = predictions_np.mean(axis=1) \n  return get_mean_softmax_value(predictions_np_mean)\n```", "```py\n\nmean_standard = get_mean_softmax_value(predictions_standard) \nmean_fgsm = get_mean_softmax_value(predictions_fgsm) \nmean_standard_mc = get_mean_softmax_value_mc(predictions_standard_mc) \nmean_fgsm_mc = get_mean_softmax_value_mc(predictions_fgsm_mc) \nprint(f\"{mean_standard=:.2%}, {mean_fgsm=:.2%}\") \nprint(f\"{mean_standard_mc=:.2%}, {mean_fgsm_mc=:.2%}\") \n# mean_standard=89.58%, mean_fgsm=89.91% \n# mean_standard_mc=89.48%, mean_fgsm_mc=85.25%\n```"]