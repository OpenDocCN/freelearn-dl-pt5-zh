["```py\n    import numpy as np\n    ```", "```py\n    def conv(image, im_filter):\n        # input dimensions\n        height = image.shape[0]\n        width = image.shape[1]\n        # output image with reduced dimensions\n        im_c = np.zeros((height - len(im_filter) + 1,\n            width - len(im_filter) + 1))\n        # iterate over all rows and columns\n        for row in range(len(im_c)):\n            for col in range(len(im_c[0])):\n                # apply the filter\n                for i in range(len(im_filter)):\n                    for j in range(len(im_filter[0])):\n                        im_c[row, col] += image[row + i, /\n                            col + j] * im_filter[i][j]\n        # fix out-of-bounds values\n        im_c[im_c > 255] = 255\n        im_c[im_c < 0] = 0\n        return im_c\n    ```", "```py\n    # blur filter\n    blur = np.full([10, 10], 1\\. / 100)\n    conv(image_grayscale, blur)\n    # sobel filters\n    sobel_x = [[-1, -2, -1],\n               [0, 0, 0],\n               [1, 2, 1]]\n    conv(image_grayscale, sobel_x)\n    sobel_y = [[-1, 0, 1],\n               [-2, 0, 2],\n               [-1, 0, 1]]\n    conv(image_grayscale, sobel_y)\n    ```", "```py\nheight_o = 1 + (height_i + 2*padding_h – filter_h) / stride\nwidth_o = 1 + (width_i + 2*padding_w – filter_w) / stride\n```", "```py\nheight_o = 1 + (height_i – filter_h) / stride\nwidth_o = 1 + (width_i – filter_w) / stride\n```", "```py\n    import torch\n    from torchsummary import summary\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    ```", "```py\n    import torchvision.transforms as transforms\n    from torchvision import datasets\n    from torch.utils.data import DataLoader\n    # Training dataset\n    train_transform = transforms.Compose([\n        transforms.RandomHorizontalFlip(),\n        transforms.RandomVerticalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(\n            [0.485, 0.456, 0.406],\n            [0.229, 0.224, 0.225])\n    ])\n    train_data = datasets.CIFAR10(\n        root='data',\n        train=True,\n        download=True,\n        transform=train_transform)\n    batch_size = 50\n    train_loader = DataLoader(\n        dataset=train_data,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=2)\n    ```", "```py\n    validation_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(\n            [0.485, 0.456, 0.406],\n            [0.229, 0.224, 0.225])\n    ])\n    validation_data = datasets.CIFAR10(\n        root='data',\n        train=False,\n        download=True,\n        transform=validation_transform)\n    validation_loader = DataLoader(\n        dataset=validation_data,\n        batch_size=100,\n        shuffle=True)\n    ```", "```py\n    from torch.nn import Sequential, Conv2d, BatchNorm2d, GELU, MaxPool2d, Dropout2d, Linear, Flatten\n    model = Sequential(\n        Conv2d(in_channels=3, out_channels=32,\n            kernel_size=3, padding=1),\n        BatchNorm2d(32),\n        GELU(),\n        Conv2d(in_channels=32, out_channels=32,\n            kernel_size=3, padding=1),\n        BatchNorm2d(32),\n        GELU(),\n        MaxPool2d(kernel_size=2, stride=2),\n        Dropout2d(0.2),\n        Conv2d(in_channels=32, out_channels=64,\n            kernel_size=3, padding=1),\n        BatchNorm2d(64),\n        GELU(),\n        Conv2d(in_channels=64, out_channels=64,\n            kernel_size=3, padding=1),\n        BatchNorm2d(64),\n        GELU(),\n        MaxPool2d(kernel_size=2, stride=2),\n        Dropout2d(p=0.3),\n        Conv2d(in_channels=64, out_channels=128,\n            kernel_size=3),\n        BatchNorm2d(128),\n        GELU(),\n        Conv2d(in_channels=128, out_channels=128,\n            kernel_size=3),\n        BatchNorm2d(128),\n        GELU(),\n        MaxPool2d(kernel_size=2, stride=2),\n        Dropout2d(p=0.5),\n        Flatten(),\n        Linear(512, 10),\n    )\n    ```", "```py\n    import tensorflow as tf\n    (X_train, Y_train), (X_validation, Y_validation) = \\\n        tf.keras.datasets.cifar10.load_data()\n    Y_train = tf.keras.utils.to_categorical(Y_train, 10)\n    Y_validation = \\\n        tf.keras.utils.to_categorical(Y_validation, 10)\n    ```", "```py\n    from tensorflow.keras.preprocessing.image import ImageDataGenerator\n    data_generator = ImageDataGenerator(\n        featurewise_center=True,\n        featurewise_std_normalization=True,\n        horizontal_flip=True,\n        vertical_flip=True)\n    # Apply z-normalization on the training set\n    data_generator.fit(X_train)\n    # Standardize the validation set\n    X_validation = \\\n        data_generator.standardize( \\\n        X_validation.astype('float32'))\n    ```", "```py\n    from tensorflow.keras.models import Sequential\n    from tensorflow.keras.layers import Conv2D, Dense, MaxPooling2D, Dropout, BatchNormalization, Activation, Flatten\n    model = Sequential(layers=[\n        Conv2D(32, (3, 3),\n            padding='same',\n            input_shape=X_train.shape[1:]),\n        BatchNormalization(),\n        Activation('gelu'),\n        Conv2D(32, (3, 3), padding='same'),\n        BatchNormalization(),\n        Activation('gelu'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Dropout(0.2),\n        Conv2D(64, (3, 3), padding='same'),\n        BatchNormalization(),\n        Activation('gelu'),\n        Conv2D(64, (3, 3), padding='same'),\n        BatchNormalization(),\n        Activation('gelu'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Dropout(0.3),\n        Conv2D(128, (3, 3)),\n        BatchNormalization(),\n        Activation('gelu'),\n        Conv2D(128, (3, 3)),\n        BatchNormalization(),\n        Activation('gelu'),\n        MaxPooling2D(pool_size=(2, 2)),\n        Dropout(0.5),\n        Flatten(),\n        Dense(10, activation='softmax')\n    ])\n    ```", "```py\n    model.compile(loss='categorical_crossentropy',\n        optimizer='adam', metrics=['accuracy'])\n    print(model.summary())\n    ```", "```py\n    batch_size = 50\n    model.fit(\n        x=data_generator.flow(x=X_train,\n            y=Y_train,\n            batch_size=batch_size),\n        steps_per_epoch=len(X_train) // batch_size,\n        epochs=100,\n        verbose=1,\n        validation_data=(X_validation, Y_validation),\n        workers=4)\n    ```", "```py\nfrom torchvision.models import mobilenet_v3_large, MobileNet_V3_Large_Weights\n# With pretrained weights:\nmodel = mobilenet_v3_large(\n        weights=MobileNet_V3_Large_Weights.IMAGENET1K_V1)\nmodel = mobilenet_v3_large(weights=\"IMAGENET1K_V1\")\n# Using no weights:\nmodel = mobilenet_v3_large(weights=None)\n```", "```py\nfrom torchvision.models import list_models, get_model\n# List available models\nall_models = list_models()\nmodel = get_model(all_models[0], weights=\"DEFAULT\")\n```", "```py\nfrom keras.applications.mobilenet_v3 import MobileNetV3Large\nmodel = MobileNetV3Large(weights='imagenet')\n```"]