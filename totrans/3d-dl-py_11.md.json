["```py\npython -m pip install 'git+https://github.com/facebookresearch/detectron2.git'\n```", "```py\ngit clone https://github.com/facebookresearch/meshrcnn.git\ncd meshrcnn && pip install -e .\n```", "```py\npython demo/demo.py \\\n--config-file configs/pix3d/meshrcnn_R50_FPN.yaml \\\n--input /path/to/image \\\n--output output_demo \\\n--onlyhighest MODEL.WEIGHTS meshrcnn://meshrcnn_R50.pth\n```", "```py\n    import torch\n    import numpy as np\n    import matplotlib.pyplot as plt\n    from pytorch3d.io import load_obj\n    from pytorch3d.structures import Meshes\n    from pytorch3d.renderer import (\n       FoVPerspectiveCameras, look_at_view_transform, look_at_rotation,\n       RasterizationSettings, MeshRenderer, MeshRasterizer, BlendParams,\n       SoftSilhouetteShader, HardPhongShader, PointLights, TexturesVertex,\n    )\n    import argparse\n    ```", "```py\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--path_to_mesh', default=\"./demo_results/0_mesh_sofa_0.887.obj\")\n    parser.add_argument('--save_path', default='./demo_results/sofa_render.png')\n    parser.add_argument('--distance', default=1, help = 'distance from camera to the object')\n    parser.add_argument('--elevation', default=150.0,  help = 'angle of elevation in degrees')\n    parser.add_argument('--azimuth', default=-10.0, help = 'rotation of the camera')\n    args = parser.parse_args()\n    ```", "```py\n    # Load the obj and ignore the textures and materials.\n    verts, faces_idx, _ = load_obj(args.path_to_mesh)\n    faces = faces_idx.verts_idx\n    # Initialize each vertex to be white in color.\n    verts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\n    textures = TexturesVertex(verts_features=verts_rgb.to(device))\n    # Create a Meshes object for the sofa. Here we have only one mesh in the batch.\n    sofa_mesh = Meshes(\n       verts=[verts.to(device)],\n       faces=[faces.to(device)],\n       textures=textures\n    )\n    ```", "```py\n    cameras = FoVPerspectiveCameras(device=device)\n    blend_params = BlendParams(sigma=1e-4, gamma=1e-4)\n    ```", "```py\n    raster_settings = RasterizationSettings(\n       image_size=256,\n       blur_radius=np.log(1\\. / 1e-4 - 1.) * blend_params.sigma,\n       faces_per_pixel=100,\n    )\n    silhouette_renderer = MeshRenderer(\n       rasterizer=MeshRasterizer(\n           cameras=cameras,\n           raster_settings=raster_settings\n       ),\n       shader=SoftSilhouetteShader(blend_params=blend_params)\n    )\n    ```", "```py\n    raster_settings = RasterizationSettings(\n       image_size=256,\n       blur_radius=0.0,\n       faces_per_pixel=1,\n    )\n    lights = PointLights(device=device, location=((2.0, 2.0, -2.0),))\n    phong_renderer = MeshRenderer(\n       rasterizer=MeshRasterizer(\n           cameras=cameras,\n           raster_settings=raster_settings\n       ),\n       shader=HardPhongShader(device=device, cameras=cameras, lights=lights)\n    )\n    ```", "```py\n    R, T = look_at_view_transform(distance, elevation, azimuth, device=device)\n    # Render the sofa providing the values of R and T.\n    silhouette = silhouette_renderer(meshes_world=sofa_mesh, R=R, T=T)\n    image_ref = phong_renderer(meshes_world=sofa_mesh, R=R, T=T)\n    ```", "```py\n    plt.figure(figsize=(10, 10))\n    plt.subplot(1, 2, 1)\n    plt.imshow(silhouette.squeeze()[..., 3])\n    plt.grid(False)\n    plt.subplot(1, 2, 2)\n    plt.imshow(image_ref.squeeze())\n    plt.grid(False)\n    plt.savefig(args.save_path)\n    ```", "```py\ndatasets/pix3d/download_pix3d.sh\n```", "```py\npython tools/train_net.py \\\n--config-file configs/pix3d/meshrcnn_R50_FPN.yaml \\\n--eval-only MODEL.WEIGHTS /path/to/checkpoint_file\n```", "```py\npython tools/train_net.py \\\n--config-file configs/pix3d/meshrcnn_R50_FPN.yaml \\\n--eval-only MODEL.WEIGHTS /path/to/checkpoint_file\n```", "```py\ndatasets/shapenet/download_shapenet.sh\n```", "```py\npython tools/preprocess_shapenet.py \\\n--shapenet_dir /path/to/ShapeNetCore.v1 \\\n--shapenet_binvox_dir /path/to/ShapeNetCore.v1.binvox \\\n--output_dir ./datasets/shapenet/ShapeNetV1processed \\\n--zip_output\n```", "```py\npython tools/train_net_shapenet.py --num-gpus 8 \\\n--config-file configs/shapenet/voxmesh_R50.yaml\n```", "```py\npython tools/train_net_shapenet.py --num-gpus 8 \\\n--config-file configs/shapenet/voxmesh_R50.yaml\n```"]