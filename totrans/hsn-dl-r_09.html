<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Deep Learning for Natural Language Processing</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, you will learn how to create document summaries. We will begin by removing parts of documents that should not be considered and tokenizing the remaining text. Next, we will apply embeddings and create clusters. These clusters will then be used to make document summaries. Also, we will learn how to use <strong>restricted Boltzmann machines</strong> (<strong>RBMs</strong>) as building blocks to create deep belief networks for topic modeling. We will begin with coding the RBM and defining the Gibbs sampling rate, contrastive divergence, and free energy for the algorithm. We will conclude by compiling multiple RBMs to create a deep belief network.</p>
<p>This chapter covers the following topics:</p>
<ul>
<li style="font-weight: 400">Formatting data using tokenization</li>
<li>Cleaning text to remove noise</li>
<li>Applying word embeddings to increase usable data</li>
<li>Clustering data into topic groups</li>
<li>Summarizing documents using model results</li>
<li>Creating an RBM</li>
<li>Defining the Gibbs sampling rate</li>
<li>Speeding up sampling with contrastive divergence </li>
<li>Computing free energy for model evaluation</li>
<li>Stacking RBMs to create a deep belief network</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Formatting data using tokenization</h1>
                </header>
            
            <article>
                
<p>The first step we will take to begin analyzing text is loading text files and then tokenizing our data by transforming the text from sentences into smaller pieces, such as words or terms. A text object can be tokenized in a number of ways. In this chapter, we will tokenize text into words, although other sized terms could also be tokenized. These are referred to as n-grams, so we can get two-word terms (2-grams), three-word terms, or a term of any arbitrary size.</p>
<p>To get started with the process of creating one-word tokens from our text objects, we will use the following steps:</p>
<ol>
<li>Let's load the libraries that we will need. For this project, we will use <kbd>tidyverse</kbd> for data manipulation, <kbd>tidytext</kbd> for special functions to manipulate text data, <kbd>spacyr</kbd> for extracting text metadata, and <kbd>textmineR</kbd> for word embeddings. To load these libraries, we run the following code:</li>
</ol>
<pre style="padding-left: 60px">library(tidyverse)<br/>library(tidytext)<br/><span class="hljs-keyword">library</span><span>(spacyr)<br/>library(textmineR)</span></pre>
<p style="padding-left: 60px">In this chapter, the data that we will use will be the 20 Newsgroups dataset. This consists of pieces of text that come from one of 20 Newsgroups. The format of the data that we will pull in has a unique ID, the group the text belongs to, and the group.</p>
<ol start="2">
<li>Let's read in the data using the following code:</li>
</ol>
<pre style="padding-left: 60px">twenty_newsgroups &lt;- read_csv("http://ssc.wisc.edu/~ahanna/20_newsgroups.csv")</pre>
<p style="padding-left: 60px">After running this code, you should see the <kbd>twenty_newsgroups</kbd> object appear in your <kbd>Environment</kbd> window. The object has 11,314 rows and 3 columns.</p>
<ol start="3">
<li>Let's take a look at a sample of the data. In this case, let's print the first row of data to our console. We look at the first row of data by running the following code:</li>
</ol>
<pre style="padding-left: 60px">twenty_newsgroups[1,]</pre>
<p style="padding-left: 60px">After running this code, you will see the following printed to your console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9bf0f4f8-febd-470f-95cd-97b18ade0ee1.png"/></p>
<ol start="4">
<li>Now, let's break this text into tokens. Tokens are some atomic portion of the text character string that we see in the preceding screenshot. In this case, we will break this string into word tokens. The final result will be a row for each word listed, alongside the ID and newsgroup ID. We tokenize the text data using the following code:</li>
</ol>
<pre style="padding-left: 60px">word_tokens &lt;- twenty_newsgroups %&gt;%<br/>  unnest_tokens(word, text)</pre>
<p style="padding-left: 60px">After running this code, we can see that our data object has grown substantially. We now have 3.5 million rows when we previously only had 11,000, since each word now gets its own row. </p>
<ol start="5">
<li>Let's take a quick look at term frequency, now that we have each word separated out into its own line. In this step, we can begin to see whether certain terms are used more than others with the text included in this dataset. To plot the frequency of each term in the data, we will use the following code:</li>
</ol>
<pre style="padding-left: 60px">word_tokens %&gt;%<br/>  group_by(word) %&gt;%<br/>  summarize(word_count = n()) %&gt;%<br/>  top_n(20) %&gt;%<br/>  ggplot(aes(x=reorder(word, word_count), word_count)) +<br/>  xlab("word") +<br/> geom_col() +<br/>  coord_flip() </pre>
<p style="padding-left: 60px">After running this code, we will see the following plot generated:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/aa636335-74b1-4319-83c6-225e5edcd427.png"/></p>
<p>We have successfully taken some text and divided it into tokens. However, we can see from the plot that terms such as <span class="packt_screen">the</span>, <span class="packt_screen">to</span>, <span class="packt_screen">of</span>, and <span class="packt_screen">a</span> are most frequent. These types of words are often bundled into a collection of terms referred to as <strong>stop words</strong>. Next, we will learn how to remove these types of terms that have no information value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Cleaning text to remove noise</h1>
                </header>
            
            <article>
                
<p>The next step we will take to prepare for text analysis is doing some preliminary cleaning. This is a common way to get started, regardless of what machine learning method will be applied later. When working with text, there are several terms and patterns that will not provide meaningful information. Some of these terms are generally not useful and steps to remove these pieces of text data can be used every time, while others will be more context-dependent.</p>
<p>As previously noted, there are collections of terms referred to as stop words. These terms have no information value and can usually be removed. To remove stop words from our data, we use the following code:</p>
<pre>word_tokens &lt;- word_tokens %&gt;%<br/>  filter(!word %in% stop_words$word)</pre>
<p>After running the preceding code, our row count goes down from 3.5 million to 1.7 million. In effect, our data (<kbd>word_tokens</kbd>) has almost been cut in half by removing all the stop words. Let's run the plot we ran earlier to see which terms are most frequent now. We can identify the term frequency as we did before with the following lines of code:</p>
<pre>word_tokens %&gt;%<br/>  group_by(word) %&gt;%<br/>  summarize(word_count = n()) %&gt;%<br/>  top_n(20) %&gt;%<br/>  ggplot(aes(x=reorder(word, word_count), word_count)) +<br/>  xlab("word") +<br/>  geom_col() +<br/>  coord_flip() </pre>
<p>After running this chunk of code, the following plot is generated:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/00d142aa-0671-49a2-bda3-0b8928435982.png"/></p>
<p>In this plot, we can see that terms such as <span class="packt_screen">the</span>, <span class="packt_screen">to</span>, <span class="packt_screen">of</span>, and <span class="packt_screen">a</span> are now removed. However, we also now see that there are some numbers showing up as frequent terms. This could be context-dependent and there may be cases where pulling numbers from text is very important for a project. However, here we will focus on actual words and will remove all terms that contain non-alphabetic characters. We can accomplish this by using some regular expressions, also known as <strong>regex</strong>. We can remove the terms that do not contain any characters from the alphabet by using the following code:</p>
<pre>word_tokens &lt;- word_tokens %&gt;%<br/>  filter(str_detect(word, "^[a-z]+[a-z]$"))</pre>
<p>After running this regex code, we can run the same plot code again, as we did previously. When we do, we generate a plot that looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7a0bb9f6-e4dc-4bee-b4db-0a4ab31c4404.png"/></p>
<p>Based on this plot, we see our top twenty terms are all words, including one possible acronym (<span class="packt_screen">nntp</span>). With our data object now reduced to 1.4 million rows, which includes only terms that start and end with characters from the alphabet, we are ready to move on to the next step where we will use embeddings to add extra context to each term.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Applying word embeddings to increase usable data</h1>
                </header>
            
            <article>
                
<p>Extracting terms from text is a good starting point for text analysis. With the text tokens we have created so far, we can compare term frequency for different categories, which begins to tell us a story about the content that dominates a particular newsgroup. However, the term alone is just one part of the overall information we can glean from a given term. The previous plot contained <kbd>people</kbd> and, of course, we know what this word means, although there are multiple nuanced details connected to this term. For instance, <kbd>people</kbd> is a noun. It is similar to terms such as <em>person</em> and <em>human</em> and is also related to a term such as <em>household</em>. All of these details for <kbd>people</kbd> could be important but, by just extracting the term, we cannot directly derive these other details. This is where embeddings are especially helpful.</p>
<p>Embeddings, in the context of natural language processing, are pre-trained neural networks that perform the type of mapping just described. We can use these embeddings to match parts of speech to terms, as well as to find the lexical distance between words. Let's get started by looking at the parts-of-speech embeddings. To examine the parts of speech for every term in our text dataset, we run the following code:</p>
<pre>spacy_install()<br/><br/>spacy_initialize(model = "en_core_web_sm")<br/><br/>spacy_parse(twenty_newsgroups$text[1], entity = TRUE, lemma = TRUE)</pre>
<p>Using the preceding code, we first install <kbd>spacy</kbd> on our machine. Next, we initialize <kbd>spacy</kbd> using a small (<kbd>sm</kbd>) English (<kbd>en</kbd>) model that is trained on web text (<kbd>web</kbd>) for the core <kbd>spacy</kbd> elements: <span>named entities, part-of-speech tags, and syntactic dependencies. Afterward, we apply the model to the first piece of text in our dataset. Once we do this, we will see the following results printed to the console:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a3b0807f-e1dd-4a72-aeb0-c1e2488d292c.png"/></p>
<p>In the preceding example, we see that <kbd>spacy</kbd> stores each token separately with a token ID and a sentence ID. The three additional pieces of data supplied are listed next to each token. Let's look at the example for the <kbd>11</kbd> <span>token ID. </span><span>In this case, <kbd>Cubs</kbd></span><span>, which the model has identified as a part of speech, is a proper noun and the named entity type is</span> <strong>organization</strong><span>. We see the</span> <kbd>ORG_B</kbd><span> </span><span>code, </span><span>which means this token begins with the name of an organization. In this case, the one-term begins and ends with the name of the organization.</span></p>
<p><span>Let's look at a few other examples. If you scroll down the results in your console, you should find a section that looks like the following output:</span></p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f268edfd-aa45-4734-ac1d-df1333588863.png"/></p>
<p>In the preceding screenshot, we see additional information that <kbd>spacy</kbd> can identify. Let's look at lines <kbd>76</kbd> and <kbd>77</kbd>. We see that the term used in the text is <kbd>won't</kbd>. However, the <kbd>spacy</kbd> model used lemmatization to break up this contraction. Of course, <kbd>won't</kbd> is just a contracted form of <kbd>will not</kbd> and the model has split out the two terms that are part of the contracted term. In addition, the part of the speech for each term is included. Another example is lines <kbd>90</kbd> and <kbd>91</kbd>. Here, the terms <kbd>this</kbd> and <kbd>season</kbd> are adjacent and the model correctly identifies these two terms together to refer to a particular date part of the speech, which means that it is not <kbd>last season</kbd> or <kbd>next season</kbd>, but <kbd>this season</kbd>. In the named entities column, <kbd>this</kbd> has a <kbd>DATE_B</kbd> <span>tag,</span><span> </span><span>which means the term refers to a date and this term is the beginning of this particular date-type. Similarly,</span> <kbd>season</kbd><span> has a tag of</span> <kbd>DATE_I</kbd><span>, which means that it refers to a date-type piece of data and the token is inside the entity. We know from these two tags that</span> <kbd>this</kbd> and <kbd>season</kbd><span> are related and together refer to a specific point in time.</span></p>
<p>Another way we can use word embedding is to cluster our text data into topic groups. Topic grouping will result in a data object with lists of terms that co-occur near each other in the text. Through this process, we can see which topics are being discussed the most in the text data that we are analyzing. We will create topic group clusters next.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Clustering data into topic groups</h1>
                </header>
            
            <article>
                
<p>Let's use word embeddings to find all semantically similar words. To do this, we will use the <kbd>textmineR</kbd> package to create a skip-gram model. The objective of the skip-gram model is to look for terms that occur often within a given window of another term. Since these terms are so frequently close to each other within sentences in our text, we can conclude they have some connection to each other. We will start by using the following steps:</p>
<ol>
<li>To begin building our skip-gram model, we first create a term co-occurrence matrix by running the following code:</li>
</ol>
<pre style="padding-left: 60px">tcm &lt;- CreateTcm(doc_vec = twenty_newsgroups$text,<br/>                 skipgram_window = 10,<br/>                 verbose = FALSE,<br/>                 cpus = 2)</pre>
<p style="padding-left: 60px">After running the code, you will have a <kbd>sparse</kbd> matrix in your environment window. The matrix has every possible term along both dimensions, as well as a value at the intersection of the terms if they occur together within the skip-gram window, which in this case is <kbd>10</kbd>. An example of what a portion of this matrix looks like can be seen here:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b912df1f-e53f-4c78-83a6-fad328df1c5b.png" style="width:26.92em;height:8.67em;"/></p>
<ol start="2">
<li>Next, we will fit a<span> </span><span><strong>Latent Dirichlet allocation</strong> (<strong>LDA</strong>) model on the text co-occurrence matrix that we just made. For our model, we will choose to create 20 topics and will have the model perform 500 Gibbs iterations, setting the <kbd>burning</kbd> value to <kbd>200</kbd>, which is the number of samples we will discard first. We will set <kbd>calc_coherence</kbd> to <kbd>TRUE</kbd> to include this metric. <kbd>coherence</kbd> is the relative distance between terms for a topic and we will use this distance value to rank the strength of the topics we have found. We define our LDA model by running the following code:</span></li>
</ol>
<pre style="padding-left: 60px">embeddings &lt;- FitLdaModel(dtm = tcm,<br/>                          k = 20,<br/>                          iterations = 500,<br/>                          burnin = 200,<br/>                          calc_coherence = TRUE)</pre>
<ol start="3">
<li>Our next step will be to get the top terms for each topic. We will use <kbd>phi</kbd>, which represents a distribution of words over topics as the topics and the argument, <kbd>M</kbd>, to choose how many terms to include in each topic cluster. We can retrieve our top terms per topic by running the following code:</li>
</ol>
<pre style="padding-left: 60px">embeddings$top_terms &lt;- GetTopTerms(phi = embeddings$phi,<br/>                                    M = 5)</pre>
<ol start="4">
<li>We will take our topics and top terms and add in the <kbd>coherence</kbd> score along with a <kbd>prevalence</kbd> score, which shows how often the terms occur in the entire text corpus we are analyzing. We can assemble this summary data object by running the following code:</li>
</ol>
<pre style="padding-left: 60px">embeddings$summary &lt;- data.frame(topic = rownames(embeddings$phi),<br/>                                 coherence = round(embeddings$coherence, 3),<br/>                                 prevalence = round(colSums(embeddings$theta), 2),<br/>                                 top_terms = apply(embeddings$top_terms, 2, function(x){<br/>                                   paste(x, collapse = ", ")<br/>                                 }),<br/>                                 stringsAsFactors = FALSE)</pre>
<ol start="5">
<li>Now that we have created this summary data object, we can look at the top five topics by the <kbd>coherence</kbd> value. We can identify which topics have the most terms that are relatively close to each other by running the following code:</li>
</ol>
<pre style="padding-left: 60px">embeddings$summary[order(embeddings$summary$coherence, decreasing = TRUE),][1:5,]</pre>
<p>When we run the preceding code, we see the top five topics that we have identified in the text object. You will see the following topics printed to your console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4f671001-4c4e-45d0-96bb-0ab8278c8584.png" style="width:42.17em;height:8.17em;"/></p>
<p>We have loaded in text data, extracted terms from the text, used a model to identify associated information for the terms<span>—s</span>uch as named entity details and part of speech<span>—</span>and organized terms according to topics discovered in the text. Next, we will reduce our text objects by using modeling to summarize documents in our text.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summarizing documents using model results</h1>
                </header>
            
            <article>
                
<p>In this last step, before moving on to building our own model, we will use the <kbd>textrank</kbd> package to summarize the text. The approach this algorithm uses to summarize text is to look for a sentence with the most words that are also used in other sentences in the text data. We can see how this type of sentence would be a good candidate for summarizing the text since it contains many words found elsewhere. To get started, let's select a piece of text from our data:</p>
<ol>
<li>Let's view the text in row <kbd>400</kbd> by running the following code:</li>
</ol>
<pre style="padding-left: 60px">twenty_newsgroups$text[400]</pre>
<p style="padding-left: 60px">When we run this line of code, we will see the following piece of text printed to the console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/456bb460-e652-457c-b8e6-3964dff1fe43.png" style="width:83.50em;height:10.50em;"/></p>
<p style="padding-left: 60px">In this email, we can see that the subject matter regards objecting to someone else's email because it is off-topic.</p>
<ol start="2">
<li>Let's see which sentence the <kbd>textrank</kbd> algorithm will extract to summarize the text. To get started, we will first perform tokenization on the text. However, unlike earlier where we created word tokens, this time we will create sentence tokens. In addition, we will use the row numbers for each sentence extracted as the sentence ID. To create sentence tokens from our text, we run the following code:</li>
</ol>
<pre style="padding-left: 60px">sentences &lt;- tibble(text = twenty_newsgroups$text[400]) %&gt;%<br/>  unnest_tokens(sentence, text, token = "sentences") %&gt;%</pre>
<pre style="padding-left: 60px">  mutate(id = row_number()) %&gt;%<br/>  select(id, sentence)</pre>
<ol start="3">
<li>Next, we will create word tokens as we did previously. Remember that the reason we create sentence and word tokens is because we need to see which words occur in the most sentences and, of those words, which sentence contains the most frequently occurring words. To create the data object with one word per row, we run the following code:</li>
</ol>
<pre style="padding-left: 60px">words &lt;- sentences %&gt;%<br/>  unnest_tokens(word, sentence)</pre>
<ol start="4">
<li>Next, we run the <kbd>textrank_sentences</kbd> function, which calculates the best summary sentences in the way previously described. We calculate the <kbd>textrank</kbd> score, which measures which sentences best summarize the text, by running the following code:</li>
</ol>
<pre style="padding-left: 60px">article_summary &lt;- textrank_sentences(data = sentences, terminology = words)</pre>
<p style="padding-left: 60px">We have now ranked the sentences. If we view the summary, we can see the top five sentences by default. However, in this case, let's start by looking at the very top-ranked sentence and see how well that does at summarizing the overall text.</p>
<ol start="5">
<li>To look at just the top-ranked sentence, we first have to look at the first object in the returned list, which is a data frame of the sentences with their corresponding <kbd>textrank</kbd> score. Next, we arrange them by descending <kbd>textrank</kbd> score to select the highest-rated sentence. Afterward, we select the top row and extract just the sentence data. To print the top-ranked sentence based on the <kbd>textrank</kbd> algorithm, we run the following code:</li>
</ol>
<pre style="padding-left: 60px">article_summary[["sentences"]] %&gt;%<br/>  arrange(desc(textrank)) %&gt;% <br/>  top_n(1) %&gt;%<br/>  pull(sentence)</pre>
<p style="padding-left: 60px">After running this code, you will see the following console output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/90529700-5a88-4e79-8489-5ce1022ff9e0.png" style="width:37.00em;height:9.33em;"/></p>
<p>The sentence selected is <span class="packt_screen">restrict the discussion to appropriate newsgroups</span>. If we read the entire text again, we can see that this sentence does capture the essence of what the writer is communicating. In fact, if the email only had this line it would convey almost the same information. In this way, we can confirm that the <kbd>textrank</kbd> algorithm performed well and that the selected sentence is a good summary of the entire text.</p>
<p>Now that we have covered some of the essential text analytics tools offered by various R packages, we will proceed with creating our own deep learning text model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating an RBM</h1>
                </header>
            
            <article>
                
<p>So far, we have extracted elements from text, added metadata, and created term clusters to discover latent topics. We will now identify latent features by using a deep learning model known as an RBM. As you may recall, we have discovered latent topics in the text by looking for term co-occurrence within a given window size. In this case, we will go back to using a neural network approach. The RBM is half the typical neural network. Instead of taking data through hidden layers to an output layer, the RBM model just takes the data to the hidden layers and this is the output. The end result is similar to factor analysis or principal component analysis. Here, we will begin the process of finding each of the 20 Newsgroups in the dataset and throughout the rest of this chapter, we will make modifications to the model to improve its performance.</p>
<p>To get started with building our RBM, we will need to load two libraries. The first library will be <kbd>tm</kbd>, which is used for text mining in R, and has functions for creating a document-term matrix and performing text cleanup. The other library that we will need is <kbd>deepnet</kbd>, which has a function for the RBM. To load these two libraries, we run the following code:</p>
<pre>library(tm)<br/>library(deepnet)</pre>
<p>Next, we will take our text data and create a corpus, which in this case will place the contents from each newsgroup email into a separate list element. From there, we will remove some non-informative elements. We will also cast all text to lowercase to decrease the unique term count, as well as group like terms together regardless of their letter case. Afterward, we will cast the remaining terms to a document-term matrix, where all the terms make up one dimension of the matrix and all the documents make up the other dimension and the represented value in the matrix if the term is present in the document. We will also use <strong>term frequency-inverse document frequency</strong> (<strong>tf-idf</strong>) weighting.</p>
<p>In this case, the value in the matrix will not be binary but rather a float representing the uniqueness of the term within the document, down-weighting terms that occur frequently in all documents and giving more weight to terms that are only present in one or some documents but not all. To perform these steps and prepare our text data to be inputted into an RBM model, we run the following code:</p>
<pre>corpus &lt;- Corpus(VectorSource(twenty_newsgroups$text))<br/><br/>corpus &lt;- tm_map(corpus, content_transformer(tolower))<br/>corpus &lt;- tm_map(corpus, removeNumbers)<br/>corpus &lt;- tm_map(corpus, removePunctuation)<br/>corpus &lt;- tm_map(corpus, removeWords, c("the", "and", stopwords("english")))<br/>corpus &lt;- tm_map(corpus, stripWhitespace)<br/><br/>news_dtm &lt;- DocumentTermMatrix(corpus, control = list(weighting = weightTfIdf))<br/>news_dtm &lt;- removeSparseTerms(news_dtm, 0.95)</pre>
<p>We will now split our data into <kbd>train</kbd> and <kbd>test</kbd> sets as with any modeling exercise. In this case, we will create our <kbd>train</kbd> and <kbd>test</kbd> sets by running the following code:</p>
<pre>split_ratio &lt;- floor(0.75 * nrow(twenty_newsgroups))<br/><br/>set.seed(614)<br/>train_index &lt;- sample(seq_len(nrow(twenty_newsgroups)), size = split_ratio)<br/><br/>train_x &lt;- news_dtm[train_index,]<br/>train_y &lt;- twenty_newsgroups$target[train_index]<br/>test_x &lt;- news_dtm[-train_index,]<br/>test_y &lt;- twenty_newsgroups$target[-train_index]</pre>
<p>With the data in the proper format and split into <kbd>train</kbd> and <kbd>test</kbd> sets, we can now train our RBM model. Training the model is quite straightforward and there are not too many parameters to configure. For now, we will modify a few arguments and make changes to others as we progress through the chapter. To start, we will train a starter RBM model by running the following code:</p>
<pre>rbm &lt;- rbm.train(x = as.matrix(train_x), hidden = 20, numepochs = 100)</pre>
<p>In the preceding code, we set the <kbd>hidden</kbd> layers to the number of newsgroups to see if there is enough latent information to map the text to the newsgroups. We start with <kbd>100</kbd> rounds and leave everything else as a default.</p>
<p>We can now explore the latent features found in the text. We use our trained model to perform this task by passing in data as input, which results in inferred hidden units being produced as output. We infer the hidden units for the <kbd>test</kbd> data by running the following code:</p>
<pre>test_latent_features &lt;- rbm.up(rbm, as.matrix(test_x))</pre>
<p>After running this code, we have defined the latent feature space for the <kbd>test</kbd> data. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining the Gibbs sampling rate</h1>
                </header>
            
            <article>
                
<p>Gibbs sampling plays a key role in constructing an RBM, so we will take a moment here to define this sampling type. We will briefly walk through a couple of quick concepts that lead to how to perform Gibbs sampling and why it matters for this type of modeling. With RBM models, we are first using a neural network to map our input or visible units to hidden units, which can be thought of as latent features. After training our model, we want to either take a new visible unit and define the probability that it belongs to the hidden units in the model, or do the reverse. We also want this to be computationally efficient, so we use a Monte Carlo approach.</p>
<p>Monte Carlo methods involve sampling random points to approximate an area or distribution. A classic example involves drawing a 10-by-10 inch square and inside this square draw a circle. We know that a circle with a 10-inch diameter has an area of 78.5 inches. Now, if we use a random number generator to choose float pairs between 0 and 10 and do this 20 times and plot the points, we will likely end up with around 15 points in the circle and 5 outside the circle. If we use just these points, then we would approximate the area is 75 inches. Now, if we try this again with a less conventional shape but something with many curves and angles, then it would be much more difficult to calculate the area. However, we could use the same approach to approximate the area. In this way, Monte Carlo approaches work well when a distribution is difficult or computational costly to define precisely, which is the case with our RBM model.</p>
<p>Next, a Markov Chain is a technique in defining conditional probability that only takes into account the event that just preceded the event probability we are trying to predict, rather than events that happened two or more steps back. This is a very simple form of conditional probability. A classic example for explaining this concept is the game Chutes and Ladders. In this game, there are 100 squares and a player rolls a six-sided die to determine the number of spaces to move, with the object being to get to square 100. Along the way, a square may contain a slide that will move the player backward a certain number of squares, or a ladder that will move the player forward a certain number of squares.</p>
<p>When determining the likelihood of landing on a given square, the only thing that matters is the previous roll that resulted in the player landing on a certain square. Whichever combination of rolls got the player to that point does not have any impact on the probability of reaching a certain square based on the square the player is currently on.</p>
<p>For context, we will discuss these two concepts briefly because they are both involved in Gibbs sampling. This type of sampling is a Monte Carlo Markov Chain method, which means that we start from an initial state and afterward we predict the likelihood that a certain event, <kbd>x</kbd>, happens given another event, <kbd>y</kbd>, and vice versa. By calculating this type of back-and-forth conditional probability over a certain number of samples, we efficiently approximate the probability that a given <kbd>visible</kbd> unit belongs to a given <kbd>hidden</kbd> unit. We can perform a few very simple examples of sampling from a Gibbs distribution. In this example, we will create a function with an argument, <kbd>rho</kbd>, as a coefficient value to modify the given term when calculating the value for the other variable, while in our RBM model the learned weights and the bias term perform this function. Let's create a sampler using the following steps:</p>
<ol>
<li>Let's first define a very simple Gibbs sampler to understand the concept by running the following code:</li>
</ol>
<pre style="padding-left: 60px">gibbs&lt;-function (n, rho) <br/>{<br/>  mat &lt;- matrix(ncol = 2, nrow = n)<br/>  x &lt;- 0<br/>  y &lt;- 0<br/>  mat[1, ] &lt;- c(x, y)<br/>  for (i in 2:n) {<br/>    x &lt;- rnorm(1, rho * y, sqrt(1 - rho^2))<br/>    y &lt;- rnorm(1, rho * x, sqrt(1 - rho^2))<br/>    mat[i, ] &lt;- c(x, y)<br/>  }<br/>  mat<br/>}</pre>
<p style="padding-left: 60px">Now that we have defined the function, let's calculate two separate 10 x 2 matrices by choosing two different values for <kbd>rho</kbd>.</p>
<ol start="2">
<li>We calculate our first 10 x 2 matrix by running the following code using a value of <kbd>0.75</kbd> for <kbd>rho</kbd>:</li>
</ol>
<pre style="padding-left: 60px">gibbs(10,0.75)</pre>
<ol start="3">
<li>Next, we calculate a 10 x 2 matrix using a value of <kbd>0.03</kbd> for <kbd>rho</kbd>, using the following line of code:</li>
</ol>
<pre style="padding-left: 60px">gibbs(10,0.03)</pre>
<p>After running each of these, you should see a 10 x 2 matrix printed to your console. This function involves drawing random values from a normal distribution, so the matrix printed to your console will be slightly different. However, you will see the way the values are generated iteratively using the values from the previous iteration to determine the value in the current iteration. We see here how the Monte Carlo randomness is employed in calculating our value along with the Markov Chain conditional probability. Now, with an understanding of Gibbs sampling, we will explore contrastive divergence, which is a way we can use what we learned about Gibbs sampling to modify our model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Speeding up sampling with contrastive divergence</h1>
                </header>
            
            <article>
                
<p>Before proceeding, we need to change up the dataset being used. While the 20 Newsgroups dataset has worked well up until this point for all the concepts on text analysis, it becomes less usable as we try to really tune our model to predict latent features. All the additional changes that we will do next actually have minimal impact on the model when using the 20 Newsgroups, so we will switch to the spam versus ham dataset, which is similar. However, instead of involving emails to a newsgroup, these are SMS text messages. In addition, instead of the target variable being a given newsgroup, the target is either that the message is spam or a legitimate text message. </p>
<p>Contrastive divergence is the argument that allows us to leverage what we learned about Gibbs sampling. The value that we pass to this argument in the model will adjust how many times the Gibbs sampling is performed. In other words, this controls the length of the Markov Chain. The lower the value, the faster each round of the model will be. If the value is higher, then each round is computationally more costly, although the model may converge more quickly. In the following steps, we can train a model with three different values for c<span>ontrastive divergence to see how adjusting this argument affects the model:</span></p>
<ol>
<li>To begin, we will load in the spam versus ham dataset using the following code:</li>
</ol>
<pre style="padding-left: 60px">spam_vs_ham &lt;- read.csv("spam.csv")</pre>
<ol start="2">
<li>Next, we will move our target variables to a vector, <kbd>y</kbd>, and the predictor text data to a variable, <kbd>x</kbd>. Afterward, we will perform some basic text preprocessing by removing special characters and one- and two-character words, as well as removing any white space. We define our target variable and predictor variables, along with cleaning the text, by running the following code:</li>
</ol>
<pre style="padding-left: 60px">y &lt;- if_else(spam_vs_ham$v1 == "spam", 1, 0)<br/>x &lt;- spam_vs_ham$v2 %&gt;% <br/>  str_replace_all("[^a-zA-Z0-9/:-_]|\r|\n|\t", " ") %&gt;% <br/>  str_replace_all("\b[a-zA-Z0-9/:-]{1,2}\b", " ") %&gt;%<br/>  str_trim("both") %&gt;%<br/>  str_squish()</pre>
<ol start="3">
<li>Next, we convert this cleaned up text into a <kbd>corpus</kbd> data object and then into a document-term matrix. We convert our text data into a suitable format for modeling by running the following code:</li>
</ol>
<pre style="padding-left: 60px">corpus &lt;- Corpus(VectorSource(x))<br/>dtm &lt;- DocumentTermMatrix(corpus)</pre>
<ol start="4">
<li>Next, let's divide our data into <kbd>train</kbd> and <kbd>test</kbd> sets, exactly as we did with the 20 Newsgroups dataset. We get our data divided and ready for modeling using the following code:</li>
</ol>
<pre style="padding-left: 60px">split_ratio &lt;- floor(0.75 * nrow(dtm))<br/><br/>set.seed(614)<br/>train_index &lt;- sample(seq_len(nrow(dtm)), size = split_ratio)<br/><br/>train_x &lt;- dtm[train_index,]<br/>train_y &lt;- y[train_index]<br/>test_x &lt;- dtm[-train_index,]<br/>test_y &lt;- y[-train_index]</pre>
<ol start="5">
<li>Now that all of our data is prepared, let's run our three models and see how they compare. We run a quick version of the three RBM models to evaluate the impact of adjusting the contrastive divergence value by running the following code:</li>
</ol>
<pre style="padding-left: 60px">rbm3 &lt;- rbm.train(x = as.matrix(train_x),hidden = 100,cd = 3,numepochs = 5)<br/>rbm5 &lt;- rbm.train(x = as.matrix(train_x),hidden = 100,cd = 5,numepochs = 5)<br/>rbm1 &lt;- rbm.train(x = as.matrix(train_x),hidden = 100,cd = 1,numepochs = 5)</pre>
<p>To measure how much change this argument has had on the model, we will use the free energy values in the model object.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Computing free energy for model evaluation</h1>
                </header>
            
            <article>
                
<p>RBMs belong to a class of energy-based models. These use a free energy equation that is analogous to the cost function in other machine learning algorithms. Just like a cost function, the objective is to minimize the free energy values. A lower free energy value equates to a higher probability that the visible unit variables are being described by the hidden units and a higher value equates to a lower likelihood. </p>
<p>Let's now look at the three models we just created and compare free energy values for these models. We compare the free energy to identify which model is performing better by running the following code:</p>
<pre>rbm5$e[1:10]<br/>rbm3$e[1:10]<br/>rbm1$e[1:10]</pre>
<p>After running this code, an output similar to the following will be printed to your console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/82ac6d3b-213c-4147-9012-f77cd576d146.png"/></p>
<p>In this case, using just one round of Gibbs sampling produces the best performing model in terms of reducing free energy in the quickest way.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Stacking RBMs to create a deep belief network</h1>
                </header>
            
            <article>
                
<p>RBM models are a neural network with just two layers: the input, that is, the visible layer, and the hidden layer with latent features. However, it is possible to add additional hidden layers and an output layer. When this is done within the context of an RBM, it is referred to as a <strong>deep belief network</strong>. In this way, deep belief networks are like other deep learning architectures. For a deep belief network, each hidden layer is fully connected meaning that it learns the entire input.</p>
<p>The first layer is the typical RBM, where latent features are calculated from the input units. In the next layer, the new hidden layer learns the latent features from the previous hidden layer. This, in turn, can lead to an output layer for classification tasks.</p>
<p>Implementing a deep belief network uses a similar syntax to what was used to train the RBM. To get started, let's first perform a quick check of the latent feature space from the RBM we just trained. To print a sample of the latent feature space from the model, we use the following code:</p>
<pre>train_latent_features &lt;- rbm.up(rbm1, as.matrix(train_x))<br/>test_latent_features &lt;- rbm.up(rbm1, as.matrix(test_x))</pre>
<p>In the preceding code, we use the <kbd>up</kbd> function to generate a matrix of latent features using the model that we just fit. The <kbd>up</kbd> function takes as input an RBM model and a matrix of visible units and outputs a matrix of hidden units. The reverse is also possible. The <kbd>down</kbd> function takes a matrix of hidden units as input and outputs visible units. Using the preceding code, we will see an output like the following printed to the console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e4c44db4-9007-4aae-b297-e035f4641a06.png"/></p>
<p>We can see variance in the feature space of this first layer. To prepare for the next step, we imagine using this matrix now as input to another RBM that will further learn features. In this way, we can code our deep belief network using an almost identical syntax to the syntax used for training the RBM. The exception will be that for the hidden layer argument, rather than a single value representing the number of units in a single hidden layer, we can now use a vector of values that represent the number of units in each successive hidden layer. For our deep belief network, we will start with <kbd>100</kbd> units, just like in our RBM.</p>
<p>We will then reduce this to <kbd>50</kbd> units in the next layer and <kbd>10</kbd> units in the layer after that. The other difference is that we now have a target variable. While an RBM is an unsupervised, generative model, we can use our deep belief network to perform a classification task. We train our deep belief network using the following code:</p>
<pre>dbn &lt;- dbn.dnn.train(x = as.matrix(train_x), y = train_y, hidden = c(100,50,10), cd = 1, numepochs = 5)</pre>
<p>With the deep belief network trained, we can now make predictions using our model. We perform this prediction task in a similar way to how we generate predictions for most machine learning tasks. However, in this case, we will use the <kbd>nn.predict</kbd> function to use our trained neural network to predict whether the new test input should be classified as spam or a legitimate text. We make a prediction on the <kbd>test</kbd> data using the following code:</p>
<pre>predictions &lt;- nn.predict(dbn, as.matrix(test_x))</pre>
<p>We now have the probability values that tell us whether a given message is or is not spam. The probabilities are currently within a constrained range; however, we can still use it. Let's make a cut in the probabilities and assign <kbd>1</kbd> for those above the threshold signifying that the message is predicted to be spam, and everything under the cut point will receive a value of <kbd>0</kbd>. After making this dividing line and creating a vector of binary values, we can create a confusion matrix to see how well our model performed. We create our binary variables and then see how well our model performed by running the following code:</p>
<pre>pred_class &lt;- if_else(predictions &gt; 0.3, 1, 0)<br/>table(test_y,pred_class)</pre>
<p>After running the preceding code, we will see the following output to our console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/42061ee5-9adc-45eb-8058-900c569ea209.png"/></p>
<p>As we can see, even this very simple implementation of a deep belief network has performed fairly well. From here, additional modification can be made to the number of hidden layers, units in these layers, the output activation function, learning rate, momentum, and dropout, along with the contrastive divergence and the number of epochs or rounds. </p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we covered a number of methods for analyzing text data. We started with techniques for extracting elements from text data, such as taking a sentence and breaking it into tokens and comparing term frequency, along with collecting topics and identifying the best summary sentence and extracting these from the text. Next, we used some embedding techniques to add additional details to our data, such as parts of speech and named entity recognition. Lastly, we used an RBM model to find latent features in the input data and stacked these RBM models to perform a classification task. In the next chapter, we will look at using deep learning for time series tasks, such as predicting stock prices, <span>in particular</span>.</p>


            </article>

            
        </section>
    </body></html>