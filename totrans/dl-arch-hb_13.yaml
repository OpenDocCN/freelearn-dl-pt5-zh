- en: '13'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Bias and Fairness
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A biased machine learning model produces and amplifies unfair or discriminatory
    predictions against certain groups. Such models can produce biased predictions
    that lead to negative consequences such as social or economic inequality. Fortunately,
    some countries have discrimination and equality laws that protect minority groups
    against unfavorable treatment. One of the worst scenarios a machine learning practitioner
    or anyone who deploys a biased model could face is either receiving a legal notice
    imposing a heavy fine or receiving a lawyer letter from being sued and forced
    to shut down their deployed model. Here are a few examples of such situations:'
  prefs: []
  type: TYPE_NORMAL
- en: The ride-hailing app Uber faced legal action from two unions in the UK for its
    facial verification system, which showed racial bias against dark-skinned people
    by displaying more frequent verification errors. This impeded their work as Uber
    drivers ([https://www.bbc.com/news/technology-58831373](https://www.bbc.com/news/technology-58831373)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creators filed a lawsuit against YouTube for its racial and other minority group
    discrimination against them as YouTube’s algorithm automatically removed their
    videos without proper explanation, removing their capability to earn ad revenue
    ([https://www.washingtonpost.com/technology/2020/06/18/black-creators-sue-youtube-alleged-race-discrimination/](https://www.washingtonpost.com/technology/2020/06/18/black-creators-sue-youtube-alleged-race-discrimination/)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Facebook was charged with racial, gender, religion, familial status, and disability
    discrimination for its housing ads by the housing department of the US and had
    to pay under 5 million US dollars ([https://www.npr.org/2019/03/28/707614254/hud-slaps-facebook-with-housing-discrimination-charge](https://www.npr.org/2019/03/28/707614254/hud-slaps-facebook-with-housing-discrimination-charge)).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Thus, utmost care must be taken to prevent bias from being perpetuated against
    sensitive and protected attributes of the underlying data and environment the
    machine learning model will be exposed to. In this chapter, we will approach this
    topic step by step, starting with an exploration of the types of bias, learning
    to detect and evaluate methods needed to identify bias and fairness, and finally
    exploring ways to mitigate bias. The concepts and techniques that will be presented
    in this chapter are relevant to all machine learning models. However, bias mitigation
    is an exception; there, we will explore a neural network-specific method that
    can reliably mitigate bias. More formally, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the types of bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the source of AI bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Discovering bias and fairness evaluation methods
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the bias and fairness of a deep learning model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tailoring bias and fairness measures across use cases
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Mitigating AI bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter includes some practical implementations in the Python programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers==4.28.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`accelerate==0.6.0`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`captum`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`catalyst`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code files are available on GitHub at [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_13](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_13).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the types of bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bias can be described as a natural tendency or inclination toward a specific
    viewpoint, opinion, or belief system, regardless of whether it is treated as positive,
    neutral, or negative. AI bias, on the other hand, specifically occurs when mathematical
    models perpetuate the biases embedded by their creators or underlying data. Be
    aware that not all information is treated as biases, as some information can also
    be knowledge. Bias is a type of subjective information, and knowledge refers to
    factual information, understanding, or awareness acquired through learning, experience,
    or research. In other words, knowledge is the truth without bias.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Do not confuse bias in this book with the bias from the infamous “bias versus
    variance” concept in machine learning. Bias in this concept refers to the specific
    bias on how simple a machine learning model is concerning a certain task to learn.
    For completeness, variance specifies the sensitivity of the model toward the change
    in the data features. Here, a high bias would correspond to a low variance and
    underfitting behavior and indicate that a machine learning model is too simple.
    A low bias in this concept would correspond to high variance and overfitting behavior.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, we will use the term bias more colloquially. More well-known
    attributes of bias, such as race, gender, and age, are considered social bias,
    or stereotyping. However, the scope of bias is much broader. Here are some interesting
    examples of other types of bias:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Cultural bias**: The influence cultural perspectives, values, and norms have
    on judgments, decisions, and behaviors. It can manifest in machine learning models
    through biased data collection, skewed training data, or algorithms that reflect
    cultural prejudices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cognitive bias**: Systematic patterns of deviation from rationality in thinking
    or decision-making. Let’s look at a few examples of cognitive bias along with
    possible phenomena that portray the bias in a machine learning project:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Confirmation bias**: Favoring data that aligns with existing beliefs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Anchoring bias**: Overemphasizing certain features or variables during model
    training'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Availability bias**: Relying on easily accessible data sources, potentially
    overlooking relevant but less accessible ones'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Overconfidence bias**: Overestimating model capabilities or accuracy'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hindsight bias**: Believing that model predictions were more predictable
    after observing them'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Automation bias**: Placing excessive trust in the model’s outputs without
    critical evaluation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Framing bias**: Influencing the model’s learning process through biased data
    presentation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Selection bias**: Non-random sampling leading to unrepresentative data'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sampling bias**: Skewed data due to an unrepresentative sample'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reporting bias**: Misrepresentation due to individuals’ preferences or beliefs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Algorithmic bias**: Presence of any biases in algorithms such as machine
    learning algorithms. Another example of such bias is **aggregation bias**, which
    has skewed predictions due to data grouping or aggregation methods.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Measurement bias**: Inaccuracies or errors in data collection and measurement.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: These bias groups are just the tip of the iceberg and can span to very niche
    bias groups such as political bias, industry bias, media bias, and so on. Now
    that we understand what bias is and what it covers, let’s discover the source
    of AI bias.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the source of AI bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'AI bias can happen at any point in the deep learning life cycle. Let’s go through
    bias at those stages one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Planning**: During the planning stage of the machine learning life cycle,
    biases can emerge as decisions are made regarding project objectives, data collection
    methods, and model design. Bias may arise from subjective choices, assumptions,
    or the use of unrepresentative data sources. Project planners need to maintain
    a critical perspective, actively consider potential biases, engage diverse perspectives,
    and prioritize fairness and ethical considerations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data preparation**: This stage involves the following phases:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data collection**: During the data collection phase, bias can creep in if
    the collected data fails to represent the target population accurately. Several
    factors can contribute to this bias, including sampling bias, selection bias,
    or the underrepresentation of specific groups. These issues can lead to the creation
    of an imbalanced dataset that does not reflect the true diversity of the intended
    population.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Data labeling**: Bias can also infiltrate the data labeling process. Each
    labeler may possess their own inherent biases, consciously or subconsciously,
    which can influence their decision-making when assigning labels to the data. If
    the labelers lack diversity or comprehensive training, their biases may seep into
    the annotations, ultimately leading to the development of biased models that perpetuate
    unfairness and discrimination. As a result, the final combined data can contain
    multiple biases which may even conflict with each other, causing difficulties
    in the learning process.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model development**: Bias can be introduced in two ways in a deep learning
    model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection**: Biases can arise from the features selected for model
    training. If certain features are correlated with protected attributes (such as
    race or gender), the model may inadvertently learn and reinforce those biases,
    leading to discriminatory outcomes.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pretrained models**: A pretrained deep learning model might be a biased model.
    For example, if the model has been trained on biased data, it may learn and perpetuate
    those biases in its predictions. Even if fine-tuning was done, the bias is not
    likely to go away.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Deliver model insights**: Bias can happen particularly when interpreting
    explanations of model behavior. The process of understanding and explaining the
    inner workings of a model involves subjective reasoning and is susceptible to
    bias. The interpretation of model insights heavily relies on the perspective and
    preconceptions of the individuals involved, which can introduce biases based on
    their own beliefs, experiences, or implicit biases. It is essential to approach
    the interpretation of model explanations with awareness of these potential biases
    and strive for objectivity and fairness to avoid misinterpretation or reinforcing
    existing biases. Critical thinking and diverse perspectives are vital to ensuring
    that the insights delivered accurately reflect the model’s behavior without introducing
    additional bias.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model deployment**: This stage covers bias that can happen when a model is
    deployed, which includes the following components:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**User interactions**: Bias can arise during model deployment when users provide
    feedback or responses and be introduced if the feedback is biased or if the system
    responds differently based on user characteristics. For example, the chat history
    mechanism in the ChatGPT UI allows a user to provide biased input.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Human-in-the-loop bias**: Biases can be introduced when human reviewers or
    operators make decisions based on the model’s predictions, exhibiting their own
    biases or interpreting outputs unfairly. This can impact the perceived fairness
    of the decision-making process.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Environment bias**: Some features might be treated and perceived differently
    in unseen areas, leading to data drift. Models were evaluated to be unbiased during
    the development stage, but with the new data, it could still produce biased predictions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**New data source for retraining bias**: New data can be collected and labeled
    for retraining, which can be a source of bias.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Model governance**: Bias can emerge when the person responsible for monitoring
    the deployed model needs to establish thresholds for various types of drift (which
    will be introduced in [*Chapter 16*](B18187_16.xhtml#_idTextAnchor238), *Governing
    Deep Learning Models*), analyze prediction summaries, and examine data summaries.
    Setting these thresholds introduces the potential for bias based on subjective
    decisions or assumptions. Additionally, when analyzing prediction and data summaries,
    there is a risk of overlooking certain biases or unintentionally reinforcing existing
    biases if not approached with diligence and a critical mindset. It is crucial
    to maintain awareness of these biases and ensure that monitoring and analysis
    processes are conducted rigorously and with a focus on fairness and accuracy.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we’ve discovered some of the possible sources of bias in each stage
    of the deep learning life cycle, we are ready to dive into discovering bias detection
    and fairness evaluation methods.
  prefs: []
  type: TYPE_NORMAL
- en: Discovering bias and fairness evaluation methods
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Fairness and bias are opposing concepts. Fairness seeks to ensure fair and equal
    treatment in decision-making for all individuals or groups, while bias refers
    to unfair or unequal treatment. Mitigating bias is a crucial step in achieving
    fairness. Bias can exist in different forms and addressing all potential biases
    is complicated. Additionally, it’s important to understand that achieving fairness
    in one aspect doesn’t guarantee the complete absence of bias in general.
  prefs: []
  type: TYPE_NORMAL
- en: 'To understand both how much bias and how fair our data and model are, what
    we need is a set of bias and fairness metrics to objectively measure and evaluate.
    This will then enable a feedback mechanism to iteratively and objectively mitigate
    bias and achieve fairness. Let’s go through a few robust bias and fairness metrics
    that you need to have in your arsenal of tools to achieve fairness:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Equal representation-based metrics**: This set of metrics focuses on the
    equal proportions of either the data or the decision outcomes without considering
    the errors:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Disparate impact**: Disparate impact examines whether the model treats different
    groups fairly or if there are significant relative disparities in the outcomes
    they receive by taking a ratio of the proportion of favorable outcomes between
    groups. Disparate impact for a chosen group in a chosen attribute can be computed
    using the following formula:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Disparate Impact (Group A) =
  prefs: []
  type: TYPE_NORMAL
- en: (  Proportion of Positive Predictions for Group A     ___________________________________________________________      Proportion
    of Positive Predictions for Reference Group or Aggregate of Other Groups  )
  prefs: []
  type: TYPE_NORMAL
- en: Disparate impact across groups can be averaged to obtain a single global representative
    value.
  prefs: []
  type: TYPE_NORMAL
- en: '**Statistical parity difference**: This extends similar benefits as disparate
    impact but provides an absolute disparity measure instead of relative by using
    a difference instead of a ratio. The absolute difference is useful when you can
    and need to translate the values into tangible impacts, such as the number of
    individuals who were discriminated against based on a new sample size. It can
    be computed using the following formula:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Statistical Parity Difference = |(Proportion of Positive Predictions for Privileged
    Group) − (Proportion of Positive Predictions for Unprivileged Group)|
  prefs: []
  type: TYPE_NORMAL
- en: '**Equal error-based metrics**: These are the metrics that consider the bias
    in error rates between groups.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Average Odd Difference** (**AOD**): This measures the average discrepancy
    in the odds of true positive and false positive outcomes across groups. AOD is
    computed by taking the average of the odds differences across different groups.
    The odds difference for a specific group is calculated as the difference between
    the odds of positive prediction for that group and the odds of positive prediction
    for a reference group using the following formula:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: AOD = ( 1 _ n ) * Σ[(TPR _ ref − TPR _ i) + (FPR _ ref − FPR _ i)]
  prefs: []
  type: TYPE_NORMAL
- en: Here, n is the total number of groups, TPR _ i is the true positive rate (sensitivity)
    for group i, FPR _ i is the false positive rate (fallout) for group i, TPR _ ref
    is the true positive rate for the reference group, and FPR _ ref is the false
    positive rate for the reference group.
  prefs: []
  type: TYPE_NORMAL
- en: '**Average Absolute Odds Difference** (**AAOD**): This extends similar benefits
    to AOD but adds an absolute term in the individual group computations, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AOD = ( 1 _ n ) * Σ[|(TPR _ ref − TPR _ i) + (FPR _ ref − FPR _ i)|]
  prefs: []
  type: TYPE_NORMAL
- en: This should be used over AOD when you care about the discrepancies in general
    and not only whether the group discrepancy is favored or non-favored.
  prefs: []
  type: TYPE_NORMAL
- en: '**Distributional fairness through g****eneralized entropy index** (**GEI**):
    This is designed to measure the level of inequality based on distribution across
    individuals of an entire population using only the numerical outcome. The formula
    for GEI is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GE(α) =  1 _ na(a − 1)  ∑ i=1 n (n a w a− n), a ≠ 0,1,
  prefs: []
  type: TYPE_NORMAL
- en: GE(α) = log(n) + ∑ i=1 n w i log(w i), a = 1,
  prefs: []
  type: TYPE_NORMAL
- en: GE(α) = − log(n) −  1 _ n  ∑ i=1 n log(w i), a = 0
  prefs: []
  type: TYPE_NORMAL
- en: Here, E T = ∑ i=1 n  E i and w i =  E i _ E T
  prefs: []
  type: TYPE_NORMAL
- en: 'E iis the value of the chosen attribute of a specific entity, E T is the total
    summed value of all, and n is the total number of individuals or entities. Two
    foundational notions of inequality can be configured through the α parameter of
    GEI:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Theil index**: The general inequality of all individuals across all groups.
    It has an α value of 1.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Coefficient of variation**: The inequality that’s measured by computing the
    variability of individuals in a population group. The more varied the population
    group is, the more biased the population group is. It has an α value of 2.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use the Theil index as your main option if you want an overall inequality and
    switch to the coefficient of variation when you want to understand inequality
    by group.
  prefs: []
  type: TYPE_NORMAL
- en: '**Individual fairness metric**: Disparity or similarity of outcome based on
    similar individuals is a single individual-based metric. Proximity algorithms
    such as KNN allow you to consider the multiple associating features of an individual
    and compute fairness metrics based on similar examples. You must perform the following
    steps:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Find a chosen number of similar examples with associative features, excluding
    the protected attribute with KNN for the individual.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the inequality of the outcome using a chosen fairness metric. The most
    common metric that’s used here is the average similarity of the outcomes of similar
    examples to the individual.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Fair accuracy-based performance metric through Balanced accuracy**: This
    provides a balanced evaluation of a classification model’s performance, especially
    when dealing with imbalanced datasets. Balanced accuracy is computed by calculating
    the average of the class-wise accuracies.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Even though the metrics we’ve introduced here only cover a partial set of bias
    and fairness metrics available in the field, it is general enough to satisfy most
    machine learning use cases. Now, let’s explore how to use these metrics practically
    in a deep learning project to measure bias and fairness.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the bias and fairness of a deep learning model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this practical example, we will be exploring the infamous real-world use
    case of face recognition. This practical example will be leveraged for the practical
    implementation of bias mitigation in the next section. The basis of face recognition
    is to generate feature vectors that can be used to carry out KNN-based classification
    so that new faces don’t need to undergo additional network training. In this example,
    we will be training a classification model and evaluating it using traditional
    classification accuracy-based metrics; we won’t be demonstrating the recognition
    part of the use case, which allows us to handle unknown facial identity classes.
  prefs: []
  type: TYPE_NORMAL
- en: The goal here is to ensure that the resulting facial classification model has
    low gender bias. We will be using a publicly available facial dataset called **BUPT-CBFace-50**,
    which has a diverse coverage of facial images that have different facial expressions,
    poses, lighting conditions, and occlusions. The dataset consists of 500,000 images
    of 10,000 facial identity classes. In this practical example, you will require
    a GPU with at least 12 GB of RAM so that training can be done in a reasonable
    time. Before starting the example, download the dataset from the official source
    ([https://buptzyb.github.io/CBFace/?reload=true](https://buptzyb.github.io/CBFace/?reload=true)).
    You can find it in the same directory as your project.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with the step-by-step code walkthrough by using Python and the
    `pytorch` library with `catalyst` again:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the necessary libraries, which include `pytorch` as the
    deep learning framework, `mlflow` for tracking and comparison, `torchvision` for
    the pretrained ResNet50 model, `catalyst` for efficient PyTorch model handling,
    and `albumentations` for simple image data processing:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The last line sets the first GPU of your machine to be visible to CUDA, the
    computing interface for the GPU to be used.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will define the configuration that will be used for different components.
    This will include training process-specific parameters such as the batch sizes,
    learning rate, number of epochs, the number of early stopping epochs before stopping
    the training process, and the number of epochs to wait for validation metric improvements
    before reducing the learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Additionally, it will include the specification of the privileged and unprivileged
    groups that we will choose for computing the gender-based bias and fairness metrics.
    There are approximately two times more males than females in the dataset, so the
    privileged group here is expected to be `Male`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will set some names for the `mlflow` experiment name, the directory
    to save the models, and the extra parameters that we won’t be enabling for now:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we’ll proceed to load the CSV file that contains the dataset metadata,
    primarily consisting of the image paths for the downloaded and unzipped `BUPT_CBFace`
    dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Additionally, we will set up the `name2class` mapper, along with the class
    ID targets array and the number of classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will perform stratified splitting on this data to put it into training
    and validation sets for the facial identity classes so that both validation and
    training will have all the available facial identity classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The logic of the last `if` clause will be covered in the next section on bias
    mitigation. For the subsequent steps, treat the usage of `pass` as an indicator.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will define the model we want to use based on the ResNet50 model.
    We will use the ARCFace layer here with the ResNet50 model base, a type of metric
    learning algorithm. It utilizes angular margin loss to enhance the discriminative
    power of the learned face embeddings, enabling more accurate and robust face recognition
    across varying poses, illuminations, and identities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will initialize the model, assign it to use GPU, define the cross-entropy
    loss variable, define the SGD optimizer variable, and define the reduced learning
    rate engine on validation degradation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will need to define the PyTorch dataset class to take in the image
    file paths and the sensitive attributes, which are gender data, the target, and
    the specified albumentation transform. The last variable will be utilized in the
    next section:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we want to apply a simple set of transform operations from the `albumentation`
    library. Let’s define the method to return a transform instance with augmentation
    for training and without augmentation for validation purposes. Both require the
    transform instance to convert the image values into PyTorch tensors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, let’s initialize the dataset and the subsequent dataset loader:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will define the helper methods to help compute the multiclass bias
    and fairness metrics, which consist of performing safe division handling and zero
    division to prevent NaN values and computing false positives, true negatives,
    total positives, total negatives, and total data. Since this is a multiclass problem,
    we have to either choose macro-averaged or micro-averaged stats by class. Micro-averaged
    treats all samples equally, while macro-averaged treats all classes equally. Macro
    has an underlying issue where if the performance concerning a minority class is
    good, it will give a fake perception that the model is good in general. So, we
    will use micro-averaged here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will utilize these helper methods to define the method that will
    compute four bias and fairness metrics using common computed results – that is,
    disparate impact, statistical parity difference, AOD, and AAOD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will start by obtaining the false positives, true negatives, total positives,
    total negatives, and total data for the two groups, which are the privileged group
    and the non-privileged group, using the helper method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will compute the true positive ratio of both the privileged and non-privileged
    groups using the computed group stats so that it can be used directly to compute
    disparate impact and statistical parity difference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will compute the two mentioned metrics using the true positive ratios:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will compute AOD and AAOD using the true positive rates, `tpr`,
    and false positive rates, `fpr`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will be computing these four metrics during training and will be able to
    monitor and track the metrics as they’re being trained. To track the experiment,
    we will record the parameters and monitor their performance by iteration and epoch.
    We will use MLflow to do this. Let’s define the `mlflow` logger and log the parameters
    we defined earlier in *step 2*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we will require a slightly specialized flow to be able to compute custom
    metrics and perform bias mitigation methods later on, we will define a custom
    runner using `catalyst` that will be used to train the ResNet50 model. We will
    need to define three custom logic for four methods: `on_loader_start` (to initialize
    the metric aggregator functionality), `handle_batch` (to obtain the loss), `on_loader_end`
    (to finalize the aggregated batch metrics and update the learning rate scheduler),
    and `get_loggers` (to log data into MLflow). Let’s start with defining `on_loader_start`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will define the batch handler logic, which will load the data from
    the batch data loader in a custom way, perform forward propagation using the model
    initialized in the runner, and then compute loss, accuracy, and multiclass fairness
    metrics:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In the same method, we will also be required to update the current batch metric
    and the aggregated batch metrics so that they can be logged properly and finally
    perform backpropagation if it is training mode instead of validation mode:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must define the final two straightforward methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will initialize the runner and train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Figure 13**.1* shows the `mlflow` plotted performance graph of the accuracy,
    AOD, disparate impact, and statistical parity difference at every epoch of both
    the train and validation partitions:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.1 – Performance graph of ResNet50 by epochs](img/B18187_13_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.1 – Performance graph of ResNet50 by epochs
  prefs: []
  type: TYPE_NORMAL
- en: The validation scores ended up with 0.424 for both AAOD and AOD, 0.841 accuracy,
    0.398 disparate impact, and 0.051 statistical parity.
  prefs: []
  type: TYPE_NORMAL
- en: The graph shows that while accuracy increased epoch after epoch, the bias and
    fairness metrics gradually became worse but stagnated at a value. The lowest bias
    model is at the 0th epoch, where everything was close to zero, including the accuracy.
    Even though the model is not biased, the model is not useful at all. Another interesting
    observation is at the 16th epoch mark – the model managed to get a better validation
    accuracy performance on the male samples as the AOD value became higher at the
    same point. Depending on the circumstances, you can opt to choose to take the
    model at the 15th epoch, which has a somewhat good accuracy score but not the
    best and a lower bias score.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To make a deeper analysis, let’s take a look at where the model is focusing
    when it’s making predictions using the integrated gradients technique from Captum.
    Let’s visualize images that are mostly frontal facing. To do this, we must define
    the necessary transform method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s define the frontal faces to visualize:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, let’s visualize the focus area:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '*Figure 13**.2* shows the original images and focus areas of the model:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 13.2 – Saliency explanations results of the trained ResNet50 model](img/B18187_13_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.2 – Saliency explanations results of the trained ResNet50 model
  prefs: []
  type: TYPE_NORMAL
- en: These visuals show that the model exhibits bias by focusing incorrectly on the
    hair of female faces. For males, the model did not focus on the hair. The model
    also focuses on the white background a little. We’ll learn how to remove this
    bias in the next section.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we understand some popular bias and fairness metrics, we need to know
    which metrics to use in different use cases.
  prefs: []
  type: TYPE_NORMAL
- en: Tailoring bias and fairness measures across use cases
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The process of figuring out bias and fairness metrics to use for our use case
    can flow similarly to the process of figuring out general model performance evaluation
    metrics, as introduced in [*Chapter 10*](B18187_10.xhtml#_idTextAnchor161), *Exploring
    Model Evaluation Methods*, in the *Engineering the base model evaluation metric*
    section. So, be sure to check that topic out! However, bias and fairness have
    unique aspects that require additional heuristical recommendations. Earlier, recommendations
    for metrics that belong to the same metric group were explored. Now, let’s explore
    general recommendations on the four metric groups:'
  prefs: []
  type: TYPE_NORMAL
- en: Equal representation is always desired when there is a sensitive and protected
    attribute. So, when you see these attributes, be sure to use equal representation-based
    metrics on both your data and the model. Examples include race, gender, religion,
    sexual orientation, disability, age, socioeconomic status, political affiliations,
    and criminal history.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predictive performance consistency is another desired trait of a machine learning
    model that deals with sensitive and protected attributes. So, when you see these
    attributes, be sure to use equal error-based metrics on your model.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both distributional fairness metrics and equal representation metrics measure
    inequality. However, distributional fairness works on continuous variables directly
    while equal representation metrics work on categorical variables. Binning can
    be done on continuous variables to transform them into categories but it’s not
    straightforward to decide on the proper binning strategy needed. So, use distributional
    fairness metrics when the variable to measure bias and fairness is a continuous
    variable.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Consider all potential aspects of bias and fairness and measure them separately
    with a chosen bias and fairness metrics:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s consider a scenario where a machine learning model is used to predict
    loan approvals. One aspect of fairness is to ensure that loan approvals are granted
    fairly across different demographic groups, such as race or gender. Ensuring equal
    representation of the data and the resulting model can help you accomplish that.
    However, solely focusing on equal representation may not capture the complete
    picture. For example, even though the overall loan approval rates are equal across
    groups, there could be a significant disparity in the interest rates assigned
    to different groups. This disparity in interest rates could lead to unfair and
    inequitable outcomes, as certain groups may be charged higher interest rates,
    resulting in financial disadvantages. Any additional evaluation and monitoring
    that uses distributional fairness metrics can help you understand the impact and
    assist in targeted bias mitigation of unfavored groups.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Evaluating the individual fairness of the outcome is useful when you deploy
    the machine learning model and receive individual data during model inferencing.
    A threshold can be set here to create an alert when the individual fairness score
    is too high and requires a human reviewer to evaluate and make a manual decision.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compute these metrics separately by sensitive groups and compare them visually
    to get a sense of fairness across groups. This can help you craft targeted bias
    mitigation responses to vulnerable groups.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Prediction explanations may help in understanding the reasons for bias and fairness.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias and fairness measures can conflict with accuracy-based performance measures.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Additionally, two global opposing views that will affect how you see fairness
    are worth mentioning:'
  prefs: []
  type: TYPE_NORMAL
- en: '**We’re All Equal** (**WAE**): The notion that data may not accurately represent
    reality due to the presence of inherent biases within it'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**What You See Is What You Get** (**WYSIWYG**): The notion is that the information
    presented by the data reflects an unbiased representation of reality, even if
    it reveals inequalities'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An extreme of either view will either remove any chances of building a model
    (WAE) or be too ignorant to care about fairness biases that will eventually lead
    to negative consequences, as mentioned earlier in this chapter (WYSIWYG). To create
    a successful machine learning model in our use case, we will need to balance the
    two views strategically so that both accuracy performance and fairness requirements
    can be satisfied. A good strategy here to employ is to apply common sense and
    accept what seems logical for a causal relationship (WYSIWYG), and practice fair
    processes that help us understand and mitigate the partial aspect of bias (WAE)
    even when true fairness can’t be achieved.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here are some examples with views to adopt that will make the most sense:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Adopting WYSIWYG**: In the domain of personalized advertising using machine
    learning, the WYSIWYG view would involve tailoring advertisements based on observed
    user behavior and preferences. For example, if a user frequently engages with
    content related to fitness, the system will present ads for fitness equipment
    or gym memberships. The goal is to provide a personalized user experience that
    aligns with their interests and needs. In this use case, any notion of the WAE
    view will fail the project as it contradicts the goal of personalization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adopting something like 75% WYSIWYG and 25% WAE**: In the context of making
    hiring decisions using machine learning, the WAE view would help advocate for
    equal consideration of all candidates without any bias related to gender, race,
    or other protected attributes. The WYSIWYG view will then allow a machine learning
    model to be built based solely on their qualifications and skills. The strategy
    to create a fair and unbiased selection process will help promote equal opportunities
    for all applicants while still allowing a functional machine learning model to
    be built successfully.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In terms of metrics, the equal representation-based metric group is associated
    with the WAE view while the equal error-based metric group is associated with
    the WYSIWYG view. So, again, choose these two metric groups if the two views are
    involved in your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll discover ways we can mitigate bias in our machine learning models.
  prefs: []
  type: TYPE_NORMAL
- en: Mitigating AI bias
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AI bias is an algorithmic bias that either comes from the model itself through
    its learning process or the data it used to learn from. The most obvious solution
    to mitigate bias is not programmatic mitigation methods but ensuring fair processes
    when collecting data. A data collection and preparation process is only truly
    fair when it not only ensures the resulting data is balanced by sensitive attributes
    but also ensures all inherent and systematic biases are not included.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, a balanced dataset based on the sensitive attribute does not
    guarantee a fair model. There can be differences in appearance among subgroups
    under the hood or associative groups of the data concerning multiple factors,
    which can potentially cause a biased system. Bias, however, can be mitigated partially
    when the dataset is balanced compared to without concerning the observable sensitive
    groups. But what are all these attributes? It might be easier to identify data
    attributes in tabular structured data with defined column names, but for unstructured
    data meant for deep learning, it’s impossible to cover all the possible attributes.
  prefs: []
  type: TYPE_NORMAL
- en: To dive deeper into actual examples, text data can contain a ton of attributes
    with examples, such as language, genre, topic, length, style, tone, time period,
    authorship, geographic origin, and cultural perspective. Image data can also contain
    a ton of attributes with examples such as subject/content, perspective, lighting,
    composition, color, texture, resolution, orientation, context, and cultural relevance.
    Finally, audio data can also contain a ton of attributes, such as genre, language,
    duration, sound quality, instrumentation, vocal style, tempo, mood, cultural influence,
    and recording environment.
  prefs: []
  type: TYPE_NORMAL
- en: It’s hard to ensure equal representation in all facets - more so when data is
    readily available and is already originally highly imbalanced with only a few
    examples for certain categories and plenty for others. Ideally, bias mitigation
    should always be executed right from the data preparation stage. However, if that's
    not possible, programmatic bias mitigation methods can be applied after data collection.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get a clearer idea of these methods, have a look at *Figure 13**.3*, which
    presents an overview of various bias-reducing approaches:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 13.3 – The four steps of bias mitigation under two machine learning
    life cycle stages](img/B18187_13_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 13.3 – The four steps of bias mitigation under two machine learning life
    cycle stages
  prefs: []
  type: TYPE_NORMAL
- en: 'Programmatic bias mitigation provides methods that can be applied in three
    different stages of the model-building process:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-processing**: This is part of the data preparation ML life cycle stage.
    Here are some examples of bias mitigation methods that belong to this group:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Eliminating protected attributes from being used in the model. However, information
    about the protected attribute could still present itself in other associative
    attributes. Additionally, some attributes are deeply interconnected with other
    key information that’s required to predict the desired label, and unfortunately
    can’t be removed easily. Examples include gender in facial images.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Disparate Impact Remover, available in the AIF360 open source library. It balances
    the dataset group proportions by dropping data rows to achieve a better disparate
    score.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Weigh privileged class loss so that it’s lower than unprivileged class loss
    during training so that errors between the classes are more equal.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Targeted data augmentation for unprivileged classes. Augmentation adds more
    data variations and can be applied in two ways:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding more data variations in unprivileged groups with augmentation will help
    increase accuracy performance there and contribute to the balancing of error rates,
    especially when the unprivileged class is underrepresented.
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Counterfactual role reversal augmentation inverts the privileged group into
    the unprivileged group and vice versa and allows for equal representation. Here
    are some augmentations that can be used based on variable type:'
  prefs:
  - PREF_IND
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '• **Text**: Use word swap'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '• **All variable types**: Use style transfer techniques. Example techniques
    are as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Using Generative AI techniques from a trained StarGAN, which is an image generator
    that can invert a person’s gender, change a person’s age, and much more.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimize the desired image to mimic the style of another image by reducing the
    distance between a chosen intermediate layer between the images. This method is
    called transfer by input optimization and is based on the neural interpretation
    technique mentioned in the previous chapter.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Targeted counterfactual role reversal not through additional augmentation but
    a permanent input replacement for a deployed model. This allows you to explicitly
    control the equality of the results during deployment.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: OpenAI changed “male” to “female” in their Dall-E text-to-image prompts randomly
    as the model is biased to depict males for roles such as professor and CEO.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**In-processing**: This is part of the model development machine learning life
    cycle stage and is the process of training a model. Here are some examples of
    bias mitigation methods that belong to this group:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Knowledge distillation**: This was introduced in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125),
    *Exploring Supervised Deep Learning*. The idea is that a teacher model that has
    been trained with a much bigger and more representative dataset will be less biased
    compared to a student model that is being trained on a much smaller custom dataset.
    Ideally, the fairness from the teacher model will be distilled into the student
    model. However, knowledge distillation can also cause an increased bias in the
    resulting student model when you distill it with a biased teacher model.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Adversarial debiasing**: This method iteratively trains a classifier to optimize
    prediction accuracy while simultaneously minimizing an adversary model’s ability
    to infer the protected attribute from the predictions. The classifier aims to
    make accurate predictions on the target variable, while the adversary tries to
    discern the sensitive attribute associated with bias. This simultaneous training
    process creates a competitive environment where the classifier learns to encode
    important information about the target variable while reducing the influence of
    potentially biased features. By doing so, adversarial debiasing promotes fairness
    by mitigating the impact of sensitive attributes and enhancing the overall equity
    of the model’s predictions.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regularization**: In deep learning, regularization involves any addition
    or modification to the neural network, data, or training process that is used
    to increase the generalization of the model to external data. This can indirectly
    contribute to reducing bias in the model. Some common regularization methods include
    dropout layers, L1/L2 regularization, batch normalization, group normalization,
    weight standardization, stochastic depth, label smoothing, and data augmentation.
    By improving generalization, regularization methods can help the model learn more
    general patterns in the data instead of fitting too closely to the training set,
    which might contain biased features. This method was explored more extensively
    in [*Chapter 2*](B18187_02.xhtml#_idTextAnchor040), *Designing Deep* *Learning
    Architectures*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Post-processing**: This is part of the model development machine learning
    life cycle stage but only covers processing the trained model’s outputs to mitigate
    bias. Here are some examples of bias mitigation methods that belong to this group:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Test-time counterfactual role reversal augmentation ensemble**: This method
    involves performing two predictions each using opposite roles and performing an
    ensemble of the predictions. A max or exponential mean operation for ensembling
    performs much better than an average operation, as shown in [https://www.amazon.science/publications/mitigating-gender-bias-in-distilled-language-models-via-counterfactual-role-reversal](https://www.amazon.science/publications/mitigating-gender-bias-in-distilled-language-models-via-counterfactual-role-reversal).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Equalized odds postprocessing**: This method involves modifying the output
    predictions of a classifier to ensure equal false positive and false negative
    rates across different groups through threshold optimization by groups. Specifically,
    prediction thresholds for each protected group are determined separately.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It is important to note that a fairer model obtained after bias mitigation would
    likely cause a reduction in accuracy-based metrics. If the loss in an accuracy-based
    metric is minor enough, a fair model is highly desired as there would not be an
    issue in using bias mitigation methods. To that end, always start by creating
    a baseline model and evaluate fairness with the chosen bias and fairness metrics
    to ensure that bias even exists in your model before using any bias mitigation
    methods. Bias mitigation methods always cause a substantial increase in training
    time needed, so make sure it’s worth it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Additionally, there are a few behaviors related to bias that are useful to
    know about:'
  prefs: []
  type: TYPE_NORMAL
- en: Decision-making algorithms tend to be biased toward more common occurrences,
    so balanced data can help reduce this bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pruned models increase bias (more at [https://arxiv.org/pdf/2106.07849.pdf](https://arxiv.org/pdf/2106.07849.pdf))
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Models with limited capacity (smaller models) tend to exploit the biases in
    the dataset ([https://aclanthology.org/2022.gebnlp-1.27.pdf](https://aclanthology.org/2022.gebnlp-1.27.pdf)),
    so be wary when you’re performing knowledge distillation on a smaller model and
    evaluate bias and fairness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'When it comes to mitigating bias, you might be wondering which of the four
    groups of methods to choose from. Here’s a suggestion: it’s best to begin by addressing
    bias as early as possible in the process, where you have the most control and
    flexibility. You don’t want to be stuck with limited options to mitigate bias
    and end up not being able to satisfactorily mitigate bias. If you don’t have access
    to the data collection stage, you can utilize data preprocessing techniques. On
    the other hand, if you don’t have access to the data itself but have a trained
    model, postprocessing techniques should be used. Techniques can be combined, so
    be sure to measure bias using the metrics introduced to ensure that each technique
    that’s applied improves the fairness of the resulting model. Also, consider using
    multiple methods to mitigate more bias.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The programmatic bias mitigation methods we’ve introduced so far are separated
    into three groups. However, a robust bias mitigation method exists for deep learning
    models that lies in the intersection of all three of them. The method is a fusion
    between counterfactual augmentation, mix-up augmentation, knowledge distillation,
    and counterfactual test time augmentation. The idea is to do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Use counterfactual augmentation to train both the teacher model and student
    on the entire dataset.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Distill the counterfactual ensembled chosen layer features of the teacher model
    with exponential max to the student model. This is similar to mix-up augmentation
    but on the feature layer.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'However, counterfactual role reversal remains to be a technique that’s more
    accessible to text. Let’s discover how to use the generic knowledge distillation
    method to reduce bias on the same use case we explored in the previous section
    on face classification. The method we will be introducing here is from [https://arxiv.org/pdf/2112.09786.pdf](https://arxiv.org/pdf/2112.09786.pdf),
    which provides two methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**(Distill and debias) D&D++**: First, train the teacher model using only privileged
    group data. Then, initialize the student model with the teacher model weights
    and train the student model with normal loss and the knowledge distillation loss
    using the cosine similarity of the chosen feature layer. Lastly, initialize a
    final student network using the previously trained student model and train on
    the entire dataset with knowledge distillation from the previous student model
    as the teacher model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**One-step distillation D&D**: To make it simpler, but still immediately effective,
    the steps can be simplified. First, train the teacher model using only privileged
    group data. Then, train the student model on the entire dataset with knowledge
    distillation from the teacher model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s explore the one-step distillation D&D method practically by using the
    use case we experimented with in the previous section:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will be training a teacher model using only the privileged group
    data, which only consists of male facial identities. Let’s define the specifically
    different configurations for this example:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will be using the same code base we introduced in the previous section but
    with some additional custom code. The first is an addition to the code in *step
    6* of the *Evaluating the bias and fairness of a deep learning model* section,
    where we will be removing the female data in both the training and validation
    data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Second is *step 7*, where we will define a special forward method and the method
    to get the last conv features. The idea is that we want to make sure the focus
    areas are the same and not the probabilities of the facial identity themselves,
    which will be useless when the model is used as a facial recognition featurizer.
    The special forward method is designed to return both the output logits and the
    last convolutional features in one forward pass, reducing latency:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next is *step 8*, where we will extract the teacher model features once before
    starting training. This will reduce the time and resources needed to train the
    student model as the teacher model’s features will remain fixed on the same data
    without augmentation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will define the training and validation dataset that takes in the
    training and validation teacher model features separately from *step 10*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The final change is to add handling of the batch loading by taking the extra
    teacher model features and loss using both cross entropy loss and the cosine similarity
    loss of the student and teacher model’s last convolutional layer features from
    *step 14*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, run through all the steps from the previous section with the changes; you’ll
    get an approximate validation metric score of 0.774 accuracy with all other scores
    of 0 as there are no females in this first step.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'To execute the next step of one-step distillation, execute the previous code
    once more but with the following configuration changes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The validation scores for the one-step distillation D&D approach end up with
    0.3838 for both AAOD and AOD, 0.76 accuracy, 0.395 disparate impact, and 0.04627
    statistical parity. For a fair comparison, the performance of basic training in
    terms of accuracy is an accuracy of 0.76333, 0.38856 for both AAOD and AOD, a
    0.3885 disparate impact, and a 0.04758 statistical parity. This means that the
    improvements are mainly from the AOD with a 0.0047 difference and statistical
    parity with a 0.00131 difference.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'By using the integrated gradients code from *step 19* again on the model we
    trained with bias mitigation methods, the following results can be obtained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 13.4: Explanations of the one-step distillation D&D model](img/B18187_13_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.4: Explanations of the one-step distillation D&D model'
  prefs: []
  type: TYPE_NORMAL
- en: The model now focuses on the right features – that is, facial features without
    backgrounds or hair regardless of gender.
  prefs: []
  type: TYPE_NORMAL
- en: Now, experiment with D&D++ for yourself and see what improvements you can get!
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we discovered a variety of mitigation techniques that apply
    to neural network models and also indirectly showed that metrics are not the only
    indicators of bias.
  prefs: []
  type: TYPE_NORMAL
- en: While this chapter predominantly focused on deep learning models and unstructured
    data, it is important to note that the concepts, techniques, and metrics we discussed
    also apply to structured data. Bias can exist in structured data in the form of
    imbalanced classes, skewed attribute distributions, or unrepresentative samples,
    and can manifest in the model’s predictions, leading to unfair outcomes. A notable
    difference is that for structured data-based use cases, biases are usually more
    directly perpetuated by the input features. The bias and fairness evaluation methods,
    such as equal representation-based metrics, equal error-based metrics, and distributional
    fairness metrics, can be used to assess the fairness of machine learning models
    trained on structured data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we focused on the critical issue of bias and fairness in machine
    learning models. The potential negative consequences of deploying biased models,
    such as legal actions and fines, were emphasized. We covered various types of
    biases and identified stages in the deep learning life cycle where bias can emerge,
    including planning, data preparation, model development, and deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Several metrics for detecting and evaluating bias and fairness were also introduced,
    including equal representation-based metrics, equal error-based metrics, distributional
    fairness metrics, and individual fairness metrics. This chapter provided recommendations
    on selecting the right metrics for specific use cases and highlighted the importance
    of balancing opposing views, such as WAE and WYSIWYG, when evaluating fairness.
    This chapter also discussed programmatic bias mitigation methods that can be applied
    during the pre-processing, in-processing, and post-processing stages of model
    building. Examples of these methods include eliminating protected attributes,
    disparate impact remover, adversarial debiasing, and equalized odds post-processing.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, this chapter presented a comprehensive bias mitigation approach for
    deep learning models, combining counterfactual augmentation, mix-up augmentation,
    knowledge distillation, and counterfactual test-time augmentation. This approach
    aims to balance accuracy performance and fairness requirements in the model.
  prefs: []
  type: TYPE_NORMAL
- en: With this, you have learned about the importance and techniques of addressing
    bias and fairness in machine learning models and their potential negative consequences
    if not properly addressed. This knowledge will help you create machine learning
    systems that not only perform well on accuracy-based metrics but also consider
    the ethical implications and fairness aspects, ensuring a responsible and effective
    deployment of AI solutions, ultimately leading to better and more equitable outcomes
    in real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: As we move forward, the next chapter will shift focus and analyze adversarial
    performance, a crucial aspect of ensuring robust and reliable machine learning
    models in production.
  prefs: []
  type: TYPE_NORMAL
