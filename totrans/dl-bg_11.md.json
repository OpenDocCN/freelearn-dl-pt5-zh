["```py\n#download data\n!wget https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\n```", "```py\nimport pandas as pd\ndf = pd.read_csv('processed.cleveland.data', header=None)\n# this next line deals with possible numeric errors\ndf = df.apply(pd.to_numeric, errors='coerce').dropna()\nX = df[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]].values\ny = df[13].values\n```", "```py\nfrom tensorflow.keras import backend as K\n\ndef sampling(z_params):\n  z_mean, z_log_var = z_params\n  batch = K.shape(z_mean)[0]\n  dims = K.int_shape(z_mean)[1]\n  epsilon = K.random_normal(shape=(batch, dims))\n  return z_mean + K.exp(0.5 * z_log_var) * epsilon\n```", "```py\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nx_train = scaler.transform(X)\noriginal_dim = x_train.shape[1]\n```", "```py\ninput_shape = (original_dim, )\nintermediate_dim = 13 \nbatch_size = 18 # comes from ceil(sqrt(x_train.shape[0]))\nlatent_dim = 2 # useful for visualization\nepochs = 500\n```", "```py\nfrom tensorflow.keras.layers import Lambda, Input, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Model\n\ninputs = Input(shape=input_shape)\nbn = BatchNormalization()(inputs)\ndp = Dropout(0.2)(bn)\nx = Dense(intermediate_dim, activation='sigmoid')(dp)\nx = Dropout(0.2)(x)\nz_mean = Dense(latent_dim)(x)\nz_log_var = Dense(latent_dim)(x)\nz_params = [z_mean, z_log_var]\nz = Lambda(sampling, output_shape=(latent_dim,))(z_params)\nencoder = Model(inputs, [z_mean, z_log_var, z])\n```", "```py\nlatent_inputs = Input(shape=(latent_dim,))\nx = Dense(intermediate_dim, activation='relu')(latent_inputs)\nr_outputs = Dense(original_dim)(x)    # reconstruction outputs\ndecoder = Model(latent_inputs, r_outputs)\n```", "```py\noutputs = decoder(encoder(inputs)[2])   # it is index 2 since we want z\nvae = Model(inputs, outputs)\n```", "```py\nfrom tensorflow.keras.losses import mse\nr_loss = mse(inputs, outputs)\n```", "```py\nfrom tensorflow.keras.losses import binary_crossentropy\nr_loss = binary_crossentropy(inputs, outputs)\n```", "```py\nr_loss = original_dim * r_loss\n```", "```py\nkl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\nkl_loss = 0.5 * K.sum(kl_loss, axis=-1)\n```", "```py\nvae_loss = K.mean(r_loss + kl_loss)\nvae.add_loss(vae_loss)\n```", "```py\nvae.compile(optimizer='adam')\n```", "```py\nhist = vae.fit(x_train, epochs=epochs,\n               batch_size=batch_size,\n               validation_data=(x_train, None))\n```", "```py\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(10,6))\nplt.plot(hist.history['loss'], color='#785ef0')\nplt.plot(hist.history['val_loss'], '--', color='#dc267f')\nplt.title('Model reconstruction loss')\nplt.ylabel('MSE Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training Set', 'Validation Set'], loc='upper right')\nplt.show()\n```", "```py\nencdd = encoder.predict(x_train)\nx_hat = decoder.predict(encdd[0])\n```", "```py\nimport numpy as np\nprint(np.around(scaler.inverse_transform(x_train[0]), decimals=1))\nprint(np.around(scaler.inverse_transform(x_hat[0]), decimals=1))\n```", "```py\n[ 63.0  1.0  1.0  145.0  233.0  1.0  2.0  150.0  0.0  2.3  3.0  0.0  6.0 ]\n[ 61.2  0.5  3.1  144.1  265.1  0.5  1.4  146.3  0.2  1.4  1.8  1.2  4.8 ]\n```", "```py\ndef generate_samples(N = 10, latent_dim = 2):\n  noise = np.random.uniform(-2.0, 2.0, (N,latent_dim))\n  gen = decoder.predict(noise)\n  return gen\n```", "```py\ngen = generate_samples(10, latent_dim)\nprint(np.around(scaler.inverse_transform(gen), decimals=1))\n```", "```py\n[[ 43.0  0.7  2.7  122.2  223.8  0.0  0.4  172.2  0.0  0.3  1.2  0.1  3.6]\n [ 57.4  0.9  3.9  133.1  247.6  0.1  1.2  129.0  0.8  2.1  2.0  1.2  6.4]\n [ 60.8  0.7  3.5  142.5  265.7  0.3  1.4  136.4  0.5  1.9  2.0  1.4  5.6]\n [ 59.3  0.6  3.2  137.2  261.4  0.2  1.2  146.2  0.3  1.2  1.7  0.9  4.7]\n [ 51.5  0.9  3.2  125.1  229.9  0.1  0.7  149.5  0.4  0.9  1.6  0.4  5.1]\n [ 60.5  0.5  3.2  139.9  268.4  0.3  1.3  146.1  0.3  1.2  1.7  1.0  4.7]\n [ 48.6  0.5  2.6  126.8  243.6  0.1  0.7  167.3  0.0  0.2  1.1  0.1  3.0]\n [ 43.7  0.8  2.9  121.2  219.7  0.0  0.5  163.8  0.1  0.5  1.4  0.1  4.4]\n [ 54.0  0.3  2.5  135.1  264.2  0.2  1.0  163.4  0.0  0.3  1.1  0.3  2.7]\n [ 52.5  1.0  3.6  123.3  227.8  0.0  0.8  137.7  0.7  1.6  1.8  0.6  6.2]]\n```", "```py\nfrom tensorflow.keras.layers import Lambda, Input, Dense, Dropout\nfrom tensorflow.keras.layers import Activation, BatchNormalization\nfrom tensorflow.keras.models import Model\n\ninpt_dim = 28*28\nltnt_dim = 2\n\ninpt_vec = Input(shape=(inpt_dim,))\n```", "```py\nel1 = Dropout(0.1)(inpt_vec)\nel2 = Dense(512)(el1)\nel3 = Activation('relu')(el2)\nel4 = Dropout(0.1)(el3)\nel5 = Dense(512)(el4)\nel6 = BatchNormalization()(el5)\nel7 = Activation('relu')(el6)\nel8 = Dropout(0.1)(el7)\n\nel9 = Dense(256)(el8)\nel10 = Activation('relu')(el9)\nel11 = Dropout(0.1)(el10)\nel12 = Dense(256)(el11)\nel13 = BatchNormalization()(el12)\nel14 = Activation('relu')(el13)\nel15 = Dropout(0.1)(el14)\n\nel16 = Dense(128)(el15)\nel17 = Activation('relu')(el16)\nel18 = Dropout(0.1)(el17)\nel19 = Dense(ltnt_dim)(el18)\nel20 = BatchNormalization()(el19)\nel21 = Activation('sigmoid')(el20)\n\nz_mean = Dense(ltnt_dim)(el21)\nz_log_var = Dense(ltnt_dim)(el21)\nz = Lambda(sampling)([z_mean, z_log_var])\nencoder = Model(inpt_vec, [z_mean, z_log_var, z])\n```", "```py\nltnt_vec = Input(shape=(ltnt_dim,))\ndl1 = Dense(128)(ltnt_vec)\ndl2 = BatchNormalization()(dl1)\ndl3 = Activation('relu')(dl2)\n\ndl4 = Dropout(0.1)(dl3)\ndl5 = Dense(256)(dl4)\ndl6 = Activation('relu')(dl5)\ndl7 = Dense(256)(dl6)\ndl8 = BatchNormalization()(dl7)\ndl9 = Activation('relu')(dl8)\n\ndl10 = Dropout(0.1)(dl9)\ndl11 = Dense(512)(dl10)\ndl12 = Activation('relu')(dl11)\ndl13 = Dense(512)(dl12)\ndl14 = BatchNormalization()(dl13)\ndl15 = Activation('relu')(dl14)\ndl16 = Dense(inpt_dim, activation='sigmoid') (dl15)\n\ndecoder = Model(ltnt_vec, dl16)\n```", "```py\noutputs = decoder(encoder(inpt_vec)[2])\nvae = Model(inpt_vec, outputs)\n```"]