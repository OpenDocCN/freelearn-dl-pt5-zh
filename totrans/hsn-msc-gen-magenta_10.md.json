["```py\nfrom magenta.common import merge_hparams\nfrom magenta.models.music_vae import Config\nfrom magenta.models.music_vae import MusicVAE\nfrom magenta.models.music_vae import lstm_models\nfrom magenta.models.music_vae.data import BASS_PROGRAMS\nfrom magenta.models.music_vae.data import NoteSequenceAugmenter\nfrom magenta.models.music_vae.data import OneHotMelodyConverter\nfrom tensorflow.contrib.training import HParams\n\ncat_bass_2bar_small = Config(\n  model=MusicVAE(lstm_models.BidirectionalLstmEncoder(),\n                 lstm_models.CategoricalLstmDecoder()),\n  hparams=merge_hparams(\n    lstm_models.get_default_hparams(),\n    HParams(\n      batch_size=512,\n      max_seq_len=32,\n      z_size=256,\n      enc_rnn_size=[512],\n      dec_rnn_size=[256, 256],\n      free_bits=0,\n      max_beta=0.2,\n      beta_rate=0.99999,\n      sampling_schedule=\"inverse_sigmoid\",\n      sampling_rate=1000,\n    )),\n  note_sequence_augmenter=NoteSequenceAugmenter(transpose_range=(-5, 5)),\n  data_converter=OneHotMelodyConverter(\n    valid_programs=BASS_PROGRAMS,\n    skip_polyphony=False,\n    max_bars=100,\n    slice_bars=2,\n    steps_per_quarter=4),\n  train_examples_path=None,\n  eval_examples_path=None,\n)\n```", "```py\nimport tensorflow as tf\nfrom magenta.models.music_vae.configs import CONFIG_MAP\nfrom magenta.models.music_vae.music_vae_train import run\n\ndef main(unused_argv):\n  CONFIG_MAP[\"cat-bass_2bar_small\"] = cat_bass_2bar_small\n  run(CONFIG_MAP)\n\nif __name__ == \"__main__\":\n  tf.app.run(main)\n```", "```py\npython chapter_07_example_01.py --config=\"cat-bass_2bar_small\" [FLAGS]\n```", "```py\ndrums_rnn_create_dataset --config=\"drum_kit\" --input=\"PATH_TO_NOTE_SEQUENCES\" --output_dir=\"sequence_examples\" --eval_ratio=0.10\n\n```", "```py\ndrums_rnn_train --config=\"drum_kit\" --run_dir=\"logdir/run1_small\" --sequence_example_file=\"sequence_examples/training_drum_tracks.tfrecord\" --hparams=\"batch_size=64,rnn_layer_sizes=[64,64]\" --num_training_steps=20000\n\n```", "```py\nINFO:tensorflow:hparams = {'batch_size': 64, 'rnn_layer_sizes': [64, 64], 'dropout_keep_prob': 0.5, 'attn_length': 32, 'clip_norm': 3, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}\n```", "```py\n2019-11-20 01:56:12.058398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 5089 MB memory) -> physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)\n```", "```py\nINFO:tensorflow:Accuracy = 0.27782458, Global Step = 10, Loss = 5.4186254, Perplexity = 225.56882 (16.487 sec)\nINFO:tensorflow:global_step/sec: 0.586516\n```", "```py\nset CUDA_VISIBLE_DEVICES=\"\"\ndrums_rnn_train --config=\"drum_kit\" --run_dir=\"logdir\\run1_small\" --sequence_example_file=\"sequence_examples/eval_drum_tracks.tfrecord\" --hparams=\"batch_size=64,rnn_layer_sizes=[64,64]\" --num_training_steps=20000 --eval\n\n```", "```py\nCUDA_VISIBLE_DEVICES=\"\" drums_rnn_train --config=\"drum_kit\" --run_dir=\"logdir/run1_small\" --sequence_example_file=\"sequence_examples/eval_drum_tracks.tfrecord\" --hparams=\"batch_size=64,rnn_layer_sizes=[64,64]\" --num_training_steps=20000 --eval\n```", "```py\nStarting evaluation at 2019-11-25-23:38:24\nINFO:tensorflow:Accuracy = 0.0, Global Step = 35, Loss = 0.0, Perplexity = 1.0\nINFO:tensorflow:Evaluation [1/3]\nINFO:tensorflow:Evaluation [2/3]\nINFO:tensorflow:Evaluation [3/3]\nFinished evaluation at 2019-11-25-23:38:30\n```", "```py\ntensorboard --logdir=logdir\n```", "```py\ndrums_rnn_train --config=\"drum_kit\" --run_dir=\"logdir/run1_small\" --sequence_example_file=\"sequence_examples/training_drum_tracks.tfrecord\" --hparams=\"batch_size=64,rnn_layer_sizes=[128,128]\" --num_training_steps=20000\n```", "```py\n[tensorflow/stream_executor/cuda/cuda_driver.cc:890] failed to alloc 8589934592 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n```", "```py\nInvalid argument: Assign requires shapes of both tensors to match. lhs shape= [128] rhs shape= [256]\n```", "```py\ndrums_rnn_train --config=\"drum_kit\" --run_dir=\"logdir/run1_diverge\" --sequence_example_file=\"sequence_examples/training_drum_tracks.tfrecord\" --hparams=\"batch_size=128,rnn_layer_sizes=[128,128,128]\" --num_training_steps=20000\n```", "```py\nE1112 20:03:08.279203 10460 basic_session_run_hooks.py:760] Model diverged with loss = NaN.\n...\n  File \"c:\\users\\magenta\\appdata\\local\\programs\\python\\python35\\lib\\site-packages\\tensorflow\\python\\training\\basic_session_run_hooks.py\", line 761, in after_run\n    raise NanLossDuringTrainingError\ntensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.\n```", "```py\ndrums_rnn_train --config=\"drum_kit\" --run_dir=\"logdir/run2_learning_rate\" --sequence_example_file=\"sequence_examples/training_drum_tracks.tfrecord\" --hparams=\"learning_rate=0.0001,batch_size=128,rnn_layer_sizes=[128,128,128]\" --num_training_steps=20000\n```", "```py\nmelody_rnn_create_dataset --config=\"attention_rnn\" --input=\"PATH_TO_NOTE_SEQUENCES\" --output_dir=\"sequence_examples\" --eval_ratio=0.10\n```", "```py\nmelody_rnn_train --config=\"attention_rnn\" --run_dir=\"logdir/run1_few_data\" --sequence_example_file=\"sequence_examples/training_melodies.tfrecord\" --hparams=\"batch_size=128,rnn_layer_sizes=[128,128]\" --num_training_steps=20000\n```", "```py\ndrums_rnn_generate --config=\"drum_kit\" --run_dir=\"logdir/run1_small\" --hparams=\"rnn_layer_sizes=[64,64]\" --output_dir=\"generated/generated1_small\"\n```", "```py\ndrums_rnn_generate --config=\"drum_kit\" --checkpoint_file=\"logdir/run1_small/train/model.ckpt-6745\" --hparams=\"batch_size=64,rnn_layer_sizes=[64,64]\" --output_dir=\"generated/generated1_small\"\n```", "```py\ndrums_rnn_generate --config=\"drum_kit\" --run_dir=\"logdir/run1_small\" --hparams=\"batch_size=64,rnn_layer_sizes=[64,64]\" --bundle_file=\"drums_rnn_dance_small.mag\" --save_generator_bundle\n```", "```py\ndrums_rnn_generate --config=\"drum_kit\" --bundle_file=\"drums_rnn_dance_small.mag\" --output_dir=\"generated/generated1_small\"\n```", "```py\nfrom magenta.music.protobuf.music_pb2 import NoteSequence\nfrom magenta.pipelines.dag_pipeline import DAGPipeline\nfrom magenta.pipelines.dag_pipeline import DagInput\nfrom magenta.pipelines.dag_pipeline import DagOutput\nfrom magenta.pipelines.pipeline import run_pipeline_serial\nfrom magenta.pipelines.pipeline import tf_record_iterator\nfrom magenta.pipelines.pipelines_common import RandomPartition\n\ndef partition(config: str, input: str, output_dir: str, eval_ratio: int):\n  modes = [\"eval\", \"train\"]\n  partitioner = RandomPartition(NoteSequence, modes, [eval_ratio])\n  dag = {partitioner: DagInput(NoteSequence)}\n  for mode in modes:\n    validator = TensorValidator(NoteSequence, f\"{mode}_TensorValidator\", config)\n    dag[validator] = partitioner[f\"{mode}\"]\n    dag[DagOutput(f\"{mode}\")] = validator\n\n  pipeline = DAGPipeline(dag)\n  run_pipeline_serial(\n    pipeline, tf_record_iterator(input, pipeline.input_type), output_dir)\n```", "```py\nfrom magenta.models.music_vae.configs import CONFIG_MAP\nfrom magenta.pipelines.pipeline import Pipeline\n\nclass TensorValidator(Pipeline):\n\n  def __init__(self, type_, name, config):\n    super(TensorValidator, self).__init__(type_, type_, name)\n    self._model = CONFIG_MAP[config]\n    self._data_converter = self._model.data_converter\n\n  def transform(self, note_sequence):\n    tensors = self._data_converter.to_tensors(note_sequence)\n    if not tensors.lengths:\n      path = str(note_sequence).split('\\n')[0:2]\n      print(f\"Empty tensor for {path}\")\n      return []\n    return [note_sequence]\n```", "```py\nimport argparse\n\nparser = argparse.ArgumentParser()\nparser.add_argument(\"--config\", type=str, required=True)\nparser.add_argument(\"--input\", type=str, required=True)\nparser.add_argument(\"--output_dir\", type=str, required=True)\nparser.add_argument(\"--eval_ratio\", type=float, default=0.1)\n\ndef main():\n  args = parser.parse_args()\n  partition(args.config, args.input, args.output_dir, args.eval_ratio)\n\nif __name__ == \"__main__\":\n  main()\n```", "```py\npython PATH_TO_PYTHON_SCRIPT --config=\"cat-drums_2bar_small\" --input=\"PATH_TO_DATASET_TFRECORDS\" --output_dir=\"sequence_examples\"\n```", "```py\n# Start the training job\nmusic_vae_train --config=\"cat-drums_2bar_small\" --run_dir=\"logdir/run1\" --mode=\"train\" --examples_path=\"sequence_examples/train.tfrecord\"\n\n# Start the evaluation job\nmusic_vae_train --config=\"cat-drums_2bar_small\" --run_dir=\"logdir/run1\" --mode=\"eval\" --examples_path=\"sequence_examples/eval.tfrecord\"\n\n# Start the TensorBoard\ntensorboard --logdir=logdir\n```", "```py\nmkdir \"trained/cat-drums_2bar_small\"\ncp logdir/run1/train/model.ckpt-STEP* \"trained/cat-drums_2bar_small\"\n```", "```py\nmusic_vae_generate --config=\"cat-drums_2bar_small\" --checkpoint_file=\"trained/cat-drums_2bar_small/model.ckpt-STEP\"\n```", "```py\nssh-keygen\n```", "```py\nssh-rsa AAAA... user@host\n```", "```py\nssh USER@IP\n```", "```py\nThis VM requires Nvidia drivers to function correctly. Installation takes ~1 minute.\nWould you like to install the Nvidia driver? [y/n] y\n```", "```py\nscp PATH_TO_CUDNN_ARCHIVE USER@IP:\n```", "```py\nssh USER@IP\n```", "```py\n# On the VM instance\ntar -xzvf CUDNN_ARCHIVE_NAME\n```", "```py\n# On the VM instance\nsudo cp cuda/include/cudnn.h /usr/local/cuda/include\nsudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64\nsudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*\n```", "```py\n# On the VM instance\nsudo apt install libasound2-dev libsndfile-dev\n```", "```py\n# On the VM instance\nconda create -n magenta python=3.6\nconda activate magenta\n```", "```py\n# On the VM instance\npip install magenta-gpu\n```", "```py\nscp PATH_TO_DATASET USER@IP:\n```", "```py\n# On the VM instance\nmelody_rnn_create_dataset --config=\"attention_rnn\" --input=\"PATH_TO_NOTE_SEQUENCE_TFRECORDS\" --output_dir=\"sequence_examples\" --eval_ratio=0.10\n\n# On the VM instance\nmelody_rnn_train --config=\"attention_rnn\" --run_dir=\"logdir/run1\" --sequence_example_file=\"sequence_examples/training_melodies.tfrecord\" --hparams=\"batch_size=128,rnn_layer_sizes=[128,128]\" --num_training_steps=20000\n\n```", "```py\n# On the VM instance\nCUDA_VISIBLE_DEVICES=\"\" melody_rnn_train --config=\"attention_rnn\" --run_dir=\"logdir/run1\" --sequence_example_file=\"sequence_examples/eval_melodies.tfrecord\" --hparams=\"batch_size=62,rnn_layer_sizes=[128,128]\" --num_training_steps=20000 --eval\n```", "```py\nssh -L 16006:127.0.0.1:6006 USER@IP\n```", "```py\n# On the VM instance\ntensorboard --logdir=logdir\n```"]