["```py\ncalc_n_plot_critical_ratio(p=2000, c=400, r=0, mean=20, std=5)\n```", "```py\nCost of underage is 1600\nCost of overage is 400\nCritical ratio is 0.8\nOptimal order qty is 24.20810616786457\n```", "```py\n    class InventoryEnv(gym.Env):\n        def __init__(self, config={}):\n            self.l = config.get(\"lead time\", 5)\n            self.storage_capacity = 4000\n            self.order_limit = 1000\n            self.step_count = 0\n            self.max_steps = 40\n            self.max_value = 100.0\n            self.max_holding_cost = 5.0\n            self.max_loss_goodwill = 10.0\n            self.max_mean = 200\n    ```", "```py\n            self.inv_dim = max(1, self.l)\n            space_low = self.inv_dim * [0]\n            space_high = self.inv_dim * [self.storage_capacity]\n            space_low += 5 * [0]\n            space_high += [\n                self.max_value,\n                self.max_value,\n                self.max_holding_cost,\n                self.max_loss_goodwill,\n                self.max_mean,\n            ]\n            self.observation_space = spaces.Box(\n                low=np.array(space_low), \n                high=np.array(space_high), \n                dtype=np.float32\n            )\n    ```", "```py\n            self.action_space = spaces.Box(\n                low=np.array([0]), \n                high=np.array([1]), \n                dtype=np.float32\n            )\n    ```", "```py\n        def _normalize_obs(self):\n            obs = np.array(self.state)\n            obs[:self.inv_dim] = obs[:self.inv_dim] / self.order_limit\n            obs[self.inv_dim] = obs[self.inv_dim] / self.max_value\n            obs[self.inv_dim + 1] = obs[self.inv_dim + 1] / self.max_value\n            obs[self.inv_dim + 2] = obs[self.inv_dim + 2] / self.max_holding_cost\n            obs[self.inv_dim + 3] = obs[self.inv_dim + 3] / self.max_loss_goodwill\n            obs[self.inv_dim + 4] = obs[self.inv_dim + 4] / self.max_mean\n            return obs\n    ```", "```py\n        def reset(self):\n            self.step_count = 0\n            price = np.random.rand() * self.max_value\n            cost = np.random.rand() * price\n            holding_cost = np.random.rand() * min(cost, self.max_holding_cost)\n            loss_goodwill = np.random.rand() * self.max_loss_goodwill\n            mean_demand = np.random.rand() * self.max_mean\n            self.state = np.zeros(self.inv_dim + 5)\n            self.state[self.inv_dim] = price\n            self.state[self.inv_dim + 1] = cost\n            self.state[self.inv_dim + 2] = holding_cost\n            self.state[self.inv_dim + 3] = loss_goodwill\n            self.state[self.inv_dim + 4] = mean_demand\n            return self._normalize_obs()\n    ```", "```py\n        def step(self, action):\n            beginning_inv_state, p, c, h, k, mu = \\\n                self.break_state()\n            action = np.clip(action[0], 0, 1)\n            action = int(action * self.order_limit)\n            done = False\n    ```", "```py\n            available_capacity = self.storage_capacity \\\n                                 - np.sum(beginning_inv_state)\n            assert available_capacity >= 0\n            buys = min(action, available_capacity)\n            # If lead time is zero, immediately\n            # increase the inventory\n            if self.l == 0:\n                self.state[0] += buys\n            on_hand = self.state[0]\n            demand_realization = np.random.poisson(mu)\n    ```", "```py\n            # Compute Reward\n            sales = min(on_hand,\n                        demand_realization)\n            sales_revenue = p * sales\n            overage = on_hand - sales\n            underage = max(0, demand_realization\n                              - on_hand)\n            purchase_cost = c * buys\n            holding = overage * h\n            penalty_lost_sale = k * underage\n            reward = sales_revenue \\\n                     - purchase_cost \\\n                     - holding \\\n                     - penalty_lost_sale\n    ```", "```py\n            # Day is over. Update the inventory\n            # levels for the beginning of the next day\n            # In-transit inventory levels shift to left\n            self.state[0] = 0\n            if self.inv_dim > 1:\n                self.state[: self.inv_dim - 1] \\\n                    = self.state[1: self.inv_dim]\n            self.state[0] += overage\n            # Add the recently bought inventory\n            # if the lead time is positive\n            if self.l > 0:\n                self.state[self.l - 1] = buys\n            self.step_count += 1\n            if self.step_count >= self.max_steps:\n                done = True\n    ```", "```py\n            # Normalize the reward\n            reward = reward / 10000\n            info = {\n                \"demand realization\": demand_realization,\n                \"sales\": sales,\n                \"underage\": underage,\n                \"overage\": overage,\n            }\n            return self._normalize_obs(), reward, done, info\n    ```", "```py\ndef get_action_from_benchmark_policy(env):\n    inv_state, p, c, h, k, mu = env.break_state()\n    cost_of_overage = h\n    cost_of_underage = p - c + k\n    critical_ratio = np.clip(\n        0, 1, cost_of_underage\n              / (cost_of_underage + cost_of_overage)\n    )\n    horizon_target = int(poisson.ppf(critical_ratio,\n                         (len(inv_state) + 1) * mu))\n    deficit = max(0, horizon_target - np.sum(inv_state))\n    buy_action = min(deficit, env.order_limit)\n    return [buy_action / env.order_limit]\n```", "```py\nimport ray\nfrom ray import tune\nfrom inventory_env import InventoryEnv\nray.init()\ntune.run(\n    \"PPO\",\n    stop={\"timesteps_total\": 1e6},\n    num_samples=5,\n    config={\n        \"env\": InventoryEnv,\n        \"rollout_fragment_length\": 40,\n        \"num_gpus\": 1,\n        \"num_workers\": 50,\n        \"lr\": tune.grid_search([0.01, 0.001, 0.0001, 0.00001]),\n        \"use_gae\": tune.choice([True, False]),\n        \"train_batch_size\": tune.choice([5000, 10000, 20000, 40000]),\n        \"sgd_minibatch_size\": tune.choice([128, 1024, 4096, 8192]),\n        \"num_sgd_iter\": tune.choice([5, 10, 30]),\n        \"vf_loss_coeff\": tune.choice([0.1, 1, 10]),\n        \"vf_share_layers\": tune.choice([True, False]),\n        \"entropy_coeff\": tune.choice([0, 0.1, 1]),\n        \"clip_param\": tune.choice([0.05, 0.1, 0.3, 0.5]),\n        \"vf_clip_param\": tune.choice([1, 5, 10]),\n        \"grad_clip\": tune.choice([None, 0.01, 0.1, 1]),\n        \"kl_target\": tune.choice([0.005, 0.01, 0.05]),\n        \"eager\": False,\n    },\n)\n```", "```py\nimport numpy as np\nimport ray\nfrom ray.tune.logger import pretty_print\nfrom ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\nfrom ray.rllib.agents.ppo.ppo import PPOTrainer\nfrom inventory_env import InventoryEnv\nconfig = DEFAULT_CONFIG.copy()\nconfig[\"env\"] = InventoryEnv\nconfig[\"num_gpus\"] = 1\nconfig[\"num_workers\"] = 50\nconfig[\"clip_param\"] = 0.3\nconfig[\"entropy_coeff\"] = 0\nconfig[\"grad_clip\"] = None\nconfig[\"kl_target\"] = 0.005\nconfig[\"lr\"] = 0.001\nconfig[\"num_sgd_iter\"] = 5\nconfig[\"sgd_minibatch_size\"] = 8192\nconfig[\"train_batch_size\"] = 20000\nconfig[\"use_gae\"] = True\nconfig[\"vf_clip_param\"] = 10\nconfig[\"vf_loss_coeff\"] = 1\nconfig[\"vf_share_layers\"] = False\n```", "```py\nray.init()\ntrainer = PPOTrainer(config=config, env=InventoryEnv)\nbest_mean_reward = np.NINF\nwhile True:\n    result = trainer.train()\n    print(pretty_print(result))\n    mean_reward = result.get(\"episode_reward_mean\", np.NINF)\n    if mean_reward > best_mean_reward:\n        checkpoint = trainer.save()\n        print(\"checkpoint saved at\", checkpoint)\n        best_mean_reward = mean_reward\n```", "```py\nAverage daily reward over 2000 test episodes: 0.15966589703658918\\. \nAverage total epsisode reward: 6.386635881463566\n```", "```py\nAverage daily reward over 2000 test episodes: 0.14262437792900876\\. \nAverage total epsisode reward: 5.70497511716035\n```"]