<html><head></head><body>
<div id="_idContainer123">
<h1 class="c apter-number" id="_idParaDest-97"><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-98"><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.2.1">Analyzing Images with Computer Vision</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Computer vision is </span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.4.1">one of the fields in which deep learning has progressed enormously, surpassing human-level performance in several tasks such as image classification and object recognition. </span><span class="koboSpan" id="kobo.4.2">Furthermore, the field has moved from academia to real-world applications, and the industry is recognizing its practitioners as adding high value </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">to businesses.</span></span></p>
<p><span class="koboSpan" id="kobo.6.1">In this chapter, we will learn how to use GluonCV, a MXNet Gluon library specific to computer vision, how to build our own networks, and how to use GluonCV’s model zoo to use pretrained models for </span><span class="No-Break"><span class="koboSpan" id="kobo.7.1">several applications.</span></span></p>
<p><span class="koboSpan" id="kobo.8.1">Specifically, we will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.10.1">Understanding convolutional </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">neural networks</span></span></li>
<li><span class="koboSpan" id="kobo.12.1">Classifying images with AlexNet </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">and ResNet</span></span></li>
<li><span class="koboSpan" id="kobo.14.1">Detecting objects with Faster R-CNN </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">and YOLO</span></span></li>
<li><span class="koboSpan" id="kobo.16.1">Segmenting objects in images with PSPNet </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">and DeepLab-v3</span></span></li>
</ul>
<h1 id="_idParaDest-99"><a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.18.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.19.1">Apart from the technical requirements specified in the </span><em class="italic"><span class="koboSpan" id="kobo.20.1">Preface</span></em><span class="koboSpan" id="kobo.21.1">, the following technical requirements apply in </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.23.1">Ensure that you have completed </span><em class="italic"><span class="koboSpan" id="kobo.24.1">Installing MXNet, Gluon, GluonCV and GluonNLP</span></em><span class="koboSpan" id="kobo.25.1">, the first recipe from </span><a href="B16591_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.26.1">Chapter 1</span></em></span></a><span class="koboSpan" id="kobo.27.1">, </span><em class="italic"><span class="koboSpan" id="kobo.28.1">Up and Running </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.29.1">with MXNet</span></em></span></li>
<li><span class="koboSpan" id="kobo.30.1">Ensure that you have completed </span><em class="italic"><span class="koboSpan" id="kobo.31.1">A toy dataset for regression – load, manage, and visualize a house sales dataset</span></em><span class="koboSpan" id="kobo.32.1">, the first recipe from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.33.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.34.1">, </span><em class="italic"><span class="koboSpan" id="kobo.35.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.36.1">and DataLoader</span></em></span></li>
</ul>
<p><span class="koboSpan" id="kobo.37.1">The code for this chapter can be found at the following GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">URL: </span></span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch05"><span class="No-Break"><span class="koboSpan" id="kobo.39.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch05</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.40.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">Furthermore, you can access each recipe directly from Google Colab – for example, for the first recipe of this </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">chapter: </span></span><a href="https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch05/5_1_Understanding_Convolutional_Neural_Networks.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.43.1">https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch05/5_1_Understanding_Convolutional_Neural_Networks.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.44.1">.</span></span></p>
<h1 id="_idParaDest-100"><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.45.1">Understanding convolutional neural networks</span></h1>
<p><span class="koboSpan" id="kobo.46.1">In the previous chapters, we have used </span><em class="italic"><span class="koboSpan" id="kobo.47.1">fully connected</span></em> <strong class="bold"><span class="koboSpan" id="kobo.48.1">Multi-Layer Perceptron</span></strong><span class="koboSpan" id="kobo.49.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.50.1">MLP</span></strong><span class="koboSpan" id="kobo.51.1">) networks </span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.52.1">to solve our regression and classification problem. </span><span class="koboSpan" id="kobo.52.2">However, as we will see, these networks are not optimal for solving </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">image-related problems.</span></span></p>
<p><span class="koboSpan" id="kobo.54.1">Images are</span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.55.1"> highly dimensional entities – for example, each pixel in a color image has three features (red, green, and blue values), and a 1,024x1,024 image has more than 1 million pixels (a 1 megapixel image) and, therefore, more than 3 million features (3 * 10</span><span class="superscript"><span class="koboSpan" id="kobo.56.1">6</span></span><span class="koboSpan" id="kobo.57.1">). </span><span class="koboSpan" id="kobo.57.2">If we connect all these points in the input layer, to a second layer of 100 neurons for a </span><em class="italic"><span class="koboSpan" id="kobo.58.1">fully connected</span></em><span class="koboSpan" id="kobo.59.1"> network, we will require more than 10</span><span class="superscript"><span class="koboSpan" id="kobo.60.1">8</span></span><span class="koboSpan" id="kobo.61.1"> parameters, and that would be only for the first layer. </span><span class="koboSpan" id="kobo.61.2">Processing images is, therefore, a </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">time-intensive operation.</span></span></p>
<p><span class="koboSpan" id="kobo.63.1">Furthermore, imagine that we are trying to detect eyes in faces; if a pixel belongs to an eye, the likelihood of nearby pixels belonging to the eye is very high (think of the pixels that make up the iris, for example). </span><span class="koboSpan" id="kobo.63.2">When inputting all our pixels directly into our network, all the information connected to pixel location </span><span class="No-Break"><span class="koboSpan" id="kobo.64.1">is lost.</span></span></p>
<p><span class="koboSpan" id="kobo.65.1">An architecture called a </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">Convolutional Neural Network</span></strong><span class="koboSpan" id="kobo.67.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.68.1">CNN</span></strong><span class="koboSpan" id="kobo.69.1">) was developed to tackle these problems, and we will analyze the most important features of CNNs and how to implement them in this recipe to solve </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">image-related problems.</span></span></p>
<h2 id="_idParaDest-101"><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.71.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.72.1">As with the</span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.73.1"> previous chapters, in this recipe, we will use a few matrix operations and linear algebra, but it will not be </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">too difficult.</span></span></p>
<h2 id="_idParaDest-102"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.75.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.76.1">In this recipe, we will take the </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.78.1">Introduce convolutional </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">layer equations.</span></span></li>
<li><span class="koboSpan" id="kobo.80.1">Understand the convolution parameters and </span><span class="No-Break"><span class="koboSpan" id="kobo.81.1">receptive field.</span></span></li>
<li><span class="koboSpan" id="kobo.82.1">Run a convolutional layer example </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">with MXNet.</span></span></li>
<li><span class="koboSpan" id="kobo.84.1">Introduce pooling </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">layer equations.</span></span></li>
<li><span class="koboSpan" id="kobo.86.1">Run a pooling layer example </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">with MXNet.</span></span></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.88.1">Summarize CNNs.</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.89.1">Introducing convolutional layer equations</span></h3>
<p><span class="koboSpan" id="kobo.90.1">The</span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.91.1"> location problems described in the recipe introduction are</span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.92.1"> formally known as </span><strong class="bold"><span class="koboSpan" id="kobo.93.1">translation invariance</span></strong><span class="koboSpan" id="kobo.94.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.95.1">locality</span></strong><span class="koboSpan" id="kobo.96.1">. </span><span class="koboSpan" id="kobo.96.2">In CNNs, these problems is solved by using the convolution/cross-correlation operation in the so-called </span><span class="No-Break"><span class="koboSpan" id="kobo.97.1">convolutional layers.</span></span></p>
<p><span class="koboSpan" id="kobo.98.1">In the convolution, we have an input image (or </span><strong class="bold"><span class="koboSpan" id="kobo.99.1">feature map</span></strong><span class="koboSpan" id="kobo.100.1"> – see the following </span><em class="italic"><span class="koboSpan" id="kobo.101.1">Important note</span></em><span class="koboSpan" id="kobo.102.1"> relating to convolutional layer equations) that is combined with a </span><strong class="bold"><span class="koboSpan" id="kobo.103.1">kernel</span></strong><span class="koboSpan" id="kobo.104.1">, which</span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.105.1"> are the learnable parameters of this layer. </span><span class="koboSpan" id="kobo.105.2">The simplest way to see how this operation works is with an example – if we had a 3x3 input and we wanted to combine it with a 2x2 kernel, it would look like a sliding window, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.106.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer083">
<span class="koboSpan" id="kobo.107.1"><img alt="Figure 5.1 – The convolutional layer" src="image/B16591_05_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.108.1">Figure 5.1 – The convolutional layer</span></p>
<p><span class="koboSpan" id="kobo.109.1">As</span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.110.1"> shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.111.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.112.1">.1</span></em><span class="koboSpan" id="kobo.113.1">, to compute 1 pixel of the output, we can intuitively place the kernel over the input and compute multiplications, and then add all these values to obtain the final result. </span><span class="koboSpan" id="kobo.113.2">This helps a network learn features from </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">the image.</span></span></p>
<p><span class="koboSpan" id="kobo.115.1">Furthermore, this operation is less computing intensive than the </span><em class="italic"><span class="koboSpan" id="kobo.116.1">fully connected</span></em><span class="koboSpan" id="kobo.117.1"> layers, addressing the computing problem </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">we identified.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.119.1">Important note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.120.1">When using a convolution layer as the first step (typical in CNNs), the input is the full image. </span><span class="koboSpan" id="kobo.120.2">Moreover, the output can be understood as another image of a lower dimension with certain properties, given by the kernel. </span><span class="koboSpan" id="kobo.120.3">As kernels are learned to highlight certain features of the image, these outputs are known as </span><em class="italic"><span class="koboSpan" id="kobo.121.1">feature maps</span></em><span class="koboSpan" id="kobo.122.1">. </span><span class="koboSpan" id="kobo.122.2">In the layer close to the input, each pixel of these feature maps is a combination of a small number of pixels of the image (for example, horizontal or vertical lines). </span><span class="koboSpan" id="kobo.122.3">As data travels through convolutional layers, these feature maps represent higher levels of abstraction (for example, eyes on </span><span class="No-Break"><span class="koboSpan" id="kobo.123.1">a face).</span></span></p>
<h3><span class="koboSpan" id="kobo.124.1">Understanding the convolution parameters and receptive field</span></h3>
<p><span class="koboSpan" id="kobo.125.1">The example shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.126.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.127.1">.1</span></em><span class="koboSpan" id="kobo.128.1"> is quite simple, for illustrative purposes; the input size is 6x6 and</span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.129.1"> the kernel size is 3x3. </span><span class="koboSpan" id="kobo.129.2">These sizes are variable and depend on the network architecture. </span><span class="koboSpan" id="kobo.129.3">However, there are three parameters that are very important to calculate the output size; these are padding, stride, </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">and dilation.</span></span></p>
<p><span class="koboSpan" id="kobo.131.1">Padding is the </span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.132.1">number of zero-valued pixels (rows/columns) that are added to the input. </span><span class="koboSpan" id="kobo.132.2">The larger the padding, the larger the output, and effectively, this increases the input size. </span><span class="koboSpan" id="kobo.132.3">In the example, padding is </span><strong class="source-inline"><span class="koboSpan" id="kobo.133.1">1</span></strong><span class="koboSpan" id="kobo.134.1"> (represented </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">as </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.136.1">p</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.137.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.138.1">As mentioned, we</span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.139.1"> can intuitively place the kernel over the input, compute multiplications, and then add all these results to obtain the final value. </span><span class="koboSpan" id="kobo.139.2">For the next value, we need to move the kernel to a different position, as shown in the </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">following diagram.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer084">
<span class="koboSpan" id="kobo.141.1"><img alt="Figure 5.2 – The stride parameter" src="image/B16591_05_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.142.1">Figure 5.2 – The stride parameter</span></p>
<p><span class="koboSpan" id="kobo.143.1">The number of spaces the kernel moves is defined by the stride. </span><span class="koboSpan" id="kobo.143.2">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.144.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.145.1">.2</span></em><span class="koboSpan" id="kobo.146.1">, we can see an example of the stride being 2, with each 3x3 kernel separated by </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">2 values.</span></span></p>
<p><span class="koboSpan" id="kobo.148.1">Dilation is </span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.149.1">defined by how separated each input value that is used in the convolution with the </span><span class="No-Break"><span class="koboSpan" id="kobo.150.1">kernel is.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer085">
<span class="koboSpan" id="kobo.151.1"><img alt="Figure 5.3 – The dilation parameter" src="image/B16591_05_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.152.1">Figure 5.3 – The dilation parameter</span></p>
<p><span class="koboSpan" id="kobo.153.1">As we can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.154.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.155.1">.3</span></em><span class="koboSpan" id="kobo.156.1">, different dilation parameters combine elements of </span><span class="No-Break"><span class="koboSpan" id="kobo.157.1">the input.</span></span></p>
<p><span class="koboSpan" id="kobo.158.1">These parameters </span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.159.1">define what is known as the </span><strong class="bold"><span class="koboSpan" id="kobo.160.1">receptive field</span></strong><span class="koboSpan" id="kobo.161.1">, which is the size of the region of the input that produces an activation. </span><span class="koboSpan" id="kobo.161.2">It is an important parameter because only a feature present in the receptive field of our model will be represented in </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">the output.</span></span></p>
<p><span class="koboSpan" id="kobo.163.1">In the example in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.164.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.165.1">.1</span></em><span class="koboSpan" id="kobo.166.1">, a 6x6 input, combined with a 3x3 kernel, with a stride of 2, padding of 1, and dilation of 1, yields a 3x3 output and has a receptive field of the full input (all pixels in the input are at least </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">used once).</span></span></p>
<p><span class="koboSpan" id="kobo.168.1">Another very interesting property of these parameters is that given the right combinations, you can have an output that is the same size as the input. </span><span class="koboSpan" id="kobo.168.2">The equation that gives the dimensions of the output is as follows (the equation needs to be applied to the height and the </span><span class="No-Break"><span class="koboSpan" id="kobo.169.1">width, respectively):</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.170.1">o</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.171.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.172.1">[</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.173.1">i</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.174.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.175.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.176.1">*</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.177.1">p</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.178.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.179.1">k</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.180.1">−</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.181.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.182.1">k</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.183.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.184.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.185.1">)</span></span><span class="_-----MathTools-_Math_Symbol_v-normal"><span class="koboSpan" id="kobo.186.1">*</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.187.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.188.1">d</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.189.1">−</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.190.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.191.1">)</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.192.1">]</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.193.1">/</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.194.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.195.1">+</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.196.1">1</span></span></p>
<p><span class="koboSpan" id="kobo.197.1">In the </span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.198.1">preceding equation, </span><em class="italic"><span class="koboSpan" id="kobo.199.1">o</span></em><span class="koboSpan" id="kobo.200.1"> is the output dimension (height or width if working with 2D images), </span><em class="italic"><span class="koboSpan" id="kobo.201.1">i</span></em><span class="koboSpan" id="kobo.202.1"> is the input dimension (height/width), </span><em class="italic"><span class="koboSpan" id="kobo.203.1">p</span></em><span class="koboSpan" id="kobo.204.1"> is padding, </span><em class="italic"><span class="koboSpan" id="kobo.205.1">k</span></em><span class="koboSpan" id="kobo.206.1"> is the kernel size, </span><em class="italic"><span class="koboSpan" id="kobo.207.1">d</span></em><span class="koboSpan" id="kobo.208.1"> is dilation, and </span><em class="italic"><span class="koboSpan" id="kobo.209.1">s</span></em><span class="koboSpan" id="kobo.210.1"> is stride. </span><span class="koboSpan" id="kobo.210.2">There are different combinations that will give the input and the output the same size, one of which is </span><em class="italic"><span class="koboSpan" id="kobo.211.1">p = 1, k = 3, d = 1, s = </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.212.1">1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.214.1">Running a convolutional layer example with MXNet</span></h3>
<p><span class="koboSpan" id="kobo.215.1">We can</span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.216.1"> implement the following example using MXNet capabilities (note that padding is 0, stride is 1, and dilation </span><span class="No-Break"><span class="koboSpan" id="kobo.217.1">is 1):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer086">
<span class="koboSpan" id="kobo.218.1"><img alt="Figure 5.4 – A convolution example" src="image/B16591_05_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.219.1">Figure 5.4 – A convolution example</span></p>
<p><span class="koboSpan" id="kobo.220.1">If we want to follow the example depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.221.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.222.1">.4</span></em><span class="koboSpan" id="kobo.223.1">, applying the convolution operation to a 3x3 matrix with a 2x2 kernel, we can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.225.1">
X = mx.np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
 K = mx.np.array([[0.0, 1.0], [2.0, 3.0]]) convolution(X, K)</span></pre> <p><span class="koboSpan" id="kobo.226.1">These operations give the following as </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">a result:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.228.1">
array([[19., 25.],
       [37., 43.]])</span></pre> <p><span class="koboSpan" id="kobo.229.1">This is </span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.230.1">the expected result for the defined convolution step (taking into account the given padding, stride, and </span><span class="No-Break"><span class="koboSpan" id="kobo.231.1">dilation parameters).</span></span></p>
<h3><span class="koboSpan" id="kobo.232.1">Introducing the pooling layer equations</span></h3>
<p><span class="koboSpan" id="kobo.233.1">As </span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.234.1">described earlier, a desirable property of neural network models when working with images is that as we traverse a network, we can increasingly process higher-level features, or equivalently, each pixel in the deep feature maps has a higher receptive field from </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">the input.</span></span></p>
<p><span class="koboSpan" id="kobo.236.1">The operation performed in these layers is similar to the convolutional layers, in the sense that we take a kernel of constant dimensions and apply a sliding window. </span><span class="koboSpan" id="kobo.236.2">However, in this case, the kernel parameters are constant and, therefore, not learned during the training of the network. </span><span class="koboSpan" id="kobo.236.3">This kernel is seen as an operation (a function) and is typically either the </span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.237.1">max function (the </span><strong class="bold"><span class="koboSpan" id="kobo.238.1">max pooling layer</span></strong><span class="koboSpan" id="kobo.239.1">) or the</span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.240.1"> average function (the </span><strong class="bold"><span class="koboSpan" id="kobo.241.1">average </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.242.1">pooling layer</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer087">
<span class="koboSpan" id="kobo.244.1"><img alt="Figure 5.5 – The max pooling layer" src="image/B16591_05_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.245.1">Figure 5.5 – The max pooling layer</span></p>
<p><span class="koboSpan" id="kobo.246.1">Furthermore, by combining pixels that are nearby, we also achieve local invariance, another desirable property to </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">process images.</span></span></p>
<h3><span class="koboSpan" id="kobo.248.1">Running a pooling layer example with MXNet</span></h3>
<p><span class="koboSpan" id="kobo.249.1">We </span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.250.1">can implement the example shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.251.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.252.1">.5</span></em><span class="koboSpan" id="kobo.253.1"> using MXNet's capabilities, as </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">shown here:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.255.1">
X = mx.np.array([[0.0, 1.0, 2.0], [3.0, 4.0, 5.0], [6.0, 7.0, 8.0]])
 maxpool(X, (2, 2))</span></pre> <p><span class="koboSpan" id="kobo.256.1">These operations give the following as </span><span class="No-Break"><span class="koboSpan" id="kobo.257.1">a result:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.258.1">
array([[4., 5.],
       [7., 8.]])</span></pre> <p><span class="koboSpan" id="kobo.259.1">This is the expected result for the defined 2x2 max </span><span class="No-Break"><span class="koboSpan" id="kobo.260.1">pooling step.</span></span></p>
<h3><span class="koboSpan" id="kobo.261.1">Summarizing CNNs</span></h3>
<p><span class="koboSpan" id="kobo.262.1">A</span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.263.1"> typical</span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.264.1"> CNN for </span><strong class="bold"><span class="koboSpan" id="kobo.265.1">image classification</span></strong><span class="koboSpan" id="kobo.266.1"> has, therefore, two </span><span class="No-Break"><span class="koboSpan" id="kobo.267.1">different parts:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.268.1">Feature extraction</span></strong><span class="koboSpan" id="kobo.269.1">: This is</span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.270.1"> also known as the network backbone. </span><span class="koboSpan" id="kobo.270.2">It is built with combinations of the convolutional and pooling layers seen in this recipe. </span><span class="koboSpan" id="kobo.270.3">The input to each layer is feature maps (an image and all its channels in the input layer), and the output is feature maps of reduced dimensions</span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.271.1"> but with a larger number of channels. </span><span class="koboSpan" id="kobo.271.2">Layers are typically stacked – a convolutional layer, an activation function (typically, </span><strong class="bold"><span class="koboSpan" id="kobo.272.1">Rectified Linear Unit</span></strong><span class="koboSpan" id="kobo.273.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.274.1">ReLU</span></strong><span class="koboSpan" id="kobo.275.1">)), and a max </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">pooling layer.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.277.1">Classifier</span></strong><span class="koboSpan" id="kobo.278.1">: The </span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.279.1">last feature map is reshaped into a vector that can then be applied to </span><em class="italic"><span class="koboSpan" id="kobo.280.1">fully connected</span></em><span class="koboSpan" id="kobo.281.1"> layers, as discussed in </span><a href="B16591_04.xhtml#_idTextAnchor075"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.282.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.283.1">, </span><em class="italic"><span class="koboSpan" id="kobo.284.1">Solving Classification Problems</span></em><span class="koboSpan" id="kobo.285.1">. </span><span class="koboSpan" id="kobo.285.2">The number of neurons in the output layer is equivalent to the number of classes that an image needs to be</span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.286.1"> classified as, using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.287.1">softmax</span></strong><span class="koboSpan" id="kobo.288.1"> function as the activation. </span><span class="koboSpan" id="kobo.288.2">The number of layers in the classifier depends on the </span><span class="No-Break"><span class="koboSpan" id="kobo.289.1">specific problem.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.290.1">Therefore, a CNN architecture</span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.291.1"> can be </span><span class="No-Break"><span class="koboSpan" id="kobo.292.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer088">
<span class="koboSpan" id="kobo.293.1"><img alt="Figure 5.6 – CNN architecture" src="image/B16591_05_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.294.1">Figure 5.6 – CNN architecture</span></p>
<p><span class="koboSpan" id="kobo.295.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.296.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.297.1">.6</span></em><span class="koboSpan" id="kobo.298.1">, we can see a CNN architecture for </span><em class="italic"><span class="koboSpan" id="kobo.299.1">image classification</span></em><span class="koboSpan" id="kobo.300.1">, composed of two stages for feature extraction, each stage combining a convolution layer (with the ReLU activation function) and a max pooling layer. </span><span class="koboSpan" id="kobo.300.2">Then, for the classifier, the remaining feature map is flattened into a vector and passed through a </span><em class="italic"><span class="koboSpan" id="kobo.301.1">fully connected</span></em><span class="koboSpan" id="kobo.302.1"> layer to finally provide the output with the </span><em class="italic"><span class="koboSpan" id="kobo.303.1">softmax</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.304.1">activation function.</span></span></p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.305.1">How it works…</span></h2>
<p><span class="koboSpan" id="kobo.306.1">In this recipe, we have introduced CNNs. </span><span class="koboSpan" id="kobo.306.2">This architecture has been developed since early the 2000s and is responsible for the revolution in </span><strong class="bold"><span class="koboSpan" id="kobo.307.1">computer vision</span></strong><span class="koboSpan" id="kobo.308.1"> applications, and making </span><strong class="bold"><span class="koboSpan" id="kobo.309.1">deep learning</span></strong><span class="koboSpan" id="kobo.310.1"> become the spotlight in most data-oriented tasks including </span><strong class="bold"><span class="koboSpan" id="kobo.311.1">natural language processing</span></strong><span class="koboSpan" id="kobo.312.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.313.1">speech recognition</span></strong><span class="koboSpan" id="kobo.314.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.315.1">image generation</span></strong><span class="koboSpan" id="kobo.316.1">, and so on, reaching state-of-the-art in all </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">of them.</span></span></p>
<p><span class="koboSpan" id="kobo.318.1">We have understood how CNNs work internally, exploring the concepts of </span><em class="italic"><span class="koboSpan" id="kobo.319.1">feature maps</span></em><span class="koboSpan" id="kobo.320.1">, </span><em class="italic"><span class="koboSpan" id="kobo.321.1">receptive field</span></em><span class="koboSpan" id="kobo.322.1">, and the mathematical concepts behind the main layers of these architectures, the </span><em class="italic"><span class="koboSpan" id="kobo.323.1">convolutional</span></em><span class="koboSpan" id="kobo.324.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.325.1">max pooling</span></em><span class="koboSpan" id="kobo.326.1"> layers, and how they are combined to build a complete </span><span class="No-Break"><span class="koboSpan" id="kobo.327.1">CNN model.</span></span></p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.328.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.329.1">CNNs have evolved rapidly; in 1998, one of the first CNNs was published, solving a practical </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">problem: </span></span><a href="http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.331.1">http://yann.lecun.com/exdb/publis/pdf/lecun-01a.pdf</span></span></a></p>
<p><span class="koboSpan" id="kobo.332.1">After that, it was not until 2012, with AlexNet, that CNNs gained worldwide attention, and since then, progress quickly developed, until they surpassed human-level performance. </span><span class="koboSpan" id="kobo.332.2">For more information on the history of CNNs, refer to this </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">article: </span></span><a href="https://towardsdatascience.com/from-lenet-to-efficientnet-the-evolution-of-cnns-3a57eb34672f"><span class="No-Break"><span class="koboSpan" id="kobo.334.1">https://towardsdatascience.com/from-lenet-to-efficientnet-the-evolution-of-cnns-3a57eb34672f</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.335.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.336.1">We briefly touched on the topics of translation invariance and locality. </span><span class="koboSpan" id="kobo.336.2">For more information on these topics, </span><span class="No-Break"><span class="koboSpan" id="kobo.337.1">visit </span></span><a href="https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html"><span class="No-Break"><span class="koboSpan" id="kobo.338.1">https://d2l.ai/chapter_convolutional-neural-networks/why-conv.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.339.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.340.1">The relationship between convolution and cross-correlation is discussed </span><span class="No-Break"><span class="koboSpan" id="kobo.341.1">here: </span></span><a href="https://towardsdatascience.com/convolution-vs-correlation-af868b6b4fb5"><span class="No-Break"><span class="koboSpan" id="kobo.342.1">https://towardsdatascience.com/convolution-vs-correlation-af868b6b4fb5</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.343.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.344.1">For a better understanding of matrix dimensions, padding, stride, dilation, and receptive field, a good explanation is provided </span><span class="No-Break"><span class="koboSpan" id="kobo.345.1">here: </span></span><a href="https://theaisummer.com/receptive-field/"><span class="No-Break"><span class="koboSpan" id="kobo.346.1">https://theaisummer.com/receptive-field/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.347.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.348.1">CNNs have been state-of-the-art for image classification until quite recently; in October 2020, </span><strong class="bold"><span class="koboSpan" id="kobo.349.1">Transformers</span></strong><span class="koboSpan" id="kobo.350.1"> were applied to computer vision tasks by Google Brain with </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.351.1">ViT</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">: </span></span><a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html"><span class="No-Break"><span class="koboSpan" id="kobo.353.1">https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.354.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.355.1">In a nutshell, instead of forcing a network with the locality principle, Transformer architectures allow the same model to decide which features matter most at any layer, local or global. </span><span class="koboSpan" id="kobo.355.2">This behavior is</span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.356.1"> called </span><strong class="bold"><span class="koboSpan" id="kobo.357.1">self-attention</span></strong><span class="koboSpan" id="kobo.358.1">. </span><span class="koboSpan" id="kobo.358.2">Transformers are state-of-the-art for image classification at the time </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">of writing.</span></span></p>
<p><span class="koboSpan" id="kobo.360.1">In this chapter, we are going to analyze in detail the following tasks – </span><strong class="bold"><span class="koboSpan" id="kobo.361.1">image classification</span></strong><span class="koboSpan" id="kobo.362.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.363.1">object detection</span></strong><span class="koboSpan" id="kobo.364.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.365.1">image segmentation</span></strong><span class="koboSpan" id="kobo.366.1">. </span><span class="koboSpan" id="kobo.366.2">However, </span><em class="italic"><span class="koboSpan" id="kobo.367.1">MXNet GluonCV Model Zoo</span></em><span class="koboSpan" id="kobo.368.1"> contains lots of models pre-trained for a large number of tasks. </span><span class="koboSpan" id="kobo.368.2">You are encouraged to explore the different examples provided </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">at </span></span><a href="https://cv.gluon.ai/model_zoo/index.html"><span class="No-Break"><span class="koboSpan" id="kobo.370.1">https://cv.gluon.ai/model_zoo/index.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.371.1">.</span></span></p>
<h1 id="_idParaDest-105"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.372.1">Classifying images with MXNet – GluonCV Model Zoo, AlexNet, and ResNet</span></h1>
<p><span class="koboSpan" id="kobo.373.1">MXNet</span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.374.1"> provides a variety of tools to compose custom deep learning models. </span><span class="koboSpan" id="kobo.374.2">In this recipe, we will see how to use MXNet to build a model from scratch, train it, and use it to classify images from a dataset. </span><span class="koboSpan" id="kobo.374.3">We will also see that although this approach works fine, it </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">is time-consuming.</span></span></p>
<p><span class="koboSpan" id="kobo.376.1">Another option, and one of </span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.377.1">the highest value features that MXNet and GluonCV provide, is their </span><strong class="bold"><span class="koboSpan" id="kobo.378.1">Model Zoo</span></strong><span class="koboSpan" id="kobo.379.1">. </span><span class="koboSpan" id="kobo.379.2">GluonCV Model Zoo</span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.380.1"> is a set of pre-trained, ready-to-go models, for use with your own applications. </span><span class="koboSpan" id="kobo.380.2">We will see how to use Model Zoo with two very</span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.381.1"> important models for</span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.382.1"> image classification – </span><strong class="bold"><span class="koboSpan" id="kobo.383.1">AlexNet</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.384.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.385.1">ResNet</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.386.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.387.1">In this recipe, we will analyze and compare these approaches to classify images on a reduced version of the </span><em class="italic"><span class="koboSpan" id="kobo.388.1">Dogs vs. </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.389.1">Cats</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.390.1"> dataset.</span></span></p>
<h2 id="_idParaDest-106"><a id="_idTextAnchor107"/><span class="koboSpan" id="kobo.391.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.392.1">As with previous chapters, in this recipe, we will use a few matrix operations and linear algebra, but it will not be </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">too difficult.</span></span></p>
<p><span class="koboSpan" id="kobo.394.1">Furthermore, we will be classifying image datasets; therefore, we will revisit some concepts that we've </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">already seen:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.396.1">Understanding image datasets- load, manage, and visualize the Fashion MNIST dataset</span></em><span class="koboSpan" id="kobo.397.1">, the third recipe from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.398.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.399.1">, </span><em class="italic"><span class="koboSpan" id="kobo.400.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.401.1">and DataLoader</span></em></span></li>
<li><a href="B16591_04.xhtml#_idTextAnchor075"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.402.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.403.1">, </span><em class="italic"><span class="koboSpan" id="kobo.404.1">Solving </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.405.1">Classification Problems</span></em></span></li>
</ul>
<h2 id="_idParaDest-107"><a id="_idTextAnchor108"/><span class="koboSpan" id="kobo.406.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.407.1">In this recipe, we will take the </span><span class="No-Break"><span class="koboSpan" id="kobo.408.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.409.1">Explore a reduced version of the </span><em class="italic"><span class="koboSpan" id="kobo.410.1">Dogs vs. </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.411.1">Cats</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.412.1"> dataset.</span></span></li>
<li><span class="koboSpan" id="kobo.413.1">Create an AlexNet custom model </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">from scratch.</span></span></li>
<li><span class="koboSpan" id="kobo.415.1">Train an </span><em class="italic"><span class="koboSpan" id="kobo.416.1">AlexNet</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.417.1">custom model.</span></span></li>
<li><span class="koboSpan" id="kobo.418.1">Evaluate an </span><em class="italic"><span class="koboSpan" id="kobo.419.1">AlexNet</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.420.1">custom model.</span></span></li>
<li><span class="koboSpan" id="kobo.421.1">Introduce </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">Model Zoo.</span></span></li>
<li><span class="koboSpan" id="kobo.423.1">Introduce ImageNet </span><span class="No-Break"><span class="koboSpan" id="kobo.424.1">pre-trained models.</span></span></li>
<li><span class="koboSpan" id="kobo.425.1">Load an </span><em class="italic"><span class="koboSpan" id="kobo.426.1">AlexNet</span></em><span class="koboSpan" id="kobo.427.1"> pre-trained model from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.428.1">Model Zoo</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.430.1">Evaluate an </span><em class="italic"><span class="koboSpan" id="kobo.431.1">AlexNet</span></em><span class="koboSpan" id="kobo.432.1"> pre-trained model from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.433.1">Model Zoo</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.435.1">Load a ResNet pre-trained model from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.436.1">Model Zoo</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.437.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.438.1">Evaluate a </span><em class="italic"><span class="koboSpan" id="kobo.439.1">ResNet</span></em><span class="koboSpan" id="kobo.440.1"> pre-trained model from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.441.1">Model Zoo</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.442.1">.</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.443.1">Exploring the reduced version of the dataset Dogs vs. </span><span class="koboSpan" id="kobo.443.2">Cats</span></h3>
<p><span class="koboSpan" id="kobo.444.1">For our </span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.445.1">image classification experiments, we will work with a new dataset, </span><em class="italic"><span class="koboSpan" id="kobo.446.1">Dogs vs. </span><span class="koboSpan" id="kobo.446.2">Cats</span></em><span class="koboSpan" id="kobo.447.1">. </span><span class="koboSpan" id="kobo.447.2">This is a Kaggle dataset (</span><a href="https://www.kaggle.com/c/dogs-vs-cats"><span class="koboSpan" id="kobo.448.1">https://www.kaggle.com/c/dogs-vs-cats</span></a><span class="koboSpan" id="kobo.449.1">) that can be downloaded manually. </span><span class="koboSpan" id="kobo.449.2">In this recipe, we will work with a reduced version of this dataset that can be downloaded </span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.450.1">from </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.451.1">Zenodo</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.452.1"> (</span></span><a href="https://zenodo.org/records/5226945"><span class="No-Break"><span class="koboSpan" id="kobo.453.1">https://zenodo.org/records/5226945</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.454.1">)</span></span></p>
<p><span class="koboSpan" id="kobo.455.1">From the set of images in the dataset (either a cat or a dog is depicted), our model will need to correctly classify these images. </span><span class="koboSpan" id="kobo.455.2">In the first step, as we saw in previous chapters, we are going to do </span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.456.1">some </span><strong class="bold"><span class="koboSpan" id="kobo.457.1">Exploratory Data </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.458.1">Analysis</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.459.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.460.1">EDA</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.461.1">).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer089">
<span class="koboSpan" id="kobo.462.1"><img alt="Figure 5.7 – The Dogs vs. Cats dataset" src="image/B16591_05_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.463.1">Figure 5.7 – The Dogs vs. </span><span class="koboSpan" id="kobo.463.2">Cats dataset</span></p>
<p><span class="koboSpan" id="kobo.464.1">As can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.465.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.466.1">.7</span></em><span class="koboSpan" id="kobo.467.1">, each image in the dataset is in color, and they are resized to 224 px by 224 px (width and height). </span><span class="koboSpan" id="kobo.467.2">There are 1,000 images in the training and validation set and 400 images in the </span><span class="No-Break"><span class="koboSpan" id="kobo.468.1">test set.</span></span></p>
<p><span class="koboSpan" id="kobo.469.1">As </span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.470.1">we did in </span><em class="italic"><span class="koboSpan" id="kobo.471.1">Understanding image datasets – load, manage, and visualize the Fashion MNIST dataset</span></em><span class="koboSpan" id="kobo.472.1">, the third recipe from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.473.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.474.1">, </span><em class="italic"><span class="koboSpan" id="kobo.475.1">Working with MXNet and Visualizing Datasets: Gluon and DataLoader</span></em><span class="koboSpan" id="kobo.476.1">, we can compute some visualizations using</span><a id="_idIndexMarker415"/><span class="koboSpan" id="kobo.477.1"> the dimensionality</span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.478.1"> reduction</span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.479.1"> techniques – </span><strong class="bold"><span class="koboSpan" id="kobo.480.1">Principal Component Analysis</span></strong><span class="koboSpan" id="kobo.481.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.482.1">PCA</span></strong><span class="koboSpan" id="kobo.483.1">), </span><strong class="bold"><span class="koboSpan" id="kobo.484.1">t-distributed stochastic neighbor embedding</span></strong><span class="koboSpan" id="kobo.485.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.486.1">t-SNE</span></strong><span class="koboSpan" id="kobo.487.1">), and </span><strong class="bold"><span class="koboSpan" id="kobo.488.1">Uniform Manifold Approximation and </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.489.1">Projection</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.490.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.491.1">UMAP</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.492.1">):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer090">
<span class="koboSpan" id="kobo.493.1"><img alt="Figure 5.8 – Dogs versus cats visualizations – PCA, t-SNE, and UMAP" src="image/1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.494.1">Figure 5.8 – Dogs versus cats visualizations – PCA, t-SNE, and UMAP</span></p>
<p><span class="koboSpan" id="kobo.495.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.496.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.497.1">.8</span></em><span class="koboSpan" id="kobo.498.1">, there is no clear boundary region to separate dogs versus cats. </span><span class="koboSpan" id="kobo.498.2">However, as we will see in the following sections, an architecture introduced in the previous chapter, CNNs, will achieve very good results on </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">the task.</span></span></p>
<h3><span class="koboSpan" id="kobo.500.1">Creating an AlexNet custom model from scratch</span></h3>
<p><span class="koboSpan" id="kobo.501.1">AlexNet was a </span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.502.1">deep</span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.503.1"> neural network that was developed by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012. </span><span class="koboSpan" id="kobo.503.2">It was designed to</span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.504.1"> compete in the </span><strong class="bold"><span class="koboSpan" id="kobo.505.1">ImageNet Large Scale Visual Recognition Challenge</span></strong><span class="koboSpan" id="kobo.506.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.507.1">ILSVRC</span></strong><span class="koboSpan" id="kobo.508.1">) in 2012 and was the first CNN-based model to win </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">this competition.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer091">
<span class="koboSpan" id="kobo.510.1"><img alt="Figure 5.9 – AlexNet" src="image/B16591_05_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.511.1">Figure 5.9 – AlexNet</span></p>
<p><span class="koboSpan" id="kobo.512.1">The</span><a id="_idIndexMarker421"/><span class="koboSpan" id="kobo.513.1"> network uses</span><a id="_idIndexMarker422"/><span class="koboSpan" id="kobo.514.1"> five convolutional layers and three </span><em class="italic"><span class="koboSpan" id="kobo.515.1">fully connected</span></em><span class="koboSpan" id="kobo.516.1"> layers. </span><span class="koboSpan" id="kobo.516.2">The activation function used is the ReLU, and it contains about 63 million </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">trainable parameters.</span></span></p>
<p><span class="koboSpan" id="kobo.518.1">To generate this network from scratch with MXNet, we can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">following code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.520.1">
def create_alexnet_network(num_classes=2):
    # Returns AlexNet architecture, as defined in MXNet source code
    net = nn.Sequential()
    net.add(
        nn.Conv2D(64, kernel_size=11, strides=4, activation='relu'),
        nn.MaxPool2D(pool_size=3, strides=2),
        nn.Conv2D(256, kernel_size=5, padding=2, activation='relu'),
        nn.MaxPool2D(pool_size=3, strides=2),
        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
        nn.Conv2D(384, kernel_size=3, padding=1, activation='relu'),
        nn.Conv2D(256, kernel_size=3, padding=1, activation='relu'),
        nn.MaxPool2D(pool_size=3, strides=2),        nn.Flatten(),
        # Last 3 layers is classifier
        # Adding dropout for regularization
        nn.Dense(4096, activation='relu'),
        nn.Dropout(0.5),
        nn.Dense(4096, activation='relu'),
        nn.Dropout(0.5),
        nn.Dense(num_classes)
    )
 return net</span></pre> <p><span class="koboSpan" id="kobo.521.1">This </span><a id="_idIndexMarker423"/><span class="koboSpan" id="kobo.522.1">code uses MXNet functions </span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.523.1">to add the corresponding 2D </span><em class="italic"><span class="koboSpan" id="kobo.524.1">convolutional</span></em><span class="koboSpan" id="kobo.525.1">, </span><em class="italic"><span class="koboSpan" id="kobo.526.1">max-pooling</span></em><span class="koboSpan" id="kobo.527.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.528.1">fully connected</span></em><span class="koboSpan" id="kobo.529.1"> layers with their corresponding activation functions, generating an </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">AlexNet architecture.</span></span></p>
<h3><span class="koboSpan" id="kobo.531.1">Training an AlexNet custom model</span></h3>
<p><span class="koboSpan" id="kobo.532.1">The</span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.533.1"> task we are dealing with is an</span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.534.1"> image classification task, which is a classification problem where input data is images, and therefore, we can use the training loop we saw in </span><a href="B16591_04.xhtml#_idTextAnchor075"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.535.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.536.1">, </span><em class="italic"><span class="koboSpan" id="kobo.537.1">Solving Classification Problems</span></em><span class="koboSpan" id="kobo.538.1"> – slightly modified for </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">this task.</span></span></p>
<p><span class="koboSpan" id="kobo.540.1">The parameters chosen are </span><span class="No-Break"><span class="koboSpan" id="kobo.541.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.542.1">Number of </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.543.1">epochs</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.544.1">: 20</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.545.1">Batch size</span></strong><span class="koboSpan" id="kobo.546.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.547.1">16 samples</span></span></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.548.1">Optimizer</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">: Adam</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.550.1">Learning </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.551.1">rate</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">: 0.0001</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.553.1">With </span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.554.1">these parameters, we </span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.555.1">obtain the following results (the best model was achieved on </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">epoch 11):</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.557.1">Training </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.558.1">loss</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">: 0.36</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.560.1">Training </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.561.1">accuracy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.562.1">: 0.83</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.563.1">Validation </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.564.1">loss</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.565.1">: 0.55</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.566.1">Validation </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.567.1">accuracy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">: 0.785</span></span></li>
</ul>
<h3><span class="koboSpan" id="kobo.569.1">Evaluating an AlexNet custom model</span></h3>
<p><span class="koboSpan" id="kobo.570.1">The </span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.571.1">accuracy obtained </span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.572.1">with the best model (in this case, the one corresponding to the last training iteration) on the test set is </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.574.1">
('accuracy', 0.7275)</span></pre> <p><span class="koboSpan" id="kobo.575.1">That’s quite a decent number for just five epochs </span><span class="No-Break"><span class="koboSpan" id="kobo.576.1">of training.</span></span></p>
<p><span class="koboSpan" id="kobo.577.1">Moreover, the confusion matrix computed is shown in the following figure (class </span><strong class="bold"><span class="koboSpan" id="kobo.578.1">0</span></strong><span class="koboSpan" id="kobo.579.1"> corresponds to cats and </span><strong class="bold"><span class="koboSpan" id="kobo.580.1">1</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.581.1">to dogs):</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer092">
<span class="koboSpan" id="kobo.582.1"><img alt="Figure 5.10 – A trained custom AlexNet confusion matrix" src="image/B16591_05_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.583.1">Figure 5.10 – A trained custom AlexNet confusion matrix</span></p>
<p><span class="koboSpan" id="kobo.584.1">As </span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.585.1">shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.586.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.587.1">.10</span></em><span class="koboSpan" id="kobo.588.1">, the </span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.589.1">model mostly predicts accurately the expected classes, with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">per-class errors:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.591.1">Cats detected as dogs</span></strong><span class="koboSpan" id="kobo.592.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.593.1">85</span></strong><span class="koboSpan" id="kobo.594.1">/200 (43% of cat images </span><span class="No-Break"><span class="koboSpan" id="kobo.595.1">were misclassified)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.596.1">Dogs detected as cats</span></strong><span class="koboSpan" id="kobo.597.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.598.1">24</span></strong><span class="koboSpan" id="kobo.599.1">/200 (12% of dog images </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">were misclassified)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.601.1">Let’s move on to the </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">next heading.</span></span></p>
<h3><span class="koboSpan" id="kobo.603.1">Introducing Model Zoo</span></h3>
<p><span class="koboSpan" id="kobo.604.1">One of the</span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.605.1"> best features that MXNet and GluonCV </span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.606.1">provide is their large pool of pre-trained models, readily available for its users to use and deploy in their own applications. </span><span class="koboSpan" id="kobo.606.2">This model library is called </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.607.1">Model Zoo</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.609.1">Moreover, depending on the task at hand, MXNet has some very interesting charts that compare the different pre-trained models optimized for tasks. </span><span class="koboSpan" id="kobo.609.2">For image classification (based on ImageNet), we have </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">the following:</span></span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer093">
<span class="koboSpan" id="kobo.611.1"><img alt="Figure 5.11 – Model Zoo for image classification (ImageNet)" src="image/B16591_05_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.612.1">Figure 5.11 – Model Zoo for image classification (ImageNet)</span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.613.1">Note</span></p>
<p class="callout"><span class="No-Break"><span class="koboSpan" id="kobo.614.1">Source: </span></span><a href="https://cv.gluon.ai/model_zoo/classification.html"><span class="No-Break"><span class="koboSpan" id="kobo.615.1">https://cv.gluon.ai/model_zoo/classification.html</span></span></a></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.616.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.617.1">.11</span></em><span class="koboSpan" id="kobo.618.1"> displays </span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.619.1">the most important pre-trained</span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.620.1"> models in </span><strong class="bold"><span class="koboSpan" id="kobo.621.1">GluonCV Model Zoo</span></strong><span class="koboSpan" id="kobo.622.1">, according to accuracy (the vertical axis) and inference performance (the samples per second and horizontal axis). </span><span class="koboSpan" id="kobo.622.2">There are no models (yet) in the top-right quadrant, meaning that, currently, we need to balance </span><span class="No-Break"><span class="koboSpan" id="kobo.623.1">these characteristics.</span></span></p>
<p><span class="koboSpan" id="kobo.624.1">Using models from GluonCV Model Zoo can be done with just a couple of lines of code, and we will explore this path to solve our reduced </span><em class="italic"><span class="koboSpan" id="kobo.625.1">Dogs vs. </span><span class="koboSpan" id="kobo.625.2">Cats</span></em><span class="koboSpan" id="kobo.626.1"> dataset in the </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">following steps.</span></span></p>
<h3><span class="koboSpan" id="kobo.628.1">ImageNet pre-trained models</span></h3>
<p><span class="koboSpan" id="kobo.629.1">Models </span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.630.1">from the </span><strong class="bold"><span class="koboSpan" id="kobo.631.1">GluonCV Model Zoo</span></strong><span class="koboSpan" id="kobo.632.1"> for </span><strong class="bold"><span class="koboSpan" id="kobo.633.1">image classification</span></strong><span class="koboSpan" id="kobo.634.1"> tasks have been pre-trained in the </span><em class="italic"><span class="koboSpan" id="kobo.635.1">ImageNet</span></em><span class="koboSpan" id="kobo.636.1"> dataset. </span><span class="koboSpan" id="kobo.636.2">This </span><a id="_idIndexMarker438"/><span class="koboSpan" id="kobo.637.1">dataset is one of the most well-known datasets in computer vision. </span><span class="koboSpan" id="kobo.637.2">It was the first large-scale image dataset and was part of the deep learning revolution when, in 2012, AlexNet won </span><span class="No-Break"><span class="koboSpan" id="kobo.638.1">the ILSVRC.</span></span></p>
<p><span class="koboSpan" id="kobo.639.1">This dataset has </span><span class="No-Break"><span class="koboSpan" id="kobo.640.1">two variants:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.641.1">Full dataset</span></strong><span class="koboSpan" id="kobo.642.1">: More than 20,000 classes in about 14 </span><span class="No-Break"><span class="koboSpan" id="kobo.643.1">million images</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.644.1">ImageNet-1k</span></strong><span class="koboSpan" id="kobo.645.1">: 1,000 classes in about 1 </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">million images</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.647.1">Due to the size and large number of classes, the full dataset is rarely used on benchmarks, with </span><em class="italic"><span class="koboSpan" id="kobo.648.1">ImageNet1k</span></em><span class="koboSpan" id="kobo.649.1"> the de facto ImageNet dataset (unless otherwise noted in the research papers, articles, and so on). </span><span class="koboSpan" id="kobo.649.2">Images in the dataset are in color and have a size of 224px by 224px (width </span><span class="No-Break"><span class="koboSpan" id="kobo.650.1">and height).</span></span></p>
<p><span class="koboSpan" id="kobo.651.1">All image classification pre-trained models in GluonCV Model Zoo have been pre-trained with ImageNet-1k, and therefore, they have 1,000 outputs. </span><span class="koboSpan" id="kobo.651.2">The outputs will be post-processed so that all ImageNet classes corresponding to cats point to class 0, all ImageNet classes corresponding to dogs point to class 1, and all other outputs point to class 2, which we will consider </span><span class="No-Break"><span class="koboSpan" id="kobo.652.1">as unknown.</span></span></p>
<h3><span class="koboSpan" id="kobo.653.1">Loading an AlexNet pre-trained model from Model Zoo</span></h3>
<p><span class="koboSpan" id="kobo.654.1">To</span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.655.1"> compare the advantages and disadvantages of using a custom-trained model and a </span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.656.1">pre-trained model from Model Zoo, in the following sections, we will work with a version of the AlexNet architecture that has been pre-trained on the </span><em class="italic"><span class="koboSpan" id="kobo.657.1">ImageNet</span></em><span class="koboSpan" id="kobo.658.1"> dataset, acquired from GluonCV </span><span class="No-Break"><span class="koboSpan" id="kobo.659.1">Model Zoo.</span></span></p>
<p><span class="koboSpan" id="kobo.660.1">Loading a pre-trained model is very easy and can be done with a single line </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.662.1">
alexnet = gcv.model_zoo.get_model("alexnet", pretrained=True, ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.663.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.664.1">get_model</span></strong><span class="koboSpan" id="kobo.665.1"> GluonCV function receives </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">three parameters:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.667.1">The name of the model</span></strong><span class="koboSpan" id="kobo.668.1">: In this </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">case, </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.670.1">alexnet</span></strong></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.671.1">pretrained</span></strong><span class="koboSpan" id="kobo.672.1">: A Boolean indicating whether we want to load the pre-trained weights and biases (if set to </span><strong class="source-inline"><span class="koboSpan" id="kobo.673.1">False</span></strong><span class="koboSpan" id="kobo.674.1">, only the uninitialized architecture will </span><span class="No-Break"><span class="koboSpan" id="kobo.675.1">be loaded)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.676.1">ctx</span></strong><span class="koboSpan" id="kobo.677.1">: The context – typically, </span><strong class="source-inline"><span class="koboSpan" id="kobo.678.1">mx.cpu()</span></strong><span class="koboSpan" id="kobo.679.1"> or </span><strong class="source-inline"><span class="koboSpan" id="kobo.680.1">mx.gpu()</span></strong><span class="koboSpan" id="kobo.681.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">if available</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.683.1">This call </span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.684.1">will </span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.685.1">download the chosen model and, if required, its pre-trained weights </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">and biases.</span></span></p>
<h3><span class="koboSpan" id="kobo.687.1">Evaluating an AlexNet pre-trained model from Model Zoo</span></h3>
<p><span class="koboSpan" id="kobo.688.1">With the</span><a id="_idIndexMarker443"/><span class="koboSpan" id="kobo.689.1"> loaded </span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.690.1">model from the previous section, we can now evaluate and compare our previous results, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.691.1">for </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.692.1">accuracy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.693.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.694.1">
('accuracy', 0.725)</span></pre> <p><span class="koboSpan" id="kobo.695.1">As we can see, this number is slightly lower than the accuracy we achieved with our custom-trained </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">AlexNet model.</span></span></p>
<p><span class="koboSpan" id="kobo.697.1">After computing the confusion matrix, we obtain the </span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">following values:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer094">
<span class="koboSpan" id="kobo.699.1"><img alt="Figure 5.12 – A pre-trained AlexNet Confusion Matrix" src="image/B16591_05_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.700.1">Figure 5.12 – A pre-trained AlexNet Confusion Matrix</span></p>
<p><span class="koboSpan" id="kobo.701.1">When we</span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.702.1"> analyze </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.703.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.704.1">.12</span></em><span class="koboSpan" id="kobo.705.1">, the most significant difference is that our previous confusion matrix was a 2x2 matrix (with the options of </span><strong class="bold"><span class="koboSpan" id="kobo.706.1">0</span></strong><span class="koboSpan" id="kobo.707.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.708.1">1</span></strong><span class="koboSpan" id="kobo.709.1"> for true labels and </span><strong class="bold"><span class="koboSpan" id="kobo.710.1">0</span></strong><span class="koboSpan" id="kobo.711.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.712.1">1</span></strong><span class="koboSpan" id="kobo.713.1"> for predicted labels). </span><span class="koboSpan" id="kobo.713.2">However, with our pre-trained model, we have obtained a 3x3 confusion matrix. </span><span class="koboSpan" id="kobo.713.3">This is because, as mentioned previously, pre-trained models have been trained in ImageNet, and they output 1,000 classes (instead of the two required for our dataset). </span><span class="koboSpan" id="kobo.713.4">These outputs have been post-processed, so all </span><em class="italic"><span class="koboSpan" id="kobo.714.1">ImageNet</span></em><span class="koboSpan" id="kobo.715.1"> classes corresponding to cats point to class </span><strong class="bold"><span class="koboSpan" id="kobo.716.1">0</span></strong><span class="koboSpan" id="kobo.717.1">, all ImageNet classes corresponding to dogs point to class </span><strong class="bold"><span class="koboSpan" id="kobo.718.1">1</span></strong><span class="koboSpan" id="kobo.719.1">, and all other outputs point to class </span><strong class="bold"><span class="koboSpan" id="kobo.720.1">2</span></strong><span class="koboSpan" id="kobo.721.1">, which we will consider as unknown. </span><span class="koboSpan" id="kobo.721.2">When taking into account this unknown class </span><strong class="bold"><span class="koboSpan" id="kobo.722.1">2</span></strong><span class="koboSpan" id="kobo.723.1">, the 3x3 matrix is computed. </span><span class="koboSpan" id="kobo.723.2">Please note how there are no images that produce a true label of </span><strong class="bold"><span class="koboSpan" id="kobo.724.1">2</span></strong><span class="koboSpan" id="kobo.725.1">; the last row is </span><span class="No-Break"><span class="koboSpan" id="kobo.726.1">all zeros.</span></span></p>
<p><span class="koboSpan" id="kobo.727.1">The </span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.728.1">model mostly behaves accurately with the expected classes, with the following per-class errors (we need to add numbers from the two </span><span class="No-Break"><span class="koboSpan" id="kobo.729.1">wrong columns):</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.730.1">Cats not detected as cats</span></strong><span class="koboSpan" id="kobo.731.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.732.1">96</span></strong><span class="koboSpan" id="kobo.733.1">/200 (48% of cats images </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">were misclassified)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.735.1">Dogs not detected as dogs</span></strong><span class="koboSpan" id="kobo.736.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.737.1">14</span></strong><span class="koboSpan" id="kobo.738.1">/200 (7% of dogs images </span><span class="No-Break"><span class="koboSpan" id="kobo.739.1">were misclassified)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.740.1">There is a significant difference in the per-class results, and this is due to the pre-trained dataset used, </span><em class="italic"><span class="koboSpan" id="kobo.741.1">ImageNet</span></em><span class="koboSpan" id="kobo.742.1">, because it has a large number of classes associated with dog breeds and, therefore, has been trained more extensively on </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">dog images.</span></span></p>
<h3><span class="koboSpan" id="kobo.744.1">Loading a ResNet pre-trained model from Model Zoo</span></h3>
<p><span class="koboSpan" id="kobo.745.1">Looking </span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.746.1">at AlexNet and later models with a higher depth, such as VGGNet, it became clear </span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.747.1">that deeper layers could help when classifying images. </span><span class="koboSpan" id="kobo.747.2">However, when training these deep networks, by using </span><em class="italic"><span class="koboSpan" id="kobo.748.1">backpropagation</span></em><span class="koboSpan" id="kobo.749.1"> and the </span><em class="italic"><span class="koboSpan" id="kobo.750.1">chain rule</span></em><span class="koboSpan" id="kobo.751.1">, the training algorithm starts computing smaller and smaller values for the gradients because of the large number of multiplications of small numbers (the activation function outputs are in the [0, 1] range), and therefore, when the gradients for the early layers are computed, the updated weights are seldom modified. </span><span class="koboSpan" id="kobo.751.2">This is known as</span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.752.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.753.1">vanishing gradient</span></strong><span class="koboSpan" id="kobo.754.1"> problem, and different </span><strong class="bold"><span class="koboSpan" id="kobo.755.1">ResNet</span></strong><span class="koboSpan" id="kobo.756.1"> architectures were developed to avoid it. </span><span class="koboSpan" id="kobo.756.2">Specifically, ResNet models use residual blocks that add direct lines to connect layers, providing shortcuts that can be leveraged during training to avoid </span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">vanishing gradients.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer095">
<span class="koboSpan" id="kobo.758.1"><img alt="Figure 5.13 – A ResNet residual block" src="image/B16591_05_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.759.1">Figure 5.13 – A ResNet residual block</span></p>
<p><span class="koboSpan" id="kobo.760.1">The architecture shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.761.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.762.1">.13</span></em><span class="koboSpan" id="kobo.763.1"> allows you to stack layers in a more scalable way, with known architectures with 18, 50, 101, and 152 layers. </span><span class="koboSpan" id="kobo.763.2">This approach proved very successful, and ResNet152 won the ILSVRC </span><span class="No-Break"><span class="koboSpan" id="kobo.764.1">in 2015.</span></span></p>
<p><span class="koboSpan" id="kobo.765.1">In this </span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.766.1">case, we </span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.767.1">will load the </span><strong class="source-inline"><span class="koboSpan" id="kobo.768.1">v1d</span></strong><span class="koboSpan" id="kobo.769.1"> version of </span><strong class="source-inline"><span class="koboSpan" id="kobo.770.1">resNet50</span></strong><span class="koboSpan" id="kobo.771.1">, with a single line </span><span class="No-Break"><span class="koboSpan" id="kobo.772.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.773.1">
resnet50 = gcv.model_zoo.get_model("resnet50_v1d", pretrained=True, ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.774.1">The model then </span><span class="No-Break"><span class="koboSpan" id="kobo.775.1">downloads successfully.</span></span></p>
<h3><span class="koboSpan" id="kobo.776.1">Evaluating a ResNet pre-trained model from Model Zoo</span></h3>
<p><span class="koboSpan" id="kobo.777.1">With the</span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.778.1"> loaded model from the previous section, we can now evaluate and compare with our previous results, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.779.1">for </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.780.1">accuracy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.781.1">:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.782.1">
('accuracy', 0.925)</span></pre> <p><span class="koboSpan" id="kobo.783.1">As we can see, this number is significantly higher than </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">previous models.</span></span></p>
<p><span class="koboSpan" id="kobo.785.1">After computing the confusion matrix, we obtain the </span><span class="No-Break"><span class="koboSpan" id="kobo.786.1">following values:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer096">
<span class="koboSpan" id="kobo.787.1"><img alt="Figure 5.14 – A pre-trained ResNet ﻿confusion ﻿matrix" src="image/B16591_05_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.788.1">Figure 5.14 – A pre-trained ResNet confusion matrix</span></p>
<p><span class="koboSpan" id="kobo.789.1">According to </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.790.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.791.1">.14</span></em><span class="koboSpan" id="kobo.792.1">, the per-class error rate is </span><span class="No-Break"><span class="koboSpan" id="kobo.793.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.794.1">Cats not detected as cats</span></strong><span class="koboSpan" id="kobo.795.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.796.1">29</span></strong><span class="koboSpan" id="kobo.797.1">/200 (14.5% of cat images </span><span class="No-Break"><span class="koboSpan" id="kobo.798.1">were misclassified)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.799.1">Dogs not detected as dogs</span></strong><span class="koboSpan" id="kobo.800.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.801.1">1</span></strong><span class="koboSpan" id="kobo.802.1">/200 (0.5% of dog images </span><span class="No-Break"><span class="koboSpan" id="kobo.803.1">were misclassified)</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.804.1">There is a significant difference in the per-class results, and this is again due to the pre-trained dataset used, </span><em class="italic"><span class="koboSpan" id="kobo.805.1">ImageNet</span></em><span class="koboSpan" id="kobo.806.1">, because it has a large number of classes associated with dog breeds and, therefore, has been trained more extensively on </span><span class="No-Break"><span class="koboSpan" id="kobo.807.1">dog images.</span></span></p>
<h2 id="_idParaDest-108"><a id="_idTextAnchor109"/><span class="koboSpan" id="kobo.808.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.809.1">In this </span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.810.1">recipe, we compared two approaches to using computer vision models for </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">image classification:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.812.1">Training a custom model </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">from scratch</span></span></li>
<li><span class="koboSpan" id="kobo.814.1">Using pre-trained models from GluonCV </span><span class="No-Break"><span class="koboSpan" id="kobo.815.1">Model Zoo</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.816.1">We applied both approaches with</span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.817.1"> AlexNet architecture and compared the results with the </span><strong class="bold"><span class="koboSpan" id="kobo.818.1">ResNet-101 Model </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.819.1">Zoo</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.820.1"> version.</span></span></p>
<p><span class="koboSpan" id="kobo.821.1">Both approaches have advantages and disadvantages. </span><span class="koboSpan" id="kobo.821.2">Training from scratch provides us direct control on the number of output classes, and we can fully handle the training process and the evolution of loss and accuracy for both the training and </span><span class="No-Break"><span class="koboSpan" id="kobo.822.1">validation datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.823.1">However, in order to train a model, we need sufficient data, and that might not be always available. </span><span class="koboSpan" id="kobo.823.2">Furthermore, adjusting the training hyperparameters (epochs, batch size, optimizer, and learning rate) and the training itself are time-consuming processes that, if not done properly, can yield suboptimal accuracy values (or </span><span class="No-Break"><span class="koboSpan" id="kobo.824.1">other metrics).</span></span></p>
<p><span class="koboSpan" id="kobo.825.1">In our examples, we used a reduced version of the </span><em class="italic"><span class="koboSpan" id="kobo.826.1">Dogs vs. </span><span class="koboSpan" id="kobo.826.2">Cats</span></em><span class="koboSpan" id="kobo.827.1"> dataset from Kaggle, and we used pre-trained models on </span><em class="italic"><span class="koboSpan" id="kobo.828.1">ImageNet</span></em><span class="koboSpan" id="kobo.829.1">. </span><span class="koboSpan" id="kobo.829.2">The Kaggle dataset contains 25,000 images (more than 10 times more) and the reader is encouraged to try the proposed solution on that dataset (all helper functions have also been tested with the </span><span class="No-Break"><span class="koboSpan" id="kobo.830.1">full dataset).</span></span></p>
<p><span class="koboSpan" id="kobo.831.1">Furthermore, the choice of the </span><em class="italic"><span class="koboSpan" id="kobo.832.1">ImageNet</span></em><span class="koboSpan" id="kobo.833.1"> dataset was not casual; </span><em class="italic"><span class="koboSpan" id="kobo.834.1">ImageNet</span></em><span class="koboSpan" id="kobo.835.1"> has dog classes and cat classes, and therefore, the expectation was that these pre-trained models would perform well, as they had already seen images from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.836.1">dataset</span></strong><span class="koboSpan" id="kobo.837.1"> class. </span><span class="koboSpan" id="kobo.837.2">However, when this is not possible, and we apply pre-trained models from a dataset to another dataset, the data probability distribution will typically be very different; hence, the accuracy obtained can be very low. </span><span class="koboSpan" id="kobo.837.3">This is </span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.838.1">known as the </span><strong class="bold"><span class="koboSpan" id="kobo.839.1">domain gap</span></strong><span class="koboSpan" id="kobo.840.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.841.1">domain adaptation problem</span></strong><span class="koboSpan" id="kobo.842.1"> between</span><a id="_idIndexMarker456"/><span class="koboSpan" id="kobo.843.1"> the source dataset (the data the model has been pre-trained on) and the target dataset (the data the model is </span><span class="No-Break"><span class="koboSpan" id="kobo.844.1">evaluated on).</span></span></p>
<p><span class="koboSpan" id="kobo.845.1">One way to tackle these issues for supervised learning problems is fine-tuning. </span><span class="koboSpan" id="kobo.845.2">This approach is explored in detail in </span><a href="B16591_07.xhtml#_idTextAnchor148"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.846.1">Chapter 7</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.847.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.848.1">We finalized the recipe evaluating our two pre-trained models, </span><em class="italic"><span class="koboSpan" id="kobo.849.1">AlexNet</span></em><span class="koboSpan" id="kobo.850.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.851.1">ResNet</span></em><span class="koboSpan" id="kobo.852.1">, and saw how CNN models have evolved through the years, increasing the </span><span class="No-Break"><span class="koboSpan" id="kobo.853.1">accuracy obtained.</span></span></p>
<h2 id="_idParaDest-109"><a id="_idTextAnchor110"/><span class="koboSpan" id="kobo.854.1">There’s more...</span></h2>
<p><span class="koboSpan" id="kobo.855.1">In this recipe, we used </span><em class="italic"><span class="koboSpan" id="kobo.856.1">ImageNet</span></em><span class="koboSpan" id="kobo.857.1"> pre-trained models; for more information about </span><em class="italic"><span class="koboSpan" id="kobo.858.1">ImageNet</span></em><span class="koboSpan" id="kobo.859.1">, and the ILSVRC, I suggest this </span><span class="No-Break"><span class="koboSpan" id="kobo.860.1">article: </span></span><a href="https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/"><span class="No-Break"><span class="koboSpan" id="kobo.861.1">https://machinelearningmastery.com/introduction-to-the-imagenet-large-scale-visual-recognition-challenge-ilsvrc/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.862.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.863.1">Although primarily about the ILSVRC, this previous link also includes some history regarding CNNs, including </span><em class="italic"><span class="koboSpan" id="kobo.864.1">AlexNet</span></em><span class="koboSpan" id="kobo.865.1">, VGGNet, </span><span class="No-Break"><span class="koboSpan" id="kobo.866.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.867.1">ResNet</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.868.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.869.1">However, computer vision datasets have been under strict scrutiny recently regarding data quality, and ImageNet is no exception, as this article </span><span class="No-Break"><span class="koboSpan" id="kobo.870.1">describes: </span></span><a href="https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/"><span class="No-Break"><span class="koboSpan" id="kobo.871.1">https://venturebeat.com/2021/03/28/mit-study-finds-systematic-labeling-errors-in-popular-ai-benchmark-datasets/</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.872.1">.</span></span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.873.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.874.1">.11</span></em><span class="koboSpan" id="kobo.875.1"> shows a static image corresponding to the accuracy versus samples per second graph for the Model Zoo for image classification (on </span><em class="italic"><span class="koboSpan" id="kobo.876.1">ImageNet</span></em><span class="koboSpan" id="kobo.877.1">). </span><span class="koboSpan" id="kobo.877.2">A snapshot of a dynamic version is available at this link and is worth taking a look </span><span class="No-Break"><span class="koboSpan" id="kobo.878.1">at: </span></span><a href="https://cv.gluon.ai/model_zoo/classification.html"><span class="No-Break"><span class="koboSpan" id="kobo.879.1">https://cv.gluon.ai/model_zoo/classification.html</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.880.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.881.1">At this link, results from different models available in GluonCV Model Zoo are included, and it is suggested that you reproduce these results, as it is an </span><span class="No-Break"><span class="koboSpan" id="kobo.882.1">interesting exercise.</span></span></p>
<p><span class="koboSpan" id="kobo.883.1">Apart from ImageNet, GluonCV Model Zoo provides models </span><a id="_idIndexMarker457"/><span class="koboSpan" id="kobo.884.1">pre-trained on </span><strong class="bold"><span class="koboSpan" id="kobo.885.1">CIFAR10</span></strong><span class="koboSpan" id="kobo.886.1">. </span><span class="koboSpan" id="kobo.886.2">A list of these models can be found </span><span class="No-Break"><span class="koboSpan" id="kobo.887.1">at </span></span><a href="https://cv.gluon.ai/model_zoo/classification.html#cifar10"><span class="No-Break"><span class="koboSpan" id="kobo.888.1">https://cv.gluon.ai/model_zoo/classification.html#cifar10.</span></span></a></p>
<p><span class="koboSpan" id="kobo.889.1">For a deeper explanation of the vanishing gradient problem, Wikipedia provides a good </span><span class="No-Break"><span class="koboSpan" id="kobo.890.1">start: </span></span><a href="https://en.wikipedia.org/wiki/Vanishing_gradient_problem"><span class="No-Break"><span class="koboSpan" id="kobo.891.1">https://en.wikipedia.org/wiki/Vanishing_gradient_problem</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.892.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.893.1">Lastly, regarding ResNet and its current research relevance, in a recently published paper, it was shown that ResNet</span><a id="_idIndexMarker458"/><span class="koboSpan" id="kobo.894.1"> can still achieve </span><strong class="bold"><span class="koboSpan" id="kobo.895.1">State-of-the-Art</span></strong><span class="koboSpan" id="kobo.896.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.897.1">SOTA</span></strong><span class="koboSpan" id="kobo.898.1">) results when the latest researched training techniques are applied to it, highlighting the importance of datasets and a training algorithm (versus optimization only for model </span><span class="No-Break"><span class="koboSpan" id="kobo.899.1">architecture): </span></span><a href="https://gdude.de/blog/2021-03-15/Revisiting-Resnets"><span class="No-Break"><span class="koboSpan" id="kobo.900.1">https://gdude.de/blog/2021-03-15/Revisiting-Resnets</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.901.1">.</span></span></p>
<h1 id="_idParaDest-110"><a id="_idTextAnchor111"/><span class="koboSpan" id="kobo.902.1">Detecting objects with MXNet – Faster R-CNN and YOLO</span></h1>
<p><span class="koboSpan" id="kobo.903.1">In this recipe, we </span><a id="_idIndexMarker459"/><span class="koboSpan" id="kobo.904.1">will see how to use MXNet and GluonCV on a pre-trained model to detect objects from a dataset. </span><span class="koboSpan" id="kobo.904.2">We will see how to use GluonCV Model Zoo with two very important models for </span><strong class="bold"><span class="koboSpan" id="kobo.905.1">object detection</span></strong><span class="koboSpan" id="kobo.906.1"> – </span><strong class="bold"><span class="koboSpan" id="kobo.907.1">Faster R-CNN</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.908.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.909.1">YOLOv3</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.910.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.911.1">In this recipe, we will compare the performance of these two pre-trained models to detect objects on the </span><em class="italic"><span class="koboSpan" id="kobo.912.1">Penn-Fudan </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.913.1">Pedestrians</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.914.1"> dataset.</span></span></p>
<h2 id="_idParaDest-111"><a id="_idTextAnchor112"/><span class="koboSpan" id="kobo.915.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.916.1">As for previous chapters, in this recipe, we will be using a few matrix operations and linear algebra, but it will not be </span><span class="No-Break"><span class="koboSpan" id="kobo.917.1">too difficult.</span></span></p>
<p><span class="koboSpan" id="kobo.918.1">As we will unpack in this recipe, object detection combines classification and regression, and therefore, chapters and recipes where we explored the foundations of these topics are recommended to revisit. </span><span class="koboSpan" id="kobo.918.2">Furthermore, we will be detecting objects on image datasets. </span><span class="koboSpan" id="kobo.918.3">This recipe will combine what we learned in the </span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">following chapters:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.920.1">Understanding image datasets: load, manage, and visualize the Fashion MNIST dataset</span></em><span class="koboSpan" id="kobo.921.1">, the third recipe from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.922.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.923.1">, </span><em class="italic"><span class="koboSpan" id="kobo.924.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.925.1">and DataLoader</span></em></span></li>
<li><a href="B16591_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.926.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.927.1">, </span><em class="italic"><span class="koboSpan" id="kobo.928.1">Solving </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.929.1">Regression Problems</span></em></span></li>
<li><a href="B16591_04.xhtml#_idTextAnchor075"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.930.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.931.1">, </span><em class="italic"><span class="koboSpan" id="kobo.932.1">Solving </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.933.1">Classification Problems</span></em></span></li>
</ul>
<h2 id="_idParaDest-112"><a id="_idTextAnchor113"/><span class="koboSpan" id="kobo.934.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.935.1">In this recipe, we </span><a id="_idIndexMarker460"/><span class="koboSpan" id="kobo.936.1">will take the </span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.938.1">Introduce </span><span class="No-Break"><span class="koboSpan" id="kobo.939.1">object detection.</span></span></li>
<li><span class="koboSpan" id="kobo.940.1">Evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.941.1">object detectors.</span></span></li>
<li><span class="koboSpan" id="kobo.942.1">Compare </span><em class="italic"><span class="koboSpan" id="kobo.943.1">Single-stage</span></em><span class="koboSpan" id="kobo.944.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.945.1">Two-stage</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.946.1">object detectors.</span></span></li>
<li><span class="koboSpan" id="kobo.947.1">Explore the </span><em class="italic"><span class="koboSpan" id="kobo.948.1">Penn-Fudan </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.949.1">Pedestrians</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.950.1"> dataset.</span></span></li>
<li><span class="koboSpan" id="kobo.951.1">Introduce the Object Detection </span><span class="No-Break"><span class="koboSpan" id="kobo.952.1">Model Zoo.</span></span></li>
<li><span class="koboSpan" id="kobo.953.1">Worke with </span><em class="italic"><span class="koboSpan" id="kobo.954.1">MS COCO</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.955.1">pre-trained models.</span></span></li>
<li><span class="koboSpan" id="kobo.956.1">Load a Faster R-CNN pre-trained model from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.957.1">Model Zoo</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.958.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.959.1">Evaluate a </span><em class="italic"><span class="koboSpan" id="kobo.960.1">Faster R-CNN</span></em><span class="koboSpan" id="kobo.961.1"> pre-trained model from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.962.1">Model Zoo</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.963.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.964.1">Load a YOLOv3 pre-trained model from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.965.1">Model Zoo</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.967.1">Evaluate a </span><em class="italic"><span class="koboSpan" id="kobo.968.1">YOLOv3</span></em><span class="koboSpan" id="kobo.969.1"> pre-trained model from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.970.1">Model Zoo</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.972.1">Conclude what we </span><span class="No-Break"><span class="koboSpan" id="kobo.973.1">have learned.</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.974.1">Introducing object detection</span></h3>
<p><span class="koboSpan" id="kobo.975.1">In some of the </span><a id="_idIndexMarker461"/><span class="koboSpan" id="kobo.976.1">previous chapters and recipes, we analyzed image classification problems, where the task of our models was to take an image and define the class most likely associated with it. </span><span class="koboSpan" id="kobo.976.2">In </span><strong class="bold"><span class="koboSpan" id="kobo.977.1">object detection</span></strong><span class="koboSpan" id="kobo.978.1">, however, there can be multiple objects per image, corresponding to different classes, and in different locations of the image, and therefore, the output is now two lists, one providing the most likely class of each detected object and another indicating the estimated location of the object. </span><span class="koboSpan" id="kobo.978.2">The class output can be modeled as a classification problem, and the bounding box output can be modeled as a regression problem. </span><span class="koboSpan" id="kobo.978.3">Typically, locations are represented with what is called a bounding box. </span><span class="koboSpan" id="kobo.978.4">An example of bounding boxes is shown </span><span class="No-Break"><span class="koboSpan" id="kobo.979.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer097">
<span class="koboSpan" id="kobo.980.1"><img alt="Figure 5.15 – Bounding box examples" src="image/B16591_05_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.981.1">Figure 5.15 – Bounding box examples</span></p>
<p><span class="koboSpan" id="kobo.982.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.983.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.984.1">.15</span></em><span class="koboSpan" id="kobo.985.1">, we can </span><a id="_idIndexMarker462"/><span class="koboSpan" id="kobo.986.1">see two examples of bounding boxes for two different classes – </span><strong class="source-inline"><span class="koboSpan" id="kobo.987.1">person</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.988.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.989.1">dog</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.990.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.991.1">Evaluating object detectors</span></h3>
<p><span class="koboSpan" id="kobo.992.1">In image classification, we</span><a id="_idIndexMarker463"/><span class="koboSpan" id="kobo.993.1"> defined a correct classification if the class identified in one image was the right one. </span><span class="koboSpan" id="kobo.993.2">However, in object detection, there are two parameters – the class and the bounding box. </span><span class="koboSpan" id="kobo.993.3">Intuitively, we can define a correct classification if, for each object that should be detected, there is a </span><em class="italic"><span class="koboSpan" id="kobo.994.1">similar enough</span></em><span class="koboSpan" id="kobo.995.1"> bounding box that has been classified properly. </span><span class="koboSpan" id="kobo.995.2">To define what </span><em class="italic"><span class="koboSpan" id="kobo.996.1">similar enough</span></em><span class="koboSpan" id="kobo.997.1"> means, we compute </span><strong class="bold"><span class="koboSpan" id="kobo.998.1">Intersection over Union</span></strong><span class="koboSpan" id="kobo.999.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1000.1">IoU</span></strong><span class="koboSpan" id="kobo.1001.1">), the</span><a id="_idIndexMarker464"/><span class="koboSpan" id="kobo.1002.1"> ratio of the intersection of the area of the bounding boxes over the area of the union of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1003.1">bounding boxes.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer098">
<span class="koboSpan" id="kobo.1004.1"><img alt="Figure 5.16 – IoU" src="image/B16591_05_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1005.1">Figure 5.16 – IoU</span></p>
<p><span class="koboSpan" id="kobo.1006.1">A </span><a id="_idIndexMarker465"/><span class="koboSpan" id="kobo.1007.1">graphical interpretation of IoU can be seen in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1008.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1009.1">.16</span></em><span class="koboSpan" id="kobo.1010.1">. </span><span class="koboSpan" id="kobo.1010.2">When the IoU is above a determined threshold, the bounding boxes are said to match. </span><span class="koboSpan" id="kobo.1010.3">By using IoU and its threshold (typically 0.5), a detection can be classified as correct (given the object was also correctly classified), and metrics such as accuracy, precision, and recall can be computed (</span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">per class).</span></span></p>
<p><span class="koboSpan" id="kobo.1012.1">Furthermore, in </span><a href="B16591_04.xhtml#_idTextAnchor075"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1013.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.1014.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1015.1">Solving Classification Problems</span></em><span class="koboSpan" id="kobo.1016.1">, we discussed several options to evaluate </span><em class="italic"><span class="koboSpan" id="kobo.1017.1">classification</span></em><span class="koboSpan" id="kobo.1018.1"> problems. </span><span class="koboSpan" id="kobo.1018.2">We introduced </span><strong class="bold"><span class="koboSpan" id="kobo.1019.1">Area Under the Curve</span></strong><em class="italic"> </em><span class="koboSpan" id="kobo.1020.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.1021.1">AUC</span></strong><span class="koboSpan" id="kobo.1022.1">), and </span><a id="_idIndexMarker466"/><span class="koboSpan" id="kobo.1023.1">we saw how changing the threshold had an influence on </span><strong class="bold"><span class="koboSpan" id="kobo.1024.1">precision</span></strong><span class="koboSpan" id="kobo.1025.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1026.1">recall</span></strong><span class="koboSpan" id="kobo.1027.1">. </span><span class="koboSpan" id="kobo.1027.2">When plotting precision and recall together (the </span><strong class="bold"><span class="koboSpan" id="kobo.1028.1">PR curve</span></strong><span class="koboSpan" id="kobo.1029.1">), we </span><a id="_idIndexMarker467"/><span class="koboSpan" id="kobo.1030.1">can see the effect of the threshold, in the same way as we did for AUC. </span><span class="koboSpan" id="kobo.1030.2">If we calculate the area covered between the curve (the </span><em class="italic"><span class="koboSpan" id="kobo.1031.1">x</span></em><span class="koboSpan" id="kobo.1032.1"> axis, </span><em class="italic"><span class="koboSpan" id="kobo.1033.1">y = 0 axis</span></em><span class="koboSpan" id="kobo.1034.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.1035.1">y = 1 axis</span></em><span class="koboSpan" id="kobo.1036.1">), we obtain a parameter that is not dependent on the threshold value; it defines the performance of our model with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1037.1">given data:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer099">
<span class="koboSpan" id="kobo.1038.1"><img alt="Figure 5.17 – The PR curve" src="image/B16591_05_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1039.1">Figure 5.17 – The PR curve</span></p>
<p><span class="koboSpan" id="kobo.1040.1">One of </span><a id="_idIndexMarker468"/><span class="koboSpan" id="kobo.1041.1">the characteristics of this </span><a id="_idIndexMarker469"/><span class="koboSpan" id="kobo.1042.1">curve that we can see clearly in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1043.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1044.1">.17</span></em><span class="koboSpan" id="kobo.1045.1"> is its zig-zag pattern. </span><span class="koboSpan" id="kobo.1045.2">As we decrease the threshold, the curve goes down with false positives and goes up again with </span><span class="No-Break"><span class="koboSpan" id="kobo.1046.1">true positives.</span></span></p>
<p><span class="koboSpan" id="kobo.1047.1">However, in order to be able to compare different models easily, instead of comparing the PR curves for each class, a single </span><a id="_idIndexMarker470"/><span class="koboSpan" id="kobo.1048.1">number metric was developed, the </span><strong class="bold"><span class="koboSpan" id="kobo.1049.1">mean Average Precision</span></strong><span class="koboSpan" id="kobo.1050.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1051.1">mAP</span></strong><span class="koboSpan" id="kobo.1052.1">). </span><span class="koboSpan" id="kobo.1052.2">In short, it is the mean of all the areas under the PR curves for </span><span class="No-Break"><span class="koboSpan" id="kobo.1053.1">a model:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1054.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1055.1">A</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1056.1">P</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1057.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1058.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1059.1">1</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1060.1"> </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1061.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1062.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1063.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1064.1"> </span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1065.1">∑</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1066.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1067.1">i</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1068.1">=</span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.1069.1">1</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1070.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1071.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1072.1"> </span></span><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1073.1">A</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1074.1">P</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1075.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable_v-normal"><span class="koboSpan" id="kobo.1076.1">i</span></span></span></p>
<p><span class="koboSpan" id="kobo.1077.1">To compute </span><em class="italic"><span class="koboSpan" id="kobo.1078.1">mAP</span></em><span class="koboSpan" id="kobo.1079.1">, the first step is to calculate the average precision for each class, which is the </span><a id="_idIndexMarker471"/><span class="koboSpan" id="kobo.1080.1">area under the PR curve and then compute its arithmetic mean. </span><span class="koboSpan" id="kobo.1080.2">This value provides a single number where object detection models evaluated on the same dataset can </span><span class="No-Break"><span class="koboSpan" id="kobo.1081.1">be compared.</span></span></p>
<h3><span class="koboSpan" id="kobo.1082.1">Comparing single-stage and two-stage object detectors</span></h3>
<p><span class="koboSpan" id="kobo.1083.1">We can </span><a id="_idIndexMarker472"/><span class="koboSpan" id="kobo.1084.1">think of object detectors as cropping a specific part of an image and passing that cropped image through an image classifier, similar to what we did with full images in the previous recipe. </span><span class="koboSpan" id="kobo.1084.2">Using this approach, our object detector will have </span><span class="No-Break"><span class="koboSpan" id="kobo.1085.1">two steps:</span></span></p>
<ol>
<li><strong class="bold"><span class="koboSpan" id="kobo.1086.1">Region proposal network</span></strong><span class="koboSpan" id="kobo.1087.1">: This is </span><a id="_idIndexMarker473"/><span class="koboSpan" id="kobo.1088.1">the module that will indicate the regions where an object could </span><span class="No-Break"><span class="koboSpan" id="kobo.1089.1">be located.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1090.1">Object classifier</span></strong><span class="koboSpan" id="kobo.1091.1">: The </span><a id="_idIndexMarker474"/><span class="koboSpan" id="kobo.1092.1">regions will be classified by the model, with the regions previously cropped and resized to match the model </span><span class="No-Break"><span class="koboSpan" id="kobo.1093.1">input constraints.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.1094.1">This approach is known </span><a id="_idIndexMarker475"/><span class="koboSpan" id="kobo.1095.1">as </span><strong class="bold"><span class="koboSpan" id="kobo.1096.1">two-stage object detection</span></strong><span class="koboSpan" id="kobo.1097.1">, and its most important characteristic is its accuracy, although it is slow due to its complex architecture, and fully accurate (non-approximate) training cannot be done end </span><span class="No-Break"><span class="koboSpan" id="kobo.1098.1">to end.</span></span></p>
<p><span class="koboSpan" id="kobo.1099.1">Faster </span><strong class="bold"><span class="koboSpan" id="kobo.1100.1">Region-based Convolutional Neural Network</span></strong><span class="koboSpan" id="kobo.1101.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1102.1">R-CNN</span></strong><span class="koboSpan" id="kobo.1103.1">) is </span><a id="_idIndexMarker476"/><span class="koboSpan" id="kobo.1104.1">one of the models that follow this approach. </span><span class="koboSpan" id="kobo.1104.2">One of the most important differences from previous versions of the model (</span><strong class="bold"><span class="koboSpan" id="kobo.1105.1">R-CNN</span></strong><span class="koboSpan" id="kobo.1106.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1107.1">Fast R-CNN</span></strong><span class="koboSpan" id="kobo.1108.1">) is that in order to provide faster computations, it uses pre-computed bounding boxes called </span><em class="italic"><span class="koboSpan" id="kobo.1109.1">anchor boxes</span></em><span class="koboSpan" id="kobo.1110.1">, where the scale and the aspect ratio of the bounding box are pre-defined. </span><span class="koboSpan" id="kobo.1110.2">This approach allows the networks to be modeled to compute the </span><em class="italic"><span class="koboSpan" id="kobo.1111.1">offset</span></em><span class="koboSpan" id="kobo.1112.1"> related to an anchor box, instead of the full bounding box coordinates, which simplifies the </span><span class="No-Break"><span class="koboSpan" id="kobo.1113.1">regression problem.</span></span></p>
<p><span class="koboSpan" id="kobo.1114.1">Another algorithm to improve computation </span><a id="_idIndexMarker477"/><span class="koboSpan" id="kobo.1115.1">times is </span><strong class="bold"><span class="koboSpan" id="kobo.1116.1">Non-Maximum Suppression</span></strong><span class="koboSpan" id="kobo.1117.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1118.1">NMS</span></strong><span class="koboSpan" id="kobo.1119.1">). </span><span class="koboSpan" id="kobo.1119.2">Typically, thousands of regions will be proposed for the next step of the object detection pipeline. </span><span class="koboSpan" id="kobo.1119.3">Many of these regions overlap with each other, and NMS is the algorithm that takes into account the confidence of the prediction, removing all overlapping regions over an </span><span class="No-Break"><span class="koboSpan" id="kobo.1120.1">IoU threshold.</span></span></p>
<p><span class="koboSpan" id="kobo.1121.1">Another approach for object detectors is to design architectures that make predictions of bounding boxes and class probabilities together, allowing end-to-end training in one step. </span><span class="koboSpan" id="kobo.1121.2">Architectures following this approach are </span><a id="_idIndexMarker478"/><span class="koboSpan" id="kobo.1122.1">known as </span><strong class="bold"><span class="koboSpan" id="kobo.1123.1">single-stage object detectors</span></strong><span class="koboSpan" id="kobo.1124.1">. </span><span class="koboSpan" id="kobo.1124.2">These</span><a id="_idIndexMarker479"/><span class="koboSpan" id="kobo.1125.1"> architectures also make use of anchor boxes and NMS to improve the regression task. </span><span class="koboSpan" id="kobo.1125.2">The two most famous architectures using this approach are </span><span class="No-Break"><span class="koboSpan" id="kobo.1126.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1127.1">You Only Look Once (YOLO</span></strong><span class="koboSpan" id="kobo.1128.1">): The image</span><a id="_idIndexMarker480"/><span class="koboSpan" id="kobo.1129.1"> is processed just once using a custom CNN architecture (a combination of convolutional and max-pooling layers), ending with two fully connected layers. </span><span class="koboSpan" id="kobo.1129.2">These architectures have been developed continuously, with </span><em class="italic"><span class="koboSpan" id="kobo.1130.1">YOLOv3</span></em><span class="koboSpan" id="kobo.1131.1"> being one of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1132.1">most popular.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1133.1">Single Shot Detector (SSD)</span></strong><span class="koboSpan" id="kobo.1134.1">: The </span><a id="_idIndexMarker481"/><span class="koboSpan" id="kobo.1135.1">image is processed using a CNN backbone architecture (such as VGG-16) to compute feature maps, and the generated multi-scale feature maps are then classified. </span><span class="koboSpan" id="kobo.1135.2">SSD512 (using </span><em class="italic"><span class="koboSpan" id="kobo.1136.1">VGG-16</span></em><span class="koboSpan" id="kobo.1137.1"> as the backbone) is one of the models that follow </span><span class="No-Break"><span class="koboSpan" id="kobo.1138.1">this architecture.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1139.1">YOLOv3 model is </span><a id="_idIndexMarker482"/><span class="koboSpan" id="kobo.1140.1">the fastest and yields reasonable accuracy metrics, the SSD512 model is a good trade-off between speed and accuracy, and the Faster R-CNN model has the highest accuracy and is the slowest of </span><span class="No-Break"><span class="koboSpan" id="kobo.1141.1">the three.</span></span></p>
<h3><span class="koboSpan" id="kobo.1142.1">Exploring the Penn-Fudan Pedestrians dataset</span></h3>
<p><span class="koboSpan" id="kobo.1143.1">For our</span><a id="_idIndexMarker483"/><span class="koboSpan" id="kobo.1144.1"> object detection </span><a id="_idIndexMarker484"/><span class="koboSpan" id="kobo.1145.1">experiments, we will work with a new dataset – </span><em class="italic"><span class="koboSpan" id="kobo.1146.1">Penn-Fudan Pedestrians</span></em><span class="koboSpan" id="kobo.1147.1">. </span><span class="koboSpan" id="kobo.1147.2">This is a publicly available dataset (</span><a href="https://www.cis.upenn.edu/~jshi/ped_html/"><span class="koboSpan" id="kobo.1148.1">https://www.cis.upenn.edu/~jshi/ped_html/</span></a><span class="koboSpan" id="kobo.1149.1">) and is a collaboration between the universities of Pennsylvania and Fudan, and it must be </span><span class="No-Break"><span class="koboSpan" id="kobo.1150.1">downloaded manually.</span></span></p>
<p><span class="koboSpan" id="kobo.1151.1">The dataset has 423 pedestrians annotated from 170 images; 345 pedestrians were annotated for the release of the dataset (2007) and 78 pedestrians were added later, as the previous ones were either small </span><span class="No-Break"><span class="koboSpan" id="kobo.1152.1">or occluded.</span></span></p>
<p><span class="koboSpan" id="kobo.1153.1">From the set of</span><a id="_idIndexMarker485"/><span class="koboSpan" id="kobo.1154.1"> images of the datasets, our models will need to correctly detect the pedestrians present in the images and localize them, using bounding boxes. </span><span class="koboSpan" id="kobo.1154.2">To understand the problem better, as we saw in previous chapters, we are going to do </span><span class="No-Break"><span class="koboSpan" id="kobo.1155.1">some EDA.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer100">
<span class="koboSpan" id="kobo.1156.1"><img alt="Figure 5.18 – The Penn-Fudan Pedestrians dataset" src="image/B16591_05_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1157.1">Figure 5.18 – The Penn-Fudan Pedestrians dataset</span></p>
<p><span class="koboSpan" id="kobo.1158.1">As we </span><a id="_idIndexMarker486"/><span class="koboSpan" id="kobo.1159.1">can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1160.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1161.1">.18</span></em><span class="koboSpan" id="kobo.1162.1">, each image can have one or more pedestrians, with their corresponding bounding boxes. </span><span class="koboSpan" id="kobo.1162.2">Each image in the dataset is in color, and they have a variable width and height, which are later resized depending on the model requirements. </span><span class="koboSpan" id="kobo.1162.3">This figure was computed using the GluonCV visualization utils package (the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1163.1">plot_bbox</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1164.1"> function):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1165.1">
gcv.utils.viz.plot_bbox(image, gt_bboxes, class_names=["person"], ax=axes)</span></pre> <p><span class="koboSpan" id="kobo.1166.1">As for this datase,t there are no different classes to be classified, and no further visualizations </span><span class="No-Break"><span class="koboSpan" id="kobo.1167.1">are computed.</span></span></p>
<h3><span class="koboSpan" id="kobo.1168.1">Introducing object detection Model Zoo</span></h3>
<p><span class="koboSpan" id="kobo.1169.1">GluonCV also </span><a id="_idIndexMarker487"/><span class="koboSpan" id="kobo.1170.1">provides pre-trained</span><a id="_idIndexMarker488"/><span class="koboSpan" id="kobo.1171.1"> models for object detection in its Model Zoo. </span><span class="koboSpan" id="kobo.1171.2">For the </span><em class="italic"><span class="koboSpan" id="kobo.1172.1">MS COCO</span></em><span class="koboSpan" id="kobo.1173.1"> dataset, this is the accuracy (mAP) versus performance (samples per </span><span class="No-Break"><span class="koboSpan" id="kobo.1174.1">second) chart:</span></span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer101">
<span class="koboSpan" id="kobo.1175.1"><img alt="Figure 5.19 – Model Zoo for object detection (MS COCO)" src="image/B16591_05_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1176.1">Figure 5.19 – Model Zoo for object detection (MS COCO)</span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1177.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1178.1">Image adapted from the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1179.1">source: </span></span><a href="https://cv.gluon.ai/model_zoo/detection.html"><span class="No-Break"><span class="koboSpan" id="kobo.1180.1">https://cv.gluon.ai/model_zoo/detection.html</span></span></a></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1181.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1182.1">.19</span></em><span class="koboSpan" id="kobo.1183.1"> displays </span><a id="_idIndexMarker489"/><span class="koboSpan" id="kobo.1184.1">the most important </span><a id="_idIndexMarker490"/><span class="koboSpan" id="kobo.1185.1">pre-trained models in GluonCV Model Zoo, comparing accuracy (mAP on the vertical axis) and inference performance (samples per second on the horizontal axis). </span><span class="koboSpan" id="kobo.1185.2">There are no models (yet) in the top-right quadrant, meaning that, currently, we need to balance between </span><span class="No-Break"><span class="koboSpan" id="kobo.1186.1">both characteristics.</span></span></p>
<p><span class="koboSpan" id="kobo.1187.1">Using models from GluonCV Model Zoo can be done with just a couple of lines of code, and we will explore this path to solve our </span><em class="italic"><span class="koboSpan" id="kobo.1188.1">Penn-Fudan Pedestrians</span></em><span class="koboSpan" id="kobo.1189.1"> dataset in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1190.1">following steps.</span></span></p>
<h3><span class="koboSpan" id="kobo.1191.1">Working with MS COCO pre-trained models</span></h3>
<p><span class="koboSpan" id="kobo.1192.1">Models</span><a id="_idIndexMarker491"/><span class="koboSpan" id="kobo.1193.1"> from the GluonCV Model Zoo for object detection tasks have been pre-trained in the </span><em class="italic"><span class="koboSpan" id="kobo.1194.1">MS COCO</span></em><span class="koboSpan" id="kobo.1195.1"> dataset. </span><span class="koboSpan" id="kobo.1195.2">This</span><a id="_idIndexMarker492"/><span class="koboSpan" id="kobo.1196.1"> dataset is one of the most popular object detection datasets in computer vision. </span><span class="koboSpan" id="kobo.1196.2">It was developed by Microsoft in 2015 and was updated until 2017. </span><span class="koboSpan" id="kobo.1196.3">In its most recent update, it contains 80 classes (plus background) and is composed of </span><span class="No-Break"><span class="koboSpan" id="kobo.1197.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1198.1">Training/validation sets</span></strong><span class="koboSpan" id="kobo.1199.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.1200.1">118,000/5,000 images</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1201.1">A test set</span></strong><span class="koboSpan" id="kobo.1202.1">: </span><span class="No-Break"><span class="koboSpan" id="kobo.1203.1">41,000 images</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1204.1">Several object detection pre-trained models in GluonCV Model Zoo have been pre-trained with </span><em class="italic"><span class="koboSpan" id="kobo.1205.1">MS COCO</span></em><span class="koboSpan" id="kobo.1206.1">, and therefore, each object detected will be classified among 80 classes. </span><span class="koboSpan" id="kobo.1206.2">As there can be several objects in each image, in MXNet GluonCV implementations, the outputs of an object detection model are structured </span><span class="No-Break"><span class="koboSpan" id="kobo.1207.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1208.1">An array of indices</span></strong><span class="koboSpan" id="kobo.1209.1">: For each object detected, this array gives the index of the class of the detected object. </span><span class="koboSpan" id="kobo.1209.2">The shape of this array is </span><em class="italic"><span class="koboSpan" id="kobo.1210.1">BxNx1</span></em><span class="koboSpan" id="kobo.1211.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.1212.1">B</span></em><span class="koboSpan" id="kobo.1213.1"> is the batch size and </span><em class="italic"><span class="koboSpan" id="kobo.1214.1">N</span></em><span class="koboSpan" id="kobo.1215.1"> is the number of objects detected per image (depending on </span><span class="No-Break"><span class="koboSpan" id="kobo.1216.1">the model).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1217.1">An array of probabilities</span></strong><span class="koboSpan" id="kobo.1218.1">: For each object detected, this array gives the probability associated with the detected object of being the detected class in the </span><strong class="bold"><span class="koboSpan" id="kobo.1219.1">array of indices</span></strong><span class="koboSpan" id="kobo.1220.1">. </span><span class="koboSpan" id="kobo.1220.2">The shape of this array is BxNx1, where B is the batch size and N is the number of objects detected per image (depending on </span><span class="No-Break"><span class="koboSpan" id="kobo.1221.1">the model).</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1222.1">An array of bounding boxes</span></strong><span class="koboSpan" id="kobo.1223.1">: For each object detected, this array gives the bounding box coordinates associated with the detected object. </span><span class="koboSpan" id="kobo.1223.2">The shape of this array is </span><em class="italic"><span class="koboSpan" id="kobo.1224.1">BxNx4</span></em><span class="koboSpan" id="kobo.1225.1">, where </span><em class="italic"><span class="koboSpan" id="kobo.1226.1">B</span></em><span class="koboSpan" id="kobo.1227.1"> is the batch size, N is the number of objects detected per image (depending on the model), and 4 is the coordinates in the format </span><em class="italic"><span class="koboSpan" id="kobo.1228.1">[x-min, y-min, </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1229.1">x-max, y-max]</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1230.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1231.1">Looking at </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1232.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1233.1">.19</span></em><span class="koboSpan" id="kobo.1234.1">, two separated groups can be seen – the Faster R-CNN family, which has the highest accuracy but is slow, and the YOLO family, which is very fast but has a lower accuracy. </span><span class="koboSpan" id="kobo.1234.2">For</span><a id="_idIndexMarker493"/><span class="koboSpan" id="kobo.1235.1"> our experiments, we have selected two of the most popular models, each of </span><a id="_idIndexMarker494"/><span class="koboSpan" id="kobo.1236.1">them corresponding to a different Faster R-CNN (the backbone of ResNet-101 with the FPN version) and </span><em class="italic"><span class="koboSpan" id="kobo.1237.1">YOLOv3</span></em><span class="koboSpan" id="kobo.1238.1"> (the backbone of Darknet53, a </span><span class="No-Break"><span class="koboSpan" id="kobo.1239.1">53 CNN).</span></span></p>
<p><span class="koboSpan" id="kobo.1240.1">Furthermore, MS COCO contains the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1241.1">person</span></strong><span class="koboSpan" id="kobo.1242.1"> class, and therefore, models pre-trained in MS COCO are well suited for the </span><em class="italic"><span class="koboSpan" id="kobo.1243.1">Penn-Fudan </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1244.1">Pedestrian</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1245.1"> dataset.</span></span></p>
<h3><span class="koboSpan" id="kobo.1246.1">Loading a Faster R-CNN pre-trained model from Model Zoo</span></h3>
<p><span class="koboSpan" id="kobo.1247.1">Faster R-CNN is </span><a id="_idIndexMarker495"/><span class="koboSpan" id="kobo.1248.1">a two-stage object detection architecture, meaning that, first, it provides regions where objects could be located, and second, by analyzing those regions, it </span><a id="_idIndexMarker496"/><span class="koboSpan" id="kobo.1249.1">provides the classes and locations of the detected objects. </span><span class="koboSpan" id="kobo.1249.2">It was developed by Ren et al. </span><span class="koboSpan" id="kobo.1249.3">(Microsoft Research) </span><span class="No-Break"><span class="koboSpan" id="kobo.1250.1">in 2014.</span></span></p>
<p><span class="koboSpan" id="kobo.1251.1">It is the third iteration of a series of architectures that have evolved from each other – R-CNN, Fast R-CNN, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1252.1">Faster R-CNN.</span></span></p>
<p><span class="koboSpan" id="kobo.1253.1">From the research papers (There's more section), we can see an </span><span class="No-Break"><span class="koboSpan" id="kobo.1254.1">R-CNN architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer102">
<span class="koboSpan" id="kobo.1255.1"><img alt="Figure 5.20 – The high-level architecture of R-CNN" src="image/B16591_05_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1256.1">Figure 5.20 – The high-level architecture of R-CNN</span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1257.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1258.1">Source: The car image has the following source: </span><em class="italic"><span class="koboSpan" id="kobo.1259.1">azerbaijan_stockers</span></em><span class="koboSpan" id="kobo.1260.1"> on </span><span class="No-Break"><span class="koboSpan" id="kobo.1261.1">Freepik: </span></span><a href="https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&amp;position=48&amp;from_view=search&amp;track=sph&amp;uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2"><span class="No-Break"><span class="koboSpan" id="kobo.1262.1">https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1263.1">#query=CAR&amp;position=48&amp;from_view=search&amp;track=sph&amp;uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2</span></span></p>
<p><span class="koboSpan" id="kobo.1264.1">Secondly, we </span><a id="_idIndexMarker497"/><span class="koboSpan" id="kobo.1265.1">can see the Fast </span><span class="No-Break"><span class="koboSpan" id="kobo.1266.1">R-CNN architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer103">
<span class="koboSpan" id="kobo.1267.1"><img alt="Figure 5.21 – The high-level architecture of Fast R-CNN" src="image/B16591_05_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1268.1">Figure 5.21 – The high-level architecture of Fast R-CNN</span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1269.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1270.1">Source: The car image has the following source: </span><em class="italic"><span class="koboSpan" id="kobo.1271.1">azerbaijan_stockers</span></em><span class="koboSpan" id="kobo.1272.1"> on </span><span class="No-Break"><span class="koboSpan" id="kobo.1273.1">Freepik: </span></span><a href="https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&amp;position=48&amp;from_view=search&amp;track=sph&amp;uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2"><span class="No-Break"><span class="koboSpan" id="kobo.1274.1">https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&amp;position=48&amp;from_view=search&amp;track=sph&amp;uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2</span></span></a></p>
<p><span class="koboSpan" id="kobo.1275.1">Finally, we can</span><a id="_idIndexMarker498"/><span class="koboSpan" id="kobo.1276.1"> see the Faster </span><span class="No-Break"><span class="koboSpan" id="kobo.1277.1">R-CNN architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer104">
<span class="koboSpan" id="kobo.1278.1"><img alt="Figure 5.22 – The high-level architecture of Faster R-CNN" src="image/B16591_05_22.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1279.1">Figure 5.22 – The high-level architecture of Faster R-CNN</span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1280.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1281.1">Source: The car image has the following source: </span><em class="italic"><span class="koboSpan" id="kobo.1282.1">azerbaijan_stockers</span></em><span class="koboSpan" id="kobo.1283.1"> on </span><span class="No-Break"><span class="koboSpan" id="kobo.1284.1">Freepik: </span></span><a href="https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&amp;position=48&amp;from_view=search&amp;track=sph&amp;uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2"><span class="No-Break"><span class="koboSpan" id="kobo.1285.1">https://www.freepik.com/free-photo/mini-coupe-high-speed-drive-road-with-front-lights_6159501.htm#query=CAR&amp;position=48&amp;from_view=search&amp;track=sph&amp;uuid=e82c3ce9-2fe8-40ef-9d39-e8d27781fdf2</span></span></a></p>
<p><span class="koboSpan" id="kobo.1286.1">The different architectures have similarities and differences, namely </span><span class="No-Break"><span class="koboSpan" id="kobo.1287.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1288.1">R-CNN</span></strong><span class="koboSpan" id="kobo.1289.1">: This </span><a id="_idIndexMarker499"/><span class="koboSpan" id="kobo.1290.1">uses the selective search algorithm to provide 2,000 region proposals. </span><span class="koboSpan" id="kobo.1290.2">Each of these regions is then fed into a CNN, which generates a feature vector for each object of 4,096 features and the four-coordinate’s bounding box. </span><span class="koboSpan" id="kobo.1290.3">These feature vectors are the input to</span><a id="_idIndexMarker500"/><span class="koboSpan" id="kobo.1291.1"> the </span><strong class="bold"><span class="koboSpan" id="kobo.1292.1">Support Vector Machine</span></strong><span class="koboSpan" id="kobo.1293.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1294.1">SVM</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1295.1">) classifier.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1296.1">Fast R-CNN</span></strong><span class="koboSpan" id="kobo.1297.1">: In</span><a id="_idIndexMarker501"/><span class="koboSpan" id="kobo.1298.1"> this iteration, instead of feeding each region to a CNN, the whole image is fed once, and a single feature map of the full image is computed, accelerating the process. </span><span class="koboSpan" id="kobo.1298.2">Then, </span><strong class="bold"><span class="koboSpan" id="kobo.1299.1">Regions Of Interest</span></strong><span class="koboSpan" id="kobo.1300.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1301.1">ROIs</span></strong><span class="koboSpan" id="kobo.1302.1">) are </span><a id="_idIndexMarker502"/><span class="koboSpan" id="kobo.1303.1">computed over this feature map (instead of the image), obtained similarly with the </span><em class="italic"><span class="koboSpan" id="kobo.1304.1">selective search</span></em><span class="koboSpan" id="kobo.1305.1"> algorithm. </span><span class="koboSpan" id="kobo.1305.2">These are then passed through an </span><em class="italic"><span class="koboSpan" id="kobo.1306.1">ROI pooling</span></em><span class="koboSpan" id="kobo.1307.1"> layer, where each object in a proposed region is assigned a feature map of the same shape. </span><span class="koboSpan" id="kobo.1307.2">This is an efficient method in which the output can now be fed into the two networks for the regressor and classifier, which provide the location and class of each object respectively. </span><span class="koboSpan" id="kobo.1307.3">These two networks are based on fully connected layers. </span><span class="koboSpan" id="kobo.1307.4">The regressor computes offsets from the ROIs, and for the classifier, the output activation function </span><span class="No-Break"><span class="koboSpan" id="kobo.1308.1">is </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1309.1">softmax</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1310.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1311.1">Faster R-CNN</span></strong><span class="koboSpan" id="kobo.1312.1">: In </span><a id="_idIndexMarker503"/><span class="koboSpan" id="kobo.1313.1">this last iteration, three changes are introduced. </span><span class="koboSpan" id="kobo.1313.2">Firstly, a backbone CNN is also used to compute the feature map of the image; however, instead of using the selective search algorithm to propose regions, some </span><em class="italic"><span class="koboSpan" id="kobo.1314.1">fully convolutional</span></em><span class="koboSpan" id="kobo.1315.1"> layers are added on top of the backbone CNN called a </span><strong class="bold"><span class="koboSpan" id="kobo.1316.1">Region Proposal Network</span></strong><span class="koboSpan" id="kobo.1317.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1318.1">RPN</span></strong><span class="koboSpan" id="kobo.1319.1">), yielding </span><a id="_idIndexMarker504"/><span class="koboSpan" id="kobo.1320.1">a much smaller computation time. </span><span class="koboSpan" id="kobo.1320.2">Secondly, these region proposals are computed as offsets associated with anchor boxes. </span><span class="koboSpan" id="kobo.1320.3">Lastly, to reduce the number of regions to</span><a id="_idIndexMarker505"/><span class="koboSpan" id="kobo.1321.1"> process, </span><strong class="bold"><span class="koboSpan" id="kobo.1322.1">Non-Maximum Suppression</span></strong><span class="koboSpan" id="kobo.1323.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1324.1">NMS</span></strong><span class="koboSpan" id="kobo.1325.1">) is used. </span><span class="koboSpan" id="kobo.1325.2">These three changes provide faster inference and </span><span class="No-Break"><span class="koboSpan" id="kobo.1326.1">higher accuracy.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1327.1">For our experiments, we will use as the backbone the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1328.1">v1d</span></strong><span class="koboSpan" id="kobo.1329.1"> version of weights from a ResNet-101 network, with</span><a id="_idIndexMarker506"/><span class="koboSpan" id="kobo.1330.1"> a </span><strong class="bold"><span class="koboSpan" id="kobo.1331.1">feature pyramid network</span></strong><span class="koboSpan" id="kobo.1332.1"> as the RPN. </span><span class="koboSpan" id="kobo.1332.2">We can load the model with a single line </span><span class="No-Break"><span class="koboSpan" id="kobo.1333.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1334.1">
faster_rcnn = gcv.model_zoo.get_model("faster_rcnn_fpn_resnet101_v1d_coco", pretrained=True, ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.1335.1">The model then </span><span class="No-Break"><span class="koboSpan" id="kobo.1336.1">downloads successfully.</span></span></p>
<p><span class="koboSpan" id="kobo.1337.1">The GluonCV implementation of this model is capable of detecting 80,000 </span><span class="No-Break"><span class="koboSpan" id="kobo.1338.1">distinct objects.</span></span></p>
<h3><span class="koboSpan" id="kobo.1339.1">Evaluating a Faster R-CNN pre-trained model from Model Zoo</span></h3>
<p><span class="koboSpan" id="kobo.1340.1">Using </span><a id="_idIndexMarker507"/><span class="koboSpan" id="kobo.1341.1">the </span><em class="italic"><span class="koboSpan" id="kobo.1342.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.1343.1"> dataset, we can now perform qualitative and quantitative evaluation of the loaded model from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1344.1">previous section.</span></span></p>
<p><span class="koboSpan" id="kobo.1345.1">Qualitatively, we </span><a id="_idIndexMarker508"/><span class="koboSpan" id="kobo.1346.1">can choose an image from the dataset and compare the output of the model with the ground-truth output from </span><span class="No-Break"><span class="koboSpan" id="kobo.1347.1">the dataset:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer105">
<span class="koboSpan" id="kobo.1348.1"><img alt="Figure 5.23 – Comparing predictions and ground-truth for Faster R-CNN" src="image/B16591_05_23.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1349.1">Figure 5.23 – Comparing predictions and ground-truth for Faster R-CNN</span></p>
<p><span class="koboSpan" id="kobo.1350.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1351.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1352.1">.23</span></em><span class="koboSpan" id="kobo.1353.1">, we can see a very strong correlation between the predicted segmentation masks and the expected results from the ground-truth, as well as strong confidence (+99%) and perfect class accuracy from the model. </span><span class="koboSpan" id="kobo.1353.2">This figure was computed using the GluonCV visualization </span><strong class="source-inline"><span class="koboSpan" id="kobo.1354.1">utils</span></strong><span class="koboSpan" id="kobo.1355.1"> package (the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1356.1">plot_bbox</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1357.1"> function).</span></span></p>
<p><span class="koboSpan" id="kobo.1358.1">Quantitatively, we can perform a mAP evaluation and the runtime spent computing </span><span class="No-Break"><span class="koboSpan" id="kobo.1359.1">this metric:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1360.1">
('VOCMeanAP', 0.6716161702078043)
 Elapsed Time:  249.30912852287292 secs</span></pre> <p><span class="koboSpan" id="kobo.1361.1">The computed mAP for this model for </span><em class="italic"><span class="koboSpan" id="kobo.1362.1">MS COCO</span></em><span class="koboSpan" id="kobo.1363.1"> (see the object detection model zoo) is 40.7; therefore, given the value of 0.67 for our model, we can conclude our model performs the</span><a id="_idIndexMarker509"/><span class="koboSpan" id="kobo.1364.1"> task accurately. </span><span class="koboSpan" id="kobo.1364.2">However, it did take some time to complete (~250 seconds), which is expected for Faster </span><span class="No-Break"><span class="koboSpan" id="kobo.1365.1">R-CNN architectures.</span></span></p>
<h3><span class="koboSpan" id="kobo.1366.1">Loading a YOLOv3 pre-trained model from Model Zoo</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.1367.1">You Only Look Once Version 3</span></strong><span class="koboSpan" id="kobo.1368.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1369.1">YOLOv3</span></strong><span class="koboSpan" id="kobo.1370.1">) is a</span><a id="_idIndexMarker510"/><span class="koboSpan" id="kobo.1371.1"> single-stage object detection architecture, meaning </span><a id="_idIndexMarker511"/><span class="koboSpan" id="kobo.1372.1">it uses an end-to-end approach that makes predictions of bounding </span><a id="_idIndexMarker512"/><span class="koboSpan" id="kobo.1373.1">boxes and class probabilities in a single step. </span><span class="koboSpan" id="kobo.1373.2">It was developed by Redmon et al. </span><span class="koboSpan" id="kobo.1373.3">(at the University of Washington) from 2016 (YOLO) to </span><span class="No-Break"><span class="koboSpan" id="kobo.1374.1">2018 (YOLOv3).</span></span></p>
<p><span class="koboSpan" id="kobo.1375.1">It is the third iteration of a series of architectures that have evolved from each other – YOLO, YOLOv2, </span><span class="No-Break"><span class="koboSpan" id="kobo.1376.1">and YOLOv3.</span></span></p>
<p><span class="koboSpan" id="kobo.1377.1">From</span><a id="_idIndexMarker513"/><span class="koboSpan" id="kobo.1378.1"> the research papers, we can see the </span><span class="No-Break"><span class="koboSpan" id="kobo.1379.1">YOLO architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer106">
<span class="koboSpan" id="kobo.1380.1"><img alt="Figure 5.24 – The architecture of YOLOv1" src="image/B16591_05_24.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1381.1">Figure 5.24 – The architecture of YOLOv1</span></p>
<p><span class="koboSpan" id="kobo.1382.1">Secondly, we can</span><a id="_idIndexMarker514"/><span class="koboSpan" id="kobo.1383.1"> see the </span><span class="No-Break"><span class="koboSpan" id="kobo.1384.1">YOLOv2 architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer107">
<span class="koboSpan" id="kobo.1385.1"><img alt="Figure 5.25 – The architecture of YOLOv2" src="image/B16591_05_25.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1386.1">Figure 5.25 – The architecture of YOLOv2</span></p>
<p><span class="koboSpan" id="kobo.1387.1">And finally, we</span><a id="_idIndexMarker515"/><span class="koboSpan" id="kobo.1388.1"> can</span><a id="_idIndexMarker516"/><span class="koboSpan" id="kobo.1389.1"> see the</span><a id="_idIndexMarker517"/> <span class="No-Break"><span class="koboSpan" id="kobo.1390.1">YOLOv3 architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer108">
<span class="koboSpan" id="kobo.1391.1"><img alt="Figure 5.26 – The architecture of YOLOv3" src="image/B16591_05_26.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1392.1">Figure 5.26 – The architecture of YOLOv3</span></p>
<p><span class="koboSpan" id="kobo.1393.1">The different architectures have similarities and differences, namely </span><span class="No-Break"><span class="koboSpan" id="kobo.1394.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1395.1">YOLO</span></strong><span class="koboSpan" id="kobo.1396.1">: The </span><a id="_idIndexMarker518"/><span class="koboSpan" id="kobo.1397.1">initial model decomposes each image into grid cells of equal size. </span><span class="koboSpan" id="kobo.1397.2">Each cell is responsible for detecting an object if the location of the center of the object is within the cell. </span><span class="koboSpan" id="kobo.1397.3">Each cell can predict two bounding boxes, their class, and their confidence score, but only one object (with a different size and location). </span><span class="koboSpan" id="kobo.1397.4">All the predictions are made simultaneously, using a CNN composed of 24 convolutional layers and 2 fully connected layers. </span><span class="koboSpan" id="kobo.1397.5">There is a large number of overlapping bounding boxes, and NMS is used to reach the </span><span class="No-Break"><span class="koboSpan" id="kobo.1398.1">final output.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1399.1">YOLOv2</span></strong><span class="koboSpan" id="kobo.1400.1">: YOLO architecture</span><a id="_idIndexMarker519"/><span class="koboSpan" id="kobo.1401.1"> struggles to detect small objects in groups. </span><span class="koboSpan" id="kobo.1401.2">To solve this issue, on this iteration, a number of changes are introduced – batch normalization to improve training accuracy, anchor boxes for regression, increased detection capabilities to five bounding boxes per cell, and a new backbone network, DarkNet-19, with 19 convolutional layers and 5 max pooling layers, with 11 more layers </span><span class="No-Break"><span class="koboSpan" id="kobo.1402.1">for detection.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1403.1">YOLOv3</span></strong><span class="koboSpan" id="kobo.1404.1">: In this</span><a id="_idIndexMarker520"/><span class="koboSpan" id="kobo.1405.1"> last iteration, three changes are introduced. </span><span class="koboSpan" id="kobo.1405.2">Firstly, to increase further the detection accuracy of small objects, the backbone network is updated to DarkNet-53, with 53 convolutional layers and 53 layers for the detection head, allowing for predictions on three different scales. </span><span class="koboSpan" id="kobo.1405.3">Secondly, the number of bounding boxes per cell is reduced from five to three; however, taking into account the three different scales, this provides nine </span><span class="No-Break"><span class="koboSpan" id="kobo.1406.1">anchor boxes.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1407.1">For our</span><a id="_idIndexMarker521"/><span class="koboSpan" id="kobo.1408.1"> experiments, we</span><a id="_idIndexMarker522"/><span class="koboSpan" id="kobo.1409.1"> will use DarkNet-53 as a backbone. </span><span class="koboSpan" id="kobo.1409.2">We can load the model with a single line </span><span class="No-Break"><span class="koboSpan" id="kobo.1410.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1411.1">
yolo = gcv.model_zoo.get_model("yolo3_darknet53_coco", pretrained=True, ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.1412.1">The model then </span><span class="No-Break"><span class="koboSpan" id="kobo.1413.1">downloads successfully.</span></span></p>
<p><span class="koboSpan" id="kobo.1414.1">The GluonCV implementation of this model is capable of detecting 100 </span><span class="No-Break"><span class="koboSpan" id="kobo.1415.1">distinct objects.</span></span></p>
<h3><span class="koboSpan" id="kobo.1416.1">Evaluating a YOLOv3 pre-trained model from Model Zoo</span></h3>
<p><span class="koboSpan" id="kobo.1417.1">Using the </span><em class="italic"><span class="koboSpan" id="kobo.1418.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.1419.1"> dataset, we can now perform qualitative and quantitative evaluation</span><a id="_idIndexMarker523"/><span class="koboSpan" id="kobo.1420.1"> of the loaded model from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1421.1">previous section.</span></span></p>
<p><span class="koboSpan" id="kobo.1422.1">Qualitatively, we </span><a id="_idIndexMarker524"/><span class="koboSpan" id="kobo.1423.1">can choose an image from the dataset and compare the output of the model with the ground-truth output from </span><span class="No-Break"><span class="koboSpan" id="kobo.1424.1">the dataset:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer109">
<span class="koboSpan" id="kobo.1425.1"><img alt="Figure 5.27 – Comparing predictions and ground-truth for YOLOv3" src="image/B16591_05_27.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1426.1">Figure 5.27 – Comparing predictions and ground-truth for YOLOv3</span></p>
<p><span class="koboSpan" id="kobo.1427.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1428.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1429.1">.27</span></em><span class="koboSpan" id="kobo.1430.1">, we can see a very strong correlation between the bounding boxes of the expected results from the ground-truth and the actual outputs of the model, as well as strong confidence (+95%) and perfect class accuracy from the model. </span><span class="koboSpan" id="kobo.1430.2">This figure was computed using the GluonCV visualization utils package (the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1431.1">plot_bbox</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1432.1"> function).</span></span></p>
<p><span class="koboSpan" id="kobo.1433.1">Quantitatively, we can perform a mAP evaluation and the runtime spent computing </span><span class="No-Break"><span class="koboSpan" id="kobo.1434.1">this metric:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1435.1">
('VOCMeanAP', 0.5339115787945962)
 Elapsed Time:  113.4049768447876 secs</span></pre> <p><span class="koboSpan" id="kobo.1436.1">The </span><a id="_idIndexMarker525"/><span class="koboSpan" id="kobo.1437.1">computed</span><a id="_idIndexMarker526"/><span class="koboSpan" id="kobo.1438.1"> mAP for this model for </span><em class="italic"><span class="koboSpan" id="kobo.1439.1">MS COCO</span></em><span class="koboSpan" id="kobo.1440.1"> (see the object detection model zoo) is 36.0; therefore, given the value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1441.1">0.53</span></strong><span class="koboSpan" id="kobo.1442.1"> for our model, we can conclude that it is performing the task accurately. </span><span class="koboSpan" id="kobo.1442.2">It took </span><strong class="source-inline"><span class="koboSpan" id="kobo.1443.1">113</span></strong><span class="koboSpan" id="kobo.1444.1"> seconds </span><span class="No-Break"><span class="koboSpan" id="kobo.1445.1">to complete.</span></span></p>
<h2 id="_idParaDest-113"><a id="_idTextAnchor114"/><span class="koboSpan" id="kobo.1446.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.1447.1">In this recipe, we tackled the object detection problem. </span><span class="koboSpan" id="kobo.1447.2">We analyzed the differences between image classification problem and object detection problem for evaluation, network architectures, </span><span class="No-Break"><span class="koboSpan" id="kobo.1448.1">and datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.1449.1">In our examples, we used a publicly available dataset, the </span><em class="italic"><span class="koboSpan" id="kobo.1450.1">Penn-Fudan Pedestrians</span></em><span class="koboSpan" id="kobo.1451.1"> dataset, and we used pre-trained models on MS COCO. </span><span class="koboSpan" id="kobo.1451.2">This choice was not casual; </span><em class="italic"><span class="koboSpan" id="kobo.1452.1">MS COCO</span></em><span class="koboSpan" id="kobo.1453.1"> has a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1454.1">person</span></strong><span class="koboSpan" id="kobo.1455.1"> class, and therefore, the expectation was that these pre-trained models would perform well, as they had already seen images from the dataset class. </span><span class="koboSpan" id="kobo.1455.2">However, as mentioned in the previous recipe, when this is not possible, and we apply pre-trained models from a dataset to another dataset, the data probability distribution will typically be very different; hence, the accuracy obtained can be very low. </span><span class="koboSpan" id="kobo.1455.3">This is known as the domain gap or domain adaptation problem between the source dataset (the images that a model has been pre-trained on) and the target dataset (the images that the model is </span><span class="No-Break"><span class="koboSpan" id="kobo.1456.1">evaluated on).</span></span></p>
<p><span class="koboSpan" id="kobo.1457.1">One way to tackle these issues for supervised learning problems is fine-tuning. </span><span class="koboSpan" id="kobo.1457.2">This approach is explored in detail in </span><a href="B16591_07.xhtml#_idTextAnchor148"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1458.1">Chapter 7</span></em></span></a><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1459.1">.</span></em></span></p>
<p><span class="koboSpan" id="kobo.1460.1">We ended the recipe by evaluating our two pre-trained models, Faster R-CNN and YOLOv3, and we were able to confirm that our Faster R-CNN model was very accurate but slow, while YOLOv3 was much faster (2x) with a slightly lower accuracy (~</span><span class="No-Break"><span class="koboSpan" id="kobo.1461.1">20% decrease).</span></span></p>
<h2 id="_idParaDest-114"><a id="_idTextAnchor115"/><span class="koboSpan" id="kobo.1462.1">There’s more...</span></h2>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1463.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1464.1">.19</span></em><span class="koboSpan" id="kobo.1465.1"> showed a static image corresponding to the mAP versus samples per second graph for the Model Zoo for object detection (on </span><em class="italic"><span class="koboSpan" id="kobo.1466.1">MS COCO</span></em><span class="koboSpan" id="kobo.1467.1">). </span><span class="koboSpan" id="kobo.1467.2">There is also a dynamic version at this link that is worth taking a look at: </span><a href="https://cv.gluon.ai/model_zoo/detection.html"><span class="koboSpan" id="kobo.1468.1">https://cv.gluon.ai/model_zoo/detection.html</span></a><span class="koboSpan" id="kobo.1469.1">. </span><span class="koboSpan" id="kobo.1469.2">On this page, results from different models available in </span><em class="italic"><span class="koboSpan" id="kobo.1470.1">GluonCV Model Zoo</span></em><span class="koboSpan" id="kobo.1471.1"> are included; I suggest that you reproduce these result,s as it is an </span><span class="No-Break"><span class="koboSpan" id="kobo.1472.1">interesting exercise.</span></span></p>
<p><span class="koboSpan" id="kobo.1473.1">To understand more about the </span><em class="italic"><span class="koboSpan" id="kobo.1474.1">MS COCO</span></em><span class="koboSpan" id="kobo.1475.1"> dataset, the original paper is available </span><span class="No-Break"><span class="koboSpan" id="kobo.1476.1">at </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1477.1">https://arxiv.org/pdf/1405.0312.pdf</span></span><span class="No-Break"><span class="koboSpan" id="kobo.1478.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1479.1">Furthermore, it is very interesting to read the original research papers and see how object detectors have evolved from an academic point </span><span class="No-Break"><span class="koboSpan" id="kobo.1480.1">of view:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1481.1">R-CNN</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1482.1">: </span></span><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1483.1">https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf</span></span></a><a href="https://www.cv-foundation.org/openaccess/content_cvpr_2014/papers/Girshick_Rich_Feature_Hierarchies_2014_CVPR_paper.pdf "/></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1484.1">Fast </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1485.1">R-CNN</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1486.1">: </span></span><a href="https://arxiv.org/pdf/1504.08083.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1487.1">https://arxiv.org/pdf/1504.08083.pdf</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1488.1">Faster </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1489.1">R-CNN</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1490.1">: </span></span><a href="https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1491.1">https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf</span></span></a><a href="https://proceedings.neurips.cc/paper/2015/file/14bfa6bb14875e45bba028a21ed38046-Paper.pdf "/></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1492.1">YOLO</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1493.1">: </span></span><a href="https://arxiv.org/pdf/1506.02640.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1494.1">https://arxiv.org/pdf/1506.02640.pdf</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1495.1">YOLOv2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1496.1">: </span></span><a href="https://arxiv.org/pdf/1612.08242.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1497.1">https://arxiv.org/pdf/1612.08242.pdf</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1498.1">YOLOv3</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1499.1">: </span></span><a href="https://arxiv.org/pdf/1804.02767.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1500.1">https://arxiv.org/pdf/1804.02767.pdf</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.1501.1">In this recipe, we have seen how Faster R-CNN was more accurate but slower, and YOLOv3 was faster but less accurate. </span><span class="koboSpan" id="kobo.1501.2">To balance the trade-off between accuracy and inference time for object detection problems, there are </span><span class="No-Break"><span class="koboSpan" id="kobo.1502.1">different possibilities.</span></span></p>
<p><span class="koboSpan" id="kobo.1503.1">One option is to estimate the difficulty of an image and apply a different object detector. </span><span class="koboSpan" id="kobo.1503.2">If it is a challenging image, use the Faster R-CNN family; if it is simpler, use YOLOv3. </span><span class="koboSpan" id="kobo.1503.3">This approach is explored in detail in this paper: </span><a href="https://arxiv.org/pdf/1803.08707.pdf"><span class="koboSpan" id="kobo.1504.1">https://arxiv.org/pdf/1803.08707.pdf</span></a><span class="koboSpan" id="kobo.1505.1">; however, </span><em class="italic"><span class="koboSpan" id="kobo.1506.1">fine-tuning</span></em><span class="koboSpan" id="kobo.1507.1"> a fast model such as YOLOv3 is recommended as an initial approach to </span><span class="No-Break"><span class="koboSpan" id="kobo.1508.1">this issue.</span></span></p>
<h1 id="_idParaDest-115"><a id="_idTextAnchor116"/><span class="koboSpan" id="kobo.1509.1">Segmenting objects in images with MXNet – PSPNet and DeepLab-v3</span></h1>
<p><span class="koboSpan" id="kobo.1510.1">In this</span><a id="_idIndexMarker527"/><span class="koboSpan" id="kobo.1511.1"> recipe, we will see how to use MXNet and GluonCV on a pre-trained model, segmenting objects in images from a dataset. </span><span class="koboSpan" id="kobo.1511.2">This means that we will be able to split objects into different classes, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.1512.1">person</span></strong><span class="koboSpan" id="kobo.1513.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1514.1">cat</span></strong><span class="koboSpan" id="kobo.1515.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1516.1">dog</span></strong><span class="koboSpan" id="kobo.1517.1">. </span><span class="koboSpan" id="kobo.1517.2">When framing the problem as segmentation, the expected output is an image of the same size as the</span><a id="_idIndexMarker528"/><span class="koboSpan" id="kobo.1518.1"> input image, with each pixel value being the classified label (we will analyze how this works in the following sections). </span><span class="koboSpan" id="kobo.1518.2">We will see how to use GluonCV Model Zoo with two very important models for </span><strong class="bold"><span class="koboSpan" id="kobo.1519.1">semantic segmentation</span></strong><span class="koboSpan" id="kobo.1520.1"> – </span><strong class="bold"><span class="koboSpan" id="kobo.1521.1">PSPNet</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1522.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1523.1">DeepLab-v3</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1524.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1525.1">In this recipe, we will compare the performance of these two pre-trained models to segment objects semantically on the dataset introduced in the previous chapter, </span><em class="italic"><span class="koboSpan" id="kobo.1526.1">Penn-Fudan Pedestrians</span></em><span class="koboSpan" id="kobo.1527.1">, as its ground-truth also includes </span><span class="No-Break"><span class="koboSpan" id="kobo.1528.1">segmentation masks.</span></span></p>
<h2 id="_idParaDest-116"><a id="_idTextAnchor117"/><span class="koboSpan" id="kobo.1529.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.1530.1">As with previous chapters, in this recipe, we will use a few matrix operations and linear algebra, but it will not be </span><span class="No-Break"><span class="koboSpan" id="kobo.1531.1">too difficult.</span></span></p>
<p><span class="koboSpan" id="kobo.1532.1">As we will unpack in this recipe, semantic segmentation is similar to classification and object detection problems, and therefore, chapters and recipes where we explored the foundations of these topics are recommended to revisit. </span><span class="koboSpan" id="kobo.1532.2">Furthermore, we will be working on image datasets. </span><span class="koboSpan" id="kobo.1532.3">This recipe will combine what we learned in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1533.1">following chapters:</span></span></p>
<ul>
<li><em class="italic"><span class="koboSpan" id="kobo.1534.1">Understanding image datasets: load, manage, and visualize the Fashion MNIST dataset</span></em><span class="koboSpan" id="kobo.1535.1">, the third recipe from </span><a href="B16591_02.xhtml#_idTextAnchor029"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1536.1">Chapter 2</span></em></span></a><span class="koboSpan" id="kobo.1537.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1538.1">Working with MXNet and Visualizing Datasets: Gluon </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1539.1">and DataLoader</span></em></span></li>
<li><a href="B16591_04.xhtml#_idTextAnchor075"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1540.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.1541.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1542.1">Solving </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1543.1">Classification Problems</span></em></span></li>
</ul>
<h2 id="_idParaDest-117"><a id="_idTextAnchor118"/><span class="koboSpan" id="kobo.1544.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.1545.1">In this recipe, we will take the </span><span class="No-Break"><span class="koboSpan" id="kobo.1546.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.1547.1">Introduce </span><span class="No-Break"><span class="koboSpan" id="kobo.1548.1">semantic segmentation.</span></span></li>
<li><span class="koboSpan" id="kobo.1549.1">Evaluate </span><span class="No-Break"><span class="koboSpan" id="kobo.1550.1">segmentation models.</span></span></li>
<li><span class="koboSpan" id="kobo.1551.1">Compare network architectures for </span><span class="No-Break"><span class="koboSpan" id="kobo.1552.1">semantic segmentation.</span></span></li>
<li><span class="koboSpan" id="kobo.1553.1">Explore the </span><em class="italic"><span class="koboSpan" id="kobo.1554.1">Penn-Fudan Pedestrians</span></em><span class="koboSpan" id="kobo.1555.1"> dataset with </span><span class="No-Break"><span class="koboSpan" id="kobo.1556.1">segmentation ground-truth.</span></span></li>
<li><span class="koboSpan" id="kobo.1557.1">Introduce Semantic segmentation </span><span class="No-Break"><span class="koboSpan" id="kobo.1558.1">Model Zoo.</span></span></li>
<li><span class="koboSpan" id="kobo.1559.1">Load a PSPNet pre-trained model from </span><span class="No-Break"><span class="koboSpan" id="kobo.1560.1">Model Zoo.</span></span></li>
<li><span class="koboSpan" id="kobo.1561.1">Evaluate a PSPNet pre-trained model from </span><span class="No-Break"><span class="koboSpan" id="kobo.1562.1">Model Zoo.</span></span></li>
<li><span class="koboSpan" id="kobo.1563.1">Load a DeepLab-v3 pre-trained model from </span><span class="No-Break"><span class="koboSpan" id="kobo.1564.1">Model Zoo.</span></span></li>
<li><span class="koboSpan" id="kobo.1565.1">Evaluate a DeepLab-v3 pre-trained model from </span><span class="No-Break"><span class="koboSpan" id="kobo.1566.1">Model Zoo.</span></span></li>
</ol>
<h3><span class="koboSpan" id="kobo.1567.1">Introducing semantic segmentation</span></h3>
<p><span class="koboSpan" id="kobo.1568.1">In some of </span><a id="_idIndexMarker529"/><span class="koboSpan" id="kobo.1569.1">the previous chapters and recipes, we analyzed image classification problems where the task of our </span><a id="_idIndexMarker530"/><span class="koboSpan" id="kobo.1570.1">models was to take an image and define the class most likely associated with it. </span><span class="koboSpan" id="kobo.1570.2">In semantic segmentation, however, there can be multiple objects per image, corresponding to different classes, and in different locations of the image. </span><span class="koboSpan" id="kobo.1570.3">In object detection, the generated output to solve this problem was two lists, one providing the most likely class of each detected object and another indicating the estimated location of the object. </span><span class="koboSpan" id="kobo.1570.4">For semantic segmentation, the output for each image is a set of binary images, one per class expected to be detected (dataset classes), where each pixel can have a value of 1 (active) if that pixel has been classified with that label, or 0 (inactive) otherwise. </span><span class="koboSpan" id="kobo.1570.5">Each of </span><a id="_idIndexMarker531"/><span class="koboSpan" id="kobo.1571.1">these images is a </span><strong class="bold"><span class="koboSpan" id="kobo.1572.1">binary </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1573.1">segmentation mask</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1574.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer110">
<span class="koboSpan" id="kobo.1575.1"><img alt="" role="presentation" src="image/B16591_05_28(a).jpg"/></span>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer111">
<span class="koboSpan" id="kobo.1576.1"><img alt="" role="presentation" src="image/B16591_05_28(b).jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1577.1">Figure 5.28 – Binary segmentation masks</span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1578.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.1579.1">The person's image has been taken as an example here from the following source: </span><em class="italic"><span class="koboSpan" id="kobo.1580.1">azerbaijan_stockers</span></em><span class="koboSpan" id="kobo.1581.1"> on </span><span class="No-Break"><span class="koboSpan" id="kobo.1582.1">Freepik: </span></span><a href="https://www.freepik.com/free-photo/young-woman-crossing-road-using-phone_10705234.htm"><span class="No-Break"><span class="koboSpan" id="kobo.1583.1">https://www.freepik.com/free-photo/young-woman-crossing-road-using-phone_10705234.htm</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1584.1">#query=person%20in%20the%20street&amp;position=13</span></span></p>
<p><span class="koboSpan" id="kobo.1585.1">&amp;from_view=search&amp;track=ais&amp;uuid=c3458125-63b6-4899-96e5-df07c307fb46The </span><em class="italic"><span class="koboSpan" id="kobo.1586.1">masks</span></em><span class="koboSpan" id="kobo.1587.1"> shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1588.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1589.1">.28</span></em><span class="koboSpan" id="kobo.1590.1"> can be seen as one-hot embeddings of the classes, and</span><a id="_idIndexMarker532"/><span class="koboSpan" id="kobo.1591.1"> they can be combined by associating each class with a different number (its class index, for example) to form a </span><span class="No-Break"><span class="koboSpan" id="kobo.1592.1">new image:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer112">
<span class="koboSpan" id="kobo.1593.1"><img alt="Figure 5.29 – Semantic segmentation" src="image/B16591_05_29.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1594.1">Figure 5.29 – Semantic segmentation</span></p>
<p><span class="koboSpan" id="kobo.1595.1">The output of a</span><a id="_idIndexMarker533"/><span class="koboSpan" id="kobo.1596.1"> semantic segmentation model is, therefore, the different binary segmentation masks (see an example in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1597.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1598.1">.29</span></em><span class="koboSpan" id="kobo.1599.1">), and the number depends on the number of classes that the model has been trained on. </span><span class="koboSpan" id="kobo.1599.2">Therefore, for each image input to the model with the shape </span><em class="italic"><span class="koboSpan" id="kobo.1600.1">[H, W]</span></em><span class="koboSpan" id="kobo.1601.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.1602.1">H</span></em><span class="koboSpan" id="kobo.1603.1"> being the height and </span><em class="italic"><span class="koboSpan" id="kobo.1604.1">W</span></em><span class="koboSpan" id="kobo.1605.1"> being the </span><em class="italic"><span class="koboSpan" id="kobo.1606.1">width</span></em><span class="koboSpan" id="kobo.1607.1">), the output array will have a shape of </span><em class="italic"><span class="koboSpan" id="kobo.1608.1">[N, H, W]</span></em><span class="koboSpan" id="kobo.1609.1"> (with </span><em class="italic"><span class="koboSpan" id="kobo.1610.1">N</span></em><span class="koboSpan" id="kobo.1611.1"> being the number </span><span class="No-Break"><span class="koboSpan" id="kobo.1612.1">of classes).</span></span></p>
<h3><span class="koboSpan" id="kobo.1613.1">Evaluating segmentation models</span></h3>
<p><span class="koboSpan" id="kobo.1614.1">An intuitive</span><a id="_idIndexMarker534"/><span class="koboSpan" id="kobo.1615.1"> approach to </span><a id="_idIndexMarker535"/><span class="koboSpan" id="kobo.1616.1">evaluate models for a semantic segmentation task is to report the percentage of pixels that have been correctly classified. </span><span class="koboSpan" id="kobo.1616.2">This metric is commonly used and is </span><a id="_idIndexMarker536"/><span class="koboSpan" id="kobo.1617.1">known as </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1618.1">pixel accuracy</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1619.1">.</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1620.1">P</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1621.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1622.1">x</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1623.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1624.1">l</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1625.1">A</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1626.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1627.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1628.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1629.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1630.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1631.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1632.1">y</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1633.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1634.1"> </span></span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.1635.1">#</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1636.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1637.1">P</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1638.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.1639.1">#</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1640.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1641.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1642.1">  </span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1643.1">_</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1644.1">_________________</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.1645.1">  </span></span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.1646.1">#</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1647.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1648.1">P</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1649.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.1650.1">#</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1651.1">T</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1652.1">N</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1653.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.1654.1">#</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1655.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1656.1">P</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.1657.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.1658.1">#</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1659.1">F</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1660.1">N</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.1661.1"> </span></span></p>
<p><span class="koboSpan" id="kobo.1662.1">However, pixel accuracy has a problem; when objects to be segmented are small in comparison to the image, this metric emphasizes the large number of pixels that have been correctly classified as not being the object (the inactive detection). </span><span class="koboSpan" id="kobo.1662.2">For example, in a 1,000x1,000 image, we have a 100x100 object, and our model classifies the image as the background for all the pixels, with a pixel accuracy </span><span class="No-Break"><span class="koboSpan" id="kobo.1663.1">of 99%.</span></span></p>
<p><span class="koboSpan" id="kobo.1664.1">To fix these issues, we can</span><a id="_idIndexMarker537"/><span class="koboSpan" id="kobo.1665.1"> use another metric to evaluate semantic segmentation models, </span><strong class="bold"><span class="koboSpan" id="kobo.1666.1">mean Intersection over </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1667.1">Union</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1668.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1669.1">mIoU</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1670.1">).</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer113">
<span class="koboSpan" id="kobo.1671.1"><img alt="Figure 5.30 – IoU for segmentation masks" src="image/B16591_05_30.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1672.1">Figure 5.30 – IoU for segmentation masks</span></p>
<p><span class="koboSpan" id="kobo.1673.1">The </span><a id="_idIndexMarker538"/><span class="koboSpan" id="kobo.1674.1">computation of this metric is similar to the IoU we saw in the previous recipe for object detection, in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1675.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1676.1">.16</span></em><span class="koboSpan" id="kobo.1677.1">. </span><span class="koboSpan" id="kobo.1677.2">However, for object detection, the analysis was based on bounding </span><a id="_idIndexMarker539"/><span class="koboSpan" id="kobo.1678.1">boxes, whereas for semantic segmentation, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1679.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1680.1">.30</span></em><span class="koboSpan" id="kobo.1681.1">, it evaluates the number of pixels common (the intersection) between the target and the predicted masks, divided by the total number of pixels present across both masks (the union), and then its arithmetic mean is computed for all classes in </span><span class="No-Break"><span class="koboSpan" id="kobo.1682.1">the dataset.</span></span></p>
<h3><span class="koboSpan" id="kobo.1683.1">Comparing network architectures for semantic segmentation</span></h3>
<p><span class="koboSpan" id="kobo.1684.1">Semantic segmentation</span><a id="_idIndexMarker540"/><span class="koboSpan" id="kobo.1685.1"> models’ output is the different segmentation masks, which</span><a id="_idIndexMarker541"/><span class="koboSpan" id="kobo.1686.1"> are the same size as the image input. </span><span class="koboSpan" id="kobo.1686.2">To reach this objective, several architectures have been proposed, with the main difference with CNNs being that there are no fully connected layers. </span><span class="koboSpan" id="kobo.1686.3">Appropriately, this network architecture is named </span><strong class="bold"><span class="koboSpan" id="kobo.1687.1">Fully Connected Networks</span></strong><span class="koboSpan" id="kobo.1688.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1689.1">FCNs</span></strong><span class="koboSpan" id="kobo.1690.1">). </span><span class="koboSpan" id="kobo.1690.2">Several models </span><a id="_idIndexMarker542"/><span class="koboSpan" id="kobo.1691.1">evolved from this initial architecture and were state-of-the-art when they were </span><span class="No-Break"><span class="koboSpan" id="kobo.1692.1">first proposed:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1693.1">Encoder–decoder</span></strong><span class="koboSpan" id="kobo.1694.1">: The</span><a id="_idIndexMarker543"/><span class="koboSpan" id="kobo.1695.1"> computation of feature maps by the convolutional and max pooling layers of the CNNs can be seen as an encoder, as image information is encoded in a multidimensional entity (feature maps). </span><span class="koboSpan" id="kobo.1695.2">In this architecture, after the CNN feature maps, a series of upsampling layers (the decoder) are cascaded until images of the same size are computed. </span><strong class="bold"><span class="koboSpan" id="kobo.1696.1">U-Net</span></strong><span class="koboSpan" id="kobo.1697.1"> is an</span><a id="_idIndexMarker544"/><span class="koboSpan" id="kobo.1698.1"> example of </span><span class="No-Break"><span class="koboSpan" id="kobo.1699.1">this architecture.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1700.1">Spatial pyramid pooling</span></strong><span class="koboSpan" id="kobo.1701.1">: One</span><a id="_idIndexMarker545"/><span class="koboSpan" id="kobo.1702.1"> problem with FCNs is that the encoder does not provide enough global scene cues to the downstream layers, resulting in objects being misclassified due to a lack of global context (for example, boats labeled as cars in water-based images, where boats are expected and not cars). </span><span class="koboSpan" id="kobo.1702.2">In this type of architecture, the feature map is aggregated to the output, along with feature maps at different grid scales computed by different modules. </span><strong class="bold"><span class="koboSpan" id="kobo.1703.1">PSPNet</span></strong><span class="koboSpan" id="kobo.1704.1"> is an </span><a id="_idIndexMarker546"/><span class="koboSpan" id="kobo.1705.1">example of </span><span class="No-Break"><span class="koboSpan" id="kobo.1706.1">this architecture.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1707.1">Context modules</span></strong><span class="koboSpan" id="kobo.1708.1">: Another </span><a id="_idIndexMarker547"/><span class="koboSpan" id="kobo.1709.1">option to capture multi-scale information is to add extra modules cascaded on top of the original network. </span><strong class="bold"><span class="koboSpan" id="kobo.1710.1">DeepLab-v3</span></strong><span class="koboSpan" id="kobo.1711.1"> can be seen as a</span><a id="_idIndexMarker548"/><span class="koboSpan" id="kobo.1712.1"> combination of this type of network and spatial pyramid </span><span class="No-Break"><span class="koboSpan" id="kobo.1713.1">pooling networks.</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer114">
<span class="koboSpan" id="kobo.1714.1"><img alt="Figure 5.31 – Network architecture for semantic segmentation" src="image/B16591_05_31.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1715.1">Figure 5.31 – Network architecture for semantic segmentation</span></p>
<p><span class="koboSpan" id="kobo.1716.1">For our </span><a id="_idIndexMarker549"/><span class="koboSpan" id="kobo.1717.1">experiments </span><a id="_idIndexMarker550"/><span class="koboSpan" id="kobo.1718.1">in this recipe, we will use pre-trained versions of PSPNet </span><span class="No-Break"><span class="koboSpan" id="kobo.1719.1">and DeepLab-v3.</span></span></p>
<h3><span class="koboSpan" id="kobo.1720.1">Exploring the Penn-Fudan Pedestrians dataset with segmentation ground-truth</span></h3>
<p><span class="koboSpan" id="kobo.1721.1">This </span><a id="_idIndexMarker551"/><span class="koboSpan" id="kobo.1722.1">dataset is the one that we worked with in the previous recipe. </span><span class="koboSpan" id="kobo.1722.2">However, the ground-truth we will use in this recipe is not the bounding boxes required for object detection but, instead, the masks required for </span><span class="No-Break"><span class="koboSpan" id="kobo.1723.1">semantic </span></span><span class="No-Break"><a id="_idIndexMarker552"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1724.1">segmentation.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer115">
<span class="koboSpan" id="kobo.1725.1"><img alt="Figure 5.32 – The Penn-Fudan Pedestrians dataset with mask ground-truth" src="image/B16591_05_32.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1726.1">Figure 5.32 – The Penn-Fudan Pedestrians dataset with mask ground-truth</span></p>
<p><span class="koboSpan" id="kobo.1727.1">As we </span><a id="_idIndexMarker553"/><span class="koboSpan" id="kobo.1728.1">can see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1729.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1730.1">.32</span></em><span class="koboSpan" id="kobo.1731.1">, each image can have one or more pedestrians, with their corresponding masks. </span><span class="koboSpan" id="kobo.1731.2">This figure was computed using the GluonCV visualization utils package (the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1732.1">plot_mask</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1733.1"> function).</span></span></p>
<p><span class="koboSpan" id="kobo.1734.1">In this dataset, there are no different classes to be classified, and no further visualizations </span><span class="No-Break"><span class="koboSpan" id="kobo.1735.1">are computed.</span></span></p>
<h3><span class="koboSpan" id="kobo.1736.1">Introducing Semantic Segmentation Model Zoo</span></h3>
<p><span class="koboSpan" id="kobo.1737.1">GluonCV also </span><a id="_idIndexMarker554"/><span class="koboSpan" id="kobo.1738.1">provides </span><a id="_idIndexMarker555"/><span class="koboSpan" id="kobo.1739.1">pre-trained models for semantic segmentation in its Model Zoo. </span><span class="koboSpan" id="kobo.1739.2">For the </span><em class="italic"><span class="koboSpan" id="kobo.1740.1">MS COCO</span></em><span class="koboSpan" id="kobo.1741.1"> dataset, this is the accuracy (mIoU) versus performance (samples per </span><span class="No-Break"><span class="koboSpan" id="kobo.1742.1">second) chart:</span></span></p>
<p class="IMG---Figure"> </p>
<div>
<div class="IMG---Figure" id="_idContainer116">
<span class="koboSpan" id="kobo.1743.1"><img alt="Figure 5.33 – Model Zoo for semantic segmentation (MS COCO)" src="image/B16591_05_33.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1744.1">Figure 5.33 – Model Zoo for semantic segmentation (MS COCO)</span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1745.1">Note</span></p>
<p class="callout"><span class="No-Break"><span class="koboSpan" id="kobo.1746.1">Source: </span></span><a href="https://cv.gluon.ai/model_zoo/detection.html"><span class="No-Break"><span class="koboSpan" id="kobo.1747.1">https://cv.gluon.ai/model_zoo/detection.html</span></span></a></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1748.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1749.1">.33</span></em><span class="koboSpan" id="kobo.1750.1"> displays</span><a id="_idIndexMarker556"/><span class="koboSpan" id="kobo.1751.1"> the most</span><a id="_idIndexMarker557"/><span class="koboSpan" id="kobo.1752.1"> important pre-trained models in GluonCV Model Zoo, comparing accuracy (mIoU on the vertical axis) and inference performance (samples per second on the horizontal axis). </span><span class="koboSpan" id="kobo.1752.2">There are no models (yet) in the top-right quadrant, meaning that, currently, we need to balance between </span><span class="No-Break"><span class="koboSpan" id="kobo.1753.1">both characteristics.</span></span></p>
<p><span class="koboSpan" id="kobo.1754.1">Using models from GluonCV Model Zoo can be done with just a couple of lines of code, and we will explore this path to solve our </span><em class="italic"><span class="koboSpan" id="kobo.1755.1">Penn-Fudan Pedestrians</span></em><span class="koboSpan" id="kobo.1756.1"> dataset for the segmentation task in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1757.1">following steps.</span></span></p>
<h3><span class="koboSpan" id="kobo.1758.1">Loading PSPNet pre-trained model from Model Zoo</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.1759.1">Pyramid Scene Parsing Network</span></strong><span class="koboSpan" id="kobo.1760.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1761.1">PSPNet</span></strong><span class="koboSpan" id="kobo.1762.1">) is a </span><a id="_idIndexMarker558"/><span class="koboSpan" id="kobo.1763.1">spatial pyramid pooling semantic segmentation</span><a id="_idIndexMarker559"/><span class="koboSpan" id="kobo.1764.1"> architecture, which</span><a id="_idIndexMarker560"/><span class="koboSpan" id="kobo.1765.1"> means that a pyramid pooling module is added to provide global cues. </span><span class="koboSpan" id="kobo.1765.2">It was developed by Zhao et al. </span><span class="koboSpan" id="kobo.1765.3">(the Chinese University of Hong Kong) in 2016. </span><span class="koboSpan" id="kobo.1765.4">It achieved first place in </span><a id="_idIndexMarker561"/><span class="koboSpan" id="kobo.1766.1">the 2016 </span><strong class="bold"><span class="koboSpan" id="kobo.1767.1">ILSVRC Scene </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1768.1">Parsing Challenge</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1769.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1770.1">Apart from the global pooling module, it differentiates from FCNs by using </span><span class="No-Break"><span class="koboSpan" id="kobo.1771.1">dilated convolutions.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer117">
<span class="koboSpan" id="kobo.1772.1"><img alt="Figure 5.34 – The PSPNet architecture" src="image/B16591_05_34.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1773.1">Figure 5.34 – The PSPNet architecture</span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.1774.1">Note</span></p>
<p class="callout"><span class="No-Break"><span class="koboSpan" id="kobo.1775.1">Source: </span></span><a href="https://github.com/hszhao/PSPNet/issues/101"><span class="No-Break"><span class="koboSpan" id="kobo.1776.1">https://github.com/hszhao/PSPNet/issues/101</span></span></a></p>
<p><span class="koboSpan" id="kobo.1777.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1778.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1779.1">.34</span></em><span class="koboSpan" id="kobo.1780.1">, we can see the overall architecture </span><span class="No-Break"><span class="koboSpan" id="kobo.1781.1">of PSPNet:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1782.1">Feature Map</span></strong><span class="koboSpan" id="kobo.1783.1">: A ResNet-based CNN architecture, with dilated convolutions, is used to compute a feature map with 1/8 the size of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1784.1">original image.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1785.1">Pyramid Pooling Module</span></strong><span class="koboSpan" id="kobo.1786.1">: Using a four-level pyramid (whole, half, quarter, and eighth), global context is computed. </span><span class="koboSpan" id="kobo.1786.2">It is concatenated with the original </span><span class="No-Break"><span class="koboSpan" id="kobo.1787.1">feature map.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1788.1">Final Prediction</span></strong><span class="koboSpan" id="kobo.1789.1">: A final computation using a convolutional layer is done to generate the </span><span class="No-Break"><span class="koboSpan" id="kobo.1790.1">final predictions.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1791.1">For our </span><a id="_idIndexMarker562"/><span class="koboSpan" id="kobo.1792.1">experiments, we</span><a id="_idIndexMarker563"/><span class="koboSpan" id="kobo.1793.1"> will use a ResNet-101 network as a backbone. </span><span class="koboSpan" id="kobo.1793.2">We can load the model with a single line </span><span class="No-Break"><span class="koboSpan" id="kobo.1794.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1795.1">
pspnet = gcv.model_zoo.get_model('psp_resnet101_coco', pretrained=True, ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.1796.1">The model then </span><span class="No-Break"><span class="koboSpan" id="kobo.1797.1">downloads successfully.</span></span></p>
<h3><span class="koboSpan" id="kobo.1798.1">Evaluating a PSPNet pre-trained model from Model Zoo</span></h3>
<p><span class="koboSpan" id="kobo.1799.1">By </span><a id="_idIndexMarker564"/><span class="koboSpan" id="kobo.1800.1">using the </span><em class="italic"><span class="koboSpan" id="kobo.1801.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.1802.1"> dataset for a segmentation task, we can now perform</span><a id="_idIndexMarker565"/><span class="koboSpan" id="kobo.1803.1"> qualitative and quantitative evaluation of the loaded model from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1804.1">previous section.</span></span></p>
<p><span class="koboSpan" id="kobo.1805.1">Qualitatively, we can choose an image from the dataset and compare the output of the model with the ground-truth output from </span><span class="No-Break"><span class="koboSpan" id="kobo.1806.1">the dataset:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer118">
<span class="koboSpan" id="kobo.1807.1"><img alt="Figure 5.35 – Comparing predicted masks and ground-truth for PSPNet" src="image/B16591_05_35.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1808.1">Figure 5.35 – Comparing predicted masks and ground-truth for PSPNet</span></p>
<p><span class="koboSpan" id="kobo.1809.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1810.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1811.1">.35</span></em><span class="koboSpan" id="kobo.1812.1">, we can see a very strong correlation between the predicted segmentation masks and the expected results from the ground-truth. </span><span class="koboSpan" id="kobo.1812.2">This figure was computed </span><a id="_idIndexMarker566"/><span class="koboSpan" id="kobo.1813.1">using the GluonCV visualization utils package (the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1814.1">plot_mask</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1815.1"> function).</span></span></p>
<p><span class="koboSpan" id="kobo.1816.1">Quantitatively, we can perform an mIoU evaluation and the runtime spent computing </span><span class="No-Break"><span class="koboSpan" id="kobo.1817.1">this metric:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1818.1">
PixAcc:  0.4650485574278924
mIoU  :  0.5612896701751177
Elapsed Time:  341.7681269645691 secs</span></pre> <p><span class="koboSpan" id="kobo.1819.1">The </span><a id="_idIndexMarker567"/><span class="koboSpan" id="kobo.1820.1">computed mIoU for this model for </span><em class="italic"><span class="koboSpan" id="kobo.1821.1">MS COCO</span></em><span class="koboSpan" id="kobo.1822.1"> (see </span><em class="italic"><span class="koboSpan" id="kobo.1823.1">Semantic Segmentation Model Zoo</span></em><span class="koboSpan" id="kobo.1824.1">) is </span><strong class="source-inline"><span class="koboSpan" id="kobo.1825.1">0.70</span></strong><span class="koboSpan" id="kobo.1826.1">; therefore, given the value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1827.1">0.56</span></strong><span class="koboSpan" id="kobo.1828.1"> for our model, we can conclude that our model performs the task accurately. </span><span class="koboSpan" id="kobo.1828.2">However, it did take some time to complete it (~</span><span class="No-Break"><span class="koboSpan" id="kobo.1829.1">340 seconds).</span></span></p>
<h3><span class="koboSpan" id="kobo.1830.1">Loading a DeepLab-v3 pre-trained model from Model Zoo</span></h3>
<p><strong class="bold"><span class="koboSpan" id="kobo.1831.1">DeepLab-v3</span></strong><span class="koboSpan" id="kobo.1832.1"> is </span><a id="_idIndexMarker568"/><span class="koboSpan" id="kobo.1833.1">a </span><a id="_idIndexMarker569"/><span class="koboSpan" id="kobo.1834.1">semantic segmentation architecture that combines</span><a id="_idIndexMarker570"/><span class="koboSpan" id="kobo.1835.1"> both </span><em class="italic"><span class="koboSpan" id="kobo.1836.1">encoder-decoder</span></em><span class="koboSpan" id="kobo.1837.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1838.1">spatial pyramid pooling</span></strong><span class="koboSpan" id="kobo.1839.1"> architectures. </span><span class="koboSpan" id="kobo.1839.2">It was developed by Chen et al. </span><span class="koboSpan" id="kobo.1839.3">(Google) from 2015 (</span><strong class="bold"><span class="koboSpan" id="kobo.1840.1">DeepLab-v1</span></strong><span class="koboSpan" id="kobo.1841.1">) to </span><span class="No-Break"><span class="koboSpan" id="kobo.1842.1">2017 (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1843.1">DeepLab-v3</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1844.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.1845.1">It is the </span><a id="_idIndexMarker571"/><span class="koboSpan" id="kobo.1846.1">third iteration of a series of architectures that have evolved from each other – DeepLab-v1, DeepLab-v2, </span><span class="No-Break"><span class="koboSpan" id="kobo.1847.1">and DeepLab-v3.</span></span></p>
<p><span class="koboSpan" id="kobo.1848.1">From the research papers, also included in the There’s more section we can see the </span><span class="No-Break"><span class="koboSpan" id="kobo.1849.1">DeepLab-v1 architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer119">
<span class="koboSpan" id="kobo.1850.1"><img alt="" role="presentation" src="image/B16591_05_36.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1851.1">Figure 5.36 – The architecture of DeepLab-v1</span></p>
<p><span class="koboSpan" id="kobo.1852.1">Secondly, we can</span><a id="_idIndexMarker572"/><span class="koboSpan" id="kobo.1853.1"> see the </span><a id="_idIndexMarker573"/><span class="koboSpan" id="kobo.1854.1">DeepLab- </span><span class="No-Break"><span class="koboSpan" id="kobo.1855.1">v2 architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer120">
<span class="koboSpan" id="kobo.1856.1"><img alt="" role="presentation" src="image/B16591_05_37.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1857.1">Figure 5.37 – The architecture of DeepLab-v2</span></p>
<p><span class="koboSpan" id="kobo.1858.1">And </span><a id="_idIndexMarker574"/><span class="koboSpan" id="kobo.1859.1">finally, we</span><a id="_idIndexMarker575"/><span class="koboSpan" id="kobo.1860.1"> can see the</span><a id="_idIndexMarker576"/> <span class="No-Break"><span class="koboSpan" id="kobo.1861.1">DeepLab-v3+ architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer121">
<span class="koboSpan" id="kobo.1862.1"><img alt="" role="presentation" src="image/B16591_05_38.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1863.1">Figure 5.38 – The architecture of DeepLab-v3+</span></p>
<p><span class="koboSpan" id="kobo.1864.1">The </span><a id="_idIndexMarker577"/><span class="koboSpan" id="kobo.1865.1">different architectures have similarities and differences, namely </span><span class="No-Break"><span class="koboSpan" id="kobo.1866.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1867.1">DeepLab-v1</span></strong><span class="koboSpan" id="kobo.1868.1">: This</span><a id="_idIndexMarker578"/><span class="koboSpan" id="kobo.1869.1"> architecture evolves from the original FCN and uses a VGG-16 backbone network as well. </span><span class="koboSpan" id="kobo.1869.2">The most important innovation is the usage of atrous or diluted convolutions instead of standard convolutions. </span><span class="koboSpan" id="kobo.1869.3">We discussed this convolution parameter when we first introduced </span><em class="italic"><span class="koboSpan" id="kobo.1870.1">convolutional layers</span></em><span class="koboSpan" id="kobo.1871.1"> in </span><a href="B16591_03.xhtml#_idTextAnchor052"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1872.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.1873.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1874.1">Solving Regression Problems</span></em><span class="koboSpan" id="kobo.1875.1">, showing how it increased the receptive field. </span><span class="koboSpan" id="kobo.1875.2">This architecture also uses fully </span><a id="_idIndexMarker579"/><span class="koboSpan" id="kobo.1876.1">connected </span><strong class="bold"><span class="koboSpan" id="kobo.1877.1">Conditional Random Fields</span></strong><span class="koboSpan" id="kobo.1878.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1879.1">CRFs</span></strong><span class="koboSpan" id="kobo.1880.1">) for post-processing to polish the final segmentation masks, although it is very slow and cannot be trained end </span><span class="No-Break"><span class="koboSpan" id="kobo.1881.1">to end.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1882.1">DeepLab-v2</span></strong><span class="koboSpan" id="kobo.1883.1">: The most</span><a id="_idIndexMarker580"/><span class="koboSpan" id="kobo.1884.1"> important innovation for this model is a new </span><a id="_idIndexMarker581"/><span class="koboSpan" id="kobo.1885.1">module called </span><strong class="bold"><span class="koboSpan" id="kobo.1886.1">Atrous Spatial Pyramid Pooling</span></strong><span class="koboSpan" id="kobo.1887.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1888.1">ASPP</span></strong><span class="koboSpan" id="kobo.1889.1">). </span><span class="koboSpan" id="kobo.1889.2">With this module, on one hand, the network can encode multi-scale features into a fixed-size feature map (which is flexible to different input sizes), and on the ther hand, by using atrous convolutions, it increases the receptive field, optimizing the computation cost. </span><span class="koboSpan" id="kobo.1889.3">In the original implementation, 4 to 6 scales were used. </span><span class="koboSpan" id="kobo.1889.4">Furthermore, it uses ResNet as the </span><span class="No-Break"><span class="koboSpan" id="kobo.1890.1">backbone network.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1891.1">DeepLab-v3</span></strong><span class="koboSpan" id="kobo.1892.1">: In </span><a id="_idIndexMarker582"/><span class="koboSpan" id="kobo.1893.1">this last iteration, several changes are introduced. </span><span class="koboSpan" id="kobo.1893.2">Firstly, the network is modified to use batch normalization and dropout. </span><span class="koboSpan" id="kobo.1893.3">Secondly, the ASPP module is modified to add a new scale in a separate channel for global image pooling, to use fine-grained details. </span><span class="koboSpan" id="kobo.1893.4">Lastly, the multi-scale part of the training is removed and is only applied to inference. </span><span class="koboSpan" id="kobo.1893.5">A by-product of these small improvements is that the CRF step is no longer needed, providing much </span><span class="No-Break"><span class="koboSpan" id="kobo.1894.1">faster results.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1895.1">For our </span><a id="_idIndexMarker583"/><span class="koboSpan" id="kobo.1896.1">experiments, we</span><a id="_idIndexMarker584"/><span class="koboSpan" id="kobo.1897.1"> will use a ResNet-152 network as a backbone. </span><span class="koboSpan" id="kobo.1897.2">We can load the model with a single line </span><span class="No-Break"><span class="koboSpan" id="kobo.1898.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1899.1">
deeplab = gcv.model_zoo.get_model('deeplab_resnet152_coco', pretrained=True, ctx=ctx)</span></pre> <p><span class="koboSpan" id="kobo.1900.1">The model then </span><span class="No-Break"><span class="koboSpan" id="kobo.1901.1">downloads successfully.</span></span></p>
<h3><span class="koboSpan" id="kobo.1902.1">Evaluating a DeepLab-v3 pre-trained model from Model Zoo</span></h3>
<p><span class="koboSpan" id="kobo.1903.1">Using </span><a id="_idIndexMarker585"/><span class="koboSpan" id="kobo.1904.1">the </span><em class="italic"><span class="koboSpan" id="kobo.1905.1">Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.1906.1"> dataset for the segmentation task, we can now perform</span><a id="_idIndexMarker586"/><span class="koboSpan" id="kobo.1907.1"> qualitative and quantitative evaluation of the loaded model from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1908.1">previous section.</span></span></p>
<p><span class="koboSpan" id="kobo.1909.1">Qualitatively, we can choose an image from the dataset and compare the output of the model with the ground-truth output from </span><span class="No-Break"><span class="koboSpan" id="kobo.1910.1">the dataset:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer122">
<span class="koboSpan" id="kobo.1911.1"><img alt="" role="presentation" src="image/B16591_05_39.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1912.1">Figure 5.39 – Comparing predicted masks and ground-truth for DeepLab-v3</span></p>
<p><span class="koboSpan" id="kobo.1913.1">From </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1914.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1915.1">.39</span></em><span class="koboSpan" id="kobo.1916.1">, we </span><a id="_idIndexMarker587"/><span class="koboSpan" id="kobo.1917.1">can see a very strong correlation between the predicted segmentation masks and the expected results from the ground-truth. </span><span class="koboSpan" id="kobo.1917.2">This figure was computed using the GluonCV visualization utils package (the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1918.1">plot_mask</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1919.1"> function).</span></span></p>
<p><span class="koboSpan" id="kobo.1920.1">Quantitatively, we</span><a id="_idIndexMarker588"/><span class="koboSpan" id="kobo.1921.1"> can perform mIoU evaluation and the runtime spent computing </span><span class="No-Break"><span class="koboSpan" id="kobo.1922.1">this metric:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1923.1">
PixAcc:  0.4653841191754281
mIoU  :  0.5616023247999165
Elapsed Time:  74.66736197471619 secs</span></pre> <p><span class="koboSpan" id="kobo.1924.1">The computed mIoU for this model for </span><em class="italic"><span class="koboSpan" id="kobo.1925.1">MS COCO</span></em><span class="koboSpan" id="kobo.1926.1"> (see </span><em class="italic"><span class="koboSpan" id="kobo.1927.1">Semantic Segmentation Model Zoo</span></em><span class="koboSpan" id="kobo.1928.1">) is </span><strong class="source-inline"><span class="koboSpan" id="kobo.1929.1">0.715</span></strong><span class="koboSpan" id="kobo.1930.1">; therefore, given the value of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1931.1">0.56</span></strong><span class="koboSpan" id="kobo.1932.1"> for our model, we can conclude our model performs the task accurately, with a similar value to PSPNet. </span><span class="koboSpan" id="kobo.1932.2">However, it is much faster (~</span><span class="No-Break"><span class="koboSpan" id="kobo.1933.1">70 secs).</span></span></p>
<h2 id="_idParaDest-118"><a id="_idTextAnchor119"/><span class="koboSpan" id="kobo.1934.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.1935.1">In this recipe, we tackled the </span><a id="_idIndexMarker589"/><span class="koboSpan" id="kobo.1936.1">semantic segmentation problem. </span><span class="koboSpan" id="kobo.1936.2">We analyzed the differences between image classification and object detection for evaluation, network architectures, </span><span class="No-Break"><span class="koboSpan" id="kobo.1937.1">and datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.1938.1">In our examples, we used a publicly available dataset, the </span><em class="italic"><span class="koboSpan" id="kobo.1939.1">Penn-Fudan Pedestrians</span></em><span class="koboSpan" id="kobo.1940.1"> dataset, and we used pre-trained models on </span><em class="italic"><span class="koboSpan" id="kobo.1941.1">MS COCO</span></em><span class="koboSpan" id="kobo.1942.1">. </span><span class="koboSpan" id="kobo.1942.2">This choice was not casual; </span><em class="italic"><span class="koboSpan" id="kobo.1943.1">MS COCO</span></em><span class="koboSpan" id="kobo.1944.1"> has a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1945.1">person</span></strong><span class="koboSpan" id="kobo.1946.1"> class, and therefore, the expectation was that these pre-trained models would perform well, as they had already seen images from the </span><span class="No-Break"><span class="koboSpan" id="kobo.1947.1">dataset class.</span></span></p>
<p><span class="koboSpan" id="kobo.1948.1">However, as </span><a id="_idIndexMarker590"/><span class="koboSpan" id="kobo.1949.1">mentioned in the previous recipe, when this is not possible, and we apply pre-trained models from a dataset to another dataset, the data probability distribution will typically be very different; hence, the accuracy obtained can be very low. </span><span class="koboSpan" id="kobo.1949.2">This is known as the </span><strong class="bold"><span class="koboSpan" id="kobo.1950.1">domain gap</span></strong><span class="koboSpan" id="kobo.1951.1"> or </span><strong class="bold"><span class="koboSpan" id="kobo.1952.1">domain adaptation problem</span></strong><span class="koboSpan" id="kobo.1953.1"> between the source dataset (the images that the model has been pre-trained on) and the target dataset (the images that the model is </span><span class="No-Break"><span class="koboSpan" id="kobo.1954.1">evaluated on).</span></span></p>
<p><span class="koboSpan" id="kobo.1955.1">One way to tackle these issues for supervised learning problems is fine-tuning. </span><span class="koboSpan" id="kobo.1955.2">This approach is explored in detail in a </span><span class="No-Break"><span class="koboSpan" id="kobo.1956.1">later chapter.</span></span></p>
<p><span class="koboSpan" id="kobo.1957.1">We ended the recipe by evaluating our two pre-trained models, PSPNet and DeepLab-v3, and we were able to compare qualitatively and quantitatively their results for accuracy and computation speed, verifying how both models yield a similar pixel accuracy (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1958.1">0.46)</span></strong><span class="koboSpan" id="kobo.1959.1"> and mIoU (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1960.1">0.56</span></strong><span class="koboSpan" id="kobo.1961.1">), although DeepLab-v3 was </span><span class="No-Break"><span class="koboSpan" id="kobo.1962.1">faster (</span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1963.1">~4.5x</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1964.1">).</span></span></p>
<h2 id="_idParaDest-119"><a id="_idTextAnchor120"/><span class="koboSpan" id="kobo.1965.1">There’s more...</span></h2>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1966.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.1967.1">.33</span></em><span class="koboSpan" id="kobo.1968.1"> showed a static image corresponding to the mIoU versus samples per second graph for the Model Zoo for Semantic Segmentation (on MS COCO); there is a dynamic version at this link that is worth taking a look at: https://cv.gluon.ai/model_zoo/segmentation.html. </span><span class="koboSpan" id="kobo.1968.2">On this page, results from different models available in GluonCV Model Zoo are included; I suggest that you reproduce these results, as it is an </span><span class="No-Break"><span class="koboSpan" id="kobo.1969.1">interesting exercise.</span></span></p>
<p><span class="koboSpan" id="kobo.1970.1">During our discussion of the evolution of network architectures, most notably DeepLab, we mentioned how dilated/atrous convolutions help with multi-scale context aggregation. </span><span class="koboSpan" id="kobo.1970.2">This research paper explores this topic in </span><span class="No-Break"><span class="koboSpan" id="kobo.1971.1">depth: </span></span><a href="https://arxiv.org/pdf/1511.07122.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1972.1">https://arxiv.org/pdf/1511.07122.pdf</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1973.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.1974.1">Furthermore, it is very interesting to read the original research papers and see how the segmentation task has evolved from an academic point </span><span class="No-Break"><span class="koboSpan" id="kobo.1975.1">of view:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1976.1">FCN</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1977.1">: </span></span><a href="https://arxiv.org/pdf/1605.06211v1.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1978.1">https://arxiv.org/pdf/1605.06211v1.pdf</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1979.1">U-Net</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1980.1">: </span></span><a href="https://arxiv.org/pdf/1505.04597.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1981.1">https://arxiv.org/pdf/1505.04597.pdf</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1982.1">PSPNet</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1983.1">: </span></span><a href="https://arxiv.org/pdf/1612.01105.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1984.1">https://arxiv.org/pdf/1612.01105.pdf</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1985.1">DeepLab-v1</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1986.1">: </span></span><a href="https://arxiv.org/pdf/1412.7062.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1987.1">https://arxiv.org/pdf/1412.7062.pdf</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1988.1">DeepLab-v2</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1989.1">: </span></span><a href="https://arxiv.org/pdf/1606.00915v2.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1990.1">https://arxiv.org/pdf/1606.00915v2.pdf</span></span></a></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1991.1">DeepLab-v3</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1992.1">: </span></span><a href="https://arxiv.org/pdf/1706.05587.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.1993.1">https://arxiv.org/pdf/1706.05587.pdf</span></span></a></li>
</ul>
<p><span class="koboSpan" id="kobo.1994.1">Semantic segmentation</span><a id="_idIndexMarker591"/><span class="koboSpan" id="kobo.1995.1"> is an active area of research and, as such, is constantly evolving, with new networks appearing and redefining the state of the art, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.1996.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1997.1">DeepLab-v3+</span></strong><span class="koboSpan" id="kobo.1998.1">: The next step from DeepLab-v3, developed by Chen et al. </span><span class="koboSpan" id="kobo.1998.2">(Google, </span><span class="No-Break"><span class="koboSpan" id="kobo.1999.1">2018): </span></span><a href="https://arxiv.org/pdf/1802.02611.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.2000.1">https://arxiv.org/pdf/1802.02611.pdf</span></span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.2001.1">Swin-V2G</span></strong><span class="koboSpan" id="kobo.2002.1">: Using Transformers, developed by Liu et al. </span><span class="koboSpan" id="kobo.2002.2">(Microsoft, </span><span class="No-Break"><span class="koboSpan" id="kobo.2003.1">2021): </span></span><a href="https://arxiv.org/pdf/2111.09883v1.pdf"><span class="No-Break"><span class="koboSpan" id="kobo.2004.1">https://arxiv.org/pdf/2111.09883v1.pdf</span></span></a></li>
</ul>
</div>
</body></html>