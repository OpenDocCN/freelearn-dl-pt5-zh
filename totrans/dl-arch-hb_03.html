<html><head></head><body>
<div id="_idContainer050">
<h1 class="chapter-number" id="_idParaDest-52"><a id="_idTextAnchor051"/><span class="koboSpan" id="kobo.1.1">3</span></h1>
<h1 id="_idParaDest-53"><a id="_idTextAnchor052"/><span class="koboSpan" id="kobo.2.1">Understanding Convolutional Neural Networks</span></h1>
<p><span class="koboSpan" id="kobo.3.1">An MLP is structured to accept one-dimensional data and cannot directly work with two-dimensional data or higher-dimensional data without preprocessing. </span><span class="koboSpan" id="kobo.3.2">One-dimensional data is also called tabular data, which commonly includes categorical data, numerical data, and maybe text data. </span><span class="koboSpan" id="kobo.3.3">Two-dimensional data, or data with higher dimensions, is some form of image data. </span><span class="koboSpan" id="kobo.3.4">Image data can be in two-dimensional format when it is a grayscale formatted image, in three-dimensional format when it has RGB layers that closely represent what humans see, or in more than three dimensions with hyperspectral images. </span><span class="koboSpan" id="kobo.3.5">Usually, to make MLP work for images, you would have to flatten the data and effectively represent the same data in a one-dimensional format. </span><span class="koboSpan" id="kobo.3.6">Flattening the data might work well in some cases, but throwing away the spatial characteristics that define that image removes the potential of capturing that relationship to your target. </span><span class="koboSpan" id="kobo.3.7">Additionally, flattening the data doesn’t scale properly to large images. </span><span class="koboSpan" id="kobo.3.8">An important characteristic of image data is that the target to be identified can be present in any spatial position of the image. </span><span class="koboSpan" id="kobo.3.9">Simple MLPs are highly dependent on the position of the data input and won’t be able to adapt to the ever-changing positions and orientation of the target in </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">an image.</span></span></p>
<p><span class="koboSpan" id="kobo.5.1">This is where the </span><strong class="bold"><span class="koboSpan" id="kobo.6.1">convolutional neural network</span></strong><span class="koboSpan" id="kobo.7.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.8.1">CNN</span></strong><span class="koboSpan" id="kobo.9.1">) layer shines and is often the go-to method for experts dealing with image data using machine learning. </span><span class="koboSpan" id="kobo.9.2">To date, top convolutional architectures always exceed the performance of MLP architectures for image datasets. </span><span class="koboSpan" id="kobo.9.3">In this chapter, we will cover the following topics while focusing </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">on CNNs:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.11.1">Understanding the convolutional neural </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">network layer</span></span></li>
<li><span class="koboSpan" id="kobo.13.1">Understanding the </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">pooling layer</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Building a </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">CNN architecture</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Designing a CNN architecture for </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">practical usage</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Exploring the CNN </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">architecture families</span></span></li>
</ul>
<h1 id="_idParaDest-54"><a id="_idTextAnchor053"/><span class="koboSpan" id="kobo.21.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.22.1">This chapter includes some practical implementations in the </span><strong class="bold"><span class="koboSpan" id="kobo.23.1">Python</span></strong><span class="koboSpan" id="kobo.24.1"> programming language. </span><span class="koboSpan" id="kobo.24.2">To complete it, you will need to have a computer with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">libraries installed:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.26.1">pandas</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.27.1">matplotlib</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.28.1">seaborn</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.29.1">scikit-learn</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.30.1">numpy</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.31.1">keras</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.32.1">pytorch</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.33.1">The code files for this chapter are available on </span><span class="No-Break"><span class="koboSpan" id="kobo.34.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_2"><span class="No-Break"><span class="koboSpan" id="kobo.35.1">https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_2</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.36.1">.</span></span></p>
<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/><span class="koboSpan" id="kobo.37.1">Understanding the convolutional neural network layer</span></h1>
<p><span class="koboSpan" id="kobo.38.1">Now, let’s focus on the foundations of the convolutional layer, starting with </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.39.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.40.1">.1</span></em><span class="koboSpan" id="kobo.41.1">, which shows the operational process of a convolutional filter. </span><span class="koboSpan" id="kobo.41.2">A filter is a small matrix of weights that’s used to extract features or patterns from an input array. </span><span class="koboSpan" id="kobo.41.3">A convolutional filter is a type of filter that slides over an image, performing convolution operations to extract features by calculating </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">dot products:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<span class="koboSpan" id="kobo.43.1"><img alt="Figure 3.1 – Operation of a convolutional filter on an image of a ﻿t-shirt from the Fashion MNIST dataset" src="image/B18187_03_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.44.1">Figure 3.1 – Operation of a convolutional filter on an image of a t-shirt from the Fashion MNIST dataset</span></p>
<p><span class="koboSpan" id="kobo.45.1">Convolutional layers are made out of multiple convolutional filters of the same sizes. </span><span class="koboSpan" id="kobo.45.2">Convolutional filters are the main pattern detectors in a CNN, where each filter will learn to identify multidimensional patterns that exist in an image. </span><span class="koboSpan" id="kobo.45.3">The patterns can range from low-level patterns such as lines and edges to mid-level patterns such as circles or squares and finally to high-level patterns such as specific t-shirt types or shoe types depending on the deepness level of the convolutional layer in a CNN. </span><span class="koboSpan" id="kobo.45.4">In </span><a href="B18187_12.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.46.1">Chapter 12</span></em></span></a><span class="koboSpan" id="kobo.47.1">, </span><em class="italic"><span class="koboSpan" id="kobo.48.1">Interpreting Neural Networks</span></em><span class="koboSpan" id="kobo.49.1">, we will explore the patterns that are learned and evaluate the patterns qualitatively </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">and objectively.</span></span></p>
<p><span class="koboSpan" id="kobo.51.1">Convolutional filters can be in multiple dimensions but for plain images, two-dimensional convolution filters are more commonly used. </span><span class="koboSpan" id="kobo.51.2">Kernels are analogous to filters in terms of a CNN layer. </span><span class="koboSpan" id="kobo.51.3">These two-dimensional filters have the same number of weights as their size and apply a dot product on a part of the input image data with the same size to obtain a single value; subsequently, these are added with a bias term. </span><span class="koboSpan" id="kobo.51.4">By operating on the same operation using the filter in a sliding window manner systematically, from top to bottom and left to right with a defined stride, the convolutional filter will complete a forward pass and obtain a two-dimensional output with smaller dimensions. </span><span class="koboSpan" id="kobo.51.5">Here, stride means the number of pixel steps taken when sliding the filter systematically from left to right and top to bottom where the minimum has to be at least 1 pixel. </span><span class="koboSpan" id="kobo.51.6">This operation builds on the fact that the target can be present in any spatial position in an image and uses the same pattern identification method across the </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">entire image.</span></span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.53.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.54.1">.1</span></em><span class="koboSpan" id="kobo.55.1"> shows a two-dimensional convolutional filter with the size of 5x5 pixels, where there are 25 weight components and one bias component that could be learned, and the input image of a t-shirt, which has a size of 28x28 pixels. </span><span class="koboSpan" id="kobo.55.2">The filter size is something that can be configured to other values that typically range from 1 to 7 and is commonly a square but can be set to be irregular rectangular shapes. </span><span class="koboSpan" id="kobo.55.3">The typical filter size values might seem too small of a </span><strong class="bold"><span class="koboSpan" id="kobo.56.1">receptive field</span></strong><span class="koboSpan" id="kobo.57.1"> to identify high-level features that are capable of predicting a shirt in this picture, but when multiple filters are applied one after another in a non-cyclical manner, the filters at the end of the operation have the receptive field of a larger part of the image. </span><span class="koboSpan" id="kobo.57.2">Receptive field refers to the region of the input space that a convolutional filter can “see” or respond to. </span><span class="koboSpan" id="kobo.57.3">It determines the spatial extent of the input that influences a particular output unit. </span><span class="koboSpan" id="kobo.57.4">To capture low-level patterns, the filter must have a small receptive field, and to capture high-level patterns, the filter must have a large receptive field. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.58.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.59.1">.2</span></em><span class="koboSpan" id="kobo.60.1"> depicts this concept for three filters applied one </span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">after another:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<span class="koboSpan" id="kobo.62.1"><img alt="Figure 3.2 – The second convolutional filter has a receptive field size of 3x3 of the input data, even when it has only a 2x2 convolutional filter size" src="image/B18187_03_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.63.1">Figure 3.2 – The second convolutional filter has a receptive field size of 3x3 of the input data, even when it has only a 2x2 convolutional filter size</span></p>
<p><span class="koboSpan" id="kobo.64.1">Now, the sliding window from left to right and top to bottom is just a good way to visualize the process. </span><span class="koboSpan" id="kobo.64.2">However, in reality, this process can be done in parallel in one go to take advantage of GPU parallel processing. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.65.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.66.1">.3</span></em><span class="koboSpan" id="kobo.67.1"> shows all the window positions of a convolutional filter with a 4x4-pixel dimension with a stride of 4 applied on the same 28x28 t-shirt image. </span><span class="koboSpan" id="kobo.67.2">This would result in a 7x7 data </span><span class="No-Break"><span class="koboSpan" id="kobo.68.1">output size:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer031">
<span class="koboSpan" id="kobo.69.1"><img alt="Figure 3.3 – All window positions using a 4x4 convolutional filter on a 28x28 image with a stride of 4 pixels in both dimensions" src="image/B18187_03_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.70.1">Figure 3.3 – All window positions using a 4x4 convolutional filter on a 28x28 image with a stride of 4 pixels in both dimensions</span></p>
<p><span class="koboSpan" id="kobo.71.1">Consider a CNN layer with 16 filters with a size of 5x5 pixels with a stride of 1 pixel. </span><span class="koboSpan" id="kobo.71.2">Running a forward pass of the t-shirt image with this layer configuration will result in a total of 16x26x26 (depth x width x height) pixels of output. </span><span class="koboSpan" id="kobo.71.3">In the case where the image is an RGB image with three channels colored red, green, and blue, since the </span><a id="_idIndexMarker230"/><span class="koboSpan" id="kobo.72.1">convolutional filters are two-dimensional, the same filter would be applied similarly to each of the three channels using a standard convolutional layer. </span><span class="koboSpan" id="kobo.72.2">The three-dimensional outputs from the filters, applied separately on the three channels, will then be added up. </span><span class="koboSpan" id="kobo.72.3">The number of convolutional filters will be the output data channel size and will serve as the input channel size for the subsequent convolutional layers. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.73.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.74.1">.4</span></em><span class="koboSpan" id="kobo.75.1"> depicts this channel-wise addition process for a 3x3 output from a convolutional filter. </span><span class="koboSpan" id="kobo.75.2">Note that the bias is only added once per filter across the channels and not </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">by channel:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer032">
<span class="koboSpan" id="kobo.77.1"><img alt="Figure 3.4 – Aggregation of a multichannel 3x3 output from a convolution filter" src="image/B18187_03_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.78.1">Figure 3.4 – Aggregation of a multichannel 3x3 output from a convolution filter</span></p>
<p><span class="koboSpan" id="kobo.79.1">In the preceding figure, </span><strong class="bold"><span class="koboSpan" id="kobo.80.1">Conv</span></strong><span class="koboSpan" id="kobo.81.1"> is short for </span><em class="italic"><span class="koboSpan" id="kobo.82.1">convolutional layer</span></em><span class="koboSpan" id="kobo.83.1"> and will be a convention that will be used in the rest of this chapter to simplify figures. </span><span class="koboSpan" id="kobo.83.2">Since it is beneficial to be aware of the size of your neural network to ensure you have the computational resources to hold and process the network, let’s also compute the number of parameters </span><a id="_idIndexMarker231"/><span class="koboSpan" id="kobo.84.1">that will be held by this convolutional layer. </span><span class="koboSpan" id="kobo.84.2">The number of parameters can be computed </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">as follows:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.86.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.87.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.88.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.89.1">b</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.90.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.91.1">r</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.92.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.93.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.94.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.95.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.96.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.97.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.98.1">t</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.99.1">c</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.100.1">h</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.101.1">a</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.102.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.103.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.104.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.105.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.106.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.107.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.108.1">n</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.109.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.110.1">m</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.111.1">b</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.112.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.113.1">r</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.114.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.115.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.116.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.117.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.118.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.119.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.120.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.121.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.122.1">s</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.123.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.124.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.125.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.126.1">w</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.127.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.128.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.129.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.130.1">h</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.131.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.132.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.133.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.134.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.135.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.136.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.137.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.138.1">r</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.139.1">x</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.140.1">h</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.141.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.142.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.143.1">g</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.144.1">h</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.145.1">t</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.146.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.147.1">f</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.148.1">f</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.149.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.150.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.151.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.152.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.153.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.154.1">)</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.155.1">+</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.156.1">1</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.157.1">)</span></span></span></p>
<p><span class="koboSpan" id="kobo.158.1">By putting the respective numbers into the equation, you will obtain 416 parameters. </span><span class="koboSpan" id="kobo.158.2">If these weights </span><a id="_idIndexMarker232"/><span class="koboSpan" id="kobo.159.1">are stored as a </span><strong class="bold"><span class="koboSpan" id="kobo.160.1">floating-point 32</span></strong><span class="koboSpan" id="kobo.161.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.162.1">fp32</span></strong><span class="koboSpan" id="kobo.163.1">) format, in bytes, this would mean 416x32 bits/8=1,664 bytes. </span><span class="koboSpan" id="kobo.163.2">With an output data size of 16x26x26, the data size in spatial dimension is decreasing at a very slow rate and since the end goal will be to reduce these values to the size of the target data, we need something to reduce the size of the data. </span><span class="koboSpan" id="kobo.163.3">This is where another layer, called the </span><strong class="bold"><span class="koboSpan" id="kobo.164.1">pooling layer</span></strong><span class="koboSpan" id="kobo.165.1">, comes </span><span class="No-Break"><span class="koboSpan" id="kobo.166.1">into play.</span></span></p>
<h1 id="_idParaDest-56"><a id="_idTextAnchor055"/><span class="koboSpan" id="kobo.167.1">Understanding the pooling layer</span></h1>
<p><span class="koboSpan" id="kobo.168.1">With just a forward pass from a CNN layer of an image, the size of the two-dimensional output data is likely </span><a id="_idIndexMarker233"/><span class="koboSpan" id="kobo.169.1">reduced but is still a substantial size. </span><span class="koboSpan" id="kobo.169.2">To reduce the size of the data further, a layer type called a pooling layer is used to aggregate and consolidate the values strategically while still maintaining useful information. </span><span class="koboSpan" id="kobo.169.3">Think of this operation as an image-resizing method while maintaining as much information as possible. </span><span class="koboSpan" id="kobo.169.4">This layer has no parameters for learning and is mainly added to simply and meaningfully reduce the output data. </span><span class="koboSpan" id="kobo.169.5">The pooling layer works by applying a similar sliding window filter process with similar configurations as the convolutional layers but instead of applying a dot product and adding a bias, a type of aggregation is done. </span><span class="koboSpan" id="kobo.169.6">The aggregation function can be either maximum aggregation, minimum aggregation, or average aggregation. </span><span class="koboSpan" id="kobo.169.7">The layers that apply these aggregations are called max pooling, min pooling, and average </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">pooling, respectively.</span></span></p>
<p><span class="koboSpan" id="kobo.171.1">Consider an average pooling layer with a filter size of 4 and a stride of 2 applied after the first CNN layer. </span><span class="koboSpan" id="kobo.171.2">A forward pass of the 16x26x26 output will result in a data size of 16x12x12. </span><span class="koboSpan" id="kobo.171.3">That reduces the </span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">size considerably!</span></span></p>
<p><span class="koboSpan" id="kobo.173.1">Another type of pooling layer applies the aggregation function globally. </span><span class="koboSpan" id="kobo.173.2">This means that the entire two-dimensional width and height component of the data will be aggregated into a single value. </span><span class="koboSpan" id="kobo.173.3">This variation of the pooling layer is commonly known as the global pooling layer. </span><span class="koboSpan" id="kobo.173.4">This layer is applied to completely break down the data into a one-dimensional </span><a id="_idIndexMarker234"/><span class="koboSpan" id="kobo.174.1">structure so that it can be compatible with one-dimensional targets. </span><span class="koboSpan" id="kobo.174.2">This layer is directly available in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.175.1">keras</span></strong><span class="koboSpan" id="kobo.176.1"> library but only available in </span><strong class="source-inline"><span class="koboSpan" id="kobo.177.1">pytorch</span></strong><span class="koboSpan" id="kobo.178.1"> indirectly through setting the pooling filter size to the same as the size of the input </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">feature map.</span></span></p>
<h1 id="_idParaDest-57"><a id="_idTextAnchor056"/><span class="koboSpan" id="kobo.180.1">Building a CNN architecture</span></h1>
<p><span class="koboSpan" id="kobo.181.1">CNN architectures are commonly made by stacking multiple conceptual logical blocks of layers one after another. </span><span class="koboSpan" id="kobo.181.2">These logical blocks are all structured the same way, with the same type </span><a id="_idIndexMarker235"/><span class="koboSpan" id="kobo.182.1">of layer and layer connections, but they can be different in terms of their parameter configurations, such as the size of the filters, the stride, the type of padding used, and the amount of padding used. </span><span class="koboSpan" id="kobo.182.2">The simplest logical convolutional block is a convolutional layer, pooling layer, and activation function, in that order. </span><strong class="bold"><span class="koboSpan" id="kobo.183.1">Padding</span></strong><span class="koboSpan" id="kobo.184.1"> is a term </span><a id="_idIndexMarker236"/><span class="koboSpan" id="kobo.185.1">that’s used to refer to any extra pixels that are added around the input image to preserve its spatial dimensions after convolution. </span><span class="koboSpan" id="kobo.185.2">Logical blocks are a way for you to describe and reference the architecture simply and efficiently. </span><span class="koboSpan" id="kobo.185.3">They also allow you to build CNN architectures in a depth-wise scalable way without the need to create and set the settings of each layer one by one. </span><span class="koboSpan" id="kobo.185.4">Depth is the same as deepness and refers to the number of neural </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">network layers.</span></span></p>
<p><span class="koboSpan" id="kobo.187.1">The parameters can be designed depending on whether the goal is to gradually scale down the feature maps or to scale up the feature maps. </span><span class="koboSpan" id="kobo.187.2">For the case of one-dimensional targets, the goal is likely to slowly scale down the features into one-dimensional features so that the features can be passed into fully connected layers. </span><span class="koboSpan" id="kobo.187.3">These fully connected layers can then further map the (still large) dimensions to the dimensions that are suitable for the targets. </span><span class="koboSpan" id="kobo.187.4">You can see a simple design of such an architecture in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.188.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.189.1">.5</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer033">
<span class="koboSpan" id="kobo.191.1"><img alt="Figure 3.5 – A simple CNN architecture from scratch while following the logical block analogy" src="image/B18187_03_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.192.1">Figure 3.5 – A simple CNN architecture from scratch while following the logical block analogy</span></p>
<p><span class="koboSpan" id="kobo.193.1">In the </span><a id="_idIndexMarker237"/><span class="koboSpan" id="kobo.194.1">code for </span><strong class="source-inline"><span class="koboSpan" id="kobo.195.1">pytorch</span></strong><span class="koboSpan" id="kobo.196.1">, this example would look </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">like this:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.198.1">
class ConvArch(nn.Module):
  def __init__(self):
       super(ConvArch, self).__init__()
       self.conv1 = nn.Conv2d(
            in_channels=1, out_channels=12, kernel_size=4,
            stride=2, padding=0
       )
       self.conv2 = nn.Conv2d(
            in_channels=12, out_channels=5, kernel_size=3,
            stride=1, padding=0
       )
       self.fc2 = nn.Linear(5, 3)
  def forward(self, x):
       x = F.relu(
            F.avg_pool2d(
                 self.conv1(x), kernel_size=3, stride=2
            )
       )
       x = F.relu(
            F.avg_pool2d(
                 self.conv2(x), kernel_size=3, stride=1
            )
       )
       x = F.avg_pool2d(x, kernel_size=x.size()[2:])
       x = self.fc2(x.reshape((x.size()[:2])))
       return x</span></pre> <p><span class="koboSpan" id="kobo.199.1">Again, the backpropagation of a CNN architecture will be handled automatically by the deep </span><span class="No-Break"><span class="koboSpan" id="kobo.200.1">learning libraries.</span></span></p>
<p><span class="koboSpan" id="kobo.201.1">The architecture </span><a id="_idIndexMarker238"/><span class="koboSpan" id="kobo.202.1">we built was based on the basic classification problem type, which justifies the need to have a fully connected network at the end of the network; this is commonly known as the head. </span><span class="koboSpan" id="kobo.202.2">The set of logical blocks with convolutional layers that were used is called the backbone of the network. </span><span class="koboSpan" id="kobo.202.3">The head of the network can be switched out with other structures, depending on the problem type, but the backbone can be completely adapted into most architectures for any problem type, such as object detection, image generation, image captioning, or image recognition by representation learning. </span><span class="koboSpan" id="kobo.202.4">Some of these problem types will be discussed in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.203.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.204.1">, </span><em class="italic"><span class="koboSpan" id="kobo.205.1">Exploring Supervised </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.206.1">Deep Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.208.1">Now that we have successfully made a simple CNN manually, we have grounded and synced our theories toward the core algorithm of convolutional networks and will be ready to have </span><a id="_idIndexMarker239"/><span class="koboSpan" id="kobo.209.1">more advanced CNN backbone architecture designs at our fingertips. </span><span class="koboSpan" id="kobo.209.2">But before that, this begs the question, how do we design a CNN for our </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">use case?</span></span></p>
<h1 id="_idParaDest-58"><a id="_idTextAnchor057"/><span class="koboSpan" id="kobo.211.1">Designing a CNN architecture for practical usage</span></h1>
<p><span class="koboSpan" id="kobo.212.1">For real-world use cases, CNNs should not be designed similarly to how an MLP is designed. </span><span class="koboSpan" id="kobo.212.2">Practical </span><a id="_idIndexMarker240"/><span class="koboSpan" id="kobo.213.1">usage means that the goal is not to research a new innovative architecture for an unexplored problem type. </span><span class="koboSpan" id="kobo.213.2">Many advancements have been made today based on CNNs. </span><span class="koboSpan" id="kobo.213.3">Advancements usually come in one of </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">two flavors:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.215.1">It sets a new baseline that completely redesigned the way CNN architectures </span><span class="No-Break"><span class="koboSpan" id="kobo.216.1">are made</span></span></li>
<li><span class="koboSpan" id="kobo.217.1">It’s built on top of existing baseline CNN architectures while complementing and improving the performance of the </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">baseline architecture</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.219.1">The key difference between the ideal design approach of a CNN compared to an MLP is that the structures of published CNN architectures should be used instead of designing the architecture from scratch. </span><span class="koboSpan" id="kobo.219.2">The structures of CNN architectures define the type of layers and the way the different types of layers connect; they are usually implemented using logical blocks. </span><span class="koboSpan" id="kobo.219.3">Additionally, the uniqueness of a certain structure defines the family of CNN architecture. </span><span class="koboSpan" id="kobo.219.4">A new CNN research advancement usually comes in different size configurations so that architectures of suitable sizes can be chosen based on either the compute resource limitations, runtime requirements, or the dataset and problem complexity. </span><span class="koboSpan" id="kobo.219.5">Similar to the design of MLP, if resources and runtime are not a limitation, after choosing a CNN architecture structure, start with a reasonably small-sized CNN based on the complexity of the problem and the dataset’s size. </span><span class="koboSpan" id="kobo.219.6">You can gradually test bigger CNNs to see if the performance improves when you increase the size, or vice versa if the </span><span class="No-Break"><span class="koboSpan" id="kobo.220.1">performance decreases.</span></span></p>
<p><span class="koboSpan" id="kobo.221.1">Different CNN architecture structures or architecture families are usually meant to capture different inherent architectural issues of the network. </span><span class="koboSpan" id="kobo.221.2">Some architectural families are designed in a way that they leverage better hardware resources, without which it wouldn’t have been even possible to execute the architecture. </span><span class="koboSpan" id="kobo.221.3">A good practice to achieve good performance is to diversify the type of architectural structures you are using in the initial stage. </span><span class="koboSpan" id="kobo.221.4">Pick size variants of architectural structures with similar sizes in terms of the floating-point operation per second and run your experiments to obtain performance scores, ideally with a small size to maximize the efficiency of the exploration. </span><span class="koboSpan" id="kobo.221.5">Instead of the number of parameters of the model, it is more relevant to consider the floating-point operation per second as an indicator of the complexity of the model; parameter count doesn’t consider the actual runtime of the model, which could benefit from parallelization. </span><span class="koboSpan" id="kobo.221.6">Once you’ve obtained these numbers, pick the top model families and try bigger size variants to benchmark with to find the best model variant for your </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">use case.</span></span></p>
<p><span class="koboSpan" id="kobo.223.1">Most research improvements on CNNs are based on a simple baseline architecture. </span><span class="koboSpan" id="kobo.223.2">This means that </span><a id="_idIndexMarker241"/><span class="koboSpan" id="kobo.224.1">all the other individual improvements that are made on the same baseline architecture are not benchmarked together. </span><span class="koboSpan" id="kobo.224.2">Testing these improvements together can often be complementary but sometimes, it can be detrimental to the metric performance of the model. </span><span class="koboSpan" id="kobo.224.3">Iteratively benchmarking the different configurations will likely be the most systematic and grounded way to obtain a satisfactory performance improvement on </span><span class="No-Break"><span class="koboSpan" id="kobo.225.1">your model.</span></span></p>
<p><span class="koboSpan" id="kobo.226.1">How do researchers grade their improvements? </span><span class="koboSpan" id="kobo.226.2">To design a CNN architecture for practical usage, knowing how to evaluate your architecture will help you slowly work toward an acceptable metric performance. </span><span class="koboSpan" id="kobo.226.3">In </span><a href="B18187_10.xhtml#_idTextAnchor161"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.227.1">Chapter 10</span></em></span></a><span class="koboSpan" id="kobo.228.1">, </span><em class="italic"><span class="koboSpan" id="kobo.229.1">Exploring Model Evaluation Methods</span></em><span class="koboSpan" id="kobo.230.1">, we will discuss the strategies for evaluation more extensively. </span><span class="koboSpan" id="kobo.230.2">One of the main evaluation methods for improvements that’s used for CNN improvements is the top-1 predicted accuracy performance on a massive publicly available image dataset called </span><strong class="source-inline"><span class="koboSpan" id="kobo.231.1">ImageNet</span></strong><span class="koboSpan" id="kobo.232.1">, which consists of millions of images with many classes. </span><strong class="source-inline"><span class="koboSpan" id="kobo.233.1">ImageNet</span></strong><span class="koboSpan" id="kobo.234.1"> is considered to be a highly complex problem-type use case, where each class has an infinite amount of variations possible in the wild, from indoors to outdoors and from real to </span><span class="No-Break"><span class="koboSpan" id="kobo.235.1">synthetic data.</span></span></p>
<p><span class="koboSpan" id="kobo.236.1">So, how do we consider whether improvements are valuable? </span><span class="koboSpan" id="kobo.236.2">Improvements are made to either improve performance in the top-1 accuracy, based on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.237.1">ImageNet</span></strong><span class="koboSpan" id="kobo.238.1"> dataset, to improve the efficiency of running a forward pass of the model, or to specifically improve the training time of </span><span class="No-Break"><span class="koboSpan" id="kobo.239.1">the network.</span></span></p>
<p><span class="koboSpan" id="kobo.240.1">Ranking architectures by their top-1 accuracy metric performance improvements on </span><strong class="source-inline"><span class="koboSpan" id="kobo.241.1">ImageNet</span></strong><span class="koboSpan" id="kobo.242.1"> alone, however, is a biased evaluation as, more often than not, the absolute ranking of models differs when applying the same architectures on a separate image dataset. </span><span class="koboSpan" id="kobo.242.2">Using it as a starting point to choose ready-made architectures is wise, but make sure you evaluate a few of the top-performing </span><strong class="source-inline"><span class="koboSpan" id="kobo.243.1">ImageNet</span></strong><span class="koboSpan" id="kobo.244.1"> architectures to figure out which works the best. </span><span class="koboSpan" id="kobo.244.2">Furthermore, while </span><strong class="source-inline"><span class="koboSpan" id="kobo.245.1">ImageNet</span></strong><span class="koboSpan" id="kobo.246.1"> was curated with manual effort, involving querying search engines and passing candidate images through a validation step on Amazon Mechanical Turk, it still contains some label noise that can obfuscate the meaning behind the </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">metric’s performance.</span></span></p>
<p><span class="koboSpan" id="kobo.248.1">As for the improvement direction of increasing the efficiency of running a forward pass of the model, this is usually done either by reducing the number of parameters of the architecture, breaking down a compute-intensive logical component into multiple components </span><a id="_idIndexMarker242"/><span class="koboSpan" id="kobo.249.1">that reduces the amount of operations, or switching out layers for layers that have higher parallelism potential. </span><span class="koboSpan" id="kobo.249.2">The improvements that are made in this direction either maintain or improve the metric performance score on the validation dataset of </span><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">ImageNet</span></strong><span class="koboSpan" id="kobo.251.1">. </span><span class="koboSpan" id="kobo.251.2">This line of improvement is a main focus for the family of CNN architectures that have been built to run in low-resource edge devices. </span><span class="koboSpan" id="kobo.251.3">We will dive into </span><span class="No-Break"><span class="koboSpan" id="kobo.252.1">this later.</span></span></p>
<p><span class="koboSpan" id="kobo.253.1">An efficient way to explore different CNN architecture families in your use case to achieve better metric performance is to pick model families that have a publicly available implementation complete with pre-trained weights trained on </span><strong class="source-inline"><span class="koboSpan" id="kobo.254.1">ImageNet</span></strong><span class="koboSpan" id="kobo.255.1">. </span><span class="koboSpan" id="kobo.255.2">Initializing your architecture using pre-trained weights presents a bunch of benefits that include faster training, less overfitting, and increased generalization, even when the dataset that was used to pre-train the weights is part of a different problem subset. </span><span class="koboSpan" id="kobo.255.3">This process is called transfer learning and we will learn about it in more detail in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.256.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.257.1">, </span><em class="italic"><span class="koboSpan" id="kobo.258.1">Exploring Supervised Deep Learning</span></em><span class="koboSpan" id="kobo.259.1">, and </span><a href="B18187_09.xhtml#_idTextAnchor149"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.260.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.261.1">, </span><em class="italic"><span class="koboSpan" id="kobo.262.1">Exploring Unsupervised </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.263.1">Deep Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.264.1">.</span></span></p>
<h1 id="_idParaDest-59"><a id="_idTextAnchor058"/><span class="koboSpan" id="kobo.265.1">Exploring the CNN architecture families</span></h1>
<p><span class="koboSpan" id="kobo.266.1">Now, instead of going through the history of CNN through the years, let’s look at a list of the different </span><a id="_idIndexMarker243"/><span class="koboSpan" id="kobo.267.1">handpicked model architecture families. </span><span class="koboSpan" id="kobo.267.2">These architecture families are selectively chosen to be sufficiently different and diverse from each other. </span><span class="koboSpan" id="kobo.267.3">One thing to note is that neural networks are advancing at an astounding pace. </span><span class="koboSpan" id="kobo.267.4">With that in mind, the architecture families that will be introduced here are ensured to be relevant today. </span><span class="koboSpan" id="kobo.267.5">Additionally, only the most important information you need to know about the architecture family will be presented, simplifying the many pages of research papers in concise but </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">sufficient detail.</span></span></p>
<p><span class="koboSpan" id="kobo.269.1">Another thing to note before diving into this topic is that the metric performance on a dataset will often be the main comparison method among different architectures, so be aware that the metric performance of a model is achieved by the collective contribution of the training </span><a id="_idIndexMarker244"/><span class="koboSpan" id="kobo.270.1">method and the architecture. </span><span class="koboSpan" id="kobo.270.2">The training method includes details not specifically related to the architecture of the model, such as the loss used, data augmentation strategy, and data resolution. </span><span class="koboSpan" id="kobo.270.3">These topics will be covered in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.271.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.272.1">, </span><em class="italic"><span class="koboSpan" id="kobo.273.1">Exploring Supervised Deep Learning</span></em><span class="koboSpan" id="kobo.274.1">. </span><span class="koboSpan" id="kobo.274.2">The architecture families we’ll introduce here are </span><strong class="bold"><span class="koboSpan" id="kobo.275.1">ResNet</span></strong><span class="koboSpan" id="kobo.276.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.277.1">DenseNet</span></strong><span class="koboSpan" id="kobo.278.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.279.1">MobileNet</span></strong><span class="koboSpan" id="kobo.280.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.281.1">EfficientNet</span></strong><span class="koboSpan" id="kobo.282.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.283.1">ShuffleNet</span></strong><span class="koboSpan" id="kobo.284.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.286.1">MicroNet</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">.</span></span></p>
<h2 id="_idParaDest-60"><a id="_idTextAnchor059"/><span class="koboSpan" id="kobo.288.1">Understanding the ResNet model family</span></h2>
<p><span class="koboSpan" id="kobo.289.1">The ResNet architecture, from 2015, was made based on the pretext that deep networks </span><a id="_idIndexMarker245"/><span class="koboSpan" id="kobo.290.1">are hard to train and aimed to change that. </span><span class="koboSpan" id="kobo.290.2">The vanishing gradient is a widely known problem that neural networks face, where the information from the data diminishes the deeper the network is. </span><span class="koboSpan" id="kobo.290.3">Plain deep CNN </span><a id="_idIndexMarker246"/><span class="koboSpan" id="kobo.291.1">architectures, however, were identified to </span><em class="italic"><span class="koboSpan" id="kobo.292.1">not</span></em><span class="koboSpan" id="kobo.293.1"> suffer from vanishing gradients, with proof from verifying gradient information. </span><span class="koboSpan" id="kobo.293.2">The key cause of vanishing gradients is when we use too many activation functions that squish the data into very small value ranges. </span><span class="koboSpan" id="kobo.293.3">An example of this is the sigmoid function, which maps data from 0 to 1. </span><span class="koboSpan" id="kobo.293.4">So, when in doubt, use ReLU in </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">your architecture!</span></span></p>
<p><span class="koboSpan" id="kobo.295.1">The </span><em class="italic"><span class="koboSpan" id="kobo.296.1">Res</span></em><span class="koboSpan" id="kobo.297.1"> part of ResNet is named </span><a id="_idIndexMarker247"/><span class="koboSpan" id="kobo.298.1">after the term </span><strong class="bold"><span class="koboSpan" id="kobo.299.1">residuals</span></strong><span class="koboSpan" id="kobo.300.1">. </span><span class="koboSpan" id="kobo.300.2">The idea was that learning from residuals is much easier than learning from unmodified feature maps. </span><span class="koboSpan" id="kobo.300.3">Residuals are achieved by adding skip connections from the earlier layers to later layers. </span><span class="koboSpan" id="kobo.300.4">In layman’s terms, that means adding (not concatenating) feature maps from earlier parts of the network to feature maps of later parts of the network. </span><span class="koboSpan" id="kobo.300.5">The result of this addition creates the residuals that will be learned by subsequent convolutional layers, which again apply more skip connections and create more residuals. </span><span class="koboSpan" id="kobo.300.6">Residuals can be easily applied to any CNN architecture with different configurations and can be considered an improvement on top of older baseline architectures. </span><span class="koboSpan" id="kobo.300.7">However, the authors also presented multiple variations of an architecture that utilized skip connections with a different number of convolutional layers, including ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152. </span><span class="koboSpan" id="kobo.300.8">The ResNet architecture family serves as a boilerplate for easy usage of residual networks and eventually led to it becoming the most popular baseline for research on new advancements. </span><span class="koboSpan" id="kobo.300.9">The actual architecture designs are not presented here formally as memorizing those designs won’t have any impact on your grasp of CNN knowledge. </span><span class="koboSpan" id="kobo.300.10">Instead, </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.301.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.302.1">.6</span></em><span class="koboSpan" id="kobo.303.1"> shows the residual computation of a single </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">logical block:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer034">
<span class="koboSpan" id="kobo.305.1"><img alt="Figure 3.6 – Example of the actual residual connection method in the ResNet model family" src="image/B18187_03_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.306.1">Figure 3.6 – Example of the actual residual connection method in the ResNet model family</span></p>
<p><span class="koboSpan" id="kobo.307.1">To summarize </span><a id="_idIndexMarker248"/><span class="koboSpan" id="kobo.308.1">the entire baseline ResNet size variants, the following </span><a id="_idIndexMarker249"/><span class="koboSpan" id="kobo.309.1">table shows a summary configuration of all the different </span><span class="No-Break"><span class="koboSpan" id="kobo.310.1">size variants:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer035">
<span class="koboSpan" id="kobo.311.1"><img alt="Figure 3.7 – Base ResNet different size variants" src="image/B18187_03_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.312.1">Figure 3.7 – Base ResNet different size variants</span></p>
<p><span class="koboSpan" id="kobo.313.1">The brackets denoted in the table are meant to signify the set of serial convolutional layers that are defined as the logical blocks. </span><span class="koboSpan" id="kobo.313.2">ResNet has also gone the extra mile to group multiple base logical blocks to a higher-level category they call “layer name.” </span><span class="koboSpan" id="kobo.313.3">The layer name has the same number of groups across different size variants. </span><span class="koboSpan" id="kobo.313.4">Logical blocks and higher-level grouping methods are just a way for you to describe and reference the complicated architecture simply and efficiently. </span><span class="koboSpan" id="kobo.313.5">The first two numbers, which with </span><a id="_idIndexMarker250"/><span class="koboSpan" id="kobo.314.1">multiplication in between in the logical block, define the convolution filter size; the number that comes after it with a comma defines the number </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">of filters.</span></span></p>
<p><span class="koboSpan" id="kobo.316.1">Skip connections </span><a id="_idIndexMarker251"/><span class="koboSpan" id="kobo.317.1">were found to smoothen the loss landscape of any neural network, making the learning process much easier and stabler. </span><span class="koboSpan" id="kobo.317.2">This allows for the neural network to converge to the optimum more easily. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.318.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.319.1">.8</span></em><span class="koboSpan" id="kobo.320.1"> shows how the loss landscape for a neural network without skip connections has an uneven terrain with many hills and valleys on the left. </span><span class="koboSpan" id="kobo.320.2">It becomes a smooth terrain with an obvious single valley when skip connections are added using a variant </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">of ResNet:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer036">
<span class="koboSpan" id="kobo.322.1"><img alt="Figure 3.8 – Loss landscape with no skip connections on the left and with skip connections on the right" src="image/B18187_03_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.323.1">Figure 3.8 – Loss landscape with no skip connections on the left and with skip connections on the right</span></p>
<p><span class="koboSpan" id="kobo.324.1">One notable piece of information you can derive from the table is that, starting from ResNet-50 onwards, the architecture utilizes 1x1 convolutional filters. </span><span class="koboSpan" id="kobo.324.2">As this filter operates exclusively with a single-dimensional filter weight across the channel dimension, the operation is equivalent to applying a fully connected layer in a windowing fashion in the channel dimension. </span><span class="koboSpan" id="kobo.324.3">Since the fully connected layer is a network itself, along with </span><a id="_idIndexMarker252"/><span class="koboSpan" id="kobo.325.1">the convolutional network, this operation is often called a </span><strong class="bold"><span class="koboSpan" id="kobo.326.1">network in a network</span></strong><span class="koboSpan" id="kobo.327.1">. </span><span class="koboSpan" id="kobo.327.2">Next, let’s explore the different improvements with the ResNet architecture as </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">a base.</span></span></p>
<h3><span class="koboSpan" id="kobo.329.1">Improving ResNets</span></h3>
<p><span class="koboSpan" id="kobo.330.1">As mentioned previously, ResNet is considered a model family and contains many different varieties, not just by size but with different architectural improvements. </span><span class="koboSpan" id="kobo.330.2">Any CNN architecture </span><a id="_idIndexMarker253"/><span class="koboSpan" id="kobo.331.1">that is based on the ResNet belongs to this model family. </span><span class="koboSpan" id="kobo.331.2">Here, we have mentioned some of the few notable architectural advancements and provided descriptions of their main advancements based on ResNet; they are ordered by the year they </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">were made:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.333.1">ResNeXt</span></strong><span class="koboSpan" id="kobo.334.1"> (2016): This converted serial convolutional layers in the logical blocks of ResNets </span><a id="_idIndexMarker254"/><span class="koboSpan" id="kobo.335.1">into parallel branches of convolutional layers and merged the parallel layers at the end of the logical block. </span><span class="koboSpan" id="kobo.335.2">It increases the metric performance on </span><strong class="source-inline"><span class="koboSpan" id="kobo.336.1">ImageNet</span></strong><span class="koboSpan" id="kobo.337.1"> compared to ResNet variants, even when the number of parameters is the same and also </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">in general.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.339.1">Squeeze-and-Excitation Networks</span></strong><span class="koboSpan" id="kobo.340.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.341.1">SE-ResNet</span></strong><span class="koboSpan" id="kobo.342.1">) (2017): Since two-dimensional convolutions don’t consider the inter-channel relationships and only consider </span><a id="_idIndexMarker255"/><span class="koboSpan" id="kobo.343.1">the local and spatial (width and height) information of the feature maps, a method to leverage the channel relationships was made that includes a squeeze block and an excitation block. </span><span class="koboSpan" id="kobo.343.2">This is a method that can be repeatedly applied to many parts of an existing CNN architecture. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.344.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.345.1">.9</span></em><span class="koboSpan" id="kobo.346.1"> shows the structure of this method, where a global average pooling is applied in the width and height dimensions and, subsequently, two fully connected networks are applied to scale down and scale up the features back to the same size to allow channel information to </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">be combined:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer037">
<span class="koboSpan" id="kobo.348.1"><img alt="Figure 3.9 – Squeeze and excitation structure" src="image/B18187_03_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.349.1">Figure 3.9 – Squeeze and excitation structure</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.350.1">The scaling </span><a id="_idIndexMarker256"/><span class="koboSpan" id="kobo.351.1">part of the structure is where the values are multiplied together. </span><span class="koboSpan" id="kobo.351.2">When combined with ResNet, the architecture is </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">called SE-ResNet.</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.353.1">ResNet-D</span></strong><span class="koboSpan" id="kobo.354.1"> (2019): This advancement made simple architecture parameter tweaks to </span><a id="_idIndexMarker257"/><span class="koboSpan" id="kobo.355.1">improve metric performance while maintaining the number of parameters, albeit at the slightly justifiable increase of the FLOP specification. </span><span class="koboSpan" id="kobo.355.2">As some path of ResNet utilizes 1x1 convolutions with a stride of 2, ¾ of the information is discarded and one of the tweaks was to the stride sizes to ensure no information gets removed explicitly. </span><span class="koboSpan" id="kobo.355.3">They also reduced computational load by changing a 7x7 convolution to three separate 3x3 </span><span class="No-Break"><span class="koboSpan" id="kobo.356.1">convolutions serially.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.357.1">ResNet-RS</span></strong><span class="koboSpan" id="kobo.358.1"> (2021): This advancement combines ResNet-D and the squeeze and excitation </span><a id="_idIndexMarker258"/><span class="koboSpan" id="kobo.359.1">network and uses an image size that depends on the size of the network. </span><span class="koboSpan" id="kobo.359.2">However, it grows slower than EfficientNets (these will be </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">introduced later).</span></span></li>
</ul>
<h2 id="_idParaDest-61"><a id="_idTextAnchor060"/><span class="koboSpan" id="kobo.361.1">Understanding the DenseNet architecture family</span></h2>
<p><strong class="bold"><span class="koboSpan" id="kobo.362.1">DenseNet</span></strong><span class="koboSpan" id="kobo.363.1"> is an architecture family that was introduced in the early months of 2018. </span><span class="koboSpan" id="kobo.363.2">The architecture </span><a id="_idIndexMarker259"/><span class="koboSpan" id="kobo.364.1">is based on the idea of skip connections, similar to the ResNet architecture family but on steroids, which means it uses a lot of skip connections. </span><span class="koboSpan" id="kobo.364.2">The skip connections in this family of architecture differ in that they use </span><strong class="bold"><span class="koboSpan" id="kobo.365.1">concatenation</span></strong><span class="koboSpan" id="kobo.366.1"> instead of residual connections by </span><strong class="bold"><span class="koboSpan" id="kobo.367.1">summation</span></strong><span class="koboSpan" id="kobo.368.1">. </span><span class="koboSpan" id="kobo.368.2">Summation allows earlier information to be directly encoded into the outputs of future layers without the </span><a id="_idIndexMarker260"/><span class="koboSpan" id="kobo.369.1">need to modify the number of neurons, albeit at the slight informational disadvantage of needing the future layers to learn to decode this information. </span><span class="koboSpan" id="kobo.369.2">Concatenation adds to the size of the architecture as you need to create extra neurons to account for the extra information, allowing the model to work on the raw data. </span><span class="koboSpan" id="kobo.369.3">Both provide similar advantages to using skip connections. </span><span class="koboSpan" id="kobo.369.4">Logical blocks </span><a id="_idIndexMarker261"/><span class="koboSpan" id="kobo.370.1">are created called </span><strong class="bold"><span class="koboSpan" id="kobo.371.1">dense blocks</span></strong><span class="koboSpan" id="kobo.372.1">, where each subsequent layer in the block has access to all the outputs of the layers before it in the </span><a id="_idIndexMarker262"/><span class="koboSpan" id="kobo.373.1">block by feature map concatenation. </span><span class="koboSpan" id="kobo.373.2">In these blocks, </span><strong class="bold"><span class="koboSpan" id="kobo.374.1">zero-padding</span></strong><span class="koboSpan" id="kobo.375.1"> is used to ensure that the spatial dimensions of the outputs of each layer are maintained so that feature maps can be concatenated. </span><span class="koboSpan" id="kobo.375.2">This setup promotes a lot of feature reuse between layers in the same block and allows the number of model parameters to stay the same while increasing the model’s learning capacity. </span><span class="koboSpan" id="kobo.375.3">The number of filters for each subsequent layer in a block for all blocks is fixed at a constant number </span><a id="_idIndexMarker263"/><span class="koboSpan" id="kobo.376.1">called the </span><strong class="bold"><span class="koboSpan" id="kobo.377.1">growth rate</span></strong><span class="koboSpan" id="kobo.378.1">, as there needs to be a structured way to add layers to not exponentially increase the number of channels. </span><span class="koboSpan" id="kobo.378.2">Taking a constant of 32 filters, the input of the second layer in the block will be a feature map with 32 channels, the input of the third layer in the block will be 64 and concatenated, and </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">so on.</span></span></p>
<p><span class="koboSpan" id="kobo.380.1">To create a full network architecture, multiple dense blocks were stacked one after another with a separate convolutional layer and pooling layer in between the dense blocks to gradually reduce the spatial dimensions of the feature map. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.381.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.382.1">.10</span></em><span class="koboSpan" id="kobo.383.1"> shows the network structure of the four different DenseNet model architectures under the DenseNet </span><span class="No-Break"><span class="koboSpan" id="kobo.384.1">model family:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer038">
<span class="koboSpan" id="kobo.385.1"><img alt="Figure 3.10 – The DenseNet model family where “conv” corresponds to sequential layers of batch normalization, ReLU, and the convolutional layer" src="image/B18187_03_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.386.1">Figure 3.10 – The DenseNet model family where “conv” corresponds to sequential layers of batch normalization, ReLU, and the convolutional layer</span></p>
<p><span class="koboSpan" id="kobo.387.1">This allows </span><a id="_idIndexMarker264"/><span class="koboSpan" id="kobo.388.1">DenseNet to improve upon its predecessor network architectures in terms of top-1 </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.389.1">ImageNet</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.390.1"> accuracy.</span></span></p>
<h2 id="_idParaDest-62"><a id="_idTextAnchor061"/><span class="koboSpan" id="kobo.391.1">Understanding the EfficientNet architecture family</span></h2>
<p><span class="koboSpan" id="kobo.392.1">Made in 2020, EfficientNet </span><a id="_idIndexMarker265"/><span class="koboSpan" id="kobo.393.1">created a family of </span><a id="_idIndexMarker266"/><span class="koboSpan" id="kobo.394.1">architectures by using an automated </span><strong class="bold"><span class="koboSpan" id="kobo.395.1">neural architecture search</span></strong><span class="koboSpan" id="kobo.396.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.397.1">NAS</span></strong><span class="koboSpan" id="kobo.398.1">) method to create a small base efficient architecture and utilize an </span><a id="_idIndexMarker267"/><span class="koboSpan" id="kobo.399.1">easy-to-use compound scaling method to scale the depth and width of the architecture, as well as the resolution of the image. </span><span class="koboSpan" id="kobo.399.2">The neural architecture search is searched in a way that it balances FLOPS and accuracy. </span><span class="koboSpan" id="kobo.399.3">The NAS method that's used is from another research called </span><strong class="bold"><span class="koboSpan" id="kobo.400.1">MnasNet</span></strong><span class="koboSpan" id="kobo.401.1"> and will be introduced properly in </span><a href="B18187_07.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.402.1">Chapter 7</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.403.1">, Deep Neural </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.404.1">Architecture Search</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.405.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.406.1">The compound scaling method is simple and can be extended to any other network, though ResNet-RS demonstrates that scaling resolution slower provides more value. </span><span class="koboSpan" id="kobo.406.2">The scaling method for depth, width, and resolution is defined in the following equation, where the result will be multiplied by the original base architecture parameters to scale up </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">the architecture:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.408.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.409.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.410.1">p</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.411.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.412.1">h</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.413.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.414.1">α</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.415.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.416.1">φ</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.417.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.418.1">w</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.419.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.420.1">d</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.421.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.422.1">h</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.423.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.424.1">β</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.425.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.426.1">φ</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.427.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.428.1">r</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.429.1">e</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.430.1">s</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.431.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.432.1">l</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.433.1">u</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.434.1">t</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.435.1">i</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.436.1">o</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.437.1">n</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.438.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.439.1">γ</span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.440.1"> </span></span></span><span class="No-Break"><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.441.1">φ</span></span></span></p>
<p><span class="koboSpan" id="kobo.442.1">Here, </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.443.1">φ</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="koboSpan" id="kobo.444.1">is the coefficient that can be scaled to different values based on requirements and the other variables are constants that should be set to optimize something such as the FLOPS increase rate when we change the coefficient. </span><span class="koboSpan" id="kobo.444.2">For EfficientNet, the constants are constrained to satisfy the </span><span class="No-Break"><span class="koboSpan" id="kobo.445.1">following condition:</span></span></p>
<p><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.446.1">α</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.447.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.448.1">β</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.449.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.450.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.451.1">∙</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.452.1">γ</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.453.1"> </span></span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.454.1">2</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.455.1">≈</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.456.1">2</span></span></p>
<p><span class="koboSpan" id="kobo.457.1">This will constrain </span><a id="_idIndexMarker268"/><span class="koboSpan" id="kobo.458.1">the increase of the coefficient </span><a id="_idIndexMarker269"/><span class="koboSpan" id="kobo.459.1">to approximately increase the FLOPS by </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.460.1">2</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.461.1"> </span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.462.1">φ</span></span><span class="koboSpan" id="kobo.463.1">. </span><span class="koboSpan" id="kobo.463.2">EfficientNet sets this to </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.464.1">α</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.465.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.466.1">1.2</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.467.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.468.1">β</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.469.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.470.1">1.1</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.471.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.472.1">γ</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.473.1">=</span></span><span class="_-----MathTools-_Math_Base"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.474.1">1.15</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="koboSpan" id="kobo.475.1">. </span><span class="koboSpan" id="kobo.475.2">This compound scaling strategy allowed seven EfficientNets to be made, named B0 to B7. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.476.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.477.1">.11</span></em><span class="koboSpan" id="kobo.478.1"> shows the structure of </span><span class="No-Break"><span class="koboSpan" id="kobo.479.1">the EfficientNet-B0:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer039">
<span class="koboSpan" id="kobo.480.1"><img alt="Figure 3.11 – EfficientNet-B0 architecture structure" src="image/B18187_03_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.481.1">Figure 3.11 – EfficientNet-B0 architecture structure</span></p>
<p><span class="koboSpan" id="kobo.482.1">Note that </span><strong class="bold"><span class="koboSpan" id="kobo.483.1">MBConv</span></strong><span class="koboSpan" id="kobo.484.1"> is also known as an inverted residual block and will be properly introduced in another network later in the </span><em class="italic"><span class="koboSpan" id="kobo.485.1">Understanding </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.486.1">MobileNetV2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.487.1"> section.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.488.1">EfficientNetV2</span></strong><span class="koboSpan" id="kobo.489.1">, made </span><a id="_idIndexMarker270"/><span class="koboSpan" id="kobo.490.1">in 2021, identified that large image resolution slows down training time, where depthwise convolutions are slow in early layers, and scaling up depth, width, and resolution at the same time is not optimal. </span><span class="koboSpan" id="kobo.490.2">EfficientNetV2 also uses NAS to find a base architecture but with the addition of a modification of an MBConv block that has more parameters and operations with the reason that it can be faster sometimes, depending on the input and output data shape, where they are in the entire architecture, and how the data is transferred to the computing processer. </span><span class="koboSpan" id="kobo.490.3">This will be explained in more detail when we formally introduce MBConv later. </span><span class="koboSpan" id="kobo.490.4">EfficientNetV2 also utilizes the original compound scaling method but adds a few improvements by setting a maximum image resolution to 480 and adding extra layers to </span><a id="_idIndexMarker271"/><span class="koboSpan" id="kobo.491.1">the last few stages of the architecture when it wants to increase the network capacity to higher proportions. </span><span class="koboSpan" id="kobo.491.2">A new training method was </span><a id="_idIndexMarker272"/><span class="koboSpan" id="kobo.492.1">added to reduce training time; we’ll </span><a id="_idIndexMarker273"/><span class="koboSpan" id="kobo.493.1">discuss this </span><a id="_idIndexMarker274"/><span class="koboSpan" id="kobo.494.1">in more detail in the </span><a id="_idIndexMarker275"/><span class="koboSpan" id="kobo.495.1">next chapter. </span><span class="koboSpan" id="kobo.495.2">These improvements resulted in four different </span><a id="_idIndexMarker276"/><span class="koboSpan" id="kobo.496.1">EfficientNetV2 models called </span><strong class="bold"><span class="koboSpan" id="kobo.497.1">EfficientNetV2-S</span></strong><span class="koboSpan" id="kobo.498.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.499.1">EfficientNetV2-M</span></strong><span class="koboSpan" id="kobo.500.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.501.1">EfficientNetV2-L</span></strong><span class="koboSpan" id="kobo.502.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.503.1">EfficientNetV2-XL</span></strong><span class="koboSpan" id="kobo.504.1">. </span><span class="koboSpan" id="kobo.504.2">These exceeded the top-1 accuracy compared to the original EfficientNet at similar </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">FLOP values:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer040">
<span class="koboSpan" id="kobo.506.1"><img alt="Figure 3.12 – EfficientNetV2-S architecture structure" src="image/B18187_03_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.507.1">Figure 3.12 – EfficientNetV2-S architecture structure</span></p>
<p><span class="koboSpan" id="kobo.508.1">EfficientNetV</span><a id="_idTextAnchor062"/><span class="koboSpan" id="kobo.509.1">2-S served as the base architecture, similar to how EfficientNetB0 was the base, where the architecture structure is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.510.1">Figure 3</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.511.1">.12</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.512.1">.</span></span></p>
<h2 id="_idParaDest-63"><a id="_idTextAnchor063"/><span class="koboSpan" id="kobo.513.1">Understanding small and fast CNN architecture families for small-scale edge devices</span></h2>
<p><span class="koboSpan" id="kobo.514.1">One of </span><a id="_idIndexMarker277"/><span class="koboSpan" id="kobo.515.1">the clear groups of architectures </span><a id="_idIndexMarker278"/><span class="koboSpan" id="kobo.516.1">that holds a place in the world of CNN architectures is the group that is built not for scalability purposes or beating the </span><strong class="source-inline"><span class="koboSpan" id="kobo.517.1">ImageNet</span></strong><span class="koboSpan" id="kobo.518.1"> benchmark, but to be used for small devices. </span><span class="koboSpan" id="kobo.518.2">Small devices are usually called </span><strong class="bold"><span class="koboSpan" id="kobo.519.1">edge devices</span></strong><span class="koboSpan" id="kobo.520.1"> as they </span><a id="_idIndexMarker279"/><span class="koboSpan" id="kobo.521.1">are small and compact enough to be mobile or to be physically deployed with actual data processing capabilities where the data originated. </span><span class="koboSpan" id="kobo.521.2">Our mobile </span><a id="_idIndexMarker280"/><span class="koboSpan" id="kobo.522.1">smartphones are examples of mobile devices that </span><a id="_idIndexMarker281"/><span class="koboSpan" id="kobo.523.1">are capable of producing images. </span><span class="koboSpan" id="kobo.523.2">Examples of edge devices that </span><em class="italic"><span class="koboSpan" id="kobo.524.1">aren’t</span></em><span class="koboSpan" id="kobo.525.1"> mobile include CCTV video cameras and doorbell cameras. </span><span class="koboSpan" id="kobo.525.2">Some of the key benefits of deploying models to the edge are </span><span class="No-Break"><span class="koboSpan" id="kobo.526.1">listed here:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.527.1">Reduced communication latency</span></strong><span class="koboSpan" id="kobo.528.1">: Images and real-time video feeds are large </span><a id="_idIndexMarker282"/><span class="koboSpan" id="kobo.529.1">compared to simple numerical or categorical data. </span><span class="koboSpan" id="kobo.529.2">By using computation at the edge, less data needs to be transferred to a centralized server, thus reducing the time needed to transfer data. </span><span class="koboSpan" id="kobo.529.3">Sometimes, the need for a transfer can be eliminated when computation is done at the edge, thus significantly simplifying </span><span class="No-Break"><span class="koboSpan" id="kobo.530.1">a system.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.531.1">Reduced bandwidth requirements</span></strong><span class="koboSpan" id="kobo.532.1">: When the images are processed where they </span><a id="_idIndexMarker283"/><span class="koboSpan" id="kobo.533.1">are produced, only simple data formats such as numerical or categorical will need to be returned, thus removing the need to have expensive equipment for high </span><span class="No-Break"><span class="koboSpan" id="kobo.534.1">bandwidth requirements.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.535.1">Increased redundancy</span></strong><span class="koboSpan" id="kobo.536.1">: A centralized server also means a single point of failure. </span><span class="koboSpan" id="kobo.536.2">Distributing </span><a id="_idIndexMarker284"/><span class="koboSpan" id="kobo.537.1">processing to the individual edge devices makes sure the failure of any one device won’t affect the </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">entire system.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.539.1">Small CNN </span><a id="_idIndexMarker285"/><span class="koboSpan" id="kobo.540.1">architectures meant for edge devices typically engineer the entire structure of a model without a single notable baseline, so there </span><a id="_idIndexMarker286"/><span class="koboSpan" id="kobo.541.1">isn’t a proper model family to categorize each of these architectures. </span><span class="koboSpan" id="kobo.541.2">For ease of referencing architectures meant for this purpose, it is recommended to think of the following architectures as under </span><em class="italic"><span class="koboSpan" id="kobo.542.1">architecture for the edge.</span></em><span class="koboSpan" id="kobo.543.1"> There are other techniques that can be applied architecture-wide to further optimize the efficiency of the model that we will explore in </span><a href="B18187_15.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.544.1">Chapter 15</span></em></span></a><em class="italic"><span class="koboSpan" id="kobo.545.1">, Deploying Deep Learning Models in Production</span></em><span class="koboSpan" id="kobo.546.1">, but for now let’s explore two of these architectures, namely </span><strong class="bold"><span class="koboSpan" id="kobo.547.1">SqueezeNet</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.548.1">and </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.549.1">MobileNet</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.550.1">.</span></span></p>
<h2 id="_idParaDest-64"><a id="_idTextAnchor064"/><span class="koboSpan" id="kobo.551.1">Understanding SqueezeNet</span></h2>
<p><span class="koboSpan" id="kobo.552.1">SqueezeNet was developed in 2016 to build small and fast CNNs with the benefits described in </span><a id="_idIndexMarker287"/><span class="koboSpan" id="kobo.553.1">the previous section but with an emphasis on deploying to hardware with limited memory. </span><span class="koboSpan" id="kobo.553.2">The three strategies that can be employed are </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.555.1">For a convolutional </span><a id="_idIndexMarker288"/><span class="koboSpan" id="kobo.556.1">layer with 3x3 filters, replace the filters partially with 1x1 filters. </span><span class="koboSpan" id="kobo.556.2">This means that the 1x1 filter and 3x3 filters co-exist theoretically in a single layer applied to the same input. </span><span class="koboSpan" id="kobo.556.3">However, the implementation details will differ as parallel branch paths for the 1x1 filters and 3x3 filters are applied on the same input. </span><span class="koboSpan" id="kobo.556.4">This is called the </span><span class="No-Break"><span class="koboSpan" id="kobo.557.1">expand layer.</span></span></li>
<li><span class="koboSpan" id="kobo.558.1">Decrease the data input channels that are passed into the parallel 1x1 and 3x3 filters by using a small number of 1x1 filters. </span><span class="koboSpan" id="kobo.558.2">This is called the </span><span class="No-Break"><span class="koboSpan" id="kobo.559.1">squeeze layer.</span></span></li>
<li><span class="koboSpan" id="kobo.560.1">Downsample the feature maps late in the architecture so that the majority of the convolutional layers have access to large feature maps based on findings and improve metric performance. </span><span class="koboSpan" id="kobo.560.2">This is done by using pooling layers with strides of 2 pixels less frequently in earlier stages and more frequently in later stages and larger intervals. </span><span class="koboSpan" id="kobo.560.3">Pooling layers are not used after every </span><span class="No-Break"><span class="koboSpan" id="kobo.561.1">convolutional layer.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.562.1">A logical block called </span><strong class="source-inline"><span class="koboSpan" id="kobo.563.1">Fire</span></strong><span class="koboSpan" id="kobo.564.1"> was made that provides the ease of creating multiple blocks to create architectures. </span><span class="koboSpan" id="kobo.564.2">This block enabled the configuration of the number of 1x1-sized filters in the squeeze layer, 1x1 filters in the expand layer, and 3x3 filters in the expand layer. </span><span class="koboSpan" id="kobo.564.3">This is depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.565.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.566.1">.13</span></em><span class="koboSpan" id="kobo.567.1">. </span><span class="koboSpan" id="kobo.567.2">Note that padding is applied to the outputs of the 3x3 convolutional layer to ensure it can be concatenated with the outputs from the 1x1 convolutional layer in the </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">expand layer:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer041">
<span class="koboSpan" id="kobo.569.1"><img alt="Figure 3.13 – The Fire module/logical block" src="image/B18187_03_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.570.1">Figure 3.13 – The Fire module/logical block</span></p>
<p><span class="koboSpan" id="kobo.571.1">Eight </span><strong class="source-inline"><span class="koboSpan" id="kobo.572.1">Fire</span></strong><span class="koboSpan" id="kobo.573.1"> blocks were </span><a id="_idIndexMarker289"/><span class="koboSpan" id="kobo.574.1">used to build an architecture called SqueezeNet that maintained the performance of </span><strong class="source-inline"><span class="koboSpan" id="kobo.575.1">ImageNet</span></strong><span class="koboSpan" id="kobo.576.1"> from the historical architecture </span><a id="_idIndexMarker290"/><span class="koboSpan" id="kobo.577.1">known as </span><strong class="bold"><span class="koboSpan" id="kobo.578.1">AlexNet</span></strong><span class="koboSpan" id="kobo.579.1"> (not introduced in this book as it is not practically useful nowadays) while being </span><span class="No-Break"><span class="koboSpan" id="kobo.580.1">50x smaller.</span></span></p>
<h2 id="_idParaDest-65"><a id="_idTextAnchor065"/><span class="koboSpan" id="kobo.581.1">Understanding MobileNet</span></h2>
<p><span class="koboSpan" id="kobo.582.1">The first MobileNet version, called MobileNetV1, was introduced in 2017 for mobile and embedded devices </span><a id="_idIndexMarker291"/><span class="koboSpan" id="kobo.583.1">by focusing on optimizing latency issues and getting small-sized networks as a side effect instead of </span><span class="No-Break"><span class="koboSpan" id="kobo.584.1">vice versa.</span></span></p>
<p><span class="koboSpan" id="kobo.585.1">The depthwise </span><a id="_idIndexMarker292"/><span class="koboSpan" id="kobo.586.1">separable convolutional layer was used extensively in the entire architecture except in the first layer in the first iteration of MobileNet, which consisted of 28 layers. </span><span class="koboSpan" id="kobo.586.2">This layer is built on factorizing the standard convolutional layer into two layers called the depthwise convolution and the pointwise convolution. </span><span class="koboSpan" id="kobo.586.3">This new two-layer setup is based on the idea that using a standard convolutional filter is expensive to compute. </span><span class="koboSpan" id="kobo.586.4">The depthwise convolutional layer uses one unique filter for one unique input </span><a id="_idIndexMarker293"/><span class="koboSpan" id="kobo.587.1">channel, where each input channel has only one filter. </span><span class="koboSpan" id="kobo.587.2">Having one filter per channel is a unique case of </span><strong class="bold"><span class="koboSpan" id="kobo.588.1">grouped convolutions</span></strong><span class="koboSpan" id="kobo.589.1">, which will be introduced more extensively later. </span><span class="koboSpan" id="kobo.589.2">The output from the depthwise convolutional layer is subsequently passed into a standard 1x1 convolution (that they call pointwise convolution) that combines the information between channels. </span><span class="koboSpan" id="kobo.589.3">Depthwise separable convolutional layers as a whole reduce the computation runtime by eight to nine times compared to the standard convolutional layer operation we introduced earlier while only reducing the accuracy on </span><strong class="source-inline"><span class="koboSpan" id="kobo.590.1">ImageNet</span></strong><span class="koboSpan" id="kobo.591.1"> by 1%. </span><span class="koboSpan" id="kobo.591.2">This breakdown also </span><a id="_idIndexMarker294"/><span class="koboSpan" id="kobo.592.1">reduces the number of parameters by around 5 times. </span><span class="koboSpan" id="kobo.592.2">This process is essentially a kind of factorization. </span><span class="koboSpan" id="kobo.592.3">The depthwise convolutional logical block is depicted in the </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">following figure:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer042">
<span class="koboSpan" id="kobo.594.1"><img alt="Figure 3.14 – Depthwise separable convolutional layer as a logical block" src="image/B18187_03_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.595.1">Figure 3.14 – Depthwise separable convolutional layer as a logical block</span></p>
<p><span class="koboSpan" id="kobo.596.1">MobileNet also made two parameters that can be configured to reduce the model’s size and computational requirements while trading off some metric performance. </span><span class="koboSpan" id="kobo.596.2">The first is a width multiplier that can configure the input and output channels across the entire architecture. </span><span class="koboSpan" id="kobo.596.3">The second is a resolution multiplier between 0 and 1 that applies to the original 224x224 image size to reduce the input and output feature map sizes when passing the image into the network. </span><span class="koboSpan" id="kobo.596.4">These parameters can also be adapted to other architectures if a faster runtime is needed. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.597.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.598.1">.15</span></em><span class="koboSpan" id="kobo.599.1"> shows the MobileNetV1 architecture’s structure and </span><span class="No-Break"><span class="koboSpan" id="kobo.600.1">layer configurations:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer043">
<span class="koboSpan" id="kobo.601.1"><img alt="Figure 3.15 – MobileNetV1 architecture" src="image/B18187_03_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.602.1">Figure 3.15 – MobileNetV1 architecture</span></p>
<p><span class="koboSpan" id="kobo.603.1">MobileNet is </span><a id="_idIndexMarker295"/><span class="koboSpan" id="kobo.604.1">considered to be a unique model family with two improvements that are based on the first version. </span><span class="koboSpan" id="kobo.604.2">They are named MobileNetv2 and MobileNetv3-small and both were introduced </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">in 2019.</span></span></p>
<h3><span class="koboSpan" id="kobo.606.1">Understanding MobileNetV2</span></h3>
<p><span class="koboSpan" id="kobo.607.1">Before we </span><a id="_idIndexMarker296"/><span class="koboSpan" id="kobo.608.1">go through MobileNetV2, let’s define what a </span><strong class="bold"><span class="koboSpan" id="kobo.609.1">bottleneck layer</span></strong><span class="koboSpan" id="kobo.610.1"> is, a core </span><a id="_idIndexMarker297"/><span class="koboSpan" id="kobo.611.1">idea that’s utilized in advancement. </span><span class="koboSpan" id="kobo.611.2">A bottleneck layer is generally a layer with fewer output feature maps compared to the layers before and after the layer. </span><span class="koboSpan" id="kobo.611.3">MobileNetV2 is built upon the idea that bottlenecks are where the information of interest will exist; nonlinearities destroy too much information in bottleneck layers, so a linear layer is applied, finally applying residuals using shortcut connections on the bottleneck layers. </span><span class="koboSpan" id="kobo.611.4">MobileNetV2 builds upon the base depthwise separable building block, adds a linear bottleneck layer without ReLU, and adds residuals </span><a id="_idIndexMarker298"/><span class="koboSpan" id="kobo.612.1">to the bottleneck layers. </span><span class="koboSpan" id="kobo.612.2">This building block is depicted in </span><a id="_idIndexMarker299"/><span class="koboSpan" id="kobo.613.1">the following figure. </span><span class="koboSpan" id="kobo.613.2">This is called the </span><strong class="bold"><span class="koboSpan" id="kobo.614.1">bottleneck inverted </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.615.1">residual block</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer044">
<span class="koboSpan" id="kobo.617.1"><img alt="Figure 3.16 – The bottleneck inverted residual block for MobileNetV2, also called MBConv" src="image/B18187_03_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.618.1">Figure 3.16 – The bottleneck inverted residual block for MobileNetV2, also called MBConv</span></p>
<p><span class="koboSpan" id="kobo.619.1">As for the entire network architecture-wise, all depthwise separable layers from the first MobileNet were replaced with the new block except for the first convolution layer with a 3x3 kernel size and 32 filters, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.620.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.621.1">.16</span></em><span class="koboSpan" id="kobo.622.1">. </span><span class="koboSpan" id="kobo.622.2">One small extra detail is that they used the </span><strong class="bold"><span class="koboSpan" id="kobo.623.1">ReLU6</span></strong><span class="koboSpan" id="kobo.624.1"> activation </span><a id="_idIndexMarker300"/><span class="koboSpan" id="kobo.625.1">function, which is robust to low-precision computation. </span><span class="koboSpan" id="kobo.625.2">The MobileNetV2 architecture used the depicted logical block to create many repeated layer blocks with different settings. </span><span class="koboSpan" id="kobo.625.3">This architecture allowed for an improvement in the performance curve on ImageNet compared to MobileNetV2 at around 5% to 10% at the same FLOPs. </span><span class="koboSpan" id="kobo.625.4">Remember that EfficientNetV2 uses this block and also another version of this block that fuses back the linear bottleneck layer with the filtering layer together. </span><span class="koboSpan" id="kobo.625.5">The purpose of having two layers instead of one was to reduce the number of operations needed but again, for edge devices, the actual latency might differ due to the bottleneck in memory access cost. </span><span class="koboSpan" id="kobo.625.6">Sometimes, using a fused version might result in a faster runtime with the benefits of having more parameters to learn more </span><span class="No-Break"><span class="koboSpan" id="kobo.626.1">information from.</span></span></p>
<h3><span class="koboSpan" id="kobo.627.1">Understanding MobileNetV3-small</span></h3>
<p><span class="koboSpan" id="kobo.628.1">For </span><strong class="bold"><span class="koboSpan" id="kobo.629.1">MobileNetV3-small</span></strong><span class="koboSpan" id="kobo.630.1">, a few </span><a id="_idIndexMarker301"/><span class="koboSpan" id="kobo.631.1">changes were made </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">to MobileNetV2:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.633.1">It used a more </span><a id="_idIndexMarker302"/><span class="koboSpan" id="kobo.634.1">advanced non-linearity called </span><strong class="bold"><span class="koboSpan" id="kobo.635.1">hard-swish</span></strong><span class="koboSpan" id="kobo.636.1">, which improved the top-1 accuracy </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">of </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.638.1">ImageNet</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.640.1">Expensive computation was reduced even further in the initial and last </span><span class="No-Break"><span class="koboSpan" id="kobo.641.1">few layers.</span></span></li>
<li><span class="koboSpan" id="kobo.642.1">For the first layer, as shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.643.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.644.1">.15</span></em><span class="koboSpan" id="kobo.645.1">, 32 filters was reduced to 16 and hard-swish nonlinearity was used to get 2 milliseconds runtime and 10 million </span><span class="No-Break"><span class="koboSpan" id="kobo.646.1">FLOPS savings.</span></span></li>
<li><span class="koboSpan" id="kobo.647.1">For the last few layers, the last 1x1 bottleneck convolution layer is moved to after the final average pooling layer, and the previous bottleneck (1x1) and filtering (3x3) layer are </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">also removed.</span></span></li>
<li><span class="koboSpan" id="kobo.649.1">It used a modified version of network architecture search that is platform-aware called </span><strong class="bold"><span class="koboSpan" id="kobo.650.1">Mnasnet</span></strong><span class="koboSpan" id="kobo.651.1"> and a </span><a id="_idIndexMarker303"/><span class="koboSpan" id="kobo.652.1">post-search layer reduction for latency reduction called </span><strong class="bold"><span class="koboSpan" id="kobo.653.1">NetAdapt</span></strong><span class="koboSpan" id="kobo.654.1"> to automatically </span><a id="_idIndexMarker304"/><span class="koboSpan" id="kobo.655.1">find an optimized architecture based on the building blocks from the MobileNetV1, MobileNetV2, and squeeze and excitation networks while considering latency and accuracy performance. </span><span class="koboSpan" id="kobo.655.2">NetAdapt and MnasNet will be introduced in </span><a href="B18187_07.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.656.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.657.1">, </span><em class="italic"><span class="koboSpan" id="kobo.658.1">Deep Neural </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.659.1">Architecture Search</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.660.1">.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.661.1">MobileNetV3-small ended up achieving a higher top-1 </span><strong class="source-inline"><span class="koboSpan" id="kobo.662.1">ImageNet</span></strong><span class="koboSpan" id="kobo.663.1"> accuracy with the same parameters and FLOPS compared </span><span class="No-Break"><span class="koboSpan" id="kobo.664.1">to MobileNetV2.</span></span></p>
<h4><span class="koboSpan" id="kobo.665.1">Understanding the ShuffleNet architecture family</span></h4>
<p><span class="koboSpan" id="kobo.666.1">ShuffleNet </span><a id="_idIndexMarker305"/><span class="koboSpan" id="kobo.667.1">has two versions, ShuffleNetV1 and ShuffleNetV2, which </span><a id="_idIndexMarker306"/><span class="koboSpan" id="kobo.668.1">we will </span><span class="No-Break"><span class="koboSpan" id="kobo.669.1">discuss separately.</span></span></p>
<p><span class="koboSpan" id="kobo.670.1">ShuffleNetV1, from 2017, reuses a </span><a id="_idIndexMarker307"/><span class="koboSpan" id="kobo.671.1">known variant of convolution called </span><strong class="bold"><span class="koboSpan" id="kobo.672.1">grouped convolutions</span></strong><span class="koboSpan" id="kobo.673.1">, where each convolutional filter is responsible only for a subset of input data channels. </span><span class="koboSpan" id="kobo.673.2">MobileNet uses a special variant of this by using one filter for each channel. </span><span class="koboSpan" id="kobo.673.3">Grouped convolutions save computation costs by operating only on a small subset of input channel features. </span><span class="koboSpan" id="kobo.673.4">However, when stacked together one after another, the information between channels doesn’t interact – ultimately causing accuracy degradation. </span><span class="koboSpan" id="kobo.673.5">ShuffleNetV1 uses channel shuffling as an operation in between stacking multiple grouped convolutional layers to manually shift information without sacrificing FLOPs. </span><span class="koboSpan" id="kobo.673.6">This allows for an efficient and </span><span class="No-Break"><span class="koboSpan" id="kobo.674.1">small network.</span></span></p>
<p><span class="koboSpan" id="kobo.675.1">ShuffleNetV2, from 2018, builds upon ShuffleNetV1 and focuses on the practical runtime efficiency of the architecture in real life while considering factors such as memory access cost, data </span><strong class="bold"><span class="koboSpan" id="kobo.676.1">input-output</span></strong><span class="koboSpan" id="kobo.677.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.678.1">I/O</span></strong><span class="koboSpan" id="kobo.679.1">), and the degree of network parallelism. </span><span class="koboSpan" id="kobo.679.2">The following four design strategies were used to craft the </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">new architecture:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.681.1">Equal channel width from input to output to minimize memory </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">access cost.</span></span></li>
<li><span class="koboSpan" id="kobo.683.1">Decrease group convolution to minimize memory </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">access cost.</span></span></li>
<li><span class="koboSpan" id="kobo.685.1">Reduce network fragmentation to increase parallelism. </span><span class="koboSpan" id="kobo.685.2">One example is the number of convolutional and pooling operations in a single </span><span class="No-Break"><span class="koboSpan" id="kobo.686.1">building block.</span></span></li>
<li><span class="koboSpan" id="kobo.687.1">Reduce element-wise operations such as ReLU as they have heavy memory </span><span class="No-Break"><span class="koboSpan" id="kobo.688.1">access costs:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer045">
<span class="koboSpan" id="kobo.689.1"><img alt="Figure 3.17 – Two building blocks of ShuffleNetv1 on the left and two building blocks of ShuffleNetv2 on the right" src="image/B18187_03_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.690.1">Figure 3.17 – Two building blocks of ShuffleNetv1 on the left and two building blocks of ShuffleNetv2 on the right</span></p>
<p><span class="koboSpan" id="kobo.691.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.692.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.693.1">.17</span></em><span class="koboSpan" id="kobo.694.1">, the first two </span><a id="_idIndexMarker308"/><span class="koboSpan" id="kobo.695.1">structures on the left show the two building blocks of ShuffleNetV1, while the last two structures on the right show the two building blocks </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">of ShuffleNetV2.</span></span></p>
<h2 id="_idParaDest-66"><a id="_idTextAnchor066"/><span class="koboSpan" id="kobo.697.1">Understanding MicroNet, the current state-of-the-art architecture for the edge</span></h2>
<p><span class="koboSpan" id="kobo.698.1">Created in 2021, MicroNet is </span><a id="_idIndexMarker309"/><span class="koboSpan" id="kobo.699.1">the current state of the art in terms of latency and achievable top-1 </span><strong class="source-inline"><span class="koboSpan" id="kobo.700.1">ImageNet</span></strong><span class="koboSpan" id="kobo.701.1"> accuracy performance, for a very low FLOP range of 4 million to 21 million. </span><span class="koboSpan" id="kobo.701.2">The novelty of MicroNet </span><span class="No-Break"><span class="koboSpan" id="kobo.702.1">is two-fold:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.703.1">It introduced factorized versions of the bottleneck/pointwise convolution layer and depthwise </span><a id="_idIndexMarker310"/><span class="koboSpan" id="kobo.704.1">convolutional layers from MobileNet, called </span><strong class="bold"><span class="koboSpan" id="kobo.705.1">micro-factorized convolutions</span></strong><span class="koboSpan" id="kobo.706.1">, in a way that the number of connections/paths for input data to output data is reduced. </span><span class="koboSpan" id="kobo.706.2">This is achieved by using multiple grouped convolutions and some dilated convolutions. </span><span class="koboSpan" id="kobo.706.3">Dilated convolutions are simply convolutions with fixed spacing in the kernels. </span><span class="koboSpan" id="kobo.706.4">Take these techniques as a form of sparse computation and only compute what’s needed most efficiently to ensure minimal input-to-output </span><span class="No-Break"><span class="koboSpan" id="kobo.707.1">path redundancy.</span></span></li>
<li><span class="koboSpan" id="kobo.708.1">It introduced a new </span><a id="_idIndexMarker311"/><span class="koboSpan" id="kobo.709.1">activation function called </span><strong class="bold"><span class="koboSpan" id="kobo.710.1">dynamic shift-max</span></strong><span class="koboSpan" id="kobo.711.1"> that leverages the output of the grouped convolutions in a way that it applies a higher order of non-linearity (two times) and strengthens connections between groups at the same time. </span><span class="koboSpan" id="kobo.711.2">This is implemented by using the grouped outputs of the blocks of squeeze and excitation (produce a single value per channel) as a weighting mechanism to obtain the maximum of a group-based weighted addition. </span><span class="koboSpan" id="kobo.711.3">Take this as an improvement on top of channel shuffling in ShuffleNet. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.712.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.713.1">.18</span></em><span class="koboSpan" id="kobo.714.1"> shows the operation structure of dynamic shift-max for </span><a id="_idIndexMarker312"/><span class="koboSpan" id="kobo.715.1">a single example feature map of 12 channels from the output of four groups using the grouped </span><span class="No-Break"><span class="koboSpan" id="kobo.716.1">convolution operation:</span></span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer046">
<span class="koboSpan" id="kobo.717.1"><img alt="Figure 3.18 – Dynamic shift-max general operation flow" src="image/B18187_03_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.718.1">Figure 3.18 – Dynamic shift-max general operation flow</span></p>
<p><span class="koboSpan" id="kobo.719.1">MicroNet utilizes concepts from ShuffleNet (the channel shuffling mechanism), ResNet (skip connections), SENet (squeeze and excitation networks), and MobileNet (create factorized versions out of the already factorized convolutions) on top of its novelties to create networks that are highly efficient by focusing on the concept of sparsity and improvements in efficient information flow. </span><span class="koboSpan" id="kobo.719.2">The specifics of this network can be overwhelming and, frankly, hard to comprehend, so the information presented here does not contain all </span><span class="No-Break"><span class="koboSpan" id="kobo.720.1">the details:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer047">
<span class="koboSpan" id="kobo.721.1"><img alt="Figure 3.19 – Diagram of three logical blocks, called micro-blocks, that ﻿are used to build different size variants of MicroNets" src="image/B18187_03_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.722.1">Figure 3.19 – Diagram of three logical blocks, called micro-blocks, that are used to build different size variants of MicroNets</span></p>
<p><span class="koboSpan" id="kobo.723.1">However, </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.724.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.725.1">.19</span></em><span class="koboSpan" id="kobo.726.1"> shows how </span><a id="_idIndexMarker313"/><span class="koboSpan" id="kobo.727.1">logical blocks are, again, in the most advanced network today to build networks with different sizes based on the </span><span class="No-Break"><span class="koboSpan" id="kobo.728.1">same ideas.</span></span></p>
<h3><span class="koboSpan" id="kobo.729.1">Summarizing CNN architectures for the edge</span></h3>
<p><span class="koboSpan" id="kobo.730.1">To summarize the architectures for the edge, you now have the intuitive knowledge that was used by </span><a id="_idIndexMarker314"/><span class="koboSpan" id="kobo.731.1">experts in the field to build highly effective CNN architectures capable of running at an amazingly tiny footprint compared to large models such as GPT-3 today. </span><span class="koboSpan" id="kobo.731.2">The following figure shows the overall top-1 </span><strong class="source-inline"><span class="koboSpan" id="kobo.732.1">ImageNet</span></strong><span class="koboSpan" id="kobo.733.1"> accuracy performance versus FLOPS graph of multiple different architecture families for </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">edge computation:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer048">
<span class="koboSpan" id="kobo.735.1"><img alt="Figure 3.20 – Top-1 accuracy performance for edge architecture families below 400 million FLOPS" src="image/B18187_03_20.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.736.1">Figure 3.20 – Top-1 accuracy performance for edge architecture families below 400 million FLOPS</span></p>
<p><span class="koboSpan" id="kobo.737.1">Take these results with a pinch of salt; training strategies might differ between the models and can </span><a id="_idIndexMarker315"/><span class="koboSpan" id="kobo.738.1">affect the achievable top-1 </span><strong class="source-inline"><span class="koboSpan" id="kobo.739.1">ImageNet</span></strong><span class="koboSpan" id="kobo.740.1"> accuracy considerably, along with giving possibly differing results between different random initializations of the different model runs. </span><span class="koboSpan" id="kobo.740.2">Additionally, latency is not directly represented purely by the number of parameters nor the FLOPS but is additionally affected by the memory access cost of individual operations, I/O access cost, and the degree of parallelism of the </span><span class="No-Break"><span class="koboSpan" id="kobo.741.1">different operations.</span></span></p>
<p><span class="koboSpan" id="kobo.742.1">To recap, </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.743.1">Figure 3</span></em></span><em class="italic"><span class="koboSpan" id="kobo.744.1">.21</span></em><span class="koboSpan" id="kobo.745.1"> shows the overall performance plots based on the FLOPS of all the CNN model families that were introduced in </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">this chapter:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer049">
<span class="koboSpan" id="kobo.747.1"><img alt="Figure 3.21 – Overall CNN model family performance in terms of ImageNet top-1 accuracy based on FLOPS" src="image/B18187_03_21.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.748.1">Figure 3.21 – Overall CNN model family performance in terms of ImageNet top-1 accuracy based on FLOPS</span></p>
<p><span class="koboSpan" id="kobo.749.1">Again, note that we should take the results presented here with a pinch of salt as the training techniques that were performed against the </span><strong class="source-inline"><span class="koboSpan" id="kobo.750.1">ImageNet</span></strong><span class="koboSpan" id="kobo.751.1"> dataset are not exactly standardized across </span><a id="_idIndexMarker316"/><span class="koboSpan" id="kobo.752.1">different benchmarks. </span><span class="koboSpan" id="kobo.752.2">Variation in the training technique can result in widely different results and will be covered more extensively in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.753.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.754.1">, </span><em class="italic"><span class="koboSpan" id="kobo.755.1">Exploring Supervised Deep Learning</span></em><span class="koboSpan" id="kobo.756.1">, and </span><a href="B18187_09.xhtml#_idTextAnchor149"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.757.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.758.1">, </span><em class="italic"><span class="koboSpan" id="kobo.759.1">Exploring Unsupervised Deep Learning</span></em><span class="koboSpan" id="kobo.760.1">. </span><span class="koboSpan" id="kobo.760.2">Another important thing to note is that even though </span><strong class="source-inline"><span class="koboSpan" id="kobo.761.1">ImageNet</span></strong><span class="koboSpan" id="kobo.762.1"> is considered to be a large enough image dataset to be considered as a benchmark, maintain a level of skepticism toward the results as the data itself has been proven to have noisy labels with systematic errors in some cases. </span><span class="koboSpan" id="kobo.762.2">A corrected form of </span><strong class="source-inline"><span class="koboSpan" id="kobo.763.1">ImageNet</span></strong><span class="koboSpan" id="kobo.764.1"> has been published called </span><strong class="source-inline"><span class="koboSpan" id="kobo.765.1">ImageNet Real</span></strong><span class="koboSpan" id="kobo.766.1"> but not all models are benchmarked or pre-trained against it. </span><span class="koboSpan" id="kobo.766.2">Train it on your dataset to be 100% sure which architecture, when pre-trained on certain datasets, performs better! </span><span class="koboSpan" id="kobo.766.3">Additionally, the FLOPS indicator does not fully represent the actual latency of the model, which can vary widely based on how the code is structured, how the model is distributed through multiple devices, how many GPUs or CPUs are available, and how parallel the model </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">architecture is.</span></span></p>
<h1 id="_idParaDest-67"><a id="_idTextAnchor067"/><span class="koboSpan" id="kobo.768.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.769.1">CNNs are the go-to model for capturing patterns in image data. </span><span class="koboSpan" id="kobo.769.2">The handpicked architectures that were introduced in this chapter are the core backbones that can be subsequently utilized as a base for solving more custom downstream tasks such as image object detection and </span><span class="No-Break"><span class="koboSpan" id="kobo.770.1">image generation.</span></span></p>
<p><span class="koboSpan" id="kobo.771.1">The CNNs that were covered here will be used practically in later chapters as a basis to help you learn other deep learning-based knowledge. </span><span class="koboSpan" id="kobo.771.2">Take your time and look into how different architectures are implemented in a deep learning library offline in this book’s GitHub repository; we won’t be presenting the actual implementation code here. </span><span class="koboSpan" id="kobo.771.3">Now that we have covered CNNs in intermediate to low-level detail, in the next chapter, we’ll shift gears and look at recurrent </span><span class="No-Break"><span class="koboSpan" id="kobo.772.1">neural networks.</span></span></p>
</div>
</body></html>