<html><head></head><body>
		<div>
			<div id="_idContainer044" class="Content">
			</div>
		</div>
		<div id="_idContainer045" class="Content">
			<h1 id="_idParaDest-61"><a id="_idTextAnchor062"/>3. Neural Networks</h1>
		</div>
		<div id="_idContainer092" class="Content">
			<p>We learned about perceptrons in the previous chapter, and there is both good news and bad news. The good news is that perceptrons are likely to represent complicated functions. For example, the perceptron can (theoretically) represent complicated processes performed by a computer, as described in the previous chapter. The bad news is that weights must be defined manually first before the appropriate weights are determined in order to meet the expected inputs and outputs. In the previous chapter, we used the truth tables with AND and OR gates to determine the appropriate weights manually.</p>
			<p>Neural networks exist to solve the bad news. More specifically, one important property of a neural network is that it can learn appropriate weight parameters from data automatically. This chapter provides an overview of neural networks and focuses on what distinguishes them. The next chapter will describe how it learns weight parameters from data.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor063"/>From Perceptrons to Neural Networks</h2>
			<p>A neural network is similar to the perceptron described in the previous chapter in many ways. How a neural network works, as well as how it differs from a perceptron, will be described in this section. </p>
			<h3 id="_idParaDest-63"><a id="_idTextAnchor064"/>Neural Network Example</h3>
			<p><em class="italics">Figure 3.1</em> shows a neural network example. Here, the left column is called an <strong class="bold">input layer</strong>, the right column is called an <strong class="bold">output layer</strong>, and the center column is called the <strong class="bold">middle layer</strong>. The middle layer is also known as a hidden layer. "Hidden" means that the neurons in the hidden layer are invisible (unlike those in the input and output layers). In this book, we'll call the layers layer 0, layer 1, and layer 2 from the input layer to the output layer (layer numbers start from layer 0 because doing so is convenient when the layers are implemented in Python later). In <em class="italics">Figure 3.1</em>, layer 0 is the input layer, layer 1 is the middle layer, and layer 2 is the output layer:</p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/fig03_1.jpg" alt="Figure 3.1: Neural network example&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.1: Neural network example</h6>
			<h4>Note</h4>
			<p class="callout">Although the network in <em class="italics">Figure 3.1</em> consists of three layers, we call it a "two-layer network" because it has two layers with weights. Some books call it a "three-layer network" based on the number of layers that constitute the network, but in this book, the network name is based on the number of layers that have weights (that is, the total number of input, hidden, and output layers, minus 1).</p>
			<p>The neural network in <em class="italics">Figure 3.1</em> is similar to the perceptron in the previous chapter in terms of its shape. In fact, in terms of how neurons are connected, it is no different from the perceptron we saw in the previous chapter. So, how are signals transmitted in a neural network?</p>
			<h3 id="_idParaDest-64"><a id="_idTextAnchor065"/>Reviewing the Perceptron</h3>
			<p>To answer this question, we first need to review the perceptron. Consider a network that has the following structure:</p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/fig03_2.jpg" alt="Figure 3.2: Reviewing the perceptron&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.2: Reviewing the perceptron</h6>
			<p><em class="italics">Figure 3.2</em> shows a perceptron that receives two input signals (x<span class="P---Subscript">1</span> and x<span class="P---Subscript">2</span>) and outputs y. As described earlier, the perceptron in <em class="italics">Figure 3.2</em> is represented by equation (3.1):</p>
			<table id="table001-1" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer048">
									<img src="image/Figure_3.2a.png" alt="4"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(3.1)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Here, b is a parameter called "bias" and controls how easily the neuron fires. Meanwhile, w<span class="P---Subscript">1</span> and w<span class="P---Subscript">2</span> are the parameters that indicate the "weights" of individual signals to control their importance.</p>
			<p>You may have noticed that the network in <em class="italics">Figure 3.2</em> has no bias, b. We can indicate the bias shown in <em class="italics">Figure 3.3</em>, if we want to. A signal of weight b and input 1 has been added in <em class="italics">Figure 3.3</em>. This perceptron receives three signals (x<span class="P---Subscript">1</span>, x<span class="P---Subscript">2</span>, and 1) as the inputs to the neuron, and multiplies the signals by each weight before transmitting them to the next neuron. The next neuron sums the weighted signals and then outputs 1 if the sum exceeds 0. It outputs 0 if it doesn't. The neuron in the following diagram is shown in solid gray to distinguish it from other neurons. This is because the input signal of the bias is always 1:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/fig03_3.jpg" alt="Figure 3.3: Showing the bias explicitly&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.3: Showing the bias explicitly</h6>
			<p>Now, we want to simplify equation (3.1). To do that, we use a single function to express the condition, where <strong class="inline">1</strong> is the output if the sum exceeds 0, and 0 is the output if it does not. Here, we will introduce a new function, <em class="italics">h</em>(<em class="italics">x</em>), and rewrite equation (3.1) to equations (3.2) and (3.3) shown here:</p>
			<table id="table002-1" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer050">
									<img src="image/Figure_3.3a.png" alt="5"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(3.2)</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer051">
									<img src="image/Figure_3.3b.png" alt="5a"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(3.3)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Equation (3.2) indicates that the <em class="italics">h</em>(<em class="italics">x</em>) function converts the sum of input signals into the output, y. The <em class="italics">h</em>(<em class="italics">x</em>) function represented by equation (3.3) returns 1 if the input exceeds 0 and returns 0 if it does not. Therefore, equations (3.2) and (3.3) operate in the same way as equation (3.1).</p>
			<h3 id="_idParaDest-65"><a id="_idTextAnchor066"/>Introducing an Activation Function</h3>
			<p>The <em class="italics">h</em>(<em class="italics">x</em>) function that appears here is generally called an <strong class="bold">activation function</strong>. It converts the sum of input signals into an output signal. As the name "activation" indicates, the activation function determines how the sum of the input signals activates (that is, how it fires).</p>
			<p>Now, we can rewrite equation (3.2) again. Equation (3.2) performs two processes: the weighted input signals are summed, and the sum is converted by the activation function. Therefore, you can divide equation (3.2) into the following two equations:</p>
			<table id="table003-1" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer052">
									<img src="image/Figure_3.3c.png" alt="7"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(3.4)</p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer053">
									<img src="image/Figure_3.3d.png" alt="8"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(3.5)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>In equation (3.4), the sum of the weighted input signals and biases becomes a. In equation (3.5), a is converted by <em class="italics">h()</em>, and <em class="italics">y</em> is output.</p>
			<p>So far, a neuron has been shown as one circle. <em class="italics">Figure 3.4</em> shows equations (3.4) and (3.5) explicitly:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/fig03_4.jpg" alt="Figure 3.4: Showing the process performed by the activation function explicitly&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.4: Showing the process performed by the activation function explicitly</h6>
			<p><em class="italics">Figure 3.4</em> explicitly shows the process that is performed by the activation function in the circle of the neuron. We can clearly see that the sum of the weighted signals becomes node <em class="italics">a</em> and that it is converted into node <em class="italics">y</em> by the activation function, <em class="italics">h</em>(). In this book, the terms "neuron" and "node" are used interchangeably. Here, circles <em class="italics">a</em> and <em class="italics">y</em> are called "nodes," which are used in the same sense as "neurons" that were used earlier.</p>
			<p>We will continue to show a neuron as one circle, as shown on the left of <em class="italics">Figure 3.5</em>. In this book, we will also show the activation process (to the right of <em class="italics">Figure 3.5</em>), if the behavior of the neural network can be clarified:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/fig03_5.jpg" alt="Figure 3.5: The left-hand image is an ordinary image that shows a neuron, while the right-hand image explicitly shows the process of activation in a neuron (a is the sum of input signals, h() is the activation function, and y is the output)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.5: The left-hand image is an ordinary image that shows a neuron, while the right-hand image explicitly shows the process of activation in a neuron (a is the sum of input signals, h() is the activation function, and y is the output)</h6>
			<p>Now, let's focus on the activation function, which serves as the bridge from a perceptron to a neural network.</p>
			<h4>Note</h4>
			<p class="callout">In this book, the algorithm indicated by the word "perceptron" is not strictly defined. Generally, a "simple perceptron" is a single-layer network where a step function that changes the output values at a threshold is used as the activation function. A "multilayer perceptron" usually means a neural network that contains multiple layers and uses a smooth activation function, such as a sigmoid function.</p>
			<h2 id="_idParaDest-66"><a id="_idTextAnchor067"/>Activation Function</h2>
			<p>The activation function represented by equation (3.3) changes output values at a threshold and is called a "step function" or a "staircase function." Therefore, we can say, "a perceptron uses a step function as the activation function." In other words, a perceptron chooses a "step function" as the activation function from many candidate functions. When a perceptron uses a step function as the activation function, what happens if a function other than a step function is used as the activation function? Well, by changing the activation function from a step function to another function, we can move to the world of a neural network. The next section will introduce an activation function for a neural network.</p>
			<h3 id="_idParaDest-67"><a id="_idTextAnchor068"/>Sigmoid Function</h3>
			<p>One of the activation functions often used in neural networks is the <strong class="bold">sigmoid function</strong>,  represented by equation (3.6):</p>
			<table id="table004" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer056">
									<img src="image/Figure_3.5a.png" alt="9"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(3.6)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>exp(-<em class="italics">x</em>) in equation (3.6) indicates <em class="italics">e</em><span class="P---Superscript">-x</span>. The real number, <em class="italics">e</em>, is Napier's number, 2.7182... The sigmoid function represented by equation (3.6) seems complicated, but it is only a "function." A function is a converter that returns output when input is provided. For example, when a value such as 1.0 and 2.0 is provided to the sigmoid function, values such as <em class="italics">h</em>(1.0) = 0.731… and <em class="italics">h</em>(2.0) = 0.880… are returned.</p>
			<p>In a neural network, a sigmoid function is often used as the activation function to convert signals, and the converted signals are transmitted to the next neuron. In fact, the main difference between the perceptron described in the previous chapter and the neural network described here is the activation function. Other aspects, such as the structure where neurons are connected in multiple layers and how signals are transmitted, are basically the same as they are for perceptrons. Now, let's look more closely at a sigmoid function (used as the activation function) by comparing it with a step function.</p>
			<h3 id="_idParaDest-68"><a id="_idTextAnchor069"/>Implementing a Step Function</h3>
			<p>Here, we will use Python to show the graph of a step function. As represented by equation (3.3), the step function outputs 1 when the input exceeds 0 and outputs 0 if it does not. The following shows a simple implementation of the step function:</p>
			<p class="source-code">    def step_function(x):</p>
			<p class="source-code">        if x &gt; 0:</p>
			<p class="source-code">            return 1</p>
			<p class="source-code">        else:</p>
			<p class="source-code">            return 0</p>
			<p>This implementation is simple and easy to understand, but it only takes a real number (a floating-point number) as argument <strong class="inline">x</strong>. Therefore, <strong class="inline">step_function(3.0)</strong> is allowed. However, the function cannot take a NumPy array as the argument. Thus, <strong class="inline">step_function(np.array([1.0, 2.0]))</strong> is not allowed. Here, we want to change to the future implementation so that it can take a NumPy array. For that purpose, we can write an implementation like the following:</p>
			<p class="source-code">    def step_function(x):</p>
			<p class="source-code">        y = x &gt; 0</p>
			<p class="source-code">        return y.astype(np.int)</p>
			<p>Although the preceding function contains only two lines, it may be a little difficult to understand because it uses a useful "trick" from NumPy. Here, the following example from the Python interpreter is used to describe what kind of trick is used. In this example, the NumPy <strong class="inline">x</strong> array is provided. For the NumPy array, a comparison operator is conducted:</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; x = np.array([-1.0, 1.0, 2.0])</p>
			<p class="source-code">&gt;&gt;&gt; x</p>
			<p class="source-code">array([-1., 1., 2.])</p>
			<p class="source-code">&gt;&gt;&gt; y = x &gt; 0</p>
			<p class="source-code">&gt;&gt;&gt; y</p>
			<p class="source-code">array([False, True, True], dtype=bool)</p>
			<p>When a <em class="italics">greater than</em> comparison is conducted for a NumPy array, each element in the array is compared to generate a Boolean array. Here, each element in the <strong class="inline">x</strong> array is converted into <strong class="inline">True</strong> when it exceeds 0 or into <strong class="inline">False</strong> when it does not. Then, the new array, <strong class="inline">y</strong>, is generated.</p>
			<p>The <strong class="inline">y</strong> array is Boolean, and the desired step function must return <strong class="inline">0</strong> or <strong class="inline">1</strong> of the <strong class="inline">int</strong> type. Therefore, we convert the type of elements of array <strong class="inline">y</strong> from Boolean into <strong class="inline">int</strong>:</p>
			<p class="source-code">&gt;&gt;&gt; y = y.astype(np.int)</p>
			<p class="source-code">&gt;&gt;&gt; y</p>
			<p class="source-code">array([0, 1, 1])</p>
			<p>As shown here, the <strong class="inline">astype()</strong> method is used to convert the type of the NumPy array. The <strong class="inline">astype()</strong> method takes the desired type (<strong class="inline">np.int</strong>, in this example) as the argument. In Python, <strong class="inline">True</strong> is converted into <strong class="inline">1</strong>, and <strong class="inline">False</strong> is converted into <strong class="inline">0</strong> by converting the Boolean type into the int type. The preceding code explains NumPy's "trick" that's used when implementing the step function.</p>
			<h3 id="_idParaDest-69"><a id="_idTextAnchor070"/>Step Function Graph</h3>
			<p>Now, let's draw the graph of the step function we defined previously. To do that, we need to use the Matplotlib library:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">import matplotlib.pylab as plt</p>
			<p class="source-code">def step_function(x):</p>
			<p class="source-code">    return np.array(x &gt; 0, dtype=np.int)</p>
			<p class="source-code">x = np.arange(-5.0, 5.0, 0.1)</p>
			<p class="source-code">y = step_function(x)</p>
			<p class="source-code">plt.plot(x, y)</p>
			<p class="source-code">plt.ylim(-0.1, 1.1) # Specify the range of the y-axis</p>
			<p class="source-code">plt.show()</p>
			<p><strong class="inline">np.arange(-5.0, 5.0, 0.1)</strong> generates a NumPy array containing values from <strong class="inline">-5.0</strong> to <strong class="inline">5.0</strong> in <strong class="inline">0.1</strong> steps, <strong class="inline">([-5.0, -4.9, …, 4.9]). step_function()</strong> takes a NumPy array as the argument. It executes the step function for each element in the array and returns an array as the result. When these <strong class="inline">x</strong> and <strong class="inline">y</strong> arrays are plotted, the graph shown in <em class="italics">Figure 3.6</em> is displayed:</p>
			<div>
				<div id="_idContainer057" class="IMG---Figure">
					<img src="image/fig03_6.jpg" alt="Figure 3.6: Step function graph&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.6: Step function graph</h6>
			<p>As shown in <em class="italics">Figure 3.6</em>, the output of the step function changes from 0 to 1 (or 1 to 0) at the threshold of 0. A step function is sometimes called a "staircase function" because the output represents the steps of stairs, as shown in <em class="italics">Figure 3.6</em>.</p>
			<h3 id="_idParaDest-70"><a id="_idTextAnchor071"/>Implementing a Sigmoid Function</h3>
			<p>Now, let's implement a sigmoid function. We can write the sigmoid function of equation (3.6) in Python as follows:</p>
			<p class="source-code">def sigmoid(x):</p>
			<p class="source-code">    return 1 / (1 + np.exp(-x))</p>
			<p>Here, <strong class="inline">np.exp(-x)</strong> corresponds to <strong class="inline">exp(−x)</strong> in the equation. This implementation is not very difficult. The correct results are returned even when a NumPy array is provided as the <strong class="inline">x</strong> argument. When this sigmoid function receives a NumPy array, it calculates correctly, as shown here:</p>
			<p class="source-code">&gt;&gt;&gt; x = np.array([-1.0, 1.0, 2.0])</p>
			<p class="source-code">&gt;&gt;&gt; sigmoid(x)</p>
			<p class="source-code">array([0.26894142,  0.73105858,  0.88079708])</p>
			<p>The implementation of the sigmoid function supports a NumPy array due to NumPy's broadcasting (refer to the <em class="italics">Broadcasting</em> section in <em class="italics">Chapter 1</em>, <em class="italics">Introduction to Python</em> for details). When an operation is performed on a scalar and a NumPy array, thanks to the broadcast, the operation is performed between the scalar and each element of the NumPy array.</p>
			<p class="source-code">&gt;&gt;&gt; t = np.array([1.0, 2.0, 3.0])</p>
			<p class="source-code">&gt;&gt;&gt; 1.0 + t</p>
			<p class="source-code">array([2., 3., 4.])</p>
			<p class="source-code">&gt;&gt;&gt; 1.0 / t</p>
			<p class="source-code">array([1.  ,  0.5  ,   0.33333333])</p>
			<p>In the preceding example, arithmetic operations (such as <strong class="inline">+</strong> and <strong class="inline">/</strong>) are performed between the scalar value (1.0 here) and the NumPy array. As a result, the scalar value and each element of the NumPy array is used in the operations, and the results are output as a NumPy array. In this implementation of the sigmoid function, because <strong class="inline">np.exp(-x)</strong> generates a NumPy array, <strong class="inline">1 / (1 + np.exp(-x))</strong> also uses each element of the NumPy array for the operation.</p>
			<p>Now, let's draw the graph of the sigmoid function. The code for drawing is almost the same as the code for the step function. The only difference is that the function that outputs <strong class="inline">y</strong> is changed to the sigmoid function:</p>
			<p class="source-code">x = np.arange(-5.0, 5.0, 0.1)</p>
			<p class="source-code">y = sigmoid(x)</p>
			<p class="source-code">plt.plot(x, y)</p>
			<p class="source-code">plt.ylim(-0.1, 1.1) # Specify the range of the y-axis</p>
			<p class="source-code">plt.show()</p>
			<p>The preceding code creates the graph shown in <em class="italics">Figure 3.7</em> when it is executed:</p>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/fig03_7.jpg" alt="Figure 3.7: Graph of the sigmoid function&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.7: Graph of the sigmoid function</h6>
			<h3 id="_idParaDest-71"><a id="_idTextAnchor072"/>Comparing the Sigmoid Function and the Step Function</h3>
			<p>Let's compare the sigmoid function and the step function. <em class="italics">Figure 3.8</em> shows the sigmoid function and the step function. In what ways are the two functions different? In what ways are they alike? We can consider <em class="italics">Figure 3.8</em> and think about this for a moment.</p>
			<p>When you look at <em class="italics">Figure 3.8</em>, you may notice the difference in smoothness. The sigmoid function is a smooth curve, where the output changes continuously based on the input. On the other hand, the output of the step function changes suddenly at <strong class="inline">0</strong>. This smoothness of the sigmoid function has an important meaning when training neural networks:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/fig03_8.jpg" alt="Figure 3.8: Step function and sigmoid function (the dashed line shows the step function)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.8: Step function and sigmoid function (the dashed line shows the step function)</h6>
			<p>In connection with the smoothness mentioned previously, they are different in that the step function returns only 0 or 1, while the sigmoid function returns real numbers such as 0.731... and 0.880... That is, binary signals of 0 and 1 flow among neurons in a perceptron, while signals of continuous real numbers flow in a neural network.</p>
			<p>When we use "water" to describe the behaviors of these two functions, the step function can be compared to a "shishi-odoshi" (a bamboo tube that clacks against a stone after water flows out of a tube), and the sigmoid function can be compared to a "waterwheel." The step function conducts two actions: it drains or stores water (0 or 1), while the sigmoid function controls the flow of water like a "waterwheel" based on the amount of water that reaches it.</p>
			<p>Now, consider the ways in which  the step and sigmoid functions are similar. They are different in "smoothness," but you may notice that they are similar in shape when you view <em class="italics">Figure 3.8</em> from a broader perspective. Actually, both of them output a value near/of 0 when the input is small, and, as the input becomes larger, the output approaches/reaches 1. The step and sigmoid functions output a large value when the input signal contains important information and output a small value when it don't. They are also similar in that they output a value between 0 and 1, no matter how small or large the value of the input signal is.</p>
			<h3 id="_idParaDest-72"><a id="_idTextAnchor073"/>Nonlinear Function</h3>
			<p>The step and sigmoid functions are similar in another way. One important similarity is that they are both <strong class="bold">nonlinear functions</strong>. The sigmoid function is represented by a curve, while the step function is represented by straight lines that look like stairs. They are both classified as nonlinear functions.</p>
			<h4>Note</h4>
			<p class="callout">The terms "nonlinear function" and "linear function" often appear when an activation function is used. A function is a "converter" that returns a value when a value is provided. A function that outputs the input values multiplied by a constant is called a linear function (represented by the equation <em class="italics">h</em>(<em class="italics">x</em>) = <em class="italics">cx</em>, where <em class="italics">c</em> is a constant). Therefore, the graph of a linear function is a straight line. Meanwhile, as its name suggests, the graph of a nonlinear function is not a simple straight line.</p>
			<p>In a neural network, a nonlinear function must be used as the activation function. In other words, a linear function may not be used as the activation function. Why may a linear function not be used? The reason is that increasing the number of layers in a neural network becomes useless if a linear function is used.</p>
			<p>The problem with a linear function is caused by the fact that a "network without a hidden layer" that does the same task always exists, no matter how many layers are added. To understand this specifically (and somewhat intuitively), let's consider a simple example. Here, a linear function, <em class="italics">h</em>(<em class="italics">x</em>) = <em class="italics">cx</em>, is used as the activation function and the calculation of <em class="italics">y</em>(<em class="italics">x</em>) = <em class="italics">h</em>(<em class="italics">h</em>(<em class="italics">h</em>(<em class="italics">x</em>))) is performed as in a three-layer network. It contains multiplications of y(x) = c×c×c×x, and the same operation can be represented by one multiplication of <em class="italics">y</em>(<em class="italics">x</em>) = <em class="italics">ax</em> (where <em class="italics">a</em> = <em class="italics">c</em><span class="P---Superscript">3</span>). Thus, it can be represented by a network without a hidden layer. As this example shows, using a linear function offsets the advantage of multiple layers. Therefore, to take advantage of multiple layers, a nonlinear function must be used as the activation function.</p>
			<h3 id="_idParaDest-73"><a id="_idTextAnchor074"/>ReLU Function</h3>
			<p>Thus far, we have learned about step and sigmoid functions as activation functions. While a sigmoid function has been used for a long time in the history of neural networks, a function called <strong class="bold">Rectified Linear Unit</strong> (<strong class="bold">ReLU</strong>) is mainly used these days.</p>
			<p>If the input exceeds 0, the ReLU function outputs the input as it is. If the input is equal to or smaller than 0, it outputs 0 (see <em class="italics">Figure 3.9</em>):</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/fig03_9.jpg" alt="Figure 3.9: ReLU function&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.9: ReLU function</h6>
			<p>Equation (3.7) represents the ReLU function:</p>
			<table id="table005" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer061">
									<img src="image/Figure_3.9a.png" alt="10"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(3.7)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>As the graph and the equation shows, the ReLU function is very simple. Therefore, we can also implement it easily, as shown here:</p>
			<p class="source-code">def relu(x):</p>
			<p class="source-code">    return np.maximum(0, x)</p>
			<p>Here, NumPy's maximum function is used. It outputs the larger of the input values.</p>
			<p>While a sigmoid function will be used as the activation function later in this chapter, the ReLU function is mainly used in the latter half of this book.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor075"/>Calculating Multidimensional Arrays</h2>
			<p>If you learn how to calculate multidimensional arrays using NumPy, you will be able to implement a neural network efficiently. First, we will look at how to use NumPy to calculate multidimensional arrays. Then, we will implement a neural network.</p>
			<h3 id="_idParaDest-75"><a id="_idTextAnchor076"/>Multidimensional Arrays</h3>
			<p>Simply put, a multidimensional array is "a set of numbers" arranged in a line, in a rectangle, in three dimensions, or (more generally) in N dimensions,  called a multidimensional array. Let's use NumPy to create a multidimensional array. First, we will create a one-dimensional array, as described so far:</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; A = np.array([1, 2, 3, 4])</p>
			<p class="source-code">&gt;&gt;&gt; print(A)</p>
			<p class="source-code">[1 2 3 4]</p>
			<p class="source-code">&gt;&gt;&gt; np.ndim(A)</p>
			<p class="source-code">1</p>
			<p class="source-code">&gt;&gt;&gt; A.shape</p>
			<p class="source-code">(4,)</p>
			<p class="source-code">&gt;&gt;&gt; A.shape[0]</p>
			<p class="source-code">4</p>
			<p>As shown here, you can use the <strong class="inline">np.ndim()</strong> function to obtain the number of dimensions of an array. You can also use the instance variable, <strong class="inline">shape</strong>, to obtain the shape of the array. The preceding example shows that <strong class="inline">A</strong> is a one-dimensional array consisting of four elements. Please note that the result of <strong class="inline">A.shape</strong> is a tuple. This is because the result is returned in the same format both for a one-dimensional array and for a multidimensional array. For example, a (4,3) tuple is returned for a two-dimensional array, and a (4,3,2) tuple is returned for a three-dimensional one. Therefore, a tuple is also returned for a one-dimensional array. Now, let's create a two-dimensional array:</p>
			<p class="source-code">&gt;&gt;&gt; B = np.array([[1,2], [3,4], [5,6]])</p>
			<p class="source-code">&gt;&gt;&gt; print(B)</p>
			<p class="source-code">[[1 2]</p>
			<p class="source-code">[3 4]</p>
			<p class="source-code">[5 6]]</p>
			<p class="source-code">&gt;&gt;&gt; np.ndim(B)</p>
			<p class="source-code">2</p>
			<p class="source-code">&gt;&gt;&gt; B.shape</p>
			<p class="source-code">(3, 2)</p>
			<p>Here, a 3x2 array, B, is created. A 3x2 array means that it has three elements in the first dimension and two elements in the next dimension. The first dimension is dimension 0, and the next dimension is dimension 1 (an index starts from 0 in Python). A two-dimensional array is called a matrix. As shown in <em class="italics">Figure 3.10</em>, a horizontal sequence in an array is called a <strong class="bold">row</strong>, while a vertical sequence is called a <strong class="bold">column</strong>:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/fig03_10.jpg" alt="Figure 3.10: A horizontal sequence is called a &quot;row,&quot; and a vertical one is called a &quot;column&quot;&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.10: A horizontal sequence is called a "row," and a vertical one is called a "column"</h6>
			<h3 id="_idParaDest-76"><a id="_idTextAnchor077"/>Matrix Multiplication</h3>
			<p>Now, consider the product of matrices (two-dimensional arrays). For 2x2 matrices, matrix multiplication is calculated as shown in <em class="italics">Figure 3.11</em> (defined as the calculation in this procedure):</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/fig03_11.jpg" alt="Figure 3.11: Calculating matrix multiplication &#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.11: Calculating matrix multiplication </h6>
			<p>As this example indicates, matrix multiplication is calculated by multiplying the elements between the (horizontal) rows of the left matrix and the (vertical) columns of the right matrix and adding the results. The calculation result is stored as the elements of a new multidimensional array. For example, the result between A's first row and B's first column becomes the first element in the first row, while the result between A's second row and B's first column becomes the first element in the second row. In this book, a matrix in an equation is shown in bold. For example, a matrix is shown as <strong class="inline">A</strong> to differentiate it from a scalar value (for example, a or b) with one element. This calculation is implemented in Python as follows:</p>
			<p class="source-code">&gt;&gt;&gt; A = np.array([[1,2], [3,4]])</p>
			<p class="source-code">&gt;&gt;&gt; A.shape</p>
			<p class="source-code">(2, 2)</p>
			<p class="source-code">&gt;&gt;&gt; B = np.array([[5,6], [7,8]])</p>
			<p class="source-code">&gt;&gt;&gt; B.shape</p>
			<p class="source-code">(2, 2)</p>
			<p class="source-code">&gt;&gt;&gt; np.dot(A, B)</p>
			<p class="source-code">array([[19, 22],</p>
			<p class="source-code">    [43, 50]])</p>
			<p>A and B are 2x2 matrices. NumPy's <strong class="inline">np.dot()</strong> function is used to calculate the product of matrices A and B (the "dot" here indicates a dot product). <strong class="inline">np.dot (dot product)</strong> calculates the inner product of vectors for one-dimensional arrays and matrix multiplication for two-dimensional arrays. You should note that <strong class="inline">np.dot(A, B)</strong> and <strong class="inline">np.dot(B, A)</strong> can return different values. Unlike regular operations (+, *, and so on), the product of matrices becomes different when the order of operands (A and B) is different.</p>
			<p>The preceding example shows the product of 2x2 matrices. You can also calculate the product of matrices in different shapes. For example, the product of 2x3 and 3x2 matrices can be implemented in Python as follows:</p>
			<p class="source-code">&gt;&gt;&gt; A = np.array([[1,2,3], [4,5,6]])</p>
			<p class="source-code">&gt;&gt;&gt; A.shape</p>
			<p class="source-code">(2, 3)</p>
			<p class="source-code">&gt;&gt;&gt; B = np.array([[1,2], [3,4], [5,6]])</p>
			<p class="source-code">&gt;&gt;&gt; B.shape</p>
			<p class="source-code">(3, 2)</p>
			<p class="source-code">&gt;&gt;&gt; np.dot(A, B)</p>
			<p class="source-code">array([[22, 28],</p>
			<p class="source-code">    [49, 64]])</p>
			<p>The preceding code shows how the product of the 2x3 matrix A and the 3x2 matrix B can be implemented. Here, you must be careful about the "shapes of matrices." Specifically, the number of elements (number of columns) in dimension 1 of matrix A must be the same as the number of elements (number of rows) in dimension 0 of matrix B. Actually, in the preceding example, matrix A is 2x3, and matrix B is 3x2. The number of elements in dimension 1 of matrix A (3) is the same as the number of elements in dimension 0 of matrix B (3). If they are different, the product of the matrices cannot be calculated. So then, if you try to calculate the product of the 2x3 matrix A and the 2x2 matrix C in Python, the following error occurs:</p>
			<p class="source-code">&gt;&gt;&gt; C = np.array([[1,2],  [3,4]])</p>
			<p class="source-code">&gt;&gt;&gt; C.shape</p>
			<p class="source-code">(2, 2)</p>
			<p class="source-code">&gt;&gt;&gt; A.shape</p>
			<p class="source-code">(2, 3)</p>
			<p class="source-code">&gt;&gt;&gt; np.dot(A, C)</p>
			<p>Traceback (most recent call last): </p>
			<p class="source-code">    File "&lt;stdin&gt;", line 1, in &lt;module&gt;</p>
			<p class="source-code">ValueError: shapes (2,3) and (2,2) not aligned: 3 (dim 1) != 2 (dim 0)</p>
			<p>This error says that dimension 1 of matrix A and dimension 0 of matrix C are different in terms of the numbers of their elements (the index of a dimension starts from zero). In other words, to calculate the product of a multidimensional array, the number of elements in the corresponding dimensions of two matrices must be the same. Because this is an important point, let's check it again in <em class="italics">Figure 3.12</em>:</p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/fig03_12.jpg" alt="Figure 3.12: The number of elements in corresponding dimensions must be the &#13;&#10;same for matrix multiplication&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.12: The number of elements in corresponding dimensions must be the same for matrix multiplication</h6>
			<p><em class="italics">Figure 3.12</em> shows an example of the product of the 3x2 matrix A and the 2x4 matrix B, resulting in the 3x4 matrix C. As we can see, the number of elements in the corresponding dimensions of matrices A and B must be the same. The resulting matrix, C, consists of as many rows as matrix A and as many columns as matrix B. This is also important.</p>
			<p>Even when A is a two-dimensional matrix and B is a one-dimensional array, the same principle (that the number of elements in the corresponding dimensions must be the same) applies, as shown in <em class="italics">Figure 3.13</em>:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/fig03_13.jpg" alt="Figure 3.13: The number of elements in the corresponding dimensions must be the same, even when A is a two-dimensional matrix and B is a one-dimensional array &#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.13: The number of elements in the corresponding dimensions must be the same, even when A is a two-dimensional matrix and B is a one-dimensional array </h6>
			<p>The sample in <em class="italics">Figure 3.13</em> can be implemented in Python as follows:</p>
			<p class="source-code">&gt;&gt;&gt; A = np.array([[1,2], [3, 4], [5,6]])</p>
			<p class="source-code">&gt;&gt;&gt; A.shape</p>
			<p class="source-code">(3, 2)</p>
			<p class="source-code">&gt;&gt;&gt; B = np.array([7,8])</p>
			<p class="source-code">&gt;&gt;&gt; B.shape</p>
			<p class="source-code">(2,)</p>
			<p class="source-code">&gt;&gt;&gt; np.dot(A,  B)</p>
			<p class="source-code">array([23, 53, 83])</p>
			<h3 id="_idParaDest-77"><a id="_idTextAnchor078"/>Matrix Multiplication in a Neural Network</h3>
			<p>Now, let's use NumPy matrices to implement a neural network, as shown in <em class="italics">Figure 3.14</em>. Let's assume that the neural network only has weights. Bias and an activation function have been omitted. </p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/fig03_14.jpg" alt="Figure 3.14: Using matrix multiplication to calculate a neural network&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.14: Using matrix multiplication to calculate a neural network</h6>
			<p>In this implementation, we must be careful about the shapes of <strong class="inline">X</strong>, <strong class="inline">W</strong>, and <strong class="inline">Y</strong>. It is very important that the number of elements in the corresponding dimensions of <strong class="inline">X</strong> and <strong class="inline">W</strong> are the same:</p>
			<p class="source-code">&gt;&gt;&gt; X = np.array([1, 2])</p>
			<p class="source-code">&gt;&gt;&gt; X.shape</p>
			<p class="source-code">(2,)</p>
			<p class="source-code">&gt;&gt;&gt; W = np.array([[1, 3, 5], [2, 4, 6]])</p>
			<p class="source-code">&gt;&gt;&gt; print(W)</p>
			<p class="source-code">[[1 3 5]</p>
			<p class="source-code">[2 4 6]]</p>
			<p class="source-code">&gt;&gt;&gt; W.shape</p>
			<p class="source-code">(2, 3)</p>
			<p class="source-code">&gt;&gt;&gt; Y = np.dot(X, W)</p>
			<p class="source-code">&gt;&gt;&gt; print(Y)</p>
			<p class="source-code">[ 5  11  17]</p>
			<p>As shown here, you can use <strong class="inline">np.dot</strong> (dot product of multidimensional matrices) to calculate the result, <strong class="inline">Y</strong>, at one time. This means that, even if the number of elements of <strong class="inline">Y</strong> is 100 or 1,000, you can calculate it all at once. Without <strong class="inline">np.dot</strong>, you must take out each element of <strong class="inline">Y</strong> (and use a <strong class="inline">for</strong> statement) for calculation, which is very tiresome. Therefore, we can say that the technique of using matrix multiplication to calculate the product of multidimensional matrices is very important.</p>
			<h2 id="_idParaDest-78"><a id="_idTextAnchor079"/>Implementing a Three-Layer Neural Network</h2>
			<p>Now, let's implement a "practical" neural network. Here, we will implement the process from its input to its output (a process in the forward direction) in the three-layer neural network shown in <em class="italics">Figure 3.15</em>. We will use NumPy's multidimensional arrays (as described in the previous section) for implementation. By making good use of NumPy arrays, you can write some short code for a forward process in the neural network.</p>
			<h3 id="_idParaDest-79"><a id="_idTextAnchor080"/>Examining the Symbols</h3>
			<p>Here, we will use symbols such as <img src="image/Figure_3.15a_-_Copy.png" alt="5c"/> and <img src="image/Figure_3.15b.png" alt="5d"/> to explain the processes performed in the neural network. They may seem a little complicated. You can skim through this section because the symbols are only used here:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="image/fig03_15.jpg" alt="Figure 3.15: A three-layer neural network consisting of two neurons in the input layer (layer 0), three neurons in the first hidden layer (layer 1), two neurons in the second hidden layer (layer 2), and two neurons in the output layer (layer 3)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.15: A three-layer neural network consisting of two neurons in the input layer (layer 0), three neurons in the first hidden layer (layer 1), two neurons in the second hidden layer (layer 2), and two neurons in the output layer (layer 3)</h6>
			<h4>Note</h4>
			<p class="callout">What is important in this section is that calculating a neural network can be conducted collectively as a matrix calculation. Calculating each layer in a neural network can be conducted collectively using matrix multiplication (this can be considered from a larger viewpoint). So, there is no problem in understanding subsequent explanations, even if you forget the detailed rules relating to these symbols.</p>
			<p>Let's begin by defining the symbols. Look at <em class="italics">Figure 3.16</em>. This diagram illustrates the weight from the input layer x<span class="P---Subscript">2</span> to the neuron <em class="italics">a<img src="image/Figure_3.15c.png" alt="5e"/></em> in the next layer.</p>
			<p>As shown in <em class="italics">Figure 3.16</em>, "(1)" is placed at the upper right of a weight or a hidden layer neuron. This number indicates the weight or neuron of layer 1. A weight has two numbers at the lower right, which are the index numbers of the next and previous layer neurons. For example, <em class="italics"><img src="image/Figure_3.15d.png" alt="5f"/></em> indicates that it is the weight from the second neuron (<em class="italics">x</em><span class="P---Subscript">2</span>) in the previous layer to the first neuron (<em class="italics"><img src="image/Figure_3.15e.png" alt="5g"/></em>) in the next layer. The index numbers at the lower right of weight must be in the order of "the number for the next layer and the number for the previous layer":</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/fig03_16.jpg" alt="Figure 3.16: Weight symbols&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.16: Weight symbols</h6>
			<h3 id="_idParaDest-80"><a id="_idTextAnchor081"/>Implementing Signal Transmission in Each Layer</h3>
			<p>Now, let's look at transmitting signals from the input layer to "the first neuron in layer 1." <em class="italics">Figure 3.17</em> shows this graphically:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/fig03_17.jpg" alt="Figure 3.17: Transmitting signals from the input layer to layer 1&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.17: Transmitting signals from the input layer to layer 1</h6>
			<p>As shown in <em class="italics">Figure 3.17</em>, ① is added as a neuron for a bias. Note that there is only one index at the lower right of the bias. This is because only one bias neuron (① neuron) exists in the previous layer. Now, let's express <em class="italics"><img src="image/Figure_3.15g.png" alt="11"/></em> as an equation to review what we have learned so far. <em class="italics"><img src="image/Figure_3.15f.png" alt="12"/></em> is the sum of the weighted signals and the bias and is calculated as follows:</p>
			<table id="table006" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer077">
									<img src="image/Figure_3.17h.png" alt="13"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>                                      (3.8)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>By using matrix multiplication, you can express "the weighted sum" of layer 1 collectively as follows:</p>
			<table id="table007" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer078">
									<img src="image/Figure_3.17i.png" alt="14"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>                                                               (3.9)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Here, A<span class="P---Superscript">(1)</span>, X, B<span class="P---Superscript">(1)</span>, and W<span class="P---Superscript">(1)</span> are as follows:</p>
			<p class="Normal"><span lang="pt-PT" xml:lang="pt-PT"><img src="image/Figure_3.17j.png" alt="15"/></span></p>
			<p>Now, let's use NumPy's multidimensional arrays to implement equation (3.9). Arbitrary values are set for input signals, weights, and biases here:</p>
			<p class="source-code">X = np.array([1.0, 0.5])</p>
			<p class="source-code">W1 = np.array([[0.1, 0.3, 0.5], [0.2, 0.4, 0.6]])</p>
			<p class="source-code">B1 = np.array([0.1, 0.2, 0.3])</p>
			<p class="source-code">print(W1.shape) # (2, 3)</p>
			<p class="source-code">print(X.shape)  # (2,)</p>
			<p class="source-code">print(B1.shape) # (3,)</p>
			<p class="source-code">A1 = np.dot(X, W1) + B1</p>
			<p>This calculation is the same as the one in the previous section. W1 is a 2x3 array and X is a one-dimensional array with two elements. Also, in this case, the number of elements in the corresponding dimensions of W1 and X are the same.</p>
			<p>Now, consider the processes performed by the activation function in layer 1. <em class="italics">Figure 3.18</em> shows these processes graphically.</p>
			<p>As shown in <em class="italics">Figure 3.18</em>, the weighted sums in a hidden layer (the total of the weighted signals and the biases) are shown as <em class="italics">a'</em>s, and the signals converted with the activation function are shown as <em class="italics">z</em>'s. Here, the activation function is shown as <em class="italics">h</em>() using a sigmoid function:</p>
			<div>
				<div id="_idContainer080" class="IMG---Figure">
					<img src="image/fig03_18.jpg" alt="Figure 3.18: Transmitting signals from the input layer to layer 1&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.18: Transmitting signals from the input layer to layer 1</h6>
			<p>This process is implemented in Python as follows:</p>
			<p class="source-code">Z1 = sigmoid(A1)</p>
			<p class="source-code">print(A1) # [0.3, 0.7, 1.1]</p>
			<p class="source-code">print(Z1) # [0.57444252, 0.66818777, 0.75026011]</p>
			<p>This <strong class="inline">sigmoid()</strong> function is the one we defined previously. It takes a NumPy array and returns a NumPy array with the same number of elements.</p>
			<p>Let's now move on to the implementation from layer 1 to layer 2 (<em class="italics">Figure 3.19</em>):<span lang="en-US" xml:lang="en-US"> </span></p>
			<div>
				<div id="_idContainer081" class="IMG---Figure">
					<img src="image/fig03_19.jpg" alt="Figure 3.19: Transmitting signals from layer 1 to layer 2&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.19: Transmitting signals from layer 1 to layer 2</h6>
			<p>This implementation is the same as the previous one, except that the output of layer 1 (Z1) is the input of layer 2. As you can see, you can write the transmission of signals from one layer to another easily by using NumPy arrays:</p>
			<p class="source-code">W2 = np.array([[0.1, 0.4], [0.2, 0.5], [0.3, 0.6]])</p>
			<p class="source-code">B2 = np.array([0.1, 0.2])</p>
			<p class="source-code">print(Z1.shape) # (3,)</p>
			<p class="source-code">print(W2.shape) # (3, 2)</p>
			<p class="source-code">print(B2.shape) # (2,)</p>
			<p class="source-code">A2 = np.dot(Z1, W2) + B2</p>
			<p class="source-code">Z2 = sigmoid(A2)</p>
			<p>Finally, let's implement the transmission of signals from layer 2 to the output layer (<em class="italics">Figure 3.20</em>). You can implement the output layer almost in the same way as the other implementations we've looked at so far. Only the last activation function is different from that of the hidden layers we've seen so far:</p>
			<p class="source-code">def identity_function(x): </p>
			<p class="source-code">    return x</p>
			<p class="source-code">W3 = np.array([[0.1, 0.3], [0.2, 0.4]])</p>
			<p class="source-code">B3 = np.array([0.1, 0.2])</p>
			<p class="source-code"><strong class="inline">A3 = np.dot(Z2, W3) + B3</strong></p>
			<p class="source-code"><strong class="inline">Y = identity_function(A3)</strong> # or Y = A3</p>
			<p>Here, we will define a function named <strong class="inline">identity_function()</strong> and use it as the activation function for the output layer. An identity function outputs the input as it is. Although you do not need to define <strong class="inline">identity_function()</strong> in this example, this implementation is used so that it is consistent with the previous ones. In <em class="italics">Figure 3.20</em>, the activation function of the output layer is shown as <strong class="inline">σ()</strong> to indicate that it is different from the activation function, <em class="italics">h</em>(), of the hidden layers (<strong class="inline">σ</strong> is called <strong class="bold">sigma</strong>):</p>
			<div>
				<div id="_idContainer082" class="IMG---Figure">
					<img src="image/fig03_20.jpg" alt="Figure 3.20: Transmitting signals from layer 2 to the output layer&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.20: Transmitting signals from layer 2 to the output layer</h6>
			<p>You can select the activation function used in the output layer, depending on what type of problem you wish to solve. Generally, an identity function is used for a regression problem, a sigmoid function for a two-class classification problem, and a softmax function for a multi-class classification problem. The activation function for an output layer will be explained in detail in the next section.</p>
			<h3 id="_idParaDest-81"><a id="_idTextAnchor082"/>Implementation Summary</h3>
			<p>This completes our investigation of a three-layer neural network. The following summarizes the implementation we've performed so far. As is customary in the implementation of a neural network, only weights are written in uppercase (for example, W1), while other items (such as a bias and intermediate result) are written in lowercase:</p>
			<p class="source-code">def init_network(): </p>
			<p class="source-code">    network = {}</p>
			<p class="source-code">    network['W1'] = np.array([[0.1,  0.3,  0.5],  [0.2,  0.4,  0.6]])</p>
			<p class="source-code">    network['b1'] = np.array([0.1,  0.2,  0.3])</p>
			<p class="source-code">    network['W2'] = np.array([[0.1,  0.4],  [0.2,  0.5],  [0.3,  0.6]])</p>
			<p class="source-code">    network['b2'] = np.array([0.1,  0.2])</p>
			<p class="source-code">    network['W3'] = np.array([[0.1,  0.3],  [0.2,  0.4]]) </p>
			<p class="source-code">    network['b3'] = np.array([0.1,  0.2])</p>
			<p class="source-code">    return network</p>
			<p class="source-code">def forward(network, x):</p>
			<p class="source-code">    W1, W2, W3 = network['W1'], network['W2'], network['W3']</p>
			<p class="source-code">    b1, b2, b3 = network['b1'], network['b2'], network['b3']</p>
			<p class="source-code">    a1 = np.dot(x, W1) + b1</p>
			<p class="source-code">    z1 = sigmoid(a1)</p>
			<p class="source-code">    a2 = np.dot(z1, W2)  +  b2 </p>
			<p class="source-code">    z2 = sigmoid(a2)</p>
			<p class="source-code">    a3 = np.dot(z2,  W3)  +  b3</p>
			<p class="source-code">    y = identity_function(a3)</p>
			<p class="source-code">    return y</p>
			<p class="source-code">network = init_network() </p>
			<p class="source-code">x =  np.array([1.0,  0.5]) </p>
			<p class="source-code">y = forward(network, x)</p>
			<p class="source-code">print(y) # [ 0.31682708 0.69627909]</p>
			<p>Here, the <strong class="inline">init_network()</strong> and <strong class="inline">forward()</strong> functions are defined. The <strong class="inline">init_network()</strong> function initializes the weights and biases and stores them in a dictionary type variable, <strong class="inline">network</strong> which stores the parameters required for individual layers, weights, and biases. The <strong class="inline">forward()</strong> function collectively implements the process of converting an input signal into an output signal.</p>
			<p>The word "forward" here indicates the transmission process from an input to an output. Later, we will look at the process in the backward direction (from output to input) when we train a neural network.</p>
			<p>This completes the implementation of a three-layer neural network in the forward direction. By using NumPy's multidimensional arrays, we were able to implement a neural network efficiently.</p>
			<h2 id="_idParaDest-82"><a id="_idTextAnchor083"/>Designing the Output Layer</h2>
			<p>You can use a neural network both for a classification problem and for a regression problem. However, you must change the activation function of the output layer, depending on which of the problems you use a neural network for. Usually, an identity function is used for a regression problem, and a softmax function is used for a classification problem.</p>
			<h4>Note</h4>
			<p class="callout">Machine learning problems can be broadly divided into "classification problems" and "regression problems." A classification problem is a problem of identifying which class the data belongs to—for example, classifying the person in an image as a man or a woman—while a regression problem is a problem of predicting a (continuous) number from certain input data—for example, predicting the weight of the person in an image. </p>
			<h2 id="_idParaDest-83"><a id="_idTextAnchor084"/>Identity Function and Softmax Function</h2>
			<p>An identity function outputs the input as it is. The function that outputs what is entered without doing anything is an identity function. Therefore, when an identity function is used for the output layer, an input signal is returned as-is. Using the diagram of the neural network we've used so far, you can represent the process by an identity function as shown in <em class="italics">Figure 3.21</em>. The process of conversion by the identity function can be represented with one arrow, in the same way in the same way as the activation function we have seen so far:</p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/fig03_21.jpg" alt="Figure 3.21: Identity function&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.21: Identity function</h6>
			<p>The softmax function, which is used for a classification problem, is expressed by the following equation:</p>
			<table id="table008" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer084">
									<img src="image/Figure_3.21a.png" alt="16"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(3.10)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p><strong class="inline">exp(x)</strong> is an exponential function that indicates e<span class="P---Superscript">x </span>(e is Napier's constant, 2.7182…). Assuming the total number of output layers is n, the equation provides the k-<span class="P---Superscript">th</span> output, y<span class="P---Subscript">k</span>. As shown in equation (3.10), the numerator of the softmax function is the exponential function of the input signal, <em class="italics">a</em><span class="P---Subscript">k</span>, and the denominator is the sum of the exponential functions of all the input signals.</p>
			<p><em class="italics">Figure 3.22</em> shows the softmax function graphically. As you can see, the output of the softmax function is connected from all the input signals with arrows. As the denominator of equation (3.10) indicates, each neuron of the output is affected by all the input signals:</p>
			<div>
				<div id="_idContainer085" class="IMG---Figure">
					<img src="image/fig03_22.jpg" alt="Figure 3.22: Softmax function&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.22: Softmax function</h6>
			<p>Now, let's implement the softmax function,using the Python interpreter to check the results, one by one:</p>
			<p class="source-code">&gt;&gt;&gt; a = np.array([0.3, 2.9, 4.0])</p>
			<p class="source-code">&gt;&gt;&gt;</p>
			<p class="source-code">&gt;&gt;&gt; exp_a = np.exp(a) # Exponential function</p>
			<p class="source-code">&gt;&gt;&gt; print(exp_a)</p>
			<p class="source-code">[ 1.34985881 18.17414537 54.59815003]</p>
			<p class="source-code">&gt;&gt;&gt;</p>
			<p class="source-code">&gt;&gt;&gt; sum_exp_a = np.sum(exp_a) # Sum of exponential functions</p>
			<p class="source-code">&gt;&gt;&gt; print(sum_exp_a)</p>
			<p class="source-code">74.1221542102</p>
			<p class="source-code">&gt;&gt;&gt;</p>
			<p class="source-code">&gt;&gt;&gt; y = exp_a / sum_exp_a</p>
			<p class="source-code">&gt;&gt;&gt; print(y)</p>
			<p class="source-code">[ 0.01821127 0.24519181 0.73659691]</p>
			<p>This implementation represents the softmax function of equation (3.10) with Python. Therefore, no special description will be required. When considering the use of the softmax function later, we will define it as a Python function, as follows:</p>
			<p class="source-code">def softmax(a):</p>
			<p class="source-code">    exp_a = np.exp(a) </p>
			<p class="source-code">    sum_exp_a = np.sum(exp_a)</p>
			<p class="source-code">    y = exp_a / sum_exp_a</p>
			<p class="source-code">    return y</p>
			<h3 id="_idParaDest-84"><a id="_idTextAnchor085"/>Issues when Implementing the Softmax Function</h3>
			<p>The preceding implementation of the softmax function represents equation (3.10) correctly, but it is defective for computer calculations. This defect is an overflow problem. Implementing the softmax function involves calculating the exponential functions, and the value of an exponential function can be very large. For example, <em class="italics">e</em><span class="P---Superscript">10</span> is larger than 20,000, and <em class="italics">e</em><span class="P---Superscript">100</span> is a large value that has more than 40 digits. The result of <em class="italics">e</em><span class="P---Superscript">1000</span> returns <strong class="inline">inf</strong>, which indicates an infinite value. Dividing these large values returns an "unstable" result.</p>
			<h4>Note</h4>
			<p class="callout">When a computer handles a "number," it is stored in finite data width, such as four or eight bytes. This means that a number has a number of significant figures. The range of a number that can be represented is limited. Therefore, there is a problem in that a very large value cannot be expressed. This is called an overflow, so we must be careful when we use a computer for calculation.</p>
			<p>Improved implementation of the softmax function is obtained from the following equation:</p>
			<table id="table009" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer086">
									<img src="image/Figure_3.22a.png" alt="17"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(3.11)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>First, equation (3.11) is transformed by multiplying both the numerator and the denominator by an arbitrary constant, <em class="italics">C</em> (the same calculations are performed because both the numerator and the denominator are multiplied by the same constant). Then, C is moved into the exponential function (exp) as log <em class="italics">C</em>. Finally, log <em class="italics">C</em> is replaced with another symbol, <em class="italics">C'</em>.</p>
			<p>Equation (3.11) says that adding or subtracting a certain constant does not change the result when the exponential functions in the softmax function are calculated. Although you can use any number as <em class="italics">C'</em> here, the largest value from the input signals is usually used to prevent an overflow. Consider the following example:</p>
			<p class="source-code">&gt;&gt;&gt; a = np.array([1010, 1000, 990])</p>
			<p class="source-code">&gt;&gt;&gt; np.exp(a) / np.sum(np.exp(a)) # Calculating the softmax function</p>
			<p class="source-code">array([  nan,   nan,   nan]) # Not calculated correctly</p>
			<p class="source-code">&gt;&gt;&gt;</p>
			<p class="source-code">&gt;&gt;&gt; c = np.max(a) # 1010</p>
			<p class="source-code">&gt;&gt;&gt; a - c</p>
			<p class="source-code">array([ 0, -10, -20])</p>
			<p class="source-code">&gt;&gt;&gt;</p>
			<p class="source-code">&gt;&gt;&gt; np.exp(a - c) / np.sum(np.exp(a - c))</p>
			<p class="source-code">array([   9.99954600e-01,	4.53978686e-05,	2.06106005e-09])</p>
			<p>As this example indicates, when the largest value of the input signals (<em class="italics">c</em>, here) is subtracted, you can calculate the function properly. Otherwise, nan (not a number: unstable) values are returned. Based on this description, we can implement the softmax function as follows:</p>
			<p class="source-code">def softmax(a):</p>
			<p class="source-code">    c = np.max(a)</p>
			<p class="source-code">    exp_a = np.exp(a - c) # Prevent an overflow</p>
			<p class="source-code">    sum_exp_a = np.sum(exp_a)</p>
			<p class="source-code">    y = exp_a / sum_exp_a </p>
			<p class="source-code">    return y</p>
			<h3 id="_idParaDest-85"><a id="_idTextAnchor086"/>Characteristics of the Softmax Function</h3>
			<p>You can use the <strong class="inline">softmax()</strong> function to calculate the output of the neural network, as follows:</p>
			<p class="source-code">&gt;&gt;&gt; a = np.array([0.3, 2.9, 4.0])</p>
			<p class="source-code">&gt;&gt;&gt; y = softmax(a)</p>
			<p class="source-code">&gt;&gt;&gt; print(y)</p>
			<p class="source-code">[ 0.01821127 0.24519181 0.73659691]</p>
			<p class="source-code">&gt;&gt;&gt; np.sum(y)</p>
			<p class="source-code">1.0</p>
			<p>The softmax function outputs a real number between 0 and 1.0. The total of the outputs of the softmax function is 1. The fact that the total is 1 is an important characteristic of the softmax function as it means we can interpret the output of the softmax function as "probability."</p>
			<p>For instance, in the preceding example, we could interpret the probability of <strong class="inline">y[0]</strong> as <strong class="inline">0.018</strong> (1.8%), the probability of <strong class="inline">y[1]</strong> as <strong class="inline">0.245</strong> (24.5%), and the probability of <strong class="inline">y[2]</strong> as <strong class="inline">0.737</strong> (73.7%). From these probabilities, we can say, "because the second element is the most probable, the answer is the second class." We can even answer probabilistically: "the answer is the second class with a probability of 74%, the first class with a probability of 25%, and the zeroth class with a probability of 1%." Thus, you can use the softmax function to handle a problem probabilistically (statistically).</p>
			<p>We should note that applying the softmax function does not change the order of the elements. This is because an exponential function, <em class="italics">(y = exp(x))</em>, increases monotonically. Actually, in the preceding example, the order of the elements in <strong class="inline">a</strong> is the same as those of the elements in <strong class="inline">y</strong>. The largest value in <strong class="inline">a</strong> is the second element, and the largest value in <strong class="inline">y</strong> is also the second element.</p>
			<p>Generally, class classification by a neural network recognizes only the class that corresponds to the neuron with the largest output. Using the softmax function does not change the position of the neuron of the largest output. Therefore, you can omit the softmax function for the output layer from neural network classification. In reality, the softmax function for the output layer is usually omitted because the exponential function requires some computation.</p>
			<h4>Note</h4>
			<p class="callout">The procedure for solving a machine learning problem consists of two phases: "training" and "predicting." First, you train a model in the training phase and then use the trained model to predict (classify) unknown data in the inference phase. As described earlier, the softmax function for the output layer is usually omitted in the inference phase. The reason we use the softmax function for the output layer will be relevant when the neural network trains (for more details, refer to the next chapter).</p>
			<h3 id="_idParaDest-86"><a id="_idTextAnchor087"/>Number of Neurons in the Output Layer</h3>
			<p>You must determine the number of neurons in the output layer as appropriate, depending on the problem to solve. For classification problems, the number of classes to classify is usually used as the number of neurons in the output layer. For example, to predict a number from <strong class="inline">0</strong> to <strong class="inline">9</strong> from an input image (10-class classification), 10 neurons are placed in the output layer, as shown in <em class="italics">Figure 3.23</em>:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="image/fig03_23.jpg" alt="Figure 3.23: The neuron in the output layer corresponds to each number&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.23: The neuron in the output layer corresponds to each number</h6>
			<p>As shown in <em class="italics">Figure 3.23</em>, the neurons in the output layer correspond to the numbers 0, 1, ..., 9 from the top. Here, the various shades of gray represent the values of the neurons in the output layer. In this example, the color of <em class="italics">y</em><span class="P---Subscript">2</span> is the darkest because the <em class="italics">y</em><span class="P---Subscript">2</span> neuron outputs the largest value. It shows that this neural network predicts that the input belongs to the class that corresponds to <em class="italics">y</em><span class="P---Subscript">2</span>; that is, "2."</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor088"/>Handwritten Digit Recognition</h2>
			<p>Now that we have covered the mechanisms of a neural network, let's consider a practical problem. We will classify some handwritten digit images. Assuming that training has already been completed, we will use trained parameters to implement "inference" in the neural network. This inference is also called forward propagation in a neural network.</p>
			<h4>Note</h4>
			<p class="callout">In the same way as the procedure for solving a machine learning problem (which consists of two phases, "training" and "inference"), to solve a problem using a neural network, we will use training data to train the weight parameters and then use the trained parameters while predicting to classify the input data.</p>
			<h3 id="_idParaDest-88"><a id="_idTextAnchor089"/>MNIST Dataset</h3>
			<p>Here, we will use a set of images of handwritten digits called MNIST. MNIST is one of the most famous datasets in the field of machine learning and is used in various ways, from simple experiments to research. When you read research papers on image recognition or machine learning, you will often notice that the MNIST dataset is used as experimental data.</p>
			<p>The MNIST dataset consists of images of numbers from 0 to 9 (<em class="italics">Figure 3. 24</em>). It contains 60,000 training images and 10,000 test images, and they are used for training and inference. When we use the MNIST dataset, we usually use training images for training and measure how correctly the trained model can classify the test images:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="image/fig03_24.jpg" alt="Figure 3.24: Examples from the MNIST image dataset&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.24: Examples from the MNIST image dataset</h6>
			<p>MNIST's image data is a 28x28 gray image (one channel), and each pixel has a value from 0 to 255. Each image data is labeled, such as "7", "2", and "1."</p>
			<p>This book provides a convenient Python script, <strong class="inline">mnist.py</strong>, which is located in the <strong class="inline">dataset</strong> directory. It supports downloading the MNIST dataset and converting image data into NumPy arrays. To use the <strong class="inline">mnist.py</strong> script, the current directory must be the <strong class="inline">ch01</strong>, <strong class="inline">ch02</strong>, <strong class="inline">ch03</strong>, ..., or <strong class="inline">ch08</strong> directory. By using the <strong class="inline">load_mnist()</strong> function in <strong class="inline">mnist.py</strong>, you can load the MNIST data easily, as follows:</p>
			<p class="source-code">import sys, os</p>
			<p class="source-code">sys.path.append(os.pardir) # Configure to import the files in the parent directory</p>
			<p class="source-code">from dataset.mnist import load_mnist</p>
			<p class="source-code"># Waits for a few minutes for the first call ... </p>
			<p class="source-code">(x_train, t_train), (x_test, t_test) = ∖</p>
			<p class="source-code"><a id="_idTextAnchor090"/>    load_mnist(flatten=True, normalize=False)</p>
			<p class="source-code"># Output the shape of each data</p>
			<p class="source-code">print(x_train.shape) # (60000, 784)</p>
			<p class="source-code">print(t_train.shape) # (60000,)</p>
			<p class="source-code">print(x_test.shape) # (10000, 784)</p>
			<p class="source-code">print(t_test.shape) # (10000,)</p>
			<p>First, configure the details for importing the files in the parent directory. Then, import the <strong class="inline">load_mnist</strong> function from <strong class="inline">dataset</strong>/<strong class="inline">mnist.py</strong>. Finally, use the imported <strong class="inline">load_mnist</strong> function to load the MNIST dataset. When you call <strong class="inline">load_mnist</strong> for the first time, an internet connection is required to download the MNIST data. A subsequent call completes immediately because it only loads the locally saved files (pickle files).</p>
			<h4>Note</h4>
			<p class="callout">The files for loading the MNIST images exist in the dataset directory of the source code provided in this book. It is assumed that this MNIST dataset is used only from the <strong class="inline">ch01</strong>, <strong class="inline">ch02</strong>, <strong class="inline">ch03</strong>, ..., or <strong class="inline">ch08</strong> directory. Therefore, to use the dataset, the <strong class="inline">sys.path.append(os.pardir)</strong> statement is required. This is because the files in the parent directory (dataset directory) must be imported.</p>
			<p>The <strong class="inline">load_mnist</strong> function returns the loaded MNIST data in the format of <strong class="inline">(training image, training label), (test image, test label)</strong>. It can take three arguments: <strong class="inline">load_mnist(normalize=True, flatten=True, one_hot_label=False)</strong>. The first argument, <strong class="inline">normalize</strong>, specifies whether to normalize the input image between 0.0 and 1.0. If <strong class="inline">False</strong> is set, the pixel of the input image remains between 0 and 255. The second argument, <strong class="inline">flatten</strong>, specifies whether to flatten the input image (convert it into a one-dimensional array). If <strong class="inline">False</strong> is set, the input image is stored as an array with three dimensions (1 × 28 × 28). If <strong class="inline">True</strong> is set, it is stored as a one-dimensional array with 784 elements. The third argument, <strong class="inline">one_hot_label</strong>, specifies whether to store the label using one-hot encoding. In a one-hot encoded array, only the element for the correct label is 1 and the other elements are 0, such as in [0,0,1,0,0,0,0,0,0,0]. When <strong class="inline">one_hot_label</strong> is <strong class="inline">False</strong>, only the correct label, such as 7 or 2, is stored. If <strong class="inline">one_hot_label</strong> is <strong class="inline">True</strong>, the labels are stored as a one-hot encoded array.</p>
			<h4>Note</h4>
			<p class="callout">Python has a convenient feature called pickle, which saves objects as files while a program is being executed. By loading the saved pickle file, you can immediately restore the object that was used during the execution of the program. The <strong class="inline">load_mnist()</strong> function, which loads the MNIST dataset, also uses pickle (for the second or subsequent loading phases). By using pickle's feature, you can prepare the MNIST data quickly.</p>
			<p>Now, let's display MNIST images to check the data. We will use the <strong class="bold">Python Image Library</strong> (<strong class="bold">PIL</strong>) module to display the images. When you execute the following code, the first training image will be displayed, as shown in <em class="italics">Figure 3.25</em> (the source code is located at <strong class="inline">ch03/mnist_show.py</strong>):</p>
			<p class="source-code">import sys, os</p>
			<p class="source-code">sys.path.append(os.pardir)</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">from dataset.mnist import load_mnist</p>
			<p class="source-code">from PIL import Image</p>
			<p class="source-code">def img_show(img):</p>
			<p class="source-code">    pil_img = Image.fromarray(np.uint8(img)) </p>
			<p class="source-code">    pil_img.show()</p>
			<p class="source-code">(x_train, t_train), (x_test, t_test) = /</p>
			<p class="source-code">    load_mnist(flatten=True, normalize=False)</p>
			<p class="source-code">    img = x_train[0] </p>
			<p class="source-code">    label = t_train[0] </p>
			<p class="source-code">    print(label) # 5</p>
			<p class="source-code">    print(img.shape)	# (784,)</p>
			<p class="source-code">    img = img.reshape(28, 28) # Reshape the image based on the original size</p>
			<p class="source-code">    print(img.shape)	# (28, 28) </p>
			<p class="source-code">    img_show(img)</p>
			<p>This results in the following output:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="image/fig03_25.jpg" alt="Figure 3.25: Displaying an MNIST image&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.25: Displaying an MNIST image</h6>
			<p>Please note that when <strong class="inline">flatten=True</strong>, the loaded image is stored as a NumPy array in a line (one-dimensionally). Therefore, to display the image, you must reshape it into its original 28x28 size. You can use the <strong class="inline">reshape()</strong> method to reshape a NumPy array by specifying the desired shape with an argument. You must also convert the image data stored as a NumPy array into the data object for PIL. You can use <strong class="inline">Image.fromarray()</strong> for this conversion.</p>
			<h3 id="_idParaDest-89"><a id="_idTextAnchor091"/>Inference for Neural Network</h3>
			<p>Now, let's implement a neural network that predicts over this MNIST dataset. The network consists of an input layer containing 784 neurons and an output layer containing 10 neurons. The number 784 for the input layer comes from the image size (28 x 28 = 784), while the number 10 for the output layer comes from 10-class classification (10 classes of numbers 0 to 9). There are two hidden layers: the first one has 50 neurons, and the second one has 100 neurons. You can change the numbers 50 and 100 as you like. First, let's define the three functions, <strong class="inline">get_data()</strong>, <strong class="inline">init_network()</strong>, and <strong class="inline">predict()</strong> (the following source code is located at <strong class="inline">ch03/neuralnet_mnist.py</strong>):</p>
			<p class="source-code">def get_data():</p>
			<p class="source-code">    (x_train, t_train), (x_test, t_test) = /</p>
			<p class="source-code">        load_mnist(normalize=True, flatten=True, one_hot_label=False)</p>
			<p class="source-code">    return x_test, t_test</p>
			<p class="source-code">def init_network():</p>
			<p class="source-code">    with open("sample_weight.pkl", 'rb') as f: </p>
			<p class="source-code">        network = pickle.load(f)</p>
			<p class="source-code">    return network</p>
			<p class="source-code">def predict(network, x):</p>
			<p class="source-code">    W1, W2, W3 = network['W1'], network['W2'], network['W3']</p>
			<p class="source-code">    b1, b2, b3 = network['b1'], network['b2'], network['b3']</p>
			<p class="source-code">    a1 = np.dot(x, W1) + b1</p>
			<p class="source-code">    z1 = sigmoid(a1)</p>
			<p class="source-code">    a2 = np.dot(z1, W2) + b2</p>
			<p class="source-code">    z2 = sigmoid(a2)</p>
			<p class="source-code">    a3 = np.dot(z2, W3) + b3</p>
			<p class="source-code">    y = softmax(a3)</p>
			<p class="source-code">    return y</p>
			<p>The <strong class="inline">init_network()</strong> function loads the trained weight parameters that are stored in the pickle file <strong class="inline">sample_weight.pkl</strong>. This file contains weight and bias parameters as a dictionary type variable. The remaining two functions are almost the same as in the implementations described so far, so these don't need to be described. Now, we will use these three functions to predict using a neural network. We want to evaluate the recognition precision—that is, how correctly it can classify:</p>
			<p class="source-code">x, t = get_data() </p>
			<p class="source-code">network = init_network()</p>
			<p class="source-code">accuracy_cnt = 0</p>
			<p class="source-code">for i in range(len(x)):</p>
			<p class="source-code">    y = predict(network, x[i])</p>
			<p class="source-code">    p = np.argmax(y) # Obtain the index of the most probable element</p>
			<p class="source-code">    if p == t[i]:</p>
			<p class="source-code">        accuracy_cnt += 1</p>
			<p class="source-code">print("Accuracy:" + str(float(accuracy_cnt) / len(x)))</p>
			<p>Here, we will obtain the MNIST dataset and build a network, then use a <strong class="inline">for</strong> statement to get each image data stored in <strong class="inline">x</strong> and use the <strong class="inline">predict()</strong> function to classify it. The <strong class="inline">predict()</strong> function returns a NumPy array containing the probability of each label. For example, an array such as [0.1, 0.3, 0.2, …, 0.04] is returned, which indicates that the probability of "0" is 0.1, that of "1" is 0.3, and so on. The index with the largest value in this probability list, which indicates the most probable element, is obtained as the prediction result. You can use <strong class="inline">np.argmax(x)</strong> to obtain the index of the largest element in an array. It returns the index of the largest element in the array specified by the <strong class="inline">x</strong> argument. Finally, the answers predicted by the neural network and the correct labels are compared, and the rate of correct predictions is displayed as the recognition precision (accuracy).</p>
			<p>When the preceding code is executed, <strong class="inline">Accuracy:0.9352</strong> is displayed. This shows that 93.52% of the classifications were correct. We will not discuss the recognition accuracy here as our goal is to run a trained neural network, but later in this book, we will improve the structure and training method of the neural network to gain higher recognition accuracy. In fact, the accuracy will exceed 99%.</p>
			<p>In this example, the argument of the <strong class="inline">load_mnist</strong> function, <strong class="inline">normalize</strong>, is set to <strong class="inline">True</strong>. When <strong class="inline">normalize</strong> is <strong class="inline">True</strong>, the function divides the value of each pixel in the image by 255 so that the data values are between 0.0 and 1.0. Converting data so that it fits in a certain range is called <strong class="bold">normalization</strong>, while converting the input data for a neural network in a defined way is called <strong class="bold">pre-processing</strong>. Here, the input image data was normalized as pre-processing.</p>
			<h4>Note</h4>
			<p class="callout">In practical usage, pre-processing is often used in a neural network (deep learning). The validity of pre-processing, as in improved discrimination and faster learning, has been proven through experiments. In the preceding example, simple normalization was conducted by dividing the value of each pixel by 255 using pre-processing. Actually, pre-processing is often conducted while considering the distribution of the whole data. Normalization is conducted by using the average and standard deviation of the whole data so that all of the data is distributed around 0 or fits in a certain range. In addition, <strong class="bold">whitening</strong> is also conducted so that all the data is distributed more evenly.</p>
			<h3 id="_idParaDest-90"><a id="_idTextAnchor092"/>Batch Processing</h3>
			<p>This process is all about implementing a neural network using the MNIST dataset. Here, we will re-examine the preceding implementation while paying attention to the "shapes" of the input data and weight parameters.</p>
			<p>Let's use the Python interpreter to output the shape of the weights for each layer in the preceding neural network:</p>
			<p class="source-code">&gt;&gt;&gt; x, _ = get_data( )</p>
			<p class="source-code">&gt;&gt;&gt; network = init_network( )</p>
			<p class="source-code">&gt;&gt;&gt; W1, W2, W3 = network['W1'], network['W2'], network['W3']</p>
			<p class="source-code">&gt;&gt;&gt;</p>
			<p class="source-code">&gt;&gt;&gt; x.shape</p>
			<p class="source-code">(10000, 784)</p>
			<p class="source-code">&gt;&gt;&gt; x[0].shape</p>
			<p class="source-code">(784,)</p>
			<p class="source-code">&gt;&gt;&gt; W1.shape</p>
			<p class="source-code">(784, 50)</p>
			<p class="source-code">&gt;&gt;&gt; W2.shape</p>
			<p class="source-code">(50, 100)</p>
			<p class="source-code">&gt;&gt;&gt; W3.shape</p>
			<p class="source-code">(100, 10)</p>
			<p>Let's check that the number of elements in the corresponding dimensions of the multidimensional arrays are the same as they are in the preceding result (biases are omitted). <em class="italics">Figure 3.26</em> shows this graphically. Here, the number of elements in the corresponding dimensions of the multidimensional arrays are the same. Verify that a one-dimensional array with 10 elements, y, is returned as the final result:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="image/fig03_26.jpg" alt="Figure 3.26: Transition of the shapes of arrays&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.26: Transition of the shapes of arrays</h6>
			<p><em class="italics">Figure 3.26</em> shows the flow where a one-dimensional array with 784 elements (originally a two-dimensional 28x28 array) is provided, and a one-dimensional array with 10 elements is returned. This is the process when a single image is input.</p>
			<p>Now, let's think about the process when multiple images are entered at once. For example, let's assume that you want to use the <strong class="inline">predict()</strong> function to process 100 images at one time. To do that, you can change the shape of <strong class="inline">x</strong> to <strong class="inline">100×784</strong> so that you can enter 100 images collectively as input data. <em class="italics">Figure 3.27</em> shows this graphically:</p>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="image/fig03_27.jpg" alt="Figure 3.27: Transition of the shapes of arrays in batch processing&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 3.27: Transition of the shapes of arrays in batch processing</h6>
			<p>As shown in <em class="italics">Figure 3.27</em>, the shape of the input data is 100x784, and that of the output data is 100x10. This indicates that the results for the input data of 100 images are returned in one go. For example, <strong class="inline">x[0]</strong> and <strong class="inline">y[0]</strong> store the image and predict the result of the 0<span class="P---Superscript">th</span> image, <strong class="inline">x[1],</strong> and <strong class="inline">y[1]</strong> store the image and predicting the result of the first image, and so on.</p>
			<p>An organized set of input data, as described here, is called a <strong class="bold">batch</strong>. A batch is a stack of images, such as a wad of bills.</p>
			<h4>Note</h4>
			<p class="callout">Batch processing has a big advantage in computer calculation. It can greatly reduce the processing time of each image since many of the libraries that handle numerical calculations are highly optimized so that large arrays can be calculated efficiently. When data transfer causes a bottleneck in neural network calculation, batch processing can reduce the load on the bus band (i.e.: the ratio of operations to data loading can be increased). Although batch processing requires a large array to be calculated, calculating a large array in one go is faster than calculating by dividing small arrays little by little.</p>
			<p>Now, let's use batch processing for our implementation. Here, the differences from the previous code are shown in bold:</p>
			<p class="source-code">x, t = get_data( ) </p>
			<p class="source-code">network = init_network( )</p>
			<p class="source-code"><strong class="inline">batch_size = 100 # Number of batches</strong></p>
			<p class="source-code">accuracy_cnt = 0</p>
			<p class="source-code"><strong class="inline">for i in range(0, len(x), batch_size):</strong></p>
			<p class="source-code"><strong class="inline">    x_batch = x[i:i+batch_size]</strong></p>
			<p class="source-code"><strong class="inline">    y_batch = predict(network, x_batch)</strong></p>
			<p class="source-code"><strong class="inline">    p = np.argmax(y_batch, axis=1)</strong></p>
			<p class="source-code"><strong class="inline">    accuracy_cnt += np.sum(p == t[i:i+batch_size])</strong></p>
			<p class="source-code">print("Accuracy:" + str(float(accuracy_cnt) / len(x)))</p>
			<p>Now, we will describe each section shown in bold. First, let's look at the <strong class="inline">range()</strong> function. You can use the <strong class="inline">range()</strong> function, such as <strong class="inline">range(start, end)</strong>, to generate a list of integers from <strong class="inline">start</strong> to <strong class="inline">end-1</strong>. By specifying three integers, as in <strong class="inline">range(start, end, step)</strong>, you can generate a list of integers where values are incremented by the value specified with the <strong class="inline">step</strong>, as in the following example:</p>
			<p class="source-code">&gt;&gt;&gt; list( range(0, 10) )</p>
			<p class="source-code">[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]</p>
			<p class="source-code">&gt;&gt;&gt; list( range(0, 10, 3) )</p>
			<p class="source-code">[0, 3, 6, 9]</p>
			<p>Based on the list returned by the <strong class="inline">range()</strong> function, <strong class="inline">x[i:i+batch_size]</strong> is used to extract a batch from the input data. <strong class="inline">x[i:i+batch_n]</strong> obtains from the <strong class="inline">i-</strong>th to <strong class="inline">i+batch_n-</strong>th data in the input data. In this example, 100 items of data are obtained from the beginning, such as x[0:100], x[100:200], …</p>
			<p>Then, <strong class="inline">argmax()</strong> obtains the index of the largest value. Please note that an argument, <strong class="inline">axis=1</strong>, is specified here. It indicates that, in a 100x10 array, the index of the largest value is found among the elements in dimension 1 (the axis is almost the same as the dimension), as shown below:</p>
			<p class="source-code">&gt;&gt;&gt; x = np.array([[0.1, 0.8, 0.1], [0.3, 0.1, 0.6],</p>
			<p class="source-code">... [0.2, 0.5, 0.3], [0.8, 0.1, 0.1]])</p>
			<p class="source-code">&gt;&gt;&gt; y = np.argmax(x, axis=1)</p>
			<p class="source-code">&gt;&gt;&gt; print(y)</p>
			<p class="source-code">[1 2 1 0]</p>
			<p>Lastly, the classification results for each batch are compared with the actual answers. To do that, a comparison operator (<strong class="inline">==</strong>) is used to compare the NumPy arrays. A Boolean array of <strong class="inline">True</strong>/<strong class="inline">False</strong> is returned, and the number of Trues is calculated, as follows:</p>
			<p class="source-code">&gt;&gt;&gt; y = np.array([1, 2, 1, 0])</p>
			<p class="source-code">&gt;&gt;&gt; t = np.array([1, 2, 0, 0])</p>
			<p class="source-code">&gt;&gt;&gt; print(y==t)</p>
			<p class="source-code">[True True False True]</p>
			<p class="source-code">&gt;&gt;&gt; np.sum(y==t)</p>
			<p class="source-code">3</p>
			<p>That's it for implementation using batch processing. Batch processing enables fast and efficient processing. When we learn about neural networks in the next chapter, batches of image data will be used for training. There, we will also build an implementation of batch processing, just like we did in this chapter.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor093"/>Summary</h2>
			<p>This chapter described forward propagation in a neural network. The neural network we explained in this chapter is the same as a perceptron in the previous chapter in that the signals of the neurons are transmitted hierarchically. However, a large difference exists in the activation functions that change signals when they are transmitted to the next neurons. As an activation function, a neural network uses a sigmoid function, which changes signals smoothly, and a perceptron uses a step function, which changes signals sharply. This difference is important in neural network training and will be described in the next chapter. This chapter covered the following points:</p>
			<ul>
				<li>A neural network uses a function that changes smoothly, such as a sigmoid function or a ReLU function, as an activation function.</li>
				<li>By using NumPy's multidimensional arrays, you can implement a neural network efficiently.</li>
				<li>Machine learning problems can be broadly divided into classification problems and regression problems.</li>
				<li>When using the activation function for the output layer, an identity function is often used for a regression problem, and a softmax function is used for a classification problem.</li>
				<li>For a classification problem, the number of classes to classify is used as the number of neurons in the output layer.</li>
				<li>A set of input data is called a batch. Predicting per batch accelerates the calculation process.</li>
			</ul>
		</div>
		<div>
			<div id="_idContainer093" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer094" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer095" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer096" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer097" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer098" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer099" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer100" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer101" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer102" class="Content">
			</div>
		</div>
	</body></html>