- en: '1'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Up and Running with MXNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**MXNet** is one of the most used deep learning frameworks and is an Apache
    open source project. Before 2016, **Amazon Web Services** (**AWS**)’s research
    efforts did not use a preferred deep learning framework, allowing each team to
    research and develop according to their choices. Although some deep learning frameworks
    have thriving communities, sometimes AWS was not able to fix code bugs at the
    required speed (among other issues). To solve these issues, at the end of 2016,
    AWS announced MXNet as its deep learning framework of choice, investing in internal
    teams to develop it further. Research institutions that support MXNet are Intel,
    Baidu, Microsoft, Carnegie Mellon University, and MIT, among others. It was co-developed
    by Carlos Guestrin at Carnegie Mellon University and the University of Washington
    (along with GraphLab).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Some of its advantages are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Imperative/symbolic programming and hybridization (which will be covered in
    *Chapters 1* and *9*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Support for multiple GPUs and distributed training (which will be covered in
    *Chapters 7* and *8*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Highly optimized for inference production systems (which will be covered in
    *Chapters 7* and *9*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large number of pre-trained models on its Model Zoos in the fields of computer
    vision and natural language processing, among others (covered in *Chapters 6*,
    *7*, and *8*)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To start working with MXNet, we need to install the library. There are several
    different versions of MXNet available to be installed, and in this chapter, we
    will cover how to choose the right version. The most important parameter will
    be the available hardware we have. In order to optimize performance, it is always
    best to maximize the use of our available hardware. We will compare the usage
    of a well-known linear algebra library, NumPy, with similar operations in MXNet.
    We will then compare the performance of the different MXNet versions versus NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: MXNet includes its own API for deep learning, Gluon, and moreover, Gluon provides
    different libraries for computer vision and natural language processing that include
    pre-trained models and utilities. These libraries are known as GluonCV and GluonNLP.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing MXNet, Gluon, GluonCV, and GluonNLP
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NumPy and MXNet ND arrays – comparing their performance
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Apart from the technical requirements specified in the *Preface*, no other requirements
    apply to this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch01](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch01).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, you can access directly each recipe from Google Colab – for example,
    use the following for the first recipe of this chapter: [https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch01/1_1_Installing_MXNet.ipynb](https://colab.research.google.com/github/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch01/1_1_Installing_MXNet.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Installing MXNet, Gluon, GluonCV, and GluonNLP
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In order to get the maximum performance out of the available software (programming
    languages) and hardware (CPU and GPU), there are different MXNet library versions
    available to install. We shall learn how to install them in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before getting started with the MXNet installation, let us review the different
    versions available of the software packages that we will use, including MXNet.
    The reason we do that is that our hardware configuration must map to the chosen
    versions of our software packages in order to maximize performance:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Python**: MXNet is available for different programming languages – Python,
    Java, R, and C++, among others. We will use MXNet for Python, and Python 3.7+
    is recommended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Jupyter**: Jupyter is an open source web application that provides an easy-to-use
    interface to show Markdown text, working code, and data visualizations. It is
    very useful for understanding deep learning, as we can describe concepts, write
    the code to run through those concepts, and visualize the results (typically comparing
    them with the input data). Jupyter Core 4.5+ is recommended.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CPUs and GPUs**: MXNet can work with any hardware configuration – that is,
    any single CPU can run MXNet. However, there are several hardware components that
    MXNet can leverage to improve performance:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Intel CPUs**: Intel developed a library known as **Math Kernel Library**
    (**MKL**) for optimized math operations. MXNet has support for this library, and
    using the optimized version can improve certain operations. Any modern version
    of Intel MKL is sufficient.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**NVIDIA GPUs**: NVIDIA developed a library known as **Compute Unified Device
    Architecture (CUDA)** for optimized parallel operations (such as matrix operations,
    which are very common in deep learning). MXNet has support for this library, and
    using the optimized version can dramatically improve large deep learning workloads,
    such as model training. CUDA 11.0+ is recommended.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MXNet version**: At the time of writing, MXNet 1.9.1 is the most up-to-date
    stable version that has been released. All the code throughout the book has been
    verified with this version. MXNet, and deep learning in general, can be considered
    a live ongoing project, and therefore, new versions will be released periodically.
    These new versions will have improved functionality and new features, but they
    might also contain breaking changes from previous APIs. If you are revisiting
    this book in a few months and a new version has been released with breaking changes,
    how to install MXNet version 1.8.0 specifically is also described here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: 'I have used Google Colab as the platform to run the code described in this
    book. At the time of writing, it provides Python 3.10.12, up-to-date Jupyter libraries,
    Intel CPUs (Xeon @ 2.3 GHz), and NVIDIA GPUs (which can vary: K80s, T4s, P4s,
    and P100s) with CUDA 11.8 pre-installed. Therefore, minimal steps are required
    to install MXNet and get it running.'
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Throughout the book, we will not only use code extensively but also clarify
    comments and headings in that code to provide structure, as well as several types
    of visual information such as images or generated graphs. For these reasons, we
    will use Jupyter as the supporting development environment. Moreover, in order
    to facilitate setup, installation, and experimentation, we will use Google Colab.
  prefs: []
  type: TYPE_NORMAL
- en: 'Google Colab is a hosted Jupyter Notebook service that requires no setup to
    use, while providing free access to computing resources, including GPUs. In order
    to set up Google Colab properly, this section is divided into two main points:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Verifying and installing libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: If you prefer, you can use any local environment that supports Python 3.7+,
    such as Anaconda, or any other Python distribution. This is highly encouraged
    if your hardware specifications are better than Google Colab’s offering, as better
    hardware will reduce computation time.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the notebook
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we will learn how to work with Google Colab and set up a new
    notebook, which we will use to verify our MXNet installation:'
  prefs: []
  type: TYPE_NORMAL
- en: Open your favorite web browser. In my case, I have used Google Chrome as the
    web browser throughout the book. Visit [https://colab.research.google.com/](https://colab.research.google.com/)
    and click on **NEW NOTEBOOK**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.1 – The Google Colab start screen](img/B16591_01_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.1 – The Google Colab start screen
  prefs: []
  type: TYPE_NORMAL
- en: Change the title of the notebook – for example, as you can see in the following
    screenshot, I have changed the title to `DL with MXNet Cookbook 1.1` `Installing
    MXNet`.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.2 – A Google Colab notebook](img/B16591_01_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.2 – A Google Colab notebook
  prefs: []
  type: TYPE_NORMAL
- en: 'Change your Google Colab runtime type to use a GPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select **Change runtime type** from the **Runtime** menu.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.3 – Change runtime type](img/B16591_01_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.3 – Change runtime type
  prefs: []
  type: TYPE_NORMAL
- en: In **Notebook settings**, select **GPU** as the **Hardware** **accelerator**
    option.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 1.4 – Hardware accelerator | GPU](img/B16591_01_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.4 – Hardware accelerator | GPU
  prefs: []
  type: TYPE_NORMAL
- en: Verifying and installing libraries
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, go to the first cell (make sure it is a code cell) and type
    the following commands:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify the Python version by typing the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Check the version, and make sure that it is 3.7+.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In Google Colab, you can directly run commands as if you were in the Linux Terminal
    by adding the `!` character to the command. Feel free to try other commands such
    as `!ls`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We now need to verify the Jupyter version (Jupyter Core 4.5.0 or above will
    suffice):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is one potential output from the previous command:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: Jupyter, an open source notebook application, is assumed to be installed, as
    is the case for Google Colab. For further instructions on how to install it, visit
    [https://jupyter.org/install](https://jupyter.org/install).
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify whether an Intel CPU is present in the hardware:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The more up to date the processor the better, but for the purposes of this book,
    the dependency is larger with the GPU than with the CPU.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Verify the NVIDIA GPU is present in the hardware (there are devices listed
    below) and that NVIDIA CUDA is installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will yield a similar output to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: CUDA 11.0 has known issues with the NVIDIA K80\. If you have an NVIDIA K80 and
    are having issues with the examples described, uninstall CUDA 11.0 and install
    CUDA 10.2\. Afterward, install MXNet for CUDA 10.2 following the steps described
    here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the CUDA version is 11.0 or above:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will yield a similar output to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Install MXNet, depending on your hardware configuration. The following are
    the different MXNet versions that you can install:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Recommended/Google Colab**: The latest MXNet version (1.9.1) with GPU support:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '**No Intel CPU nor NVIDIA GPU**: Install MXNet with the following command:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '**Intel CPU without NVIDIA GPU**: Install MXNet with Intel MKL, with the following
    command:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '**No Intel CPU with NVIDIA GPU**: Install MXNet with NVIDIA CUDA 10.2, with
    the following command:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '**Intel CPU and NVIDIA GPU**: Install MXNet with Intel MKL and NVIDIA CUDA
    11.0, with the following command:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: '`pip3`, a Python 3 package manager, is assumed to be installed, as is the case
    for Google Colab. If a different installation method for MXNet is preferred, visit
    [https://mxnet.apache.org/versions/master/get_started](https://mxnet.apache.org/versions/master/get_started)
    for instructions.'
  prefs: []
  type: TYPE_NORMAL
- en: After version 1.6.0, MXNet is released by default with the Intel MKL library
    extension; therefore, there is no need to add the `mkl` suffix anymore when installing
    the most recent versions, as seen previously in the recommended installation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Verify that the MXNet installation has been successful with the following two
    steps:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The following commands must not return any error and must successfully display
    MXNet version 1.9.1:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The list of features that appear in the following contain the `CUDA`, `CUDNN`,
    and `MKLDNN` features:'
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The output will list all the features and `True` for each one.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Install GluonCV and GluonNLP:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This command will install the latest versions of GluonCV and GluonNLP, which
    at the time of writing were, respectively, 0.10 and 0.10.
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training, inference, and evaluation of deep learning networks are highly
    complex operations, involving hardware and several layers of software, including
    drivers, low-level performance libraries such as MKL and CUDA, and high-level
    programming languages and libraries such as Python and MXNet.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'MXNet is an actively developed project, part of the Apache Incubator program.
    Therefore, new versions are expected to be released, and they might contain breaking
    changes. The preceding command will install the latest stable version available.
    Throughout this book, the version of MXNet used is 1.9.1\. If your code fails
    and it uses a different MXNet version, try installing MXNet version 1.9.1 by running
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`!python3 -m pip` `install mxnet-cu117==1.9.1`'
  prefs: []
  type: TYPE_NORMAL
- en: By checking all the hardware and software components, we can install the most
    optimized version of MXNet. We can use Google Colab, which easily transfers to
    other local configurations such as the Anaconda distribution.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we can identify the right combination of CUDA drivers and MXNet versions
    that will maximize performance and verify a successful installation.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: It is highly recommended to always use the latest versions of all the software
    components discussed. Deep learning is an evolving field and there are always
    improvements such as new functionalities added, changes in the APIs, and updates
    in the internal functions to increase performance, among other changes.
  prefs: []
  type: TYPE_NORMAL
- en: However, it is very important that all components (CPU, GPU, CUDA, and the MXNet
    version) are compatible. To match these components, it is highly recommended to
    visit [https://mxnet.apache.org/versions/master/get_started](https://mxnet.apache.org/versions/master/get_started)
    and check for the latest CUDA and MXNet versions you can install to maximize your
    hardware performance.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, for a Python 3-based Linux distribution, installed using `pip3`,
    these are the MXNet versions available (note with/without CPU acceleration and/or
    with GPU acceleration).
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are interested in knowing more about Intel’s **MKL**, the following
    link is a very good starting point: [https://software.intel.com/content/www/us/en/develop/articles/getting-started-with-intel-optimization-for-mxnet.html](https://software.intel.com/content/www/us/en/develop/articles/getting-started-with-intel-optimization-for-mxnet.html).'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy and MXNet ND arrays
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If you have worked with data previously in Python, chances are you have found
    yourself working with NumPy and its **N-dimensional arrays** (**ND arrays**).
    These are also known as tensors, and the 0D variants are called **scalars**, the
    1D variants are called **vectors**, and the 2D variants are called **matrixes**.
  prefs: []
  type: TYPE_NORMAL
- en: MXNet provides its own ND array type, and there are two different ways to work
    with them. On one hand, there is the `nd` module, MXNet’s native and optimized
    way to work with MXNet ND arrays. On the other hand, there is the `np` module,
    which has the same interfaces and syntax as the NumPy ND array type and has also
    been optimized, but it’s limited due to the interface constraints. With MXNet
    ND arrays, we can leverage its underlying engine, with compute optimizations such
    as Intel MKL and/or NVIDIA CUDA, if our hardware configuration is compatible.
    This means we will be able to use almost the same syntax as when working with
    NumPy, but accelerated with the MXNet engine and our GPUs, not supported by NumPy.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, as we will see in the next chapters, a very common operation that
    we will execute on MXNet is automatic differentiation on these ND arrays. By using
    MXNet ND array libraries, this operation will also leverage our hardware for optimum
    performance. NumPy does not provide automatic differentiation out of the box.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'If you have already installed MXNet, as described in the previous recipe, in
    terms of executing accelerated code, the only remaining steps before using MXNet
    ND arrays is importing their libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: However, it is worth noting here an important underlying difference between
    NumPy ND array operations and MXNet ND array operations. NumPy follows an eager
    evaluation strategy – that is, all operations are evaluated at the moment of execution.
    Conversely, MXNet uses a lazy evaluation strategy, more optimal for large compute
    loads, where the actual calculation is deferred until the values are actually
    needed.
  prefs: []
  type: TYPE_NORMAL
- en: 'Therefore, when comparing performances, we will need to force MXNet to finalize
    all calculations before computing the time needed for them. As we will see in
    the examples, this is achieved by calling the `wait_to_read()` function, Furthermore,
    when accessing the data with functions such as `print()` or `.asnumpy()`, execution
    is then completed before calling these functions, yielding the wrong impression
    that these functions are actually time-consuming:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s check a specific example and start by running it on the CPU:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will yield a similar output to the following:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'However, let’s see what happens if we measure the time without the call to
    `wait_to_read()`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following will be the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, the first experiment indicated that the computation took ~50
    ms to complete; however, the second experiment indicated that the computation
    took ~1 ms (50 times less!), and the visualization was more than 40 ms. This is
    an incorrect result. This is because we measured our performance incorrectly in
    the second experiment. Refer to the first experiment and the call to `wait_to_read()`
    for a proper performance measurement.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we will compare performance in terms of computation time for
    two compute-intensive operations:'
  prefs: []
  type: TYPE_NORMAL
- en: Matrix creation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matrix multiplication
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We will compare five different compute profiles for each operation:'
  prefs: []
  type: TYPE_NORMAL
- en: Using the NumPy library (no CPU or GPU acceleration)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the MXNet `np` module with CPU acceleration but no GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the MXNet `np` module with CPU acceleration and GPU acceleration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the MXNet `nd` module with CPU acceleration but no GPU
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the MXNet `nd` module with CPU acceleration and GPU acceleration
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To finalize, we will plot the results and draw some conclusions.
  prefs: []
  type: TYPE_NORMAL
- en: Timing data structures
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will store the computation time in five dictionaries, one for each compute
    profile (`timings_np`, `timings_mx_cpu`, and `timings_mx_gpu`). The initialization
    of the data structures is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We will run each operation (matrix generation and matrix multiplication) with
    matrixes in a different order, namely the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Matrix creation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We define three functions to generate matrixes; the first function will use
    the NumPy library to generate a matrix, and it will receive as an input parameter
    the matrix order. The second function will use the MXNet np module, and the third
    function will use the MXNet and module. For the second and third functions, as
    input parameters we will provide the context where the matrix needs to be created,
    apart from the matrix order. This context specifies whether the result (the created
    matrix in this case) must be computed in the CPU or the GPU (and which GPU if
    there are multiple devices available):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'To store necessary data for our performance comparison later, we use the structures
    created previously, with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Matrix multiplication
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We define three functions to compute the matrixes multiplication; the first
    function will use the NumPy library and will receive as input parameters the matrixes
    to multiply. The second function will use the MXNet np module, and the third function
    will use the MXNet nd module. For the second and third functions, the same parameters
    are used. The context where the multiplication will happen is given by the context
    where the matrixes were created; no parameter needs to be added. Both matrixes
    need to have been created in the same context, or an error will be triggered:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'To store the necessary data for our performance comparison later, we will use
    the structures created previously, with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Drawing conclusions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step before making any assessments is to plot the data we have captured
    in the previous steps. For this step, we will use the `pyplot` module from a library
    called Matplotlib, which will allow us to create charts easily. The following
    code plots the runtime (in seconds) for the matrix generation and all the matrix
    orders computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Quite similarly as shown in the previous code block, the following code plots
    the runtime (in seconds) for the matrix multiplication and all the matrix orders
    computed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'These are the plots displayed (the results will vary according to the hardware
    configuration):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 1.5 – Runtimes – a) Matrix creation, and b) Matrix multiplication](img/B16591_01_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 1.5 – Runtimes – a) Matrix creation, and b) Matrix multiplication
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that the charts use a logarithmic scale for both axes, horizontal and vertical
    (the differences are larger than they seem). Furthermore, the actual values depend
    on the hardware architecture that the computations are run on; therefore, your
    specific results will vary.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several conclusions that can be drawn, both from each individual
    operation and collectively:'
  prefs: []
  type: TYPE_NORMAL
- en: For smaller matrix orders, using NumPy is much faster in both operations. This
    is because MXNet works in a different memory space, and the amount of time to
    move the data to this memory space is longer than the actual compute time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In matrix creation, for larger matrix orders, the difference between NumPy (remember,
    it’s CPU only) and MXNet with the np module and CPU acceleration is negligible,
    but with the nd module and CPU, acceleration is ~2x faster. For matrix multiplication,
    and depending on your hardware, MXNet with CPU acceleration can be ~2x faster
    (regardless of the module). This is because MXNet uses Intel MKL to optimize CPU
    computations.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the ranges that are interesting for deep learning – that is, large computational
    loads involving matrix orders > 1,000 (which can represent data such as images
    composed of several megapixels or large language dictionaries), GPUs deliver typical
    gains of several orders of magnitude (~200x for creation, and ~40x for multiplication,
    exponentially growing with every increase of matrix order). This is by far the
    most compelling reason to work with GPUs when running deep learning experiments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When using the GPU, the MXNet np module is faster than the MXNet nd module in
    creation (~7x), but the difference is negligible in multiplication. Typically,
    deep learning algorithms are more similar to multiplications to terms of computational
    loads, and therefore, a priori, there is no significant advantage in using the
    np module or the nd module. However, *MXNet recommends using the native MXNet
    nd module* (and the author subscribes to this recommendation) because some operations
    on the np module are not supported by `autograd` (MXNet’s auto-differentiation
    module). We will see in the upcoming chapters, when we train neural networks,
    how the `autograd` module is used and why it is critical.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How it works...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'MXNet provides two optimized modules to work with ND arrays, including one
    that is an in-place substitute for NumPy. The advantages of operating with MXNet
    ND arrays are twofold:'
  prefs: []
  type: TYPE_NORMAL
- en: MXNet ND array operations support automatic differentiation. As we will see
    in the following chapters, automatic differentiation is a key feature that allows
    developers to concentrate on the forward pass of the models, letting the backward
    pass be automatically derived.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Conversely, operations with MXNet ND arrays are optimized for the underlying
    hardware, yielding impressive results with GPU acceleration. We computed results
    for matrix creation and matrix multiplication to validate this conclusion experimentally.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we have barely scratched the surface of MXNet operations with
    ND arrays. If you want to read more about MXNet and ND arrays, this is the link
    to the official MXNet API reference: https://mxnet.apache.org/versions/1.0.0/api/python/ndarray/ndarray.html.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, a very interesting tutorial can be found in the official MXNet
    documentation: https://gluon.mxnet.io/chapter01_crashcourse/ndarray.html.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Moreover, we have taken a glimpse at how to measure performance on MXNet. We
    will revisit this topic in the following chapters; however, a good deep-dive into
    the topic is given in the official MXNet documentation: [https://mxnet.apache.org/versions/1.8.0/api/python/docs/tutorials/performance/backend/profiler.html](https://mxnet.apache.org/versions/1.8.0/api/python/docs/tutorials/performance/backend/profiler.html).'
  prefs: []
  type: TYPE_NORMAL
