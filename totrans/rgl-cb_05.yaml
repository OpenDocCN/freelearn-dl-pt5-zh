- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularization with Data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Even though there are plenty of regularization methods for models (with each
    model having a unique set of hyperparameters), sometimes, the most effective regularization
    comes from the data itself. Indeed, sometimes, even the most powerful model can’t
    have good performance if the data is not transformed properly beforehand.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll look at some methods that help regularize models from
    data:'
  prefs: []
  type: TYPE_NORMAL
- en: Hashing high cardinality features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregating features
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Undersampling an imbalanced dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oversampling an imbalanced dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Resampling imbalanced data with SMOTE
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will apply several tricks to data, as well as resample
    datasets or download new data via the command line. To do so, you will need the
    following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: imbalanced-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: category_encoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kaggle API
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hashing high cardinality features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'High cardinality features are qualitative features with many possible values.
    High cardinality features may appear in many applications, such as a country in
    a customer database, a phone model in advertising, or vocabulary in NLP applications.
    High cardinality issues can be manifold: not only may they lead to a very highly
    dimensional dataset, but they can also evolve as more and more values become available.
    Indeed, even if the data for the number of countries or vocabulary is arguably
    quite stable, there are new phone models every week, if not every day.'
  prefs: []
  type: TYPE_NORMAL
- en: Hashing is a very popular and useful way to deal with such problems. In this
    recipe, we’ll see what it is and how to use it in practice on a dataset to predict
    whether employees will leave a company.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hashing is a very useful trick in computer science in general, and it is widely
    used in cryptography or blockchain, for example. It is also useful in machine
    learning when dealing with high cardinality features. It does not necessarily
    help with regularization per se, but it can sometimes be a side effect.
  prefs: []
  type: TYPE_NORMAL
- en: What is hashing?
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Hashing is often used in machine learning at the production level for dealing
    with high cardinality features. High cardinality features tend to have a growing
    number of possible outcomes. This can include things such as a mobile phone model,
    a software version, an item ID, and so on. In such cases, using one-hot encoding
    on high cardinality features may lead to several problems:'
  prefs: []
  type: TYPE_NORMAL
- en: The required space is not fixed and can’t be controlled
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We need to figure out how to encode a new value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using hashing instead of one-hot encoding can address these limitations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we must use a hash function that converts an input into a controlled
    output. One well-known hash function is `md5`. If we apply `md5` to some strings,
    we’ll get the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, hashing has several interesting properties:'
  prefs: []
  type: TYPE_NORMAL
- en: No matter the input size, the output size is fixed
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two similar inputs may lead to very different outputs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These properties allow hash functions to be very effective when used with high
    cardinality features. All we have to do is this:'
  prefs: []
  type: TYPE_NORMAL
- en: Choose a hash function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define the expected space dimension of the output.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Encode our feature with that function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Of course, there are some drawbacks to hashing:'
  prefs: []
  type: TYPE_NORMAL
- en: There may be collisions – two different inputs may have the same output (even
    if this does not necessarily hurt performance if it’s not that severe)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We may want similar inputs to have similar outputs (a well-chosen hashing function
    can have such a property)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Required installations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We need to do some preparation for this recipe. Since we will download a Kaggle
    dataset, first, we need to install the Kaggle API:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Install the library with `pip`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: If you haven’t already done so, create a Kaggle account at [www.kaggle.com](https://www.kaggle.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Go to your profile page and create your API token by clicking `kaggle.json`
    file to your computer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.1 – Screenshot of the Kaggle website](img/B19629_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – Screenshot of the Kaggle website
  prefs: []
  type: TYPE_NORMAL
- en: 'You need to move the freshly downloaded `kaggle.json` file to `~/.kaggle` via
    the following command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You can now download the dataset with the following command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We should now have a file called `aug-train.zip`, which contains the data we
    will use for this recipe. We also need to install the `category_encoders`, `pandas`,
    and `sklearn` libraries with the following command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will load and quickly prepare the dataset (quickly in the
    sense that more data preparation could lead to better results), and then apply
    a logistic regression model to this classification task. On the selected dataset,
    the `city` feature has 123 possible outcomes, so it can be considered a high cardinality
    feature. Also, we can fairly assume that the production data could contain more
    cities, so the hashing trick would make sense here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required modules, functions, and classes: `pandas` for loading the
    data, `train_test_split` for splitting the data, `StandardScaler` for rescaling
    quantitative features, `HashingEncoder` for encoding qualitative features, and
    `LogisticRegression` as the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset with `pd.read_csv()`. Note that we do not need to unzip the
    dataset first since the zip only contains one CSV file – `pandas` will take care
    of that for us:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As we can see, the `city` feature has `123` possible values in the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Remove any missing data. We take a very brutal policy here: we remove all the
    features that have a large amount of missing data, and then we remove all the
    rows with remaining missing data. This is not a recommended approach in general
    since we lose a lot of potentially useful information. Since dealing with missing
    data isn’t the subject here, we will take this simplistic approach:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and test sets with the `train_test_split` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select and rescale any quantitative features. We will use the standard scaler
    to rescale the selected quantitative features, but any other scaler may work fine
    too:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Select and prepare “regular” qualitative features. Here, we will use the one-hot
    encoder from `scikit-learn`, though we could also apply the hashing trick to those
    features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Encode the high cardinality `''city''` feature with hashing. Since there are
    currently `123` possible values for this feature, we could use only 7 bits to
    encode the whole space of possibilities. This is what is denoted by the `n_components=7`
    parameter. For safety, we could set it to 8 or more bits, to consider a growing
    set of possible cities in the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, all the values are encoded in seven columns, spanning 2^7 = 128
    possible values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Concatenate all the prepared data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate and train the logistic regression model. Here, we will use the
    default hyperparameters proposed by `scikit-learn` for logistic regression:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the accuracy for both the training and test sets using the `.``score()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we have an accuracy of about 78% on the test set, with no apparent
    overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: It is possible that adding some features (for example, with feature engineering)
    could help improve the model since the model in itself seems to have no room for
    much improvement based on the fact there is no overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The official documentation of the category encoders library: [https://contrib.scikit-learn.org/category_encoders/](https://contrib.scikit-learn.org/category_encoders/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The category encoders page about hashing: [https://contrib.scikit-learn.org/category_encoders/hashing.xhtml](https://contrib.scikit-learn.org/category_encoders/hashing.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Aggregating features
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When you’re looking at high cardinality features, one possible solution is to
    reduce the actual cardinality of that feature. Here, aggregating is one possible
    solution, and it may work very well in some cases. In this recipe, we will explain
    what aggregating is and discuss when we should use it. Once we’ve done that, we
    will apply it.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When dealing with high cardinality features, one-hot encoding leads to high-dimensionality
    datasets. Because of the so-called curse of dimensionality, the ability for models
    to generalize properly can be a real issue for one-hot encoded high cardinality
    features, even with very large training datasets. Thus, aggregating is a way to
    lower the dimensionality of the one-hot encoding, and then lower the risk of overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are several ways to aggregate. Let’s, for example, assume that we have
    a database of clients that contains the “phone model” feature, which consists
    of many of the possible phone models (that is, hundreds). There could be at least
    two ways of aggregating such a feature:'
  prefs: []
  type: TYPE_NORMAL
- en: '**By occurrence probability**: Any model appearing less than X% in the data
    is considered as “others”'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**By a given similarity**: We could gather models by generation, brand, or
    even price'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These methods have their pros and cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pros**: Aggregating by occurrence is simple, works all the time, and does
    not require any subject matter knowledge'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Cons**: Aggregating by a given similarity can be more relevant but requires
    knowledge about the feature that may not be available, or it could take too long
    (for example, if there are millions of values)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Aggregating is also sometimes useful when there is a long tail distribution
    of values for a feature, which means that some values appear a lot, while many
    others appear only a small fraction of the time.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will apply aggregating to a dataset that contains many cities
    as features but with no information about the city names. This will leave us with
    the only option being to aggregate by occurrence. We will reuse the same dataset
    as in the previous recipe, so we will require the Kaggle API. For that, please
    refer to the previous recipe. Using the Kaggle API, the dataset can be downloaded
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also need the `pandas` and `scikit-learn` libraries, which can be installed
    with the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will use the same dataset as in the previous recipe. To prepare for this
    recipe, we will aggregate the cities based on a given threshold on their occurrences
    in the dataset, and then train and evaluate a model on this data:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required modules, functions, and classes: `pandas` for loading the
    data, `train_test_split` for splitting the data, `StandardScaler` for rescaling
    quantitative features, `OneHotEncoder` for encoding qualitative features, and
    `LogisticRegression` as the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset with `pandas`. There’s no need to unzip the file first – this
    is all handled by `pandas`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Remove any missing data. As we did in the previous recipe, we will use a simple
    policy, remove all features that have a large amount of missing data, and then
    remove the rows with missing data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and test sets with the `train_test_split` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Rescale any quantitative features with the standard scaler provided by `scikit-learn`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we must aggregate the `city` feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the qualitative features with one-hot encoding, including the newly
    aggregated `city` feature:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Concatenate the quantitative and qualitative features back together:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate and train the logistic regression model. Here, we will just keep
    the default hyperparameters of the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute and print the model’s accuracy on both the training and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE121]'
  prefs: []
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: For this specific case, aggregating did not seem to help much in terms of giving
    us more robust results, but at the very least it has helped the model be less
    unpredictable and robust to new cities.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since the aggregation code may have looked complicated, let’s take a look at
    what we did.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, we have the `city` feature, which has many possible values; there’s a frequency
    for each value in the training set. These can be computed with the `.``value_counts(normalize=True)`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE123]'
  prefs: []
  type: TYPE_PRE
- en: 'It appears that, in the entire dataset, `city_103` is the value more than 23%
    of the time, while other values such as `city_111` appear less than 1% of the
    time. We will just apply a threshold to those values so that we get the list of
    cities appearing more than the given threshold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE124]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE125]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, all we have to do is get the index (that is, the city name) of all the
    true values. This is exactly what we can do with the following full line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE126]'
  prefs: []
  type: TYPE_PRE
- en: 'This shows the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE127]'
  prefs: []
  type: TYPE_PRE
- en: As expected, this returns list of cities that occur more than the threshold.
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling an imbalanced dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A typical case in machine learning is what we call an imbalanced dataset. An
    imbalanced dataset simply means that for a given class, some occurrences are much
    more likely than others, hence the lack of balance. There are plenty of cases
    of imbalanced datasets: rare diseases in medicine, customer behavior, and more.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this recipe, we will propose one possible way to handle imbalanced datasets:
    undersampling. After explaining this process, we will apply it to a credit card
    fraud detection dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The problem with imbalanced data is that it may bias the results of a machine
    learning model. Let’s assume we’re undertaking a classification task of detecting
    rare diseases present in only 1% of a dataset. A common pitfall with such data
    is to have a model predicting as always healthy as it would still have 99% accuracy.
    So, it would be very likely for a machine learning model to minimize its losses.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In such situations, other metrics such as the F1-score or **ROC Area Under Curve**
    (**ROC AUC**) are usually more relevant.
  prefs: []
  type: TYPE_NORMAL
- en: 'One way to prevent this from happening is to undersample the dataset. More
    specifically, we can undersample the overrepresented class by removing some samples
    of it:'
  prefs: []
  type: TYPE_NORMAL
- en: We keep all the samples of the underrepresented class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We keep only a subsample of the overrepresented class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'By doing this, we can artificially balance the dataset and avoid the pitfalls
    of an imbalanced dataset. For example, let’s say we have a dataset composed of
    the following attributes:'
  prefs: []
  type: TYPE_NORMAL
- en: 100 samples with disease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9,900 samples with no disease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A perfectly balanced undersampling would give us the following results in the
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 100 samples with disease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 100 randomly selected samples with no disease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Of course, the drawback is that we lose a lot of data in the process.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this recipe, we first need to download the dataset. To do so, we will use
    the Kaggle API (refer to the *Hashing high cardinality features* recipe to learn
    how to install it). The dataset can be downloaded with the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE128]'
  prefs: []
  type: TYPE_PRE
- en: 'The following libraries are also needed: `pandas` for loading the data, `scikit-learn`
    for modeling, `matplotlib` for displaying the data, and `imbalanced-learn` for
    undersampling. They can be installed with the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE129]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will apply undersampling to a credit card fraud dataset.
    This is a rather extreme case of an imbalanced dataset since only about 0.18%
    of the samples are positive:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required modules, classes, and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pandas` for data loading and manipulation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` for data splitting'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StandardScaler` for data rescaling (the dataset holds only quantitative features)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomUnderSampler` for undersampling'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LogisticRegression` for modeling'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roc_auc_score` for displaying the ROC and ROC AUC computations:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE130]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE136]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data with `pandas`. We can load the ZIP file directly. We will also
    display the relative amount of each label: we have around 99.8% of regular transactions
    compared to less than 0.18% of fraudulent transactions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into training and test sets. The need to stratify the label
    in such cases can be critical:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply random undersampling, up to a 10% sampling strategy. This means we must
    undersample the overrepresented class until there is a 10 to 1 ratio in the class
    balance. We could go up to a 1 to 1 ratio, but this would be at the cost of even
    more dropped data. This ratio is defined by the `sampling_strategy=0.1` parameter.
    We must also set the random state for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This gives us the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE151]'
  prefs: []
  type: TYPE_PRE
- en: After undersampling, we end up with `3940` regular transaction samples compared
    to `394` fraudulent transactions.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rescale the data using a standard scaler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE153]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE154]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE155]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Arguably, we could apply rescaling before resampling. This would give more weight
    to the overrepresented class when rescaling but would not be considered data leakage.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate and train the logistic regression model on the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE157]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the ROC AUC on both the training and test sets. To do so, we need the
    predicted probabilities for each sample, which we can get with the `predict_proba()`
    method, as well as the imported `roc_auc_score()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE158]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE159]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE161]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE166]'
  prefs: []
  type: TYPE_PRE
- en: We got a ROC AUC of about 97% on the test set and close to 99% on the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Optionally, we can plot the ROC curves for both the training and test sets.
    To do so, we can use the `roc_curve()` function from `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE167]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.2 – ROC curve for the train and test sets. Plot produced by the
    code](img/B19629_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – ROC curve for the train and test sets. Plot produced by the code
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, while the ROC AUC is very similar for the training and test sets,
    the curve for the test set is a bit lower. This means that, as expected, the model
    is overfitting slightly.
  prefs: []
  type: TYPE_NORMAL
- en: Note that fine-tuning `sampling_strategy` might be helpful to get better results.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To optimize the sampling strategy and the model hyperparameters at the same
    time, you can use scikit-learn’s `Pipeline` class.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The documentation for `RandomUnderSampler`: [https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The documentation for `Pipeline`: [https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here’s a great code example of a two-step pipeline hyperparameter optimization:
    [https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.xhtml](https://scikit-learn.org/stable/tutorial/statistical_inference/putting_together.xhtml%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Oversampling an imbalanced dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another solution when dealing with imbalanced datasets is random oversampling.
    This is the opposite of random undersampling. In this recipe, we’ll learn how
    to use it on the credit card fraud detection dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Random oversampling can be seen as the opposite of random undersampling: the
    idea is to duplicate samples of the underrepresented dataset to rebalance the
    dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As for the previous recipe, let’s assume a 1%-99% imbalanced dataset that contains
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 100 samples with disease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9,900 samples with no disease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To apply oversampling to this dataset using a 1/1 strategy (so, a perfectly
    balanced dataset), we would need to have 99 duplicates of each sample of the disease
    class. So, the oversampled dataset would need to contain the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 9,900 samples with disease (100 original samples duplicated 99 times on average)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 9,900 samples with no disease
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can easily guess the pros and cons of this method:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pro**: Unlike undersampling, we do not waste any data from the overrepresented
    class, which means our model can be trained on the full picture of the data we
    have'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Con**: We may have a lot of duplicates of the underrepresented class, leading
    to potential overfitting on this data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fortunately, we can choose a rebalancing strategy below 1/1 so that the underrepresented
    data duplication can be limited.
  prefs: []
  type: TYPE_NORMAL
- en: For this recipe, we need to download the dataset. If you completed the *Undersampling
    an imbalanced dataset* recipe, you don’t need to do anything else.
  prefs: []
  type: TYPE_NORMAL
- en: 'Otherwise, using the Kaggle API (refer to the *Hashing high cardinality features*
    recipe to learn how to install it), we need to download the dataset via the following
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE168]'
  prefs: []
  type: TYPE_PRE
- en: 'The following libraries are also needed: `pandas` for loading the data, `scikit-learn`
    for modeling, `matplotlib` for displaying the data, and `imbalanced-learn` for
    the oversampling part. They can be installed via the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE169]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will apply oversampling to the credit card fraud dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required modules, classes, and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pandas` for data loading and manipulation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` for data splitting'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StandardScaler` for data rescaling (the dataset only contains quantitative
    features)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomOverSampler` for oversampling'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LogisticRegression` for modeling'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roc_auc_score` for displaying the ROC and ROC AUC computations:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE170]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data with pandas. We can load the ZIP file directly. As we did in
    the previous recipe, we will display the relative amount of each label to remind
    us that we have about 99.8% of regular transactions and less than 0.18% of fraudulent
    transactions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE177]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE178]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE179]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into training and test sets. We must specify stratification
    on the labels to make sure the balance is still the same:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE180]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE182]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply random oversampling with a 10% sampling strategy. This means that we
    oversample the underrepresented class until there is a 10 to 1 ratio in the class
    balance. This ratio is defined by the `sampling_strategy=0.1` parameter. We must
    also set the random state for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE184]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE185]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE191]'
  prefs: []
  type: TYPE_PRE
- en: After oversampling, we now have `227451` regular transactions in the training
    set (which is unchanged) versus `22745` fraudulent transactions.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'It is possible to change the sampling strategy. As usual, it is a matter of
    doing a tradeoff: a greater sampling strategy means more duplicated samples for
    more balance, while a smaller sampling strategy means fewer duplicated samples
    but less balance.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Rescale the data using a standard scaler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate and train the logistic regression model on the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the ROC AUC on both the training and test sets. To do so, we need the
    predicted probabilities for each sample, which we can get by using the `predict_proba()`
    method, as well as the imported `roc_auc_score()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE201]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This returns the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs: []
  type: TYPE_PRE
- en: The results are quite comparable to the ones we obtained with undersampling.
    However, this does not mean that these two techniques are always equal.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Optionally, just like we did in the *Undersampling an imbalanced dataset* recipe,
    we can plot the ROC curves for both the training and test sets using the `roc_curve()`
    function from `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE207]'
  prefs: []
  type: TYPE_PRE
- en: '![Figure 5.3 – ROC curve for the train and test sets. Plot produced by the
    code](img/B19629_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – ROC curve for the train and test sets. Plot produced by the code
  prefs: []
  type: TYPE_NORMAL
- en: In this case, the ROC AUC curve for the test set is clearly below the one for
    the training set, which means that the model is overfitting slightly.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The documentation for `RandomUnderSampler` is available at [https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: Resampling imbalanced data with SMOTE
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Finally, a more complex solution for dealing with imbalanced datasets is a method
    called SMOTE. After explaining the SMOTE algorithm, we will apply this method
    to the credit card fraud detection dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**SMOTE** stands for **Synthetic Minority Oversampling TEchnique**. As its
    name suggests, it creates synthetic samples for an underrepresented class. But
    how exactly does it create synthetic data?'
  prefs: []
  type: TYPE_NORMAL
- en: 'This method uses the k-NN algorithm on the underrepresented class. The SMOTE
    algorithm can be summarized with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Randomly pick a sample, ![](img/F_05_001.png), in the minority class.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Using k-NN, randomly pick one of the k-nearest neighbors of ![](img/F_05_002.png)
    in the minority class. Let’s call this sample ![](img/F_05_003.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Compute the new synthetic sample, ![](img/F_05_004.png), with 𝜆 being randomly
    drawn in the [0, 1] range:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Visual representation of SMOTE](img/B19629_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Visual representation of SMOTE
  prefs: []
  type: TYPE_NORMAL
- en: 'Compared to random oversampling, this method is more complex since it has one
    hyperparameter: the number of nearest neighbors, *k*, to consider. This method
    also comes with pros and cons:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pro**: Unlike random oversampling, it limits the risks of overfitting on
    the underrepresented class since samples are not duplicated'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Con**: Creating synthetic data is a risky bet; nothing assures you that it
    has a meaning and would ever be likely on real data'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To complete this recipe, you will need to download the credit card fraud dataset
    if you haven’t done so already (check out the *Undersampling an imbalanced dataset*
    or *Oversampling an imbalanced dataset* recipe to learn how to do this).
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the Kaggle API (refer to the *Hashing high cardinality features* recipe
    to learn how to install it), we have to download the dataset via the following
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE208]'
  prefs: []
  type: TYPE_PRE
- en: 'The following libraries are needed: `pandas` for loading the data, `scikit-learn`
    for modeling, `matplotlib` for displaying the data, and `imbalanced-learn` for
    undersampling. They can be installed via the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE209]'
  prefs: []
  type: TYPE_PRE
- en: How to do it...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will apply SMOTE to the credit card fraud dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required modules, classes, and functions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`pandas` for data loading and manipulation'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` for data splitting'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`StandardScaler` for data rescaling (the dataset contains only quantitative
    features)'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`SMOTE` for the SMOTE oversampling'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`LogisticRegression` for modeling'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`roc_auc_score` for displaying the ROC and ROC AUC computations:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE211]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE212]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE213]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE216]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data with `pandas`. We can load the ZIP file directly. As we did in
    the previous two recipes, we will display the relative amount of each label. Again,
    we will have about 99.8% of regular transactions and less than 0.18% of fraudulent
    transactions:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE219]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into training and test sets. We must specify stratification
    on the labels to make sure the balance is still the same:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE223]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Apply SMOTE with a 10% sampling strategy with the `sampling_strategy=0.1` parameter.
    By doing this, we will generate synthetic data of the underrepresented class until
    there is a 10 to 1 ratio in the class balance. We must also set the random state
    for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE225]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE228]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE229]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE230]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'With this, we will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE231]'
  prefs: []
  type: TYPE_PRE
- en: After oversampling, we now have `227451` regular transactions in the training
    set (which is unchanged) versus `22745` fraudulent transactions, including many
    synthetically generated samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'Rescale the data using a standard scaler:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE232]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE233]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE234]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE235]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate and train the logistic regression model on the training set:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE236]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE237]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the ROC AUC on both the training and test sets. To do so, we need the
    predicted probabilities for each sample, which we can get by using the `predict_proba()`
    method, as well as the imported `roc_auc_score()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE238]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE239]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE240]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE241]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE242]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE243]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE244]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE245]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, the output should be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE246]'
  prefs: []
  type: TYPE_PRE
- en: The results are slightly different from the ones we got for random undersampling
    and oversampling. While the performances on the test set are quite similar, there
    seems to be more overfitting in this case. There are several possible explanations
    for such results, with one of them being that the synthetic samples were not very
    helpful for the model.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The results that we got for the resampling strategies on this dataset are not
    necessarily representative of the results we would get on any other dataset. Moreover,
    we had to fine-tune the sampling strategies and models to get a proper performance
    comparison.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more...
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Optionally, we can plot the ROC curves for both the training and test sets
    using the `roc_curve()` function from `scikit-learn`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE247]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – ROC curve for the train and test sets after using SMOTE](img/B19629_05_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – ROC curve for the train and test sets after using SMOTE
  prefs: []
  type: TYPE_NORMAL
- en: Compared to random undersampling and oversampling, overfitting appears to be
    even clearer here.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The official documentation for SMOTE can be found at [https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTE.xhtml).
  prefs: []
  type: TYPE_NORMAL
- en: It is not recommended that you apply this implementation to categorical features
    as it assumes that a feature value for a sample can be any linear combination
    of values of other samples. This is not true for categorical features.
  prefs: []
  type: TYPE_NORMAL
- en: 'Working implementations for categorical features have also been proposed, including
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**SMOTENC**: For working with datasets that contain both categorical and non-categorical
    features: [https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTENC.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SMOTEN**: For working with datasets that contain only categorical features:
    [https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.xhtml](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
