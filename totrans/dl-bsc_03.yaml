- en: 2\. Perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter describes an algorithm called a *perceptron*. Invented by the US
    researcher Frank Rosenblatt in 1957, it is from this traditional algorithm that
    neural networks (i.e., deep learning) originated and is thus a necessary first
    step to the more advanced study of both. This chapter will describe a perceptron
    and use one to solve easy problems. Throughout this process, you will familiarize
    yourself with the mechanics of perceptrons.
  prefs: []
  type: TYPE_NORMAL
- en: What Is a Perceptron?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A perceptron receives multiple signals as inputs and outputs one signal. The
    "signal" here "flows" like an electric current or a river. In the same way that
    an electric current flows through a conductor and pushes electrons forward, the
    signal in a perceptron makes flow and transfers information. Unlike an electric
    current, the signal in a perceptron is binary: "Flow (1) or Do not flow (0)."
    In this book, 0 indicates "do not flow a signal" and 1 indicates "flow a signal."'
  prefs: []
  type: TYPE_NORMAL
- en: (In the interest of precision, note that the perceptron described in this chapter
    is more accurately called an "artificial neuron" or a "simple perceptron." Here,
    we will call it a "perceptron" because the basic processes are often the same.)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2.1* shows an example of a perceptron that receives two signals as
    input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Perceptron with two inputs'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.1: Perceptron with two inputs'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: '*x*1 and *x*2 are input signals, *y* is an output signal, and *w*1 and *w*2
    are weights (w is the initial letter of "weight"). The circle in the preceding
    diagram is called a "neuron" or a "node." When input signals are sent to a neuron,
    each of them is multiplied by its own weight (*w*1*x*1 and *w*2*x*2). The neuron
    sums the signals that it receives and outputs 1 when the sum exceeds a certain
    limit value. This is sometimes called "firing a neuron." Here, the limit value
    is called a **threshold** and is represented by the *θ* symbol.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is all about the operating principle of a perceptron. Equation (2.1) shows
    what we described here:'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![1](img/Firgure_2.1a.png) | (2.1) |'
  prefs: []
  type: TYPE_TB
- en: A perceptron has a specific weight for each of multiple inputs, while the weight
    controls the importance of each signal. The larger the weight, the more important
    the signal for the weight.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: A weight is equivalent to electrical "resistance." Resistance is a parameter
    that measures the difficulty of passing an electric current. The smaller the resistance,
    the larger the current. Meanwhile, when the weight of a perceptron is larger,
    the signal that flows becomes larger. Both resistance and weight work in the same
    way in that they both control the difficulty (or ease) of passing a signal.
  prefs: []
  type: TYPE_NORMAL
- en: Simple Logic Circuits
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: AND Gate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The following are some easy problems that use a perceptron. We will look at
    logic circuits here. Let''s think about an AND gate first. An AND gate consists
    of two inputs and one output. The table of input and output signals in *Figure
    2.2* is called a "truth table." As shown in *Figure 2.2*, the AND gate outputs
    1 when two inputs are 1\. Otherwise, it outputs 0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: Truth table of an AND gate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.2: Truth table of an AND gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Now, we will use a perceptron to express this AND gate. We will determine the
    values of *w*1, *w*2, and *θ* so that they satisfy the truth table of *Figure
    2.2*. What values can we set to create a perceptron that satisfies the conditions
    of *Figure 2.2*?
  prefs: []
  type: TYPE_NORMAL
- en: Actually, there is an infinite number of combinations of the parameters that
    satisfy *Figure 2.2*. For example, when (*w*1, *w*2, *θ*) = (0.5, 0.5, 0.7), the
    perceptron works as shown in *Figure 2.2*. (0.5, 0.5, 0.8) and (1.0, 1.0, 1.0)
    also satisfy the conditions of the AND gate. If these parameters are set, the
    sum of weighted signals exceeds the given threshold, *θ*, when both *x*1 and *x*2
    are 1.
  prefs: []
  type: TYPE_NORMAL
- en: NAND and OR gates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Now, let's look at a NAND gate. NAND means Not AND, and the output of the NAND
    gate is the opposite of the AND gate. As shown in the truth table provided in
    *Figure 2.3*, it outputs 0 when both *x*1 and *x*2 are 1\. Otherwise, it outputs
    1\. What combinations of parameters are available for a NAND gate?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: Truth table of a NAND gate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.3: Truth table of a NAND gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: A combination of (*w*1, *w*2, *θ*) = (-0.5, -0.5, -0.7) can represent a NAND
    gate, and there is an infinite number of other combinations. In fact, you can
    build a NAND gate by inverting all the signs of the parameter values that build
    an AND gate.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's look at an OR gate, as shown in *Figure 2.4*. This is a logic circuit
    that outputs 1 if at least one of the input signals is 1\. What parameters do
    you think we can set for the OR gate?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: Truth table of an OR gate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.4: Truth table of an OR gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: We are the ones that determine the perceptron parameters here—not a computer.
    While looking at the "training data," also known as a truth table, we considered
    (found) the parameter values manually. In machine learning problems, we have a
    computer determines the parameter values automatically. **Training** is the task
    that determines the appropriate parameters, and we consider the structure (model)
    of the perceptron and give training data to the computer.
  prefs: []
  type: TYPE_NORMAL
- en: As described previously, we can use a perceptron to build AND, NAND, and OR
    logic circuits. What is important here is that the structure of a perceptron is
    the same for all of the AND, NAND, and OR gates. The differences between the three
    gates lie in the parameter values (weights and thresholds). Just like a versatile
    actor plays a wide variety of characters, the perceptron of the same structure
    changes into AND, NAND, and OR when the parameter values are adjusted appropriately.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing Perceptrons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Easy Implementation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s implement the preceding logic circuits with Python. Here, we will define
    the AND function, which takes `x1` and `x2` as arguments:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `w1`, `w2`, and `theta` parameters are initialized within the function.
    When the sum of the weighted inputs exceeds the threshold, it returns 1; otherwise,
    it returns 0\. Let''s check that the outputs are the same as the ones shown in
    *Figure 2.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: The outputs are as we expected. With that, you have built an AND gate. Although
    you can use a similar procedure to build a NAND or OR gate, we will change the
    implementation a little.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Weights and Bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Although the preceding implementation of an AND gate is simple and easy to
    understand, we will change it to a different implementation for the subsequent
    sections, switching *θ* in equation (2.1) to -b and representing the behavior
    of the perceptron in equation (2.2):'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![2](img/Figure_2.4a.png) | (2.2) |'
  prefs: []
  type: TYPE_TB
- en: 'Although the notation of the symbols has changed, equations (2.1) and (2.2)
    represent exactly the same thing. Here, b is called a bias and *w*1 and *w*2 are
    called **weights**. As equation (2.2) shows, the perceptron sums the input signal
    values multiplied by the weights and the bias. It outputs 1 if the sum exceeds
    0, and outputs 0 otherwise. Now, let''s use NumPy to implement equation (2.2).
    We will use the Python interpreter to check the results one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: As shown in this example, when NumPy arrays are multiplied, each of their elements
    is multiplied if the two arrays have the same number of elements. Therefore, when
    calculating `w*x`, each element is multiplied, ([0, 1] * [0.5, 0.5] => [0, 0.5]).
    In `np.sum(w*x)`, each element is summed. When the bias is added to this weighted
    sum, the calculation of equation (2.2) is complete.
  prefs: []
  type: TYPE_NORMAL
- en: Implementation with Weights and Bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can use weights and bias to implement an AND gate, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, -*θ* is called the bias, *b*. Note that the bias works differently from
    the weights, *w*1, and *w*2\. Specifically, *w*1 and *w*2 work as parameters that
    control the importance of input signals, while the bias works as the parameter
    that adjusts the ease of firing—that is, how likely it is that the output signal
    is 1\. For example, if *b* is -0.1, the neuron fires when the weighted sum of
    the input signals exceeds 0.1\. On the other hand, if *b* is -20.0, the neuron
    fires only when the weighted sum of input signals exceeds 20.0\. Thus, the value
    of the bias determines how easily the neuron fires. Although *w*1 and *w*2 are
    called "weights" and *b* is called "bias," all the parameters (that is, *b*, *w*1,
    and *w*2) are sometimes called "weights," depending on the context.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: The word "bias" also means "padding." It indicates that the output is increased
    if nothing is input (if the input is 0). Actually, if inputs *x*1 and *x*2 are
    0, the output is just the value of the bias when *b* + *w*1*x*1 + *w*2*x*2 is
    calculated in equation (2.2).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s implement the NAND and OR gates:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: As described in the previous section, the AND, NAND, and OR gates are the same
    in terms of structure for the perceptron and differ only in terms of the values
    of the weight parameters. When implementing the NAND and OR gates, only the values
    of the weights and bias are different from the AND gate.
  prefs: []
  type: TYPE_NORMAL
- en: Limitations of Perceptrons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As described so far, we can use a perceptron to implement AND, NAND, and OR
    logic gates. In this next section, you will consider an XOR gate.
  prefs: []
  type: TYPE_NORMAL
- en: XOR Gate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: An XOR gate is a gate circuit that is also called an *exclusive OR*. As shown
    in *Figure 2.5*, the output is 1 when either *x*1 or *x*2 is 1 ("exclusive" means
    "limited to only one person"). What should be the value of the weights to realize
    the XOR gate by using a perceptron?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: Truth table of an XOR gate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.5: Truth table of an XOR gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: In fact, we cannot build this XOR gate by using the perceptron that we have
    learned about so far. Why can we not build XOR even though we can build AND and
    OR gates?
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s examine the behavior of an OR gate visually. An OR gate satisfies
    the truth table in *Figure 2.5* when the weight parameters are (*b*, *w*1, *w*2)
    = (-0.5, 1.0, 1.0), for example. In this case, the perceptron is represented by
    equation (2.3):'
  prefs: []
  type: TYPE_NORMAL
- en: '| ![3](img/Firgure_2.5a.png) | (2.3) |'
  prefs: []
  type: TYPE_TB
- en: 'The perceptron represented by equation (2.3) generates two areas that are divided
    by the straight line -0.5 + *x*1 + *x*2 = 0\. One of the areas divided by the
    straight line outputs 1, while the other outputs 0\. *Figure 2.6* shows this graphically:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6: Visualizing a perceptron – the perceptron outputs 0 in the gray
    area, which satisfies the characteristics of an OR gate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.6: Visualizing a perceptron – the perceptron outputs 0 in the gray
    area, which satisfies the characteristics of an OR gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: An OR gate outputs 0 when (*x*1, *x*2) = (0, 0) and outputs 1 when (*x*1, *x*2)
    = (0, 1), (1, 0), and (1, 1). Here, a circle indicates 0, and a triangle indicates
    1\. To create an OR gate, we must divide between circles and triangles with a
    straight line. The straight line can actually divide four points correctly.
  prefs: []
  type: TYPE_NORMAL
- en: So, how about the case of an XOR gate? Can we create areas that divide between
    circles and triangles with a straight line, as in the case of an OR gate?
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7: Circles and triangles indicate the outputs of an XOR gate.'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.7: Circles and triangles indicate the outputs of an XOR gate.'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: However hard you may be trying to solve this, you cannot divide between circles
    and triangles with a straight line. One straight line cannot divide them.
  prefs: []
  type: TYPE_NORMAL
- en: Linear and Nonlinear
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: You cannot divide between circles and triangles with a straight line. However,
    you can divide them if you can remove the restriction of a "straight line." For
    example, you can create the areas that divide between circles and triangles, as
    shown in *Figure 2.8*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The limit of a perceptron is that it can only represent the areas divided by
    a straight line. It cannot represent a curve, as shown in *Figure 2.8*. The areas
    divided by a curve in *Figure 2.8* are called *nonlinear* areas, while those divided
    by a straight line are called *linear* areas. The words *linear* and *nonlinear*
    are often used in machine learning. You can visualize them with *Figures 2.6*
    and *2.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8: A curve can divide between circles and triangles'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.8: A curve can divide between circles and triangles'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Multilayer Perceptrons
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unfortunately, we cannot use a perceptron to represent an XOR gate. However,
    this is not terrible news. Actually, the merit of a perceptron lies in the fact
    that multiple layers of perceptrons can be stacked (the outline of this section
    is that multiple layers can represent XOR). We will look at the stacking layers
    later. Here, we can consider the problem of the XOR gate from another viewpoint.
  prefs: []
  type: TYPE_NORMAL
- en: Combining the Existing Gates
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: There are some methods we can follow to make an XOR gate. One of them is to
    combine the AND, NAND, and OR gates that we have created so far and wire them.
    Here, the AND, NAND, and OR gates are shown with symbols in *Figure 2.9*. The
    circle at the tip of the NAND gate in *Figure 2.9* indicates that an output has
    been reversed.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9: Symbols of the AND, NAND, and OR gates'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.9: Symbols of the AND, NAND, and OR gates'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Now, let''s think about how we can wire AND, NAND, and OR to create an XOR
    gate. Note that you can assign AND, NAND, or OR to each of the *?* symbols in
    *Figure 2.10* to complete an XOR gate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10: Replace the "?" symbols with an AND, NAND, or OR gate to complete
    an XOR gate!'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.10: Replace the "?" symbols with an AND, NAND, or OR gate to complete
    an XOR gate!'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: To be specific, the limitation of a perceptron described in the previous section
    is that a single layer of a perceptron cannot represent an XOR gate or divide
    nonlinear areas. Here, we will see that an XOR gate can be built by combining
    perceptrons (i.e., stacking layers).
  prefs: []
  type: TYPE_NORMAL
- en: 'The wiring in *Figure 2.11* can build an XOR gate. Here, *x*1 and *x*2 indicate
    input signals, while *y* indicates an output signal. *x*1 and *x*2 are the inputs
    to the NAND and OR gates, and the outputs of the NAND and OR gates are the inputs
    to the AND gate:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11: A combination of the AND, NAND, and OR gates constructs an XOR
    gate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.11: A combination of the AND, NAND, and OR gates constructs an XOR
    gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: 'Let''s check that the wiring in *Figure 2.11* can really form an XOR gate.
    Assuming that the output of the NAND is *s*1 and that of the OR is *s*2, we will
    complete the truth table. *Figure 2.12* shows the results. When we look at *x*1,
    *x*2, and *y*, we can see that they represent the outputs of the XOR:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12: Truth table of an XOR gate'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.12: Truth table of an XOR gate'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Implementing an XOR Gate
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Now, we will use Python to implement the XOR gate represented by the wiring
    in *Figure 2.11*. By using the AND, NAND, and OR functions that we defined previously,
    we can implement this as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The XOR function outputs the results as expected:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Now, we can build an XOR gate. After doing this, we will represent the XOR that
    we have just implemented with perceptrons (by showing neurons explicitly). *Figure
    2.13* shows this representation.
  prefs: []
  type: TYPE_NORMAL
- en: An XOR is a multilayer network, as shown in *Figure 2.13.* Here, we will call
    the leftmost column `Layer 0`, the next `Layer 1`, and the rightmost `Layer 2`.
  prefs: []
  type: TYPE_NORMAL
- en: 'The perceptron in *Figure 2.13* differs in shape from the AND and OR perceptrons
    we have looked at so far (*Figure 2.1*). The AND and OR perceptrons are single-layer,
    while the XOR perceptron is two-layer. A perceptron with multiple layers is sometimes
    called a **multilayered perceptron**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13: Representation of an XOR by perceptrons'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/fig02_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.13: Representation of an XOR by perceptrons'
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Although the perceptron in *Figure 2.13* consists of three layers, we will call
    it a "two-layer perceptron" because only the two layers (between layers 0 and
    1 and between layers 1 and 2) have weight. Some literature calls the perceptron
    in *Figure 2.13* a "three-layer perceptron" because it consists of three layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'A two-layer perceptron, as shown in *Figure 2.13*, sends and receives signals
    between the neurons in layers 0 and 1 and then between layers 1 and 2\. The following
    describes this behavior in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: Two neurons in layer 0 receive input signals and send signals to the neurons
    in layer 1.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The neurons in layer 1 send signals to the neuron in layer 2, which outputs
    y.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The behavior of this two-layer perceptron can be compared to an assembly through
    a pipeline. The worker on the first level (or first layer) works on the "component"
    that arrives and passes it to the worker on the second level (the second layer)
    when the task is finished. The worker in the second layer works on the "component"
    that was received from the worker in the first layer to complete and ship (output)
    it.
  prefs: []
  type: TYPE_NORMAL
- en: Thus, the perceptrons in an XOR gate "pass components" between workers. This
    two-layer structure enables perceptrons to build an XOR gate. This can be interpreted
    as "what cannot be achieved with a single-layer perceptron can be achieved by
    adding one layer." Perceptrons can provide a more flexible representation by stacking
    layers (deepening layers).
  prefs: []
  type: TYPE_NORMAL
- en: From NAND to a Computer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Multilayer perceptrons can create more complicated circuits than those we have
    examined so far. For example, an adder circuit that add numbers can be created
    with perceptrons. An encoder that converts a binary number into a decimal number
    and a circuit that outputs 1 when certain conditions are met (circuit for parity
    checks) can be represented with perceptrons. As a matter of fact, we can even
    use perceptrons to represent a computer.
  prefs: []
  type: TYPE_NORMAL
- en: A computer is a machine that processes information. When it receives input,
    a computer processes it in a certain way and outputs the result. Processing in
    a certain way means that both a computer and a perceptron have inputs and outputs
    and calculate them based on fixed rules.
  prefs: []
  type: TYPE_NORMAL
- en: Although it seems that a computer conducts very complicated processes inside
    it, in fact (surprisingly), a combination of NAND gates can reproduce what a computer
    does. The surprising fact that NAND gates are all we need to create a computer
    means that perceptrons can also represent a computer because a NAND gate can itself
    be made with perceptrons. Simply put, if we can create a computer by combining
    NAND gates, we can also represent one by combining only perceptrons (a combination
    of perceptrons can be represented as one multilayer perceptron).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You may find it hard to believe that a combination of NAND gates can create
    a computer. If you are interested in this topic, *The Elements of Computing Systems:
    Building a Modern Computer from First Principles* (The MIT Press) is recommended.
    This book aims to understand computers deeply. Under the motto "From NANDs to
    Tetris," it uses NANDs to create a computer that runs Tetris. If you read this
    book, you will realize that computers can be created from simple elements—that
    is, NANDs.'
  prefs: []
  type: TYPE_NORMAL
- en: Thus, multilayer perceptrons can achieve as complicated a representation as
    creating a computer. So, what perceptron structure can represent a computer? How
    many layers are needed to create a computer?
  prefs: []
  type: TYPE_NORMAL
- en: The answer is that, theoretically, a computer can be created with two-layer
    perceptrons. It has been proven that any function can be represented with two-layer
    perceptrons (to be accurate, when an activation function is a nonlinear sigmoid
    function – refer to the next chapter for details). However, it will be a very
    laborious job to create a computer by specifying the appropriate weights in a
    structure of two-layer perceptrons. Actually, to start from low-level components
    such as NANDs to create a computer, creating the required components (modules)
    step by step is natural—starting from AND and OR gates and proceeding to half
    adders and full adders, **arithmetic and logic units** (**ALUs**), and a CPU.
    Therefore, creating a structure of many layers is a natural way when representing
    a computer with perceptrons.
  prefs: []
  type: TYPE_NORMAL
- en: Although we will not create a computer in this book, please keep in mind that
    multilayer perceptrons enable nonlinear representations and that they can, in
    principle, represent what a computer does.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we covered perceptrons. The perceptron is a very simple algorithm,
    so you should be able to understand how it works quickly. The perceptron is the
    basis of a neural network, which we will learn about in the next chapter. These
    points may be summed up in the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: A perceptron is an algorithm with inputs and outputs. When it receives a certain
    input, it outputs a fixed value.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A perceptron has "weight" and "bias" parameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use perceptrons to represent logic circuits such as AND and OR gates.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An XOR gate cannot be represented with a single-layer perceptron.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A two-layer perceptron can be used to represent an XOR gate.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A single-layer perceptron can only represent linear areas, while a multilayer
    perceptron can represent nonlinear areas.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multilayer perceptrons can represent a computer (theoretically).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
