<html><head></head><body>
        <section>

                            <header class="header-title chapter-title">
                    Recurrent Neural Networks
                </header>
            
            <article>
                
<p class="mce-root">This chapter introduces recurrent neural networks, starting with the basic model and moving on to <em>newer</em> recurrent layers that are able to handle internal memory learning to remember, or forget, certain patterns found in datasets. We will begin by showing that recurrent networks are powerful in the case of inferring patterns that are temporal or sequential, and then we will introduce an improvement on the traditional paradigm for a model that has internal memory, which can be applied in both directions in the temporal space.</p>
<p class="mce-root">We will approach the learning task by looking at a sentiment analysis problem as a sequence-to-vector application, and then we will focus on an autoencoder as a vector-to-sequence and sequence-to-sequence model at the same time. By the end of this chapter, you will be able to explain why a long short-term memory model is better than the traditional dense approach. You will be able to describe how a bi-directional <span>long short-term memory model might represent an advantage over the single directional approach. You will be able to implement your own recurrent networks and apply them to NLP problems or to image-related applications, including sequence-to-vector, vector-to-sequence, and sequence-to-sequence modeling.</span></p>
<p class="mce-root">This chapter is organized as follows:</p>
<ul>
<li>Introduction to recurrent neural networks</li>
<li>Long short-term memory models</li>
<li>Sequence-to-vector models</li>
<li>Vector-to-sequence models</li>
<li>Sequence-to-sequence models</li>
<li>Ethical implications</li>
</ul>
<h1 id="uuid-97fa8734-83d0-49fb-9aee-bebcfcd14ec9">Introduction to recurrent neural networks</h1>
<p><strong>Recurrent neural networks</strong> (<strong>RNNs</strong>) are based on the early work of Rumelhart (Rumelhart, D. E., et al. (1986)), who was a psychologist who worked closely with Hinton, whom we have already mentioned here several times. The concept is simple, but revolutionary in the area of pattern recognition that uses sequences of data. </p>
<div class="packt_infobox">A <strong>sequence of data</strong> is any piece of data that has high correlation in either time or space. Examples include audio sequences and images.</div>
<p>The concept of recurrence in RNNs can be illustrated as shown in the following diagram. If you think of a dense layer of neural units, these can be stimulated using some input at different time steps, <img class="fm-editor-equation" src="assets/8e859b39-e2bf-4310-b7df-7d1aa72d46ca.png" style="width:0.58em;height:1.25em;"/>. <em>Figures 13.1 (b)</em> and <em>(c)</em> show an RNN with five time steps, <img class="fm-editor-equation" src="assets/9c53c6b3-e449-4a6f-9c95-12ff4cb30010.png" style="width:2.75em;height:1.08em;"/>. We can see in <em>Figures 13.1 (b)</em> and <em>(c)</em> how the input is accessible to the different time steps, but more importantly, the output of the neural units is also available to the next layer of neurons: </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/1f337f3e-cee6-404b-a74b-db34d983cdfa.png" style="width:40.33em;height:40.92em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13.1. Different representations of recurrent layers: (a) will be the preferred use in this book; (b) depicts the neural units and the feedback loop; and (c) is the expanded version of (b), showing what really happens during training</div>
<p>The ability of an RNN to see how the previous layer of neurons is stimulated helps the network to interpret sequences much better than without that additional piece of information. However, this comes at a cost: there will be more parameters to be calculated in comparison to a traditional dense layer due to the fact that there are weights associated with the input <img class="fm-editor-equation" src="assets/37624e33-af8b-4b40-8359-c74bd9dec3cd.png" style="width:1.17em;height:1.00em;"/> and the previous output <img class="fm-editor-equation" src="assets/5869c1cc-4186-404b-802e-caadae5f3bbe.png" style="width:2.33em;height:1.08em;"/>. </p>
<h2 id="uuid-441924ac-b014-4da8-a554-f96110a25867">Simple RNNs</h2>
<p>In Keras, we can create a simple RNN with <strong>five time steps</strong> and <strong>10 neural units</strong> (see <em>Figure 13.1</em>) as follows:</p>
<pre>from tensorflow.keras import Sequential<br/>from tensorflow.keras.layers import <strong>SimpleRNN</strong><br/><br/>n_units = <strong>10</strong><br/>t_steps = <strong>5</strong><br/>inpt_ftrs=2<br/>model = Sequential()<br/>model.add(<strong>SimpleRNN</strong>(n_units, input_shape=(t_steps, inpt_ftrs)))<br/>model.summary()</pre>
<p>This gives the following summary:</p>
<pre>Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)            Output Shape  Param # <br/>=================================================================<br/>simple_rnn (SimpleRNN)  (None, 10)    130 <br/>=================================================================<br/>Total params: 130<br/>Trainable params: 130<br/>Non-trainable params: 0</pre>
<p>The preceding sample code assumes that the number of <strong>features in the input</strong> would be just <strong>two</strong>; for example, we can have sequential data in two dimensions. These types of RNNs are called <em>simple</em> because they resemble the simplicity of dense networks with <kbd>tanh</kbd> activations and a recurrence aspect to it.</p>
<p>RNNs are usually tied to embedding layers, which we discuss next.</p>
<h2 id="uuid-fce08f99-342c-4e5c-a09c-734c1006c8d3">Embedding layers</h2>
<p>An embedding layer is usually paired with RNNs when there are sequences that require additional processing in order to make RNNs more robust. Consider the case when you have the sentence <em>"This is a small vector"</em>, and you want to train an RNN to detect when sentences are correctly written or poorly written. You can train an RNN with all the sentences of length five that you can think of, including <em>"This is a small vector".</em> For this, you will have to figure out a way to transform a sentence into something that the RNN can understand. Embedding layers come to the rescue.</p>
<p>There is a technique called <strong>word embedding</strong>, which is tasked with converting a word into a vector. There are several successful approaches out there, such as Word2Vec (Mikolov, T., et al. (2013)) or GloVe (Pennington, J., et al. (2014)). However, we will focus on a simple technique that is readily available. We will do this in steps:</p>
<ol>
<li>Determine the length of the sentences you want to work on. This will become the dimensionality of the input for the RNN layer. This step is not necessary for the design of the embedding layer, but you will need it for the RNN layer very soon, and it is important that you decide this early on.</li>
<li>Determine the number of different words in your dataset and assign a number to them, creating a dictionary: word-to-index. This is known as a vocabulary. </li>
</ol>
<div class="packt_tip">Most people will determine the vocabulary and then calculate the frequency of each word to rank the words in the vocabulary so as to have the index 0 corresponding to the most common word in the dataset, and the last index corresponding to the most uncommon word. This can be helpful if you want to ignore the most common words or the most uncommon words, for example.</div>
<ol start="3">
<li>Substitute the words in all the sentences of the dataset with their corresponding index.</li>
<li>Determine the dimensionality of the word embedding and train an embedding layer to map from the numerical index into a real-valued vector with the desired dimensions. </li>
</ol>
<p>Look at the example in <em>Figure 13.2</em>. If we take the word <em>This</em>, whose given index is 7, some trained embedding layer can map that number into a vector of size 10, as you can see in <em>Figure 13.2 (b)</em>. That is the word embedding process.</p>
<p>You can repeat this process for the complete sentence <em>"This is a small vector"</em>, which can be mapped to a <strong>sequence</strong> of indices [7, 0, 6, 1, 28], and it will produce for you a <strong>sequence</strong> of vectors; see <em>Figure 13.2 (c)</em>. In other words, it will produce a <strong>sequence of word embeddings</strong>. The RNN can easily process these sequences and determine whether the sentence that these sequences represent is a correct sentence.</p>
<p>However, we must say that determining whether a sentence is correct is a challenging and interesting problem (Rivas, P. et al. (2019)):</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/d91edafe-3594-4b85-890f-9cb37ef14142.png" style="width:37.75em;height:37.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13.2. Embedding layer: (a) will be the preferred use in this book; (b) shows an example of a word embedding; and (c) shows a sequence of words and its corresponding matrix of word embeddings</div>
<p class="mce-root">Based on the model shown in <em>Figure 13.2</em>, an embedding layer in Keras can be created as follows:</p>
<pre>from tensorflow.keras import Sequential<br/>from tensorflow.keras.layers import <strong>Embedding</strong><br/><br/>vocab_size = 30<br/>embddng_dim = 10<br/>seqnc_lngth = 5<br/><br/>model = Sequential()<br/>model.add(<strong>Embedding</strong>(<strong>vocab_size</strong>, <strong>embddng_dim</strong>, input_length=<strong>seqnc_lngth</strong>))<br/>model.summary()</pre>
<p>This produces the following summary:</p>
<pre>Model: "sequential"<br/>_________________________________________________________________<br/>Layer (type)             Output Shape     Param # <br/>=================================================================<br/>embedding (Embedding)    (None, 5, 10)    300 <br/>=================================================================<br/>Total params: 300<br/>Trainable params: 300<br/>Non-trainable params: 0</pre>
<p>Note, however, that the vocabulary size is usually in the order of thousands for typical NLP tasks in most common languages. Just think of your good old-fashioned dictionary ... How many entries does it have? Several thousand, usually.</p>
<p>Similarly, sentences are usually longer than five words, so you should expect to have longer sequences than in the preceding example.</p>
<p>Finally, the embedding dimension depends on how rich you want your model to be in the embedding space, or on your model space constraints. If you want a smaller model, consider having embeddings of 50 dimensions for example. But if space is not a problem and you have an excellent dataset with millions of entries, and you have unlimited GPU power, you should try embedding dimensions of 500, 700, or even 1000+ dimensions.</p>
<p>Now, let's try to put the pieces together with a real-life example.</p>
<h2 id="uuid-b3e4798c-19bc-421d-8408-af95631c888b">Word embedding and RNNs on IMDb </h2>
<p>The IMDb dataset was explained in previous chapters, but to keep things brief, we will say that it has movie reviews based on text and a positive (1) or negative (0) review associated with every entry.</p>
<p>Keras lets you have access to this dataset and gives a couple of nice features to optimize time when designing a model. For example, the dataset is already processed according to the frequency of each word such that the smallest index is associated with frequent words and vice versa. With this in mind, you can also exclude the most common words in the English language, say 10 or 20. And you can even limit the size of the vocabulary to, say, 5,000 or 10,000 words.</p>
<p>Before we go further, we will have to justify some things you are about see:</p>
<ul>
<li> A vocabulary size of 10,000. We can make an argument in favor of keeping a vocabulary size of 10,000 since the task here is to determine whether a review is positive or negative. That is, we do not need an overly complex vocabulary to determine this.</li>
<li>Eliminating the top 20 words. The most common words in English include words such as "a" or "the"; words like these are probably not very important in determining whether a movie review is positive or negative. So, eliminating the 20 most common should be OK.</li>
<li>Sentence length of 128 words. Having smaller sentences, such as 5-word sentences, might be lacking enough content, and it would not make a lot of sense having longer sentences, such as 300-word sentences, since we can probably sense the tone of a review in fewer words than that. The choice of 128 words is completely arbitrary, but justified in the sense explained.</li>
</ul>
<p>With such considerations, we can easily load the dataset as follows:</p>
<pre>from keras.datasets import <strong>imdb</strong><br/>from keras.preprocessing import sequence<br/><br/>inpt_dim = <strong>128</strong><br/>index_from = 3<br/><br/>(x_train, y_train),(x_test, y_test)=<strong>imdb.load_data</strong>(num_words=<strong>10000</strong>,<br/>                                                   start_char=1,<br/>                                                   oov_char=2,<br/>                                                   index_from=index_from,<br/>                                                   skip_top=<strong>20</strong>)<br/>x_train = sequence.pad_sequences(x_train, <br/>                                 maxlen=<strong>inpt_dim</strong>).astype('float32')<br/>x_test = sequence.pad_sequences(x_test, maxlen=<strong>inpt_dim</strong>).astype('float32')<br/><br/># let's print the shapes<br/>print('x_train shape:', x_train.shape)<br/>print('x_test shape:', x_test.shape)</pre>
<p>We can also print some data for verification purposes like this:</p>
<pre># let's print the indices of sample #7<br/>print(' '.join(str(int(id)) for id in x_train[7]))<br/><br/># let's print the actual words of sample #7<br/>wrd2id = imdb.<strong>get_word_index</strong>()<br/>wrd2id = {k:(v+index_from) for k,v in wrd2id.items()}<br/>wrd2id["&lt;PAD&gt;"] = 0<br/>wrd2id["&lt;START&gt;"] = 1<br/>wrd2id["&lt;UNK&gt;"] = 2<br/>wrd2id["&lt;UNUSED&gt;"] = 3<br/><br/>id2wrd = {value:key for key,value in wrd2id.items()}<br/>print(' '.join(id2wrd[id] for id in x_train[7] ))</pre>
<p>This will output the following:</p>
<pre>x_train shape: (25000, 128)<br/>x_test shape: (25000, 128)<br/><br/> 55   655    707    6371     956    225    1456    841     42 1310   225     2 ...<br/>very middle class suburban setting there's zero atmosphere or mood there's &lt;UNK&gt; ...</pre>
<p>The first part of the preceding code shows how to load the dataset split into training and test sets, <kbd>x_train</kbd> and <kbd>y_train</kbd>, <kbd>x_test</kbd> and <kbd>y_test</kbd>, respectively. The remaining part is simply to display the shape of the dataset (dimensionality) for purposes of verification, and also for verification, we can print out sample #7 in its original form (the indices) and also its corresponding word. Such a portion of the code is a little bit strange if you have not used IMDb before. But the major points are that we need to reserve certain indices for special tokens: beginning of the sentence <kbd>&lt;START&gt;</kbd>, unused index <kbd>&lt;UNUSED&gt;</kbd>, unknown word index <kbd>&lt;UNK&gt;</kbd>, and zero padding index <kbd>&lt;PAD&gt;</kbd>. One we have made a special allocation for these tokens, we can easily map from the indices back to words. These indices will be learned by the RNN, and it will learn how to handle them, either by ignoring those, or by giving specific weights to them.</p>
<p>Now, let's implement the architecture shown in the following diagram, which uses all the layers explained previously:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/50bfc3af-30bc-4ba2-8e63-699a4b32fbec.png" style="width:36.58em;height:26.75em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13.3. An RNN architecture for the IMDb dataset</div>
<p>The diagram shows the same example (#7 from the training set) that is associated with a negative review. The architecture depicted in the diagram along with the code that loads the data is the following:</p>
<pre>from keras.datasets import imdb<br/>from keras.preprocessing import sequence<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import SimpleRNN, Embedding, BatchNormalization<br/>from tensorflow.keras.layers import Dense, Activation, Input, Dropout<br/><br/>seqnc_lngth = 128<br/>embddng_dim = 64<br/>vocab_size = 10000<br/><br/>(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=<strong>vocab_size</strong>, <br/>                                                      skip_top=20)<br/>x_train = sequence.pad_sequences(x_train, <br/>                                 maxlen=<strong>seqnc_lngth</strong>).astype('float32')<br/>x_test = sequence.pad_sequences(x_test, <br/>                                maxlen=<strong>seqnc_lngth</strong>).astype('float32')</pre>
<p>The layers of the model are defined as follows:</p>
<pre>inpt_vec = Input(shape=(<strong>seqnc_lngth</strong>,))<br/>l1 = <strong>Embedding</strong>(<strong>vocab_size</strong>, <strong>embddng_dim</strong>, input_length=<strong>seqnc_lngth</strong>)(inpt_vec)<br/>l2 = Dropout(0.3)(l1)<br/>l3 = <strong>SimpleRNN</strong>(32)(l2)<br/>l4 = BatchNormalization()(l3)<br/>l5 = Dropout(0.2)(l4)<br/>output = Dense(1, activation='sigmoid')(l5)<br/><br/>rnn = Model(inpt_vec, output)<br/><br/>rnn.compile(loss='binary_crossentropy', optimizer='adam', <br/>            metrics=['accuracy'])<br/>rnn.summary()</pre>
<p>This model uses the standard loss and optimizer that we have used before, and the summary produced is the following:</p>
<pre>Model: "functional"<br/>_________________________________________________________________<br/>Layer (type)                 Output Shape     Param # <br/>=================================================================<br/>input_1 (InputLayer)         [(None, 128)]    0 <br/>_________________________________________________________________<br/>embedding (Embedding)        (None, 128, 64)  640000 <br/>_________________________________________________________________<br/>dropout_1 (Dropout)          (None, 128, 64)  0 <br/>_________________________________________________________________<br/>simple_rnn (SimpleRNN)       (None, 32)       3104 <br/>_________________________________________________________________<br/>batch_normalization (BatchNo (None, 32)       128 <br/>_________________________________________________________________<br/>dropout_2 (Dropout)          (None, 32)       0 <br/>_________________________________________________________________<br/>dense (Dense)                (None, 1)        33 <br/>=================================================================<br/>Total params: 643,265<br/>Trainable params: 643,201<br/>Non-trainable params: 64</pre>
<p>Then we can train the network using the callbacks that we have used before: a) early stopping, and b) automatic learning rate reduction. The learning can be executed as follows:</p>
<pre>from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping<br/>import matplotlib.pyplot as plt<br/><br/>#callbacks<br/>reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, <br/>                              min_delta=1e-4, mode='min', verbose=1)<br/><br/>stop_alg = EarlyStopping(monitor='val_loss', patience=7, <br/>                         restore_best_weights=True, verbose=1)<br/><br/>#training<br/>hist = rnn.fit(x_train, y_train, batch_size=100, epochs=1000, <br/>               callbacks=[stop_alg, reduce_lr], shuffle=True, <br/>               validation_data=(x_test, y_test))<br/><br/></pre>
<p>Then we save the model and display the loss like so:</p>
<pre># save and plot training process<br/>rnn.save_weights("rnn.hdf5")<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.plot(hist.history['loss'], color='#785ef0')<br/>plt.plot(hist.history['val_loss'], color='#dc267f')<br/>plt.title('Model Loss Progress')<br/>plt.ylabel('Brinary Cross-Entropy Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training Set', 'Test Set'], loc='upper right')<br/>plt.show()</pre>
<p>The preceding code produces the plot shown in the following diagram, which indicates that the network starts to overfit after epoch #3: </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/42c839cf-deb9-4108-879b-f8df79fe75cf.png" style="width:42.83em;height:27.17em;"/></div>
<p class="mce-root"/>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13.4. RNN loss during training</div>
<p>Overfitting is quite common in recurrent networks and you should not be surprised by this behavior. As of today, with the current algorithms, this happens a lot. However, one interesting fact about RNNs is that they also converge really fast compared to other traditional models. As you can see, convergence after three epochs is not too bad. </p>
<p>Next, we must examine the actual classification performance by looking at the balanced accuracy, the confusion matrix, and the <strong>area under the ROC curve</strong> (<strong>AUC</strong>). We will do this only in the test set as follows:</p>
<pre>from sklearn.metrics import confusion_matrix<br/>from sklearn.metrics import balanced_accuracy_score<br/>from sklearn.metrics import roc_curve, auc<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/><br/>y_hat = rnn.predict(x_test)<br/><br/># gets the ROC<br/>fpr, tpr, thresholds = roc_curve(y_test, y_hat)<br/>roc_auc = auc(fpr, tpr)<br/><br/># plots ROC<br/>fig = plt.figure(figsize=(10,6))<br/>plt.plot(fpr, tpr, color='#785ef0', <br/>         label='ROC curve (AUC = %0.2f)' % roc_auc)<br/>plt.plot([0, 1], [0, 1], color='#dc267f', linestyle='--')<br/>plt.xlim([0.0, 1.0])<br/>plt.ylim([0.0, 1.05])<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.title('Receiver Operating Characteristic Curve')<br/>plt.legend(loc="lower right")<br/>plt.show()<br/><br/># finds optimal threshold and gets ACC and CM<br/>optimal_idx = np.argmax(tpr - fpr)<br/>optimal_threshold = thresholds[optimal_idx]<br/>print("Threshold value is:", optimal_threshold)<br/>y_pred = np.where(y_hat&gt;=optimal_threshold, 1, 0)<br/>print(balanced_accuracy_score(y_test, y_pred))<br/>print(confusion_matrix(y_test, y_pred))</pre>
<p>First, let's analyze the plot produced here, which is shown in <em>Figure 13.5</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a2deb947-03bf-45c9-a981-eb58bf955efb.png" style="width:42.08em;height:26.33em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13.5. ROC and AUC of the RNN model calculated in the test set</div>
<p>The diagram shows a good combination of <strong>True Positive Rates</strong> (<strong>TPR</strong>) and <strong>False Positive Rates</strong> (<strong>FPR</strong>), although it is not ideal: we would like to see a sharper step-like curve. The AUC is 0.92, which again is good, but the ideal would be an AUC <span>of 1.0</span>.</p>
<p>Similarly, the code produces the balanced accuracy and confusion matrix, which would look something like this:</p>
<pre>Threshold value is: 0.81700134<br/><br/>0.8382000000000001<br/><br/>[[10273 2227]<br/> [ 1818 10682]]</pre>
<p>First of all, we calculate here the optimal threshold value as a function of the TPR and FPR. We want to choose the threshold that will give us the maximum TPR and minimum FPR. The threshold and results shown here <strong>will vary</strong> depending on the initial state of the network; however, the accuracy should typically be around a very similar value. </p>
<p>Once the optimal threshold is calculated, we can use NumPy's <kbd>np.where()</kbd> method to threshold the entire predictions, mapping them to {0, 1}. After this, the balanced accuracy is calculated to be 83.82%, which again is not too bad, but also not ideal.</p>
<p>One of the possible ways to improve on the RNN model shown in <em>Figure 13.3</em> would be to somehow give the recurrent layer the ability to <em>remember</em> or <em>forget </em>specific words across layers and have them continue to stimulate neural units across the sequence. The next section will introduce a type of RNN with such capabilities.</p>
<h1 id="uuid-c77e916b-7e4d-4036-a69f-86aa05ba4df9">Long short-term memory models</h1>
<p>Initially proposed by Hochreiter, <strong>Long Short-Term Memory Models</strong> (<strong>LSTMs</strong>) gained traction as an improved version of recurrent models [Hochreiter, S., <em>et al.</em> (1997)]. LSTMs promised to alleviate the following problems associated with traditional RNNs:</p>
<ul>
<li>Vanishing gradients</li>
<li>Exploding gradients</li>
<li>The inability to remember or forget certain aspects of the input sequences</li>
</ul>
<p>The following diagram shows a very simplified version of an LSTM. In <em>(b)</em>, we can see the additional self-loop that is attached to some memory, and in <em>(c)</em>, we can observe what the network looks like when unfolded or expanded:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/a4f0eb33-ca40-4135-b0b9-9a854216321a.png" style="width:36.25em;height:47.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13.6. Simplified representation of an LSTM</div>
<p>There is much more to the model, but the most essential elements are shown in <em>Figure 13.6</em>. Observe how an LSTM layer receives from the previous time step not only the previous output, but also something called <strong>state</strong>, which acts as a type of memory. In the diagram, you can see that while the current output and state are available to the next layer, these are also available to use at any point if they are needed.</p>
<p>Some of the things that we are not showing in <em>Figure 13.6</em> include the mechanisms by which the LSTM remembers or forgets. These can be complex to explain in this book for beginners. However, all you need to know at this point is that there are three major mechanisms: </p>
<ul>
<li style="list-style-type: none">
<ul>
<li><strong>Output control</strong>: How much an output neuron is stimulated by the previous output and the current state</li>
<li><strong>Memory control</strong>: How much of the previous state will be forgotten in the current state</li>
<li><strong>Input control</strong>: How much of the previous output and new state (memory) will be considered to determine the new current state</li>
</ul>
</li>
</ul>
<p>These mechanisms are trainable and optimized for each and every single dataset of sequences. But to show the advantages of using an LSTM as our recurrent layer, we will repeat the exact same code as before, only changing the RNN by an LSTM.</p>
<p>The code to load the dataset and build the model is the following:</p>
<pre>from keras.datasets import imdb<br/>from keras.preprocessing import sequence<br/>from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import <strong>LSTM</strong>, Embedding, BatchNormalization<br/>from tensorflow.keras.layers import Dense, Activation, Input, Dropout<br/><br/>seqnc_lngth = 128<br/>embddng_dim = 64<br/>vocab_size = 10000<br/><br/>(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=vocab_size, <br/>                                                      skip_top=20)<br/>x_train = sequence.pad_sequences(x_train, maxlen=seqnc_lngth).astype('float32')<br/>x_test = sequence.pad_sequences(x_test, maxlen=seqnc_lngth).astype('float32')</pre>
<p>The model can be specified as follows:</p>
<pre>inpt_vec = Input(shape=(seqnc_lngth,))<br/>l1 = Embedding(vocab_size, embddng_dim, input_length=seqnc_lngth)(inpt_vec)<br/>l2 = Dropout(0.3)(l1)<br/>l3 = <strong>LSTM</strong>(32)(l2)<br/>l4 = BatchNormalization()(l3)<br/>l5 = Dropout(0.2)(l4)<br/>output = Dense(1, activation='sigmoid')(l5)<br/><br/>lstm = Model(inpt_vec, output)<br/><br/>lstm.compile(loss='binary_crossentropy', optimizer='adam', <br/>             metrics=['accuracy'])<br/>lstm.summary()</pre>
<p>This produces the following output:</p>
<pre>Model: "functional"<br/>_________________________________________________________________<br/>Layer (type) Output Shape Param # <br/>=================================================================<br/>input (InputLayer)          [(None, 128)] 0 <br/>_________________________________________________________________<br/>embedding (Embedding)       (None, 128, 64) 640000 <br/>_________________________________________________________________<br/>dropout_1 (Dropout)         (None, 128, 64) 0 <br/>_________________________________________________________________<br/>lstm (LSTM)                 (None, 32) 12416 <br/>_________________________________________________________________<br/>batch_normalization (Batch  (None, 32) 128 <br/>_________________________________________________________________<br/>dropout_2 (Dropout)         (None, 32) 0 <br/>_________________________________________________________________<br/>dense (Dense)               (None, 1) 33 <br/>=================================================================<br/>Total params: 652,577<br/>Trainable params: 652,513<br/>Non-trainable params: 64</pre>
<p>This essentially replicates the model shown in the following diagram: </p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/33464f77-9425-4bb9-8172-8e17ad37ce68.png" style="width:36.67em;height:27.00em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13.7. LSTM-based neural architecture for the IMDb dataset</div>
<p>Notice that this model has nearly 10,000 more parameters than the simple RNN approach. However, the premise is that this increase in parameters should also result in an increase in performance.</p>
<p>We then train our model the same as before, like so:</p>
<pre>from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping<br/>import matplotlib.pyplot as plt<br/><br/>#callbacks<br/>reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, <br/>                              min_delta=1e-4, mode='min', verbose=1)<br/><br/>stop_alg = EarlyStopping(monitor='val_loss', patience=7, <br/>                         restore_best_weights=True, verbose=1)<br/><br/>#training<br/>hist = lstm.fit(x_train, y_train, batch_size=100, epochs=1000, <br/>                callbacks=[stop_alg, reduce_lr], shuffle=True, <br/>                validation_data=(x_test, y_test))</pre>
<p>Next we save the model and display its performance as follows:</p>
<pre># save and plot training process<br/>lstm.save_weights("lstm.hdf5")<br/><br/>fig = plt.figure(figsize=(10,6))<br/>plt.plot(hist.history['loss'], color='#785ef0')<br/>plt.plot(hist.history['val_loss'], color='#dc267f')<br/>plt.title('Model Loss Progress')<br/>plt.ylabel('Brinary Cross-Entropy Loss')<br/>plt.xlabel('Epoch')<br/>plt.legend(['Training Set', 'Test Set'], loc='upper right')<br/>plt.show()</pre>
<p>This code will produce the plot shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/e37d404e-9f38-4e4b-8609-84255ffe6f5d.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13.8. Loss across epochs of training an LSTM</div>
<p>Notice from the diagram that the model begins to overfit after <strong>one epoch</strong>. Using the trained model at the best point, we can calculate the actual performance as follows:</p>
<pre>from sklearn.metrics import confusion_matrix<br/>from sklearn.metrics import balanced_accuracy_score<br/>from sklearn.metrics import roc_curve, auc<br/>import matplotlib.pyplot as plt<br/>import numpy as np<br/><br/>y_hat = lstm.predict(x_test)<br/><br/># gets the ROC<br/>fpr, tpr, thresholds = roc_curve(y_test, y_hat)<br/>roc_auc = auc(fpr, tpr)<br/><br/># plots ROC<br/>fig = plt.figure(figsize=(10,6))<br/>plt.plot(fpr, tpr, color='#785ef0', <br/>         label='ROC curve (AUC = %0.2f)' % roc_auc)<br/>plt.plot([0, 1], [0, 1], color='#dc267f', linestyle='--')<br/>plt.xlim([0.0, 1.0])<br/>plt.ylim([0.0, 1.05])<br/>plt.xlabel('False Positive Rate')<br/>plt.ylabel('True Positive Rate')<br/>plt.title('Receiver Operating Characteristic Curve')<br/>plt.legend(loc="lower right")<br/>plt.show()<br/><br/># finds optimal threshold and gets ACC and CM<br/>optimal_idx = np.argmax(tpr - fpr)<br/>optimal_threshold = thresholds[optimal_idx]<br/>print("Threshold value is:", optimal_threshold)<br/>y_pred = np.where(y_hat&gt;=optimal_threshold, 1, 0)<br/>print(balanced_accuracy_score(y_test, y_pred))<br/>print(confusion_matrix(y_test, y_pred))</pre>
<p>This produces the ROC shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/f2baf0a7-1cf9-4b01-b57e-38169a66ec94.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13.9. ROC curve of an LSTM-based architecture</div>
<p>From the plot, we can see that there is a slight gain in the model, producing an AUC of 0.93 when the simple RNN model had an AUC of 0.92. </p>
<p>When looking at the balanced accuracy and the confusion matrix, which was produced by the preceding code, this shows numbers like these:</p>
<pre>Threshold value is: 0.44251397<br/>0.8544400000000001<br/>[[10459 2041]<br/> [ 1598 10902]]</pre>
<p>Here, we can appreciate that the accuracy was of 85.44%, which is a gain of about 2% over the simple RNN. We undertook this experiment simply to show that by switching the RNN models, we can easily see improvements. Of course there are other ways to improve the models, such as the following:</p>
<ul>
<li>Increase/reduce the vocabulary size</li>
<li>Increase/reduce the sequence length</li>
<li>Increase/reduce the embedding dimension</li>
<li>Increase/reduce the neural units in recurrent layers</li>
</ul>
<p>And there may be others besides. </p>
<p>So far, you have seen how to take text representations (movie reviews), which is a common NLP task, and find a way to represent those in a space where you can classify them into negative or positive reviews. We did this through embedding and LSTM layers, but at the end of this, there is a dense layer with one neuron that gives the final output. We can think of this as mapping from the text space into a one-dimensional space where we can perform classification. We say this because there are three main ways in which to consider these mappings:</p>
<ul>
<li><strong>Sequence-to-vector</strong>: Just like the example covered here, mapping sequences to an <em>n-</em>dimensional space.</li>
<li><strong>Vector-to-sequence</strong>: This goes the opposite way, from an <em>n</em>-dimensional space to a sequence.</li>
<li><strong>Sequence-to-sequence</strong>: This maps from a sequence to a sequence, usually going through an <em>n</em>-dimensional mapping in the middle.</li>
</ul>
<p>To exemplify these things, we will use an autoencoder architecture and MNIST in the next sections.</p>
<h1 id="uuid-2be10790-d623-406d-b3e1-2d10077477f1">Sequence-to-vector models</h1>
<p>In the previous section, you <em>technically </em>saw a sequence-to-vector model, which took a sequence (of numbers representing words) and mapped to a vector (of one dimension corresponding to a movie review). However, to appreciate these models further, we will move back to MNIST as the source of input to build a model that will take one MNIST numeral and map it to a latent vector.</p>
<h2 id="uuid-6c092d5d-190a-4ee6-be62-6f454cccba38">Unsupervised model</h2>
<p>Let's work in the autoencoder architecture shown in the following diagram. We have studied autoencoders before and now we will use them again since we learned that they are powerful in finding vectorial representations (latent spaces) that are robust and driven by unsupervised learning:</p>
<div class="packt_figref CDPAlignCenter CDPAlign"><img src="assets/6af9a8ff-db2e-4cdc-b0c7-2ba2350e2ca2.png" style="width:43.83em;height:28.75em;"/></div>
<div class="packt_figref CDPAlignCenter CDPAlign">Figure 13.10. LSTM-based autoencoder architecture for MNIST</div>
<p>The goal here is to take an image and find its latent representation, which, in the example of <em>Figure 13.10</em>, would be two dimensions. However, you might be wondering: how can an image be a sequence?</p>
<p>We can interpret an image as a sequence of rows or as a sequence of columns. Let's say that we interpret a two-dimensional image, 28x28 pixels, as a sequence of rows; we can look at every row from top to bottom as a sequence of 28 vectors whose dimensions are each 1x28. In this way, we can use an LSTM to process those sequences, taking advantage of the LSTM's ability to understand temporal relationships in sequences. By this, we mean that, for example in the case of MNIST, the chances that a particular row in an image will look like the previous or next row are very high.</p>
<p>Notice further that the model proposed in <em>Figure 13.10</em> does not require an embedding layer as we did before when processing text. Recall that when processing text, we need to embed (vectorize) every single word into a sequence of vectors. However, with images, they already are sequences of vectors, which eliminates the need for an embedding layer.</p>
<p>The code that we will show here has nothing new to show except for two useful data manipulation tools:</p>
<ul>
<li><kbd>RepeatVector()</kbd>: This will allow us to arbitrarily repeat a vector. It helps in the decoder (see <em>Figure 13.10</em>) to go from a vector to a sequence.</li>
<li><kbd>TimeDistributed()</kbd>: This will allow us to assign a specific type of layer to every element of a sequence.</li>
</ul>
<p>These two are part of the <kbd>tensorflow.keras.layers</kbd> collection. These are implemented in the following code:</p>
<pre>from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import Dense, Activation, Input<br/>from tensorflow.keras.layers import BatchNormalization, Dropout<br/>from tensorflow.keras.layers import Embedding, LSTM<br/>from tensorflow.keras.layers import RepeatVector, TimeDistributed<br/>from tensorflow.keras.datasets import mnist<br/>from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping<br/>import numpy as np<br/><br/>seqnc_lngth = 28    # length of the sequence; must be 28 for MNIST<br/>ltnt_dim = 2        # latent space dimension; it can be anything reasonable<br/><br/>(x_train, y_train), (x_test, y_test) = mnist.load_data()<br/><br/>x_train = x_train.astype('float32') / 255.<br/>x_test = x_test.astype('float32') / 255.<br/><br/>print('x_train shape:', x_train.shape)<br/>print('x_test shape:', x_test.shape)</pre>
<p>After loading the data we can define the encoder part of the model as follows:</p>
<pre>inpt_vec = Input(shape=(seqnc_lngth, seqnc_lngth,))<br/>l1 = Dropout(0.1)(inpt_vec)<br/>l2 = LSTM(seqnc_lngth, activation='tanh', <br/>          recurrent_activation='sigmoid')(l1)<br/>l3 = BatchNormalization()(l2)<br/>l4 = Dropout(0.1)(l3)<br/>l5 = Dense(ltnt_dim, activation='sigmoid')(l4)<br/><br/># model that takes input and encodes it into the latent space<br/>encoder = Model(inpt_vec, l5)</pre>
<p><span>Next we can define the decoder part of the model as follows:</span></p>
<pre>l6 = RepeatVector(seqnc_lngth)(l5)<br/>l7 = LSTM(seqnc_lngth, activation='tanh', recurrent_activation='sigmoid', <br/>          return_sequences=True)(l6)<br/>l8 = BatchNormalization()(l7)<br/>l9 = TimeDistributed(Dense(seqnc_lngth, activation='sigmoid'))(l8)<br/><br/>autoencoder = Model(inpt_vec, l9)</pre>
<p>Finally we compile and train the model like this:</p>
<pre>autoencoder.compile(loss='binary_crossentropy', optimizer='adam')<br/>autoencoder.summary()<br/><br/>reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, <br/>                              min_delta=1e-4, mode='min', verbose=1)<br/><br/>stop_alg = EarlyStopping(monitor='val_loss', patience=15, <br/>                         restore_best_weights=True, verbose=1)<br/><br/>hist = autoencoder.fit(x_train, x_train, batch_size=100, epochs=1000, <br/>                       callbacks=[stop_alg, reduce_lr], shuffle=True, <br/>                       validation_data=(x_test, x_test))</pre>
<p>The code should print the following output, corresponding to the dimensions of the dataset, a summary of the model parameters, followed by the training steps, which we omitted in order to save space:</p>
<pre>x_train shape: (60000, 28, 28)<br/>x_test shape: (10000, 28, 28)<br/><br/>Model: "functional"<br/>_________________________________________________________________<br/>Layer (type)               Output Shape      Param # <br/>=================================================================<br/>input (InputLayer)         [(None, 28, 28)]  0 <br/>_________________________________________________________________<br/>dropout_1 (Dropout)        (None, 28, 28)    0 <br/>_________________________________________________________________<br/>lstm_1 (LSTM)              (None, 28)        6384 <br/>_________________________________________________________________<br/>batch_normalization_1 (Bat (None, 28)        112 <br/>_________________________________________________________________<br/>.<br/>.<br/>.<br/>time_distributed (TimeDist (None, 28, 28)    812 <br/>=================================================================<br/>Total params: 10,950<br/>Trainable params: 10,838<br/>Non-trainable params: 112<br/>_________________________________________________________________<br/><br/>Epoch 1/1000<br/>600/600 [==============================] - 5s 8ms/step - loss: 0.3542 - val_loss: 0.2461<br/>.<br/>.<br/>.<br/><br/></pre>
<p>The model will eventually converge to a valley where it is stopped automatically by the callback. After this, we can simply invoke the <kbd>encoder</kbd> model to literally convert any valid sequence (for example, MNIST images) into a vector, which we will do next.</p>
<h2 id="uuid-b656b90b-a603-4c2f-b28c-e91a8574ee4b">Results</h2>
<p><span>We can invoke the </span><kbd>encoder</kbd><span> model to convert any valid sequence into a vector like so:</span></p>
<pre>encoder.predict(x_test[0:1])</pre>
<p class="mce-root">This will produce a two-dimensional vector with values corresponding to a vectorial representation of the sequence <kbd>x_test[0]</kbd>, which is the first image of the test set of MNIST. It might look something like this:</p>
<pre class="mce-root">array([[3.8787320e-01, 4.8048562e-01]], dtype=float32)</pre>
<p class="mce-root">However, remember that this model was trained without supervision, hence, the numbers shown here will be different for sure! The encoder model is literally our sequence-to-vector model. The rest of the autoencoder model is meant to do the reconstruction.</p>
<p>If you are curious about how the autoencoder model is able to reconstruct a 28x28 image from a vector of just two values, or if you are curious about how the entire test set of MNIST would look when projected in the learned two-dimensional space, you can run the following code:</p>
<pre>import matplotlib.pyplot as plt<br/>import numpy as np<br/><br/>x_hat = autoencoder.predict(x_test)<br/><br/>smp_idx = [3,2,1,18,4,8,11,0,61,9]      # samples for 0,...,9 digits<br/>plt.figure(figsize=(12,6))<br/>for i, (img, y) in enumerate(zip(x_hat[smp_idx].reshape(10, 28, 28), y_test[smp_idx])):<br/>  plt.subplot(2,5,i+1)<br/>  plt.imshow(img, cmap='gray')<br/>  plt.xticks([])<br/>  plt.yticks([])<br/>  plt.title(y)<br/>plt.show()</pre>
<p>Which displays samples of the original digits, as shown in Figure 11.</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/2ee1636e-8781-4b4a-bbfd-b288336795e6.png" style="width:31.33em;height:14.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 11. MNIST original digits<span> </span>0-9</div>
<p class="mce-root">The following code produces samples of the reconstructed digits:</p>
<pre>plt.figure(figsize=(12,6))<br/>for i, (img, y) in enumerate(zip(x_test[smp_idx].reshape(10, 28, 28), y_test[smp_idx])):<br/>  plt.subplot(2,5,i+1)<br/>  plt.imshow(img, cmap='gray')<br/>  plt.xticks([])<br/>  plt.yticks([])<br/>  plt.title(y)<br/>plt.show()</pre>
<div><span>The reconstructed digits appear as shown in </span><em>Figure 12:</em></div>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/4cce3195-d9fc-46ef-83bf-5cee6a876b8d.png" style="width:37.50em;height:17.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 12. MNIST reconstructed digits 0-9 using an LSTM-based<span> autoencoder</span></div>
<p>The next piece of code will display a scatter plot of the original data projected into the latent space, which is shown in <em>Figure 13</em>:</p>
<pre>y_ = list(map(int, y_test))<br/>X_ = encoder.predict(x_test)<br/><br/>plt.figure(figsize=(10,8))<br/>plt.title('LSTM-based Encoder')<br/>plt.scatter(X_[:,0], X_[:,1], s=5.0, c=y_, alpha=0.75, cmap='tab10')<br/>plt.xlabel('First encoder dimension')<br/>plt.ylabel('Second encoder dimension')<br/>plt.colorbar()</pre>
<p>Recall that these results may vary due to the unsupervised nature of the autoencoder. Similarly, the learned space can be visually conceived to look like the one shown in <em>Figure 13</em>, where every dot corresponds to a sequence (MNIST digit) that was made a vector of two dimensions:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/0424aed3-9b57-4bf1-89ec-b2578136e808.png" style="width:36.92em;height:32.67em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 13. Learned vector space based on the MNIST dataset</div>
<p>From <em>Figure 13</em>, we can see that the sequence-to-vector model is working decently even when the reconstruction was based only in two-dimensional vectors. We will see larger representations in the next section. However, you need to know that sequence-to-vector models have been very useful in the last few years [Zhang, Z., <em>et al.</em> (2017)].</p>
<p>Another useful strategy is to create vector-to-sequence models, which is going from a vectorial representation to a sequential representation. In an autoencoder, this would correspond to the decoder part. Let's go ahead and discuss this next.</p>
<h1 id="uuid-cd4ba758-b890-4894-8d94-55209833a7ea">Vector-to-sequence models</h1>
<p>If you look back at <em>Figure 10</em>, the vector-to-sequence model would correspond to the decoder funnel shape. The major philosophy is that most models usually can go from large inputs down to rich representations with no problems. However, it is only recently that the machine learning community regained traction in producing sequences from vectors very successfully (Goodfellow, I., et al. (2016)).</p>
<p>You can think of <em>Figure 10</em> again and the model represented there, which will produce a sequence back from an original sequence. In this section, we will focus on that second part, the decoder, and use it as a vector-to-sequence model. However, before we go there, we will introduce another version of an RNN, a bi-directional LSTM.</p>
<h2 id="uuid-6bd60999-d585-4251-b405-aff9fc77bc90">Bi-directional LSTM</h2>
<p>A <strong>Bi-directional LSTM</strong> (<strong>BiLSTM</strong>), simply put, is an LSTM that analyzes a sequence going forward and backward, as shown in <em>Figure 14</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/65dedd87-5a7e-4f19-8f0b-00c6aa7dede3.png"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 14. A bi-directional LSTM representation</div>
<p>Consider the following examples of sequences analyzed going forward and backward:</p>
<ul>
<li>An audio sequence that is analyzed in natural sound, and then going backward (some people do this to look for <em>subliminal </em>messages).</li>
<li>A text sequence, like a sentence, that is analyzed for good style going forward, and also going backward since some patterns (at least in the English and Spanish languages) make reference backward; for example, a verb that makes reference to a subject that appears at the beginning of the sentence.</li>
<li>An image that has peculiar shapes going from top to bottom, or bottom to top, or from side to side and backwards; if you think of the number 9, going from top to bottom, a traditional LSTM might forget the round part at the top and remember the slim part at the bottom, but a BiLSTM might be able to recall both important aspects of the number by going top to bottom and bottom to top.</li>
</ul>
<p>From <em>Figure 14 (b)</em>, we can also observe that the state and output of both the forward and backward pass are available at any point in the sequence.</p>
<p>We can implement the bi-directional LSTM by simply invoking the <kbd>Bidirectional()</kbd> <span>wrapper </span>around a simple LSTM layer. We will then take the architecture in <em>Figure 10</em> and modify it to have the following:</p>
<ul>
<li>100 dimensions in the latent space</li>
<li>A BiLSTM replacing LSTM layers</li>
<li>An additional dropout layer going from the latent space into the decoder</li>
</ul>
<p>The new architecture will look like <em>Figure 15</em>:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/66ead033-612e-4d8c-be82-593830cb24ab.png" style="width:41.75em;height:27.08em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 15. Implementing BiLSTMs with a view to building a vector-to-sequence model</div>
<p>Recall that the most important point here is to make the latent space (the input to the vector-to-sequence model) as rich as possible in order to generate better sequences. We are trying to achieve this by increasing the latent space dimensionality and adding BiLSTMS. Let's go ahead and implement this and look a the results.</p>
<h2 id="uuid-4d6cbe8e-dc54-4150-bff8-1d9e67118e1d">Implementation and results</h2>
<p>The code to implement the architecture in <em>Figure 15</em> is the following:</p>
<pre>from tensorflow.keras.models import Model<br/>from tensorflow.keras.layers import Dense, Activation, Input<br/>from tensorflow.keras.layers import BatchNormalization, Dropout<br/>from tensorflow.keras.layers import <strong>Bidirectional</strong>, LSTM<br/>from tensorflow.keras.layers import RepeatVector, TimeDistributed<br/>from tensorflow.keras.datasets import mnist<br/>from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping<br/>import numpy as np<br/><br/>seqnc_lngth = 28<br/>ltnt_dim = <strong>100</strong> <br/><br/>(x_train, y_train), (x_test, y_test) = mnist.load_data()<br/><br/>x_train = x_train.astype('float32') / 255.<br/>x_test = x_test.astype('float32') / 255.</pre>
<p>We define the encoder portion of the model as follows:</p>
<pre>inpt_vec = Input(shape=(seqnc_lngth, seqnc_lngth,))<br/>l1 = Dropout(0.5)(inpt_vec)<br/>l2 = <strong>Bidirectional</strong>(LSTM(seqnc_lngth, activation='tanh', <br/>                        recurrent_activation='sigmoid'))(l1)<br/>l3 = BatchNormalization()(l2)<br/>l4 = Dropout(0.5)(l3)<br/>l5 = Dense(ltnt_dim, activation='sigmoid')(l4)<br/><br/># sequence to vector model<br/>encoder = Model(inpt_vec, l5, name='encoder')</pre>
<p>The decoder portion of the model can be defined as follows:</p>
<pre>ltnt_vec = Input(shape=(ltnt_dim,))<br/>l6 = Dropout(0.1)(ltnt_vec)<br/>l7 = RepeatVector(seqnc_lngth)(l6)<br/>l8 = <strong>Bidirectional</strong>(LSTM(seqnc_lngth, activation='tanh', <br/>                   recurrent_activation='sigmoid', <br/>                   return_sequences=True))(l7)<br/>l9 = BatchNormalization()(l8)<br/>l10 = TimeDistributed(Dense(seqnc_lngth, activation='sigmoid'))(l9)<br/><br/><strong># vector to sequence model</strong><br/><strong>decoder</strong> = Model(ltnt_vec, l10, name='decoder')</pre>
<p>Next we compile the autoencoder and train it:</p>
<pre>recon = decoder(encoder(inpt_vec))<br/>autoencoder = Model(inpt_vec, recon, name='ae')<br/><br/>autoencoder.compile(loss='binary_crossentropy', optimizer='adam')<br/>autoencoder.summary()<br/><br/>reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, <br/>                              min_delta=1e-4, mode='min', verbose=1)<br/><br/>stop_alg = EarlyStopping(monitor='val_loss', patience=15, <br/>                         restore_best_weights=True, verbose=1)<br/><br/>hist = autoencoder.fit(x_train, x_train, batch_size=100, epochs=1000, <br/>                       callbacks=[stop_alg, reduce_lr], shuffle=True, <br/>                       validation_data=(x_test, x_test))</pre>
<p>There is nothing new here, except for the <kbd>Bidirectional()</kbd> wrapper used that has been explained previously. The output should produce a summary of the full autoencoder model and the full training operation and will look something like this:</p>
<pre>Model: "ae"<br/>_________________________________________________________________<br/>Layer (type)          Output Shape     Param # <br/>=================================================================<br/>input (InputLayer)    [(None, 28, 28)] 0 <br/>_________________________________________________________________<br/>encoder (Functional)  (None, 100)      18692 <br/>_________________________________________________________________<br/>decoder (Functional)  (None, 28, 28)   30716 <br/>=================================================================<br/>Total params: 49,408<br/>Trainable params: 49,184<br/>Non-trainable params: 224<br/>_________________________________________________________________<br/>Epoch 1/1000<br/>600/600 [==============================] - 9s 14ms/step - loss: 0.3150 - val_loss: 0.1927<br/>.<br/>.<br/>.</pre>
<p>Now, after a number of epochs of unsupervised learning, the training will stop automatically and we can use the <kbd>decoder</kbd> model as our vector-to-sequence model. But before we do that, we might want to quickly check the quality of the reconstructions by running the same code as before to produce the images shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/43b0a05b-9283-44a1-84cd-12274c6959e5.png" style="width:36.67em;height:17.42em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 16. MNIST digits reconstructed with a BiLSTM autoencoder</div>
<p>If you compare <em>Figure 11</em> with <em>Figure 16</em>, you will notice that the reconstructions are much better and the level of detail is better when compared to the previous model reconstructions in <em>Figure 12</em>.</p>
<p>Now we can call our vector-to-sequence model directly with any compatible vector as follows:</p>
<pre>z = np.random.rand(1,100)<br/>x_ = <strong>decoder</strong>.predict(z)<br/>print(x_.shape)<br/>plt.imshow(x_[0], cmap='gray')</pre>
<p>This produces the following output and the plot in <em>Figure 17</em>:</p>
<pre>(1, 28, 28)</pre>
<div class="CDPAlignCenter CDPAlign packt_figref"><br/>
<img src="assets/bda7e5f5-ec39-4f70-8ac7-4e566aae3894.png" style="width:12.92em;height:12.83em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 17. Sequence produced by a model from a random vector</div>
<p>You can generate as many random vectors as you wish and test your vector-to-sequence model. And another interesting thing to observe is a sequence-to-sequence model, which we will cover next.</p>
<h1 id="uuid-d0519997-ed8a-4fdb-af45-361923aab676">Sequence-to-sequence models</h1>
<p>A Google Brain scientist (Vinyals, O., et al. (2015)) <span>wrote the following</span>:</p>
<div class="packt_quote">"Sequences have become first-class citizens in supervised learning thanks to the resurgence of recurrent neural networks. Many complex tasks that require mapping from or to a sequence of observations can now be formulated with the <strong>sequence-to-sequence</strong> (<strong>seq2seq</strong>) framework, which employs the chain rule to efficiently represent the joint probability of sequences." </div>
<p class="mce-root"/>
<p>This is astoundingly correct because now the applications have grown. Just think about the following sequence-to-sequence project ideas:</p>
<ul>
<li>Document summarization. Input sequence: a document. Output sequence: an abstract.</li>
<li>Image super resolution. Input sequence: a low-resolution image. Output sequence: a high-resolution image.</li>
<li>Video subtitles. Input sequence: video. Output sequence: text captions.</li>
<li>Machine translation. Input sequence: text in source language. Output sequence: text in a target language.</li>
</ul>
<p>These are exciting and extremely challenging applications. If you have used online translators, chances are you have used some type of sequence-to-sequence model.</p>
<p>In this section, to keep it simple, we will continue using the autoencoder in <em>Figure 15</em> as our main focus, but just to make sure we are all on the same page with respect to the generality of sequence-to-sequence models, we will point out the following notes:</p>
<ul>
<li>Sequence-to-sequence models can map across domains; for example, video to text or text to audio.</li>
<li><span>Sequence-to-sequence models can map in different dimensions; for example, a low-res image to high-res or vice versa for compression.</span></li>
<li><span>Sequence-to-sequence models can use many different tools, such as dense layers, convolutional layers, and recurrent layers.</span></li>
</ul>
<p>With this in mind, you can pretty much build a s<span>equence-to-sequence model depending of your application. For now, we will come back to the model in <em>Figure 15</em> and show that the autoencoder is a sequence-to-sequence model in the sense that it takes a sequence of rows of an image and produces a sequence of rows of another image. Since this is an autoencoder, the input and output dimensions must match.</span></p>
<p>We will limit our showcase of the previously trained sequence-to-sequence model (autoencoder) to the following short code snippet, which builds up from the code in the previous section:</p>
<pre>plt.figure(figsize=(12,6))<br/>for i in range(10):<br/>  plt.subplot(2,5,i+1)<br/>  rnd_vec = np.round(np.mean(x_test[y_test==i],axis=0))   #(a)<br/>  rnd_vec = np.reshape(rnd_vec, (1,28,28))                #(b)<br/>  z = <strong>encoder</strong>.predict(rnd_vec)                            #(c)<br/>  decdd = <strong>decoder</strong>.predict(z)                              #(d)<br/>  plt.imshow(decdd[0], cmap='gray')<br/>  plt.xticks([])<br/>  plt.yticks([])<br/>  plt.title(i)<br/>plt.show()</pre>
<p>Let's explain some of these steps. In <em>(a)</em>, we calculate the average sequence for every single number; this is in response to the question: what can we use as our input sequence since doing random is so easy? Well, using the average sequences to form the test set sounds interesting enough. </p>
<p>Next, <em>(b)</em> is simply to make the input compatible with the encoder input dimensions. Then, <em>(c)</em> takes the average sequence and makes a vector out of it. Finally, <em>(d)</em> uses that vector to recreate the sequence, producing the plot shown in the following diagram:</p>
<div class="CDPAlignCenter CDPAlign packt_figref"><img src="assets/c00afefc-9376-43b2-845f-c1b8b8883fa3.png" style="width:22.17em;height:10.50em;"/></div>
<div class="CDPAlignCenter CDPAlign packt_figref">Figure 18. Sequence-to-sequence example outputs</div>
<p>From the diagram, you can easily observe well-defined patterns consistent with handwritten numbers, which are generated as sequences of rows by bi-directional LSTMs.</p>
<p>Before we finish this, let's have a word on the ethical implications of some of these models.</p>
<h1 id="uuid-2fe6fa4c-f814-4b98-b10c-7c3d67429db2">Ethical implications</h1>
<p>With the resurgence of recurrent models and their applicability in capturing temporal information in sequences, there is a risk of finding latent spaces that are not properly being fairly distributed. This can be of higher risk in unsupervised models that operate in data that is not properly curated. If you think about it, the model does not care about the relationships that it finds; it only cares about minimizing a loss function, and therefore if it is trained with magazines or newspapers from the 1950s, it may find spaces where the word "women" may be close (in terms of Euclidean distance) to home labor words such as "broom", "dishes", and "cooking", while the word "man" may be close to all other labor such as "driving", "teaching", "doctor", and "scientist". This is an example of a bias that has been introduced into the latent space (<span>Shin, S.,<em> </em></span>et al.<span> (2020))</span>.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>The risk here is that the vector-to-sequence or sequence-to-sequence models will find it much easier to associate a doctor with a man than with a woman, and cooking with a woman than with a man, just to name a couple of examples. You can take this to images of faces as well and find that certain people with certain features might be associated incorrectly. This is why it is so important to undertake the type of analysis we are doing here, trying to visualize the latent space whenever possible, trying to look at what the model outputs, and so on.</p>
<p>The key takeaway here is that, while the models discussed here are extremely interesting and powerful, they also carry the risk of learning things about our societies that are particularly perceived as unwanted. If the risk exists and goes undetected, it might cause bias (<span>Amini, A.,</span><span> </span>et al.<span> (2019))</span>. And if bias goes undetected, it might lead to several forms of discrimination. Please always be careful about these things as well as things beyond your own societal context. </p>
<h1 id="uuid-8798f224-b9f4-41a2-8997-674fbf7f2d19">Summary</h1>
<p>This advanced chapter showed you how to create RNNs. You learned about LSTMs and its bi-directional implementation, which is one of the most powerful approaches for sequences that can have distant temporal correlations. You also learned to create an LSTM-based sentiment analysis model for the classification of movie reviews. You designed an autoencoder to learn a latent space for MNIST using simple and bi-directional LSTMs and used it both as a vector-to-sequence <span>model </span>and as a sequence-to-sequence model.</p>
<p>At this point, you should feel confident explaining the motivation behind memory in RNNs founded in the need for more robust models. You should feel comfortable coding your own recurrent network using Keras/TensorFlow. Furthermore, you should feel confident implementing both supervised and unsupervised recurrent networks.</p>
<p>LSTMs are great in encoding highly correlated spatial information, such as images, or audio, or text, just like CNNs. However, both CNNs and LSTMs learn very specific latent spaces that may lack diversity. This can cause a problem if there is a malicious hacker that is trying to break your system; if your model is very specific to your data, it may create certain sensitivity to variations, leading to disastrous consequences in your outputs. Autoencoders solve this by using a generative approach called the variational autoencoder, which learns the distribution of the data rather than the data itself. However, the question remains: How can we implement this idea of generative approaches in other types of networks that are not necessarily autoencoders? To find out the answer, you cannot miss the next chapter, <a href="7b09fe4b-078e-4c57-8a81-dc0863eba43d.xhtml">Chapter 14</a>, <em>Generative Neural Networks</em>. The next chapter will present a way of overcoming the fragility of neural networks by attacking them and teaching them to be more robust. But before you go, quiz yourself with the following questions.</p>
<p class="mce-root"/>
<h1 id="uuid-0bc33f67-ccda-4d78-83c0-2b62ecceeecd">Questions and answers</h1>
<ol>
<li><strong>If both CNNs and LSTMs can model spatially correlated data, what makes LSTMs particularly better?</strong></li>
</ol>
<p style="padding-left: 60px">Nothing in general, other than the fact that LSTMs have memory. But in certain applications, such as NLP, where a sentence is discovered sequentially as you go forward and backward, there are references to certain words at the beginning, middle, and end, and multiples at a time. It is easier for BiLSTMs to model that behavior faster than a CNN. A CNN may learn to do that, but it may take longer to do so in comparison.</p>
<ol start="2">
<li><strong>Does adding more recurrent layers make the network better? </strong></li>
</ol>
<p style="padding-left: 60px">No. It can make things worse. It is recommended to keep it simple to no more than three layers, unless you are a scientist and are experimenting with something new. Otherwise, there should be no more than three recurrent layers in a row in an encoder model.</p>
<ol start="3">
<li><strong>What other applications are there for LSTMs?</strong></li>
</ol>
<p style="padding-left: 60px">Audio processing and classification; image denoising; image super-resolution; text summarization and other text-processing and classification tasks; word completion; chatbots; text completion; text generation; audio generation; image generation.</p>
<ol start="4">
<li><strong>It seems like LSTMs and CNNs haver similar applications. What makes you choose one over the other?</strong></li>
</ol>
<p style="padding-left: 60px">LSTMs are faster to converge; thus, if time is a factor, LSTMs are better. CNNs are more stable than LSTMs; thus, if your input is very unpredictable, chances are an LSTM might carry the problem across recurrent layers, making it worse every time, in which case CNNs could alleviate that with pooling. On a personal level, I usually try CNNs <span>first </span>for image-related applications, and LSTMs first for NLP applications. </p>
<h1 id="uuid-ca1662b2-9c8a-4e4d-97d7-b620f6b8ec3e">References</h1>
<ul>
<li>Rumelhart, D. E., Hinton, G. E., and Williams, R. J. (1986). <em>Learning representations by backpropagating errors</em>. <em>Nature</em>, 323(6088), 533-536.</li>
<li>Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., and Dean, J. (2013). <em>Distributed representations of words and phrases and their compositionality</em>. In <em>Advances in neural information processing systems</em> (pp. 3111-3119).</li>
<li>Pennington, J., Socher, R., and Manning, C. D. (October 2014). <em>Glove: Global vectors for word representation</em>. In <em>Proceedings of the 2014 conference on empirical methods in natural language processing</em> (EMNLP) (pp. 1532-1543).</li>
<li>Rivas, P., and Zimmermann, M. (December 2019). <em>Empirical Study of Sentence Embeddings for English Sentences Quality Assessment</em>. In <em>2019 International Conference on Computational Science and Computational Intelligence</em> (CSCI) (pp. 331-336). IEEE.</li>
<li>Hochreiter, S., and Schmidhuber, J. (1997). <em>Long short-term memory</em>. <em>Neural computation</em>, 9(8), 1735-1780.</li>
<li>Zhang, Z., Liu, D., Han, J., and Schuller, B. (2017). <em>Learning audio sequence representations for acoustic event classification</em>. <em>arXiv preprint</em> arXiv:1707.08729.</li>
<li>Goodfellow, I., Bengio, Y., and Courville, A. (2016). <em>Sequence modeling: Recurrent and recursive nets</em>. <em>Deep learning</em>, 367-415.</li>
<li>Vinyals, O., Bengio, S., and Kudlur, M. (2015). <em>Order matters: Sequence to sequence for sets</em>. <em>arXiv preprint</em> arXiv:1511.06391.</li>
<li>Shin, S., Song, K., Jang, J., Kim, H., Joo, W., and Moon, I. C. (2020). <em>Neutralizing Gender Bias in Word Embedding with Latent Disentanglement and Counterfactual Generation</em>. <em>arXiv preprint</em> arXiv:2004.03133.</li>
<li>Amini, A., Soleimany, A. P., Schwarting, W., Bhatia, S. N., and Rus, D. (January 2019). <em>Uncovering and mitigating algorithmic bias through learned latent structure</em>. In <em>Proceedings of the 2019 AAAI/ACM Conference on AI, Ethics, and Society</em> (pp. 289-295).</li>
</ul>


            </article>

            
        </section>
    </body></html>