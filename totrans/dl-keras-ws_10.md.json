["```py\n    import pandas as pd\n    feats = pd.read_csv('../data/OSI_feats_e3.csv')\n    target = pd.read_csv('../data/OSI_target_e2.csv')\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    test_size = 0.2\n    random_state = 13\n    X_train, X_test, y_train, y_test = \\\n    train_test_split(feats, target, test_size=test_size, \\\n                     random_state=random_state)\n    ```", "```py\n    print(f'Shape of X_train: {X_train.shape}')\n    print(f'Shape of y_train: {y_train.shape}')\n    print(f'Shape of X_test: {X_test.shape}')\n    print(f'Shape of y_test: {y_test.shape}')\n    ```", "```py\n    Shape of X_train: (9864, 68)\n    Shape of y_train: (9864, 1)\n    Shape of X_test: (2466, 68)\n    Shape of y_test: (2466, 1)\n    ```", "```py\n    import numpy as np\n    from sklearn.linear_model import LogisticRegressionCV\n    Cs = np.logspace(-2, 6, 9)\n    model_l1 = LogisticRegressionCV(Cs=Cs, penalty='l1', \\\n                                    cv=10, solver='liblinear', \\\n                                    random_state=42, max_iter=10000)\n    model_l2 = LogisticRegressionCV(Cs=Cs, penalty='l2', cv=10, \\\n                                    random_state=42, max_iter=10000)\n    ```", "```py\n    model_l1.fit(X_train, y_train['Revenue'])\n    model_l2.fit(X_train, y_train['Revenue'])\n    ```", "```py\n    print(f'Best hyperparameter for l1 regularization model: \\\n    {model_l1.C_[0]}')\n    print(f'Best hyperparameter for l2 regularization model: \\\n    {model_l2.C_[0]}')\n    ```", "```py\n    Best hyperparameter for l1 regularization model: 1000000.0\n    Best hyperparameter for l2 regularization model: 1.0\n    ```", "```py\n    y_pred_l1 = model_l1.predict(X_test)\n    y_pred_l2 = model_l2.predict(X_test)\n    ```", "```py\n    from sklearn import metrics\n    accuracy_l1 = metrics.accuracy_score(y_pred=y_pred_l1, \\\n                                         y_true=y_test)\n    accuracy_l2 = metrics.accuracy_score(y_pred=y_pred_l2, \\\n                                         y_true=y_test)\n    print(f'Accuracy of the model with l1 regularization is \\\n    {accuracy_l1*100:.4f}%')\n    print(f'Accuracy of the model with l2 regularization is \\\n    {accuracy_l2*100:.4f}%')\n    ```", "```py\n    Accuracy of the model with l1 regularization is 89.2133%\n    Accuracy of the model with l2 regularization is 89.2944%\n    ```", "```py\n    precision_l1, recall_l1, fscore_l1, _ = \\\n    metrics.precision_recall_fscore_support(y_pred=y_pred_l1, \\\n                                            y_true=y_test, \\\n                                            average='binary')\n    precision_l2, recall_l2, fscore_l2, _ = \\\n    metrics.precision_recall_fscore_support(y_pred=y_pred_l2, \\\n                                            y_true=y_test, \\\n                                            average='binary')\n    print(f'l1\\nPrecision: {precision_l1:.4f}\\nRecall: \\\n    {recall_l1:.4f}\\nfscore: {fscore_l1:.4f}\\n\\n')\n    print(f'l2\\nPrecision: {precision_l2:.4f}\\nRecall: \\\n    {recall_l2:.4f}\\nfscore: {fscore_l2:.4f}')\n    ```", "```py\n    l1\n    Precision: 0.7300\n    Recall: 0.4078\n    fscore: 0.5233\n    l2\n    Precision: 0.7350\n    Recall: 0.4106\n    fscore: 0.5269\n    ```", "```py\n    coef_list = [f'{feature}: {coef}' for coef, \\\n                 feature in sorted(zip(model_l1.coef_[0], \\\n                                   X_train.columns.values.tolist()))]\n    for item in coef_list:\n        print(item)\n    ```", "```py\n    coef_list = [f'{feature}: {coef}' for coef, \\\n                 feature in sorted(zip(model_l2.coef_[0], \\\n                                       X_train.columns.values.tolist()))]\n    for item in coef_list:\n        print(item)\n    ```", "```py\n    import pandas as pd\n    feats = pd.read_csv('../data/OSI_feats.csv')\n    target = pd.read_csv('../data/OSI_target.csv')\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    test_size = 0.2\n    random_state = 42\n    X_train, X_test, y_train, y_test = \\\n    train_test_split(feats, target, test_size=test_size, \\\n                     random_state=random_state)\n    ```", "```py\n    from keras.models import Sequential\n    import numpy as np\n    from tensorflow import random\n    np.random.seed(random_state)\n    random.set_seed(random_state)\n    model = Sequential()\n    ```", "```py\n    from keras.layers import Dense\n    model.add(Dense(1, input_dim=X_train.shape[1]))\n    ```", "```py\n    from keras.layers import Activation\n    model.add(Activation('sigmoid'))\n    ```", "```py\n    model.compile(optimizer='adam', loss='binary_crossentropy', \\\n                  metrics=['accuracy'])\n    ```", "```py\n    print(model.summary())\n    ```", "```py\n    history = model.fit(X_train, y_train['Revenue'], epochs=10, \\\n                        validation_split=0.2, shuffle=False)\n    ```", "```py\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    # Plot training and validation accuracy values\n    plt.plot(history.history['accuracy'])\n    plt.plot(history.history['val_accuracy'])\n    plt.title('Model accuracy')\n    plt.ylabel('Accuracy')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n    # Plot training and validation loss values\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.title('Model loss')\n    plt.ylabel('Loss')\n    plt.xlabel('Epoch')\n    plt.legend(['Train', 'Validation'], loc='upper left')\n    plt.show()\n    ```", "```py\n    test_loss, test_acc = model.evaluate(X_test, y_test['Revenue'])\n    print(f'The loss on the test set is {test_loss:.4f} \\\n    and the accuracy is {test_acc*100:.3f}%')\n    ```", "```py\n    2466/2466 [==============================] - 0s 15us/step\n    The loss on the test set is 0.3632 and the accuracy is 86.902%\n    ```", "```py\n    # import required packages from Keras\n    from keras.models import Sequential \n    from keras.layers import Dense, Activation \n    import numpy as np\n    import pandas as pd\n    from tensorflow import random\n    from sklearn.model_selection import train_test_split\n    # import required packages for plotting\n    import matplotlib.pyplot as plt \n    import matplotlib\n    %matplotlib inline \n    import matplotlib.patches as mpatches\n    # import the function for plotting decision boundary\n    from utils import plot_decision_boundary\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    seed = 1\n    ```", "```py\n    \"\"\"\n    load the dataset, print the shapes of input and output and the number of examples\n    \"\"\"\n    feats = pd.read_csv('../data/outlier_feats.csv')\n    target = pd.read_csv('../data/outlier_target.csv')\n    print(\"X size = \", feats.shape)\n    print(\"Y size = \", target.shape)\n    print(\"Number of examples = \", feats.shape[0])\n    ```", "```py\n    X size = (3359, 2)\n    Y size = (3359, 1)\n    Number of examples = 3359\n    ```", "```py\n    class_1=plt.scatter(feats.loc[target['Class']==0,'feature1'], \\\n                        feats.loc[target['Class']==0,'feature2'], \\\n                        c=\"red\", s=40, edgecolor='k')\n    class_2=plt.scatter(feats.loc[target['Class']==1,'feature1'], \\\n                        feats.loc[target['Class']==1,'feature2'], \\\n                        c=\"blue\", s=40, edgecolor='k')\n    plt.legend((class_1, class_2),('Fail','Pass'))\n    plt.xlabel('Feature 1')\n    plt.ylabel('Feature 2')\n    ```", "```py\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model = Sequential()\n    model.add(Dense(1, activation='sigmoid', input_dim=2))\n    model.compile(optimizer='sgd', loss='binary_crossentropy')\n    ```", "```py\n    model.fit(feats, target, batch_size=5, epochs=100, verbose=1, \\\n              validation_split=0.2, shuffle=False)\n    ```", "```py\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n    plot_decision_boundary(lambda x: model.predict(x), feats, target)\n    plt.title(\"Logistic Regression\")\n    ```", "```py\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model = Sequential() \n    model.add(Dense(3, activation='relu', input_dim=2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='sgd', loss='binary_crossentropy')\n    ```", "```py\n    model.fit(feats, target, batch_size=5, epochs=200, verbose=1, \\\n              validation_split=0.2, shuffle=False)\n    ```", "```py\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n    plot_decision_boundary(lambda x: model.predict(x), feats, target)\n    plt.title(\"Decision Boundary for Neural Network with \"\\\n              \"hidden layer size 3\")\n    ```", "```py\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model = Sequential() \n    model.add(Dense(6, activation='relu', input_dim=2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='sgd', loss='binary_crossentropy')\n    ```", "```py\n    model.fit(feats, target, batch_size=5, epochs=400, verbose=1, \\\n              validation_split=0.2, shuffle=False)\n    ```", "```py\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n    plot_decision_boundary(lambda x: model.predict(x), feats, target)\n    plt.title(\"Decision Boundary for Neural Network with \"\\\n              \"hidden layer size 6\")\n    ```", "```py\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model = Sequential() \n    model.add(Dense(3, activation='tanh', input_dim=2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='sgd', loss='binary_crossentropy')\n    ```", "```py\n    model.fit(feats, target, batch_size=5, epochs=200, verbose=1, \\\n              validation_split=0.2, shuffle=False)\n    ```", "```py\n    plot_decision_boundary(lambda x: model.predict(x), feats, target) \n    plt.title(\"Decision Boundary for Neural Network with \"\\\n              \"hidden layer size 3\")\n    ```", "```py\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model = Sequential() \n    model.add(Dense(6, activation='tanh', input_dim=2))\n    model.add(Dense(1, activation='sigmoid'))\n    model.compile(optimizer='sgd', loss='binary_crossentropy')\n    ```", "```py\n    model.fit(feats, target, batch_size=5, epochs=400, verbose=1, \\\n              validation_split=0.2, shuffle=False)\n    ```", "```py\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n    plot_decision_boundary(lambda x: model.predict(x), feats, target)\n    plt.title(\"Decision Boundary for Neural Network with \"\\\n              \"hidden layer size 6\")\n    ```", "```py\n    import pandas as pd\n    import numpy as np\n    from tensorflow import random\n    from sklearn.model_selection import train_test_split\n    from sklearn.preprocessing import StandardScaler\n    from keras.models import Sequential\n    from keras.layers import Dense\n    import matplotlib.pyplot as plt \n    import matplotlib\n    %matplotlib inline\n    X = pd.read_csv('../data/HCV_feats.csv')\n    y = pd.read_csv('../data/HCV_target.csv')\n    ```", "```py\n    print(\"Number of Examples in the Dataset = \", X.shape[0])\n    print(\"Number of Features for each example = \", X.shape[1]) \n    print(\"Possible Output Classes = \", \\\n          y['AdvancedFibrosis'].unique())\n    ```", "```py\n    Number of Examples in the Dataset = 1385\n    Number of Features for each example = 28\n    Possible Output Classes = [0 1]\n    ```", "```py\n    seed = 1\n    np.random.seed(seed)\n    sc = StandardScaler()\n    X = pd.DataFrame(sc.fit_transform(X), columns=X.columns)\n    X_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.2, random_state=seed)\n    # Print the information regarding dataset sizes\n    print(X_train.shape)\n    print(y_train.shape)\n    print(X_test.shape)\n    print(y_test.shape)\n    print (\"Number of examples in training set = \", X_train.shape[0])\n    print (\"Number of examples in test set = \", X_test.shape[0])\n    ```", "```py\n    (1108, 28)\n    (1108, 1)\n    (277, 28)\n    (277, 1)\n    Number of examples in training set = 1108\n    Number of examples in test set = 277\n    ```", "```py\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # define the keras model\n    classifier = Sequential()\n    classifier.add(Dense(units = 3, activation = 'tanh', \\\n                         input_dim=X_train.shape[1]))\n    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n    classifier.compile(optimizer = 'sgd', loss = 'binary_crossentropy', \\\n                       metrics = ['accuracy'])\n    classifier.summary()\n    ```", "```py\n    history=classifier.fit(X_train, y_train, batch_size = 20, \\\n                           epochs = 100, validation_split=0.1, \\\n                           shuffle=False)\n    ```", "```py\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    ```", "```py\n    print(f\"Best Accuracy on training set = \\\n    {max(history.history['accuracy'])*100:.3f}%\")\n    print(f\"Best Accuracy on validation set = \\\n    {max(history.history['val_accuracy'])*100:.3f}%\") \n    test_loss, test_acc = \\\n    classifier.evaluate(X_test, y_test['AdvancedFibrosis'])\n    print(f'The loss on the test set is {test_loss:.4f} and \\\n    the accuracy is {test_acc*100:.3f}%')\n    ```", "```py\n    Best Accuracy on training set = 52.959%\n    Best Accuracy on validation set = 58.559%\n    277/277 [==============================] - 0s 25us/step\n    The loss on the test set is 0.6885 and the accuracy is 55.235%\n    ```", "```py\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # define the keras model\n    classifier = Sequential()\n    classifier.add(Dense(units = 4, activation = 'tanh', \\\n                         input_dim = X_train.shape[1]))\n    classifier.add(Dense(units = 2, activation = 'tanh'))\n    classifier.add(Dense(units = 1, activation = 'sigmoid'))\n    classifier.compile(optimizer = 'sgd', loss = 'binary_crossentropy', \\\n                       metrics = ['accuracy'])\n    classifier.summary()\n    ```", "```py\n    history=classifier.fit(X_train, y_train, batch_size = 20, \\\n                           epochs = 100, validation_split=0.1, \\\n                           shuffle=False)\n    ```", "```py\n    # plot training error and test error plots \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    ```", "```py\n    print(f\"Best Accuracy on training set = \\\n    {max(history.history['accuracy'])*100:.3f}%\")\n    print(f\"Best Accuracy on validation set = \\\n    {max(history.history['val_accuracy'])*100:.3f}%\") \n    test_loss, test_acc = \\\n    classifier.evaluate(X_test, y_test['AdvancedFibrosis'])\n    print(f'The loss on the test set is {test_loss:.4f} and \\\n    the accuracy is {test_acc*100:.3f}%')\n    ```", "```py\n    Best Accuracy on training set = 57.272%\n    Best Accuracy on test set = 54.054%\n    277/277 [==============================] - 0s 41us/step\n    The loss on the test set is 0.7016 and the accuracy is 49.819%\n    ```", "```py\n    # Load the dataset\n    import pandas as pd\n    X = pd.read_csv('../data/HCV_feats.csv')\n    y = pd.read_csv('../data/HCV_target.csv')\n    # Print the sizes of the dataset\n    print(\"Number of Examples in the Dataset = \", X.shape[0])\n    print(\"Number of Features for each example = \", X.shape[1]) \n    print(\"Possible Output Classes = \", \\\n          y['AdvancedFibrosis'].unique())\n    ```", "```py\n    Number of Examples in the Dataset = 1385\n    Number of Features for each example = 28\n    Possible Output Classes = [0 1]\n    ```", "```py\n    from keras.models import Sequential\n    from keras.layers import Dense\n    # Create the function that returns the keras model\n    def build_model():\n        model = Sequential()\n        model.add(Dense(4, input_dim=X.shape[1], activation='tanh'))\n        model.add(Dense(2, activation='tanh'))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(loss='binary_crossentropy', optimizer='adam', \\\n                      metrics=['accuracy'])\n        return model\n    ```", "```py\n    # import required packages\n    import numpy as np\n    from tensorflow import random\n    from keras.wrappers.scikit_learn import KerasClassifier\n    from sklearn.model_selection import StratifiedKFold\n    from sklearn.model_selection import cross_val_score\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    X = pd.DataFrame(sc.fit_transform(X), columns=X.columns)\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    seed = 1\n    np.random.seed(seed)\n    random.set_seed(seed)\n    \"\"\"\n    determine the number of folds for k-fold cross-validation, number of epochs and batch size\n    \"\"\"\n    n_folds = 5\n    epochs = 100\n    batch_size = 20\n    # build the scikit-learn interface for the keras model\n    classifier = KerasClassifier(build_fn=build_model, \\\n                                 epochs=epochs, \\\n                                 batch_size=batch_size, \\\n                                 verbose=1, shuffle=False)\n    # define the cross-validation iterator\n    kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, \\\n                            random_state=seed)\n    \"\"\"\n    perform the k-fold cross-validation and store the scores in results\n    \"\"\"\n    results = cross_val_score(classifier, X, y, cv=kfold)\n    ```", "```py\n    # print accuracy for each fold\n    for f in range(n_folds):\n        print(\"Test accuracy at fold \", f+1, \" = \", results[f])\n    print(\"\\n\")\n    \"\"\"\n    print overall cross-validation accuracy plus the standard deviation of the accuracies\n    \"\"\"\n    print(\"Final Cross-validation Test Accuracy:\", results.mean())\n    print(\"Standard Deviation of Final Test Accuracy:\", results.std())\n    ```", "```py\n    Test accuracy at fold 1 = 0.5198556184768677\n    Test accuracy at fold 2 = 0.4693140685558319\n    Test accuracy at fold 3 = 0.512635350227356\n    Test accuracy at fold 4 = 0.5740072131156921\n    Test accuracy at fold 5 = 0.5523465871810913\n    Final Cross-Validation Test Accuracy: 0.5256317675113678\n    Standard Deviation of Final Test Accuracy: 0.03584760640500936\n    ```", "```py\n    # import the required packages\n    from keras.models import Sequential\n    from keras.layers import Dense\n    from keras.wrappers.scikit_learn import KerasClassifier\n    from sklearn.model_selection import StratifiedKFold\n    from sklearn.model_selection import cross_val_score\n    import numpy as np\n    import pandas as pd\n    from sklearn.preprocessing import StandardScaler\n    from tensorflow import random\n    # Load the dataset\n    X = pd.read_csv('../data/HCV_feats.csv')\n    y = pd.read_csv('../data/HCV_target.csv')\n    sc = StandardScaler()\n    X = pd.DataFrame(sc.fit_transform(X), columns=X.columns)\n    ```", "```py\n    # Create the function that returns the keras model 1\n    def build_model_1(activation='relu', optimizer='adam'):\n        # create model 1\n        model = Sequential()\n        model.add(Dense(4, input_dim=X.shape[1], \\\n                        activation=activation))\n        model.add(Dense(4, activation=activation))\n        model.add(Dense(4, activation=activation))\n        model.add(Dense(1, activation='sigmoid'))\n        # Compile model\n        model.compile(loss='binary_crossentropy', \\\n                      optimizer=optimizer, metrics=['accuracy'])\n        return model\n    # Create the function that returns the keras model 2\n    def build_model_2(activation='relu', optimizer='adam'):\n        # create model 2\n        model = Sequential()\n        model.add(Dense(4, input_dim=X.shape[1], \\\n                        activation=activation))\n        model.add(Dense(2, activation=activation))\n        model.add(Dense(1, activation='sigmoid'))\n        # Compile model\n        model.compile(loss='binary_crossentropy', \\\n                      optimizer=optimizer, metrics=['accuracy'])\n        return model\n    # Create the function that returns the keras model 3\n    def build_model_3(activation='relu', optimizer='adam'):\n        # create model 3\n        model = Sequential()\n        model.add(Dense(8, input_dim=X.shape[1], \\\n                        activation=activation))\n        model.add(Dense(8, activation=activation))\n        model.add(Dense(1, activation='sigmoid'))\n        # Compile model\n        model.compile(loss='binary_crossentropy', \\\n                      optimizer=optimizer, metrics=['accuracy'])\n        return model\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    seed = 2\n    np.random.seed(seed)\n    random.set_seed(seed)\n    \"\"\"\n    determine the number of folds for k-fold cross-validation, number of epochs and batch size\n    \"\"\"\n    n_folds = 5\n    batch_size=20\n    epochs=100\n    # define the list to store cross-validation scores\n    results_1 = []\n    # define the possible options for the model\n    models = [build_model_1, build_model_2, build_model_3]\n    # loop over models\n    for m in range(len(models)):\n        # build the scikit-learn interface for the keras model\n        classifier = KerasClassifier(build_fn=models[m], \\\n                                     epochs=epochs, \\\n                                     batch_size=batch_size, \\\n                                     verbose=0, shuffle=False)\n        # define the cross-validation iterator\n        kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, \\\n                                random_state=seed)\n        \"\"\"\n        perform the k-fold cross-validation and store the scores \n        in result\n        \"\"\"\n        result = cross_val_score(classifier, X, y, cv=kfold)\n        # add the scores to the results list \n        results_1.append(result)\n    # Print cross-validation score for each model\n    for m in range(len(models)):\n        print(\"Model\", m+1,\"Test Accuracy =\", results_1[m].mean())\n    ```", "```py\n    Model 1 Test Accuracy = 0.4996389865875244\n    Model 2 Test Accuracy = 0.5148014307022095\n    Model 3 Test Accuracy = 0.5097472846508027\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # determine the number of folds for k-fold cross-validation\n    n_folds = 5\n    # define possible options for epochs and batch_size\n    epochs = [100, 200]\n    batches = [10, 20]\n    # define the list to store cross-validation scores\n    results_2 = []\n    # loop over all possible pairs of epochs, batch_size\n    for e in range(len(epochs)):\n        for b in range(len(batches)):\n            # build the scikit-learn interface for the keras model\n            classifier = KerasClassifier(build_fn=build_model_2, \\\n                                         epochs=epochs[e], \\\n                                         batch_size=batches[b], \\\n                                         verbose=0)\n            # define the cross-validation iterator\n            kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, \\\n                                    random_state=seed)\n            # perform the k-fold cross-validation. \n            # store the scores in result\n            result = cross_val_score(classifier, X, y, cv=kfold)\n            # add the scores to the results list \n            results_2.append(result)\n    \"\"\"\n    Print cross-validation score for each possible pair of epochs, batch_size\n    \"\"\"\n    c = 0\n    for e in range(len(epochs)):\n        for b in range(len(batches)):\n            print(\"batch_size =\", batches[b],\", epochs =\", epochs[e], \\\n                  \", Test Accuracy =\", results_2[c].mean())\n            c += 1\n    ```", "```py\n    batch_size = 10 , epochs = 100 , Test Accuracy = 0.5010830342769623\n    batch_size = 20 , epochs = 100 , Test Accuracy = 0.5126353740692139\n    batch_size = 10 , epochs = 200 , Test Accuracy = 0.5176895320416497\n    batch_size = 20 , epochs = 200 , Test Accuracy = 0.5075812220573426\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    \"\"\"\n    determine the number of folds for k-fold cross-validation, number of epochs and batch size\n    \"\"\"\n    n_folds = 5\n    batch_size = 10\n    epochs = 200\n    # define the list to store cross-validation scores\n    results_3 = []\n    # define possible options for optimizer and activation\n    optimizers = ['rmsprop', 'adam','sgd']\n    activations = ['relu', 'tanh']\n    # loop over all possible pairs of optimizer, activation\n    for o in range(len(optimizers)):\n        for a in range(len(activations)):\n            optimizer = optimizers[o]\n            activation = activations[a]\n            # build the scikit-learn interface for the keras model\n            classifier = KerasClassifier(build_fn=build_model_2, \\\n                                         epochs=epochs, \\\n                                         batch_size=batch_size, \\\n                                         verbose=0, shuffle=False)\n            # define the cross-validation iterator\n            kfold = StratifiedKFold(n_splits=n_folds, shuffle=True, \\\n                                    random_state=seed)\n            # perform the k-fold cross-validation. \n            # store the scores in result\n            result = cross_val_score(classifier, X, y, cv=kfold)\n            # add the scores to the results list \n            results_3.append(result)\n    \"\"\"\n    Print cross-validation score for each possible pair of optimizer, activation\n    \"\"\"\n    c = 0\n    for o in range(len(optimizers)):\n        for a in range(len(activations)):\n            print(\"activation = \", activations[a],\", optimizer = \", \\\n                  optimizers[o], \", Test accuracy = \", \\\n                  results_3[c].mean())\n            c += 1\n    ```", "```py\n    activation =  relu , optimizer =  rmsprop , \n    Test accuracy =  0.5234657049179077\n    activation =  tanh , optimizer =  rmsprop , \n    Test accuracy =  0.49602887630462644\n    activation =  relu , optimizer =  adam , \n    Test accuracy =  0.5039711117744445\n    activation =  tanh , optimizer =  adam , \n    Test accuracy =  0.4989169597625732\n    activation =  relu , optimizer =  sgd , \n    Test accuracy =  0.48953068256378174\n    activation =  tanh , optimizer =  sgd , \n    Test accuracy =  0.5191335678100586\n    ```", "```py\n    # import the required packages\n    from keras.models import Sequential\n    from keras.layers import Dense\n    from keras.wrappers.scikit_learn import KerasRegressor\n    from sklearn.model_selection import KFold\n    from sklearn.model_selection import cross_val_score\n    from sklearn.preprocessing import StandardScaler\n    from sklearn.pipeline import make_pipeline\n    import numpy as np\n    import pandas as pd\n    from tensorflow import random\n    ```", "```py\n    # Load the dataset\n    # Load the dataset\n    X = pd.read_csv('../data/traffic_volume_feats.csv')\n    y = pd.read_csv('../data/traffic_volume_target.csv') \n    # Print the sizes of input data and output data\n    print(\"Input data size = \", X.shape)\n    print(\"Output size = \", y.shape)\n    # Print the range for output\n    print(f\"Output Range = ({y['Volume'].min()}, \\\n    { y['Volume'].max()})\")\n    ```", "```py\n    Input data size =  (10000, 10)\n    Output size =  (10000, 1)\n    Output Range = (0.000000, 584.000000)\n    ```", "```py\n    # Create the function that returns the keras model 1\n    def build_model_1(optimizer='adam'):\n        # create model 1\n        model = Sequential()\n        model.add(Dense(10, input_dim=X.shape[1], activation='relu'))\n        model.add(Dense(1))\n        # Compile model\n        model.compile(loss='mean_squared_error', optimizer=optimizer)\n        return model\n    # Create the function that returns the keras model 2\n    def build_model_2(optimizer='adam'):\n        # create model 2\n        model = Sequential()\n        model.add(Dense(10, input_dim=X.shape[1], activation='relu'))\n        model.add(Dense(10, activation='relu'))\n        model.add(Dense(1))\n        # Compile model\n        model.compile(loss='mean_squared_error', optimizer=optimizer)\n        return model\n    # Create the function that returns the keras model 3\n    def build_model_3(optimizer='adam'):\n        # create model 3\n        model = Sequential()\n        model.add(Dense(10, input_dim=X.shape[1], activation='relu'))\n        model.add(Dense(10, activation='relu'))\n        model.add(Dense(10, activation='relu'))\n        model.add(Dense(1))\n        # Compile model\n        model.compile(loss='mean_squared_error', optimizer=optimizer)\n        return model\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    seed = 1\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # determine the number of folds for k-fold cross-validation\n    n_folds = 5\n    # define the list to store cross-validation scores\n    results_1 = []\n    # define the possible options for the model\n    models = [build_model_1, build_model_2, build_model_3]\n    # loop over models\n    for i in range(len(models)):\n        # build the scikit-learn interface for the keras model\n        regressor = KerasRegressor(build_fn=models[i], epochs=100, \\\n                                   batch_size=50, verbose=0, \\\n                                   shuffle=False)\n        \"\"\"\n        build the pipeline of transformations so for each fold training \n        set will be scaled and test set will be scaled accordingly.\n        \"\"\"\n        model = make_pipeline(StandardScaler(), regressor)\n        # define the cross-validation iterator\n        kfold = KFold(n_splits=n_folds, shuffle=True, \\\n                      random_state=seed)\n        # perform the k-fold cross-validation. \n        # store the scores in result\n        result = cross_val_score(model, X, y, cv=kfold)\n        # add the scores to the results list \n        results_1.append(result)\n    # Print cross-validation score for each model\n    for i in range(len(models)):\n        print(\"Model \", i+1,\" test error rate = \", \\\n              abs(results_1[i].mean()))\n    ```", "```py\n    Model  1  test error rate =  25.48777518749237\n    Model  2  test error rate =  25.30460816860199\n    Model  3  test error rate =  25.390239462852474\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # determine the number of folds for k-fold cross-validation\n    n_folds = 5\n    # define the list to store cross-validation scores\n    results_2 = []\n    # define possible options for epochs and batch_size\n    epochs = [80, 100]\n    batches = [50, 25]\n    # loop over all possible pairs of epochs, batch_size\n    for i in range(len(epochs)):\n        for j in range(len(batches)):\n            # build the scikit-learn interface for the keras model\n            regressor = KerasRegressor(build_fn=build_model_2, \\\n                                       epochs=epochs[i], \\\n                                       batch_size=batches[j], \\\n                                       verbose=0, shuffle=False)\n            \"\"\"\n            build the pipeline of transformations so for each fold \n            training set will be scaled and test set will be scaled \n            accordingly.\n            \"\"\"\n            model = make_pipeline(StandardScaler(), regressor)\n            # define the cross-validation iterator\n            kfold = KFold(n_splits=n_folds, shuffle=True, \\\n                          random_state=seed)\n            # perform the k-fold cross-validation. \n            # store the scores in result\n            result = cross_val_score(model, X, y, cv=kfold)\n            # add the scores to the results list \n            results_2.append(result)\n    \"\"\"\n    Print cross-validation score for each possible pair of epochs, batch_size\n    \"\"\"\n    c = 0\n    for i in range(len(epochs)):\n        for j in range(len(batches)):\n            print(\"batch_size = \", batches[j],\\\n                  \", epochs = \", epochs[i], \\\n                  \", Test error rate = \", abs(results_2[c].mean()))\n            c += 1\n    ```", "```py\n    batch_size = 50 , epochs = 80 , Test error rate = 25.270704221725463\n    batch_size = 25 , epochs = 80 , Test error rate = 25.309741401672362\n    batch_size = 50 , epochs = 100 , Test error rate = 25.095393986701964\n    batch_size = 25 , epochs = 100 , Test error rate = 25.24592453837395\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # determine the number of folds for k-fold cross-validation\n    n_folds = 5\n    # define the list to store cross-validation scores\n    results_3 = []\n    # define the possible options for the optimizer\n    optimizers = ['adam', 'sgd', 'rmsprop']\n    # loop over optimizers\n    for i in range(len(optimizers)):\n        optimizer=optimizers[i]\n        # build the scikit-learn interface for the keras model\n        regressor = KerasRegressor(build_fn=build_model_2, \\\n                                   epochs=100, batch_size=50, \\\n                                   verbose=0, shuffle=False)\n        \"\"\"\n        build the pipeline of transformations so for each fold training \n        set will be scaled and test set will be scaled accordingly.\n        \"\"\"\n        model = make_pipeline(StandardScaler(), regressor)\n        # define the cross-validation iterator\n        kfold = KFold(n_splits=n_folds, shuffle=True, \\\n                      random_state=seed)\n        # perform the k-fold cross-validation. \n        # store the scores in result\n        result = cross_val_score(model, X, y, cv=kfold)\n        # add the scores to the results list \n        results_3.append(result)\n    # Print cross-validation score for each optimizer\n    for i in range(len(optimizers)):\n        print(\"optimizer=\", optimizers[i],\" test error rate = \", \\\n              abs(results_3[i].mean()))\n    ```", "```py\n    optimizer= adam  test error rate =  25.391812739372256\n    optimizer= sgd  test error rate =  25.140230269432067\n    optimizer= rmsprop  test error rate =  25.217947859764102\n    ```", "```py\n    # Load the dataset\n    import pandas as pd\n    X = pd.read_csv('../data/avila-tr_feats.csv')\n    y = pd.read_csv('../data/avila-tr_target.csv')\n    \"\"\"\n    Split the dataset into training set and test set with a 0.8-0.2 ratio\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n    seed = 1\n    X_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.2, random_state=seed)\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    import numpy as np\n    from tensorflow import random\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # define the keras model\n    from keras.models import Sequential\n    from keras.layers import Dense\n    model_1 = Sequential()\n    model_1.add(Dense(10, input_dim=X_train.shape[1], \\\n                      activation='relu'))\n    model_1.add(Dense(6, activation='relu'))\n    model_1.add(Dense(4, activation='relu'))\n    model_1.add(Dense(1, activation='sigmoid'))\n    model_1.compile(loss='binary_crossentropy', optimizer='sgd', \\\n                    metrics=['accuracy'])\n    ```", "```py\n    history=model_1.fit(X_train, y_train, batch_size = 20, epochs = 100, \\\n                        validation_data=(X_test, y_test), \\\n                        verbose=0, shuffle=False)\n    ```", "```py\n    import matplotlib.pyplot as plt \n    import matplotlib\n    %matplotlib inline \n    # plot training error and test error\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim(0,1)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    # print the best accuracy reached on the test set\n    print(\"Best Accuracy on Validation Set =\", \\\n          max(history.history['val_accuracy']))\n    ```", "```py\n    \"\"\"\n    set up a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # define the keras model with l2 regularization with lambda = 0.01\n    from keras.regularizers import l2\n    l2_param = 0.01\n    model_2 = Sequential()\n    model_2.add(Dense(10, input_dim=X_train.shape[1], \\\n                      activation='relu', \\\n                      kernel_regularizer=l2(l2_param)))\n    model_2.add(Dense(6, activation='relu', \\\n                      kernel_regularizer=l2(l2_param)))\n    model_2.add(Dense(4, activation='relu', \\\n                      kernel_regularizer=l2(l2_param)))\n    model_2.add(Dense(1, activation='sigmoid'))\n    model_2.compile(loss='binary_crossentropy', optimizer='sgd', \\\n                    metrics=['accuracy'])\n    # train the model using training set while evaluating on test set\n    history=model_2.fit(X_train, y_train, batch_size = 20, epochs = 100, \\\n                        validation_data=(X_test, y_test), \\\n                        verbose=0, shuffle=False)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim(0,1)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    # print the best accuracy reached on the test set\n    print(\"Best Accuracy on Validation Set =\", \\\n          max(history.history['val_accuracy']))\n    ```", "```py\n    \"\"\"\n    set up a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    from keras.regularizers import l2\n    l2_param = 0.1\n    model_3 = Sequential()\n    model_3.add(Dense(10, input_dim=X_train.shape[1], \\\n                      activation='relu', \\\n                      kernel_regularizer=l2(l2_param)))\n    model_3.add(Dense(6, activation='relu', \\\n                      kernel_regularizer=l2(l2_param)))\n    model_3.add(Dense(4, activation='relu', \\\n                      kernel_regularizer=l2(l2_param)))\n    model_3.add(Dense(1, activation='sigmoid'))\n    model_3.compile(loss='binary_crossentropy', optimizer='sgd', \\\n                    metrics=['accuracy'])\n    # train the model using training set while evaluating on test set\n    history=model_3.fit(X_train, y_train, batch_size = 20, \\\n                        epochs = 100, validation_data=(X_test, y_test), \\\n                        verbose=0, shuffle=False)\n    # plot training error and test error\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim(0,1)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    # print the best accuracy reached on the test set\n    print(\"Best Accuracy on Validation Set =\", \\\n          max(history.history['val_accuracy']))\n    ```", "```py\n    \"\"\"\n    set up a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # define the keras model with l2 regularization with lambda = 0.05\n    from keras.regularizers import l2\n    l2_param = 0.005\n    model_4 = Sequential()\n    model_4.add(Dense(10, input_dim=X_train.shape[1], \\\n                      activation='relu', \\\n                      kernel_regularizer=l2(l2_param)))\n    model_4.add(Dense(6, activation='relu', \\\n                      kernel_regularizer=l2(l2_param)))\n    model_4.add(Dense(4, activation='relu', \\\n                      kernel_regularizer=l2(l2_param)))\n    model_4.add(Dense(1, activation='sigmoid'))\n    model_4.compile(loss='binary_crossentropy', optimizer='sgd', \\\n                    metrics=['accuracy'])\n    # train the model using training set while evaluating on test set\n    history=model_4.fit(X_train, y_train, batch_size = 20, \\\n                        epochs = 100, validation_data=(X_test, y_test), \\\n                        verbose=0, shuffle=False)\n    # plot training error and test error\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim(0,1)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    # print the best accuracy reached on the test set\n    print(\"Best Accuracy on Validation Set =\", \\\n          max(history.history['val_accuracy'])) \n    ```", "```py\n    \"\"\"\n    set up a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # define the keras model with l1 regularization with lambda = 0.01\n    from keras.regularizers import l1\n    l1_param = 0.01\n    model_5 = Sequential()\n    model_5.add(Dense(10, input_dim=X_train.shape[1], \\\n                      activation='relu', \\\n                      kernel_regularizer=l1(l1_param)))\n    model_5.add(Dense(6, activation='relu', \\\n                      kernel_regularizer=l1(l1_param)))\n    model_5.add(Dense(4, activation='relu', \\\n                      kernel_regularizer=l1(l1_param)))\n    model_5.add(Dense(1, activation='sigmoid'))\n    model_5.compile(loss='binary_crossentropy', optimizer='sgd', \\\n                    metrics=['accuracy'])\n    # train the model using training set while evaluating on test set\n    history=model_5.fit(X_train, y_train, batch_size = 20, \\\n                        epochs = 100, validation_data=(X_test, y_test), \\\n                        verbose=0, shuffle=True)\n    # plot training error and test error\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim(0,1)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    # print the best accuracy reached on the test set\n    print(\"Best Accuracy on Validation Set =\", \\\n          max(history.history['val_accuracy']))\n    ```", "```py\n    \"\"\"\n    set up a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # define the keras model with l1 regularization with lambda = 0.1\n    from keras.regularizers import l1\n    l1_param = 0.005\n    model_6 = Sequential()\n    model_6.add(Dense(10, input_dim=X_train.shape[1], \\\n                      activation='relu', \\\n                      kernel_regularizer=l1(l1_param)))\n    model_6.add(Dense(6, activation='relu', \\\n                      kernel_regularizer=l1(l1_param)))\n    model_6.add(Dense(4, activation='relu', \\\n                      kernel_regularizer=l1(l1_param)))\n    model_6.add(Dense(1, activation='sigmoid'))\n    model_6.compile(loss='binary_crossentropy', optimizer='sgd', \\\n                    metrics=['accuracy'])\n    # train the model using training set while evaluating on test set\n    history=model_6.fit(X_train, y_train, batch_size = 20, \\\n                        epochs = 100, validation_data=(X_test, y_test), \\\n                        verbose=0, shuffle=False)\n    # plot training error and test error\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim(0,1)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    # print the best accuracy reached on the test set\n    print(\"Best Accuracy on Validation Set =\", \\\n           max(history.history['val_accuracy']))\n    ```", "```py\n    \"\"\"\n    set up a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    \"\"\"\n    define the keras model with l1_l2 regularization with l1_lambda = 0.005 and l2_lambda = 0.005\n    \"\"\"\n    from keras.regularizers import l1_l2\n    l1_param = 0.005\n    l2_param = 0.005\n    model_7 = Sequential()\n    model_7.add(Dense(10, input_dim=X_train.shape[1], \\\n                activation='relu', \\\n                kernel_regularizer=l1_l2(l1=l1_param, l2=l2_param)))\n    model_7.add(Dense(6, activation='relu', \\\n                      kernel_regularizer=l1_l2(l1=l1_param, \\\n                                               l2=l2_param)))\n    model_7.add(Dense(4, activation='relu', \\\n                      kernel_regularizer=l1_l2(l1=l1_param, \\\n                                               l2=l2_param)))\n    model_7.add(Dense(1, activation='sigmoid'))\n    model_7.compile(loss='binary_crossentropy', optimizer='sgd', \\\n                    metrics=['accuracy'])\n    # train the model using training set while evaluating on test set\n    history=model_7.fit(X_train, y_train, batch_size = 20, \\\n                        epochs = 100, validation_data=(X_test, y_test), \\\n                        verbose=0, shuffle=True)\n\n    # plot training error and test error\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim(0,1)\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    # print the best accuracy reached on the test set\n    print(\"Best Accuracy on Validation Set =\", \\\n           max(history.history['val_accuracy']))\n    ```", "```py\n    # Load the dataset\n    import pandas as pd\n    X = pd.read_csv('../data/traffic_volume_feats.csv')\n    y = pd.read_csv('../data/traffic_volume_target.csv')\n    \"\"\"\n    Split the dataset into training set and test set with an 80-20 ratio\n    \"\"\"\n    from sklearn.model_selection import train_test_split\n    seed=1\n    X_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.2, random_state=seed)\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    import numpy as np\n    from tensorflow import random\n    np.random.seed(seed)\n    random.set_seed(seed)\n    from keras.models import Sequential\n    from keras.layers import Dense\n    # create model\n    model_1 = Sequential()\n    model_1.add(Dense(10, input_dim=X_train.shape[1], \\\n                      activation='relu'))\n    model_1.add(Dense(10, activation='relu'))\n    model_1.add(Dense(1))\n    # Compile model\n    model_1.compile(loss='mean_squared_error', optimizer='rmsprop')\n    ```", "```py\n    # train the model using training set while evaluating on test set\n    history=model_1.fit(X_train, y_train, batch_size = 50, \\\n                        epochs = 200, validation_data=(X_test, y_test), \\\n                        verbose=0) \n    ```", "```py\n    import matplotlib.pyplot as plt \n    import matplotlib\n    %matplotlib inline \n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0) \n    # plot training error and test error plots \n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim((0, 25000))\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    # print the best accuracy reached on the test set\n    print(\"Lowest error on training set = \", \\\n          min(history.history['loss']))\n    print(\"Lowest error on validation set = \", \\\n          min(history.history['val_loss']))\n    ```", "```py\n    Lowest error on training set =  24.673954981565476\n    Lowest error on validation set =  25.11553382873535\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    from keras.layers import Dropout\n    # create model\n    model_2 = Sequential()\n    model_2.add(Dense(10, input_dim=X_train.shape[1], \\\n                      activation='relu'))\n    model_2.add(Dropout(0.1))\n    model_2.add(Dense(10, activation='relu'))\n    model_2.add(Dense(1))\n    # Compile model\n    model_2.compile(loss='mean_squared_error', \\\n                    optimizer='rmsprop')\n    # train the model using training set while evaluating on test set\n    history=model_2.fit(X_train, y_train, batch_size = 50, \\\n                        epochs = 200, validation_data=(X_test, y_test), \\\n                        verbose=0, shuffle=False)\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim((0, 25000))\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    # print the best accuracy reached on the test set\n    print(\"Lowest error on training set = \", \\\n          min(history.history['loss']))\n    print(\"Lowest error on validation set = \", \\\n          min(history.history['val_loss']))\n    ```", "```py\n    Lowest error on training set =  407.8203821182251\n    Lowest error on validation set =  54.58488750457764\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # create model\n    model_3 = Sequential()\n    model_3.add(Dense(10, input_dim=X_train.shape[1], \\\n                      activation='relu'))\n    model_3.add(Dropout(0.1))\n    model_3.add(Dense(10, activation='relu'))\n    model_3.add(Dropout(0.1))\n    model_3.add(Dense(1))\n    # Compile model\n    model_3.compile(loss='mean_squared_error', \\\n                    optimizer='rmsprop')\n    # train the model using training set while evaluating on test set\n    history=model_3.fit(X_train, y_train, batch_size = 50, \\\n                        epochs = 200, validation_data=(X_test, y_test), \\\n                        verbose=0, shuffle=False)\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim((0, 25000))\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    # print the best accuracy reached on the test set\n    print(\"Lowest error on training set = \", \\\n          min(history.history['loss']))\n    print(\"Lowest error on validation set = \", \\\n          min(history.history['val_loss']))\n    ```", "```py\n    Lowest error on training set =  475.9299939632416\n    Lowest error on validation set =  61.646054649353026\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # create model\n    model_4 = Sequential()\n    model_4.add(Dense(10, input_dim=X_train.shape[1], \\\n                      activation='relu'))\n    model_4.add(Dropout(0.2))\n    model_4.add(Dense(10, activation='relu'))\n    model_4.add(Dropout(0.1))\n    model_4.add(Dense(1))\n    # Compile model\n    model_4.compile(loss='mean_squared_error', optimizer='rmsprop')\n    # train the model using training set while evaluating on test set\n    history=model_4.fit(X_train, y_train, batch_size = 50, epochs = 200, \\\n                        validation_data=(X_test, y_test), verbose=0)\n    matplotlib.rcParams['figure.figsize'] = (10.0, 8.0)\n    plt.plot(history.history['loss'])\n    plt.plot(history.history['val_loss'])\n    plt.ylim((0, 25000))\n    plt.ylabel('loss')\n    plt.xlabel('epoch')\n    plt.legend(['train loss', 'validation loss'], loc='upper right')\n    # print the best accuracy reached on the test set\n    print(\"Lowest error on training set = \", \\\n          min(history.history['loss']))\n    print(\"Lowest error on validation set = \", \\\n          min(history.history['val_loss']))\n    ```", "```py\n    Lowest error on training set =  935.1562484741211\n    Lowest error on validation set =  132.39965686798095\n    ```", "```py\n    # Load The dataset\n    import pandas as pd\n    X = pd.read_csv('../data/avila-tr_feats.csv')\n    y = pd.read_csv('../data/avila-tr_target.csv')\n    ```", "```py\n    # Create the function that returns the keras model\n    from keras.models import Sequential\n    from keras.layers import Dense\n    from keras.regularizers import l2\n    def build_model(lambda_parameter):\n        model = Sequential()\n        model.add(Dense(10, input_dim=X.shape[1], \\\n                        activation='relu', \\\n                        kernel_regularizer=l2(lambda_parameter)))\n        model.add(Dense(6, activation='relu', \\\n                        kernel_regularizer=l2(lambda_parameter)))\n        model.add(Dense(4, activation='relu', \\\n                        kernel_regularizer=l2(lambda_parameter)))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(loss='binary_crossentropy', \\\n                      optimizer='sgd', metrics=['accuracy'])\n        return model\n    ```", "```py\n    from keras.wrappers.scikit_learn import KerasClassifier\n    from sklearn.model_selection import GridSearchCV\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    import numpy as np\n    from tensorflow import random\n    seed = 1\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # create the Keras wrapper with scikit learn\n    model = KerasClassifier(build_fn=build_model, verbose=0, \\\n                            shuffle=False)\n    # define all the possible values for each hyperparameter\n    lambda_parameter = [0.01, 0.5, 1]\n    epochs = [50, 100]\n    batch_size = [20]\n    \"\"\"\n    create the dictionary containing all possible values of hyperparameters\n    \"\"\"\n    param_grid = dict(lambda_parameter=lambda_parameter, \\\n                      epochs=epochs, batch_size=batch_size)\n    # perform 5-fold cross-validation for ??????? store the results\n    grid_seach = GridSearchCV(estimator=model, \\\n                              param_grid=param_grid, cv=5)\n    results_1 = grid_seach.fit(X, y)\n    ```", "```py\n    print(\"Best cross-validation score =\", results_1.best_score_)\n    print(\"Parameters for Best cross-validation score=\", \\\n          results_1.best_params_)\n    # print the results for all evaluated hyperparameter combinations\n    accuracy_means = results_1.cv_results_['mean_test_score']\n    accuracy_stds = results_1.cv_results_['std_test_score']\n    parameters = results_1.cv_results_['params']\n    for p in range(len(parameters)):\n        print(\"Accuracy %f (std %f) for params %r\" % \\\n              (accuracy_means[p], accuracy_stds[p], parameters[p]))\n    ```", "```py\n    Best cross-validation score = 0.7673058390617371\n    Parameters for Best cross-validation score= {'batch_size': 20, \n    'epochs': 100, 'lambda_parameter': 0.01}\n    Accuracy 0.764621 (std 0.004330) for params {'batch_size': 20, \n    'epochs': 50, 'lambda_parameter': 0.01}\n    Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, \n    'epochs': 50, 'lambda_parameter': 0.5}\n    Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, \n    'epochs': 50, 'lambda_parameter': 1}\n    Accuracy 0.767306 (std 0.015872) for params {'batch_size': 20, \n    'epochs': 100, 'lambda_parameter': 0.01}\n    Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, \n    'epochs': 100, 'lambda_parameter': 0.5}\n    Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, \n    'epochs': 100, 'lambda_parameter': 1}\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # create the Keras wrapper with scikit learn\n    model = KerasClassifier(build_fn=build_model, verbose=0, shuffle=False)\n    # define all the possible values for each hyperparameter\n    lambda_parameter = [0.001, 0.01, 0.05, 0.1]\n    epochs = [100]\n    batch_size = [20]\n    \"\"\"\n    create the dictionary containing all possible values of hyperparameters\n    \"\"\"\n    param_grid = dict(lambda_parameter=lambda_parameter, \\\n                      epochs=epochs, batch_size=batch_size)\n    \"\"\"\n    search the grid, perform 5-fold cross-validation for each possible combination, store the results\n    \"\"\"\n    grid_seach = GridSearchCV(estimator=model, \\\n                              param_grid=param_grid, cv=5)\n    results_2 = grid_seach.fit(X, y)\n    # print the results for best cross-validation score\n    print(\"Best cross-validation score =\", results_2.best_score_)\n    print(\"Parameters for Best cross-validation score =\", \\\n          results_2.best_params_)\n    # print the results for the entire grid\n    accuracy_means = results_2.cv_results_['mean_test_score']\n    accuracy_stds = results_2.cv_results_['std_test_score']\n    parameters = results_2.cv_results_['params']\n    for p in range(len(parameters)):\n        print(\"Accuracy %f (std %f) for params %r\" % \\\n              (accuracy_means[p], accuracy_stds[p], parameters[p]))\n    ```", "```py\n    Best cross-validation score = 0.786385428905487\n    Parameters for Best cross-validation score = {'batch_size': 20, \n    'epochs': 100, 'lambda_parameter': 0.001}\n    Accuracy 0.786385 (std 0.010177) for params {'batch_size': 20, \n    'epochs': 100, 'lambda_parameter': 0.001}\n    Accuracy 0.693960 (std 0.084994) for params {'batch_size': 20, \n    'epochs': 100, 'lambda_parameter': 0.01}\n    Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, \n    'epochs': 100, 'lambda_parameter': 0.05}\n    Accuracy 0.589070 (std 0.008244) for params {'batch_size': 20, \n    'epochs': 100, 'lambda_parameter': 0.1}\n    ```", "```py\n    # Create the function that returns the keras model\n    from keras.layers import Dropout\n    def build_model(rate):\n        model = Sequential()\n        model.add(Dense(10, input_dim=X.shape[1], activation='relu'))\n        model.add(Dropout(rate))\n        model.add(Dense(6, activation='relu'))\n        model.add(Dropout(rate))\n        model.add(Dense(4, activation='relu'))\n        model.add(Dropout(rate))\n        model.add(Dense(1, activation='sigmoid'))\n        model.compile(loss='binary_crossentropy', \\\n                      optimizer='sgd', metrics=['accuracy'])\n        return model\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # create the Keras wrapper with scikit learn\n    model = KerasClassifier(build_fn=build_model, verbose=0,shuffle=False)\n    # define all the possible values for each hyperparameter\n    rate = [0, 0.1, 0.2]\n    epochs = [50, 100]\n    batch_size = [20]\n    \"\"\"\n    create the dictionary containing all possible values of hyperparameters\n    \"\"\"\n    param_grid = dict(rate=rate, epochs=epochs, batch_size=batch_size)\n    \"\"\"\n    perform 5-fold cross-validation for 10 randomly selected combinations, store the results\n    \"\"\"\n    grid_seach = GridSearchCV(estimator=model, \\\n                              param_grid=param_grid, cv=5)\n    results_3 = grid_seach.fit(X, y)\n    # print the results for best cross-validation score\n    print(\"Best cross-validation score =\", results_3.best_score_)\n    print(\"Parameters for Best cross-validation score =\", \\\n          results_3.best_params_)\n    # print the results for the entire grid\n    accuracy_means = results_3.cv_results_['mean_test_score']\n    accuracy_stds = results_3.cv_results_['std_test_score']\n    parameters = results_3.cv_results_['params']\n    for p in range(len(parameters)):\n        print(\"Accuracy %f (std %f) for params %r\" % \\\n              (accuracy_means[p], accuracy_stds[p], parameters[p]))\n    ```", "```py\n    Best cross-validation score= 0.7918504476547241\n    Parameters for Best cross-validation score= {'batch_size': 20, \n    'epochs': 100, 'rate': 0}\n    Accuracy 0.786769 (std 0.008255) for params {'batch_size': 20, \n    'epochs': 50, 'rate': 0}\n    Accuracy 0.764717 (std 0.007691) for params {'batch_size': 20, \n    'epochs': 50, 'rate': 0.1}\n    Accuracy 0.752637 (std 0.013546) for params {'batch_size': 20, \n    'epochs': 50, 'rate': 0.2}\n    Accuracy 0.791850 (std 0.008519) for params {'batch_size': 20, \n    'epochs': 100, 'rate': 0}\n    Accuracy 0.779291 (std 0.009504) for params {'batch_size': 20, \n    'epochs': 100, 'rate': 0.1}\n    Accuracy 0.767306 (std 0.005773) for params {'batch_size': 20, \n    'epochs': 100, 'rate': 0.2}\n    ```", "```py\n    \"\"\"\n    define a seed for random number generator so the result will be reproducible\n    \"\"\"\n    np.random.seed(seed)\n    random.set_seed(seed)\n    # create the Keras wrapper with scikit learn\n    model = KerasClassifier(build_fn=build_model, verbose=0, shuffle=False)\n    # define all the possible values for each hyperparameter\n    rate = [0.0, 0.05, 0.1]\n    epochs = [100]\n    batch_size = [20]\n    \"\"\"\n    create the dictionary containing all possible values of hyperparameters\n    \"\"\"\n    param_grid = dict(rate=rate, epochs=epochs, batch_size=batch_size)\n    \"\"\"\n    perform 5-fold cross-validation for 10 randomly selected combinations, store the results\n    \"\"\"\n    grid_seach = GridSearchCV(estimator=model, \\\n                              param_grid=param_grid, cv=5)\n    results_4 = grid_seach.fit(X, y)\n    # print the results for best cross-validation score\n    print(\"Best cross-validation score =\", results_4.best_score_)\n    print(\"Parameters for Best cross-validation score =\", \\\n          results_4.best_params_)\n    # print the results for the entire grid\n    accuracy_means = results_4.cv_results_['mean_test_score']\n    accuracy_stds = results_4.cv_results_['std_test_score']\n    parameters = results_4.cv_results_['params']\n    for p in range(len(parameters)):\n        print(\"Accuracy %f (std %f) for params %r\" % \\\n              (accuracy_means[p], accuracy_stds[p], parameters[p]))\n    ```", "```py\n    Best cross-validation score= 0.7862895488739013\n    Parameters for Best cross-validation score= {'batch_size': 20, \n    'epochs': 100, 'rate': 0.0}\n    Accuracy 0.786290 (std 0.013557) for params {'batch_size': 20, \n    'epochs': 100, 'rate': 0.0}\n    Accuracy 0.786098 (std 0.005184) for params {'batch_size': 20, \n    'epochs': 100, 'rate': 0.05}\n    Accuracy 0.772004 (std 0.013733) for params {'batch_size': 20, \n    'epochs': 100, 'rate': 0.1}\n    ```", "```py\n    # Import the libraries\n    import numpy as np\n    import pandas as pd\n    # Load the Data\n    X = pd.read_csv(\"../data/aps_failure_training_feats.csv\")\n    y = pd.read_csv(\"../data/aps_failure_training_target.csv\")\n    # Use the head function to get a glimpse data\n    X.head()\n    ```", "```py\n    # Split the data into training and testing sets\n    from sklearn.model_selection import train_test_split\n    seed = 13\n    X_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.3, random_state=seed)\n    ```", "```py\n    # Initialize StandardScaler\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    # Transform the training data\n    X_train = sc.fit_transform(X_train)\n    X_train = pd.DataFrame(X_train, columns=X_test.columns)\n    # Transform the testing data\n    X_test = sc.transform(X_test)\n    X_test = pd.DataFrame(X_test, columns = X_train.columns)\n    ```", "```py\n    # Import the relevant Keras libraries\n    from keras.models import Sequential\n    from keras.layers import Dense\n    from keras.layers import Dropout\n    from tensorflow import random\n    ```", "```py\n    # Initiate the Model with Sequential Class\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model = Sequential()\n    ```", "```py\n    # Add the hidden dense layers and with dropout Layer\n    model.add(Dense(units=64, activation='relu', \\\n                    kernel_initializer='uniform', \\\n                    input_dim=X_train.shape[1]))\n    model.add(Dropout(rate=0.5))\n    model.add(Dense(units=32, activation='relu', \\\n                    kernel_initializer='uniform', \\\n                    input_dim=X_train.shape[1]))\n    model.add(Dropout(rate=0.4))\n    model.add(Dense(units=16, activation='relu', \\\n                    kernel_initializer='uniform', \\\n                    input_dim=X_train.shape[1]))\n    model.add(Dropout(rate=0.3))\n    model.add(Dense(units=8, activation='relu', \\\n                    kernel_initializer='uniform', \\\n                    input_dim=X_train.shape[1]))\n    model.add(Dropout(rate=0.2))\n    model.add(Dense(units=4, activation='relu', \\\n                    kernel_initializer='uniform'))\n    model.add(Dropout(rate=0.1))\n    ```", "```py\n    # Add Output Dense Layer\n    model.add(Dense(units=1, activation='sigmoid', \\\n                    kernel_initializer='uniform'))\n    ```", "```py\n    # Compile the Model\n    model.compile(optimizer='adam', loss='binary_crossentropy', \\\n                  metrics=['accuracy'])\n    ```", "```py\n    # Fit the Model\n    model.fit(X_train, y_train, epochs=100, batch_size=20, \\\n              verbose=1, validation_split=0.2, shuffle=False)\n    ```", "```py\n    test_loss, test_acc = model.evaluate(X_test, y_test)\n    print(f'The loss on the test set is {test_loss:.4f} and \\\n    the accuracy is {test_acc*100:.4f}%')\n    ```", "```py\n    18000/18000 [==============================] - 0s 19us/step\n    The loss on the test set is 0.0766 and the accuracy is 98.9833%\n    ```", "```py\n    # Use the value_count function to calculate distinct class values\n    y_test['class'].value_counts()\n    ```", "```py\n    0    17700\n    1      300\n    Name: class, dtype: int64\n    ```", "```py\n    # Calculate the null accuracy\n    y_test['class'].value_counts(normalize=True).loc[0]\n    ```", "```py\n    0.9833333333333333\n    ```", "```py\n    # Import the libraries\n    import numpy as np\n    import pandas as pd\n    # Load the Data\n    X = pd.read_csv(\"../data/aps_failure_training_feats.csv\")\n    y = pd.read_csv(\"../data/aps_failure_training_target.csv\")\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    seed = 42\n    X_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, test_size=0.20, random_state=seed)\n    ```", "```py\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    # Transform the training data\n    X_train = sc.fit_transform(X_train)\n    X_train = pd.DataFrame(X_train,columns=X_test.columns)\n    # Transform the testing data\n    X_test = sc.transform(X_test)\n    X_test = pd.DataFrame(X_test,columns=X_train.columns)\n    ```", "```py\n    # Import the relevant Keras libraries\n    from keras.models import Sequential\n    from keras.layers import Dense\n    from keras.layers import Dropout\n    from tensorflow import random\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model = Sequential()\n    # Add the hidden dense layers with dropout Layer\n    model.add(Dense(units=64, activation='relu', \\\n                    kernel_initializer='uniform', \\\n                    input_dim=X_train.shape[1]))\n    model.add(Dropout(rate=0.5))\n    model.add(Dense(units=32, activation='relu', \\\n                    kernel_initializer='uniform'))\n    model.add(Dropout(rate=0.4))\n    model.add(Dense(units=16, activation='relu', \\\n                    kernel_initializer='uniform'))\n    model.add(Dropout(rate=0.3))\n    model.add(Dense(units=8, activation='relu', \\\n              kernel_initializer='uniform'))\n    model.add(Dropout(rate=0.2))\n    model.add(Dense(units=4, activation='relu', \\\n                    kernel_initializer='uniform'))\n    model.add(Dropout(rate=0.1))\n    # Add Output Dense Layer\n    model.add(Dense(units=1, activation='sigmoid', \\\n                    kernel_initializer='uniform'))\n    # Compile the Model\n    model.compile(optimizer='adam', loss='binary_crossentropy', \\\n                  metrics=['accuracy'])\n    ```", "```py\n    model.fit(X_train, y_train, epochs=100, batch_size=20, \\\n              verbose=1, validation_split=0.2, shuffle=False)\n    ```", "```py\n    y_pred_prob = model.predict_proba(X_test)\n    ```", "```py\n    from sklearn.metrics import roc_curve\n    fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n    ```", "```py\n    import matplotlib.pyplot as plt\n    plt.plot(fpr, tpr)\n    plt.title(\"ROC Curve for APS Failure\")\n    plt.xlabel(\"False Positive rate (1-Specificity)\")\n    plt.ylabel(\"True Positive rate (Sensitivity)\")\n    plt.grid(True)\n    plt.show()\n    ```", "```py\n    from sklearn.metrics import roc_auc_score\n    roc_auc_score(y_test,y_pred_prob)\n    ```", "```py\n    0.944787151628455\n    ```", "```py\n    # Import the Libraries \n    from keras.models import Sequential\n    from keras.layers import Conv2D, MaxPool2D, Flatten, Dense\n    import numpy as np\n    from tensorflow import random\n    ```", "```py\n    # Initiate the classifier\n    seed = 1\n    np.random.seed(seed)\n    random.set_seed(seed)\n    classifier=Sequential()\n    ```", "```py\n    classifier.add(Conv2D(32,(3,3),input_shape=(64,64,3),\\\n                   activation='relu'))\n    classifier.add(Conv2D(32,(3,3),activation = 'relu'))\n    classifier.add(Conv2D(32,(3,3),activation = 'relu'))\n    ```", "```py\n    classifier.add(MaxPool2D(pool_size=(2,2)))\n    ```", "```py\n    classifier.add(Flatten())\n    ```", "```py\n    classifier.add(Dense(units=128,activation='relu')) \n    ```", "```py\n    classifier.add(Dense(128,activation='relu'))\n    classifier.add(Dense(128,activation='relu'))\n    classifier.add(Dense(128,activation='relu'))\n    ```", "```py\n    classifier.add(Dense(units=1,activation='softmax')) \n    ```", "```py\n    # Compile The network\n    classifier.compile(optimizer='adam', loss='binary_crossentropy', \\\n                       metrics=['accuracy'])\n    ```", "```py\n    from keras.preprocessing.image import ImageDataGenerator\n    train_datagen = ImageDataGenerator(rescale = 1./255, \\\n                                       shear_range = 0.2, \\\n                                       zoom_range = 0.2, \\\n                                       horizontal_flip = True)\n    test_datagen = ImageDataGenerator(rescale = 1./255)\n    ```", "```py\n    training_set = \\\n    train_datagen.flow_from_directory('../dataset/training_set', \\\n                                      target_size = (64, 64), \\\n                                      batch_size = 32, \\\n                                      class_mode = 'binary')\n    ```", "```py\n    test_set = \\\n    test_datagen.flow_from_directory('../dataset/test_set', \\\n                                     target_size = (64, 64), \\\n                                     batch_size = 32, \\\n                                     class_mode = 'binary')\n    ```", "```py\n    classifier.fit_generator(training_set, steps_per_epoch = 10000, \\\n                             epochs = 2, validation_data = test_set, \\\n                             validation_steps = 2500, shuffle=False)\n    ```", "```py\n    Epoch 1/2\n    10000/10000 [==============================] - 2452s 245ms/step - loss: 8.1783 - accuracy: 0.4667 - val_loss: 11.4999 - val_accuracy: 0.4695\n    Epoch 2/2\n    10000/10000 [==============================] - 2496s 250ms/step - loss: 8.1726 - accuracy: 0.4671 - val_loss: 10.5416 - val_accuracy: 0.4691\n    ```", "```py\n    from keras.preprocessing import image\n    new_image = \\\n    image.load_img('../test_image_2.jpg', target_size = (64, 64))\n    new_image\n    ```", "```py\n    training_set.class_indices\n    ```", "```py\n    new_image = image.img_to_array(new_image)\n    new_image = np.expand_dims(new_image, axis = 0)\n    ```", "```py\n    result = classifier.predict(new_image)\n    ```", "```py\n    if result[0][0] == 1:\n        prediction = 'It is a flower'\n    else:\n        prediction = 'It is a car'\n    print(prediction)\n    ```", "```py\n    It is a flower\n    ```", "```py\n    import numpy as np\n    from keras.applications.vgg16 import VGG16, preprocess_input\n    from keras.preprocessing import image \n    ```", "```py\n    classifier = VGG16()\n    classifier.summary()\n    ```", "```py\n    new_image = \\\n    image.load_img('../Data/Prediction/test_image_1.jpg', \\\n                   target_size=(224, 224))\n    new_image\n    ```", "```py\n    transformed_image = image.img_to_array(new_image)\n    transformed_image.shape\n    ```", "```py\n    (224, 224, 3)\n    ```", "```py\n    transformed_image = np.expand_dims(transformed_image, axis=0)\n    transformed_image.shape\n    ```", "```py\n    (1, 224, 224, 3)\n    ```", "```py\n    transformed_image = preprocess_input(transformed_image)\n    transformed_image\n    ```", "```py\n    y_pred = classifier.predict(transformed_image)\n    y_pred\n    ```", "```py\n    y_pred.shape\n    ```", "```py\n    (1, 1000)\n    ```", "```py\n    from keras.applications.vgg16 import decode_predictions\n    decode_predictions(y_pred, top=5)\n    ```", "```py\n    [[('n03785016', 'moped', 0.8433369),\n      ('n03791053', 'motor_scooter', 0.14188054),\n      ('n03127747', 'crash_helmet', 0.007004856),\n      ('n03208938', 'disk_brake', 0.0022349996),\n      ('n04482393', 'tricycle', 0.0007717237)]]\n    ```", "```py\n    label = decode_predictions(y_pred)\n    \"\"\"\n    Most likely result is retrieved, for example, the highest probability\n    \"\"\"\n    decoded_label = label[0][0]\n    # The classification is printed\n    print('%s (%.2f%%)' % (decoded_label[1], decoded_label[2]*100 ))\n    ```", "```py\n    moped (84.33%)\n    ```", "```py\n    import numpy as np\n    from keras.applications.resnet50 import ResNet50, preprocess_input\n    from keras.preprocessing import image \n    ```", "```py\n    classifier = ResNet50()\n    classifier.summary()\n    ```", "```py\n    new_image = \\\n    image.load_img('../Data/Prediction/test_image_4.jpg', \\\n                   target_size=(224, 224))\n    new_image\n    ```", "```py\n    transformed_image = image.img_to_array(new_image)\n    transformed_image.shape\n    ```", "```py\n    transformed_image = np.expand_dims(transformed_image, axis=0)\n    transformed_image.shape\n    ```", "```py\n    transformed_image = preprocess_input(transformed_image)\n    transformed_image\n    ```", "```py\n    y_pred = classifier.predict(transformed_image)\n    y_pred\n    ```", "```py\n    y_pred.shape\n    ```", "```py\n    (1, 1000)\n    ```", "```py\n    from keras.applications.resnet50 import decode_predictions\n    decode_predictions(y_pred, top=5)\n    ```", "```py\n    [[('n04404412', 'television', 0.99673873),\n      ('n04372370', 'switch', 0.0009829825),\n      ('n04152593', 'screen', 0.00095111143),\n      ('n03782006', 'monitor', 0.0006477369),\n      ('n04069434', 'reflex_camera', 8.5398955e-05)]]\n    ```", "```py\n    label = decode_predictions(y_pred)\n    \"\"\"\n    Most likely result is retrieved, for example, \n    the highest probability\n    \"\"\"\n    decoded_label = label[0][0]\n    # The classification is printed \n    print('%s (%.2f%%)' % (decoded_label[1], decoded_label[2]*100 ))\n    ```", "```py\n    television (99.67%)\n    ```", "```py\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    from tensorflow import random\n    ```", "```py\n    dataset_training = pd.read_csv('../AMZN_train.csv')\n    dataset_training.head()\n    ```", "```py\n    training_data = dataset_training[['Open']].values \n    training_data\n    ```", "```py\n    array([[ 398.799988],\n           [ 398.290009],\n           [ 395.850006],\n           ...,\n           [1454.199951],\n           [1473.349976],\n           [1510.800049]])\n    ```", "```py\n    from sklearn.preprocessing import MinMaxScaler\n    sc = MinMaxScaler(feature_range = (0, 1))\n    training_data_scaled = sc.fit_transform(training_data)\n    training_data_scaled\n    ```", "```py\n    array([[0.06523313],\n           [0.06494233],\n           [0.06355099],\n           ...,\n           [0.66704299],\n           [0.67796271],\n           [0.69931748]])\n    ```", "```py\n    X_train = []\n    y_train = []\n    for i in range(60, 1258):\n        X_train.append(training_data_scaled[i-60:i, 0])\n        y_train.append(training_data_scaled[i, 0])\n    X_train, y_train = np.array(X_train), np.array(y_train)\n    ```", "```py\n    X_train = np.reshape(X_train, (X_train.shape[0], \\\n                         X_train.shape[1], 1))\n    ```", "```py\n    from keras.models import Sequential\n    from keras.layers import Dense, LSTM, Dropout\n    ```", "```py\n    seed = 1\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model = Sequential()\n    ```", "```py\n    model.add(LSTM(units = 50, return_sequences = True, \\\n              input_shape = (X_train.shape[1], 1)))\n    # Adding a second LSTM layer\n    model.add(LSTM(units = 50, return_sequences = True))\n    # Adding a third LSTM layer\n    model.add(LSTM(units = 50, return_sequences = True))\n    # Adding a fourth LSTM layer\n    model.add(LSTM(units = 50))\n    # Adding the output layer\n    model.add(Dense(units = 1))\n    ```", "```py\n    # Compiling the RNN\n    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n    # Fitting the RNN to the Training set\n    model.fit(X_train, y_train, epochs = 100, batch_size = 32)\n    ```", "```py\n    dataset_testing = pd.read_csv('../AMZN_test.csv')\n    actual_stock_price = dataset_testing[['Open']].values\n    actual_stock_price\n    ```", "```py\n    total_data = pd.concat((dataset_training['Open'], \\\n                            dataset_testing['Open']), axis = 0)\n    ```", "```py\n    inputs = total_data[len(total_data) \\\n             - len(dataset_testing) - 60:].values\n    inputs = inputs.reshape(-1,1)\n    inputs = sc.transform(inputs)\n    X_test = []\n    for i in range(60, 81):\n        X_test.append(inputs[i-60:i, 0])\n    X_test = np.array(X_test)\n    X_test = np.reshape(X_test, (X_test.shape[0], \\\n                                 X_test.shape[1], 1))\n    predicted_stock_price = model.predict(X_test)\n    predicted_stock_price = \\\n    sc.inverse_transform(predicted_stock_price)\n    ```", "```py\n    # Visualizing the results\n    plt.plot(actual_stock_price, color = 'green', \\\n             label = 'Real Amazon Stock Price',ls='--')\n    plt.plot(predicted_stock_price, color = 'red', \\\n             label = 'Predicted Amazon Stock Price',ls='-')\n    plt.title('Predicted Stock Price')\n    plt.xlabel('Time in days')\n    plt.ylabel('Real Stock Price')\n    plt.legend()\n    plt.show()\n    ```", "```py\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    from tensorflow import random\n    ```", "```py\n    dataset_training = pd.read_csv('../AMZN_train.csv')\n    dataset_training.head()\n    ```", "```py\n    training_data = dataset_training[['Open']].values\n    training_data\n    ```", "```py\n    array([[ 398.799988],\n           [ 398.290009],\n           [ 395.850006],\n           ...,\n           [1454.199951],\n           [1473.349976],\n           [1510.800049]])\n    ```", "```py\n    from sklearn.preprocessing import MinMaxScaler\n    sc = MinMaxScaler(feature_range = (0, 1))\n    training_data_scaled = sc.fit_transform(training_data)\n    training_data_scaled\n    ```", "```py\n    array([[0.06523313],\n           [0.06494233],\n           [0.06355099],\n           ...,\n           [0.66704299],\n           [0.67796271],\n           [0.69931748]])\n    ```", "```py\n    X_train = []\n    y_train = []\n    for i in range(60, 1258):\n        X_train.append(training_data_scaled[i-60:i, 0])\n        y_train.append(training_data_scaled[i, 0])\n    X_train, y_train = np.array(X_train), np.array(y_train)\n    ```", "```py\n    X_train = np.reshape(X_train, (X_train.shape[0], \\\n                                   X_train.shape[1], 1))\n    ```", "```py\n    from keras.models import Sequential\n    from keras.layers import Dense, LSTM, Dropout\n    ```", "```py\n    seed = 1\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model = Sequential()\n    ```", "```py\n    model.add(LSTM(units = 50, return_sequences = True, \\\n                   input_shape = (X_train.shape[1], 1)))\n    model.add(Dropout(0.2))\n    # Adding a second LSTM layer and some Dropout regularization\n    model.add(LSTM(units = 50, return_sequences = True))\n    model.add(Dropout(0.2))\n    # Adding a third LSTM layer and some Dropout regularization\n    model.add(LSTM(units = 50, return_sequences = True))\n    model.add(Dropout(0.2))\n    # Adding a fourth LSTM layer and some Dropout regularization\n    model.add(LSTM(units = 50))\n    model.add(Dropout(0.2))\n    # Adding the output layer\n    model.add(Dense(units = 1))\n    ```", "```py\n    # Compiling the RNN\n    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n    # Fitting the RNN to the Training set\n    model.fit(X_train, y_train, epochs = 100, batch_size = 32)\n    ```", "```py\n    dataset_testing = pd.read_csv('../AMZN_test.csv')\n    actual_stock_price = dataset_testing[['Open']].values\n    actual_stock_price \n    ```", "```py\n    total_data = pd.concat((dataset_training['Open'], \\\n                            dataset_testing['Open']), axis = 0)\n    ```", "```py\n    inputs = total_data[len(total_data) \\\n             - len(dataset_testing) - 60:].values\n    inputs = inputs.reshape(-1,1)\n    inputs = sc.transform(inputs)\n    X_test = []\n    for i in range(60, 81):\n        X_test.append(inputs[i-60:i, 0])\n    X_test = np.array(X_test)\n    X_test = np.reshape(X_test, (X_test.shape[0], \\\n                                 X_test.shape[1], 1))\n    predicted_stock_price = model.predict(X_test)\n    predicted_stock_price = \\\n    sc.inverse_transform(predicted_stock_price)\n    ```", "```py\n    # Visualizing the results\n    plt.plot(actual_stock_price, color = 'green', \\\n             label = 'Real Amazon Stock Price',ls='--')\n    plt.plot(predicted_stock_price, color = 'red', \\\n             label = 'Predicted Amazon Stock Price',ls='-')\n    plt.title('Predicted Stock Price')\n    plt.xlabel('Time in days')\n    plt.ylabel('Real Stock Price')\n    plt.legend()\n    plt.show()\n    ```", "```py\n    import numpy as np\n    import matplotlib.pyplot as plt\n    import pandas as pd\n    from tensorflow import random\n    ```", "```py\n    dataset_training = pd.read_csv('../AMZN_train.csv')\n    dataset_training.head()\n    ```", "```py\n    training_data = dataset_training[['Open']].values\n    training_data\n    ```", "```py\n    from sklearn.preprocessing import MinMaxScaler\n    sc = MinMaxScaler(feature_range = (0, 1))\n    training_data_scaled = sc.fit_transform(training_data)\n    training_data_scaled\n    ```", "```py\n    X_train = []\n    y_train = []\n    for i in range(60, 1258):\n        X_train.append(training_data_scaled[i-60:i, 0])\n        y_train.append(training_data_scaled[i, 0])\n    X_train, y_train = np.array(X_train), np.array(y_train)\n    ```", "```py\n    X_train = np.reshape(X_train, (X_train.shape[0], \\\n                                   X_train.shape[1], 1))\n    ```", "```py\n    from keras.models import Sequential\n    from keras.layers import Dense, LSTM, Dropout\n    ```", "```py\n    seed = 1\n    np.random.seed(seed)\n    random.set_seed(seed)\n    model = Sequential()\n    ```", "```py\n    model.add(LSTM(units = 100, return_sequences = True, \\\n                   input_shape = (X_train.shape[1], 1)))\n    # Adding a second LSTM layer\n    model.add(LSTM(units = 100, return_sequences = True))\n    # Adding a third LSTM layer\n    model.add(LSTM(units = 100, return_sequences = True))\n    # Adding a fourth LSTM layer\n    model.add(LSTM(units = 100))\n    # Adding the output layer\n    model.add(Dense(units = 1))\n    ```", "```py\n    # Compiling the RNN\n    model.compile(optimizer = 'adam', loss = 'mean_squared_error')\n    # Fitting the RNN to the Training set\n    model.fit(X_train, y_train, epochs = 100, batch_size = 32)\n    ```", "```py\n    dataset_testing = pd.read_csv('../AMZN_test.csv')\n    actual_stock_price = dataset_testing[['Open']].values\n    actual_stock_price\n    ```", "```py\n    total_data = pd.concat((dataset_training['Open'], \\\n                            dataset_testing['Open']), axis = 0)\n    ```", "```py\n    inputs = total_data[len(total_data) \\\n             - len(dataset_testing) - 60:].values\n    inputs = inputs.reshape(-1,1)\n    inputs = sc.transform(inputs)\n    X_test = []\n    for i in range(60, 81):\n        X_test.append(inputs[i-60:i, 0])\n    X_test = np.array(X_test)\n    X_test = np.reshape(X_test, (X_test.shape[0], \\\n                                 X_test.shape[1], 1))\n    predicted_stock_price = model.predict(X_test)\n    predicted_stock_price = \\\n    sc.inverse_transform(predicted_stock_price)\n    ```", "```py\n    plt.plot(actual_stock_price, color = 'green', \\\n             label = 'Actual Amazon Stock Price',ls='--')\n    plt.plot(predicted_stock_price, color = 'red', \\\n             label = 'Predicted Amazon Stock Price',ls='-')\n    plt.title('Predicted Stock Price')\n    plt.xlabel('Time in days')\n    plt.ylabel('Real Stock Price')\n    plt.legend()\n    plt.show()\n    ```"]