["```py\nlibrary(tidyverse)\nlibrary(lubridate)\nlibrary(xgboost)\nlibrary(Metrics) \nlibrary(DataExplorer)\nlibrary(caret)\nla_no2 <- readr::read_csv(\"data/LondonAir_TH_MER_NO2.csv\")\n```", "```py\nutils::str(la_no2)\n```", "```py\nla_no2 %>% dplyr::group_by(Units, Site, Species) %>% dplyr::summarise(count = n())\n```", "```py\nla_no2 <- la_no2 %>%\ndplyr::select(c(-Site,-Species,-Units)) %>%\n  dplyr::mutate(\n    Value = as.numeric(Value),\n    reading_date = lubridate::dmy_hm(ReadingDateTime),\n    reading_year = lubridate::year(reading_date),\n    reading_month = lubridate::month(reading_date),\n    reading_day = lubridate::day(reading_date),\n    reading_hour = lubridate::hour(reading_date),\n    reading_minute = lubridate::minute(reading_date)\n  ) %>%\n  dplyr::select(c(-ReadingDateTime, -reading_date)) \n```", "```py\nDataExplorer::plot_missing(la_no2)\n```", "```py\nla_no2 <- la_no2 %>% filter(!is.na(Value))\n```", "```py\nDataExplorer::plot_bar(la_no2)\n```", "```py\nla_no2 %>% dplyr::group_by(`Provisional or Ratified`) %>% dplyr::summarise(count = n())\n```", "```py\nla_no2 <- la_no2 %>%\n  dplyr::filter(\n    `Provisional or Ratified` == 'R'\n  )\n```", "```py\nla_no2 <- la_no2 %>%\n  dplyr::select(-`Provisional or Ratified`)\n```", "```py\nrange(la_no2$reading_year)\n```", "```py\nla_no2 <- la_no2 %>%\n  dplyr::select(-reading_year)\n```", "```py\nDataExplorer::plot_histogram(la_no2)\n```", "```py\nDataExplorer::plot_correlation(la_no2)\n```", "```py\nset.seed(1)\npartition <- sample(nrow(la_no2), 0.75*nrow(la_no2), replace=FALSE)\ntrain <- la_no2[partition,]\ntest <- la_no2[-partition,]\n\ntarget <- train$Value\n\ndtrain <- xgboost::xgb.DMatrix(data = as.matrix(train), label= target)\ndtest <- xgboost::xgb.DMatrix(data = as.matrix(test))\n```", "```py\nparams <-list(\n  objective = \"reg:linear\",\n  booster = \"gbtree\",\n  eval_metric = \"rmse\",\n  eta=0.1, \n  subsample=0.8,\n  colsample_bytree=0.75,\n  print_every_n = 10,\n  verbose = TRUE\n)\n```", "```py\nxgb <- xgboost::xgb.train( \n  params = params, \n  data = dtrain,\n  nrounds = 100\n)\n```", "```py\npred <- predict(xgb, dtest)\n```", "```py\nMetrics::rmse(test$Value,XGBpred) \n```", "```py\nxgb_cv <- xgboost::xgb.cv( \n  params = params, \n  data = dtrain, \n  nrounds = 10000, \n  nfold = 5, \n  showsd = T, \n  stratified = T,\n  print_every_n = 100,\n  early_stopping_rounds = 25, \n  maximize = F)\n```", "```py\nxgb_grid <- expand.grid(\n  nrounds = 500,\n  eta = 0.01,\n  max_depth = c(2,3,4),\n  gamma = c(0,0.5,1),\n  colsample_bytree = 0.75,\n  min_child_weight = c(1,3,5),\n  subsample = 0.8\n)\n```", "```py\nxgb_tc = caret::trainControl(\n  method = \"cv\",\n  number = 5,\n  search = \"grid\",\n  returnResamp = \"final\",\n  savePredictions = \"final\",\n  verboseIter = TRUE,\n  allowParallel = TRUE\n)\n```", "```py\nxgb_param_tune = caret::train(\n  x = dtrain,\n  y = target,\n  trControl = xgb_tc,\n  tuneGrid = xgb_grid,\n  method = \"xgbTree\",\n  verbose = TRUE\n)\n```", "```py\nparams <-list(\n  objective = \"reg:linear\",\n  booster = \"gbtree\",\n  eval_metric = \"rmse\",\n  eta=0.01, \n  subsample=0.8,\n  colsample_bytree=0.75,\n  max_depth = 4,\n  min_child_weight = 1,\n  gamma = 1\n)\nxgb <- xgboost::xgb.train( \n  params = params, \n  data = dtrain,\n  nrounds = 3162,\n  print_every_n = 10,\n  verbose = TRUE,\n  maximize = FALSE\n)\n\npred <- stats::predict(xgb, dtest)\nMetrics::rmse(test$Value,pred) \n```"]