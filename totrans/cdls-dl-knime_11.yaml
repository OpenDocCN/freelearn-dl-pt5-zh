- en: '*Chapter 9:* Convolutional Neural Networks for Image Classification'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapters, we talked about **Recurrent Neural Networks** (**RNNs**)
    and how they can be applied to different types of sequential data and use cases.
    In this chapter, we want to talk about another family of neural networks, called
    **Convolutional Neural Networks** (**CNNs**). CNNs are especially powerful when
    used on data with grid-like topology and spatial dependencies, such as images
    or videos.
  prefs: []
  type: TYPE_NORMAL
- en: We will start with a general introduction to CNNs, explaining the basic idea
    behind a convolution layer and introducing some related terminology such as padding,
    pooling, filters, and stride.
  prefs: []
  type: TYPE_NORMAL
- en: 'Afterward, we will build and train a CNN for image classification from scratch.
    We will cover all required steps: from reading and preprocessing of the images
    to defining, training, and applying the CNN.'
  prefs: []
  type: TYPE_NORMAL
- en: To train a neural network from scratch, a huge amount of labeled data is usually
    required. For some specific domains, such as images or videos, such a large amount
    of data might not be available, and the training of a network might become impossible.
    Transfer learning is a proposed solution to handle this problem. The idea behind
    transfer learning consists of using a state-of-the-art neural network trained
    for a task A as a starting point for another, related, task B.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Classifying Images with CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to Transfer Learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applying Transfer Learning for Cancer Type Prediction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs are commonly used in image processing and have been the winning models
    in several image-processing competitions. They are often used, for example, for
    image classification, object detection, and semantic segmentation.
  prefs: []
  type: TYPE_NORMAL
- en: 'Sometimes, CNNs are also used for non-image-related tasks, such as recommendation
    systems, videos, or time-series analysis. Indeed, CNNs are not only applied to
    two-dimensional data with a grid structure but can also work when applied to one-
    or three-dimensional data. In this chapter, however, we focus on the most common
    CNN application area: **image processing**.'
  prefs: []
  type: TYPE_NORMAL
- en: A CNN is a neural network with at least one **convolution layer**. As the name
    states, convolution layers perform a convolution mathematical transformation on
    the input data. Through such a mathematical transformation, convolution layers
    acquire the ability to detect and extract a number of features from an image,
    such as edges, corners, and shapes. Combinations of such extracted features are
    used to classify images or to detect specific objects within an image.
  prefs: []
  type: TYPE_NORMAL
- en: A convolution layer is often found together with a **pooling layer**, also commonly
    used in the feature extraction part of image processing.
  prefs: []
  type: TYPE_NORMAL
- en: The goal of this section is thus to explain how convolution layers and pooling
    layers work separately and together and to detail the different setting options
    for the two layers.
  prefs: []
  type: TYPE_NORMAL
- en: As mentioned, in this chapter we will focus on CNNs for image analysis. So,
    before we dive into the details of CNNs, let’s quickly review how images are stored.
  prefs: []
  type: TYPE_NORMAL
- en: How are Images Stored?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A grayscale image can be stored as a matrix, where each cell represents one
    pixel of the image and the cell value represents the gray level of the pixel.
    For example, a black and white image, with size ![](img/Formula_B16391_09_001.png)
    pixels, can be represented as a matrix with dimensions ![](img/Formula_B16391_09_002.png),
    where each value of the matrix ranges between ![](img/Formula_B16391_09_003.png)
    and ![](img/Formula_B16391_09_004.png). ![](img/Formula_B16391_09_005.png) is
    a black pixel, ![](img/Formula_B16391_09_006.png) is a white pixel, and a value
    in between corresponds to a level of gray in the grayscale.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.1* here depicts an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.1 – Matrix representation of a grayscale 5 x 5 image](img/B16391_09_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.1 – Matrix representation of a grayscale 5 x 5 image
  prefs: []
  type: TYPE_NORMAL
- en: 'As each pixel is represented by one gray value only, one **channel** (matrix)
    is sufficient to represent this image. For color images, on the other hand, more
    than one value is needed to define the color of each pixel. One option is to use
    the three values specifying the intensity of red, green, and blue to define the
    pixel color. In the following screenshot, to represent a color image, three channels
    are used instead of one: (*Figure 9.2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.2 – Representing a 28 x 28 color image using three channels for
    RGB](img/B16391_09_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – Representing a 28 x 28 color image using three channels for RGB
  prefs: []
  type: TYPE_NORMAL
- en: Moving from a grayscale image to a **red, green, and blue** (**RGB**) image,
    the more general concept of **tensor**—instead of a simple matrix—becomes necessary.
    In this way, the grayscale image can be described as a tensor of ![](img/Formula_B16391_09_007.png),
    while a color image with ![](img/Formula_B16391_09_008.png) pixels can be represented
    with a ![](img/Formula_B16391_09_009.png) tensor.
  prefs: []
  type: TYPE_NORMAL
- en: In general, a tensor representing an image with ![](img/Formula_B16391_09_010.png)
    pixels height, ![](img/Formula_B16391_09_011.png) pixels width, and ![](img/Formula_B16391_09_012.png)
    channels has the dimension ![](img/Formula_B16391_09_013.png) x ![](img/Formula_B16391_09_014.png)
    x ![](img/Formula_B16391_09_015.png).
  prefs: []
  type: TYPE_NORMAL
- en: But why do we need special networks to analyze images? Couldn’t we just **flatten**
    the image, represent each image as a long vector, and train a standard fully connected
    feedforward neural network?
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The process of transforming a matrix representation of an image into a vector
    is called **flattening**.
  prefs: []
  type: TYPE_NORMAL
- en: Why do we need CNNs?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For basic binary images, flattening and fully connected feedforward networks
    might yield acceptable performance. However, with more complex images, with strong
    pixel dependencies throughout the image, the combination of flattening and feedforward
    neural networks usually fails.
  prefs: []
  type: TYPE_NORMAL
- en: Indeed, the spatial dependency is lost when the image is flattened into a vector.
    As a result, fully connected feedforward networks are not translation-invariant.
    This means that they produce different results for shifted versions of the same
    image. For example, a network might learn to identify a cat in the upper-left
    corner of an image, but the same network is not able to detect a cat in the lower-right
    corner of the same image.
  prefs: []
  type: TYPE_NORMAL
- en: In addition, the flattening of an image produces a very long vector, and therefore
    it requires a very large fully connected feedforward network with many weights.
    For example, for a ![](img/Formula_B16391_09_016.png) pixel image with three channels,
    the network needs ![](img/Formula_B16391_09_017.png) inputs. If the next layer
    has ![](img/Formula_B16391_09_018.png) neurons, we would need to train ![](img/Formula_B16391_09_019.png)
    weights only in the first layer. You see that the number of weights can quickly
    become unmanageable, likely leading to overfitting during training.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution layers, which are the main building block of a CNN, allow us to
    solve this problem by exploiting the spatial properties of the image. So, let’s
    find out how a convolution layer works.
  prefs: []
  type: TYPE_NORMAL
- en: How does a Convolution Layer work?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The idea of CNNs is to use filters to detect patterns—also called features—such
    as corners, vertical edges, and horizontal edges, in different parts of an image.
  prefs: []
  type: TYPE_NORMAL
- en: For an image with one channel a **filter** is a small matrix, often of size
    ![](img/Formula_B16391_09_020.png) or ![](img/Formula_B16391_09_021.png), called
    a **kernel**. Different kernels—that is, matrices with different values—filter
    different patterns. A kernel moves across an image and performs a convolution
    operation. That convolution operation gives a name to the layer. The output of
    such a convolution is called a **feature map**.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: For an input image with three channels (for example, an input tensor with shape
    ![](img/Formula_B16391_09_022.png)), a kernel with kernel size 2 has the shape
    ![](img/Formula_B16391_09_023.png). This means the kernel can incorporate information
    from all channels but only within a small (2 x 2, in this example) region of the
    input image.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.3* here shows an example of how a convolution is calculated for an
    image of size ![](img/Formula_B16391_09_024.png) and a kernel with size ![](img/Formula_B16391_09_025.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.3 – Example of a convolution obtained by applying a 3 x 3 kernel
    to a 4 x 4 image](img/B16391_09_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – Example of a convolution obtained by applying a 3 x 3 kernel to
    a 4 x 4 image
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, we start by applying the kernel to the upper-left ![](img/Formula_B16391_09_026.png)
    region of the image. The image values are elementwise multiplied with the kernel
    values and then summed up, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_B16391_09_027.png)'
  prefs: []
  type: TYPE_IMG
- en: The result of this elementwise multiplication and sum is the first value, in
    the upper-left corner, in the output feature map. The kernel is then moved across
    the whole image to calculate all other values of the output feature map.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation is denoted with a * and is different from a matrix
    multiplication. Even though the layer is called convolution, most neural network
    libraries actually implement a related function called **cross-correlation**.
    To perform a correct convolution, according to its mathematical definition, the
    kernel in addition must be flipped. For CNNs this doesn’t make a difference because
    the weights are learned anyway.
  prefs: []
  type: TYPE_NORMAL
- en: In a convolution layer, a large number of filters (kernels) are trained in parallel
    on the input dataset and for the required task. That is, the weights in the kernel
    are not set manually but are adjusted automatically as weights during the network
    training procedure. During execution, all trained kernels are applied to calculate
    the feature map.
  prefs: []
  type: TYPE_NORMAL
- en: The dimension of the feature map is then a tensor of size ![](img/Formula_B16391_09_028.png).
    In the example in *Figure 9.3*, we applied only one kernel, and the dimension
    of the feature map is ![](img/Formula_B16391_09_029.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Historically, kernels were designed manually for selected tasks. For example,
    the kernel in *Figure 9.3* detects vertical lines. *Figure 9.4* here shows you
    the impact of some other handcrafted kernels:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Impact of some hand-crafted kernels on the original image](img/B16391_09_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Impact of some hand-crafted kernels on the original image
  prefs: []
  type: TYPE_NORMAL
- en: The convolution operation is just a part of the convolution layer. After that,
    a bias and a non-linear activation function are applied to each entry in the feature
    map. For example, we can add a bias value to each value in the feature map and
    then apply **rectified liner unit** (**ReLU**) as an activation function to set
    all values below the bias to 0.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: In [*Chapter 3*](B16391_03_Final_PG_ePUB.xhtml#_idTextAnchor073), *Getting Started
    with Neural Networks*, we introduced dense layers. In a dense layer, the weighted
    sum of the input is first calculated; then, a bias value is added to the sum,
    and the activation function is applied. In a convolutional layer, the weighted
    sum of the dense layer is replaced by the convolution.
  prefs: []
  type: TYPE_NORMAL
- en: 'A convolution layer has multiple setting options. We have already introduced
    three of them along the way, and they are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: The kernel size, which is often ![](img/Formula_B16391_09_030.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The number of filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The activation function, where ReLU is the one most commonly used
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are three more setting options: padding, stride, and dilation rate. Let’s
    continue with padding.'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Padding
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When we applied the filter in the example in *Figure 9.3*, the dimension of
    the feature map shrunk compared to the dimension of the input image. The input
    image had a size of ![](img/Formula_B16391_09_031.png) and the feature map a size
    of ![](img/Formula_B16391_09_032.png).
  prefs: []
  type: TYPE_NORMAL
- en: In addition, by looking at the feature map, we can see that pixels in the inner
    part of the input image (cells with values f, g, j, and k) are more often considered
    in the convolution than pixels at corners and borders. This implies that inner
    values will get a higher weight in further analysis. To overcome this issue, images
    can be zero-padded by adding zeros in additional external cells (*Figure 9.5*).
    This is a process called **padding**.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.5* here shows you an example of a zero-padded input:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Example of a zero-padded image](img/B16391_09_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.5 – Example of a zero-padded image
  prefs: []
  type: TYPE_NORMAL
- en: Here, two cells with value zero have been added to each row and column, all
    around the original image. If a kernel of size ![](img/Formula_B16391_09_033.png)
    is now applied to this padded image, the output dimension of the feature map would
    be the same as the dimension of the original image. The number of cells to use
    for zero padding is one more setting available in convolution layers.
  prefs: []
  type: TYPE_NORMAL
- en: Two other settings that influence the output size, if no padding is used, are
    called **stride** and **dilation rate**.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Stride and Dilation Rate
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the example in *Figure 9.3*, we applied the filter to every pixel. For images
    of a large size, it is not always necessary to perform the convolution on every
    single pixel. Instead of always shifting the kernel by one pixel, we could shift
    it by more than one horizontal or vertical pixel.
  prefs: []
  type: TYPE_NORMAL
- en: The number of pixels used for the kernel shift is called **stride**. The stride
    is normally defined by a tuple, specifying the number of cells for the shift in
    the horizontal and vertical direction. A higher stride value, without padding,
    leads to a downsampling of the input image.
  prefs: []
  type: TYPE_NORMAL
- en: The top part of *Figure 9.6* shows how a kernel of size 3 x 3 moves across an
    image with stride 2, 2.
  prefs: []
  type: TYPE_NORMAL
- en: Another setting option for a convolution layer is the **dilation rate**. The
    dilation rate indicates that only one cell out of ![](img/Formula_B16391_09_034.png)
    consecutive cells in the input image is used for the convolution operation. A
    dilation rate of ![](img/Formula_B16391_09_035.png) uses only one every two pixels
    from the input image for the convolution. A dilation rate of ![](img/Formula_B16391_09_036.png)
    uses one of three consecutive pixels. As for the stride, a dilation rate is a
    tuple of values for the horizontal and vertical direction. When using a dilation
    rate higher than ![](img/Formula_B16391_09_037.png), the kernel gets dilated to
    a larger field of view on the original image. So, a 3 x 3 kernel with dilation
    rate ![](img/Formula_B16391_09_038.png) explores a field of view of size 5 x 5
    in the input image, while using only nine convolution parameters.
  prefs: []
  type: TYPE_NORMAL
- en: 'For a ![](img/Formula_B16391_09_039.png) kernel and a dilation rate of ![](img/Formula_B16391_09_040.png),
    the kernel scans an area of ![](img/Formula_B16391_09_041.png) on the input image
    using only its corner values (see the lower part of *Figure 9.6*). This means
    for a dilation rate of ![](img/Formula_B16391_09_042.png), we have a gap of size
    1\. For a dilation rate of ![](img/Formula_B16391_09_043.png), we would have a
    gap size of 2, and so on:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Impact of different stride and dilation rate values on the output
    feature map](img/B16391_09_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.6 – Impact of different stride and dilation rate values on the output
    feature map
  prefs: []
  type: TYPE_NORMAL
- en: Another commonly used layer in CNNs is the pooling layer.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The idea of **pooling** is to replace an area of the feature map with summary
    statistics. For example, pooling can replace each ![](img/Formula_B16391_09_044.png)
    area of the feature map with its maximum value, called **max pooling**, or its
    average value, called **average pooling** (*Figure 9.7*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.7 – Results of max and average pooling](img/B16391_09_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.7 – Results of max and average pooling
  prefs: []
  type: TYPE_NORMAL
- en: A pooling layer reduces the dimension of the input image in a more efficient
    way and allows the extraction of dominant, rotational, and positional-invariant
    features.
  prefs: []
  type: TYPE_NORMAL
- en: As with a filter, in pooling we need to define the size of the explored area
    for which to calculate the summary statistics. A commonly used setting is a pooling
    size of ![](img/Formula_B16391_09_045.png) pixels and a stride of two pixels in
    each direction. This setting halves the image dimension.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers don’t have any weights, and all settings are defined during the
    configuration of the layer. They are static layers, and their parameters do not
    get trained like the other weights in the network.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling layers are normally used after one convolution layer or multiple-stacked
    convolution layers.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution layers can be applied to input images as well as to feature maps.
    Indeed, multiple convolution layers are often stacked on top of each other in
    a CNN. In such a hierarchy, the first convolution layer may extract low-level
    features, such as edges. The filters in the next layer then work on top of the
    extracted features and may learn to detect shapes, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The final extracted features can then be used for different tasks. In the case
    of image classification, the feature map—resulting from the stacking of multiple
    convolution layers—is flattened, and a classifier network is applied on top of
    it.
  prefs: []
  type: TYPE_NORMAL
- en: To summarize, a standard CNN for image classification first uses a series of
    convolution and pooling layers, then a flattened layer, and then a series of dense
    layers for the final classification.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we are familiar with convolutional layers and pooling layers, let’s
    see how they can be introduced inside a network for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: Classifying Images with CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will see how to build and train from scratch a CNN for image
    classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal is to classify handwritten digits between 0 and 9 with the data from
    the **MNIST database**, a large database of handwritten digits commonly used for
    training various image-processing applications. The MNIST database contains 60,000
    training images and 10,000 testing images of handwritten digits and can be downloaded
    from this website: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/).'
  prefs: []
  type: TYPE_NORMAL
- en: To read and preprocess images, KNIME Analytics Platform offers a set of dedicated
    nodes and components, available after installing the **KNIME Image Processing
    Extension**.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The KNIME Image Processing Extension ([https://www.knime.com/community/image-processing](https://www.knime.com/community/image-processing))
    allows you to read in more than 140 different format types of images (thanks to
    the Bio-Formats **Application Processing Interface** (**API**)). In addition,
    it can be used to apply well-known image-processing techniques such as segmentation,
    feature extraction, tracking, and classification, taking advantage of the graphical
    user interface within KNIME Analytics Platform.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the nodes operate on multi-dimensional image data (for example,
    videos, 3D images, multi-channel images, or even a combination of these), via
    the internal library `ImgLib2-API`. Several nodes calculate image features (for
    example, Zernike, texture, or histogram features) for segmented images (for example,
    a single cell). Machine learning algorithms are applied on the resulting feature
    vectors for the final classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'To apply and train neural networks on images, we need one further extension:
    the **KNIME Image Processing - Deep Learning Extension**. This extension introduces
    a number of useful image operations—for example, some conversions necessary for
    image data to feed the **Keras Network Learner** node.'
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: 'To train and apply neural networks on images, you need to install the following
    extensions:'
  prefs: []
  type: TYPE_NORMAL
- en: KNIME Image Processing ([https://www.knime.com/community/image-processing](https://www.knime.com/community/image-processing))
  prefs: []
  type: TYPE_NORMAL
- en: KNIME Image Processing – Deep Learning Extension ([https://hub.knime.com/bioml-konstanz/extensions/org.knime.knip.dl.feature/latest](https://hub.knime.com/bioml-konstanz/extensions/org.knime.knip.dl.feature/latest))
  prefs: []
  type: TYPE_NORMAL
- en: Let’s get started with reading and preprocessing the handwritten digits.
  prefs: []
  type: TYPE_NORMAL
- en: Reading and Preprocessing Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For this case study, we use a subset of the MNIST dataset: 10,000 image samples
    for training and 1,500 for testing. Each image has ![](img/Formula_B16391_09_046.png)
    pixels and only one channel. The training and testing images are saved in two
    different folders, with progressive numbers as filenames. In addition, we have
    a table with the image labels, sorted by the order of the image filenames.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The goal of the reading and preprocessing workflow is to read the images and
    to match them with their labels. Therefore, the following steps are implemented
    (also shown in *Figure 9.8*):'
  prefs: []
  type: TYPE_NORMAL
- en: Read and sort the images for training.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Import the digit labels for the training images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Match the labels with the images.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Transform the pixel type from unsigned byte to float.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Convert the labels into a collection cell.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These steps are performed by the workflow shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.8 – This workflow reads a subset of the MNIST dataset, adds the
    corresponding labels, and transforms the pixel type from unsigned byte to float](img/B16391_09_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.8 – This workflow reads a subset of the MNIST dataset, adds the corresponding
    labels, and transforms the pixel type from unsigned byte to float
  prefs: []
  type: TYPE_NORMAL
- en: 'To read the images, we use the **Image Reader (Table)** node. This node expects
    an input column with the **Uniform Resource Locator** (**URL**) paths to the image
    files. To create the sorted list of URLs, the **List Files** node first gets all
    paths to the image files in the training folder. Then, the **Sort images** metanode
    is used. *Figure 9.9* here shows you the inside of the metanode:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Inside of the Sort images metanode](img/B16391_09_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.9 – Inside of the Sort images metanode
  prefs: []
  type: TYPE_NORMAL
- en: The metanode extracts the image number from the filename with a **String Manipulation**
    node and sorts them with a **Sorter** node. The **Image Reader (Table)** node
    then reads the images.
  prefs: []
  type: TYPE_NORMAL
- en: The **File Reader** node, in the lower branch, reads the table with the image
    labels.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next step, the **Column Appender** node appends the correct label to
    each image. Since images have been sorted as to match their corresponding label,
    a simple appending operation is sufficient. *Figure 9.10* here shows a subset
    of the output of the **Column Appender** node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Output of the Column Appender node, with the digit image and
    the corresponding label](img/B16391_09_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.10 – Output of the Column Appender node, with the digit image and the
    corresponding label
  prefs: []
  type: TYPE_NORMAL
- en: Next, the **Image Calculator** node changes the pixel type from *unsigned byte*
    to *float*, by dividing each pixel value by 255.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the **Create Collection Column** node creates a collection cell for
    each label. The collection cell is required to create the one-hot vector-encoded
    classes, to use during training.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have read and preprocessed the training images, we can design the
    network structure.
  prefs: []
  type: TYPE_NORMAL
- en: Designing the Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this section, you will learn how to define a classical CNN for image classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'A classical CNN for image classification consists of two parts, which are trained
    together in an end-to-end fashion, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Feature Extraction**: The first part performs the feature extraction of the
    images, by training a number of filters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Classification**: The second part trains a classification network on the
    extracted features, available in the flattened feature map resulting from the
    feature extraction part.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We start with a simple network structure with only one convolution layer, followed
    by a pooling layer for the feature extraction part. The resulting feature maps
    are then flattened, and a simple classifier network, with just one hidden layer
    with the ReLU activation function, is trained on them.
  prefs: []
  type: TYPE_NORMAL
- en: 'The workflow here in *Figure 9.11* shows this network structure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – This workflow snippet builds a simple CNN for the classification
    of the MNIST dataset](img/B16391_09_011.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – This workflow snippet builds a simple CNN for the classification
    of the MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: The workflow starts with a **Keras Input Layer** node to define the input shape.
    The images of the MNIST dataset have ![](img/Formula_B16391_09_047.png) pixels
    and only one channel, as they are grayscale images. Thus, the input is a tensor
    of shape ![](img/Formula_B16391_09_048.png), and therefore the input shape is
    set to ![](img/Formula_B16391_09_049.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the convolutional layer is implemented with a **Keras Convolution 2D
    Layer** node. *Figure 9.12* here shows you the configuration window of the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Keras Convolution 2D Layer node and its configuration window](img/B16391_09_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.12 – Keras Convolution 2D Layer node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: The setting named **Filters** sets the number of filters to apply. This will
    be the last dimension of the feature map. In this example, we decided to train
    *32* filters.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you can set the **Kernel size** option in pixels—that is, an integer tuple
    defining the height and width of each kernel. For the MNIST dataset, we use a
    kernel size of ![](img/Formula_B16391_09_050.png). This means the setting is ![](img/Formula_B16391_09_051.png).50.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you can set the `dilation_rate` greater than 1.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you can select whether you want to use zero padding or not. The **Padding**
    option allows you to select between **Valid** and **Same**. **Valid** means no
    padding is performed. **Same** means zero padding is performed, such that the
    output dimension of the feature map is the same as the input dimension. As we
    have mainly black pixels on the border of the images, we decided not to zero-pad
    the images and selected **Valid**.
  prefs: []
  type: TYPE_NORMAL
- en: Next, you can select the **Dilation rate** option, as an integer tuple. Currently,
    specifying any dilation rate value greater than 1 is incompatible with specifying
    any stride value greater than 1\. A dilation rate of ![](img/Formula_B16391_09_052.png)
    means that no pixels are skipped. A dilation rate of ![](img/Formula_B16391_09_053.png)
    means every second pixel is used. This means a gap size of 1\. We use ![](img/Formula_B16391_09_054.png)
    for the dilation rate .52.
  prefs: []
  type: TYPE_NORMAL
- en: 'Last, the **Activation function** option must be selected. For this case study,
    we went for the most commonly used activation function for convolutional layers:
    **ReLU**.'
  prefs: []
  type: TYPE_NORMAL
- en: The output tensor of the convolutional layer (that is, our feature map) has
    the dimension ![](img/Formula_B16391_09_055.png), as we have ![](img/Formula_B16391_09_056.png)
    filters and we don’t use padding.
  prefs: []
  type: TYPE_NORMAL
- en: Next, a **Keras Max Pooling 2D Layer** node is used to apply max pooling on
    the two dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.13* here shows you the configuration window of the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Keras Max Pooling 2D Layer node and its configuration window](img/B16391_09_013.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.13 – Keras Max Pooling 2D Layer node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: In the configuration window of the **Keras Max Pooling 2D Layer** node, you
    can define the **Pool size**. Again, this is an integer tuple defining the pooling
    window. Remember, the idea of max pooling is to represent each area of the size
    of the pooling window with the maximum value in the area.
  prefs: []
  type: TYPE_NORMAL
- en: The **stride** is again an integer tuple, setting the step size to shift the
    pooling window.
  prefs: []
  type: TYPE_NORMAL
- en: Lastly, you can select whether to apply zero padding by selecting **Valid**
    for no padding, and **Same** to apply padding.
  prefs: []
  type: TYPE_NORMAL
- en: For this MNIST example, we set **Pool size** as ![](img/Formula_B16391_09_057.png),
    **Strides** as ![](img/Formula_B16391_09_058.png), and applied no padding. Therefore,
    the dimension of output of the pooling layer is ![](img/Formula_B16391_09_059.png).
  prefs: []
  type: TYPE_NORMAL
- en: Next, a **Keras Flatten Layer** node is used to transform the feature map into
    a vector, with dimension ![](img/Formula_B16391_09_060.png).
  prefs: []
  type: TYPE_NORMAL
- en: After the **Keras Flatten Layer** node, we build a simple classification network
    with one hidden layer and one output layer. The hidden layer with the ReLU activation
    function and 100 units is implemented by the first **Keras Dense Layer** node
    in *Figure 9.11*, while the output layer is implemented by the second (and last)
    **Keras Dense Layer** node in *Figure 9.11*. As it is a multiclass classification
    problem with 10 different classes, here the softmax activation function with 10
    units is used. In addition, the **Name prefix** *output* is used so that we can
    identify the output layer more easily when applying the network to new data.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have defined the network structure, we can move on to train the
    CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Training and Applying the Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To train the CNN built in the previous section, we again use the **Keras Network
    Learner** node. In the previous chapters, we already saw that this node offers
    many conversion types for input and target data (such as, for example, the **From
    Collection of Number (integers) to One-Hot Tensor** option). Installing the **KNIME
    Image Processing – Deep Learning Extension** adds one more conversion option:
    **From Image (Auto-mapping)**. This new conversion option allows us to select
    an image column from the input table and to automatically create the tensor to
    feed into the network.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.14* here shows the **Input Data** tab of the configuration window
    of the **Keras Network Learner** node, including this additional conversion option:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Input Data tab of the configuration window of the Keras Network
    Learner node with the additional conversion option, From Image (Auto-mapping)](img/B16391_09_014.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.14 – Input Data tab of the configuration window of the Keras Network
    Learner node with the additional conversion option, From Image (Auto-mapping)
  prefs: []
  type: TYPE_NORMAL
- en: In the **Target Data** tab, the conversion option from **From Collection of
    Number (integer) to One-Hot Tensor** is selected for the column with the collection
    cell of the image label.
  prefs: []
  type: TYPE_NORMAL
- en: On the bottom, the *Categorical cross entropy* activation function is selected,
    as the problem is a multiclass classification problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the **Options** tab, the following training parameters are set:'
  prefs: []
  type: TYPE_NORMAL
- en: '`10`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`200`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Adadelta with the default settings`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 9.15* here shows the progress of the training procedure in the **Learning
    Monitor** view of the **Keras Network Learner** node after node execution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.15 – The Learning Monitor view shows the training progress of the
    network](img/B16391_09_015.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.15 – The Learning Monitor view shows the training progress of the network
  prefs: []
  type: TYPE_NORMAL
- en: The **Learning Monitor** view shows the progress of the network during training
    over the many training batches. On the right-hand side, you can see the accuracy
    for the last few batches. **Current Value** shows you the accuracy for the last
    batch, which is in this case **0.995**.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a trained CNN satisfactorily performing on the training set,
    we can apply it to the test set. Here, the same reading and preprocessing steps
    as for the training set must also be applied on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: The **Keras Network Executor** node applies the trained network on the images
    in the test set. In the configuration window, the last layer, producing the probability
    distribution of the different digits, is selected as output.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, a bit of postprocessing is required in order to extract the final
    prediction from the network output.
  prefs: []
  type: TYPE_NORMAL
- en: Prediction Extraction and Model Evaluation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The output of the **Keras Network Executor** node is a table with 12 columns,
    comprising the following:'
  prefs: []
  type: TYPE_NORMAL
- en: The image column
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The true class value, named **Actual Value**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '10 columns with the probability values for the image classes with the column
    headers: `output/Softmax:0_x`, where `x` is a number between 0 and 9 encoding
    the class'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The goal of the postprocessing is to extract the class with the highest probability
    and then to evaluate the network performance. This is implemented by the workflow
    snippet shown here in *Figure 9.16*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.16 – This workflow snippet extracts the digit class with the highest
    probability and evaluates the network performance on the test set](img/B16391_09_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.16 – This workflow snippet extracts the digit class with the highest
    probability and evaluates the network performance on the test set
  prefs: []
  type: TYPE_NORMAL
- en: The **Many to One** node extracts the column header of the column with the highest
    probability in each row.
  prefs: []
  type: TYPE_NORMAL
- en: Then, the **Column Expression** node extracts the class from the column header.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: The **Column Expression** node is a very powerful node. It provides the possibility
    to append an arbitrary number of new columns or modify existing columns using
    expressions.
  prefs: []
  type: TYPE_NORMAL
- en: For each column to be appended or modified, a separate expression can be defined.
    These expressions can be simply created using predefined functions, similarly
    to the `=`).
  prefs: []
  type: TYPE_NORMAL
- en: Available flow variables and columns of the input table can be accessed via
    the provided access functions variable ("`variableName`") and column ("`columnName`").
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.17* here shows you the configuration window of the **Column Expression**
    node, with the expression used in the workflow snippet in *Figure 9.16* to extract
    the class information. In this case, the expression extracts the last character
    from the strings in the column named **Detected Digit**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17 – The Column Expression node and its configuration window](img/B16391_09_017.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.17 – The Column Expression node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: Next, the data type of the predicted class is converted from `String` to `Integer`
    with the **String to Number** node, and the network performance is evaluated on
    the test set with the **Scorer** node.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.18* here shows the view produced by the **Scorer** node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – View of the Scorer node, showing the performance of the network
    on the test set](img/B16391_09_018.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.18 – View of the Scorer node, showing the performance of the network
    on the test set
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, this simple CNN has already reached an accuracy of 94% and
    a Cohen’s kappa of 0.934 on the test set. The complete workflow is available on
    the KNIME Hub: [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%209/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%209/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we built and trained from scratch a simple CNN, reaching an
    acceptable performance for this rather simple image classification task. Of course,
    we could try to further improve the performance of this network by doing the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the number of training epochs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding a second convolutional layer together with a pooling layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using batch normalization for training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using augmentation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We leave this up to you, and continue with another way of network learning,
    called transfer learning.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to transfer learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The general idea of **transfer learning** is to reuse the knowledge gained by
    a network trained for task **A** on another related task **B**. For example, if
    we train a network to recognize sailing boats (task A), we can use this network
    as a starting point to train a new model to recognize motorboats (task B). In
    this case, task A is called the *source task* and task B the *target task*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Reusing a trained network as the starting point to train a new network is different
    from the traditional way of training networks, whereby neural networks are trained
    on their own for specific tasks on specific datasets. *Figure 9.19* here visualizes
    the traditional way of network training, whereby different systems are trained
    for different tasks and domains:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19 – Traditional way of training machine learning models and neural
    networks](img/B16391_09_019.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.19 – Traditional way of training machine learning models and neural
    networks
  prefs: []
  type: TYPE_NORMAL
- en: But why should we use transfer learning instead of training models in the traditional,
    isolated way?
  prefs: []
  type: TYPE_NORMAL
- en: Why use Transfer Learning?
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Current state-of-the-art neural networks have shown amazing performance in tackling
    specific complex tasks. Sometimes, these models are even better than humans, beating
    world champions at board games or at detecting objects in images. To train these
    successful networks, usually a huge amount of labeled data is required, as well
    as a vast amount of computational resources and time.
  prefs: []
  type: TYPE_NORMAL
- en: To get a comprehensive labeled dataset for a new domain, in order to be able
    to train a network to reach state-of-art-performance, can be difficult or even
    impossible. As an example, the often-used **ImageNet database**, which is used
    to train state-of-the-art models, has been developed over the course of many years.
    It would take time to create a similar new dataset for a new image domain. However,
    when these state-of-the-art models are applied to other related domains, they
    often suffer a considerable loss in performance, or, even worse, they break down.
    This happens due to the model bias toward the training data and domain.
  prefs: []
  type: TYPE_NORMAL
- en: Transfer learning allows us to use the knowledge gained during training on a
    task and domain where sufficient labeled data was available as a starting point,
    to train new models in domains where not enough labeled data is yet available.
    This approach has shown great results in many computer vision and **natural language
    processing** (**NLP**) tasks.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.20* here visualizes the idea behind transfer learning:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.20 – Idea behind transfer learning](img/B16391_09_020.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.20 – Idea behind transfer learning
  prefs: []
  type: TYPE_NORMAL
- en: Before we talk about how we can apply transfer learning when training a neural
    network, let’s have a quick look at the formal definition of transfer learning
    and the many scenarios in which it can be applied.
  prefs: []
  type: TYPE_NORMAL
- en: Formal Definition of Transfer Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A formal definition of transfer learning and of the related scenarios can be
    found in the paper by Sinno Jialin Pan and Qiang Yang, *A Survey on Transfer Learning*,
    IEEE Transactions on Knowledge and Data Engineering, 2009([https://ieeexplore.ieee.org/abstract/document/5288526](https://ieeexplore.ieee.org/abstract/document/5288526)).
  prefs: []
  type: TYPE_NORMAL
- en: The definition involves the concept of a **domain** and a **task**.
  prefs: []
  type: TYPE_NORMAL
- en: In the paper, a **domain** ![](img/Formula_B16391_09_061.png) is introduced
    as tuple {![](img/Formula_B16391_09_062.png)where ![](img/Formula_B16391_09_063.png)
    is the feature space and a ![](img/Formula_B16391_09_064.png) the marginal probability
    distribution for ![](img/Formula_B16391_09_065.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'For a given domain, ![](img/Formula_B16391_09_066.png), a task ![](img/Formula_B16391_09_067.png)
    consists of the following two components as well:'
  prefs: []
  type: TYPE_NORMAL
- en: A label space ![](img/Formula_B16391_09_068.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A predictive function ![](img/Formula_B16391_09_069.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, the predictive function ![](img/Formula_B16391_09_070.png) could be the
    conditional probability distribution ![](img/Formula_B16391_09_071.png) In general,
    the predictive function is a function trained on the labeled training data to
    predict the label ![](img/Formula_B16391_09_072.png) for any sample ![](img/Formula_B16391_09_073.png)
    in the feature space.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this terminology, **transfer learning** is defined by Sinno Jialin Pan
    and Qiang Yang in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '*"Given a source domain* ![](img/Formula_B16391_09_074.png) *and learning task*
    ![](img/Formula_B16391_09_075.png)*, a target domain* ![](img/Formula_B16391_09_076.png)
    *and learning task* ![](img/Formula_B16391_09_077.png) *, transfer learning aims
    to help improve the learning of the target predictive function* ![](img/Formula_B16391_09_078.png)
    *in* ![](img/Formula_B16391_09_079.png) *using the knowledge in* ![](img/Formula_B16391_09_080.png)
    *and* ![](img/Formula_B16391_09_081.png)*, where* ![](img/Formula_B16391_09_082.png)*,
    or* ![](img/Formula_B16391_09_083.png)."'
  prefs: []
  type: TYPE_NORMAL
- en: 'Sebastian Ruder uses this definition in his article, *Transfer Learning - Machine
    Learning’s Next Frontier*, *2017* ( [https://ruder.io/transfer-learning/](https://ruder.io/transfer-learning/))
    to describe the following *four scenarios* in which transfer learning can be used:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Different feature spaces: ![](img/Formula_B16391_09_084.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An example in the paper is cross-lingual adaptation, where we have documents
    in different languages.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Different marginal probabilities: ![](img/Formula_B16391_09_085.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An example comes in the form of documents that discuss different topics. This
    scenario is called *domain adaption*.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Different label spaces: ![](img/Formula_B16391_09_086.png)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: (for example, if we have documents with different labels).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Different conditional probabilities ![](img/Formula_B16391_09_087.png) This
    usually occurs together with scenario 3.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Now that we have a basic understanding of transfer learning, let’s find out
    next how transfer learning can be applied to the field of deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Transfer Learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a neural network, the knowledge gained during training is stored in the weights
    of the layers. For example, in the case of CNNs, a number of filters are trained
    to extract a number of features. Thus, the knowledge of how to extract such features
    from an image is stored in the weights of the kernels for the implemented filters.
  prefs: []
  type: TYPE_NORMAL
- en: In a stacked CNN for image classification, the initial convolution layers are
    responsible for extracting low-level features such as edges, while the next convolution
    layers extract higher-level features such as body parts, animals, or faces. The
    last layers are trained to classify the images, based on the extracted features.
  prefs: []
  type: TYPE_NORMAL
- en: So, if we want to train a CNN for a different image-classification task, on
    different images and with different labels, we must not train the new filters
    from scratch, but we can use the previously trained convolution layers in a state-of-the-art
    network as the starting point. Hopefully, the new training procedure will be faster
    and will require a smaller amount of data.
  prefs: []
  type: TYPE_NORMAL
- en: 'To use the trained layers from another network as the training starting point,
    we need to extract the convolution layers from the original network and then build
    some new layers on top. To do so, we have the following two options:'
  prefs: []
  type: TYPE_NORMAL
- en: We freeze the weights of the trained layers and just train the added layers
    based on the output of the frozen layers. This approach is often used in NLP applications,
    where trained embeddings are reused.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use the trained weights to initialize new convolution layers in the network
    and then fine-tune them while training the added layers. In this case, a small
    training rate is used to not unlearn the learned knowledge from the source task.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the last case study of this book, we want to train a neural network to predict
    cancer type from histopathology slide images. To speed up the learning process
    and considering the relatively small dataset we have, we will apply transfer learning
    starting from the convolution layers in the popular VGG16 network used here as
    the source network.
  prefs: []
  type: TYPE_NORMAL
- en: Applying Transfer Learning for Cancer Type Prediction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We will introduce here a new (and final) case study. We will start from the
    state-of-the-art VGG16 network as a source network to train a new target network
    on a dataset of images describing three different subtypes of lymphoma, which
    are **chronic lymphocytic leukemia** (**CLL**), **follicular lymphoma** (**FL**),
    and **mantle cell lymphoma** (**MCL**).
  prefs: []
  type: TYPE_NORMAL
- en: A typical task for a pathologist in a hospital is to look at histopathology
    slide images and make a decision about the type of lymphoma. Even for experienced
    pathologists this is a difficult task and, in many cases, follow-up tests are
    required to confirm the diagnosis. An assistive technology that can guide pathologists
    and speed up their job would be of great value.
  prefs: []
  type: TYPE_NORMAL
- en: VGG16 is one of the winner models on the ImageNet Challenge from 2014\. It is
    a stacked CNN network, using kernels of size ![](img/Formula_B16391_09_026.png)
    with an increasing depth—that is, with an increasing number of filters. The original
    network was trained on the ImageNet dataset, containing images ![](img/Formula_B16391_09_089.png),
    referring to more than 1,000 classes.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.21* shows you the network structure of the VGG16 model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'It starts with two convolution layers, each with 64 filters. After a max pooling
    layer, again two convolution layers are used, this time each with 128 filters.
    Then, another max pooling layer is followed by three convolution layers, each
    with 256 filters. After one more max pooling layer, there are again three convolution
    layers, each with 512 filters, followed by another pooling layer and three convolution
    layers each with 512 filters. After one last pooling layer, three dense layers
    are used:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.21 – Network structure of the VGG16 model](img/B16391_09_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.21 – Network structure of the VGG16 model
  prefs: []
  type: TYPE_NORMAL
- en: In this case study, we would like to reuse the trained convolution layers of
    the VGG16 model and add some layers on top for the cancer cell classification
    task. During training, the convolution layers will be frozen and only the added
    layers will be trained.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do so, we build three separate sub-workflows: one workflow to download the
    data, one workflow to preprocess the images, and a third workflow to train the
    neural network, using transfer learning. You can download the workflow with the
    three sub-workflows from the KNIME Hub: [https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%209/](https://hub.knime.com/kathrin/spaces/Codeless%20Deep%20Learning%20with%20KNIME/latest/Chapter%209/).
    Let’s start with the workflow to download the data.'
  prefs: []
  type: TYPE_NORMAL
- en: Downloading the Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The full dataset with images of cancer cells is available as a single `tar.gz`
    file containing 374 images: [https://ome.grc.nia.nih.gov/iicbu2008/lymphoma/index.html](https://ome.grc.nia.nih.gov/iicbu2008/lymphoma/index.html).
    The workflow shown in *Figure 9.22* downloads the file and creates a table with
    the file path and the class information for each image:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.22 – This workflow downloads the full labeled dataset of images
    of cancer cells](img/B16391_09_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.22 – This workflow downloads the full labeled dataset of images of
    cancer cells
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, the workflow first defines a directory for the downloaded data using
    the `tar.gz` file into the created directory. The `.table` file.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to preprocess the images.
  prefs: []
  type: TYPE_NORMAL
- en: Reading and Preprocessing the Images
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the next step, the table created by the workflow in *Figure 9.22* is read
    and the images are preprocessed. Each image has dimensions 1388px, 1040px, and
    three-color channels; this means ![](img/Formula_B16391_09_090.png). To reduce
    the spatial complexity of computation, we use a similar approach as that taken
    in the paper *Histology Image Classification Using Supervised Classification and
    Multimodal Fusion* ([https://ieeexplore.ieee.org/document/5693834](https://ieeexplore.ieee.org/document/5693834)),
    where each image is chopped into 25 blocks. For this use case, we decided to chop
    each image into blocks of size ![](img/Formula_B16391_09_091.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'The loading and preprocessing steps are performed by the workflow shown here
    in *Figure 9.23*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.23 – This workflow loads and preprocesses the image](img/B16391_09_023.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.23 – This workflow loads and preprocesses the image
  prefs: []
  type: TYPE_NORMAL
- en: This second workflow starts with reading the table created in the first workflow,
    including the paths to the images as well as the class information. Next, the
    **Category To Number** node encodes the different nominal class values (FL, MCL,
    and CLL) with an index, before the dataset is split into a training set and a
    test set using the **Partitioning** node. For this case study, we decided to use
    60% of the data for training and 40% of the data for testing, using stratified
    sampling on the **class** column.
  prefs: []
  type: TYPE_NORMAL
- en: In the **Load and preprocess images (Local Files)** component, the images are
    uploaded and preprocessed.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.24* here shows you the inside of this component:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.24 – Inside of the Load and preprocess images (Local Files) component](img/B16391_09_024.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.24 – Inside of the Load and preprocess images (Local Files) component
  prefs: []
  type: TYPE_NORMAL
- en: The component uses a loop to load and preprocess one image after the other.
    The **Chunk Loop Start** node, with one row per chunk, starts the loop, while
    the **Loop End** node, concatenating the resulting rows from the loop iterations,
    ends the loop.
  prefs: []
  type: TYPE_NORMAL
- en: In the loop body, one image is always loaded with the **Image Reader (Table)**
    node. The image is then normalized using the **Image Calculator** node, dividing
    each pixel value by 255.
  prefs: []
  type: TYPE_NORMAL
- en: Next, the **Image Cropper** node is used to crop the image to a size that is
    dividable by 64\. Since the original size of the images is 1388px 1040px, the
    first 44 pixels of the left side and the top 16 pixels of each image are cropped.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9.25* here shows you the configuration window of the node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.25 – Image Cropper node and its configuration window](img/B16391_09_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.25 – Image Cropper node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, the **Splitter** node splits each image into 336 images of size 64 x
    64 pixels, storing each new sub-image in a new column, for a total of ~75,000
    patches. *Figure 9.26* here shows you the **Advanced** tab of the configuration
    window of the **Splitter** node, where the maximum size for each dimension of
    the resulting images has been set:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.26 – The Splitter node and its configuration window](img/B16391_09_026.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.26 – The Splitter node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: Next, the table is transposed into one column and renamed, before the class
    information is added to each image with the **Cross Joiner** node.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the prepared images, we can continue with the last workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Training the Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first step of the training workflow is to define the network structure using
    *VGG16’s convolution layers* as a starting point.
  prefs: []
  type: TYPE_NORMAL
- en: The VGG16 model was originally trained to predict the classes in the ImageNet
    dataset. Despite the 1,000 classes in the dataset, none of them matches the three
    cancer types for this study. Therefore, we recycle only the trained convolution
    layers of the VGG16 network. We will then add some new neural layers on top for
    the classification task, and finally fine-tune the resulting network to our task.
  prefs: []
  type: TYPE_NORMAL
- en: 'To train the final network, we will use the **Keras Network Learner** node
    and the ~75,000 patches created from the training set images. These steps are
    performed by the workflow shown here in *Figure 9.27*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.27 – Training workflow to train the new network to classify images
    of cancer cells](img/B16391_09_027.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.27 – Training workflow to train the new network to classify images
    of cancer cells
  prefs: []
  type: TYPE_NORMAL
- en: The workflow first reads the VGG16 network with the `.h5` file with the complete
    network structure and weights, or networks are saved in `.json` or `.yaml` files
    with just the network structure.
  prefs: []
  type: TYPE_NORMAL
- en: In this case, we read the `.h5` file of the trained VGG16 network because we
    aim to use all of the knowledge embedded inside the network.
  prefs: []
  type: TYPE_NORMAL
- en: The output tensor of the VGG16 network has dimensions ![](img/Formula_B16391_09_092.png),
    which is the size of the output of the last max pooling layer. Before we can add
    some dense layers for the classification task, we flatten the output using the
    **Keras Flatten Layer** node.
  prefs: []
  type: TYPE_NORMAL
- en: Now, a dense layer with **ReLU** activation and 64 neurons is added using a
    **Keras Dense Layer** node. Next, a **Dropout Layer** node is introduced, with
    a dropout rate of ![](img/Formula_B16391_09_093.png) Finally, one last **Keras
    Dense Layer** node defines the output of the network. As we are dealing with a
    classification problem with three different classes, the **softmax** activation
    function with three units is adopted.
  prefs: []
  type: TYPE_NORMAL
- en: If we were to connect the output of the last **Keras Dense Layer** node to a
    **Keras Network Learner** node, we would fine-tune all layers, including the trained
    convolution layers from the VGG16 model. We do not want to lose all that knowledge!
    So, we decided to not fine-tune the layers of the VGG16 model but to train only
    the newly added layers. Therefore, the layers of the VGG16 model must be frozen.
  prefs: []
  type: TYPE_NORMAL
- en: 'To freeze layers of a network, we use the **Keras Freeze Layers** node. *Figure
    9.28* here shows you the configuration window of this node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.28 – The Keras Freeze Layers node and its configuration window](img/B16391_09_028.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.28 – The Keras Freeze Layers node and its configuration window
  prefs: []
  type: TYPE_NORMAL
- en: In the configuration window, you can select the layer(s) to freeze. Later on,
    when training the network, the weights of the selected layers will not be updated.
    All other layers will be trained. We froze every layer except the ones we added
    at the end of the VGG16 network.
  prefs: []
  type: TYPE_NORMAL
- en: In the lower branch of the workflow, we read the training data using the **Table
    Reader** node and we one-hot encode the class using the **One to Many** node.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have the training data and the network structure, we can fine-tune
    it with the **Keras Network Learner** node.
  prefs: []
  type: TYPE_NORMAL
- en: As with all other case studies in this book, the columns for the input data
    and target data are selected in the configuration window of the **Keras Network
    Learner** node, together with the required conversion type. In this case, the
    **From Image** conversion for the input column and from Number (double) for the
    target column have been selected. Because this is a multiclass classification
    task, the **Categorical cross entropy** loss function has been adopted. To fine-tune
    this network, it has been trained for 5 epochs using a training batch size of
    64 and RMSProp with the default settings as optimizer.
  prefs: []
  type: TYPE_NORMAL
- en: Once the network has been fine-tuned, we evaluate its performance on the test
    images. The preprocessed test images, as patches of 64 x 64 px, are read with
    a **Table Reader** node. To predict the class of an image, we generate predictions
    for each of the 64 x 64px patches using the **Keras Network Executor** node. Then,
    all predictions are combined using a simple majority voting scheme, implemented
    in the **Extract Prediction** metanode.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the network is evaluated using the **Scorer** node. The classifier
    has achieved 96% accuracy (fine-tuning for a few more epochs can push the accuracy
    to 98%).
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: In this use case, the VGG16 model is only used for feature extraction. Therefore,
    another approach is to apply the convolutional layers of the VGG16 model to extract
    the features beforehand and to feed them as input into a classic feedforward neural
    network. This has the advantage that the forward pass through VGG16 would be done
    only once per image, instead of doing it in every batch update.
  prefs: []
  type: TYPE_NORMAL
- en: We could now save the network and deploy it to allow a pathologist to access
    those predictions via a web browser, for example. How this can be done using KNIME
    Analytics Platform and KNIME Server is shown in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored CNNs, focusing on image data.
  prefs: []
  type: TYPE_NORMAL
- en: We started with an introduction to convolution layers, which motivates the name
    of this new family of neural networks. In this introduction, we explained why
    CNNs are so commonly used for image data, how convolutional networks work, and
    the impact of the many setting options. Next, we discussed pooling layers, commonly
    used in CNNs to efficiently downsample the data.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we put all this knowledge to work by building and training from scratch
    a CNN to classify images of digits between 0 and 9 from the MNIST dataset. Afterward,
    we discussed the concept of transfer learning, introduced four scenarios in which
    transfer learning can be applied, and showed how we can use transfer learning
    in the field of neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: In the last section, we applied transfer learning to train a CNN to classify
    histopathology slide images. Instead of training it from scratch, this time we
    reused the convolutional layers of a trained VGG16 model for the feature extraction
    of the images.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have covered the many different use cases, we will move on to the
    next step, which is the deployment of the trained neural networks. In the next
    chapter, you will learn about different deployment options with KNIME software.
  prefs: []
  type: TYPE_NORMAL
- en: Questions and Exercises
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What is the kernel size in a convolutional layer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) The area summarized by a statistical value
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) The size of the matrix moving across an image
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) The number of pixels to shift the matrix
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) The size of the area used by a layer
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: What is a pooling layer?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) A pooling layer is a commonly used layer in RNNs
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) A pooling layer summarizes an area with a statistical value
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) A pooling layer is a commonly used layer in feedforward networks
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) A pooling layer can be used to upsample images
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: When is transfer learning helpful?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: a) To transfer data to another system
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: b) If no model is available
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: c) If not enough labeled data is available
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: d) To compare different models
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
