<html><head></head><body>
		<div>
			<div id="_idContainer008" class="Content">
			</div>
		</div>
		<div id="_idContainer009" class="Content">
			<h1 id="_idParaDest-13"><a id="_idTextAnchor012"/>1. Introduction to Machine Learning with Keras</h1>
		</div>
		<div id="_idContainer046" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter introduces machine learning with Python. We will use real-life datasets to demonstrate the basics of machine learning, which include preprocessing data for machine learning models and building a classification model using the logistic regression model with scikit-learn. We will then advance our model-building skills by incorporating regularization into our models and evaluating their performance with model evaluation metrics. By the end of this chapter, you will be able to confidently create models to solve classification tasks using the scikit-learn library in Python and evaluate the performance of those models effectively.</p>
			<h1 id="_idParaDest-14"><a id="_idTextAnchor013"/>Introduction</h1>
			<p>Machine learning is the science of utilizing machines to emulate human tasks and to have the machine improve its performance of that task over time. By feeding machines data in the form of observations of real-world events, they can develop patterns and relationships that will optimize an objective function, such as the accuracy of a binary classification task or the error in a regression task. </p>
			<p>In general, the usefulness of machine learning is in the machine's ability to learn highly complex and non-linear relationships in large datasets and to replicate the results of that learning many times. One branch of machine learning algorithms has shown a lot of promise in learning highly complex and non-linear relationships associated with large, often unstructured datasets such as images, audio, and text data—<strong class="bold">Artificial Neural Networks</strong> (<strong class="bold">ANNs</strong>). ANNs, however, can be complicated to program, train, and evaluate, and this can be intimidating for beginners in the field. Keras is a Python library that presents a facile introduction to building, training, and evaluating ANNs that is incredibly useful to those studying machine learning.</p>
			<p>Take, for example, the classification of a dataset of pictures of either dogs or cats into classes of their respective type. For a human, this is simple, and the accuracy would likely be very high. However, it may take around a second to categorize each picture and scaling the task can only be achieved by increasing the number of humans, which may not be feasible. While it may be difficult, though certainly not impossible, for machines to reach the same level of accuracy as humans for this task, machines can classify many images per second, and scaling can be easily done by increasing the processing power of a single machine or making the algorithm more efficient:</p>
			<div>
				<div id="_idContainer010" class="IMG---Figure">
					<img src="image/B15777_01_01.jpg" alt="Figure 1.1: The classification of images as either dog or cat is a simple task for humans, but quite difficult for machines&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.1: The classification of images as either dog or cat is a simple task for humans, but quite difficult for machines</p>
			<p>While the trivial task of classifying dogs and cats may be simple for us humans, the same principles that are used to create a machine learning model that classifies dogs and cats can be applied to other classification tasks that humans may struggle with. An example of this is identifying tumors in Magnetic Resonance Images (MRIs). For humans, this task requires a medical professional with years of experience, whereas a machine may only need a dataset of labeled images. The following image shows MRI images of the brain, some of which include tumors:</p>
			<div>
				<div id="_idContainer011" class="IMG---Figure">
					<img src="image/B15777_01_02.jpg" alt="Figure 1.2: A non-trivial classification task for humans – MRIs of brains, some of which include the presence of tumors&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.2: A non-trivial classification task for humans – MRIs of brains, some of which include the presence of tumors</p>
			<h1 id="_idParaDest-15"><a id="_idTextAnchor014"/>Data Representation</h1>
			<p>We build models so that we can learn something about the data we are training on and about the relationships between the features of the dataset. This learning can inform us when we encounter new observations. However, we must realize that the observations we interact within the real world and the format of the data that's needed to train machine learning models are very different. Working with text data is a prime example of this. When we read the text, we are able to understand each word and apply the context that's given by each word in relation to the surrounding words—not a trivial task. However, machines are unable to interpret this contextual information. Unless it is specifically encoded, they have no idea how to convert text into something that can be a numerical input. Therefore, we must represent the data appropriately, often by converting non-numerical data types—for example, converting text, dates, and categorical variables into numerical ones.</p>
			<h2 id="_idParaDest-16"><a id="_idTextAnchor015"/>Tables of Data</h2>
			<p>Much of the data that's fed into machine learning problems is two-dimensional and can be represented as rows or columns. Images are a good example of a dataset that may be three-or even four-dimensional. The shape of each image will be two-dimensional (a height and a width), the number of images together will add a third dimension, and a color channel (red, green, and blue) will add a fourth:</p>
			<div>
				<div id="_idContainer012" class="IMG---Figure">
					<img src="image/B15777_01_03.jpg" alt="Figure 1.3: A color image and its representation as red, green, and blue images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.3: A color image and its representation as red, green, and blue images</p>
			<p>The following figure shows a few rows from a dataset that has been taken from the UCI repository, which documents the online session activity of various users of a shopping website. The columns of the dataset represent various attributes of the session activity and general attributes of the page, while the rows represent the various sessions, all corresponding to different users. The column named <strong class="source-inline">Revenue</strong> represents whether the user ended the session by purchasing products from the website.</p>
			<p class="callout-heading">Not<a id="_idTextAnchor016"/>e</p>
			<p class="callout">The dataset that documents the online session activity of various users of a shopping website can be found here: <a href="https://packt.live/39rdA7S">https://packt.live/39rdA7S</a>.</p>
			<p>One objective of analyzing the dataset could be to try and use the information given to predict whether a given user will purchase any products from the website. We can then check whether we were correct by comparing our predictions to the column named <strong class="source-inline">Revenue</strong>. The long-term benefit of this is that we could then use our model to identify important attributes of a session or web page that may predict purchase intent:</p>
			<div>
				<div id="_idContainer013" class="IMG---Figure">
					<img src="image/B15777_01_04.jpg" alt="Figure 1.4: An image showing the first 20 instances of the online shoppers purchasing intention dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.4: An image showing the first 20 instances of the online shoppers purchasing intention dataset</p>
			<h2 id="_idParaDest-17"><a id="_idTextAnchor017"/>Loading Data</h2>
			<p>Data can be in different forms and can be available in many places. Datasets for beginners are often given in a flat format, which means that they are two-dimensional, with rows and columns. Other common forms of data may include images, <strong class="source-inline">JSON</strong> objects, and text documents. Each type of data format has to be loaded in specific ways. For example, numerical data can be loaded into memory using the <strong class="source-inline">NumPy</strong> library, which is an efficient library for working with matrices in Python. </p>
			<p>However, we would not be able to load our marketing data <strong class="source-inline">.csv</strong> file into memory using the <strong class="source-inline">NumPy</strong> library because the dataset contains string values. For our dataset, we will use the <strong class="source-inline">pandas</strong> library because of its ability to easily work with various data types, such as strings, integers, floats, and binary values. In fact, <strong class="source-inline">pandas</strong> is dependent on <strong class="source-inline">NumPy</strong> for operations on numerical data types. <strong class="source-inline">pandas</strong> is also able to read JSON, Excel documents, and databases using SQL queries, which makes the library common among practitioners for loading and manipulating data in Python.</p>
			<p>Here is an example of how to load a CSV file using the <strong class="source-inline">NumPy</strong> library. We use the <strong class="source-inline">skiprows</strong> argument in case there is a header, which usually contains column names:</p>
			<p class="source-code">import numpy as np</p>
			<p class="source-code">data = np.loadtxt(filename, delimiter=",", skiprows=1)</p>
			<p>Here's an example of loading data using the <strong class="source-inline">pandas</strong> library:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">data = pd.read_csv(filename, delimiter=",")</p>
			<p>Here, we are loading in a <strong class="source-inline">.CSV</strong> file. The default delimiter is a comma, so passing this as an argument is not necessary but is useful to see. The pandas library can also handle non-numeric datatypes, which makes the library more flexible:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">data = pd.read_json(filename)</p>
			<p>The <strong class="source-inline">pandas</strong> library will flatten out the JSON and return a <strong class="source-inline">DataFrame</strong>.</p>
			<p>The library can even connect to a database, queries can be fed directly into the function, and the table that's returned will be loaded as a <strong class="source-inline">pandas</strong> DataFrame:</p>
			<p class="source-code">import pandas as pd</p>
			<p class="source-code">data = pd.read_sql(con, "SELECT * FROM table")</p>
			<p>We have to pass a database connection to the function in order for this to work. There is a myriad of ways for this to be achieved, depending on the database flavor.</p>
			<p>Other forms of data that are common in deep learning, such as images and text, can also be loaded in and will be discussed later in this course.</p>
			<p class="callout-heading">Note<a id="_idTextAnchor018"/></p>
			<p class="callout">You can find all the documentation for pandas here: <a href="https://pandas.pydata.org/pandas-docs/stable/">https://pandas.pydata.org/pandas-docs/stable/</a>. The documentation for NumPy can be found here: <a href="https://docs.scipy.org/doc/">https://docs.scipy.org/doc/</a>.</p>
			<h2 id="_idParaDest-18"><a id="_idTextAnchor019"/>Exercise 1.01: Loading a Dataset from the UCI Machine Learning Repository</h2>
			<p class="callout-heading">Note</p>
			<p class="callout">For all the exercises and activities in this chapter, you will need to have Python 3.7, Jupyter, and pandas installed on your system. Refer to the <em class="italic">Preface</em> for installation instructions. The exercises and activities are performed in Jupyter notebooks. It is recommended to keep a separate notebook for different assignments. You can download all the notebooks from this book's GitHub repository, which can be found here: <a href="https://packt.live/2OL5E9t">https://packt.live/2OL5E9t</a>.</p>
			<p>In this exercise, we will be loading the <strong class="source-inline">online shoppers purchasing intention</strong> dataset from the UCI Machine Learning Repository. The goal of this exercise will be to load in the CSV data and identify a target variable to predict and the feature variables to use to model the target variable. Finally, we will separate the feature and target columns and save them to <strong class="source-inline">.CSV</strong> files so that we can use them in subsequent activities and exercises.</p>
			<p>The dataset is related to the online behavior and activity of customers of an online store and indicates whether the user purchased any products from the website. You can find the dataset in the GitHub repository at: <a href="https://packt.live/39rdA7S">https://packt.live/39rdA7S</a>.</p>
			<p>Follow these steps to complete this exercise:</p>
			<ol>
				<li>Open a new Jupyter Notebook and load the data into memory using the pandas library with the <strong class="source-inline">read_csv</strong> function. Import the <strong class="source-inline">pandas</strong> library and read in the <strong class="source-inline">data</strong> file:<p class="source-code">import pandas as pd</p><p class="source-code">data = pd.read_csv('../data/online_shoppers_intention.csv')</p><p class="callout-heading">Note</p><p class="callout">The code above assumes that you are using the same folder and file structure as in the GitHub repository. If you get an error that the file cannot be found, then check to make sure your working directory is correctly structured. Alternatively, you can edit the file path in the code so that it points to the correct file location on your system, though you will need to ensure you are consistent with this when saving and loading files in later exercises.</p></li>
				<li>To verify that we have loaded the data into the memory correctly, we can print the first few rows. Let's print out the top <strong class="source-inline">20</strong> values of the variable:<p class="source-code">data.head(20)</p><p>The printed output should look like this:</p><div id="_idContainer014" class="IMG---Figure"><img src="image/B15777_01_05.jpg" alt="Figure 1.5: The first 20 rows and first 8 columns of the pandas DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 1.5: The first 20 rows and first 8 columns of the pandas DataFrame</p></li>
				<li>We can also print the <strong class="source-inline">shape</strong> of the <strong class="source-inline">DataFrame</strong>:<p class="source-code">data.shape</p><p>The printed output should look as follows, showing that the DataFrame has <strong class="source-inline">12330</strong> rows and <strong class="source-inline">18</strong> columns:</p><p class="source-code">(12330, 18)</p><p>We have successfully loaded the data into memory, so now we can manipulate and clean the data so that a model can be trained using this data. Remember that machine learning models require data to be represented as numerical data types in order to be trained. We can see from the first few rows of the dataset that some of the columns are string types, so we will have to convert them into numerical data types later in this chapter.</p></li>
				<li>We can see that there is a given output variable for the dataset, known as <strong class="source-inline">Revenue</strong>, which indicates whether or not the user purchased a product from the website. This seems like an appropriate target to predict, since the design of the website and the choice of the products featured may be based upon the user's behavior and whether they resulted in purchasing a particular product. Create <strong class="source-inline">feature</strong> and <strong class="source-inline">target</strong> datasets as follows, providing the <strong class="source-inline">axis=1</strong> argument:<p class="source-code">feats = data.drop('Revenue', axis=1)</p><p class="source-code">target = data['Revenue']</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">axis=1</strong> argument tells the function to drop columns rather than rows.</p></li>
				<li>To verify that the shapes of the datasets are as expected, print out the number of <strong class="source-inline">rows</strong> and <strong class="source-inline">columns</strong> of each:<p class="callout-heading">Note </p><p class="callout">The code snippet shown here uses a backslash ( <strong class="source-inline">\</strong> ) to split the logic across multiple lines. When the code is executed, Python will ignore the backslash, and treat the code on the next line as a direct continuation of the current line.</p><p class="source-code">print(f'Features table has {feats.shape[0]} \</p><p class="source-code">rows and {feats.shape[1]} columns')</p><p class="source-code">print(f'Target table has {target.shape[0]} rows')</p><p>This preceding code produces the following output:</p><p class="source-code">Features table has 12330 rows and 17 columns</p><p class="source-code">Target table has 12330 rows</p><p>We can see two important things here that we should always verify before continuing: first, the number of rows of the <strong class="source-inline">feature</strong> DataFrame and <strong class="source-inline">target</strong> DataFrame are the same. Here, we can see that both have <strong class="source-inline">12330</strong> rows. Second, the number of columns of the feature DataFrame should be one fewer than the total DataFrame, and the target DataFrame has exactly one column.</p><p>Regarding the second point, we have to verify that the target is not contained in the feature dataset; otherwise, the model will quickly find that this is the only column needed to minimize the total error, all the way down to zero. The target column doesn't necessarily have to be one column, but for binary classification, as in our case, it will be. Remember that these machine learning models are trying to minimize some cost function in which the <strong class="source-inline">target</strong> variable will be part of that cost function, usually a difference function between the predicted value and <strong class="source-inline">target</strong> variable.</p></li>
				<li>Finally, save the DataFrames as CSV files so that we can use them later:<p class="source-code">feats.to_csv('../data/OSI_feats.csv', index=False)</p><p class="source-code">target.to_csv('../data/OSI_target.csv', \</p><p class="source-code">              header='Revenue', index=False)</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">header='Revenue'</strong> parameter is used to provide a column name. We will do this to reduce confusion later on. The <strong class="source-inline">index=False</strong> parameter is used so that the index column is not saved.</p></li>
			</ol>
			<p>In this section, we have successfully demonstrated how to load data into Python using the <strong class="source-inline">pandas</strong> library. This will form the basis of loading data into memory for most tabular data. Images and large documents, both of which are other common forms of data for machine learning applications, have to be loaded in using other methods, all of which will be discussed later in this book.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YZRAyB">https://packt.live/2YZRAyB</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3dVR0pF">https://packt.live/3dVR0pF</a>.</p>
			<h1 id="_idParaDest-19"><a id="_idTextAnchor020"/>Data Preprocessing</h1>
			<p>To fit models to the data, it must be represented in numerical format since the mathematics used in all machine learning algorithms only works on matrices of numbers (you cannot perform linear algebra on an image). This will be one goal of this section: to learn how to encode all the features into numerical representations. For example, in binary text, values that contain one of two possible values may be represented as zeros or ones. An example of this can be seen in the following diagram. Since there are only two possible values, the value <strong class="source-inline">0</strong> is assumed to be a <strong class="source-inline">cat</strong> and the value <strong class="source-inline">1</strong> is assumed to be a <strong class="source-inline">dog</strong>. </p>
			<p>We can also rename the column for interpretation:</p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B15777_01_06.jpg" alt="Figure 1.6: A numerical encoding of binary text values&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.6: A numerical encoding of binary text values</p>
			<p>Another goal will be to appropriately represent the data in numerical format - by appropriately, we mean that we want to encode relevant information numerically through the distribution of numbers. For example, one method to encode the months of the year would be to use the number of the month in the year. For example, <strong class="source-inline">January</strong> would be encoded as <strong class="source-inline">1</strong>, since it is the first month, and <strong class="source-inline">December</strong> would be <strong class="source-inline">12</strong>. Here's an example of how this would look in practice:</p>
			<div>
				<div id="_idContainer016" class="IMG---Figure">
					<img src="image/B15777_01_07.jpg" alt="Figure 1.7: A numerical encoding of months&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.7: A numerical encoding of months</p>
			<p>Not encoding information appropriately into numerical features can lead to machine learning models learning unintuitive representations, as well as relationships between the <strong class="source-inline">feature</strong> data and <strong class="source-inline">target</strong> variables that will prove useless for human interpretation.</p>
			<p>An understanding of the machine learning algorithms you are looking to use will also help encode features into numerical representations appropriately. For example, algorithms for classification tasks such as Artificial Neural Networks (ANNs) and logistic regression are susceptible to large variations in the scale between the features that may hamper their model-fitting ability. </p>
			<p>Take, for example, a regression problem attempting to fit house attributes, such as the area in square feet and the number of bedrooms, to the house price. The bounds of the area may be anywhere from <strong class="source-inline">0</strong> to <strong class="source-inline">5000</strong>, whereas the number of bedrooms may only vary from <strong class="source-inline">0</strong> to <strong class="source-inline">6</strong>, so there is a large difference between the scale of the variables. </p>
			<p>An effective way to combat the large variation in scale between the features is to normalize the data. Normalizing the data will scale the data appropriately so that it is all of a similar magnitude. This ensures that any model coefficients or weights can be compared correctly. Algorithms such as decision trees are unaffected by data scaling, so this step can be omitted for models using tree-based algorithms.</p>
			<p>In this section, we demonstrated a number of different ways to encode information numerically. There is a myriad of alternative techniques that can be explored elsewhere. Here, we will show some simple and popular methods that can be used to tackle common data formats.</p>
			<h2 id="_idParaDest-20"><a id="_idTextAnchor021"/>Exercise 1.02: Cleaning the Data</h2>
			<p>It is important that we clean the data appropriately so that it can be used for training models. This often includes converting non-numerical datatypes into numerical datatypes. This will be the focus of this exercise - to convert all the columns in the feature dataset into numerical columns. To complete the exercise, perform the following steps:</p>
			<ol>
				<li value="1">First, we load the <strong class="source-inline">feature</strong> dataset into memory:<p class="source-code">%matplotlib inline</p><p class="source-code">import pandas as pd</p><p class="source-code">data = pd.read_csv('../data/OSI_feats.csv')</p></li>
				<li>Again, look at the first <strong class="source-inline">20</strong> rows to check out the data:<p class="source-code">data.head(20)</p><p>The following screenshot shows the output of the preceding code:</p><div id="_idContainer017" class="IMG---Figure"><img src="image/B15777_01_08.jpg" alt="Figure 1.8: First 20 rows and 8 columns of the pandas feature DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 1.8: First 20 rows and 8 columns of the pandas feature DataFrame</p><p>Here, we can see that there are a number of columns that need to be converted into the numerical format. The numerical columns we may not need to modify are the columns named <strong class="bold">Administrative</strong>, <strong class="bold">Administrative_Duration</strong>, <strong class="bold">Informational</strong>, <strong class="bold">Informational_Duration</strong>, <strong class="bold">ProductRelated</strong>, <strong class="bold">ProductRelated_Duration</strong>, <strong class="bold">BounceRates</strong>, <strong class="bold">ExitRates</strong>, <strong class="bold">PageValues</strong>, <strong class="bold">SpecialDay</strong>, <strong class="bold">OperatingSystems</strong>, <strong class="bold">Browser</strong>, <strong class="bold">Region</strong>, and <strong class="bold">TrafficType</strong>.</p><p>There is also a binary column that has either one of two possible values. This is the column named <strong class="bold">Weekend</strong>.</p><p>Finally, there are also categorical columns that are string types, but there are a limited number of choices (<strong class="source-inline">&gt;2</strong>) that the column can take. These are the columns named <strong class="bold">Month</strong> and <strong class="bold">VisitorType</strong>.</p></li>
				<li>For the numerical columns, use the <strong class="source-inline">describe</strong> function to get a quick indication of the bounds of the numerical columns:<p class="source-code">data.describe()</p><p>The following screenshot shows the output of the preceding code:</p><div id="_idContainer018" class="IMG---Figure"><img src="image/B15777_01_09.jpg" alt="Figure 1.9: Output of the describe function in the feature DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 1.9: Output of the describe function in the feature DataFrame</p></li>
				<li>Convert the binary column, <strong class="bold">Weekend</strong>, into a numerical column. To do so, we will examine the possible values by printing the count of each value and plotting the result, and then convert one of the values into <strong class="source-inline">1</strong> and the other into <strong class="source-inline">0</strong>. If appropriate, rename the column for interpretability.<p>For context, it is helpful to see the distribution of each value. We can do that using the <strong class="source-inline">value_counts</strong> function. We can try this out on the <strong class="source-inline">Weekend</strong> column:</p><p class="source-code">data['Weekend'].value_counts()</p><p>We can also look at these values as a bar graph by plotting the value counts by calling the <strong class="source-inline">plot</strong> method of the resulting DataFrame and passing the <strong class="source-inline">kind='bar'</strong> argument:</p><p class="source-code">data['Weekend'].value_counts().plot(kind='bar')</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">kind='bar'</strong> argument will plot the data as a bar graph. The default is a <strong class="source-inline">line graph</strong>. When plotting in Jupyter notebooks, in order to make the plots within the notebook, the following command may need to be run: <strong class="source-inline">%matplotlib inline</strong>.</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer019" class="IMG---Figure"><img src="image/B15777_01_10.jpg" alt="Figure 1.10: A plot of the distribution of values of the default column&#13;&#10;"/></div><p class="figure-caption">Figure 1.10: A plot of the distribution of values of the default column</p></li>
				<li>Here, we can see that this distribution is skewed toward <strong class="source-inline">false</strong> values. This column represents whether the visit to the website occurred on a weekend, corresponding to a <strong class="source-inline">true</strong> value, or a weekday, corresponding to a <strong class="source-inline">false</strong> value. Since there are more weekdays than weekends, this skewed distribution makes sense. Convert the column into a numerical value by converting the <strong class="source-inline">True</strong> values into <strong class="source-inline">1</strong> and the <strong class="source-inline">False</strong> values into <strong class="source-inline">0</strong>. We can also change the name of the column from its default to <strong class="source-inline">is_weekend</strong>. This makes it a bit more obvious what the column means:<p class="source-code">data['is_weekend'] = data['Weekend'].apply(lambda \</p><p class="source-code">                     row: 1 if row == True else 0)</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">apply</strong> function iterates through each element in the column and applies the function provided as the argument. A function has to be supplied as the argument. Here, a <strong class="source-inline">lambda</strong> function is supplied.</p></li>
				<li>Take a look at the original and converted columns side by side. Take a sample of the last few rows to see examples of both values being manipulated so that they're numerical data types:<p class="source-code">data[['Weekend','is_weekend']].tail()</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">tail</strong> function is identical to the <strong class="source-inline">head</strong> function, except the function returns the bottom <strong class="source-inline">n</strong> values of the DataFrame instead of the top <strong class="source-inline">n</strong>.</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer020" class="IMG---Figure"><img src="image/B15777_01_11.jpg" alt="Figure 1.11: The original and manipulated column&#13;&#10;"/></div><p class="figure-caption">Figure 1.11: The original and manipulated column</p><p>Here, we can see that <strong class="source-inline">True</strong> is converted into <strong class="source-inline">1</strong> and <strong class="source-inline">False</strong> is converted into <strong class="source-inline">0</strong>.</p></li>
				<li>Now we can drop the <strong class="source-inline">Weekend</strong> column, as only the <strong class="source-inline">is_weekend</strong> column is needed:<p class="source-code">data.drop('Weekend', axis=1, inplace=True)</p></li>
				<li>Next, we have to deal with categorical columns. We will approach the conversion of categorical columns into numerical values slightly differently than with binary text columns, but the concept will be the same. Convert each categorical column into a set of dummy columns. With dummy columns, each categorical column will be converted into <strong class="source-inline">n</strong> columns, where <strong class="source-inline">n</strong> is the number of unique values in the category. The columns will be <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong>, depending on the value of the categorical column.<p>This is achieved with the <strong class="source-inline">get_dummies</strong> function. If we need any help understanding this function, we can use the <strong class="source-inline">help</strong> function or any function:</p><p class="source-code">help(pd.get_dummies)</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer021" class="IMG---Figure"><img src="image/B15777_01_12.jpg" alt="Figure 1.12: The output of the help command being applied to the pd.get_dummies function&#13;&#10;"/></div><p class="figure-caption">Figure 1.12: The output of the help command being applied to the pd.get_dummies function</p></li>
				<li>Let's demonstrate how to manipulate categorical columns with the <strong class="source-inline">age</strong> column. Again, it is helpful to see the distribution of values, so look at the value counts and plot them:<p class="source-code">data['VisitorType'].value_counts()</p><p class="source-code">data['VisitorType'].value_counts().plot(kind='bar')</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer022" class="IMG---Figure"><img src="image/B15777_01_13.jpg" alt="Figure 1.13: A plot of the distribution of values of the age column&#13;&#10;"/></div><p class="figure-caption">Figure 1.13: A plot of the distribution of values of the age column</p></li>
				<li>Call the <strong class="source-inline">get_dummies</strong> function on the <strong class="source-inline">VisitorType</strong> column and take a look at the rows alongside the original:<p class="source-code">colname = 'VisitorType'</p><p class="source-code">visitor_type_dummies = pd.get_dummies(data[colname], \</p><p class="source-code">                                      prefix=colname)</p><p class="source-code">pd.concat([data[colname], \</p><p class="source-code">           visitor_type_dummies], axis=1).tail(n=10)</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer023" class="IMG---Figure"><img src="image/B15777_01_14.jpg" alt="Figure 1.14: Dummy columns from the VisitorType column&#13;&#10;"/></div><p class="figure-caption">Figure 1.14: Dummy columns from the VisitorType column</p><p>Here, we can see that, in each of the rows, there can be one value of <strong class="source-inline">1</strong>, which is in the column corresponding to the value in the <strong class="source-inline">VisitorType</strong> column.</p><p>In fact, when using dummy columns, there is some redundant information. Because we know there are three values, if two of the values in the dummy columns are <strong class="source-inline">0</strong> for a particular row, then the remaining column must be equal to <strong class="source-inline">1</strong>. It is important to eliminate any redundancy and correlations in features as it becomes difficult to determine which feature is the most important in minimizing the total error.</p></li>
				<li>To remove the interdependency, drop the <strong class="source-inline">VisitorType_Other</strong> column because it occurs with the lowest frequency:<p class="source-code">visitor_type_dummies.drop('VisitorType_Other', \</p><p class="source-code">                          axis=1, inplace=True)</p><p class="source-code">visitor_type_dummies.head()</p><p class="callout-heading">Note</p><p class="callout">In the <strong class="source-inline">drop</strong> function, the <strong class="source-inline">inplace</strong> argument will apply the function in place, so a new variable does not have to be declared. </p><p>Looking at the first few rows, we can see what remains of our dummy columns for the original <strong class="source-inline">VisitorType</strong> column:</p><div id="_idContainer024" class="IMG---Figure"><img src="image/B15777_01_15.jpg" alt="Figure 1.15: Final dummy columns from the VisitorType column&#13;&#10;"/></div><p class="figure-caption">Figure 1.15: Final dummy columns from the VisitorType column</p></li>
				<li>Finally, add these dummy columns to the original feature data by concatenating the two DataFrames column-wise and dropping the original column:<p class="source-code">data = pd.concat([data, visitor_type_dummies], axis=1)</p><p class="source-code">data.drop('VisitorType', axis=1, inplace=True) </p></li>
				<li>Repeat the exact same steps with the remaining categorical column, <strong class="source-inline">Month</strong>. First, examine the distribution of column values, which is an optional step. Second, create dummy columns. Third, drop one of the columns to remove redundancy. Fourth, concatenate the dummy columns into a feature dataset. Finally, drop the original column if it remains in the dataset. You can do this using the following code:<p class="source-code">colname = 'Month'</p><p class="source-code">month_dummies = pd.get_dummies(data[colname], prefix=colname)</p><p class="source-code">month_dummies.drop(colname+'_Feb', axis=1, inplace=True)</p><p class="source-code">data = pd.concat([data, month_dummies], axis=1)</p><p class="source-code">data.drop('Month', axis=1, inplace=True) </p></li>
				<li>Now, we should have our entire dataset as numerical columns. Check the types of each column to verify this:<p class="source-code">data.dtypes</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer025" class="IMG---Figure"><img src="image/B15777_01_16.jpg" alt="Figure 1.16: The datatypes of the processed feature dataset&#13;&#10;"/></div><p class="figure-caption">Figure 1.16: The datatypes of the processed feature dataset</p></li>
				<li>Now that we have verified the datatypes, we have a dataset we can use to train a model, so let's save this for later:<p class="source-code">data.to_csv('../data/OSI_feats_e2.csv', index=False)</p></li>
				<li>Let's do the same for the <strong class="source-inline">target</strong> variable. First, load the data in, convert the column into a numerical datatype, and save the column as a CSV file:<p class="source-code">target = pd.read_csv('../data/OSI_target.csv')</p><p class="source-code">target.head(n=10)</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer026" class="IMG---Figure"><img src="image/B15777_01_17.jpg" alt="Figure 1.17: First 10 rows of the target dataset&#13;&#10;"/></div><p class="figure-caption">Figure 1.17: First 10 rows of the target dataset</p><p>Here, we can see that this is a <strong class="source-inline">Boolean</strong> datatype and that there are two unique values.</p></li>
				<li>Convert this into a binary numerical column, much like we did with the binary columns in the feature dataset:<p class="source-code">target['Revenue'] = target['Revenue'].apply(lambda row: 1 \</p><p class="source-code">                    if row==True else 0)</p><p class="source-code">target.head(n=10)</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer027" class="IMG---Figure"><img src="image/B15777_01_18.jpg" alt="Figure 1.18: First 10 rows of the target dataset when converted into integers&#13;&#10;"/></div><p class="figure-caption">Figure 1.18: First 10 rows of the target dataset when converted into integers</p></li>
				<li>Finally, save the target dataset to a CSV file:<p class="source-code">target.to_csv('../data/OSI_target_e2.csv', index=False)</p></li>
			</ol>
			<p>In this exercise, we learned how to clean the data appropriately so that it can be used to train models. We converted the non-numerical datatypes into numerical datatypes; that is, we converted all the columns in the feature dataset into numerical columns. Lastly, we saved the target dataset as a CSV file so that we can use it in the following exercises and activities.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2YW1DVi">https://packt.live/2YW1DVi</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2BpO4EI">https://packt.live/2BpO4EI</a>.</p>
			<h2 id="_idParaDest-21"><a id="_idTextAnchor022"/>Appropriate Representation of the Data</h2>
			<p>In our online shoppers purchase intention dataset, we have some columns that are defined as numerical variables when, upon closer inspection, they are actually categorical variables that have been given numerical labels. These columns are <strong class="source-inline">OperatingSystems</strong>, <strong class="source-inline">Browser</strong>, <strong class="source-inline">TrafficType</strong>, and <strong class="source-inline">Region</strong>. Currently, we have treated them as numerical variables, though they are categorical, which should be encoded into the features if we want the models we build to learn the relationships between the features and the target. </p>
			<p>We do this because we may be encoding some misleading relationships in the features. For example, if the value of the <strong class="source-inline">OperatingSystems</strong> field is equal to <strong class="source-inline">2</strong>, does that mean it is twice the value as that which has the value <strong class="source-inline">1</strong>? Probably not, since it refers to the operating system. For this reason, we will convert the field into a categorical variable. The same may be applied to the <strong class="source-inline">Browser</strong>, <strong class="source-inline">TrafficType</strong>, and <strong class="source-inline">Region</strong> columns.</p>
			<h2 id="_idParaDest-22"><a id="_idTextAnchor023"/>Exercise 1.03: Appropriate Representation of the Data</h2>
			<p>In this exercise, we will convert the <strong class="source-inline">OperatingSystems</strong>, <strong class="source-inline">Browser</strong>, <strong class="source-inline">TrafficType</strong>, and <strong class="source-inline">Region</strong> columns into categorical types to accurately reflect the information. To do this, we will create dummy variables from the column in a similar manner to what we did in <em class="italic">Exercise 1.02</em>, <em class="italic">Cleaning the Data</em>. To do so, perform the following steps:</p>
			<ol>
				<li value="1">Open a Jupyter Notebook.</li>
				<li>Load the dataset into memory. We can use the same feature dataset that was the output from <em class="italic">Exercise 1.02</em>, <em class="italic">Cleaning the Data</em>, which contains the original numerical versions of the <strong class="source-inline">OperatingSystems</strong>, <strong class="source-inline">Browser</strong>, <strong class="source-inline">TrafficType</strong>, and <strong class="source-inline">Region</strong> columns:<p class="source-code">import pandas as pd</p><p class="source-code">data = pd.read_csv('../data/OSI_feats_e2.csv')</p></li>
				<li>Look at the distribution of values in the <strong class="source-inline">OperatingSystems</strong> column:<p class="source-code">data['OperatingSystems'].value_counts()</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer028" class="IMG---Figure"><img src="image/B15777_01_19.jpg" alt="Figure 1.19: The distribution of values in the OperatingSystems column&#13;&#10;"/></div><p class="figure-caption">Figure 1.19: The distribution of values in the OperatingSystems column</p></li>
				<li>Create dummy variables from the <strong class="source-inline">OperatingSystem</strong> column:<p class="source-code">colname = 'OperatingSystems'</p><p class="source-code">operation_system_dummies = pd.get_dummies(data[colname], \</p><p class="source-code">                           prefix=colname)</p></li>
				<li>Drop the dummy variable representing the value with the lowest occurring frequency and join back with the original data:<p class="source-code">operation_system_dummies.drop(colname+'_5', axis=1, \</p><p class="source-code">                              inplace=True)</p><p class="source-code">data = pd.concat([data, operation_system_dummies], axis=1)</p></li>
				<li>Repeat this for the <strong class="source-inline">Browser</strong> column:<p class="source-code">data['Browser'].value_counts()</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer029" class="IMG---Figure"><img src="image/B15777_01_20.jpg" alt="Figure 1.20: The distribution of values in the Browser column&#13;&#10;"/></div><p class="figure-caption">Figure 1.20: The distribution of values in the Browser column</p></li>
				<li>Create dummy variables, drop the dummy variable with the lowest occurring frequency, and join back with the original data:<p class="source-code">colname = 'Browser'</p><p class="source-code">browser_dummies = pd.get_dummies(data[colname], \</p><p class="source-code">                  prefix=colname)</p><p class="source-code">browser_dummies.drop(colname+'_9', axis=1, inplace=True)</p><p class="source-code">data = pd.concat([data, browser_dummies], axis=1)</p></li>
				<li>Repeat this for the <strong class="source-inline">TrafficType</strong> and <strong class="source-inline">Region</strong> columns:<p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">#</strong> symbol in the code snippet below denotes a code comment. Comments are added into code to help explain specific bits of logic. </p><p class="source-code">colname = 'TrafficType'</p><p class="source-code">data[colname].value_counts()</p><p class="source-code">traffic_dummies = pd.get_dummies(data[colname], prefix=colname)</p><p class="source-code"># value 17 occurs with lowest frequency</p><p class="source-code">traffic_dummies.drop(colname+'_17', axis=1, inplace=True)</p><p class="source-code">data = pd.concat([data, traffic_dummies], axis=1)</p><p class="source-code">colname = 'Region'</p><p class="source-code">data[colname].value_counts()</p><p class="source-code">region_dummies = pd.get_dummies(data[colname], \</p><p class="source-code">                 prefix=colname)</p><p class="source-code"># value 5 occurs with lowest frequency</p><p class="source-code">region_dummies.drop(colname+'_5', axis=1, inplace=True)</p><p class="source-code">data = pd.concat([data, region_dummies], axis=1)</p></li>
				<li>Check the column types to verify they are all numerical:<p class="source-code">data.dtypes</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer030" class="IMG---Figure"><img src="image/B15777_01_21.jpg" alt="Figure 1.21: The datatypes of the processed feature dataset&#13;&#10;"/></div><p class="figure-caption">Figure 1.21: The datatypes of the processed feature dataset</p></li>
				<li>Finally, save the dataset to a CSV file for later use:<p class="source-code">data.to_csv('../data/OSI_feats_e3.csv', index=False)</p></li>
			</ol>
			<p>Now, we can accurately test whether the browser type, operating system, traffic type, or region will affect the target variable. This exercise has demonstrated how to appropriately represent data for use in machine learning algorithms. We have presented some techniques that we can use to convert data into numerical datatypes that cover many situations that may be encountered when working with tabular data.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3dXOTBy">https://packt.live/3dXOTBy</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/3iBvDxw">https://packt.live/3iBvDxw</a>.</p>
			<h1 id="_idParaDest-23"><a id="_idTextAnchor024"/>Life Cycle of Model Creation</h1>
			<p>In this section, we will cover the life cycle of creating performant machine learning models, from engineering features to fitting models to training data, and evaluating our models using various metrics. The following diagram demonstrates the iterative process of building machine learning models. Features are engineered that represent potential correlations between the features and the target, the model is fit, and then models are evaluated. </p>
			<p>Depending on how the model is scored according to the model's evaluation metrics, the features are engineered further, and the process continues. Many of the steps that are implemented to create models are highly transferable between all machine learning libraries. We'll start with scikit-learn, which has the advantage of being widely used, and as such, there is a lot of documentation, tutorials, and learning materials to be found across the internet:</p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/B15777_01_22.jpg" alt="Figure 1.22: The life cycle of model creation&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.22: The life cycle of model creation</p>
			<h2 id="_idParaDest-24"><a id="_idTextAnchor025"/>Machine Learning Libraries</h2>
			<p>While this book is an introduction to deep learning with Keras, as we mentioned earlier, we will start by utilizing scikit-learn. This will help us establish the fundamentals of building a machine learning model using the Python programming language.</p>
			<p>Similar to scikit-learn, Keras makes it easy to create models in the Python programming language through an easy-to-use API. However, the goal of Keras is the creation and training of neural networks, rather than machine learning models in general. ANNs represent a large class of machine learning algorithms, and they are so-called because their architecture resembles the neurons in the human brain. The Keras library has many general-purpose functions built-in, such as <strong class="source-inline">optimizers</strong>, <strong class="source-inline">activation functions</strong>, and <strong class="source-inline">layer properties</strong>, so that users, like in scikit-learn, do not have to code these algorithms from scratch.</p>
			<h1 id="_idParaDest-25"><a id="_idTextAnchor026"/>scikit-learn</h1>
			<p>Scikit-learn was initially created by David Cournapeau in 2007 as a way to easily create machine learning models in the Python programming language. Since its inception, the library has grown immensely in popularity because of its ease of use, wide adoption within the machine learning community, and flexibility of use. scikit-learn is usually the first machine learning package that's implemented by practitioners using Python because of the large number of algorithms available for <strong class="source-inline">classification</strong>, <strong class="source-inline">regression</strong>, and <strong class="source-inline">clustering</strong> tasks and the speed with which results can be obtained.</p>
			<p>For example, scikit-learn's <strong class="source-inline">LinearRegression</strong> class is an excellent choice if you wish to quickly train a simple regression model, whereas if a more complex algorithm is required that's capable of learning nonlinear relationships, scikit-learn's <strong class="source-inline">GradientBoostingRegressor</strong> or any one of the <strong class="source-inline">support vector machine</strong> algorithms are great choices. Likewise, with classification or clustering tasks, scikit-learn offers a wide variety of algorithms to choose from.</p>
			<p>The following are a few of the advantages and disadvantages of using scikit-learn for machine learning purposes.</p>
			<p>The advantages of scikit-learn are as follows:</p>
			<ul>
				<li><strong class="bold">Mature</strong>: Scikit-learn is well-established within the community and used by members of the community of all skill levels. The package includes most of the common machine learning algorithms for classification, regression, and clustering tasks.</li>
				<li><strong class="bold">User-friendly</strong>: Scikit-learn features an easy-to-use API that allows beginners to efficiently prototype without having to have a deep understanding or having to code each specific mode.</li>
				<li><strong class="bold">Open source</strong>: There is an active open source community working to improve the library, add documentation, and release regular updates, which ensures that the package is stable and up to date.</li>
			</ul>
			<p>The disadvantage of scikit-learn is as follows:</p>
			<p><strong class="bold">Neural network support is lacking</strong>: Estimators with ANN algorithms are minimal.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find all the documentation for the scikit-learn library here: <a href="https://scikit-learn.org/stable/documentation.html">https://scikit-learn.org/stable/documentation.html</a>.</p>
			<p>The estimators in scikit-learn can generally be classified into supervised learning and unsupervised learning techniques. Supervised learning occurs when a <strong class="source-inline">target</strong> variable is present. A <strong class="source-inline">target</strong> variable is a variable of the dataset that you are trying to predict, given the other variables. <strong class="source-inline">Supervised learning</strong> requires the target variable to be known and models are trained to correctly predict this variable. <strong class="source-inline">Binary classification</strong> using <strong class="source-inline">logistic regression</strong> is a good example of a supervised learning technique.</p>
			<p>In <strong class="source-inline">unsupervised learning</strong>, no target variable is given in the training data, but models aim to assign a target variable. An example of an unsupervised learning technique is k-means clustering. This algorithm partitions data into a given number of clusters based on its proximity to neighboring data points. The <strong class="source-inline">target</strong> variable that's assigned may be either the cluster number or cluster center.</p>
			<p>An example of utilizing a clustering example in practice may look as follows. Imagine that you are a jacket manufacturer and your goal is to develop dimensions for various jacket sizes. You cannot create a custom-fit jacket for each customer, so one option you have to determine the dimensions for jackets is to sample the population of customers for various parameters that may be correlated to fit, such as height and weight. Then, you can group the population into clusters using scikit-learn's <strong class="source-inline">k-means clustering</strong> algorithm with a cluster number that matches the number of jacket sizes you wish to produce. The cluster-centers that are created from the clustering algorithm become the parameters that the jacket sizes are based on. </p>
			<p>This is visualized in the following figure:</p>
			<div>
				<div id="_idContainer032" class="IMG---Figure">
					<img src="image/B15777_01_23.jpg" alt="Figure 1.23: An unsupervised learning example of grouping customer parameters into clusters&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.23: An unsupervised learning example of grouping customer parameters into clusters</p>
			<p>There are even <strong class="source-inline">semi-supervised learning</strong> techniques in which unlabeled data is used in the training of machine learning models. This technique may be used if there is only a small amount of labeled data and a copious amount of unlabeled data. In practice, semi-supervised learning produces a significant improvement in model performance compared to unsupervised learning.</p>
			<p>The scikit-learn library is ideal for beginners as the general concepts for building machine learning pipelines can be learned easily. Concepts such as data preprocessing (the preparation of data for use in machine learning models), hyperparameter tuning (the process of selecting the appropriate model parameters), model evaluation (the quantitative evaluation of a model's performance), and many more are all included in the library. Even experienced users find the library easy to use in order to rapidly prototype models before using a more specialized machine learning library.</p>
			<p>Indeed, the various machine learning techniques we've discussed, such as supervised and unsupervised learning, can be applied with Keras using neural networks with different architectures, all of which will be discussed throughout this book.</p>
			<h1 id="_idParaDest-26"><a id="_idTextAnchor027"/>Keras</h1>
			<p>Keras is designed to be a high-level neural network API that is built on top of frameworks such as TensorFlow, CNTK, and Theano. One of the great benefits of using Keras as an introduction to deep learning for beginners is that it is very user-friendly; advanced functions such as optimizers and layers are already built into the library and do not have to be written from scratch. This is why Keras is popular not only among beginners but also seasoned experts. Also, the library allows the rapid prototyping of neural networks, supports a wide variety of network architectures, and can be run on both CPUs and GPUs.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can find the library and all the documentation for Keras here: <a href="https://Keras.io/">https://Keras.io/</a>.</p>
			<p>Keras is used to create and train neural networks and does not offer much in terms of other machine learning algorithms, including supervised algorithms such as support vector machines and unsupervised algorithms such as <strong class="source-inline">k-means clustering</strong>. What Keras does offer, though, is a well-designed API that can be used to create and train neural networks, which takes away much of the effort that's required to apply linear algebra and multivariate calculus accurately.</p>
			<p>The specific modules that are available from the Keras library, such as <strong class="source-inline">neural layers</strong>, <strong class="source-inline">cost functions</strong>, <strong class="source-inline">optimizers</strong>, <strong class="source-inline">initialization schemes</strong>, <strong class="source-inline">activation functions</strong>, and <strong class="source-inline">regularization schemes</strong>, will be explained thoroughly throughout this book. All these modules have relevant functions that can be used to optimize performance for training neural networks for specific tasks.</p>
			<h2 id="_idParaDest-27"><a id="_idTextAnchor028"/>Advantages of Keras</h2>
			<p>Here are a few of the main advantages of using Keras for machine learning purposes:</p>
			<ul>
				<li><strong class="bold">User-friendly</strong>: Much like scikit-learn, Keras features an easy-to-use API that allows users to focus on model-building rather than the specifics of the algorithms.</li>
				<li><strong class="bold">Modular</strong>: The API consists of fully configurable modules that can all be plugged together and work seamlessly.</li>
				<li><strong class="bold">Extensible</strong>: It is relatively simple to add new modules to the library. This allows users to take advantage of the many robust modules within the library while providing them the flexibility to create their own.</li>
				<li><strong class="bold">Open source</strong>: Keras is an open source library and is constantly improving and adding modules to its code base thanks to the work of many collaborators working in conjunction to build improvements and help create a robust library for all.</li>
				<li><strong class="bold">Works with Python</strong>: Keras models are declared directly in Python rather than in separate configuration files, which allows Keras to take advantage of working with Python, such as ease of debugging and extensibility.</li>
			</ul>
			<h2 id="_idParaDest-28"><a id="_idTextAnchor029"/>Disadvantages of Keras</h2>
			<p>Here are a few of the main disadvantages of using Keras for machine learning purposes:</p>
			<ul>
				<li><strong class="bold">Advanced customization</strong>: While simple surface-level customization such as creating simple custom loss functions or neural layers is facile, it can be difficult to change how the underlying architecture works.</li>
				<li><strong class="bold">Lack of examples</strong>: Beginners often rely on examples to kick-start their learning. Advanced examples can be lacking in the Keras documentation, which can prevent beginners from advancing in their learning.</li>
			</ul>
			<p>Keras offers those familiar with the Python programming language and machine learning the ability to create neural network architectures easily. Since neural networks are quite complicated, we will use scikit-learn to introduce many machine learning concepts before applying them to the Keras library.</p>
			<h2 id="_idParaDest-29"><a id="_idTextAnchor030"/>More Than Building Models</h2>
			<p>While machine learning libraries such as scikit-learn and Keras were created to help build and train predictive models, their practicality extends much further. One common use case of building models is that they can be utilized to perform predictions on new data. Once a model has been trained, new observations can be fed into the model to generate predictions. Models may even be used as intermediate steps. For example, neural network models can be used as <strong class="source-inline">feature extractors</strong>, classifying objects in an image that can then be fed into a subsequent model, as illustrated in the following image:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/B15777_01_24.jpg" alt="Figure 1.24: Classifying objects using deep learning&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.24: Classifying objects using deep learning</p>
			<p>Another common use case for models is that they can be used to summarize datasets by learning representations of the data. Such models are known as auto-encoders, a type of neural network architecture that can be used to learn such representations of a given dataset. Therefore, the dataset can thus be represented in a reduced dimension with minimal loss of information:</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/B15777_01_25.jpg" alt="Figure 1.25: An example of using deep learning for text summarization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.25: An example of using deep learning for text summarization</p>
			<h1 id="_idParaDest-30"><a id="_idTextAnchor031"/>Model Training</h1>
			<p>In this section, we will begin fitting our model to the datasets that we have created. In this chapter, we will review the minimum steps that are required to create a machine learning model that can be applied when building models with any machine learning library, including scikit-learn and Keras.</p>
			<h2 id="_idParaDest-31"><a id="_idTextAnchor032"/>Classifiers and Regression Models</h2>
			<p>This book is concerned with applications of deep learning. The vast majority of deep learning tasks are supervised learning, in which there is a given target, and we want to fit a model so that we can understand the relationship between the features and the target.</p>
			<p>An example of supervised learning is identifying whether a picture contains a <strong class="source-inline">dog</strong> or a <strong class="source-inline">cat</strong>. We want to determine the relationship between the <strong class="source-inline">input</strong> (a matrix of pixel values) and the <strong class="source-inline">target</strong> variable, that is, whether the image is of a <strong class="source-inline">dog</strong> or a <strong class="source-inline">cat</strong>:</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/B15777_01_26.jpg" alt="Figure 1.26: A simple supervised learning task to classify images as dogs and cats&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.26: A simple supervised learning task to classify images as dogs and cats</p>
			<p>Of course, we may need many more images in our training dataset to robustly classify new images, but models that are trained on such a dataset are able to identify the various relationships that differentiate cats and dogs, which can then be used to predict labels for new data.</p>
			<p><strong class="bold">Supervised learning models</strong> are generally used for either classification or regression tasks.</p>
			<h2 id="_idParaDest-32"><a id="_idTextAnchor033"/>Classification Tasks</h2>
			<p>The goal of classification tasks is to fit models from data with discrete categories that can be used to label <strong class="source-inline">unlabeled data</strong>. For example, these types of models can be used to classify images as dogs or cats. But it doesn't stop at binary classification; multi-label classification is also possible. Another example of how this may be a <strong class="source-inline">classification</strong> task would be to predict the existence of dogs within the images. A positive prediction would indicate the presence of dogs within the images, while a negative prediction would indicate no presence of dogs. Note that this could easily be converted into a <strong class="source-inline">regression</strong> task, that is, the estimation of a continuous variable as opposed to a discrete variable, which classification tasks estimate, by predicting the number of dogs within the images.</p>
			<p>Most classification tasks output a probability for each unique class. This prediction is determined as the class with the highest probability, as can be seen in the following figure: </p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/B15777_01_27.jpg" alt="Figure 1.27: An illustration of a classification model labeling an image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.27: An illustration of a classification model labeling an image</p>
			<p>Some of the most common classification algorithms are as follows:</p>
			<ul>
				<li><strong class="bold">Logistic regression</strong>: This algorithm is similar to linear regression, in which feature coefficients are learned and predictions are made by taking the sum of the product of the feature coefficients and features. </li>
				<li><strong class="bold">Decision trees</strong>: This algorithm follows a tree-like structure. Decisions are made at each node and branches represent possible options at the node, terminating in the predicted result.</li>
				<li><strong class="bold">ANNs</strong>: ANNs replicate the structure and performance of a biological neural network to perform pattern recognition tasks. An ANN consists of interconnected neurons, laid out with a set architecture, that pass information to each other until a result is achieved.</li>
			</ul>
			<h2 id="_idParaDest-33"><a id="_idTextAnchor034"/>Regression Tasks</h2>
			<p>While the aim of <strong class="source-inline">classification</strong> tasks is to label datasets with discrete variables, the aim of <strong class="source-inline">regression</strong> tasks is to provide input data with continuous variables and output a numerical value. For example, if you have a dataset of stock market prices, a classification task may predict whether to buy, sell, or hold, whereas a regression task will predict what the stock market price will be.</p>
			<p>A simple yet very popular algorithm for regression tasks is linear regression. It consists of only one independent feature (<strong class="source-inline">x</strong>), whose relationship with its dependent feature (<strong class="source-inline">y</strong>) is linear. Due to its simplicity, it is often overlooked, even though it performs very well for simple data problems.</p>
			<p>Some of the most common regression algorithms are as follows:</p>
			<ul>
				<li><strong class="bold">Linear regression</strong>: This algorithm learns feature coefficients and predictions are made by taking the sum of the product of the feature coefficients and features.</li>
				<li><strong class="bold">Support Vector Machines</strong>: This algorithm uses kernels to map input data into a multi-dimensional feature space to understand relationships between features and the target.</li>
				<li><strong class="bold">ANNs</strong>: ANNs replicate the structure and performance of a biological neural network to perform pattern recognition tasks. An ANN consists of interconnected neurons, laid out with a set architecture, that pass information to each other until a result is achieved.</li>
			</ul>
			<h2 id="_idParaDest-34"><a id="_idTextAnchor035"/>Training Datasets and Test Datasets</h2>
			<p>Whenever we create machine learning models, we separate the data into <strong class="source-inline">training</strong> and <strong class="source-inline">test</strong> datasets. The training data is the set of data that's used to train the model. Typically, it is a large proportion—around <strong class="source-inline">80%</strong>—of the total dataset. The test dataset is a sample of the dataset that is held out from the beginning and is used to provide an unbiased evaluation of the model. The test dataset should represent real-world data as accurately as possible. Any model evaluation metrics that are reported should be applied to the test dataset unless it's explicitly stated that the metrics have been evaluated on the training dataset. The reason for this is that models will typically perform better on the data they are trained on.</p>
			<p>Furthermore, models can overfit the training dataset, meaning that they perform well on the training dataset but perform poorly on the <strong class="source-inline">test</strong> dataset. A model is said to be overfitted to the data if the model's performance is very good when evaluated on the <strong class="source-inline">training</strong> dataset, but it performs poorly on the <strong class="source-inline">test</strong> dataset. Conversely, a model can be underfitted to the data. In this case, the model will fail to learn relationships between the <strong class="source-inline">features</strong> and the <strong class="source-inline">target</strong>, which will lead to poor performance when evaluated on both the <strong class="source-inline">training</strong> and <strong class="source-inline">test</strong> datasets.</p>
			<p>We aim for a balance of the two, not relying so heavily on the <strong class="source-inline">training</strong> dataset that we overfit but allowing the model to learn the relationships between the <strong class="source-inline">features</strong> and the <strong class="source-inline">target</strong> so that the model generalizes well to new data. This concept is illustrated in the following figure:</p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/B15777_01_28.jpg" alt="Figure 1.28: An example of underfitting and overfitting a dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.28: An example of underfitting and overfitting a dataset</p>
			<p>There are many ways to split the dataset via <strong class="source-inline">sampling</strong> methods. One way to split a dataset into training is to simply randomly sample the data until you have the desired number of data points. This is often the default method in functions such as the scikit-learn <strong class="source-inline">train_test_spilt</strong> function.</p>
			<p>Another method is to stratify the sampling. In stratified sampling, each subpopulation is sampled independently. Each subpopulation is determined by the target variable. This can be advantageous in examples such as <strong class="source-inline">binary classification</strong>, where the target variable is highly skewed toward one value or another, and random sampling may not provide data points of both values in the <strong class="source-inline">training</strong> and <strong class="source-inline">test</strong> datasets. There are also validation datasets, which we will address later in this chapter.</p>
			<h2 id="_idParaDest-35"><a id="_idTextAnchor036"/>Model Evaluation Metrics</h2>
			<p>It is important to be able to evaluate our models effectively, not just in terms of the model's performance but also in the context of the problem we are trying to solve. For example, let's say we built a <strong class="source-inline">classification</strong> task to predict whether to buy, sell, or hold stock based on historical stock market prices. If our model only predicted to buy every time, this would not be a useful result because we may not have infinite resources to buy stock. It may be better to be less accurate yet also include some sell predictions.</p>
			<p>Common evaluation metrics for <strong class="source-inline">classification</strong> tasks include accuracy, precision, recall, and f1 score. <strong class="source-inline">Accuracy</strong> is defined as the number of correct predictions divided by the total number of predictions. <strong class="source-inline">Accuracy</strong> is very interpretable and relatable and good for when there are balanced classes. When the classes are highly skewed, accuracy can be misleading, however:</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/B15777_01_29.jpg" alt="Figure 1.29: Formula to calculate accuracy&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.29: Formula to calculate accuracy</p>
			<p><strong class="source-inline">Precision</strong> is another useful metric. It's defined as the number of true positive results divided by the total number of positive results (true and false) predicted by the model:</p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/B15777_01_30.jpg" alt="Figure 1.30: Formula to calculate precision&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.30: Formula to calculate precision</p>
			<p><strong class="source-inline">Recall</strong> is defined as the number of correct positive results divided by all the positive results from the ground truth:</p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/B15777_01_31.jpg" alt="Figure 1.31: Formula to calculate recall&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.31: Formula to calculate recall</p>
			<p>Both <strong class="source-inline">precision</strong> and <strong class="source-inline">recall</strong> are scored between <strong class="source-inline">zero</strong> and <strong class="source-inline">one</strong> but scoring well on one may mean scoring poorly on the other. For example, a model may have high precision but low recall, which indicates that the model is very accurate but misses a large number of positive instances. It is useful to have a metric that combines recall and precision. Enter the <strong class="source-inline">F1 score</strong>, which determines how precise and robust your model is:</p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B15777_01_32.jpg" alt="Figure 1.32: Formula to calculate the F1 score&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.32: Formula to calculate the F1 score</p>
			<p>When evaluating models, it is helpful to look at a range of different evaluation metrics. They will help determine the most appropriate model and evaluate where the model is misclassifying predictions.</p>
			<p>For example, take a model that helps doctors predict the presence of a rare disease in their patients. By predicting a negative result for every instance, the model might provide a highly accurate evaluation, but this would not help the doctors or patients very much. Instead, examining the <strong class="source-inline">precision</strong> or <strong class="source-inline">recall</strong> may be much more informative.</p>
			<p>A high precision model is very picky and will likely ensure that all predictions labeled positive are indeed positive. A high recall model is likely to recall many of the <strong class="source-inline">true</strong> positive instances, at the cost of incurring many false positives.</p>
			<p>A <strong class="source-inline">high precision model</strong> is desired when you want to be sure the predictions labeled as <strong class="source-inline">true</strong> have a high likelihood of being true. In our example, this may be desired if the cost of treating a rare disease or risk of treatment complications is high. A <strong class="source-inline">high recall model</strong> is desired if you want to make sure your model recalls as many <strong class="source-inline">true</strong> positives as possible. In our example, this may be the case if the rare disease is highly contagious and we want to be sure all cases of the disease are treated.</p>
			<h2 id="_idParaDest-36"><a id="_idTextAnchor037"/>Exercise 1.04: Creating a Simple Model</h2>
			<p>In this exercise, we will create a simple <strong class="source-inline">logistic regression model</strong> from the <strong class="source-inline">scikit-learn</strong> package. Then, we will create some model evaluation metrics and test the predictions against those model evaluation metrics.</p>
			<p>We should always approach training any machine learning model as an iterative approach, beginning with a simple model and using model evaluation metrics to evaluate the performance of the models. In this model, our goal is to classify the users in the online shoppers purchasing intention dataset into those that will purchase during their session and those that will not. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Load in the data:<p class="source-code">import pandas as pd</p><p class="source-code">feats = pd.read_csv('../data/OSI_feats_e3.csv')</p><p class="source-code">target = pd.read_csv('../data/OSI_target_e2.csv')</p></li>
				<li>Begin by creating a <strong class="source-inline">test</strong> and <strong class="source-inline">training</strong> dataset. Train the data using the <strong class="source-inline">training</strong> dataset and evaluate the performance of the model on the <strong class="source-inline">test</strong> dataset.<p>We will use <strong class="source-inline">test_size = 0.2</strong>, which means that <strong class="source-inline">20%</strong> of the data will be reserved for testing, and we will set a number for the <strong class="source-inline">random_state</strong> parameter:</p><p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">test_size = 0.2</p><p class="source-code">random_state = 42</p><p class="source-code">X_train, X_test, \</p><p class="source-code">y_train, y_test = train_test_split(feats, target, \</p><p class="source-code">                                   test_size=test_size, \</p><p class="source-code">                                   random_state=random_state)</p></li>
				<li>Print out the <strong class="source-inline">shape</strong> of each DataFrame to verify that the dimensions are correct:<p class="source-code">print(f'Shape of X_train: {X_train.shape}')</p><p class="source-code">print(f'Shape of y_train: {y_train.shape}')</p><p class="source-code">print(f'Shape of X_test: {X_test.shape}')</p><p class="source-code">print(f'Shape of y_test: {y_test.shape}')</p><p>The preceding code produces the following output:</p><p class="source-code">Shape of X_train: (9864, 68)</p><p class="source-code">Shape of y_train: (9864, 1)</p><p class="source-code">Shape of X_test: (2466, 68)</p><p class="source-code">Shape of y_test: (2466, 1)</p><p>These dimensions look correct; each of the <strong class="source-inline">target</strong> datasets has a single column, the training feature and <strong class="source-inline">target</strong> DataFrames have the same number of rows, the same applies to the <strong class="source-inline">test</strong> feature and <strong class="source-inline">target</strong> DataFrames, and the test DataFrames are <strong class="source-inline">20%</strong> of the total dataset.</p></li>
				<li>Next, instantiate the model:<p class="source-code">from sklearn.linear_model import LogisticRegression</p><p class="source-code">model = LogisticRegression(random_state=42)</p><p>While there are many arguments we can add to scikit-learn's logistic regression model (such as the type and value of the regularization parameter, the type of solver, and the maximum number of iterations for the model to have), we will only pass <strong class="source-inline">random_state</strong>.</p></li>
				<li>Then, <strong class="source-inline">fit</strong> the model to the training data:<p class="source-code">model.fit(X_train, y_train['Revenue'])</p></li>
				<li>To test the performance of the model, compare the predictions of the model with the true values:<p class="source-code">y_pred = model.predict(X_test)</p><p>There are many types of model evaluation metrics that we can use. Let's start with the <strong class="source-inline">accuracy</strong>, which is defined as the proportion of predicted values that equal the true values:</p><p class="source-code">from sklearn import metrics</p><p class="source-code">accuracy = metrics.accuracy_score(y_pred=y_pred, \</p><p class="source-code">                                  y_true=y_test)</p><p class="source-code">print(f'Accuracy of the model is {accuracy*100:.4f}%')</p><p>The preceding code produces the following output:</p><p class="source-code">Accuracy of the model is 87.0641%</p></li>
				<li>Other common evaluation metrics for classification models include <strong class="source-inline">precision</strong>, <strong class="source-inline">recall</strong>, and <strong class="source-inline">fscore</strong>. Use the scikit-learn <strong class="source-inline">precison_recall_fscore_support</strong> function, which can calculate all three:<p class="source-code">precision, recall, fscore, _ = \</p><p class="source-code">metrics.precision_recall_fscore_support(y_pred=y_pred, \</p><p class="source-code">                                        y_true=y_test, \</p><p class="source-code">                                        average='binary')</p><p class="source-code">print(f'Precision: {precision:.4f}\nRecall: \</p><p class="source-code">{recall:.4f}\nfscore: {fscore:.4f}')</p><p class="callout-heading">Note</p><p class="callout">The underscore is used in Python for many reasons. It can be used to recall the value of the last expression in the interpreter, but in this case, we're using it to ignore specific values that are output by the function.</p><p>The following figure shows the output of the preceding code:</p><p class="source-code">Precision: 0.7347</p><p class="source-code">Recall: 0.3504</p><p class="source-code">fscore: 0.4745</p><p>Since these metrics are scored between <strong class="source-inline">0</strong> and <strong class="source-inline">1</strong>, the <strong class="source-inline">recall</strong> and <strong class="source-inline">fscore</strong> are not as impressive as the <strong class="source-inline">accuracy</strong>, though looking at all of these metrics together can help us find where our models are doing well and where they could be improved by examining in which observations the model gets predictions incorrect.</p></li>
				<li>Look at the coefficients that the model outputs to observe which features have a greater impact on the overall result of the prediction:<p class="source-code">coef_list = [f'{feature}: {coef}' for coef, \</p><p class="source-code">             feature in sorted(zip(model.coef_[0], \</p><p class="source-code">             X_train.columns.values.tolist()))]</p><p class="source-code">for item in coef_list:</p><p class="source-code">    print(item)</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer042" class="IMG---Figure"><img src="image/B15777_01_33.jpg" alt="Figure 1.33: The sorted important features of the model with their respective coefficients&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 1.33: The sorted important features of the model with their respective coefficients</p>
			<p>This exercise has taught us how to create and train a predictive model to predict a <strong class="source-inline">target</strong> variable when given <strong class="source-inline">feature</strong> variables. We split the <strong class="source-inline">feature</strong> and <strong class="source-inline">target</strong> dataset into <strong class="source-inline">training</strong> and <strong class="source-inline">test</strong> datasets. Then, we trained our model on the <strong class="source-inline">training</strong> dataset and evaluated our model on the <strong class="source-inline">test</strong> dataset. Finally, we observed the trained coefficients for this model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Aq3ZCc">https://packt.live/2Aq3ZCc</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2VIRSaL">https://packt.live/2VIRSaL</a>.</p>
			<h1 id="_idParaDest-37"><a id="_idTextAnchor038"/>Model Tuning</h1>
			<p>In this section, we will delve further into evaluating model performance and examine techniques that we can use to generalize models to new data using <strong class="source-inline">regularization</strong>. Providing the context of a model's performance is extremely important. Our aim is to determine whether our model is performing well compared to trivial or obvious approaches. We do this by creating a baseline model against which machine learning models we train are compared. It is important to stress that all model evaluation metrics are evaluated and reported via the <strong class="source-inline">test</strong> dataset since that will give us an understanding of how the model will perform on new data.</p>
			<h2 id="_idParaDest-38"><a id="_idTextAnchor039"/>Baseline Models</h2>
			<p>A baseline model should be a simple and well-understood procedure, and the performance of this model should be the lowest acceptable performance for any model we build. For classification models, a useful and easy baseline model is to calculate the model outcome value. For example, if there are <strong class="source-inline">60%</strong> <strong class="source-inline">false</strong> values, our baseline model would be to predict false for every value, which would give us an <strong class="source-inline">accuracy</strong> of <strong class="source-inline">60%</strong>. For <strong class="source-inline">regression models</strong>, the <strong class="source-inline">mean</strong> or <strong class="source-inline">median</strong> can be used as the baseline.</p>
			<h2 id="_idParaDest-39"><a id="_idTextAnchor040"/>Exercise 1.05: Determining a Baseline Model</h2>
			<p>In this exercise, we will put the model performance into context. The accuracy we attained from our model seemed good, but we need something to compare it to. Since machine learning model performance is relative, it is important to develop a robust baseline with which to compare models. Once again, we are using the online shoppers purchasing intention dataset, and our <strong class="source-inline">target</strong> variable is whether or not each user will purchase a product in their session. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">pandas</strong> library and load in the <strong class="source-inline">target</strong> dataset:<p class="source-code">import pandas as pd</p><p class="source-code">target = pd.read_csv('../data/OSI_target_e2.csv')</p></li>
				<li>Next, calculate the relative proportion of each value of the <strong class="source-inline">target</strong> variables:<p class="source-code">target['Revenue'].value_counts()/target.shape[0]*100</p><p>The following figure shows the output of the preceding code:</p><div id="_idContainer043" class="IMG---Figure"><img src="image/B15777_01_34.jpg" alt="Figure 1.34: Relative proportion of each value&#13;&#10;"/></div><p class="figure-caption">Figure 1.34: Relative proportion of each value</p></li>
				<li>Here, we can see that <strong class="source-inline">0</strong> is represented <strong class="source-inline">84.525547%</strong> of the time—that is, there is no purchase by the user, and this is our <strong class="source-inline">baseline</strong> accuracy. Now, for the other model evaluation metrics:<p class="source-code">from sklearn import metrics</p><p class="source-code">y_baseline = pd.Series(data=[0]*target.shape[0])</p><p class="source-code">precision, recall, \</p><p class="source-code">fscore, _ = metrics.precision_recall_fscore_support\</p><p class="source-code">            (y_pred=y_baseline, \</p><p class="source-code">             y_true=target['Revenue'], average='macro')</p><p>Here, we've set the baseline model to predict <strong class="source-inline">0</strong> and have repeated the value so that it's the same as the number of rows in the <strong class="source-inline">test</strong> dataset.</p><p class="callout-heading">Note</p><p class="callout">The average parameter in the <strong class="source-inline">precision_recall_fscore_support</strong> function has to be set to <strong class="source-inline">macro</strong> because when it is set to <strong class="source-inline">binary</strong>, as it was previously, the function is looking for <strong class="source-inline">true</strong> values, and our <strong class="source-inline">baseline</strong> model only consists of <strong class="source-inline">false</strong> values.</p></li>
				<li>Print the final output for precision, recall, and fscore:<p class="source-code">print(f'Precision: {precision:.4f}\nRecall:\</p><p class="source-code">{recall:.4f}\nfscore: {fscore:.4f}')</p><p>The preceding code produces the following output:</p><p class="source-code">Precision: 0.9226</p><p class="source-code">Recall: 0.5000</p><p class="source-code">Fscore: 0.4581</p></li>
			</ol>
			<p>Now, we have a baseline model that we can compare to our previous model, as well as any subsequent models. By doing this, we can tell that while the accuracy of our previous model seemed high, it did not score much better than this <strong class="source-inline">baseline</strong> model.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/31MD1jH">https://packt.live/31MD1jH</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2VFFSXO">https://packt.live/2VFFSXO</a>.</p>
			<h2 id="_idParaDest-40"><a id="_idTextAnchor041"/>Regularization</h2>
			<p>Earlier in this chapter, we learned about <strong class="source-inline">overfitting</strong> and what it looks like. The hallmark of <strong class="source-inline">overfitting</strong> is when a model is trained on the training data and performs extremely well yet performs terribly on <strong class="source-inline">test</strong> data. One reason for this could be that the model may be relying too heavily on certain features that lead to good performance in the training dataset but do not generalize well to new observations of data or the test dataset. </p>
			<p>One technique that can be used to avoid this is called <strong class="source-inline">regularization</strong>. Regularization constrains the values of the coefficients toward zero, which discourages a complex model. There are many different types of regularization techniques. For example, in <strong class="source-inline">linear</strong> and <strong class="source-inline">logistic</strong> regression, <strong class="source-inline">ridge</strong> and <strong class="source-inline">lasso</strong> regularization are most common. In tree-based models, limiting the maximum depth of the trees acts as regularization.</p>
			<p>There are two different types of regularization, namely <strong class="source-inline">L1</strong> and <strong class="source-inline">L2</strong>. This term is either the <strong class="source-inline">L2</strong> norm (the sum of the squared values) of the weights or the <strong class="source-inline">L1</strong> norm (the sum of the absolute values) of the weights. Since the <strong class="source-inline">l1</strong> regularization parameter acts as a feature selector, it is able to reduce the coefficient of features to zero. We can use the output of this model to observe which features do not contribute much to the performance and remove them entirely if desired. The <strong class="source-inline">l2</strong> regularization parameter will not reduce the coefficient of features to zero, so we will observe that they all have non-zero values.</p>
			<p>The following code shows how to instantiate the models using these regularization techniques:</p>
			<p class="source-code">model_l1 = LogisticRegressionCV(Cs=Cs, penalty='l1', \</p>
			<p class="source-code">                                cv=10, solver='liblinear', \</p>
			<p class="source-code">                                random_state=42)</p>
			<p class="source-code">model_l2 = LogisticRegressionCV(Cs=Cs, penalty='l2', \</p>
			<p class="source-code">                                cv=10, random_state=42)</p>
			<p>The following code shows how to fit the models:</p>
			<p class="source-code">model_l1.fit(X_train, y_train['Revenue'])</p>
			<p class="source-code">model_l2.fit(X_train, y_train['Revenue'])</p>
			<p>The same concepts in lasso and ridge regularization can be applied to ANNs. However, penalization occurs on the weight matrices rather than the coefficients. Dropout is another form of regularization that's used to prevent overfitting in ANNs. Dropout randomly selects nodes at each iteration and removes them, along with their connections, as shown in the following figure:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B15777_01_35.jpg" alt="Figure 1.35: Dropout regularization in ANNs&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.35: Dropout regularization in ANNs</p>
			<h2 id="_idParaDest-41"><a id="_idTextAnchor042"/>Cross-Validation</h2>
			<p>Cross-validation is often used in conjunction with regularization to help tune hyperparameters. Take, for example, the <strong class="source-inline">penalization</strong> parameter in ridge and lasso regression, or the proportion of nodes to drop out at each iteration using the dropout technique with ANNs. How will you determine which parameter to use? One way is to run models for each value of the regularization parameter and evaluate them on the test set; however, using the test set often can introduce bias into the model.</p>
			<p>One popular example of cross-validation is called k-fold cross-validation. This technique gives us the ability to test our model on unseen data while retaining a test set that we will use to test at the end. Using this method, the data is divided into <strong class="source-inline">k</strong> subsets. In each of the <strong class="source-inline">k</strong> iterations, <strong class="source-inline">k-1</strong> of the subsets are used as training data and the remaining subset is used as a validation set. This is repeated <strong class="source-inline">k</strong> times until all <em class="italic">k</em> subsets have been used as validation sets. </p>
			<p>By using this technique, there is a significant reduction in bias, since most of the data is used for fitting. There is also a reduction in variation since most of the data is also used for validation. Typically, there are between <strong class="source-inline">5</strong> and <strong class="source-inline">10</strong> folds, and the technique can even be stratified, which is useful when there is a large imbalance of classes.</p>
			<p>The following example shows <strong class="source-inline">5-fold cross-validation</strong> with <strong class="source-inline">20%</strong> of the data being held out as a test set. The remaining <strong class="source-inline">80%</strong> is separated into 5 folds. Four of those folds comprise the training data, and the remaining fold is the validation data. This is repeated a total of five times until every fold has been used once for validation:</p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B15777_01_36.jpg" alt="Figure 1.36: A figure demonstrating how 5-fold cross-validation works&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 1.36: A figure demonstrating how 5-fold cross-validation works</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor043"/>Activity 1.01: Adding Regularization to the Model</h2>
			<p>In this activity, we will utilize the same logistic regression model from the scikit-learn package. This time, however, we will add regularization to the model and search for the optimum regularization parameter—a process often called hyperparameter tuning. After training the models, we will test the predictions and compare the model evaluation metrics to those produced by the baseline model and the model without regularization.</p>
			<p>The steps we will take are as follows:</p>
			<ol>
				<li value="1">Load in the feature and target datasets of the online shoppers purchasing intention dataset from <strong class="source-inline">'../data/OSI_feats_e3.csv'</strong> and <strong class="source-inline">'../data/OSI_target_e2.csv'</strong>.</li>
				<li>Create <strong class="source-inline">training</strong> and <strong class="source-inline">test</strong> datasets for each of the <strong class="source-inline">feature</strong> and <strong class="source-inline">target</strong> datasets. The <strong class="source-inline">training</strong> datasets will be used to train on, and the models will be evaluated using the <strong class="source-inline">test</strong> datasets.</li>
				<li>Instantiate a model instance of the <strong class="source-inline">LogisticRegressionCV</strong> class of scikit-learn's <strong class="source-inline">linear_model</strong> package.</li>
				<li>Fit the model to the <strong class="source-inline">training</strong> data.</li>
				<li>Make predictions on the <strong class="source-inline">test</strong> dataset using the trained model.</li>
				<li>Evaluate the models by comparing how they scored against the <strong class="source-inline">true</strong> values using the evaluation metrics.</li>
			</ol>
			<p>After implementing these steps, you should get the following expected output:</p>
			<p class="source-code">l1</p>
			<p class="source-code">Precision: 0.7300</p>
			<p class="source-code">Recall: 0.4078</p>
			<p class="source-code">fscore: 0.5233</p>
			<p class="source-code">l2</p>
			<p class="source-code">Precision: 0.7350</p>
			<p class="source-code">Recall: 0.4106</p>
			<p class="source-code">fscore: 0.5269</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The solution for this activity can be found on page 348.</p>
			<p>This activity has taught us how to use <strong class="source-inline">regularization</strong> in <strong class="source-inline">conjunction</strong> with <strong class="source-inline">cross-validation</strong> to appropriately score a model. We have learned how to fit a model to data using regularization and cross-validation. Regularization is an important technique to use to ensure that models don't overfit the training data. Models that have been trained with regularization will perform better on new data, which is generally the goal of machine learning models—to predict a target when given new observations of the input data. Choosing the optimal regularization parameter may require iterating over a number of different choices. </p>
			<p><strong class="source-inline">Cross-validation</strong> is a technique that's used to determine which set of regularization parameters fit the data best. Cross-validation will train multiple models with different values for the regularization parameters on different cuts of the data. This technique ensures the best set of regularization parameters are chosen, without adding bias and minimizing variance.</p>
			<h1 id="_idParaDest-43"><a id="_idTextAnchor044"/>Summary</h1>
			<p>In this chapter, we covered how to prepare data and construct machine learning models. We achieved this by utilizing Python and libraries such as pandas and scikit-learn. We also used the algorithms in scikit-learn to build our machine learning models.</p>
			<p>Then, we learned how to load data into Python, as well as how to manipulate data so that a machine learning model can be trained on the data. This involved converting all the columns into numerical data types. We also created a basic logistic regression classification model using scikit-learn algorithms. We divided the dataset into training and test datasets and fit the model to the training dataset. We evaluated the performance of the model on the test dataset using the model evaluation metrics, that is, accuracy, precision, recall, and fscore.</p>
			<p>Finally, we iterated on this basic model by creating two models with different types of regularization for the model. We utilized cross-validation to determine the optimal parameter to use for the regularization parameter.</p>
			<p>In the next chapter, we will use these same concepts to create the model using the Keras library. We will use the same dataset and attempt to predict the same target value for the same classification task. By doing so, we will learn how to use <strong class="source-inline">regularization</strong>, <strong class="source-inline">cross-validation</strong>, and <strong class="source-inline">model evaluation metrics</strong> when fitting our neural network to the data.</p>
		</div>
	</body></html>