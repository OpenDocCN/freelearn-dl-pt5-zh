- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Large Language Models in Depth
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In recent years, interest in transformers has skyrocketed in the academic world,
    industry, and even the general public. The state-of-the-art transformer-based
    architectures today are called **large language models** (**LLMs**). The most
    captivating feature is their text-generation capabilities, and the most popular
    example is ChatGPT ([https://chat.openai.com/](https://chat.openai.com/)). But
    in their core lies the humble transformer we introduced in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202).
    Luckily, we already have a solid foundation of transformers. One remarkable aspect
    of this architecture is that it has changed little in the years since it was introduced.
    Instead, the capabilities of LLMs have grown with their size (the name gives it
    away), lending credibility to the phrase *quantitative change leads to* *qualitative
    change*.
  prefs: []
  type: TYPE_NORMAL
- en: The success of LLMs has further fueled the research in the area (or is it the
    other way around?). On the one hand, large industrial labs (such as Google, Meta,
    Microsoft, or OpenAI) invest heavily to push the boundaries for even larger LLMs.
    On the other hand, the agile, open source community finds creative ways to achieve
    a lot with limited resources.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we’ll explore the current LLM landscape from theoretical and
    practical perspectives. We will survey many of the latest LLMs, their properties,
    and their training. Furthermore, we’ll see how to apply them for our purposes
    with the help of the Hugging Face Transformers library.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LLM architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Emergent abilities of LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing Hugging Face Transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We’ll implement the example in this chapter using Python, PyTorch, and the
    Hugging Face Transformers library ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)).
    If you don’t have an environment with these tools, fret not—the example is available
    as a Jupyter notebook on Google Colab. The code examples are in the book’s GitHub
    repository: [https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter08](https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter08).'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’ll take a more systematic approach and dive deeper into
    transformer-based architectures. As we mentioned in the introduction, the transformer
    block has changed remarkedly little since its introduction in 2017\. Instead,
    the main advances have come in terms of larger models and larger training sets.
    For example, the original GPT model (GPT-1) has 117M parameters, while GPT-3 (*Language
    Models are Few-Shot Learners*, [https://arxiv.org/abs/2005.14165](https://arxiv.org/abs/2005.14165))
    has 175B, a thousandfold increase. We can distinguish two informal transformer
    model categories based on size:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Pre-trained language models** (**PLMs**): Transformers with fewer parameters,
    such as **Bidirectional Encoder Representations from Transformers** (**BERT**)
    and **generative pre-trained transformers** (**GPT**), fall into this category.
    Starting with BERT, these transformers introduced the two-step pre-training/FT
    paradigm. The combination of the attention mechanism and unsupervised pre-training
    (**masked language modeling** (**MLM**) or **next-word prediction** (**NWP**)
    creates effective general-purpose semantic features, which we can use for a number
    of downstream tasks. Because of this, PLMs perform better than other **natural
    language processing** (**NLP**) algorithms, such as **recurrent neural networks**
    (**RNNs**). Combined with their highly parallelizable architecture, this has inspired
    a lot of follow-up work on transformers, which produced improved models and eventually
    led to the next category.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LLMs**: These are transformer models with billions of parameters. LLMs differ
    qualitatively from PLMs in the following ways:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Emergent capabilities**: They can solve a series of complex tasks, which
    we will discuss in the *Emergent abilities of* *LLMs* section'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Prompting interface**: LLMs can interact with humans with natural language
    instead of special APIs'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Fusion of research and engineering**: The scale of LLMs requires researchers
    to have strong engineering skills in large-scale data processing and parallel
    training'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Today, LLMs are almost exclusively decoder-only models because the main applications
    of the current LLMs revolve around text generation (for example, chatbots such
    as ChatGPT). This has happened at the expense of encoder-only and encoder-decoder
    architectures. To better understand why, let’s see how a chatbot works. It starts
    with a user-generated message (known as a **prompt**). A prompt is the initial
    input sequence to the decoder-based model, which generates a response one token
    at a time. The response is added back to the input sequence. A special token separates
    the prompts and the responses. Once the LLM generates a response, the user may
    make another prompt. In this case, we concatenate the new prompt to the existing
    sequence and task the LLM to create a new response based on the extended sequence.
    The LLM has no mechanism for memorizing the existing chat session other than including
    it as part of the input sequence. This process can continue indefinitely. However,
    once it reaches the maximum length of the context window, it will start truncating
    the initial parts of the sequence (we can think of this as a sliding window).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Parts of this chapter are based on the paper *A Survey of Large Language Models*
    ([https://arxiv.org/abs/2303.18223](https://arxiv.org/abs/2303.18223)). We’ll
    refer to it simply as *the survey*.
  prefs: []
  type: TYPE_NORMAL
- en: LLM architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202), we introduced the **multi-head
    attention** (**MHA**) mechanism and the three major transformer variants—encoder-decoder,
    encoder-only, and decoder-only (we used BERT and GPT as prototypical encoder and
    decoder models). In this section, we’ll discuss various bits and pieces of the
    LLM architecture. Let’s start by focusing our attention (yes—it’s the same old
    joke) on the attention mechanism.
  prefs: []
  type: TYPE_NORMAL
- en: LLM attention variants
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The attention we discussed so far is known as **global attention**. The following
    diagram displays the **connectivity matrix** of a bidirectional global self-attention
    mechanism (context window with size *n=8*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Global self-attention with a context window with size n=8](img/B19627_08_1.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Global self-attention with a context window with size n=8
  prefs: []
  type: TYPE_NORMAL
- en: Each row and column represent the full input token sequence, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/737.png).
    The dotted colored diagonal cells represent the current input token (query), ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/738.png).
    The uninterrupted colored cells of each column represent all tokens (keys) that
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/338.png)
    can attend to. For example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:math>](img/740.png)
    attends to all preceding tokens, [t 1…t 4],
  prefs: []
  type: TYPE_NORMAL
- en: and all succeeding tokens, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/741.png).
    The term *global* implies that ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/742.png)
    attends to all tokens. Hence all cells are colored. As we’ll see in the *Sparse
    attention* section, there are attention variants where not all tokens participate.
    We’ll denote these tokens with transparent cells. The diagram depicts bidirectional
    self-attention, as the query can attend to both preceding (down) and succeeding
    (up) elements. The query will only attend to the elements below the current input
    token in the unidirectional case. For example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:math>](img/740.png)
    will only attend to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mfenced
    open="[" close="]" separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>…</mml:mo><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/744.png).
  prefs: []
  type: TYPE_NORMAL
- en: As we’ll see, one of the main challenges of the attention mechanism is its time
    and space complexity.
  prefs: []
  type: TYPE_NORMAL
- en: Attention complexity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Despite its advantages, the attention mechanism (particularly global attention)
    has some drawbacks. One of them is that space and time complexity increase quadratically
    with the increase of the context window. That’s because the mechanism is implemented
    with the help of matrices and matrix multiplication.
  prefs: []
  type: TYPE_NORMAL
- en: Matrix multiplication time complexity
  prefs: []
  type: TYPE_NORMAL
- en: The time complexity of the multiplication of two *n×n* matrices is ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/745.png)
    because the classic implementation uses three nested loops. In practice, the algorithm
    is optimized and is less complex. For the purposes of this section, we’ll use
    the complexity of the classic implementation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, a context window with size *n=4* results in *n×n=4x4* **Q** and
    **V** matrices with 16 total cells each. But a context window of *n=8* results
    in *n×n=8x8* **Q** and **V** matrices with 64 total cells each. Therefore, a two-times-larger
    context window requires four times more memory. Since the time complexity of matrix
    multiplication is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/746.png),
    increasing the context window from *n=4* to *n=8* would increase the number of
    operations from ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>4</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>64</mml:mn></mml:math>](img/747.png)
    to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msup><mml:mrow><mml:mn>8</mml:mn></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msup><mml:mo>=</mml:mo><mml:mn>512</mml:mn></mml:math>](img/748.png).
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s focus on the transformer block, where we have a **feed-forward network**
    (**FFN**),
  prefs: []
  type: TYPE_NORMAL
- en: 'multi-head self-attention, and four linear projections (**fully connected**
    (**FC**) layers)—three for the **Q**/**K**/**V** pre-attention split and one that
    combines the attention heads’ outputs. We’ll discuss each component’s relative
    weight in the block’s computational load. Let’s denote the embedding size with
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/693.png),
    the key dimension with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub></mml:math>](img/680.png),
    the value dimension with ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub></mml:math>](img/695.png)
    (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>k</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>v</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub><mml:mtext>/</mml:mtext><mml:mi>h</mml:mi><mml:mo>=</mml:mo><mml:mi>d</mml:mi></mml:math>](img/752.png)),
    the context window size with *n*, the number of heads with *h*, and the size of
    the hidden layer in the FFN with *ffn* (the usual convention is *ffn=4*d*). The
    time complexity of the different components is shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mn>4</mml:mn><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/753.png):
    The three input linear projections for all heads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/754.png):
    The *h* self-attention heads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>h</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/755.png):
    The fourth output linear projection after the self-attention heads'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>n</mml:mi><mml:mo>+</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi><mml:mo>×</mml:mo><mml:mi>f</mml:mi><mml:mi>f</mml:mi><mml:mi>n</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mn>8</mml:mn><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:mrow></mml:mfenced></mml:math>](img/756.png):
    The FFN module'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full combined complexity of the block is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msup><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>+</mml:mo><mml:msup><mml:mrow><mml:mi>n</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/757.png).
    We can see that it depends on the ratio between the length of the context window,
    *n*, and the embedding size, *d*. If *d>>n*, then the computational time of the
    linear projections will overshadow the time of the attention heads and vice versa.
    In practice, *d>>n* is the most common scenario. But in either case, the attention
    mechanism has at least quadratic space and time complexity. Let’s see some solutions
    to this challenge.
  prefs: []
  type: TYPE_NORMAL
- en: Multi-query and grouped-query attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MHA branches the input data to multiple heads using three linear projections
    per head. The following diagram shows two optimizations of this configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Left: MHA; center: multi-query attention (MQA); right: grouped-query
    attention (GQA) (inspired by https://arxiv.org/abs/2305.13245)](img/B19627_08_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.2 – Left: MHA; center: multi-query attention (MQA); right: grouped-query
    attention (GQA) (inspired by [https://arxiv.org/abs/2305.13245](https://arxiv.org/abs/2305.13245))'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss them (apart from MHA, which we introduced in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202)):'
  prefs: []
  type: TYPE_NORMAL
- en: '**MQA** (*Fast transformer decoding: One write-head is all you need*, [https://arxiv.org/abs/1911.02150):](https://arxiv.org/abs/1911.02150):)
    The different heads share key and value projections, as opposed to unique projections
    in MHA. Since the input sequence is the same as well, all heads share the same
    key-value store and only differ in their queries. This optimization reduces both
    the memory and computational requirements, with little performance penalty.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GQA** (*GQA: Training Generalized Multi-Query Transformer Models from Multi-Head
    Checkpoints*, [https://arxiv.org/abs/2305.13245):](https://arxiv.org/abs/2305.13245):)
    A hybrid between MHA and MQA, which shares single key and value heads for a *subgroup*
    of query heads. The authors show that GQA is almost as fast as MQA and achieves
    quality close to MHA.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the next section, we’ll discuss attention optimization, which takes into
    account the specifics of GPU memory management.
  prefs: []
  type: TYPE_NORMAL
- en: FlashAttention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this section, we’ll introduce FlashAttention (*FlashAttention: Fast and
    Memory-Efficient Exact Attention with IO-Awareness*, [https://arxiv.org/abs/2205.14135;](https://arxiv.org/abs/2205.14135;)
    *FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning*,
    [https://arxiv.org/abs/2307.08691](https://arxiv.org/abs/2307.08691)). This is
    not a new attention mechanism but an implementation of global attention, which
    considers the specifics of the GPU hardware. A GPU has a large number of computational
    cores that can perform relatively simple but highly parallelizable operations
    (such as matrix multiplication). It has two memory levels: small but fast cache
    (L1 and L2) and large but relatively slow **high bandwidth memory** (**HBM**).
    To perform an operation, it transfers the necessary data from the HBM to the cache.
    The cores use the cache for their calculations. Once the operation is done, the
    result is stored back in the HBM. The main bottleneck in this pipeline is the
    data transfers rather than the actual computation (the fewer data transfers, the
    better).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s focus on the attention block, which has five operations: 1) matrix
    multiplication (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">Q</mml:mi><mml:msup><mml:mrow><mml:mi mathvariant="bold">K</mml:mi></mml:mrow><mml:mrow><mml:mi>⊤</mml:mi></mml:mrow></mml:msup></mml:math>](img/758.png)),
    2) mask, 3) softmax, 4) dropout, and 5) matrix multiplication (**V**). The standard
    implementation performs the operations sequentially, starting with the first matrix
    multiplication. Once it’s done, it proceeds with the mask, and so on. Each operation
    involves two-way data transfer between the HBM and the cache. These transfers
    are unnecessary because the results of operation *i* are transferred from the
    cache to the HBM just to be sent back from the HBM to the cache for operation
    *i+1*. FlashAttention proposes a special **fused kernel** to solve the inefficiency.
    It splits the **Q**/**K**/**V** matrices into smaller blocks that can fit in the
    cache. Once these blocks are transferred there, the fused kernel performs all
    five operations without intermediate data transfers. Only the final result is
    sent back to the HBM. Splitting the matrices into blocks is possible because matrix
    multiplication is embarrassingly parallel. But the other innovation of FlashAttention
    is the ability to split the softmax operation, which isn’t as trivial (we won’t
    go into details about how it’s implemented). The operation is done once all matrix
    blocks pass through this pipeline.'
  prefs: []
  type: TYPE_NORMAL
- en: Splitting matrix multiplication
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say we want to multiply the matrices **A** and **B**. Because of the
    way matrix multiplication works, we can split **B** by column into two matrices,
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/759.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/760.png).
    Then, we perform two matrix multiplications on each device: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi mathvariant="bold">A</mml:mi><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/761.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi
    mathvariant="bold">A</mml:mi><mml:msub><mml:mrow><mml:mi mathvariant="bold">B</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/762.png).
    Finally, we concatenate the output of the two operations in a single matrix, equivalent
    to the matrix produced by the original multiplication, **AB**.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll discuss solving the performance issue with new attention
    mechanisms.
  prefs: []
  type: TYPE_NORMAL
- en: Sparse attention
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Sparse attention** is a class of methods where the output vector attends
    to a subset of all key vectors instead of the entire context window. For example,
    if we can attend to four vectors of interest from the entire eight-vector context,
    we could reduce the necessary computations twice.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram displays three bidirectional sparse attention mechanisms:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Left: local attention; center: dilated local attention; right:
    random attention; context window size n=12](img/B19627_08_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.3 – Left: local attention; center: dilated local attention; right:
    random attention; context window size n=12'
  prefs: []
  type: TYPE_NORMAL
- en: The mechanisms follow the same notation as the ones in *Figure 8**.2*, with
    one addition—the transparent cells represent tokens (keys), which the query doesn’t
    attend to.
  prefs: []
  type: TYPE_NORMAL
- en: On the left, we have bidirectional **local attention** (or **sliding window
    attention**), first introduced in *Image Transformer*, [https://arxiv.org/abs/1802.05751](https://arxiv.org/abs/1802.05751)).
    The query attends to a limited context window of the nearest *w* keys around the
    current token (½*w* to the left and ½*w* to the right). The self-attention block
    still takes the full *n*-sized sequence as input, but each token attends to a
    limited *w*-sized local context. This way, the memory footprint is the same as
    global attention, but the time complexity is reduced to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>w</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/763.png)
    instead of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/764.png).
  prefs: []
  type: TYPE_NORMAL
- en: To understand why local attention works, let’s return to **convolutional neural
    networks** (**CNNs**). Recall that the earlier layers of a CNN have small receptive
    fields and capture smaller, simpler features. Conversely, the deeper CNN layers
    have large receptive fields that capture larger and more complex features. We
    can apply the same principle to transformers. Research has shown that the initial
    transformer blocks learn simple token features and local syntax, while the deeper
    layers learn more complex context-dependent aspects of token semantics. Because
    of this, we can apply local attention to the earlier transformer blocks and reserve
    global attention for the deeper ones without sacrificing performance.
  prefs: []
  type: TYPE_NORMAL
- en: '**Dilated attention** (*Figure 8**.3*, center) is a modification of local attention,
    which works in a similar way to the dilated convolutions we introduced in [*Chapter
    4*](B19627_04.xhtml#_idTextAnchor107). Unlike local attention, here, the context
    window is not continuous. Instead, there is a gap of *g* cells (which could be
    more than one) between each context token. This makes it possible to attend to
    a wider context with the same *n* of computations.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, we have bidirectional **random attention** (*Figure 8**.3*, right), where
    the current query (token) attends to a subset of *r* keys (tokens) from the full
    context window. The time complexity is reduced to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>r</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/765.png)
    instead of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/766.png).
    The attention pattern can be viewed as a directed graph. In the case of random
    attention, this graph is also random. That is, the information can flow rapidly
    between any pair of nodes without considering the actual structure of the data,
    which might be biased.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is also possible to combine global and local attention. One such example
    is **Longformer** (*Longformer: The Long-Document Transformer*, [https://arxiv.org/abs/2004.05150](https://arxiv.org/abs/2004.05150)),
    displayed in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Combined local and global attention; left: Longformer block;
    right: Big Bird block](img/B19627_08_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.4 – Combined local and global attention; left: Longformer block; right:
    Big Bird block'
  prefs: []
  type: TYPE_NORMAL
- en: It introduces a drop-in replacement self-attention block in an otherwise unmodified
    transformer model. The block represents a combination of global and local (or
    dilated) attention. It applies local attention to most input tokens, but a few
    can use global attention. The left section of *Figure 8**.4* shows the combined
    self-attention block and one example of input tokens that apply local and global
    attention. More specifically, the authors use the Longformer block in a unidirectional
    BERT-style model to solve MLM and `[CLS]` in MLM tasks. As the diagram shows,
    global attention works in both directions. The special token can attend to all
    other tokens, but the other tokens can also attend to the special token in addition
    to their local attention context. In the case of autoregressive language modeling
    (unidirectional model), they apply only dilated local attention, as there are
    no tokens with special significance. The full Longformer model uses dilated attention
    with a larger context window and *g* in the deeper layers, leaving the earlier
    ones with only local attention.
  prefs: []
  type: TYPE_NORMAL
- en: '**Big Bird** (*Figure 8**.4*, right; *Big Bird: Transformers for Longer Sequences*,
    [https://arxiv.org/abs/2007.14062](https://arxiv.org/abs/2007.14062)) is similar
    to Longformer but adds random attention.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s discuss the **sparse transformer** attention developed by OpenAI
    (*Generating Long Sequences with Sparse Transformers*, [https://arxiv.org/abs/1904.10509](https://arxiv.org/abs/1904.10509)).
    A sparse transformer introduces unidirectional strided and fixed attention schemes,
    displayed in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – Left: strided sparse attention with l=4; right: fixed sparse
    attention; input image size 4×4; sequence length n=12 (inspired by https://arxiv.org/abs/1904.10509)](img/B19627_08_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.5 – Left: strided sparse attention with l=4; right: fixed sparse attention;
    input image size 4×4; sequence length n=12 (inspired by [https://arxiv.org/abs/1904.10509](https://arxiv.org/abs/1904.10509))'
  prefs: []
  type: TYPE_NORMAL
- en: To understand how they work, let’s discuss the context of the paper. It proposes
    a unified decoder-only model to generate new images, text, or audio. Depending
    on the use case, the input and output data can be a two-dimensional image tensor
    (we’ll omit the color dimension for simplicity). However, the transformer accepts
    as input a one-dimensional sequence. We can solve this by concatenating the rows
    of the image in a single one-dimensional tensor. Once done, we can treat the image
    like a regular sequence and feed it to the model. *Figure 8**.5* displays a strided
    (left) and fixed attention (right) connectivity matrix for a two-dimensional image
    (top) and its equivalent concatenated one-dimensional sequence (bottom). Let’s
    note that the bottom expanded sequence doesn’t match the dimensions of the top
    image—it should be with length *n=16*, which reflects the 4×4 image, instead of
    *n=12* as it is now. Since this is a generative decoder-only model, it uses unidirectional
    attention, even though the concept of direction doesn’t exist in the same way
    in images as in text.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s discuss the two attention schemes. We’ll start with strided attention,
    where the current token attends to the preceding row and column of the input image.
    These are two separate mechanisms split between different attention heads:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Row head**: Equivalent to unidirectional local attention, which attends to
    the previous ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mo>≈</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math>](img/767.png)
    tokens, where ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math>](img/768.png)
    is the length of one entire row of the 2D input image. Let’s denote the index
    of the current input token with *i* and the tokens it attends to with *j*. We
    can summarize the row mechanism in the following way:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mfenced separators="|"><mml:mrow><mml:mi>i</mml:mi><mml:mo>-</mml:mo><mml:mi>j</mml:mi></mml:mrow></mml:mfenced><mml:mo><</mml:mo><mml:mi>l</mml:mi></mml:math>](img/769.png)'
  prefs: []
  type: TYPE_IMG
- en: '**Column head**: Equivalent to unidirectional dilated attention with a stride
    (gap) of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>l</mml:mi><mml:mo>≈</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math>](img/770.png)
    (the same as the row head). Assuming that the input image is square, the column
    head jumps the equivalent of one row (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt></mml:math>](img/768.png))
    and attends to a location representing the previous cell in a virtual column of
    the one-dimensional sequence. We can summarize column strided attention in the
    following way:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mfenced
    open="(" close=")"><mrow><mi>i</mi><mo>−</mo><mi>j</mi></mrow></mfenced><mtext>mod</mtext><mi>l</mi><mo>=</mo><mn>0</mn></mrow></mrow></math>](img/772.png)'
  prefs: []
  type: TYPE_IMG
- en: This scheme performs best for 2D input data, such as images, because the row/column
    split reflects the underlying data structure. The time complexity of this scheme
    is ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:mi>l</mml:mi><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced><mml:mo>≈</mml:mo><mml:mi>O</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>n</mml:mi><mml:mo>×</mml:mo><mml:msqrt><mml:mi>n</mml:mi></mml:msqrt><mml:mo>×</mml:mo><mml:mi>d</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/773.png).
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we have **fixed attention**, which attends to a fixed column and the
    elements after the latest column element. It performs better on non-periodic data,
    such as text. Once again, this is a combination of two separate mechanisms split
    between different heads:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Column head**: Attends to a fixed column, which doesn’t necessarily match
    the column of the current input token, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mi>i</mml:mi></mml:mrow></mml:msub></mml:math>](img/774.png).
    Multiple input tokens attend to the same column, which makes it possible to attend
    to the entire length of the sequence. We can summarize the column mechanism in
    the following way:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mi>l</mi><mo>−</mo><mi>c</mi><mo>≤</mo><mi>j</mi><mtext>mod</mtext><mi>l</mi><mo>≤</mo><mi>l</mi></mrow></mrow></math>](img/775.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *c* is a parameter (8, 16, or 32). For example, if *l=64* and *c=16*,
    then all positions greater than 64 can attend to positions 48-64, all positions
    greater than 128 can attend to 112-128, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: '**Row head**: The first head is similar to the row head in strided attention.
    But instead of attending to the length of one entire row, it only attends to the
    location of the current column head. The row head provides local context. We can
    summarize it in the following way:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>j</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>f</mml:mi><mml:mi>l</mml:mi><mml:mi>o</mml:mi><mml:mi>o</mml:mi><mml:mi>r</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mi>i</mml:mi></mml:mrow><mml:mrow><mml:mi>l</mml:mi></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:math>](img/776.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *floor* rounds down the result of the division to the nearest whole number.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s focus our attention (I can’t stop myself) on a special case of decoder-only
    architecture and various aspects of LLM architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Prefix decoder
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this section, we’ll introduce **prefix** (or **non-causal**) **decoder**
    (*Unified Language Model Pre-training for Natural Language Understanding and Generation*,
    [https://arxiv.org/abs/1905.03197](https://arxiv.org/abs/1905.03197)). This is
    a decoder-only model that introduces a new type of attention pattern, displayed
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Prefix decoder self-attention pattern (inspired by https://arxiv.org/abs/1905.03197)](img/B19627_08_6.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Prefix decoder self-attention pattern (inspired by https://arxiv.org/abs/1905.03197)
  prefs: []
  type: TYPE_NORMAL
- en: We split the input sequence into two segments—![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/777.png)
    through ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:math>](img/778.png)
    (**source** or **prefix**), and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>5</mml:mn></mml:mrow></mml:msub></mml:math>](img/779.png)
    through ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:math>](img/780.png)
    (**target**). The tokens of the source segment have bidirectional access to all
    other tokens of that segment. However, the target segment tokens have unidirectional
    access to the preceding tokens of the whole (source and target) input sequence.
    For example, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>3</mml:mn></mml:mrow></mml:msub></mml:math>](img/781.png)
    is part of the source segment and can attend to
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/782.png),
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/783.png),
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>4</mml:mn></mml:mrow></mml:msub></mml:math>](img/784.png).
    Conversely, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>7</mml:mn></mml:mrow></mml:msub></mml:math>](img/785.png)
    is part of the target and can only attend to tokens ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/782.png)
    through ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>6</mml:mn></mml:mrow></mml:msub></mml:math>](img/787.png)
    (but not ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">t</mml:mi></mml:mrow><mml:mrow><mml:mn>8</mml:mn></mml:mrow></mml:msub></mml:math>](img/788.png)).'
  prefs: []
  type: TYPE_NORMAL
- en: The prefix decoder is a hybrid between encoder-decoder and decoder models. The
    source segment acts as an encoder, and the target acts as a decoder, yet the underlying
    architecture is decoder-based.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the prefix decoder for `[SOS]`) and end-of-sequence (`[EOS]`) tokens.
    For example, let’s take the text summarization task. We represent the text sequence
    to summarize (`S1`) and its summarization (`S2`) as a single sequence: `[[SOS],S1,[EOS],S2,[EOS]]`.
    The source sequence, `[[SOS],S1,[EOS]]`, falls within the bidirectional part of
    the attention pattern, and the target sequence, `[S2,[EOS]]`, falls within the
    unidirectional one. We pre-train the model with the help of MLM, where we mask
    random tokens from the full sequence. We fine-tune the model by randomly masking
    some tokens in the target sequence and learning to recover the masked words. Let’s
    note that the `[EOS]` token can also participate in the masking. In this way,
    the model learns when to generate `[EOS]` tokens and terminate the generation
    of the target sequence.'
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s get into more details about various aspects of the LLM architecture.
  prefs: []
  type: TYPE_NORMAL
- en: Transformer nuts and bolts
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following table provides a detailed summary of the main transformer network
    configurations and their variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Different transformer configurations (source: https://arxiv.org/abs/2303.18223)](img/B19627_08_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.7 – Different transformer configurations (source: https://arxiv.org/abs/2303.18223)'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re already familiar with many of these—we introduced the three different
    normalization positions in [*Chapter 7*](B19627_07.xhtml#_idTextAnchor202). We
    also introduced two of the three normalization methods in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079).
    By default, most transformers use **layer normalization** (**LN**). However, some
    models use **RMSNorm** because of its superior training speed and performance.
    Last but not least, **DeepNorm** (*DeepNet: Scaling Transformers to 1,000 Layers*,
    [https://arxiv.org/abs/2203.00555](https://arxiv.org/abs/2203.00555)) is new to
    us. As the paper’s name suggests, this normalization helped build a 1,000-layer
    transformer. The authors argue that in pre-**layer normalization** (**pre-ln**)
    architectures, the gradients at the bottom layers tend to be larger than the ones
    at the top layers, degrading the performance compared to **post-layer normalization**
    (**post-ln**) models. On the other hand, post-ln models are unstable due to exploding
    gradients. To overcome this, they propose a simple yet effective normalization
    of the residual connections:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>LayerNorm</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mi>α</mml:mi><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:mtext>SubLayer</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced></mml:mrow></mml:mfenced></mml:math>](img/789.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *α* is a constant applied at the output of the residual connection. Its
    value depends on the transformer type (encoder or decoder) and the model depth
    (number of blocks). The theoretical justification of DeepNorm is that it bounds
    the model update by that constant.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s discuss the activation functions. More specifically, we’ll discuss
    the activation function (`ActivationFunc`) of the first layer of the **feed-forward
    network** (**FFN**) sublayer, as this is the only explicit activation in the transformer
    block. As a reminder, we can define the original FFN as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mtext>FFN</mml:mtext><mml:mfenced separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mtext>Activation</mml:mtext><mml:mtext>Func</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>b</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/790.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We discussed most activations in [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079),
    except for **SwiGLU** and **GeGLU** (*GLU Variants Improve Transformer*, [https://arxiv.org/abs/2002.05202](https://arxiv.org/abs/2002.05202)).
    They are variations of **Gated Linear Unit** (**GLU**, *Language Modeling with
    Gated Convolutional Networks*, [https://arxiv.org/abs/1612.08083](https://arxiv.org/abs/1612.08083)),
    which is more of a fusion between layer and activation function rather than pure
    activation. We can define GLU as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mtext>GLU</mml:mtext><mml:mfenced
    separators="|"><mml:mrow><mml:mtext>x</mml:mtext></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi
    mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">W</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:mi>b</mml:mi></mml:mrow></mml:mfenced><mml:mo>⊗</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">x</mml:mi><mml:mo>+</mml:mo><mml:mi>c</mml:mi></mml:mrow></mml:mfenced></mml:math>](img/791.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *ActivationFunc* is a specific activation function (Swish for *Swi*GLU
    and *Ge*LU for *Ge*GLU), ⊗ is the element-wise product of two vectors, and **W**
    and **V** are weight matrices, which represent linear projections (that is, FC
    layers). GLU introduces an additional linear projection, **V**, parallel to the
    original path of the network, **W**. Thanks to the element-wise product, the path
    with activation, **W**, acts as a gate to the signal coming from the **V** path.
    This is like **Long Short-Term Memory** (**LSTM**) gates. We can now define the
    FFN with GLU activation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mtext>FFN</mml:mtext></mml:mrow><mml:mrow><mml:mtext>Activation</mml:mtext><mml:mtext>GLU</mml:mtext></mml:mrow></mml:msub><mml:mfenced
    separators="|"><mml:mrow><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>=</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>A</mml:mi><mml:mi>c</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>v</mml:mi><mml:mi>a</mml:mi><mml:mi>t</mml:mi><mml:mi>i</mml:mi><mml:mi>o</mml:mi><mml:mi>n</mml:mi><mml:mi
    mathvariant="normal">F</mml:mi><mml:mi mathvariant="normal">u</mml:mi><mml:mi
    mathvariant="normal">n</mml:mi><mml:mi mathvariant="normal">c</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:msub><mml:mrow><mml:mi mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mi
    mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:mo>⊗</mml:mo><mml:mi
    mathvariant="bold">V</mml:mi><mml:mi mathvariant="bold">x</mml:mi></mml:mrow></mml:mfenced><mml:msub><mml:mrow><mml:mi
    mathvariant="bold">W</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/792.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s note that the authors have excluded the bias from the modified FFN. This
    is also a good place to mention that different LLMs have different bias configurations,
    listed next:'
  prefs: []
  type: TYPE_NORMAL
- en: Use bias in both the linear projections and the attention blocks themselves
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use bias in the linear projections but not in the attention blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Don’t use bias in either the linear projections or the attention blocks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: According to some experiments, the lack of biases stabilizes the training.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s focus on the various types of positional embeddings we haven’t mentioned
    so far. Unfortunately (or fortunately), discussing them in detail goes beyond
    the scope of this book. But the important thing to remember is that we have either
    absolute (static) or relative (dynamic) positional encodings. In the first case,
    we modify the input token embedding vectors. In the second case, we modify the
    **K**/**V** attention matrices relative to their position of the current input
    token.
  prefs: []
  type: TYPE_NORMAL
- en: The survey summarizes the suggestions from existing literature for detailed
    transformer configuration. For stronger generalization, it suggests using pre-RMSNorm
    normalization, and SwiGLU or GeGLU activation functions. In addition, using LN
    immediately after embedding layers is likely to incur performance degradation.
    As for position embeddings, **Rotary Positional Embedding** (**RoPE**) or **Attention
    with Linear Biases** (**AliBi**) perform better on long sequences than other methods.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’re familiar with the architecture properties of LLMs, let’s discuss
    specific model instances.
  prefs: []
  type: TYPE_NORMAL
- en: Models
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The following table represents a summary of some of the popular recent LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Model cards of recent LLMs with public configuration details
    (modified from https://arxiv.org/abs/2303.18223p)](img/B19627_08_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Model cards of recent LLMs with public configuration details (modified
    from https://arxiv.org/abs/2303.18223p)
  prefs: []
  type: TYPE_NORMAL
- en: Here, **PE** denotes position embedding, **#L** denotes the number of transformer
    layers, **#H** denotes the number of attention heads per layer, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/707.png)
    denotes the size of hidden states, and **MCL** denotes the maximum context length
    during training.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with the GPT series of models (developed by OpenAI), which is outlined
    in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – The evolution of the GPT series of models (inspired by https://arxiv.org/abs/2303.18223)](img/B19627_08_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – The evolution of the GPT series of models (inspired by https://arxiv.org/abs/2303.18223)
  prefs: []
  type: TYPE_NORMAL
- en: We’re already familiar with GPT-1, so let’s move on to `gpt-3.5-turbo` with
    a context length of 4,096 tokens and `gpt-3.5-turbo-16k` with 16,384 tokens. The
    current version of Copilot is based on GPT-3.5\. The newest model, GPT-4, accepts
    multimodal inputs (images and text) but outputs text only. It is also closed,
    but it might have more than 1T parameters. According to Sam Altman, CEO of OpenAI,
    training GPT-4 has cost more than $100 million (https://www.wired.com/story/openai-ceo-sam-altman-the-age-of-giant-ai-models-is-already-over/).
    GPT-4 is also available through OpenAI’s API with two subvariants—`gpt-4` with
    a context length of 8,192 tokens and `gpt-4-32k` with 32,768 tokens.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s discuss the **LlaMa** series of pre-trained (and not fine-tuned)
    models released by Meta. The first version (*LLaMA: Open and Efficient Foundation
    Language Models*, [https://arxiv.org/abs/2302.13971](https://arxiv.org/abs/2302.13971))
    has four variants, ranging from 6B to 65B parameters. This is one of the most
    popular LLMs in the open source community because Meta has also released its weights
    (although they are not licensed for commercial use). This way, the company has
    done the heavy lifting of pre-training the model. The open source community uses
    it as a **foundation model** because it can be fine-tuned with relatively little
    compute. Recently, Meta released **Llama 2**—an updated version of Llama (*Llama
    2: Open Foundation and Fine-Tuned Chat Models*, [https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models](https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models)).
    It has three variants with 7B, 13B, and 70B parameters. Llama 2 uses GQA and 40%
    more pre-training data than Llama 1\. In addition, each variant also has a version
    fine-tuned using RLHF. The model’s license allows commercial use (with some limitations).'
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our survey on the architecture of LLMs. Next, let’s discuss their
    training.
  prefs: []
  type: TYPE_NORMAL
- en: Training LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Since most LLMs are decoder-only, the most common LLM pre-training task is
    NWP. The large number of model parameters (up to hundreds of billions) requires
    comparatively large training datasets to prevent overfitting and realize the full
    capabilities of the models. This requirement poses two significant challenges:
    ensuring training data quality and the ability to process large volumes of data.
    In the following sections, we’ll discuss various aspects of the LLM training pipeline,
    starting from the training datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: Training datasets
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can categorize the training data into two broad categories:'
  prefs: []
  type: TYPE_NORMAL
- en: '**General**: Examples include web pages, books, or conversational text. LLMs
    almost always train on general data because it’s widely available and diverse,
    improving the language modeling and generalization capabilities of LLMs.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Specialized**: Code, scientific articles, textbooks, or multilingual data
    for providing LLMs with task-specific capabilities.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The following table lists the most popular language modeling datasets:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Language modeling datasets (modified from https://arxiv.org/abs/2303.18223)](img/B19627_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Language modeling datasets (modified from https://arxiv.org/abs/2303.18223)
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s discuss them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Books**: We’ll focus on two datasets:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BookCorpus** (*Aligning Books and Movies: Towards Story-like Visual Explanations
    by Watching Movies and Reading Books*, [https://arxiv.org/abs/1506.06724](https://arxiv.org/abs/1506.06724)):
    Includes 11,000 fictional books with close to 1B words (released in 2015).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Project Gutenberg** ([https://www.gutenberg.org/](https://www.gutenberg.org/)):
    Includes 70,000 fictional books.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Common Crawl** ([https://commoncrawl.org/](https://commoncrawl.org/)): Petabyte-sized
    web crawling database. The data is split by the date obtained, starting from 2008\.
    The latest archive contains 3.1B web pages (390 TiB of uncompressed content),
    scraped from 44 million hosts or 35 million registered domains. It contains a
    lot of low-quality data, but there are multiple subsets with higher-quality data:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Colossal, cleaned version of Common Crawl** (**C4**): An 800 GiB dataset
    developed by Google. The original dataset is unavailable for download, but Google
    has published the tools to recreate it from the Common Crawl database. In 2019,
    the **Allen Institute for AI** (**AI2**, https://allenai.org/) released a recreation,
    available at [https://huggingface.co/datasets/allenai/c4](https://huggingface.co/datasets/allenai/c4).
    Its most popular sub-variant is the *en* version, which removes all documents
    that contain words from the so-called *badwords filter* (a list of bad words is
    available at [https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words](https://github.com/LDNOOBW/List-of-Dirty-Naughty-Obscene-and-Otherwise-Bad-Words)).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CC-News**: Articles from news sites all over the world.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**RealNews**: News articles extracted from the 5,000 news domains indexed by
    *Google News*.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**CC-Stories-R**: A dataset for common sense reasoning and language modeling.
    It consists of Common Crawl documents with the most overlapping *n*-grams with
    the questions in common sense reasoning tasks. The new training corpus represents
    the top 1.0% of the highest-ranked documents.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reddit links**: One way to overcome the low signal-to-noise ratio of Common
    Crawl is to rely on human-curated content. Enter Reddit, where users can post
    textual content or links, and other users can upvote these submissions (the upvotes
    are known as *karma*). We’ll mention two Reddit-based datasets:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WebText** (released alongside the GPT-2 model): Contains a subset of 45 million
    Reddit-submitted links with a karma of three or more. The documents behind these
    links form the LLM training data. WebText is not publicly available, but there
    is an open source version called **OpenWebText** ([https://github.com/jcpeterson/openwebtext](https://github.com/jcpeterson/openwebtext)).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pushshift** ([https://arxiv.org/abs/2001.08435](https://arxiv.org/abs/2001.08435)):
    Contains all link submissions and comments posted on Reddit.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Reddit API pricing controversy
  prefs: []
  type: TYPE_NORMAL
- en: The rise of LLMs has made Reddit data much more valuable than before. Because
    of this, the company has decided to introduce fees for access to its previously
    free API. This measure mainly targets AI companies that plan to train their LLMs
    using the data. However, the proposal has led many of the site’s voluntary moderators
    (Reddit relies on them) to announce a strike by temporarily closing the previously
    open communities they moderate. At the time of writing, the disagreement is still
    ongoing.
  prefs: []
  type: TYPE_NORMAL
- en: '**The Pile** (*An 800GB Dataset of Diverse Text for Language Modeling*, [https://arxiv.org/abs/2101.00027](https://arxiv.org/abs/2101.00027)):
    Composed of 22 diverse and high-quality datasets derived from various sources,
    including PubMed, arXiv, GitHub, Stack Exchange, Hacker News, YouTube, and others.
    The Pile also introduces the OpenWebText2 and BookCorpus2 extensions of the original
    OpenWebText and BookCorpus datasets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ROOTS** (*The BigScience ROOTS Corpus: A 1.6TB Composite Multilingual Dataset*,
    [https://arxiv.org/abs/2303.03915](https://arxiv.org/abs/2303.03915)): A web-scale
    curated dataset covering 46 natural languages and 13 programming languages.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wikimedia** ([https://dumps.wikimedia.org/](https://dumps.wikimedia.org/)):
    Because of its high-quality content, this is an excellent source of training data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Stack Exchange** ([https://archive.org/details/stackexchange](https://archive.org/details/stackexchange)):
    A network of QA topic sites with a rating system. The most popular representative
    is **Stack Overflow**. It releases a tri-monthly anonymized data dump with all
    user-contributed content.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**arXiv** (https://www.kaggle.com/datasets/Cornell-University/arxiv): The primary
    scientific data source, which contains more than 2.2B scientific articles.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GitHub**: The GH Archive project ([https://www.gharchive.org/](https://www.gharchive.org/))
    records, archives, and provides access to the public GitHub timeline.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In practice, the LLM pre-training step uses a mix of several datasets. The
    following screenshot shows the distribution of the sources of pre-training data
    for several representative LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Ratios of various data sources in the pre-training data for
    existing LLMs (source: https://arxiv.org/abs/2303.18223)](img/B19627_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.11 – Ratios of various data sources in the pre-training data for existing
    LLMs (source: https://arxiv.org/abs/2303.18223)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Mixing datasets is not a trivial process and requires several processing steps.
    Let’s discuss them:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Remove low-quality or irrelevant data**: For example, web pages contain large
    amounts of HTML tags, JavaScript, or **cascading style sheets** (**CSS**). Yet,
    we’re only interested in'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: human-readable text (except when we want to train the model explicitly to understand
    HTML). In this case, we’ll have to remove the HTML and JavaScript and only leave
    the text.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**Remove personally identifiable information (PII)**: Data is often extracted
    from web pages, which might contain personal information. This step aims to remove
    such data from the training set.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tokenization**: We discussed tokenization in depth in [*Chapter 6*](B19627_06.xhtml#_idTextAnchor185),
    and we won’t discuss it here.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, let’s introduce a practical transformer scaling law (*Scaling Laws
    for Neural Language Models*, https://arxiv.org/abs/2001.08361). Because of their
    scale, training LLMs can be expensive. Therefore, it is important not to train
    the model more (or less) than necessary. Based on empirical experiments, the scaling
    law proposes an optimal ratio between the amount of training compute (expressed
    in **floating-point operations per second**, or **FLOPS**), *C*, the model size
    (number of parameters), *N*, and the training dataset size (number of tokens),
    *D*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:mi>C</mml:mi><mml:mo>≈</mml:mo><mml:mn>6</mml:mn><mml:mi>N</mml:mi><mml:mi>D</mml:mi></mml:math>](img/794.png)'
  prefs: []
  type: TYPE_IMG
- en: Now that we know the steps to build the training set, let’s focus on the actual
    pre-training.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training properties
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Similar to other **neural networks** (**NNs**), pre-training of LLMs works with
    gradient descent and backpropagation. But because of their size, the training
    has specific properties, which we’ll discuss in this section.
  prefs: []
  type: TYPE_NORMAL
- en: Adam optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Most LLMs use Adam (*Adam: A Method for Stochastic Optimization*, [https://arxiv.org/abs/1412.6980](https://arxiv.org/abs/1412.6980))
    or one of its modifications. Although we’ve used it in many examples so far, we
    haven’t discussed it in detail. Time to remedy this omission.'
  prefs: []
  type: TYPE_NORMAL
- en: A reminder of the weight update formula
  prefs: []
  type: TYPE_NORMAL
- en: 'In [*Chapter 2*](B19627_02.xhtml#_idTextAnchor047), we learned that we use
    backpropagation to compute the gradient (first derivative) of the loss function,
    *J(θ)*, with respect to every parameter, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/795.png):
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/796.png).
    Once we have the gradient, we can perform the weight update with the formula ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/797.png),
    where *η* is the learning rate. We can add momentum (or velocity) to that formula.
    To do so, we’ll assume we are at step *t* of the training process. Then, we can
    calculate the momentum of the current update based on the momentum of the update
    at step *t-1*: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>μ</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:math>](img/798.png),
    where *µ* is a momentum rate in the [0:1] range. In addition, we can add L2 regularization
    (or weight decay; see [*Chapter 3*](B19627_03.xhtml#_idTextAnchor079)): ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:mi>μ</mml:mi><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mi>η</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mfenced separators="|"><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/799.png),
    where *λ* is the weight decay coefficient. Finally, we can perform the weight
    update: ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/800.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Adam calculates individual and adaptive learning rates for every weight based
    on previous weight updates (momentum). Let’s see how that works:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Compute the first moment (or mean) and the second moment (or variance) of the
    gradient:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>m</mi><mi>t</mi></msub><mo>←</mo><msub><mi>β</mi><mn>1</mn></msub><msub><mi>m</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo>+</mo><mfenced
    open="(" close=")"><mrow><mn>1</mn><mo>−</mo><msub><mi>β</mi><mn>1</mn></msub></mrow></mfenced><mfrac><mrow><mo>∂</mo><mi>J</mi><mfenced
    open="(" close=")"><mi>θ</mi></mfenced></mrow><mrow><mo>∂</mo><msub><mi>θ</mi><mi>j</mi></msub></mrow></mfrac></mrow></mrow></math>](img/801.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>+</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:mrow></mml:mfenced><mml:msup><mml:mrow><mml:mfenced
    separators="|"><mml:mrow><mml:mfrac><mml:mrow><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mo>∂</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfrac></mml:mrow></mml:mfenced></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msup></mml:math>](img/802.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/803.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/804.png)
    are hyperparameters with default values of 0.9 and 0.95, respectively. The two
    formulas are very similar to the momentum one. The relationship between ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/805.png)
    *(**![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/806.png)**)*
    and
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/807.png)
    *(**![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/808.png)**)*
    acts as a simulation of a moving average. But instead of averaging across multiple
    previous values, we take the latest previous value, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/809.png)
    *(**![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi><mml:mo>-</mml:mo><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/810.png)**)*,
    and assign it a weight coefficient, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/803.png)
    (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>2</mml:mn></mml:mrow></mml:msub></mml:math>](img/804.png)).'
  prefs: []
  type: TYPE_NORMAL
- en: 2. The initial values of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/813.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/187.png)
    are 0, so they will have a bias toward 0 in the initial phase of the training.
    To understand why this could be a problem, let’s assume that at *t=1*, ![<mml:math
    xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>β</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn></mml:math>](img/815.png)
    and
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mo>∂</mml:mo><mml:mi>J</mml:mi><mml:mfenced
    separators="|"><mml:mrow><mml:mi>θ</mml:mi></mml:mrow></mml:mfenced><mml:mtext>/</mml:mtext><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>5</mml:mn></mml:math>](img/816.png).
    Then, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub><mml:mo>=</mml:mo><mml:mn>0.9</mml:mn><mml:mi>*</mml:mi><mml:mn>0</mml:mn><mml:mo>+</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mn>1</mml:mn><mml:mo>-</mml:mo><mml:mn>0.9</mml:mn></mml:mrow></mml:mfenced><mml:mi>*</mml:mi><mml:mn>5</mml:mn><mml:mo>=</mml:mo><mml:mn>0.5</mml:mn></mml:math>](img/817.png)
    is much less than the actual gradient of 5\. We can compensate for this bias with
    bias-corrected versions of ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/813.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/187.png):'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mover><msub><mi>m</mi><mi>t</mi></msub><mo
    stretchy="true">ˆ</mo></mover><mo>←</mo><mfrac><msub><mi>m</mi><mi>t</mi></msub><mrow><mn>1</mn><mo>−</mo><msubsup><mi>β</mi><mn>1</mn><mi>t</mi></msubsup></mrow></mfrac></mrow></mrow></math>](img/820.png)'
  prefs: []
  type: TYPE_IMG
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><mover><msub><mi>v</mi><mi>t</mi></msub><mo
    stretchy="true">ˆ</mo></mover><mo>←</mo><mfrac><msub><mi>v</mi><mi>t</mi></msub><mrow><mn>1</mn><mo>−</mo><msubsup><mi>β</mi><mn>2</mn><mi>t</mi></msubsup></mrow></mfrac></mrow></mrow></math>](img/821.png)'
  prefs: []
  type: TYPE_IMG
- en: '3. Perform the weight update with the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><mrow><mrow><msub><mi>θ</mi><mi>j</mi></msub><mo>←</mo><msub><mi>θ</mi><mi>j</mi></msub><mo>−</mo><mi>η</mi><mfrac><mover><msub><mi>m</mi><mi>t</mi></msub><mo
    stretchy="true">ˆ</mo></mover><msqrt><mrow><mover><msub><mi>v</mi><mi>t</mi></msub><mo
    stretchy="true">ˆ</mo></mover><mo>+</mo><mi>ε</mi></mrow></msqrt></mfrac></mrow></mrow></math>](img/822.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *ε* is some small value to prevent division by 0.
  prefs: []
  type: TYPE_NORMAL
- en: '**AdamW** (*Decoupled Weight Decay Regularization*, [https://arxiv.org/abs/1711.05101](https://arxiv.org/abs/1711.05101))
    improves Adam with decoupled weight decay:'
  prefs: []
  type: TYPE_NORMAL
- en: '![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"
    display="block"><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>←</mml:mo><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub><mml:mo>-</mml:mo><mml:mfenced
    separators="|"><mml:mrow><mml:mi>η</mml:mi><mml:mfrac><mml:mrow><mml:mover accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover></mml:mrow><mml:mrow><mml:msqrt><mml:mover
    accent="true"><mml:mrow><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:mrow><mml:mo>^</mml:mo></mml:mover><mml:mo>+</mml:mo><mml:mi>ε</mml:mi></mml:msqrt></mml:mrow></mml:mfrac><mml:mo>+</mml:mo><mml:mi>λ</mml:mi><mml:msub><mml:mrow><mml:mi>θ</mml:mi></mml:mrow><mml:mrow><mml:mi>j</mml:mi></mml:mrow></mml:msub></mml:mrow></mml:mfenced></mml:math>](img/823.png)'
  prefs: []
  type: TYPE_IMG
- en: Recall that the L2 regularization participates in the loss function and then,
    through the derivative process, is transferred (as weight decay) to the weight
    update formula. In this case, the regularization will pass through all the transformations
    of the cost function and will be subject to them. As the name suggests, decoupled
    weight decay bypasses all these transformations and participates directly in the
    preceding formula.
  prefs: []
  type: TYPE_NORMAL
- en: One issue with Adam and AdamW is the increased memory consumption—the optimizer
    stores at least two additional values (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>m</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/813.png)
    and ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>v</mml:mi></mml:mrow><mml:mrow><mml:mi>t</mml:mi></mml:mrow></mml:msub></mml:math>](img/187.png))
    for every model parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Parallel processing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The scale of LLMs necessitates special steps for efficient training. First,
    we’ll discuss how to train LLMs across multiple devices. More specifically, we’ll
    discuss a combination of three different types of parallelism (also referred to
    as 3D parallelism):'
  prefs: []
  type: TYPE_NORMAL
- en: '**Data parallelism**: It works when the model is small enough to fit on a single
    device:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Create identical copies of the entire model and its optimizer states (including
    the random seeds) across all devices.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Split each batch of the training set into unique subsets (shards) and distribute
    them across all devices.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Each device computes its gradient based on its unique subset of the input batch.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Aggregate the gradients of all devices into a single gradient update.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Distribute the aggregated updates across the devices and perform weight updates
    on each device. This way, we start and end each training step with identical models.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Model (or pipeline)**: Split the model across multiple devices on an operation
    (layer) level. For example, if our model has 9 layers, we can send layers 1 through
    6 to one device and layers 7 through 9 to another. In this way, we can train models
    that don’t fit in the memory of a single device. Not only that, but we can apply
    this method even on a single device. In this case, we’ll load the first set of
    operations (1-6) and compute their output. Then, we’ll unload them and load the
    following subset (7-9). The output of the first set will serve as input for the
    second. Backpropagation works in the same way but in the opposite direction. One
    issue with model parallelism is that if we use multiple devices, the second one
    will idle until the first produces output.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Tensor (or horizontal)**: Split the model across different devices on the
    tensor level, which solves the idling problem of model parallelism. To understand
    how this works, let’s recall that matrix multiplication is the most computationally
    intensive operation of contemporary NNs. But, as we discussed in the *FlashAttention*
    section, it is also embarrassingly parallel. Therefore, we can split it across
    devices.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zero redundancy optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Zero redundancy optimizer** (**ZeRO**) (*ZeRO: Memory Optimizations Toward
    Training Trillion Parameter Models*, https://arxiv.org/abs/1910.02054) is a hybrid
    between data and model parallelism. The following diagram illustrates the three
    stages of ZeRO:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – ZeRO (inspired by https://arxiv.org/abs/1910.02054)](img/B19627_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – ZeRO (inspired by https://arxiv.org/abs/1910.02054)
  prefs: []
  type: TYPE_NORMAL
- en: 'The first line represents a case of a data-parallel system. Each GPU receives
    a unique shard of the input mini-batch. It also holds an identical copy of the
    model parameters (first colored rectangle of the GPUi block), gradients (second
    rectangle), and optimizer states (third rectangle). The size of the optimizer
    states that they take up most of the memory during training (for example, Adam
    stores multiple values per model parameter). The following three lines represent
    the three stages of ZeRO:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Optimizer state partitioning** (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mtext>os</mml:mtext></mml:mrow></mml:msub></mml:math>](img/826.png)):
    Each GPU holds an identical copy of the entire model parameters and its gradients,
    but the optimizer states are partitioned across the GPUs, and each holds only
    a portion.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Add gradient partitioning** (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mtext>os+g</mml:mtext></mml:mrow></mml:msub></mml:math>](img/827.png)):
    Each GPU holds an identical copy of the entire model parameters, but the gradients
    and the optimizer states are partitioned.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '**Add model parameters** (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mtext>os+g+p</mml:mtext></mml:mrow></mml:msub></mml:math>](img/828.png)):
    Each GPU holds a portion of all components.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: To understand how the algorithm works, we’ll assume we use ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>P</mml:mtext></mml:mrow><mml:mrow><mml:mtext>os+g+p</mml:mtext></mml:mrow></mml:msub></mml:math>](img/829.png),
    a model with *N* layers and *N* GPUs. Each layer is distributed on one GPU—the
    first layer is on ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/830.png),
    the second layer is on ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/831.png),
  prefs: []
  type: TYPE_NORMAL
- en: and so on. Let’s start with the forward phase. First, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/832.png)
    receives ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>Data</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/833.png).
    Since it holds the first layer of the model, it can feed it the input and calculate
    its activations independently. At the same time, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/834.png)
    broadcasts the parameters of the first layer to all other GPUs. Each GPU now holds
    the first layer parameters in addition to its own portion of the model parameters.
    In this way, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msub></mml:math>](img/835.png)
    can process its own input, ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>Data</mml:mtext></mml:mrow><mml:mrow><mml:mtext>i</mml:mtext></mml:mrow></mml:msub></mml:math>](img/836.png),
    through the first layer, just as ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/832.png)
    did. Once a GPU computes the activations of the first layer, it deletes its parameters
    from its memory (except ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>0</mml:mn></mml:mrow></mml:msub></mml:math>](img/838.png),
    which preserves them). We repeat the same steps, this time with the second layer.
    ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/839.png)
    broadcasts its parameters so that all GPUs can continue with the forward phase.
    After this, all but ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mtext>GPU</mml:mtext></mml:mrow><mml:mrow><mml:mn>1</mml:mn></mml:mrow></mml:msub></mml:math>](img/840.png)
    delete the second layer parameters. This process continues until all GPUs produce
    model output. Then, the loss function is aggregated across all GPUs. Next, the
    backward phase starts, which works in the same way as the forward one, but this
    time the GPUs broadcast both the gradients and the optimizer states.
  prefs: []
  type: TYPE_NORMAL
- en: Mixed precision training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '**Mixed precision training** ([https://arxiv.org/abs/1710.03740](https://arxiv.org/abs/1710.03740))
    is the idea that not all values have to be stored with 32-bit (double or full)
    floating-point precision (**FP32** or **Float32** data format). Research has shown
    that storing some values into lower 16-bit (single or half) floating-point precision
    (**FP16** of **Float16**) doesn’t degrade the model performance. The weights,
    activations, gradients, and optimizer states are stored as FP16\. In addition,
    the model retains an FP32 master copy of the weights. The forward and backward
    passes use FP16 weights, but the results are optimal when the weight update operation
    uses the FP32 master copy. One possible explanation is that the weight update
    formula uses the gradients multiplied by the learning rate, and the result might
    become too small to be represented in FP16\. Another explanation is that the ratio
    of the weight value to the weight update is very large, which could lead to the
    weight update becoming zero.'
  prefs: []
  type: TYPE_NORMAL
- en: Bfloat16 and TensorFloat32
  prefs: []
  type: TYPE_NORMAL
- en: The Google Brain division developed the **brain floating-point** format (hence
    the name, **bfloat**) for **machine learning** (**ML**) applications. The standard
    FP16 format has one sign bit, five exponent bits, and ten mantissa bits. In contrast,
    bfloat16 has eight exponent and seven mantissa bits. The exponent bits are the
    same as FP32\. Bfloat16 comes close to FP32 in terms of performance on ML tasks.
    We also have **TensorFloat-32** (**TF32**)—a 19-bit format developed by NVIDIA
    for ML purposes with 8 exponent and 10 mantissa bits.
  prefs: []
  type: TYPE_NORMAL
- en: Pre-training peculiarities and summary
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In this section, we’ll discuss some LLM pre-training peculiarities. Let’s start
    with the mini-batch size. Ideally, we would compute the gradients over the entire
    training dataset and only then perform one weight update. However, large datasets
    and models make this computationally infeasible. The opposite extreme is to perform
    one weight update per training sample. But then, the training would be susceptible
    to outlier samples, which might steer the loss function to suboptimal local minima.
    Mini-batch training is a compromise that makes it possible to fit within the available
    computational resources and avoid the influence of outlier samples. But in theory,
    the larger the mini-batch size, the better. LLM training is distributed across
    multiple devices, which makes it possible (and even desirable) to use large batch
    sizes. The batch size can vary between 32K to 8.25M tokens depending on the model.
    In addition, it can be dynamic and increase as the training progresses. Empirical
    experiments have demonstrated that this technique stabilizes the training.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s focus on the learning rate, *η*. Although Adam implements an adaptive
    learning rate, most LLMs start with a **warmup phase** to stabilize the training.
    More specifically, during the first 0.1% to 0.5% of the training steps, the learning
    rate gradually increases from around ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>*</mml:mi><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>6</mml:mn></mml:mrow></mml:msup></mml:math>](img/841.png)
    to ![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:mi>η</mml:mi><mml:mo>=</mml:mo><mml:mn>1</mml:mn><mml:mi>*</mml:mi><mml:msup><mml:mrow><mml:mn>10</mml:mn></mml:mrow><mml:mrow><mml:mo>-</mml:mo><mml:mn>5</mml:mn></mml:mrow></mml:msup></mml:math>](img/842.png).
  prefs: []
  type: TYPE_NORMAL
- en: Then, the learning rate gradually decreases to around 10% of its maximum value
    following a cosine (or linear) decay strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'LLM training also uses **gradient clipping**—a technique that prevents the
    exploding gradients problem. One way to implement it is to clip by value:'
  prefs: []
  type: TYPE_NORMAL
- en: If l**g**l ≥ *max_threshold* or l**g**l ≤ *min_threshold* then **g** ← *relevant_threshold*
  prefs: []
  type: TYPE_NORMAL
- en: Here, **g** is a vector with all gradient values (l**g**l is the norm or the
    vector or its absolute value). First, we select *min_threshold* and *max_threshold*
    values. Then, we clip the value of the gradient to the threshold in the weight
    update formula if it exceeds these boundaries.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another option is to clip by norm:'
  prefs: []
  type: TYPE_NORMAL
- en: If l**g**l ≥ *threshold*, then **g** ← *threshold* * **g**/l**g**l
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, **g**/l**g**l is a unit vector. It has the same direction as the original,
    but its length is 1\. The value of every element in the unit vector is in the
    [0:1] range. By multiplying it by the *threshold*, every element lies within the
    [0: threshold] range. In this way, norm clipping scales the gradients within the
    pre-defined threshold.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The following table summarizes the training properties of some popular LLMs:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – LLM training properties (modified from https://arxiv.org/abs/2303.18223)](img/B19627_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – LLM training properties (modified from https://arxiv.org/abs/2303.18223)
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our introduction to pre-training LLMs. Next, let’s focus on the
    FT phase.
  prefs: []
  type: TYPE_NORMAL
- en: FT with RLHF
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, we have focused on the pre-training phase of LLMs. The pre-training
    objective of an LLM is to predict the next token based on (primarily) a web page
    training dataset. However, pre-trained models can express undesirable behaviors.
    They can often make up facts, generate biased or toxic text, or simply not follow
    user instructions. Yet, their purpose is to interact with humans in a *helpful*,
    *honest*, and *harmless* way. In this section, we’ll discuss the technique of
    RLHF, which makes it possible to fine-tune the LLM for better alignment with human
    values (also known as **alignment tuning**). More specifically, we’ll focus on
    the technique described in *Training language models to follow instructions with
    human feedback* ([https://arxiv.org/abs/2203.02155](https://arxiv.org/abs/2203.02155))
    by OpenAI. They apply RLHF on a GPT-3 model to produce the GPT-3.5 family of models.
    This is part of the secret sauce that makes ChatGPT so good at interacting with
    users.
  prefs: []
  type: TYPE_NORMAL
- en: 'The FT starts where the pre-training ends—with the pre-trained LLM. The following
    diagram shows the three steps of the RLHF process:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – Left: supervised FT; middle: reward model training; right:
    LLM RLHF (inspired by https://arxiv.org/abs/2203.02155)](img/B19627_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.14 – Left: supervised FT; middle: reward model training; right: LLM
    RLHF (inspired by https://arxiv.org/abs/2203.02155)'
  prefs: []
  type: TYPE_NORMAL
- en: 'First is `[prompt: response]` samples, where `prompt` and `response` are source
    and target token sequences, respectively. This dataset serves to fine-tune the
    LLM using the same target as pre-training—to predict the next token of the response,
    given the prompt. The fine-tuned LLM serves as a base for the next two steps.'
  prefs: []
  type: TYPE_NORMAL
- en: On the need for pre-training and three-step FT
  prefs: []
  type: TYPE_NORMAL
- en: The SFT step implicitly answers an unasked question—why do we need pre-training
    and three-step FT to train our model? The reason is that generating a human-labeled
    training dataset is not scalable and represents a major bottleneck. For example,
    the pre-training dataset can have over a trillion tokens; generating prompts and
    their respective responses of such magnitude with human labelers is infeasible.
    Therefore, we need pre-training to provide the LLM with a solid foundation, which
    we can fine-tune with a much smaller dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The second step is `[(A, B), (A, C), (B, C)]`. This dataset trains the RM, which
    is based on the fine-tuned LLM. Its output next-token classifier is replaced with
    a randomly initialized regression layer, which outputs the predicted scalar score
    of a given response. The RM computes the scores of both responses of each pair.
    The difference between them participates in the loss function. This is an example
    of **transfer learning** (**TL**), which aims to train the new regression layer
    on top of the original LLM.
  prefs: []
  type: TYPE_NORMAL
- en: The third step is to train the LLM using RL with the RM and **proximal policy
    optimization** (**PPO**) (*Figure 8**.14*—right).
  prefs: []
  type: TYPE_NORMAL
- en: A recap of RL
  prefs: []
  type: TYPE_NORMAL
- en: To understand the third step, let’s recap our introduction to RL from [*Chapter
    1*](B19627_01.xhtml#_idTextAnchor016). We have a system of environment and an
    agent. The agent can take one of a number of actions that change the state of
    the environment. The environment reacts to the agent’s actions and provides its
    modified state and reward (or penalty) signals that help the agent to decide its
    next action. The decision-making algorithm of the agent is called a policy. The
    agent’s goal is to maximize the total rewards received throughout the training
    episodes.
  prefs: []
  type: TYPE_NORMAL
- en: In this scenario, the policy of the agent is represented by the fine-tuned LLM.
    The token vocabulary represents the actions it can take—that is, the agent’s action
    is to select the next token in the sequence. The environment presents the LLM
    with a random prompt, and the agent (LLM) generates a response. Then, the RM,
    part of the environment, scores the generated response. The RM score is the reward
    sent to the agent and serves to update its parameters.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll discuss what makes LLMs different from other models.
  prefs: []
  type: TYPE_NORMAL
- en: Emergent abilities of LLMs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we’ll discuss the phenomenon of **emergent abilities** of
    LLMs, first summarized in [https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682).
    The paper defines emergent abilities as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: An ability is emergent if it is not present in smaller models but is present
    in larger models.
  prefs: []
  type: TYPE_NORMAL
- en: These abilities represent a qualitative difference between large and small language
    models, which cannot be predicted by extrapolation.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with the ability known as **few-shot prompting** (or **in-context
    learning**), popularized by GPT-3\. Here, the initial user prompt is an instruction
    the LLM has to follow through its response without any additional training. The
    prompt itself may describe with natural text one or more training examples (hence,
    the term *few-shot*). This is the only context that the LLM can use for training
    before generating its response. The following diagram shows an example of a few-shot
    prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.15 – An example of a few-shot prompt (inspired by https://arxiv.org/abs/2206.07682)](img/B19627_08_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.15 – An example of a few-shot prompt (inspired by https://arxiv.org/abs/2206.07682)
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s discuss the ability of LLMs to solve complex multi-step reasoning
    tasks with the help of а **chain-of-thought** (**CoT**) prompting strategy (*Chain-of-Thought
    Prompting Elicits Reasoning in Large Language Models*, https://arxiv.org/abs/2201.11903).
    This type of prompt presents the LLM with a series of intermediate steps that
    can guide the model to reach the final task answer. The following diagram shows
    a comparison between regular and CoT prompts:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.16 – Left: regular one-shot prompt; right: CoT one-shot prompt (inspired
    by https://arxiv.org/abs/2201.11903)](img/B19627_08_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.16 – Left: regular one-shot prompt; right: CoT one-shot prompt (inspired
    by https://arxiv.org/abs/2201.11903)'
  prefs: []
  type: TYPE_NORMAL
- en: It is speculated that this ability is obtained by including source code in the
    training data.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s also note that the alignment tuning we discussed in the *FT with RLHF*
    section is also an emergent ability, as it only improves the performance of large
    models.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following diagram shows how the performance on various tasks significantly
    improves with the scale of the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.17 – Emergent abilities are only present in large-scale models (source:
    https://arxiv.org/abs/2206.07682)](img/B19627_08_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 8.17 – Emergent abilities are only present in large-scale models (source:
    [https://arxiv.org/abs/2206.07682](https://arxiv.org/abs/2206.07682))'
  prefs: []
  type: TYPE_NORMAL
- en: 'The *x* axis shows the training computational time for each model (measured
    in FLOPS), and the *y* axis shows the model accuracy. The graphs show the model
    accuracy on three different benchmarks:'
  prefs: []
  type: TYPE_NORMAL
- en: An arithmetic benchmark that tests multiplication of 2-digit numbers, as well
    as the addition and subtraction of 3-digit numbers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 57 college-level tests covering a range of topics, including math, history,
    law, and more
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Chain-of-thought versus regular prompt on math word problems, like the one described
    in *Figure 8**.16*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This concludes our theoretical introduction to LLMs. Next, let’s see how to
    use them in practice.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing Hugging Face Transformers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we have discussed in depth the architecture and training properties
    of LLMs. But the sad truth is that these models are so large it is unlikely that
    you or I would build one from scratch. Instead, we’ll probably use a pre-trained
    model. In this section, we’ll see how to do that with the Hugging Face Transformers
    library ([https://github.com/huggingface/transformers](https://github.com/huggingface/transformers)).
    As the name suggests, its focus is the transformer architecture. It supports three
    different backends—PyTorch, TensorFlow, and JAX (as usual, we’ll focus on PyTorch).
    It is open source and available for commercial use. The company behind it, Hugging
    Face, also develops the Hugging Face Hub—a complementary service to the library
    cloud-based platform. It supports hosting and/or running Git repositories (such
    as GitHub), transformer models, datasets, and web applications (intended for **proof-of-concept**
    (**POC**) demos of ML applications). With that, let’s proceed with our first example.
  prefs: []
  type: TYPE_NORMAL
- en: 'We’ll start with a basic use case—we’ll load a pre-trained Llama 2 chat 7B
    model and use it to generate a response to the user’s prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we add in the `import` statements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we define the model’s name in a variable:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Every transformer model has a unique identifier, which works for the Hugging
    Face model hub. The Hub hosts all models, and the library can automatically download
    the model weights behind the scenes. In this case, we use the smallest Llama 2
    7B RLHF-optimized model to preserve computational resources.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, let’s load the model tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The various LLM models use different tokenizers. The `AutoTokenizer` instance
    can select the right one based on the model identifier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s see the `tokenizer` properties by printing it with `print(tokenizer)`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The tokenizer is based on a byte-level **byte-pair encoding** (**BPE**). This
    output gives us useful information about the token vocabulary size, special tokens,
    and other properties.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we create a `pipeline` instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The pipeline abstraction makes it easy to use the models for inference. The
    `task` parameter determines the type of task to solve. The library supports multiple
    tasks, also covering images and audio. `pipeline` will return different objects,
    depending on the task. It also takes care of downloading and initializing the
    model. In addition, we set the datatype to `torch.bfloat16` to reduce the memory
    footprint. The `device_map='auto'` parameter allows the Accelerate library ([https://github.com/huggingface/accelerate](https://github.com/huggingface/accelerate))
    to run the model across any distributed configuration automatically.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can see the model definition with the following command: `print(text_gen_pipeline.model)`.
    For example, the command output for the largest 70B Llama 2 model, `Llama-2-70b-hf`,
    is this:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To fit the page line length, I have modified the original output: `in` stands
    for `in_features`, `out` stands for `out_features`, and all linear layers have
    an additional `bias=False` parameter. The token vocabulary size is 32,000, and
    the embedding size (![<mml:math xmlns:mml="http://www.w3.org/1998/Math/MathML"
    xmlns:m="http://schemas.openxmlformats.org/officeDocument/2006/math"><mml:msub><mml:mrow><mml:mi>d</mml:mi></mml:mrow><mml:mrow><mml:mi>m</mml:mi><mml:mi>o</mml:mi><mml:mi>d</mml:mi><mml:mi>e</mml:mi><mml:mi>l</mml:mi></mml:mrow></mml:msub></mml:math>](img/844.png))
    is 8,192\. The model has 80 identical decoder blocks (`LlamaDecoderLayer`). Each
    block contains a'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: self-attention sublayer (`*_proj` are the projections), a FFN with a single
    hidden layer (`LlamaMLP`), rotary embeddings (`LlamaRotaryEmbedding`), `LlamaRMSNorm`),
    and SiLU activation (`SiLUActivation`). Let’s note that the activation differs
    from the SwiGLU activation defined in the paper.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Then, we run the inference:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here, `text_inputs` is the user prompt, which serves as the initial input sequence.
    The `num_return_sequences=2` parameter indicates that model will generate two
    separate responses (more on that later). Here’s the first response:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s analyze the rest of the arguments of the `text_gen_pipeline` call, as
    they all relate to the strategy of generating new tokens. The LLM ends with a
    softmax operation, which outputs a probability distribution over all tokens of
    the vocabulary. The simplest way to select the next token is a greedy strategy,
    which always takes the one with the highest probability. However, this is often
    suboptimal because it can hide high-probability words behind low-probability ones.
    To clarify, a token might be assigned a low probability at the current state of
    the generated sequence, and another token would be selected in its place. This
    means that the potential sequence, which includes the current low-probability
    token, will not exist. Therefore, even if it had high-probability tokens down
    the line, we would never know because the low-probability token blocks it from
    further exploration. One way to solve this is with a `do_sample=True`. In this
    case, the algorithm takes the probability of the entire current sequence rather
    than just the probability of the latest token. Therefore, the new token will be
    the one that maximizes the overall probability of the sequence instead of its
    local probability. The `num_beams=2` parameter indicates that the algorithm always
    keeps the two sequences (beams) with the highest probability. Since we can have
    more than one output sequence, the `num_return_sequences=2` parameter indicates
    the number of sequences to return. For example, if `num_beams=5` and `num_return_sequences=3`,
    the algorithm will return the three highest-probability sequences out of all five
    available (`num_return_sequences > num_beams` are invalid arguments). The `early_stopping=True`
    parameter indicates that the generation is finished when all beam hypotheses reach
    the end-of-sequence (`[EOS]`) token. The `top_k=10` parameter indicates that the
    algorithm will only sample the top 10 highest-probability tokens, regardless of
    their sequence probabilities. `top_p=0.9` is like `top_k`, but instead of sampling
    only from the most likely *k* tokens, it selects from the smallest possible set
    of tokens whose combined probability exceeds the probability *p*.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes our introduction to the Transformers library and the whole chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LLMs are very large transformers with various modifications to accommodate the
    large size. In this chapter, we discussed these modifications, as well as the
    qualitative differences between LLMs and regular transformers. First, we focused
    on their architecture, including more efficient attention mechanisms such as sparse
    attention and prefix decoders. We also discussed the nuts and bolts of the LLM
    architecture. Next, we surveyed the latest LLM architectures with special attention
    given to the GPT and LlaMa series of models. Then, we discussed LLM training,
    including training datasets, the Adam optimization algorithm, and various performance
    improvements. We also discussed the RLHF technique and the emergent abilities
    of LLMs. Finally, we introduced the Hugging Face Transformers library.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we’ll discuss transformers for **computer vision** (**CV**),
    multimodal transformers, and we’ll continue our introduction to the Transformers
    library.
  prefs: []
  type: TYPE_NORMAL
