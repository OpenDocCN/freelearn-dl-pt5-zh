["```py\ncosine_similarity = np.dot(Image_embeddings, Text_embeddings.T) * np.exp(learned_variable)\nlabels = np.arange(number_of_samples)\nloss_image = cross_entropy_loss(cosine_similarity, labels, axis=0)\nloss_text = cross_entropy_loss(cosine_similarity, labels, axis=1)\nloss = (loss_image + loss_text)/2\n```", "```py\n    import os\n    from tqdm import tqdm\n    import numpy as np\n    import clip\n    import torch\n    from PIL import Image\n    import pandas as pd\n    ```", "```py\n    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n    model, preprocess = clip.load('ViT-B/32', device)\n    ```", "```py\n    def predict_with_pil_image(image_input, clip_labels, top_k=5):\n      return similarity_score_probability, indices\n    ```", "```py\n    image_input = preprocess(cars_image).unsqueeze(0).to(\n    device)\n    text_inputs = torch.cat([clip.tokenize(cl) for cl in clip_labels]).to(device)\n    ```", "```py\n    with torch.no_grad():\n      image_features = model.encode_image(image_input)\n      text_features = model.encode_text(text_inputs)\n    ```", "```py\n    image_features /= image_features.norm(dim=-1, keepdim=True)\n    text_features /= text_features.norm(dim=-1, keepdim=True)\n    similarity_score_probability = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n    ```", "```py\n    percentages, indices = similarity[0].topk(5)\n    similarity_score_probability = percentages.numpy() * 100.0\n    indices = indices.numpy()\n    ```", "```py\n    image = Image.open('clips-data-2020/clips/clips-25001.png')\n    ```", "```py\n    raw_labels = list(range(100))\n    clip_labels = ['{} number of paper clips where some paper clips are partially occluded'.format(label) for label in raw_labels]\n    ```", "```py\n    percentages, indices = predict_with_pil_image(image, clip_labels)\n    print(\"\\nTop 5 predictions:\\n\")\n    for percent, index in zip(percentages, indices):\n        print(f\"{raw_labels[index]}: {percent:.2f}%\")\n    Top 5 predictions: 2: 4.64%, 4: 4.13% ,3: 4.03%, 0: 3.83%, 1: 3.67%\n    ```", "```py\n    medium_image = Image.open('clips-data-2020/clips/clips-25086.png'))\n    hard_image = Image.open('clips-data-2020/clips/clips-25485.png')\n    Top 5 predictions: 4: 3.98%, 3: 3.78%, 2: 3.71%, 6: 3.23%, 5: 3.16%\n    ```", "```py\n    Top 5 predictions: 18: 1.32%, 9: 1.29%, 0: 1.26%, 19: 1.22% ,16: 1.20%\n    ```", "```py\n    testing_data = pd.read_csv('train.csv')\n    errors = []\n    for idx, row in tqdm(testing_data.iterrows(), total=1000):\n         image = Image.open(\n          'clips-data-2020/clips/clips-     {}.png'.format(row['id'])\n         )\n        percentages, indices = predict_with_pil_image(image, clip_labels, 1)\n        errors.append(abs(row['clip_count'] - raw_labels[indices[0]]))\n        if idx == 1000:\n            break\n    print('{} average count error'.format(np.mean(errors)))\n    ```"]