<html><head></head><body>
		<div>
			<div id="_idContainer023" class="Content">
			</div>
		</div>
		<div id="_idContainer024" class="Content">
			<h1 id="_idParaDest-44"><a id="_idTextAnchor044"/>2. Perceptrons</h1>
		</div>
		<div id="_idContainer041" class="Content">
			<p>This chapter describes an algorithm called a <em class="italics">perceptron</em>. Invented by the US researcher Frank Rosenblatt in 1957, it is from this traditional algorithm that neural networks (i.e., deep learning) originated and is thus a necessary first step to the more advanced study of both. This chapter will describe a perceptron and use one to solve easy problems. Throughout this process, you will familiarize yourself with the mechanics of perceptrons.</p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor045"/>What Is a Perceptron?</h2>
			<p>A perceptron  receives multiple signals as inputs and outputs one signal. The "signal" here "flows" like an electric current or a river. In the same way that an electric current flows through a conductor and pushes electrons forward, the signal in a perceptron makes flow and transfers information. Unlike an electric current, the signal in a perceptron is binary: "Flow (1) or Do not flow (0)." In this book, 0 indicates "do not flow a signal" and 1 indicates "flow a signal."</p>
			<p>(In the interest of precision, note that the perceptron described in this chapter is more accurately called an "artificial neuron" or a "simple perceptron." Here, we will call it a "perceptron" because the basic processes are often the same.)</p>
			<p><em class="italics">Figure 2.1</em> shows an example of a perceptron that receives two signals as input:</p>
			<div>
				<div id="_idContainer025" class="IMG---Figure">
					<img src="image/fig02_1.jpg" alt="Figure 2.1: Perceptron with two inputs&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.1: Perceptron with two inputs</h6>
			<p><em class="italics">x</em><span class="P---Subscript">1</span> and <em class="italics">x</em><span class="P---Subscript">2</span> are input signals, <em class="italics">y</em> is an output signal, and <em class="italics">w</em><span class="P---Subscript">1</span> and <em class="italics">w</em><span class="P---Subscript">2</span> are weights (w is the initial letter of "weight"). The circle in the preceding diagram is called a "neuron" or a "node." When input signals are sent to a neuron, each of them is multiplied by its own weight (<em class="italics">w</em><span class="P---Subscript">1</span><em class="italics">x</em><span class="P---Subscript">1</span> and <em class="italics">w</em><span class="P---Subscript">2</span><em class="italics">x</em><span class="P---Subscript">2</span>). The neuron sums the signals that it receives and outputs 1 when the sum exceeds a certain limit value. This is sometimes called "firing a neuron." Here, the limit value is called a <strong class="bold">threshold</strong> and is represented by the <em class="italics">θ</em> symbol.</p>
			<p>This is all about the operating principle of a perceptron. Equation (2.1) shows what we described here:</p>
			<table id="table001" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer026">
									<img src="image/Firgure_2.1a.png" alt="1"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(2.1)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>A perceptron has a specific weight for each of multiple inputs, while the weight controls the importance of each signal. The larger the weight, the more important the signal for the weight.</p>
			<h4>Note</h4>
			<p class="callout">A weight is equivalent to electrical "resistance." Resistance is a parameter that measures the difficulty of passing an electric current. The smaller the resistance, the larger the current. Meanwhile, when the weight of a perceptron is larger, the signal that flows becomes larger. Both resistance and weight work in the same way in that they both control the difficulty (or ease) of passing a signal.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor046"/>Simple Logic Circuits</h2>
			<h3 id="_idParaDest-47"><a id="_idTextAnchor047"/>AND Gate</h3>
			<p>The following are some easy problems that use a perceptron. We will look at logic circuits here. Let's think about an AND gate first. An AND gate consists of two inputs and one output. The table of input and output signals in <em class="italics">Figure 2.2</em> is called a "truth table." As shown in <em class="italics">Figure 2.2</em>, the AND gate outputs 1 when two inputs are 1. Otherwise, it outputs 0:</p>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/fig02_2.jpg" alt="Figure 2.2: Truth table of an AND gate&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.2: Truth table of an AND gate</h6>
			<p>Now, we will use a perceptron to express this AND gate. We will determine the values of <em class="italics">w</em><span class="P---Subscript">1</span>, <em class="italics">w</em><span class="P---Subscript">2</span>, and <em class="italics">θ</em> so that they satisfy the truth table of <em class="italics">Figure 2.2</em>. What values can we set to create a perceptron that satisfies the conditions of <em class="italics">Figure 2.2</em>?</p>
			<p>Actually, there is an infinite number of combinations of the parameters that satisfy <em class="italics">Figure 2.2</em>. For example, when (<em class="italics">w</em><span class="P---Subscript">1</span>, <em class="italics">w</em><span class="P---Subscript">2</span>, <em class="italics">θ</em>) = (0.5, 0.5, 0.7), the perceptron works as shown in<em class="italics"> Figure 2.2</em>. (0.5, 0.5, 0.8) and (1.0, 1.0, 1.0) also satisfy the conditions of the AND gate. If these parameters are set, the sum of weighted signals exceeds the given threshold, <em class="italics">θ</em>, when both <em class="italics">x</em><span class="P---Subscript">1</span> and <em class="italics">x</em><span class="P---Subscript">2</span> are 1.</p>
			<h3 id="_idParaDest-48"><a id="_idTextAnchor048"/>NAND and OR gates</h3>
			<p>Now, let's look at a NAND gate. NAND means Not AND, and the output of the NAND gate is the opposite of the AND gate. As shown in the truth table provided in <em class="italics">Figure 2.3</em>, it outputs 0 when both <em class="italics">x</em><span class="P---Subscript">1</span> and <em class="italics">x</em><span class="P---Subscript">2</span> are 1. Otherwise, it outputs 1. What combinations of parameters are available for a NAND gate?</p>
			<div>
				<div id="_idContainer028" class="IMG---Figure">
					<img src="image/fig02_3.jpg" alt="Figure 2.3: Truth table of a NAND gate&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.3: Truth table of a NAND gate</h6>
			<p>A combination of (<em class="italics">w</em><span class="P---Subscript">1</span>, <em class="italics">w</em><span class="P---Subscript">2</span>, <em class="italics">θ</em>) = (-0.5, -0.5, -0.7) can represent a NAND gate, and there is an infinite number of other combinations. In fact, you can build a NAND gate by inverting all the signs of the parameter values that build an AND gate.</p>
			<p>Now, let's look at an OR gate, as shown in <em class="italics">Figure 2.4</em>. This is a logic circuit that outputs 1 if at least one of the input signals is 1. What parameters do you think we can set for the OR gate?</p>
			<div>
				<div id="_idContainer029" class="IMG---Figure">
					<img src="image/fig02_4.jpg" alt="Figure 2.4: Truth table of an OR gate&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.4: Truth table of an OR gate</h6>
			<h4>Note</h4>
			<p class="callout">We are the ones that determine the perceptron parameters here—not a computer. While looking at the "training data," also known as a truth table, we considered (found) the parameter values manually. In machine learning problems, we have a computer determines the parameter values automatically. <strong class="bold">Training</strong> is the task that determines the appropriate parameters, and we consider the structure (model) of the perceptron and give training data to the computer.</p>
			<p>As described previously, we can use a perceptron to build AND, NAND, and OR logic circuits. What is important here is that the structure of a perceptron is the same for all of the AND, NAND, and OR gates. The differences between the three gates lie in the parameter values (weights and thresholds). Just like a versatile actor plays a wide variety of characters, the perceptron of the same structure changes into AND, NAND, and OR when the parameter values are adjusted appropriately.</p>
			<h2 id="_idParaDest-49"><a id="_idTextAnchor049"/>Implementing Perceptrons</h2>
			<h3 id="_idParaDest-50"><a id="_idTextAnchor050"/>Easy Implementation</h3>
			<p>Let's implement the preceding logic circuits with Python. Here, we will define the AND function, which takes <strong class="inline">x1</strong> and <strong class="inline">x2</strong> as arguments:</p>
			<p class="source-code">def AND(x1, x2):</p>
			<p class="source-code">    w1, w2, theta = 0.5, 0.5, 0.7</p>
			<p class="source-code">    tmp = x1*w1 + x2*w2</p>
			<p class="source-code">    if tmp &lt;= theta:</p>
			<p class="source-code">        return 0</p>
			<p class="source-code">    elif tmp &gt; theta:</p>
			<p class="source-code">        return 1</p>
			<p>The <strong class="inline">w1</strong>, <strong class="inline">w2</strong>, and <strong class="inline">theta</strong> parameters are initialized within the function. When the sum of the weighted inputs exceeds the threshold, it returns 1; otherwise, it returns 0. Let's check that the outputs are the same as the ones shown in <em class="italics">Figure 2.2</em>:</p>
			<p class="source-code">AND(0, 0) # 0 (output)</p>
			<p class="source-code">AND(1, 0) # 0 (output)</p>
			<p class="source-code">AND(0, 1) # 0 (output)</p>
			<p class="source-code">AND(1, 1) # 1 (output)</p>
			<p>The outputs are as we expected. With that, you have built an AND gate. Although you can use a similar procedure to build a NAND or OR gate, we will change the implementation a little.</p>
			<h3 id="_idParaDest-51"><a id="_idTextAnchor051"/>Introducing Weights and Bias</h3>
			<p>Although the preceding implementation of an AND gate is simple and easy to understand, we will change it to a different implementation for the subsequent sections, switching <em class="italics">θ</em> in equation (2.1) to -b and representing the behavior of the perceptron in equation (2.2):</p>
			<table id="table002" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer030">
									<img src="image/Figure_2.4a.png" alt="2"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(2.2)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Although the notation of the symbols has changed, equations (2.1) and (2.2) represent exactly the same thing. Here, b is called a bias and <em class="italics">w</em><span class="P---Subscript">1</span> and <em class="italics">w</em><span class="P---Subscript">2</span> are called <strong class="bold">weights</strong>. As equation (2.2) shows, the perceptron sums the input signal values multiplied by the weights and the bias. It outputs 1 if the sum exceeds 0, and outputs 0 otherwise. Now, let's use NumPy to implement equation (2.2). We will use the Python interpreter to check the results one by one:</p>
			<p class="source-code">&gt;&gt;&gt; import numpy as np</p>
			<p class="source-code">&gt;&gt;&gt; x = np.array([0, 1]) # Input</p>
			<p class="source-code">&gt;&gt;&gt; w = np.array([0.5, 0.5]) # Weight</p>
			<p class="source-code">&gt;&gt;&gt; b = -0.7	# Bias</p>
			<p class="source-code">&gt;&gt;&gt; w*x</p>
			<p class="source-code">array([ 0. ,  0.5])</p>
			<p class="source-code">&gt;&gt;&gt; np.sum(w*x)</p>
			<p class="source-code">0.5</p>
			<p class="source-code">&gt;&gt;&gt; np.sum(w*x) + b</p>
			<p class="source-code">-0.19999999999999996 # About -0.2 (Operation error with floatingpoint numbers)</p>
			<p>As shown in this example, when NumPy arrays are multiplied, each of their elements is multiplied if the two arrays have the same number of elements. Therefore, when calculating <strong class="inline">w*x</strong>, each element is multiplied, ([0, 1] * [0.5, 0.5] =&gt; [0, 0.5]). In <strong class="inline">np.sum(w*x)</strong>, each element is summed. When the bias is added to this weighted sum, the calculation of equation (2.2) is complete.</p>
			<h3 id="_idParaDest-52"><a id="_idTextAnchor052"/>Implementation with Weights and Bias</h3>
			<p>You can use weights and bias to implement an AND gate, as follows:</p>
			<p class="source-code">def AND(x1, x2):</p>
			<p class="source-code">    x = np.array([x1, x2])</p>
			<p class="source-code">    w = np.array([0.5, 0.5])</p>
			<p class="source-code">    b = -0.7</p>
			<p class="source-code">    tmp = np.sum(w*x) + b</p>
			<p class="source-code">    if tmp &lt;= 0:</p>
			<p class="source-code">        return 0</p>
			<p class="source-code">    else:</p>
			<p class="source-code">        return 1</p>
			<p>Here, -<em class="italics">θ</em> is called the bias, <em class="italics">b</em>. Note that the bias works differently from the weights, <em class="italics">w</em><span class="P---Subscript">1,</span> and <em class="italics">w</em><span class="P---Subscript">2</span>. Specifically, <em class="italics">w</em><span class="P---Subscript">1</span> and <em class="italics">w</em><span class="P---Subscript">2</span> work as parameters that control the importance of input signals, while the bias works as the parameter that adjusts the ease of firing—that is, how likely it is that the output signal is 1. For example, if <em class="italics">b</em> is -0.1, the neuron fires when the weighted sum of the input signals exceeds 0.1. On the other hand, if <em class="italics">b</em> is -20.0, the neuron fires only when the weighted sum of input signals exceeds 20.0. Thus, the value of the bias determines how easily the neuron fires. Although <em class="italics">w</em><span class="P---Subscript">1</span> and <em class="italics">w</em><span class="P---Subscript">2</span> are called "weights" and <em class="italics">b</em> is called "bias," all the parameters (that is, <em class="italics">b</em>, <em class="italics">w</em><span class="P---Subscript">1</span>, and <em class="italics">w</em><span class="P---Subscript">2</span>) are sometimes called "weights," depending on the context.</p>
			<h4>Note</h4>
			<p class="callout">The word "bias" also means "padding." It indicates that the output is increased if nothing is input (if the input is 0). Actually, if inputs <em class="italics">x</em><span class="P---Subscript">1</span> and <em class="italics">x</em><span class="P---Subscript">2</span> are 0, the output is just the value of the bias when <em class="italics">b</em> + <em class="italics">w</em><span class="P---Subscript">1</span><em class="italics">x</em><span class="P---Subscript">1</span> + <em class="italics">w</em><span class="P---Subscript">2</span><em class="italics">x</em><span class="P---Subscript">2</span> is calculated in equation (2.2).</p>
			<p>Now, let's implement the NAND and OR gates:</p>
			<p class="source-code">def NAND(x1, x2):</p>
			<p class="source-code">    x = np.array([x1, x2])</p>
			<p class="source-code">    <strong class="inline">w = np.array([-0.5, -0.5])</strong> # Only the weights and bias are different from AND!</p>
			<p class="source-code">    <strong class="inline">b = 0.7</strong></p>
			<p class="source-code">    tmp = np.sum(w*x) + b</p>
			<p class="source-code">    if tmp &lt;= 0:</p>
			<p class="source-code">        return 0</p>
			<p class="source-code">    else:</p>
			<p class="source-code">        return 1</p>
			<p class="source-code">def OR(x1, x2):</p>
			<p class="source-code">    x = np.array([x1, x2])</p>
			<p class="source-code">    <strong class="inline">w = np.array([0.5, 0.5])</strong> # Only the weights and bias are different from AND!</p>
			<p class="source-code">    <strong class="inline">b = -0.2</strong></p>
			<p class="source-code">    tmp = np.sum(w*x) + b</p>
			<p class="source-code">    if tmp &lt;= 0:</p>
			<p class="source-code">        return 0</p>
			<p class="source-code">    else:</p>
			<p class="source-code">        return 1</p>
			<p>As described in the previous section, the AND, NAND, and OR gates are the same in terms of structure for the perceptron and differ only in terms of the values of the weight parameters. When implementing the NAND and OR gates, only the values of the weights and bias are different from the AND gate.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor053"/>Limitations of Perceptrons</h2>
			<p>As described so far, we can use a perceptron to implement AND, NAND, and OR logic gates. In this next section, you will consider an XOR gate.</p>
			<h3 id="_idParaDest-54"><a id="_idTextAnchor054"/>XOR Gate</h3>
			<p>An XOR gate is a gate circuit that is also called an<em class="italics"> exclusive OR</em>. As shown in <em class="italics">Figure 2.5</em>, the output is 1 when either <em class="italics">x</em><span class="P---Subscript">1</span> or <em class="italics">x</em><span class="P---Subscript">2</span> is 1 ("exclusive" means "limited to only one person"). What should be the value of the weights to realize the XOR gate by using a perceptron?</p>
			<div>
				<div id="_idContainer031" class="IMG---Figure">
					<img src="image/fig02_5.jpg" alt="Figure 2.5: Truth table of an XOR gate&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.5: Truth table of an XOR gate</h6>
			<p>In fact, we cannot build this XOR gate by using the perceptron that we have learned about so far. Why can we not build XOR even though we can build AND and OR gates? </p>
			<p>First, let's examine the behavior of an OR gate visually. An OR gate satisfies the truth table in <em class="italics">Figure 2.5</em> when the weight parameters are (<em class="italics">b</em>, <em class="italics">w</em><span class="P---Subscript">1</span>, <em class="italics">w</em><span class="P---Subscript">2</span>) = (-0.5, 1.0, 1.0), for example. In this case, the perceptron is represented by equation (2.3):</p>
			<table id="table003" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style CellOverride-1">
							<div>
								<div id="_idContainer032">
									<img src="image/Firgure_2.5a.png" alt="3"/>
								</div>
							</div>
						</td>
						<td class="No-Table-Style">
							<p>(2.3)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>The perceptron represented by equation (2.3) generates two areas that are divided by the straight line -0.5 + <em class="italics">x</em><span class="P---Subscript">1</span> + <em class="italics">x</em><span class="P---Subscript">2</span> = 0. One of the areas divided by the straight line outputs 1, while the other outputs 0. <em class="italics">Figure 2.6</em> shows this graphically:</p>
			<div>
				<div id="_idContainer033" class="IMG---Figure">
					<img src="image/fig02_6.jpg" alt="Figure 2.6: Visualizing a perceptron – the perceptron outputs 0 in the gray area, which satisfies the characteristics of an OR gate&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.6: Visualizing a perceptron – the perceptron outputs 0 in the gray area, which satisfies the characteristics of an OR gate</h6>
			<p>An OR gate outputs 0 when (<em class="italics">x</em><span class="P---Subscript">1</span>, <em class="italics">x</em><span class="P---Subscript">2</span>) = (0, 0) and outputs 1 when (<em class="italics">x</em><span class="P---Subscript">1</span>, <em class="italics">x</em><span class="P---Subscript">2</span>) = (0, 1), (1, 0), and (1, 1). Here, a circle indicates 0, and a triangle indicates 1. To create an OR gate, we must divide between circles and triangles with a straight line. The straight line can actually divide four points correctly.</p>
			<p>So, how about the case of an XOR gate? Can we create areas that divide between circles and triangles with a straight line, as in the case of an OR gate?</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/fig02_7.jpg" alt="Figure 2.7: Circles and triangles indicate the outputs of an XOR gate. &#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.7: Ci<a id="_idTextAnchor055"/>rcles and triangles indicate the outputs of an XOR gate. </h6>
			<p>However hard you may be trying to solve this, you cannot divide between circles and triangles with a straight line. One straight line cannot divide them.</p>
			<h3 id="_idParaDest-55"><a id="_idTextAnchor056"/>Linear and Nonlinear</h3>
			<p>You cannot divide between circles and triangles with a straight line. However, you can divide them if you can remove the restriction of a "straight line." For example, you can create the areas that divide between circles and triangles, as shown in <em class="italics">Figure 2.8</em>.</p>
			<p>The limit of a perceptron is that it can only represent the areas divided by a straight line. It cannot represent a curve, as shown in <em class="italics">Figure 2.8</em>. The areas divided by a curve in <em class="italics">Figure 2.8</em> are called <em class="italics">nonlinear</em> areas, while those divided by a straight line are called <em class="italics">linear</em> areas. The words <em class="italics">linear</em> and <em class="italics">nonlinear</em> are often used in machine learning. You can visualize them with <em class="italics">Figures 2.6</em> and<em class="italics"> 2.8</em>:</p>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/fig02_8.jpg" alt="Figure 2.8: A curve can divide between circles and triangles&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.8: A curve can divide between circles and triangles</h6>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor057"/>Multilayer Perceptrons</h2>
			<p>Unfortunately, we cannot use a perceptron to represent an XOR gate. However, this is not terrible news. Actually, the merit of a perceptron lies in the fact that multiple layers of perceptrons can be stacked (the outline of this section is that multiple layers can represent XOR). We will look at the stacking layers later. Here, we can consider the problem of the XOR gate from another viewpoint.</p>
			<h3 id="_idParaDest-57"><a id="_idTextAnchor058"/>Combining the Existing Gates</h3>
			<p>There are some methods we can follow to make an XOR gate. One of them is to combine the AND, NAND, and OR gates that we have created so far and wire them. Here, the AND, NAND, and OR gates are shown with symbols in <em class="italics">Figure 2.9</em>. The circle at the tip of the NAND gate in <em class="italics">Figure 2.9</em> indicates that an output has been reversed.</p>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/fig02_9.jpg" alt="Figure 2.9: Symbols of the AND, NAND, and OR gates&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.9: Symbols of the AND, NAND, and OR gates</h6>
			<p>Now, let's think about how we can wire AND, NAND, and OR to create an XOR gate. Note that you can assign AND, NAND, or OR to each of the <em class="italics">?</em> symbols in <em class="italics">Figure 2.10</em> to complete an XOR gate:</p>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/fig02_10.jpg" alt="Figure 2.10: Replace the &quot;?&quot; symbols with an AND, NAND, or OR gate to complete an XOR gate!&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.10: Replace the "?" symbols with an AND, NAND, or OR gate to complete an XOR gate!</h6>
			<p>To be specific, the limitation of a perceptron described in the previous section is that a single layer of a perceptron cannot represent an XOR gate or divide nonlinear areas. Here, we will see that an XOR gate can be built by combining perceptrons (i.e., stacking layers).</p>
			<p>The wiring in <em class="italics">Figure 2.11</em> can build an XOR gate. Here, <em class="italics">x</em><span class="P---Subscript">1</span> and <em class="italics">x</em><span class="P---Subscript">2</span> indicate input signals, while <em class="italics">y</em> indicates an output signal. <em class="italics">x</em><span class="P---Subscript">1</span> and <em class="italics">x</em><span class="P---Subscript">2</span> are the inputs to the NAND and OR gates, and the outputs of the NAND and OR gates are the inputs to the AND gate:</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/fig02_11.jpg" alt="Figure 2.11: A combination of the AND, NAND, and OR gates constructs an XOR gate&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.11: A combination of the AND, NAND, and OR gates constructs an XOR gate</h6>
			<p>Let's check that the wiring in <em class="italics">Figure 2.11</em> can really form an XOR gate. Assuming that the output of the NAND is <em class="italics">s</em><span class="P---Subscript">1</span> and that of the OR is <em class="italics">s</em><span class="P---Subscript">2</span>, we will complete the truth table.<em class="italics"> Figure 2.12</em> shows the results. When we look at <em class="italics">x</em><span class="P---Subscript">1, </span><em class="italics">x</em><span class="P---Subscript">2</span>, and <em class="italics">y</em>, we can see that they represent the outputs of the XOR:</p>
			<div>
				<div id="_idContainer039" class="IMG---Figure">
					<img src="image/fig02_12.jpg" alt="Figure 2.12: Truth table of an XOR gate&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.12: Truth table of an XOR gate</h6>
			<h3 id="_idParaDest-58"><a id="_idTextAnchor059"/>Implementing an XOR Gate</h3>
			<p>Now, we will use Python to implement the XOR gate represented by the wiring in <em class="italics">Figure 2.11</em>. By using the AND, NAND, and OR functions that we defined previously, we can implement this as follows:</p>
			<p class="source-code">def XOR(x1, x2):</p>
			<p class="source-code">    s1 = NAND(x1, x2)</p>
			<p class="source-code">    s2 = OR(x1, x2)</p>
			<p class="source-code">    y = AND(s1, s2)</p>
			<p class="source-code">    return y</p>
			<p>The XOR function outputs the results as expected:</p>
			<p class="source-code">XOR(0, 0) # 0 (output)</p>
			<p class="source-code">XOR(1, 0) # 1 (output)</p>
			<p class="source-code">XOR(0, 1) # 1 (output)</p>
			<p class="source-code">XOR(1, 1) # 0 (output)</p>
			<p>Now, we can build an XOR gate. After doing this, we will represent the XOR that we have just implemented with perceptrons (by showing neurons explicitly). <em class="italics">Figure 2.13</em> shows this representation.</p>
			<p>An XOR is a multilayer network, as shown in <em class="italics">Figure 2.13.</em> Here, we will call the leftmost column <strong class="inline">Layer 0</strong>, the next <strong class="inline">Layer 1</strong>, and the rightmost <strong class="inline">Layer 2</strong>.</p>
			<p>The perceptron in <em class="italics">Figure 2.13</em> differs in shape from the AND and OR perceptrons we have looked at so far (<em class="italics">Figure 2.1</em>). The AND and OR perceptrons are single-layer, while the XOR perceptron is two-layer. A perceptron with multiple layers is sometimes called a <strong class="bold">multilayered perceptron</strong>:</p>
			<div>
				<div id="_idContainer040" class="IMG---Figure">
					<img src="image/fig02_13.jpg" alt="Figure 2.13: Representation of an XOR by perceptrons&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 2.13: Representation of an XOR by perceptrons</h6>
			<h4>Note</h4>
			<p class="callout">Although the perceptron in <em class="italics">Figure 2.13</em> consists of three layers, we will call it a "two-layer perceptron" because only the two layers (between layers 0 and 1 and between layers 1 and 2) have weight. Some literature calls the perceptron in <em class="italics">Figure 2.13</em> a "three-layer perceptron" because it consists of three layers.</p>
			<p>A two-layer perceptron, as shown in <em class="italics">Figure 2.13</em>, sends and receives signals between the neurons in layers 0 and 1 and then between layers 1 and 2. The following describes this behavior in more detail:</p>
			<ol>
				<li>Two neurons in layer 0 receive input signals and send signals to the neurons in layer 1.</li>
				<li>The neurons in layer 1 send signals to the neuron in layer 2, which outputs y.</li>
			</ol>
			<p>The behavior of this two-layer perceptron can be compared to an assembly through a pipeline. The worker on the first level (or first layer) works on the "component" that arrives and passes it to the worker on the second level (the second layer) when the task is finished. The worker in the second layer works on the "component" that was received from the worker in the first layer to complete and ship (output) it.</p>
			<p>Thus, the perceptrons in an XOR gate "pass components" between workers. This two-layer structure enables perceptrons to build an XOR gate. This can be interpreted as "what cannot be achieved with a single-layer perceptron can be achieved by adding one layer." Perceptrons can provide a more flexible representation by stacking layers (deepening layers).</p>
			<h2 id="_idParaDest-59"><a id="_idTextAnchor060"/>From NAND to a Computer</h2>
			<p>Multilayer perceptrons can create more complicated circuits than those we have examined so far. For example, an adder circuit that add numbers can be created with perceptrons. An encoder that converts a binary number into a decimal number and a circuit that outputs 1 when certain conditions are met (circuit for parity checks) can be represented with perceptrons. As a matter of fact, we can even use perceptrons to represent a computer.</p>
			<p>A computer is a machine that processes information. When it receives input, a computer processes it in a certain way and outputs the result. Processing in a certain way means that both a computer and a perceptron have inputs and outputs and calculate them based on fixed rules.</p>
			<p>Although it seems that a computer conducts very complicated processes inside it, in fact (surprisingly), a combination of NAND gates can reproduce what a computer does. The surprising fact that NAND gates are all we need to create a computer means that perceptrons can also represent a computer because a NAND gate can itself be made with perceptrons. Simply put, if we can create a computer by combining NAND gates, we can also represent one by combining only perceptrons (a combination of perceptrons can be represented as one multilayer perceptron).</p>
			<h4>Note</h4>
			<p class="callout">You may find it hard to believe that a combination of NAND gates can create a computer. If you are interested in this topic, <em class="italics">The Elements of Computing Systems: Building a Modern Computer from First Principles</em> (The MIT Press) is recommended. This book aims to understand computers deeply. Under the motto "From NANDs to Tetris," it uses NANDs to create a computer that runs Tetris. If you read this book, you will realize that computers can be created from simple elements—that is, NANDs.</p>
			<p>Thus, multilayer perceptrons can achieve as complicated a representation as creating a computer. So, what perceptron structure can represent a computer? How many layers are needed to create a computer?</p>
			<p>The answer is that, theoretically, a computer can be created with two-layer perceptrons. It has been proven that any function can be represented with two-layer perceptrons (to be accurate, when an activation function is a nonlinear sigmoid function – refer to the next chapter for details). However, it will be a very laborious job to create a computer by specifying the appropriate weights in a structure of two-layer perceptrons. Actually, to start from low-level components such as NANDs to create a computer, creating the required components (modules) step by step is natural—starting from AND and OR gates and proceeding to half adders and full adders, <strong class="bold">arithmetic and logic units</strong> (<strong class="bold">ALUs</strong>), and a CPU. Therefore, creating a structure of many layers is a natural way when representing a computer with perceptrons.</p>
			<p>Although we will not create a computer in this book, please keep in mind that multilayer perceptrons enable nonlinear representations and that they can, in principle, represent what a computer does.</p>
			<h2 id="_idParaDest-60"><a id="_idTextAnchor061"/>Summary</h2>
			<p>In this chapter, we covered perceptrons. The perceptron is a very simple algorithm, so you should be able to understand how it works quickly. The perceptron is the basis of a neural network, which we will learn about in the next chapter. These points may be summed up in the following list:</p>
			<ul>
				<li>A perceptron is an algorithm with inputs and outputs. When it receives a certain input, it outputs a fixed value.</li>
				<li>A perceptron has "weight" and "bias" parameters.</li>
				<li>You can use perceptrons to represent logic circuits such as AND and OR gates.</li>
				<li>An XOR gate cannot be represented with a single-layer perceptron.</li>
				<li>A two-layer perceptron can be used to represent an XOR gate.</li>
				<li>A single-layer perceptron can only represent linear areas, while a multilayer perceptron can represent nonlinear areas.</li>
				<li>Multilayer perceptrons can represent a computer (theoretically).</li>
			</ul>
		</div>
		<div>
			<div id="_idContainer042" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer043" class="Content">
			</div>
		</div>
	</body></html>