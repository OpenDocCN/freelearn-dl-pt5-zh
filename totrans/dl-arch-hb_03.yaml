- en: '3'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Understanding Convolutional Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An MLP is structured to accept one-dimensional data and cannot directly work
    with two-dimensional data or higher-dimensional data without preprocessing. One-dimensional
    data is also called tabular data, which commonly includes categorical data, numerical
    data, and maybe text data. Two-dimensional data, or data with higher dimensions,
    is some form of image data. Image data can be in two-dimensional format when it
    is a grayscale formatted image, in three-dimensional format when it has RGB layers
    that closely represent what humans see, or in more than three dimensions with
    hyperspectral images. Usually, to make MLP work for images, you would have to
    flatten the data and effectively represent the same data in a one-dimensional
    format. Flattening the data might work well in some cases, but throwing away the
    spatial characteristics that define that image removes the potential of capturing
    that relationship to your target. Additionally, flattening the data doesn’t scale
    properly to large images. An important characteristic of image data is that the
    target to be identified can be present in any spatial position of the image. Simple
    MLPs are highly dependent on the position of the data input and won’t be able
    to adapt to the ever-changing positions and orientation of the target in an image.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is where the **convolutional neural network** (**CNN**) layer shines and
    is often the go-to method for experts dealing with image data using machine learning.
    To date, top convolutional architectures always exceed the performance of MLP
    architectures for image datasets. In this chapter, we will cover the following
    topics while focusing on CNNs:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the convolutional neural network layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the pooling layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a CNN architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Designing a CNN architecture for practical usage
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the CNN architecture families
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter includes some practical implementations in the **Python** programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`seaborn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`keras`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files for this chapter are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_2](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_2).'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the convolutional neural network layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let’s focus on the foundations of the convolutional layer, starting with
    *Figure 3**.1*, which shows the operational process of a convolutional filter.
    A filter is a small matrix of weights that’s used to extract features or patterns
    from an input array. A convolutional filter is a type of filter that slides over
    an image, performing convolution operations to extract features by calculating
    dot products:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.1 – Operation of a convolutional filter on an image of a \uFEFF\
    t-shirt from the Fashion MNIST dataset](img/B18187_03_1.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.1 – Operation of a convolutional filter on an image of a t-shirt from
    the Fashion MNIST dataset
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers are made out of multiple convolutional filters of the same
    sizes. Convolutional filters are the main pattern detectors in a CNN, where each
    filter will learn to identify multidimensional patterns that exist in an image.
    The patterns can range from low-level patterns such as lines and edges to mid-level
    patterns such as circles or squares and finally to high-level patterns such as
    specific t-shirt types or shoe types depending on the deepness level of the convolutional
    layer in a CNN. In [*Chapter 12*](B18187_12.xhtml#_idTextAnchor184), *Interpreting
    Neural Networks*, we will explore the patterns that are learned and evaluate the
    patterns qualitatively and objectively.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional filters can be in multiple dimensions but for plain images, two-dimensional
    convolution filters are more commonly used. Kernels are analogous to filters in
    terms of a CNN layer. These two-dimensional filters have the same number of weights
    as their size and apply a dot product on a part of the input image data with the
    same size to obtain a single value; subsequently, these are added with a bias
    term. By operating on the same operation using the filter in a sliding window
    manner systematically, from top to bottom and left to right with a defined stride,
    the convolutional filter will complete a forward pass and obtain a two-dimensional
    output with smaller dimensions. Here, stride means the number of pixel steps taken
    when sliding the filter systematically from left to right and top to bottom where
    the minimum has to be at least 1 pixel. This operation builds on the fact that
    the target can be present in any spatial position in an image and uses the same
    pattern identification method across the entire image.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 3**.1* shows a two-dimensional convolutional filter with the size of
    5x5 pixels, where there are 25 weight components and one bias component that could
    be learned, and the input image of a t-shirt, which has a size of 28x28 pixels.
    The filter size is something that can be configured to other values that typically
    range from 1 to 7 and is commonly a square but can be set to be irregular rectangular
    shapes. The typical filter size values might seem too small of a **receptive field**
    to identify high-level features that are capable of predicting a shirt in this
    picture, but when multiple filters are applied one after another in a non-cyclical
    manner, the filters at the end of the operation have the receptive field of a
    larger part of the image. Receptive field refers to the region of the input space
    that a convolutional filter can “see” or respond to. It determines the spatial
    extent of the input that influences a particular output unit. To capture low-level
    patterns, the filter must have a small receptive field, and to capture high-level
    patterns, the filter must have a large receptive field. *Figure 3**.2* depicts
    this concept for three filters applied one after another:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.2 – The second convolutional filter has a receptive field size of
    3x3 of the input data, even when it has only a 2x2 convolutional filter size](img/B18187_03_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.2 – The second convolutional filter has a receptive field size of 3x3
    of the input data, even when it has only a 2x2 convolutional filter size
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the sliding window from left to right and top to bottom is just a good
    way to visualize the process. However, in reality, this process can be done in
    parallel in one go to take advantage of GPU parallel processing. *Figure 3**.3*
    shows all the window positions of a convolutional filter with a 4x4-pixel dimension
    with a stride of 4 applied on the same 28x28 t-shirt image. This would result
    in a 7x7 data output size:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.3 – All window positions using a 4x4 convolutional filter on a 28x28
    image with a stride of 4 pixels in both dimensions](img/B18187_03_3.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.3 – All window positions using a 4x4 convolutional filter on a 28x28
    image with a stride of 4 pixels in both dimensions
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider a CNN layer with 16 filters with a size of 5x5 pixels with a stride
    of 1 pixel. Running a forward pass of the t-shirt image with this layer configuration
    will result in a total of 16x26x26 (depth x width x height) pixels of output.
    In the case where the image is an RGB image with three channels colored red, green,
    and blue, since the convolutional filters are two-dimensional, the same filter
    would be applied similarly to each of the three channels using a standard convolutional
    layer. The three-dimensional outputs from the filters, applied separately on the
    three channels, will then be added up. The number of convolutional filters will
    be the output data channel size and will serve as the input channel size for the
    subsequent convolutional layers. *Figure 3**.4* depicts this channel-wise addition
    process for a 3x3 output from a convolutional filter. Note that the bias is only
    added once per filter across the channels and not by channel:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.4 – Aggregation of a multichannel 3x3 output from a convolution
    filter](img/B18187_03_4.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.4 – Aggregation of a multichannel 3x3 output from a convolution filter
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding figure, **Conv** is short for *convolutional layer* and will
    be a convention that will be used in the rest of this chapter to simplify figures.
    Since it is beneficial to be aware of the size of your neural network to ensure
    you have the computational resources to hold and process the network, let’s also
    compute the number of parameters that will be held by this convolutional layer.
    The number of parameters can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: number of input channels x number of filters x ((width of filter x height of
    filter) + 1)
  prefs: []
  type: TYPE_NORMAL
- en: By putting the respective numbers into the equation, you will obtain 416 parameters.
    If these weights are stored as a **floating-point 32** (**fp32**) format, in bytes,
    this would mean 416x32 bits/8=1,664 bytes. With an output data size of 16x26x26,
    the data size in spatial dimension is decreasing at a very slow rate and since
    the end goal will be to reduce these values to the size of the target data, we
    need something to reduce the size of the data. This is where another layer, called
    the **pooling layer**, comes into play.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the pooling layer
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With just a forward pass from a CNN layer of an image, the size of the two-dimensional
    output data is likely reduced but is still a substantial size. To reduce the size
    of the data further, a layer type called a pooling layer is used to aggregate
    and consolidate the values strategically while still maintaining useful information.
    Think of this operation as an image-resizing method while maintaining as much
    information as possible. This layer has no parameters for learning and is mainly
    added to simply and meaningfully reduce the output data. The pooling layer works
    by applying a similar sliding window filter process with similar configurations
    as the convolutional layers but instead of applying a dot product and adding a
    bias, a type of aggregation is done. The aggregation function can be either maximum
    aggregation, minimum aggregation, or average aggregation. The layers that apply
    these aggregations are called max pooling, min pooling, and average pooling, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: Consider an average pooling layer with a filter size of 4 and a stride of 2
    applied after the first CNN layer. A forward pass of the 16x26x26 output will
    result in a data size of 16x12x12\. That reduces the size considerably!
  prefs: []
  type: TYPE_NORMAL
- en: Another type of pooling layer applies the aggregation function globally. This
    means that the entire two-dimensional width and height component of the data will
    be aggregated into a single value. This variation of the pooling layer is commonly
    known as the global pooling layer. This layer is applied to completely break down
    the data into a one-dimensional structure so that it can be compatible with one-dimensional
    targets. This layer is directly available in the `keras` library but only available
    in `pytorch` indirectly through setting the pooling filter size to the same as
    the size of the input feature map.
  prefs: []
  type: TYPE_NORMAL
- en: Building a CNN architecture
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNN architectures are commonly made by stacking multiple conceptual logical
    blocks of layers one after another. These logical blocks are all structured the
    same way, with the same type of layer and layer connections, but they can be different
    in terms of their parameter configurations, such as the size of the filters, the
    stride, the type of padding used, and the amount of padding used. The simplest
    logical convolutional block is a convolutional layer, pooling layer, and activation
    function, in that order. **Padding** is a term that’s used to refer to any extra
    pixels that are added around the input image to preserve its spatial dimensions
    after convolution. Logical blocks are a way for you to describe and reference
    the architecture simply and efficiently. They also allow you to build CNN architectures
    in a depth-wise scalable way without the need to create and set the settings of
    each layer one by one. Depth is the same as deepness and refers to the number
    of neural network layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The parameters can be designed depending on whether the goal is to gradually
    scale down the feature maps or to scale up the feature maps. For the case of one-dimensional
    targets, the goal is likely to slowly scale down the features into one-dimensional
    features so that the features can be passed into fully connected layers. These
    fully connected layers can then further map the (still large) dimensions to the
    dimensions that are suitable for the targets. You can see a simple design of such
    an architecture in *Figure 3**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.5 – A simple CNN architecture from scratch while following the logical
    block analogy](img/B18187_03_5.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.5 – A simple CNN architecture from scratch while following the logical
    block analogy
  prefs: []
  type: TYPE_NORMAL
- en: 'In the code for `pytorch`, this example would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Again, the backpropagation of a CNN architecture will be handled automatically
    by the deep learning libraries.
  prefs: []
  type: TYPE_NORMAL
- en: The architecture we built was based on the basic classification problem type,
    which justifies the need to have a fully connected network at the end of the network;
    this is commonly known as the head. The set of logical blocks with convolutional
    layers that were used is called the backbone of the network. The head of the network
    can be switched out with other structures, depending on the problem type, but
    the backbone can be completely adapted into most architectures for any problem
    type, such as object detection, image generation, image captioning, or image recognition
    by representation learning. Some of these problem types will be discussed in [*Chapter
    8*](B18187_08.xhtml#_idTextAnchor125), *Exploring Supervised* *Deep Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have successfully made a simple CNN manually, we have grounded and
    synced our theories toward the core algorithm of convolutional networks and will
    be ready to have more advanced CNN backbone architecture designs at our fingertips.
    But before that, this begs the question, how do we design a CNN for our use case?
  prefs: []
  type: TYPE_NORMAL
- en: Designing a CNN architecture for practical usage
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For real-world use cases, CNNs should not be designed similarly to how an MLP
    is designed. Practical usage means that the goal is not to research a new innovative
    architecture for an unexplored problem type. Many advancements have been made
    today based on CNNs. Advancements usually come in one of two flavors:'
  prefs: []
  type: TYPE_NORMAL
- en: It sets a new baseline that completely redesigned the way CNN architectures
    are made
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s built on top of existing baseline CNN architectures while complementing
    and improving the performance of the baseline architecture
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The key difference between the ideal design approach of a CNN compared to an
    MLP is that the structures of published CNN architectures should be used instead
    of designing the architecture from scratch. The structures of CNN architectures
    define the type of layers and the way the different types of layers connect; they
    are usually implemented using logical blocks. Additionally, the uniqueness of
    a certain structure defines the family of CNN architecture. A new CNN research
    advancement usually comes in different size configurations so that architectures
    of suitable sizes can be chosen based on either the compute resource limitations,
    runtime requirements, or the dataset and problem complexity. Similar to the design
    of MLP, if resources and runtime are not a limitation, after choosing a CNN architecture
    structure, start with a reasonably small-sized CNN based on the complexity of
    the problem and the dataset’s size. You can gradually test bigger CNNs to see
    if the performance improves when you increase the size, or vice versa if the performance
    decreases.
  prefs: []
  type: TYPE_NORMAL
- en: Different CNN architecture structures or architecture families are usually meant
    to capture different inherent architectural issues of the network. Some architectural
    families are designed in a way that they leverage better hardware resources, without
    which it wouldn’t have been even possible to execute the architecture. A good
    practice to achieve good performance is to diversify the type of architectural
    structures you are using in the initial stage. Pick size variants of architectural
    structures with similar sizes in terms of the floating-point operation per second
    and run your experiments to obtain performance scores, ideally with a small size
    to maximize the efficiency of the exploration. Instead of the number of parameters
    of the model, it is more relevant to consider the floating-point operation per
    second as an indicator of the complexity of the model; parameter count doesn’t
    consider the actual runtime of the model, which could benefit from parallelization.
    Once you’ve obtained these numbers, pick the top model families and try bigger
    size variants to benchmark with to find the best model variant for your use case.
  prefs: []
  type: TYPE_NORMAL
- en: Most research improvements on CNNs are based on a simple baseline architecture.
    This means that all the other individual improvements that are made on the same
    baseline architecture are not benchmarked together. Testing these improvements
    together can often be complementary but sometimes, it can be detrimental to the
    metric performance of the model. Iteratively benchmarking the different configurations
    will likely be the most systematic and grounded way to obtain a satisfactory performance
    improvement on your model.
  prefs: []
  type: TYPE_NORMAL
- en: How do researchers grade their improvements? To design a CNN architecture for
    practical usage, knowing how to evaluate your architecture will help you slowly
    work toward an acceptable metric performance. In [*Chapter 10*](B18187_10.xhtml#_idTextAnchor161),
    *Exploring Model Evaluation Methods*, we will discuss the strategies for evaluation
    more extensively. One of the main evaluation methods for improvements that’s used
    for CNN improvements is the top-1 predicted accuracy performance on a massive
    publicly available image dataset called `ImageNet`, which consists of millions
    of images with many classes. `ImageNet` is considered to be a highly complex problem-type
    use case, where each class has an infinite amount of variations possible in the
    wild, from indoors to outdoors and from real to synthetic data.
  prefs: []
  type: TYPE_NORMAL
- en: So, how do we consider whether improvements are valuable? Improvements are made
    to either improve performance in the top-1 accuracy, based on the `ImageNet` dataset,
    to improve the efficiency of running a forward pass of the model, or to specifically
    improve the training time of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Ranking architectures by their top-1 accuracy metric performance improvements
    on `ImageNet` alone, however, is a biased evaluation as, more often than not,
    the absolute ranking of models differs when applying the same architectures on
    a separate image dataset. Using it as a starting point to choose ready-made architectures
    is wise, but make sure you evaluate a few of the top-performing `ImageNet` architectures
    to figure out which works the best. Furthermore, while `ImageNet` was curated
    with manual effort, involving querying search engines and passing candidate images
    through a validation step on Amazon Mechanical Turk, it still contains some label
    noise that can obfuscate the meaning behind the metric’s performance.
  prefs: []
  type: TYPE_NORMAL
- en: As for the improvement direction of increasing the efficiency of running a forward
    pass of the model, this is usually done either by reducing the number of parameters
    of the architecture, breaking down a compute-intensive logical component into
    multiple components that reduces the amount of operations, or switching out layers
    for layers that have higher parallelism potential. The improvements that are made
    in this direction either maintain or improve the metric performance score on the
    validation dataset of `ImageNet`. This line of improvement is a main focus for
    the family of CNN architectures that have been built to run in low-resource edge
    devices. We will dive into this later.
  prefs: []
  type: TYPE_NORMAL
- en: An efficient way to explore different CNN architecture families in your use
    case to achieve better metric performance is to pick model families that have
    a publicly available implementation complete with pre-trained weights trained
    on `ImageNet`. Initializing your architecture using pre-trained weights presents
    a bunch of benefits that include faster training, less overfitting, and increased
    generalization, even when the dataset that was used to pre-train the weights is
    part of a different problem subset. This process is called transfer learning and
    we will learn about it in more detail in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125),
    *Exploring Supervised Deep Learning*, and [*Chapter 9*](B18187_09.xhtml#_idTextAnchor149),
    *Exploring Unsupervised* *Deep Learning*.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the CNN architecture families
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now, instead of going through the history of CNN through the years, let’s look
    at a list of the different handpicked model architecture families. These architecture
    families are selectively chosen to be sufficiently different and diverse from
    each other. One thing to note is that neural networks are advancing at an astounding
    pace. With that in mind, the architecture families that will be introduced here
    are ensured to be relevant today. Additionally, only the most important information
    you need to know about the architecture family will be presented, simplifying
    the many pages of research papers in concise but sufficient detail.
  prefs: []
  type: TYPE_NORMAL
- en: Another thing to note before diving into this topic is that the metric performance
    on a dataset will often be the main comparison method among different architectures,
    so be aware that the metric performance of a model is achieved by the collective
    contribution of the training method and the architecture. The training method
    includes details not specifically related to the architecture of the model, such
    as the loss used, data augmentation strategy, and data resolution. These topics
    will be covered in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125), *Exploring
    Supervised Deep Learning*. The architecture families we’ll introduce here are
    **ResNet**, **DenseNet**, **MobileNet**, **EfficientNet**, **ShuffleNet**, and
    **MicroNet**.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the ResNet model family
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The ResNet architecture, from 2015, was made based on the pretext that deep
    networks are hard to train and aimed to change that. The vanishing gradient is
    a widely known problem that neural networks face, where the information from the
    data diminishes the deeper the network is. Plain deep CNN architectures, however,
    were identified to *not* suffer from vanishing gradients, with proof from verifying
    gradient information. The key cause of vanishing gradients is when we use too
    many activation functions that squish the data into very small value ranges. An
    example of this is the sigmoid function, which maps data from 0 to 1\. So, when
    in doubt, use ReLU in your architecture!
  prefs: []
  type: TYPE_NORMAL
- en: 'The *Res* part of ResNet is named after the term **residuals**. The idea was
    that learning from residuals is much easier than learning from unmodified feature
    maps. Residuals are achieved by adding skip connections from the earlier layers
    to later layers. In layman’s terms, that means adding (not concatenating) feature
    maps from earlier parts of the network to feature maps of later parts of the network.
    The result of this addition creates the residuals that will be learned by subsequent
    convolutional layers, which again apply more skip connections and create more
    residuals. Residuals can be easily applied to any CNN architecture with different
    configurations and can be considered an improvement on top of older baseline architectures.
    However, the authors also presented multiple variations of an architecture that
    utilized skip connections with a different number of convolutional layers, including
    ResNet-18, ResNet-34, ResNet-50, ResNet-101, and ResNet-152\. The ResNet architecture
    family serves as a boilerplate for easy usage of residual networks and eventually
    led to it becoming the most popular baseline for research on new advancements.
    The actual architecture designs are not presented here formally as memorizing
    those designs won’t have any impact on your grasp of CNN knowledge. Instead, *Figure
    3**.6* shows the residual computation of a single logical block:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.6 – Example of the actual residual connection method in the ResNet
    model family](img/B18187_03_6.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.6 – Example of the actual residual connection method in the ResNet
    model family
  prefs: []
  type: TYPE_NORMAL
- en: 'To summarize the entire baseline ResNet size variants, the following table
    shows a summary configuration of all the different size variants:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.7 – Base ResNet different size variants](img/B18187_03_7.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.7 – Base ResNet different size variants
  prefs: []
  type: TYPE_NORMAL
- en: The brackets denoted in the table are meant to signify the set of serial convolutional
    layers that are defined as the logical blocks. ResNet has also gone the extra
    mile to group multiple base logical blocks to a higher-level category they call
    “layer name.” The layer name has the same number of groups across different size
    variants. Logical blocks and higher-level grouping methods are just a way for
    you to describe and reference the complicated architecture simply and efficiently.
    The first two numbers, which with multiplication in between in the logical block,
    define the convolution filter size; the number that comes after it with a comma
    defines the number of filters.
  prefs: []
  type: TYPE_NORMAL
- en: 'Skip connections were found to smoothen the loss landscape of any neural network,
    making the learning process much easier and stabler. This allows for the neural
    network to converge to the optimum more easily. *Figure 3**.8* shows how the loss
    landscape for a neural network without skip connections has an uneven terrain
    with many hills and valleys on the left. It becomes a smooth terrain with an obvious
    single valley when skip connections are added using a variant of ResNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.8 – Loss landscape with no skip connections on the left and with
    skip connections on the right](img/B18187_03_8.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.8 – Loss landscape with no skip connections on the left and with skip
    connections on the right
  prefs: []
  type: TYPE_NORMAL
- en: One notable piece of information you can derive from the table is that, starting
    from ResNet-50 onwards, the architecture utilizes 1x1 convolutional filters. As
    this filter operates exclusively with a single-dimensional filter weight across
    the channel dimension, the operation is equivalent to applying a fully connected
    layer in a windowing fashion in the channel dimension. Since the fully connected
    layer is a network itself, along with the convolutional network, this operation
    is often called a **network in a network**. Next, let’s explore the different
    improvements with the ResNet architecture as a base.
  prefs: []
  type: TYPE_NORMAL
- en: Improving ResNets
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'As mentioned previously, ResNet is considered a model family and contains many
    different varieties, not just by size but with different architectural improvements.
    Any CNN architecture that is based on the ResNet belongs to this model family.
    Here, we have mentioned some of the few notable architectural advancements and
    provided descriptions of their main advancements based on ResNet; they are ordered
    by the year they were made:'
  prefs: []
  type: TYPE_NORMAL
- en: '`ImageNet` compared to ResNet variants, even when the number of parameters
    is the same and also in general.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Squeeze-and-Excitation Networks** (**SE-ResNet**) (2017): Since two-dimensional
    convolutions don’t consider the inter-channel relationships and only consider
    the local and spatial (width and height) information of the feature maps, a method
    to leverage the channel relationships was made that includes a squeeze block and
    an excitation block. This is a method that can be repeatedly applied to many parts
    of an existing CNN architecture. *Figure 3**.9* shows the structure of this method,
    where a global average pooling is applied in the width and height dimensions and,
    subsequently, two fully connected networks are applied to scale down and scale
    up the features back to the same size to allow channel information to be combined:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.9 – Squeeze and excitation structure](img/B18187_03_9.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.9 – Squeeze and excitation structure
  prefs: []
  type: TYPE_NORMAL
- en: The scaling part of the structure is where the values are multiplied together.
    When combined with ResNet, the architecture is called SE-ResNet.
  prefs: []
  type: TYPE_NORMAL
- en: '**ResNet-D** (2019): This advancement made simple architecture parameter tweaks
    to improve metric performance while maintaining the number of parameters, albeit
    at the slightly justifiable increase of the FLOP specification. As some path of
    ResNet utilizes 1x1 convolutions with a stride of 2, ¾ of the information is discarded
    and one of the tweaks was to the stride sizes to ensure no information gets removed
    explicitly. They also reduced computational load by changing a 7x7 convolution
    to three separate 3x3 convolutions serially.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**ResNet-RS** (2021): This advancement combines ResNet-D and the squeeze and
    excitation network and uses an image size that depends on the size of the network.
    However, it grows slower than EfficientNets (these will be introduced later).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding the DenseNet architecture family
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**DenseNet** is an architecture family that was introduced in the early months
    of 2018\. The architecture is based on the idea of skip connections, similar to
    the ResNet architecture family but on steroids, which means it uses a lot of skip
    connections. The skip connections in this family of architecture differ in that
    they use **concatenation** instead of residual connections by **summation**. Summation
    allows earlier information to be directly encoded into the outputs of future layers
    without the need to modify the number of neurons, albeit at the slight informational
    disadvantage of needing the future layers to learn to decode this information.
    Concatenation adds to the size of the architecture as you need to create extra
    neurons to account for the extra information, allowing the model to work on the
    raw data. Both provide similar advantages to using skip connections. Logical blocks
    are created called **dense blocks**, where each subsequent layer in the block
    has access to all the outputs of the layers before it in the block by feature
    map concatenation. In these blocks, **zero-padding** is used to ensure that the
    spatial dimensions of the outputs of each layer are maintained so that feature
    maps can be concatenated. This setup promotes a lot of feature reuse between layers
    in the same block and allows the number of model parameters to stay the same while
    increasing the model’s learning capacity. The number of filters for each subsequent
    layer in a block for all blocks is fixed at a constant number called the **growth
    rate**, as there needs to be a structured way to add layers to not exponentially
    increase the number of channels. Taking a constant of 32 filters, the input of
    the second layer in the block will be a feature map with 32 channels, the input
    of the third layer in the block will be 64 and concatenated, and so on.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To create a full network architecture, multiple dense blocks were stacked one
    after another with a separate convolutional layer and pooling layer in between
    the dense blocks to gradually reduce the spatial dimensions of the feature map.
    *Figure 3**.10* shows the network structure of the four different DenseNet model
    architectures under the DenseNet model family:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.10 – The DenseNet model family where “conv” corresponds to sequential
    layers of batch normalization, ReLU, and the convolutional layer](img/B18187_03_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.10 – The DenseNet model family where “conv” corresponds to sequential
    layers of batch normalization, ReLU, and the convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: This allows DenseNet to improve upon its predecessor network architectures in
    terms of top-1 `ImageNet` accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the EfficientNet architecture family
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Made in 2020, EfficientNet created a family of architectures by using an automated
    **neural architecture search** (**NAS**) method to create a small base efficient
    architecture and utilize an easy-to-use compound scaling method to scale the depth
    and width of the architecture, as well as the resolution of the image. The neural
    architecture search is searched in a way that it balances FLOPS and accuracy.
    The NAS method that's used is from another research called **MnasNet** and will
    be introduced properly in [*Chapter 7*](B18187_07.xhtml#_idTextAnchor107)*, Deep
    Neural* *Architecture Search*.
  prefs: []
  type: TYPE_NORMAL
- en: 'The compound scaling method is simple and can be extended to any other network,
    though ResNet-RS demonstrates that scaling resolution slower provides more value.
    The scaling method for depth, width, and resolution is defined in the following
    equation, where the result will be multiplied by the original base architecture
    parameters to scale up the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: depth = α φ , width = β φ, resolution = γ φ
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, φ is the coefficient that can be scaled to different values based on
    requirements and the other variables are constants that should be set to optimize
    something such as the FLOPS increase rate when we change the coefficient. For
    EfficientNet, the constants are constrained to satisfy the following condition:'
  prefs: []
  type: TYPE_NORMAL
- en: α ∙ β 2 ∙ γ 2 ≈ 2
  prefs: []
  type: TYPE_NORMAL
- en: 'This will constrain the increase of the coefficient to approximately increase
    the FLOPS by 2 φ. EfficientNet sets this to α = 1.2, β = 1.1, γ = 1.15 . This
    compound scaling strategy allowed seven EfficientNets to be made, named B0 to
    B7\. *Figure 3**.11* shows the structure of the EfficientNet-B0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.11 – EfficientNet-B0 architecture structure](img/B18187_03_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.11 – EfficientNet-B0 architecture structure
  prefs: []
  type: TYPE_NORMAL
- en: Note that **MBConv** is also known as an inverted residual block and will be
    properly introduced in another network later in the *Understanding* *MobileNetV2*
    section.
  prefs: []
  type: TYPE_NORMAL
- en: '**EfficientNetV2**, made in 2021, identified that large image resolution slows
    down training time, where depthwise convolutions are slow in early layers, and
    scaling up depth, width, and resolution at the same time is not optimal. EfficientNetV2
    also uses NAS to find a base architecture but with the addition of a modification
    of an MBConv block that has more parameters and operations with the reason that
    it can be faster sometimes, depending on the input and output data shape, where
    they are in the entire architecture, and how the data is transferred to the computing
    processer. This will be explained in more detail when we formally introduce MBConv
    later. EfficientNetV2 also utilizes the original compound scaling method but adds
    a few improvements by setting a maximum image resolution to 480 and adding extra
    layers to the last few stages of the architecture when it wants to increase the
    network capacity to higher proportions. A new training method was added to reduce
    training time; we’ll discuss this in more detail in the next chapter. These improvements
    resulted in four different EfficientNetV2 models called **EfficientNetV2-S**,
    **EfficientNetV2-M**, **EfficientNetV2-L**, and **EfficientNetV2-XL**. These exceeded
    the top-1 accuracy compared to the original EfficientNet at similar FLOP values:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.12 – EfficientNetV2-S architecture structure](img/B18187_03_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.12 – EfficientNetV2-S architecture structure
  prefs: []
  type: TYPE_NORMAL
- en: EfficientNetV2-S served as the base architecture, similar to how EfficientNetB0
    was the base, where the architecture structure is shown in *Figure 3**.12*.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding small and fast CNN architecture families for small-scale edge
    devices
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'One of the clear groups of architectures that holds a place in the world of
    CNN architectures is the group that is built not for scalability purposes or beating
    the `ImageNet` benchmark, but to be used for small devices. Small devices are
    usually called **edge devices** as they are small and compact enough to be mobile
    or to be physically deployed with actual data processing capabilities where the
    data originated. Our mobile smartphones are examples of mobile devices that are
    capable of producing images. Examples of edge devices that *aren’t* mobile include
    CCTV video cameras and doorbell cameras. Some of the key benefits of deploying
    models to the edge are listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Reduced communication latency**: Images and real-time video feeds are large
    compared to simple numerical or categorical data. By using computation at the
    edge, less data needs to be transferred to a centralized server, thus reducing
    the time needed to transfer data. Sometimes, the need for a transfer can be eliminated
    when computation is done at the edge, thus significantly simplifying a system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Reduced bandwidth requirements**: When the images are processed where they
    are produced, only simple data formats such as numerical or categorical will need
    to be returned, thus removing the need to have expensive equipment for high bandwidth
    requirements.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Increased redundancy**: A centralized server also means a single point of
    failure. Distributing processing to the individual edge devices makes sure the
    failure of any one device won’t affect the entire system.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Small CNN architectures meant for edge devices typically engineer the entire
    structure of a model without a single notable baseline, so there isn’t a proper
    model family to categorize each of these architectures. For ease of referencing
    architectures meant for this purpose, it is recommended to think of the following
    architectures as under *architecture for the edge.* There are other techniques
    that can be applied architecture-wide to further optimize the efficiency of the
    model that we will explore in [*Chapter 15*](B18187_15.xhtml#_idTextAnchor217)*,
    Deploying Deep Learning Models in Production*, but for now let’s explore two of
    these architectures, namely **SqueezeNet** and **MobileNet**.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding SqueezeNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'SqueezeNet was developed in 2016 to build small and fast CNNs with the benefits
    described in the previous section but with an emphasis on deploying to hardware
    with limited memory. The three strategies that can be employed are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: For a convolutional layer with 3x3 filters, replace the filters partially with
    1x1 filters. This means that the 1x1 filter and 3x3 filters co-exist theoretically
    in a single layer applied to the same input. However, the implementation details
    will differ as parallel branch paths for the 1x1 filters and 3x3 filters are applied
    on the same input. This is called the expand layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decrease the data input channels that are passed into the parallel 1x1 and 3x3
    filters by using a small number of 1x1 filters. This is called the squeeze layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Downsample the feature maps late in the architecture so that the majority of
    the convolutional layers have access to large feature maps based on findings and
    improve metric performance. This is done by using pooling layers with strides
    of 2 pixels less frequently in earlier stages and more frequently in later stages
    and larger intervals. Pooling layers are not used after every convolutional layer.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A logical block called `Fire` was made that provides the ease of creating multiple
    blocks to create architectures. This block enabled the configuration of the number
    of 1x1-sized filters in the squeeze layer, 1x1 filters in the expand layer, and
    3x3 filters in the expand layer. This is depicted in *Figure 3**.13*. Note that
    padding is applied to the outputs of the 3x3 convolutional layer to ensure it
    can be concatenated with the outputs from the 1x1 convolutional layer in the expand
    layer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.13 – The Fire module/logical block](img/B18187_03_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.13 – The Fire module/logical block
  prefs: []
  type: TYPE_NORMAL
- en: Eight `Fire` blocks were used to build an architecture called SqueezeNet that
    maintained the performance of `ImageNet` from the historical architecture known
    as **AlexNet** (not introduced in this book as it is not practically useful nowadays)
    while being 50x smaller.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MobileNet
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The first MobileNet version, called MobileNetV1, was introduced in 2017 for
    mobile and embedded devices by focusing on optimizing latency issues and getting
    small-sized networks as a side effect instead of vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: 'The depthwise separable convolutional layer was used extensively in the entire
    architecture except in the first layer in the first iteration of MobileNet, which
    consisted of 28 layers. This layer is built on factorizing the standard convolutional
    layer into two layers called the depthwise convolution and the pointwise convolution.
    This new two-layer setup is based on the idea that using a standard convolutional
    filter is expensive to compute. The depthwise convolutional layer uses one unique
    filter for one unique input channel, where each input channel has only one filter.
    Having one filter per channel is a unique case of `ImageNet` by 1%. This breakdown
    also reduces the number of parameters by around 5 times. This process is essentially
    a kind of factorization. The depthwise convolutional logical block is depicted
    in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.14 – Depthwise separable convolutional layer as a logical block](img/B18187_03_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.14 – Depthwise separable convolutional layer as a logical block
  prefs: []
  type: TYPE_NORMAL
- en: 'MobileNet also made two parameters that can be configured to reduce the model’s
    size and computational requirements while trading off some metric performance.
    The first is a width multiplier that can configure the input and output channels
    across the entire architecture. The second is a resolution multiplier between
    0 and 1 that applies to the original 224x224 image size to reduce the input and
    output feature map sizes when passing the image into the network. These parameters
    can also be adapted to other architectures if a faster runtime is needed. *Figure
    3**.15* shows the MobileNetV1 architecture’s structure and layer configurations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.15 – MobileNetV1 architecture](img/B18187_03_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.15 – MobileNetV1 architecture
  prefs: []
  type: TYPE_NORMAL
- en: MobileNet is considered to be a unique model family with two improvements that
    are based on the first version. They are named MobileNetv2 and MobileNetv3-small
    and both were introduced in 2019.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MobileNetV2
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Before we go through MobileNetV2, let’s define what a **bottleneck layer**
    is, a core idea that’s utilized in advancement. A bottleneck layer is generally
    a layer with fewer output feature maps compared to the layers before and after
    the layer. MobileNetV2 is built upon the idea that bottlenecks are where the information
    of interest will exist; nonlinearities destroy too much information in bottleneck
    layers, so a linear layer is applied, finally applying residuals using shortcut
    connections on the bottleneck layers. MobileNetV2 builds upon the base depthwise
    separable building block, adds a linear bottleneck layer without ReLU, and adds
    residuals to the bottleneck layers. This building block is depicted in the following
    figure. This is called the **bottleneck inverted** **residual block**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.16 – The bottleneck inverted residual block for MobileNetV2, also
    called MBConv](img/B18187_03_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.16 – The bottleneck inverted residual block for MobileNetV2, also called
    MBConv
  prefs: []
  type: TYPE_NORMAL
- en: As for the entire network architecture-wise, all depthwise separable layers
    from the first MobileNet were replaced with the new block except for the first
    convolution layer with a 3x3 kernel size and 32 filters, as shown in *Figure 3**.16*.
    One small extra detail is that they used the **ReLU6** activation function, which
    is robust to low-precision computation. The MobileNetV2 architecture used the
    depicted logical block to create many repeated layer blocks with different settings.
    This architecture allowed for an improvement in the performance curve on ImageNet
    compared to MobileNetV2 at around 5% to 10% at the same FLOPs. Remember that EfficientNetV2
    uses this block and also another version of this block that fuses back the linear
    bottleneck layer with the filtering layer together. The purpose of having two
    layers instead of one was to reduce the number of operations needed but again,
    for edge devices, the actual latency might differ due to the bottleneck in memory
    access cost. Sometimes, using a fused version might result in a faster runtime
    with the benefits of having more parameters to learn more information from.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MobileNetV3-small
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'For **MobileNetV3-small**, a few changes were made to MobileNetV2:'
  prefs: []
  type: TYPE_NORMAL
- en: It used a more advanced non-linearity called `ImageNet`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Expensive computation was reduced even further in the initial and last few layers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the first layer, as shown in *Figure 3**.15*, 32 filters was reduced to
    16 and hard-swish nonlinearity was used to get 2 milliseconds runtime and 10 million
    FLOPS savings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: For the last few layers, the last 1x1 bottleneck convolution layer is moved
    to after the final average pooling layer, and the previous bottleneck (1x1) and
    filtering (3x3) layer are also removed.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It used a modified version of network architecture search that is platform-aware
    called **Mnasnet** and a post-search layer reduction for latency reduction called
    **NetAdapt** to automatically find an optimized architecture based on the building
    blocks from the MobileNetV1, MobileNetV2, and squeeze and excitation networks
    while considering latency and accuracy performance. NetAdapt and MnasNet will
    be introduced in [*Chapter 7*](B18187_07.xhtml#_idTextAnchor107), *Deep Neural*
    *Architecture Search*.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MobileNetV3-small ended up achieving a higher top-1 `ImageNet` accuracy with
    the same parameters and FLOPS compared to MobileNetV2.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the ShuffleNet architecture family
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: ShuffleNet has two versions, ShuffleNetV1 and ShuffleNetV2, which we will discuss
    separately.
  prefs: []
  type: TYPE_NORMAL
- en: ShuffleNetV1, from 2017, reuses a known variant of convolution called **grouped
    convolutions**, where each convolutional filter is responsible only for a subset
    of input data channels. MobileNet uses a special variant of this by using one
    filter for each channel. Grouped convolutions save computation costs by operating
    only on a small subset of input channel features. However, when stacked together
    one after another, the information between channels doesn’t interact – ultimately
    causing accuracy degradation. ShuffleNetV1 uses channel shuffling as an operation
    in between stacking multiple grouped convolutional layers to manually shift information
    without sacrificing FLOPs. This allows for an efficient and small network.
  prefs: []
  type: TYPE_NORMAL
- en: 'ShuffleNetV2, from 2018, builds upon ShuffleNetV1 and focuses on the practical
    runtime efficiency of the architecture in real life while considering factors
    such as memory access cost, data **input-output** (**I/O**), and the degree of
    network parallelism. The following four design strategies were used to craft the
    new architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: Equal channel width from input to output to minimize memory access cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decrease group convolution to minimize memory access cost.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reduce network fragmentation to increase parallelism. One example is the number
    of convolutional and pooling operations in a single building block.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Reduce element-wise operations such as ReLU as they have heavy memory access
    costs:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.17 – Two building blocks of ShuffleNetv1 on the left and two building
    blocks of ShuffleNetv2 on the right](img/B18187_03_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.17 – Two building blocks of ShuffleNetv1 on the left and two building
    blocks of ShuffleNetv2 on the right
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 3**.17*, the first two structures on the left show the two building
    blocks of ShuffleNetV1, while the last two structures on the right show the two
    building blocks of ShuffleNetV2.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding MicroNet, the current state-of-the-art architecture for the edge
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Created in 2021, MicroNet is the current state of the art in terms of latency
    and achievable top-1 `ImageNet` accuracy performance, for a very low FLOP range
    of 4 million to 21 million. The novelty of MicroNet is two-fold:'
  prefs: []
  type: TYPE_NORMAL
- en: It introduced factorized versions of the bottleneck/pointwise convolution layer
    and depthwise convolutional layers from MobileNet, called **micro-factorized convolutions**,
    in a way that the number of connections/paths for input data to output data is
    reduced. This is achieved by using multiple grouped convolutions and some dilated
    convolutions. Dilated convolutions are simply convolutions with fixed spacing
    in the kernels. Take these techniques as a form of sparse computation and only
    compute what’s needed most efficiently to ensure minimal input-to-output path
    redundancy.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'It introduced a new activation function called **dynamic shift-max** that leverages
    the output of the grouped convolutions in a way that it applies a higher order
    of non-linearity (two times) and strengthens connections between groups at the
    same time. This is implemented by using the grouped outputs of the blocks of squeeze
    and excitation (produce a single value per channel) as a weighting mechanism to
    obtain the maximum of a group-based weighted addition. Take this as an improvement
    on top of channel shuffling in ShuffleNet. *Figure 3**.18* shows the operation
    structure of dynamic shift-max for a single example feature map of 12 channels
    from the output of four groups using the grouped convolution operation:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 3.18 – Dynamic shift-max general operation flow](img/B18187_03_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.18 – Dynamic shift-max general operation flow
  prefs: []
  type: TYPE_NORMAL
- en: 'MicroNet utilizes concepts from ShuffleNet (the channel shuffling mechanism),
    ResNet (skip connections), SENet (squeeze and excitation networks), and MobileNet
    (create factorized versions out of the already factorized convolutions) on top
    of its novelties to create networks that are highly efficient by focusing on the
    concept of sparsity and improvements in efficient information flow. The specifics
    of this network can be overwhelming and, frankly, hard to comprehend, so the information
    presented here does not contain all the details:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 3.19 – Diagram of three logical blocks, called micro-blocks, that\
    \ \uFEFFare used to build different size variants of MicroNets](img/B18187_03_19.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 3.19 – Diagram of three logical blocks, called micro-blocks, that are
    used to build different size variants of MicroNets
  prefs: []
  type: TYPE_NORMAL
- en: However, *Figure 3**.19* shows how logical blocks are, again, in the most advanced
    network today to build networks with different sizes based on the same ideas.
  prefs: []
  type: TYPE_NORMAL
- en: Summarizing CNN architectures for the edge
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To summarize the architectures for the edge, you now have the intuitive knowledge
    that was used by experts in the field to build highly effective CNN architectures
    capable of running at an amazingly tiny footprint compared to large models such
    as GPT-3 today. The following figure shows the overall top-1 `ImageNet` accuracy
    performance versus FLOPS graph of multiple different architecture families for
    edge computation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.20 – Top-1 accuracy performance for edge architecture families below
    400 million FLOPS](img/B18187_03_20.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.20 – Top-1 accuracy performance for edge architecture families below
    400 million FLOPS
  prefs: []
  type: TYPE_NORMAL
- en: Take these results with a pinch of salt; training strategies might differ between
    the models and can affect the achievable top-1 `ImageNet` accuracy considerably,
    along with giving possibly differing results between different random initializations
    of the different model runs. Additionally, latency is not directly represented
    purely by the number of parameters nor the FLOPS but is additionally affected
    by the memory access cost of individual operations, I/O access cost, and the degree
    of parallelism of the different operations.
  prefs: []
  type: TYPE_NORMAL
- en: 'To recap, *Figure 3**.21* shows the overall performance plots based on the
    FLOPS of all the CNN model families that were introduced in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 3.21 – Overall CNN model family performance in terms of ImageNet top-1
    accuracy based on FLOPS](img/B18187_03_21.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 3.21 – Overall CNN model family performance in terms of ImageNet top-1
    accuracy based on FLOPS
  prefs: []
  type: TYPE_NORMAL
- en: Again, note that we should take the results presented here with a pinch of salt
    as the training techniques that were performed against the `ImageNet` dataset
    are not exactly standardized across different benchmarks. Variation in the training
    technique can result in widely different results and will be covered more extensively
    in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125), *Exploring Supervised Deep
    Learning*, and [*Chapter 9*](B18187_09.xhtml#_idTextAnchor149), *Exploring Unsupervised
    Deep Learning*. Another important thing to note is that even though `ImageNet`
    is considered to be a large enough image dataset to be considered as a benchmark,
    maintain a level of skepticism toward the results as the data itself has been
    proven to have noisy labels with systematic errors in some cases. A corrected
    form of `ImageNet` has been published called `ImageNet Real` but not all models
    are benchmarked or pre-trained against it. Train it on your dataset to be 100%
    sure which architecture, when pre-trained on certain datasets, performs better!
    Additionally, the FLOPS indicator does not fully represent the actual latency
    of the model, which can vary widely based on how the code is structured, how the
    model is distributed through multiple devices, how many GPUs or CPUs are available,
    and how parallel the model architecture is.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: CNNs are the go-to model for capturing patterns in image data. The handpicked
    architectures that were introduced in this chapter are the core backbones that
    can be subsequently utilized as a base for solving more custom downstream tasks
    such as image object detection and image generation.
  prefs: []
  type: TYPE_NORMAL
- en: The CNNs that were covered here will be used practically in later chapters as
    a basis to help you learn other deep learning-based knowledge. Take your time
    and look into how different architectures are implemented in a deep learning library
    offline in this book’s GitHub repository; we won’t be presenting the actual implementation
    code here. Now that we have covered CNNs in intermediate to low-level detail,
    in the next chapter, we’ll shift gears and look at recurrent neural networks.
  prefs: []
  type: TYPE_NORMAL
