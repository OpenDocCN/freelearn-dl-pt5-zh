<html><head></head><body>
		<div>
			<div id="_idContainer046" class="Content">
			</div>
		</div>
		<div id="_idContainer047" class="Content">
			<h1 id="_idParaDest-46">2. <a id="_idTextAnchor045"/>Neural Networks</h1>
		</div>
		<div id="_idContainer084" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">This chapter starts with an introduction to biological neurons; we see how an artificial neural network is inspired by biological neural networks. We will examine the structure and inner workings of a simple single-layer neuron called a perceptron and learn how to implement it in TensorFlow. We will move on to building multilayer neural networks to solve more complex multiclass classification tasks and discuss the practical considerations of designing a neural network. As we build deep neural networks, we will move on to Keras to build modular and easy-to-customize neural network models in Python. By the end of this chapter, you'll be adept at building neural networks to solve complex problems.</p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor046"/>Introduction</h1>
			<p>In the previous chapter, we learned how to implement basic mathematical concepts such as quadratic equations, linear algebra, and matrix multiplication in TensorFlow. Now that we have learned the basics, let's dive into <strong class="bold">Artificial Neural Networks</strong> (<strong class="bold">ANNs</strong>), which are central to artificial intelligence and deep learning.</p>
			<p>Deep learning is a subset of machine learning. In supervised learning, we often use traditional machine learning techniques, such as support vector machines or tree-based models, where features are explicitly engineered by humans. However, in deep learning, the model explores and identifies the important features of a labeled dataset without human intervention. ANNs, inspired by biological neurons, have a layered representation, which helps them learn labels incrementally—from the minute details to the complex ones. Consider the example of image recognition: in a given image, an ANN would just as easily identify basic details such as light and dark areas as it would identify more complex structures such as shapes. Though neural network techniques are tremendously successful at tasks such as identifying objects in images, how they do so is a black box, as the features are learned implicitly. Deep learning techniques have turned out to be powerful at tackling very complex problems, such as speech/image recognition, and hence are used across industry in building self-driving cars, Google Now, and many more applications.</p>
			<p>Now that we know the importance of deep learning techniques, we will take a pragmatic step-by-step approach to understanding a mix of theory and practical considerations in building deep-learning-based solutions. We will start with the smallest component of a neural network, which is an artificial neuron, also referred to as a perceptron, and incrementally increase the complexity to explore <strong class="bold">Multi-Layer Perceptrons</strong> (<strong class="bold">MLPs</strong>) and advanced models such as <strong class="bold">Recurrent Neural Networks</strong> (<strong class="bold">RNNs</strong>) and <strong class="bold">Convolutional Neural Networks</strong> (<strong class="bold">CNNs</strong>).</p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor047"/>Neural Networks and the Structure of Perceptrons</h1>
			<p>A neuron is a basic building block of the human nervous system, which relays electric signals across the body. The human brain consists of billions of interconnected biological neurons, and they are constantly communicating with each other by sending minute electrical binary signals by turning themselves on or off. The general meaning of a neural network is a network of interconnected neurons. In the current context, we are referring to ANNs, which are actually modeled on a biological neural network. The term artificial intelligence is derived from the fact that natural intelligence exists in the human brain (or any brain for that matter), and we humans are trying to simulate this natural intelligence artificially. Though ANNs are inspired by biological neurons, some of the advanced neural network architectures, such as CNNs and RNNs, do not actually mimic the behavior of a biological neuron. However, for ease of understanding, we will begin by drawing an analogy between the biological neuron and an artificial neuron (perceptron).</p>
			<p>A simplified version of a biological neuron is represented in <em class="italic">Figure 2.1</em>:</p>
			<p> </p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B15385_02_01.jpg" alt="Figure 2.1: Biological neuron&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1: Biological neuron</p>
			<p>This is a highly simplified representation. There are three main components:</p>
			<ul>
				<li>The dendrites, which receive the input signals</li>
				<li>The cell body, where the signal is processed in some form</li>
				<li>The tail-like axon, through which the neuron transfers the signal out to the next neuron</li>
			</ul>
			<p>A perceptron can also be represented in a similar way, although it is not a physical entity but a mathematical model. <em class="italic">Figure 2.2</em> shows a high-level representation of an artificial neuron:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B15385_02_02.jpg" alt="Figure 2.2: Representation of an artificial neuron&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2: Representation of an artificial neuron</p>
			<p>In an artificial neuron, as in a biological one, there is an input signal. The central node conflates all the signals and fires the output signal if it is above a certain threshold. A more detailed representation of a perceptron is shown in <em class="italic">Figure 2.3</em>. Each component of this perceptron is explained in the sections that follow:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B15385_02_03.jpg" alt="Figure 2.3: Representation of a perceptron&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3: Representation of a perceptron</p>
			<p>A perceptron has the following components:</p>
			<ul>
				<li>Input layer</li>
				<li>Weights</li>
				<li>Bias</li>
				<li>Net input function</li>
				<li>Activation function</li>
			</ul>
			<p>Let's look at these components and their TensorFlow implementations in detail by considering an <strong class="source-inline">OR</strong> table dataset.</p>
			<h3 id="_idParaDest-49">Inp<a id="_idTextAnchor048"/>ut Layer</h3>
			<p>Each example of input data is fed through the input layer. Referring to the representation shown in <em class="italic">Figure 2.3</em>, depending on the size of the input example, the number of nodes will vary from <em class="italic">x</em><span class="subscript">1</span> to <em class="italic">x</em><span class="subscript">m</span>. The input data can be structured data (such as a CSV file) or unstructured data, such as an image. These inputs, <em class="italic">x</em><span class="subscript">1 </span>to <em class="italic">x</em><span class="subscript">m</span>, are called features (<strong class="source-inline">m</strong> refers to the number of features). Let's illustrate this with an example.</p>
			<p>Let's say the data is in the form of a table as follows:</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B15385_02_04.jpg" alt="Figure 2.4: Sample input and output data – OR table&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4: Sample input and output data – OR table</p>
			<p>Here, the inputs to the neuron are the columns <em class="italic">x</em><span class="subscript">1</span> and <em class="italic">x</em><span class="subscript">2</span>, which correspond to one row. At this point, it may be difficult to comprehend, but for now, accept it that the data is fed one row at a time in an iterative manner during training. We will represent the input data and the true labels (output <strong class="source-inline">y</strong>) with the TensorFlow <strong class="source-inline">Variable</strong> class as follows:</p>
			<p class="source-code">X = tf.Variable([[0.,0.],[0.,1.],\</p>
			<p class="source-code">                 [1.,0.],[1.,1.]], \</p>
			<p class="source-code">                 tf.float32)</p>
			<p class="source-code">y = tf.Variable([0, 1, 1, 1], tf.float32)</p>
			<h3 id="_idParaDest-50">Weig<a id="_idTextAnchor049"/>hts</h3>
			<p>Weights are associated with each neuron, and the input features dictate how much influence each of the input features should have in computing the next node. Each neuron will be connected to all the input features. In the example, since there were two inputs (<em class="italic">x</em><span class="subscript">1</span> and <em class="italic">x</em><span class="subscript">2</span>) and the input layer is connected to one neuron, there will be two weights associated with it: <em class="italic">w</em><span class="subscript">1</span> and <em class="italic">w</em><span class="subscript">2</span>. A weight is a real number; it can be positive or negative and is mathematically represented as <strong class="bold">R</strong>. When we say that a neural network is learning, what is happening is that the network is adjusting its weights and biases to get the correct predictions by adjusting to the error feedback. We will see this in more detail in the sections that follow. For now, we will initialize the weights as zeros and use the same TensorFlow <strong class="source-inline">Variable</strong> class as follows:</p>
			<p class="source-code">number_of_features = x.shape[1]</p>
			<p class="source-code">number_of_units = 1</p>
			<p class="source-code">Weight = tf.Variable(tf.zeros([number_of_features, \</p>
			<p class="source-code">                               number_of_units]), \</p>
			<p class="source-code">                               tf.float32)</p>
			<p>Weights would be of the following dimension: <em class="italic">number of input features × output size</em>.</p>
			<h3 id="_idParaDest-51">Bias<a id="_idTextAnchor050"/></h3>
			<p>In <em class="italic">Figure 2.3</em>, bias is represented by <em class="italic">b</em>, which is called additive bias. Every neuron has one bias. When <em class="italic">x</em> is zero, that is, no information is coming from the independent variables, then the output should be biased to just <em class="italic">b</em>. Like the weights, the bias also a real number, and the network has to learn the bias value to get the correct predictions.</p>
			<p>In TensorFlow, bias is the same size as the output size and can be represented as follows:</p>
			<p class="source-code">B = tf.Variable(tf.zeros([1, 1]), tf.float32)</p>
			<h3 id="_idParaDest-52">Net <a id="_idTextAnchor051"/>Input Function</h3>
			<p>The net input function, also commonly referred to as the input function, can be described as the sum of the products of the inputs and their corresponding weights plus the bias. Mathematically, it is represented as follows:</p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B15385_02_05.jpg" alt="Figure 2.5: Net input function in mathematical form&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5: Net input function in mathematical form</p>
			<p>Here:</p>
			<ul>
				<li><em class="italic">x</em><span class="subscript">i</span>: input data—<em class="italic">x</em><span class="subscript">1</span> to <em class="italic">x</em><span class="subscript">m</span></li>
				<li><em class="italic">w</em><span class="subscript">i</span>: weights—<em class="italic">w</em><span class="subscript">1</span> to <em class="italic">w</em><span class="subscript">m</span></li>
				<li><em class="italic">b</em>: additive bias</li>
			</ul>
			<p>As you can see, this formula involves inputs and their associated weights and biases. This can be written in vectorized form, and we can use matrix multiplication, which we learned about in <em class="italic">Chapter 1</em>,<em class="italic"> Building Blocks of Deep Learning</em>. We will see this when we start the code demo. Since all the variables are numbers, the result of the net input function is just a number, a real number. The net input function can be easily implemented using the TensorFlow <strong class="source-inline">matmul</strong> functionality as follows:</p>
			<p class="source-code">z = tf.add(tf.matmul(X, W), B)</p>
			<p><strong class="source-inline">W</strong> stands for weight, <strong class="source-inline">X</strong> stands for input, and <strong class="source-inline">B</strong> stands for bias.</p>
			<h3 id="_idParaDest-53">Activ<a id="_idTextAnchor052"/>ation Function (G)</h3>
			<p>The output of the net input function (<strong class="source-inline">z</strong>) is fed as input to the activation function. The activation function squashes the output of the net input function (<strong class="source-inline">z</strong>) into a new output range depending on the choice of activation function. There are a variety of activation functions, such as sigmoid (logistic), ReLU, and tanh. Each activation function has its own pros and cons. We will take a deep dive into activation functions later in the chapter. For now, we will start with a sigmoid activation function, also known as a logistic function. With the sigmoid activation function, the linear output <strong class="source-inline">z</strong> is squashed into a new output range of (0,1). The activation function provides non-linearity between layers, which gives neural networks the ability to approximate any continuous function.</p>
			<p>The mathematical equation of the sigmoid function is as follows, where <em class="italic">G(z)</em> is the sigmoid function and the right-hand equation details the derivative with respect to <em class="italic">z</em>:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B15385_02_06.jpg" alt="Figure 2.6: Mathematical form of the sigmoid function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.6: Mathematical form of the sigmoid function</p>
			<p>As you can see in <em class="italic">Figure 2.7</em>, the sigmoid function is a more or less S-shaped curve with values between 0 and 1, no matter what the input is:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B15385_02_07.jpg" alt="Figure 2.7: Sigmoid curve&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.7: Sigmoid curve</p>
			<p>And if we set a threshold (say <strong class="source-inline">0.5</strong>), we can convert this into a binary output. Any output greater than or equal to <strong class="source-inline">.5</strong> is considered <strong class="source-inline">1</strong>, and any value less than <strong class="source-inline">.5</strong> is considered <strong class="source-inline">0</strong>.</p>
			<p>Activation functions such as sigmoid are provided out of the box in TensorFlow. A sigmoid function can be implemented in TensorFlow as follows:</p>
			<p class="source-code">output = tf.sigmoid(z)</p>
			<p>Now that we have seen the structure of a perceptron and its code representation in TensorFlow, let's put all the components together to make a perceptron.</p>
			<h3 id="_idParaDest-54"><a id="_idTextAnchor053"/>Perceptrons in TensorFlow</h3>
			<p>In TensorFlow, a perceptron can be implemented just by defining a simple function, as follows:</p>
			<p class="source-code">def perceptron(X):</p>
			<p class="source-code">    z = tf.add(tf.matmul(X, W), B)</p>
			<p class="source-code">    output = tf.sigmoid(z)</p>
			<p class="source-code">    return output</p>
			<p>At a very high level, we can see that the input data passes through the net input function. The output of the net input function is passed to the activation function, which, in turn, gives us the predicted output. Now, let's look at each line of the code:</p>
			<p class="source-code">z = tf.add(tf.matmul(X, W), B)</p>
			<p>The output of the net input function is stored in <strong class="source-inline">z</strong>. Let's see how we got that result by breaking it down further into two parts, that is, the matrix multiplication part contained in <strong class="source-inline">tf.matmul</strong> and the addition contained in <strong class="source-inline">tf.add</strong>.</p>
			<p>Let's say we're storing the result of the matrix multiplication of <strong class="source-inline">X</strong> and <strong class="source-inline">W</strong> in a variable called <strong class="source-inline">m</strong>:</p>
			<p class="source-code">m = tf.matmul(X, W)</p>
			<p>Now, let's consider how we got that result. For example, let's say <strong class="source-inline">X</strong> is a row matrix, like [ X<span class="subscript">1</span> X<span class="subscript">2</span> ], and <strong class="source-inline">W</strong> is a column matrix, as follows:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B15385_02_08.jpg" alt="Figure 2.8: Column matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.8: Column matrix</p>
			<p>Recall from the previous chapter that <strong class="source-inline">tf.matmul</strong> will perform matrix multiplication. So, the result is this:</p>
			<p class="source-code">m = x1*w1 + x2*w2</p>
			<p>And then, we add the output, <strong class="source-inline">m</strong>, to the bias, <strong class="source-inline">B</strong>, as follows:</p>
			<p class="source-code">z = tf.add(m, B)</p>
			<p>Note that what we do in the preceding step is the same as the mere addition of the two variables <strong class="source-inline">m</strong> and <strong class="source-inline">b</strong>:</p>
			<p class="source-code">m + b</p>
			<p>Hence, the final output is:</p>
			<p class="source-code">z = x1*w1 + x2*w2 + b</p>
			<p><strong class="source-inline">z</strong> would be the output of the net input function.</p>
			<p>Now, let's consider the next line:</p>
			<p class="source-code">output= tf.sigmoid(z)</p>
			<p>As we learned earlier, <strong class="source-inline">tf.sigmoid</strong> is a readily available implementation of the sigmoid function. The net input function's output (<strong class="source-inline">z</strong>) computed in the previous line is fed as input to the sigmoid function. The result of the sigmoid function is the output of the perceptron, which is in the range of 0 to 1. During training, which will be explained later in the chapter, we will feed the data in batches to this function, which will calculate the predicted values.</p>
			<h2 id="_idParaDest-55">Exercise<a id="_idTextAnchor054"/> 2.01: Perceptron Implementation</h2>
			<p>In this exercise, we will implement the perceptron in TensorFlow for an <strong class="source-inline">OR</strong> table. Let's set the input data in TensorFlow and freeze the design parameters of perceptron:</p>
			<ol>
				<li>Let's import the necessary package, which, in our case, is <strong class="source-inline">tensorflow</strong>:<p class="source-code">import tensorflow as tf</p></li>
				<li>Set the input data and labels of the <strong class="source-inline">OR</strong> table data in TensorFlow:<p class="source-code">X = tf.Variable([[0.,0.],[0.,1.],\</p><p class="source-code">                 [1.,0.],[1.,1.]], \</p><p class="source-code">                 dtype=tf.float32)</p><p class="source-code">print(X)</p><p>As you can see in the output, we will have a 4 × 2 matrix of input data:</p><p class="source-code">&lt;tf.Variable 'Variable:0' shape=(4, 2) dtype=float32, </p><p class="source-code">numpy=array([[0., 0.],</p><p class="source-code">             [0., 1.],</p><p class="source-code">             [1., 0.],</p><p class="source-code">             [1., 1.]], dtype=float32)&gt;</p></li>
				<li>We will set the actual labels in TensorFlow and use the <strong class="source-inline">reshape()</strong> function to reshape the <strong class="source-inline">y</strong> vector into a 4 × 1 matrix:<p class="source-code">y = tf.Variable([0, 1, 1, 1], dtype=tf.float32)</p><p class="source-code">y = tf.reshape(y, [4,1])</p><p class="source-code">print(y)</p><p>The output is a 4 × 1 matrix, as follows:</p><p class="source-code">tf.Tensor(</p><p class="source-code">[[0.]</p><p class="source-code"> [1.]</p><p class="source-code"> [1.]</p><p class="source-code"> [1.]], shape=(4, 1), dtype=float32)</p></li>
				<li>Now let's design parameters of a perceptron.<p><em class="italic">Number of neurons (units) = 1</em></p><p><em class="italic">Number of features (inputs) = 2 (number of examples × number of features)</em></p><p>The activation function will be the sigmoid function, since we are doing binary classification:</p><p class="source-code">NUM_FEATURES = X.shape[1]</p><p class="source-code">OUTPUT_SIZE = 1</p><p>In the preceding code, <strong class="source-inline">X.shape[1]</strong> will equal <strong class="source-inline">2</strong> (since the indices start with zero, <strong class="source-inline">1</strong> refers to the second index, which is <strong class="source-inline">2</strong>).</p></li>
				<li>Define the connections weight matrix in TensorFlow:<p class="source-code">W = tf.Variable(tf.zeros([NUM_FEATURES, \</p><p class="source-code">                          OUTPUT_SIZE]), \</p><p class="source-code">                          dtype=tf.float32)</p><p class="source-code">print(W)</p><p>The weight matrix would essentially be a columnar matrix as shown in the following figure. It will have the following dimension: <em class="italic">number of features (columns) × output size</em>:</p><div id="_idContainer056" class="IMG---Figure"><img src="image/B15385_02_09.jpg" alt="Figure 2.9: A columnar matrix&#13;&#10;"/></div><p class="figure-caption">Figure 2.9: A columnar matrix</p><p>The output size will be dependent on the number of neurons—in this case, it is <strong class="source-inline">1</strong>. So, if you are developing a layer of 10 neurons with two features, the shape of this matrix will be [2,10]. The <strong class="source-inline">tf.zeros</strong> function creates a tensor with the given shape and initializes all the elements to zeros.</p><p>So, this will result in a zero columnar matrix like this:</p><p class="source-code">&lt;tf.Variable 'Variable:0' shape=(2, 1) dtype=float32, \</p><p class="source-code">numpy=array([[0.], [0.]], dtype=float32)&gt;</p></li>
				<li>Now create the variable for the bias:<p class="source-code">B = tf.Variable(tf.zeros([OUTPUT_SIZE, 1]), dtype=tf.float32)</p><p class="source-code">print(B)</p><p>There is only one bias per neuron, so in this case, the bias is just one number in the form of a single-element array. However, if we had a layer of 10 neurons, then it would be an array of 10 numbers—1 for each neuron.</p><p>This will result in a 0-row matrix with a single element like this:</p><p class="source-code">&lt;tf.Variable 'Variable:0' shape=(1, 1) dtype=float32, </p><p class="source-code">numpy=array([[0.]], dtype=float32)&gt;</p></li>
				<li>Now that we have the weights and bias, the next step is to perform the computation to get the net input function, feed it to the activation function, and then get the final output. Let's define a function called <strong class="source-inline">perceptron</strong> to get the output:<p class="source-code">def perceptron(X):</p><p class="source-code">    z = tf.add(tf.matmul(X, W), B)</p><p class="source-code">    output = tf.sigmoid(z)</p><p class="source-code">    return output</p><p class="source-code">print(perceptron(X))</p><p>The output will be a 4 × 1 array that contains the predictions by our perceptron:</p><p class="source-code">tf.Tensor<a id="_idTextAnchor055"/>(</p><p class="source-code">[[0.5]</p><p class="source-code"> [0.5]</p><p class="source-code"> [0.5]</p><p class="source-code"> [0.5]], shape=(4, 1), dtype=float32)</p><p>As we can see, the predictions are not quite accurate. We will learn how to improve the results in the sections that follow.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3feF7MO">https://packt.live/3feF7MO</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2CkMiEE">https://packt.live/2CkMiEE</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we implemented a perceptron, which is a mathematical implementation of a single artificial neuron. Keep in mind that it is just the implementation of the model; we have not done any training. In the next section, we will see how to train the perceptron.</p>
			<h1 id="_idParaDest-56">Training <a id="_idTextAnchor056"/>a Perceptron</h1>
			<p>To train a perceptron, we need the following components:</p>
			<ul>
				<li>Data representation</li>
				<li>Layers</li>
				<li>Neural network representation</li>
				<li>Loss function</li>
				<li>Optimizer</li>
				<li>Training loop</li>
			</ul>
			<p>In the previous section, we covered most of the preceding components: the <strong class="bold">data representation</strong> of the input data and the true labels in TensorFlow. For <strong class="bold">layers</strong>, we have the linear layer and the activation functions, which we saw in the form of the net input function and the sigmoid function respectively. For the <strong class="bold">neural network representation</strong>, we made a function called <strong class="source-inline">perceptron()</strong>, which uses a linear layer and a sigmoid layer to perform predictions. What we did in the previous section using input data and initial weights and biases is called <strong class="bold">forward propagation</strong>. The actual neural network training involves two stages: forward propagation and backward propagation. We will explore them in detail in the next few steps. Let's look at the training process at a higher level:</p>
			<ul>
				<li>A training iteration where the neural network goes through all the training examples is called an Epoch. This is one of the hyperparameters to be tweaked in order to train a neural network.</li>
				<li>In each pass, a neural network does forward propagation, where data travels from the input to the output. As seen in <em class="italic">Exercise 2.01</em>, <em class="italic">Perceptron Implementation</em>, inputs are fed to the perceptron. Input data passes through the net input function and the activation function to produce the predicted output. The predicted output is compared with the labels or the ground truth, and the error or loss is calculated.</li>
				<li>In order to make a neural network learn, learning being the adjustment of weights and biases in order to make correct predictions, there needs to be a <strong class="bold">loss function</strong>, which will calculate the error between an actual label and the predicted label.</li>
				<li>To minimize the error in the neural network, the training loop needs an <strong class="bold">optimizer</strong>, which will minimize the loss on the basis of a loss function.</li>
				<li>Once the error is calculated, the neural network then sees which nodes of the network contributed to the error and by how much. This is essential in order to make the predictions better in the next epoch. This way of propagating the error backward is called <strong class="bold">backward propagation</strong> (backpropagation). Backpropagation uses the chain rule from calculus to propagate the error (the error gradient) in reverse order until it reaches the input layer. As it propagates the error back through the network, it uses gradient descent to make fine adjustments to the weights and biases in the network by utilizing the error gradient calculated before.</li>
			</ul>
			<p>This cycle continues until the loss is minimized.</p>
			<p>Let's implement the theory we have discussed in TensorFlow. Revisit the code in <em class="italic">Exercise 2.01</em>, <em class="italic">Perceptron Implementation,</em> where the perceptron we created just did one forward pass. We got the following predictions, and we saw that our perceptron had not learned anything:</p>
			<p class="source-code">tf.Tensor(</p>
			<p class="source-code">[[0.5]</p>
			<p class="source-code"> [0.5]</p>
			<p class="source-code"> [0.5]</p>
			<p class="source-code"> [0.5]], shape=(4, 1), dtype=float32)</p>
			<p>In order to make our perceptron learn, we need additional components, such as a training loop, a loss function, and an optimizer. Let's see how to implement these components in TensorFlow.</p>
			<h2 id="_idParaDest-57">Perceptro<a id="_idTextAnchor057"/>n Training Process in TensorFlow</h2>
			<p>In the next exercise, when we train our model, we will use a <strong class="bold">Stochastic Gradient Descent</strong> (<strong class="bold">SGD</strong>) optimizer to minimize the loss. There are a few more advanced optimizers available and provided by TensorFlow out of the box. We will look at the pros and cons of each of them in later sections. The following code will instantiate a stochastic gradient descent optimizer using TensorFlow:</p>
			<p class="source-code">learning_rate = 0.01</p>
			<p class="source-code">optimizer = tf.optimizers.SGD(learning_rate)</p>
			<p>The <strong class="source-inline">perceptron</strong> function takes care of the forward propagation. For the backpropagation of the error, we have used an optimizer. <strong class="source-inline">Tf.optimizers.SGD</strong> creates an instance of an optimizer. SGD will update the parameters of the networks—weights and biases—on each example from the input data. We will discuss the functioning of the gradient descent optimizer in greater detail later in this chapter. We will also discuss the significance of the <strong class="source-inline">0.01</strong> parameter, which is known as the learning rate. The learning rate is the magnitude by which SGD takes a step in order to reach the global optimum of the loss function. The learning rate is another hyperparameter that needs to be tweaked in order to train a neural network.</p>
			<p>The following code can be used to define the epochs, training loop, and loss function:</p>
			<p class="source-code">no_of_epochs = 1000</p>
			<p class="source-code">for n in range(no_of_epochs):</p>
			<p class="source-code">    loss = lambda:abs(tf.reduce_mean(tf.nn.\</p>
			<p class="source-code">           sigmoid_cross_entropy_with_logits\</p>
			<p class="source-code">           (labels=y,logits=perceptron(X))))</p>
			<p class="source-code">    optimizer.minimize(loss, [W, B])</p>
			<p>Inside the training loop, the loss is calculated using the loss function, which is defined as a lambda function.</p>
			<p>The <strong class="source-inline">tf.nn.sigmoid_cross_entropy_with_logits</strong> function calculates the loss value of each observation. It takes two parameters: <strong class="source-inline">Labels = y</strong> and <strong class="source-inline">logit = perceptron(x)</strong>.</p>
			<p><strong class="source-inline">perceptron(X)</strong> returns the predicted value, which is the result of the forward propagation of the input, <strong class="source-inline">x</strong>. This is compared with the corresponding label value stored in <strong class="source-inline">y</strong>. The mean value is calculated using <strong class="source-inline">Tf.reduce_mean</strong>, and the magnitude is taken. The sign is ignored using the <strong class="source-inline">abs</strong> function. <strong class="source-inline">Optimizer.minimize</strong> takes the loss value and adjusts the weights and bias as a part of the backward propagation of the error.</p>
			<p>The forward propagation is executed again with the new values of weights and bias. And this forward and backward process continues for the number of iterations we define.</p>
			<p>During the backpropagation, the weights and biases are updated only if the loss is less than the previous cycle. Otherwise, the weights and biases remain unchanged. In this way, the optimizer ensures that even though it loops through the required number of iterations, it only stores the values of <strong class="source-inline">w</strong> and <strong class="source-inline">b</strong> for which the loss is minimal.</p>
			<p>We have set the number of epochs for the training to 1,000 iterations. There is no rule of thumb for setting the number of epochs since the number of epochs is a hyperparameter. But how do we know when training has taken place successfully?</p>
			<p>When we can see that the values of weights and biases have changed, we can conclude the training has taken place. Let's say we used a training loop for the <strong class="source-inline">OR</strong> data we saw in <em class="italic">Exercise 2.01</em>, <em class="italic">Perceptron Implementation</em>, we would see weights somewhat equal to the following:</p>
			<p class="source-code">[[0.412449151]</p>
			<p class="source-code">[0.412449151]]</p>
			<p>And the bias would be something like this:</p>
			<p class="source-code">0.236065879</p>
			<p>When the network has learned, that is, the weights and biases have been updated, we can see whether it is making accurate predictions using <strong class="source-inline">accuracy_score</strong> from the <strong class="source-inline">scikit-learn</strong> package. We can use it to measure the accuracy of the predictions as follows:</p>
			<p class="source-code">from sklearn.metrics import accuracy_score</p>
			<p class="source-code">print(accuracy_score(y, ypred))</p>
			<p>Here, <strong class="source-inline">accuracy_score</strong> takes two parameters—the label values (<strong class="source-inline">y</strong>) and the predicted values (<strong class="source-inline">ypred</strong>)—and measures the accuracy. Let's say the result is <strong class="source-inline">1.0</strong>. This means the perceptron is 100% accurate.</p>
			<p>In the next exercise, we will train our perceptron to perform a binary classification.</p>
			<h2 id="_idParaDest-58">Exercise <a id="_idTextAnchor058"/>2.02: Perceptron as a Binary Classifier</h2>
			<p>In the previous section, we learned how to train a perceptron. In this exercise, we will train our perceptron to approximate a slightly more complicated function. We will be using randomly generated external data with two classes: class <strong class="source-inline">0</strong> and class <strong class="source-inline">1</strong>. Our trained perceptron should be able to classify the random numbers based on their class:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The data is in a CSV file called <strong class="source-inline">data.csv</strong>. You can download the file from GitHub by visiting <a href="https://packt.live/2BVtxIf">https://packt.live/2BVtxIf</a>.</p>
			<ol>
				<li value="1">Import the required libraries:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p>Apart from <strong class="source-inline">tensorflow</strong>, we will need <strong class="source-inline">pandas</strong> to read the data from the CSV file, <strong class="source-inline">confusion_matrix</strong> and <strong class="source-inline">accuracy_score</strong> to measure the accuracy of our perceptron after the training, and <strong class="source-inline">matplotlib</strong> to visualize the data.</p></li>
				<li>Read the data from the <strong class="source-inline">data.csv</strong> file. It should be in the same path as the Jupyter Notebook file in which you are running this exercise's code. Otherwise, you will have to change the path in the code before executing it:<p class="source-code">df = pd.read_csv('data.csv')</p></li>
				<li>Examine the data:<p class="source-code">df.head()</p><p>The output will be as follows:</p><p> </p><div id="_idContainer057" class="IMG---Figure"><img src="image/B15385_02_10.jpg" alt="Figure 2.10: Contents of the DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 2.10: Contents of the DataFrame</p><p>As you can see, the data has three columns. <strong class="source-inline">x1</strong> and <strong class="source-inline">x2</strong> are the features, and the <strong class="source-inline">label</strong> column contains the labels <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong> for each observation. The best way to see this kind of data is through a scatter plot.</p></li>
				<li>Visualize the data by plotting it using <strong class="source-inline">matplotlib</strong>:<p class="source-code">plt.scatter(df[df['label'] == 0]['x1'], \</p><p class="source-code">            df[df['label'] == 0]['x2'], \</p><p class="source-code">            marker='*')</p><p class="source-code">plt.scatter(df[df['label'] == 1]['x1'], \</p><p class="source-code">            df[df['label'] == 1]['x2'], marker='&lt;')</p><p>The output will be as follows:</p><p> </p><div id="_idContainer058" class="IMG---Figure"><img src="image/B15385_02_11.jpg" alt="Figure 2.11: Scatter plot of external data&#13;&#10;"/></div><p class="figure-caption">Figure 2.11: Scatter plot of external data</p><p>This shows the two distinct classes of the data shown by the two different shapes. Data with the label <strong class="source-inline">0</strong> is represented by a star, while data with the label <strong class="source-inline">1</strong> is represented by a triangle.</p></li>
				<li>Prepare the data. This step is not unique to neural networks; you must have seen it in regular machine learning as well. Before submitting the data to a model for training, you split it into features and labels:<p class="source-code">X_input = df[['x1','x2']].values</p><p class="source-code">y_label = df[['label']].values</p><p><strong class="source-inline">x_input</strong> contains the features, <strong class="source-inline">x1</strong> and <strong class="source-inline">x2</strong>. The values at the end convert it into matrix format, which is what is expected as input when the tensors are created. <strong class="source-inline">y_label</strong> contains the labels in matrix format.</p></li>
				<li>Create TensorFlow variables for features and labels and typecast them to <strong class="source-inline">float</strong>:<p class="source-code">x = tf.Variable(X_input, dtype=tf.float32)</p><p class="source-code">y = tf.Variable(y_label, dtype=tf.float32)</p></li>
				<li>The rest of the code is for the training of the perceptron, which we saw in <em class="italic">Exercise 2.01</em>, <em class="italic">Perceptron Implementation</em>:<p class="source-code-heading">Exercise2.02.ipynb</p><p class="source-code">Number_of_features = 2</p><p class="source-code">Number_of_units = 1</p><p class="source-code">learning_rate = 0.01</p><p class="source-code"># weights and bias</p><p class="source-code">weight = tf.Variable(tf.zeros([Number_of_features, \</p><p class="source-code">                               Number_of_units]))</p><p class="source-code">bias = tf.Variable(tf.zeros([Number_of_units]))</p><p class="source-code">#optimizer</p><p class="source-code">optimizer = tf.optimizers.SGD(learning_rate)</p><p class="source-code">def perceptron(x):</p><p class="source-code">    z = tf.add(tf.matmul(x,weight),bias)</p><p class="source-code">    output = tf.sigmoid(z)</p><p class="source-code">    return output</p><p class="source-code-link">The complete code for this step can be found at <a href="https://packt.live/3gJ73bY">https://packt.live/3gJ73bY</a>.</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">#</strong> symbol in the code snippet above denotes a code comment. Comments are added into code to help explain specific bits of logic. </p></li>
				<li>Display the values of <strong class="source-inline">weight</strong> and <strong class="source-inline">bias</strong> to show that the perceptron has been trained:<p class="source-code">tf.print(weight, bias)</p><p>The output is as follows:</p><p class="source-code">[[-0.844034135]</p><p class="source-code"> [0.673354745]] [0.0593947917]</p></li>
				<li>Pass the input data to check whether the perceptron classifies it correctly:<p class="source-code">ypred = perceptron(x)</p></li>
				<li>Round off the output to convert it into binary format:<p class="source-code">ypred = tf.round(ypred)</p></li>
				<li>Measure the accuracy using the <strong class="source-inline">accuracy_score</strong> method, as we did in the previous exercise:<p class="source-code">acc = accuracy_score(y.numpy(), ypred.numpy())</p><p class="source-code">print(acc)</p><p>The output is as follows:</p><p class="source-code">1.0</p><p>The perceptron gives 100% accuracy.</p></li>
				<li>The confusion matrix helps to get the performance measurement of a model. We will plot the confusion matrix using the <strong class="source-inline">scikit-learn</strong> package.<p class="source-code">cnf_matrix = confusion_matrix(y.numpy(), \</p><p class="source-code">                              ypred.numpy())</p><p class="source-code">print(cnf_matrix)</p><p>The output will be as follows:</p><p class="source-code">[[12  0]</p><p class="source-code">[ 0  9]]</p><p>All the numbers are along the diagonal, that is, 12 values corresponding to class 0 and 9 values corresponding to class 1 are properly classified by our trained perceptron (which has achieved 100% accuracy).</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3gJ73bY">https://packt.live/3gJ73bY</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2DhelFw">https://packt.live/2DhelFw</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we trained our perceptron into a binary classifier, and it has done pretty well. In the next exercise, we will see how to create a multiclass classifier.</p>
			<h2 id="_idParaDest-59">Multiclass <a id="_idTextAnchor059"/>Classifier</h2>
			<p>A classifier that can handle two classes is known as a <strong class="bold">binary classifier</strong>, like the one we saw in the preceding exercise. A classifier that can handle more than two classes is known as a <strong class="bold">multiclass classifier</strong>. We cannot build a multiclass classifier with a single neuron. Now we move from one neuron to one layer of multiple neurons, which is required for multiclass classifiers.</p>
			<p>A single layer of multiple neurons can be trained to be a multiclass classifier. Some of the key points are detailed here. You need as many neurons as the number of classes; that is, for a 3-class classifier, you need 3 neurons; for a 10-class classifier you need 10 neurons, and so on.</p>
			<p>As we saw in binary classification, we used sigmoid (logistic layer) to get predictions in the range of 0 to 1. In multiclass classification, we use a special type of activation function called the <strong class="bold">Softmax</strong> activation function to get probabilities across each class that sums to 1. With the sigmoid function in a multiclass setting, the probabilities do not necessarily add up to 1, so Softmax is preferred.</p>
			<p>Before we implement the multiclass classifier, let's explore the Softmax activation function.</p>
			<h3 id="_idParaDest-60"><a id="_idTextAnchor060"/>The Softmax Activation Function</h3>
			<p>The Softmax function is also known as the <strong class="bold">normalized exponential function</strong>. As the word <strong class="bold">normalized</strong> suggests, the Softmax function normalizes the input into a probability distribution that sums to 1. Mathematically, it is represented as follows:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B15385_02_12.jpg" alt="Figure 2.12: Mathematical form of the Softmax function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.12: Mathematical form of the Softmax function</p>
			<p>To understand what Softmax does, let's use TensorFlow's built-in <strong class="source-inline">softmax</strong> function and see the output.</p>
			<p>So, for the following code:</p>
			<p class="source-code">values = tf.Variable([3,1,7,2,4,5], dtype=tf.float32)</p>
			<p class="source-code">output = tf.nn.softmax(values)</p>
			<p class="source-code">tf.print(output)</p>
			<p>The output will be:</p>
			<p class="source-code">[0.0151037546 0.00204407098 0.824637055 </p>
			<p class="source-code"> 0.00555636082 0.0410562605 0.111602485]</p>
			<p>As you can see in the output, the <strong class="source-inline">values</strong> input is mapped to a probability distribution that sums to 1. Note that <strong class="source-inline">7</strong> (the highest value in the original input values) received the highest weight, <strong class="source-inline">0.824637055</strong>. This is what the Softmax function is mainly used for: to focus on the largest values and suppress values that are below the maximum value. Also, if we sum the output, it adds up to ~ 1.</p>
			<p>Illustrating the example in more detail, let's say we want to build a multiclass classifier with 3 classes. We will need 3 neurons connected to a Softmax activation function:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B15385_02_13.jpg" alt="Figure 2.13: Softmax activation function used in a multiclass classification setting&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.13: Softmax activation function used in a multiclass classification setting</p>
			<p>As seen in <em class="italic">Figure 2.13</em>, <strong class="source-inline">x</strong><span class="subscript">1</span>, <strong class="source-inline">x</strong><span class="subscript">2</span>, and <strong class="source-inline">x</strong><span class="subscript">3</span> are the input features, which go through the net input function of each of the three neurons, which have the weights and biases (<strong class="source-inline">W</strong><span class="subscript">i, j</span> and <strong class="source-inline">b</strong><span class="subscript">i</span>) associated with it. Lastly, the output of the neuron is fed to the common Softmax activation function instead of the individual sigmoid functions. The Softmax activation function spits out the probabilities of the 3 classes: <strong class="source-inline">P1</strong>, <strong class="source-inline">P2</strong>, and <strong class="source-inline">P3</strong>. The sum of these three probabilities will add to 1 because of the Softmax layer.</p>
			<p>As we saw in the previous section, Softmax highlights the maximum value and suppresses the rest of the values. Suppose a neural network is trained to classify the input into three classes, and for a given set of inputs, the output is class 2; then it would say that <strong class="source-inline">P2</strong> has the highest value since it is passed through a Softmax layer. As you can see in the following figure, <strong class="source-inline">P2</strong> has the highest value, which means the prediction is correct:</p>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B15385_02_14.jpg" alt="Figure 2.14: Probability P2 is the highest&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.14: Probability P2 is the highest</p>
			<p>An associated concept is one-hot encoding. As we have three different classes, <strong class="source-inline">class1</strong>, <strong class="source-inline">class2</strong>, and <strong class="source-inline">class3</strong>, we need to encode the class labels into a format that we can work with more easily; so, after applying one-hot encoding, we would see the following output:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B15385_02_15.jpg" alt="Figure 2.15: One-hot encoded data for three classes&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.15: One-hot encoded data for three classes</p>
			<p>This makes the results quick and easy to interpret. In this case, the output that has the highest value is set to 1, and all others are set to 0. The one-hot encoded output of the preceding example would be like this:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B15385_02_16.jpg" alt=" Figure 2.16: One-hot encoded output probabilities&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 2.16: One-hot encoded output probabilities</p>
			<p>The labels of the training data also need to be one-hot encoded. And if they have a different format, they need to be converted into one-hot-encoded format before training the model. Let's do an exercise on multiclass classification with one-hot encoding.</p>
			<h2 id="_idParaDest-61">Exercise 2.03: M<a id="_idTextAnchor061"/>ulticlass Classification Using a Perceptron</h2>
			<p>To perform multiclass classification, we will be using the Iris dataset (<a href="https://archive.ics.uci.edu/ml/datasets/Iris">https://archive.ics.uci.edu/ml/datasets/Iris</a>), which has 3 classes of 50 instances each, where each class refers to a type of Iris. We will have a single layer of three neurons using the Softmax activation function:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can download the dataset from GitHub using this link: <a href="https://packt.live/3ekiBBf">https://packt.live/3ekiBBf</a>.</p>
			<ol>
				<li value="1">Import the required libraries:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.metrics import confusion_matrix</p><p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code">from pandas import get_dummies</p><p>You must be familiar with all of these imports as they were used in the previous exercise, except for <strong class="source-inline">get_dummies</strong>. This function converts a given label data into the corresponding one-hot-encoded format.</p></li>
				<li>Load the <strong class="source-inline">iris.csv</strong> data:<p class="source-code">df = pd.read_csv('iris.csv')</p></li>
				<li>Let's examine the first five rows of the data:<p class="source-code">df.head()</p><p>The output will be as follows:</p><p> </p><div id="_idContainer064" class="IMG---Figure"><img src="image/B15385_02_17.jpg" alt="Figure 2.17: Contents of the DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 2.17: Contents of the DataFrame</p></li>
				<li>Visualize the data by using a scatter plot:<p class="source-code">plt.scatter(df[df['species'] == 0]['sepallength'],\</p><p class="source-code">            df[df['species'] == 0]['sepalwidth'], marker='*')</p><p class="source-code">plt.scatter(df[df['species'] == 1]['sepallength'],\</p><p class="source-code">            df[df['species'] == 1]['sepalwidth'], marker='&lt;')</p><p class="source-code">plt.scatter(df[df['species'] == 2]['sepallength'], \</p><p class="source-code">            df[df['species'] == 2]['sepalwidth'], marker='o')</p><p>The resulting plot will be as follows. The <em class="italic">x</em> axis denotes the sepal length and the <em class="italic">y</em> axis denotes the sepal width. The shapes in the plot represent the three species of Iris, setosa (star), versicolor (triangle), and virginica (circle):</p><div id="_idContainer065" class="IMG---Figure"><img src="image/B15385_02_18.jpg" alt="Figure 2.18: Iris data scatter plot&#13;&#10;"/></div><p class="figure-caption">Figure 2.18: Iris data scatter plot</p><p>There are three classes, as can be seen in the visualization, denoted by different shapes.</p></li>
				<li>Separate the features and the labels:<p class="source-code">x = df[['petallength', 'petalwidth', \</p><p class="source-code">        'sepallength', 'sepalwidth']].values</p><p class="source-code">y = df['species'].values</p><p><strong class="source-inline">values</strong> will transform the features into matrix format.</p></li>
				<li>Prepare the data by doing one-hot encoding on the classes:<p class="source-code">y = get_dummies(y)</p><p class="source-code">y = y.values</p><p><strong class="source-inline">get_dummies(y)</strong> will convert the labels into one-hot-encoded format.</p></li>
				<li>Create a variable to load the features and typecast it to <strong class="source-inline">float32</strong>:<p class="source-code">x = tf.Variable(x, dtype=tf.float32)</p></li>
				<li>Implement the <strong class="source-inline">perceptron</strong> layer with three neurons:<p class="source-code">Number_of_features = 4</p><p class="source-code">Number_of_units = 3 </p><p class="source-code"> </p><p class="source-code"># weights and bias</p><p class="source-code">weight = tf.Variable(tf.zeros([Number_of_features, \</p><p class="source-code">                               Number_of_units]))</p><p class="source-code">bias = tf.Variable(tf.zeros([Number_of_units]))   </p><p class="source-code">def perceptron(x):</p><p class="source-code">    z = tf.add(tf.matmul(x, weight), bias)</p><p class="source-code">    output = tf.nn.softmax(z)</p><p class="source-code">    return output</p><p>The code looks very similar to the single perceptron implementation. Only the <strong class="source-inline">Number_of_units</strong> parameter is set to <strong class="source-inline">3</strong>. Therefore, the weight matrix will be 4 x 3 and the bias matrix will be 1 x 3.</p><p>The other change is in the activation function:</p><p><strong class="source-inline">Output=tf.nn.softmax(x)</strong></p><p>We are using <strong class="source-inline">softmax</strong> instead of <strong class="source-inline">sigmoid</strong>.</p></li>
				<li>Create an instance of the <strong class="source-inline">optimizer</strong>. We will be using the <strong class="source-inline">Adam</strong> optimizer. At this point, you can think of <strong class="source-inline">Adam</strong> as an improved version of gradient descent that converges faster. We will cover it in detail later in the chapter:<p class="source-code">optimizer = tf.optimizers.Adam(.01)</p></li>
				<li>Define the training function:<p class="source-code">def train(i):</p><p class="source-code">    for n in range(i):</p><p class="source-code">        loss=lambda: abs(tf.reduce_mean\</p><p class="source-code">                        (tf.nn.softmax_cross_entropy_with_logits(\</p><p class="source-code">                         labels=y, logits=perceptron(x))))</p><p class="source-code">        optimizer.minimize(loss, [weight, bias])</p><p>Again, the code looks very similar to the single-neuron implementation except for the loss function. Instead of <strong class="source-inline">sigmoid_cross_entropy_with_logits</strong>, we use <strong class="source-inline">softmax_cross_entropy_with_logits</strong>.</p></li>
				<li>Run the training for <strong class="source-inline">1000</strong> iterations:<p class="source-code">train(1000)</p></li>
				<li>Print the values of the weights to see if they have changed. This is also an indication that our perceptron is learning:<p class="source-code">tf.print(weight) </p><p>The output shows the learned weights of our perceptron:</p><p class="source-code">[[0.684310317 0.895633 -1.0132345]</p><p class="source-code"> [2.6424644 -1.13437736 -3.20665336]</p><p class="source-code"> [-2.96634197 -0.129377216 3.2572844]</p><p class="source-code"> [-2.97383809 -3.13501668 3.2313652]]</p></li>
				<li>To test the accuracy, we feed the features to predict the output and then calculate the accuracy using <strong class="source-inline">accuracy_score</strong>, like in the previous exercise:<p class="source-code">ypred=perceptron(x)</p><p class="source-code">ypred=tf.round(ypred)</p><p class="source-code">accuracy_score(y, ypred)</p><p>The output is:</p><p class="source-code">0.98</p><p>It has given 98% accuracy, which is pretty good.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2Dhes3U">https://packt.live/2Dhes3U</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3iJJKkm">https://packt.live/3iJJKkm</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we performed multiclass classification using our perceptron. Let's do a more complex and interesting case study of the handwritten digit recognition dataset in the next section.</p>
			<h2 id="_idParaDest-62"><a id="_idTextAnchor062"/>MNIST Case Study</h2>
			<p>N<a id="_idTextAnchor063"/>ow that we have seen how to train a single neuron and a single layer of neurons, let's take a look at more realistic data. MNIST is a famous case study. In the next exercise, we will create a 10-class classifier to classify the MNIST dataset. However, before that, you should get a good understanding of the MNIST dataset.</p>
			<p><strong class="bold">Modified National Institute of Standards and Technology</strong> (<strong class="bold">MNIST</strong>) refers to the modified dataset that the team led by Yann LeCun worked with at NIST. This project was aimed at handwritten digit recognition using neural networks.</p>
			<p>We need to understand the dataset before we get into writing the code. The MNIST dataset is integrated into the TensorFlow library. It consists of 70,000 handwritten images of the digits 0 to 9:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B15385_02_19.jpg" alt="Figure 2.19: Handwritten digits&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.19: Handwritten digits</p>
			<p>When we say images, you might think these are JPEG files, but they are not. They are actually stored in the form of pixel values. As far as the computer is concerned, an image is a bunch of numbers. These numbers are pixel values ranging from 0 to 255. The dimension of each of these images is 28 x 28. The images are stored in the form of a 28 x 28 matrix, each cell containing real numbers ranging from 0 to 255. These are grayscale images (commonly known as black and white). 0 indicates white and 1 indicates complete black, and values in between indicate a certain shade of gray. The MNIST dataset is split into 60,000 training images and 10,000 test images. </p>
			<p>Each image has a label associated with it ranging from 0 to 9. In the next exercise, let's build a 10-class classifier to classify the handwritten MNIST images.</p>
			<h2 id="_idParaDest-63">Exercise 2.04: Clas<a id="_idTextAnchor064"/>sifying Handwritten Digits</h2>
			<p>In this exercise, we will build a single-layer 10-class classifier consisting of 10 neurons with the Softmax activation function. It will have an input layer of 784 pixels:</p>
			<ol>
				<li value="1">Import the required libraries and packages just like we did in the earlier exercise:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd</p><p class="source-code">from sklearn.metrics import accuracy_score</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code">from pandas import get_dummies</p></li>
				<li>Create an instance of the MNIST dataset:<p class="source-code">mnist = tf.keras.datasets.mnist</p></li>
				<li>Load the MNIST dataset's <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong> data:<p class="source-code">(train_features, train_labels), (test_features, test_labels) = \</p><p class="source-code">mnist.load_data()</p></li>
				<li>Normalize the data:<p class="source-code">train_features, test_features = train_features / 255.0, \</p><p class="source-code">                                test_features / 255.0</p></li>
				<li>Flatten the 2-dimensional images into row matrices. So, a 28 × 28 pixel gets flattened to <strong class="source-inline">784</strong> using the <strong class="source-inline">reshape</strong> function:<p class="source-code">x = tf.reshape(train_features,[60000, 784])</p></li>
				<li>Create a <strong class="source-inline">Variable</strong> with the features and typecast it to <strong class="source-inline">float32</strong>:<p class="source-code">x = tf.Variable(x)</p><p class="source-code">x = tf.cast(x, tf.float32)</p></li>
				<li>Create a one-hot encoding of the labels and transform it into a matrix:<p class="source-code">y_hot = get_dummies(train_labels)</p><p class="source-code">y = y_hot.values</p></li>
				<li>Create the single-layer neural network with <strong class="source-inline">10</strong> neurons and train it for <strong class="source-inline">1000</strong> iterations:<p class="source-code-heading">Exercise2.04.ipynb</p><p class="source-code">#defining the parameters</p><p class="source-code">Number_of_features = 784</p><p class="source-code">Number_of_units = 10  </p><p class="source-code"># weights and bias</p><p class="source-code">weight = tf.Variable(tf.zeros([Number_of_features, \</p><p class="source-code">                               Number_of_units]))</p><p class="source-code">bias = tf.Variable(tf.zeros([Number_of_units]))</p><p class="source-code-link">The complete code for this step can be accessed from <a href="https://packt.live/3efd7Yh">https://packt.live/3efd7Yh</a>.</p></li>
				<li>Prepare the test data to measure the accuracy:<p class="source-code"># Prepare the test data to measure the accuracy. </p><p class="source-code">test = tf.reshape(test_features, [10000, 784])</p><p class="source-code">test = tf.Variable(test)</p><p class="source-code">test = tf.cast(test, tf.float32)</p><p class="source-code">test_hot = get_dummies(test_labels)</p><p class="source-code">test_matrix = test_hot.values</p></li>
				<li>Run the predictions by passing the test data through the network:<p class="source-code">ypred = perceptron(test)</p><p class="source-code">ypred = tf.round(ypred)</p></li>
				<li>Calculate the accuracy:<p class="source-code">accuracy_score(test_hot, ypred)</p><p>The predicted accuracy is:</p><p class="source-code">0.9304</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/3efd7Yh">https://packt.live/3efd7Yh</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/2Oc83ZW">https://packt.live/2Oc83ZW</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we saw how to create a single-layer multi-neuron neural network and train it as a multiclass classifier.</p>
			<p>The next step is to build a multilayer neural network. However, before we do that, we must learn about the Keras API, since we use Keras to build dense neural networks.</p>
			<h1 id="_idParaDest-64">Keras as a High-Lev<a id="_idTextAnchor065"/>el API</h1>
			<p>In TensorFlow 1.0, there were several APIs, such as Estimator, Contrib, and layers. In TensorFlow 2.0, Keras is very tightly integrated with TensorFlow, and it provides a high-level API that is user-friendly, modular, composable, and easy to extend in order to build and train deep learning models. This also makes developing code for neural networks much easier. Let's see how it works.</p>
			<h2 id="_idParaDest-65">Exercise 2.05: Bina<a id="_idTextAnchor066"/>ry Classification Using Keras</h2>
			<p>In this exercise, we will implement a very simple binary classifier with a single neuron using the Keras API. We will use the same <strong class="source-inline">data.csv</strong> file that we used in <em class="italic">Exercise 2.02</em>, <em class="italic">Perceptron as a Binary Classifier</em>:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset can be downloaded from GitHub by accessing the following GitHub link: https://packt.live/2BVtxIf.</p>
			<ol>
				<li value="1">Import the required libraries:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code"># Import Keras libraries</p><p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">from tensorflow.keras.layers import Dense</p><p>In the code, <strong class="source-inline">Sequential</strong> is the type of Keras model that we will be using because it is very easy to add layers to it. <strong class="source-inline">Dense</strong> is the type of layer that will be added. These are the regular neural network layers as opposed to the convolutional layers or pooling layers that will be used later on.</p></li>
				<li>Import the data:<p class="source-code">df = pd.read_csv('data.csv')</p></li>
				<li>Inspect the data:<p class="source-code">df.head()</p><p>The following will be the output:</p><p> </p><div id="_idContainer067" class="IMG---Figure"><img src="image/B15385_02_20.jpg" alt="Figure 2.20: Contents of the DataFrame&#13;&#10;"/></div><p class="figure-caption">Figure 2.20: Contents of the DataFrame</p></li>
				<li>Visualize the data using a scatter plot:<p class="source-code">plt.scatter(df[df['label'] == 0]['x1'], \</p><p class="source-code">            df[df['label'] == 0]['x2'], marker='*')</p><p class="source-code">plt.scatter(df[df['label'] == 1]['x1'], \</p><p class="source-code">            df[df['label'] == 1]['x2'], marker='&lt;')</p><p>The resulting plot is as follows, with the <em class="italic">x</em> axis denoting <strong class="source-inline">x1</strong> values and the y-axis denoting <strong class="source-inline">x2</strong> values:</p><div id="_idContainer068" class="IMG---Figure"><img src="image/B15385_02_21.jpg" alt="Figure 2.21: Scatter plot of the data&#13;&#10;"/></div><p> </p><p class="figure-caption">Figure 2.21: Scatter plot of the data</p></li>
				<li>Prepare the data by separating the features and labels and setting the <strong class="source-inline">tf</strong> variables:<p class="source-code">x_input = df[['x1','x2']].values</p><p class="source-code">y_label = df[['label']].values</p></li>
				<li>Create a neural network model consisting of a single layer with a neuron and a sigmoid activation function:<p class="source-code">model = Sequential()</p><p class="source-code">model.add(Dense(units=1, input_dim=2, activation='sigmoid'))</p><p>The parameters in <strong class="source-inline">mymodel.add(Dense())</strong> are as follows: <strong class="source-inline">units</strong> is the number of neurons in the layer; <strong class="source-inline">input_dim</strong> is the number of features, which in this case is <strong class="source-inline">2</strong>; and <strong class="source-inline">activation</strong> is <strong class="source-inline">sigmoid</strong>.</p></li>
				<li>Once the model is created, we use the <strong class="source-inline">compile</strong> method to pass the additional parameters that are needed for training, such as the type of the optimizer, the loss function, and so on:<p class="source-code">model.compile(optimizer='adam', \</p><p class="source-code">              loss='binary_crossentropy',\</p><p class="source-code">              metrics=['accuracy'])</p><p>In this case, we are using the <strong class="source-inline">adam</strong> optimizer, which is an enhanced version of the gradient descent optimizer, and the loss function is <strong class="source-inline">binary_crossentropy</strong>, since this is a binary classifier.</p><p>The <strong class="source-inline">metrics</strong> parameter is almost always set to <strong class="source-inline">['accuracy']</strong>, which is used to display information such as the number of epochs, the training loss, the training accuracy, the test loss, and the test accuracy during the training process.</p></li>
				<li>The model is now ready to be trained. However, it is a good idea to check the configuration of the model by using the <strong class="source-inline">summary</strong> function:<p class="source-code">model.summary()</p><p>The output will be as follows:</p><p> </p><div id="_idContainer069" class="IMG---Figure"><img src="image/B15385_02_22.jpg" alt="Figure 2.22: Summary of the sequential model&#13;&#10;"/></div><p class="figure-caption">Figure 2.22: Summary of the sequential model</p></li>
				<li>Train the model by calling the <strong class="source-inline">fit()</strong> method:<p class="source-code">model.fit(x_input, y_label, epochs=1000)</p><p>It takes the features and labels as the data parameters along with the number of epochs, which in this case is <strong class="source-inline">1000</strong>. The model will start training and will continuously provide the status as shown here:</p><p> </p><div id="_idContainer070" class="IMG---Figure"><img src="image/B15385_02_23.jpg" alt="Figure 2.23: Model training logs using Keras&#13;&#10;"/></div><p class="figure-caption">Figure 2.23: Model training logs using Keras</p></li>
				<li>We will evaluate our model using Keras's <strong class="source-inline">evaluate</strong> functionality:<p class="source-code">model.evaluate(x_input, y_label)</p><p>The output is as follows:</p><p class="source-code">21/21 [==============================] - 0s 611us/sample - loss:  0.2442 - accuracy: 1.0000</p><p class="source-code">[0.24421504139900208, 1.0]</p><p>As you can see, our Keras model is able to train well, as our accuracy is 100%.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZVV1VY">https://packt.live/2ZVV1VY</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/38CzhTc">https://packt.live/38CzhTc</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we have learned how to build a perceptron using Keras. As you have seen, Keras makes the code more modular and more readable, and the parameters easier to tweak. In the next section, we will see how to build a multilayer or deep neural network using Keras.</p>
			<h2 id="_idParaDest-66">Multilayer Neural Netwo<a id="_idTextAnchor067"/>rk or Deep Neural Network</h2>
			<p>In the previous example, we developed a single-layer neural network, often referred to as a shallow neural network. A diagram of this follows:</p>
			<p> </p>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="image/B15385_02_24.jpg" alt="Figure 2.24: Shallow neural network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.24: Shallow neural network</p>
			<p>One layer of neurons is not sufficient to solve more complex problems, such as face recognition or object detection. You need to stack up multiple layers. This is often referred to as creating a deep neural network. A diagram of this follows:</p>
			<p> </p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="image/B15385_02_25.jpg" alt="Figure 2.25: Deep neural network&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.25: Deep neural network</p>
			<p>Before we jump into the code, let's try to understand how this works. Input data is fed to the neurons in the first layer. It must be noted that every input is fed to every neuron in the first layer, and every neuron has one output. The output from each neuron in the first layer is fed to every neuron in the second layer. The output of each neuron in the second layer is fed to every neuron in the third layer, and so on.</p>
			<p>That is why this kind of network is also referred to as a dense neural network or a fully connected neural network. There are other types of neural networks with different workings, such as CNNs, but that is something we will discuss in the next chapter. There is no set rule about the number of neurons in each layer. This is usually determined by trial and error in a process known as hyperparameter tuning (which we'll learn about later in the chapter). However, when it comes to the number of neurons in the last layers, there are some restrictions. The configuration of the last layer is determined as follows:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="image/B15385_02_26.jpg" alt="Figure 2.26: Last layer configuration&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.26: Last layer configuration</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor068"/>ReLU Activation Function</h2>
			<p>O<a id="_idTextAnchor069"/>ne last thing to do before we implement the code for deep neural networks is learn about the ReLU activation function. This is one of the most popular activation functions used in multilayer neural networks.</p>
			<p><strong class="bold">ReLU</strong> is a shortened form of <strong class="bold">Rectified Linear Unit</strong>. The output of the ReLU function is always a non-negative value that is greater than or equal to 0:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="image/B15385_02_27.jpg" alt="Figure 2.27: ReLU activation function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.27: ReLU activation function</p>
			<p>The mathematical expression for ReLU is:</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="image/B15385_02_28.jpg" alt="Figure 2.28: ReLU activation function&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.28: ReLU activation function</p>
			<p>ReLU converges much more quickly than the sigmoid activation function, and therefore it is by far the most widely used activation function. ReLU is used in almost every deep neural network. It is used in all the layers except the last layer, where either sigmoid or Softmax is used.</p>
			<p>The ReLU activation function is provided by TensorFlow out of the box. To see how it is implemented, let's give some sample input values to a ReLU function and see the output:</p>
			<p class="source-code">values = tf.Variable([1.0, -2., 0., 0.3, -1.5], dtype=tf.float32)</p>
			<p class="source-code">output = tf.nn.relu(values)</p>
			<p class="source-code">tf.print(output)</p>
			<p>The output is as follows:</p>
			<p class="source-code">[1 0 0 0.3 0]</p>
			<p>As you can see, all the positive values are retained, and the negative values are suppressed to zero. Let's use this ReLU activation function in the next exercise to do a multilayer binary classification task.</p>
			<h2 id="_idParaDest-68">Exercise 2.06: Multilayer Bi<a id="_idTextAnchor070"/>nary Classifier</h2>
			<p>In this exercise, we will implement a multilayer binary classifier using the <strong class="source-inline">data.csv</strong> file that we used in <em class="italic">Exercise 2.02</em>, <em class="italic">Perceptron as a Binary Classifier</em>.</p>
			<p>We will build a binary classifier with a deep neural network of the following configuration. There will be an input layer with 2 nodes and 2 hidden layers, the first with 50 neurons and the second with 20 neurons, and lastly a single neuron to do the final prediction belonging to any binary class:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The dataset can be downloaded from GitHub using the following link: https://packt.live/2BVtxIf .</p>
			<ol>
				<li value="1">Import the required libraries and packages:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd </p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code">##Import Keras libraries</p><p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">from tensorflow.keras.layers import Dense</p></li>
				<li>Import and inspect the data:<p class="source-code">df = pd.read_csv('data.csv')</p><p class="source-code">df.head()</p><p>The output is as follows:</p><p> </p><div id="_idContainer076" class="IMG---Figure"><img src="image/B15385_02_29.jpg" alt="Figure 2.29: The first five rows of the data&#13;&#10;"/></div><p class="figure-caption">Figure 2.29: The first five rows of the data</p></li>
				<li>Visualize the data using a scatter plot:<p class="source-code">plt.scatter(df[df['label'] == 0]['x1'], \</p><p class="source-code">            df[df['label'] == 0]['x2'], marker='*')</p><p class="source-code">plt.scatter(df[df['label'] == 1]['x1'], \</p><p class="source-code">            df[df['label'] == 1]['x2'], marker='&lt;')</p><p>The resulting output is as follows, with the <em class="italic">x</em> axis showing <strong class="source-inline">x1</strong> values and the <em class="italic">y</em> axis showing <strong class="source-inline">x2</strong> values:</p><div id="_idContainer077" class="IMG---Figure"><img src="image/B15385_02_30.jpg" alt="Figure 2.30: Scatter plot for given data&#13;&#10;"/></div><p class="figure-caption">Figure 2.30: Scatter plot for given data</p></li>
				<li>Prepare the data by separating the features and labels and setting the <strong class="source-inline">tf</strong> variables:<p class="source-code">x_input = df[['x1','x2']].values</p><p class="source-code">y_label = df[['label']].values</p></li>
				<li>Build the <strong class="source-inline">Sequential</strong> model:<p class="source-code">model = Sequential()</p><p class="source-code">model.add(Dense(units = 50,input_dim=2, activation = 'relu'))</p><p class="source-code">model.add(Dense(units = 20 , activation = 'relu'))</p><p class="source-code">model.add(Dense(units = 1,input_dim=2, activation = 'sigmoid'))</p><p>Here are a couple of points to consider. We provide the input details for the first layer, then use the ReLU activation function for all the intermediate layers, as discussed earlier. Furthermore, the last layer has only one neuron with a sigmoid activation function for binary classifiers.</p></li>
				<li>Provide the training parameters using the <strong class="source-inline">compile</strong> method:<p class="source-code">model.compile(optimizer='adam', \</p><p class="source-code">              loss='binary_crossentropy', metrics=['accuracy'])</p></li>
				<li>Inspect the <strong class="source-inline">model</strong> configuration using the <strong class="source-inline">summary</strong> function:<p class="source-code">model.summary()</p><p>The output will be as follows:</p><div id="_idContainer078" class="IMG---Figure"><img src="image/B15385_02_31.jpg" alt="Figure 2.31: Deep neural network model summary using Keras&#13;&#10;"/></div><p class="figure-caption">Figure 2.31: Deep neural network model summary using Keras</p><p>In the model summary, we can see that there are a total of <strong class="source-inline">1191</strong> parameters—weights and biases—to learn across the hidden layers to the output layer.</p></li>
				<li>Train the model by calling the <strong class="source-inline">fit()</strong> method:<p class="source-code">model.fit(x_input, y_label, epochs=50)</p><p>Notice that, in this case, the model reaches 100% accuracy within <strong class="source-inline">50</strong> epochs, unlike the single-layer model, which needed about 1,000 epochs:</p><div id="_idContainer079" class="IMG---Figure"><img src="image/B15385_02_32.jpg" alt="Figure 2.32: Multilayer model train logs&#13;&#10;"/></div><p class="figure-caption">Figure 2.32: Multilayer model train logs</p></li>
				<li>Let's evaluate the model's performance:<p class="source-code">model.evaluate(x_input, y_label)</p><p>The output is as follows:</p><p class="source-code">21/21 [==============================] - 0s 6ms/sample - loss:   0.1038 - accuracy: 1.0000</p><p class="source-code">[0.1037961095571518, 1.0]</p><p>Our model has now been trained and demonstrates 100% accuracy.</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2ZUkM94">https://packt.live/2ZUkM94</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/3iKsD1W">https://packt.live/3iKsD1W</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we learned how to build a multilayer neural network using Keras. This is a binary classifier. In the next exercise, we will build a deep neural network for a multiclass classifier with the MNIST dataset.</p>
			<h2 id="_idParaDest-69">Exercise 2.07: Deep Neural Netwo<a id="_idTextAnchor071"/>rk on MNIST Using Keras</h2>
			<p>In this exercise, we will perform a multiclass classification by implementing a deep neural network (multi-layer) for the MNIST dataset where our input layer comprises 28 × 28 pixel images flattened to 784 input nodes followed by 2 hidden layers, the first with 50 neurons and the second with 20 neurons. Lastly, there will be a Softmax layer consisting of 10 neurons since we are classifying the handwritten digits into 10 classes:</p>
			<ol>
				<li value="1">Import the required libraries and packages:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd </p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">%matplotlib inline</p><p class="source-code"># Import Keras libraries</p><p class="source-code">from tensorflow.keras.models import Sequential</p><p class="source-code">from tensorflow.keras.layers import Dense</p><p class="source-code">from tensorflow.keras.layers import Flatten</p></li>
				<li>Load the MNIST data:<p class="source-code">mnist = tf.keras.datasets.mnist</p><p class="source-code">(train_features,train_labels), (test_features,test_labels) = \</p><p class="source-code">mnist.load_data()</p><p><strong class="source-inline">train_features</strong> has the training images in the form of 28 x 28 pixel values.</p><p><strong class="source-inline">train_labels</strong> has the training labels. Similarly, <strong class="source-inline">test_features</strong> has the test images in the form of 28 x 28 pixel values. <strong class="source-inline">test_labels</strong> has the test labels.</p></li>
				<li>Normalize the data:<p class="source-code">train_features, test_features = train_features / 255.0, \</p><p class="source-code">                                test_features / 255.0</p><p>The pixel values of the images range from 0-255. We need to normalize the values by dividing them by 255 so that the range goes from 0 to 1.</p></li>
				<li>Build the <strong class="source-inline">sequential</strong> model:<p class="source-code">model = Sequential()</p><p class="source-code">model.add(Flatten(input_shape=(28,28)))</p><p class="source-code">model.add(Dense(units = 50, activation = 'relu'))</p><p class="source-code">model.add(Dense(units = 20 , activation = 'relu'))</p><p class="source-code">model.add(Dense(units = 10, activation = 'softmax'))</p><p>There are couple of points to note. The first layer in this case is not actually a layer of neurons but a <strong class="source-inline">Flatten</strong> function. This flattens the 28 x 28 image into a single array of <strong class="source-inline">784</strong>, which is fed to the first hidden layer of <strong class="source-inline">50</strong> neurons. The last layer has <strong class="source-inline">10</strong> neurons corresponding to the 10 classes with a <strong class="source-inline">softmax</strong> activation function.</p></li>
				<li>Provide training parameters using the <strong class="source-inline">compile</strong> method:<p class="source-code">model.compile(optimizer = 'adam', \</p><p class="source-code">              loss = 'sparse_categorical_crossentropy', \</p><p class="source-code">              metrics = ['accuracy'])</p><p class="callout-heading">Note</p><p class="callout">The loss function used here is different from the binary classifier. For a multiclass classifier, the following loss functions are used: <strong class="source-inline">sparse_categorical_crossentropy</strong>, which is used when the labels are not one-hot encoded, as in this case; and, <strong class="source-inline">categorical_crossentropy</strong>, which is used when the labels are one-hot encoded.</p></li>
				<li>Inspect the model configuration using the <strong class="source-inline">summary</strong> function:<p class="source-code">model.summary()</p><p>The output is as follows:</p><div id="_idContainer080" class="IMG---Figure"><img src="image/B15385_02_33.jpg" alt="Figure 2.33: Deep neural network summary&#13;&#10;"/></div><p class="figure-caption">Figure 2.33: Deep neural network summary</p><p>In the model summary, we can see that there are a total of 40,480 parameters—weights and biases—to learn across the hidden layers to the output layer.</p></li>
				<li>Train the model by calling the <strong class="source-inline">fit</strong> method:<p class="source-code">model.fit(train_features, train_labels, epochs=50)</p><p>The output will be as follows:</p><div id="_idContainer081" class="IMG---Figure"><img src="image/B15385_02_34.jpg" alt="Figure 2.34: Deep neural network training logs&#13;&#10;"/></div><p class="figure-caption">Figure 2.34: Deep neural network training logs</p></li>
				<li>Test the model by calling the <strong class="source-inline">evaluate()</strong> function:<p class="source-code">model.evaluate(test_features, test_labels)</p><p>The output will be:</p><p class="source-code">10000/10000 [==============================] - 1s 76us/sample - loss:   0.2072 - accuracy: 0.9718</p><p class="source-code">[0.20719025060918111, 0.9718]</p><p>Now that the model is trained and tested, in the next few steps, we will run the prediction with some images selected randomly.</p></li>
				<li>Load a random image from a test dataset. Let's locate the 200<span class="superscript">th</span> image:<p class="source-code">loc = 200</p><p class="source-code">test_image = test_features[loc]</p></li>
				<li>Let's see the shape of the image using the following command:<p class="source-code">test_image.shape</p><p>The output is:</p><p class="source-code">(28,28)</p><p>We can see that the shape of the image is 28 x 28. However, the model expects 3-dimensional input. We need to reshape the image accordingly.</p></li>
				<li>Use the following code to reshape the image:<p class="source-code">test_image = test_image.reshape(1,28,28)</p></li>
				<li>Let's call the <strong class="source-inline">predict()</strong> method of the model and store the output in a variable called <strong class="source-inline">result</strong>:<p class="source-code">result = model.predict(test_image)</p><p class="source-code">print(result)</p><p><strong class="source-inline">result</strong> has the output in the form of 10 probability values, as shown here:</p><p class="source-code">[[2.9072076e-28 2.1215850e-29 1.7854708e-21 </p><p class="source-code">  1.0000000e+00 0.0000000e+00 1.2384960e-15 </p><p class="source-code">  1.2660366e-34 1.7712217e-32 1.7461657e-08 </p><p class="source-code">  9.6417470e-29]]</p></li>
				<li>The position of the highest value will be the prediction. Let's use the <strong class="source-inline">argmax</strong> function we learned about in the previous chapter to find out the prediction:<p class="source-code">result.argmax()</p><p>In this case, it is <strong class="source-inline">3</strong>:</p><p class="source-code">3</p></li>
				<li>In order to check whether the prediction is correct, we check the label of the corresponding image:<p class="source-code">test_labels[loc]</p><p>Again, the value is <strong class="source-inline">3</strong>:</p><p class="source-code">3</p></li>
				<li>We can also visualize the image using <strong class="source-inline">pyplot</strong>:<p class="source-code">plt.imshow(test_features[loc])</p><p>The output will be as follows:</p><div id="_idContainer082" class="IMG---Figure"><img src="image/B15385_02_35.jpg" alt="Figure 2.35: Test image visualized&#13;&#10;"/></div></li>
			</ol>
			<p class="figure-caption">Figure 2.35: Test image visualized</p>
			<p>And this shows that the prediction is correct.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2O5KRgd">https://packt.live/2O5KRgd</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/2O8JHR0">https://packt.live/2O8JHR0</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>In this exercise, we created a multilayer multiclass neural network model using Keras to classify the MNIST data. With the model we built, we were able to correctly predict a random handwritten digit.</p>
			<h1 id="_idParaDest-70">Exploring the Optimizers and Hyperp<a id="_idTextAnchor072"/>arameters of Neural Networks</h1>
			<p>Training a neural network to get good predictions requires tweaking a lot of hyperparameters such as optimizers, activation functions, the number of hidden layers, the number of neurons in each layer, the number of epochs, and the learning rate. Let's go through each of them one by one and discuss them in detail.</p>
			<h2 id="_idParaDest-71"><a id="_idTextAnchor073"/>Gradient Descent Optimizers</h2>
			<p>In an e<a id="_idTextAnchor074"/><a id="_idTextAnchor075"/>arlier section titled <em class="italic">Perceptron Training Process in TensorFlow</em>, we briefly touched upon the gradient descent optimizer without going into the details of how it works. This is a good time to explore the gradient descent optimizer in a little more detail. We will provide an intuitive explanation without going into the mathematical details.</p>
			<p>The gradient descent optimizer's function is to minimize the loss or error. To understand how gradient descent works, you can think of this analogy: imagine a person at the top of a hill who wants to reach the bottom. At the beginning of the training, the loss is large, like the height of the hill's peak. The functioning of the optimizer is akin to the person descending the hill to the valley at the bottom, or rather, the lowest point of the hill, and not climbing up the hill that is on the other side of the valley.</p>
			<p>Remember the learning rate parameter that we used while creating the optimizer? That can be compared to the size of the steps the person takes to climb down the hill. If these steps are large, it is fine at the beginning since the person can climb down faster, but once they near the bottom, if the steps are too large, the person crosses over to the other side of the valley. Then, in order to climb back down to the bottom of the valley, the person will try to move back but will move over to the other side again. This results in going back and forth without reaching the bottom of the valley.</p>
			<p>On the other hand, if the person takes very small steps (a very small learning rate), they will take forever to reach the bottom of the valley; in other words, the model will take forever to converge. So, finding a learning rate that is neither too small nor too big is very important. However, unfortunately, there is no rule of thumb to find out in advance what the right value should be—we have to find it by trial and error.</p>
			<p>There are two main types of gradient-based optimizers: batch and stochastic gradient descent. Before we jump into them, let's recall that one epoch means a training iteration where the neural network goes through all the training examples:</p>
			<ul>
				<li>In an epoch, when we reduce the loss across all the training examples, it is called <strong class="bold">batch gradient descent</strong>. This is also known as <strong class="bold">full batch gradient descent</strong>. To put it simply, after going through a full batch, we take a step to adjust the weights and biases of the network to reduce the loss and improve the predictions. There is a similar form of it called mini-batch gradient descent, where we take steps, that is, we adjust weights and biases, after going through a subset of the full dataset.</li>
				<li>In contrast to batch gradient descent, when we take a step at one example per iteration, we have <strong class="bold">stochastic gradient descent</strong> (<strong class="bold">SGD</strong>). The word <em class="italic">stochastic</em> tells us there is randomness involved here, which, in this case, is the batch that is randomly selected.</li>
			</ul>
			<p>Though SGD works relatively well, there are advanced optimizers that can speed up the training process. They include SGD with momentum, Adagrad, and Adam.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor076"/>The Vanishing Gradient Problem</h2>
			<p>In the <em class="italic">Training a Perceptron</em> section, we learned about the forward and backward propagation of neural networks. When a neural network performs forward propagation, the error gradient is calculated with respect to the true label, and backpropagation is performed to see which parameters (the weights and biases) of the neural network have contributed to the error and the extent to which they have done so. The error gradient is propagated from the output layer to the input layer to calculate gradients with respect to each parameter, and in the last step, the gradient descent step is performed to adjust the weights and biases according to the calculated gradient. As the error gradient is propagated backward, the gradients calculated at each parameter become smaller and smaller as it advances to the lower (initial) layers. This decrease in the gradients means that the changes to the weights and biases become smaller and smaller. Hence, our neural network struggles to find the global minimum and does not give good results. This is called the vanishing gradient problem. The problem happens with the use of the sigmoid (logistic) function as an activation function, and hence we use the ReLU activation function to train deep neural network models to avoid gradient complications and improve the results.</p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor077"/>Hyperparameter Tuning</h2>
			<p>Like any othe<a id="_idTextAnchor078"/>r model training process in machine learning, it is possible to perform hyperparameter tuning to improve the performance of the neural network model. One of the parameters is the learning rate. The other parameters are as follows:</p>
			<ul>
				<li><strong class="bold">Number of epochs</strong>: Increasing the number of epochs generally increases the accuracy and lowers the loss</li>
				<li><strong class="bold">Number of layers</strong>: Increasing the number of layers increases the accuracy, as we saw in the exercises with MNIST</li>
				<li><strong class="bold">Number of neurons per layer</strong>: This also increases the accuracy</li>
			</ul>
			<p>And once again, there is no way to know in advance what the right number of layers or the right number of neurons per layer is. This has to be figured out by trial and error. It has to be noted that the larger the number of layers and the larger the number of neurons per layer, the greater the computational power required. Therefore, we start with the smallest possible numbers and slowly increase the number of layers and neurons.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor079"/>Overfitting and Dropout</h2>
			<p>Neural netw<a id="_idTextAnchor080"/>orks with complex architectures and too many parameters tend to fit on all the data points, including noisy labels, leading to the problem of overfitting and neural networks that are not able to generalize well on unseen datasets. To tackle this issue, there is a technique called <strong class="bold">dropout</strong>:</p>
			<p> </p>
			<div>
				<div id="_idContainer083" class="IMG---Figure">
					<img src="image/B15385_02_36.jpg" alt="Figure 2.36: Dropout illustrated&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.36: Dropout illustrated</p>
			<p>In this technique, a certain number of neurons are deactivated randomly during the training process. The number of neurons to be deactivated is provided as a parameter in the form of a percentage. For example, <strong class="source-inline">Dropout = .2</strong> means 20% of the neurons in that layer will be randomly deactivated during the training process. The same neurons are not deactivated more than once, but a different set of neurons is deactivated in each epoch. During testing, however, all the neurons are activated.</p>
			<p>Here is an example of how we can add <strong class="source-inline">Dropout</strong> to a neural network model using Keras:</p>
			<p class="source-code">model.add(Dense(units = 300, activation = 'relu')) #Hidden layer1</p>
			<p class="source-code">model.add(Dense(units = 200, activation = 'relu')) #Hidden Layer2</p>
			<p class="source-code">model.add(Dropout(.20))</p>
			<p class="source-code">model.add(Dense(units = 100, activation = 'relu')) #Hidden Layer3</p>
			<p>In this case, a dropout of 20% is added to <strong class="source-inline">Hidden Layer2</strong>. It is not necessary for the dropout to be added to all layers. As a data scientist, you can experiment and decide what the <strong class="source-inline">dropout</strong> value should be and how many layers need it.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">A more detailed explanation of dropout can be found in the paper by Nitish Srivastava et al. available here: <a href="http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf">http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf</a>.</p>
			<p>As we have come to the end of this chapter, let's test what we have learned so far with the following activity.</p>
			<h1 id="_idParaDest-75">Activity 2.01: Build a Multilayer Ne<a id="_idTextAnchor081"/>ural Network to Classify Sonar Signals</h1>
			<p>In this activity, we will use the Sonar dataset (<a href="https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)">https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)</a>), which has patterns obtained by bouncing sonar signals off a metal cylinder at various angles and under various conditions. You will build a neural network-based classifier to classify between sonar signals bounced off a metal cylinder (the Mine class), and those bounced off a roughly cylindrical rock (the Rock class). We recommend using the Keras API to make your code more readable and modular, which will allow you to experiment with different parameters easily:</p>
			<p class="callout-heading">Note</p>
			<p class="callout">You can download the sonar dataset from this link <a href="https://packt.live/31Xtm9M">https://packt.live/31Xtm9M</a>.</p>
			<ol>
				<li value="1">The first step is to understand the data so that you can figure out whether this is a binary classification problem or a multiclass classification problem.</li>
				<li>Once you understand the data and the type of classification that needs to be done, the next step is network configuration: the number of neurons, the number of hidden layers, which activation function to use, and so on.<p>Recall the network configuration steps that we've covered so far. Let's just reiterate a crucial point, the activation function part: for the output (the last) layer, we use sigmoid to do binary classification and Softmax to do multiclass classification.</p></li>
				<li>Open the <strong class="source-inline">sonar.csv</strong> file to explore the dataset and see what the target variables are.</li>
				<li>Separate the input features and the target variables.</li>
				<li>Preprocess the data to make it neural network-compatible. Hint: one-hot encoding.</li>
				<li>Define a neural network using Keras and compile it with the right loss function.</li>
				<li>Print out a model summary to verify the network parameters and considerations.</li>
			</ol>
			<p>You are expected to get an accuracy value above 95% by designing a proper multilayer neural network using these steps.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The detailed steps for this activity, along with the solutions and additional commentary, are presented on page 390.</p>
			<h1 id="_idParaDest-76"><a id="_idTextAnchor082"/>Summary</h1>
			<p>In this chapter, we started <a id="_idTextAnchor083"/><a id="_idTextAnchor084"/>off by looking at biological neurons and then moved on to artificial neurons. We saw how neural networks work and took a practical approach to building single-layer and multilayer neural networks to solve supervised learning tasks. We looked at how a perceptron works, which is a single unit of a neural network, all the way to a deep neural network capable of performing multiclass classification. We saw how Keras makes it very easy to create deep neural networks with a minimal amount of code. Lastly, we looked at practical considerations to take into account when building a successful neural network, which involved important concepts such as gradient descent optimizers, overfitting, and dropout.</p>
			<p>In the next chapter, we will go to the next level and build a more complicated neural network called a CNN, which is widely used in image recognition.</p>
		</div>
	</body></html>