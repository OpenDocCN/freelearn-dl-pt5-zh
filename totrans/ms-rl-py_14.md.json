["```py\n    virtualenv coinenv\n    source coinenv/bin/activate\n    ```", "```py\n    sudo apt-get install mpich build-essential qt5-default pkg-config\n    ```", "```py\n    git clone https://github.com/openai/coinrun.git\n    cd coinrun\n    pip install tensorflow==1.15.3 # or tensorflow-gpu\n    pip install -r requirements.txt\n    pip install -e .\n    ```", "```py\n    python -m coinrun.interactive\n    ```", "```py\npython -m coinrun.train_agent –help\n```", "```py\npython -m coinrun.train_agent --run-id BaseAgent --num-levels 500 --num-envs 60\n```", "```py\nmpiexec -np 3 python -m coinrun.enjoy --test-eval --restore-id BaseAgent -num-eval 20 -rep 5\n```", "```py\npython -m coinrun.enjoy --restore-id BaseAgent –hres\n```", "```py\npython -m coinrun.train_agent --run-id LargeAgent --num-levels 500 --num-envs 60 --architecture impalalarge\n```", "```py\npython -m coinrun.train_agent --run-id AgentDOut01 --num-levels 500 --num-envs 60 --dropout 0.1\n```", "```py\npython -m coinrun.train_agent --run-id AgentL2_00001 --num-levels 500 --num-envs 60 --l2-weight 0.0001\n```", "```py\nfrom tensorflow.keras import layers \n...\nx = layers.Dense(512, activation=\"relu\")(x) \nx = layers.Dropout(0.1)(x)\n...\n```", "```py\nfrom tensorflow.keras import regularizers\n...\nx = layers.Dense(512, activation=\"relu\", kernel_regularizer=regularizers.l2(0.0001))(x) \n```", "```py\npython -m coinrun.train_agent --run-id AgentL2_00001 --num-levels 500 --num-envs 60 --use-data-augmentation 1\n```", "```py\nclass RndDense(tf.keras.layers.Layer):\n    def __init__(self, units=32):\n        super(RndDense, self).__init__()\n        self.units = units\n    def build(self, input_shape):  \n        self.w_init = tf.keras.initializers.GlorotNormal()\n        self.w_shape = (input_shape[-1], self.units)\n        self.w = tf.Variable(\n            initial_value=self.w_init(shape=self.w_shape, dtype=\"float32\"),\n            trainable=True,\n        )\n    def call(self, inputs):  \n        self.w.assign(self.w_init(shape=self.w_shape, dtype=\"float32\"))\n        return tf.nn.relu(tf.matmul(inputs, self.w))\n```", "```py\nimport ray\nfrom ray.tune.logger import pretty_print\nfrom ray.rllib.agents.ppo.ppo import PPOTrainer\nfrom ray.rllib.agents.ppo.ppo import DEFAULT_CONFIG\nconfig = DEFAULT_CONFIG.copy()\nconfig[\"model\"][\"use_lstm\"] = True\n# The length of the input sequence\nconfig[\"model\"][\"max_seq_len\"] = 8\n# Size of the hidden state\nconfig[\"model\"][\"lstm_cell_size\"] = 64\n# Whether to use\nconfig[\"model\"][\"lstm_use_prev_action_reward\"] = True\n```", "```py\nconfig[\"model\"][\"fcnet_hiddens\"] = [32]\nconfig[\"model\"][\"fcnet_activation\"] = \"linear\"\n```", "```py\nfrom ray.tune.registry import register_env\ndef env_creator(env_config):\n    return MyEnv(env_config)    # return an env instance\nregister_env(\"my_env\", env_creator)\nconfig[\"env\"] = \"my_env\"\nray.init()\ntrainer = PPOTrainer(config=config)\nwhile True:\n    results = trainer.train()\n    print(pretty_print(results))\n    if results[\"timesteps_total\"] >= MAX_STEPS:\n        break\nprint(trainer.save())\n```", "```py\n...\nfrom ray.rllib.models.tf.attention_net import GTrXLNet\n...\nconfig[\"model\"] = {\n    \"custom_model\": GTrXLNet,\n    \"max_seq_len\": 50,\n    \"custom_model_config\": {\n        \"num_transformer_units\": 1,\n        \"attn_dim\": 64,\n        \"num_heads\": 2,\n        \"memory_tau\": 50,\n        \"head_dim\": 32,\n        \"ff_hidden_dim\": 32,\n    },\n}\n```"]