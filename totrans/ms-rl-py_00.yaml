- en: Preface
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Reinforcement** **Learning** (**RL**) is a field of artificial intelligence
    used for creating self-learning autonomous agents. This book takes a pragmatic
    approach to RL and uses practical examples inspired by real-world business and
    industry problems to teach you about RL techniques.'
  prefs: []
  type: TYPE_NORMAL
- en: Starting with an overview of RL elements, you'll get to grips with Markov chains
    and Markov decision processes, which comprise the mathematical foundations of
    modeling an RL problem. You'll then cover Monte Carlo methods and **temporal**
    **difference** (**TD**) learning methods that are used for solving RL problems.
    Next, you'll learn about deep Q-learning, policy gradient algorithms, actor-critic
    methods, model-based methods, and multi-agent reinforcement learning. As you advance,
    you'll delve into many novel algorithms with advanced implementations using modern
    Python libraries. You'll also find out how to implement RL to solve real-world
    challenges faced in areas such as autonomous systems, supply chain management,
    games, finance, smart cities, and cybersecurity. Finally, you'll gain a clear
    understanding of which method to use and when, how to avoid common pitfalls, and
    how to overcome challenges faced in implementing RL.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this reinforcement learning book, you'll have mastered how to
    train and deploy your own RL agents for solving RL problems.
  prefs: []
  type: TYPE_NORMAL
- en: Who this book is for
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This book is for expert machine learning practitioners and deep learning researchers
    looking to implement advanced RL concepts in real-world projects. This book will
    also appeal to RL experts who want to tackle complex sequential decision-making
    problems through self-learning agents. Working knowledge of Python programming
    and machine learning along with prior experience RL is required.
  prefs: []
  type: TYPE_NORMAL
- en: What this book covers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '[*Chapter 1*](B14160_01_Final_SK_ePub.xhtml#_idTextAnchor016), *Introduction
    to Reinforcement Learning*, provides an introduction to RL, gives motivating examples
    and success stories, and looks at RL applications in industry. It then gives fundamental
    definitions to refresh your mind on RL concepts and concludes with a section on
    software and hardware setup.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 2*](B14160_02_Final_SK_ePub.xhtml#_idTextAnchor038), *Multi-Armed
    Bandits*, covers a rather simpler RL setting, that is, bandit problems without
    context, which have tremendous applications in industry as an alternative to the
    traditional A/B testing. The chapter also serves as an introduction to a very
    fundamental RL concept: exploration versus exploitation. We also solve a prototype
    online advertising case with four different methods.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 3*](B14160_03_Final_SK_ePub.xhtml#_idTextAnchor059), *Contextual
    Bandits*, takes the discussion on MABs to an advanced level by adding context
    to the decision-making process and involving deep neural networks in decision
    making. We adapt a real dataset from the US Census to an online advertising problem.
    We conclude the chapter with a section on the applications of bandit problems
    in industry and business.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 4*](B14160_04_Final_SK_ePub.xhtml#_idTextAnchor080), *Making of the
    Markov Decision Process*, builds the mathematical theory with which we model RL
    problems. We start with Markov chains, where we describe types of states, ergodicity,
    transitionary, and steady-state behavior. Then we go into Markov reward and decision
    processes. Along the way, we introduce return, discount, policy, and value functions,
    and Bellman optimality, which are key concepts in RL theory that will be frequently
    referred to in later chapters. We conclude the chapter with a discussion on partially
    observed Markov decision processes. Throughout the chapter, we use a grid world
    example to illustrate the concepts.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 5*](B14160_05_Final_SK_ePub.xhtml#_idTextAnchor106), *Solving the
    Reinforcement Learning Problem*, introduces DP methods, which are fundamental
    to understanding how to solve an MDP. Key concepts such as policy evaluation,
    policy iteration, and value iteration are introduced and illustrated. Throughout
    the chapter, we solve an example inventory replenishment problem. We conclude
    the chapter with a discussion on the issues with using DP in real-world examples.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B14160_06_Final_SK_ePub.xhtml#_idTextAnchor124), *Deep Q-Learning
    at Scale*, provides an introduction to deep RL and covers deep Q-learning end
    to end. We start with a discussion on why deep RL is needed, then introduce RLlib,
    a popular and scalable RL library. After introducing the case studies we will
    work with (one simple, one medium-difficulty, and one video game example), we
    will build deep Q-learning methods from fitted Q-iteration to DQN to Rainbow.
    Then we will go into more advanced topics on distributed DQN (APEX), continuous
    DQN, and a discussion on important hyperparameters to tune. For classical DQN,
    you will implement it in TensorFlow. For Rainbow, we will use RLlib.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B14160_07_Final_SK_ePub.xhtml#_idTextAnchor147), *Policy-Based
    Methods*, introduces the second important class of RL methods: policy-based methods.
    You will first learn how they are different and why they are needed. We then go
    into the details of several state-of-the-art policy gradient and trust region
    methods. We conclude the chapter with Actor-Critic algorithms. We mostly rely
    on RLlib implementations of these algorithms and focus on how and when to use
    them rather than lengthy implementation details.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B14160_08_Final_SK_ePub.xhtml#_idTextAnchor177), *Model-Based
    Methods*, shows what assumptions model-based methods make and what advantages
    they have over other methods. We also discuss the model behind the famous AlphaGo
    Zero. We conclude the chapter with an exercise that uses a model-based algorithm.
    The chapter includes a mix of using manual and RLlib implementations.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B14160_09_Final_SK_ePub.xhtml#_idTextAnchor200), *Multi-Agent
    Reinforcement Learning*, gives you a framework to model multi-agent RL problems
    and introduces MADDPG to solve such problems. The chapter uses an RLlib MADDPG
    implementation.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B14160_10_Final_SK_ePub.xhtml#_idTextAnchor220), *Machine Teaching*,
    discusses the machine teaching approach to break down complex problems into smaller
    pieces and make them solvable. This approach is necessary for many real-life problems
    and you will learn practical tips and tricks for how to design an RL model and
    go beyond algorithm selection in solving RL problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 11*](B14160_11_Final_SK_ePub.xhtml#_idTextAnchor239), *Generalization
    and Domain Randomization*, covers why partial observability and the sim2real gap
    are a problem, and how to overcome those by using LSTM-like models and domain
    randomization.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 12*](B14160_12_Final_SK_ePub.xhtml#_idTextAnchor260), *Meta-Reinforcement
    Learning*, introduces approaches that allow us to use a single model for multiple
    tasks. As sample efficiency is a major problem in RL, this chapter exposes you
    to a very important future direction in RL.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 13*](B14160_13_Final_SK_ePub.xhtml#_idTextAnchor276), *Other Advanced
    Topics*, introduces cutting-edge RL research. Many approaches discussed so far
    have certain assumptions and limitations. The topics discussed in this chapter
    address these limitations and give ideas about how to overcome them. At the end
    of this chapter, you will learn which approaches to look into when you hit the
    limitations of the algorithms we covered in earlier chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 14*](B14160_14_Final_SK_ePub.xhtml#_idTextAnchor306), *Autonomous
    Systems*, covers the potential of RL for creating real-life autonomous systems.
    We cover success stories and sample problems for autonomous robots and self-driving
    cars.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 15*](B14160_15_Final_SK_ePub.xhtml#_idTextAnchor329), *Supply Chain
    Management*, gives you hands-on experience in inventory planning and bin packing
    problems. We model them as an RL problem and solve sample cases.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 16*](B14160_16_Final_SK_ePub.xhtml#_idTextAnchor348), *Marketing,
    Personalization, and Finance*, covers RL applications in marketing, advertising,
    recommendation systems, and finance. This chapter gives you a broad understanding
    of how RL can be utilized in business, and what the opportunities and limitations
    are. In this chapter, we also go into examples of contextual multi-armed bandit
    problems.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 17*](B14160_17_Final_SK_ePub.xhtml#_idTextAnchor365), *Smart City
    and Cybersecurity*, covers sample problems from the area of smart cities and cybersecurity,
    such as traffic control, service provision regulation, and intrusion detection.
    We also discuss how multi-agent approaches can be used in these applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 18*](B14160_18_Final_SK_ePub.xhtml#_idTextAnchor388), *Challenges
    and Future Directions in Reinforcement Learning*, goes into the details of what
    these challenges are and what state-of-the-art research suggests to overcome them.
    This chapter teaches you how to assess the feasibility of the RL approach for
    a given problem.'
  prefs: []
  type: TYPE_NORMAL
- en: To get the most out of this book
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**If you are using the digital version of this book, we advise you to type
    the code yourself or access the code via the GitHub repository (link available
    in the next section). Doing so will help you avoid any potential errors related
    to the copying and pasting of code.**'
  prefs: []
  type: TYPE_NORMAL
- en: Download the example code files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can download the example code files for this book from your account at [www.packt.com](http://www.packt.com).
    If you purchased this book elsewhere, you can visit [www.packtpub.com/support](http://www.packtpub.com/support)
    and register to have the files emailed directly to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the code files by following these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Log in or register at [www.packt.com](http://www.packt.com).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Select the **Support** tab.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Click on **Code Downloads**.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Enter the name of the book in the **Search** box and follow the onscreen instructions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once the file is downloaded, please make sure that you unzip or extract the
    folder using the latest version of:'
  prefs: []
  type: TYPE_NORMAL
- en: WinRAR/7-Zip for Windows
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Zipeg/iZip/UnRarX for Mac
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 7-Zip/PeaZip for Linux
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code bundle for the book is also hosted on GitHub at [https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python).
    In case there's an update to the code, it will be updated on the existing GitHub
    repository.
  prefs: []
  type: TYPE_NORMAL
- en: We also have other code bundles from our rich catalog of books and videos available
    at [https://github.com/PacktPublishing/](https://github.com/PacktPublishing/).
    Check them out!
  prefs: []
  type: TYPE_NORMAL
- en: Download the color images
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We also provide a PDF file that has color images of the screenshots/diagrams
    used in this book. You can download it here: [https://static.packt-cdn.com/downloads/9781838644147_ColorImages.pdf](https://static.packt-cdn.com/downloads/9781838644147_ColorImages.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: Conventions used
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are a number of text conventions used throughout this book.
  prefs: []
  type: TYPE_NORMAL
- en: '`Code in text`: Indicates code words in text, database table names, folder
    names, filenames, file extensions, pathnames, dummy URLs, user input, and Twitter
    handles. Here is an example: "Install NVIDIA Modprobe, for example, for Ubuntu,
    using `sudo apt-get install nvidia-modprobe`."'
  prefs: []
  type: TYPE_NORMAL
- en: 'A block of code is set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'When we wish to draw your attention to a particular part of a code block, the
    relevant lines or items are set in bold:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Tips or important notes
  prefs: []
  type: TYPE_NORMAL
- en: Appear like this.
  prefs: []
  type: TYPE_NORMAL
- en: Get in touch
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Feedback from our readers is always welcome.
  prefs: []
  type: TYPE_NORMAL
- en: '**General feedback**: If you have questions about any aspect of this book,
    mention the book title in the subject of your message and email us at [customercare@packtpub.com](mailto:customercare@packtpub.com).'
  prefs: []
  type: TYPE_NORMAL
- en: '**Errata**: Although we have taken every care to ensure the accuracy of our
    content, mistakes do happen. If you have found a mistake in this book, we would
    be grateful if you would report this to us. Please visit [www.packtpub.com/support/errata](http://www.packtpub.com/support/errata),
    selecting your book, clicking on the Errata Submission Form link, and entering
    the details.'
  prefs: []
  type: TYPE_NORMAL
- en: '**Piracy**: If you come across any illegal copies of our works in any form
    on the Internet, we would be grateful if you would provide us with the location
    address or website name. Please contact us at [copyright@packt.com](mailto:copyright@packt.com)
    with a link to the material.'
  prefs: []
  type: TYPE_NORMAL
- en: '**If you are interested in becoming an author**: If there is a topic that you
    have expertise in and you are interested in either writing or contributing to
    a book, please visit [authors.packtpub.com](http://authors.packtpub.com).'
  prefs: []
  type: TYPE_NORMAL
- en: Reviews
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Please leave a review. Once you have read and used this book, why not leave
    a review on the site that you purchased it from? Potential readers can then see
    and use your unbiased opinion to make purchase decisions, we at Packt can understand
    what you think about our products, and our authors can see your feedback on their
    book. Thank you!
  prefs: []
  type: TYPE_NORMAL
- en: For more information about Packt, please visit [packt.com](http://packt.com).
  prefs: []
  type: TYPE_NORMAL
