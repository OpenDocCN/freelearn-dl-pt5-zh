["```py\nimport sklearn\nimport os\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport nltk\nfrom sklearn.datasets import load_files\npath = './movie_reviews/'\n# we will consider only the most 1000 common words\nmax_tokens = 1000\n# load files -- there are 2000 files\nmovie_reviews = load_files(path)\n# the names of the categories (the labels) are automatically generated from the names of the folders in path\n# 'pos' and 'neg'\nlabels = movie_reviews.target_names\n# Split data into training and test sets\n# since this is just an example, we will omit the dev test set\n# 'movie_reviews.data' is the movie reviews\n# 'movie_reviews.target' is the categories assigned to each review\n# 'test_size = .20' is the proportion of the data that should be reserved for testing\n# 'random_state = 42' is an integer that controls the randomization of the\n# data so that the results are reproducible\nfrom sklearn.model_selection import train_test_split\nmovies_train, movies_test, sentiment_train, sentiment_test = train_test _split(movie_reviews.data,                                                                          movie_reviews.target,                                                                                                                                                    test_size  = 0.20,                                                                                                                                                      random_state = 42)\n```", "```py\n# initialize TfidfVectorizer to create the tfIdf representation of the corpus\n# the parameters are: min_df -- the percentage of documents that the word has\n# to occur in to be considered, the tokenizer to use, and the maximum\n# number of words to consider (max_features)\nvectorizer = TfidfVectorizer(min_df = .1,\n                             tokenizer = nltk.word_tokenize,\n                             max_features = max_tokens)\n# fit and transform the text into tfidf format, using training text\n# here is where we build the tfidf representation of the training data\nmovies_train_tfidf = vectorizer.fit_transform(movies_train)\n```", "```py\nfrom sklearn.naive_bayes import MultinomialNB\n# Initialize the classifier and train it\nclassifier = MultinomialNB()\nclassifier.fit(movies_train_tfidf, sentiment_train)\n```", "```py\n# find accuracy based on test set\nmovies_test_tfidf = vectorizer.fit_transform(movies_test)\n# for each document in the test data, use the classifier to predict whether its sentiment is positive or negative\nsentiment_pred = classifier.predict(movies_test_tfidf)\nsklearn.metrics.accuracy_score(sentiment_test,\n    sentiment_pred)\n0.64\n# View the results as a confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(sentiment_test,\n    sentiment_pred,normalize=None)\nprint(conf_matrix)\n[[132  58]\n [ 86 124]]\n```", "```py\nimport numpy as np\nfrom sklearn.datasets import load_files\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\n# the directory root will be wherever the movie review data is located\ndirectory_root = \"./lab/movie_reviews/\"\nmovie_reviews = load_files(directory_root,\n    encoding='utf-8',decode_error=\"replace\")\n# count the number of reviews in each category\nlabels, counts = np.unique(movie_reviews.target,\n    return_counts=True)\n# convert review_data.target_names to np array\nlabels_str = np.array(movie_reviews.target_names)[labels]\nprint(dict(zip(labels_str, counts)))\n{'neg': 1000, 'pos': 1000}\nfrom sklearn.model_selection import train_test_split\nmovies_train, movies_test, sentiment_train, sentiment_test\n    = train_test_split(movie_reviews.data,\n        movie_reviews.target, test_size = 0.20,\n        random_state = 42)\n```", "```py\n# We will work with a TF_IDF representation, as before\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import classification_report, accuracy_score\n# Use the Pipeline function to construct a list of two processes\n# to run, one after the other -- the vectorizer and the classifier\nsvc_tfidf = Pipeline([\n        (\"tfidf_vectorizer\", TfidfVectorizer(\n        stop_words = \"english\", max_features=1000)),\n        (\"linear svc\", SVC(kernel=\"linear\"))\n    ])\nmodel = svc_tfidf\nmodel.fit(movies_train, sentiment_train)\nsentiment_pred = model.predict(movies_test)\naccuracy_result = accuracy_score( sentiment_test,\n    sentiment_pred)\nprint(accuracy_result)\n0.8125\n# View the results as a confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconf_matrix = confusion_matrix(sentiment_test,\n    sentiment_pred,normalize=None)\nprint(conf_matrix)\n[[153  37]\n [ 38 172]]\n```", "```py\nfrom sklearn.multiclass import OneVsRestClassifier\nmodel = OneVsRestClassifier(SVC())\n```", "```py\n{tokens\": \"show me science fiction films directed by steven spielberg\"\n\"entities\": [\n  {\"entity\": {\n    \"tokens\": \"science fiction films\",\n    \"name\": \"GENRE\"\n    }},\n    {\n    \"entity\": {\n     \"tokens\": \"steven spielberg\",\n     \"name\": \"DIRECTOR\"\n     }}\n     ]\n     }\n```", "```py\nfrom IPython.display import display, Markdown\nwith open(\"examples/restaurant_search.md\", \"r\") as f:\n    display(Markdown(f.read()))\n```", "```py\nimport sklearn_crfsuite\nfrom spacy_crfsuite import read_file\ntrain_data = read_file(\"examples/restaurant_search.md\")\ntrain_data\nIn [ ]:\nimport spacy\nfrom spacy_crfsuite.tokenizer import SpacyTokenizer\nfrom spacy_crfsuite.train import gold_example_to_crf_tokens\nnlp = spacy.load(\"en_core_web_sm\", disable=[\"ner\"])\ntokenizer = SpacyTokenizer(nlp)\ntrain_dataset = [\n    gold_example_to_crf_tokens(ex, tokenizer=tokenizer)\n    for ex in train_data\n]\ntrain_dataset[0]\n```", "```py\nfrom spacy_crfsuite import CRFExtractor\ncrf_extractor = CRFExtractor(\n    component_config=component_config)\ncrf_extractor\nrs = crf_extractor.fine_tune(train_dataset, cv=5,\n    n_iter=50, random_state=42)\nprint(\"best_params:\", rs.best_params_, \", score:\",\n    rs.best_score_)\ncrf_extractor.train(train_dataset)\nclassification_report = crf_extractor.eval(train_dataset)\nprint(classification_report[1])\n```", "```py\n              precision    recall  f1-score   support\nU-atmosphere      1.000     1.000     1.000         1\n   U-cuisine      1.000     1.000     1.000         9\n  U-location      1.000     1.000     1.000         6\n      U-meal      1.000     1.000     1.000         2\n     B-price      1.000     1.000     1.000         1\n     I-price      1.000     1.000     1.000         1\n     L-price      1.000     1.000     1.000         1\n   U-quality      1.000     1.000     1.000         1\n   micro avg      1.000     1.000     1.000        22\n   macro avg      1.000     1.000     1.000        22\nweighted avg      1.000     1.000     1.000        22\n```", "```py\nexample = {\"text\": \"show some good chinese restaurants near me\"}\ntokenizer.tokenize(example, attribute=\"text\")\ncrf_extractor.process(example)\n[{'start': 10,\n  'end': 14,\n  'value': 'good',\n  'entity': 'quality',\n  'confidence': 0.9468721304898786},\n {'start': 15,\n  'end': 22,\n  'value': 'chinese',\n  'entity': 'cuisine',\n  'confidence': 0.9591743424660175}]\n```", "```py\n[{'start': 10,\n  'end': 14,\n  'value': 'good',\n  'entity': 'quality',\n  'confidence': 0.6853277275481114},\n {'start': 15,\n  'end': 23,\n  'value': 'japanese',\n  'entity': 'cuisine',\n  'confidence': 0.537198793062902}]\n```"]