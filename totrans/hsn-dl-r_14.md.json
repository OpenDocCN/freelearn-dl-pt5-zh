["```py\nreset = function(self) {\n  position = runif(1, -0.6, -0.4)\n  velocity = 0\n  state = matrix(c(position, velocity), ncol = 2)\n  state\n}\n\n```", "```py\n  step = function(self, action) {\n  position = self$state[1]\n  velocity = self$state[2]\n  velocity = (action - 1L) * 0.001 + cos(3 * position) * (-0.0025)\n  velocity = min(max(velocity, -0.07), 0.07)\n  position = position + velocity\n  if (position < -1.2) {\n    position = -1.2\n    velocity = 0\n  }\n  state = matrix(c(position, velocity), ncol = 2)\n  reward = -1\n  if (position >= 0.5) {\n    done = TRUE\n    reward = 0\n  } else {\n    done = FALSE\n  }\n  list(state, reward, done)\n}\n```", "```py\nportable = FALSE,\nlock_objects = FALSE,\npublic = list(\n  state_size = NULL,\n  action_size = NA, \n  initialize = function(state_size, action_size) {\n      self$state_size = state_size\n      self$action_size = action_size\n      self$memory = deque()\n      self$gamma = 0.95 \n      self$epsilon = 1.0 \n      self$epsilon_min = 0.01\n      self$epsilon_decay = 0.995\n      self$learning_rate = 0.001\n      self$model = self$build_model()\n  }\n)\n```", "```py\nbuild_model = function(...){\n        model = keras_model_sequential() %>% \n        layer_dense(units = 24, activation = \"relu\", input_shape = self$state_size) %>%\n        layer_dense(units = 24, activation = \"relu\") %>%\n        layer_dense(units = self$action_size, activation = \"linear\")\n\n        compile(model, loss = \"mse\", optimizer = optimizer_adam(lr = self$learning_rate), metrics = \"accuracy\")\n\n        return(model)\n    }\n```", "```py\nmemorize = function(state, action, reward, next_state, done){\n        pushback(self$memory,state)\n        pushback(self$memory,action)\n        pushback(self$memory,reward)\n        pushback(self$memory, next_state)\n        pushback(self$memory, done)\n    }\n```", "```py\nact = function(state){\n        if (runif(1) <= self$epsilon){\n            return(sample(self$action_size, 1))\n            } else {\n        act_values <- predict(self$model, state)\n        return(which(act_values==max(act_values)))\n            }\n    }\n```", "```py\nreplay = function(batch_size){\n        minibatch = sample(length(self$memory), batch_size) \n            state = minibatch[1]\n            action = minibatch[2]\n            target = minibatch[3]\n            next_state = minibatch[4]\n            done = minibatch[5]\n            if (done == FALSE){\n                target = (target + self$gamma *\n                          max(predict(self$model, next_state)))\n            target_f = predict(self$model, state)\n            target_f[0][action] = target\n            self$model(state, target_f, epochs=1, verbose=0)\n            }\n        if (self$epsilon > self$epsilon_min){\n            self$epsilon = self$epsilon * self$epsilon_decay\n            }\n    }\n```", "```py\nload = function(name) {\n    self$model %>% load_model_tf(name)\n},\nsave = function(name) {\n    self$model %>% save_model_tf(name)\n}\n```", "```py\nstate_size = 2\naction_size = 20\nagent = DQNAgent(state_size, action_size)\n\n```", "```py\nenv = makeEnvironment(step = step, reset = reset)\n```", "```py\ndone = FALSE\nbatch_size = 32\n```", "```py\nstate = reset(env)\nfor (j in 1:5000) {\n  action = agent$act(state)\n  nrd = step(env,action)\n  next_state = unlist(nrd[1])\n  reward = as.integer(nrd[2])\n  done = as.logical(nrd[3])\n  next_state = matrix(c(next_state[1],next_state[2]), ncol = 2)\n  reward = dplyr::if_else(done == TRUE, reward, as.integer(-10))\n  agent$memorize(state, action, reward, next_state, done)\n  state = next_state\n  env$state = next_state\n  if (done == TRUE) {\n    cat(sprintf(\"score: %d, e: %.2f\",j,agent$epsilon))\n    break\n  } \n  if (length(agent$memory) > batch_size) {\n    agent$replay(batch_size)\n  } \n  if (j %% 10 == 0) {\n    cat(sprintf(\"try: %d, state: %f,%f \",j,state[1],state[2])) \n  }\n\n}\n```"]