<html><head></head><body>
		<div>
			<div id="_idContainer307" class="Content">
			</div>
		</div>
		<div id="_idContainer308" class="Content">
			<h1 id="_idParaDest-171"><a id="_idTextAnchor180"/>7. Convolutional Neural Networks</h1>
		</div>
		<div id="_idContainer340" class="Content">
			<p>This chapter describes <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>). CNNs are used everywhere in AI, including image recognition and speech recognition. This chapter will detail the mechanisms of CNNs and how to implement them in Python.</p>
			<h2 id="_idParaDest-172"><a id="_idTextAnchor181"/>Overall Architecture</h2>
			<p>First, let's look at the network architecture of CNNs. You can create a CNN by combining layers, much in the same way as the neural networks that we have seen so far. However, CNNs have other layers as well: a convolution layer and a pooling layer. We will look at the details of the convolution and pooling layers in the following sections. This section describes how layers are combined to create a CNN.</p>
			<p>In the neural networks that we have seen so far, all the neurons in adjacent layers are connected. These layers are called <strong class="bold">fully connected</strong> layers, and we implemented them as Affine layers. You can use Affine layers to create a neural network consisting of five fully connected layers, for example, as shown in <em class="italics">Figure 7.1</em>.</p>
			<p>As <em class="italics">Figure 7.1</em> shows, the ReLU layer (or the Sigmoid layer) for the activation function follows the Affine layer in a fully connected neural network. Here, after four pairs of <strong class="bold">Affine – ReLU</strong> layers, comes the Affine layer, which is the fifth layer. And finally, the Softmax layer outputs the final result (probability):</p>
			<div>
				<div id="_idContainer309" class="IMG---Figure">
					<img src="image/fig07_1.jpg" alt="Figure 7.1: Sample network consisting of fully connected layers (Affine layers)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.1: Sample network consisting of fully connected layers (Affine layers)</h6>
			<p>So, what architecture does a CNN have? <em class="italics">Figure 7.2</em> shows a sample CNN:</p>
			<div>
				<div id="_idContainer310" class="IMG---Figure">
					<img src="image/fig07_2.jpg" alt="Figure 7.2: Sample CNN – convolution and pooling layers are added (they are shown as gray rectangles)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.2: Sample CNN – convolution and pooling layers are added (they are shown as gray rectangles)</h6>
			<p>As shown in <em class="italics">Figure 7.2</em>, CNN has additional convolution and pooling layers. In the CNN, layers are connected in the order of <strong class="bold">Convolution – ReLU – (Pooling) </strong>(a pooling layer is sometimes omitted). We can consider the previous <strong class="bold">Affine – ReLU</strong> connection as being replaced with "Convolution – ReLU – (Pooling)."</p>
			<p>In the CNN of <em class="italics">Figure 7.2</em>, note that the layers near the output are the previous "Affine – ReLU" pairs, while the last output layers are the previous "Affine – Softmax" pairs. This is the structure often seen in an ordinary CNN.</p>
			<h2 id="_idParaDest-173"><a id="_idTextAnchor182"/>The Convolution Layer</h2>
			<p>There are some CNN-specific terms, such as padding and stride. The data that flows through each layer in a CNN is data with shape (such as three-dimensional data), unlike in previous fully connected networks. Therefore, you may feel that CNNs are difficult when you learn about them for the first time. Here, we will look at the mechanism of the convolution layer used in CNNs.</p>
			<h3 id="_idParaDest-174"><a id="_idTextAnchor183"/>Issues with the Fully Connected Layer</h3>
			<p>The fully connected neural networks that we have seen so far used fully connected layers (Affine layers). In a fully connected layer, all the neurons in the adjacent layer are connected, and the number of outputs can be determined arbitrarily.</p>
			<p>The issue with a fully connected layer, though, is that the shape of the data is <em class="italics">ignored</em>. For example, when the input data is an image, it usually has a three-dimensional shape, determined by the height, the width, and the channel dimension. However, three-dimensional data must be converted into one-dimensional flat data when it is provided to a fully connected layer. In the previous examples that we used for the MNIST dataset, the input images had the shape of 1, 28, 28 (1 channel, 28 pixels in height, and 28 pixels in width), but the elements were arranged in a line, and the resulting 784 pieces of data were provided to the first Affine layer.</p>
			<p>Let's say an image has a three-dimensional shape and that the shape contains important spatial information. Essential patterns to recognize this information may hide in three-dimensional shapes. Spatially close pixels have similar values, the RBG channels are closely related to each other, and the distant pixels are not related. However, a fully connected layer ignores the shape and treats all the input data as equivalent neurons (neurons with the same number of dimensions), so it cannot use the information regarding the shape.</p>
			<p>On the other hand, a convolution layer maintains the shape. For images, it receives the input data as three-dimensional data and outputs three-dimensional data to the next layer. Therefore, CNNs can understand data with a shape, such as images, properly.</p>
			<p>In a CNN, the input/output data for a convolution layer is sometimes called a <strong class="bold">feature map</strong>. The input data for a convolution layer is called an <strong class="bold">input feature map</strong>, while the output data for a convolution layer is called an <strong class="bold">output feature map</strong>. In this book, <em class="italics">input/output data</em> and <em class="italics">feature map</em> will be used interchangeably.</p>
			<h3 id="_idParaDest-175"><a id="_idTextAnchor184"/>Convolution Operations</h3>
			<p>The processing performed in a convolution layer is called a "convolution operation" and is equivalent to the "filter operation" in image processing. Let's look at example (<em class="italics">Figure 7.3</em>) to understand a convolution operation:</p>
			<div>
				<div id="_idContainer311" class="IMG---Figure">
					<img src="image/fig07_3.jpg" alt="Figure 7.3: Convolution operation – the ⊛ symbol indicates a convolution operation&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.3: Convolution operation – the <span lang="en-US" xml:lang="en-US">⊛</span> symbol indicates a convolution operation</h6>
			<p>As shown in <em class="italics">Figure 7.3</em>, a convolution operation applies a filter to input data. In this example, the shape of the input data has a height and width, and so does the shape of the filter. When we indicate the shape of the data and filter as (height, width), the input size is (4, 4), the filter size is (3, 3), and the output size is (2, 2) in this example. Some literature uses the word "kernel" for the term "filter."</p>
			<p>Now, let's break down the calculation performed in the convolution operation shown in <em class="italics">Figure 7.3</em>. <em class="italics">Figure 7.4</em> shows the calculation procedure of the convolution operation.</p>
			<p>A convolution operation is applied to the input data while the filter window is shifted at a fixed interval. The window here indicates the gray 3x3 section shown in <em class="italics">Figure 7.4</em>. As shown in <em class="italics">Figure 7.4</em>, the element of the filter and the corresponding element of the input are multiplied and summed at each location (this calculation is sometimes called a <strong class="bold">multiply-accumulate operation</strong>). The result is stored in the corresponding position of the output. The output of the convolution operation can be obtained by performing this process at all locations.</p>
			<p>A fully connected neural network has biases as well as weight parameters. In a CNN, the filter parameters correspond to the previous "weights." It also has biases. The convolution operation of <em class="italics">Figure 7.3</em> shows the stage when a filter was applied. <em class="italics">Figure 7.5</em> shows the processing flow of a convolution operation, including biases:</p>
			<div>
				<div id="_idContainer312" class="IMG---Figure">
					<img src="image/fig07_4.jpg" alt="Figure 7.4: Calculation procedure of a convolution operation&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.4: Calculation procedure of a convolution operation</h6>
			<div>
				<div id="_idContainer313" class="IMG---Figure">
					<img src="image/fig07_5.jpg" alt="Figure 7.5: Bias in a convolution operation – a fixed value (bias) is added to the element &#13;&#10;after the filter is applied&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.5: Bias in a convolution operation – a fixed value (bias) is added to the element after the filter is applied</h6>
			<p>As shown in <em class="italics">Figure 7.5</em>, a bias term is added to the data after the filter is applied. Here, the bias is always only one (1x1) where one bias exists for the four pieces of data after the filter is applied. This one value is added to all the elements after the filter is applied.</p>
			<h3 id="_idParaDest-176"><a id="_idTextAnchor185"/>Padding</h3>
			<p>Before a convolution layer is processed, fixed data (such as 0) is sometimes filled around the input data. This is called <strong class="bold">padding</strong> and is often used in a convolution operation. For example, in <em class="italics">Figure 7.6</em>, padding of 1 is applied to the (4, 4) input data. The padding of 1 means filling the circumference with zeros with the width of one pixel:</p>
			<div>
				<div id="_idContainer314" class="IMG---Figure">
					<img src="image/fig07_6.jpg" alt="Figure 7.6: Padding in a convolution operation – add zeros around the input data (padding is shown by dashed lines here, and the zeros are omitted)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.6: Padding in a convolution operation – add zeros around the input data (padding is shown by dashed lines here, and the zeros are omitted)</h6>
			<p>As shown in <em class="italics">Figure 7.6</em>, padding converts the (4, 4) input data into (6, 6) data. After the (3, 3) filter is applied, (4, 4) output data is generated. In this example, the padding of 1 was used. You can set any integer, such as 2 or 3, as the padding value. If the padding value was 2, the size of the input data would be (8, 8). If the padding was 3, the size would be (10, 10).</p>
			<h4>Note</h4>
			<p class="callout">Padding is used mainly for adjusting the output size. For example, when a (3, 3) filter is applied to (4, 4) input data, the output size is (2, 2). The output size is smaller than the input size by two elements. This causes a problem in deep networks, where convolution operations are repeated many times. If each convolution operation spatially reduces the size, the output size will reach 1 at a certain time, and no more convolution operations will be available. To avoid such a situation, you can use padding. In the previous example, the output size (4, 4) remains the same as the input size (4, 4) when the width of padding is 1. Therefore, you can pass the data of the same spatial size to the next layer after performing a convolution operation.</p>
			<h3 id="_idParaDest-177"><a id="_idTextAnchor186"/>Stride</h3>
			<p>The interval of the positions for applying a filter is called a <strong class="bold">stride</strong>. In all previous examples, the stride was 1. When the stride is 2, for example, the interval of the window for applying a filter will be two elements, as shown in <em class="italics">Figure 7.7</em>.</p>
			<p>In <em class="italics">Figure 7.7</em>, a filter is applied to the (7, 7) input data with the stride of 2. When the stride is 2, the output size becomes (3, 3). Thus, the stride specifies the interval for applying a filter.</p>
			<div>
				<div id="_idContainer315" class="IMG---Figure">
					<img src="image/fig07_7.jpg" alt="Figure 7.7: Sample convolution operation where the stride is 2&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.7: Sample convolution operation where the stride is 2</h6>
			<p>As we have seen so far, the larger the stride, the smaller the output size, and the larger the padding, the larger the output size. How can we represent such relations in equations? Let's see how the output size is calculated based on padding and stride.</p>
			<p>Here, the input size is (<em class="italics">H</em>, <em class="italics">W</em>), the filter size is (<em class="italics">FH</em>, <em class="italics">FW</em>), the output size is (<em class="italics">OH</em>, <em class="italics">OW</em>), the padding is <em class="italics">P</em>, and the stride is <em class="italics">S</em>. In this case, you can calculate the output size with the following equation—that is, equation (7.1):</p>
			<table id="table001-5" class="No-Table-Style">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><img src="image/Figure_7.7a.png" alt="90"/></p>
						</td>
						<td class="No-Table-Style">
							<p>(7.1)</p>
						</td>
					</tr>
				</tbody>
			</table>
			<p>Now, let's use this equation to do some calculations:</p>
			<ol>
				<li><strong class="bold">Example 1: Example is shown in Figure 7.6</strong><p>Input size: (4, 4), padding: 1, stride: 1, filter size: (3, 3):</p><div id="_idContainer317" class="IMG---Figure"><img src="image/Figure_7.7c.jpg" alt="91"/></div></li>
				<li><strong class="bold">Example 2: Example is shown in Figure 7.7</strong><p>Input size: (7, 7), padding: 0, stride: 2, filter size: (3, 3):</p><div id="_idContainer318" class="IMG---Figure"><img src="image/Figure_7.7e.jpg" alt="92"/></div></li>
				<li><strong class="bold">Example 3</strong><p>Input size: (28, 31), padding: 2, stride: 3, filter size:(5, 5):</p><div id="_idContainer319" class="IMG---Figure"><img src="image/Figure_7.7g.jpg" alt="93"/></div></li>
			</ol>
			<p>As these examples show, you can calculate the output size by assigning values to equation (7.1). You can only obtain the output size by assignment, but note that you must assign values so that <img src="image/Figure_7.7i.png" alt="94"/> and <img src="image/Figure_7.7j.png" alt="95"/> in equation (7.1) are divisible. If the output size is not divisible (i.e., the result is a decimal), you must handle that by generating an error. Some deep learning frameworks advance this process without generating an error; for example, they round the value to the nearest integer when it is not divisible.</p>
			<h3 id="_idParaDest-178"><a id="_idTextAnchor187"/>Performing a Convolution Operation on Three-Dimensional Data</h3>
			<p>The examples we've looked at so far targeted two-dimensional shapes that have a height and a width. For images, we must handle three-dimensional data that has a channel dimension, as well as a height and a width. Here, we will look at an example of a convolution operation on three-dimensional data using the same technique we used in the previous examples.</p>
			<p><em class="italics">Figure 7.8</em> shows an example of convolution operation, while <em class="italics">Figure 7.9</em> shows the calculation procedure. Here, we can see the result of performing a convolution operation on three-dimensional data. You can see that the feature maps have increased in depth (the channel dimension) compared to the two-dimensional data (the example shown in <em class="italics">Figure 7.3</em>). If there are multiple feature maps in the channel dimension, a convolution operation using the input data and the filter is performed for each channel, and the results are added to obtain one output:</p>
			<div>
				<div id="_idContainer322" class="IMG---Figure">
					<img src="image/fig07_8.jpg" alt="Figure 7.8: Convolution operation for three-dimensional data&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.8: Convolution operation for three-dimensional data</h6>
			<div>
				<div id="_idContainer323" class="IMG---Figure">
					<img src="image/fig07_9.jpg" alt="Figure 7.9: Calculation procedure of the convolution operation for three-dimensional data&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.9: Calculation procedure of the convolution operation for three-dimensional data</h6>
			<h4>Note</h4>
			<p class="callout">In a three-dimensional convolution operation, as shown in this example, the input data and the filter must be the same in terms of the number of channels they have. In this example, the number of channels in the input data and the filter are the same; there are three. On the other hand, you can set the filter size to whatever you like. In this example, the filter size is (3, 3). You can set it to any size, such as (2, 2), (1, 1), or (5,5). However, as mentioned earlier, the number of channels must be the same as that of the input data. In this example, there must be three.</p>
			<h3 id="_idParaDest-179"><a id="_idTextAnchor188"/>Thinking in Blocks</h3>
			<p>In a three-dimensional convolution operation, you can consider the data and filter as rectangular blocks. A block here is a three-dimensional cuboid, as shown in <em class="italics">Figure 7.10</em>. We will represent three-dimensional data as a multidimensional array in the order channel, height, width. So, when the number of channels is C, the height is H, and the width is W for shape, it is represented as (C, H, W). We will represent a filter in the same order so that when the number of channels is C, the height is <strong class="bold">FH</strong> (<strong class="bold">Filter Height</strong>), and the width is <strong class="bold">FW</strong> (<strong class="bold">Filter Width</strong>) for a filter, it is represented as (C, FH, FW):</p>
			<div>
				<div id="_idContainer324" class="IMG---Figure">
					<img src="image/fig07_10.jpg" alt="Figure 7.10: Using blocks to consider a convolution operation&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.10: Using blocks to consider a convolution operation</h6>
			<p>In this example, the data's output is one feature map. One feature map means that the size of the output channel is one. So, how can we provide multiple outputs of convolution operations in the channel dimension? To do that, we use multiple filters (weights). <em class="italics">Figure 7.11</em> shows this graphically:</p>
			<div>
				<div id="_idContainer325" class="IMG---Figure">
					<img src="image/fig07_11.jpg" alt="Figure 7.11: Sample convolution operation with multiple filters&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.11: Sample convolution operation with multiple filters</h6>
			<p>As shown in <em class="italics">Figure 7.11</em>, when the number of filters applied is FN, the number of output maps generated is also FN. By combining FN maps, you can create a block of the shape (FN, OH, OW). Passing this completed block to the next layer is the process a CNN.</p>
			<p>You must also consider the number of filters in a convolution operation. To do that, we will write the filter weight data as four-dimensional data (output_channel, input_channel, height, width). For example, when there are 20 filters with three channels where the size is 5 x 5, it is represented as (20, 3, 5, 5).</p>
			<p>A convolution operation has biases (like a fully connected layer). <em class="italics">Figure 7.12</em> shows the example provided in <em class="italics">Figure 7.11</em> when you add biases.</p>
			<p>As we can see, each channel has only one bias data. Here, the shape of the bias is (FN, 1, 1), while the shape of the filter output is (FN, OH, OW). Adding these two blocks adds the same bias value to each channel in the filter output result, (FN, OH, OW). NumPy's broadcasting facilitates blocks of different shapes (please refer to <em class="italics">Broadcasting</em> section in <em class="italics">Chapter 1</em>, <em class="italics">Introduction to Python</em>):</p>
			<div>
				<div id="_idContainer326" class="IMG---Figure">
					<img src="image/fig07_12.jpg" alt="Figure 7.12: Process flow of a convolution operation (the bias term is also added)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.12: Process flow of a convolution operation (the bias term is also added)</h6>
			<h3 id="_idParaDest-180"><a id="_idTextAnchor189"/>Batch Processing</h3>
			<p>Input data is processed in batches in neural network processing. The implementations we've looked at so far for fully connected neural networks have supported batch processing, which enables more efficient processing and supports mini-batches in the training process.</p>
			<p>We can also support batch processing in a convolution operation by storing the data that flows through each layer as four-dimensional data. Specifically, the data is stored in the order (batch_num, channel, height, width). For example, when the processing shown in <em class="italics">Figure 7.12</em> is conducted for N data in batches, the shape of the data becomes as follows.</p>
			<p>In the data flow for batch processing shown here, the dimensions for the batches are added at the beginning of each piece of data. Thus, the data passes each layer as four-dimensional data. Please note that four-dimensional data that flows in the network indicates that a convolution operation is performed for N data; that is, N processes are conducted at one time:</p>
			<div>
				<div id="_idContainer327" class="IMG---Figure">
					<img src="image/Figure_7.13.jpg" alt="Figure 7.13: Process flow of a convolution operation (batch processing)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.13: Process flow of a convolution operation (batch processing)</h6>
			<h2 id="_idParaDest-181"><a id="_idTextAnchor190"/>The Pooling Layer</h2>
			<p>A pooling operation makes the space of the height and width smaller. As shown in <em class="italics">Figure 7.14,</em> it converts a 2 x 2 area into one element to reduce the space's size:</p>
			<div>
				<div id="_idContainer328" class="IMG---Figure">
					<img src="image/fig07_14.jpg" alt="Figure 7.14: Procedure of max pooling&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.14: Procedure of max pooling</h6>
			<p>This example shows this procedure when 2 x 2 max-pooling is conducted with a stride of 2. "Max pooling" takes the maximum value of a region, while "2 x 2" indicates the size of the target region. As we can see, it takes the maximum element in a 2 x 2 region. The stride is 2 in this example, so the 2 x 2 window moves by two elements at one time. Generally, the same value is used for the pooling window size and the stride. For example, the stride is 3 for a 3 x 3 window, and the stride is 4 for a 4 x 4 window.</p>
			<h4>Note</h4>
			<p class="callout">In addition to max pooling, average pooling can also be used. Max pooling takes the maximum value in the target region, while average pooling averages the values in the target region. In image recognition, max pooling is mainly used. Therefore, a "pooling layer" in this book indicates max pooling.</p>
			<h3 id="_idParaDest-182"><a id="_idTextAnchor191"/>Characteristics of a Pooling Layer</h3>
			<p>A pooling layer has various characteristics, described below.</p>
			<p><strong class="bold">There are no parameters to learn</strong></p>
			<p>Unlike a convolution layer, a pooling layer has no parameters to learn. Pooling has no parameters to learn because it only takes the maximum value (or averages the values) in the target region.</p>
			<p><strong class="bold">The number of channels does not change</strong></p>
			<p>In pooling, the number of channels in the output data is the same as that in the input data. As shown in <em class="italics">Figure 7.15</em>, this calculation is performed independently for each channel:</p>
			<div>
				<div id="_idContainer329" class="IMG---Figure">
					<img src="image/fig07_15.jpg" alt="Figure 7.15: Pooling does not change the number of channels&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.15: Pooling does not change the number of channels</h6>
			<p><strong class="bold">It is robust to a tiny position change</strong></p>
			<p>Pooling returns the same result, even when the input data is shifted slightly. Therefore, it is robust to a tiny shift of input data. For example, in 3 x 3 pooling, pooling absorbs the shift of input data, as shown in <em class="italics">Figure 7.16</em>:</p>
			<div>
				<div id="_idContainer330" class="IMG---Figure">
					<img src="image/fig07_16.jpg" alt="Figure 7.16: Even when the input data is shifted by one element in terms of width, the output is the same (it may not be the same, depending on the data)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.16: Even when the input data is shifted by one element in terms of width, the output is the same (it may not be the same, depending on the data)</h6>
			<h2 id="_idParaDest-183">Implement<a id="_idTextAnchor192"/>ing the Convolution and Pooling Layers</h2>
			<p>So far, we have seen convolution and pooling layers in detail. In this section, we will implement these two layers in Python. As described in <em class="italics">Chapter 5</em>, <em class="italics">Backpropagation</em>, the class that will be implemented here also provides forward and backward methods so that it can be used as a module.</p>
			<p>You may feel that implementing convolution and pooling layers is complicated, but you can implement them easily if you use a certain "trick." This section describes this trick and makes the task at hand easy. Then, we will implement a convolution layer.</p>
			<h3 id="_idParaDest-184"><a id="_idTextAnchor193"/>Four-Dimensional Arrays</h3>
			<p>As described earlier, four-dimensional data flows in each layer in a CNN. For example, when the shape of the data is (10, 1, 28, 28), it indicates that ten pieces of data with a height of 28, width of 28, and 1 channel exist. You can implement this in Python as follows:</p>
			<p class="source-code">&gt;&gt;&gt; x = np.random.rand(10, 1, 28, 28) # Generate data randomly</p>
			<p class="source-code">&gt;&gt;&gt; x.shape</p>
			<p class="source-code">(10, 1, 28, 28)</p>
			<p>To access the first piece of data, you can write <strong class="inline">x[0]</strong> ( the index begins at 0 in Python). Similarly, you can write <strong class="inline">x[1]</strong> to access the second piece of data:</p>
			<p class="source-code">&gt;&gt;&gt; x[0].shape # (1, 28, 28)</p>
			<p class="source-code">&gt;&gt;&gt; x[1].shape # (1, 28, 28)</p>
			<p>To access the spatial data in the first channel of the first piece of data, you can write the following:</p>
			<p class="source-code">&gt;&gt;&gt; x[0, 0] # or x[0][0]</p>
			<p>You can handle four-dimensional data in this way in a CNN. Therefore, implementing a convolution operation may be complicated. However, a "trick" called <strong class="inline">im2col</strong> makes this task easy.</p>
			<h3 id="_idParaDest-185"><a id="_idTextAnchor194"/>Expansion by im2col</h3>
			<p>To implement a convolution operation, you normally need to nest <strong class="inline">for</strong> statements several times. Such an implementation is slightly troublesome and <strong class="inline">for</strong> statements in NumPy slow down the processing speed (in NumPy, it is desirable that you do not use any <strong class="inline">for</strong> statements to access elements). Here, we will not use any <strong class="inline">for</strong> statements. Instead, we will use a simple function called <strong class="inline">im2col</strong> for a simple implementation.</p>
			<p>The <strong class="inline">im2col</strong> function expands input data conveniently for a filter (weight). As shown in <em class="italics">Figure 7.17</em>, <strong class="inline">im2col</strong> converts three-dimensional input data into a two-dimensional matrix (to be exact, it converts four-dimensional data, including the number of batches, into two-dimensional data).</p>
			<p><strong class="inline">im2col</strong> expands the input data conveniently for a filter (weight). Specifically, it expands the area that a filter will be applied to in the input data (a three-dimensional block) into a row, as shown in <em class="italics">Figure 7.18</em>. <strong class="inline">im2col</strong> expands all the locations to apply a filter to.</p>
			<p>In <em class="italics">Figure 7.18,</em> a large stride is used so that the filter areas do not overlap. This is done for visibility reasons. In actual convolution operations, the filter areas will overlap in most cases, in which case, the number of elements after expansion by <strong class="inline">im2col</strong> will be larger than that in the original block. Therefore, an implementation using <strong class="inline">im2col</strong> has the disadvantage of consuming more memory than usual. However, putting data into a large matrix is beneficial to perform calculations with a computer. For example, matrix calculation libraries (linear algebra libraries) highly optimize matrix calculations so that they can multiply large matrices quickly. Therefore, you can use a linear algebra library effectively by converting input data into a matrix:</p>
			<div>
				<div id="_idContainer331" class="IMG---Figure">
					<img src="image/fig07_17.jpg" alt="Figure 7.17: Overview of im2col&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.17: Overview of im2col</h6>
			<div>
				<div id="_idContainer332" class="IMG---Figure">
					<img src="image/fig07_18.jpg" alt="Figure 7.18: Expanding the filter target area from the beginning in a row&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.18: Expanding the filter target area from the beginning in a row</h6>
			<h4>Note</h4>
			<p class="callout">The name <strong class="inline">im2col</strong> is an abbreviation of "image to column," meaning the conversion of images into matrices. Deep learning frameworks such as Caffe and Chainer provide the <strong class="inline">im2col</strong> function, which is used to implement a convolution layer.</p>
			<p>After using <strong class="inline">im2col</strong> to expand input data, all you have to do is expand the filter (weight) for the convolution layer into a row and multiply the two matrices (see <em class="italics">Figure 7.19</em>). This process is almost the same as that of a fully connected Affine layer:</p>
			<div>
				<div id="_idContainer333" class="IMG---Figure">
					<img src="image/fig07_19.jpg" alt="Figure 7.19: Details of filtering in a convolution operation – expand the filter into a column and multiply the matrix by the data expanded by im2col. Lastly, reshape the result of the size of the output data.&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.19: Details of filtering in a convolution operation – expand the filter into a column and multiply the matrix by the data expanded by im2col. Lastly, reshape the result of the size of the output data.</h6>
			<p>As shown in <em class="italics">Figure 7.19</em>, the output of using the <strong class="inline">im2col</strong> function is a two-dimensional matrix. You must transform two-dimensional output data into an appropriate shape because a CNN stores data as four-dimensional arrays. The next section covers the flow of implementing a convolution layer.</p>
			<h3 id="_idParaDest-186"><a id="_idTextAnchor195"/>Implementing a Convolution Layer</h3>
			<p>This book uses the <strong class="inline">im2col</strong> function, and we will use it as a black box without considering its implementation. The <strong class="inline">im2col</strong> implementation is located at <strong class="inline">common/util.py</strong>. It is a simple function that is about 10 lines in length. Please refer to it if you are interested.</p>
			<p>This <strong class="inline">im2col</strong> function has the following interface:</p>
			<p class="source-code">im2col (input_data, filter_h, filter_w, stride=1, pad=0)</p>
			<ul>
				<li><strong class="inline">input_data</strong>: Input data that consists of arrays of four dimensions (amount of data, channel, height, breadth)</li>
				<li><strong class="inline">filter_h</strong>: Height of the filter</li>
				<li><strong class="inline">filter_w</strong>: Width of the filter</li>
				<li><strong class="inline">stride</strong>: Stride</li>
				<li><strong class="inline">pad</strong>: Padding</li>
			</ul>
			<p>The <strong class="inline">im2col</strong> function considers the "filter size," "stride," and "padding" to expand input data into a two-dimensional array, as follows:</p>
			<p class="source-code">import sys, os</p>
			<p class="source-code">sys.path.append(os.pardir)</p>
			<p class="source-code">from common.util import im2col</p>
			<p class="source-code">x1 = np.random.rand(1, 3, 7, 7)</p>
			<p class="source-code">col1 = im2col(x1, 5, 5, stride=1, pad=0)</p>
			<p class="source-code">print(col1.shape) # (9, 75)</p>
			<p class="source-code">x2 = np.random.rand(10, 3, 7, 7)</p>
			<p class="source-code">col2 = im2col(x2, 5, 5, stride=1, pad=0)</p>
			<p class="source-code">print(col2.shape) # (90, 75)</p>
			<p>The preceding code shows two examples. The first one uses 7x7 data with a batch size of 1, where the number of channels is 3. The second one uses data of the same shape with a batch size of 10. When we use the <strong class="inline">im2col</strong> function, the number of elements in the second dimension is 75 in both cases. This is the total number of elements in the filter (3 channels, size 5x5). When the batch size is 1, the result from <strong class="inline">im2col</strong> is (9, 75) in size. On the other hand, it is (90, 75) in the second example because the batch size is 10. It can store 10 times as much data.</p>
			<p>Now, we will use <strong class="inline">im2col</strong> to implement a convolution layer as a class called <strong class="inline">Convolution</strong>:</p>
			<p class="source-code">class Convolution:</p>
			<p class="source-code">    def __init__(self, W, b, stride=1, pad=0):</p>
			<p class="source-code">        self.W = W</p>
			<p class="source-code">        self.b = b</p>
			<p class="source-code">        self.stride = stride</p>
			<p class="source-code">        self.pad = pad</p>
			<p class="source-code">    def forward(self, x):</p>
			<p class="source-code">        FN, C, FH, FW = self.W.shape</p>
			<p class="source-code">        N, C, H, W = x.shape</p>
			<p class="source-code">        out_h = int(1 + (H + 2*self.pad - FH) / self.stride)</p>
			<p class="source-code">        out_w = int(1 + (W + 2*self.pad - FW) / self.stride)</p>
			<p class="source-code"><strong class="inline">        col = im2col(x, FH, FW, self.stride, self.pad)</strong></p>
			<p class="source-code"><strong class="inline">        col_W = self.W.reshape(FN, -1).T # Expand the filter</strong></p>
			<p class="source-code"><strong class="inline">        out = np.dot(col, col_W) + self.b</strong></p>
			<p class="source-code">        out = out.reshape(N, out_h, out_w, -1).transpose(0, 3, 1, 2)</p>
			<p class="source-code">        return out</p>
			<p>The initialization method of the convolution layer takes the filter (weight), bias, stride, and padding as arguments. The filter is four-dimensional, (<strong class="inline">FN</strong>, <strong class="inline">C</strong>, <strong class="inline">FH</strong>, and <strong class="inline">FW</strong>). <strong class="inline">FN</strong> stands for filter number (number of filters), <strong class="inline">C</strong> stands for a channel, <strong class="inline">FH</strong> stands for filter height, and <strong class="inline">FW</strong> stands for filter width.</p>
			<p>In the implementation of a convolution layer, an important section has been shown in bold. Here, <strong class="inline">im2col</strong> is used to expand the input data, while <strong class="inline">reshape</strong> is used to expand the filter into a two-dimensional array. The expanded matrices are multiplied.</p>
			<p>The section of code that expands the filter (the section in bold in the preceding code) expands the block of each filter into one line, as shown in <em class="italics">Figure 7.19</em>. Here, <strong class="inline">-1</strong> is specified as <strong class="inline">reshape (FN, -1)</strong>, which is one of the convenient features of <strong class="inline">reshape</strong>. When <strong class="inline">-1</strong> is specified for <strong class="inline">reshape</strong>, the number of elements is adjusted so that it matches the number of elements in a multidimensional array. For example, an array with the shape of (10, 3, 5, 5) has 750 elements in total. When <strong class="inline">reshape(10, -1)</strong> is specified here, it is reshaped into an array with the shape of (10, 75).</p>
			<p>The <strong class="inline">forward</strong> function adjusts the output size appropriately at the end. NumPy's <strong class="inline">transpose</strong> function is used there. The <strong class="inline">transpose</strong> function changes the order of axes in a multidimensional array. As shown in <em class="italics">Figure 7.20</em>, you can specify the order of indices (numbers) that starts at 0 to change the order of axes.</p>
			<p>Thus, you can implement the forward process of a convolution layer in almost the same way as a fully connected Affine layer by using <strong class="inline">im2col</strong> for expansion (see <em class="italics">Implementing the Affine and Softmax Layers</em> section in<em class="italics"> Chapter 5</em>, <em class="italics">Backpropagation </em>). Next, we will implement backward propagation in the convolution layer. Note that backward propagation in the convolution layer must do the reverse of <strong class="inline">im2col</strong>. This is handled by the col2im function, which is provided in this book (located at <strong class="inline">common/util.py</strong>). Except for when col2im is used, you can implement backward propagation in the convolution layer in the same way as the Affine layer. The implementation of backward propagation in the convolution layer is located at <strong class="inline">common/layer.py</strong>. </p>
			<div>
				<div id="_idContainer334" class="IMG---Figure">
					<img src="image/fig07_20.jpg" alt="Figure 7.20: Using NumPy's transpose to change the order of the axes – specifying the indices &#13;&#10;(numbers) to change the order of axes&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.20: Using NumPy's transpose to change the order of the axes – specifying the indices (numbers) to change the order of axes</h6>
			<h3 id="_idParaDest-187"><a id="_idTextAnchor196"/>Implementing a Pooling Layer</h3>
			<p>You can use <strong class="inline">im2col</strong> to expand the input data when implementing a pooling layer, as in the case of a convolution layer. What is different is that pooling is independent of the channel dimension, unlike a convolution layer. As shown in <em class="italics">Figure 7.21</em>, the target pooling area is expanded independently for each channel.</p>
			<p>After this expansion, you have only to take the maximum value in each row of the expanded matrix and transform the result into an appropriate shape (<em class="italics">Figure 7.22</em>).</p>
			<p>This is how the forward process in a pooling layer is implemented. The following shows a sample implementation in Python:</p>
			<p class="source-code">class Pooling:</p>
			<p class="source-code">    def __init__(self, pool_h, pool_w, stride=1, pad=0):</p>
			<p class="source-code">        self.pool_h = pool_h</p>
			<p class="source-code">        self.pool_w = pool_w</p>
			<p class="source-code">        self.stride = stride</p>
			<p class="source-code">        self.pad = pad</p>
			<p class="source-code">    def forward(self, x):</p>
			<p class="source-code">        N, C, H, W = x.shape</p>
			<p class="source-code">        out_h = int(1 + (H - self.pool_h) / self.stride)</p>
			<p class="source-code">        out_w = int(1 + (W - self.pool_w) / self.stride)</p>
			<p class="source-code">        # Expansion (1)</p>
			<p class="source-code">        col = im2col(x, self.pool_h, self.pool_w, self.stride, self.pad)</p>
			<p class="source-code">        col = col.reshape(-1, self.pool_h*self.pool_w)</p>
			<p class="source-code">        # Maximum value (2)</p>
			<p class="source-code"><strong class="inline">        out = np.max(col, axis=1)</strong></p>
			<p class="source-code">        # Reshape (3)</p>
			<p class="source-code">        out = out.reshape(N, out_h, out_w, C).transpose(0, 3, 1, 2)</p>
			<p class="source-code">            return out</p>
			<div>
				<div id="_idContainer335" class="IMG---Figure">
					<img src="image/fig07_21.jpg" alt="Figure 7.21: Expanding the target pooling area of the input data (pooling of 2x2)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.21: Expanding the target pooling area of the input data (pooling of 2x2)</h6>
			<p>As shown in <em class="italics">Figure 7.22</em>, there are three steps when it comes to implementing a pooling layer:</p>
			<ol>
				<li value="1">Expand the input data.</li>
				<li>Take the maximum value in each row.</li>
				<li>Reshape the output appropriately.</li>
			</ol>
			<p>The implementation of each step is simple and is only one or two lines in length:</p>
			<div>
				<div id="_idContainer336" class="IMG---Figure">
					<img src="image/fig07_22.jpg" alt="Figure 7.22: Flow of implementation of a pooling layer – the maximum elements &#13;&#10;in the pooling area are shown in gray&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.22: Flow of implementation of a pooling layer – the maximum elements in the pooling area are shown in gray</h6>
			<h4>Note</h4>
			<p class="callout">You can use NumPy's <strong class="inline">np.max</strong> method to take the maximum value. By specifying the axis argument in np.max, you can take the maximum value along the specified axis. For example, <strong class="inline">np.max(x, axis=1)</strong> returns the maximum value of <strong class="inline">x</strong> on each axis of the first dimension.</p>
			<p>That's all for the forward process in a pooling layer. As shown here, after expanding the input data into a shape that's suitable for pooling, subsequent implementations of it are very simple.</p>
			<p>For the backward process in a pooling layer, backward propagation of <strong class="inline">max</strong> (used in the implementation of the ReLU layer in the <em class="italics">ReLU Layer</em> sub-section in <em class="italics">Chapter 5</em>, <em class="italics">Backpropagation</em>), provides more information on this. The implementation of a pooling layer is located at <strong class="inline">common/layer.py</strong>. </p>
			<h2 id="_idParaDest-188"><a id="_idTextAnchor197"/>Implementing a CNN</h2>
			<p>So far, we have implemented convolution and pooling layers. Now, we will combine these layers to create a CNN that recognizes handwritten digits and implement it, as shown in <em class="italics">Figure 7.23</em>.</p>
			<p>As shown in <em class="italics">Figure 7.23</em>, the network consists of "Convolution – ReLU – Pooling – Affine – ReLU – Affine – Softmax" layers. We will implement this as a class named <strong class="inline">SimpleConvNet</strong>:</p>
			<div>
				<div id="_idContainer337" class="IMG---Figure">
					<img src="image/fig07_23.jpg" alt="Figure 7.23: Network configuration of a simple CNN&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.23: Network configuration of a simple CNN</h6>
			<p>Now, let's look at the initialization of <strong class="inline">SimpleConvNet (__init__)</strong>. It takes the following arguments:</p>
			<ul>
				<li><strong class="inline">input_dim</strong>: Dimensions of the input data (<strong class="bold">channel</strong>, <strong class="bold">height</strong>, <strong class="bold">width</strong>).</li>
				<li><strong class="inline">conv_param</strong>: Hyperparameters of the convolution layer (dictionary). The following are the dictionary keys:</li>
				<li><strong class="inline">filter_num</strong>: Number of filters</li>
				<li><strong class="inline">filter_size</strong>: Size of the filter</li>
				<li><strong class="inline">stride</strong>: Stride</li>
				<li><strong class="inline">pad</strong>: Padding</li>
				<li><strong class="inline">hidden_size</strong>: Number of neurons in the hidden layer (fully connected)</li>
				<li><strong class="inline">output_size</strong>: Number of neurons in the output layer (fully connected)</li>
				<li><strong class="inline">weight_init_std</strong>: Standard deviation of the weights at initialization</li>
			</ul>
			<p>Here, the hyperparameters of the convolution layer are provided as a dictionary called <strong class="inline">conv_param</strong>. We assume that the required hyperparameter values are stored using <strong class="inline">{'filter_num':30, 'filter_size':5, 'pad':0, 'stride':1}</strong>.</p>
			<p>The implementation of the initialization of <strong class="inline">SimpleConvNet</strong> is a little long, so here it's divided into three parts to make this easier to follow. The following code shows the first part of the initialization process:</p>
			<p class="source-code">class SimpleConvNet:</p>
			<p class="source-code">    def __init__(self, input_dim=(1, 28, 28),</p>
			<p class="source-code">                conv_param={'filter_num':30, 'filter_size':5,</p>
			<p class="source-code">                    'pad':0, 'stride':1},</p>
			<p class="source-code">                hidden_size=100, output_size=10, weight_init_std=0.01):</p>
			<p class="source-code">        filter_num = conv_param['filter_num']</p>
			<p class="source-code">        filter_size = conv_param['filter_size']</p>
			<p class="source-code">        filter_pad = conv_param['pad']</p>
			<p class="source-code">        filter_stride = conv_param['stride']</p>
			<p class="source-code">        input_size = input_dim[1]</p>
			<p class="source-code">        conv_output_size = (input_size - filter_size + 2*filter_pad) / \</p>
			<p class="source-code">                        filter_stride + 1</p>
			<p class="source-code">        pool_output_size = int(filter_num * (conv_output_size/2) *(conv_output_size/2))</p>
			<p>Here, the hyperparameters of the convolution layer that are provided by the initialization argument are taken out of the dictionary (so that we can use them later). Then, the output size of the convolution layer is calculated. The following code initializes the weight parameters:</p>
			<p class="source-code">    self.params = {}</p>
			<p class="source-code">    self.params['W1'] = weight_init_std * \</p>
			<p class="source-code">    np.random.randn(filter_num, input_dim[0],</p>
			<p class="source-code">    filter_size, filter_size)</p>
			<p class="source-code">    self.params['b1'] = np.zeros(filter_num)</p>
			<p class="source-code">    self.params['W2'] = weight_init_std * \</p>
			<p class="source-code">    np.random.randn(pool_output_size,hidden_size)</p>
			<p class="source-code">    self.params['b2'] = np.zeros(hidden_size)</p>
			<p class="source-code">    self.params['W3'] = weight_init_std * \</p>
			<p class="source-code">    np.random.randn(hidden_size, output_size)</p>
			<p class="source-code">    self.params['b3'] = np.zeros(output_size)</p>
			<p>The parameters required for training are the weights and biases of the first (convolution) layer and the remaining two fully connected layers. The parameters are stored in the instance dictionary variable, <strong class="inline">params</strong>. The <strong class="inline">W1</strong> key is used for the weight, while the <strong class="inline">b1</strong> key is used for the bias of the first (convolution) layer. In the same way, the <strong class="inline">W2</strong> and <strong class="inline">b2</strong> keys are used for the weight and bias of the second (fully connected) layer and the <strong class="inline">W3</strong> and <strong class="inline">b3</strong> keys are used for the weight and bias of the third (fully connected) layer, respectively. Lastly, the required layers are generated, as follows:</p>
			<p class="source-code">    self.layers = OrderedDict( )</p>
			<p class="source-code">    self.layers['Conv1'] = Convolution(self.params['W1'],</p>
			<p class="source-code">                                self.params['b1'],</p>
			<p class="source-code">                                conv_param['stride'],</p>
			<p class="source-code">                                conv_param['pad'])</p>
			<p class="source-code">    self.layers['Relu1'] = Relu( )</p>
			<p class="source-code">    self.layers['Pool1'] = Pooling(pool_h=2, pool_w=2, stride=2)</p>
			<p class="source-code">    self.layers['Affine1'] = Affine(self.params['W2'],</p>
			<p class="source-code">                                self.params['b2'])</p>
			<p class="source-code">    self.layers['Relu2'] = Relu( )</p>
			<p class="source-code">    self.layers['Affine2'] = Affine(self.params['W3'],</p>
			<p class="source-code">                                self.params['b3'])</p>
			<p class="source-code">    self.last_layer = SoftmaxWithLoss( )</p>
			<p>Layers are added to the ordered dictionary (<strong class="inline">OrderedDict</strong>) in an appropriate order. Only the last layer, <strong class="inline">SoftmaxWithLoss</strong>, is added to another variable, <strong class="inline">last-layer</strong>.</p>
			<p>This is the initialization of <strong class="inline">SimpleConvNet</strong>. After the initialization, you can implement the <strong class="inline">predict</strong> method for predicting and the <strong class="inline">loss</strong> method for calculating the value of the loss function, as follows:</p>
			<p class="source-code">def predict(self, x):</p>
			<p class="source-code">    for layer in self.layers.values( ):</p>
			<p class="source-code">        x = layer.forward(x)</p>
			<p class="source-code">    return x</p>
			<p class="source-code">def loss(self, x, t):</p>
			<p class="source-code">    y = self.predict(x)</p>
			<p class="source-code">return self.lastLayer.forward(y, t)</p>
			<p>Here, the <strong class="inline">x</strong> argument is the input data and the <strong class="inline">t</strong> argument is the label. The <strong class="inline">predict</strong> method only calls the added layers in order from the top, and passes the result to the next layer. In addition to forward processing in the <strong class="inline">predict</strong> method, the <strong class="inline">loss</strong> method performs forward processing until the last layer, <strong class="inline">SoftmaxWithLoss</strong>.</p>
			<p>The following implementation obtains the gradients via backpropagation, as follows:</p>
			<p class="source-code">def gradient(self, x, t):</p>
			<p class="source-code">    # forward</p>
			<p class="source-code">    self.loss(x, t)</p>
			<p class="source-code">    # backward</p>
			<p class="source-code">    dout = 1</p>
			<p class="source-code">    dout = self.lastLayer.backward(dout)</p>
			<p class="source-code">    layers = list(self.layers.values( ))</p>
			<p class="source-code">    layers.reverse( )</p>
			<p class="source-code">    for layer in layers:</p>
			<p class="source-code">        dout = layer.backward(dout)</p>
			<p class="source-code">    # Settings</p>
			<p class="source-code">    grads = {}</p>
			<p class="source-code">    grads['W1'] = self.layers['Conv1'].dW</p>
			<p class="source-code">    grads['b1'] = self.layers['Conv1'].db</p>
			<p class="source-code">    grads['W2'] = self.layers['Affine1'].dW</p>
			<p class="source-code">    grads['b2'] = self.layers['Affine1'].db</p>
			<p class="source-code">    grads['W3'] = self.layers['Affine2'].dW</p>
			<p class="source-code">    grads['b3'] = self.layers['Affine2'].db</p>
			<p class="source-code">    return grads</p>
			<p>Backpropagation is used to obtain the gradients of the parameters. To do that, forward propagation and backward propagation are conducted one after the other. Because the forward and backward propagation are implemented properly in each layer, we have only to call them in an appropriate order here. Lastly, the gradient of each weight parameter is stored in the <strong class="inline">grads</strong> dictionary. Thus, you can implement <strong class="inline">SimpleConvNet</strong>.</p>
			<p>Now, let's train the <strong class="inline">SimpleConvNet</strong> class using the MNIST dataset. The code for training is almost the same as that described in the <em class="italics">Implementing a Training Algorithm</em> section in <em class="italics">Chapter 4</em>, <em class="italics">Neural Network Training</em>. Therefore, the code won't be shown here (the source code is located at <strong class="inline">ch07/train_convnet.py</strong>).</p>
			<p>When <strong class="inline">SimpleConvNet</strong> is used to train the MNIST dataset, the recognition accuracy of the training data is 99.82%, while the recognition accuracy of the test data is 98.96% (the recognition accuracies are slightly different from training to training). 99% is a very high recognition accuracy for the test data for a relatively small network. In the next chapter, we will add layers to create a network where the recognition accuracy of the test data exceeds 99%.</p>
			<p>As we have seen here, convolution and pooling layers are indispensable modules in image recognition. A CNN can read the spatial characteristics of images and achieve high accuracy in handwritten digit recognition.</p>
			<h2 id="_idParaDest-189"><a id="_idTextAnchor198"/>Visualizing a CNN</h2>
			<p>What does the convolution layer used in a CNN "see"? Here, we will visualize a convolution layer to explore what happens in a CNN.</p>
			<h3 id="_idParaDest-190"><a id="_idTextAnchor199"/>Visualizing the Weight of the First Layer</h3>
			<p>Earlier, we conducted simple CNN training for the MNIST dataset. The shape of the weight of the first (convolution) layer was (30, 1, 5, 5). It was 5x5 in size, had 1 channel, and 30 filters. When the filter is 5x5 in size and has 1 channel, it can be visualized as a one-channel gray image. Now, let's show the filters of the convolution layer (the first layer) as images. Here, we will compare the weights before and after training. <em class="italics">Figure 7.24</em> shows the results (the source code is located at <strong class="inline">ch07/visualize_filter.py</strong>):</p>
			<div>
				<div id="_idContainer338" class="IMG---Figure">
					<img src="image/fig07_24.jpg" alt="Figure 7.24: Weight of the first (convolution) layer before and after training. The elements of the weight are real numbers, but they are normalized between 0 and 255 to show the images so that the smallest value is black (0) and the largest value is white (255)&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.24: Weight of the first (convolution) layer before and after training. The elements of the weight are real numbers, but they are normalized between 0 and 255 to show the images so that the smallest value is black (0) and the largest value is white (255)</h6>
			<p>As shown in <em class="italics">Figure 7.24</em>, the filters before training are initialized randomly. Black-and-white shades have no pattern. On the other hand, the filters after training are images with a pattern. Some filters have gradations from white to black, while some filters have small areas of color (called "blobs"), which indicates that training provided a pattern to the filters.</p>
			<p>The filters with a pattern on the right-hand side of <em class="italics">Figure 7.24</em> "see" edges (boundaries of colors) and blobs. For example, when a filter is white in the left half and black in the right half, it reacts to a vertical edge, as shown in <em class="italics">Figure 7.25</em>.</p>
			<p><em class="italics">Figure 7.25</em> shows the results when two learned filters are selected, and convolution processing is performed on the input image. You can see that "filter 1" reacted to a vertical edge and that "filter 2" reacted to a horizontal edge:</p>
			<div>
				<div id="_idContainer339" class="IMG---Figure">
					<img src="image/fig07_25.jpg" alt="Figure 7.25: Filters reacting to horizontal and vertical edges. White pixels appear at a vertical edge in output image 1. Meanwhile, many white pixels appear at a horizontal edge in output image 2.&#13;&#10;"/>
				</div>
			</div>
			<h6>Figure 7.25: Filters reacting to horizontal and vertical edges. White pixels appear at a vertical edge in output image 1. Meanwhile, many white pixels appear at a horizontal edge in output image 2.</h6>
			<p>Thus, you can see that the filters in a convolution layer extract primitive information such as edges and blobs. The CNN that was implemented earlier passes such primitive information to subsequent layers.</p>
			<h3 id="_idParaDest-191"><a id="_idTextAnchor200"/>Using a Hierarchical Structure to Extract Information</h3>
			<p>The preceding result comes from the first (convolution) layer. It extracts low-level information such as edges and blobs. So, what type of information does each layer in a CNN with multiple layers extract? Research on visualization in deep learning [(<em class="italics">Matthew D. Zeiler and Rob Fergus (2014): Visualizing and Understanding Convolutional Networks. In David Fleet, Tomas Pajdla, Bernt Schiele, &amp; Tinne Tuytelaars, eds. Computer Vision – ECCV 2014. Lecture Notes in Computer Science. Springer International Publishing, 818 – 833</em>) and (<em class="italics">A. Mahendran and A. Vedaldi (2015): Understanding deep image representations by inverting them. In the 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 5188 – 5196. DOI:</em> (<a href="http://dx.doi.org/10.1109/CVPR.2015.7299155">http://dx.doi.org/10.1109/CVPR.2015.7299155</a>)] has stated that the deeper a layer, the more abstract the extracted information (to be precise, neurons that react strongly).</p>
			<h2 id="_idParaDest-192"><a id="_idTextAnchor201"/>Typical CNNs</h2>
			<p>CNNs of various architectures have been proposed so far. In this section, we will look at two important networks. One is LeNet (<em class="italics">Y. Lecun, L. Bottou, Y. Bengio, and P. Haffner (1998): Gradient-based learning applied to document recognition. Proceedings of the IEEE 86, 11 (November 1998), 2278 – 2324. DOI</em>: (<a href="http://dx.doi.org/10.1109/5.726791">http://dx.doi.org/10.1109/5.726791</a>)). It was one of the first CNNs and was first proposed in 1998. The other is AlexNet (<em class="italics">Alex Krizhevsky, Ilya Sutskever, and Geoffrey E. Hinton (2012): ImageNet Classification with Deep Convolutional Neural Networks. In F. Pereira, C. J. C. Burges, L. Bottou, &amp; K. Q. Weinberger, eds. Advances in Neural Information Processing Systems 25. Curran Associates, Inc., 1097 – 1105</em>). It was proposed in 2012 and drew attention to deep learning.</p>
			<h3 id="_idParaDest-193"><a id="_idTextAnchor202"/>LeNet</h3>
			<p>LeNet is a network for handwritten digit recognition that was proposed in 1998. In the network, a convolution layer and a pooling layer (i.e., a subsampling layer that only "thins out elements") are repeated, and finally, a fully connected layer outputs the result.</p>
			<p>There are some differences between LeNet and the "current CNN." One is that there's an activation function. A sigmoid function is used in LeNet, while ReLU is mainly used now. Subsampling is used in the original LeNet to reduce the size of intermediate data, while max pooling is mainly used now:</p>
			<p>In this way, there are some differences between LeNet and the "current CNN," but they are not significant. This is surprising when we consider that LeNet was the "first CNN" to be proposed almost 20 years ago.</p>
			<h3 id="_idParaDest-194"><a id="_idTextAnchor203"/>AlexNet</h3>
			<p>AlexNet was published nearly 20 years after LeNet was proposed. Although AlexNet created a boom in deep learning, its network architecture hasn't changed much from LeNet:</p>
			<p>AlexNet stacks a convolution layer and a pooling layer and outputs the result through a fully connected layer. Its architecture is not much different from LeNet, but there are some differences, as follows:</p>
			<ul>
				<li>ReLU is used as the activation function</li>
				<li>A layer for local normalization called <strong class="bold">Local Response Normalization</strong> (<strong class="bold">LRN</strong>) is used</li>
				<li>Dropout is used (see <em class="italics">Dropout</em> sub-section in <em class="italics">Chapter 6</em>, <em class="italics">Training Techniques</em>)</li>
			</ul>
			<p>LeNet and AlexNet are not very different in terms of their network architectures. However, the surrounding environment and computer technologies have advanced greatly. Now, everyone can obtain a large quantity of data, and widespread GPUs that are good at large parallel computing enable massive operations at high speed. Big data and GPUs greatly motivated the development of deep learning.</p>
			<h4>Note</h4>
			<p class="callout">Many parameters often exist in deep learning (a network with many layers). Many calculations are required for training, and a large quantity of data is required to "satisfy" these parameters. We can say that GPUs and big data cast light on these challenges.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor204"/>Summary</h2>
			<p>In this chapter, we learned about CNNs. Specifically, we covered convolution layers and pooling layers (the basic modules that constitute CNNs) in great detail in order to understand them at the implementation level. CNNs are mostly used when looking at data regarding images. Please ensure that you understand the content of this chapter before moving on.</p>
			<p>In this chapter, we learned about the following:</p>
			<ul>
				<li>In a CNN, convolution, and pooling layers are added to the previous network, which consists of fully connected layers.</li>
				<li>You can use <strong class="inline">im2col</strong> (a function for expanding images into arrays) to implement convolution and pooling layers simply and efficiently.</li>
				<li>Visualizing a CNN enables you to see how advanced information is extracted as the layer becomes deeper.</li>
				<li>Typical CNNs include LeNet and AlexNet.</li>
				<li>Big data and GPUs contribute significantly to the development of deep learning.</li>
			</ul>
		</div>
		<div>
			<div id="_idContainer341" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer342" class="Content">
			</div>
		</div>
	</body></html>