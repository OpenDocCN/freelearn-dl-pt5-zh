<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer189">
<h1 class="chapter-number" id="_idParaDest-162"><a id="_idTextAnchor162"/>6</h1>
<h1 id="_idParaDest-163"><a id="_idTextAnchor163"/>Deep Learning Reminders</h1>
<p><strong class="bold">Deep learning</strong> is the <a id="_idIndexMarker297"/>specific domain of machine learning based on neural networks. Deep learning is known to be particularly powerful with unstructured data, such as text, audio, and image, but can be useful for time series and structured data too. In this chapter, we will review the basics of deep learning, from a perceptron to training a neural network. We will provide recipes for training neural networks for three main use cases: regression, binary classification, and <span class="No-Break">multiclass classification.</span></p>
<p>In this chapter, we’ll cover the <span class="No-Break">following recipes:</span></p>
<ul>
<li>Training <span class="No-Break">a perceptron</span></li>
<li>Training a neural network <span class="No-Break">for regression</span></li>
<li>Training a neural network for <span class="No-Break">binary classification</span></li>
<li>Training a multiclass classification <span class="No-Break">neural network</span></li>
</ul>
<h1 id="_idParaDest-164"><a id="_idTextAnchor164"/>Technical requirements</h1>
<p>In this chapter, you will train a perceptron, as well as several neural networks. To do so, the following libraries <span class="No-Break">are required:</span></p>
<ul>
<li><span class="No-Break">NumPy</span></li>
<li><span class="No-Break">pandas</span></li>
<li><span class="No-Break">scikit-learn</span></li>
<li><span class="No-Break">PyTorch</span></li>
<li><span class="No-Break">torchvision</span></li>
</ul>
<h1 id="_idParaDest-165"><a id="_idTextAnchor165"/>Training a perceptron</h1>
<p>The perceptron<a id="_idIndexMarker298"/> is arguably the building block of deep learning. Even if the perceptron is not directly used in production systems, understanding what it is can be an asset for building a strong foundation in <span class="No-Break">deep learning.</span></p>
<p>In this recipe, we will review what a perceptron is and then train one using scikit-learn on the <span class="No-Break">Iris dataset.</span></p>
<h2 id="_idParaDest-166"><a id="_idTextAnchor166"/>Getting started</h2>
<p>The perceptron<a id="_idIndexMarker299"/> is a machine learning method first proposed to mimic a biological neuron. It was first proposed in the 1940s and then implemented in <span class="No-Break">the 1950s.</span></p>
<p>From a high-level point of view, a neuron can be described as a cell that receives input signals and fires a signal itself when the sum of the input signals is above a given threshold. This is exactly what a perceptron does; all you have to do is <span class="No-Break">the following:</span></p>
<ul>
<li>Replace the input signals <span class="No-Break">with features</span></li>
<li>Apply a weighted sum to those features and apply an activation function <span class="No-Break">to it</span></li>
<li>Replace the output signal with <span class="No-Break">a prediction</span></li>
</ul>
<p>More formally, assuming <em class="italic">n</em> input features <img alt="" height="18" src="image/B19629_06_01.png" width="19"/> and n weights <img alt="" height="22" src="image/formula_06_002.png" width="24"/>, the output ŷ of a perceptron is <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer152">
<img alt="" height="111" src="image/formula_06_003.jpg" width="401"/>
</div>
</div>
<p>Where <img alt="" height="18" src="image/formula_06_004.png" width="26"/> is the bias and <em class="italic">g</em> is the activation function; historically, this is the step function, which returns 1 for a positive input value, 0 otherwise. So, at the end, for <em class="italic">n</em> input features, a perceptron is made of <em class="italic">n+1</em> parameters: one parameter per feature plus <span class="No-Break">the bias.</span></p>
<p class="callout-heading">Tip</p>
<p class="callout">The step function is also called the <strong class="bold">Heaviside function</strong> and is widely used in other fields, such <span class="No-Break">as physics.</span></p>
<p>The perceptron forward computation is summarized in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.1</em>. As you can see, given a list of features <img alt="" height="18" src="image/formula_06_005.png" width="19"/> and weights <img alt="" height="18" src="image/formula_06_006.png" width="24"/>, the forward computation is just the weighted sum, to which an activation function <span class="No-Break">is applied.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer156">
<img alt="Figure 6.1 – A mathematical representation of a perceptron: from input features to output, through weights and the activation function" height="679" src="image/B19629_06_01.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.1 – A mathematical representation of a perceptron: from input features to output, through weights and the activation function</p>
<p>On the <a id="_idIndexMarker300"/>practical side, scikit-learn is the only thing required for installation for this recipe. It can be installed with the <strong class="source-inline">pip install </strong><span class="No-Break"><strong class="source-inline">scikit-learn</strong></span><span class="No-Break"> command.</span></p>
<h2 id="_idParaDest-167"><a id="_idTextAnchor167"/>How to do it…</h2>
<p>We will use the Iris dataset again since the perceptron does not really perform well on complex <span class="No-Break">classification tasks:</span></p>
<ol>
<li>Make the required imports <span class="No-Break">from scikit-learn:</span><ul><li><strong class="source-inline">load_iris</strong>: A function to load <span class="No-Break">the dataset</span></li><li><strong class="source-inline">train_test_split</strong>: A function to split <span class="No-Break">the data</span></li><li><strong class="source-inline">StandardScaler</strong>: A class allowing us to rescale <span class="No-Break">the data</span></li><li><strong class="source-inline">Perceptron</strong>: The class containing the implementation of <span class="No-Break">the perceptron:</span><pre class="source-code">
from sklearn.datasets import load_iris</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.preprocessing import StandardScaler</pre><pre class="source-code">
from sklearn.linear_model import Perceptron</pre></li></ul></li>
<li>Load the <span class="No-Break">Iris dataset:</span><pre class="source-code">
# Load the Iris dataset</pre><pre class="source-code">
X, y = load_iris(return_X_y=True)</pre></li>
<li>Split the data into training and test sets using the <strong class="source-inline">train_test_split</strong> function, with <strong class="source-inline">random state</strong> set to <strong class="source-inline">0</strong> <span class="No-Break">for reproducibility:</span><pre class="source-code">
# Split the data</pre><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    X, y, random_state=0)</pre></li>
<li>Since <a id="_idIndexMarker301"/>all the features are quantitative here, we simply rescale all the features with a <span class="No-Break">standard scaler:</span><pre class="source-code">
# Rescale the data</pre><pre class="source-code">
scaler = StandardScaler()</pre><pre class="source-code">
X_train = scaler.fit_transform(X_train)</pre><pre class="source-code">
X_test = scaler.transform(X_test)</pre></li>
<li>Instantiate the model with the default parameters and fit it on the training set with the <strong class="source-inline">.</strong><span class="No-Break"><strong class="source-inline">fit()</strong></span><span class="No-Break"> method:</span><pre class="source-code">
perc = Perceptron()perc.fit(X_train, y_train)</pre></li>
<li>Evaluate the model on both the training and test sets with the <strong class="source-inline">.score()</strong> method of the <strong class="source-inline">LinearRegression</strong> class, providing the <span class="No-Break">accuracy score:</span><pre class="source-code">
# Print the R2-score on train and test</pre><pre class="source-code">
print('R2-score on train set:',</pre><pre class="source-code">
    perc.score(X_train, y_train))</pre><pre class="source-code">
print('R2-score on test set:',</pre><pre class="source-code">
    perc.score(X_test, y_test))</pre></li>
</ol>
<p>Here is <span class="No-Break">the output:</span></p>
<pre class="source-code">
<strong class="bold">R2-score on train set: 0.9285714285714286</strong>
<strong class="bold">R2-score on test set: 0.8421052631578947</strong></pre>
<ol>
<li value="7">Out of curiosity, we can have a look at the weights in <strong class="source-inline">.coef_</strong> and the bias <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">.intercept_</strong></span><span class="No-Break">.</span><pre class="source-code">
print('weights:', perc.coef_)</pre><pre class="source-code">
print('bias:', perc.intercept_)</pre></li>
</ol>
<p>Here is <span class="No-Break">the output:</span></p>
<pre class="source-code">
<strong class="bold">weights: [[-0.49201984  2.77164495 -3.07208498 -2.51124259]</strong>
<strong class="bold">  [ 0.41482008 -1.94508614  3.2852582  -2.60994774]</strong>
<strong class="bold">  [-0.32320969  0.48524348  5.73376173  4.93525738]] bias: [-2. -3. -6.]</strong></pre>
<p class="callout-heading">Important note</p>
<p class="callout">There are three sets of four weights and one bias, since scikit-learn handles on its own the One-vs-Rest multiclass classification, so we have one perceptron <span class="No-Break">per class.</span></p>
<h2 id="_idParaDest-168"><a id="_idTextAnchor168"/>There’s more…</h2>
<p>The perceptron<a id="_idIndexMarker302"/> is not only a machine learning model. It can be used to simulate logical gates: OR, AND, NOR, NAND, and XOR. Let’s have <span class="No-Break">a look.</span></p>
<p>We can easily implement a forward propagation for the perceptron with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
import numpy as np
class LogicalGatePerceptron:
    def __init__(self, weights: np.array, bias: float):
        self.weights = weights
        self.bias = bias
    def forward(self, X: np.array) -&gt; int:
        return (np.dot(
            X, self.weights) + self.bias &gt; 0).astype(int)</pre>
<p>This code does not consider many edge cases, but is used here simply to explain and demonstrate <span class="No-Break">simple concepts.</span></p>
<p>The AND gate has the inputs and expected outputs defined in the following <span class="No-Break">truth table:</span></p>
<table class="T---Table" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">Input 1</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">Input 2</strong></span></p>
</td>
<td class="T---Table T---Body T---Body">
<p><span class="No-Break"><strong class="bold">Output</strong></span></p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
<td class="T---Table T---Body T---Body">
<p>0</p>
</td>
</tr>
<tr class="T---Table">
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
<td class="T---Table T---Body T---Body">
<p>1</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Figure">Table 6.1 – AND gate truth table</p>
<p>Let’s reproduce this data with an array <strong class="source-inline">X</strong> that has two features (input 1 and input 2) and four samples, and an array <strong class="source-inline">y</strong> with the <span class="No-Break">expected outputs:</span></p>
<pre class="source-code">
# Define X and y
X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])
y = [0, 0, 0, 1]</pre>
<p>We can now<a id="_idIndexMarker303"/> find a set of weights and bias that will allow the perceptron to act as an AND gate, and check the results to see whether it’s <span class="No-Break">actually working:</span></p>
<pre class="source-code">
gate = LogicalGatePerceptron(np.array([1, 1]), -1)
y_pred = gate.forward(X)
print('Error:', (y - y_pred).sum())</pre>
<p>Here is <span class="No-Break">the output:</span></p>
<pre class="source-code">
Error: 0</pre>
<p>With the same logic, most basic logic gates can be created out of <span class="No-Break">a perceptron:</span></p>
<ul>
<li>AND gate: weights [1, 1] and <span class="No-Break">bias -1</span></li>
<li>OR gate: weights [1, 1] and <span class="No-Break">bias 0</span></li>
<li>NOR gate: weights [-1, -1] and <span class="No-Break">bias 1</span></li>
<li>NAND gate: weights [-1, -1] and <span class="No-Break">bias 2</span></li>
<li>XOR gate: this requires <span class="No-Break">two perceptrons</span></li>
</ul>
<p class="callout-heading">Tip</p>
<p class="callout">You can guess the weights and bias with a trial-and-error approach. But you can also use the truth table of a logic gate to make an educated guess or even to solve a set <span class="No-Break">of equations.</span></p>
<p>This means that using perceptrons, any logic function can <span class="No-Break">be computed.</span></p>
<h2 id="_idParaDest-169"><a id="_idTextAnchor169"/>See also</h2>
<p>The official documentation of the scikit-learn <span class="No-Break">implementation: </span><a href="https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.xhtml"><span class="No-Break">https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Perceptron.xhtml</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-170"><a id="_idTextAnchor170"/>Training a neural network for regression</h1>
<p>A<a id="_idIndexMarker304"/> perceptron is not a powerful and commonly used machine learning model. But having many perceptrons employed together in a neural network can become a powerful machine learning model. In this recipe, we will review a simple neural network, sometimes <a id="_idIndexMarker305"/>called a <strong class="bold">multi-layer perceptron</strong> or <strong class="bold">vanilla neural network</strong>. And <a id="_idIndexMarker306"/>we will then train such a neural network on a regression task on the California housing dataset with PyTorch, a widely used framework in <span class="No-Break">deep learning.</span></p>
<h2 id="_idParaDest-171"><a id="_idTextAnchor171"/>Getting started</h2>
<p>Let’s start by reviewing what a neural network is, and how to feed forward a neural network from <span class="No-Break">input features.</span></p>
<p>A neural network can be divided into <span class="No-Break">three parts:</span></p>
<ul>
<li><strong class="bold">The input layer</strong>, containing<a id="_idIndexMarker307"/> the <span class="No-Break">input features</span></li>
<li><strong class="bold">The hidden layers</strong>, which<a id="_idIndexMarker308"/> can be any number of layers <span class="No-Break">and units</span></li>
<li><strong class="bold">The output layer</strong>, which <a id="_idIndexMarker309"/>is defined by the expected output of the <span class="No-Break">neural network</span></li>
</ul>
<p>In both the hidden and output layers, we consider each unit (or neuron) to be a perceptron, with its own weights <span class="No-Break">and bias.</span></p>
<p>These three parts are well represented in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer157">
<img alt="Figure 6.2 – A typical representation of a neural network: on the left the input layer, in the middle the hidden layers, on the right the output layer" height="715" src="image/B19629_06_02.jpg" width="1428"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.2 – A typical representation of a neural network: on the left the input layer, in the middle the hidden layers, on the right the output layer</p>
<p>We will <a id="_idIndexMarker310"/>note the input features <img alt="" height="24" src="image/formula_06_007.png" width="162"/>, the activation of the unit <em class="italic">i</em> of the layer <em class="italic">l</em>, and <img alt="" height="26" src="image/formula_06_008.png" width="37"/>, the we­­­ights of the unit <em class="italic">i</em> of the <span class="No-Break">layer </span><span class="No-Break"><em class="italic">l</em></span><span class="No-Break">.</span></p>
<p class="callout-heading">Tip</p>
<p class="callout">We consider a neural network to involve deep learning if there is at least one <span class="No-Break">hidden layer.</span></p>
<p>Training a neural network in regression is not so different from training a linear regression. It is made of the <span class="No-Break">same ingredients:</span></p>
<ul>
<li>Forward propagation, from input features and weights to <span class="No-Break">a prediction</span></li>
<li>A loss function <span class="No-Break">to minimize</span></li>
<li>An algorithm to update <span class="No-Break">the weights</span></li>
</ul>
<p>Let’s have a look at <span class="No-Break">those ingredients.</span></p>
<h3>Forward propagation</h3>
<p>The forward propagation<a id="_idIndexMarker311"/> is what allows to compute and output from the input features. It must be computed from left to right, from the input layer (the input features) to the output layer (the output prediction). Each unit being a perceptron, the first hidden layer is fairly easy <span class="No-Break">to compute:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer160">
<img alt="" height="60" src="image/formula_06_009.jpg" width="530"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer161">
<img alt="" height="57" src="image/formula_06_010.jpg" width="529"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer162">
<img alt="" height="60" src="image/formula_06_011.jpg" width="532"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer163">
<img alt="" height="61" src="image/formula_06_012.jpg" width="536"/>
</div>
</div>
<p>Where <img alt="" height="22" src="image/formula_06_013.png" width="38"/> is the bias, and <img alt="" height="25" src="image/formula_06_014.png" width="35"/> is the activation function of <span class="No-Break">layer 1.</span></p>
<p>Now, if we <a id="_idIndexMarker312"/>want to compute the activations of the second hidden layer <img alt="" height="27" src="image/formula_06_015.png" width="39"/>, we would use the exact same formulas, but with the activations of the first hidden layer as input (<img alt="" height="23" src="image/formula_06_016.png" width="32"/>), instead of the <span class="No-Break">input features:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer168">
<img alt="" height="58" src="image/formula_06_017.jpg" width="597"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer169">
<img alt="" height="59" src="image/formula_06_018.jpg" width="598"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer170">
<img alt="" height="58" src="image/formula_06_019.jpg" width="597"/>
</div>
</div>
<div>
<div class="IMG---Figure" id="_idContainer171">
<img alt="" height="58" src="image/formula_06_020.jpg" width="596"/>
</div>
</div>
<p class="callout-heading">Tip</p>
<p class="callout">You can easily generalize to any number of hidden layers and any number of units per layer – the principle remains <span class="No-Break">the same.</span></p>
<p>Finally, the output layer would be computed in exactly the same way, except in this case we have only one <span class="No-Break">output neuron:</span></p>
<p class="IMG---Figure"><img alt="" height="31" src="image/formula_06_021.png" width="299"/></p>
<p>One interesting thing to underline: the activation function is also layer dependent, meaning that each layer can have a different activation function. This is particularly critical for the output layer, which needs a specific output function depending on the task and <span class="No-Break">expected output.</span></p>
<p>For a<a id="_idIndexMarker313"/> regression task, it is common to have a linear activation function, allowing the output values of the neural network to be <span class="No-Break">any number.</span></p>
<p class="callout-heading">Tip</p>
<p class="callout">The activation function plays a decisive role in neural networks: it adds non-linearity. If we have only linear activation functions for hidden layers, no matter the number of layers, it is equivalent to having no <span class="No-Break">hidden layer.</span></p>
<h3>Loss function</h3>
<p>The loss function in <a id="_idIndexMarker314"/>a regression task can be the same as in a linear regression: the mean squared error. In our example, if we consider the prediction <em class="italic">ŷ</em> to be our output value <img alt="" height="27" src="image/formula_06_022.png" width="84"/>, the loss <em class="italic">L</em> is simply <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer174">
<img alt="" height="124" src="image/formula_06_023.jpg" width="458"/>
</div>
</div>
<p>Assuming <em class="italic">j</em> is the <span class="No-Break">sample index.</span></p>
<h3>Updating the weights</h3>
<p>Updating the<a id="_idIndexMarker315"/> weights is done by trying to minimize the loss function. Again, this is almost the same as in a linear regression. The tricky part is that, unlike linear regression, we have several layers of units with weights and biases that all need to be updated. This is where the so-called backpropagation allows the updating of each layer step by step, from the rightmost to the leftmost (following the convention in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.2</em></span><span class="No-Break">).</span></p>
<p>The details of the backpropagation, although useful and interesting, are beyond this <span class="No-Break">book’s scope.</span></p>
<p>Also, just like there are several algorithms to optimize the weights in a logistic regression (the <strong class="source-inline">solver</strong> parameter in scikit-learn’s <strong class="source-inline">LogisticRegression</strong>), there are several algorithms to train a neural network. They are commonly <a id="_idIndexMarker316"/>called <strong class="bold">optimizers</strong>. Among the most frequently used<a id="_idIndexMarker317"/> are<a id="_idIndexMarker318"/> the <strong class="bold">Stochastic Gradient Descent</strong> (<strong class="bold">SGD</strong>) and <strong class="bold">Adaptive </strong><span class="No-Break"><strong class="bold">momentum</strong></span><span class="No-Break"> (</span><span class="No-Break"><strong class="bold">Adam</strong></span><span class="No-Break">).</span></p>
<h3>PyTorch</h3>
<p>PyTorch <a id="_idIndexMarker319"/>is a <a id="_idIndexMarker320"/>widely used framework for deep learning, allowing us to easily train and reuse deep <span class="No-Break">learning models.</span></p>
<p>It is fairly easy to use and can be easily installed with the <span class="No-Break">following command:</span></p>
<pre class="source-code">
pip install torch</pre>
<p>For this recipe, we will also need scikit-learn and matplotlib, which can be installed with <strong class="source-inline">pip install </strong><span class="No-Break"><strong class="source-inline">scikit-learn matplotlib</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-172"><a id="_idTextAnchor172"/>How to do it…</h2>
<p>In this recipe, we <a id="_idIndexMarker321"/>will build and train a neural network on the California <span class="No-Break">housing dataset:</span></p>
<ol>
<li>First, we<a id="_idIndexMarker322"/> need the required imports. Among the imports are some from scikit-learn that we have already used in <span class="No-Break">this book:</span><ul><li><strong class="source-inline">fetch_california_housing</strong> to load <span class="No-Break">the dataset</span></li><li><strong class="source-inline">train_test_split</strong> to split the data into training and <span class="No-Break">test sets</span></li><li><strong class="source-inline">StandardScaler</strong> to rescale the <span class="No-Break">quantitative data</span></li><li><strong class="source-inline">r2_score</strong> to evaluate <span class="No-Break">the model</span></li></ul></li>
<li>For display purposes, we also <span class="No-Break">import matplotlib:</span><pre class="source-code">
from sklearn.datasets import fetch_california_housing</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.preprocessing import StandardScaler</pre><pre class="source-code">
from sklearn.metrics import r2_score</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre></li>
<li>We also need some imports <span class="No-Break">from torch:</span><ul><li><strong class="source-inline">torch</strong> itself for some functions at the lower level of <span class="No-Break">the library</span></li><li><strong class="source-inline">torch.nn</strong> containing many useful classes for building a <span class="No-Break">neural network</span></li><li><strong class="source-inline">torch.nn.functional</strong> for some <span class="No-Break">useful functions</span></li><li><strong class="source-inline">Dataset</strong> and <strong class="source-inline">DataLoader</strong> for handling the <span class="No-Break">data operations:</span><pre class="source-code">
import torch</pre><pre class="source-code">
import torch.nn as nn</pre><pre class="source-code">
import torch.nn.functional as F</pre><pre class="source-code">
from torch.utils.data import Dataset, DataLoader</pre></li></ul></li>
<li>We<a id="_idIndexMarker323"/> need to load the data using the <strong class="source-inline">fetch_california_housing</strong> function and return the features <span class="No-Break">and labels:</span><pre class="source-code">
X, y = fetch_california_housing(return_X_y=True)</pre></li>
<li>We can<a id="_idIndexMarker324"/> then split the data into training and test sets using the <strong class="source-inline">train_test_split</strong> function. We set a test size of 20% and a random state <span class="No-Break">for reproducibility:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    X.astype(np.float32), y.astype(np.float32),</pre><pre class="source-code">
       test_size=0.2, random_state=0)</pre></li>
<li>We can now rescale the data with a <span class="No-Break">standard scaler:</span><pre class="source-code">
scaler = StandardScaler()</pre><pre class="source-code">
X_train = scaler.fit_transform(X_train)</pre><pre class="source-code">
X_test = scaler.transform(X_test)</pre></li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">Note that we convert the <em class="italic">X</em> and <em class="italic">y</em> variables to float32 variables. This is necessary to prevent later troubles with PyTorch not properly handling <span class="No-Break">float64 variables.</span></p>
<ol>
<li value="7">For PyTorch, we need to create the dataset class. Nothing complicated here though; this class requires only the following to <span class="No-Break">work properly:</span><ul><li>It has to inherit from the <strong class="source-inline">Dataset</strong> class (<span class="No-Break">imported earlier)</span></li><li>It has to have a constructor (<strong class="source-inline">__init__</strong> method) that deals with (and optionally prepares) <span class="No-Break">the data</span></li><li>It has to<a id="_idIndexMarker325"/> have a <strong class="source-inline">__len__</strong> method, so that the number of samples can <span class="No-Break">be fetched</span></li><li>It has to have a <strong class="source-inline">__getitem__</strong> method, in order to get <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> for any <span class="No-Break">given index</span></li></ul></li>
</ol>
<p>Let’s<a id="_idIndexMarker326"/> implement this for the California dataset, and let’s call our <span class="No-Break">class </span><span class="No-Break"><strong class="source-inline">CaliforniaDataset</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
class CaliforniaDataset(Dataset):
    def __init__(self, X: np.array, y: np.array):
        self.X = torch.from_numpy(X)
        self.y = torch.from_numpy(y)
    def __len__(self) -&gt; int:
        return len(self.X)
    def __getitem__(self, idx: int) -&gt; tuple[torch.Tensor]:
        return self.X[idx], self.y[idx]</pre>
<p>If we break this class down, we have the <span class="No-Break">following functions:</span></p>
<ul>
<li>The <strong class="source-inline">init</strong> constructor simply converts <strong class="source-inline">X</strong> and <strong class="source-inline">y</strong> to torch tensors with the <strong class="source-inline">torch.from_numpy</strong> function and stores the results as <span class="No-Break">class attributes</span></li>
<li>The <strong class="source-inline">len</strong> method just returns the length of the <strong class="source-inline">X</strong> attribute; it would work equally using the length of the <span class="No-Break"><strong class="source-inline">y</strong></span><span class="No-Break"> attribute</span></li>
<li>The <strong class="source-inline">getitem</strong> method simply returns a tuple with the given item <strong class="source-inline">idx</strong> of the <strong class="source-inline">X</strong> and <span class="No-Break"><strong class="source-inline">y</strong></span><span class="No-Break"> tensors</span></li>
</ul>
<p>This is quite straightforward, and will then allow <strong class="source-inline">pytorch</strong> to know what the data is, how many samples are in the dataset, and what the sample <strong class="source-inline">i</strong> is. For that, we will need to instantiate a <span class="No-Break"><strong class="source-inline">DataLoader</strong></span><span class="No-Break"> class.</span></p>
<p class="callout-heading">Tip</p>
<p class="callout">Rescaling can also be computed in this <strong class="source-inline">CaliforniaDataset</strong> class, as well as <span class="No-Break">any preprocessing.</span></p>
<ol>
<li value="8">Now we<a id="_idIndexMarker327"/> instantiate the <strong class="source-inline">CaliforniaDataset</strong> objects for the training and test datasets. Then we instantiate the associated loaders using the imported <span class="No-Break"><strong class="source-inline">DataLoader</strong></span><span class="No-Break"> class:</span><pre class="source-code">
# Instantiate datasets</pre><pre class="source-code">
training_data = CaliforniaDataset(X_train, y_train)</pre><pre class="source-code">
test_data = CaliforniaDataset(X_test, y_test)</pre><pre class="source-code">
# Instantiate data loaders</pre><pre class="source-code">
train_dataloader = DataLoader(training_data,</pre><pre class="source-code">
    batch_size=64, shuffle=True)</pre><pre class="source-code">
test_dataloader = DataLoader(test_data, batch_size=64,</pre><pre class="source-code">
    shuffle=True)</pre></li>
</ol>
<p>The <a id="_idIndexMarker328"/>data loader instances have a couple of options available. Here, we specify <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="source-inline">batch_size</strong>: The batch size for training. It may have an impact on the <span class="No-Break">final results.</span></li>
<li><strong class="source-inline">shuffle</strong>: Determines whether to shuffle the data at <span class="No-Break">each epoch.</span></li>
</ul>
<ol>
<li value="9">We can finally create the neural network model class. For this class, we only need to fill in <span class="No-Break">two methods:</span><ul><li>The constructor with whatever is useful, such as parameters <span class="No-Break">and attributes</span></li><li>The <strong class="source-inline">forward</strong> method that computes the <span class="No-Break">forward propagation:</span><pre class="source-code">
class Net(nn.Module):</pre><pre class="source-code">
    def __init__(self, input_shape: int,</pre><pre class="source-code">
        hidden_units: int = 24):</pre><pre class="source-code">
            super(Net, self).__init__()</pre><pre class="source-code">
            self.hidden_units = hidden_units</pre><pre class="source-code">
            self.fc1 = nn.Linear(input_shape,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.fc2 = nn.Linear(self.hidden_units,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.output = nn.Linear(self.hidden_units,</pre><pre class="source-code">
                1)</pre><pre class="source-code">
    def forward(self,</pre><pre class="source-code">
        x: torch.Tensor) -&gt; torch.Tensor:</pre><pre class="source-code">
            x = self.fc1(x)</pre><pre class="source-code">
            x = F.relu(x)</pre><pre class="source-code">
            x = self.fc2(x)</pre><pre class="source-code">
            x = F.relu(x)</pre><pre class="source-code">
            output = self.output(x)</pre><pre class="source-code">
            return output</pre></li></ul></li>
</ol>
<p>If we break it<a id="_idIndexMarker329"/> down, we have designed a class that takes two <span class="No-Break">input parameters:</span></p>
<ul>
<li><strong class="source-inline">input_shape</strong> is the input shape of the neural networks – this is basically the number of features in <span class="No-Break">the dataset</span></li>
<li><strong class="source-inline">hidden_units</strong> is the number of units in the hidden layers, which defaults <span class="No-Break">to 24</span></li>
</ul>
<p>The <a id="_idIndexMarker330"/>neural network itself comprises <span class="No-Break">the following:</span></p>
<ul>
<li>Two hidden layers of <strong class="source-inline">hidden_units</strong> units with ReLU <span class="No-Break">activation functions</span></li>
<li>One output layer of one unit, since we need to predict <span class="No-Break">one value</span></li>
</ul>
<p class="callout-heading">Important note</p>
<p class="callout">More context about ReLU and other activation functions will be given in the next <em class="italic">There’s </em><span class="No-Break"><em class="italic">more</em></span><span class="No-Break"> subsection.</span></p>
<ol>
<li value="10">We can now instantiate a neural network and test it on random data of the expected shape to check whether the <strong class="source-inline">forward</strong> method is <span class="No-Break">working properly:</span><pre class="source-code">
# Instantiate the network</pre><pre class="source-code">
net = Net(X_train.shape[1])</pre><pre class="source-code">
# Generate one random sample of 8 features</pre><pre class="source-code">
random_data = torch.rand((1, X_train.shape[1]))</pre><pre class="source-code">
# Compute the forward</pre><pre class="source-code">
propagationprint(net(random_data))</pre></li>
</ol>
<p>We’ll get <span class="No-Break">this output:</span></p>
<pre class="source-code">
<strong class="bold">tensor([[-0.0003]], grad_fn=&lt;AddmmBackward0&gt;)</strong></pre>
<p>As we can see, the computation of the forward propagation on the random data worked well and returns one single value, as expected. Any error in that step would mean we did <span class="No-Break">something wrong.</span></p>
<ol>
<li value="11">Before<a id="_idIndexMarker331"/> being able to train the neural network on the data, we need to define the loss function and the optimizer. Fortunately, the mean squared error is already <a id="_idIndexMarker332"/>implemented and available as <strong class="source-inline">nn.MSELoss()</strong>. There are plenty of optimizers available; we decided to use Adam here, but other optimizers can also <span class="No-Break">be tested:</span><pre class="source-code">
criterion = nn.MSELoss()</pre><pre class="source-code">
optimizer = torch.optim.Adam(net.parameters(), lr=0.001)</pre></li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">The optimizer needs the network parameters as input to <span class="No-Break">its constructor.</span></p>
<ol>
<li value="12">Finally, we can train the neural networks on 10 epochs with the following piece <span class="No-Break">of code:</span><pre class="source-code">
losses = []</pre><pre class="source-code">
# Loop over the dataset multiple times</pre><pre class="source-code">
for epoch in range(10):</pre><pre class="source-code">
    # Reset the loss for this epoch</pre><pre class="source-code">
    running_loss = 0.</pre><pre class="source-code">
    For I, data in enumerate(train_dataloader, 0):</pre><pre class="source-code">
        # Get the inputs per batch: data is a list of [inputs, labels]</pre><pre class="source-code">
        inputs, labels = data</pre><pre class="source-code">
        # Zero the parameter gradients</pre><pre class="source-code">
        optimizer.zero_grad()</pre><pre class="source-code">
        # Forward propagate + backward + optimize</pre><pre class="source-code">
        outputs = net(inputs)</pre><pre class="source-code">
        # Unsqueeze for dimension matching</pre><pre class="source-code">
        labels = labels.unsqueeze(1)</pre><pre class="source-code">
        # Compute the loss</pre><pre class="source-code">
        Loss = criterion(outputs, labels)</pre><pre class="source-code">
        # Backpropagate and update the weights</pre><pre class="source-code">
        loss.backward()</pre><pre class="source-code">
        optimizer.step()</pre><pre class="source-code">
        # Add this loss to the running loss</pre><pre class="source-code">
        running_loss += loss.item()</pre><pre class="source-code">
     # Compute the loss for this epoch and add it to the list</pre><pre class="source-code">
    epoch_loss = running_loss / len(</pre><pre class="source-code">
        train_dataloader)</pre><pre class="source-code">
    losses.append(epoch_loss)</pre><pre class="source-code">
    # Print the epoch and training loss</pre><pre class="source-code">
    print(f'[epoch {epoch + 1}] loss: {</pre><pre class="source-code">
        epoch_loss:.3f}')print('Finished Training')</pre></li>
</ol>
<p>Hopefully, the comments are self-explanatory. Basically, there are two <span class="No-Break">nested loops:</span></p>
<ul>
<li>One outer loop over the epochs: the number of times the model is trained over the <span class="No-Break">whole dataset</span></li>
<li>One inner loop over the samples: for each step, a batch of <strong class="source-inline">batch_size</strong> samples is used to train <span class="No-Break">the model</span></li>
</ul>
<p>For each<a id="_idIndexMarker333"/> step in the inner loop, we have the following <span class="No-Break">main steps:</span></p>
<ul>
<li>Get a batch of the data: both features <span class="No-Break">and labels</span></li>
<li>Forward propagate on this data and get <span class="No-Break">output predictions</span></li>
<li>Compute the loss: the mean squared error between predictions <span class="No-Break">and labels</span></li>
<li>Update the weights of the network <span class="No-Break">with backpropagation</span></li>
</ul>
<p>At the end of each step, we print the loss, which hopefully decreases with <span class="No-Break">each epoch.</span></p>
<ol>
<li value="13">We <a id="_idIndexMarker334"/>can plot the loss as a function of the epoch. This is quite visual and lets us ensure the network is learning if the loss <span class="No-Break">is decreasing:</span><pre class="source-code">
plt.plot(losses)</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('loss (MSE)')plt.show()</pre></li>
</ol>
<p>Here is the <span class="No-Break">resulting graph:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer175">
<img alt="Figure 6.3 – Resulting MSE loss as a function of the epoch" height="413" src="image/B19629_06_03.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.3 – Resulting MSE loss as a function of the epoch</p>
<p class="callout-heading">Important note</p>
<p class="callout">We could also keep track of the loss on the test set and display it at this step for more information. We will do that in the next recipe to avoid being drowned in too <span class="No-Break">much information.</span></p>
<ol>
<li value="14">We can<a id="_idIndexMarker335"/> finally evaluate the model on both<a id="_idIndexMarker336"/> the training and test sets. As we did previously in this book with regression tasks, we will use the R2-score. Any other relevant metric can be <span class="No-Break">used too:</span><pre class="source-code">
# Compute the predictions with the trained neural</pre><pre class="source-code">
Network</pre><pre class="source-code">
y_train_pred = net(torch.tensor((</pre><pre class="source-code">
    X_train))).detach().numpy()</pre><pre class="source-code">
y_test_pred = net(torch.tensor((</pre><pre class="source-code">
    X_test))).detach().numpy()</pre><pre class="source-code">
# Compute the R2-score</pre><pre class="source-code">
print('R2-score on training set:',</pre><pre class="source-code">
    r2_score(y_train, y_train_pred))</pre><pre class="source-code">
print('R2-score on test set:',</pre><pre class="source-code">
    r2_score(y_test, y_test_pred))</pre></li>
</ol>
<p>Here’s <span class="No-Break">the output:</span></p>
<pre class="source-code">
<strong class="bold">R2-score on training set: 0.7542622050620708 R2-score on test set: 0.7401526252651656</strong></pre>
<p>As we<a id="_idIndexMarker337"/> can<a id="_idIndexMarker338"/> see here, we have a reasonable R2-score of 0.74 on the training set, with <span class="No-Break">minor overfitting.</span></p>
<h2 id="_idParaDest-173"><a id="_idTextAnchor173"/>There’s more…</h2>
<p>In this recipe, we mentioned activation functions without really explaining what they are or why they <span class="No-Break">are needed.</span></p>
<p>Put simply, they add non-linearities, allowing the model to learn more complex patterns. Indeed, if we had a neural network with no activation function, the whole model would be equivalent to a linear model (e.g., a linear regression), no matter the number of layers or the number of units. This is summarized in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer176">
<img alt="Figure 6.4 – On the left, a neural network with no activation functions can only learn linearly separable decision functions. On the right, a neural network with activation functions can learn complex decision functions" height="466" src="image/B19629_06_04.jpg" width="940"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.4 – On the left, a neural network with no activation functions can only learn linearly separable decision functions. On the right, a neural network with activation functions can learn complex decision functions</p>
<p>There are many available activation functions, but some of the most common ones for hidden layers are sigmoid, ReLU, <span class="No-Break">and tanh.</span></p>
<h3>Sigmoid</h3>
<p>The <a id="_idIndexMarker339"/>sigmoid function <a id="_idIndexMarker340"/>is the same as that used in <span class="No-Break">logistic regression:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer177">
<img alt="" height="97" src="image/formula_06_024.jpg" width="446"/>
</div>
</div>
<p>This function’s values range from 0 to 1, and outputs 0.5 if <em class="italic">x = </em><span class="No-Break"><em class="italic">0</em></span><span class="No-Break">.</span></p>
<h3>tanh</h3>
<p>The<a id="_idIndexMarker341"/> tanh or <a id="_idIndexMarker342"/>hyperbolic tangent function ranges from -1 to 1, with a value of 0 if <em class="italic">x</em> <span class="No-Break">is 0:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer178">
<img alt="" height="101" src="image/formula_06_025.jpg" width="391"/>
</div>
</div>
<h3>ReLU</h3>
<p>The <strong class="bold">ReLU</strong> or <strong class="bold">Rectified Linear Unit</strong> function <a id="_idIndexMarker343"/>just returns 0 for any <a id="_idIndexMarker344"/>input negative value, and <em class="italic">x</em> for any positive input value <em class="italic">x</em>. Unlike sigmoid and tanh, it does not plateau and thus limits vanishing gradient problems. Its formula is <span class="No-Break">the following:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer179">
<img alt="" height="47" src="image/formula_06_026.jpg" width="398"/>
</div>
</div>
<h3>Visualization</h3>
<p>We<a id="_idIndexMarker345"/> can visualize these three activation functions (sigmoid, tanh, and ReLU) together for a more intuitive understanding with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
import numpy as np
import matplotlib.pyplot as plt
x = np.arange(-2, 2, 0.02)
sigmoid = 1./(1+np.exp(-x))
tanh = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))
relu = np.max([np.zeros(len(x)), x], axis=0)
plt.plot(x, sigmoid)
plt.plot(x, tanh)
plt.plot(x, relu)plt.grid()
plt.xlabel('x')
plt.ylabel('activation')
plt.legend(['sigmoid', 'tanh', 'relu'])
plt.show()</pre>
<p>You’ll get<a id="_idIndexMarker346"/> this output upon running the previous code, which computes the output values of these functions in the [-2, <span class="No-Break">2] range:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer180">
<img alt="Figure 6.5 – Resulting plot of the sigmoid, tanh, and ReLU activation functions in the [-2, 2] input range" height="410" src="image/B19629_06_05.jpg" width="554"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.5 – Resulting plot of the sigmoid, tanh, and ReLU activation functions in the [-2, 2] input range</p>
<p>For more <a id="_idIndexMarker347"/>about the available activation functions in PyTorch, have a look at the following <span class="No-Break">link: </span><a href="https://pytorch.org/docs/stable/nn.xhtml#non-linear-activations-weighted-sum-nonlinearity%0D"><span class="No-Break">https://pytorch.org/docs/stable/nn.xhtml#non-linear-activations-weighted-sum-nonlinearity</span><span class="No-Break">.</span></a></p>
<h2 id="_idParaDest-174"><a id="_idTextAnchor174"/>See also</h2>
<p>Here are several links to PyTorch tutorials that can be helpful for gaining familiarity with it, along with a deeper understanding of how <span class="No-Break">it works:</span></p>
<ul>
<li><a href="https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.xhtml"><span class="No-Break">https://pytorch.org/tutorials/recipes/recipes/defining_a_neural_network.xhtml</span></a></li>
<li><a href="https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.xhtml"><span class="No-Break">https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.xhtml</span></a></li>
<li><a href="https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.xhtml"><span class="No-Break">https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.xhtml</span></a></li>
<li><a href="https://pytorch.org/tutorials/beginner/basics/data_tutorial.xhtml?highlight=dataset"><span class="No-Break">https://pytorch.org/tutorials/beginner/basics/data_tutorial.xhtml?highlight=dataset</span></a></li>
</ul>
<p>And the following link is for a very well-written website about deep learning, for those who wish to have a better understanding of neural networks, gradient descent, and <span class="No-Break">backpropagation: </span><a href="http://neuralnetworksanddeeplearning.com/%0D"><span class="No-Break">http://neuralnetworksanddeeplearning.com/</span><span class="No-Break">.</span></a></p>
<h1 id="_idParaDest-175"><a id="_idTextAnchor175"/>Training a neural network for binary classification</h1>
<p>In this<a id="_idIndexMarker348"/> recipe, let’s train our first neural network for a binary classification task on the breast cancer dataset. We will also learn more about the impact of the learning rate and the optimizer on the optimization, as well as how to evaluate the model against the <span class="No-Break">test set.</span></p>
<h2 id="_idParaDest-176"><a id="_idTextAnchor176"/>Getting ready</h2>
<p>As we will see in this recipe, training a neural network for binary classification is not so different from training a neural network for regression. Primarily, two changes have to <span class="No-Break">be made:</span></p>
<ul>
<li>The output layer’s <span class="No-Break">activation function</span></li>
<li>The <span class="No-Break">loss function</span></li>
</ul>
<p>In the previous recipe for a regression task, the output layer had no activation function. Indeed, for a regression, one can expect the prediction to take <span class="No-Break">any value.</span></p>
<p>For a binary classification, we expect the output to be a probability, so a value between 0 and 1, just like the logistic regression. This is why when doing a binary classification, the output layer’s activation function is usually the sigmoid function. The resulting predictions will be just like those of a logistic regression: a number on which to apply a threshold (e.g., 0.5) above which we consider the prediction to be <span class="No-Break">class 1.</span></p>
<p>As the labels are 0s and 1s, and the predictions are values between 0 and 1, the mean squared error is no longer suited to train such a model. So, just like for a logistic regression, we would use binary <span class="No-Break">cross-entropy loss.</span></p>
<p>The required libraries for this recipe are matplotlib, scikit-learn, and PyTorch, they can be installed with <strong class="source-inline">pip install matplotlib </strong><span class="No-Break"><strong class="source-inline">scikit-learn torch</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-177"><a id="_idTextAnchor177"/>How to do it…</h2>
<p>We will <a id="_idIndexMarker349"/>train a simple neural network with two hidden layers on a binary classification task on the breast cancer dataset. Even though this dataset is not really suited for deep learning since it is a small dataset, it allows us to easily understand all the steps involved in training a neural network for <span class="No-Break">binary classification:</span></p>
<ol>
<li>We have the following required imports <span class="No-Break">from scikit-learn:</span><ul><li><strong class="source-inline">load_breast_cancer</strong> to load <span class="No-Break">the dataset</span></li><li><strong class="source-inline">train_test_split</strong> to split the data into training and <span class="No-Break">test sets</span></li><li><strong class="source-inline">StandardScaler</strong> to rescale the <span class="No-Break">quantitative data</span></li><li><strong class="source-inline">accuracy_score</strong> to evaluate <span class="No-Break">the model</span></li></ul></li>
</ol>
<p>We also need matplotlib for display, and we need the following <span class="No-Break">from torch:</span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline">torch</strong></span><span class="No-Break"> itself</span></li>
<li><strong class="source-inline">torch.nn</strong> containing required classes for building a <span class="No-Break">neural network</span></li>
<li><strong class="source-inline">torch.nn.functional</strong> for activation functions such <span class="No-Break">as ReLU</span></li>
<li><strong class="source-inline">Dataset</strong> and <strong class="source-inline">DataLoader</strong> for handling <span class="No-Break">the data</span><pre class="source-code">
from sklearn.datasets import load_breast_cancer</pre><pre class="source-code">
from sklearn.model_selection import train_test_split</pre><pre class="source-code">
from sklearn.preprocessing import StandardScaler</pre><pre class="source-code">
from sklearn.metrics import accuracy_score</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre><pre class="source-code">
import torchimport torch.nn as nn</pre><pre class="source-code">
import torch.nn.functional as F</pre><pre class="source-code">
from torch.utils.data import Dataset, DataLoader</pre></li>
</ul>
<ol>
<li value="2">Load the features and labels with the <span class="No-Break"><strong class="source-inline">load_breast_cancer</strong></span><span class="No-Break"> function:</span><pre class="source-code">
X, y = load_breast_cancer(return_X_y=True)</pre></li>
<li>Split the data into training and test sets, specifying the random state for reproducibility. Also cast the features and labels for <strong class="source-inline">float32</strong> for later compatibility <span class="No-Break">with PyTorch:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    X.astype(np.float32), y.astype(np.float32),</pre><pre class="source-code">
    test_size=0.2, random_state=0)</pre></li>
<li>Create<a id="_idIndexMarker350"/> the <strong class="source-inline">Dataset</strong> class for handling the data. Note that in this recipe we integrate the data rescaling in this step, unlike in the <span class="No-Break">previous recipe:</span><pre class="source-code">
class BreastCancerDataset(Dataset):</pre><pre class="source-code">
    def __init__(self, X: np.array, y: np.array,</pre><pre class="source-code">
        x_scaler: StandardScaler = None):</pre><pre class="source-code">
            if x_scaler is None:</pre><pre class="source-code">
                self.x_scaler = StandardScaler()</pre><pre class="source-code">
                X = self.x_scaler.fit_transform(X)</pre><pre class="source-code">
            else:</pre><pre class="source-code">
                self.x_scaler = x_scaler</pre><pre class="source-code">
                X = self.x_scaler.transform(X)</pre><pre class="source-code">
            self.X = torch.from_numpy(X)</pre><pre class="source-code">
            self.y = torch.from_numpy(y)</pre><pre class="source-code">
    def __len__(self) -&gt; int:</pre><pre class="source-code">
        return len(self.X)</pre><pre class="source-code">
    def __getitem__(self, idx: int) -&gt; tuple[torch.Tensor]:</pre><pre class="source-code">
        return self.X[idx], self.y[idx]</pre></li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">Having the scaler in the class has pros and cons, where a pro is properly handling data leakage between train and <span class="No-Break">test sets.</span></p>
<ol>
<li value="5">Instantiate the training and test sets and loaders. Note that no scaler is provided to the training dataset, while the test dataset is given the training set scaler to ensure that all the data is processed the same with no <span class="No-Break">data leakage:</span><pre class="source-code">
training_data = BreastCancerDataset(X_train, y_train)</pre><pre class="source-code">
test_data = BreastCancerDataset(X_test, y_test,</pre><pre class="source-code">
    training_data.x_scaler)</pre><pre class="source-code">
train_dataloader = DataLoader(training_data,</pre><pre class="source-code">
    batch_size=64, shuffle=True)</pre><pre class="source-code">
test_dataloader = DataLoader(test_data, batch_size=64,</pre><pre class="source-code">
    shuffle=True)</pre></li>
<li>Build<a id="_idIndexMarker351"/> the neural network. Here, we build a neural network with two hidden layers. In the <strong class="source-inline">forward</strong> method, the <strong class="source-inline">torch.sigmoid()</strong> function is applied to the output layer before returning the value, ensuring we have a prediction between 0 and 1. The only parameter needed to instantiate the model is the input shape, which is simply the number of <span class="No-Break">features here:</span><pre class="source-code">
class Net(nn.Module):</pre><pre class="source-code">
    def __init__(self, input_shape: int,</pre><pre class="source-code">
        hidden_units: int = 24):</pre><pre class="source-code">
            super(Net, self).__init__()</pre><pre class="source-code">
                self.hidden_units = hidden_units</pre><pre class="source-code">
                self.fc1 = nn.Linear(input_shape,</pre><pre class="source-code">
                    self.hidden_units)</pre><pre class="source-code">
                self.fc2 = nn.Linear(</pre><pre class="source-code">
                    self.hidden_units,</pre><pre class="source-code">
                    self.hidden_units)</pre><pre class="source-code">
                self.output = nn.Linear(</pre><pre class="source-code">
                    self.hidden_units, 1)</pre><pre class="source-code">
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:</pre><pre class="source-code">
        x = self.fc1(x)</pre><pre class="source-code">
        x = F.relu(x)</pre><pre class="source-code">
        x = self.fc2(x)</pre><pre class="source-code">
        x = F.relu(x)</pre><pre class="source-code">
        output = torch.sigmoid(self.output(x))</pre><pre class="source-code">
        return output</pre></li>
<li>We can now instantiate the model with the right input shape and check that the forward propagation works properly on a given <span class="No-Break">random tensor:</span><pre class="source-code">
# Instantiate the network</pre><pre class="source-code">
net = Net(X_train.shape[1])</pre><pre class="source-code">
# Generate one random sample</pre><pre class="source-code">
random_data = torch.rand((1, X_train.shape[1]))</pre><pre class="source-code">
# Compute the forward propagation</pre><pre class="source-code">
print(net(random_data))</pre></li>
</ol>
<p>After running the previous code, we get the <span class="No-Break">following output:</span></p>
<pre class="source-code">
<strong class="bold">tensor([[0.4487]], grad_fn=&lt;SigmoidBackward0&gt;)</strong></pre>
<ol>
<li value="8">Define <a id="_idIndexMarker352"/>the loss function and the optimizer. As stated, we will use the binary cross-entropy loss, available as <strong class="source-inline">nn.BCELoss()</strong> in PyTorch. The chosen optimizer is <strong class="source-inline">Adam</strong>, but other optimizers can be <span class="No-Break">tested too:</span><pre class="source-code">
criterion = nn.BCELoss()</pre><pre class="source-code">
optimizer = torch.optim.Adam(net.parameters(),</pre><pre class="source-code">
    lr=0.001)</pre></li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">More explanations about the optimizer are provided in the next <em class="italic">There’s </em><span class="No-Break"><em class="italic">more </em></span><span class="No-Break">subsection.</span></p>
<ol>
<li value="9">We can now train the neural network for 50 epochs. We also compute both the training and test set loss at each epoch, so we can plot them afterward. To do so, we need to switch mode for <span class="No-Break">the model:</span><ul><li>Before training on the training set, switch to train mode <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">model.train()</strong></span></li><li>Before evaluating the test set, switch to the <strong class="source-inline">eval</strong> model <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">model.eval()</strong></span><pre class="source-code">
train_losses = []</pre><pre class="source-code">
test_losses = []</pre><pre class="source-code">
# Loop over the dataset 50 times</pre><pre class="source-code">
for epoch in range(50):</pre><pre class="source-code">
    ## Train the model on the training set</pre><pre class="source-code">
    running_train_loss = 0.</pre><pre class="source-code">
    # Switch to train mode</pre><pre class="source-code">
    net.train()</pre><pre class="source-code">
    # Loop over the batches in train set</pre><pre class="source-code">
    for i, data in enumerate(train_dataloader, 0):</pre><pre class="source-code">
        # Get the inputs: data is a list of [inputs, labels]</pre><pre class="source-code">
        inputs, labels = data</pre><pre class="source-code">
        # Zero the parameter gradients</pre><pre class="source-code">
        optimizer.zero_grad()</pre><pre class="source-code">
        # Forward + backward + optimize</pre><pre class="source-code">
        outputs = net(inputs)</pre><pre class="source-code">
        loss = criterion(outputs, labels.unsqueeze(1))</pre><pre class="source-code">
        loss.backward()</pre><pre class="source-code">
        optimizer.step()</pre><pre class="source-code">
        # Add current loss to running loss</pre><pre class="source-code">
        running_train_loss += loss.item()</pre><pre class="source-code">
    # Once epoch is over, compute and store the epoch loss</pre><pre class="source-code">
    train_epoch_loss = running_train_loss / len(</pre><pre class="source-code">
        train_dataloader)</pre><pre class="source-code">
    train_losses.append(train_epoch_loss)</pre><pre class="source-code">
    ## Evaluate the model on the test set</pre><pre class="source-code">
    running_test_loss = 0.</pre><pre class="source-code">
    # Switch to eval model</pre><pre class="source-code">
    net.eval()</pre><pre class="source-code">
    with torch.no_grad():</pre><pre class="source-code">
        # Loop over the batches in test set</pre><pre class="source-code">
        for i, data in enumerate(test_dataloader, 0):</pre><pre class="source-code">
            # Get the inputs</pre><pre class="source-code">
            inputs, labels = data</pre><pre class="source-code">
            # Compute forward propagation</pre><pre class="source-code">
            outputs = net(inputs)</pre><pre class="source-code">
            # Compute loss</pre><pre class="source-code">
            loss = criterion(outputs,</pre><pre class="source-code">
                labels.unsqueeze(1))</pre><pre class="source-code">
            # Add to running loss</pre><pre class="source-code">
            running_test_loss += loss.item()</pre><pre class="source-code">
            # Compute and store the epoch loss</pre><pre class="source-code">
            test_epoch_loss = running_test_loss / len(</pre><pre class="source-code">
                test_dataloader)</pre><pre class="source-code">
            test_losses.append(test_epoch_loss)</pre><pre class="source-code">
    # Print stats</pre><pre class="source-code">
    print(f'[epoch {epoch + 1}] Training loss: {</pre><pre class="source-code">
        train_epoch_loss:.3f} | Test loss: {</pre><pre class="source-code">
            test_epoch_loss:.3f}')</pre><pre class="source-code">
    print('Finished Training')</pre></li></ul></li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">Notice the use of <strong class="source-inline">with</strong> <strong class="source-inline">torch.no_grad()</strong> around the evaluation part. This line of code allows us to deactivate the autograd engine and speed <span class="No-Break">up processing.</span></p>
<ol>
<li value="10">Now we plot the losses for the train and test sets as a function of the epoch, using <a id="_idIndexMarker353"/>the two computed lists in the <span class="No-Break">previous step:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.xlabel('epoch')plt.ylabel('loss (BCE)')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here’s <span class="No-Break">the output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer181">
<img alt="Figure 6.6 – Resulting MSE loss for the train and test sets" height="413" src="image/B19629_06_06.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.6 – Resulting MSE loss for the train and test sets</p>
<p>As we can see, both losses are decreasing. At first, the train and test losses are almost equal, but after 10 epochs, the train loss keeps decreasing while the test does not, meaning the model has overfit on the <span class="No-Break">training set.</span></p>
<ol>
<li value="11">It is <a id="_idIndexMarker354"/>possible to evaluate the model using the accuracy scores from both the training and test sets, via the <strong class="source-inline">accuracy_score</strong> function of scikit-learn. It requires a few more steps to compute the predictions, since we have to do the following operations before getting actual <span class="No-Break">class predictions:</span><ul><li>Rescale the data with the scaler used for training, available in the <span class="No-Break"><strong class="source-inline">training_data.x_scaler</strong></span><span class="No-Break"> attribute</span></li><li>Cast the NumPy data to the torch tensor <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">torch.tensor()</strong></span></li><li>Apply forward propagation to <span class="No-Break">the model</span></li><li>Cast the output torch tensor back to NumPy <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">.detach().numpy()</strong></span></li><li>Apply a threshold to convert a probability prediction (between 0 and 1) to a class prediction with <strong class="source-inline">&gt; </strong><span class="No-Break"><strong class="source-inline">0.5</strong></span><pre class="source-code">
# Compute the predictions with the trained neural network</pre><pre class="source-code">
y_train_pred = net(torch.tensor((</pre><pre class="source-code">
    training_data.x_scaler.transform(</pre><pre class="source-code">
        X_train)))).detach().numpy() &gt; 0.5</pre><pre class="source-code">
y_test_pred = net(torch.tensor((</pre><pre class="source-code">
    training_data.x_scaler.transform(</pre><pre class="source-code">
        X_test)))).detach().numpy() &gt; 0.5</pre><pre class="source-code">
# Compute the accuracy score</pre><pre class="source-code">
print('Accuracy on training set:', accuracy_score(</pre><pre class="source-code">
    y_train, y_train_pred))</pre><pre class="source-code">
print('Accuracy on test set:', accuracy_score(y_test,</pre><pre class="source-code">
    y_test_pred))</pre></li></ul></li>
</ol>
<p>Here is the <a id="_idIndexMarker355"/>output of the <span class="No-Break">preceding code:</span></p>
<pre class="source-code">
<strong class="bold">Accuracy on training set: 0.9912087912087912 Accuracy on test set: 0.9649122807017544</strong></pre>
<p>We get an accuracy of 99% on the training and 96% on the test set, proving there is indeed overfitting, as expected from the curve of the train and test losses as a function of <span class="No-Break">the epoch.</span></p>
<h2 id="_idParaDest-178"><a id="_idTextAnchor178"/>There’s more…</h2>
<p>As we have seen here, the loss is decreasing over time, meaning the model is <span class="No-Break">actually learning.</span></p>
<p class="callout-heading">Important note</p>
<p class="callout">Even if it’s sometimes a bit noisy with bumps in the loss, as long as the overall trend remains good, there is nothing to <span class="No-Break">worry about.</span></p>
<p>There are two important notions that may alter these results, somewhat related to each other: the learning rate and the optimizer. As with logistic regression or linear regression, the optimizer’s goal is to find the parameters that provide the lowest possible loss value. Therefore, this is a minimization problem and can be represented as in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.7</em>: we seek to find the set of parameters that give the lowest <span class="No-Break">possible value.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer182">
<img alt="Figure 6.7 – Representation of the loss L as a function of the parameters w. The red cross is the optimal point, while the blue cross is a random arbitrary set of weights" height="717" src="image/B19629_06_07.jpg" width="690"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.7 – Representation of the loss L as a function of the parameters w. The red cross is the optimal point, while the blue cross is a random arbitrary set of weights</p>
<p>Let’s see how the learning rate can impact the <span class="No-Break">learning curve.</span></p>
<h3>Learning rate</h3>
<p>The<a id="_idIndexMarker356"/> learning rate is set in PyTorch when instantiating the optimizer, with the <em class="italic">lr=0.001</em> parameter for example. Arguably, we can have four main cases for the learning rate value, as presented in <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.8</em>, from a low learning rate to a very high <span class="No-Break">learning rate.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer183">
<img alt="Figure 6.8 – The four main categories of learning rate: too low learning rate, good learning rate, high learning rate, very high learning rate (diverging loss)" height="794" src="image/B19629_06_08.jpg" width="908"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.8 – The four main categories of learning rate: too low learning rate, good learning rate, high learning rate, very high learning rate (diverging loss)</p>
<p>In terms <a id="_idIndexMarker357"/>of loss, the rates can be intuited from <span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em>, presenting the <a id="_idIndexMarker358"/>evolution of the weights and the loss for <span class="No-Break">several epochs.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer184">
<img alt="Figure 6.9 – A visual interpretation of the four cases of learning rate: a) a low learning rate, b) a good learning rate, c) a high learning rate, d) a very high learning rate" height="1679" src="image/B19629_06_09.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.9 – A visual interpretation of the four cases of learning rate: a) a low learning rate, b) a good learning rate, c) a high learning rate, d) a very high learning rate</p>
<p><span class="No-Break"><em class="italic">Figure 6</em></span><em class="italic">.9</em> can be <a id="_idIndexMarker359"/>further explained with <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">A low learning rate (a)</strong>: The loss will decrease over the epochs but too slowly and may take a very long time to converge. It may also get the model stuck in a <span class="No-Break">local minimum.</span></li>
<li><strong class="bold">A good learning rate (b)</strong>: The loss will decrease steadily until it gets close enough to the <span class="No-Break">global minimum.</span></li>
<li><strong class="bold">A slightly too large learning rate (c)</strong>: The loss will decrease steeply at first but may soon jump over the global minimum without being able to ever <span class="No-Break">reach it.</span></li>
<li><strong class="bold">A very high learning rate (d)</strong>: The loss will rapidly diverge, taking learning steps that are way <span class="No-Break">too large.</span></li>
</ul>
<p>Tuning the learning rate may sometimes help to produce the best results. Several techniques, such as the so-called learning rate decay, decrease the learning rate over time to hopefully more accurately catch the <span class="No-Break">global minimum.</span></p>
<h3>Optimizer</h3>
<p>There <a id="_idIndexMarker360"/>are many robust and useful optimizers in deep learning besides the arguably most famous ones (the stochastic gradient descent and Adam). Without getting into the details of those optimizers, let’s just give some insight into how they work and their differences, summarized in <span class="No-Break"><em class="italic">Figure 6</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">:</span></p>
<ul>
<li><strong class="bold">Stochastic gradient descent</strong> simply computes the gradients from the loss for each batch, without any further sophistication. It means that sometimes, the optimization of one batch may be almost in the opposite direction of <span class="No-Break">another batch.</span></li>
<li><strong class="bold">Adam</strong> uses momentum, meaning that for each batch, not only the gradient from this batch is used, but also the momentum of the previously computed gradients. This allows Adam to keep an overall more consistent direction, and hopefully to <span class="No-Break">converge faster.</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer185">
<img alt="Figure 6.10 – A visual representation of training towards a global minimum, on the left with stochastic gradient descent; on the right with Adam keeping the momentum of previous steps" height="789" src="image/B19629_06_10.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.10 – A visual representation of training towards a global minimum, on the left with stochastic gradient descent; on the right with Adam keeping the momentum of previous steps</p>
<p>The optimization process can be summarized with quite a simple metaphor. This is like hiking somewhere up a mountain, and then trying to go down (to the global minimum) while surrounded by fog. You can either go down with stochastic gradient descent <span class="No-Break">or Adam:</span></p>
<ul>
<li>With stochastic gradient descent, you look around you, choose the direction with the steepest slope downward, and take a step in that direction. And then do <span class="No-Break">it again.</span></li>
<li>With Adam, you do the same as stochastic gradient descent but running. You quickly look around you, see the direction with the steepest slope downward, and try to take a step in that direction while keeping the inertia from your previous steps since you’re running. And then do <span class="No-Break">it again.</span></li>
</ul>
<p>Note that in this analogy, the step size would be the <span class="No-Break">learning rate.</span></p>
<h2 id="_idParaDest-179"><a id="_idTextAnchor179"/>See also</h2>
<p>A list of <a id="_idIndexMarker361"/>available optimizers on <span class="No-Break">PyTorch: </span><a href="https://pytorch.org/docs/stable/optim.xhtml#algorithms"><span class="No-Break">https://pytorch.org/docs/stable/optim.xhtml#algorithms</span></a></p>
<h1 id="_idParaDest-180"><a id="_idTextAnchor180"/>Training a multiclass classification neural network</h1>
<p>In this recipe, we <a id="_idIndexMarker362"/>will have a look at another very common task: multiclass classification with neural networks, in this instance using PyTorch. We will work on a very iconic dataset in deep learning: <strong class="bold">MNIST handwritten digit recognition</strong>. This <a id="_idIndexMarker363"/>dataset is a set of small grayscale images of 28x28 pixels, depicting handwritten digits between 0 and 9, having thus <span class="No-Break">10 classes.</span></p>
<h2 id="_idParaDest-181"><a id="_idTextAnchor181"/>Getting ready</h2>
<p>In classical machine learning, multiclass classification is usually not handled natively. For example, when training logistic regression with scikit-learn on a three-class task (e.g., the Iris dataset), scikit-learn will automatically train three models, using the <span class="No-Break">one-versus-the-rest method.</span></p>
<p>In deep learning, it is possible for the model to natively handle more than two classes. To do so, only a few changes are required compared to <span class="No-Break">binary classification:</span></p>
<ul>
<li>The output layer has as many units as classes: this way, each unit will be responsible for predicting the probability of <span class="No-Break">one class</span></li>
<li>The output layer’s activation function is the softmax function, a function such that the sum of the units is equal to 1, allowing us to consider it as <span class="No-Break">a probability</span></li>
<li>The loss function is the cross-entropy loss, considering multiple classes, unlike the binary <span class="No-Break">cross entropy</span></li>
</ul>
<p>In our <a id="_idIndexMarker364"/>case, we will need a few other changes in the code that are specific to the data itself. Since the input is now an image, some transformations <span class="No-Break">are required:</span></p>
<ul>
<li>The image, a 2D (or 3D if RGB color image) array, must be flattened to <span class="No-Break">be 1D</span></li>
<li>The data must be normalized, just like rescaling for <span class="No-Break">quantitative data</span></li>
</ul>
<p>To do that, we will require the following libraries: torch, torchvision (for dataset loading and image transformation), and matplotlib for visualization. They can be installed with <strong class="source-inline">pip install torch </strong><span class="No-Break"><strong class="source-inline">torchvision matplotlib</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-182"><a id="_idTextAnchor182"/>How to do it…</h2>
<p>In this recipe, we will reuse the same pattern as previously in this chapter: we will train a two-hidden-layer neural network. But a few things will <span class="No-Break">change, though:</span></p>
<ul>
<li>The input data is a grayscale image from the MNIST handwritten digits dataset, so it’s a 2D array that needs to <span class="No-Break">be flattened</span></li>
<li>The output layer will have not one, but ten units for the ten classes of the dataset; the loss will <span class="No-Break">change accordingly</span></li>
<li>We will not only compute the training and test losses in the training loop, but also <span class="No-Break">the accuracy</span></li>
</ul>
<p>Let’s see how to do that in <span class="No-Break">practice now:</span></p>
<ol>
<li>Import the required libraries. As in previous recipes, we import several useful torch modules <span class="No-Break">and functions:</span><ul><li><span class="No-Break"><strong class="source-inline">torch</strong></span><span class="No-Break"> itself</span></li><li><strong class="source-inline">torch.nn</strong> containing the required classes for building a <span class="No-Break">neural network</span></li><li><strong class="source-inline">torch.nn.functional</strong> for activation functions such <span class="No-Break">as ReLU</span></li><li><strong class="source-inline">DataLoader</strong> for handling <span class="No-Break">the data</span></li></ul></li>
</ol>
<p>We also need some imports <span class="No-Break">from torchvision:</span></p>
<ul>
<li><strong class="source-inline">MNIST</strong> for loading <span class="No-Break">the dataset</span></li>
<li><strong class="source-inline">transforms</strong> for transforming the dataset, both rescaling and flattening <span class="No-Break">the data:</span><pre class="source-code">
import torch</pre><pre class="source-code">
import torch.nn as nn</pre><pre class="source-code">
import torch.nn.functional as F</pre><pre class="source-code">
from torch.utils.data import DataLoader</pre><pre class="source-code">
from torchvision.datasets import MNIST</pre><pre class="source-code">
import torchvision.transforms as transforms</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre></li>
</ul>
<ol>
<li value="2">Instantiate<a id="_idIndexMarker365"/> the transformations. We use the <strong class="source-inline">Compose</strong> class, allowing us to compose two or more transformations. Here, we compose <span class="No-Break">three transformations:</span><ul><li><strong class="source-inline">transforms.ToTensor()</strong>: Converts the input image to <span class="No-Break"><strong class="source-inline">torch.Tensor</strong></span><span class="No-Break"> format.</span></li><li><strong class="source-inline">transforms.Normalize()</strong>: Normalizes the image with a mean value and standard deviation. It will subtract the mean (i.e., 0.1307) and then divide by the standard deviation (i.e., 0.3081) for each <span class="No-Break">pixel value.</span></li><li><strong class="source-inline">transforms.Lambda(torch.flatten)</strong>: Flattens the 2D tensor to a <span class="No-Break">1D tensor:</span><pre class="source-code">
transform = transforms.Compose([transforms.ToTensor(),</pre><pre class="source-code">
    transforms.Normalize((0.1307), (0.3081)),</pre><pre class="source-code">
    transforms.Lambda(torch.flatten)])</pre></li></ul></li>
</ol>
<p class="callout-heading">Important note</p>
<p class="callout">Images are commonly normalized with a mean and standard deviation of 0.5. We normalize with the specific values used in the preceding code block because the dataset is made with specific images, but 0.5 would work fine too. Check the <em class="italic">See also</em> subsection of this recipe for <span class="No-Break">an explanation.</span></p>
<ol>
<li value="3">Load <a id="_idIndexMarker366"/>the train and test sets, as well as the train and data loaders. Using the <strong class="source-inline">MNIST</strong> class, we both get the train and test sets using the <strong class="source-inline">train</strong> parameter <strong class="source-inline">as</strong> <strong class="source-inline">True</strong> and <strong class="source-inline">False</strong>, respectively. We directly apply the previously defined transformations while loading the data with the MNIST class too. Then we instantiate the data loaders with a batch size <span class="No-Break">of 64:</span><pre class="source-code">
trainset = MNIST('./data', train=True, download=True,</pre><pre class="source-code">
    transform=transform)</pre><pre class="source-code">
train_dataloader = DataLoader(trainset, batch_size=64,</pre><pre class="source-code">
    shuffle=True)</pre><pre class="source-code">
testset = MNIST('./data', train=False, download=True,</pre><pre class="source-code">
    transform=transform)</pre><pre class="source-code">
test_dataloader = DataLoader(testset, batch_size=64,</pre><pre class="source-code">
    shuffle=True)</pre></li>
<li>Define the neural network. We define here by default a neural network with 2 hidden layers of 24 units. The output layer has 10 units for the 10 classes of the data (our digits between 0 and 9). Note that the softmax function is applied to the output layer, allowing the sum of the 10 units to be strictly equal <span class="No-Break">to 1:</span><pre class="source-code">
class Net(nn.Module):</pre><pre class="source-code">
    def __init__(self, input_shape: int,</pre><pre class="source-code">
        hidden_units: int = 24):</pre><pre class="source-code">
            super(Net, self).__init__()</pre><pre class="source-code">
            self.hidden_units = hidden_units</pre><pre class="source-code">
            self.fc1 = nn.Linear(input_shape,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.fc2 = nn.Linear(</pre><pre class="source-code">
                self.hidden_units,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.output = nn.Linear(</pre><pre class="source-code">
                self.hidden_units, 10)</pre><pre class="source-code">
    def forward(self,</pre><pre class="source-code">
        x: torch.Tensor) -&gt; torch.Tensor:</pre><pre class="source-code">
            x = self.fc1(x)</pre><pre class="source-code">
            x = F.relu(x)</pre><pre class="source-code">
            x = self.fc2(x)</pre><pre class="source-code">
            x = F.relu(x)</pre><pre class="source-code">
            output = torch.softmax(self.output(x),</pre><pre class="source-code">
                dim=1)</pre><pre class="source-code">
            return output</pre></li>
<li>We<a id="_idIndexMarker367"/> can now instantiate the model with the right input shape of 784 (28x28 pixels), and check the forward propagation works properly on a given <span class="No-Break">random tensor:</span><pre class="source-code">
# Instantiate the model</pre><pre class="source-code">
net = Net(784)</pre><pre class="source-code">
# Generate randomly one random 28x28 image as a 784 values tensor</pre><pre class="source-code">
random_data = torch.rand((1, 784))</pre><pre class="source-code">
result = net(random_data)</pre><pre class="source-code">
print('Resulting output tensor:', result)</pre><pre class="source-code">
print('Sum of the output tensor:', result.sum())</pre></li>
</ol>
<p>Here is <span class="No-Break">the output:</span></p>
<pre class="source-code">
<strong class="bold">Resulting output tensor: tensor([[0.0918, 0.0960, 0.0924, 0.0945, 0.0931, 0.0745, 0.1081, 0.1166, 0.1238,              0.1092]], grad_fn=&lt;SoftmaxBackward0&gt;) Sum of the output tensor: tensor(1.0000, grad_fn=&lt;SumBackward0&gt;)</strong></pre>
<p class="callout-heading">Tip</p>
<p class="callout">Note the output is a tensor of 10 values, with a sum <span class="No-Break">of 1.</span></p>
<ol>
<li value="6">Define the loss function as the cross-entropy loss, available as <strong class="source-inline">nn.CrossEntropyLoss()</strong> in PyTorch, and the optimizer <span class="No-Break">as Adam:</span><pre class="source-code">
criterion = nn.CrossEntropyLoss()</pre><pre class="source-code">
optimizer = torch.optim.Adam(net.parameters(),</pre><pre class="source-code">
    lr=0.001)</pre></li>
<li>Before<a id="_idIndexMarker368"/> training, we implement an <strong class="source-inline">epoch_step</strong> helper function that works for both the train and test sets, allowing us to loop over all the data, compute the loss and the accuracy, and train the model for the <span class="No-Break">training set:</span><pre class="source-code">
def epoch_step(net, dataloader, training_set: bool):</pre><pre class="source-code">
    running_loss = 0.</pre><pre class="source-code">
    Correct = 0.</pre><pre class="source-code">
    For i, data in enumerate(dataloader, 0):</pre><pre class="source-code">
        # Get the inputs: data is a list of [inputs, labels]</pre><pre class="source-code">
        inputs, labels = data</pre><pre class="source-code">
        if training_set:</pre><pre class="source-code">
            # Zero the parameter gradients</pre><pre class="source-code">
            optimizer.zero_grad()</pre><pre class="source-code">
            # Forward + backward + optimize</pre><pre class="source-code">
            outputs = net(inputs)</pre><pre class="source-code">
            loss = criterion(outputs, labels)</pre><pre class="source-code">
            if training_set:</pre><pre class="source-code">
                loss.backward()</pre><pre class="source-code">
                optimizer.step()</pre><pre class="source-code">
            # Add correct predictions for this batch</pre><pre class="source-code">
            correct += (outputs.argmax(</pre><pre class="source-code">
                dim=1) == labels).float().sum()</pre><pre class="source-code">
            # Compute loss for this batch</pre><pre class="source-code">
            running_loss += loss.item()</pre><pre class="source-code">
    return running_loss, correct</pre></li>
<li>We can <a id="_idIndexMarker369"/>now train the neural network on 20 epochs. For each epoch, we also compute <span class="No-Break">the following:</span><ul><li>The loss for both train and <span class="No-Break">test sets</span></li><li>The accuracy for both train and <span class="No-Break">test sets</span></li></ul></li>
</ol>
<p>As for the previous recipe, before training, the model is switched to train mode with <strong class="source-inline">model.train()</strong>, while before evaluating on the test set, it is switched to <strong class="source-inline">eval</strong> model <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">model.eval()</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
# Create empty lists to store the losses and accuracies
train_losses = []
test_losses = []
train_accuracy = []
test_accuracy = []
# Loop over the dataset 20 times for 20 epochs
for epoch in range(20):
    ## Train the model on the training set
    net.train()
    running_train_loss, correct = epoch_step(net,
        dataloader=train_dataloader,training_set=True)
    # Compute and store loss and accuracy for this epoch
    train_epoch_loss = running_train_loss / len(
        train_dataloader)
    train_losses.append(train_epoch_loss)
    train_epoch_accuracy = correct / len(trainset)
     rain_accuracy.append(train_epoch_accuracy)
    ## Evaluate the model on the test set
    net.eval()
    with torch.no_grad():
        running_test_loss, correct = epoch_step(net,
            dataloader=test_dataloader,training_set=False)
        test_epoch_loss = running_test_loss / len(
            test_dataloader)
        test_losses.append(test_epoch_loss)
        test_epoch_accuracy = correct / len(testset)
        test_accuracy.append(test_epoch_accuracy)
    # Print stats
    print(f'[epoch {epoch + 1}] Training: loss={train_epoch_loss:.3f} accuracy={train_epoch_accuracy:.3f} |\
\t Test: loss={test_epoch_loss:.3f} accuracy={test_epoch_accuracy:.3f}')
print('Finished Training')</pre>
<ol>
<li value="9">We can <a id="_idIndexMarker370"/>plot the loss for both the train and test sets as a function of the epoch, since we stored those values for <span class="No-Break">each epoch:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.xlabel('epoch')plt.ylabel('loss (CE)')</pre><pre class="source-code">
plt.legend()plt.show()</pre></li>
</ol>
<p>Here is <span class="No-Break">the output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer186">
<img alt="Figure 6.11 – Resulting cross-entropy loss as a function of the epoch, for both the train and test sets" height="413" src="image/B19629_06_11.jpg" width="552"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.11 – Resulting cross-entropy loss as a function of the epoch, for both the train and test sets</p>
<p>Since <a id="_idIndexMarker371"/>the loss seems to keep improving on both the <strong class="bold">train</strong> and <strong class="bold">test</strong> sets after 20 epochs, it could be interesting in terms of performance to keep training <span class="No-Break">more epochs.</span></p>
<ol>
<li value="10">It is also possible to do the same with the accuracy score, showing the <span class="No-Break">equivalent results:</span><pre class="source-code">
plt.plot(train_accuracy, label='train')</pre><pre class="source-code">
plt.plot(test_accuracy, label='test')</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('Accuracy')plt.legend()plt.show()</pre></li>
</ol>
<p>Here are <span class="No-Break">the results:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer187">
<img alt="Figure 6.12 – Resulting accuracy as a function of the epoch for the train and test sets" height="413" src="image/B19629_06_12.jpg" width="551"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.12 – Resulting accuracy as a function of the epoch for the train and test sets</p>
<p>At the end, the <a id="_idIndexMarker372"/>accuracy is about 97% on the train set and 96% on the <span class="No-Break">test set.</span></p>
<p>Once the model has been trained, it is, of course, possible to store it so that it can be used on new data directly. There<a id="_idIndexMarker373"/> are several ways to save <span class="No-Break">a model:</span></p>
<ul>
<li><strong class="bold">Saving the state dict</strong>: This<a id="_idIndexMarker374"/> saves only the weights, meaning that on loading, the <strong class="source-inline">net</strong> class must first <span class="No-Break">be instantiated.</span></li>
<li><strong class="bold">Saving the entire model</strong>: This<a id="_idIndexMarker375"/> saves both the weights and the architecture, meaning only the file needs to <span class="No-Break">be loaded.</span></li>
<li><strong class="bold">Saving in torchscript format</strong>: This saves the entire model using a more efficient <a id="_idIndexMarker376"/>representation. This method is more suited for deployment and inference <span class="No-Break">at scale.</span></li>
</ul>
<p>For now, let’s just save the <strong class="source-inline">state</strong> dict, reload it, and then compute inferences on <span class="No-Break">an image:</span></p>
<pre class="source-code">
# Save the model's state dict
torch.save(net.state_dict(), 'path_to_model.pt')
# Instantiate a new model
new_model = Net(784)
# Load the model's weights
new_model.load_state_dict(torch.load('path_to_model.pt'))</pre>
<p>It is now possible to compute inferences using that loaded, already-trained model on a <span class="No-Break">given image:</span></p>
<pre class="source-code">
plt.figure(figsize=(12, 8))
for i in range(6):
    plt.subplot(3, 3, i+1)
    # Compute the predicted number
    pred = new_model(
        testset[i][0].unsqueeze(0)).argmax(axis=1)
    # Display the image and predicted number as title
    plt.imshow(testset[i][0].detach().numpy().reshape(
        28, 28), cmap='gray_r')
    plt.title(f'Prediction: {pred.detach().numpy()}')
    plt.axis('off')</pre>
<p>This<a id="_idIndexMarker377"/> is what <span class="No-Break">we get:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer188">
<img alt="Figure 6.13 – The resulting output with six input images and their predictions from the trained model" height="612" src="image/B19629_06_13.jpg" width="1167"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 6.13 – The resulting output with six input images and their predictions from the trained model</p>
<p>As expected, the loaded model can correctly predict the right number on <span class="No-Break">most images.</span></p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor183"/>There’s more…</h2>
<p>Deep learning is often used in heavily computational tasks requiring a lot of resources. In such cases, the use of a GPU is often a necessity. PyTorch of course allows us to train and infer models on GPUs. Only a few steps are required to do so: declaring a device variable and moving both the model and data to this device. Let’s have a quick look at how to <span class="No-Break">do it.</span></p>
<h3>Choosing the device</h3>
<p>Declaring a<a id="_idIndexMarker378"/> device variable to be the GPU can be done with the following <span class="No-Break">Python code:</span></p>
<pre class="source-code">
device = torch.device(
    "cuda" if torch.cuda.is_available() else "cpu") print(device)</pre>
<p>This line instantiates a <strong class="source-inline">torch.device</strong> object, containing <strong class="source-inline">"cuda"</strong> if CUDA is available, else it contains <strong class="source-inline">"cpu"</strong>. Indeed, if CUDA is not installed, or if there is no GPU on your hardware, the CPU will be used (which is the <span class="No-Break">default behavior).</span></p>
<p>If the GPU has been correctly detected, the output of <strong class="source-inline">print(device)</strong> is <strong class="source-inline">"cuda"</strong>. Otherwise, the output <span class="No-Break">is </span><span class="No-Break"><strong class="source-inline">"cpu"</strong></span><span class="No-Break">.</span></p>
<h3>Moving the model and data to the GPU</h3>
<p>Once the<a id="_idIndexMarker379"/> device is correctly set <a id="_idIndexMarker380"/>to the GPU, both the model and data have to be moved to the GPU memory. To do so, you only need to call the<strong class="source-inline">.to(device)</strong> method on both the model and the data. For example, the training and evaluation code that we used in this recipe would become <span class="No-Break">the following:</span></p>
<pre class="source-code">
train_losses = []
test_losses = []
train_accuracy = []
test_accuracy = []
# Move the model to the GPU
net = net.to(device)
for epoch in range(20):
    running_train_loss = 0.
    correct = 0.
    net.train()
    for i, data in enumerate(train_dataloader, 0):
        inputs, labels = data
        # Move the data to the device
        inputs = inputs.to(device)
        labels = labels.to(device)
    running_test_loss = 0.
    correct = 0.
    net.eval()
    with torch.no_grad():
        for i, data in enumerate(test_dataloader, 0):
            inputs, labels = data
            # Move the data to the device
            inputs = inputs.to(device)
            labels = labels.to(device)
print('Finished Training')</pre>
<p>At the beginning, the model is moved to the GPU device once with <strong class="source-inline">net = </strong><span class="No-Break"><strong class="source-inline">net.to(device)</strong></span><span class="No-Break">.</span></p>
<p>At <a id="_idIndexMarker381"/>each iteration loop for <a id="_idIndexMarker382"/>both the training and evaluation, the inputs and labels tensors are moved to the device with <strong class="source-inline">tensor = </strong><span class="No-Break"><strong class="source-inline">tensor.to(device)</strong></span><span class="No-Break">.</span></p>
<p class="callout-heading">Tip</p>
<p class="callout">The data can be either fully loaded on the GPU at loading, or done one batch at a time during training. However, since only rather small datasets can be fully loaded in the GPU memory, we did not present this <span class="No-Break">solution here.</span></p>
<h2 id="_idParaDest-184"><a id="_idTextAnchor184"/>See also</h2>
<ul>
<li>The <a id="_idIndexMarker383"/>official documentation on saving and loading PyTorch <span class="No-Break">models: </span><a href="https://pytorch.org/tutorials/beginner/saving_loading_models.xhtml"><span class="No-Break">https://pytorch.org/tutorials/beginner/saving_loading_models.xhtml</span></a></li>
<li>The reason the images are transformed with such specific values for the MNIST <span class="No-Break">dataset: </span><a href="https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457"><span class="No-Break">https://discuss.pytorch.org/t/normalization-in-the-mnist-example/457</span></a></li>
</ul>
</div>
</div></body></html>