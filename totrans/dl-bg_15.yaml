- en: Convolutional Neural Networks
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces convolutional neural networks, starting with the convolution
    operation and moving forward to ensemble layers of convolutional operations, with
    the aim of learning about filters that operate over datasets. The pooling strategy
    is then introduced to show how such changes can improve the training and performance
    of a model. The chapter concludes by showing how to visualize the filters learned.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this chapter, you will be familiar with the motivation behind
    convolutional neural networks and will know how the convolution operation works
    in one and two dimensions. When you finish this chapter, you will know how to
    implement convolution in layers so as to learn filters through gradient descent.
    Finally, you will have a chance to use many tools that you learned previously,
    including dropout and batch normalization, but now you will know how to use pooling
    as an alternative to reduce the dimensionality of the problem and create levels
    of information abstraction.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter is organized as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to convolutional neural networks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolution in *n*-dimensions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling strategies
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualization of filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to convolutional neural networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Previously, in [Chapter 11](03e9a734-fb56-485d-ae90-66fb98ecd4d1.xhtml), *Deep
    and Wide Neural Networks*, we used a dataset that was very challenging for a general-purpose
    network. However, **convolutional neural networks** (**CNNs**) will prove to be
    more effective, as you will see. CNNs have been around since the late 80s (LeCun,
    Y., et al. (1989)). They have transformed the world of computer vision and audio
    processing (Li, Y. D., et al. (2016)). If you have some kind of AI-based object
    recognition capability in your smartphone, chances are it is using some kind of
    CNN architecture; for example:'
  prefs: []
  type: TYPE_NORMAL
- en: The recognition of objects in images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The recognition of a digital fingerprint
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The recognition of voice commands
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'CNNs are interesting because they have solved some of the most challenging
    problems in computer vision, including beating a human being at an image recognition
    problem called ImageNet (Krizhevsky, A., et al. (2012)*)*. If you can think of
    the most complex object recognition tasks, CNNs should be your first choice for
    experimentation: they will never disappoint!'
  prefs: []
  type: TYPE_NORMAL
- en: The key to the success of CNNs is their unique ability to **encode spatial relationships**.
    If we contrast two different datasets, one about student school records that includes
    current and past grades, attendance, online activity, and so on, and a second
    dataset about images of cats and dogs, if we aim to classify students or cats
    and dogs, the data is different. In one we have student features that have no
    spatial relationships.
  prefs: []
  type: TYPE_NORMAL
- en: For example, if grades are the first feature, attendance does not have to be
    next to it, so their positions can be interchanged and the classification performance
    should not be affected, right? However, with images of cats and dogs, features
    (pixels) of eyes have to be adjacent to a nose or an ear; when you change the
    spatial features and observe an ear in the middle of two eyes (strange), the performance
    of the classifier should be affected because there is usually no cat or dog that
    has an ear in between its eyes. This is the type of spatial relationship that
    CNNs are good at encoding. You can also think of audio or speech processing. You
    know that some sounds must come after others in certain words. If the dataset
    allows for spatial relationships, CNNs have the potential to perform well.
  prefs: []
  type: TYPE_NORMAL
- en: Convolution in n-dimensions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The name of CNNs comes from their signature operation: **convolution**. This
    operation is a mathematical operation that is very common in the signal processing
    area. Let's go ahead and discuss the convolution operation.
  prefs: []
  type: TYPE_NORMAL
- en: 1-dimension
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let''s start with the discrete-time convolution function in one dimension.
    Suppose that we have input data, ![](img/42ad14d7-4801-41de-8544-14d064f7699d.png),
    and some weights, ![](img/e36d108f-f5b2-4205-bfb8-f85808f76dcd.png), we can define
    the discrete-time convolution operation between the two as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e5873a07-cbe5-4f7c-ba38-1088178ab472.png).'
  prefs: []
  type: TYPE_NORMAL
- en: In this equation, the convolution operation is denoted by a ***** symbol. Without
    complicating things too much, we can say that ![](img/8f014c06-f6a8-402e-9f00-0e79616523e0.png)
    is inverted, ![](img/a2dbdcc5-92fb-4538-b80d-949e4edb4024.png), and then shifted, ![](img/3cfe2848-09e8-4fb0-9069-e4c711729d34.png).
    The resulting vector is ![](img/781cc345-61b4-4a24-936a-8aeba31cd79c.png), which
    can be interpreted as the *filtered* version of the input when the filter ![](img/4dea30d8-7115-4bcc-8634-64707a585ecd.png)
    is applied.
  prefs: []
  type: TYPE_NORMAL
- en: If we define the two vectors as follows, ![](img/667d186e-3d70-423b-a31b-05a9832f428a.png) and ![](img/04ea475a-0074-4c06-b144-ed21e87c8651.png),
    then the convolution operation yields ![](img/45c8af6e-5ee5-494c-8f9c-033d812a2478.png).
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.1* shows every single step involved in obtaining this result by
    inverting and shifting the filter and multiplying across the input data:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/54f721f1-4fd2-497f-ae21-6733dc208006.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.1 - Example of a convolution operation involving two vectors
  prefs: []
  type: TYPE_NORMAL
- en: 'In NumPy, we can achieve this by using the `convolve()` method as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, if you think about it, the most "complete" information is when the filter
    fully overlaps with the input data, and that is for ![](img/1c903d14-4aa7-4725-8ac5-12b0e536298d.png).
    In Python, you can get that by using the `''valid''` argument as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This simply gives the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once again, this is only to maximize the *relevant* information because the
    convolution operation is more *uncertain* around the edges of the vector, that
    is, at the beginning and the end where the vectors do not fully overlap. Furthermore,
    for convenience, we could obtain an output vector of the same size as the input
    by using the `''same''` argument as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'This prints the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Here are some practical reasons for each of the three ways of using convolution:'
  prefs: []
  type: TYPE_NORMAL
- en: Use `'valid'` when you need all the *good *information without any of the noise
    caused by the partial overlaps of the filter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use `'same'` when you want to make it easier for the computations to work. This
    will make it easy in the sense that you will have the same dimensions in the input
    and the output.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Otherwise, use nothing to obtain the full analytical solution to the convolution
    operation for any purposes that you want.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Convolution became very popular with the surge of microprocessors specialized
    in multiplying and adding numbers extremely quickly and with the development of
    the **fast Fourier transform** (**FFT**) algorithm. The FFT exploits the mathematical
    property that convolution in the discrete time domain is equivalent to multiplication
    in the Fourier domain and vice versa.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's move on to the next dimension.
  prefs: []
  type: TYPE_NORMAL
- en: 2-dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A two-dimensional convolution is very similar to the one-dimensional convolution.
    However, rather than having a vector, we will have a matrix, and that's why images
    are directly applicable here.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say that we have two matrices: one represents some input data, and the
    other is a filter, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/747990d4-5b12-4a09-a25d-26fb3ac8571f.png).'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can calculate the two-dimensional discrete convolution by inverting (in
    both dimensions) and shifting (also in both dimensions) the filter. The equation
    is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6e04a69a-26bc-4b1a-b54b-76a0d621c719.png)'
  prefs: []
  type: TYPE_IMG
- en: 'This is very similar to the one-dimensional version. The following diagram
    illustrates the first two steps and the last one, to save space and avoid repetition:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e1f06f2e-de3f-47be-8e56-57bf9822d95d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.2 - Two-dimensional discrete convolution example
  prefs: []
  type: TYPE_NORMAL
- en: 'In Python, we can calculate the two-dimensional convolution using SciPy''s
    `convolve2d` method, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This outputs the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The results shown here are the full analytical result. However, similar to
    the one-dimensional implementation, if you only want results that fully overlap,
    you can invoke a `''valid''` result, or if you want a result of the same size
    as the input, you can invoke the `''same''` alternative as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'This would yield the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, let's move on to n-dimensional convolutions.
  prefs: []
  type: TYPE_NORMAL
- en: n-dimensions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Once you have understood convolution in one and two dimensions, you have understood
    the basic concept behind it. However, you might still need to perform convolutions
    in larger dimensions, for example, in multispectral datasets. For this, we can
    simply prepare NumPy arrays of any number of  dimensions and then use SciPy''s `convolve()`
    functionality. Consider the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Here, vectors ![](img/a239161e-8542-499f-bb20-3ad3a3fd22ca.png) are three-dimensional
    arrays, and can be convolved successfully, producing the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: The only difficult part about n-dimensional convolutions could be visualizing
    them or imagining them in your mind. We humans can easily understand one, two,
    and three dimensions, but larger dimensional spaces are tricky to illustrate.
    But remember, if you understand how convolution works in one and two dimensions,
    you can trust that the math works and the algorithms work in any dimensions.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's look at how to *learn* such convolutional filters by defining Keras
    layers and adding them to a model.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Convolution has a number of properties that are very interesting in the field
    of deep learning:'
  prefs: []
  type: TYPE_NORMAL
- en: It can successfully encode and decode spatial properties of the data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be calculated relatively quickly with the latest developments.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be used to address several computer vision problems.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It can be combined with other types of layers for maximum performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Keras has wrapper functions for TensorFlow that involve the most popular dimensions,
    that is, one, two, and three dimensions: `Conv1D`, `Conv2D`, and `Conv3D`. In
    this chapter, we will continue to focus on two-dimensional convolutions, but be
    sure that if you have understood the concept, you can easily go ahead and use
    the others.'
  prefs: []
  type: TYPE_NORMAL
- en: Conv2D
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The two-dimensional convolution method has the following signature: `tensorflow.keras.layers.Conv2D`.
    The most common arguments used in a convolutional layer are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`filters` refers to the number of filters to be learned in this particular
    layer and affects the dimension of the output of the layer.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`kernel_size` refers to the size of the filters; for example, in the case of
    *Figure 12.2*, it would be size (3,3).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`strides=(1, 1)` is new for us. Strides is defined as the size of the steps
    that are taken when the filters are sliding across the input. All the examples
    we have shown so far assume that we follow the original definition of convolution
    and take unit steps. However, in convolutional layers, you can take larger steps,
    which will lead to smaller outputs but also the loss of information.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`padding=''valid''` refers to the way of dealing with the information in the
    edges of the convolution result. Note that the options here are only `''valid''`
    or `''same''`, and that there is no way of obtaining the full analytical result.
    The meaning is the same as we have seen before in this chapter.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`activation=None` gives the option to include an activation function in the
    layer if you need one; for example, `activation=''relu''`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To exemplify this, consider a convolutional layer such as the one shown in
    the following diagram, where the first layer is convolutional (in 2D) with 64
    filters of size 9x9 and a stride of 2, 2 (that is, two in each direction). We
    will explain the rest of the model in the following diagram as we proceed:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0025a4ec-aba0-4f83-9a6c-ce8f45623b47.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.3 - Architecture of a convolutional neural network for CIFAR 10
  prefs: []
  type: TYPE_NORMAL
- en: 'The first convolutional layer in the diagram can be defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'This essentially will create a convolutional layer with the given specifications.
    The print statement will effectively produce the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: If you do the math, each and every single filter out of the 64 will produce
    a 23x23 `'valid'` output, but since a (2,2) stride is being used, an 11.5x11.5
    output should be obtained. However, since we cannot have fractions, TensorFlow
    will round up to 12x12\. Therefore, we end up with the preceding shape as the
    output.
  prefs: []
  type: TYPE_NORMAL
- en: The layer+activation combo
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As mentioned previously, the `Conv2D` class has the ability to include an activation
    function of your choice. This is much appreciated because it will save some lines
    of code for all who want to learn to code efficiently. However, we have to be
    careful not to forget to document somewhere the type of activation used.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 12.3* shows the activation in a separate block. This is a good idea
    to keep track of what activations are used throughout. The most common activation
    function for a convolutional layer is a ReLU, or any of the activations of the
    ReLU family, for example, leaky ReLU and ELU. The next *new *element is a pooling
    layer. Let''s talk about this.'
  prefs: []
  type: TYPE_NORMAL
- en: Pooling strategies
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You will usually find pooling accompanying convolutional layers. Pooling is
    an idea that is intended to reduce the number of computations by reducing the
    dimensionality of the problem. We have a few pooling strategies available to us
    in Keras, but the most important and popular ones are the following two:'
  prefs: []
  type: TYPE_NORMAL
- en: AveragePooling2D
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: MaxPooling2D
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These also exist for other dimensions, such as 1D. However, in order to understand
    pooling, we can simply look at the example in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d6c68d88-284b-460f-abb6-48b69dfe190c.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.4 - Max pooling example in 2D
  prefs: []
  type: TYPE_NORMAL
- en: In the diagram, you can observe how max pooling would look at individual 2x2
    squares moving two spaces at a time, which leads to a 2x2 result. The whole point
    of pooling is to **find a smaller summary of the data** in question. When it comes
    to neural networks, we often look at neurons that are *excited* the most, and
    so it makes sense to look at the maximum values as good representatives of larger
    portions of data. However, remember that you can also look at the average of the
    data (`AveragePooling2D`), which is also good in all senses.
  prefs: []
  type: TYPE_NORMAL
- en: There is a slight difference in time performance in favor of max pooling, but
    this is very small.
  prefs: []
  type: TYPE_NORMAL
- en: 'In Keras, we can implement pooling very easily. In the case of max pooling
    in 2D, for example, we can simply do the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the same output as in *Figure 12.4*:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We can also do the same for average pooling as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Both pooling strategies work perfectly fine in terms of summarizing the data.
    You will be safe in choosing either one.
  prefs: []
  type: TYPE_NORMAL
- en: Now for the big reveal. We will put all of this together in a CNN next.
  prefs: []
  type: TYPE_NORMAL
- en: Convolutional neural network for CIFAR-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We have reached the point where we can actually implement a fully functional
    CNN after looking at the individual pieces: understanding the convolution operation,
    understanding pooling, and understanding how to implement convolutional layers
    and pooling. Now we will be implementing the CNN architecture shown in *Figure
    12.3*.'
  prefs: []
  type: TYPE_NORMAL
- en: Implementation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will be implementing the network in *Figure 12.3* step by step, broken down
    into sub-sections.
  prefs: []
  type: TYPE_NORMAL
- en: Loading data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let''s load the CIFAR-10 dataset as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'This should effectively load the dataset and print its shape, which is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This is very straightforward, but we can go further and verify that the data
    is loaded correctly by loading and plotting the first image of every class in
    the `x_train` set as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce the output shown in the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/3031d060-a19c-492f-91ca-df7a029dffb0.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.5 - Samples of CIFAR-10
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will implement the layers of the network.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Again, recall the model in *Figure 12.3*, and how we can implement it as shown
    here. Everything you are about to see is something we have looked at in this and
    previous chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'We continue adding more convolutional layers like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can compile the model and print a summary as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output a summary of the network that will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: One thing that must be very obvious to you at this point is the number of parameters
    of this network. If you recall from the previous chapter, you will be surprised
    that this network has nearly a quarter of a million parameters, while the wide
    or deep network had a few million parameters. Furthermore, you will see shortly
    that this relatively small network, while still *overparameterized*, is going
    to perform better than the networks in the previous chapter that had more parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Next, let's train the network.
  prefs: []
  type: TYPE_NORMAL
- en: Training the CNN
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: We can train the CNN using the *callbacks* that we studied in [Chapter 11](03e9a734-fb56-485d-ae90-66fb98ecd4d1.xhtml), *Deep
    and Wide Neural Networks*, to stop the network early if there is no progress,
    and to reduce the learning rate to focus the efforts of the gradient descent algorithm
    if it reaches a *plateau*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will train it as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of this will vary from computer to computer. For example, it may
    take fewer or more epochs, or the gradient might take a different direction if
    the mini-batches (which are selected at random) contain several edge cases. However,
    for the most part, you should get a similar result to this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: At this point, when the training is finished, you can get an estimate of the
    accuracy of 83.15%. Be careful, this is not a **balanced** accuracy. For that,
    we will take a look at the **Balanced Error Rate** (**BER**) metric in the next
    section. But before we do that, we can look at the training curve to see how the
    loss was minimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will produce what we want:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'This gives the plot shown in *Figure 12.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/cbf15643-e4bd-451a-86c6-1d71011cb5c4.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.6 - Loss minimization for a CNN on CIFAR-10
  prefs: []
  type: TYPE_NORMAL
- en: From this diagram, you can appreciate the bumps that the learning curve has,
    particularly visible on the training set curve, which are due to the reduction
    in the learning rate through the callback function, `ReduceLROnPlateau`. The training
    stops after the loss no longer improves on the test set, thanks to the `EarlyStopping`
    callback.
  prefs: []
  type: TYPE_NORMAL
- en: Results
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s look at objective, numerical results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'This will give us the following numerical results, which we can compare with
    the results from the previous chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Accuracy for specific classes can be as high as 87%, while the lowest accuracy
    is 66%. This is much better than the previous models in the previous chapter.
    The BER is of 0.2288, which can all be interpreted as a balanced accuracy of 77.12%.
    This matches the accuracy reported in the test set during training, which indicates
    that the model was trained properly. For comparison purposes, the following diagram
    shows a visual representation of the confusion matrix:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/db92150e-3920-4e15-b975-40e7e6f287f7.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.7 - Confusion matrix for a CNN trained over CIFAR-10
  prefs: []
  type: TYPE_NORMAL
- en: It might be a bit clearer from the visual confusion matrix that classes 3 and
    5 can be confused between themselves more than other classes. Classes 3 and 5
    correspond to cats and dogs, respectively.
  prefs: []
  type: TYPE_NORMAL
- en: 'That''s it. As you can see, this is a nice result already, but you could perform
    more experiments on your own. You can edit and add more convolutional layers to
    your model and make it better. If you are curious, there are other larger CNNs
    that have been very successful. Here are the two most famous ones:'
  prefs: []
  type: TYPE_NORMAL
- en: 'VGG-19: This contains 12 convolutional layers and 3 dense layers (Simonyan,
    K., et al. (2014)).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'ResNet: This contains 110 convolutional layers and 1 dense layer (He, K., et
    al. (2016)). This particular configuration can achieve an error rate as low as
    6.61% (±0.16%) on CIFAR-10.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's discuss next how to visualize the filters learned.
  prefs: []
  type: TYPE_NORMAL
- en: Visualization of filters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This last piece in this chapter deals with the visualization of the learned
    filters. This may be useful to you if you want to do research on what the network
    is learning. It may help with the *explainability *of the network. However, note
    that the deeper the network is, the more complicated it gets to understand it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code will help you visualize the filters of the first convolutional
    layer of the network:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'This code depends heavily on knowing which layer you want to visualize, the
    number of filters you want to visualize, and the size of the filters themselves.
    In this case, we want to visualize the first convolutional layer. It has 64 filters
    (displayed in an 8x8 grid), and each filter is 9x9x3 because the input is color
    images. *Figure 12.8* shows the resulting plot of the code shown previously:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f37fe0c8-877d-4ae1-8d94-b468a47f6295.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 12.8 - Filters learned in the first convolutional layer
  prefs: []
  type: TYPE_NORMAL
- en: If you are an expert in image processing, you may recognize some of these patterns
    as they resemble Gabor filters (Jain, A. K., et al. (1991)). Some of these filters
    are designed to look for edges, textures, or specific shapes. The literature suggests
    that in convolutional networks, deeper layers usually encode highly complex information,
    while the first layers are used to detect features such as edges.
  prefs: []
  type: TYPE_NORMAL
- en: Feel free to go ahead and try to display another layer by making the necessary
    modifications.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This intermediate chapter showed how to create CNNs. You learned about the convolution
    operation, which is the fundamental concept behind them. You also learned how
    to create convolutional layers and aggregated pooling strategies. You designed
    a network to learn filters to recognize objects based on CIFAR-10 and learned
    how to display the learned filters.
  prefs: []
  type: TYPE_NORMAL
- en: At this point, you should feel confident explaining the motivation behind convolutional
    neural networks rooted in computer vision and signal processing. You should feel
    comfortable coding the convolution operation in one and two dimensions using NumPy,
    SciPy, and Keras/TensorFlow. Furthermore, you should feel confident implementing
    convolution operations in layers and learning filters through gradient descent
    techniques. If you are asked to show what the network has learned, you should
    feel prepared to implement a simple visualization method to display the filters
    learned.
  prefs: []
  type: TYPE_NORMAL
- en: CNNs are great at encoding highly correlated spatial information, such as images,
    audio, or text. However, there is an interesting type of network that is meant
    to encode information that is sequential in nature. [Chapter 13](a6e892c5-e890-4c0a-ad92-c5442328a64a.xhtml), *Recurrent
    Neural Networks*, will present the most fundamental concepts of recurrent networks,
    leading to long short-term memory models. We will explore multiple variants of
    sequential models with applications in image classification and natural language
    processing.
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**What data summarization strategy discussed in this chapter can reduce the
    dimensionality of a convolutional model?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Pooling.
  prefs: []
  type: TYPE_NORMAL
- en: '**Does adding more convolutional layers make the network better? **'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Not always. It has been shown that more layers has a positive effect on networks,
    but there are certain occasions when there is no gain. You should determine the
    number of layers, filter sizes, and pooling experimentally.
  prefs: []
  type: TYPE_NORMAL
- en: '**What other applications are there for CNNs?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Audio processing and classification; image denoising; image super-resolution;
    text summarization and other text-processing and classification tasks; the encryption
    of data.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: LeCun, Y., Boser, B., Denker, J. S., Henderson, D., Howard, R. E., Hubbard,
    W., and Jackel, L. D. (1989). *Backpropagation applied to handwritten zip code
    recognition*. *Neural computation*, 1(4), 541-551.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Li, Y. D., Hao, Z. B., and Lei, H. (2016). *Survey of convolutional neural networks*.
    *Journal of Computer Applications*, 36(9), 2508-2515.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Krizhevsky, A., Sutskever, I., and Hinton, G. E. (2012). *Imagenet classification
    with deep convolutional neural networks*. In *Advances in neural information processing
    systems* (pp. 1097-1105).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan, K., and Zisserman, A. (2014). *Very deep convolutional networks for
    large-scale image recognition*. arXiv preprint arXiv:1409.1556.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He, K., Zhang, X., Ren, S., and Sun, J. (2016). *Deep residual learning for
    image recognition*. In *Proceedings of the IEEE conference on computer vision
    and pattern recognition* (pp. 770-778).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Jain, A. K., and Farrokhnia, F. (1991). *Unsupervised texture segmentation using
    Gabor filters*. *Pattern recognition*, 24(12), 1167-1186.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
