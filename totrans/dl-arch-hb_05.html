<html><head></head><body>
<div id="_idContainer064">
<h1 class="chapter-number" id="_idParaDest-85"><a id="_idTextAnchor085"/><span class="koboSpan" id="kobo.1.1">5</span></h1>
<h1 id="_idParaDest-86"><a id="_idTextAnchor086"/><span class="koboSpan" id="kobo.2.1">Understanding Autoencoders</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Autoencoders are a type of model that was built mainly to accomplish </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">representation learning</span></strong><span class="koboSpan" id="kobo.5.1">. </span><span class="koboSpan" id="kobo.5.2">Representation learning is a type of deep</span><a id="_idIndexMarker349"/><span class="koboSpan" id="kobo.6.1"> learning task that focuses on generating a compact and representative feature to represent any single data sample, be it image, text, audio, video, or multimodal data. </span><span class="koboSpan" id="kobo.6.2">After going through some form of representation learning, a model will be able to map inputs into more representable features, which can be used to differentiate itself from other sample inputs. </span><span class="koboSpan" id="kobo.6.3">The representation obtained will exist in a latent space where different input samples</span><a id="_idIndexMarker350"/><span class="koboSpan" id="kobo.7.1"> will co-exist together. </span><span class="koboSpan" id="kobo.7.2">These representations are also known as </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">embeddings</span></strong><span class="koboSpan" id="kobo.9.1">. </span><span class="koboSpan" id="kobo.9.2">The applications of autoencoders will be tied closely to representation learning applications, and some applications include generating predictive features for other subsequent supervised learning objectives, comparing and contrasting samples in the wild, and performing effective </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">sample recognition.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">Note that autoencoders are not the only way to execute representation learning. </span><span class="koboSpan" id="kobo.11.2">The topic of representation learning will be discussed further in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.12.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.13.1">, </span><em class="italic"><span class="koboSpan" id="kobo.14.1">Exploring Supervised Deep Learning</span></em><span class="koboSpan" id="kobo.15.1">, and </span><a href="B18187_09.xhtml#_idTextAnchor149"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.16.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.17.1">, </span><em class="italic"><span class="koboSpan" id="kobo.18.1">Exploring Unsupervised </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.19.1">Deep Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.21.1">Now, we know that an autoencoder learns to generate distinctive representations or, in other words, embeddings. </span><span class="koboSpan" id="kobo.21.2">But what’s the architecture like? </span><span class="koboSpan" id="kobo.21.3">Let’s discover a standard form of the architecture and then discover a couple more </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">useful advancements.</span></span></p>
<p><span class="koboSpan" id="kobo.23.1">In this chapter, the following topics will </span><span class="No-Break"><span class="koboSpan" id="kobo.24.1">be covered:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.25.1">Decoding the </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">standard autoencoder</span></span></li>
<li><span class="koboSpan" id="kobo.27.1">Exploring </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">autoencoder variations</span></span></li>
<li><span class="koboSpan" id="kobo.29.1">Building a </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">CNN autoencoder</span></span></li>
</ul>
<h1 id="_idParaDest-87"><a id="_idTextAnchor087"/><span class="koboSpan" id="kobo.31.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.32.1">This chapter includes some practical implementations in the </span><strong class="bold"><span class="koboSpan" id="kobo.33.1">Python</span></strong><span class="koboSpan" id="kobo.34.1"> programming language. </span><span class="koboSpan" id="kobo.34.2">To complete it, you will need to have a computer with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.35.1">libraries installed:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.36.1">pandas</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.37.1">Matplotlib</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.38.1">Seaborn</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.39.1">scikit-learn</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.40.1">NumPy</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.41.1">Keras</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.42.1">PyTorch</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.43.1">The code files are available on </span><span class="No-Break"><span class="koboSpan" id="kobo.44.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_5"><span class="No-Break"><span class="koboSpan" id="kobo.45.1">https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_5</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.46.1">.</span></span></p>
<h1 id="_idParaDest-88"><a id="_idTextAnchor088"/><span class="koboSpan" id="kobo.47.1">Decoding the standard autoencoder</span></h1>
<p><span class="koboSpan" id="kobo.48.1">Autoencoders are more of a concept</span><a id="_idIndexMarker351"/><span class="koboSpan" id="kobo.49.1"> than an actual neural network architecture. </span><span class="koboSpan" id="kobo.49.2">This is due to the fact that they can be based on different base neural network layers. </span><span class="koboSpan" id="kobo.49.3">When dealing with images, you build CNN autoencoders, and when dealing with text, you might want to build RNN autoencoders. </span><span class="koboSpan" id="kobo.49.4">When dealing with multimodal datasets with images, text, audio, numerical, and categorical data, well, you </span><a id="_idIndexMarker352"/><span class="koboSpan" id="kobo.50.1">use a combination</span><a id="_idIndexMarker353"/><span class="koboSpan" id="kobo.51.1"> of different layers as a base. </span><span class="koboSpan" id="kobo.51.2">Autoencoders</span><a id="_idIndexMarker354"/><span class="koboSpan" id="kobo.52.1"> are mainly based on three components, called the </span><strong class="bold"><span class="koboSpan" id="kobo.53.1">encoder</span></strong><span class="koboSpan" id="kobo.54.1">, the </span><strong class="bold"><span class="koboSpan" id="kobo.55.1">bottleneck layers</span></strong><span class="koboSpan" id="kobo.56.1">, and the </span><strong class="bold"><span class="koboSpan" id="kobo.57.1">decoder</span></strong><span class="koboSpan" id="kobo.58.1">. </span><span class="koboSpan" id="kobo.58.2">This is illustrated in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.59.1">Figure 5</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.60.1">.1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.61.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer060">
<span class="koboSpan" id="kobo.62.1"><img alt="Figure 5.1 – The autoencoder concept" src="image/B18187_05_001.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.63.1">Figure 5.1 – The autoencoder concept</span></p>
<p><span class="koboSpan" id="kobo.64.1">The encoder for a standard autoencoder typically takes in high-dimensional data and compresses it to an arbitrary scale smaller</span><a id="_idIndexMarker355"/><span class="koboSpan" id="kobo.65.1"> than the original data dimensions, which will result in what is known as a </span><strong class="bold"><span class="koboSpan" id="kobo.66.1">bottleneck representation</span></strong><span class="koboSpan" id="kobo.67.1">, where it ties itself to the bottleneck, signifying a compact representation without any useless information. </span><span class="koboSpan" id="kobo.67.2">The bottleneck component then gets passed into the decoder, where it will expand the dimensionality using the exact opposite of the scale used by the encoder, resulting in output with the same</span><a id="_idIndexMarker356"/><span class="koboSpan" id="kobo.68.1"> dimensions as the </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">input data.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.70.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.71.1">Encoder and decoder structures are not exclusive to autoencoders but are also utilized in other architectures, such </span><span class="No-Break"><span class="koboSpan" id="kobo.72.1">as transformers.</span></span></p>
<p><span class="koboSpan" id="kobo.73.1">The difference is the bottleneck component that holds the representative features. </span><span class="koboSpan" id="kobo.73.2">It is commonly compressed and in smaller dimensions, but sometimes, it can be made larger to hold more representative features for </span><span class="No-Break"><span class="koboSpan" id="kobo.74.1">predictive power.</span></span></p>
<p><span class="koboSpan" id="kobo.75.1">Autoencoders, in general, are trained to</span><a id="_idIndexMarker357"/><span class="koboSpan" id="kobo.76.1"> reconstruct input data. </span><span class="koboSpan" id="kobo.76.2">The training process of the autoencoder model involves comparing the distance between the generated output data and the input data. </span><span class="koboSpan" id="kobo.76.3">After being optimized to generate the input data, when the model is capable of reconstructing the original input data completely, it can be said that the bottleneck has a more compact and summarized representation of the input data than the original input data itself. </span><span class="koboSpan" id="kobo.76.4">The compact representation can then be used subsequently to achieve other tasks, such as sample recognition, or can even be used generally to save space, by storing the smaller bottleneck feature instead of the original large input data. </span><span class="koboSpan" id="kobo.76.5">The encoder and decoder are not constrained to a single layer and can be defined with multiple layers. </span><span class="koboSpan" id="kobo.76.6">However, the standard autoencoder</span><a id="_idIndexMarker358"/><span class="koboSpan" id="kobo.77.1"> only has</span><a id="_idIndexMarker359"/><span class="koboSpan" id="kobo.78.1"> a single bottleneck</span><a id="_idIndexMarker360"/><span class="koboSpan" id="kobo.79.1"> feature. </span><span class="koboSpan" id="kobo.79.2">This is also known as </span><strong class="bold"><span class="koboSpan" id="kobo.80.1">code</span></strong><span class="koboSpan" id="kobo.81.1">, or a </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.82.1">latent feature</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.84.1">Now, let’s explore the different variations </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">of autoencoders.</span></span></p>
<h1 id="_idParaDest-89"><a id="_idTextAnchor089"/><span class="koboSpan" id="kobo.86.1">Exploring autoencoder variations</span></h1>
<p><span class="koboSpan" id="kobo.87.1">For tabular data, the network</span><a id="_idIndexMarker361"/><span class="koboSpan" id="kobo.88.1"> structure can be pretty straightforward. </span><span class="koboSpan" id="kobo.88.2">It simply uses an MLP with multiple fully connected layers that gradually shrink the number of features for the encoder, and multiple fully connected layers that gradually increase the data outputs to the same dimension and size as the input for </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">the decoder.</span></span></p>
<p><span class="koboSpan" id="kobo.90.1">For time-series or sequential data, RNN-based autoencoders can be used. </span><span class="koboSpan" id="kobo.90.2">One of the most cited research projects about RNN-based autoencoders is a version where LSTM-based encoders and decoders are used. </span><span class="koboSpan" id="kobo.90.3">The research paper is called </span><em class="italic"><span class="koboSpan" id="kobo.91.1">Sequence to Sequence Learning with Neural Networks</span></em><span class="koboSpan" id="kobo.92.1"> by Ilya Sutskever, Oriol Vinyals, and Quoc V. </span><span class="koboSpan" id="kobo.92.2">Le (</span><a href="https://arxiv.org/abs/1409.3215"><span class="koboSpan" id="kobo.93.1">https://arxiv.org/abs/1409.3215</span></a><span class="koboSpan" id="kobo.94.1">). </span><span class="koboSpan" id="kobo.94.2">Instead of stacking encoder LSTMs and decoder LSTMs, using the hidden state output sequence of each of the LSTM cells vertically, the decoder layer sequentially continues the sequential flow of the encoder LSTM and outputs the reconstructed input in reversed order. </span><span class="koboSpan" id="kobo.94.3">An additional decoder LSTM layer was also used to concurrently optimize, to predict future sequences. </span><span class="koboSpan" id="kobo.94.4">This structure is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.95.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.96.1">.2</span></em><span class="koboSpan" id="kobo.97.1">. </span><span class="koboSpan" id="kobo.97.2">Note that it is also possible to adapt this to the video image modality using raw flattened</span><a id="_idIndexMarker362"/><span class="koboSpan" id="kobo.98.1"> image pixels</span><a id="_idIndexMarker363"/><span class="koboSpan" id="kobo.99.1"> as input. </span><span class="koboSpan" id="kobo.99.2">The architecture is also </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.101.1">seq2seq</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer061">
<span class="koboSpan" id="kobo.103.1"><img alt="Figure 5.2 – An LSTM-based autoencoder structure" src="image/B18187_05_002.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.104.1">Figure 5.2 – An LSTM-based autoencoder structure</span></p>
<p><span class="koboSpan" id="kobo.105.1">The hidden state outputs of the encoder LSTM</span><a id="_idIndexMarker364"/><span class="koboSpan" id="kobo.106.1"> can be considered to be the latent feature of the LSTM autoencoder. </span><span class="koboSpan" id="kobo.106.2">Transformers, a new architecture that can also deal with sequential data, which will be introduced later in this chapter, have some variations that can also be considered a kind of autoencoder – some transformers can be autoencoders, but not all transformers </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">are autoencoders.</span></span></p>
<p><span class="koboSpan" id="kobo.108.1">For image data, by using convolutional layers, we can scale down the features gradually with multiple convolutional and pooling layers, until the stage where a global pooling layer is applied and the data becomes a 1-dimensional feature. </span><span class="koboSpan" id="kobo.108.2">This represents the encoder of the autoencoder generating the bottleneck feature. </span><span class="koboSpan" id="kobo.108.3">This workflow is the same as the one we discussed in the previous section on CNN. </span><span class="koboSpan" id="kobo.108.4">However, for the decoder, to scale up the 1-dimensional</span><a id="_idIndexMarker365"/><span class="koboSpan" id="kobo.109.1"> pooled feature into 2-dimensional image-like data again, a special form of convolution is needed, called the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.110.1">transpose convolution</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.112.1">The variations mentioned</span><a id="_idIndexMarker366"/><span class="koboSpan" id="kobo.113.1"> in this topic are all about using the standard autoencoder structures but implementing them using different neural network types. </span><span class="koboSpan" id="kobo.113.2">There are also two additional variations of the autoencoder, where one variation is based on data input manipulation, and another variation is based on an actual modification of the autoencoder structure to achieve the data </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">generation goal.</span></span></p>
<p><span class="koboSpan" id="kobo.115.1">For the variation used for data manipulation, the idea is to add noise to the input data during training and maintain the original input data without the added noise, to be used as the target to predictively</span><a id="_idIndexMarker367"/><span class="koboSpan" id="kobo.116.1"> reconstruct the data. </span><span class="koboSpan" id="kobo.116.2">This variation of the autoencoder is called a </span><strong class="bold"><span class="koboSpan" id="kobo.117.1">denoising autoencoder</span></strong><span class="koboSpan" id="kobo.118.1"> because its goal is primarily to </span><em class="italic"><span class="koboSpan" id="kobo.119.1">denoise</span></em><span class="koboSpan" id="kobo.120.1"> the data. </span><span class="koboSpan" id="kobo.120.2">Since the goal has shifted from compressing the data to denoising the data, the bottleneck features are not restricted to being small in size. </span><span class="koboSpan" id="kobo.120.3">The features that are utilized afterward are not constrained to just the single bottleneck feature but, instead, can be the features from multiple intermediate layers in the network, or just simply the denoised reconstructed output. </span><span class="koboSpan" id="kobo.120.4">This method takes advantage of the innate capability of neural networks to perform automated feature engineering. </span><span class="koboSpan" id="kobo.120.5">The most notable usage of a denoising autoencoder is in the first-place solution of a Kaggle competition based on tabular data, hosted by Porto Seguro, an insurance company, where multiple features from intermediate layers were fed into a separate MLP to predict whether a driver will file an insurance claim in the </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">future (</span></span><a href="https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629"><span class="No-Break"><span class="koboSpan" id="kobo.122.1">https://www.kaggle.com/competitions/porto-seguro-safe-driver-prediction/discussion/44629</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.123.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.124.1">For the variation that uses a modification of the autoencoder structure, the idea is to produce two bottleneck feature vectors that represent a list of standard deviation values and mean values so that different bottleneck feature values can be sampled, based on the mean and standard deviation values. </span><span class="koboSpan" id="kobo.124.2">These sampled bottleneck feature values can then be passed into the decoder</span><a id="_idIndexMarker368"/><span class="koboSpan" id="kobo.125.1"> to generate new random data outputs. </span><span class="koboSpan" id="kobo.125.2">This variation of the autoencoder is called a </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.126.1">variational autoencoder</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.127.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.128.1">Now that we have covered a good overview</span><a id="_idIndexMarker369"/><span class="koboSpan" id="kobo.129.1"> of autoencoder variations, let’s dig further into CNN autoencoders and build a CNN autoencoder using deep </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">learning libraries.</span></span></p>
<h1 id="_idParaDest-90"><a id="_idTextAnchor090"/><span class="koboSpan" id="kobo.131.1">Building a CNN autoencoder</span></h1>
<p><span class="koboSpan" id="kobo.132.1">Let’s start by going through what a </span><strong class="bold"><span class="koboSpan" id="kobo.133.1">transpose convolution</span></strong><span class="koboSpan" id="kobo.134.1"> is. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.135.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.136.1">.3</span></em><span class="koboSpan" id="kobo.137.1"> shows an example transpose convolution</span><a id="_idIndexMarker370"/><span class="koboSpan" id="kobo.138.1"> operation on a 2x2 sized input with a 2x2 sized </span><a id="_idIndexMarker371"/><span class="koboSpan" id="kobo.139.1">convolutional filter, with a stride </span><span class="No-Break"><span class="koboSpan" id="kobo.140.1">of 1.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer062">
<span class="koboSpan" id="kobo.141.1"><img alt="Figure 5.3 – A transposed convolutional filter operation" src="image/B18187_05_003.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.142.1">Figure 5.3 – A transposed convolutional filter operation</span></p>
<p><span class="koboSpan" id="kobo.143.1">In </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.144.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.145.1">.3</span></em><span class="koboSpan" id="kobo.146.1">, note that each of the 2x2 input data is marked with a number from </span><strong class="bold"><span class="koboSpan" id="kobo.147.1">1</span></strong><span class="koboSpan" id="kobo.148.1"> to </span><strong class="bold"><span class="koboSpan" id="kobo.149.1">4</span></strong><span class="koboSpan" id="kobo.150.1">. </span><span class="koboSpan" id="kobo.150.2">These numbers are used to map the output results, presented as 3x3 outputs. </span><span class="koboSpan" id="kobo.150.3">The convolutional kernel applies each of its weights individually to every value in the input data in a sliding window manner, and the outputs from the four convolutional operations are presented in the bottom part of the figure. </span><span class="koboSpan" id="kobo.150.4">After the operation is done, each of the outputs will be elementwise added to form the final output and subjected to a bias. </span><span class="koboSpan" id="kobo.150.5">This example process depicts how a 2x2 input can be scaled up to a 3x3 data size without relying completely </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">on padding.</span></span></p>
<p><span class="koboSpan" id="kobo.152.1">Let’s implement a convolutional </span><a id="_idIndexMarker372"/><span class="koboSpan" id="kobo.153.1">autoencoder model in </span><strong class="source-inline"><span class="koboSpan" id="kobo.154.1">Pytorch</span></strong><span class="koboSpan" id="kobo.155.1"> below and train it on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.156.1">Fashion MNIST</span></strong><span class="koboSpan" id="kobo.157.1"> image dataset, an image dataset comprising fashion items such as shoes, bags, </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">and clothes:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.159.1">Let’s start by importing the </span><span class="No-Break"><span class="koboSpan" id="kobo.160.1">necessary libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.161.1">
import torch.nn as nn
import torchvision
from PIL import Image</span></pre></li> <li><span class="koboSpan" id="kobo.162.1">Next, we will define the overall convolutional </span><span class="No-Break"><span class="koboSpan" id="kobo.163.1">autoencoder structure:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.164.1">
class ConvAutoencoder(nn.Module):
  def __init__(self):
    super(ConvAutoencoder, self).__init__()
    self.encoder = None
    self.decoder = None
  def forward(self, x):
    bottleneck_feature = self.encoder(x)
    reconstructed_x = self.decoder(
      bottleneck_feature
    )
    return reconstructed_x</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.165.1">The code presented here is a convolutional autoencoder structure in </span><strong class="source-inline"><span class="koboSpan" id="kobo.166.1">PyTorch</span></strong><span class="koboSpan" id="kobo.167.1"> with an encoder and decoder variable placeholder. </span><span class="koboSpan" id="kobo.167.2">The encoder is responsible for taking in an image and reducing its dimensionality until it has a single dimension with a small representation footprint – the bottleneck feature. </span><span class="koboSpan" id="kobo.167.3">The decoder will then take in the bottleneck feature and produce a feature map of the same size as the original input image. </span><span class="koboSpan" id="kobo.167.4">The encoder and decoder will be defined in the next </span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">two steps.</span></span></p></li> <li><span class="koboSpan" id="kobo.169.1">The encoder will be designed</span><a id="_idIndexMarker373"/><span class="koboSpan" id="kobo.170.1"> to take in a grayscale image (one channel) of size 28x28. </span><span class="koboSpan" id="kobo.170.2">This image dimension is the default size of the Fashion MNIST image dataset. </span><span class="koboSpan" id="kobo.170.3">The following logic shows the code to define the encoder, replacing the placeholder defined in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.171.1">step 2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.172.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.173.1">
self.encoder = nn.Sequential(
      nn.Conv2d(1, 16, 4),
    nn.ReLU(),
    nn.MaxPool2d(2, 2),
    nn.Conv2d(16, 4, 4),
    nn.ReLU(),
    nn.AvgPool2d(9),
)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.174.1">The defined encoder has two convolutional layers, each followed by the non-linear activation, </span><strong class="source-inline"><span class="koboSpan" id="kobo.175.1">ReLU</span></strong><span class="koboSpan" id="kobo.176.1">, and a pooling layer. </span><span class="koboSpan" id="kobo.176.2">The filter sizes of the convolutional layers are </span><strong class="source-inline"><span class="koboSpan" id="kobo.177.1">16</span></strong><span class="koboSpan" id="kobo.178.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.179.1">4</span></strong><span class="koboSpan" id="kobo.180.1"> The second pooling layer is a global average pooling layer meant to reduce the 4x9x9 feature map to 4x1x1, where each channel will have only one value to represent itself. </span><span class="koboSpan" id="kobo.180.2">This means that the encoder will squeeze the dimensionality of the original 28x28 image, which adds up to around 784 pixels for only four features, which is a 99.4% </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">compression rate!</span></span></p></li> <li><span class="koboSpan" id="kobo.182.1">The decoder will then take these four features per image and produce an output feature map of 28x28 again, reproducing the original image size. </span><span class="koboSpan" id="kobo.182.2">The entire model has no padding applied. </span><span class="koboSpan" id="kobo.182.3">The decoder will be defined as follows, replacing the placeholder decoder defined in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.183.1">step 2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.184.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.185.1">
self.decoder = nn.Sequential(
  nn.ConvTranspose2d(4, 16, 5, stride=2),
  nn.ReLU(),
  nn.ConvTranspose2d(16, 4, 5, stride=2),
  nn.ReLU(),
  nn.ConvTranspose2d(4, 1, 4, stride=2),
  nn.Sigmoid(),
)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.186.1">Three convolutional transpose </span><a id="_idIndexMarker374"/><span class="koboSpan" id="kobo.187.1">layers are used here. </span><span class="koboSpan" id="kobo.187.2">Each convolutional layer is followed by a non-linear activation layer, where the first two layers used </span><strong class="source-inline"><span class="koboSpan" id="kobo.188.1">ReLU</span></strong><span class="koboSpan" id="kobo.189.1"> (being the standard non-linear activation) and the last layer used </span><strong class="source-inline"><span class="koboSpan" id="kobo.190.1">sigmoid</span></strong><span class="koboSpan" id="kobo.191.1">. </span><strong class="source-inline"><span class="koboSpan" id="kobo.192.1">sigmoid</span></strong><span class="koboSpan" id="kobo.193.1"> is used here, as the </span><strong class="source-inline"><span class="koboSpan" id="kobo.194.1">fashion MNIST</span></strong><span class="koboSpan" id="kobo.195.1"> data is already normalized to have values between </span><strong class="source-inline"><span class="koboSpan" id="kobo.196.1">0</span></strong><span class="koboSpan" id="kobo.197.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.198.1">1</span></strong><span class="koboSpan" id="kobo.199.1">. </span><span class="koboSpan" id="kobo.199.2">The convolutional transpose layer defined here adopts a similar number of filter configurations from the encoder, from </span><strong class="source-inline"><span class="koboSpan" id="kobo.200.1">16</span></strong><span class="koboSpan" id="kobo.201.1"> to </span><strong class="source-inline"><span class="koboSpan" id="kobo.202.1">4</span></strong><span class="koboSpan" id="kobo.203.1"> and finally, to </span><strong class="source-inline"><span class="koboSpan" id="kobo.204.1">1</span></strong><span class="koboSpan" id="kobo.205.1"> filter to produce only one channel </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">grayscale image.</span></span></p></li> <li><span class="koboSpan" id="kobo.207.1">Now that we have defined the convolutional autoencoder, let’s load up the fashion MNIST data from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.208.1">torchvision</span></strong><span class="koboSpan" id="kobo.209.1"> library. </span><span class="koboSpan" id="kobo.209.2">This tutorial will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.210.1">Catalyst</span></strong><span class="koboSpan" id="kobo.211.1"> library for ease of training, so let’s take the </span><strong class="source-inline"><span class="koboSpan" id="kobo.212.1">fashion MNIST</span></strong><span class="koboSpan" id="kobo.213.1"> dataset loader and feeder class from </span><strong class="source-inline"><span class="koboSpan" id="kobo.214.1">torchvision</span></strong><span class="koboSpan" id="kobo.215.1"> and modify it for usage in the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.216.1">Catalyst</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.217.1"> library:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.218.1">
class FashionMNISTImageTarget(
  torchvision.datasets.FashionMNIST
):
  def __getitem__(self, index):
   img = self.data[index]
   img = Image.fromarray(
     img.numpy(), mode="L"
   )
   if self.transform is not None:
     img = self.transform(img)
   return img, img</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.219.1">The class from </span><strong class="source-inline"><span class="koboSpan" id="kobo.220.1">torchvision</span></strong><span class="koboSpan" id="kobo.221.1"> already has the necessary logic to download and load the </span><strong class="source-inline"><span class="koboSpan" id="kobo.222.1">fashion MNIST</span></strong><span class="koboSpan" id="kobo.223.1"> dataset. </span><span class="koboSpan" id="kobo.223.2">However, the data feeder method, </span><strong class="source-inline"><span class="koboSpan" id="kobo.224.1">getitem</span></strong><span class="koboSpan" id="kobo.225.1">, is not in the expected format for image generation and, thus, requires this modification for this experiment </span><span class="No-Break"><span class="koboSpan" id="kobo.226.1">to work.</span></span></p></li> <li><span class="koboSpan" id="kobo.227.1">Note that the </span><strong class="source-inline"><span class="koboSpan" id="kobo.228.1">Pillow</span></strong><span class="koboSpan" id="kobo.229.1"> library is used to load the image in </span><em class="italic"><span class="koboSpan" id="kobo.230.1">step 5</span></em><span class="koboSpan" id="kobo.231.1">. </span><span class="koboSpan" id="kobo.231.2">This is so that we can easily use the tool from </span><strong class="source-inline"><span class="koboSpan" id="kobo.232.1">torchvision</span></strong><span class="koboSpan" id="kobo.233.1"> to perform different transformation steps, such as image augmentation. </span><span class="koboSpan" id="kobo.233.2">However, in this experiment, we will directly convert the </span><strong class="source-inline"><span class="koboSpan" id="kobo.234.1">pillow</span></strong><span class="koboSpan" id="kobo.235.1"> image into </span><strong class="source-inline"><span class="koboSpan" id="kobo.236.1">Pytorch</span></strong><span class="koboSpan" id="kobo.237.1"> tensors, using the transform logic </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">that follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.239.1">
def transform_image(image):
    return torchvision.transforms.ToTensor()(image)</span></pre></li> <li><span class="koboSpan" id="kobo.240.1">Now, let’s load the training</span><a id="_idIndexMarker375"/><span class="koboSpan" id="kobo.241.1"> and validation datasets of </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">fashion MNIST</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.244.1">
train_fashion_mnist_data = FashionMNISTImageTarget(
  'fashion_mnist/', download=True, train=True,
  transform=transform_image,
)
valid_fashion_mnist_data = FashionMNISTImageTarget(
  'fashion_mnist/', download=True, train=False,
  transform=transform_image,
)
loaders = {
  "train": DataLoader(
    train_fashion_mnist_data, batch_size=32,
    shuffle=True
  ),
  "valid": DataLoader(
    valid_fashion_mnist_data, batch_size=32
  ),
}</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.245.1">The preceding code downloads the dataset into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.246.1">fashion_mnist</span></strong><span class="koboSpan" id="kobo.247.1"> folder if it doesn’t already exist. </span><span class="koboSpan" id="kobo.247.2">Additionally, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.248.1">loaders</span></strong><span class="koboSpan" id="kobo.249.1"> variable is here so that it can be consumed by the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.250.1">Catalyst</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.251.1"> library.</span></span></p></li> <li><span class="koboSpan" id="kobo.252.1">Since the optimization goal</span><a id="_idIndexMarker376"/><span class="koboSpan" id="kobo.253.1"> is to reduce the different of reproduced pixel values compared to the target pixel values, we will use the mean squared error as the reconstruction </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">loss here:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.255.1">
criterion = nn.MSELoss()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.256.1">It’s important to note that while reconstruction loss is a common objective in unsupervised representation learning, there may be other metrics or objectives used, depending on the specific</span><a id="_idIndexMarker377"/><span class="koboSpan" id="kobo.257.1"> algorithm or approach. </span><span class="koboSpan" id="kobo.257.2">For example, in </span><strong class="bold"><span class="koboSpan" id="kobo.258.1">variational autoencoders</span></strong><span class="koboSpan" id="kobo.259.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.260.1">VAEs</span></strong><span class="koboSpan" id="kobo.261.1">), the objective is to maximize the </span><strong class="bold"><span class="koboSpan" id="kobo.262.1">evidence</span></strong> <strong class="bold"><span class="koboSpan" id="kobo.263.1">lower bound</span></strong><span class="koboSpan" id="kobo.264.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.265.1">ELBO</span></strong><span class="koboSpan" id="kobo.266.1">), which consists of both the reconstruction loss and the KL divergence that encourages the learned latent space to follow</span><a id="_idIndexMarker378"/><span class="koboSpan" id="kobo.267.1"> a specific probability distribution. </span><span class="koboSpan" id="kobo.267.2">Another example is perceptual loss, which can be used as a loss function for autoencoders when the goal is to preserve high-level semantic features, rather than achieving </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">pixel-wise accuracy.</span></span></p></li> <li><span class="koboSpan" id="kobo.269.1">Now, let’s define the supervised</span><a id="_idIndexMarker379"/><span class="koboSpan" id="kobo.270.1"> runner instance from </span><strong class="source-inline"><span class="koboSpan" id="kobo.271.1">Catalyst</span></strong><span class="koboSpan" id="kobo.272.1"> so that we can train </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">our model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.274.1">
runner = dl.SupervisedRunner(
     input_key="features", output_key="scores", target_key="targets", loss_key="loss"
)</span></pre></li> <li><span class="koboSpan" id="kobo.275.1">Next, we will define a generally usable function that can make it easy to perform multiple training and validation experiments </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">through code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.277.1">
def train_and_evaluate_mlp(
  trial_number, net, epochs,
  load_on_stage_start=False, best_or_last='last',
  verbose=False
):
  model = net
  optimizer = optim.Adam(
    model.parameters(), lr=0.02
  )
  checkpoint_logdir = "logs/trial_{}_autoencoder".format(
trial_number)
  runner.train(
    model=model,
    criterion=criterion,
    optimizer=optimizer,
    loaders=loaders,
    num_epochs=epochs,
    callbacks=[
            dl.CheckpointCallback(
                logdir=checkpoint_logdir,
                loader_key="valid",
                metric_key="loss",
                load_on_stage_end='best',
            )
    ],
    logdir="./logs",
    valid_loader="valid",
    valid_metric="loss",
    minimize_valid_metric=True,
    verbose=verbose,
  )
  with open(
    os.path.join(checkpoint_logdir, '_metrics.json'),
    'r'
  ) as f:
    metrics = json.load(f)
    if best_or_last == 'last':
      valid_loss = metrics['last']['_score_']
    else:
      valid_loss = metrics['best']['valid']['loss']
  return valid_loss</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.278.1">These are just basic training and evaluation boiler code without any tricks applied. </span><span class="koboSpan" id="kobo.278.2">We will explore in depth the tricks to train supervised models in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.279.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.280.1">, </span><em class="italic"><span class="koboSpan" id="kobo.281.1">Exploring Supervised </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.282.1">Deep Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.283.1">.</span></span></p></li> <li><span class="koboSpan" id="kobo.284.1">Now, we are ready to train</span><a id="_idIndexMarker380"/><span class="koboSpan" id="kobo.285.1"> and evaluate the CNN autoencoder model through the </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">following logic:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.287.1">
cnn_autoencoder = ConvAutoencoder()
best_valid_loss = train_and_evaluate_mlp(
    0, cnn_autoencoder, 20, load_on_stage_start=False, best_or_last='last', verbose=True
)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.288.1">The best performing </span><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">cnn_autoencoder</span></strong><span class="koboSpan" id="kobo.290.1"> weights based on the validation loss will be automatically loaded after training on 20 epochs </span><span class="No-Break"><span class="koboSpan" id="kobo.291.1">is complete.</span></span></p></li> <li><span class="koboSpan" id="kobo.292.1">After training the preceding model with the provided training code, on the Fashion MNIST dataset with 1x28x28 image dimensions, you should get something similar to the example input/output pairs shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.293.1">Figure 5</span></em></span><em class="italic"><span class="koboSpan" id="kobo.294.1">.4</span></em><span class="koboSpan" id="kobo.295.1"> through the </span><span class="No-Break"><span class="koboSpan" id="kobo.296.1">following code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.297.1">
input_image = valid_fashion_mnist_data[0][0].numpy()
predicted_image = cnn_autoencoder(
  torch.unsqueeze(valid_fashion_mnist_data[0][0], 0)
)
predicted_image = predicted_image.detach().numpy(
).squeeze(0).squeeze(0)
f, axarr = plt.subplots(2,1,  figsize=(5, 5))
axarr[0].imshow(predicted_image, cmap='gray')
axarr[1].imshow(input_image.squeeze(0), cmap='gray')</span></pre></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer063">
<span class="koboSpan" id="kobo.298.1"><img alt="Figure 5.4 – Autoencoder sample results on the Fashion MNIST dataset; the bottom represents the original image, and the top represents the reproduced image" src="image/B18187_05_004.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.299.1">Figure 5.4 – Autoencoder sample results on the Fashion MNIST dataset; the bottom represents the original image, and the top represents the reproduced image</span></p>
<p><span class="koboSpan" id="kobo.300.1">With the results, it’s clear that the bottleneck</span><a id="_idIndexMarker381"/><span class="koboSpan" id="kobo.301.1"> features are capable of reproducing the entire picture somewhat completely, and they can be treated as more representative and compact features for use instead of the original input data. </span><span class="koboSpan" id="kobo.301.2">Increasing the number of representation values of the bottleneck feature from 4 to something like 10 features should also increase the quality of the reproducible image. </span><span class="koboSpan" id="kobo.301.3">Feel free to try it out and experiment with </span><span class="No-Break"><span class="koboSpan" id="kobo.302.1">the parameters!</span></span></p>
<h1 id="_idParaDest-91"><a id="_idTextAnchor091"/><span class="koboSpan" id="kobo.303.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.304.1">Autoencoders are considered a fundamental method to achieve representation learning across data modalities. </span><span class="koboSpan" id="kobo.304.2">Consider the architecture as a shell that you can fit in a variety of other neural network components, allowing you to ingest data of different modalities or benefit from more advanced neural </span><span class="No-Break"><span class="koboSpan" id="kobo.305.1">network components.</span></span></p>
<p><span class="koboSpan" id="kobo.306.1">However, do note that they are not the only method to learn representative features. </span><span class="koboSpan" id="kobo.306.2">There are many more applications for autoencoders that primarily revolve around different training objectives using the same architecture. </span><span class="koboSpan" id="kobo.306.3">Two of these adaptations that were briefly introduced in this chapter are denoising autoencoders and variational autoencoders, which will be introduced properly in </span><a href="B18187_09.xhtml#_idTextAnchor149"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.307.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.308.1">, </span><em class="italic"><span class="koboSpan" id="kobo.309.1">Exploring Unsupervised Deep Learning</span></em><span class="koboSpan" id="kobo.310.1">. </span><span class="koboSpan" id="kobo.310.2">Now, let’s shift gears again to discover the model family </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">of transformers!</span></span></p>
</div>
</body></html>