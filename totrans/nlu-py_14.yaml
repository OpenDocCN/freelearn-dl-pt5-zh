- en: '14'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What to Do If the System Isn’t Working
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will discuss how to improve systems. If the original model’s
    first round of training fails to produce a satisfactory performance or the real-world
    scenario that the system addresses undergoes changes, we need to modify something
    to enhance the system’s performance. In this chapter, we will discuss techniques
    such as adding new data and changing the structure of an application, while at
    the same time ensuring that new data doesn’t degrade the performance of the existing
    system. Clearly, this is a big topic, and there is a lot of room to explore how
    to improve the performance of **natural language understanding** (**NLU**) systems.
    It isn’t possible to cover all the possibilities here, but this chapter should
    give you a good perspective on the most important options and techniques that
    can improve system performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will cover the following topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Figuring out that a system isn’t working
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fixing accuracy problems
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moving on to deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Problems after deployment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The first step is to find out that a system isn’t working as well as desired.
    This chapter will include a number of examples of tools that can help with this.
    We will start by listing the software requirements needed to run these examples.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We will be using the following data and software to run the examples in this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Our usual development environment – that is, Python 3 and Jupyter Notebook
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The TREC dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Matplotlib and Seaborn packages, which we will use to display graphical
    charts
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas and NumPy for numerical manipulation of data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The BERT NLU system, previously used in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193)
    and [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Keras machine learning library, for working with BERT
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: NLTK, which we will use for generating new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An OpenAI API key which we will use to access the OpenAI tools
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Figuring out that a system isn’t working
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Figuring out whether a system isn’t working as well as it should be is important,
    both during initial development as well as during ongoing deployment. We’ll start
    by looking at poor performance during initial development.
  prefs: []
  type: TYPE_NORMAL
- en: Initial development
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary techniques we will use to determine that our system isn’t working
    as well as we'd like are the evaluation techniques we learned about in [*Chapter
    13*](B19005_13.xhtml#_idTextAnchor226). We will apply those in this chapter. We
    will also use confusion matrices to detect specific classes that don’t work as
    well as the other classes.
  prefs: []
  type: TYPE_NORMAL
- en: It is always a good idea to look at the dataset at the outset and check the
    balance of categories because unbalanced data is a common source of problems.
    Unbalanced data does not necessarily mean that there will be accuracy problems,
    but it’s valuable to understand our class balance at the beginning. That way,
    we will be prepared to address accuracy issues caused by class imbalance as system
    development progresses.
  prefs: []
  type: TYPE_NORMAL
- en: Checking category balance
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our data exploration in this chapter, we will use the **Text Retrieval Conference**
    (**TREC**) dataset, which is a commonly used multi-class classification dataset
    and can be downloaded from Hugging Face ([https://huggingface.co/datasets/trec](https://huggingface.co/datasets/trec)).
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citations
  prefs: []
  type: TYPE_NORMAL
- en: '*Learning Question Classifiers*, Li, Xin and Roth, Dan, *{COLING} 2002:* *The
    19th International Conference on Computational Linguistics*, 2002, [https://www.aclweb.org/anthology/C02-1150](https://www.aclweb.org/anthology/C02-1150'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: '*Toward Semantics-Based Answer Pinpointing*, Hovy, Eduard and Gerber, Laurie
    and Hermjakob, Ulf and Lin, Chin-Yew and Ravichandran, Deepak, *Proceedings of
    the First International Conference on Human Language Technology Research*, 2001,
    [https://www.aclweb.org/anthology/H01-1069](https://www.aclweb.org/anthology/H01-1069'
  prefs: []
  type: TYPE_NORMAL
- en: )
  prefs: []
  type: TYPE_NORMAL
- en: The dataset consists of 5,452 training examples of questions that users might
    ask of a system and 500 test examples. The goal of the classification task is
    to identify the general topic of a question as the first step in answering it.
    The question topics are organized into two levels, consisting of six broad categories
    and 50 more specific subcategories that fall under the broader topics.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be working with the broad categories, which are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Abbreviation (`ABBR`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description (`DESC`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entity (`ENTY`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human (`HUM`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Location (`LOC`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number (`NUM`)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One important task at the beginning is to find out how many documents are in
    each class. We want to see whether all of the classes have enough texts for effective
    training and whether no classes are significantly more or less common than the
    others.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far in this book, we have seen many ways to load datasets. One of the easiest
    ways to load a dataset is based on data being organized into folders, with separate
    folders for each class. Then, we can load the dataset with the `tf.keras.utils.text_dataset_from_directory()`
    function, which we used several times in previous chapters, and see the class
    names. This is shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then count the number of files in each class and display them in a bar
    graph with this code, using the `matplotlib` and `seaborn` graphics libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'While this code prints out the count of texts in each class as text output,
    it is also very helpful to see the totals as a bar graph. We can use the graphics
    libraries to create this graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Coarse-grained class counts in the TREC data](img/B19005_14_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – Coarse-grained class counts in the TREC data
  prefs: []
  type: TYPE_NORMAL
- en: As *Figure 14**.1* shows, the `DESC` class is much smaller than the others,
    and it is possible that there will be accuracy problems with this class. There
    are ways to address this situation, which is one of the main topics of this chapter,
    but for now, we won’t make any changes until we see that this actually causes
    a problem.
  prefs: []
  type: TYPE_NORMAL
- en: Doing initial evaluations
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we have done this initial exploration, we will want to try training one
    or more initial models for the data and evaluate them using some of the techniques
    we learned in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226).
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exploration, we will use the BERT-based training process that was
    covered in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), so we won’t duplicate
    that here. However, there are a few changes in the model that we need to make
    because we are now working with a *categorical* classification problem (six classes),
    rather than a binary classification problem (two classes), and it is worth pointing
    these out. We can see the new model definition in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The two changes that are needed in the model definition for the categorical
    task are in the final layer, which has six outputs, corresponding to the six classes,
    and a softmax activation function, as opposed to the sigmoid activation function
    that we used for binary problems.
  prefs: []
  type: TYPE_NORMAL
- en: 'The other changes that are needed for categorical data are changes in the loss
    function and the metrics, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Here, we will define the categorical loss and metrics functions. Other metrics
    are available, but we will just look at accuracy here.
  prefs: []
  type: TYPE_NORMAL
- en: After training the model, as we did in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226),
    we can look at the final scores. If the model does not meet the overall performance
    expectations for the application using the metrics that have been chosen, you
    can try different hyperparameter settings, or you can try other models. This was
    the process we followed in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), where
    we compared the performance of three different models on the movie review data.
  prefs: []
  type: TYPE_NORMAL
- en: Keeping in mind that larger models are likely to have better performance, you
    can try increasing the size of the models. There is a limit to this strategy –
    at some point, the larger models will become very slow and unwieldy. You might
    also see that the payoff from larger and larger models becomes smaller, and performance
    levels off. This probably means that increasing the size of the models will not
    solve the problem.
  prefs: []
  type: TYPE_NORMAL
- en: There are many different possibilities to look at different hyperparameter settings.
    This is, in general, a huge search space that can’t be fully explored, but there
    are some heuristics that you can use to find settings that could improve your
    results. Looking at the training history charts of loss and accuracy changes over
    epochs should give you a good idea of whether additional training epochs are likely
    to be helpful. Different batch sizes, learning rates, optimizers, and dropout
    layers can also be explored.
  prefs: []
  type: TYPE_NORMAL
- en: Another strategy to diagnose system performance is to look at the data itself.
  prefs: []
  type: TYPE_NORMAL
- en: One initial evaluation we can do is a more fine-grained check for weak classes,
    by looking at the probabilities of the classifications for a large number of items
    in the dataset. We will look at this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for weak classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Low probabilities for a class of items are a sign that a system is not able
    to classify items with high confidence and has a good chance of making errors.
    To check for this, we can use the model to predict the classification of a subset
    of our data and look at the average scores, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This code goes through a subset of the TREC training data, predicts each item’s
    class, saves the predicted class in the `classification` variable, and then adds
    it to the `scores` list for the predicted class.
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step in the code is to iterate through the scores list and print
    the length and average score for each class. The results are shown in *Table 14.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **Number** **of items** | **Average score** |'
  prefs: []
  type: TYPE_TB
- en: '| `ABBR` | 792 | 0.9070532 |'
  prefs: []
  type: TYPE_TB
- en: '| `DESC` | 39 | 0.8191106 |'
  prefs: []
  type: TYPE_TB
- en: '| `HUM` | 794 | 0.8899161 |'
  prefs: []
  type: TYPE_TB
- en: '| `ENTY` | 767 | 0.9638871 |'
  prefs: []
  type: TYPE_TB
- en: '| `LOC` | 584 | 0.9767452 |'
  prefs: []
  type: TYPE_TB
- en: '| `NUM` | 544 | 0.9651737 |'
  prefs: []
  type: TYPE_TB
- en: Table 14.1 – The number of items and the average score for each class
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see from *Table 14.1* that the number of items and average probabilities
    of the class predictions vary quite a bit. As you will recall from the counts
    we did in *Figure 14**.1*, we were already concerned about the `DESC` class because
    it was so small relative to the other classes. We can investigate this a bit further
    by looking at the predicted classifications of the individual items in each class
    with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Let’s look at the histograms for the `DESC` and `LOC` classes, which are at
    the extreme ends of the set of average scores. The `LOC` class is shown in *Figure
    14**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – The distribution of probability scores for the LOC class](img/B19005_14_2.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – The distribution of probability scores for the LOC class
  prefs: []
  type: TYPE_NORMAL
- en: We can see in *Figure 14**.2* that not only is the average probability very
    high (which we saw in *Table 14.1* as well) but there also are very few probabilities
    under `LOC` class. This class is likely to be very accurate in the deployed application.
  prefs: []
  type: TYPE_NORMAL
- en: There is a second, less obvious advantage to classes that show the pattern in
    *Figure 14**.2*. In a deployed interactive application, we don’t want a system
    to give users answers that it’s not very confident of. This is because they’re
    more likely to be wrong, which would mislead the users. For that reason, developers
    should define a *threshold* probability score, which an answer has to exceed before
    the system provides that answer to the user.
  prefs: []
  type: TYPE_NORMAL
- en: If the probability is lower than the threshold, the system should respond to
    the user that it doesn’t know the answer. The value of the threshold has to be
    set by the developer, based on the trade-off between the risk of giving users
    wrong answers and the risk of annoying users by saying *I don’t know* too frequently.
    *In* *Figure 14**.2* we can see that if we set the threshold to **0.9**, the system
    will not have to say *I don’t know* very often, which will improve user satisfaction
    with the system.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s contrast *Figure 14**.2* with a histogram for the `DESC` class, which
    we can see in *Figure 14**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – The distribution of probability scores for the DESC class](img/B19005_14_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.3 – The distribution of probability scores for the DESC class
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14**.3* shows many probability scores less than `DESC` class will be
    problematic in deployment.'
  prefs: []
  type: TYPE_NORMAL
- en: 'A confusion matrix, such as the one we reviewed in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226),
    can also help detect underperforming classes. We can generate a confusion matrix
    for the TREC data with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'This code generates the predicted classes from the test data (represented in
    the `predicted_classes` variable) and compares them to the true classes (represented
    in the `y-test` variable). We can use the scikit-learn `confusion_matrix` function
    to display the confusion matrix as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the resulting confusion matrix in *Figure 14**.4*. The confusion
    matrix tells us how often each class was predicted to be each other class, including
    itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – The confusion matrix for the TREC test set](img/B19005_14_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 – The confusion matrix for the TREC test set
  prefs: []
  type: TYPE_NORMAL
- en: The correct predictions can be seen on the main diagonal. For example, `ABBR`
    was correctly predicted as `ABBR` *137* times. We can also see the prediction
    errors for each class. The most frequent error was incorrectly classifying `ENTY`
    as `ABBR` *11* times. In this particular example, we don’t see a lot of evidence
    that specific classes get confused with each other, although there is a tendency
    for `ENTY` to be confused with `ABBR`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can look at the classification report to see the `precision`, `recall`,
    and `F1` scores for each class, as well as the overall averages for the entire
    test set. The recall scores in the classification report for `DESC` and `ENTY`
    are somewhat lower than the other recall scores, which reflects the fact that
    some of the items in those classes are incorrectly recognized as `ABBR`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: It’s worth pointing out at this point that the decision of whether the system
    is *good enough* really depends on the application and the developer’s decision.
    In some applications, it’s better to give the user some result, even if it might
    be wrong, while in other applications, it’s important for every result to be correct,
    even if the system has to say *I don’t know* almost all the time. Going back to
    the ideas of precision and recall that we covered in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226),
    another way of putting this is to say that in some applications, recall is more
    important, and in other cases, precision is more important.
  prefs: []
  type: TYPE_NORMAL
- en: If we want to improve the performance of the TREC application, the next step
    is to decide how to address our performance concerns and improve overall accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Fixing accuracy problems
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at fixing performance problems through two strategies.
    The first one involves issues that can be addressed by changing data, and the
    second strategy involves issues that require restructuring the application. Generally,
    changing the data is easier, and it is a better strategy if it is important to
    keep the structure of the application the same – that is, we don’t want to remove
    classes or introduce new classes. We’ll start by discussing changing the data
    and then discuss restructuring the application.
  prefs: []
  type: TYPE_NORMAL
- en: Changing data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Changing data can greatly improve the performance of your system; however, you
    won’t always have this option. For example, you might not have control over the
    dataset if you work with a standard dataset that you intend to compare to other
    researchers’ work. You can’t change the data if you are in that situation because
    if you do, your system’s performance won’t be comparable to that of other researchers.
    If your system’s performance isn’t satisfactory but you can’t change the data,
    the only options are to improve the algorithms by using a different model or adjusting
    the hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if you work on an application where you do have control over
    a dataset, changing data can be a very effective way to improve your system.
  prefs: []
  type: TYPE_NORMAL
- en: Many performance issues are the result of not having enough data, either overall,
    or in specific classes. Other performance issues can be due to annotation errors
    . We’ll start with a brief discussion of annotation errors.
  prefs: []
  type: TYPE_NORMAL
- en: Annotation errors
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: It is possible that the poor performance of systems in supervised learning applications
    is due to annotation errors. Another way of putting this is to say that the supervision
    of data was wrong, and the system was trained to do the wrong thing. Perhaps an
    annotator accidentally assigned some data to the wrong class. If the data is training
    data, data in the wrong class will make the model less accurate, or if the data
    is test data, the item would be scored incorrectly because the model was wrong.
  prefs: []
  type: TYPE_NORMAL
- en: Checking for occasional annotation errors by reviewing the annotation of every
    item in the dataset can be very time-consuming, and it is not likely to improve
    the system much. This is because if the dataset is large enough, this kind of
    sporadic error is unlikely to have much of an impact on the quality of the overall
    system. However, if you suspect that annotation errors are causing problems, a
    simple check for low-confidence items can be helpful without requiring every annotation
    to be checked. This can be done by using a variation of the code we used in the
    *Checking for weak classes* section to check for weak classes. In that code, we
    predicted the class of each item in the dataset, kept track of its probabilities
    (scores), and averaged the probabilities of all the items in the class. To modify
    the code to look instead for individual items with low probabilities, you could
    record each item and its probability individually, and then look for low-probability
    items in the final list. You are encouraged to try this exercise for yourself.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, it is also possible that data contains not only occasional
    mistakes but also systematic annotation errors. Systematic errors might be due
    to differences in the annotators’ understanding of the meanings of the classes,
    leading to the similar items being assigned to different classes by different
    annotators. Ideally, these kinds of errors can be avoided, or at least reduced,
    by preparing clear annotation guidelines for annotators before the annotation
    process begins, or even by giving them training classes.
  prefs: []
  type: TYPE_NORMAL
- en: Tools such as the *kappa* statistic, which was mentioned in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107),
    can measure divergent annotations among annotators. If the kappa statistic shows
    that there is a lot of divergence across annotators, some of the data might need
    to be re-annotated using clarified guidelines. It can also happen that it is impossible
    to get annotators to agree because the decisions that the annotators have to make
    are inherently too subjective for people to agree on, no matter how much guidance
    they are given. This is a sign that the problem is not really suitable for NLU
    in the first place because there might not be a real correct classification for
    this data.
  prefs: []
  type: TYPE_NORMAL
- en: However, assuming that we do have a problem with objective classifications,
    in addition to addressing annotation errors, we can also improve system performance
    by creating a more balanced dataset. To do this, we will first look at adding
    and removing existing data from classes.
  prefs: []
  type: TYPE_NORMAL
- en: Adding and removing existing data from classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Unbalanced amounts of data in different classes are a common situation that
    can lead to poor model performance. The main reason that a dataset can be unbalanced
    is that this imbalance represents the actual situation in the application domain.
    For example, an application that is supposed to detect online hate speech will
    most likely encounter many more examples of non-hate speech than actual hate speech,
    but it is nevertheless important to find instances of hate speech, even if they
    are rare. Another example of a naturally unbalanced dataset would be a banking
    application where we find many more utterances about checking account balances
    than utterances about changing account addresses. Changing the address on an account
    just doesn’t happen very often compared to checking balances.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to make the sizes of the classes more even.
  prefs: []
  type: TYPE_NORMAL
- en: 'Two common approaches are to duplicate data in the smaller classes or remove
    data from the larger classes. Adding data is called **oversampling** and removing
    data is called **undersampling**. The obvious approach to oversampling is to randomly
    copy some of the data instances and add them to the training data. Similarly,
    you can undersample by randomly removing instances from the classes that are too
    large. There are also other more sophisticated approaches to undersampling and
    oversampling, and you can find many online discussions about these topics – here,
    for example: [https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data](https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data).
    However, we will not review these here because they can become quite complex.'
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling and oversampling can be helpful, but you should understand that
    they have to be used thoughtfully. For example, in the TREC dataset, trying to
    undersample the five frequent classes so that they have no more instances than
    the `DESC` class would require throwing out hundreds of instances from the larger
    classes, along with the information that they contain. Similarly, oversampling
    a small class such as `DESC` so that it contains the same number of instances
    as the larger classes means that there will be many duplicate instances of the
    `DESC` texts. This could result in overfitting the examples in `DESC` and consequently
    make it hard for the model to generalize to new test data.
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to see that while undersampling and oversampling can potentially
    be useful, they are not automatic solutions. They are probably most helpful when
    classes are not extremely different in size and where there are plenty of examples,
    even in the smallest classes. You should also keep in mind that the classes don’t
    have to be exactly balanced for a system to perform well.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach to adding data is to create new data, which we will discuss
    in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Generating new data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If your dataset has underrepresented classes, or is too small overall, you
    can also add generated data to the entire dataset or just to the smaller classes.
    We will look at the following three ways to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: Generating new data from rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating new data from LLMs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using crowdworkers to get new data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating new data from rules
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One way to create new data is to write rules to generate new examples of data,
    based on the data that you already have. The `restaurant search` class. You could
    write a `parse` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Note that the rules in an NLTK CFG can be any context-free rules; they don’t
    have to correspond to actual linguistic categories. For example, we could have
    called the last Adj_Cuisine instead. We might want to do this if we want to be
    able to generate sentences with other adjectives, such as `good` or `low-priced`.
    The rule names and the rules themselves don’t matter to the NLTK CFG package;
    the only thing that matters is that the CFG is written in the syntax that the
    NLTK CFG package expects. The names and the rules can be any rules that you find
    convenient to generate new examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last two lines in the preceding code will generate 10 examples of sentences
    from this grammar, with the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: If you want to generate all of the possible sentences from these rules, you
    will leave out the parameter, `n=10`.
  prefs: []
  type: TYPE_NORMAL
- en: This is a fast way to generate a lot of sentences, but as you can see, the sentences
    are quite repetitious. This is because the NLTK `generate` method will produce
    every possible sentence that the grammar covers. Adding a lot of repetitious sentences
    to your training set could skew the model to these kinds of sentences, which in
    turn might make it harder for the model to recognize more varied restaurant search
    sentences. One approach to getting a wider variety of sentences from an NLTK CFG
    would be to write a broader grammar, generate all the sentences it covers, and
    then randomly select a subset of the generated sentences to add to the training
    set.
  prefs: []
  type: TYPE_NORMAL
- en: Using **large language models** (**LLMs**) to generate new examples is another
    useful and easy option, which we will discuss in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Generating new data from LLMs
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Online LLMs such as ChatGPT are another very good way to get more training
    data because you can simply ask them to generate the appropriate training data.
    For example, let’s say ChatGPT was given the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '*generate 20 requests to find local restaurants of different cuisines and*
    *price ranges*'
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGPT ([chat.openai.com/chat](https://chat.openai.com/chat)) would produce
    the following answer:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5 – The ChatGPT-generated restaurant query data](img/B19005_14_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.5 – The ChatGPT-generated restaurant query data
  prefs: []
  type: TYPE_NORMAL
- en: (For brevity, not all the results are shown.)
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that these sentences are much less repetitious than the ones from
    NLTK’s `generate` method. In the initial ChatGPT prompt, you can also restrict
    the question style – for example, you could ask for generated questions in an
    informal style. This results in informal sentences such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6 – ChatGPT-generated restaurant query data in an informal style](img/B19005_14_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.6 – ChatGPT-generated restaurant query data in an informal style
  prefs: []
  type: TYPE_NORMAL
- en: You can also control the variation among the responses by changing the *temperature*
    parameter, which is available in the ChatGPT API. Temperature settings vary between
    zero and two. A temperature setting of zero means that the responses will be less
    varied, and a higher setting means that they will be more varied.
  prefs: []
  type: TYPE_NORMAL
- en: Within ChatGPT, a low temperature means that the generation model chooses the
    next word for a response among the higher probability words, and a high temperature
    means that the model will select the next word from among the words with lower
    probabilities. The result with a higher temperature setting will include more
    varied responses, but some of them might not make sense. *Figure 14**.7* shows
    how to set the temperature in code directly from the API.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.7 – Setting the GPT temperature using the OpenAI API](img/B19005_14_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 14.7 – Setting the GPT temperature using the OpenAI API
  prefs: []
  type: TYPE_NORMAL
- en: The code in *Figure 14**.7* sets the value of the `temperature` parameter to
    `1.5`, which results in a fairly diverse set of responses. You can see these at
    the bottom of *Figure 14**.7*. The code also sets the `model` parameter to use
    the `gpt-3.5-turbo` model and sets the message to send in the `messages` parameter.
    If you are interested in experimenting with other GPT API calls, you can find
    other API parameters in the OpenAI API documentation at [https://platform.openai.com/docs/api-reference](https://platform.openai.com/docs/api-reference).
  prefs: []
  type: TYPE_NORMAL
- en: Note that you will need to set the `openai.api_key` variable at line 3 to your
    own OpenAI user key to run this code, since the OpenAI API is a paid service.
  prefs: []
  type: TYPE_NORMAL
- en: If you use an LLM to generate data, be sure to check the results and decide
    whether the responses represent the kind of examples that your users would really
    say and, hence, should be included in your training data. For example, some of
    the informal requests in *Figure 14**.6* might be more informal than many users
    would say to a chatbot.
  prefs: []
  type: TYPE_NORMAL
- en: A final way to add new data to underrepresented classes is to hire crowdworkers
    to create more data.
  prefs: []
  type: TYPE_NORMAL
- en: Using crowdworkers to get new data
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Getting more data from crowdworkers is time-consuming and possibly expensive,
    depending on how much data you need and how complicated it is. Nevertheless, it
    would be an option if you didn't get enough data using other methods.
  prefs: []
  type: TYPE_NORMAL
- en: Of course, all of these methods we have outlined in this section (using rules,
    using LLMs, and using crowdworkers) can be combined – not all of the new training
    data has to come from the same place.
  prefs: []
  type: TYPE_NORMAL
- en: Another approach similar to changing the data is to change the application itself,
    which we will discuss in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Restructuring an application
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases, the best solution to classes that are not being predicted well
    is to restructure an application. As in the case of changing the data, you won’t
    always have the option to do this if this is a standard dataset that the research
    community uses to compare work among different labs, as the application has to
    have the same structure as that used by other researchers for the results to be
    comparable.
  prefs: []
  type: TYPE_NORMAL
- en: If you do have control over the application structure, you can add, remove,
    or combine classes that don’t perform well. This can greatly improve the overall
    application performance. Let’s start by looking at an artificial example of an
    application that needs restructuring, and the different ways that this restructuring
    might be done.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the need for class restructuring
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Visualizing datasets can often provide immediate insight into potential performance
    problems. We can visualize how similar classes in a dataset are to each other
    in a couple of ways.
  prefs: []
  type: TYPE_NORMAL
- en: First, confusion matrices such as the one in *Figure 14**.4* are a good source
    of information about which classes are similar to each other and consequently
    get confused for each other. We saw immediately from *Figure 14**.4* that `ENTY`
    and `DESC` were quite often confused with `ABBR`. We might want to add data to
    those classes, as discussed in the previous section, or we could also consider
    restructuring the application, which we will discuss next.
  prefs: []
  type: TYPE_NORMAL
- en: A second visualization technique is to use the topic modeling techniques that
    we saw in [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217), to see problems with
    the application structure.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14**.8* shows how an artificially constructed dataset of four classes
    might look if we clustered them based on the [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217)
    tools, **Sentence Bert** and **BERTopic**. We can immediately see that there are
    some problems with the classes in this dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 14.\uFEFF8 – Unsupervised clustering of four classes with artificially\
    \ generated data](img/B19005_14_08.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 14.8 – Unsupervised clustering of four classes with artificially generated
    data
  prefs: []
  type: TYPE_NORMAL
- en: First of all, the instances of **Class 1**, represented by circles, seem to
    cluster into two different classes, one centered around the point (*0.5, 0.5*)
    and the other centered around the point (0*.5*, *1.75*). It seems unlikely that
    these clusters should both be grouped into the same class if they are actually
    that different. **Class 1** should probably be split, and the instances currently
    assigned to Class 1 should be assigned to at least two, and possibly three, new
    classes.
  prefs: []
  type: TYPE_NORMAL
- en: '**Class 2**, represented by squares, and **Class 3**, represented by triangles,
    seem problematic. They are not completely mixed together, but they are not completely
    separate either. Some of the instances of both classes are likely to be misclassified
    because of their similarity to the other class. If you see classes such as **Class
    3** and **Class 4**, with this kind of overlap, consider merging the classes if
    they appear to be similar in meaning (if they aren’t similar in meaning, consider
    adding more data to one or both classes). Finally, **Class 4**, represented by
    stars, is compact and doesn’t overlap with any other classes. It shouldn’t require
    any adjustments.'
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now take a look at three restructuring options – merging classes, dividing
    classes, and introducing new classes.
  prefs: []
  type: TYPE_NORMAL
- en: Merging classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classes that are much smaller than the rest of the classes can be merged with
    other semantically similar classes, especially if they are frequently confused
    with those classes. This can be a good strategy because, in many real applications,
    there are classes that simply don’t occur very often, but unlike the hate speech
    example mentioned earlier, it isn’t always critical to be able to tell the difference
    between the original classes. Of course, this is only possible with a multi-class
    problem – that is, a problem with more than two classes – since merging the classes
    in a binary (two-class) problem will put everything in one class and leave us
    with nothing to classify.
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, merging classes can be accomplished by adding all the data from
    one class to the data of the other class, which is the simplest restructuring
    that we can do. A slightly more complex merger of classes can be done if the new
    structure is more complicated – for example, if it involves adding slots. For
    example, it is possible that classes such as **Class 2** and **Class 3** in *Figure
    14**.8* are actually not different enough to be worth trying to separate.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, suppose we work on a generic personal assistant application,
    with classes such as `play music`, `find a restaurant`, `get weather forecast`,
    `find a bookstore`, and `find a bank`. It might turn out that `find a restaurant`
    has much more data than `find a bookstore`, and as a result, `find a bookstore`
    is often confused with `find a restaurant`. In that case, it would be worth considering
    whether all the `find a` classes should be merged into one larger class. This
    class could be called `local business search`, with `bookstore`, `restaurant`,
    and `bank` being treated as slots, as discussed in [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173).
  prefs: []
  type: TYPE_NORMAL
- en: Another strategy is to separate classes such as **Class 1** in *Figure 14**.5*
    into two different classes. The next section discusses dividing classes.
  prefs: []
  type: TYPE_NORMAL
- en: Dividing classes
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes it makes sense to divide a large class into separate smaller classes
    if there appear to be systematic differences between two or more groups within
    the class. Tools such as BERTopic can help suggest names for new classes if the
    new name isn’t obvious from looking at the instances in each group. Unfortunately,
    dividing a class into new classes isn’t as easy as merging classes because the
    examples in the new classes will need to be annotated with their new names. Although
    re-annotation is more work, dividing and re-annotating classes is necessary if
    you have to divide a large class into more meaningful new classes.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing an "other" class
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Introducing an `other` class is a variant of the strategy of merging classes.
    If there are several small classes that don’t really have enough training data
    to be reliably classified, it can sometimes be useful to group them together in
    an `other` class – that is, a class that contains items that don’t fit into any
    of the other classes. One type of application that this approach can be useful
    for is call routing in a telephone self-service application.
  prefs: []
  type: TYPE_NORMAL
- en: In these applications, there can sometimes be hundreds of destinations where
    a call can be routed. In nearly every application of this kind, there are some
    infrequent classes for which there is much less data than other classes. Sometimes,
    it is best to not try to identify these classes because trying to do so accurately
    will be difficult with the small amounts of data available. A better strategy
    would be to group them together into an `other` class. It still might be hard
    to identify items in the `other` category because the items it contains will not
    be very similar, but it will keep them from interfering with the overall application
    accuracy. How items in the `other` class are handled depends on the specific application’s
    goals, but options include handling them manually (for example, with a human call
    center agent) or simply telling users that the system can’t handle their question.
  prefs: []
  type: TYPE_NORMAL
- en: After the accuracy issues that were identified during initial development have
    been identified and addressed, it is time to deploy the system.
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we’ve fixed the performance issues we’ve discussed so far, we will have trained
    a model that meets our performance expectations, and we can move on to deployment,
    when the system is installed and does the task that it was designed for. Like
    any software, a deployed NLU model can have problems with system and hardware
    issues, such as network issues, scalability, and general software problems. We
    won’t discuss these kinds of problems because they aren’t specific to NLU.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will cover considerations to address NLU performance problems
    that occur after deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Problems after deployment
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After an NLU system is developed and put into place in an application, it still
    requires monitoring. Once the system has reached an acceptable level of performance
    and has been deployed, it can be tempting to leave it alone and assume that it
    doesn’t need any more attention, but this is not the case. At the very least,
    the deployed system will receive a continuous stream of new data that can be challenging
    to the existing system if it is different from the training data in some way.
    On the other hand, if it is not different, it can be used as new training data.
    Clearly, it is better to detect performance problems from internal testing than
    to learn about them from negative customer feedback.
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, we can think of new performance problems as either being due
    to a change in the system itself, or due to a change in the deployment context.
  prefs: []
  type: TYPE_NORMAL
- en: Changes in system performance due to system changes should be detected by testing
    before the new system is deployed. This kind of testing is very similar to the
    kind of testing that has to be done for any software deployment, so we won’t cover
    it in any detail. Degradation in performance can be detected by versioning the
    system and running an evaluation with a fixed set of data and metrics after every
    change. This is useful to both detect decreases in performance but also to document
    improvements in performance.
  prefs: []
  type: TYPE_NORMAL
- en: As with any machine-learning-based system, new data can cause problems with
    an NLU system because it is different in some significant way from the training
    data. These kinds of differences are frequently due to changes in the deployment
    context.
  prefs: []
  type: TYPE_NORMAL
- en: What do we mean by changes in the **deployment context**? The deployment context
    refers to everything about the application, except for the NLU system itself.
    Specifically, it can include the users, their demographics, their geographical
    locations, the backend information that’s being provided, and even events in the
    world such as weather. Any of these can change the characteristics of texts that
    the application processes. These changes alter the correspondence between the
    training data and the new data being processed, which will lead to a decrease
    in performance.
  prefs: []
  type: TYPE_NORMAL
- en: Some changes in the deployment context can be predicted. For example, if a company
    introduces a new product, this will introduce new vocabulary that a customer support
    chatbot, voice assistant, or email router needs to recognize, since, after all,
    we expect customers to be talking about it. It is a best practice to perform an
    evaluation on new data after changes like the introduction of a new product occurs,
    and decide whether the system should be retrained with additional data.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, some changes can’t be predicted – for example, the COVID-19
    pandemic introduced a lot of new vocabulary and concepts that medical or public
    health NLU applications needed to be trained on. Because some deployment context
    changes can’t be predicted, it is a good idea to periodically perform an evaluation
    using new data coming in from the deployment.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned about a number of important strategies to
    improve the performance of NLU applications. You first learned how to do an initial
    survey of the data and identify possible problems with the training data. Then,
    you learned how to find and diagnose problems with accuracy. We then described
    different strategies to improve performance – specifically, adding data and restructuring
    the application. The final topic we covered was a review of problems that can
    occur in deployed applications and how they can be addressed.
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter, we will provide an overview of the book and a look to
    the future. We will discuss where there is potential for improvement in the state
    of the art of NLU performance, as well as faster training, more challenging applications,
    and what we can expect from NLU technology as the new LLMs become more widely
    used.
  prefs: []
  type: TYPE_NORMAL
