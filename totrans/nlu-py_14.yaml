- en: '14'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '14'
- en: What to Do If the System Isn’t Working
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 如果系统不工作该怎么办
- en: In this chapter, we will discuss how to improve systems. If the original model’s
    first round of training fails to produce a satisfactory performance or the real-world
    scenario that the system addresses undergoes changes, we need to modify something
    to enhance the system’s performance. In this chapter, we will discuss techniques
    such as adding new data and changing the structure of an application, while at
    the same time ensuring that new data doesn’t degrade the performance of the existing
    system. Clearly, this is a big topic, and there is a lot of room to explore how
    to improve the performance of **natural language understanding** (**NLU**) systems.
    It isn’t possible to cover all the possibilities here, but this chapter should
    give you a good perspective on the most important options and techniques that
    can improve system performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论如何改进系统。如果原始模型的第一轮训练未能产生令人满意的性能，或者系统所解决的真实世界场景发生变化，我们需要修改一些内容以增强系统的性能。在本章中，我们将讨论如添加新数据和更改应用程序结构的技术，同时确保新数据不会降低现有系统的性能。显然，这是一个大课题，我们有很多探索提高**自然语言理解**（**NLU**）系统性能的空间。在这里不可能涵盖所有可能性，但本章应该能给您一个关于最重要的可以提高系统性能的选项和技术的良好视角。
- en: 'We will cover the following topics in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中涵盖以下主题：
- en: Figuring out that a system isn’t working
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弄清楚系统不起作用
- en: Fixing accuracy problems
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修复准确性问题
- en: Moving on to deployment
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进入部署阶段
- en: Problems after deployment
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署后的问题
- en: The first step is to find out that a system isn’t working as well as desired.
    This chapter will include a number of examples of tools that can help with this.
    We will start by listing the software requirements needed to run these examples.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是弄清楚系统是否运行得不如预期。本章将包括一些有助于此的工具示例。我们将从列出运行这些示例所需的软件要求开始。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'We will be using the following data and software to run the examples in this
    chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下数据和软件来运行本章的示例：
- en: Our usual development environment – that is, Python 3 and Jupyter Notebook
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通常的开发环境——即Python 3和Jupyter Notebook
- en: The TREC dataset
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TREC数据集
- en: The Matplotlib and Seaborn packages, which we will use to display graphical
    charts
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将用于显示图表的Matplotlib和Seaborn包
- en: pandas and NumPy for numerical manipulation of data
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于数据的数值操作的pandas和NumPy
- en: The BERT NLU system, previously used in [*Chapter 11*](B19005_11.xhtml#_idTextAnchor193)
    and [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以前在[*第11章*](B19005_11.xhtml#_idTextAnchor193)和[*第13章*](B19005_13.xhtml#_idTextAnchor226)中使用的BERT自然语言理解系统
- en: The Keras machine learning library, for working with BERT
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于与BERT一起工作的Keras机器学习库
- en: NLTK, which we will use for generating new data
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NLTK，我们将用它来生成新数据
- en: An OpenAI API key which we will use to access the OpenAI tools
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将用于访问OpenAI工具的OpenAI API密钥
- en: Figuring out that a system isn’t working
  id: totrans-19
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 弄清楚系统不起作用
- en: Figuring out whether a system isn’t working as well as it should be is important,
    both during initial development as well as during ongoing deployment. We’ll start
    by looking at poor performance during initial development.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 弄清楚系统是否运行得不如预期同样重要，无论是在初步开发阶段还是在持续部署期间。我们将从初步开发阶段的性能不佳开始。
- en: Initial development
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 初始开发
- en: The primary techniques we will use to determine that our system isn’t working
    as well as we'd like are the evaluation techniques we learned about in [*Chapter
    13*](B19005_13.xhtml#_idTextAnchor226). We will apply those in this chapter. We
    will also use confusion matrices to detect specific classes that don’t work as
    well as the other classes.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们在[*第13章*](B19005_13.xhtml#_idTextAnchor226)中学到的评估技术来确定我们的系统是否达到了我们想要的效果。我们将在本章中应用这些技术。我们还将使用混淆矩阵来检测不如其他类别表现好的特定类别。
- en: It is always a good idea to look at the dataset at the outset and check the
    balance of categories because unbalanced data is a common source of problems.
    Unbalanced data does not necessarily mean that there will be accuracy problems,
    but it’s valuable to understand our class balance at the beginning. That way,
    we will be prepared to address accuracy issues caused by class imbalance as system
    development progresses.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始时查看数据集并检查类别的平衡总是一个好主意，因为不平衡的数据是问题的常见来源。不平衡的数据并不一定意味着会出现准确性问题，但了解我们的类别平衡在开始阶段是有价值的。这样，我们将能够在系统开发进展中准备好解决由类别不平衡引起的准确性问题。
- en: Checking category balance
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For our data exploration in this chapter, we will use the **Text Retrieval Conference**
    (**TREC**) dataset, which is a commonly used multi-class classification dataset
    and can be downloaded from Hugging Face ([https://huggingface.co/datasets/trec](https://huggingface.co/datasets/trec)).
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
- en: Dataset citations
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
- en: '*Learning Question Classifiers*, Li, Xin and Roth, Dan, *{COLING} 2002:* *The
    19th International Conference on Computational Linguistics*, 2002, [https://www.aclweb.org/anthology/C02-1150](https://www.aclweb.org/anthology/C02-1150'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
- en: '*Toward Semantics-Based Answer Pinpointing*, Hovy, Eduard and Gerber, Laurie
    and Hermjakob, Ulf and Lin, Chin-Yew and Ravichandran, Deepak, *Proceedings of
    the First International Conference on Human Language Technology Research*, 2001,
    [https://www.aclweb.org/anthology/H01-1069](https://www.aclweb.org/anthology/H01-1069'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
- en: )
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
- en: The dataset consists of 5,452 training examples of questions that users might
    ask of a system and 500 test examples. The goal of the classification task is
    to identify the general topic of a question as the first step in answering it.
    The question topics are organized into two levels, consisting of six broad categories
    and 50 more specific subcategories that fall under the broader topics.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
- en: 'We will be working with the broad categories, which are as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
- en: Abbreviation (`ABBR`)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Description (`DESC`)
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Entity (`ENTY`)
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human (`HUM`)
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Location (`LOC`)
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Number (`NUM`)
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One important task at the beginning is to find out how many documents are in
    each class. We want to see whether all of the classes have enough texts for effective
    training and whether no classes are significantly more or less common than the
    others.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: 'So far in this book, we have seen many ways to load datasets. One of the easiest
    ways to load a dataset is based on data being organized into folders, with separate
    folders for each class. Then, we can load the dataset with the `tf.keras.utils.text_dataset_from_directory()`
    function, which we used several times in previous chapters, and see the class
    names. This is shown in the following code:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can then count the number of files in each class and display them in a bar
    graph with this code, using the `matplotlib` and `seaborn` graphics libraries:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'While this code prints out the count of texts in each class as text output,
    it is also very helpful to see the totals as a bar graph. We can use the graphics
    libraries to create this graph:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.1 – Coarse-grained class counts in the TREC data](img/B19005_14_01.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
- en: Figure 14.1 – Coarse-grained class counts in the TREC data
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: As *Figure 14**.1* shows, the `DESC` class is much smaller than the others,
    and it is possible that there will be accuracy problems with this class. There
    are ways to address this situation, which is one of the main topics of this chapter,
    but for now, we won’t make any changes until we see that this actually causes
    a problem.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: Doing initial evaluations
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Once we have done this initial exploration, we will want to try training one
    or more initial models for the data and evaluate them using some of the techniques
    we learned in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: 'For this exploration, we will use the BERT-based training process that was
    covered in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), so we won’t duplicate
    that here. However, there are a few changes in the model that we need to make
    because we are now working with a *categorical* classification problem (six classes),
    rather than a binary classification problem (two classes), and it is worth pointing
    these out. We can see the new model definition in the following code:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The two changes that are needed in the model definition for the categorical
    task are in the final layer, which has six outputs, corresponding to the six classes,
    and a softmax activation function, as opposed to the sigmoid activation function
    that we used for binary problems.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
- en: 'The other changes that are needed for categorical data are changes in the loss
    function and the metrics, as shown in the following code:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Here, we will define the categorical loss and metrics functions. Other metrics
    are available, but we will just look at accuracy here.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
- en: After training the model, as we did in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226),
    we can look at the final scores. If the model does not meet the overall performance
    expectations for the application using the metrics that have been chosen, you
    can try different hyperparameter settings, or you can try other models. This was
    the process we followed in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226), where
    we compared the performance of three different models on the movie review data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
- en: Keeping in mind that larger models are likely to have better performance, you
    can try increasing the size of the models. There is a limit to this strategy –
    at some point, the larger models will become very slow and unwieldy. You might
    also see that the payoff from larger and larger models becomes smaller, and performance
    levels off. This probably means that increasing the size of the models will not
    solve the problem.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: There are many different possibilities to look at different hyperparameter settings.
    This is, in general, a huge search space that can’t be fully explored, but there
    are some heuristics that you can use to find settings that could improve your
    results. Looking at the training history charts of loss and accuracy changes over
    epochs should give you a good idea of whether additional training epochs are likely
    to be helpful. Different batch sizes, learning rates, optimizers, and dropout
    layers can also be explored.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Another strategy to diagnose system performance is to look at the data itself.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: One initial evaluation we can do is a more fine-grained check for weak classes,
    by looking at the probabilities of the classifications for a large number of items
    in the dataset. We will look at this in the next section.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Checking for weak classes
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Low probabilities for a class of items are a sign that a system is not able
    to classify items with high confidence and has a good chance of making errors.
    To check for this, we can use the model to predict the classification of a subset
    of our data and look at the average scores, as shown in the following code:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This code goes through a subset of the TREC training data, predicts each item’s
    class, saves the predicted class in the `classification` variable, and then adds
    it to the `scores` list for the predicted class.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
- en: 'The final step in the code is to iterate through the scores list and print
    the length and average score for each class. The results are shown in *Table 14.1*:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
- en: '| **Class** | **Number** **of items** | **Average score** |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
- en: '| `ABBR` | 792 | 0.9070532 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
- en: '| `DESC` | 39 | 0.8191106 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
- en: '| `HUM` | 794 | 0.8899161 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
- en: '| `ENTY` | 767 | 0.9638871 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
- en: '| `LOC` | 584 | 0.9767452 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
- en: '| `NUM` | 544 | 0.9651737 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
- en: Table 14.1 – The number of items and the average score for each class
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see from *Table 14.1* that the number of items and average probabilities
    of the class predictions vary quite a bit. As you will recall from the counts
    we did in *Figure 14**.1*, we were already concerned about the `DESC` class because
    it was so small relative to the other classes. We can investigate this a bit further
    by looking at the predicted classifications of the individual items in each class
    with the following code:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Let’s look at the histograms for the `DESC` and `LOC` classes, which are at
    the extreme ends of the set of average scores. The `LOC` class is shown in *Figure
    14**.2*:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.2 – The distribution of probability scores for the LOC class](img/B19005_14_2.jpg)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
- en: Figure 14.2 – The distribution of probability scores for the LOC class
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
- en: We can see in *Figure 14**.2* that not only is the average probability very
    high (which we saw in *Table 14.1* as well) but there also are very few probabilities
    under `LOC` class. This class is likely to be very accurate in the deployed application.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
- en: There is a second, less obvious advantage to classes that show the pattern in
    *Figure 14**.2*. In a deployed interactive application, we don’t want a system
    to give users answers that it’s not very confident of. This is because they’re
    more likely to be wrong, which would mislead the users. For that reason, developers
    should define a *threshold* probability score, which an answer has to exceed before
    the system provides that answer to the user.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
- en: If the probability is lower than the threshold, the system should respond to
    the user that it doesn’t know the answer. The value of the threshold has to be
    set by the developer, based on the trade-off between the risk of giving users
    wrong answers and the risk of annoying users by saying *I don’t know* too frequently.
    *In* *Figure 14**.2* we can see that if we set the threshold to **0.9**, the system
    will not have to say *I don’t know* very often, which will improve user satisfaction
    with the system.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s contrast *Figure 14**.2* with a histogram for the `DESC` class, which
    we can see in *Figure 14**.3*:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.3 – The distribution of probability scores for the DESC class](img/B19005_14_03.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
- en: Figure 14.3 – The distribution of probability scores for the DESC class
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14**.3* shows many probability scores less than `DESC` class will be
    problematic in deployment.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
- en: 'A confusion matrix, such as the one we reviewed in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226),
    can also help detect underperforming classes. We can generate a confusion matrix
    for the TREC data with the following code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This code generates the predicted classes from the test data (represented in
    the `predicted_classes` variable) and compares them to the true classes (represented
    in the `y-test` variable). We can use the scikit-learn `confusion_matrix` function
    to display the confusion matrix as follows:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We can see the resulting confusion matrix in *Figure 14**.4*. The confusion
    matrix tells us how often each class was predicted to be each other class, including
    itself:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.4 – The confusion matrix for the TREC test set](img/B19005_14_04.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
- en: Figure 14.4 – The confusion matrix for the TREC test set
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
- en: The correct predictions can be seen on the main diagonal. For example, `ABBR`
    was correctly predicted as `ABBR` *137* times. We can also see the prediction
    errors for each class. The most frequent error was incorrectly classifying `ENTY`
    as `ABBR` *11* times. In this particular example, we don’t see a lot of evidence
    that specific classes get confused with each other, although there is a tendency
    for `ENTY` to be confused with `ABBR`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, we can look at the classification report to see the `precision`, `recall`,
    and `F1` scores for each class, as well as the overall averages for the entire
    test set. The recall scores in the classification report for `DESC` and `ENTY`
    are somewhat lower than the other recall scores, which reflects the fact that
    some of the items in those classes are incorrectly recognized as `ABBR`:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It’s worth pointing out at this point that the decision of whether the system
    is *good enough* really depends on the application and the developer’s decision.
    In some applications, it’s better to give the user some result, even if it might
    be wrong, while in other applications, it’s important for every result to be correct,
    even if the system has to say *I don’t know* almost all the time. Going back to
    the ideas of precision and recall that we covered in [*Chapter 13*](B19005_13.xhtml#_idTextAnchor226),
    another way of putting this is to say that in some applications, recall is more
    important, and in other cases, precision is more important.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
- en: If we want to improve the performance of the TREC application, the next step
    is to decide how to address our performance concerns and improve overall accuracy.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
- en: Fixing accuracy problems
  id: totrans-98
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will look at fixing performance problems through two strategies.
    The first one involves issues that can be addressed by changing data, and the
    second strategy involves issues that require restructuring the application. Generally,
    changing the data is easier, and it is a better strategy if it is important to
    keep the structure of the application the same – that is, we don’t want to remove
    classes or introduce new classes. We’ll start by discussing changing the data
    and then discuss restructuring the application.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一节中，我们将通过两种策略来解决性能问题。第一种策略涉及通过更改数据来解决的问题，第二种策略则是涉及需要重构应用程序的问题。通常来说，更改数据较为容易，而且如果保持应用程序结构不变很重要——即我们不想删除类或引入新类——那么这是一种更好的策略。我们将先讨论更改数据，然后再讨论重构应用程序。
- en: Changing data
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 更改数据
- en: Changing data can greatly improve the performance of your system; however, you
    won’t always have this option. For example, you might not have control over the
    dataset if you work with a standard dataset that you intend to compare to other
    researchers’ work. You can’t change the data if you are in that situation because
    if you do, your system’s performance won’t be comparable to that of other researchers.
    If your system’s performance isn’t satisfactory but you can’t change the data,
    the only options are to improve the algorithms by using a different model or adjusting
    the hyperparameters.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 更改数据可以大大提升系统性能；然而，你并不总是能有这个选择。例如，如果你使用的是标准数据集并打算将其与其他研究者的工作进行比较，那么你可能无法控制数据集。在这种情况下，你不能更改数据，因为这样一来，你的系统性能就无法与其他研究者的成果进行比较。如果你的系统性能不令人满意，但又无法更改数据，那么唯一的选择就是通过使用不同的模型或调整超参数来改进算法。
- en: On the other hand, if you work on an application where you do have control over
    a dataset, changing data can be a very effective way to improve your system.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你处理的是可以控制的数据集的应用，那么更改数据可以是提升系统性能的一个非常有效的方法。
- en: Many performance issues are the result of not having enough data, either overall,
    or in specific classes. Other performance issues can be due to annotation errors
    . We’ll start with a brief discussion of annotation errors.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 许多性能问题是由于数据不足，无论是整体数据不足还是某些特定类别的数据不足。其他性能问题则可能由于标注错误导致。我们将先简要讨论标注错误。
- en: Annotation errors
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 标注错误
- en: It is possible that the poor performance of systems in supervised learning applications
    is due to annotation errors. Another way of putting this is to say that the supervision
    of data was wrong, and the system was trained to do the wrong thing. Perhaps an
    annotator accidentally assigned some data to the wrong class. If the data is training
    data, data in the wrong class will make the model less accurate, or if the data
    is test data, the item would be scored incorrectly because the model was wrong.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 系统在监督学习应用中性能不佳，可能是由于标注错误。换句话说，数据的监督是错误的，系统被训练去做错误的事情。也许标注员不小心将某些数据分配到了错误的类别。如果这些数据是训练数据，那么错误类别的数据会使得模型的准确性降低；如果数据是测试数据，模型会因错误而给出不正确的评分。
- en: Checking for occasional annotation errors by reviewing the annotation of every
    item in the dataset can be very time-consuming, and it is not likely to improve
    the system much. This is because if the dataset is large enough, this kind of
    sporadic error is unlikely to have much of an impact on the quality of the overall
    system. However, if you suspect that annotation errors are causing problems, a
    simple check for low-confidence items can be helpful without requiring every annotation
    to be checked. This can be done by using a variation of the code we used in the
    *Checking for weak classes* section to check for weak classes. In that code, we
    predicted the class of each item in the dataset, kept track of its probabilities
    (scores), and averaged the probabilities of all the items in the class. To modify
    the code to look instead for individual items with low probabilities, you could
    record each item and its probability individually, and then look for low-probability
    items in the final list. You are encouraged to try this exercise for yourself.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看数据集中的每个项目的标注来检查偶尔出现的标注错误可能非常耗时，而且不太可能对系统的改进产生太大影响。这是因为如果数据集足够大，这种零星的错误不太可能对整体系统的质量产生显著影响。然而，如果你怀疑标注错误正在导致问题，检查低置信度项的简单方法可能会有所帮助，而不需要检查每个标注。你可以使用我们在*检查弱类别*部分使用的代码变体来检查弱类别。在那段代码中，我们预测了数据集中每个项的类别，跟踪了其概率（得分），并计算了该类别中所有项的概率平均值。要修改代码以查找低概率的单个项，你可以单独记录每个项目及其概率，然后在最终列表中查找低概率项。鼓励你自己尝试这个练习。
- en: On the other hand, it is also possible that data contains not only occasional
    mistakes but also systematic annotation errors. Systematic errors might be due
    to differences in the annotators’ understanding of the meanings of the classes,
    leading to the similar items being assigned to different classes by different
    annotators. Ideally, these kinds of errors can be avoided, or at least reduced,
    by preparing clear annotation guidelines for annotators before the annotation
    process begins, or even by giving them training classes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，数据中也可能不仅包含偶尔的错误，还包括系统性的标注错误。系统性错误可能是由于标注者对类别含义的理解存在差异，导致不同标注者将相似的项分配到不同的类别。理想情况下，通过在标注过程开始之前为标注者准备明确的标注指南，或者通过为他们提供培训课程，可以避免或至少减少这类错误。
- en: Tools such as the *kappa* statistic, which was mentioned in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107),
    can measure divergent annotations among annotators. If the kappa statistic shows
    that there is a lot of divergence across annotators, some of the data might need
    to be re-annotated using clarified guidelines. It can also happen that it is impossible
    to get annotators to agree because the decisions that the annotators have to make
    are inherently too subjective for people to agree on, no matter how much guidance
    they are given. This is a sign that the problem is not really suitable for NLU
    in the first place because there might not be a real correct classification for
    this data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 像*kappa*统计量这样的工具，可以衡量标注者之间的标注差异。如果kappa统计量显示标注者之间存在较大差异，则可能需要重新标注某些数据，并使用明确的指南进行标注。也可能发生无法让标注者达成一致的情况，因为标注者必须做出的决策本质上过于主观，无论提供多少指导，都难以达成一致。这表明问题本身可能不适合自然语言理解（NLU），因为此数据可能没有一个真正正确的分类。
- en: However, assuming that we do have a problem with objective classifications,
    in addition to addressing annotation errors, we can also improve system performance
    by creating a more balanced dataset. To do this, we will first look at adding
    and removing existing data from classes.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，假设我们确实存在客观分类问题，除了处理标注错误外，我们还可以通过创建更平衡的数据集来提高系统性能。为此，我们将首先考虑从各个类别中添加和移除现有数据。
- en: Adding and removing existing data from classes
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 从类别中添加和移除现有数据
- en: Unbalanced amounts of data in different classes are a common situation that
    can lead to poor model performance. The main reason that a dataset can be unbalanced
    is that this imbalance represents the actual situation in the application domain.
    For example, an application that is supposed to detect online hate speech will
    most likely encounter many more examples of non-hate speech than actual hate speech,
    but it is nevertheless important to find instances of hate speech, even if they
    are rare. Another example of a naturally unbalanced dataset would be a banking
    application where we find many more utterances about checking account balances
    than utterances about changing account addresses. Changing the address on an account
    just doesn’t happen very often compared to checking balances.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to make the sizes of the classes more even.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: 'Two common approaches are to duplicate data in the smaller classes or remove
    data from the larger classes. Adding data is called **oversampling** and removing
    data is called **undersampling**. The obvious approach to oversampling is to randomly
    copy some of the data instances and add them to the training data. Similarly,
    you can undersample by randomly removing instances from the classes that are too
    large. There are also other more sophisticated approaches to undersampling and
    oversampling, and you can find many online discussions about these topics – here,
    for example: [https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data](https://www.kaggle.com/code/residentmario/undersampling-and-oversampling-imbalanced-data).
    However, we will not review these here because they can become quite complex.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: Undersampling and oversampling can be helpful, but you should understand that
    they have to be used thoughtfully. For example, in the TREC dataset, trying to
    undersample the five frequent classes so that they have no more instances than
    the `DESC` class would require throwing out hundreds of instances from the larger
    classes, along with the information that they contain. Similarly, oversampling
    a small class such as `DESC` so that it contains the same number of instances
    as the larger classes means that there will be many duplicate instances of the
    `DESC` texts. This could result in overfitting the examples in `DESC` and consequently
    make it hard for the model to generalize to new test data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: It is easy to see that while undersampling and oversampling can potentially
    be useful, they are not automatic solutions. They are probably most helpful when
    classes are not extremely different in size and where there are plenty of examples,
    even in the smallest classes. You should also keep in mind that the classes don’t
    have to be exactly balanced for a system to perform well.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: Another approach to adding data is to create new data, which we will discuss
    in the next section.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Generating new data
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'If your dataset has underrepresented classes, or is too small overall, you
    can also add generated data to the entire dataset or just to the smaller classes.
    We will look at the following three ways to do this:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Generating new data from rules
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating new data from LLMs
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using crowdworkers to get new data
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generating new data from rules
  id: totrans-122
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One way to create new data is to write rules to generate new examples of data,
    based on the data that you already have. The `restaurant search` class. You could
    write a `parse` library:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Note that the rules in an NLTK CFG can be any context-free rules; they don’t
    have to correspond to actual linguistic categories. For example, we could have
    called the last Adj_Cuisine instead. We might want to do this if we want to be
    able to generate sentences with other adjectives, such as `good` or `low-priced`.
    The rule names and the rules themselves don’t matter to the NLTK CFG package;
    the only thing that matters is that the CFG is written in the syntax that the
    NLTK CFG package expects. The names and the rules can be any rules that you find
    convenient to generate new examples.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: 'The last two lines in the preceding code will generate 10 examples of sentences
    from this grammar, with the following result:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: If you want to generate all of the possible sentences from these rules, you
    will leave out the parameter, `n=10`.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: This is a fast way to generate a lot of sentences, but as you can see, the sentences
    are quite repetitious. This is because the NLTK `generate` method will produce
    every possible sentence that the grammar covers. Adding a lot of repetitious sentences
    to your training set could skew the model to these kinds of sentences, which in
    turn might make it harder for the model to recognize more varied restaurant search
    sentences. One approach to getting a wider variety of sentences from an NLTK CFG
    would be to write a broader grammar, generate all the sentences it covers, and
    then randomly select a subset of the generated sentences to add to the training
    set.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Using **large language models** (**LLMs**) to generate new examples is another
    useful and easy option, which we will discuss in the following section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Generating new data from LLMs
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Online LLMs such as ChatGPT are another very good way to get more training
    data because you can simply ask them to generate the appropriate training data.
    For example, let’s say ChatGPT was given the following prompt:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '*generate 20 requests to find local restaurants of different cuisines and*
    *price ranges*'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: 'ChatGPT ([chat.openai.com/chat](https://chat.openai.com/chat)) would produce
    the following answer:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.5 – The ChatGPT-generated restaurant query data](img/B19005_14_05.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
- en: Figure 14.5 – The ChatGPT-generated restaurant query data
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: (For brevity, not all the results are shown.)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that these sentences are much less repetitious than the ones from
    NLTK’s `generate` method. In the initial ChatGPT prompt, you can also restrict
    the question style – for example, you could ask for generated questions in an
    informal style. This results in informal sentences such as the following:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.6 – ChatGPT-generated restaurant query data in an informal style](img/B19005_14_06.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
- en: Figure 14.6 – ChatGPT-generated restaurant query data in an informal style
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: You can also control the variation among the responses by changing the *temperature*
    parameter, which is available in the ChatGPT API. Temperature settings vary between
    zero and two. A temperature setting of zero means that the responses will be less
    varied, and a higher setting means that they will be more varied.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Within ChatGPT, a low temperature means that the generation model chooses the
    next word for a response among the higher probability words, and a high temperature
    means that the model will select the next word from among the words with lower
    probabilities. The result with a higher temperature setting will include more
    varied responses, but some of them might not make sense. *Figure 14**.7* shows
    how to set the temperature in code directly from the API.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 14.7 – Setting the GPT temperature using the OpenAI API](img/B19005_14_07.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
- en: Figure 14.7 – Setting the GPT temperature using the OpenAI API
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: The code in *Figure 14**.7* sets the value of the `temperature` parameter to
    `1.5`, which results in a fairly diverse set of responses. You can see these at
    the bottom of *Figure 14**.7*. The code also sets the `model` parameter to use
    the `gpt-3.5-turbo` model and sets the message to send in the `messages` parameter.
    If you are interested in experimenting with other GPT API calls, you can find
    other API parameters in the OpenAI API documentation at [https://platform.openai.com/docs/api-reference](https://platform.openai.com/docs/api-reference).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Note that you will need to set the `openai.api_key` variable at line 3 to your
    own OpenAI user key to run this code, since the OpenAI API is a paid service.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: If you use an LLM to generate data, be sure to check the results and decide
    whether the responses represent the kind of examples that your users would really
    say and, hence, should be included in your training data. For example, some of
    the informal requests in *Figure 14**.6* might be more informal than many users
    would say to a chatbot.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: A final way to add new data to underrepresented classes is to hire crowdworkers
    to create more data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: Using crowdworkers to get new data
  id: totrans-149
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Getting more data from crowdworkers is time-consuming and possibly expensive,
    depending on how much data you need and how complicated it is. Nevertheless, it
    would be an option if you didn't get enough data using other methods.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: Of course, all of these methods we have outlined in this section (using rules,
    using LLMs, and using crowdworkers) can be combined – not all of the new training
    data has to come from the same place.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Another approach similar to changing the data is to change the application itself,
    which we will discuss in the next section.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: Restructuring an application
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In some cases, the best solution to classes that are not being predicted well
    is to restructure an application. As in the case of changing the data, you won’t
    always have the option to do this if this is a standard dataset that the research
    community uses to compare work among different labs, as the application has to
    have the same structure as that used by other researchers for the results to be
    comparable.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: If you do have control over the application structure, you can add, remove,
    or combine classes that don’t perform well. This can greatly improve the overall
    application performance. Let’s start by looking at an artificial example of an
    application that needs restructuring, and the different ways that this restructuring
    might be done.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the need for class restructuring
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Visualizing datasets can often provide immediate insight into potential performance
    problems. We can visualize how similar classes in a dataset are to each other
    in a couple of ways.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: First, confusion matrices such as the one in *Figure 14**.4* are a good source
    of information about which classes are similar to each other and consequently
    get confused for each other. We saw immediately from *Figure 14**.4* that `ENTY`
    and `DESC` were quite often confused with `ABBR`. We might want to add data to
    those classes, as discussed in the previous section, or we could also consider
    restructuring the application, which we will discuss next.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: A second visualization technique is to use the topic modeling techniques that
    we saw in [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217), to see problems with
    the application structure.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 14**.8* shows how an artificially constructed dataset of four classes
    might look if we clustered them based on the [*Chapter 12*](B19005_12.xhtml#_idTextAnchor217)
    tools, **Sentence Bert** and **BERTopic**. We can immediately see that there are
    some problems with the classes in this dataset.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 14.\uFEFF8 – Unsupervised clustering of four classes with artificially\
    \ generated data](img/B19005_14_08.jpg)"
  id: totrans-161
  prefs: []
  type: TYPE_IMG
- en: Figure 14.8 – Unsupervised clustering of four classes with artificially generated
    data
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: First of all, the instances of **Class 1**, represented by circles, seem to
    cluster into two different classes, one centered around the point (*0.5, 0.5*)
    and the other centered around the point (0*.5*, *1.75*). It seems unlikely that
    these clusters should both be grouped into the same class if they are actually
    that different. **Class 1** should probably be split, and the instances currently
    assigned to Class 1 should be assigned to at least two, and possibly three, new
    classes.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
- en: '**Class 2**, represented by squares, and **Class 3**, represented by triangles,
    seem problematic. They are not completely mixed together, but they are not completely
    separate either. Some of the instances of both classes are likely to be misclassified
    because of their similarity to the other class. If you see classes such as **Class
    3** and **Class 4**, with this kind of overlap, consider merging the classes if
    they appear to be similar in meaning (if they aren’t similar in meaning, consider
    adding more data to one or both classes). Finally, **Class 4**, represented by
    stars, is compact and doesn’t overlap with any other classes. It shouldn’t require
    any adjustments.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: Let’s now take a look at three restructuring options – merging classes, dividing
    classes, and introducing new classes.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
- en: Merging classes
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Classes that are much smaller than the rest of the classes can be merged with
    other semantically similar classes, especially if they are frequently confused
    with those classes. This can be a good strategy because, in many real applications,
    there are classes that simply don’t occur very often, but unlike the hate speech
    example mentioned earlier, it isn’t always critical to be able to tell the difference
    between the original classes. Of course, this is only possible with a multi-class
    problem – that is, a problem with more than two classes – since merging the classes
    in a binary (two-class) problem will put everything in one class and leave us
    with nothing to classify.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
- en: In some cases, merging classes can be accomplished by adding all the data from
    one class to the data of the other class, which is the simplest restructuring
    that we can do. A slightly more complex merger of classes can be done if the new
    structure is more complicated – for example, if it involves adding slots. For
    example, it is possible that classes such as **Class 2** and **Class 3** in *Figure
    14**.8* are actually not different enough to be worth trying to separate.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: As an example, suppose we work on a generic personal assistant application,
    with classes such as `play music`, `find a restaurant`, `get weather forecast`,
    `find a bookstore`, and `find a bank`. It might turn out that `find a restaurant`
    has much more data than `find a bookstore`, and as a result, `find a bookstore`
    is often confused with `find a restaurant`. In that case, it would be worth considering
    whether all the `find a` classes should be merged into one larger class. This
    class could be called `local business search`, with `bookstore`, `restaurant`,
    and `bank` being treated as slots, as discussed in [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Another strategy is to separate classes such as **Class 1** in *Figure 14**.5*
    into two different classes. The next section discusses dividing classes.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Dividing classes
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Sometimes it makes sense to divide a large class into separate smaller classes
    if there appear to be systematic differences between two or more groups within
    the class. Tools such as BERTopic can help suggest names for new classes if the
    new name isn’t obvious from looking at the instances in each group. Unfortunately,
    dividing a class into new classes isn’t as easy as merging classes because the
    examples in the new classes will need to be annotated with their new names. Although
    re-annotation is more work, dividing and re-annotating classes is necessary if
    you have to divide a large class into more meaningful new classes.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: Introducing an "other" class
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Introducing an `other` class is a variant of the strategy of merging classes.
    If there are several small classes that don’t really have enough training data
    to be reliably classified, it can sometimes be useful to group them together in
    an `other` class – that is, a class that contains items that don’t fit into any
    of the other classes. One type of application that this approach can be useful
    for is call routing in a telephone self-service application.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: In these applications, there can sometimes be hundreds of destinations where
    a call can be routed. In nearly every application of this kind, there are some
    infrequent classes for which there is much less data than other classes. Sometimes,
    it is best to not try to identify these classes because trying to do so accurately
    will be difficult with the small amounts of data available. A better strategy
    would be to group them together into an `other` class. It still might be hard
    to identify items in the `other` category because the items it contains will not
    be very similar, but it will keep them from interfering with the overall application
    accuracy. How items in the `other` class are handled depends on the specific application’s
    goals, but options include handling them manually (for example, with a human call
    center agent) or simply telling users that the system can’t handle their question.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: After the accuracy issues that were identified during initial development have
    been identified and addressed, it is time to deploy the system.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Moving on to deployment
  id: totrans-177
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: If we’ve fixed the performance issues we’ve discussed so far, we will have trained
    a model that meets our performance expectations, and we can move on to deployment,
    when the system is installed and does the task that it was designed for. Like
    any software, a deployed NLU model can have problems with system and hardware
    issues, such as network issues, scalability, and general software problems. We
    won’t discuss these kinds of problems because they aren’t specific to NLU.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: The next section will cover considerations to address NLU performance problems
    that occur after deployment.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: Problems after deployment
  id: totrans-180
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After an NLU system is developed and put into place in an application, it still
    requires monitoring. Once the system has reached an acceptable level of performance
    and has been deployed, it can be tempting to leave it alone and assume that it
    doesn’t need any more attention, but this is not the case. At the very least,
    the deployed system will receive a continuous stream of new data that can be challenging
    to the existing system if it is different from the training data in some way.
    On the other hand, if it is not different, it can be used as new training data.
    Clearly, it is better to detect performance problems from internal testing than
    to learn about them from negative customer feedback.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: At a high level, we can think of new performance problems as either being due
    to a change in the system itself, or due to a change in the deployment context.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Changes in system performance due to system changes should be detected by testing
    before the new system is deployed. This kind of testing is very similar to the
    kind of testing that has to be done for any software deployment, so we won’t cover
    it in any detail. Degradation in performance can be detected by versioning the
    system and running an evaluation with a fixed set of data and metrics after every
    change. This is useful to both detect decreases in performance but also to document
    improvements in performance.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: As with any machine-learning-based system, new data can cause problems with
    an NLU system because it is different in some significant way from the training
    data. These kinds of differences are frequently due to changes in the deployment
    context.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: What do we mean by changes in the **deployment context**? The deployment context
    refers to everything about the application, except for the NLU system itself.
    Specifically, it can include the users, their demographics, their geographical
    locations, the backend information that’s being provided, and even events in the
    world such as weather. Any of these can change the characteristics of texts that
    the application processes. These changes alter the correspondence between the
    training data and the new data being processed, which will lead to a decrease
    in performance.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Some changes in the deployment context can be predicted. For example, if a company
    introduces a new product, this will introduce new vocabulary that a customer support
    chatbot, voice assistant, or email router needs to recognize, since, after all,
    we expect customers to be talking about it. It is a best practice to perform an
    evaluation on new data after changes like the introduction of a new product occurs,
    and decide whether the system should be retrained with additional data.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, some changes can’t be predicted – for example, the COVID-19
    pandemic introduced a lot of new vocabulary and concepts that medical or public
    health NLU applications needed to be trained on. Because some deployment context
    changes can’t be predicted, it is a good idea to periodically perform an evaluation
    using new data coming in from the deployment.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, you have learned about a number of important strategies to
    improve the performance of NLU applications. You first learned how to do an initial
    survey of the data and identify possible problems with the training data. Then,
    you learned how to find and diagnose problems with accuracy. We then described
    different strategies to improve performance – specifically, adding data and restructuring
    the application. The final topic we covered was a review of problems that can
    occur in deployed applications and how they can be addressed.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: In the final chapter, we will provide an overview of the book and a look to
    the future. We will discuss where there is potential for improvement in the state
    of the art of NLU performance, as well as faster training, more challenging applications,
    and what we can expect from NLU technology as the new LLMs become more widely
    used.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
