- en: '2'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Introducing Foundations of Vector Representation
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Vectors** and **vector representation** are at the very core of neural search
    since the quality of vectors determines the quality of search results. In this
    chapter, you will learn about the concept of **vectors** within **machine learning**
    (**ML**). You will see common search algorithms using vector representation as
    well as their weaknesses and strengths.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We’re going to cover the following main topics in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introducing vectors in ML
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Measuring the similarity between two vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Local and distributed representations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a solid understanding of how every
    type of data can be represented in vectors and why this concept is at the very
    core of neural search.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter has the following technical requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: A laptop with a minimum of 4 GB RAM (8 GB or more is preferred)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python installed with version 3.7, 3.8, or 3.9 on a Unix-like operating system,
    such as macOS or Ubuntu
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The code for this chapter can be found at [https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter02](https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter02).
  prefs: []
  type: TYPE_NORMAL
- en: Introducing vectors in ML
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Text is an important means of recording human knowledge. As of June 2021, the
    number of web pages indexed by mainstream search engines such as Google and Bing
    has reached 2.4 billion, and the majority of information is stored as text. How
    to store this textual information, and even how to efficiently retrieve the required
    information from the repository, has become a major issue in information retrieval.
    The first step in solving these problems lies in representing text in a format
    that is *comprehensible* to computers.
  prefs: []
  type: TYPE_NORMAL
- en: As network-based information has become increasingly diverse, in addition to
    text, web pages contain a large amount of multimedia information, such as pictures,
    music, and video files. These files are more diverse than text in terms of form
    and content and satisfy users’ needs from different perspectives. How to represent
    and retrieve these types of information, as well as how to pinpoint the multimodal
    information needed by users from the vast mass of data available on the internet
    is also an important factor to be considered in the design of search engines.
    To achieve this, we need to represent each document as its vector representation.
  prefs: []
  type: TYPE_NORMAL
- en: 'A *vector* is an object that has both a magnitude and a direction, as you may
    remember learning in school. If we can represent our data using vector representation,
    then we’re able to use the angle to measure the similarity of two pieces of information.
    To be more concrete, we can say the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Two pieces of information are represented as vectors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both vectors start from the origin [*0, 0*] (assuming two dimensions)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Two vectors form an angle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Figure 2.1* illustrates the relationship between two vectors with respect
    to their angle:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – An example of vector representation ](img/Figure_2.1_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.1 – An example of vector representation
  prefs: []
  type: TYPE_NORMAL
- en: '**vec1** and **vec2** have the same direction but different lengths. **vec2**
    and **vec3** have the same lengths but point in opposite directions. If the angle
    is 0 degrees, the two vectors are identical. If the vector is 180 degrees, the
    two vectors are completely opposite. We can measure the similarity between two
    vectors by the angle: the smaller the angle, the closer the vectors are. This
    method is also called **cosine similarity**.'
  prefs: []
  type: TYPE_NORMAL
- en: In reality, cosine similarity is one of the most commonly used similarity measurements
    to determine the similarity between two vectors, but not the only one. We’ll dive
    into it in more detail, as well as other similarity metrics, in the *Measuring
    the similarity between two vectors* section. Before that, you might be wondering
    how we can encode our raw information, such as text or audio, into a vector of
    numeric values. In this section, we’re going to do that.
  prefs: []
  type: TYPE_NORMAL
- en: We’ll dive into the details of cosine similarity using *Python* and the *NumPy
    library*. As well as that, we will introduce other similarity metrics and briefly
    cover local and distributed vector representation in the following subsections.
  prefs: []
  type: TYPE_NORMAL
- en: Using vectors to represent data
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Let’s start with the most common scenario: **representing text information**.'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, let’s define the concept of a **feature vector**. Let’s say we
    want to build a search system for Wikipedia (in English). As of July 2022, English
    Wikipedia has over 6.5 million articles containing over 4 billion words (180,000
    unique words). We can call these unique words the Vocabulary of Wikipedia.
  prefs: []
  type: TYPE_NORMAL
- en: Each of the articles in this Wikipedia collection should be encoded into a series
    of numerical values; this is referred to as a feature vector. To this end, we
    can encode 6.5 million articles into 6.5 million indexed feature vectors, then
    use a similarity metric, such as cosine similarity, to measure the similarity
    between the encoded query feature vector and the indexed 6.5 million feature vectors.
  prefs: []
  type: TYPE_NORMAL
- en: The encoding process involves finding an optimal function to transform the original
    data into its vector representation. How can we achieve this goal?
  prefs: []
  type: TYPE_NORMAL
- en: 'Again, we start with the simplest method: using a **bit vector**. A bit vector
    means all the values inside the vector will be either 0 or 1, depending on the
    occurrence of the word. Let’s say we loop over all unique words in the Vocabulary;
    if the word occurs in this particular document, *d*, then we set the value of
    the location of this unique word to be 1, otherwise 0.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s refresh what we introduced in [*Chapter 1*](B17488_01.xhtml#_idTextAnchor014),
    *Neural Networks for Neural Search*, in the *How does the traditional search system
    work?* section, imagining we have two documents:'
  prefs: []
  type: TYPE_NORMAL
- en: '`doc1` = *Jina is a neural search framework*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc2` = *Jina is built with cutting edge technology called deep learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we merge these two documents, we have a Vocabulary (of unique words), as
    follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Imagining the preceding variable, `vocab`, is our Vocabulary, after preprocessing
    (tokenizing and stemming), we get a list of tokens, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note that the aforementioned Vocabulary has been sorted alphabetically.
  prefs: []
  type: TYPE_NORMAL
- en: 'To encode `doc1` into a vector representation, we loop through all the words
    inside the `doc1`, and create the bit vector:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding code block encodes `doc1` into a bit vector. In the `encode`
    function, we firstly created a Python list filled with 0s; the length of the list
    is identical to the size of Vocabulary. Then, we loop over the Vocabulary to check
    the occurrence of the word inside the document to encode. If present, we set the
    value of the encoded vector as `1`. In the end, we get this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we’ve successfully encoded a document into its bit vector representation.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: You might have noticed that in the preceding example, the output of the bit
    vector contains a lot of 0s values. In a real-world scenario, as the size of the
    Vocabulary gets much larger, and the dimensionality of the vector gets very high,
    there is a high chance that most of the dimensions in the encoded documents are
    filled with 0s, which is extremely inefficient to store and retrieve. This is
    also called a **sparse vector**. Some Python libraries, such as SciPy, have strong
    sparse vector support. Some deep learning libraries, such as TensorFlow and PyTorch,
    have built-in sparse tensor support. Meanwhile, Jina primitive data types support
    SciPy, TensorFlow, and PyTorch sparse representations.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned that a vector is an object that has both a magnitude
    and a direction. We also managed to create the simplest form of vector representation
    of two text documents using a bit vector. Now, it would be very interesting to
    know how similar these two documents are. Let us learn more about this in the
    next section.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring similarity between two vectors
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Measuring similarity between two vectors is important in a neural search system.
    Once all of the documents have been indexed into their vector representation,
    given a user query, we carry out the same encoding process to the query. In the
    end, we compare the encoded query vector against all the encoded document vectors
    to find out what the most similar documents are.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can continue our example from the previous section, trying to measure the
    similarity between `doc1` and `doc2`. First of all, we need to run the script
    two times to encode both `doc1` and `doc2`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we can produce a vector representation for both of them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the dimension of the encoded result is always identical to the size of
    Vocabulary, the problem has been converted to how to measure the similarity between
    two vector representations: `encoded_doc1` and `encoded_doc2`.'
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The aforementioned vector representation of `encoded_doc1` and `encoded_doc2`
    has a depth of 15\. It is easy for us to visualize 1D data as a point, 2D data
    as a line, or 3D data, but not for high-dimensional data. Practically, we might
    perform dimensionality reduction to reduce high-dimensional vectors to 3D or 2D
    in order to plot them. The most common technique is called **t-sne**.
  prefs: []
  type: TYPE_NORMAL
- en: 'Imaging two encoded vector representations can be plotted in a 2D vector space.
    We can visualize `encoded_doc1` and `encoded_doc2` as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – Cosine similarity ](img/Figure_2.2_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.2 – Cosine similarity
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, we can measure the similarity between `encoded_doc1` and `encoded_doc2`
    using their angles, specifically, the cosine similarity. The law of cosine tells
    us that:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_2.1_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s say *p* is represented as [x1, y1] and *q* is represented as [x2, y2];
    then, the aforementioned formula can be rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_2.2_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Since cosine similarity also works for high-dimensional data, the aforementioned
    formula can be again rewritten as:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_2.3_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Based on the formula, we can compute the cosine similarity between `encoded_doc1`
    and `encoded_doc2`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If we print out the result of the similarity between `encoded_doc1` and `encoded_doc2`,
    we get the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Here, we get the cosine similarity between two encoded vectors, roughly equal
    to *0.405*. In a search system, when the user submits a query, we will encode
    the query into its vector representation. We have encoded all the documents (that
    we want to search) into their vector representations individually offline. In
    this way, we can compute the similarity score of the query vector against all
    document vectors to produce the final ranking list.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: The preceding code illustrates how you can compute the cosine similarity. The
    code is not optimized. In reality, you should always use NumPy to perform vectorized
    operations over vectors (NumPy arrays) to achieve higher performance.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics beyond cosine similarity
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Through cosine similarity is the most commonly used similarity/distance metric,
    there are some other commonly used metrics as well. We will cover another two
    commonly used distance functions in this section, namely, **Euclidean distance**
    and **Manhattan distance**.
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: Similarity metrics measure how alike two documents are. On the other hand, distance
    metrics measure the dissimilarity between two documents. In the search scenario,
    you always want to get the top k matches against your query. So, if you are using
    similarity metrics, always get the first k items from the ranked list. On the
    other hand, while using distance metrics, always get the last k items from the
    ranked list or reverse the ranked list and get the first k items.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike cosine similarity, which takes the angle of two vectors as its similarity
    measure, the Euclidean distance takes the length of the line segment between two
    data points. For instance, consider two 2D docs in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Euclidean distance ](img/Figure_2.3_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.3 – Euclidean distance
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in *Figure 2.3*, previously, we used the angle between `vec1`
    and `vec2` to compute their cosine similarity. For Euclidean distance, we compute
    it in a different way. Both `vec1` and `vec2` have a starting point of 0 and the
    `p` and `q` endpoints, respectively. Now, the distance between these two vectors
    becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_2.4_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Another distance metric is called `p` at (p1, p2) and `q` at (q1, q2), the
    distance between these two vectors becomes:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_2.5_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'As can be seen in *Figure 2.4*, the hyperplane has been split into small blocks.
    Each block has a width of 1 and a height of 1\. The distance between `p` and `q`
    becomes 4:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Manhattan distance ](img/Figure_2.4_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 2.4 – Manhattan distance
  prefs: []
  type: TYPE_NORMAL
- en: 'There are many other distance metrics as well, such as the **Hamming distance**
    and **angular distance**, but we won’t cover each of them given the fact that
    cosine and Euclidean are the most commonly used similarity metrics. This, in turn,
    leads to an interesting question: which distance/similarity metric should I use
    to make vector similarity computation more effective? The answer is *it depends*.'
  prefs: []
  type: TYPE_NORMAL
- en: First of all, it depends on your task and your data. But, in general, when performing
    text retrieval and related tasks, cosine similarity will be your first choice.
    It has been widely adopted for applications such as measuring similarity between
    two pieces of encoded text documents.
  prefs: []
  type: TYPE_NORMAL
- en: 'The deep learning model might also impact your similarity/distance metric choice.
    For instance, if you applied metric learning techniques to fine-tune your ML model
    to optimize certain similarity metrics, then you might stick with the same similarity
    metric that you optimized. To be more specific, note the following:'
  prefs: []
  type: TYPE_NORMAL
- en: You can apply *Siamese neural networks* to optimize pairs of inputs (query and
    document) based on the Euclidean distance and get a new model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: When extracting features with the model, it’s better to use the *Euclidean distance*
    as the similarity measure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If your vectors have extremely high dimensions, it might be a good idea to switch
    from the Euclidean distance to the *Manhattan distance* since it delivers more
    robust results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: In application, different ANN libraries might use different distance metrics
    as default configuration. For instance, Annoy encourages users to use the angular
    distance to compute vector distances. It is a variation of the Euclidean distance.
    More about ANN will be introduced in [*Chapter 3*](B17488_03.xhtml#_idTextAnchor044),
    *System Design and Engineering Challenges*.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple ways to encode data into vector representations. Generally
    speaking, this can be classified into two forms: **local representation** and
    **distributed representation**. The aforementioned way of encoding data into vector
    representation can be classified into local representation since it treats each
    unique word as one dimension.'
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we’ll introduce the most important local representation
    and distributed representation algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Local and distributed representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we’ll dive into **local representations** and **distributed
    representations**. We will go through the characteristics of two different representations
    and list the most widely used local and global representations to encode different
    modalities of data.
  prefs: []
  type: TYPE_NORMAL
- en: Local vector representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As a classic method of text representation, **local representation** only makes
    use of the **disjointed dimensions** in the vector for a certain word when it
    is represented as a vector. Disjointed dimension means that each dimensionality
    of the vector represents a single token.
  prefs: []
  type: TYPE_NORMAL
- en: When only one dimension is used, it is called **one-hot representation**. *One-hot*
    means that the word is represented as a long vector, and the dimension of the
    vector is the total number of words to be represented. Most dimensions are 0,
    while only one dimension has a value of 1\. Different words with a dimension of
    1 are not used. If this method of representation is stored sparsely, that is,
    assigning a digital ID to each word based on the dimension of 1, it will be concise.
  prefs: []
  type: TYPE_NORMAL
- en: One-hot also means that no additional learning process is required under the
    assumption that all words are independent of each other. This maintains the orthogonality
    between vectors representing words and therefore has a strong discriminative ability.
    With maximum entropy, a support vector machine, conditional random field, and
    other ML algorithms, the one-hot representation has great effects on multiple
    aspects, such as text classification, text clustering, and part-of-speech tagging.
    For an application scenario of ad hoc retrieval where keyword matching plays a
    leading role, the bag-of-words model based on one-hot representation is still
    the mainstream choice.
  prefs: []
  type: TYPE_NORMAL
- en: However, one-hot representation ignores the semantic relationships between words.
    In addition, when representing a Vocabulary, **V**, that contains **N** words,
    the one-hot representation needs to construct a vector of dimension **N**. This
    leads to the problems of parameter explosion and data sparseness.
  prefs: []
  type: TYPE_NORMAL
- en: Another type of local representation is referred to as **bag-of-words**, or
    *bit vector representation*, which we introduced earlier in the chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'As a method of vector representation, the bag-of-words model regards the text
    as a collection of words, only documenting whether the words appear in the text
    or not but ignoring the word order and grammar in a body of text. Based on the
    one-hot representation of words, bag-of-words represents the text as a vector
    composed of 0s and 1s, and offers great support for bit operations. This method
    can conduct regular query processing in retrieval scenarios. Because it also maintains
    the orthogonality between words, it still works well for tasks such as text classification.
    Now, we will build a bit vector representation using a *Python ML framework* called
    **scikit-learn**:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'The output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on the bag-of-words (bit vector) model, the bag-of-words representation
    algorithm takes into account the frequency of words appearing in a body of text.
    Therefore, the bag-of-words encoded feature values corresponding to different
    words are no longer 0 or 1, but the frequency of such words appears in the body
    of text. Generally speaking, the more frequently a word appears in the text, the
    more important the word is to the text. To get the representation, you can simply
    put `binary=False` in the preceding implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As you can discover from the following output, the term frequency has been taken
    into consideration. For example, since the `neural` token occurred two times,
    the value of the encoded result has increased by `1:`
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: Last but not least, we have one of the most-used local representations, called
    **term frequency-inverse document frequency** (**tf-idf**) **representation**.
  prefs: []
  type: TYPE_NORMAL
- en: 'tf-idf is a common representation method for information retrieval and data
    mining. The TF-IDF value of word *i* in text j is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_2.6_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Here, *ni, j* denotes the frequency of word *i* appearing in the text *j*;
    *|d_j |* denotes the total number of words in the text; *|D|* indicates the number
    of tokens in the corpus, and ![](img/Formula_2.7_B17488.png) represents the number
    of documents containing the word *i*. By factoring in the frequency of words appearing
    in the text, the TF-IDF algorithm further considers the universal importance of
    the word in the entire body of text by calculating the IDF of the word. That is,
    the more frequently a word appears in the text, the less frequently it appears
    in other parts of the body of text. This shows that the more important the word
    is for the current text, the higher weight it will be given. The scikit-learn
    implementation of this algorithm is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The Tf-Idf weighted encoding result looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Up until now, we have introduced local vector representation. In the next section,
    we will dive deep into a distributed vector representation, why we need it, and
    the commonly used algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: Distributed vector representation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Although the local representation of texts has advantages in tasks such as text
    classification and data recall, it has the problem of data sparseness.
  prefs: []
  type: TYPE_NORMAL
- en: To be more specific, if a corpus has 100,000 distinct tokens, the dimensionality
    of the vector will become 100,000\. Suppose we have a document that contains 200
    tokens. In order to represent this document, only 200 entries of the vector out
    of 100,000 are non-zero. All other dimensions still get a 0 value since the tokens
    of the Vocabulary did not occur in the document.
  prefs: []
  type: TYPE_NORMAL
- en: This has posed great challenges to data storage and retrieval. Accordingly,
    a natural idea is to obtain a low-dimensional dense vector of the text, which
    is called a **distributed representation** of the text.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, the distributed representation of single modalities, such as
    text, images, and audio, is first described; then, the distributed representation
    method of multimodal joint learning is presented. We’ll also selectively introduce
    several important representation learning algorithms based on the modality of
    the data, that is, text, image, audio, and cross-modal representation learning.
    Let’s first look at text-based algorithms.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following table, we have listed some selected models to encode different
    modalities of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Model** | **Modality** | **Domain** | **Application** |'
  prefs: []
  type: TYPE_TB
- en: '| `BERT` | Text | Dense retrieval | Text-to-text search, question answering
    |'
  prefs: []
  type: TYPE_TB
- en: '| `VGGNet` | Image | Content-based image retrieval | Image-to-image search
    |'
  prefs: []
  type: TYPE_TB
- en: '| `ResNet` | Image | Content-based image retrieval | Image-to-image search
    |'
  prefs: []
  type: TYPE_TB
- en: '| `Wave2Vec` | Acoustic | Content-based audio retrieval | Audio-to-audio search
    |'
  prefs: []
  type: TYPE_TB
- en: '| `CLIP` | Text and image | Cross-modal retrieval | Text-to-image search |'
  prefs: []
  type: TYPE_TB
- en: Table 1.1 – Selected models that can be served as encoders for different modality
    of inputs
  prefs: []
  type: TYPE_NORMAL
- en: Text-based algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Because text carries important information, the distributed representation of
    texts serves as a major function of search engines and has been extensively studied
    in academic works and the industry. Given the fact that we have a huge amount
    of unlabeled text data (such as Wikipedia), when it comes to text-based algorithms,
    we normally employ unsupervised pretraining on a large corpus.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the belief that similar words have a similar context, Mikolov et al.
    proposed the *word2vec* algorithm, which includes two simple neural network models
    for learning: the **Continuous-Bag-of-Words** (**CBOW**) and **skip-gram** (**SG**)
    models.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the CBOW model is used to derive the representation of a word,
    ![](img/WT.png), using its surrounding words, such as two words before and two
    words after. For example, given a sentence in a Wikipedia document, we randomly
    mask out one token inside this sentence. We try to predict the masked token by
    its surrounding tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'In the preceding document, we masked the token search and tried to predict
    the vector representation of the masked token, *u*, by summing up the representation
    of surrounding tokens, ![](img/Formula_2.8_B17488.png), and conducting the dot
    product between *u* and ![](img/Formula_2.8_B174881.png). At training time, we’ll
    select a token, *y*, to maximize the dot product:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_2.10_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: It should be noted that before training, we will randomly initialize the vector
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'On the other hand, SG tries to predict the vector representations of the surrounding
    tokens from the current token. The difference between CBOW and SG is illustrated
    in *Figure 2.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – CBOW and SG (source: Efficient estimation of word representations
    in vector space) ](img/Figure_2.5_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 2.5 – CBOW and SG (source: Efficient estimation of word representations
    in vector space)'
  prefs: []
  type: TYPE_NORMAL
- en: Both models are used to learn the word representation by maximizing the log-likelihood
    of the objective function on the entire corpus. To alleviate the burden of numerous
    calculations caused by the softmax function at the output layer, Mikolov et al.
    created two optimization methods, namely, **hierarchical softmax** and **negative
    sampling**. Conventional deep neural networks predict each next word as a classification
    task. This network must have many output classes as unique tokens. For example,
    when predicting the next word in the English Wikipedia, the number of classes
    is over 160,000\. This is extremely inefficient. Hierarchical softmax and negative
    sampling replace the flat softmax layer with a hierarchical layer that has the
    words as leaves and convert the multiclass classification problem into a binary
    classification problem by classifying whether two tokens are a true pair (semantically
    similar) or a false pair (independent tokens). This greatly improves the prediction
    speed of word embeddings.
  prefs: []
  type: TYPE_NORMAL
- en: After pretraining, we can give a token to this word2vec model and get a so-called
    word embedding. This word embedding is represented by a vector. Some pretrained
    `word2vec` vectors are represented as 300D word vectors. The dimensionality is
    much smaller than the sparse vector space model we introduced before. So, we also
    refer to these vectors as dense vectors.
  prefs: []
  type: TYPE_NORMAL
- en: In algorithms such as *word2vec* and *GloVe*, the representation vector of a
    word generally remains unchanged after training and can be applied to downstream
    applications, such as named entity recognition.
  prefs: []
  type: TYPE_NORMAL
- en: However, the semantics of the same word in different contexts may vary or even
    have significantly different meanings. In 2019, Google announced **Bidirectional
    Encoder Representations from Transformers** (**BERT**), a transformer-based neural
    network for natural language processing. BERT uses a transformer network to represent
    the text and obtains the contextual information of the text through a masked language
    model. In addition, BERT also employs **next sentence prediction** (**NSP**) to
    strengthen the textual representation of relationships and has achieved good results
    for many textual representation tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to word2vec, BERT has been pretrained on the Wikipedia dataset and some
    other datasets, such as BookCorpus. They form a Vocabulary of above 3 billion
    tokens. BERT has also been trained in different languages, such as English and
    German, as well as on multilingual datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'BERT can be trained on a large amount of corpus without any annotations through
    the pretrain and fine-tune paradigm. During prediction, the text to be predicted
    is put in a well-trained network again to obtain a dynamic vector representation
    containing contextual information. During training, BERT replaces the words in
    the original text according to a certain ratio and uses the training model to
    make correct predictions. BERT will also add some special characters, such as
    `[CLS]` and `[SEP]`, to help the model correctly determine whether the two input
    sentences are continuous. Again, we have `doc1` and `doc2`, as follows; `doc2`
    is the next sentence of `doc1`:'
  prefs: []
  type: TYPE_NORMAL
- en: '`doc1` = *Jina is a neural search framework*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`doc2` = *Jina is built with cutting edge technology called deep learning*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'During pretraining, we consider two documents as two sentences, and represent
    documents as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After the text is input, BERT’s input consists of three types of vectors, that
    is, `[MASK]` token. According to the author of the BERT paper (*BERT: Pre-training
    of Deep Bidirectional Transformers for Language Understanding*), around 15% of
    tokens are masked out (Jacob et al.).'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure. 2.6 – BERT input representation. Each input embedding is the sum
    of three embeddings ](img/Figure_2.6_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure. 2.6 – BERT input representation. Each input embedding is the sum of
    three embeddings
  prefs: []
  type: TYPE_NORMAL
- en: At the pretraining time, since we use NSP as the training objective, around
    50% of the second sentences are the “true” next sentence, while another 50% of
    the sentences are randomly selected from the corpus, which means they’re not the
    sentence that follows on from the first sentence. This helps us provide positive
    pairs and negative pairs to improve model pretraining. The objective function
    of BERT is to correctly predict the masked token as well as whether the next sentence
    is the correct one.
  prefs: []
  type: TYPE_NORMAL
- en: As was mentioned before, after pretraining BERT, we can fine-tune the model
    for specific tasks. The author of the BERT paper fine-tuned a pretrained model
    on different downstream tasks, such as question answering and language understanding,
    and it achieved state-of-the-art performance on 11 downstream datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Vision-based algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: With the rapid development of the internet, information carriers on the internet
    are increasingly diversified and images provide a variety of visual features.
    Many researchers expect to encode images as vectors for representation. The most
    widely used model architecture for imagery analysis is called **convolutional
    neural network** (**CNN**).
  prefs: []
  type: TYPE_NORMAL
- en: A CNN receives an image of shape (`Height`, `Width`, `Num_Channels`) as input
    (normally, it’s a three-channel RGB image or a one-channel grayscale image). The
    image will be passing through one of multiple convolutional layers. This takes
    a kernel (or filter) and slides through the input, and the image becomes an abstracted
    activation map.
  prefs: []
  type: TYPE_NORMAL
- en: After one of multiple convolutional operations, the output of the activation
    map will be sent through a pooling layer. The pooling layer takes a small cluster
    of neurons in the feature map and applies max or mean operations in this cluster.
    This is referred to as max pooling and mean pooling. The pooling layer can significantly
    reduce the dimensionality of the feature map into a more compact representation.
  prefs: []
  type: TYPE_NORMAL
- en: Normally, a combination of one of multiple convolutional layers and one pooling
    layer is named a convolutional block. For example, three convolutional layers
    plus one pooling layer make a convolutional block. At the end of the convolutional
    block, we normally apply a flatten operation to get the vector representation
    of the image data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following screenshot, we demonstrate a beautifully designed CNN model
    named VGG16\. As can be seen, it consists of five convolutional blocks, each one
    containing two or three convolutional layers and a max pooling layer. At the end
    of these blocks, the activation map is flattened as a feature vector:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure. 2.7 – VGG16 consists of five convolutional blocks and produces classification
    results with a softmax classification head ](img/Figure_2.7_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure. 2.7 – VGG16 consists of five convolutional blocks and produces classification
    results with a softmax classification head
  prefs: []
  type: TYPE_NORMAL
- en: It is worth mentioning that VGG16 is designed for ImageNet classification. So,
    after the activation map is flattened as a feature vector, it is connected to
    two fully connected layers (dense layers) and a softmax classification head.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, we will remove the softmax classification head to turn this classification
    model into an embedding model. Given an input image, this embedding model produces
    a flattened feature map rather than the classified classes of the objects in the
    image. Besides, ResNet is a more complicated but frequently used vision feature
    extractor compared with VGGNet.
  prefs: []
  type: TYPE_NORMAL
- en: Apart from text and images, audio search is an important search application,
    for instance, to identify music from a short clip or search for music with a similar
    style. In the next section, we will list several deep learning models in this
    direction.
  prefs: []
  type: TYPE_NORMAL
- en: Acoustic-based algorithms
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Given a sequence of acoustic inputs, deep learning-powered algorithms have a
    huge impact on the acoustic domain. For instance, they have been widely used for
    text-to-speech tasks. Given a piece of music as query, finding similar (or the
    same) music is commonly used for music applications.
  prefs: []
  type: TYPE_NORMAL
- en: One of the latest state-of-the-art algorithms trained on audio data is called
    **wave2vec 2.0**. Similar to BERT, wave2vec is trained in an unsupervised fashion.
    Taking a piece of audio data, during pretraining, wave2vec masks out parts of
    the audio inputs and tries to learn what has been masked out.
  prefs: []
  type: TYPE_NORMAL
- en: The major difference between wave2vec and BERT is that audio is a continuous
    signal with no clear segmentation into tokens. Wave2vec considers each 25 ms-long
    audio as a basic unit and feeds each 25 ms basic unit into a CNN model to learn
    a unit-level feature representation. Then, part of the input is masked out and
    fed into a BERT-like transformer model to predict the masked output. The training
    objective is to minimize the contrastive loss between the original audio and predicted
    audio.
  prefs: []
  type: TYPE_NORMAL
- en: 'It is worth mentioning that contrastive (self-supervised) pretraining is also
    widely used in the representation learning of text or images. For example, given
    an image as input, we can augment the image content a little bit to produce two
    views of the same image: even though these two views look different, we know they
    come from the same image.'
  prefs: []
  type: TYPE_NORMAL
- en: 'This self-supervised contrastive learning has been widely used for representation
    learning: to learn a good feature vector given any kind of input. When applying
    the model to a specific domain, it is still recommended to give some labeled data
    to fine-tune the model with some extra labels.'
  prefs: []
  type: TYPE_NORMAL
- en: Algorithms beyond text, visual, and acoustic
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In real life, many kinds of information carriers exist. In addition to text,
    images, and speech, videos, actions, and even proteins contain a wealth of information.
    Therefore, many attempts have been made to obtain vector representations. Researchers
    at DeepMind have developed the *AlphaFold* and *AlphaFold2* algorithms. Based
    on traditional features, such as those of an amino acid sequence, AlphaFold algorithms
    can be used to obtain protein expression vectors and calculate its 3D structure
    in space, which greatly improves experiment efficiency in the field of protein
    analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, in 2021, GitHub launched Copilot to help programmers with the automatic
    completion of code. Prior to this, OpenAI developed the *Codex* model, which was
    able to convert natural language into code. Based on Codex’s model architecture,
    GitHub uses their open source TB-level code base to train the model on a large
    scale and completes the Copilot model to help programmers write new code. Copilot
    also supports the generation and completion of multiple programming languages,
    such as Python, JavaScript, and Go. In the search field, if we want to perform
    a code search or evaluate the similarity of two pieces of code, the Codex model
    can be employed to encode the source code into a vector representation.
  prefs: []
  type: TYPE_NORMAL
- en: The aforesaid operations mostly focus on separate encodings of text, images,
    or audio, so the encoded vector space may vary significantly. To map the information
    of different modalities to the same vector space, OpenAI researchers proposed
    the CLIP model, which can effectively map an image to text. Specifically, CLIP
    includes an image encoder and a text encoder. After inputting an image and multiple
    texts, CLIP encodes them at the same time and hopes to find the text most suitable
    for each image. By training CLIP on a large-scale dataset, CLIP can acquire an
    excellent representation of images and text and map them in the same vector space.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter described the method of vector representation, which is a major
    step in the operation of search engines.
  prefs: []
  type: TYPE_NORMAL
- en: First, we introduced the importance of vector representation and how to use
    it, and then addressed local and distributed vector representation algorithms.
    In terms of distributed vector representation, the commonly used representation
    algorithms for text, images, and audio were covered, and common representation
    methods for other modalities and multimodality were summarized. Hence, we found
    that the dense vector representation method often entails relatively rich contextual
    information when compared with sparse vectors.
  prefs: []
  type: TYPE_NORMAL
- en: When building a scalable neural search system, it is important to create an
    encoder that can encode raw documents into high-quality embeddings. This encoding
    process needs to be performed fast to reduce the indexing time. At search time,
    it is critical to apply the same encoding process and find the top-ranked documents
    in a reasonable amount of time. In the next chapter, we’ll utilize the ideas in
    this chapter and build a mental map on how to create a scalable neural search
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers
    for language understanding.” *arXiv preprint arXiv:1810.04805* (2018).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Simonyan, Karen and Andrew Zisserman. “Very deep convolutional networks for
    large-scale image recognition.” *arXiv preprint arXiv:1409.1556* (2014).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: He, Kaiming et al. “Deep residual learning for image recognition.” *Proceedings
    of the IEEE conference on computer vision and pattern recognition*. 2016.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Schneider, Steffen, et al. “wav2vec: Unsupervised pre-training for speech recognition.”
    *arXiv preprint arXiv:1904.05862* (2019).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Radford, Alec et al. “Learning transferable visual models from natural language
    supervision.” *International Conference on Machine Learning*. PMLR, 2021.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
