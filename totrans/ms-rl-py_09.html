<html><head></head><body>
		<div id="_idContainer1237">
			<h1 id="_idParaDest-146"><em class="italic"><a id="_idTextAnchor147"/>Chapter 7</em>: Policy-Based Methods</h1>
			<p>Value-based methods, which we covered in the previous chapter, achieve great results in many environments with discrete control spaces. However, a lot of applications, such as robotics, require continuous control. In this chapter, we'll go into another important class of algorithms, called policy-based methods, which enable us to solve continuous-control problems. In addition, these methods directly optimize a policy network and hence stand on a stronger theoretical foundation. Finally, policy-based methods are able to learn truly stochastic policies, which are needed in partially observable environments and games, and which value-based methods cannot learn. All in all, policy-based approaches complement value-based methods in many ways. This chapter goes into the details of policy-based methods, so you will gain a strong understanding of how they work.</p>
			<p>In particular, we'll discuss the following topics in this chapter:</p>
			<ul>
				<li>Why should we use policy-based methods?</li>
				<li>Vanilla policy gradient</li>
				<li>Actor-critic methods</li>
				<li>Trust-region methods</li>
				<li>Off-policy methods</li>
				<li>A comparison of the policy-based methods in Lunar Lander</li>
				<li>How to pick the right algorithm</li>
				<li>Open source implementations of policy-based methods</li>
			</ul>
			<p>Let's dive right in!</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor148"/>Why should we use policy-based methods?</h1>
			<p>We'll start this chapter by first<a id="_idIndexMarker681"/> discussing why we need policy-based methods as we have already introduced many value-based methods. Policy-based methods i) are arguably more principled as they directly optimize based on the policy parameters, ii) allow us to use continuous action spaces, and iii) are able to learn truly random stochastic policies. Let's now go into the details of each of these points.</p>
			<h2 id="_idParaDest-148"><a id="_idTextAnchor149"/>A more principled approach</h2>
			<p>In Q-learning, a policy is obtained in an<a id="_idIndexMarker682"/> indirect manner by learning action values, which are then used to determine the best action(s). But do we really need to know the value of an action? Most of the time we don't, as they are only proxies to get us to optimal policies. Policy-based methods learn function approximations that directly give policies without such an intermediate step. This is arguably a more principled approach because we can take gradient steps directly to optimize the policy, not the proxy action-value representation. The latter is especially inefficient when there are many actions with similar values, perhaps all uninteresting to us because they are all bad actions.</p>
			<h2 id="_idParaDest-149"><a id="_idTextAnchor150"/>The ability to use continuous action spaces</h2>
			<p>All of the methods we<a id="_idIndexMarker683"/> mentioned in the previous section about value-based methods worked with discrete action spaces. On the other hand, there are many use cases for which we need to use continuous action spaces, such as robotics, where the discretization of actions results in poor agent behavior. But what is the issue with using continuous action spaces with value-based methods? Neural networks can certainly learn value representations for continuous actions – after all, we don't have such a restriction on states. </p>
			<p>However, remember how we used maximization over actions while calculating the target values:</p>
			<p><img src="image/Formula_07_001.png" alt=""/> and while obtaining the best action to act in the environment using <img src="image/Formula_07_002.png" alt=""/>. It is not very straightforward to do these maximizations over continuous action spaces, although we can make it work using approaches such as the following:</p>
			<ol>
				<li>During maximization, sample discrete actions from the continuous action space and use the action with the maximum value. Alternatively, fit a function to the values of the sampled <a id="_idIndexMarker684"/>actions and do the maximization over that function, which is called the <strong class="bold">cross-entropy method (CEM)</strong>.</li>
				<li>Instead of using a neural network, use a function approximator such as a function quadratic in actions, the<a id="_idIndexMarker685"/> maximum of which can be analytically calculated. An example of this is <strong class="bold">Normalized Advantage Functions (NAF)</strong> (<em class="italic">Gu et al, 2016</em> **). </li>
				<li>Learn a separate function<a id="_idIndexMarker686"/> approximation to obtain the maximum, such as in the <strong class="bold">Deep Deterministic Policy Gradient (DDPG)</strong> algorithm.</li>
			</ol>
			<p>Now, the downside of CEM <a id="_idIndexMarker687"/>and NAF is that they are less powerful compared to a neural network that directly represents a continuous-action policy. DDPG, on the other hand, is still a competitive alternative, which we will cover later in the chapter.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">Most policy-based methods work with both discrete and continuous action spaces.</p>
			<h2 id="_idParaDest-150"><a id="_idTextAnchor151"/>The ability to learn truly random stochastic policies</h2>
			<p>Throughout Q-learning, we used<a id="_idIndexMarker688"/> soft-policies such as <img src="image/Formula_05_272.png" alt=""/>-greedy to enable the agent to explore the environment during training. Although this approach works pretty well in practice, and it can be made more sophisticated by annealing the <img src="image/Formula_07_004.png" alt=""/>, it is still not a learned parameter. Policy-based methods can learn random policies that lead to a more principled exploration during training.</p>
			<p>Perhaps a bigger issue is that we may need to learn random policies not just for training but also for inference. There are two reasons why we might want to do this:</p>
			<ul>
				<li>In <strong class="bold">partially observable environments</strong> (<strong class="bold">POMDPs</strong>), we may have what are called <strong class="bold">aliased states</strong>, which emit the same<a id="_idIndexMarker689"/> observation although<a id="_idIndexMarker690"/> the states themselves are different, for which the optimal actions might be different. Consider the following example:</li>
			</ul>
			<div>
				<div id="_idContainer952" class="IMG---Figure">
					<img src="image/B14160_07_1.jpg" alt="Figure 7.1 – Robot in a partially observable environment&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – Robot in a partially observable environment</p>
			<p>The agent only observes the<a id="_idIndexMarker691"/> shapes of the states it is in but cannot tell what the state is. The agent is randomly initialized in a state other than 3, and its goal is to reach the coins in state 3 in the minimum number of steps by going left or right. The optimal action when the agent observes a hexagon is a random one because a deterministic policy (say, to always go left) would make the agent get stuck either between 1 and 2 (if left is always chosen) or 4 and 5.</p>
			<ul>
				<li>In game settings with adversarial agents, there could be cases where the only optimal policy is a random one. The canonical example for this is rock-paper-scissors, where the optimal policy is to select an action uniformly at random. Any other policy could be exploited by the opponents in the environment. </li>
			</ul>
			<p>Value-based methods don't have the ability to learn such random policies for inference whereas policy-based methods allow that.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">If the environment is fully observable, and it is not a game setting, there is always a deterministic policy that is optimal (although there could be more than one optimal policy and some of them could be random). In such cases, we don't need a random policy during inference.</p>
			<p>With this introduction, let's dive into the<a id="_idIndexMarker692"/> most popular policy-based methods. Next, we will give an overview of the vanilla policy gradient to set the stage for more complex algorithms that we will cover later.</p>
			<h1 id="_idParaDest-151"><a id="_idTextAnchor152"/>The vanilla policy gradient</h1>
			<p>We'll start by discussing the<a id="_idIndexMarker693"/> policy-based methods with the most fundamental algorithm: a vanilla policy gradient approach. Although such an algorithm is rarely useful in realistic problem settings, it is very important to understand it to build a strong intuition and a theoretical background for the more complex algorithms we will cover later.</p>
			<h2 id="_idParaDest-152"><a id="_idTextAnchor153"/>The objective in policy gradient methods</h2>
			<p>In value-based methods, we<a id="_idIndexMarker694"/> focused on finding good estimates for action values, with which we then obtained policies. Policy gradient methods, on the other hand, directly focus on optimizing the policy with respect to the reinforcement learning objective – although, we will still make use of value estimates. If you don't remember what this objective was, it is the expected discounted return:</p>
			<div>
				<div id="_idContainer953" class="IMG---Figure">
					<img src="image/Formula_07_005.jpg" alt=""/>
				</div>
			</div>
			<p>This is a slightly more rigorous way of writing this objective compared to how we wrote it before. Let's unpack what we have here:</p>
			<ul>
				<li>The objective is denoted by <img src="image/Formula_07_006.png" alt=""/> and it is a function of the policy at hand, <img src="image/Formula_07_007.png" alt=""/>.</li>
				<li>The policy itself is parametrized by <img src="image/Formula_07_008.png" alt=""/>, which we are trying to determine.</li>
				<li>The trajectory the agent observes, <img src="image/Formula_07_009.png" alt=""/>, is a random one, with a probability distribution <img src="image/Formula_07_010.png" alt=""/>. It is, as you would expect, a function of the policy, hence a function of <img src="image/Formula_07_011.png" alt=""/>.</li>
				<li><img src="image/Formula_07_012.png" alt=""/> is a function (unknown to the agent) that gives a reward based on the environment dynamics given the state <img src="image/Formula_07_013.png" alt=""/> and the action <img src="image/Formula_07_014.png" alt=""/>.</li>
			</ul>
			<p>Now, we have an objective function <img src="image/Formula_07_015.png" alt=""/> that we want to maximize, which depends on parameter <img src="image/Formula_07_016.png" alt=""/>, which we can <a id="_idIndexMarker695"/>control. A natural thing to do is to take a gradient step in the ascending direction:</p>
			<div>
				<div id="_idContainer965" class="IMG---Figure">
					<img src="image/Formula_07_017.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_07_018.png" alt=""/> is some step size. That's the main idea behind policy gradient methods, which, again, directly optimize the policy.</p>
			<p>Now, the million-dollar question (okay, maybe not that much) is how to figure out the gradient term. Let's look into it next.</p>
			<h2 id="_idParaDest-153"><a id="_idTextAnchor154"/>Figuring out the gradient</h2>
			<p>Understanding how the <a id="_idIndexMarker696"/>gradient of the objective function with respect to the policy parameters, <img src="image/Formula_07_019.png" alt=""/>, is obtained is important to get the idea behind different variants of policy gradient methods. Let's derive what we use in the vanilla policy gradient step by step.</p>
			<h3>A different way of expressing the objective function</h3>
			<p>First, let's express the objective function slightly differently:</p>
			<p class="figure-caption"><img src="image/Formula_07_020.png" alt=""/></p>
			<p class="figure-caption"><img src="image/Formula_07_021.png" alt=""/></p>
			<p>Here, we just expressed the trajectory and the reward that corresponds to it as a whole rather than individual state-action pairs, <img src="image/Formula_07_022.png" alt=""/>. Then, we used the definition of an expectation to write it as an integral (with a slight abuse of notation since we use the same <img src="image/Formula_07_023.png" alt=""/> both to denote the random variable and a realization of it, but it should be apparent from the context). Keep in mind that the probability of observing a particular trajectory <img src="image/Formula_07_024.png" alt=""/> is the following:</p>
			<div>
				<div id="_idContainer973" class="IMG---Figure">
					<img src="image/Formula_07_025.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">
</p>
			<div>
				<div id="_idContainer974" class="IMG---Figure">
					<img src="image/Formula_07_026.jpg" alt=""/>
				</div>
			</div>
			<p>This is simply a chain of products for probabilities of observing a state, taking a particular action with the policy<a id="_idIndexMarker697"/> given the state and observing the next state. Here, <img src="image/Formula_07_027.png" alt=""/> denotes the environment transition probabilities.</p>
			<p>Next, we use this to find a convenient formula for the gradient.</p>
			<h3>Coming up with a convenient expression for the gradient</h3>
			<p>Now let's go back to the<a id="_idIndexMarker698"/> objective function. We can write the gradient of it as follows:</p>
			<div>
				<div id="_idContainer976" class="IMG---Figure">
					<img src="image/Formula_07_028.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">
<img src="image/Formula_07_029.png" alt=""/></p>
			<p>Now we have the term <img src="image/Formula_07_030.png" alt=""/> we need to deal with. We will do a simple trick to get rid of it:</p>
			<div>
				<div id="_idContainer979" class="IMG---Figure">
					<img src="image/Formula_07_031.jpg" alt=""/>
				</div>
			</div>
			<p>which just follows from the definition of <img src="image/Formula_07_032.png" alt=""/>. Putting it back to the integral, we end up with an expectation for the gradient of the objective (be careful – not for the objective itself):</p>
			<div>
				<div id="_idContainer981" class="IMG---Figure">
					<img src="image/Formula_07_033.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">
<img src="image/Formula_07_034.png" alt=""/>
<img src="image/Formula_07_035.png" alt=""/></p>
			<p>Now, this will turn out to be <a id="_idIndexMarker699"/>a very convenient formula for the gradient. Taking a step back, we now have an expectation for the gradient. Of course, we cannot fully evaluate it because we don't know <img src="image/Formula_07_036.png" alt=""/>, but we can take samples from the environment. </p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Whenever you see an expectation in RL formulations, you can reasonably expect that we will use samples from the environment to evaluate it.</p>
			<p>This formulation forms the essence of the policy gradient methods. Next, let's see how we can conveniently do it.</p>
			<h3>Obtaining the gradient</h3>
			<p>Before going into estimating the <a id="_idIndexMarker700"/>gradient from samples, we need to get rid of one more term, <img src="image/Formula_07_037.png" alt=""/>, because we don't really know what that is. It turns out that we can do so by writing the explicit probability product for the trajectory:</p>
			<div>
				<div id="_idContainer986" class="IMG---Figure">
					<img src="image/Formula_07_038.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">
</p>
			<div>
				<div id="_idContainer987" class="IMG---Figure">
					<img src="image/Formula_07_039.jpg" alt=""/>
				</div>
			</div>
			<p>When we take the gradient with respect to <img src="image/Formula_06_096.png" alt=""/>, the first and last terms in the sum drop since they don't depend on <img src="image/Formula_07_041.png" alt=""/>. With that, we can write this gradient in terms of what we know, namely, <img src="image/Formula_07_042.png" alt=""/>, the policy that we possess: </p>
			<div>
				<div id="_idContainer991" class="IMG---Figure">
					<img src="image/Formula_07_043.jpg" alt=""/>
				</div>
			</div>
			<p>We can then estimate the <a id="_idIndexMarker701"/>gradient from a batch of <img src="image/Formula_07_044.png" alt=""/> trajectories as follows:</p>
			<div>
				<div id="_idContainer993" class="IMG---Figure">
					<img src="image/Formula_07_045.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer994" class="IMG---Figure">
					<img src="image/Formula_07_046.jpg" alt=""/>
				</div>
			</div>
			<p>This gradient aims to increase the likelihood of trajectories that have high total rewards, and reduce the ones (or increase them less) with low total rewards.</p>
			<p>This gives us all the ingredients to put together a policy gradient algorithm, namely REINFORCE, which we turn to next.</p>
			<h2 id="_idParaDest-154"><a id="_idTextAnchor155"/>REINFORCE</h2>
			<p>The REINFORCE algorithm, one of the<a id="_idIndexMarker702"/> earliest policy gradient methods, uses the ingredients we presented above. We will need a lot of improvements <a id="_idIndexMarker703"/>on top of this to come up with methods with which we can attack realistic problems. On the other hand, understanding REINFORCE is useful to formalize these ideas in the context of an algorithm.</p>
			<p>REINFORCE works as follows in finite horizon problems with the discount factors ignored:</p>
			<ol>
				<li value="1">Initialize a policy <img src="image/Formula_07_047.png" alt=""/>:<p><em class="italic">while some stopping criterion is not met do:</em></p></li>
				<li>Collect <img src="image/Formula_07_048.png" alt=""/> trajectories <img src="image/Formula_07_049.png" alt=""/> from the environment using <img src="image/Formula_07_050.png" alt=""/>.</li>
				<li>Calculate<p><img src="image/Formula_07_051.png" alt=""/>.</p></li>
				<li>Update <img src="image/Formula_07_052.png" alt=""/>:<p><em class="italic">end while</em></p></li>
			</ol>
			<p>The REINFORCE algorithm simply suggests sampling trajectories from the environment using the policy on hand, then estimating the gradient using these samples and taking a gradient step to update the policy parameters.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">The fact that the sampled trajectories are used to obtain a gradient estimate for the policy parameters at hand makes policy gradient methods <strong class="bold">on-policy</strong>. Therefore, we cannot use samples obtained under a different policy to improve the existing policy, unlike in value-based methods. Having said that, we will have a section at the end of the chapter to discuss several off-policy approaches.</p>
			<p class="callout">The REINFORCE algorithm requires complete trajectories for network updates, therefore it is a Monte Carlo method.</p>
			<p>Next, let's discuss why we<a id="_idIndexMarker704"/> need improvements on top<a id="_idIndexMarker705"/> of REINFORCE.</p>
			<h2 id="_idParaDest-155"><a id="_idTextAnchor156"/>The problem with REINFORCE and all policy gradient methods</h2>
			<p>The most important issue with the <a id="_idIndexMarker706"/>policy gradient algorithms, in <a id="_idIndexMarker707"/>general, is the high variance in the <img src="image/Formula_07_053.png" alt=""/> estimates. If you think about it, there are many factors contributing to this:</p>
			<ul>
				<li><strong class="bold">Randomness in the environment</strong> could lead to many different trajectories for the agent even with the same policy, whose gradients are likely to vary significantly.</li>
				<li><strong class="bold">The length of sample trajectories</strong> could vary significantly, resulting in very different sums for the log and reward terms.</li>
				<li><strong class="bold">Environments with sparse rewards</strong> could be especially problematic (by the definition of sparse reward).</li>
				<li><strong class="bold">The size of </strong><img src="image/Formula_07_054.png" alt=""/> is usually kept at a few thousand to make the learning practical, but it may not be enough to capture the full distribution of trajectories.</li>
			</ul>
			<p>As a result, the gradient estimates we obtain from samples could have a high variance, which is likely to destabilize the learning. Reducing this variance is an important goal to make learning feasible, and<a id="_idIndexMarker708"/> we employ various tricks towards this end. Next, we cover the first of those tricks.</p>
			<h3>Replacing the reward sum with reward-to-go</h3>
			<p>Let's first rearrange the terms in the <a id="_idIndexMarker709"/>gradient estimation as follows:</p>
			<div>
				<div id="_idContainer1003" class="IMG---Figure">
					<img src="image/Formula_07_055.jpg" alt=""/>
				</div>
			</div>
			<p>This original form implies that each of the <img src="image/Formula_07_056.png" alt=""/> terms are weighed by the total reward obtained throughout the entire trajectory. Intuition, on the other hand, tells us that we should be weighing the log term only by the sum of rewards following that state-action pair as they cannot affect what came before it (causality). More formally, we can write the gradient estimate as follows:</p>
			<div>
				<div id="_idContainer1005" class="IMG---Figure">
					<img src="image/Formula_07_057.jpg" alt=""/>
				</div>
			</div>
			<p>It turns out that this still gives us an unbiased estimate of the gradient. The variance also reduces as the sums get smaller as we add fewer reward terms, and as a result, the weights that multiply the log terms get smaller. The <img src="image/Formula_07_058.png" alt=""/> is called the reward-to-go at time <img src="image/Formula_07_059.png" alt=""/>. Notice how this is actually an estimate for <img src="image/Formula_07_060.png" alt=""/>. We will make use of it later in the actor-critic algorithms.</p>
			<p>This improvement <a id="_idIndexMarker710"/>over the REINFORCE algorithm is a form of a <em class="italic">vanilla policy gradient</em> method. Next, we'll show how to use the RLlib vanilla policy gradient implementation.</p>
			<h2 id="_idParaDest-156"><a id="_idTextAnchor157"/>Vanilla policy gradient using RLlib</h2>
			<p>RLlib allows us to use the vanilla<a id="_idIndexMarker711"/> policy gradient with multiple rollout workers (actors) to parallelize the sample collection. You will notice<a id="_idIndexMarker712"/> that, unlike in value-based methods, the sample collection will be synchronized with network weight updates as the vanilla policy gradient algorithm is an on-policy method.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">Since policy gradient methods are on-policy, we need to make sure that the samples we use to update the neural network parameters (weights) come from the existing policy suggested by the network. This dictates synchronizing the policy in use across the rollout workers.</p>
			<p>The overall architecture of the parallelized vanilla policy gradient is therefore as follows:</p>
			<div>
				<div id="_idContainer1009" class="IMG---Figure">
					<img src="image/B14160_07_2.jpg" alt="Figure 7.2 – Vanilla policy gradient architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Vanilla policy gradient architecture</p>
			<p>At this point, it is worth noting that RLlib implementation transmits samples as <img src="image/Formula_07_061.png" alt=""/> from actors to the learner and concatenates them in the learner to restore the full trajectories.</p>
			<p>Using the vanilla <a id="_idIndexMarker713"/>policy gradient in RLlib is pretty simple and <a id="_idIndexMarker714"/>very similar to how we used value-based methods in the previous chapter. Let's train a model for the OpenAI Lunar Lander environment with continuous action space. Follow along!</p>
			<ol>
				<li value="1">First, to avoid running into issues with the Gym and Box2D packages, install Gym using the following:<p class="source-code"><strong class="bold">pip install gym[box2d]==0.15.6</strong></p></li>
				<li>Then go to implementing the Python code. Import the packages we need for argument parsing, <strong class="source-inline">ray</strong> and <strong class="source-inline">tune</strong>:<p class="source-code">import argparse</p><p class="source-code">import pprint</p><p class="source-code">from ray import tune</p><p class="source-code">import ray</p></li>
				<li>Import the vanilla <strong class="bold">policy</strong> <strong class="bold">gradient</strong> (<strong class="bold">PG</strong>) trainer class and the <a id="_idIndexMarker715"/>corresponding config dictionary:<p class="source-code">from ray.rllib.agents.pg.pg import (</p><p class="source-code">    DEFAULT_CONFIG,</p><p class="source-code">    PGTrainer as trainer)</p><p>Note that this part will be different when we want to use different algorithms.</p></li>
				<li>Create a main function, which receives the Gym environment name as an argument:<p class="source-code">if __name__ == "__main__":</p><p class="source-code">    parser = argparse.ArgumentParser()</p><p class="source-code">    parser.add_argument('--env',</p><p class="source-code">                        help='Gym env name.')</p><p class="source-code">    args = parser.parse_args()</p></li>
				<li>Modify the<a id="_idIndexMarker716"/> config dictionary with the <a id="_idIndexMarker717"/>number of GPUs we want to use for training and the number of CPUs we want to use for sample collection and evaluation:<p class="source-code">    config = DEFAULT_CONFIG.copy()</p><p class="source-code">    config_update = {</p><p class="source-code">                 "env": args.env,</p><p class="source-code">                 "num_gpus": 1,</p><p class="source-code">                 "num_workers": 50,</p><p class="source-code">                 "evaluation_num_workers": 10,</p><p class="source-code">                 "evaluation_interval": 1</p><p class="source-code">            }</p><p class="source-code">    config.update(config_update)</p><p class="source-code">    pp = pprint.PrettyPrinter(indent=4)</p><p class="source-code">    pp.pprint(config)</p><p>The <strong class="source-inline">print</strong> statement is for you to see what other configurations are available to you if you want to change it. You can modify things like the learning rate, for example. For now, we are not going into such hyperparameter optimization details. And for the vanilla policy gradient, the number of hyperparameters is considerably less than a more sophisticated algorithm would involve. One final note: the reason we set a separate set of evaluation workers is to make the training consistent with the off-policy algorithms we will introduce later. Normally, we don't need to do that since on-policy methods follow the same policy during training and evaluation.</p></li>
				<li>Implement the<a id="_idIndexMarker718"/> section to initialize <strong class="source-inline">ray</strong> and train the agent for a given number of iterations:<p class="source-code">    ray.init()</p><p class="source-code">    tune.run(trainer,</p><p class="source-code">             stop={"timesteps_total": 2000000},</p><p class="source-code">             config=config</p><p class="source-code">             )</p></li>
				<li>Save this code in a<a id="_idIndexMarker719"/> Python file, say, <strong class="source-inline">pg_agent.py</strong>. You can then train the agent as follows:<p class="source-code"><strong class="bold">python pg_agent.py --env "LunarLanderContinuous-v2"</strong></p></li>
				<li>Monitor the training on TensorBoard:<p class="source-code"><strong class="bold">tensorboard --logdir=~/ray_results</strong></p><p>The training progress will look like the following:</p></li>
			</ol>
			<div>
				<div id="_idContainer1011" class="IMG---Figure">
					<img src="image/B14160_07_3.jpg" alt="Figure 7.3 – Training progress for a vanilla policy gradient agent in Gym's continuous Lunar Lander&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.3 – Training progress for a vanilla policy gradient agent in Gym's continuous Lunar Lander</p>
			<p>That's it about the vanilla policy gradient method! Not a bad performance for an algorithm without many of the improvements that we will introduce in the upcoming sections. Feel free to try<a id="_idIndexMarker720"/> this algorithm with the other Gym <a id="_idIndexMarker721"/>environments. Hint: the Pendulum environment could give you some headaches.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">To save the best performing models while training the model with Tune, you will need to write a simple wrapper training function, which is described here: <a href="https://github.com/ray-project/ray/issues/7983">https://github.com/ray-project/ray/issues/7983</a>. You save the model whenever you observe an improvement in the evaluation score.</p>
			<p>Next, we'll cover a more powerful class of algorithms: actor-critic methods.</p>
			<h1 id="_idParaDest-157"><a id="_idTextAnchor158"/>Actor-critic methods</h1>
			<p>Actor-critic methods<a id="_idIndexMarker722"/> propose further remedies to the high variance problem in the policy gradient algorithm. Just like REINFORCE and other policy gradient methods, actor-critic algorithms have been around for decades now. Combining this approach with deep reinforcement learning, however, has enabled them to solve more realistic RL problems. We'll start this section by presenting the ideas behind the actor-critic approach, and later, we'll define them in more detail.</p>
			<h2 id="_idParaDest-158"><a id="_idTextAnchor159"/>Further reducing the variance in policy-based methods </h2>
			<p>Remember that, earlier, to reduce<a id="_idIndexMarker723"/> the variance in gradient estimates, we replaced the reward sum obtained in a trajectory with a<a id="_idIndexMarker724"/> reward-to-go term. Although a step in the right direction, it is usually not enough. We'll now introduce two more methods to further reduce this variance.</p>
			<h3>Estimating the reward-to-go </h3>
			<p>The reward-to-go term, <img src="image/Formula_07_062.png" alt=""/>, obtained in a trajectory is an estimate of the action-value <img src="image/Formula_07_063.png" alt=""/> under the existing policy <img src="image/Formula_07_064.png" alt=""/>. </p>
			<p class="callout-heading">Info</p>
			<p class="callout">Notice the difference between<a id="_idIndexMarker725"/> the action-value estimate we use here, <img src="image/Formula_07_065.png" alt=""/>, and what the Q-learning methods estimate, <img src="image/Formula_07_066.png" alt=""/>. The former estimates the action-value under the existing behavior policy <img src="image/Formula_05_046.png" alt=""/>, whereas the latter estimates the action-value under the target policy that is <img src="image/Formula_07_068.png" alt=""/>.</p>
			<p>Now, every trajectory that visits a particular <img src="image/Formula_07_069.png" alt=""/> pair is likely to yield a different reward-to-go estimate. This adds to the variance in gradient estimates. What if we could use a single estimate for a given <img src="image/Formula_07_070.png" alt=""/> in a policy update cycle? That would eliminate the variance caused by those noisy reward-to-go (action-value) estimates. But how do we obtain such an estimate? The answer is to train a neural network that will generate that estimate for us. We then train this network using the sampled reward-to-go values. When we query it to obtain an estimate for a specific state-action pair, it gives us a single number rather than many different estimates, which, in turn, reduces the variance.</p>
			<p>In essence, what such a network does is it evaluates the policy, which is why we call it the <strong class="bold">critic</strong>, while the policy network <a id="_idIndexMarker726"/>tells the agent how to <strong class="bold">act</strong> in the environment – hence the<a id="_idIndexMarker727"/> name <strong class="bold">actor-critic</strong>.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">We don't train the critic network from scratch after each policy update. Instead, just like the policy network, we apply gradient updates with new sampled data. As a result, the critic is biased towards the old policies. However, we are willing to make this trade-off to reduce the variance.</p>
			<p>Last but not least, we use baselines<a id="_idIndexMarker728"/> to reduce the variance, which we turn to next.</p>
			<h3>Using a baseline</h3>
			<p>The intuition behind the policy gradient <a id="_idIndexMarker729"/>methods is that we would like to adjust the parameters of the policy so that actions that result in high-reward trajectories become more likely, and ones that led to low-reward trajectories become less likely:</p>
			<div>
				<div id="_idContainer1021" class="IMG---Figure">
					<img src="image/Formula_07_071.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">
</p>
			<div>
				<div id="_idContainer1022" class="IMG---Figure">
					<img src="image/Formula_07_072.jpg" alt=""/>
				</div>
			</div>
			<p>One shortcoming in this formulation is that the direction and the magnitude of the gradient steps are heavily determined by the total reward in the trajectory, <img src="image/Formula_07_073.png" alt=""/>. Consider the following two examples:</p>
			<ul>
				<li>A maze environment that the agent tries to exit in the minimum time. The reward is the negative of the time elapsed until the exit is reached.</li>
				<li>The same maze environment but the reward is $1M, minus a dollar penalty for each second that passes until the exit is reached.</li>
			</ul>
			<p>Mathematically, these two are the same optimization problems. Now, think about what gradient steps a particular trajectory<a id="_idIndexMarker730"/> obtained under some policy <img src="image/Formula_07_074.png" alt=""/> would lead to under these two different reward structures. The first reward structure would lead to a negative gradient step for all trajectories (although some smaller than others) regardless of the quality of the policy, and the second reward structure would (almost certainly) lead to a positive gradient step. Moreover, under the latter, the impact of the elapsed seconds would be negligible since the fixed reward is too big, making the learning for the policy network very difficult.</p>
			<p>Ideally, we want to measure the <strong class="bold">relative</strong> reward performance observed in a particular trajectory as a result of the sequence of actions<a id="_idIndexMarker731"/> taken in it compared to the other trajectories. This way, we can take positive gradient steps in the direction of the parameters that result in high-reward trajectories and take negative gradient steps for the others. To measure the relative performance, a simple trick is to subtract a baseline <img src="image/Formula_07_075.png" alt=""/> from the reward sum:</p>
			<div>
				<div id="_idContainer1026" class="IMG---Figure">
					<img src="image/Formula_07_076.jpg" alt=""/>
				</div>
			</div>
			<p>The most obvious choice for the baseline is the average trajectory reward, sampled as follows:</p>
			<div>
				<div id="_idContainer1027" class="IMG---Figure">
					<img src="image/Formula_07_077.jpg" alt=""/>
				</div>
			</div>
			<p>It turns out that subtracting such a term still gives an unbiased estimate for the gradient, but with less variance, and the difference could be dramatic in some settings. So, using baselines is almost always good.</p>
			<p>When combined with using reward-to-go estimates, a natural choice for the baseline is the state value, which results in the following gradient estimate:</p>
			<div>
				<div id="_idContainer1028" class="IMG---Figure">
					<img src="image/Formula_07_078.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">
</p>
			<div>
				<div id="_idContainer1029" class="IMG---Figure">
					<img src="image/Formula_07_079.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_07_080.png" alt=""/> is the <strong class="bold">advantage</strong> term, which measures how much the agent is better off by taking action <img src="image/Formula_07_081.png" alt=""/> in state <img src="image/Formula_07_082.png" alt=""/> as opposed to <a id="_idIndexMarker732"/>following the existing policy.</p>
			<p>Having a critic estimating the advantage, directly or indirectly, gives rise to the <strong class="bold">advantage actor-critic</strong> algorithms, which we'll cover next.</p>
			<h2 id="_idParaDest-159"><a id="_idTextAnchor160"/>Advantage Actor-Critic – A2C</h2>
			<p>What we have covered <a id="_idIndexMarker733"/>so far is almost sufficient to put together what is known as the A2C algorithm. Let's discuss in a bit more detail how to<a id="_idIndexMarker734"/> estimate the advantage term before going into the full algorithm and the RLlib implementation. </p>
			<h3>How to estimate the advantage</h3>
			<p>There are different <a id="_idIndexMarker735"/>ways of estimating the advantage using a critic. The critic network could do the following:</p>
			<ul>
				<li>Directly estimate <img src="image/Formula_07_083.png" alt=""/>.</li>
				<li>Estimate <img src="image/Formula_07_084.png" alt=""/>, from which we can recover <img src="image/Formula_07_085.png" alt=""/>.</li>
			</ul>
			<p>Note that both of these approaches involve maintaining a network that gives outputs that depend on both the state and the action. However, we can get away with a simpler structure. Remember the definition of action-value:</p>
			<div>
				<div id="_idContainer1036" class="IMG---Figure">
					<img src="image/Formula_07_086.jpg" alt=""/>
				</div>
			</div>
			<p>When we sample a<a id="_idIndexMarker736"/> transition of a single step, we already observe the reward and the next state and obtain a tuple <img src="image/Formula_07_087.png" alt=""/>. We can therefore obtain the estimate <img src="image/Formula_07_088.png" alt=""/> as follows:</p>
			<div>
				<div id="_idContainer1039" class="IMG---Figure">
					<img src="image/Formula_07_089.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_07_090.png" alt=""/> is some estimate of the true state value <img src="image/Formula_05_021.png" alt=""/>.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">Notice the nuances in the notation. <img src="image/Formula_07_092.png" alt=""/> and <img src="image/Formula_07_093.png" alt=""/> represent the true values where <img src="image/Formula_07_094.png" alt=""/> and <img src="image/Formula_07_095.png" alt=""/> are their estimates. <img src="image/Formula_07_096.png" alt=""/>, and <img src="image/Formula_07_097.png" alt=""/> are random variables, whereas <img src="image/Formula_07_098.png" alt=""/>, and <img src="image/Formula_07_099.png" alt=""/> are their realizations.</p>
			<p>We can finally estimate the advantage as follows:</p>
			<div>
				<div id="_idContainer1050" class="IMG---Figure">
					<img src="image/Formula_07_100.jpg" alt=""/>
				</div>
			</div>
			<p>This allows us to use a neural<a id="_idIndexMarker737"/> network that simply estimates the state values to obtain an advantage estimate. To train this network, we can do bootstrapping to obtain the target values for state values. So, using a sampled tuple <img src="image/Formula_07_101.png" alt=""/>, the target for <img src="image/Formula_07_102.png" alt=""/> is calculated as <img src="image/Formula_07_103.png" alt=""/> (the same as the <img src="image/Formula_07_104.png" alt=""/> estimate because we happen to obtain the actions from the existing stochastic policy).</p>
			<p>Before presenting the full A2C algorithm, let's take a look at the implementation architecture.</p>
			<h3>A2C architecture</h3>
			<p>A2C suggests a synchronized<a id="_idIndexMarker738"/> sample collection between different agents, that is, all of the rollout workers use the same policy network at a given time to collect samples. Those samples are then passed to the learner to update the actor (policy network) and the critic (value network). In this sense, the architecture is pretty much the same with the vanilla policy gradient, for which we provided the schema above. Except, this time, we have a critic network. Then, the question is how to bring the critic network in.</p>
			<p>The design of the actor and critic can range from completely isolated neural networks, as shown in the following figure on the left, to a completely shared design (except the last layers):</p>
			<div>
				<div id="_idContainer1055" class="IMG---Figure">
					<img src="image/B14160_07_4.jpg" alt="Figure 7.4 – Isolated versus shared neural networks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Isolated versus shared neural networks</p>
			<p>The advantage of the isolated <a id="_idIndexMarker739"/>design is that it is usually more stable. This is because the variance and the magnitude of the target values of the actor and critic targets could be very different. Having them share the neural network requires careful tuning of hyperparameters such as the learning rate, otherwise, the learning would be unstable. On the other hand, using a shared architecture has the advantage of cross-learning and using common feature extraction capabilities. This could be especially handy when feature extraction is a major part of training, such as when observations are images. Of course, any architecture in between is also possible.</p>
			<p>Finally, it is time to present the A2C algorithm.</p>
			<h3>The A2C algorithm</h3>
			<p>Let's put all these ideas<a id="_idIndexMarker740"/> together and form the A2C algorithm:</p>
			<ol>
				<li value="1">Initialize the actor and critic network(s), <img src="image/Formula_07_105.png" alt=""/> and <img src="image/Formula_07_106.png" alt=""/>.<p><em class="italic">while some stopping criterion is not met do:</em></p></li>
				<li>Collect a batch of <img src="image/Formula_07_107.png" alt=""/> samples <img src="image/Formula_07_108.png" alt=""/> from the (parallel) environment(s) using <img src="image/Formula_07_109.png" alt=""/>.</li>
				<li>Obtain the state-value targets <img src="image/Formula_07_110.png" alt=""/>.</li>
				<li>Use gradient descent to<a id="_idIndexMarker741"/> update <img src="image/Formula_07_111.png" alt=""/> with respect to a loss function <img src="image/Formula_07_112.png" alt=""/>, such as squared loss.</li>
				<li>Obtain the advantage value estimates:<p> <img src="image/Formula_07_113.png" alt=""/>.</p></li>
				<li>Calculate<p><img src="image/Formula_07_114.png" alt=""/>.</p></li>
				<li>Update<p><img src="image/Formula_07_115.png" alt=""/>. </p></li>
				<li>Broadcast the new <img src="image/Formula_07_116.png" alt=""/> to the rollout workers.<p><em class="italic">end while</em></p></li>
			</ol>
			<p>Note that we could also<a id="_idIndexMarker742"/> use multi-step learning rather than using single-step estimation for advantage estimates and state value targets. We will present a generalized version of multi-step learning at the end of the actor-critic section. But now, let's see how you can use RLlib's A2C algorithm.</p>
			<h3>A2C using RLlib</h3>
			<p>To train an RL agent in RLlib using <a id="_idIndexMarker743"/>A2C is very similar to how we did it for the vanilla policy gradient. Therefore, rather than presenting the full flow again, we'll just describe what the difference is. The main difference is importing the A2C class:</p>
			<p class="source-code">from ray.rllib.agents.a3c.a2c import (</p>
			<p class="source-code">    A2C_DEFAULT_CONFIG as DEFAULT_CONFIG,</p>
			<p class="source-code">    A2Ctrainer as trainer)</p>
			<p>You can then train the agent the same way as the vanilla policy gradient agent. Instead of presenting the results from our training here, we will compare all the algorithms at the end of this chapter.</p>
			<p>Next, we present another famous actor-critic algorithm: A3C.</p>
			<h2 id="_idParaDest-160"><a id="_idTextAnchor161"/>Asynchronous Advantage Actor-Critic: A3C</h2>
			<p>A3C is pretty much the<a id="_idIndexMarker744"/> same as A2C in terms of the loss functions and how it uses the critic. In fact, A3C is a predecessor of A2C, although we<a id="_idIndexMarker745"/> presented them here in reverse order for pedagogical reasons. The differences between A2C and A3C are architectural and how the gradients are calculated and applied. Next, let's discuss the A3C architecture.</p>
			<h3>A3C architecture</h3>
			<p>A3C architecture differs from that of A2C as follows:</p>
			<ul>
				<li>The asynchrony in <a id="_idIndexMarker746"/>A3C is due to the fact the<a id="_idIndexMarker747"/> rollout workers pull the <img src="image/Formula_07_117.png" alt=""/> parameters from the main policy network at their own pace, not in sync with other workers. </li>
				<li>As a result, the workers are likely to be using different policies at the same time.</li>
				<li>To avoid calculating gradients at the central learner using samples that are likely to have been obtained under different policies, the gradients are calculated by the rollout workers with respect to the policy parameters in use at that time in the worker.</li>
				<li>What is passed to the learner is therefore not the samples but the gradients.</li>
				<li>Those gradients are applied to the main policy network, again, asynchronously as they arrive.</li>
			</ul>
			<p>The following diagram depicts the A3C architecture:</p>
			<div>
				<div id="_idContainer1069" class="IMG---Figure">
					<img src="image/B14160_07_5.jpg" alt="Figure 7.5 – A3C architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – A3C architecture</p>
			<p>There are two important <a id="_idIndexMarker748"/>downsides to A3C:</p>
			<ul>
				<li>The gradients that update the main policy network are likely to be obsolete and obtained under a different <img src="image/Formula_07_011.png" alt=""/> than what is in the main policy network. This is theoretically problematic as those are not the true gradients for the policy network parameters.</li>
				<li>Passing around gradients, which could be a large vector of numbers, especially when the neural network is big, could create a significant communication overhead compared to passing only samples.</li>
			</ul>
			<p>The main motivation behind A3C despite these disadvantages is obtaining decorrelated samples and gradient updates, similar to the role played by experience replay in deep Q-learning. On the other hand, in many experiments, people found A2C to be as good as A3C, and sometimes even better. As a result, A3C is not very commonly used. Still, we have presented it so you <a id="_idIndexMarker749"/>understand how these algorithms have evolved and what are the key differences between them. Let's also look into how you can use RLlib's A3C module.</p>
			<h3>A3C using RLlib</h3>
			<p>RLlib's A3C algorithm can <a id="_idIndexMarker750"/>simply be accessed by importing the corresponding trainer class:</p>
			<p class="source-code">from ray.rllib.agents.a3c.a3c import (</p>
			<p class="source-code">    DEFAULT_CONFIG,</p>
			<p class="source-code">    A3CTrainer as trainer)</p>
			<p>Then, you can train an agent following the code we provided earlier in the section.</p>
			<p>Finally, we'll close the discussion in this section with a generalization of multi-step RL in the context of policy gradient methods.</p>
			<h2 id="_idParaDest-161"><a id="_idTextAnchor162"/>Generalized advantage estimators</h2>
			<p>We previously mentioned that you <a id="_idIndexMarker751"/>can use multi-step estimates for the<a id="_idIndexMarker752"/> advantage function. Namely, instead of using only a single step transition as in the following:</p>
			<div>
				<div id="_idContainer1071" class="IMG---Figure">
					<img src="image/Formula_07_119.jpg" alt=""/>
				</div>
			</div>
			<p>using <img src="image/Formula_05_231.png" alt=""/>-step transitions could yield a more accurate estimate of the advantage function:</p>
			<div>
				<div id="_idContainer1073" class="IMG---Figure">
					<img src="image/Formula_07_121.jpg" alt=""/>
				</div>
			</div>
			<p>Of course, when <img src="image/Formula_07_122.png" alt=""/>, we are simply back to using sampled reward-to-go, which we had abandoned to reduce the variance in the advantage estimation. On the other hand, <img src="image/Formula_07_123.png" alt=""/> is likely to introduce too much bias towards the existing <img src="image/Formula_07_124.png" alt=""/> estimates. Therefore, the hyper-parameter <img src="image/Formula_07_125.png" alt=""/> is a way to control the bias-variance trade-off while estimating the advantage.</p>
			<p>A natural question is then whether we have to use a "single" <img src="image/Formula_05_193.png" alt=""/> in the advantage estimation. For example, we can calculate advantage estimates using <img src="image/Formula_07_127.png" alt=""/>, <img src="image/Formula_07_128.png" alt=""/>, and <img src="image/Formula_07_129.png" alt=""/>, and take their average. What about taking a weighted average (convex combination) of all possible <img src="image/Formula_07_130.png" alt=""/>, up to infinity? That is exactly what the <strong class="bold">generalized advantage estimator (GAE)</strong> does. More <a id="_idIndexMarker753"/>specifically, it weighs the <img src="image/Formula_07_131.png" alt=""/> terms in an exponentially decaying fashion:</p>
			<div>
				<div id="_idContainer1084" class="IMG---Figure">
					<img src="image/Formula_07_132.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">
</p>
			<div>
				<div id="_idContainer1085" class="IMG---Figure">
					<img src="image/Formula_07_133.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_07_134.png" alt=""/> and <img src="image/Formula_07_135.png" alt=""/> is a hyperparameter. Therefore, GAE gives us another knob to control the bias-variance trade-off. Specifically, <img src="image/Formula_07_136.png" alt=""/> results in <img src="image/Formula_07_137.png" alt=""/>, which has <a id="_idIndexMarker754"/>high bias; and <img src="image/Formula_07_138.png" alt=""/> results in <img src="image/Formula_07_139.png" alt=""/>, which is equivalent to using the sampled reward-to-to minus the baseline, which has high bias. Any value of <img src="image/Formula_07_140.png" alt=""/> is a compromise between the two.</p>
			<p>Let's close this section by noting that you can turn on or off the GAE in RLlib's actor-critic implementations using the config flag <strong class="source-inline">"use_gae"</strong> along with <strong class="source-inline">"lambda"</strong>.</p>
			<p>This concludes our discussion on actor-critic functions. Next, we'll look into a recent approach called <strong class="bold">trust-region methods</strong>, which have<a id="_idIndexMarker755"/> resulted in significant improvements over A2C and A3C.</p>
			<h1 id="_idParaDest-162"><a id="_idTextAnchor163"/>Trust-region methods</h1>
			<p>One of the important<a id="_idIndexMarker756"/> developments in the world of policy-based methods has been the evolution of trust-region methods. In particular, TRPO and PPO algorithms have led to a significant improvement over algorithms such as A2C and A3C. For example, the famous Dota 2 AI agent, which reached expert-level performance in competitions, was trained using PPO and GAE. In this section, we'll go into the details of those algorithms to help you gain a solid understanding of how they work.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">Prof. Sergey Levine, who co-authored the TRPO and PPO papers, goes deep into the details of the math behind these methods in his online lecture more than we do in this section. That lecture is available at <a href="https://youtu.be/uR1Ubd2hAlE">https://youtu.be/uR1Ubd2hAlE</a> and I highly recommend you watch it to improve your theoretical understanding of these algorithms.</p>
			<p>Without further ado, let's dive in!</p>
			<h2 id="_idParaDest-163"><a id="_idTextAnchor164"/>Policy gradient as policy iteration</h2>
			<p>In the earlier chapters, we <a id="_idIndexMarker757"/>described how most of the RL algorithms can be thought of as a form of policy iteration, alternating <a id="_idIndexMarker758"/>between policy evaluation and improvement. You can think of policy gradient methods in the same context: </p>
			<ul>
				<li>Sample collection and advantage estimation: policy evaluation</li>
				<li>Gradient step: policy improvement</li>
			</ul>
			<p>Now we will use a policy iteration point of view to set the stage for the upcoming algorithms. First, let's look at how we can quantify the improvement in the RL objective.</p>
			<h3>Quantifying the improvement</h3>
			<p>In general, a policy<a id="_idIndexMarker759"/> improvement step aims to improve the policy on hand as much as possible. More formally, the objective is the following:</p>
			<div>
				<div id="_idContainer1093" class="IMG---Figure">
					<img src="image/Formula_07_141.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer1094" class="IMG---Figure">
					<img src="image/Formula_07_142.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_06_036.png" alt=""/> is the existing policy. Using the definition of <img src="image/Formula_07_144.png" alt=""/> and some algebra, we can show the following:</p>
			<div>
				<div id="_idContainer1097" class="IMG---Figure">
					<img src="image/Formula_07_145.jpg" alt=""/>
				</div>
			</div>
			<p>Let's unpack what this equation tells us:</p>
			<ul>
				<li>The improvement obtained by a new policy <img src="image/Formula_07_146.png" alt=""/> over an existing policy <img src="image/Formula_06_154.png" alt=""/> can be quantified using the advantage function under the existing policy.</li>
				<li>The expectation operation we need for this calculation is under the new policy <img src="image/Formula_07_148.png" alt=""/>.</li>
			</ul>
			<p>Remember that it is almost never practical to fully calculate such expectations or advantage functions. We<a id="_idIndexMarker760"/> always estimate them using the policy on hand, <img src="image/Formula_07_149.png" alt=""/> in this case, and interacting with the environment. Now, the former is a happy point since we know how to estimate advantages using samples – the previous section was all about this. We also know that we can collect samples to estimate expectations, which is what we have in <img src="image/Formula_07_150.png" alt=""/>. The problem here, though, is that the expectation is with respect to a new policy <img src="image/Formula_07_151.png" alt=""/>. We don't know what <img src="image/Formula_07_152.png" alt=""/> is, in fact, that is what we are trying to find out. So we cannot collect samples from the environment using <img src="image/Formula_07_153.png" alt=""/>. Everything from here on will be about how to get around this problem so that we can iteratively improve the policy.</p>
			<h3>Getting rid of <img src="image/Formula_07_154.png" alt=""/></h3>
			<p>Let's expand this expectation <a id="_idIndexMarker761"/>and write it down in terms of the marginal probabilities that compose <img src="image/Formula_07_155.png" alt=""/>:</p>
			<div>
				<div id="_idContainer1108" class="IMG---Figure">
					<img src="image/Formula_07_156.jpg" alt=""/>
				</div>
			</div>
			<p>which uses the following:</p>
			<div>
				<div id="_idContainer1109" class="IMG---Figure">
					<img src="image/Formula_07_157.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer1110" class="IMG---Figure">
					<img src="image/Formula_07_158.jpg" alt=""/>
				</div>
			</div>
			<p>We can get rid of the <img src="image/Formula_07_159.png" alt=""/> in the inner <a id="_idIndexMarker762"/>expectation using importance sampling:</p>
			<div>
				<div id="_idContainer1112" class="IMG---Figure">
					<img src="image/Formula_07_160.jpg" alt=""/>
				</div>
			</div>
			<p>Now, getting rid of the <img src="image/Formula_07_153.png" alt=""/> in the outer expectation is the challenging part. <em class="italic">The key idea is to stay "sufficiently close" to the existing policy during the optimization, that is, </em><img src="image/Formula_07_162.png" alt=""/>. In that case, it can be shown that <img src="image/Formula_07_163.png" alt=""/>, and we can replace the former with the latter. </p>
			<p>One of the key questions here is how to measure the "closeness" of the policies so that we can make sure the preceding approximation is valid. A popular choice for such measurements, due to its nice <a id="_idIndexMarker763"/>mathematical properties, is the <strong class="bold">Kullback-Leibler</strong> (<strong class="bold">KL</strong>) divergence.</p>
			<p class="callout-heading"> Info</p>
			<p class="callout">If you are not too familiar with the KL divergence or if you need to refresh your mind, a nice explanation of it is available here: <a href="https://youtu.be/2PZxw4FzDU?t=226">https://youtu.be/2PZxw4FzDU?t=226</a>.</p>
			<p>Using the approximation due to the closeness of the policies and bounding this closeness results in the following optimization function:</p>
			<div>
				<div id="_idContainer1116" class="IMG---Figure">
					<img src="image/Formula_07_164.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer1117" class="IMG---Figure">
					<img src="image/Formula_07_165.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_05_272.png" alt=""/> is some bound.</p>
			<p>Next, let's see how we can use other approximations to further simplify this optimization problem.</p>
			<h3>Using Taylor series expansion in the optimization</h3>
			<p>Now that we<a id="_idIndexMarker764"/> have a function <img src="image/Formula_07_167.png" alt=""/> and we know that we evaluate it in close proximity to another point <img src="image/Formula_07_168.png" alt=""/>, this should ring bells to use Taylor series expansion.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">If you need to refresh your mind on Taylor series, or deepen your intuition, a great resource is this video: <a href="https://youtu.be/3d6DsjIBzJ4">https://youtu.be/3d6DsjIBzJ4</a>. I also recommend subscribing to the channel – 3Blue1Brown is one of the best resources to visually internalize many math concepts.</p>
			<p>The first order expansion at <img src="image/Formula_07_169.png" alt=""/> is as follows:</p>
			<p class="figure-caption"><img src="image/Formula_07_170.png" alt=""/></p>
			<p>Notice that the first term does not depend on <img src="image/Formula_07_171.png" alt=""/>, so we can get rid of it in the optimization. Also note that the<a id="_idIndexMarker765"/> gradient term is with respect to <img src="image/Formula_07_172.png" alt=""/> rather than <img src="image/Formula_07_173.png" alt=""/>, which should come in handy.</p>
			<p>Our goal then becomes the following:</p>
			<div>
				<div id="_idContainer1126" class="IMG---Figure">
					<img src="image/Formula_07_174.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer1127" class="IMG---Figure">
					<img src="image/Formula_07_175.jpg" alt=""/>
				</div>
			</div>
			<p>Finally, let's look into how we can calculate the gradient term.</p>
			<h3>Calculating <img src="image/Formula_07_176.png" alt=""/></h3>
			<p>First, let's<a id="_idIndexMarker766"/> look at what <img src="image/Formula_07_177.png" alt=""/> looks like. Remember that we can write the following:</p>
			<div>
				<div id="_idContainer1130" class="IMG---Figure">
					<img src="image/Formula_07_178.jpg" alt=""/>
				</div>
			</div>
			<p>because, by definition, <img src="image/Formula_07_179.png" alt=""/>. Then we can write:</p>
			<div>
				<div id="_idContainer1132" class="IMG---Figure">
					<img src="image/Formula_07_180.jpg" alt=""/>
				</div>
			</div>
			<p>Now, remember that we are looking for <img src="image/Formula_07_181.png" alt=""/>, not <img src="image/Formula_07_182.png" alt=""/>. Replacing all <img src="image/Formula_07_183.png" alt=""/> with <img src="image/Formula_07_184.png" alt=""/> results in the following:</p>
			<div>
				<div id="_idContainer1137" class="IMG---Figure">
					<img src="image/Formula_07_185.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">
</p>
			<div>
				<div id="_idContainer1138" class="IMG---Figure">
					<img src="image/Formula_07_186.jpg" alt=""/>
				</div>
			</div>
			<p>We have arrived at a result that should look familiar! The <img src="image/Formula_07_187.png" alt=""/> is exactly what goes into the gradient estimate <img src="image/Formula_07_188.png" alt=""/> in advantage actor-critic. The objective of maximizing the policy improvement between policy updates has led us to the same objective with a regular gradient ascent approach. Of course, we should not forget the constraint. So, the <a id="_idIndexMarker767"/>optimization problem we aim to solve has become the following:</p>
			<div>
				<div id="_idContainer1141" class="IMG---Figure">
					<img src="image/Formula_07_189.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer1142" class="IMG---Figure">
					<img src="image/Formula_07_190.jpg" alt=""/>
				</div>
			</div>
			<p>This is a key result! Let's unpack what we have obtained so far:</p>
			<ul>
				<li>Regular actor-critic with gradient ascent and trust-region methods have the same objective of moving in the direction of the gradient.</li>
				<li>The trust-region approach aims to stay close to the existing policy by limiting the KL divergence.</li>
				<li>Regular gradient ascent, on the other hand, moves in the direction of the gradient by a particular step size, as in <img src="image/Formula_07_191.png" alt=""/>. </li>
				<li>Regular gradient ascent, therefore, aims to keep <img src="image/Formula_07_192.png" alt=""/> closer to <img src="image/Formula_07_193.png" alt=""/>, as opposed to keeping <img src="image/Formula_07_194.png" alt=""/> close to <img src="image/Formula_07_195.png" alt=""/>. </li>
				<li>Using a single step size for all dimensions in <img src="image/Formula_07_196.png" alt=""/>, as regular gradient ascent does, may result in very slow convergence, or not converging at all, since some dimensions in the <a id="_idIndexMarker768"/>parameter vector could have a much greater impact on the policy (change) than others. <p class="callout-heading">Tip</p><p class="callout">The key objective in the trust-region methods is to stay sufficiently close to the existing policy <img src="image/Formula_07_197.png" alt=""/> while updating the policy to some <img src="image/Formula_07_198.png" alt=""/>. This is different than simply aiming to keep <img src="image/Formula_07_199.png" alt=""/> closer to <img src="image/Formula_07_200.png" alt=""/>, which is what regular gradient ascent does.</p></li>
			</ul>
			<p>So, we know that <img src="image/Formula_07_201.png" alt=""/> and <img src="image/Formula_07_202.png" alt=""/> should be close, but have not discussed how to achieve this. In fact, two different algorithms, TRPO and PPO will handle this requirement differently. Next, we turn to details of the TRPO algorithm.</p>
			<h2 id="_idParaDest-164"><a id="_idTextAnchor165"/>TRPO – Trust Region Policy Optimization</h2>
			<p>TRPO is an important <a id="_idIndexMarker769"/>algorithm that <a id="_idIndexMarker770"/>precedes PPO. Let's understand in this section how it handles the optimization problem we arrived at above and what the challenges are with the TRPO solution.</p>
			<h3>Handling the KL divergence</h3>
			<p>The TRPO algorithm<a id="_idIndexMarker771"/> approximates the KL divergence constraint with its second-order Taylor expansion:</p>
			<div>
				<div id="_idContainer1155" class="IMG---Figure">
					<img src="image/Formula_07_203.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_07_204.png" alt=""/> is called the Fisher information matrix and defined as follows:</p>
			<div>
				<div id="_idContainer1157" class="IMG---Figure">
					<img src="image/Formula_07_205.jpg" alt=""/>
				</div>
			</div>
			<p>where the expectation is to be estimated from the samples. Note that if <img src="image/Formula_07_206.png" alt=""/> is an <img src="image/Formula_07_207.png" alt=""/> dimensional vector, <img src="image/Formula_07_208.png" alt=""/> becomes an <img src="image/Formula_07_209.png" alt=""/> matrix.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">The Fisher information matrix is an important concept that you might want to learn more about, and the Wikipedia page is a good start: <a href="https://en.wikipedia.org/wiki/Fisher_information">https://en.wikipedia.org/wiki/Fisher_information</a>.</p>
			<p>This approximation results in the following gradient update step (we omit the derivation):</p>
			<div>
				<div id="_idContainer1162" class="IMG---Figure">
					<img src="image/Formula_07_210.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_07_211.png" alt=""/> and <img src="image/Formula_05_254.png" alt=""/> are hyperparameters, <img src="image/Formula_07_213.png" alt=""/> and <img src="image/Formula_07_214.png" alt=""/>.</p>
			<p>If this looks scary to you, you<a id="_idIndexMarker772"/> are not alone! TRPO is indeed not the easiest algorithm to implement. Let's next look into what kind of challenges TRPO involves.</p>
			<h3>Challenges with TRPO</h3>
			<p>Here are some of the challenges<a id="_idIndexMarker773"/> involved in implementing TRPO:</p>
			<ul>
				<li>Since the KL divergence constraint is approximated with its second-order Taylor expansion, there could be cases where the constraint is violated.</li>
				<li>This is where the <img src="image/Formula_07_215.png" alt=""/> term comes in: It shrinks the magnitude of the gradient update until the constraint is satisfied. To this end, there is a line search followed once <img src="image/Formula_07_216.png" alt=""/> and <img src="image/Formula_07_217.png" alt=""/> are estimated: Starting with <img src="image/Formula_07_218.png" alt=""/>, <img src="image/Formula_07_219.png" alt=""/> increases by one until the magnitude of the update shrinks enough so that the constraint is satisfied. </li>
				<li>Remember that <img src="image/Formula_07_220.png" alt=""/> is an <img src="image/Formula_07_221.png" alt=""/> matrix, which could be huge depending on the size of the policy network and therefore expensive to store.</li>
				<li>Since <img src="image/Formula_07_222.png" alt=""/> is estimated through samples, given its size, there could be a lot of inaccuracies introduced during the estimation.</li>
				<li>Calculating and storing <img src="image/Formula_07_223.png" alt=""/> is an even more painful step. </li>
				<li>To avoid the complexity of dealing with <img src="image/Formula_07_224.png" alt=""/> and <img src="image/Formula_07_225.png" alt=""/>, the authors use the conjugate gradient algorithm, which allows you to take gradient steps without building the whole matrix and taking the inverse.</li>
			</ul>
			<p>As you can see, implementing<a id="_idIndexMarker774"/> TRPO could be complex and we have omitted the details of its implementation. That is why a simpler algorithm that works along the same lines, PPO, is more popular and more widely used, which we'll cover next. </p>
			<h2 id="_idParaDest-165"><a id="_idTextAnchor166"/>PPO – Proximal Policy Optimization</h2>
			<p>PPO is again motivated by <a id="_idIndexMarker775"/>maximizing the policy<a id="_idIndexMarker776"/> improvement objective:</p>
			<div>
				<div id="_idContainer1178" class="IMG---Figure">
					<img src="image/Formula_07_226.jpg" alt=""/>
				</div>
			</div>
			<p>while keeping <img src="image/Formula_07_227.png" alt=""/> close to <img src="image/Formula_07_228.png" alt=""/>. There are two variants of PPO: PPO-Penalty and PPO-Clip. The latter is simpler, which we'll focus<a id="_idIndexMarker777"/> on here.</p>
			<h3>PPO-clip surrogate objective</h3>
			<p>A simpler way of achieving the<a id="_idIndexMarker778"/> closeness between the old and new policy compared to TRPO is to clip the objective so that deviating from the existing policy would not bring in additional benefits. More formally, PPO-clip maximizes the surrogate objective here: </p>
			<div>
				<div id="_idContainer1181" class="IMG---Figure">
					<img src="image/Formula_07_229.jpg" alt=""/>
				</div>
			</div>
			<p>Where <img src="image/Formula_07_230.png" alt=""/> is defined as follows:</p>
			<div>
				<div id="_idContainer1183" class="IMG---Figure">
					<img src="image/Formula_07_231.jpg" alt=""/>
				</div>
			</div>
			<p>This simply says, if the advantage <img src="image/Formula_07_232.png" alt=""/> is positive, then the minimization takes the form:</p>
			<div>
				<div id="_idContainer1185" class="IMG---Figure">
					<img src="image/Formula_07_233.jpg" alt=""/>
				</div>
			</div>
			<p>which therefore clips the maximum value the ratio can take. This means even if the tendency is to increase the likelihood of taking action <img src="image/Formula_07_234.png" alt=""/> in state <img src="image/Formula_07_235.png" alt=""/> because it corresponds to a positive advantage, we clip how much this likelihood can deviate from the existing policy. As a<a id="_idIndexMarker779"/> result, further deviation does not contribute to the advantage.</p>
			<p>Conversely, if the advantage is negative, the expression instead becomes:</p>
			<div>
				<div id="_idContainer1188" class="IMG---Figure">
					<img src="image/Formula_07_236.jpg" alt=""/>
				</div>
			</div>
			<p>which in similar ways, limits how much the likelihood of taking action <img src="image/Formula_07_237.png" alt=""/> in state <img src="image/Formula_07_238.png" alt=""/> can decrease. This ratio is bounded by <img src="image/Formula_07_239.png" alt=""/>.</p>
			<p>Next, let's lay out the full PPO algorithm.</p>
			<h3>The PPO algorithm</h3>
			<p>The PPO algorithm <a id="_idIndexMarker780"/>works as follows:</p>
			<ol>
				<li value="1">Initialize the actor and critic network(s), <img src="image/Formula_07_117.png" alt=""/> and <img src="image/Formula_07_241.png" alt=""/>.<p><em class="italic">while some stopping criterion is not met do:</em></p></li>
				<li>Collect a batch of <img src="image/Formula_07_242.png" alt=""/> samples <img src="image/Formula_07_243.png" alt=""/> from the (parallel) environment(s) using <img src="image/Formula_07_244.png" alt=""/>.</li>
				<li>Obtain the state-value targets <img src="image/Formula_07_245.png" alt=""/>.</li>
				<li>Use gradient descent to update <img src="image/Formula_07_246.png" alt=""/> with respect to a loss function <img src="image/Formula_07_247.png" alt=""/>, such as squared loss.</li>
				<li>Obtain the<a id="_idIndexMarker781"/> advantage value estimates<p> <img src="image/Formula_07_248.png" alt=""/>.</p></li>
				<li>Take a gradient ascent step towards maximizing the surrogate objective function<p class="figure-caption">  </p><div id="_idContainer1201" class="IMG---Figure"><img src="image/Formula_07_249.jpg" alt=""/></div><p>and update <img src="image/Formula_07_250.png" alt=""/>. Although we don't provide the explicit form of this gradient update, it can be easily achieved through packages such as TensorFlow.</p></li>
				<li>Broadcast the new <img src="image/Formula_06_096.png" alt=""/> to the rollout workers.<p><em class="italic">end while</em></p></li>
			</ol>
			<p>Finally, note that the <a id="_idIndexMarker782"/>architecture of a PPO implementation would be very similar to that of A2C with synchronous sampling and policy updates in the rollout workers. </p>
			<h3>PPO using RLlib</h3>
			<p>Very similar to how we<a id="_idIndexMarker783"/> imported the agent trainer classes for the earlier algorithms, the PPO class can be imported as follows:</p>
			<p class="source-code">from ray.rllib.agents.ppo.ppo import (</p>
			<p class="source-code">    DEFAULT_CONFIG,</p>
			<p class="source-code">    PPOTrainer as trainer)</p>
			<p>Again, we will present the training results later in the chapter.</p>
			<p>That concludes our discussion on trust-region methods. The last class of algorithms we'll present in this chapter is off-policy approaches for policy-based methods.</p>
			<h1 id="_idParaDest-166"><a id="_idTextAnchor167"/>Off-policy methods</h1>
			<p>One of the challenges<a id="_idIndexMarker784"/> with policy-based methods is that they are on-policy, which requires collecting new samples after every policy update. If it is costly to collect samples from the environment, then training on-policy methods could be really expensive. On the other hand, the value-based methods we covered in the previous chapter are off-policy but they only work with discrete action spaces. Therefore, there is a need for a class of methods that work with continuous action spaces and off-policy. In this section, we'll cover such algorithms. Let's start with the first one: Deep <a id="_idIndexMarker785"/>Deterministic Policy Gradient.</p>
			<h2 id="_idParaDest-167"><a id="_idTextAnchor168"/>DDPG – Deep Deterministic Policy Gradient</h2>
			<p>DDPG, in some sense, is an <a id="_idIndexMarker786"/>extension of deep Q-learning to continuous action spaces. Remember that deep Q-learning methods<a id="_idIndexMarker787"/> learn a representation for action values,<img src="image/Formula_07_252.png" alt=""/>. The best action is then given by <img src="image/Formula_07_253.png" alt=""/> in a given state <img src="image/Formula_05_010.png" alt=""/>. Now, if the action space is continuous, learning the action-value representation is not a problem. However, then, it would be quite cumbersome to execute the max operation to get the best action over a continuous action space. DDPG addresses this issue. Let's see how next.  </p>
			<h3>How DDPG handles continuous action spaces</h3>
			<p>DDPG simply learns another approximation, <img src="image/Formula_07_255.png" alt=""/>, that estimates the best action given the state. If you wonder why this works, consider the following thought process:</p>
			<ul>
				<li>DDPG assumes that the continuous action space is differentiable with respect to <img src="image/Formula_07_256.png" alt=""/>. </li>
				<li>For a moment, also assume that the action values, <img src="image/Formula_07_257.png" alt=""/>, are known or have been learned. </li>
				<li>Then the problem simply becomes learning a function approximation, <img src="image/Formula_07_258.png" alt=""/>,  whose input is <img src="image/Formula_05_048.png" alt=""/>, output is <img src="image/Formula_07_260.png" alt=""/>, and the parameters are <img src="image/Formula_07_261.png" alt=""/>. The "reward" for the optimization procedure is simply provided by <img src="image/Formula_07_262.png" alt=""/>. </li>
				<li>We can therefore use a<a id="_idIndexMarker788"/> gradient ascent method to optimize <img src="image/Formula_07_246.png" alt=""/>.</li>
				<li>Over time, the learned action values will hopefully converge and be less of a moving target for the policy function training. <p class="callout-heading">Info</p><p class="callout">Because DDPG assumes the differentiability of the policy function with respect to action, it can only be used with continuous action spaces.</p></li>
			</ul>
			<p>Next, let's look into a few more details about the DDPG algorithm.</p>
			<h3>The DDPG algorithm</h3>
			<p>Since DDPG is an extension of <a id="_idIndexMarker789"/>deep Q-learning, plus learning a policy function, we don't need to write the full algorithm here. In addition, many approaches we discussed in the previous chapter, such as prioritized experience replay and multi-step learning, can be used to form a DDPG variate. The original DDPG algorithm, on the other hand, is closer to the DQN algorithm and uses the following:</p>
			<ul>
				<li>A <strong class="bold">replay buffer</strong> to store<a id="_idIndexMarker790"/> experience tuples, from which the sampling is done uniformly at random.</li>
				<li>A <strong class="bold">target</strong> <strong class="bold">network</strong>, which is <a id="_idIndexMarker791"/>updated using Polyak averaging as in <img src="image/Formula_07_264.png" alt=""/>, rather than syncing it with the behavior network every <img src="image/Formula_07_265.png" alt=""/> steps.</li>
			</ul>
			<p>DDPG then replaces the target calculation in the DQN algorithm: </p>
			<div>
				<div id="_idContainer1218" class="IMG---Figure">
					<img src="image/Formula_07_266.jpg" alt=""/>
				</div>
			</div>
			<p>with this one:</p>
			<div>
				<div id="_idContainer1219" class="IMG---Figure">
					<img src="image/Formula_07_267.jpg" alt=""/>
				</div>
			</div>
			<p>Another important difference <a id="_idIndexMarker792"/>between DQN and DDPG is that DQN uses <img src="image/Formula_07_269.png" alt=""/>-greedy actions during training. The policy network in DDPG, however, provides a deterministic action for a given state, hence the word "deterministic" in the name. To enable exploration during training, some noise is added to the action. More formally, the action is obtained as follows:</p>
			<div>
				<div id="_idContainer1221" class="IMG---Figure">
					<img src="image/Formula_07_270.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_05_272.png" alt=""/> can be chosen as white noise (although the original implementation uses what is called OU noise). In this operation, <img src="image/Formula_07_272.png" alt=""/> and <img src="image/Formula_07_273.png" alt=""/> represent the boundaries of the continuous action space.</p>
			<p>That's what DDPG is about. Let's look<a id="_idIndexMarker793"/> into how it can be parallelized next.</p>
			<h3>Ape-X DDPG</h3>
			<p>Given the similarity between <a id="_idIndexMarker794"/>deep Q-learning and DDPG, parallelization for DDPG can easily be achieved using the Ape-X framework. In fact, the original Ape-X paper presents DDPG next to DQN in their implementation. It improves the regular DDPG performance by several orders of magnitude in some benchmarks. The authors also show that the wall-clock time performance consistently increased with the increased number of rollout workers (actors).</p>
			<h3>DDPG and Ape-X DPG using RLlib</h3>
			<p>The trainer class and the <a id="_idIndexMarker795"/>configs for DDPG can be imported as follows:</p>
			<p class="source-code">from ray.rllib.agents.ddpg.ddpg import(</p>
			<p class="source-code">    DEFAULT_CONFIG,</p>
			<p class="source-code">    DDPGTrainer as trainer)</p>
			<p>Similarly, for Ape-X DDPG, we import the following:</p>
			<p class="source-code">from ray.rllib.agents.ddpg.apex import (</p>
			<p class="source-code">    APEX_DDPG_DEFAULT_CONFIG as DEFAULT_CONFIG,</p>
			<p class="source-code">    ApexDDPGTrainer as trainer) </p>
			<p>That's it! The rest is pretty much the same as the training flow we described at the beginning of the chapter. Now, before going into algorithms that improve DDPG, let's discuss where the DDPG <a id="_idIndexMarker796"/>algorithm falls short. </p>
			<h3>DDPG's shortcomings</h3>
			<p>Despite its initial popularity, there are<a id="_idIndexMarker797"/> several issues that the algorithm runs into:</p>
			<ul>
				<li>It can be quite sensitive to hyperparameter selections.</li>
				<li>It runs into the issue of maximization bias while learning action values.</li>
				<li>Spikes in the action-value estimates (which are potentially erroneous) are exploited by the policy network and derail the learning.</li>
			</ul>
			<p>Next, we'll look into the TD3 algorithm, which introduces a set of improvements to address these issues.</p>
			<h2 id="_idParaDest-168"><a id="_idTextAnchor169"/>TD3 – Twin Delayed Deep Deterministic Policy Gradient</h2>
			<p>The deal with the TD3 algorithm is<a id="_idIndexMarker798"/> that it addresses the<a id="_idIndexMarker799"/> function approximation errors in DDPG. As a result, it greatly outperforms DDPG, PPO, TRPO, and SAC in terms of the maximum reward in OpenAI's continuous control benchmarks. Let's look into what TD3 proposes.</p>
			<h3>TD3 improvements over DDPG</h3>
			<p>There are three<a id="_idIndexMarker800"/> main improvements in TD3 over DDPG:</p>
			<ul>
				<li>It learns two (twin) Q networks rather than one, which in turn creates two target Q networks. The <img src="image/Formula_07_274.png" alt=""/> targets are then obtained using the following:</li>
			</ul>
			<div>
				<div id="_idContainer1226" class="IMG---Figure">
					<img src="image/Formula_07_275.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_07_276.png" alt=""/> is the target action for a given state <img src="image/Formula_07_277.png" alt=""/>. This is a form of double Q-learning to overcome the maximization bias.</p>
			<ul>
				<li>During training, the policy and the<a id="_idIndexMarker801"/> target networks are updated more slowly compared to the Q network updates, where the recommended cycle is to have a policy update for every two Q network updates, hence the word "delayed" in the algorithm name. </li>
				<li>The target action, <img src="image/Formula_07_278.png" alt=""/>, is obtained after some noise is added to the policy network outcome:</li>
			</ul>
			<div>
				<div id="_idContainer1230" class="IMG---Figure">
					<img src="image/Formula_07_279.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_05_272.png" alt=""/> is some white noise. Note that this is different noise than what is used to explore in the environment. The role of this noise is to act as a regularizer and prevent the policy network from exploiting some action values that are incorrectly estimated by the Q network as very high and non-smooth in its region.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">Just like DDPG, TD3 can only be used with continuous action spaces.</p>
			<p>Next, let's see how you can train an RL agent using RLlib's TD3 implementation.</p>
			<h3>TD3 using RLlib</h3>
			<p>The TD3 trainer <a id="_idIndexMarker802"/>class can be imported as follows:</p>
			<p class="source-code">from ray.rllib.agents.ddpg.td3 import (</p>
			<p class="source-code">    TD3_DEFAULT_CONFIG as DEFAULT_CONFIG,</p>
			<p class="source-code">    TD3Trainer as trainer)</p>
			<p>On the other hand, if you look into the code in the <strong class="source-inline">td3.py</strong> module of RLlib, you will see that it simply modifies the default DDPG configs and uses the DDPG trainer class under the hood. This means that TD3 improvements are optionally available in the DDPG trainer class, and you can modify them to obtain an Ape-X TD3 variant.</p>
			<p>That's it with TD3. Next, we'll discuss SAC.</p>
			<h2 id="_idParaDest-169"><a id="_idTextAnchor170"/>SAC – Soft actor-critic</h2>
			<p>SAC is another popular algorithm<a id="_idIndexMarker803"/> that brings even further<a id="_idIndexMarker804"/> improvements to TD3. It uses entropy as part of the reward to encourage exploration:</p>
			<div>
				<div id="_idContainer1232" class="IMG---Figure">
					<img src="image/Formula_07_281.jpg" alt=""/>
				</div>
			</div>
			<p>where <img src="image/Formula_07_282.png" alt=""/> is the entropy term and <img src="image/Formula_07_283.png" alt=""/> is the corresponding weight.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">SAC can be used both with continuous and discrete action spaces.</p>
			<p>To import the SAC trainer, use the following code:</p>
			<p class="source-code">from ray.rllib.agents.sac.sac import (</p>
			<p class="source-code">    DEFAULT_CONFIG,</p>
			<p class="source-code">    SACTrainer as trainer)</p>
			<p>The final algorithm we will discuss is IMPALA.</p>
			<h2 id="_idParaDest-170"><a id="_idTextAnchor171"/>IMPALA – Importance Weighted Actor-Learner Architecture</h2>
			<p>IMPALA is an <a id="_idIndexMarker805"/>algorithm that is of the policy-gradient type, as opposed to DDPG, TD3, and SAC, which are essentially <a id="_idIndexMarker806"/>value-based methods. As a result, IMPALA is not completely an off-policy method. Actually, it is similar to A3C but with the following key differences:</p>
			<ul>
				<li>Unlike A3C, it sends sampled experiences (asynchronously) to the learner(s) rather than parameter gradients. This significantly reduces the communication overhead.</li>
				<li>When a sample trajectory arrives, chances are it was obtained under a policy that is several updates behind the policy in the learner. IMPALA uses truncated importance sampling to account for the policy lag while calculating the value function targets.</li>
				<li>IMPALA allows multiple synchronous worker learners to calculate gradients from samples.</li>
			</ul>
			<p>The IMPALA trainer class can be imported in to RLlib as follows:</p>
			<p class="source-code">from ray.rllib.agents.impala.impala import (</p>
			<p class="source-code">    DEFAULT_CONFIG,</p>
			<p class="source-code">    ImpalaTrainer as trainer)</p>
			<p>This concludes our discussion on algorithms. Now the fun part! Let's compare their performance in OpenAI's continuous-control Lunar Lander environment!</p>
			<h1 id="_idParaDest-171"><a id="_idTextAnchor172"/>A comparison of the policy-based methods in Lunar Lander</h1>
			<p>Here is a comparison of <a id="_idIndexMarker807"/>evaluation reward performance progress for different policy-based algorithms over a single training session in the<a id="_idIndexMarker808"/> Lunar Lander environment:</p>
			<div>
				<div id="_idContainer1235" class="IMG---Figure">
					<img src="image/B14160_07_6.jpg" alt="Figure 7.6 – Lunar Lander training performance of various policy-based algorithms&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Lunar Lander training performance of various policy-based algorithms</p>
			<p>To also give a sense of how long each training session took and what the performance at the end of the training was, here is a TensorBoard tooltip for the preceding plot:</p>
			<div>
				<div id="_idContainer1236" class="IMG---Figure">
					<img src="image/B14160_07_7.jpg" alt="Figure 7.7 – Wall-clock time and end-of-training performance comparisons&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – Wall-clock time and end-of-training performance comparisons</p>
			<p>Before going into further discussions, let's make the following disclaimer: The comparisons here <a id="_idIndexMarker809"/>should not be taken as a benchmark <a id="_idIndexMarker810"/>of different algorithms for multiple reasons:</p>
			<ul>
				<li>We did not perform any hyperparameter tuning,</li>
				<li>The plots come from a single training trial for each algorithm. Training an RL agent is a highly stochastic process and a fair comparison should include the average of at least 5-10 training trials.</li>
				<li>We use RLlib's implementations of these algorithms, which could be less or more efficient than <a id="_idIndexMarker811"/>other open source implementations.</li>
			</ul>
			<p>After this disclaimer, let's discuss<a id="_idIndexMarker812"/> what we observe in these results:</p>
			<ul>
				<li>PPO attains the highest reward at the end of the training.</li>
				<li>The vanilla policy gradient algorithm is the fastest (in terms of wall-clock time) to reach a "reasonable" reward.</li>
				<li>TD3 and DDPG are really slow in terms of wall-clock time, although they achieve higher rewards than A2C and IMPALA.</li>
				<li>The TD3 training graph is significantly more unstable compared to other algorithms.</li>
				<li>At some point, TD3 achieved higher rewards than PPO with the same number of samples.</li>
				<li>IMPALA was super-fast to reach (and go beyond) 2M samples.</li>
			</ul>
			<p>You can extend this list, but the idea here is that different algorithms could have different advantages and disadvantages. Next, let's discuss what criteria you should consider in picking an algorithm for your application.</p>
			<h1 id="_idParaDest-172"><a id="_idTextAnchor173"/>How to pick the right algorithm</h1>
			<p>As in all machine learning domains, there is no silver bullet in terms of which algorithm to use for different applications. There are many criteria you should consider, and in some cases, some of<a id="_idIndexMarker813"/> them will be more important than others.</p>
			<p>Here are different dimensions of algorithm performance that you should look into when picking your algorithm:</p>
			<ul>
				<li><strong class="bold">Highest reward</strong>: When you<a id="_idIndexMarker814"/> are not bound by compute and time resources and your goal is simply to train the best possible agent for your application, the highest reward is the criterion you should pay attention to. PPO and SAC are promising alternatives here.</li>
				<li><strong class="bold">Sample efficiency</strong>: If your sampling process is costly/time-consuming, then sample efficiency (achieving higher rewards using fewer samples is important). When this is the case, you should look into off-policy algorithms as they reuse past experience for training as on-policy methods are often incredibly wasteful in how they consume samples. SAC is a good starting point in this case.</li>
				<li><strong class="bold">Wall-clock time efficiency</strong>: If your simulator is fast and/or you have resources to do massive parallelization, PPO, IMPALA, and Ape-X SAC are often good choices.</li>
				<li><strong class="bold">Stability</strong>: Your ability to achieve good rewards without running many trials with the same algorithm and them improving consistently during training is also important. Off-policy algorithms could be hard to stabilize. PPO is often a good choice in this respect.</li>
				<li><strong class="bold">Generalization</strong>: If an algorithm requires extensive tuning for each environment you train it for, this could cost you a lot of time and resources. SAC, due to its use of entropy in its reward, is known to be less sensitive to hyperparameter choices.</li>
				<li><strong class="bold">Simplicity</strong>: Having an algorithm that is easy to implement is important to avoid bugs and ensure maintainability. That is the reason TRPO has been abandoned in favor of PPO.</li>
			</ul>
			<p>That is the end of the discussion on the algorithm picking criteria. Lastly, let's go into some resources where you can find easy-to-understand implementations of these algorithms.</p>
			<h1 id="_idParaDest-173"><a id="_idTextAnchor174"/>Open source implementations of policy-gradient methods</h1>
			<p>In this chapter, we have<a id="_idIndexMarker815"/> covered many algorithms. It is not quite possible to explicitly implement all these algorithms given the space<a id="_idIndexMarker816"/> limitations here. We instead relied on RLlib implementations to train agents for our use case. RLlib is open source, so you can go to <a href="https://github.com/ray-project/ray/tree/ray-0.8.5/rllib">https://github.com/ray-project/ray/tree/ray-0.8.5/rllib</a> and dive into implementations of these algorithms.</p>
			<p>Having said that, RLlib implementations are built for production systems and therefore involve many other implementations regarding error-handling and preprocessing. In addition, there is a lot of code reuse, resulting in implementations with multiple class inheritances. A much easier set of implementations is provided by OpenAI's Spinning Up repo at <a href="https://github.com/openai/spinningup">https://github.com/openai/spinningup</a>. I highly recommend you go into that repo and dive into the implementation details of the algorithms we discussed in this chapter.</p>
			<p class="callout-heading">Info</p>
			<p class="callout">OpenAI's Spinning Up is also a great resource to see an overview of RL topics and algorithms, available at <a href="https://spinningup.openai.com">https://spinningup.openai.com</a>. </p>
			<p>That's it! We have come a long way and covered policy-based methods in depth. Congratulations on reaching this important milestone!</p>
			<h1 id="_idParaDest-174"><a id="_idTextAnchor175"/>Summary</h1>
			<p>In this chapter, we covered an important class of algorithms called policy-based methods. These methods directly optimize a policy network, unlike the value-based methods we covered in the previous chapter. As a result, they have a stronger theoretical foundation. In addition, they can be used with continuous action spaces. With this, we have covered model-free approaches in detail. In the next chapter, we go into model-based methods, which aim to learn the dynamics of the environment the agent is in.</p>
			<h1 id="_idParaDest-175"><a id="_idTextAnchor176"/>References</h1>
			<ul>
				<li><em class="italic">Kinds of RL Algorithms</em>: <a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html">https://spinningup.openai.com/en/latest/spinningup/rl_intro2.html</a></li>
				<li><em class="italic">Policy Gradient Methods for Reinforcement Learning with Function Approximation</em>, Richard S. Sutton, David McAllester, Satinder Singh, Yishay Mansour: <a href="https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf">https://papers.nips.cc/paper/1713-policy-gradient-methods-for-reinforcement-learning-with-function-approximation.pdf</a></li>
				<li><em class="italic">Simple Statistical Gradient-Following Algorithms for Connectionist Reinforcement Learning</em>, Ronald J. Williams: <a href="https://link.springer.com/content/pdf/10.1007/BF00992696.pdf">https://link.springer.com/content/pdf/10.1007/BF00992696.pdf</a></li>
				<li><em class="italic">Dota 2 with Large Scale Deep Reinforcement Learning</em>, Christopher Berner, Greg Brockman, et. al: <a href="https://arxiv.org/pdf/1912.06680.pdf">https://arxiv.org/pdf/1912.06680.pdf</a></li>
			</ul>
		</div>
	</body></html>