- en: '11'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Explaining Neural Network Predictions
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever wondered why a facial recognition system flagged a photo of a
    person with a darker skin tone as a false positive while identifying people with
    lighter skin tones correctly? Or why a self-driving car decided to swerve and
    cause an accident, instead of braking and avoiding the collision? These questions
    illustrate the importance of understanding why a model predicts a certain value
    for critical use cases. By providing explanations for a model’s predictions, we
    can gain insights into how the model works and why it made a specific decision,
    which is crucial for transparency, accountability, trust, regulatory compliance,
    and improved performance.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore neural network-specific methods for explaining
    model predictions. Additionally, we will discuss how to quantify the quality of
    an explanation method. We will also discuss the challenges and limitations of
    model explanations and how to evaluate their effectiveness.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following topics will be covered:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the value of prediction explanations
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demystifying prediction explanation techniques
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring gradient-based prediction explanations
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trusting and understanding integrated gradients
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using integrated gradients to aid in understanding the predictions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining prediction explanations automatically
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring common pitfalls in prediction explanations and how to avoid them
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter includes some practical implementations in the Python programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas`'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`captum`'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers-interpret`'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers`'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch`'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_11](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_11).'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the value of prediction explanations
  id: totrans-21
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First off, the concept of explaining a model through its predictions is referred
    to by many other names, including explainable AI, trustable AI, transparent AI,
    interpretable machine learning, responsible AI, and ethical AI. Here, we will
    refer to the paradigm as **prediction explanations**, which is a clear and short
    way to refer to it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction explanations** is not a technique that is adopted by most machine
    learning practitioners. The value of prediction explanations highly depends on
    the exact use case. Even though it is stated that explanations can increase transparency,
    accountability, trust, regulatory compliance, and improved model performance,
    not everybody cares about these points. Instead of understanding the benefits,
    let’s look at it from a different perspective and explore some of the common factors
    that drove practitioners to adopt prediction explanations that can be attributed
    to the following conditions:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
- en: 'The prediction explanations technique provides the following benefits regarding
    the utilization of your built model:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 预测解释技术为利用您构建的模型提供以下好处：
- en: '**Transparency**: Prediction explanations allow model prediction consumers
    to have access to reasons, which will, in turn, enforce the confidence of the
    consumer in the predictions made. A transparent model allows consumers to objectively
    gauge their intelligence, which increases trust and increases adoption.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**透明性**：预测解释使得模型预测的消费者能够了解理由，这反过来会增强消费者对预测结果的信任。一个透明的模型使消费者能够客观评估其智能，从而增加信任度并促进采用。'
- en: '**Accountability**: Prediction explanations allow consumers to perform root
    cause analysis, which is especially important in critical use cases or in use
    cases where there is a human in the loop to make the final decision with a model’s
    prediction as a reference.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问责制**：预测解释使得消费者能够进行根本原因分析，这在关键使用案例或在决策过程中有人工干预、以模型预测作为参考的使用案例中尤其重要。'
- en: '**Trust**: Without trust, nobody will use the prediction of the model. Prediction
    explanations provide a small boost toward achieving higher trust.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**信任**：没有信任，没人会使用模型的预测。预测解释有助于在建立更高信任的过程中提供一小步推动。'
- en: '**Regulatory compliance**: Some governments enforce laws that require decisions
    made by computer systems to be explainable in certain industries such as banks
    and insurance.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**合规性要求**：一些政府执行法律，要求在某些行业（如银行和保险）中，由计算机系统做出的决策必须可解释。'
- en: '**Metric performance**: The capability to perform root cause analysis can lead
    to a better understanding of either the model’s behavior or the training dataset.
    This will, in turn, allow machine learning practitioners to improve or fix the
    issues found using prediction explanations, and eventually lead to improved metric
    performance.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**度量性能**：进行根本原因分析的能力可以帮助更好地理解模型的行为或训练数据集。这反过来又能使机器学习从业者通过预测解释来改进或修复发现的问题，最终提高度量性能。'
- en: 'This list of benefits makes it worth it to utilize prediction explanations
    on any use case and model. However, the value of using prediction explanations
    increases exponentially with certain conditions. When the utilization is based
    on a specific goal, the method becomes twice as useful. Let’s take a step further
    and explore some of the common conditions that drove practitioners to utilize
    the prediction explanations technique:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这份好处清单使得在任何使用案例和模型中使用预测解释都变得值得。然而，在某些条件下，使用预测解释的价值呈指数增长。当使用是基于特定目标时，该方法的效果将加倍。让我们进一步探讨一些促使从业者采用预测解释技术的常见条件：
- en: '**Critical and high-impact use case**: In these use cases, the model’s decisions
    can typically result in significant consequences for human welfare, safety, or
    well-being. Understanding the model’s behavior in every way can help mitigate
    the worst case from happening. This can range from billions of money lost to actual
    human life lost.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**关键和高影响力的使用案例**：在这些使用案例中，模型的决策通常会对人类福利、安全或福祉产生重大后果。全面理解模型的行为有助于降低最坏情况的发生风险。这些后果可能从数十亿的损失到实际的人命丧失。'
- en: '**Failing to achieve the required threshold for success**: A machine learning
    project can’t move forward to the model deployment stage in the machine learning
    life cycle if it can’t even achieve success thresholds during the model development
    stage. Understanding the behavior of the model toward different inputs can help
    signal whether the data is of bad quality, help indicate whether the data has
    biased patterns that promote overfitting, and generally help debug how to improve
    the model’s performance.'
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**未达到成功所需的阈值**：如果机器学习项目在模型开发阶段无法达到成功的阈值，那么它就无法进入机器学习生命周期中的模型部署阶段。理解模型对不同输入的行为可以帮助判断数据质量是否差，帮助发现数据是否存在促进过拟合的偏差模式，并且通常有助于调试如何改进模型的性能。'
- en: '**The model makes wrong predictions with the simplest examples**: Making wrong
    predictions with the most complex examples is expected, especially when humans
    also could have made mistakes in making the same decisions. When it comes to the
    simplest examples, making an error would indicate that the model is not learning
    the right things. Understanding what a model is focusing on could be key to figuring
    out why a model failed.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regulatory laws mandate accountability for using machine learning models
    in decision-making**: This means prediction explanations will be required not
    typically as a method for the machine learning practitioner to understand how
    the model behaves or with different inputs, but instead, to be used after a model
    is deployed, which will allow the people that consume the predictions understand
    why a certain decision has been made for accountability.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The model fails to make proper predictions after it is deployed**: Have you
    ever thought about whether a good model with high accuracy metric performance
    in your cross-validation setup means that the model pays attention to the right
    things? Even with cross-validation partitioning strategies, data can still be
    very biased toward a certain set of conditions. This means that when the model
    gets deployed into the real world, the data it encounters during the production
    operation can be from a distribution and set of conditions that are different
    than what was available in the original dataset used for model development. When
    this happens, prediction explanations can help uncover biases and unwanted behaviors
    of the model either in the new data or data used for model development. In other
    words, you’ll need to quantify wrong predictions from right predictions.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is fine to not utilize prediction explanation techniques in your use case
    and development workflow. However, when these edge cases happen, know that prediction
    explanation techniques are your key tool to help overcome your obstacles. But
    what exactly do prediction explanations explain? In the next section, we will
    discuss a short overview of prediction explanation methods and dive into the method
    category we will be introducing in this chapter, which is specifically for neural
    networks.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying prediction explanation techniques
  id: totrans-37
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prediction explanations is a technique that attempts to explain the logic behind
    a model’s decision, given input data. Some machine learning models are built to
    be more transparent and explainable out of the box. One example is a decision
    tree model, which is built from the ground up using explicit conditioning rules
    to split data into multiple partitions that result in specific decisions, allowing
    the predictions to be explained through the explicit rules that were used to predict
    the data sample. However, models such as neural networks are treated like a black
    box without any straightforward way to retrieve the reasons the decision was made
    directly.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
- en: 'The logic of a model’s decision on a data sample can be explained and presented
    in a variety of ways, so long it contains information on how the final decision
    was made. Additionally, predictions made by a machine learning model can be explained
    in either a model-agnostic or model-specific way. There are a few types of explanations
    that can be made using the model’s predictions:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: '**Saliency-based explanation**: This is also known as importance attribution,
    feature importance, or feature impact'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparent model logic-based explanation**: Provide a rationale on why a
    decision is made'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exemplar-based explanation**: Using similar data to reason why a label is
    predicted'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Neural networks, being a black box out of the box, can only be explained through
    saliency-based explanation or exemplar-based explanation. However, workarounds
    have been invented to achieve indirect transparent model logic-based explanations
    for neural networks using knowledge distillation methods from a neural network
    to more transparent and interpretable models such as linear models. Additionally,
    attention mechanisms implemented in transformers provide a shallow way to check
    for feature importance through its attention maps as a saliency method but fall
    short in the following components:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: Attention weights are not feature-specific as they are obtained by calculating
    the interactions between all input tokens and the output token of interest. This
    means the weight does not represent a true reliance on an isolated feature.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inputs attended to by the attention mechanism can still not be used in later
    parts of the network.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention maps can be biased and cause some features to be neglected.
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, neural-networks-based explanation techniques should be opted over
    model-agnostic explanation techniques as they are capable of providing more detailed
    and nuanced explanations while being more efficient in terms of both computational
    requirements and time. In this chapter, we will focus on saliency-based explanations
    for neural networks using more reliable methods.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dive into the core workhorse behind neural network
    model-specific explanations, which are gradient-based saliency explanations.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Exploring gradient-based prediction explanations
  id: totrans-49
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most up-to-date neural network-based explanation techniques today are variations
    of using the gradients that can be obtained through backpropagation. Gradient-based
    explanations for neural network models work because they rely on the fundamental
    principle of how the weights in a neural network are updated during the training
    process using backpropagation. During backpropagation, the partial derivatives
    of the loss function concerning the weights in the network are calculated, which
    gives us the gradient of the loss function concerning the weights.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
- en: This gradient provides us with a measure of how much the input data contributes
    to the overall loss. Remember that gradients measure the sensitivity of the input
    value concerning the loss function. This means it provides the degree of fluctuation
    of the predictions when you modify the specific input value, which represents
    the importance of the input data. Input data can be chosen to be the weights of
    the neural network or the actual input to the entire neural network. In most cases,
    the actual input-based explanations are enough to provide clarity on the important
    feature groups. However, sometimes, a more fine-grained explanation is needed
    to decode the underlying characteristics of the highly attributed and important
    actual input data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个梯度为我们提供了输入数据对整体损失的贡献程度。记住，梯度衡量的是输入值相对于损失函数的敏感度。这意味着它提供了修改特定输入值时预测结果波动的程度，代表了输入数据的重要性。输入数据可以选择为神经网络的权重，或者是整个神经网络的实际输入。在大多数情况下，基于实际输入的解释已足以提供对重要特征组的清晰解释。然而，有时需要更精细的解释来解码高度归因且重要的实际输入数据的潜在特征。
- en: 'For example, consider the use case of identifying animal breeds with image
    data using a CNN. For an image with a dog standing on grass, if an attribution
    method signals that both the dog and grass are important, why is the grass important?
    This can be for one of the following reasons:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，考虑使用卷积神经网络（CNN）根据图像数据识别动物品种的情况。如果一张狗站在草地上的图像，某个归因方法显示狗和草地都很重要，那么草地为什么重要呢？这可能是以下几个原因之一：
- en: The fur of the dog and the grass are both identified by a single convolutional
    filter.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狗的毛发和草地是通过一个卷积滤波器识别的。
- en: Fur and grass are explicitly separately identified by different filters but
    shown as important similarly because the model might have overfitted to think
    that dogs need to be accompanied by grass. This might mean that the training dataset
    only contains images of dogs with grass and signals the need to add more dog images
    without grass.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 毛发和草地通过不同的滤波器被明确区分，但由于模型可能过拟合，认为狗必须伴随草地出现，因此它们也被视为同样重要。这可能意味着训练数据集只包含带有草地的狗的图像，暗示需要添加更多没有草地的狗的图像。
- en: This will require diving deeper into important filters that are attributed highly
    among the filters that are activated and have something to do with the grass and
    the dog. Even after pinpointing the filters that are both important, highly activated
    for the image, and have something to do with the grass and the dog, you can’t
    know for sure what patterns the filter is identifying. Is it a high-level feature
    such as the dog’s shape? Or is it fur? In cases like this, it is highly beneficial
    to visualize the patterns directly. This topic will be discussed in more detail
    in [*Chapter 12*](B18187_12.xhtml#_idTextAnchor184), *Interpreting* *Neural Networks*.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这将需要更深入地挖掘在激活的滤波器中与草地和狗相关的那些重要滤波器。即便确定了那些既重要又高度激活的滤波器，并且与草地和狗有关，仍然无法确切知道该滤波器识别的模式是什么。它是一个高级特征，比如狗的形状？还是毛发？在这种情况下，直接可视化这些模式是非常有益的。这个主题将在[*第12章*](B18187_12.xhtml#_idTextAnchor184)中详细讨论，*解释*
    *神经网络*。
- en: Gradients by themselves, however, are not reliable enough to be used as-is as
    an explanation method. More generally, the quality of pure gradients to be used
    as a feature importance explanation method is low. However, the quality of explanation
    methods is not a quantitative metric. The perceived quality of the explanations
    generated is subjective to the requirements of the consumer.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，梯度本身并不可靠，不能直接作为解释方法。更一般来说，纯梯度作为特征重要性解释方法的质量较低。然而，解释方法的质量并不是一个定量度量。生成的解释的感知质量是主观的，取决于消费者的需求。
- en: Consider a scenario where a financial institution wants to develop a credit
    scoring model to determine loan approvals. The institution is interested in using
    an explanation technique to understand which features are most important in the
    model’s predictions. A bank might prioritize an explanation technique that focuses
    on the features that have the greatest impact on creditworthiness, such as credit
    history, income, and outstanding debts. This would help the bank make informed
    lending decisions and manage its risks effectively.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, an individual loan applicant may prefer a more comprehensive
    explanation that provides insight into how the model evaluated their specific
    financial situation, including factors beyond the top three most important features,
    such as their employment history and recent financial hardships. This would help
    the applicant make informed decisions about how to improve their creditworthiness
    in the future. Selecting an appropriate explanation technique is important as
    it should cater to the audience and their specific needs to make the explanation
    clear and useful for its intended purpose.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
- en: Even so, there are axiomatic-based evaluations, which involve evaluating based
    on a set of principles or axioms that are considered desirable. Of all techniques,
    a technique called **integrated gradients** stands out for its focus on neural
    network models while being developed to satisfy widely accepted axioms.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll understand how integrated gradients explain a neural network prediction
    and understand how the method satisfies widely accepted axioms.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
- en: Trusting and understanding integrated gradients
  id: totrans-61
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First off, the integrated gradient technique is available off-the-shelf in a
    few open source libraries such as **shap** and **captum**. We can utilize the
    method from the library without reimplementing the method. However, obtaining
    explanations without understanding the technicalities behind the technique can
    reduce trust in the explanations. If you don’t trust the technique, the explanation
    results themselves hardly mean anything. Subsequently, if you don’t explain your
    predictions, the prediction results themselves hardly mean anything! In this section,
    we will dive into what integrated gradients do so that you can trust what integrated
    gradients can explain to you.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
- en: 'Integrated gradients add a few extra components to the basic gradient-based
    feature importance-based explanations. These components are geared toward satisfying
    a few critical axioms that make or break the reliability of an explanation method.
    These axioms are as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitivity**: A model that produces two different predictions on two data
    samples that differ in that only a single feature should be given a non-zero importance
    score. Activation functions such as ReLU break this axiom with a zero input value
    as the gradients would also be reduced to zero, even when the predictions are
    different between the two predictions.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation invariance**: Two models that have the same performance across
    all possible data must produce the same importance scores. Assume that any external
    data and data received during the entire lifetime of the model is included.'
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**实现不变性**：两个在所有可能数据上表现相同的模型，必须产生相同的重要性得分。假设任何外部数据以及在整个模型生命周期中接收到的数据都包括在内。'
- en: '**Completeness**: One of the best ways to make a good-quality explanation for
    both human and machine learning explanations is to provide counterfactual reports.
    To evaluate model accuracy performance, having a baseline model works the same
    way. For explanation methods, this involves engineering a baseline data sample
    that can produce a neutral prediction. Completeness means that the importance
    score for every feature column must add up to the difference in prediction score
    between the baseline sample and the targeted sample. This axiom can be more useful
    in some use cases, such as in regression, where the prediction is directly used
    as an output rather than in a multiclass setting where the predictions are only
    used for choosing the top class.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**完整性**：对于人类和机器学习的解释，提供反事实报告是制作高质量解释的最佳方法之一。为了评估模型的准确性表现，使用基线模型的效果相同。对于解释方法，这意味着构建一个可以产生中立预测的基线数据样本。完整性意味着每个特征列的重要性得分必须加起来等于基线样本与目标样本之间预测得分的差异。这个公理在一些使用场景中可能更有用，比如回归问题中，预测值直接作为输出，而不是在多类设置中，预测值仅用于选择最优类别。'
- en: '**Linearity**: If you linearly combine two neural network models, which is
    a simple weighted addition, the explanation of the combined neural network on
    a data sample must be the same weighted addition of the individual explanation
    of the two neural network models. Linear behaviors that are implemented in the
    model should be respected. A straightforward axiom is specifically designed as
    some methods generate unaccountable importance values.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**线性**：如果你线性地组合两个神经网络模型，即简单的加权相加，那么在数据样本上的组合神经网络的解释必须等于两个单独神经网络模型解释的加权相加。模型中实现的线性行为应当得到尊重。一个直接的公理专门设计用于一些方法产生无法解释的重要性值。'
- en: 'Now that we understand the core axioms that can be used to compare methods,
    know that integrated gradients satisfy all of these axioms. Simply put, integrated
    gradients integrate the gradients from samples that are sampled from a straight-line
    path (linear interpolation) from a chosen baseline data sample and the target
    data sample and multiply it by the difference between the predictions of the target
    sample and baseline sample. The path integral of the gradients is the area under
    the curve of the gradient values along the straight path. This value represents
    the change in the model output as the input feature’s value changes along the
    path. The rate of change directly translates to feature importance. By multiplying
    this value by the difference between the actual input value and the baseline input
    value, we get the contribution of the feature to the model output for integrated
    gradients. *Figure 11**.1* showcases this process with the results obtained by
    computing integrated gradients on the predictions of a pre-trained ResNet50 model
    on an image of an orange. The model is capable of predicting the orange class
    and predicts the image to be an orange with a 46% probability:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经理解了可以用来比较方法的核心公理，值得注意的是，集成梯度满足所有这些公理。简而言之，集成梯度通过从一条直线路径（线性插值）中采样的样本来集成梯度，路径从选定的基线数据样本到目标数据样本，然后将其乘以目标样本和基线样本预测值之间的差异。梯度的路径积分就是梯度值沿直线路径下的曲线下面积。这个值表示输入特征值沿路径变化时，模型输出的变化。变化速率直接转化为特征重要性。通过将这个值与实际输入值和基线输入值之间的差异相乘，我们得到集成梯度中该特征对模型输出的贡献。*图
    11.1*展示了这个过程，结果是通过在一个橙子图像上计算预训练的 ResNet50 模型预测的集成梯度。该模型能够预测橙子类别，并以46%的概率预测该图像为橙子：
- en: '![Figure 11.1 – An illustration of how integrated gradients are computed for
    an image of an orange with a pre-trained ResNet50 model that is capable of predicting
    the orange class](img/B18187_11_01.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.1 – 使用预训练的 ResNet50 模型计算橙子图像的集成梯度的示意图，该模型能够预测橙子类别](img/B18187_11_01.jpg)'
- en: Figure 11.1 – An illustration of how integrated gradients are computed for an
    image of an orange with a pre-trained ResNet50 model that is capable of predicting
    the orange class
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
- en: 'More intuitively, this means that integrated gradients measure the rate of
    change of model predictions as the target input feature’s value changes toward
    a neutral baseline sample and provides accountable importance that is properly
    scaled according to the predictions:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
- en: Feature importances = prediction difference x Area under gradients along path
    curve
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
- en: This allows it to capture the sensitivity of the model’s output to the feature
    across its entire range of values. Integrated gradients ensure that the contribution
    of each feature is proportional to its effect on the model’s output by aggregating
    the gradients from samples in the linear interpolated path between the baseline
    and target. Using plain gradients as the core allows integrated gradients to satisfy
    implementation invariance as the chain rule used to obtain partial derivatives
    allows gradients to be computed partially.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
- en: For integrated gradients, if a feature has twice the effect on the model’s output
    compared to another feature, its attribution score will be twice as high because
    the gradients along the path will be twice as high for that feature, thus satisfying
    linearity. Additionally, since the importance is scaled by the difference in predictions
    between the baseline input and target input, integrated gradients will satisfy
    the completeness axiom. These are local explanations that provide reasons for
    predictions through individual data samples. As a bonus, by aggregating the local
    feature importance of all data samples through mean or median, you can obtain
    the global feature importance of the model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will attempt to use integrated gradients to explain predictions from
    a model.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
- en: Using integrated gradients to aid in understanding predictions
  id: totrans-76
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of writing, two packages provide easy-to-use classes and methods
    to compute integrated gradients, which are the `captum` and `shap` libraries.
    In this tutorial, we will be using the `captum` library. The `captum` library
    supports models from TensorFlow and PyTorch. We will be using PyTorch here. In
    this tutorial, we will be working on explaining a SoTA transformer model called
    **DeBERTA** on the task of text sentiment multiclass classification. Let’s go
    through the use case step by step:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the necessary libraries and methods:'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, we will import a custom text sentiment dataset made for this tutorial:'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This data contains 100 rows with a balanced distribution between three sentiment
    classes called “neutral”, “positive,” and “negative.” We will dive into a few
    samples of this dataset later.
  id: totrans-82
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we will define the label mapping that will be used to map the sentiment
    labels to a numeric ID and vice versa:'
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we will create a method that applies text pre-processing that tokenizes
    the text data using a pre-trained, byte-pair-encoding-based tokenizer:'
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-86
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To train a model, we need to have a cross-validation strategy, so we will use
    a simple train and validation split here:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Since we will be using the PyTorch model from the Hugging Face Transformers
    library, we need to use the PyTorch dataset format and split the data that will
    be used to train the model. Here, we will define the PyTorch dataset and initialize
    both the training and validation dataset instances:'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now that the dataset has been pre-processed and is ready to be used for training,
    let’s load our randomly initialized DeBERTA model from Hugging Face:'
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-92
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Since this is a multiclass setup, we will use the accuracy metric that will
    be computed at each epoch, along with the cross-entropy loss, which will be used
    to train the model:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now, we will define the training arguments:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This will save the checkpoints in the `results` folder, train the model with
    100 iterations of warmup, load the best model at the end of training, and set
    the model learning rate to a very small number of 0.000025, which is required
    to properly converge to a solution.
  id: totrans-97
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will initialize the Hugging Face `trainer` instance, which will take
    in the training arguments and use them to execute the actual training process:'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Notice that an early stopping of 20 iterations is used. This stops the model
    from being trained when the model doesn’t improve on the validation partition
    for the specified iterations, along with 1,000 epochs of training.
  id: totrans-100
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, let’s train and print the final best model evaluation score!
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-102
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This will result in the following output:'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '95.23% accuracy is a pretty good score for a multiclass model. You might think
    that the model is good enough to be deployed, but is it more than what meets the
    eye? Let’s investigate the model’s behavior through prediction explanations using
    integrated gradients and see whether the model is performing. First, let’s define
    the explainer instance from the `transformers_interpret` library:'
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-106
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `transformers_interpret` library uses the `captum` library under the hood
    and implements methods and classes to make it straightforward to explain and visualize
    text importance based on models made using the Hugging Face Transformers library.
    Among all these things, we’re mapping the importance score to a color code and
    mapping token IDs back to the actual token string.
  id: totrans-107
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we’ll explain two samples for the negative and positive labels from the
    validation partition, which is conveniently located in the first three indices:'
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-109
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This will result in the output displayed in *Figure 11**.2*:'
  id: totrans-110
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 11.2 – \uFEFFtransformers_interpret-based integrated gradients results\
    \ on the validation dataset](img/B18187_11_02.jpg)"
  id: totrans-111
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – transformers_interpret-based integrated gradients results on the
    validation dataset
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: The words highlighted in green represent a positive attribution, while those
    highlighted in red represent a negative attribution toward the predicted label
    class. The darker the green, the stronger the positive attribution toward the
    predicted label class. The darker the red, the stronger the negative attribution
    toward the predicted label class in general.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
- en: The first and second examples show correctly predicted positive sentiment sentences.
    For the first example, based on common human sense, *amazing* is the word that
    should contribute the most to positive attribution. For the second example, *happy*
    should be the word that is emphasized as the word contribution toward positive
    sentiment. However, for both examples, the word *bored* is used instead as a strong
    indicator of the positive sentiment prediction, which is not the behavior that
    we want. This indicates that the dataset has a bias where the word *bored* could
    be present in all samples labeled with positive sentiment.
  id: totrans-114
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The third and fourth examples show a correctly predicted negative sentiment.
    In the third example, the sentences *never going to end* and *keeps dragging on*
    should be the focus of the negative sentiment prediction. In the fourth example,
    the word *wrong* and the phrases *has gone wrong* and *so frustrated* should be
    the focus of the negative sentiment prediction. However, both samples consistently
    show dependence on the word *day* to get their negative sentiment prediction.
    This indicates that the dataset has a bias regarding the word *day* occurring
    frequently in the samples that are labeled with negative sentiment.
  id: totrans-115
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All of this means that either the data has to be prepared all over again or
    more data that diversifies the distribution of word usage should be added to the
    dataset so that a proper model can be built.
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This tutorial demonstrates a single type of benefit that can be derived from
    utilizing prediction explanations. In particular, this shows the case where the
    model is not capturing the behavior required to be reliably deployed, even when
    the accuracy-based metric has a good score.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: The integrated gradients technique is a flexible technique that can be applied
    to any neural network model for any type of input variable type. To benefit from
    the explanation results, you will need to derive meaningful insights in the context
    of the business goal. To derive meaningful insights and conclusions from the results
    of integrated gradients, it’s essential to apply common sense and logical reasoning,
    as was presented in the tutorial manually. However, there is a method that you
    can use to try to obtain assistance in obtaining meaningful insights, especially
    when there is too much data and too many variations to decode manually. We will
    dive into this in the next section.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Explaining prediction explanations automatically
  id: totrans-119
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One useful method that helps in deriving insights from prediction explanations
    is none other than using **large language models** (**LLMs**) such as ChatGPT.
    ChatGPT is a transformer model that is trained to provide results that match logical
    reasoning related to the instructions provided. The theory here is that if you
    can format your prediction explanations data in a way that it can be fed into
    a transformer model, and instruct the LLM to derive insights from it, you will
    be able to obtain insights from multiple different perspectives.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous tutorial, we attempted to explain the explanations of four
    different samples consisting of two correctly predicted positive sentiment examples
    and two correctly predicted negative sentiment examples. Now, let’s use an LLM
    model to gain insights. Here, we will separately generate insights for the two
    correctly identified positive sentiment and two correctly identified negative
    sentiment examples as it somehow tends to provide more reasonable results compared
    to just plugging in all four examples. We used the following prompt:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Note that the list of word tokens and attribution tuples are the contents of
    the `word_attributions` variable in the previous tutorial. For this prompt, the
    ChatGPT model returned the following response without modification:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: '**Based on the given examples, it seems that the model is performing well in
    predicting positive sentiment, as the predicted label is correct in both cases.
    However, it is important to note that the token attributions should be interpreted
    with caution, as they are not always a reliable indicator of the importance of
    each word in** **the sentence.**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '**Looking at the token attributions for the first example, we can see that
    the highest attributed word is “bored” with a score of 0.494\. This might seem
    odd for a positive sentiment sentence, but it is possible that the model is attributing
    high importance to this word because it is negating a negative sentiment that
    was initially present. The model might have also learned that overcoming boredom
    is something that people consider to be positive. The second highest attributed
    words are “I” and “was”, both with scores above 0.3, which is reasonable since
    they indicate the writer’s personal experience and opinion. The other highly attributed
    words such as “to”, “from”, and “new” also seem to make sense in the context of**
    **the sentence.**'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '**Looking at the token attributions for the second example, we can see that
    the highest attributed word is “bored” again, with a score of 0.709\. This could
    be because the model is placing a lot of emphasis on the fact that the job keeps
    the writer from getting bored, which is seen as a positive thing. The second highest
    attributed word is “from” with a score of 0.415, which makes sense in the context
    of the sentence since it is indicating what the job is keeping the writer from
    doing. Other highly attributed words such as “to” and “that” also seem to make
    sense in the context of** **the sentence.**'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall, while the model is performing well in predicting positive sentiment,
    it is important to note that token attributions can be noisy and should be interpreted
    with caution. The highly attributed words seem to make sense in the context of
    the sentences, but it is important to look at the entire sentence and not just
    individual words to fully understand** **the sentiment.**'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: 'This response provides a different take on our original insight and mentions
    the possibility that nothing is wrong with some skepticism. Overall, this is a
    nice response that gives us a different perspective on what could happen and signals
    that an LLM can be used to explain the results of prediction explanation as an
    assistance tool. Now, let’s replace the attribution results with the two examples
    that are correctly identified as negative sentiments with the following list:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The results of the ChatGPT model provided the following response:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '**From the provided examples, it seems like the model is correctly predicting
    the negative sentiment in both cases. However, there are a few things that could
    be analyzed to determine whether the model is performing well** **or not.**'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: '**Firstly, the token attributions provided by the integrated gradients method
    indicate the contribution of each token towards the final prediction of the model.
    In both examples, the tokens with the highest positive attribution scores are
    “day” and “has gone wrong”, which seems to make sense as they are indicative of**
    **negative sentiment.**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '**However, one issue with the provided token attributions is that some of the
    tokens have negative attribution scores, which means that they are contributing
    negatively towards the final prediction. For example, in the first example, the
    token “feels” has a negative attribution score, but it could be argued that it
    is actually indicative of negative sentiment. Similarly, in the second example,
    the token “’m” has a negative attribution score, but it is a contraction of “I
    am”, which is indicative of** **negative sentiment.**'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '**Another thing to consider is the relative score of the highest attributed
    tokens. In the first example, the token “day” has a significantly higher attribution
    score than any other token, which indicates that it is the most important feature
    for predicting negative sentiment in this case. However, in the second example,
    there are multiple tokens with similarly high attribution scores, such as “has
    gone wrong” and “frustrated”, which suggests that they are both equally important
    features for predicting** **negative sentiment.**'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall, based on the provided examples and token attributions, it seems
    like the model is performing reasonably well in predicting negative sentiment.
    However, the analysis of the token attributions suggests that there may be some
    issues with the way the integrated gradients method is attributing scores to certain
    tokens. It is also important to consider the relative score and interpretability
    of the highest attributed tokens to determine whether they make sense in the context
    of the** **predicted label.**'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of ChatGPT’s responses seem to signal that it could be a valid behavior.
    Since most neural networks are configured to be non-linear models with many non-linear
    activations, remember that the model is capable of learning complex interactions
    between tokens. This means that tokens are mostly perceived by the model in a
    complex way that might make them depend on one another. A token that has a high
    attribution in the original context when applied to another context might not
    produce the same attribution as multiple tokens can depend on each other as a
    prerequisite to produce a strong attribution. Additionally, tokens are not restricted
    to interacting with tokens that are directly beside them and can be in any position
    of a text row. One way to verify whether the token is overfitted is by predicting
    on text data that only has the `day` token to see whether it will predict it as
    a negative sentiment and the `bored` token to see whether it will predict it as
    a positive sentiment:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This will return the following response:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: It is interesting to see that a single token for both `day` and `bored` will
    result in the prediction being a neutral sentiment. The more repetition tokens
    you add, the more the prediction will skew toward negative and positive sentiment,
    respectively. This proves that the model is indeed biased toward these specific
    words and is not using the words in the right way, as ChatGPT said. Note that
    it could very much be what ChatGPT predicted for your case.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial showcases the explanations of text-based neural networks. However,
    it can be applied similarly to other data types, such as numerical data, categorical
    data, or any data that can be reliably represented in text format. Specifically
    for image-based prediction explanations, you can use models such as GPT-4 from
    OpenAI, which accepts image data along with text data. You can also consider using
    the visualization from the `transformers_interpret` library as an image and feed
    it into GPT-4!
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will dive into common pitfalls when trying to explain your predictions
    and recommendations to avoid these pitfalls.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Exploring common pitfalls in prediction explanations and how to avoid them
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although prediction explanations have proven to be valuable tools in understanding
    AI models, several common pitfalls can hinder their effectiveness. In this section,
    we will discuss these pitfalls and provide strategies to avoid them, ensuring
    that prediction explanations remain a valuable resource for understanding and
    improving AI models. Some of the common pitfalls, along with their solutions,
    are as follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: '**Over-reliance on explanations**: While prediction explanations can provide
    valuable insights into a model’s decision-making process, over-relying on these
    explanations can lead to incorrect conclusions. It’s important to remember that
    prediction explanations are just one piece of the puzzle and should be used in
    conjunction with other evaluation methods to gain a comprehensive understanding
    of a model’s performance. The solution here is to use a combination of evaluation
    methods, including performance metrics, cross-validation, and expert domain knowledge,
    to analyze and validate a model’s performance.'
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Misinterpreting explanation results**: Interpreting prediction explanation
    results can be challenging, particularly when dealing with complex models and
    large datasets. Misinterpretation of these results can lead to incorrect conclusions
    about a model’s behavior and performance. The solution here is to collaborate
    with domain experts to help interpret explanation results and ensure that conclusions
    drawn from these explanations align with real-world knowledge and expectations.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ignoring model limitations**: Prediction explanations can provide valuable
    insights into a model’s decision-making process, but they cannot address the inherent
    limitations of the model itself. It’s essential to acknowledge and address these
    limitations to ensure that the model performs optimally. The solution here is
    to conduct thorough model evaluations to identify and address any limitations,
    such as overfitting, underfitting, or biased training data. Continuously reevaluate
    and update the model as needed to maintain optimal performance.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explanations for the wrong audience**: Different stakeholders may require
    different types of explanations based on their expertise and needs. Providing
    explanations that are too technical or too simplistic for the intended audience
    can hinder their understanding and use of the predictions. The solution is to
    tailor explanations to the needs and expertise of the target audience. Sometimes,
    a more global explanation of the model is needed instead of the per-prediction
    explanation, in which case you can consider using neural network interpretation
    techniques and aggregated evaluation metrics. For non-technical users, explaining
    predictions by providing raw feature importance isn’t enough and requires a natural
    language explanation of the insights obtained through feature importance, as introduced
    in the *Explaining prediction explanations automatically* section. For a technical-aware
    user, prediction explanations can be suitable.'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By being aware of these common pitfalls and implementing strategies to avoid
    them, practitioners can ensure that prediction explanations remain a valuable
    tool for understanding and improving AI models. By combining prediction explanations
    with other evaluation methods and collaborating with domain experts, it’s possible
    to gain a comprehensive understanding of a model’s performance, behavior, and
    limitations, ultimately leading to more accurate and reliable AI systems that
    better serve their intended purpose.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we gained a broad view of the prediction explanations landscape
    and dived into the integrated gradients technique, applied it practically to a
    use case, and even attempted to explain the integrated gradients results manually
    and automatically through LLMs. We also discussed common pitfalls in prediction
    explanations and provided strategies to avoid them, ensuring the effectiveness
    of these explanations in understanding and improving AI models.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
- en: Integrated gradients is a useful technique and tool to provide a form of saliency-based
    explanation of the predictions that your neural network makes. The process of
    understanding a model through prediction explanations provides many benefits that
    can help fulfill the criteria required to have a successful machine learning project
    and initiative. Even when everything is going well and the machine learning use
    case is not critical, uncovering the model’s behavior that you will potentially
    deploy through any prediction explanations technique can help you improve how
    your model is utilized.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: However, saliency-based explanations only allow you to understand which input
    data or input neurons are important. But what patterns a neuron is capable of
    detecting from the input data remains unknown. In the next chapter, we will expand
    on this direction and uncover techniques to understand exactly what a neuron detects.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: By being aware of common pitfalls and implementing strategies to avoid them,
    practitioners can ensure that prediction explanations remain a valuable tool for
    understanding and improving AI models. Combining prediction explanations with
    other evaluation methods and collaborating with domain experts can lead to more
    accurate and reliable AI systems that better serve their intended purpose.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
- en: Before we wrap up, as a final word, be sure to allocate some time to understand
    your neural network model through prediction explanation techniques such as integrated
    gradients and consider the potential pitfalls to maximize their effectiveness!
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mukund Sundararajan, Ankur Taly, Qiqi Yan. *Axiomatic Attribution for Deep
    Networks*, In: International Conference on Machine Learning. 2017\. URL: [https://arxiv.org/abs/1703.01365v2](https://arxiv.org/abs/1703.01365v2).'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
