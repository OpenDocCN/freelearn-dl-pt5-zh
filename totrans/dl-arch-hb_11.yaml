- en: '11'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Explaining Neural Network Predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Have you ever wondered why a facial recognition system flagged a photo of a
    person with a darker skin tone as a false positive while identifying people with
    lighter skin tones correctly? Or why a self-driving car decided to swerve and
    cause an accident, instead of braking and avoiding the collision? These questions
    illustrate the importance of understanding why a model predicts a certain value
    for critical use cases. By providing explanations for a model’s predictions, we
    can gain insights into how the model works and why it made a specific decision,
    which is crucial for transparency, accountability, trust, regulatory compliance,
    and improved performance.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will explore neural network-specific methods for explaining
    model predictions. Additionally, we will discuss how to quantify the quality of
    an explanation method. We will also discuss the challenges and limitations of
    model explanations and how to evaluate their effectiveness.
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, the following topics will be covered:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the value of prediction explanations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Demystifying prediction explanation techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring gradient-based prediction explanations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Trusting and understanding integrated gradients
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using integrated gradients to aid in understanding the predictions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Explaining prediction explanations automatically
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring common pitfalls in prediction explanations and how to avoid them
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter includes some practical implementations in the Python programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`captum`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers-interpret`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`transformers`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code files are available on GitHub: [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_11](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_11).'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the value of prediction explanations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First off, the concept of explaining a model through its predictions is referred
    to by many other names, including explainable AI, trustable AI, transparent AI,
    interpretable machine learning, responsible AI, and ethical AI. Here, we will
    refer to the paradigm as **prediction explanations**, which is a clear and short
    way to refer to it.
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction explanations** is not a technique that is adopted by most machine
    learning practitioners. The value of prediction explanations highly depends on
    the exact use case. Even though it is stated that explanations can increase transparency,
    accountability, trust, regulatory compliance, and improved model performance,
    not everybody cares about these points. Instead of understanding the benefits,
    let’s look at it from a different perspective and explore some of the common factors
    that drove practitioners to adopt prediction explanations that can be attributed
    to the following conditions:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The prediction explanations technique provides the following benefits regarding
    the utilization of your built model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Transparency**: Prediction explanations allow model prediction consumers
    to have access to reasons, which will, in turn, enforce the confidence of the
    consumer in the predictions made. A transparent model allows consumers to objectively
    gauge their intelligence, which increases trust and increases adoption.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Accountability**: Prediction explanations allow consumers to perform root
    cause analysis, which is especially important in critical use cases or in use
    cases where there is a human in the loop to make the final decision with a model’s
    prediction as a reference.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Trust**: Without trust, nobody will use the prediction of the model. Prediction
    explanations provide a small boost toward achieving higher trust.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regulatory compliance**: Some governments enforce laws that require decisions
    made by computer systems to be explainable in certain industries such as banks
    and insurance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Metric performance**: The capability to perform root cause analysis can lead
    to a better understanding of either the model’s behavior or the training dataset.
    This will, in turn, allow machine learning practitioners to improve or fix the
    issues found using prediction explanations, and eventually lead to improved metric
    performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This list of benefits makes it worth it to utilize prediction explanations
    on any use case and model. However, the value of using prediction explanations
    increases exponentially with certain conditions. When the utilization is based
    on a specific goal, the method becomes twice as useful. Let’s take a step further
    and explore some of the common conditions that drove practitioners to utilize
    the prediction explanations technique:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Critical and high-impact use case**: In these use cases, the model’s decisions
    can typically result in significant consequences for human welfare, safety, or
    well-being. Understanding the model’s behavior in every way can help mitigate
    the worst case from happening. This can range from billions of money lost to actual
    human life lost.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Failing to achieve the required threshold for success**: A machine learning
    project can’t move forward to the model deployment stage in the machine learning
    life cycle if it can’t even achieve success thresholds during the model development
    stage. Understanding the behavior of the model toward different inputs can help
    signal whether the data is of bad quality, help indicate whether the data has
    biased patterns that promote overfitting, and generally help debug how to improve
    the model’s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The model makes wrong predictions with the simplest examples**: Making wrong
    predictions with the most complex examples is expected, especially when humans
    also could have made mistakes in making the same decisions. When it comes to the
    simplest examples, making an error would indicate that the model is not learning
    the right things. Understanding what a model is focusing on could be key to figuring
    out why a model failed.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regulatory laws mandate accountability for using machine learning models
    in decision-making**: This means prediction explanations will be required not
    typically as a method for the machine learning practitioner to understand how
    the model behaves or with different inputs, but instead, to be used after a model
    is deployed, which will allow the people that consume the predictions understand
    why a certain decision has been made for accountability.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The model fails to make proper predictions after it is deployed**: Have you
    ever thought about whether a good model with high accuracy metric performance
    in your cross-validation setup means that the model pays attention to the right
    things? Even with cross-validation partitioning strategies, data can still be
    very biased toward a certain set of conditions. This means that when the model
    gets deployed into the real world, the data it encounters during the production
    operation can be from a distribution and set of conditions that are different
    than what was available in the original dataset used for model development. When
    this happens, prediction explanations can help uncover biases and unwanted behaviors
    of the model either in the new data or data used for model development. In other
    words, you’ll need to quantify wrong predictions from right predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is fine to not utilize prediction explanation techniques in your use case
    and development workflow. However, when these edge cases happen, know that prediction
    explanation techniques are your key tool to help overcome your obstacles. But
    what exactly do prediction explanations explain? In the next section, we will
    discuss a short overview of prediction explanation methods and dive into the method
    category we will be introducing in this chapter, which is specifically for neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Demystifying prediction explanation techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Prediction explanations is a technique that attempts to explain the logic behind
    a model’s decision, given input data. Some machine learning models are built to
    be more transparent and explainable out of the box. One example is a decision
    tree model, which is built from the ground up using explicit conditioning rules
    to split data into multiple partitions that result in specific decisions, allowing
    the predictions to be explained through the explicit rules that were used to predict
    the data sample. However, models such as neural networks are treated like a black
    box without any straightforward way to retrieve the reasons the decision was made
    directly.
  prefs: []
  type: TYPE_NORMAL
- en: 'The logic of a model’s decision on a data sample can be explained and presented
    in a variety of ways, so long it contains information on how the final decision
    was made. Additionally, predictions made by a machine learning model can be explained
    in either a model-agnostic or model-specific way. There are a few types of explanations
    that can be made using the model’s predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Saliency-based explanation**: This is also known as importance attribution,
    feature importance, or feature impact'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transparent model logic-based explanation**: Provide a rationale on why a
    decision is made'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Exemplar-based explanation**: Using similar data to reason why a label is
    predicted'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Neural networks, being a black box out of the box, can only be explained through
    saliency-based explanation or exemplar-based explanation. However, workarounds
    have been invented to achieve indirect transparent model logic-based explanations
    for neural networks using knowledge distillation methods from a neural network
    to more transparent and interpretable models such as linear models. Additionally,
    attention mechanisms implemented in transformers provide a shallow way to check
    for feature importance through its attention maps as a saliency method but fall
    short in the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Attention weights are not feature-specific as they are obtained by calculating
    the interactions between all input tokens and the output token of interest. This
    means the weight does not represent a true reliance on an isolated feature.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The inputs attended to by the attention mechanism can still not be used in later
    parts of the network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Attention maps can be biased and cause some features to be neglected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, neural-networks-based explanation techniques should be opted over
    model-agnostic explanation techniques as they are capable of providing more detailed
    and nuanced explanations while being more efficient in terms of both computational
    requirements and time. In this chapter, we will focus on saliency-based explanations
    for neural networks using more reliable methods.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, we will dive into the core workhorse behind neural network
    model-specific explanations, which are gradient-based saliency explanations.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring gradient-based prediction explanations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Most up-to-date neural network-based explanation techniques today are variations
    of using the gradients that can be obtained through backpropagation. Gradient-based
    explanations for neural network models work because they rely on the fundamental
    principle of how the weights in a neural network are updated during the training
    process using backpropagation. During backpropagation, the partial derivatives
    of the loss function concerning the weights in the network are calculated, which
    gives us the gradient of the loss function concerning the weights.
  prefs: []
  type: TYPE_NORMAL
- en: This gradient provides us with a measure of how much the input data contributes
    to the overall loss. Remember that gradients measure the sensitivity of the input
    value concerning the loss function. This means it provides the degree of fluctuation
    of the predictions when you modify the specific input value, which represents
    the importance of the input data. Input data can be chosen to be the weights of
    the neural network or the actual input to the entire neural network. In most cases,
    the actual input-based explanations are enough to provide clarity on the important
    feature groups. However, sometimes, a more fine-grained explanation is needed
    to decode the underlying characteristics of the highly attributed and important
    actual input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, consider the use case of identifying animal breeds with image
    data using a CNN. For an image with a dog standing on grass, if an attribution
    method signals that both the dog and grass are important, why is the grass important?
    This can be for one of the following reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: The fur of the dog and the grass are both identified by a single convolutional
    filter.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fur and grass are explicitly separately identified by different filters but
    shown as important similarly because the model might have overfitted to think
    that dogs need to be accompanied by grass. This might mean that the training dataset
    only contains images of dogs with grass and signals the need to add more dog images
    without grass.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This will require diving deeper into important filters that are attributed highly
    among the filters that are activated and have something to do with the grass and
    the dog. Even after pinpointing the filters that are both important, highly activated
    for the image, and have something to do with the grass and the dog, you can’t
    know for sure what patterns the filter is identifying. Is it a high-level feature
    such as the dog’s shape? Or is it fur? In cases like this, it is highly beneficial
    to visualize the patterns directly. This topic will be discussed in more detail
    in [*Chapter 12*](B18187_12.xhtml#_idTextAnchor184), *Interpreting* *Neural Networks*.
  prefs: []
  type: TYPE_NORMAL
- en: Gradients by themselves, however, are not reliable enough to be used as-is as
    an explanation method. More generally, the quality of pure gradients to be used
    as a feature importance explanation method is low. However, the quality of explanation
    methods is not a quantitative metric. The perceived quality of the explanations
    generated is subjective to the requirements of the consumer.
  prefs: []
  type: TYPE_NORMAL
- en: Consider a scenario where a financial institution wants to develop a credit
    scoring model to determine loan approvals. The institution is interested in using
    an explanation technique to understand which features are most important in the
    model’s predictions. A bank might prioritize an explanation technique that focuses
    on the features that have the greatest impact on creditworthiness, such as credit
    history, income, and outstanding debts. This would help the bank make informed
    lending decisions and manage its risks effectively.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, an individual loan applicant may prefer a more comprehensive
    explanation that provides insight into how the model evaluated their specific
    financial situation, including factors beyond the top three most important features,
    such as their employment history and recent financial hardships. This would help
    the applicant make informed decisions about how to improve their creditworthiness
    in the future. Selecting an appropriate explanation technique is important as
    it should cater to the audience and their specific needs to make the explanation
    clear and useful for its intended purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Even so, there are axiomatic-based evaluations, which involve evaluating based
    on a set of principles or axioms that are considered desirable. Of all techniques,
    a technique called **integrated gradients** stands out for its focus on neural
    network models while being developed to satisfy widely accepted axioms.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we’ll understand how integrated gradients explain a neural network prediction
    and understand how the method satisfies widely accepted axioms.
  prefs: []
  type: TYPE_NORMAL
- en: Trusting and understanding integrated gradients
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: First off, the integrated gradient technique is available off-the-shelf in a
    few open source libraries such as **shap** and **captum**. We can utilize the
    method from the library without reimplementing the method. However, obtaining
    explanations without understanding the technicalities behind the technique can
    reduce trust in the explanations. If you don’t trust the technique, the explanation
    results themselves hardly mean anything. Subsequently, if you don’t explain your
    predictions, the prediction results themselves hardly mean anything! In this section,
    we will dive into what integrated gradients do so that you can trust what integrated
    gradients can explain to you.
  prefs: []
  type: TYPE_NORMAL
- en: 'Integrated gradients add a few extra components to the basic gradient-based
    feature importance-based explanations. These components are geared toward satisfying
    a few critical axioms that make or break the reliability of an explanation method.
    These axioms are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sensitivity**: A model that produces two different predictions on two data
    samples that differ in that only a single feature should be given a non-zero importance
    score. Activation functions such as ReLU break this axiom with a zero input value
    as the gradients would also be reduced to zero, even when the predictions are
    different between the two predictions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Implementation invariance**: Two models that have the same performance across
    all possible data must produce the same importance scores. Assume that any external
    data and data received during the entire lifetime of the model is included.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Completeness**: One of the best ways to make a good-quality explanation for
    both human and machine learning explanations is to provide counterfactual reports.
    To evaluate model accuracy performance, having a baseline model works the same
    way. For explanation methods, this involves engineering a baseline data sample
    that can produce a neutral prediction. Completeness means that the importance
    score for every feature column must add up to the difference in prediction score
    between the baseline sample and the targeted sample. This axiom can be more useful
    in some use cases, such as in regression, where the prediction is directly used
    as an output rather than in a multiclass setting where the predictions are only
    used for choosing the top class.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Linearity**: If you linearly combine two neural network models, which is
    a simple weighted addition, the explanation of the combined neural network on
    a data sample must be the same weighted addition of the individual explanation
    of the two neural network models. Linear behaviors that are implemented in the
    model should be respected. A straightforward axiom is specifically designed as
    some methods generate unaccountable importance values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we understand the core axioms that can be used to compare methods,
    know that integrated gradients satisfy all of these axioms. Simply put, integrated
    gradients integrate the gradients from samples that are sampled from a straight-line
    path (linear interpolation) from a chosen baseline data sample and the target
    data sample and multiply it by the difference between the predictions of the target
    sample and baseline sample. The path integral of the gradients is the area under
    the curve of the gradient values along the straight path. This value represents
    the change in the model output as the input feature’s value changes along the
    path. The rate of change directly translates to feature importance. By multiplying
    this value by the difference between the actual input value and the baseline input
    value, we get the contribution of the feature to the model output for integrated
    gradients. *Figure 11**.1* showcases this process with the results obtained by
    computing integrated gradients on the predictions of a pre-trained ResNet50 model
    on an image of an orange. The model is capable of predicting the orange class
    and predicts the image to be an orange with a 46% probability:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 11.1 – An illustration of how integrated gradients are computed for
    an image of an orange with a pre-trained ResNet50 model that is capable of predicting
    the orange class](img/B18187_11_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 11.1 – An illustration of how integrated gradients are computed for an
    image of an orange with a pre-trained ResNet50 model that is capable of predicting
    the orange class
  prefs: []
  type: TYPE_NORMAL
- en: 'More intuitively, this means that integrated gradients measure the rate of
    change of model predictions as the target input feature’s value changes toward
    a neutral baseline sample and provides accountable importance that is properly
    scaled according to the predictions:'
  prefs: []
  type: TYPE_NORMAL
- en: Feature importances = prediction difference x Area under gradients along path
    curve
  prefs: []
  type: TYPE_NORMAL
- en: This allows it to capture the sensitivity of the model’s output to the feature
    across its entire range of values. Integrated gradients ensure that the contribution
    of each feature is proportional to its effect on the model’s output by aggregating
    the gradients from samples in the linear interpolated path between the baseline
    and target. Using plain gradients as the core allows integrated gradients to satisfy
    implementation invariance as the chain rule used to obtain partial derivatives
    allows gradients to be computed partially.
  prefs: []
  type: TYPE_NORMAL
- en: For integrated gradients, if a feature has twice the effect on the model’s output
    compared to another feature, its attribution score will be twice as high because
    the gradients along the path will be twice as high for that feature, thus satisfying
    linearity. Additionally, since the importance is scaled by the difference in predictions
    between the baseline input and target input, integrated gradients will satisfy
    the completeness axiom. These are local explanations that provide reasons for
    predictions through individual data samples. As a bonus, by aggregating the local
    feature importance of all data samples through mean or median, you can obtain
    the global feature importance of the model.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will attempt to use integrated gradients to explain predictions from
    a model.
  prefs: []
  type: TYPE_NORMAL
- en: Using integrated gradients to aid in understanding predictions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'At the time of writing, two packages provide easy-to-use classes and methods
    to compute integrated gradients, which are the `captum` and `shap` libraries.
    In this tutorial, we will be using the `captum` library. The `captum` library
    supports models from TensorFlow and PyTorch. We will be using PyTorch here. In
    this tutorial, we will be working on explaining a SoTA transformer model called
    **DeBERTA** on the task of text sentiment multiclass classification. Let’s go
    through the use case step by step:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import the necessary libraries and methods:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will import a custom text sentiment dataset made for this tutorial:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This data contains 100 rows with a balanced distribution between three sentiment
    classes called “neutral”, “positive,” and “negative.” We will dive into a few
    samples of this dataset later.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, we will define the label mapping that will be used to map the sentiment
    labels to a numeric ID and vice versa:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create a method that applies text pre-processing that tokenizes
    the text data using a pre-trained, byte-pair-encoding-based tokenizer:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To train a model, we need to have a cross-validation strategy, so we will use
    a simple train and validation split here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since we will be using the PyTorch model from the Hugging Face Transformers
    library, we need to use the PyTorch dataset format and split the data that will
    be used to train the model. Here, we will define the PyTorch dataset and initialize
    both the training and validation dataset instances:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that the dataset has been pre-processed and is ready to be used for training,
    let’s load our randomly initialized DeBERTA model from Hugging Face:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since this is a multiclass setup, we will use the accuracy metric that will
    be computed at each epoch, along with the cross-entropy loss, which will be used
    to train the model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we will define the training arguments:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will save the checkpoints in the `results` folder, train the model with
    100 iterations of warmup, load the best model at the end of training, and set
    the model learning rate to a very small number of 0.000025, which is required
    to properly converge to a solution.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will initialize the Hugging Face `trainer` instance, which will take
    in the training arguments and use them to execute the actual training process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that an early stopping of 20 iterations is used. This stops the model
    from being trained when the model doesn’t improve on the validation partition
    for the specified iterations, along with 1,000 epochs of training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Finally, let’s train and print the final best model evaluation score!
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will result in the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '95.23% accuracy is a pretty good score for a multiclass model. You might think
    that the model is good enough to be deployed, but is it more than what meets the
    eye? Let’s investigate the model’s behavior through prediction explanations using
    integrated gradients and see whether the model is performing. First, let’s define
    the explainer instance from the `transformers_interpret` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The `transformers_interpret` library uses the `captum` library under the hood
    and implements methods and classes to make it straightforward to explain and visualize
    text importance based on models made using the Hugging Face Transformers library.
    Among all these things, we’re mapping the importance score to a color code and
    mapping token IDs back to the actual token string.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we’ll explain two samples for the negative and positive labels from the
    validation partition, which is conveniently located in the first three indices:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This will result in the output displayed in *Figure 11**.2*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 11.2 – \uFEFFtransformers_interpret-based integrated gradients results\
    \ on the validation dataset](img/B18187_11_02.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 11.2 – transformers_interpret-based integrated gradients results on the
    validation dataset
  prefs: []
  type: TYPE_NORMAL
- en: The words highlighted in green represent a positive attribution, while those
    highlighted in red represent a negative attribution toward the predicted label
    class. The darker the green, the stronger the positive attribution toward the
    predicted label class. The darker the red, the stronger the negative attribution
    toward the predicted label class in general.
  prefs: []
  type: TYPE_NORMAL
- en: The first and second examples show correctly predicted positive sentiment sentences.
    For the first example, based on common human sense, *amazing* is the word that
    should contribute the most to positive attribution. For the second example, *happy*
    should be the word that is emphasized as the word contribution toward positive
    sentiment. However, for both examples, the word *bored* is used instead as a strong
    indicator of the positive sentiment prediction, which is not the behavior that
    we want. This indicates that the dataset has a bias where the word *bored* could
    be present in all samples labeled with positive sentiment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The third and fourth examples show a correctly predicted negative sentiment.
    In the third example, the sentences *never going to end* and *keeps dragging on*
    should be the focus of the negative sentiment prediction. In the fourth example,
    the word *wrong* and the phrases *has gone wrong* and *so frustrated* should be
    the focus of the negative sentiment prediction. However, both samples consistently
    show dependence on the word *day* to get their negative sentiment prediction.
    This indicates that the dataset has a bias regarding the word *day* occurring
    frequently in the samples that are labeled with negative sentiment.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: All of this means that either the data has to be prepared all over again or
    more data that diversifies the distribution of word usage should be added to the
    dataset so that a proper model can be built.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: This tutorial demonstrates a single type of benefit that can be derived from
    utilizing prediction explanations. In particular, this shows the case where the
    model is not capturing the behavior required to be reliably deployed, even when
    the accuracy-based metric has a good score.
  prefs: []
  type: TYPE_NORMAL
- en: The integrated gradients technique is a flexible technique that can be applied
    to any neural network model for any type of input variable type. To benefit from
    the explanation results, you will need to derive meaningful insights in the context
    of the business goal. To derive meaningful insights and conclusions from the results
    of integrated gradients, it’s essential to apply common sense and logical reasoning,
    as was presented in the tutorial manually. However, there is a method that you
    can use to try to obtain assistance in obtaining meaningful insights, especially
    when there is too much data and too many variations to decode manually. We will
    dive into this in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Explaining prediction explanations automatically
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One useful method that helps in deriving insights from prediction explanations
    is none other than using **large language models** (**LLMs**) such as ChatGPT.
    ChatGPT is a transformer model that is trained to provide results that match logical
    reasoning related to the instructions provided. The theory here is that if you
    can format your prediction explanations data in a way that it can be fed into
    a transformer model, and instruct the LLM to derive insights from it, you will
    be able to obtain insights from multiple different perspectives.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the previous tutorial, we attempted to explain the explanations of four
    different samples consisting of two correctly predicted positive sentiment examples
    and two correctly predicted negative sentiment examples. Now, let’s use an LLM
    model to gain insights. Here, we will separately generate insights for the two
    correctly identified positive sentiment and two correctly identified negative
    sentiment examples as it somehow tends to provide more reasonable results compared
    to just plugging in all four examples. We used the following prompt:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the list of word tokens and attribution tuples are the contents of
    the `word_attributions` variable in the previous tutorial. For this prompt, the
    ChatGPT model returned the following response without modification:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Based on the given examples, it seems that the model is performing well in
    predicting positive sentiment, as the predicted label is correct in both cases.
    However, it is important to note that the token attributions should be interpreted
    with caution, as they are not always a reliable indicator of the importance of
    each word in** **the sentence.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Looking at the token attributions for the first example, we can see that
    the highest attributed word is “bored” with a score of 0.494\. This might seem
    odd for a positive sentiment sentence, but it is possible that the model is attributing
    high importance to this word because it is negating a negative sentiment that
    was initially present. The model might have also learned that overcoming boredom
    is something that people consider to be positive. The second highest attributed
    words are “I” and “was”, both with scores above 0.3, which is reasonable since
    they indicate the writer’s personal experience and opinion. The other highly attributed
    words such as “to”, “from”, and “new” also seem to make sense in the context of**
    **the sentence.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Looking at the token attributions for the second example, we can see that
    the highest attributed word is “bored” again, with a score of 0.709\. This could
    be because the model is placing a lot of emphasis on the fact that the job keeps
    the writer from getting bored, which is seen as a positive thing. The second highest
    attributed word is “from” with a score of 0.415, which makes sense in the context
    of the sentence since it is indicating what the job is keeping the writer from
    doing. Other highly attributed words such as “to” and “that” also seem to make
    sense in the context of** **the sentence.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall, while the model is performing well in predicting positive sentiment,
    it is important to note that token attributions can be noisy and should be interpreted
    with caution. The highly attributed words seem to make sense in the context of
    the sentences, but it is important to look at the entire sentence and not just
    individual words to fully understand** **the sentiment.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'This response provides a different take on our original insight and mentions
    the possibility that nothing is wrong with some skepticism. Overall, this is a
    nice response that gives us a different perspective on what could happen and signals
    that an LLM can be used to explain the results of prediction explanation as an
    assistance tool. Now, let’s replace the attribution results with the two examples
    that are correctly identified as negative sentiments with the following list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The results of the ChatGPT model provided the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '**From the provided examples, it seems like the model is correctly predicting
    the negative sentiment in both cases. However, there are a few things that could
    be analyzed to determine whether the model is performing well** **or not.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Firstly, the token attributions provided by the integrated gradients method
    indicate the contribution of each token towards the final prediction of the model.
    In both examples, the tokens with the highest positive attribution scores are
    “day” and “has gone wrong”, which seems to make sense as they are indicative of**
    **negative sentiment.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**However, one issue with the provided token attributions is that some of the
    tokens have negative attribution scores, which means that they are contributing
    negatively towards the final prediction. For example, in the first example, the
    token “feels” has a negative attribution score, but it could be argued that it
    is actually indicative of negative sentiment. Similarly, in the second example,
    the token “’m” has a negative attribution score, but it is a contraction of “I
    am”, which is indicative of** **negative sentiment.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Another thing to consider is the relative score of the highest attributed
    tokens. In the first example, the token “day” has a significantly higher attribution
    score than any other token, which indicates that it is the most important feature
    for predicting negative sentiment in this case. However, in the second example,
    there are multiple tokens with similarly high attribution scores, such as “has
    gone wrong” and “frustrated”, which suggests that they are both equally important
    features for predicting** **negative sentiment.**'
  prefs: []
  type: TYPE_NORMAL
- en: '**Overall, based on the provided examples and token attributions, it seems
    like the model is performing reasonably well in predicting negative sentiment.
    However, the analysis of the token attributions suggests that there may be some
    issues with the way the integrated gradients method is attributing scores to certain
    tokens. It is also important to consider the relative score and interpretability
    of the highest attributed tokens to determine whether they make sense in the context
    of the** **predicted label.**'
  prefs: []
  type: TYPE_NORMAL
- en: 'Both of ChatGPT’s responses seem to signal that it could be a valid behavior.
    Since most neural networks are configured to be non-linear models with many non-linear
    activations, remember that the model is capable of learning complex interactions
    between tokens. This means that tokens are mostly perceived by the model in a
    complex way that might make them depend on one another. A token that has a high
    attribution in the original context when applied to another context might not
    produce the same attribution as multiple tokens can depend on each other as a
    prerequisite to produce a strong attribution. Additionally, tokens are not restricted
    to interacting with tokens that are directly beside them and can be in any position
    of a text row. One way to verify whether the token is overfitted is by predicting
    on text data that only has the `day` token to see whether it will predict it as
    a negative sentiment and the `bored` token to see whether it will predict it as
    a positive sentiment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'This will return the following response:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: It is interesting to see that a single token for both `day` and `bored` will
    result in the prediction being a neutral sentiment. The more repetition tokens
    you add, the more the prediction will skew toward negative and positive sentiment,
    respectively. This proves that the model is indeed biased toward these specific
    words and is not using the words in the right way, as ChatGPT said. Note that
    it could very much be what ChatGPT predicted for your case.
  prefs: []
  type: TYPE_NORMAL
- en: This tutorial showcases the explanations of text-based neural networks. However,
    it can be applied similarly to other data types, such as numerical data, categorical
    data, or any data that can be reliably represented in text format. Specifically
    for image-based prediction explanations, you can use models such as GPT-4 from
    OpenAI, which accepts image data along with text data. You can also consider using
    the visualization from the `transformers_interpret` library as an image and feed
    it into GPT-4!
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will dive into common pitfalls when trying to explain your predictions
    and recommendations to avoid these pitfalls.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring common pitfalls in prediction explanations and how to avoid them
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Although prediction explanations have proven to be valuable tools in understanding
    AI models, several common pitfalls can hinder their effectiveness. In this section,
    we will discuss these pitfalls and provide strategies to avoid them, ensuring
    that prediction explanations remain a valuable resource for understanding and
    improving AI models. Some of the common pitfalls, along with their solutions,
    are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Over-reliance on explanations**: While prediction explanations can provide
    valuable insights into a model’s decision-making process, over-relying on these
    explanations can lead to incorrect conclusions. It’s important to remember that
    prediction explanations are just one piece of the puzzle and should be used in
    conjunction with other evaluation methods to gain a comprehensive understanding
    of a model’s performance. The solution here is to use a combination of evaluation
    methods, including performance metrics, cross-validation, and expert domain knowledge,
    to analyze and validate a model’s performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Misinterpreting explanation results**: Interpreting prediction explanation
    results can be challenging, particularly when dealing with complex models and
    large datasets. Misinterpretation of these results can lead to incorrect conclusions
    about a model’s behavior and performance. The solution here is to collaborate
    with domain experts to help interpret explanation results and ensure that conclusions
    drawn from these explanations align with real-world knowledge and expectations.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Ignoring model limitations**: Prediction explanations can provide valuable
    insights into a model’s decision-making process, but they cannot address the inherent
    limitations of the model itself. It’s essential to acknowledge and address these
    limitations to ensure that the model performs optimally. The solution here is
    to conduct thorough model evaluations to identify and address any limitations,
    such as overfitting, underfitting, or biased training data. Continuously reevaluate
    and update the model as needed to maintain optimal performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Explanations for the wrong audience**: Different stakeholders may require
    different types of explanations based on their expertise and needs. Providing
    explanations that are too technical or too simplistic for the intended audience
    can hinder their understanding and use of the predictions. The solution is to
    tailor explanations to the needs and expertise of the target audience. Sometimes,
    a more global explanation of the model is needed instead of the per-prediction
    explanation, in which case you can consider using neural network interpretation
    techniques and aggregated evaluation metrics. For non-technical users, explaining
    predictions by providing raw feature importance isn’t enough and requires a natural
    language explanation of the insights obtained through feature importance, as introduced
    in the *Explaining prediction explanations automatically* section. For a technical-aware
    user, prediction explanations can be suitable.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By being aware of these common pitfalls and implementing strategies to avoid
    them, practitioners can ensure that prediction explanations remain a valuable
    tool for understanding and improving AI models. By combining prediction explanations
    with other evaluation methods and collaborating with domain experts, it’s possible
    to gain a comprehensive understanding of a model’s performance, behavior, and
    limitations, ultimately leading to more accurate and reliable AI systems that
    better serve their intended purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we gained a broad view of the prediction explanations landscape
    and dived into the integrated gradients technique, applied it practically to a
    use case, and even attempted to explain the integrated gradients results manually
    and automatically through LLMs. We also discussed common pitfalls in prediction
    explanations and provided strategies to avoid them, ensuring the effectiveness
    of these explanations in understanding and improving AI models.
  prefs: []
  type: TYPE_NORMAL
- en: Integrated gradients is a useful technique and tool to provide a form of saliency-based
    explanation of the predictions that your neural network makes. The process of
    understanding a model through prediction explanations provides many benefits that
    can help fulfill the criteria required to have a successful machine learning project
    and initiative. Even when everything is going well and the machine learning use
    case is not critical, uncovering the model’s behavior that you will potentially
    deploy through any prediction explanations technique can help you improve how
    your model is utilized.
  prefs: []
  type: TYPE_NORMAL
- en: However, saliency-based explanations only allow you to understand which input
    data or input neurons are important. But what patterns a neuron is capable of
    detecting from the input data remains unknown. In the next chapter, we will expand
    on this direction and uncover techniques to understand exactly what a neuron detects.
  prefs: []
  type: TYPE_NORMAL
- en: By being aware of common pitfalls and implementing strategies to avoid them,
    practitioners can ensure that prediction explanations remain a valuable tool for
    understanding and improving AI models. Combining prediction explanations with
    other evaluation methods and collaborating with domain experts can lead to more
    accurate and reliable AI systems that better serve their intended purpose.
  prefs: []
  type: TYPE_NORMAL
- en: Before we wrap up, as a final word, be sure to allocate some time to understand
    your neural network model through prediction explanation techniques such as integrated
    gradients and consider the potential pitfalls to maximize their effectiveness!
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Mukund Sundararajan, Ankur Taly, Qiqi Yan. *Axiomatic Attribution for Deep
    Networks*, In: International Conference on Machine Learning. 2017\. URL: [https://arxiv.org/abs/1703.01365v2](https://arxiv.org/abs/1703.01365v2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
