["```py\nimport numpy as np\nh = np.convolve([2, 3, 2], [-1, 2, -1])\nprint(h)\n```", "```py\n[-2, 1, 2, 1, -2]\n```", "```py\nimport numpy as np\nh = np.convolve([2, 3, 2], [-1, 2, -1], 'valid')\nprint(h)\n```", "```py\n2\n```", "```py\nimport numpy as np\nh = np.convolve([2, 3, 2], [-1, 2, -1], 'same')\nprint(h)\n```", "```py\n[1 2 1]\n```", "```py\nimport numpy as np\nfrom scipy.signal import convolve2d\nx = np.array([[2,2,2],[2,3,2],[2,2,2]])\nw = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])\nh = convolve2d(x,w)\nprint(h)\n```", "```py\n[[-2 -4 -6 -4 -2]\n [-4  9  5  9 -4]\n [-6  5  8  5 -6]\n [-4  9  5  9 -4]\n [-2 -4 -6 -4 -2]]\n```", "```py\nimport numpy as np\nfrom scipy.signal import convolve2d\nx = np.array([[2,2,2],[2,3,2],[2,2,2]])\nw = np.array([[-1,-1,-1],[-1,8,-1],[-1,-1,-1]])\nh = convolve2d(x,w,mode='valid')\nprint(h)\nh = convolve2d(x,w,mode='same')\nprint(h)\n```", "```py\n[[8]]\n\n[[9 5 9]\n [5 8 5]\n [9 5 9]]\n```", "```py\nimport numpy as np\nfrom scipy.signal import convolve\nx = np.array([[[1,1],[1,1]],[[2,2],[2,2]]])\nw = np.array([[[1,-1],[1,-1]],[[1,-1],[1,-1]]])\nh = convolve(x,w)\nprint(h)\n```", "```py\n[[[ 1 0 -1]\n  [ 2 0 -2]\n  [ 1 0 -1]]\n\n [[ 3 0 -3]\n  [ 6 0 -6]\n  [ 3 0 -3]]\n\n [[ 2 0 -2]\n  [ 4 0 -4]\n  [ 2 0 -2]]]\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Conv2D\ninput_shape = (1, 32, 32, 3)\nx = tf.random.normal(input_shape)\nl = Conv2D(64, (9,9), strides=(2,2), activation='relu', \n           input_shape=input_shape)(l)\nprint(l.shape)\n```", "```py\n(1, 12, 12, 64)\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.keras.layers import MaxPooling2D\nx = tf.constant([[-2, -4, -6, -4],\n                 [-4, 9, 5, 9],\n                 [-6, 5, 8, 5],\n                 [-4, 9, 5, 9]])\nx = tf.reshape(x, [1, 4, 4, 1])\ny = MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\nprint(tf.reshape(y(x), [2, 2]))\n```", "```py\ntf.Tensor(\n[[9 9]\n [9 9]], shape=(2, 2), dtype=int32)\n```", "```py\nimport tensorflow as tf\nfrom tensorflow.keras.layers import AveragePooling2D\nx = tf.constant([[-2., -4., -6., -4],\n                 [-4., 9., 5., 9.],\n                 [-6., 5., 8., 5.],\n                 [-4., 9., 5., 9.]])\nx = tf.reshape(x, [1, 4, 4, 1])\ny = AveragePooling2D(pool_size=(2, 2), strides=(2, 2), padding='valid')\nprint(tf.reshape(y(x), [2, 2]))\n```", "```py\ntf.Tensor(\n[[-0.25 1\\. ]\n [ 1\\. 6.75]], shape=(2, 2), dtype=float32)\n```", "```py\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.utils import to_categorical\nimport numpy as np\n\n# The data, split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nx_train = x_train.astype('float32') / 255.\nx_test = x_test.astype('float32') / 255.\n\ny_train = to_categorical(y_train, 10)\ny_test = to_categorical(y_test, 10)\nprint('x_train shape:', x_train.shape)\nprint('x_test shape:', x_test.shape)\n```", "```py\nx_train shape: (50000, 32, 32, 3)\nx_test shape: (10000, 32, 32, 3)\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n(_, _), (_, labels) = cifar10.load_data()\nidx = [3, 6, 25, 46, 58, 85, 93, 99, 108, 133]\n\nclsmap = {0: 'airplane',\n          1: 'automobile', \n          2: 'bird', \n          3: 'cat', \n          4: 'deer',\n          5: 'dog',\n          6: 'frog',\n          7: 'horse',\n          8: 'ship',\n          9: 'truck'}\n\nplt.figure(figsize=(10,4))\nfor i, (img, y) in enumerate(zip(x_test[idx].reshape(10, 32, 32, 3), labels[idx])):\n  plt.subplot(2, 5, i+1)\n  plt.imshow(img, cmap='gray')\n  plt.xticks([])\n  plt.yticks([])\n  plt.title(str(y[0]) + \": \" + clsmap[y[0]])\nplt.show()\n```", "```py\n# Importing the Keras libraries and packages\nfrom tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten\nfrom tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import RMSprop\n\n# dimensionality of input and latent encoded representations\ninpt_dim = (32, 32, 3)\n\ninpt_img = Input(shape=inpt_dim)\n\n# Convolutional layer\ncl1 = Conv2D(64, (9, 9), strides=(2, 2), input_shape = inpt_dim, \n             activation = 'relu')(inpt_img)\n\n# Pooling and BatchNorm\npl2 = MaxPooling2D(pool_size = (2, 2))(cl1)\nbnl3 = BatchNormalization()(pl2)\n```", "```py\n# Add a second convolutional layer\ncl4 = Conv2D(128, (3, 3), strides=(1, 1), activation = 'relu')(bnl3)\npl5 = MaxPooling2D(pool_size = (2, 2))(cl4)\nbnl6 = BatchNormalization()(pl5)\n\n# Flattening for compatibility\nfl7 = Flatten()(bnl6)\n\n# Dense layers + Dropout\ndol8 = Dropout(0.5)(fl7)\ndl9 = Dense(units = 256, activation = 'relu')(dol8)\ndol10 = Dropout(0.2)(dl9)\ndl11 = Dense(units = 64, activation = 'relu')(dol10)\ndol12 = Dropout(0.1)(dl11)\noutput = Dense(units = 10, activation = 'sigmoid')(dol12)\n\nclassifier = Model(inpt_img, output)\n```", "```py\n# Compiling the CNN with RMSprop optimizer\nopt = RMSprop(learning_rate=0.001)\n\nclassifier.compile(optimizer = opt, loss = 'binary_crossentropy', \n                   metrics = ['accuracy'])\n\nprint(classifier.summary())\n```", "```py\nModel: \"model\"\n_________________________________________________________________\nLayer (type)                 Output Shape          Param # \n=================================================================\ninput_1 (InputLayer)         [(None, 32, 32, 3)]   0 \n_________________________________________________________________\nconv2d (Conv2D)              (None, 12, 12, 64)    15616 \n_________________________________________________________________\nmax_pooling2d_4 (MaxPooling2 (None, 6, 6, 64)      0 \n_________________________________________________________________\nbatch_normalization (BatchNo (None, 6, 6, 64)      256 \n_________________________________________________________________\n.\n.\n.\n_________________________________________________________________\ndropout_2 (Dropout)          (None, 64)            0 \n_________________________________________________________________\ndense_2 (Dense)              (None, 10)            650 \n=================================================================\nTotal params: 238,666\nTrainable params: 238,282\nNon-trainable params: 384\n```", "```py\n# Fitting the CNN to the images\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=10, \n                              min_delta=1e-4, mode='min', verbose=1)\n\nstop_alg = EarlyStopping(monitor='val_loss', patience=35, \n                         restore_best_weights=True, verbose=1)\n\nhist = classifier.fit(x_train, y_train, batch_size=100, epochs=1000, \n                   callbacks=[stop_alg, reduce_lr], shuffle=True, \n                   validation_data=(x_test, y_test))\n\nclassifier.save_weights(\"cnn.hdf5\")\n```", "```py\nEpoch 1/1000\n500/500 [==============================] - 3s 5ms/step - loss: 0.2733 - accuracy: 0.3613 - val_loss: 0.2494 - val_accuracy: 0.4078 - lr: 0.0010\nEpoch 2/1000\n500/500 [==============================] - 2s 5ms/step - loss: 0.2263 - accuracy: 0.4814 - val_loss: 0.2703 - val_accuracy: 0.4037 - lr: 0.0010\n.\n.\n.\nEpoch 151/1000\n492/500 [============================>.] - ETA: 0s - loss: 0.0866 - accuracy: 0.8278\nEpoch 00151: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n500/500 [==============================] - 2s 4ms/step - loss: 0.0866 - accuracy: 0.8275 - val_loss: 0.1153 - val_accuracy: 0.7714 - lr: 7.8125e-06\nEpoch 152/1000\n500/500 [==============================] - 2s 4ms/step - loss: 0.0864 - accuracy: 0.8285 - val_loss: 0.1154 - val_accuracy: 0.7707 - lr: 3.9063e-06\nEpoch 153/1000\n500/500 [==============================] - 2s 4ms/step - loss: 0.0861 - accuracy: 0.8305 - val_loss: 0.1153 - val_accuracy: 0.7709 - lr: 3.9063e-06\nEpoch 154/1000\n500/500 [==============================] - 2s 4ms/step - loss: 0.0860 - accuracy: 0.8306 - val_loss: 0.1153 - val_accuracy: 0.7709 - lr: 3.9063e-06\nEpoch 155/1000\n500/500 [==============================] - 2s 4ms/step - loss: 0.0866 - accuracy: 0.8295 - val_loss: 0.1153 - val_accuracy: 0.7715 - lr: 3.9063e-06\nEpoch 156/1000\n496/500 [============================>.] - ETA: 0s - loss: 0.0857 - accuracy: 0.8315Restoring model weights from the end of the best epoch.\n500/500 [==============================] - 2s 4ms/step - loss: 0.0857 - accuracy: 0.8315 - val_loss: 0.1153 - val_accuracy: 0.7713 - lr: 3.9063e-06\nEpoch 00156: early stopping\n```", "```py\nimport matplotlib.pyplot as plt\n\nfig = plt.figure(figsize=(10,6))\nplt.plot(hist.history['loss'], color='#785ef0')\nplt.plot(hist.history['val_loss'], color='#dc267f')\nplt.title('Model Loss Progress')\nplt.ylabel('Brinary Cross-Entropy Loss')\nplt.xlabel('Epoch')\nplt.legend(['Training Set', 'Test Set'], loc='upper right')\nplt.show()\n```", "```py\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import balanced_accuracy_score\nimport matplotlib.pyplot as plt\nimport numpy as np\n\n(_, _), (_, labels) = cifar10.load_data()\n\ny_ = labels\ny_hat = classifier.predict(x_test)\ny_pred = np.argmax(y_hat, axis=1)\n\nprint(classification_report(np.argmax(y_test, axis=1), \n                            np.argmax(y_hat, axis=1), \n                            labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]))\ncm = confusion_matrix(np.argmax(y_test, axis=1), \n                      np.argmax(y_hat, axis=1), \n                      labels=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\nprint(cm)\nber = 1- balanced_accuracy_score(np.argmax(y_test, axis=1), \n                                 np.argmax(y_hat, axis=1))\nprint('BER', ber)\n```", "```py\n  precision  recall  f1-score  support\n\n0      0.80    0.82      0.81     1000\n1      0.89    0.86      0.87     1000\n2      0.73    0.66      0.69     1000\n3      0.57    0.63      0.60     1000\n4      0.74    0.74      0.74     1000\n5      0.67    0.66      0.66     1000\n6      0.84    0.82      0.83     1000\n7      0.82    0.81      0.81     1000\n8      0.86    0.88      0.87     1000\n9      0.81    0.85      0.83     1000\n\n               accuracy  0.77     10000\n\n[[821  12  36  18  12   8   4   4  51  34]\n [ 17 860   3   7   2   6   8   1  22  74]\n [ 61   2 656  67  72  53  43  24  11  11]\n [ 11   7  47 631  55 148  38  36  10  17]\n [ 21   2  48  63 736  28  31  54  12   5]\n [ 12   3  35 179  39 658  16  41   4  13]\n [  2   4  32  67  34  20 820   8   8   5]\n [ 12   3  18  41  42  52   5 809   3  15]\n [ 43  22  12  12   2   5   3   0 875  26]\n [ 29  51  10  19   2   3   5   9  26 846]]\n\nBER 0.2288\n```", "```py\nfrom sklearn.preprocessing import MinMaxScaler\n\ncnnl1 = classifier.layers[1].name   # get the name of the first conv layer\nW = classifier.get_layer(name=cnnl1).get_weights()[0]   #get the filters\nwshape = W.shape  #save the original shape\n\n# this part will scale to [0, 1] for visualization purposes\nscaler = MinMaxScaler()\nscaler.fit(W.reshape(-1,1))\nW = scaler.transform(W.reshape(-1,1))\nW = W.reshape(wshape)\n\n# since there are 64 filters, we will display them 8x8\nfig, axs = plt.subplots(8,8, figsize=(24,24))\nfig.subplots_adjust(hspace = .25, wspace=.001)\naxs = axs.ravel()\nfor i in range(W.shape[-1]):\n  # we reshape to a 3D (RGB) image shape and display\n  h = np.reshape(W[:,:,:,i], (9,9,3))\n  axs[i].imshow(h)\n  axs[i].set_title('Filter ' + str(i))\n```"]