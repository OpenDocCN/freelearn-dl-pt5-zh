- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Supervised Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '*Chapters 2* to *6* explored the core workhorse behind **deep learning** (**DL**)
    technology and included some minimal technical implementations for easy digestion.
    It is important to understand the intricacies of how different **neural networks**
    (**NNs**) work. One reason is that when things go wrong with any NN model, you
    can identify what the root cause is and mitigate it. Those chapters are also important
    to showcase how flexible DL architectures are to solve different types of real-world
    problems. But what are the problems exactly? Also, how should we train a DL model
    effectively in varying situations?'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will attempt to answer the preceding two points specifically
    for supervised deep learning, but we will leave answering the same questions for
    unsupervised deep learning for the next chapter. This chapter will cover the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring supervised use cases and problem types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing neural network layers for foundational problem types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training supervised deep learning models effectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring general techniques to realize and improve supervised deep learning-based
    solutions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Breaking down the multitask paradigm in supervised deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter includes some practical implementations in the Python programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pytorch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`catalyst==22.04`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`scikit-learn`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_8](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_8).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring supervised use cases and problem types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Supervised learning requireslabeled data. Labels, targets, and ground truth
    all refer to the same thing. The provided labels essentially supervise the learning
    process of the **machine learning** (**ML**) model and provide the feedback needed
    for a DL model to generate gradients and update itself. Labels can exist in many
    different forms. They are **continuous numerical format**, **categorical format**,
    **text format**, **multiple categorical formats**, **image format**, **video format**,
    **audio format**, and **multiple target formats**. All of these are then categorized
    as either of the following supervised problem types:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Binary classification**: This is when the target has categorical data with
    only two unique values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiclassification**: This is when the target has categorical data with
    more than two unique values.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Regression**: This is when the target has continuous numerical data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-target/problem**:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multilabel**: This is when the target has more than one binary associated
    with a single data row.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multi-regression**: This is when the target has more than one regression
    target associated with a single data row.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multiple problems**: Either multiple targets associated with a single data
    row in a single dataset or a chain of problems that is sequential in nature and
    multiple models are learned through different datasets.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Supervised representation learning**: This can be in many forms, and the
    main goal is to learn meaningful data representations given input data. The results
    of learned representation can subsequently be utilized for many purposes, including
    **transfer learning** (**TL**), and to realize recommendation systems.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The definition of these problems may be hard to understand by itself. To create
    a better understanding of the different problems, we will check out an extensive
    set of use cases that can take advantage of DL technologies. Given the problems
    specified previously, the following table lists their use cases:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Problem Types** | **Supervised Deep Learning Model** **Use Cases** |'
  prefs: []
  type: TYPE_TB
- en: '| Binary classification |'
  prefs: []
  type: TYPE_TB
- en: Gender prediction of babies with ultrasound imagery
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Semiconductor chip-quality rejection prediction in manufacturing with image
    data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using email data that can contain text, images, documents, audio, or any data
    to predict spam
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Multiclassification |'
  prefs: []
  type: TYPE_TB
- en: Document data topic classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hate/toxic speech or text classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: General image object classification
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentiment prediction of text or speech data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Regression |'
  prefs: []
  type: TYPE_TB
- en: '**Click-through rate** (**CTR**) prediction of advertisements with image or
    text or both'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Human age prediction with a facial input image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting GPS location from an image
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Multi-target |'
  prefs: []
  type: TYPE_TB
- en: '**Text topic classification**: Multilabel, multiple topics can exist in a single
    text data row.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image object detection**: A multiple-target problem consisting of multiple
    regression targets and multiclass classification. First, with an image bounding
    box as multiple regression targets, a single *x* and *y* numerical coordinate,
    and its width and height as the two extra targets forms a rectangular-shaped bounding
    box. Next, the bounding box will be used to extract a cropped image for multiclassification
    purposes to predict the type of object.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Image segmentation**: A kind of multilabel problem, where every pixel will
    serve as binary targets.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: '| Supervised representation learning |'
  prefs: []
  type: TYPE_TB
- en: Face feature representation for face recognition.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Audio representation for speaker recognition using **K-Nearest** **Neighbors**
    (**KNN**).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Representing categories with their own representative feature vectors. This
    can be achieved with a method called **categorical embeddings,** which is an NN
    layer type that holds a feature vector for each category in a categorical feature
    column. It is learnable and serves as a lookup table. The method can reduce the
    feature dimensions of high-cardinality categorical data when compared to basic
    one-hot-encoding but still maintain around the same performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '|'
  prefs: []
  type: TYPE_NORMAL
- en: Table 8.1 – A table of DL problem use cases
  prefs: []
  type: TYPE_NORMAL
- en: Binary classification, multiclass classification, and regression problems are
    rather straightforward to approach. Multi-target types, however, pose complicated
    setups and require more architecting to be done depending on the nature of the
    problem. Multi-target tasks also can be straightforward, such as multilabel or
    multi-regression problems. This task falls into the bigger envelope of multitask
    solutions and will be discussed further in the *Exploring general techniques to
    realize and improve DL-based solutions* section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will implement the basic NN layers of realizing the foundational problems,
    which include binary classification, multiclass classification, and regression
    problems.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing neural network layers for foundational problem types
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In *Chapters 2* to *7*, although many types of NN layers were introduced, the
    core layers for the problem types were either not used or not explained. Here,
    we will go through each of them for clarity and intuition.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the binary classification layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Binary means two options for categorical data. Note that this does not necessarily
    mean a strict rule for the categories to be true or false nor positive or negative
    in the raw data. The two options can be in any format possible in terms of raw
    data, in strings, numbers, or symbols. However, note that NNs can always only
    produce numerical outputs. This means that the target itself has to be represented
    numerically, for which the optimal numbers are the binary values of zero and one.
    This means that the data column to be used as a target for training with only
    two unique values must go through preprocessing to map itself into zero or one.
  prefs: []
  type: TYPE_NORMAL
- en: Generally, there are two ways to define binary outputs in NNs. The first is
    to use a linear layer with a size of one. The second method is to use a linear
    layer with a size of two. There is no significant difference between the two in
    terms of task-quality metrics, but method one takes slightly less space for storage
    and memory, so feel free to always use that version. The outputs from method 1
    will be constrained to values between `0` and `1` by using a sigmoid layer. For
    method 2, the outputs need to be passed into a softmax layer so that the probabilities
    for the two outputs will add up to one. Both methods usually can be optimized
    using **cross-entropy**. Cross-entropy is also known as **log loss**. Log loss
    measures the difference between predicted probabilities and true labels using
    a logarithmic scale. This scale penalizes incorrect predictions more heavily,
    emphasizing the importance of a model’s ability to assign high probabilities to
    the correct class and low probabilities to the incorrect class.
  prefs: []
  type: TYPE_NORMAL
- en: 'Translating the layer from method one into actual `pytorch` code will look
    like the following using the `nn` module from `torch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Next, we will implement the multiclass classification layer.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing the multiclass classification layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`pytorch` code for a 100-class multiclass classification problem with 10 logits
    will look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Another sub-problem of multiclass classification is when the classes are ordinal.
    This sub-problem and task is called ordinal classification. This means that the
    classes have an incremental relationship with each other. A plain multiclass classification
    layer strategy represents ordinal classes sub-optimally as the classes in the
    multiclass are considered to have an equal relationship with each other. A good
    strategy here to add the information of ordinal classes is to utilize a technique
    based on the multilabel classification task, which is a multiple-binary classification
    task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s say that we have five ordinal classes represented as numerical numbers
    from 1 to 5 for simplicity. In reality, this could be represented by any categorical
    data. In an NN, five binary classification heads would be used for this case where
    the classes will be assigned to the respective head in an ascending ordered manner.
    The raw predictions from this NN will be consumed in a way where the final predicted
    ordinal class will be derived from the position of the furthest consecutive positive
    binary prediction. Once there is a negative prediction, the rest of the prediction
    heads on the right will then be ignored. *Figure 8**.1* depicts this strategy
    by simulating the output predictions of the five binary classification heads:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Ordinal classification processing strategy using the output
    predictions of the five binary classification heads](img/B18187_08_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Ordinal classification processing strategy using the output predictions
    of the five binary classification heads
  prefs: []
  type: TYPE_NORMAL
- en: The learning process will be, as usual, using the cross-entropy loss for multiple
    binary targets. Additionally, the performance at every epoch can be monitored
    using robust metrics that don’t depend on probabilities such as recall or precision.
    The ordinal encoding method allows the model to learn that the targets have an
    ordinal relationship.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will dive into the implementation of a regression layer.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing a regression layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`pytorch` code will look like the following using the `nn` module from `torch`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: '`0` and `1`. Coupled with the scaling of target values, the bounds can then
    be enforced in the NN by using the sigmoid layer, which similarly scales activation
    values between `0` and `1`. During the inference stage, the predicted values can
    then be mapped into actual values by descaling values between `0` and `1` into
    the known minimum and maximum boundaries specified earlier.'
  prefs: []
  type: TYPE_NORMAL
- en: The unbounded method allows for some form of generalization by allowing extrapolation,
    and the bounded method allows the addition of informed bias to the NN. Both methods
    have their own benefits and disadvantages, and thus the choice of approach needs
    to be evaluated on a case-by-case basis.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will dive into the implementation of representation layers.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing representation layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Most methods focus on the interactions between architectures for different data
    modalities or the training methods that optimize the represented features. These
    are topics we will dive into further in the next topic after this. One key layer
    type that truly represents the representation layer is the embedding layer. Embeddings
    are a type of layer structure that maps categorical data types into learnable
    vectors. Through this layer, each category will be able to learn a representation
    that is able to perform well against the specified target. The method can be used
    for converting text word tokens into more representative features or plainly as
    a replacement for one-hot encoding. Categorical embeddings make it possible to
    automate the feature engineering process for categorical data types. One-hot encoding
    produces an encoding that enforces the same distance between all the categories
    to every other category. Categorical embedding, however, allows for the possibility
    of obtaining an appropriate distance based on its interactions with the target
    variable and with other data if any extra data exists.
  prefs: []
  type: TYPE_NORMAL
- en: However, categorical embeddings are also not a silver bullet for all ML use
    cases, even if they are decoupled from an actual NN model after training and just
    act as a featurizer. They can sometimes perform better against one-hot-encoding
    in general and, vice versa, can happen other times to perform worse against one-hot-encoding.
    The method still remains a key method to experiment with for any dataset with
    categorical data as input.
  prefs: []
  type: TYPE_NORMAL
- en: Training supervised deep learning models effectively
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015), *Deep Learning Life Cycle*,
    it is emphasized that ML projects have a cyclical life cycle. In other words,
    a lot of iterative processes are carried out in the course of the project’s lifetime.
    To train supervised deep learning models effectively, there are a lot of general
    directions that should be taken based on different conditions, but the one that
    absolutely stands out across every problem is proper tooling. The tooling is more
    commonly known as `pytorch` or `keras` with `tensorflow`, ease of deployment,
    ease of model comparisons using different metrics, ease of model tuning, good
    visualization of model training monitoring, and, finally, good feedback about
    the progress (this can be sent through messages and notifications for alerts).
    If no advanced tools that truly simplify the entire process are at your disposal,
    you can focus on the important bits of making a model work well instead of dealing
    with infrastructure issues such as coordinating the saving of models into different
    folders like **DataRobot**, a paid-for tool, then open sourced tools such as MLflow,
    Kubeflow, or Metaflow will be the next-best alternative. Once the tool of choice
    is picked, carrying out training in DL models will be a breeze. We will be using
    MLflow as an example tool to demonstrate some effective methods for DL model training
    in the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Data preparation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Configuring and tuning DL hyperparameters
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Executing, visualizing, tracking, and comparing experiments
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Additionally, we will explore some extra tips when building a model before ending
    this topic.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s explore each in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data for DL training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Data is the core of any ML model. Data ultimately determines the achievable
    model performance, the quality of the final trained model, and the validity of
    the final trained model. In [*Chapter 1*](B18187_01.xhtml#_idTextAnchor015), *Deep
    Learning Life Cycle*, we explored what it takes for a dataset setup to be DL-worthy,
    along with the qualities needed when acquiring data, coupled with **exploratory
    data analysis** (**EDA**) to verify causality and validity. The general idea there
    was to identify and add extra features and data modalities that have causal effects
    toward the desired target. In this section, we will cover more in-depth essential
    steps that convert the data into a DL trainable state listed here:'
  prefs: []
  type: TYPE_NORMAL
- en: Data partitioning
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data representation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Partitioning the data for DL training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The first step we will cover is data partitioning. Having a good data-partitioning
    strategy for training, validating, and testing your model is essential for a good-performing
    model. The training partition will be the partition that will strictly be used
    for training. The validation partition will be the partition that will strictly
    be used for validating a model during training. In DL, a **validation partition**
    is often used as a guide to signal when to stop training or extract the best-performing
    weights using external data out of the training data. Since the validation data
    will affect the learning process of the model and add some bias that will cause
    overfitting toward the nature of the out-of-training validation data, a testing
    partition will be the final partition that will be used to verify the generalizability
    of the trained model. The testing partition is also known as the holdout partition.
    To be extra safe in preventing overfitting and to ensure the generalizability
    of the model, the validation partition can also be used exclusively to be validated
    only once after the model is trained instead of being used for validation at every
    epoch. This strategy, however, requires that a smaller internal validation partition
    is created from the original training partition. The following figure depicts
    the two different strategies:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Two different cross-validation data-partitioning strategies](img/B18187_08_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Two different cross-validation data-partitioning strategies
  prefs: []
  type: TYPE_NORMAL
- en: 'This process of partitioning the data is called **cross-validation**. The preceding
    figure shows simple cross-validation strategies with only a single partitioning
    setting. This might create issues where the model’s performance metrics reported
    are biased toward a specific resulting partitioning setting. The resulting partitioning
    may have some inherent distribution or nature that allowed results to perform
    particularly well or badly toward it. When that happens, having mismatched expectations
    of performance during the deployment stage will create more operational issues.
    To safely remove the possibility of such bias, **k-fold cross-validation** is
    typically used to report a more comprehensive validation score that could better
    reflect the performance of the model in the wild. To perform this partitioning
    method, a single testing set is removed from the original dataset, and validation
    scores are averaged across different ordered *k* cross-validation training and
    validation partitions. This is better visualized in *Figure 8**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – K-fold cross-validation as a strategy to eliminate metric reporting
    bias](img/B18187_08_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – K-fold cross-validation as a strategy to eliminate metric reporting
    bias
  prefs: []
  type: TYPE_NORMAL
- en: Finally, for testing performance reporting and deployment purposes, the model
    either gets retrained on the training and validation dataset combined or the model
    trained in the first fold is extracted for deployment purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Recall that stratified partitioning is a recommended strategy to split your
    data into the three mentioned partitions. This means that the data will be approximately
    evenly separated into three partitions based on the label associated with the
    dataset. This ensures that no labels get left out in any partition, which could
    potentially cause misinformation. Take a simple case of binary classification
    where the dataset is randomly partitioned into three partitions of training, validation,
    and testing with prespecified sizes. Since each data row was randomly placed into
    one of the three partitions, there is a probability the partitions will only contain
    one label from the two binary labels. Since the validation or testing partition
    is usually assigned with smaller data sizes, they have more potential to face
    this issue. Let’s say the model is mistakenly trained to predict only a single
    label. If the label is exactly the label that exclusively exists in the validation
    and testing of ML learning practitioners, we will mistakenly think the model is
    doing extremely well, but in fact, it is useless. You never know when you will
    be unlucky when doing full random partitioning, so use stratified random partitioning
    whenever you can!
  prefs: []
  type: TYPE_NORMAL
- en: The data-partitioning strategy described here builds only a single model that
    will be utilized during inference mode in model deployment. This strategy is the
    standard option when the inference runtime of the final model setup is a concern
    and having a faster model is more important than having small improvements in
    the accuracy metrics. When it is okay to trade runtime for some accuracy performance,
    an alternative strategy called **k-fold cross-validation ensemble** can be used.
    This is a method that is widely advocated in many ML competitions, especially
    in the ones hosted on Kaggle. The method uses the *k*-fold cross-validation described
    previously but actually uses *k* models trained during cross-validation and performs
    an ensemble of the k model’s predictions. An ensembling method called blending
    aggregates predictions of models and almost always improves the accuracy metrics
    from a single model. This process can be thought of as a method that leverages
    the best ideas and expertise of each *k* model, making the final outcome better
    as an aggregate. This aggregate can be as simple as an average or median of the
    *k* predictions.
  prefs: []
  type: TYPE_NORMAL
- en: A final tip before moving on to the next method is to always remember to make
    sure partitions match when comparing models between experiments. Misinformation
    often happens in the field when two models are separately developed using different
    data-partitioning strategies and data partitions. Even when one of the models
    achieves a significant performance advantage over the other, it does not mean
    anything and will not amount to any meaningful comparisons.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will dive into the data representation component for different data
    modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Representing different data modalities for training DL models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: So far, we have brushed over the utilization of numerical, categorical, text,
    audio, image, and video modalities. These are the most common modalities utilized
    across multiple industries. Representing different data modalities is a complicated
    topic as, in addition to the common modalities, there are actually a lot of rare
    data modalities out there. Examples of rare modalities are chemical formulas (a
    special structured form of textual data), document data (another special form
    of textual data with complex positional information), and graph data. In this
    section, we will only discuss the representation of the common unstructured modalities
    here to ensure the relevancy of content to our readers. Both numerical and categorical
    data are considered structured data and are have been covered properly in previous
    sections. Let’s now start with the text data modality.
  prefs: []
  type: TYPE_NORMAL
- en: Representing text data for supervised deep learning
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Text data representation in general has improved tremendously over the years.
    The following list shows a few relevant methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Term frequency-inverse document frequency (TF-IDF) with N-grams**: The term
    here is implemented with N-grams. An N-gram is an adjacent sequence of *n* textual
    characters. N-grams are produced by a method called tokenization. The tokenization
    can be a representation as low level as single characters, or it can be a higher-level
    representation such as words. Once represented as N-grams, TF-IDF is computed
    using the following formula.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TF-IDF = term frequency x inverse document frequency
  prefs: []
  type: TYPE_NORMAL
- en: '**Term frequency** is simply the count array of a single row. **Inverse document
    frequency** is computed through the following formula:'
  prefs: []
  type: TYPE_NORMAL
- en: IDF = log( number of text samples   ______________________________________    number
    of documents containing the term for each term )
  prefs: []
  type: TYPE_NORMAL
- en: The representation is an efficient and lightweight way to extract useful information
    from text, where words that are rare have higher values and more frequent words
    such as “the” and “and” will be suppressed. Outputs of TF-IDF can be directly
    fed into a simple **multilayer perceptron** (**MLP**) or any ML model to produce
    a predictive model. In simpler use cases, this representation will be enough to
    achieve a good metric performance. However, in more complex use cases that require
    the decoding of complex interactions that can happen with the different compositions
    of text and labels, it will underperform.
  prefs: []
  type: TYPE_NORMAL
- en: '**Word/token embeddings**: Word embeddings can be trained from scratch or pre-trained
    from a bigger dataset. Pre-trained embeddings can be pre-trained in either a supervised
    fashion or an unsupervised fashion, usually on a larger dataset. However, the
    embedding method suffers from the issue of token mismatch during the training,
    evaluation, and testing stages. This means that it is required to perform a lot
    of tinkering with the way the specific text token is preprocessed before looking
    up to the embeddings table. This occurrence is known as **out of vocabulary**
    (**OOV**) during the evaluation and testing stages. In the training stage, different
    variations of the same word will have their own meaning, which is inefficient
    in terms of learning and resource-space utilization. In practice, methods such
    as stemming, lemmatization, lowercasing, and known word-to-word replacements are
    applied to mitigate OOV, but the problem won’t be mitigated completely. These
    word embeddings can be paired with either **recurrent NNs** (**RNNs**) or transformers.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Subword-based tokenization**: This family of methods attempts to solve the
    token mismatch issue and the large vocabulary size of tokens. *Subword* might
    sound unintuitive, as we as humans use full words to perceive the meaning of text.
    This family of algorithms only performs subword tokenization when the word can’t
    be identified or is considered to be rare. For common words, they will remain
    full word tokens. Examples of such methods include **byte-pair encoding** (**BPE**),
    **WordPiece**, and **SentencePiece**. We will go through these methods briefly
    as a simple guide:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**BPE tokenization**: BPE treats the text as characters and groups up the most
    common consecutive characters iteratively during training. The number of iterations
    determines when the training iterations to group up the most common characters
    should be stopped. This formulation allows for rare words to remain as subword
    tokens and common words to be grouped up into a single token. This representation
    is notably used by **generative pre-trained transformer** (**GPT**) models. However,
    this representation faces an issue where there will be multiple ways to encode
    a particular word.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**WordPiece**: WordPiece improves upon BPE by utilizing a language model to
    choose the most likely pair of tokens to group up. This enforces a kind of intelligent
    choice when deciding the way to encode a particular word. This algorithm is utilized
    by **Bidirectional Encoder Representations from Transformers** (**BERT**) and
    **Efficiently Learning an Encoder that Classifies Token Replacements** **Accurately**
    (**ELECTRA**).'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**SentencePiece**: SentencePiece is a method that optimizes the tokens generated
    by base tokenizers such as BPE. It uses a couple of components, such as using
    a form of Unicode text conversion to ensure no language-dependent logic exists
    and using a method called subword regularization that performs a form of subword
    token group augmentation (probabilistically and randomly choose a single sample
    from the top-k predicted subword token to group up using language models) to solve
    multiple representation issues. This algorithm is used by XLNet and **A Lite BERT**
    (**ALBERT**) most notably.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Text data are represented as tokens for DL. This also means that there will
    be strict limits to the number of tokens so that the NN model can be initialized
    with the right parameters. Pick a token size limit that is reasonable for your
    use case based on what’s needed to get a good model performance. Since the size
    limit will affect model size, be sure to make sure the model size doesn’t get
    so big that it overshoots your inference runtime requirements.
  prefs: []
  type: TYPE_NORMAL
- en: In terms of missing text data, which can happen in real-world use cases with
    multimodal data, using an empty string is the most natural way to work as an imputation
    method. Under the hood, these models would usually use all zeros to represent
    the text array.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve briefly covered supervised text data representations, let’s discover
    supervised audio data representations next.
  prefs: []
  type: TYPE_NORMAL
- en: Representing audio data for DL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Audio data is time-series data with one or two arrays of values where each value
    in the array represents a single piece of audio data at a specific timestamp.
    Audio data can be either represented as a simple normalized form from the original
    raw data, represented as something called a spectrogram, which is a two-dimensional
    data, or as **Mel-frequency cepstral** **coefficients** (**MFCCs**).
  prefs: []
  type: TYPE_NORMAL
- en: A spectrogram is the resulting output of a process called `wav2vec 2.0`, which
    is a type of transformer.
  prefs: []
  type: TYPE_NORMAL
- en: 'The STFT process has hyperparameters that can affect the resulting representation.
    The following list summarizes these hyperparameters and tips on how to set it
    up properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Sampling rate**: This specifies the samples per second (Hertz/Hz) parameter
    that will be used before applying STFT. Audio data might be recorded in different
    sampling rates, and to build a model, this data will be required to be unified
    to a single sampling rate through resampling algorithms. The most typically used
    value is 16,000 Hz. As this value will affect the size and runtime of the model
    given a fixed time window a model is built to handle, be sure to only increase
    it if it’s necessary in terms of metric performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**STFT window length**: Each window size will be responsible for the data for
    a fixed duration and specific position. Each window will produce a single value
    at the same time window for a range of frequencies. The typical value of this
    parameter is 4096 or 2048\. Configure this based on the prediction resolution
    you require for your use case if there is a strict requirement there. This parameter
    will also affect the model size.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Window stride**: This is similar to a convolutional layer filter stride and
    does not have many significant tuning methods. Using a small percentage of the
    window length, such as 10%, should be a good enough setting.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Whether to use Mel scaling**: A **Mel scale** is a logarithmic transformation
    of the audio signal’s frequency. Fundamentally, it is a transformation to mimic
    how humans perceive audio. It makes higher-frequency changes matter less and lower-frequency
    changes matter more. Use this when it involves some form of human judgment to
    improve the metric performance.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As for empty audio rows, imputing them with a single pre-generated random noise
    audio or using an array of zeros with the same length should work well in multimodal
    datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve briefly covered supervised audio data representations, let’s
    discover supervised image and video data representations next.
  prefs: []
  type: TYPE_NORMAL
- en: Representing image and video data for DL
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Image data doesn’t require a lot of introduction here as we have gone through
    a few tutorials using them directly. The key is to perform some sort of normalization
    before feeding it to NN models such as CNNs, and the NN will extract great representations.
    Transformers have also been making tight competition with CNNs in image-based
    tasks and can be used to both extract representative features and predict directly
    on the task. One thing to note is that unless the resolution of the image is crucial
    in identifying certain patterns, it is usually much more effective as a model
    to utilize a smaller image resolution. One might not be able to visually certain
    patterns when the resolution of the image is smaller, but a computer would still
    be able to. The resolution of the image affects the runtime of the training, the
    runtime of the model in production, and sometimes the model size, as for transformers,
    so make sure this is done conservatively.
  prefs: []
  type: TYPE_NORMAL
- en: Video data, however, is an extended form of image data where a number of images
    are aligned sequentially to form a video. This means that video data is a form
    of sequential data just like text without absolute timestamp information. Each
    sequential image is known as a **frame**. Video can have a variety of frame rates.
    Commonly, this would be in rates of 24, 30, or 48 **frames per second** (**FPS**)
    but can generally be any number. For **computer vision** (**CV**) use cases, make
    sure to set a low FPS so that the processing load can be reduced depending on
    the use case. For example, the use case of lip reading has lower FPS requirements
    than for asking a model to identify whether a person is running or not. For the
    frame resolution, the same guide for image resolution applies here. Once the video
    properties have been decided, representative features have to be learned and extracted.
    The current SoTA features are extracted through models similar to image-based
    use cases. Examples of such models are 3D CNNs and 3D transformers.
  prefs: []
  type: TYPE_NORMAL
- en: There is, however, an intersection of these two data types, which are images
    extracted through video data. For this type of image data, it is possible to reduce
    the probability of predictive models making wrong predictions. ML models are not
    perfect predictors, so whenever there is a chance to reduce incorrect predictions
    such as false positives or false negatives without compromising the true predictions,
    do consider taking it. Consider using manual image processing techniques from
    the OpenCV library to perform any preliminary steps before a model takes the image
    as input. For example, the motion detection technique in OpenCV can be used as
    a preliminary condition checker before feeding the video array to the DL model.
    Since motion is required to identify most use cases of video data, it doesn’t
    make sense to predict anything if nothing is moving. This also reduces any false
    predictions that can happen from predicting on multiple unchanged video frames.
    The motion detector in OpenCV utilizes a simple change in pixel value without
    using a probabilistic model and thus is a far more reliable indicator of motion.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we’ve covered representing different data modalities, let’s move on
    to the topic of data augmentation.
  prefs: []
  type: TYPE_NORMAL
- en: Augmenting the data for training better DL models
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Augmentation is widely used in DL to increase the generalization of the resulting
    trained model and increase the metric performance of the model. By accounting
    for the additional unique variations brought in by augmentation, the model would
    be able to attend to these unique variations on external data during the validation,
    testing, and inference stage. Naturally, this will also reduce any over-dependence
    on a specific pattern and any benefits that come with a sufficiently sized dataset.
    Augmentation increases the amount of training data and thus the variations of
    patterns in the training data. The process is usually done randomly and individually
    in every training iteration in memory. This makes sure there are no limitations
    to the additional data variations available for training and also removes the
    need for additional storage. However, you can’t just randomly use all the available
    types of augmentation known for a specific modality. The following list shows
    the different types of augmentation you can perform on your data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Image**: Image sharpening through **Contrast Limited Adaptive Histogram Equalization**
    (**CLAHE**), hue and saturation variation, color channel shuffling, contrast variation,
    brightness variation, horizontal/vertical flip, grayscale conversion, blurring,
    image masking, mixup (weighted combination of images and their labels), cutmix
    (mixup, but only by random patches from the original image), and more. Look into
    [https://github.com/albumentations-team/albumentations](https://github.com/albumentations-team/albumentations)
    for more than 70 augmentations!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text**: Synonym replacement, back translation (a process that translates
    a text into another language and then translates it back into the original language),
    and more.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Video**: Video mixup (same as image mixup but for videos), all the same augmentation
    for images.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`librosa` library.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Choosing the type of augmentation to use requires some understanding of the
    expected environment that you will face when you deploy a model. Bad choices add
    noise to the model and might confuse the model during the training process, resulting
    in a degraded metric performance. Good choices revolve around estimating the variations
    that can realistically happen in the wild. Let’s take an example of a manufacturing
    use case where the goal is to deploy an image-based model that will predict product
    characteristics on a conveyor belt for sorting purposes using a camera sensor.
    If you can assume the camera will be fixed almost perfectly straight on the machine
    in all the setups, using image rotation augmentation likely wouldn’t be smart.
    Even if you want to use the augmentation, the rotation variation you should use
    should only be in the low end, such as below 10 degrees variation. Grayscale augmentation
    would also be unintuitive if the cameras do not provide grayscale images.
  prefs: []
  type: TYPE_NORMAL
- en: This concludes the data preparation stage of training effective models. Next,
    we will dive into the model training stage of the workflow.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring and tuning DL hyperparameters
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Hyperparameter configuration and tuning play a crucial role in training DL models
    effectively. They control the learning process of the model and can significantly
    impact the model’s performance, generalization, and convergence. In this section,
    we will discuss some essential hyperparameters and their impact on training DL
    models.
  prefs: []
  type: TYPE_NORMAL
- en: The most general set of impactful hyperparameters that need to be configured
    for training each NN model are its epochs, early stopping epochs, and learning
    rate. These three parameters are considered a set of parameters that together
    form a **learning schedule**. There have been a few notable learning schedules
    that focused on obtaining the best model with the least amount of time spent.
    However, they depend a lot on the initial estimation of the total number of epochs
    a model can converge with the method. Methods that depend on an estimation of
    the total number of epochs needed are fragile, and their configuration strategies
    are not easily transferable from one problem to another. Here, we will focus on
    using a validation dataset to track the number of epochs needed to achieve the
    best possible metric performance.
  prefs: []
  type: TYPE_NORMAL
- en: Early stopping epochs is a parameter that controls how many epochs you want
    to keep training before you stop. This strategy means that the epochs’ hyperparameter
    can either be set to an infinite number or a very large number so that the best-performing
    model on the validation dataset can be found. Early stopping reduces the number
    of training epochs you need to spend dynamically based on the validation dataset.
    By saving the best-performing model weights on the validation dataset, when the
    model is stopped early, you will then be able to load the best-performing weights.
    The typical early stopping epoch is `10`.
  prefs: []
  type: TYPE_NORMAL
- en: As for the learning rate, there are two general directions that work consistently
    well in practice. One is to immediately start with a large learning rate such
    as `0.1` and to gradually decay the learning rate. The gradual decay of the learning
    rate can be through percentage reductions from validation score monitoring when
    it doesn’t improve after `3` to `5` epochs. The second method is to use a smaller
    learning rate as a warmup method in initializing a base weight for the NN before
    using method 1\. In the initial stage of learning, as models are initially in
    a randomized state, the learning process can be very unstable where the loss will
    seem to not follow a proper improvement trend. Using a warmup helps to initialize
    the foundation needed to make stable loss progressions. Note that if pre-trained
    weights are used to initialize the model, a warmup is usually not needed as the
    model will already be in a stable state, especially if the pre-trained weights
    are obtained from a similar dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Batch size is another crucial hyperparameter in the training of DL models, as
    it determines the number of training samples used in a single update of the model’s
    weights during the optimization process. The choice of batch size can significantly
    impact the model’s training speed, memory requirements, and convergence. Smaller
    batch sizes, such as 16 or 32, provide a more accurate estimate of the gradient,
    leading to more stable convergence, but may require more training iterations and
    can be slower due to less parallelism in computation. On the other hand, larger
    batch sizes, such as 128 or 256, increase the level of parallelism, speeding up
    the training process and reducing memory requirements, but may lead to a less
    accurate gradient estimate and potentially less stable convergence. In practice,
    it’s essential to experiment with different batch sizes to find the one that provides
    the best balance between training speed and convergence stability for your specific
    problem. Additionally, modern DL frameworks often support adaptive batch size
    techniques, which can automatically adjust the batch size during training to optimize
    the learning process.
  prefs: []
  type: TYPE_NORMAL
- en: The foundational strategy discussed here is robust and can easily obtain the
    best-performing model most of the time while sacrificing some additional training
    time. It is worth noting that regularization methods, optimizers, and different
    activation functions have been covered in [*Chapter 2*](B18187_02.xhtml#_idTextAnchor040),
    *Designing Deep Learning Architectures*, and I encourage you to refer to that
    chapter for more information on those topics.
  prefs: []
  type: TYPE_NORMAL
- en: As much as there can be a manual strategy for the configuration of these hyperparameters,
    there will always be space to tune the hyperparameters further to optimize the
    metric performance of the model. Commonly, tuning can be executed through either
    grid search, random search, or tuning through more intelligent searching mechanisms.
    Grid search, otherwise known as brute-force searching, explores and validates
    all possible combinations of specified hyperparameter values to identify the optimal
    configuration for a given problem through cross-validation. For more intelligent
    tuning methods, refer back to [*Chapter 7*](B18187_07.xhtml#_idTextAnchor107),
    *Deep Neural Architecture Search*, for more insights on it. Additionally, as model
    evaluation metrics contribute to this hyperparameter-tuning process, we will explore
    more on this in [*Chapter 10*](B18187_10.xhtml#_idTextAnchor161), *Exploring Model*
    *Evaluation Methods*.
  prefs: []
  type: TYPE_NORMAL
- en: The core of hyperparameter tuning depends on the process and workflow to iteratively
    execute, visualize, track, and compare modeling experiments, with each configuration
    being part of a modeling experiment. This brings us to the next topic, diving
    into the actual workflow of training DL models effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Executing, visualizing, tracking, and comparing experiments
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The key to executing ML projects effectively is to iterate quickly between experiments.
    A lot of exploration is needed in any ML project, both in the initial stage of
    the project to gauge the viability of the use case, and in the later stage to
    improve the model’s performance. When this exploration process can be optimized,
    things meant to fail can fail quickly, and things that are viable can succeed
    quickly. Failure in ML projects is very common in practice. Once we acknowledge
    that and fail quickly, we can utilize the recovered time to tackle more valuable
    use cases. A good MLOps platform will help us execute, visualize, track, and compare
    experiments effectively and efficiently.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s go through an example practically with the Iris dataset and an MLP using
    the MLflow MLOps platform. We will also be using the `catalyst` library, which
    is also considered to be an MLOps platform, albeit partially and mostly focused
    on providing common `pytorch` DL model training tools. Since `catalyst` provides
    most of the model versioning and model storing mechanisms, we will only utilize
    the tracking feature in MLflow. The steps for this example are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import all the necessary libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, as we will be using the `pytorch`-based MLP, we will again set the random
    seed in `pytorch`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The dataset we will be using for the practical implementation here is the Iris
    dataset again. The dataset consists of the petal and sepal lengths of various
    flowers with three different iris types. We will now load this dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Scaling is a type of regularization method that can reduce memorization and
    reduce bias. Let’s perform a straightforward minimum and maximum scaling here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To train a model, we need a proper cross-validation strategy to verify the
    validity and performance of the model. We will use 77% of the data for training
    and 33% for validation. Let’s prepare the data for cross-validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will need to prepare the data loaders using the prepared data in `numpy`
    format for cross-validation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Since this is a multiclass problem of three classes, we will use the cross-entropy
    loss in `pytorch`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will be using the `pytorch` high-level wrapper library called `catalyst`
    here. To train a model in `catalyst`, we have to define a model trainer class
    instance called a runner:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will be using an MLP in this project with an MLP constructor class that
    allows us to specify the input data size, the hidden layer configuration, and
    the output data size. The hidden layer configuration is a list of layer sizes
    that simultaneously specifies the number of layers and the layer size at each
    layer. Let’s say that we want to randomly obtain 20 different hidden layer configurations
    and find out which performs the best on the validation partition. Let’s first
    define a method that generates the configuration randomly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s define a method that will allow us to train and evaluate the different
    MLP configurations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The trial number here plainly differentiates the different experiments. Apart
    from layer configuration, we can also configure the epochs that we want to run.
    In this method, we will create an MLP model instance based on the layer configuration
    passed in:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will use the `Adam` optimizer for gradient descent and set the checkpoint
    directory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will define the MLflow logger helper class available in `catalyst`
    to log experiments in MLflow format. In this setup, we log the mean and standard
    deviation of the training and validation log loss:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we will start the training process that trains for the specified number
    of epochs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As the last code step, we will loop through each randomly generated layer configuration
    and perform the training and evaluation process:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we need to start up the MLflow server service. We can do this by running
    the following command in the command line in the same directory as the directory
    that contains the introduced code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After running this command, the same directory should contain the following
    file named `.catalyst`, which instructs `catalyst` to enable MLflow support. This
    file should have the following content:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the command is executed, and by opening the HTTP website link, you should
    see the screen of MLflow, as shown in *Figure 8**.4*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: "![Figure 8.4 – M\uFEFFLflow interface](img/B18187_08_004.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – MLflow interface
  prefs: []
  type: TYPE_NORMAL
- en: 'The interface shows a convenient way to visualize performance differences between
    different experiments while showing the utilized parameters. The numerical metric
    values can be sorted to obtain the best-performing model on the validation partition,
    as shown in the figure. The process of preparing data and training a model requires
    iterative comparisons to be made between different setups or experiments. Experiments
    can be compared more objectively with quantitative metrics with their experimentation
    parameters. When displayed visually automatically through code in an interface
    instead of plugging it manually into an Excel or Google sheet, this makes the
    process much more dependable and organized. Additionally, if you click into any
    of the experiments, you’ll be able to check out the loss curves at each epoch
    interactively, as shown in *Figure 8**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: "![Figure 8.5 – M\uFEFFLflow interface showing an example loss curve of the\
    \ best model](img/B18187_08_005.jpg)"
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – MLflow interface showing an example loss curve of the best model
  prefs: []
  type: TYPE_NORMAL
- en: While ensuring speed of iteration, it is also required to organize and track
    all artifacts you generated for your model properly. This means that you need
    to version your model, your dataset, and any key components that affect the resulting
    model output. Artifacts can be model weights, metric performance reports, performance
    plots, embedding visualization, and loss visualization plots. This task can obviously
    be done manually through manual coding. However, organizing the artifacts of models’
    built-in experiments gets messy when the number of experiments goes up. Any custom
    files, graphs, and metrics can be tied into each of these experiment records and
    viewed in the MLflow interface. Experiments here can differ by using different
    models, different datasets, different featurization methods, different hyperparameters
    of a model, or a different sample size of the same dataset. Additionally, models
    can be stored directly in MLflow’s model registry, which allows MLflow to deploy
    the model directly.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring model-building tips
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This practical content serves as an example of how an MLOps platform such as
    MLflow can ease the process of building and choosing the right model programmatically
    and visually. As much as MLOps is great and helps in training models efficiently,
    there are a few things that an MLOps platform does not handle for you but are
    considered key components before a model can be properly utilized and have its
    predictions consumed. These components are listed next:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Prediction consistency validation test**: This is a test that ensures the
    predictions made by the same trained model are consistent on the same data. A
    model’s predictions can’t be utilized if its logic is not deterministic. This
    will be discussed further in [*Chapter 10*](B18187_10.xhtml#_idTextAnchor161),
    *Exploring Model* *Evaluation Methods*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch`, this can be done globally through the following code:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In `tensorflow` and `keras`, this can be done globally through the following
    code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This method automatically seeds both the `random` and `numpy` libraries. These
    global settings can help to set the random seed for layers that did not explicitly
    set random number generator seeds locally.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: One last piece of advice in experimentation is to make sure a baseline is created
    at the start of the project. A baseline is the simplest version of a solution
    possible. The solution can even be a non-DL model with simple features. Having
    a baseline can help ensure that any improvements or complications you add are
    justified by metric performance monitoring. Refrain from adding complications
    for the sake of them. Remember that the value of an ML project is not how complicated
    the process is but the results that can be extracted from it.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will dive into actual techniques that can be used to realize and improve
    a solution that utilizes DL.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring general techniques to realize and improve supervised deep learning
    based solutions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Notice that earlier in the chapter we focused on use cases based on problem
    types and not the problems themselves. Solutions in turn solve and take care of
    the problem. DL and ML in general are great solvers of issues related to staffing
    difficulties and for the automation of mundane tasks. Furthermore, ML models in
    computers can process data much quicker than an average human can, allowing a
    much quicker response time and much more efficient scaling of any process. In
    many cases, ML models can help to increase the accuracy and efficiency of processes.
    Sometimes, they improve current processes, and other times, they make previously
    unachievable processes possible. However, a single DL model may or may not be
    enough to solve the problem. Let’s take an example of a solution that *can* be
    solved sufficiently with a single DL model.
  prefs: []
  type: TYPE_NORMAL
- en: Consider the use case of using a DL model to predict the genders of babies with
    ultrasound imagery. Traditionally, a doctor would perform a visual-based gender
    analysis of the resulting ultrasound imagery of a baby in the mother’s womb in
    real time and offline before finally providing their prediction of the gender.
    Based on the amount of prior experience and knowledge, the doctor would have different
    levels of competency and accuracy in decoding the gender. Things might get more
    complicated when there are abnormalities in the baby. The probable underlying
    problem would be that experienced and capable doctors are scarce and expensive
    to hire. If we had a system that could decode the gender from the ultrasound imagery
    automatically, it would either be of good assistance to the judgment of real doctors
    or a replacement as a cheaper alternative. The same analogies can be applied to
    identifying diseases or symptoms in any advanced imaging results such as X-ray
    images.
  prefs: []
  type: TYPE_NORMAL
- en: This example depicts a DL model as a component of a solution and a solution
    where a single DL model is enough to obtain the desired output. This is an example
    of staffing issues but not so much on the efficiency side. Note that for some
    use cases, it is required to explain in some form the reasons that drove the predictions
    that were made. In other words, you’d have to explain the decisions that were
    made by the model. To provide assistance to a doctor, pinpointing where and which
    types of patterns contributed to the decision would be more helpful than the decision
    itself, as doctors would be able to utilize the extra information to make their
    own decisions. This will be thoroughly introduced in [*Chapter 11*](B18187_11.xhtml#_idTextAnchor172),
    *Explaining Neural* *Network Predictions*.
  prefs: []
  type: TYPE_NORMAL
- en: Explanations aside, not all solutions to problems can be accomplished by a single
    ML or DL model alone. At times, DL models have to be coupled with general ML methods,
    and at others, multiple DL models have to be coupled together. In some special
    cases, intermediate data needs to be specially processed and prepared before feeding
    it to the next task in a pipeline. Creating and architecting logical pipelines
    are essential when dealing with such problems. Let’s take an example of a problem
    and solution that requires multiple datasets, multiple DL models, and constructing
    a task pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Consider the problem of finding criminals who have just robbed a bank. By using
    CCTV cameras deployed in the city, you can use a face detection and recognition
    solution if you have identified the face of the criminals that did the robbery.
    The following figure shows an example solution task pipeline for this problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – Task pipeline of the solution for finding criminals through
    CCTV cameras](img/B18187_08_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – Task pipeline of the solution for finding criminals through CCTV
    cameras
  prefs: []
  type: TYPE_NORMAL
- en: Face detection is an image-object-detection process where there is an image
    bounding box regressor and a binary classifier that predicts whether the bounding
    box is a face or not. The representative facial features extraction utilizes a
    DL model that can be trained using supervised representation learning methods
    that are trained against the goal of optimizing the discriminative effects of
    facial features against the facial features of different persons. Next, a separate
    task is needed to build the database of criminal facial features that will be
    passed into the KNN ML algorithm to find the matched facial ID based on queried
    facial features obtained from CCTV cameras deployed in the city. This solution
    shows the need to break a solution into multiple components in order to obtain
    the final result of finding the criminals.
  prefs: []
  type: TYPE_NORMAL
- en: The preceding example is part of a larger paradigm called multitask learning
    and multitask problems. The multitask paradigm is a set of topics that allows
    for greater advancement in the ML space, not only for DL but definitely much more
    achievable through DL, due to its inherent flexibility. In the next topic, we
    will dive into the multitask paradigm.
  prefs: []
  type: TYPE_NORMAL
- en: Breaking down the multitask paradigm in supervised deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Multitask is a paradigm that covers a wide spectrum of tasks that involves
    the execution of ML models on multiple problems coupled with their respective
    datasets to achieve a goal. This paradigm is usually built based on two reasons:'
  prefs: []
  type: TYPE_NORMAL
- en: To achieve better predictive performance and generalization.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To break down complicated goals into smaller tasks that are directly solvable
    using separate ML models. This reiterates the point made in the previous topic.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s dive into four multitask techniques, starting with multitask pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Multitask pipelines
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: This variation of multitask systems revolves around realizing solutions that
    can’t be directly solved by using a single ML model. Breaking down highly complicated
    tasks into smaller tasks can allow solutions to be made with multiple ML models
    handling different smaller tasks. These tasks can be sequential or parallel in
    their paths and generally form a **directed acyclic graph** (**DAG**)-like pipeline,
    similar to the example shown in *Figure 8**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: However, this does not mean that the tasks should exclusively be ML models.
    Problems for different industries and businesses can be in many forms, and being
    flexible in assigning components needed to produce a solution is key to deriving
    value from ML technology. For example, if human supervision is needed to accomplish
    a certain task after breaking down the larger task, do not hesitate to utilize
    it along with ML models to achieve value. Let’s go through another use case that
    utilizes multitask pipelines to create a solution, which is **recommendation systems**.
    First, we need to perform either supervised or unsupervised representation learning
    for feature extraction. Second, using the features extracted, create a database
    used to match extracted query features. Third, obtain the top-k closest data from
    the database and apply a regression model to predict the rank of the top-k data
    for fine-tuned e-tuned high-performance ranking.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will discover another paradigm of multitasking, called **TL**.
  prefs: []
  type: TYPE_NORMAL
- en: TL
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TL is a technique that involves using what was learned from one task in another
    task. The core reasons can be one of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Increasing the metric performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Decreasing the required number of epochs needed for the network to reach a state
    of convergence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Allowing more stable learning trajectories. In other cases, networks just take
    longer to converge when the initial learning process is unstable. However, in
    some other cases, networks cannot converge at all when networks don’t have a stable
    foundation to start learning. TL can help models that originally fail to learn
    anything reach convergence in the learning process.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Increasing generalization and reducing the probability of overfitting. When
    the second task involves only a small subset of variations from the actual data
    population, knowledge learned from a first task that covers a wider range of variations
    helps to prevent narrow oversights.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Concretely, TL in DL is achieved by using the network parameters that were learned
    from the first task in the second task. The parameters involve all the weights
    and biases that are associated with a network. The parameters can be used as an
    initialization step for the same network instead of the usual randomly initialized
    parameters. These are known as **pre-trained weights**. The process of network
    learning with pre-trained weights is called **fine-tuning**. Additionally, the
    network parameters can also opt to be completely frozen and plainly act as a **featurizer**
    component that provides features for another SL algorithm.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are a couple of automated strategies that focus on improving the results
    you can get with fine-tuning. However, these methods are not silver bullets and
    can take a lot of time to carry out. The practical strategy to achieve a better
    performance using TL is to choose the number of layers you want to train by gauging
    the transferability component of the two tasks. *Table 8.2* shows an easy way
    to decide on a TL strategy based on task similarity and dataset size of the second
    task:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | **Dataset Size** |'
  prefs: []
  type: TYPE_TB
- en: '| **Small** | **Big** |'
  prefs: []
  type: TYPE_TB
- en: '| Task similarity | Low | Train the entire network as usual. | Train the entire
    network as usual. |'
  prefs: []
  type: TYPE_TB
- en: '| High | Freeze all base network parameters, add an extra linear prediction
    layer, and only train this linear layer on the dataset. | Train the entire network
    as usual. |'
  prefs: []
  type: TYPE_TB
- en: Table 8.2 – Deep TL (DTL) strategy guide
  prefs: []
  type: TYPE_NORMAL
- en: For clarity purposes, let’s say “big” is when the dataset has at least 10,000
    examples. For task transferability/similarity, human intuition is required to
    obtain an evaluation on a case-by-case basis. Here in the guide, we assume that
    a big dataset size means a dataset with large variations that represent the population
    adequately. A hidden component not presented in the preceding figure, however,
    is the size of the dataset of the first task. TL has the best impact when the
    task similarity is high, the second task dataset size is small, and, additionally,
    when the dataset size of the first task is big. The size of the first dataset
    usually also limits the range of NN sizes that can be used. Let’s say that the
    first dataset size is small; the best-performing models in this case are usually
    smaller-sized models. When TL is highly beneficial, even when the dataset size
    of the second dataset size is medium or big, the smaller models can still outperform
    bigger-sized models. An act of balancing is required in complex cases such as
    this to obtain the ideal model.
  prefs: []
  type: TYPE_NORMAL
- en: 'One prominent issue in TL is called **catastrophic forgetting**. This is a
    phenomenon where the network performance regresses to earlier tasks as the network
    trains on new tasks. If the performance of the previous task is not of concern
    to you, this issue can be ignored. Practically, if it is required to maintain
    the performance of the previous task, you can follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Use a unified metric that takes care of the performance of the first task and
    second task by additionally validating on the validation dataset of the first
    task.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Combine the dataset from the first task and second task and train it as a single
    model. If the targets are not relevant to each other, use different fully connected
    layer prediction heads.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Lastly, there is an additional popular technique for TL known as **knowledge
    distillation**. This method involves two models, where one pre-trained teacher
    model is used to distill its knowledge to a student model. Typically, the teacher
    model is a bigger model that has the capacity to learn more accurate information
    but is slower in runtime, and the student model is a smaller model that can be
    run at reasonable speeds during runtime. The method distills knowledge by using
    an additional similarity-based loss of a chosen layer output between the teacher
    and student model, which is typically the logit layer, on top of the base cross-entropy
    loss. This method encourages the student model to produce similar features to
    the teacher model. The technique is typically used to obtain a smaller model with
    better accuracy than if trained without knowledge distillation, so the deployment
    infrastructure can be cheaper. This technique will be practically introduced in
    [*Chapter 13*](B18187_13.xhtml#_idTextAnchor196), *Exploring Bias and Fairness*,
    as a key technique to also mitigate bias.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will dive into another type of multitask execution, called multiple
    objective learning.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple objective learning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Multiple objective learning is a type of multitasking process that involves
    training with simultaneously different goals. Different goals direct the learning
    trajectory of a network toward different paths. Multiple objective learning can
    be further broken down into the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: Multiple losses on the same outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Multiple targets, which are taken care of by separate NN prediction heads,
    each with their respective losses. This can be further broken down into the following
    categories:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multiple targets with real impact and usage.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: A single or multiple main targets and a single or multiple auxiliary targets.
    Auxiliary targets are paired with their own losses called auxiliary losses.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Aside from option *2(A)*, the other options (that is, *1* and *2(B)* ) for multiple
    objective learning are mainly used to improve metric performance. Metrics can
    be as simple as accuracy or log loss, or more intricate, such as the degree of
    bias toward a minority class. A simple more straightforward example of multiple
    objective learning is the multilabel target type. Multilabel is where multiple
    labels can be associated with a single data row. This means that the setup will
    be a multiple binary classification target, which is the case with *2(A)*.
  prefs: []
  type: TYPE_NORMAL
- en: Multiple targets and their associated losses mean there might be issues of conflicting
    gradients during the learning process. This phenomenon is more commonly known
    as **negative transfer**. A more extreme case of negative transfer is when gradients
    from the two losses cancel each other out when they have the same magnitude in
    exactly opposite directions. This will block the learning process of the model
    where the model will never converge. In reality, this issue can be at a lower
    scale and dampen the speed of convergence or, worse, introduce huge fluctuations
    that make it hard to learn anything. Unfortunately, there are no silver-bullet
    mitigation methods here other than to understand the background behind why a model
    learns poorly. Iterative experiments are usually required to figure out how to
    balance these losses properly to encourage a stable learning process.
  prefs: []
  type: TYPE_NORMAL
- en: Next, we will dive into multimodal NN training.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal NN training
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Multimodal NNs are a type of multitask system in the sense that networks responsible
    for different modalities learn in the same task in completely different paths.
    A common method of handling multimodality in NNs is to assign different neural
    blocks at the initial stage for different data modalities. Neural blocks contain
    the networks specific to each modality. The neural blocks for different modalities
    will then be merged using a series of intermediate fully connected layers and
    an output fully connected layer. This is depicted in *Figure 8**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – Typical multimodal NN structure](img/B18187_08_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – Typical multimodal NN structure
  prefs: []
  type: TYPE_NORMAL
- en: 'The idea of leveraging multimodality is that the additional data input can
    allow for more comprehensive patterns to be identified and thus should improve
    the overall metric performance. In reality, this commonly will not be the case
    without careful handling of the training process. Different modalities exist in
    entirely different distributions and learn at different rates with different paths.
    A single global optimization strategy applied to all the data modalities will
    likely produce suboptimal results. A common and effective strategy is to do the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: Pretrain the individual modality neural block (unimodal) with a temporary prediction
    output layer until a certain degree of convergence
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Remove the temporary prediction output layer and train the multimodal NN as
    usual with the pre-trained weights from the unimodal training process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other than that, freezing the weights of the unimodal trained NN and only training
    the multimodal aggregation fully connected layer prediction head is also a sound
    strategy. Many more complex strategies exist to tackle this issue but are out
    of scope in this book.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we explored supervised deep learning, including the types of
    problems it can be used to solve and the techniques for implementing and training
    DL models. Supervised deep learning involves training a model on labeled data
    to make predictions on new data. We also covered a variety of supervised learning
    use cases on different problem types, including binary classification, multiclassification,
    regression, and multitask and representation learning. The chapter also covered
    techniques for training DL models effectively, including regularization and hyperparameter
    tuning, and provided practical implementations in the Python programming language
    using popular DL frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: Supervised deep learning can be used for a wide range of real-world applications
    in tasks such as image classification, **natural language processing** (**NLP**),
    and speech recognition. With the knowledge provided in this chapter, you should
    be able to identify supervised learning applications and train DL models effectively.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will explore **unsupervised learning** for DL.
  prefs: []
  type: TYPE_NORMAL
