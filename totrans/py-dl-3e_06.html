<html><head></head><body>
<div id="_idContainer693">
<h1 class="chapter-number" id="_idParaDest-111" lang="en-GB"><a id="_idTextAnchor185"/><span class="koboSpan" id="kobo.1.1">6</span></h1>
<h1 id="_idParaDest-112" lang="en-GB"><a id="_idTextAnchor186"/><span class="koboSpan" id="kobo.2.1">Natural Language Processing and Recurrent Neural Networks</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.3.1">This chapter will introduce two different topics that nevertheless complement each other – </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">natural language processing</span></strong><span class="koboSpan" id="kobo.5.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.6.1">NLP</span></strong><span class="koboSpan" id="kobo.7.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">recurrent neural networks</span></strong><span class="koboSpan" id="kobo.9.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.10.1">RNNs</span></strong><span class="koboSpan" id="kobo.11.1">). </span><span class="koboSpan" id="kobo.11.2">NLP teaches computers to process</span><a id="_idIndexMarker776"/><span class="koboSpan" id="kobo.12.1"> and analyze natural language</span><a id="_idIndexMarker777"/><span class="koboSpan" id="kobo.13.1"> text to perform tasks such as machine translation, sentiment analysis, and text generation. </span><span class="koboSpan" id="kobo.13.2">Unlike images in computer vision, natural text represents a different type of data, where the order (or sequence) of the elements matters. </span><span class="koboSpan" id="kobo.13.3">Thankfully, RNNs are suitable for processing sequential data, such as text or time series. </span><span class="koboSpan" id="kobo.13.4">They help us deal with sequences of variable length by defining a recurrence relation over these sequences (hence the name). </span><span class="koboSpan" id="kobo.13.5">This makes NLP and RNNs natural allies. </span><span class="koboSpan" id="kobo.13.6">In fact, RNNs can be applied to any problem since it has been proven that they are Turing-complete – theoretically, they can simulate any program that a regular computer would not be able </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">to compute.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.15.1">However, it is not only good news, and we’ll have to start with a disclaimer. </span><span class="koboSpan" id="kobo.15.2">Although RNNs have great theoretical properties, we now know that there are practical limitations to their use. </span><span class="koboSpan" id="kobo.15.3">These limitations</span><a id="_idIndexMarker778"/><span class="koboSpan" id="kobo.16.1"> have been mostly surpassed by a more recent </span><strong class="bold"><span class="koboSpan" id="kobo.17.1">neural network</span></strong><span class="koboSpan" id="kobo.18.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.19.1">NN</span></strong><span class="koboSpan" id="kobo.20.1">) architecture called </span><strong class="bold"><span class="koboSpan" id="kobo.21.1">transformer</span></strong><span class="koboSpan" id="kobo.22.1">, which we’ll discuss in </span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.23.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.24.1">. </span><span class="koboSpan" id="kobo.24.2">In theory, the transformer</span><a id="_idIndexMarker779"/><span class="koboSpan" id="kobo.25.1"> has more limitations compared to RNNs. </span><span class="koboSpan" id="kobo.25.2">But as sometimes happens, it works better in practice. </span><span class="koboSpan" id="kobo.25.3">Nevertheless, I believe that this chapter will be beneficial to you. </span><span class="koboSpan" id="kobo.25.4">On one hand, RNNs have elegant architecture and still represent one of the major NN classes; on the other hand, the progression of knowledge presented in this and the next three chapters will closely match the real-world progression of research on these topics. </span><span class="koboSpan" id="kobo.25.5">So, you’ll be able to apply the concepts you’ll learn here in the next few chapters as well. </span><span class="koboSpan" id="kobo.25.6">This chapter will also allow you to fully appreciate the advantages of the </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">newer models.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.27.1">This chapter will cover the </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">following topics:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.29.1">Natural </span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">language processing</span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.31.1">Introducing RNNs</span></span></li>
</ul>
<h1 id="_idParaDest-113" lang="en-GB"><a id="_idTextAnchor187"/><span class="koboSpan" id="kobo.32.1">Technical requirements</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.33.1">We’ll implement the example in this chapter using Python, PyTorch, and the TorchText package (</span><a href="https://github.com/pytorch/text"><span class="koboSpan" id="kobo.34.1">https://github.com/pytorch/text</span></a><span class="koboSpan" id="kobo.35.1">). </span><span class="koboSpan" id="kobo.35.2">If you don’t have an environment set up with these tools, fret not – the example is available as a Jupyter Notebook on Google Colab. </span><span class="koboSpan" id="kobo.35.3">You can find the code examples in this book’s GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.36.1">repository: </span></span><a href="https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter06"><span class="No-Break"><span class="koboSpan" id="kobo.37.1">https://github.com/PacktPublishing/Python-Deep-Learning-Third-Edition/tree/main/Chapter06</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.38.1">.</span></span></p>
<h1 id="_idParaDest-114" lang="en-GB"><a id="_idTextAnchor188"/><span class="koboSpan" id="kobo.39.1">Natural language processing</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.40.1">NLP is a subfield of machine learning</span><a id="_idIndexMarker780"/><span class="koboSpan" id="kobo.41.1"> that allows computers to interpret, manipulate, and comprehend human language. </span><span class="koboSpan" id="kobo.41.2">This definition sounds a little dry, so, to provide a little clarity, let’s start with a non-exhaustive list of the types of tasks that fall under the </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">NLP umbrella:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.43.1">Text classification</span></strong><span class="koboSpan" id="kobo.44.1">: This assigns a single label to the entire</span><a id="_idIndexMarker781"/><span class="koboSpan" id="kobo.45.1"> input text. </span><span class="koboSpan" id="kobo.45.2">For</span><a id="_idIndexMarker782"/><span class="koboSpan" id="kobo.46.1"> example, </span><strong class="bold"><span class="koboSpan" id="kobo.47.1">sentiment analysis</span></strong><span class="koboSpan" id="kobo.48.1"> can determine whether a product review </span><a id="_idIndexMarker783"/><span class="koboSpan" id="kobo.49.1">is positive </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">or negative.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.51.1">Token classification</span></strong><span class="koboSpan" id="kobo.52.1">: This assigns a label for each token of the input</span><a id="_idIndexMarker784"/><span class="koboSpan" id="kobo.53.1"> text. </span><span class="koboSpan" id="kobo.53.2">A token</span><a id="_idIndexMarker785"/><span class="koboSpan" id="kobo.54.1"> is a building block (or a unit) of text. </span><span class="koboSpan" id="kobo.54.2">Words</span><a id="_idIndexMarker786"/><span class="koboSpan" id="kobo.55.1"> can be tokens. </span><span class="koboSpan" id="kobo.55.2">A popular token classification task is </span><strong class="bold"><span class="koboSpan" id="kobo.56.1">named entity recognition</span></strong><span class="koboSpan" id="kobo.57.1">, which assigns each token to a list of predefined classes such as place, company, or person. </span><strong class="bold"><span class="koboSpan" id="kobo.58.1">Part-of-speech</span></strong><span class="koboSpan" id="kobo.59.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.60.1">POS</span></strong><span class="koboSpan" id="kobo.61.1">) tagging assigns</span><a id="_idIndexMarker787"/><span class="koboSpan" id="kobo.62.1"> each word to a particular part of speech, such as a noun, verb, </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">or adjective.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.64.1">Text generation</span></strong><span class="koboSpan" id="kobo.65.1">: This uses the input text to generate</span><a id="_idIndexMarker788"/><span class="koboSpan" id="kobo.66.1"> new text with arbitrary length. </span><span class="koboSpan" id="kobo.66.2">Text generation</span><a id="_idIndexMarker789"/><span class="koboSpan" id="kobo.67.1"> tasks include machine translation, question answering, and text </span><a id="_idIndexMarker790"/><span class="koboSpan" id="kobo.68.1">summarization (creating a shorter version of the original text while preserving </span><span class="No-Break"><span class="koboSpan" id="kobo.69.1">its essence).</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.70.1">Solving NLP problems is not trivial. </span><span class="koboSpan" id="kobo.70.2">To understand why, let’s go back to computer vision (</span><a href="B19627_04.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.71.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.72.1">), where the input images are represented as 2D tensors of pixel intensities with the </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">following properties:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.74.1">The image is composed of pixels and doesn’t have any other explicitly </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">defined structure</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.76.1">The pixels form implicit hierarchical structures of larger objects, based on their proximity to </span><span class="No-Break"><span class="koboSpan" id="kobo.77.1">each other</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.78.1">There is only one type of pixel, which is defined only by its </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">scalar intensity</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.80.1">Thanks to its homogenous</span><a id="_idIndexMarker791"/><span class="koboSpan" id="kobo.81.1"> structure, we can feed the (almost) raw image to a </span><strong class="bold"><span class="koboSpan" id="kobo.82.1">convolutional neural network</span></strong><span class="koboSpan" id="kobo.83.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.84.1">CNN</span></strong><span class="koboSpan" id="kobo.85.1">) and let it do its magic with relatively little </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">data pre-processing.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.87.1">Now, let’s return</span><a id="_idIndexMarker792"/><span class="koboSpan" id="kobo.88.1"> to text data, which has the </span><span class="No-Break"><span class="koboSpan" id="kobo.89.1">following properties:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.90.1">There are different types of characters with different semantical meanings, such as letters, digits, and punctuation marks. </span><span class="koboSpan" id="kobo.90.2">In addition, we might encounter previously </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">unknown symbols.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.92.1">The natural text has an explicit hierarchy in the form of characters, words, sentences, and paragraphs. </span><span class="koboSpan" id="kobo.92.2">We also have quotes, titles, and a hierarchy </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">of headings.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.94.1">Some parts of the text may be related to distant parts of the sequence, rather than their immediate context. </span><span class="koboSpan" id="kobo.94.2">For example, a fictional story can introduce a person by their name but later refer to them only as </span><em class="italic"><span class="koboSpan" id="kobo.95.1">he</span></em><span class="koboSpan" id="kobo.96.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.97.1">she</span></em><span class="koboSpan" id="kobo.98.1">. </span><span class="koboSpan" id="kobo.98.2">These references can be separated by long text sequences, yet, we still have to be able to find </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">this relation.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.100.1">The complexity</span><a id="_idIndexMarker793"/><span class="koboSpan" id="kobo.101.1"> of natural text requires several pre-processing steps before the actual NN model comes into play. </span><span class="koboSpan" id="kobo.101.2">The first step is </span><strong class="bold"><span class="koboSpan" id="kobo.102.1">normalization</span></strong><span class="koboSpan" id="kobo.103.1">, which involves operations</span><a id="_idIndexMarker794"/><span class="koboSpan" id="kobo.104.1"> such as removing extra whitespace and converting all letters into lowercase. </span><span class="koboSpan" id="kobo.104.2">The next steps are not as straightforward, so we’ll dedicate the next two sections </span><span class="No-Break"><span class="koboSpan" id="kobo.105.1">to them.</span></span></p>
<h2 id="_idParaDest-115" lang="en-GB"><a id="_idTextAnchor189"/><span class="koboSpan" id="kobo.106.1">Tokenization</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.107.1">One intuitive way to approach</span><a id="_idIndexMarker795"/><span class="koboSpan" id="kobo.108.1"> an NLP task is to split the corpus into words, which will represent the basic input units of our model. </span><span class="koboSpan" id="kobo.108.2">However, using words as input is not set in stone and we can use other elements, such</span><a id="_idIndexMarker796"/><span class="koboSpan" id="kobo.109.1"> as individual characters, phrases, or even whole sentences. </span><span class="koboSpan" id="kobo.109.2">The generic term for these units is </span><strong class="bold"><span class="koboSpan" id="kobo.110.1">tokens</span></strong><span class="koboSpan" id="kobo.111.1">. </span><span class="koboSpan" id="kobo.111.2">A token refers to a text corpus in the same way</span><a id="_idIndexMarker797"/><span class="koboSpan" id="kobo.112.1"> as a pixel refers to an image. </span><span class="koboSpan" id="kobo.112.2">The process of splitting the corpus into tokens is called </span><strong class="bold"><span class="koboSpan" id="kobo.113.1">tokenization</span></strong><span class="koboSpan" id="kobo.114.1"> (what a surprise!). </span><span class="koboSpan" id="kobo.114.2">The entity </span><br/><span class="koboSpan" id="kobo.115.1">(for example, an algorithm) that performs</span><a id="_idIndexMarker798"/><span class="koboSpan" id="kobo.116.1"> this tokenization is called </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">a </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.118.1">tokenizer</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.120.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.121.1">The tokenizers we’ll discuss in this section are universal in the sense that they can work with different NLP ML algorithms. </span><span class="koboSpan" id="kobo.121.2">Therefore, the pre-processing algorithms in this section are commonly used with transformer models, which we’ll introduce in </span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.122.1">Chapter 7</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.123.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.124.1">With that, let’s discuss the types </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">of tokenizers:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.126.1">Word-based</span></strong><span class="koboSpan" id="kobo.127.1">: Each word represents a unique</span><a id="_idIndexMarker799"/><span class="koboSpan" id="kobo.128.1"> token. </span><span class="koboSpan" id="kobo.128.2">This is the most intuitive</span><a id="_idIndexMarker800"/><span class="koboSpan" id="kobo.129.1"> type of tokenization, but it has serious drawbacks. </span><span class="koboSpan" id="kobo.129.2">For example, the words </span><em class="italic"><span class="koboSpan" id="kobo.130.1">don’t</span></em><span class="koboSpan" id="kobo.131.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.132.1">do not</span></em><span class="koboSpan" id="kobo.133.1"> will be represented by different tokens, but they mean the same thing. </span><span class="koboSpan" id="kobo.133.2">Another example is the words </span><em class="italic"><span class="koboSpan" id="kobo.134.1">car</span></em><span class="koboSpan" id="kobo.135.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.136.1">cars</span></em><span class="koboSpan" id="kobo.137.1"> or </span><em class="italic"><span class="koboSpan" id="kobo.138.1">ready</span></em><span class="koboSpan" id="kobo.139.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.140.1">readily</span></em><span class="koboSpan" id="kobo.141.1">, which will be represented by different tokens, whereas a single token would be more appropriate. </span><span class="koboSpan" id="kobo.141.2">Because natural language is so diverse, there are many corner cases like these. </span><span class="koboSpan" id="kobo.141.3">The issue isn’t just that semantically similar words will have unrelated tokens, but also the large number of unique tokens that come out of this. </span><span class="koboSpan" id="kobo.141.4">This will make the model computationally inefficient. </span><span class="koboSpan" id="kobo.141.5">It will also produce many tokens with a small number of occurrences, which will prove challenging for the model to learn. </span><span class="koboSpan" id="kobo.141.6">Finally, we might encounter unknown words in a new </span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">text corpus.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.143.1">Character-based</span></strong><span class="koboSpan" id="kobo.144.1">: Each character (letter, digit, punctuation, and so on) in the</span><a id="_idIndexMarker801"/><span class="koboSpan" id="kobo.145.1"> text is a unique </span><a id="_idIndexMarker802"/><span class="koboSpan" id="kobo.146.1">token. </span><span class="koboSpan" id="kobo.146.2">In this way, we have fewer tokens, as the total number of characters is limited and finite. </span><span class="koboSpan" id="kobo.146.3">Since we know all the characters in advance, we cannot encounter </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">unknown symbols.</span></span><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.148.1">However, this tokenization is less intuitive than the word-based model because a context composed of characters is less meaningful than a context based on words. </span><span class="koboSpan" id="kobo.148.2">While the number of unique tokens</span><a id="_idIndexMarker803"/><span class="koboSpan" id="kobo.149.1"> is relatively small, the total number of tokens in the corpus</span><a id="_idIndexMarker804"/><span class="koboSpan" id="kobo.150.1"> will be very large (equal to the total number </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">of characters).</span></span></p></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.152.1">Subword tokenization</span></strong><span class="koboSpan" id="kobo.153.1">: This is a two-step process that starts by splitting the corpus</span><a id="_idIndexMarker805"/><span class="koboSpan" id="kobo.154.1"> into words. </span><span class="koboSpan" id="kobo.154.2">The most obvious way to split the text</span><a id="_idIndexMarker806"/><span class="koboSpan" id="kobo.155.1"> is on whitespace. </span><span class="koboSpan" id="kobo.155.2">In addition, we can also split it on whitespace </span><em class="italic"><span class="koboSpan" id="kobo.156.1">and punctuation marks</span></em><span class="koboSpan" id="kobo.157.1">. </span><span class="koboSpan" id="kobo.157.2">In NLP parlance, this step is known as </span><strong class="bold"><span class="koboSpan" id="kobo.158.1">pre-tokenization</span></strong> <br/><span class="koboSpan" id="kobo.159.1">(the prefix implies that tokenization will follow). </span><span class="koboSpan" id="kobo.159.2">Then, it preserves</span><a id="_idIndexMarker807"/><span class="koboSpan" id="kobo.160.1"> the frequently used words and decomposes the rare words into meaningful subwords, which are more frequent. </span><span class="koboSpan" id="kobo.160.2">For example, we can decompose the word </span><em class="italic"><span class="koboSpan" id="kobo.161.1">tokenization</span></em><span class="koboSpan" id="kobo.162.1"> into the core word </span><em class="italic"><span class="koboSpan" id="kobo.163.1">token</span></em><span class="koboSpan" id="kobo.164.1"> and the suffix </span><em class="italic"><span class="koboSpan" id="kobo.165.1">ization</span></em><span class="koboSpan" id="kobo.166.1">, each with its own token. </span><span class="koboSpan" id="kobo.166.2">Then, when we encounter the word </span><em class="italic"><span class="koboSpan" id="kobo.167.1">carbonization</span></em><span class="koboSpan" id="kobo.168.1">, we can decompose it into </span><em class="italic"><span class="koboSpan" id="kobo.169.1">carbon</span></em><span class="koboSpan" id="kobo.170.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.171.1">ization</span></em><span class="koboSpan" id="kobo.172.1">. </span><span class="koboSpan" id="kobo.172.2">In this way, we’ll have two occurrences of </span><em class="italic"><span class="koboSpan" id="kobo.173.1">ization</span></em><span class="koboSpan" id="kobo.174.1"> instead of a single occurrence of </span><em class="italic"><span class="koboSpan" id="kobo.175.1">tokenization</span></em><span class="koboSpan" id="kobo.176.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.177.1">carbonization</span></em><span class="koboSpan" id="kobo.178.1">. </span><span class="koboSpan" id="kobo.178.2">Subword tokenization also makes it possible to decompose unknown words into </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">known tokens.</span></span></li>
</ul>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.180.1">Special service tokens</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.181.1">For the concept of tokenization to work, it introduces some service tokens. </span><span class="koboSpan" id="kobo.181.2">These include </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">the following:</span></span></p>
<ul>
<li class="callout" lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.183.1">UNK</span></strong><span class="koboSpan" id="kobo.184.1">: Replaces unknown tokens</span><a id="_idIndexMarker808"/><span class="koboSpan" id="kobo.185.1"> in the corpus (think of rare words such as </span><span class="No-Break"><span class="koboSpan" id="kobo.186.1">alphanumeric designations)</span></span></li>
<li class="callout" lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.187.1">EOS</span></strong><span class="koboSpan" id="kobo.188.1">: An end-of-sentence</span><a id="_idIndexMarker809"/><span class="koboSpan" id="kobo.189.1"> (or </span><span class="No-Break"><span class="koboSpan" id="kobo.190.1">sequence) token</span></span></li>
<li class="callout" lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.191.1">BOS</span></strong><span class="koboSpan" id="kobo.192.1">: A beginning-of-sentence</span><a id="_idIndexMarker810"/><span class="koboSpan" id="kobo.193.1"> (or </span><span class="No-Break"><span class="koboSpan" id="kobo.194.1">sequence) token</span></span></li>
<li class="callout" lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.195.1">SEP</span></strong><span class="koboSpan" id="kobo.196.1">: This separates two semantically</span><a id="_idIndexMarker811"/><span class="koboSpan" id="kobo.197.1"> different text sequences, such as question </span><span class="No-Break"><span class="koboSpan" id="kobo.198.1">and answer</span></span></li>
<li class="callout" lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.199.1">PAD</span></strong><span class="koboSpan" id="kobo.200.1">: This is a padding token</span><a id="_idIndexMarker812"/><span class="koboSpan" id="kobo.201.1"> that is appended to an existing sequence so that it can reach some predefined length and fit in a </span><span class="No-Break"><span class="koboSpan" id="kobo.202.1">fixed-length mini-batch.</span></span></li>
</ul>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.203.1">For example, we can tokenize the sentence </span><em class="italic"><span class="koboSpan" id="kobo.204.1">I bought a product called FD543C</span></em><span class="koboSpan" id="kobo.205.1"> into </span><em class="italic"><span class="koboSpan" id="kobo.206.1">BOS I bought a product called UNK EOS PAD PAD</span></em><span class="koboSpan" id="kobo.207.1"> to fit a fixed input with a length </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">of 10.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.209.1">Subword tokenization</span><a id="_idIndexMarker813"/><span class="koboSpan" id="kobo.210.1"> is the most popular type of tokenization</span><a id="_idIndexMarker814"/><span class="koboSpan" id="kobo.211.1"> because it combines the best features of character-based (smaller vocabulary size) and word-based (meaningful context) tokenization. </span><span class="koboSpan" id="kobo.211.2">In the next few sections, we’ll discuss some of the most popular </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">subword tokenizers.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.213.1">Byte-Pair Encoding and WordPiece</span></h3>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.214.1">Byte-Pair Encoding</span></strong><span class="koboSpan" id="kobo.215.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.216.1">BPE</span></strong><span class="koboSpan" id="kobo.217.1">, Neural Machine Translation of Rare Words with Subword Units, </span><a href="https://arxiv.org/abs/1508.07909"><span class="koboSpan" id="kobo.218.1">https://arxiv.org/abs/1508.07909</span></a><span class="koboSpan" id="kobo.219.1">) is a popular subword tokenization algorithm. </span><span class="koboSpan" id="kobo.219.2">As with other</span><a id="_idIndexMarker815"/><span class="koboSpan" id="kobo.220.1"> such tokenizers, it begins with pre-tokenization, which splits</span><a id="_idIndexMarker816"/><span class="koboSpan" id="kobo.221.1"> the corpus into words. </span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.222.1">Using this dataset as a starting point, BPE works in the </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">following way:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.224.1">Start with an initial </span><strong class="bold"><span class="koboSpan" id="kobo.225.1">base</span></strong><span class="koboSpan" id="kobo.226.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.227.1">seed</span></strong><span class="koboSpan" id="kobo.228.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.229.1">vocabulary</span></strong><span class="koboSpan" id="kobo.230.1">, which consists of the individual characters</span><a id="_idIndexMarker817"/><span class="koboSpan" id="kobo.231.1"> of all words in the text corpus. </span><span class="koboSpan" id="kobo.231.2">Therefore, each word is a sequence of </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">single-character tokens.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.233.1">Repeat the following until the size of the token vocabulary reaches a certain </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">maximum threshold:</span></span><ol><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.235.1">Find the pair of tokens (initially, these are single characters) that occur together most frequently and merge them into a new </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">composite token.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.237.1">Extend the existing token vocabulary with the new </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">composite token.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.239.1">Update the tokenized</span><a id="_idIndexMarker818"/><span class="koboSpan" id="kobo.240.1"> text corpus with the new </span><span class="No-Break"><span class="koboSpan" id="kobo.241.1">token structure.</span></span></li></ol></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.242.1">To understand BPE, let’s assume</span><a id="_idIndexMarker819"/><span class="koboSpan" id="kobo.243.1"> that our corpus consists of the following (imaginary) words: </span><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">{dab: 5, adab: 4, aab: 7, bub: 9, bun: 2}</span></strong><span class="koboSpan" id="kobo.245.1">. </span><span class="koboSpan" id="kobo.245.2">The digit following </span><a id="_idIndexMarker820"/><span class="koboSpan" id="kobo.246.1">each word indicates the number of occurrences of that word in the text. </span><span class="koboSpan" id="kobo.246.2">And here is the same corpus, but split into tokens (that is, characters): </span><strong class="source-inline"><span class="koboSpan" id="kobo.247.1">{(d, a, b): 5, (a, d, a, b): 4, (a, a, b): 7, (b, u, b): 9, (b, u, c): 2}</span></strong><span class="koboSpan" id="kobo.248.1">. </span><span class="koboSpan" id="kobo.248.2">Based on this, we can build our initial token vocabulary with occurrence counts for each token: </span><strong class="source-inline"><span class="koboSpan" id="kobo.249.1">{b: 36, a: 27, u: 11, d: 9, c: 2}</span></strong><span class="koboSpan" id="kobo.250.1">. </span><span class="koboSpan" id="kobo.250.2">The following list illustrates the first four </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">merge operations:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.252.1">The most common pair of tokens is </span><strong class="source-inline"><span class="koboSpan" id="kobo.253.1">(a, b)</span></strong><span class="koboSpan" id="kobo.254.1">, which occurs </span><strong class="source-inline"><span class="koboSpan" id="kobo.255.1">freq((a, b)) = 5 + 4 + 7 = 16</span></strong><span class="koboSpan" id="kobo.256.1"> times. </span><span class="koboSpan" id="kobo.256.2">Therefore, we merge them, and the corpus becomes </span><strong class="source-inline"><span class="koboSpan" id="kobo.257.1">{(d, </span></strong><strong class="bold"><span class="koboSpan" id="kobo.258.1">ab</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.259.1">): 5, (a, d, </span></strong><strong class="bold"><span class="koboSpan" id="kobo.260.1">ab</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.261.1">): 4, (a, </span></strong><strong class="bold"><span class="koboSpan" id="kobo.262.1">ab</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.263.1">): 7, (b, u, b): 9, (b, u, c): 2}</span></strong><span class="koboSpan" id="kobo.264.1">. </span><span class="koboSpan" id="kobo.264.2">The new token vocabulary is </span><strong class="source-inline"><span class="koboSpan" id="kobo.265.1">{b: 20, </span></strong><strong class="bold"><span class="koboSpan" id="kobo.266.1">ab</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.267.1">: 16, a: 11, u: 11, d: 9, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.268.1">c: 2}</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.270.1">The new most common token pair is </span><strong class="source-inline"><span class="koboSpan" id="kobo.271.1">(b, u)</span></strong><span class="koboSpan" id="kobo.272.1"> with </span><strong class="source-inline"><span class="koboSpan" id="kobo.273.1">freq((b, u)) = 9 + 2 = 11</span></strong><span class="koboSpan" id="kobo.274.1"> occurrences. </span><span class="koboSpan" id="kobo.274.2">Again, we proceed to combine them in a new token: </span><strong class="source-inline"><span class="koboSpan" id="kobo.275.1">{(d, ab): 5, (a, d, ab): 4, (a, ab): 7, (</span></strong><strong class="bold"><span class="koboSpan" id="kobo.276.1">bu</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.277.1">, b): 9, (</span></strong><strong class="bold"><span class="koboSpan" id="kobo.278.1">bu</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.279.1">, c): 2}</span></strong><span class="koboSpan" id="kobo.280.1">. </span><span class="koboSpan" id="kobo.280.2">The updated token vocabulary is </span><strong class="source-inline"><span class="koboSpan" id="kobo.281.1">{ab: 16, a: 11, </span></strong><strong class="bold"><span class="koboSpan" id="kobo.282.1">bu</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.283.1">: 11, b: 9, d: 9, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.284.1">c: 2}</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.285.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.286.1">The next token pair is </span><strong class="source-inline"><span class="koboSpan" id="kobo.287.1">(d, ab)</span></strong><span class="koboSpan" id="kobo.288.1"> and it occurs </span><strong class="source-inline"><span class="koboSpan" id="kobo.289.1">freq((d, ab)) = 5 + 4 = 9</span></strong><span class="koboSpan" id="kobo.290.1"> times. </span><span class="koboSpan" id="kobo.290.2">After combining them, the tokenized corpus becomes </span><strong class="source-inline"><span class="koboSpan" id="kobo.291.1">{(</span></strong><strong class="bold"><span class="koboSpan" id="kobo.292.1">dab</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.293.1">): 5, (a, </span></strong><strong class="bold"><span class="koboSpan" id="kobo.294.1">dab</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.295.1">): 4, (a, ab): 7, (bu, b): 9, (bu, c): 2}</span></strong><span class="koboSpan" id="kobo.296.1">. </span><span class="koboSpan" id="kobo.296.2">The new token vocabulary is </span><strong class="source-inline"><span class="koboSpan" id="kobo.297.1">{a: 11, bu: 11, b: 9, </span></strong><strong class="bold"><span class="koboSpan" id="kobo.298.1">dab</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.299.1">: 9, ab: 7, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.300.1">c: 2}</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.302.1">The new pair of tokens is </span><strong class="source-inline"><span class="koboSpan" id="kobo.303.1">(bu, b)</span></strong><span class="koboSpan" id="kobo.304.1"> with nine occurrences. </span><span class="koboSpan" id="kobo.304.2">After merging them, the corpus becomes </span><strong class="source-inline"><span class="koboSpan" id="kobo.305.1">{(dab): 5, (a, dab): 4, (a, ab): 7, (</span></strong><strong class="bold"><span class="koboSpan" id="kobo.306.1">bub</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.307.1">): 9, (bu, c): 2}</span></strong><span class="koboSpan" id="kobo.308.1">, </span><br/><span class="koboSpan" id="kobo.309.1">and the token vocabulary becomes </span><strong class="source-inline"><span class="koboSpan" id="kobo.310.1">{a: 11, </span></strong><strong class="bold"><span class="koboSpan" id="kobo.311.1">bub</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.312.1">: 9, </span></strong><strong class="bold"><span class="koboSpan" id="kobo.313.1">dab</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.314.1">: 9, ab: 7, bu: 2, </span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.315.1">c: 2}</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.317.1">BPE stores all token-merge rules and their order and not just the final token vocabulary. </span><span class="koboSpan" id="kobo.317.2">During model inference, it applies the rules in the same order on the new unknown text to </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">tokenize it.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.319.1">End-of-word tokens</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.320.1">The original BPE implementation</span><a id="_idIndexMarker821"/><span class="koboSpan" id="kobo.321.1"> appends a special end-of-word token, </span><strong class="source-inline"><span class="koboSpan" id="kobo.322.1">&lt;w/&gt;</span></strong><span class="koboSpan" id="kobo.323.1">, at the end of each word – for example, the word </span><strong class="source-inline"><span class="koboSpan" id="kobo.324.1">aab</span></strong><span class="koboSpan" id="kobo.325.1"> becomes </span><strong class="source-inline"><span class="koboSpan" id="kobo.326.1">aab&lt;w/&gt;</span></strong><span class="koboSpan" id="kobo.327.1">. </span><span class="koboSpan" id="kobo.327.2">Other implementations can place the special token at the beginning of the word, instead of the end. </span><span class="koboSpan" id="kobo.327.3">This makes it possible for the algorithm to distinguish between, say, the token </span><strong class="source-inline"><span class="koboSpan" id="kobo.328.1">ab</span></strong><span class="koboSpan" id="kobo.329.1">, as presented in the word </span><strong class="bold"><span class="koboSpan" id="kobo.330.1">ab</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.331.1">ca&lt;w/&gt;</span></strong><span class="koboSpan" id="kobo.332.1">, and the same token in </span><strong class="source-inline"><span class="koboSpan" id="kobo.333.1">a</span></strong><strong class="bold"><span class="koboSpan" id="kobo.334.1">ab</span></strong><strong class="source-inline"><span class="koboSpan" id="kobo.335.1">&lt;w/&gt;</span></strong><span class="koboSpan" id="kobo.336.1">. </span><span class="koboSpan" id="kobo.336.2">In this way, the algorithm can restore</span><a id="_idIndexMarker822"/><span class="koboSpan" id="kobo.337.1"> the original corpus from the tokenized one (</span><strong class="bold"><span class="koboSpan" id="kobo.338.1">de-tokenization</span></strong><span class="koboSpan" id="kobo.339.1">), which wouldn’t be possible otherwise. </span><span class="koboSpan" id="kobo.339.2">In this section, we have omitted the end-of-word token </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">for clarity.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.341.1">Let’s recall that our base vocabulary includes all characters of the text corpus. </span><span class="koboSpan" id="kobo.341.2">If these are Unicode characters (which is the usual case), we could end up with a vocabulary of up to 150,000 tokens. </span><span class="koboSpan" id="kobo.341.3">And this is before</span><a id="_idIndexMarker823"/><span class="koboSpan" id="kobo.342.1"> we even start the token-merge process. </span><span class="koboSpan" id="kobo.342.2">One trick to solve this issue is with the help of </span><strong class="bold"><span class="koboSpan" id="kobo.343.1">byte-level BPE</span></strong><span class="koboSpan" id="kobo.344.1">. </span><span class="koboSpan" id="kobo.344.2">Each Unicode character can be encoded with multiple (up to 4) bytes. </span><span class="koboSpan" id="kobo.344.3">Byte-level BPE initially splits the corpus into a sequence of bytes, instead of full-fledged Unicode characters. </span><span class="koboSpan" id="kobo.344.4">If a character is encoded with </span><em class="italic"><span class="koboSpan" id="kobo.345.1">n</span></em><span class="koboSpan" id="kobo.346.1"> bytes, the tokenizer will treat it as a sequence of </span><em class="italic"><span class="koboSpan" id="kobo.347.1">n</span></em><span class="koboSpan" id="kobo.348.1"> one-byte tokens. </span><span class="koboSpan" id="kobo.348.2">In this way, the size of the base vocabulary will always be 256 (the maximum unique values that we can store in a byte). </span><span class="koboSpan" id="kobo.348.3">In addition, byte-level BPE guarantees</span><a id="_idIndexMarker824"/><span class="koboSpan" id="kobo.349.1"> that we won’t encounter</span><a id="_idIndexMarker825"/> <span class="No-Break"><span class="koboSpan" id="kobo.350.1">unknown tokens.</span></span></p>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.351.1">WordPiece</span></strong><span class="koboSpan" id="kobo.352.1"> (</span><a href="https://arxiv.org/abs/1609.08144"><span class="koboSpan" id="kobo.353.1">https://arxiv.org/abs/1609.08144</span></a><span class="koboSpan" id="kobo.354.1">) is another subword tokenization algorithm. </span><span class="koboSpan" id="kobo.354.2">It is</span><a id="_idIndexMarker826"/><span class="koboSpan" id="kobo.355.1"> similar to BPE</span><a id="_idIndexMarker827"/><span class="koboSpan" id="kobo.356.1"> but with one main difference. </span><span class="koboSpan" id="kobo.356.2">Like BPE, it starts with a base vocabulary of individual characters and then proceeds to merge them into new composite tokens. </span><span class="koboSpan" id="kobo.356.3">However, it defines the merge order based on a score, computed with the following formula (unlike BPE, which uses </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">frequent co-occurrence):</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.358.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;s&lt;/mi&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;×&lt;/mo&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;q&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;k&lt;/mi&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mn&gt;2&lt;/mn&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/397.png" style="vertical-align:-0.871em;height:2.353em;width:20.288em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.359.1">In this way, the algorithm prioritizes the merging of pairs where the individual tokens are less frequent in the corpus. </span><span class="koboSpan" id="kobo.359.2">Let’s compare this approach with BPE, which merges tokens based only on the potential gains of the new token. </span><span class="koboSpan" id="kobo.359.3">In contrast, WordPiece balances the gain (the nominator in the formula) with the potential loss of the existing tokens (the denominator). </span><span class="koboSpan" id="kobo.359.4">This makes sense because the new token will exist instead of the old pair of tokens, rather than </span><span class="No-Break"><span class="koboSpan" id="kobo.360.1">alongside them.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.361.1">In-word tokens</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.362.1">WordPiece adds</span><a id="_idIndexMarker828"/><span class="koboSpan" id="kobo.363.1"> a special </span><em class="italic"><span class="koboSpan" id="kobo.364.1">##</span></em><span class="koboSpan" id="kobo.365.1"> prefix to all tokens inside a word, except for the first. </span><span class="koboSpan" id="kobo.365.2">For example, it will tokenize the word </span><em class="italic"><span class="koboSpan" id="kobo.366.1">aab</span></em><span class="koboSpan" id="kobo.367.1"> as </span><strong class="source-inline"><span class="koboSpan" id="kobo.368.1">[a, ##a, ##b]</span></strong><span class="koboSpan" id="kobo.369.1">. </span><span class="koboSpan" id="kobo.369.2">The token merge removes the </span><em class="italic"><span class="koboSpan" id="kobo.370.1">##</span></em><span class="koboSpan" id="kobo.371.1"> between the tokens. </span><span class="koboSpan" id="kobo.371.2">So, when we merge </span><em class="italic"><span class="koboSpan" id="kobo.372.1">##a</span></em><span class="koboSpan" id="kobo.373.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.374.1">##b</span></em><span class="koboSpan" id="kobo.375.1">, </span><em class="italic"><span class="koboSpan" id="kobo.376.1">aab</span></em><span class="koboSpan" id="kobo.377.1"> becomes </span><strong class="source-inline"><span class="koboSpan" id="kobo.378.1">[</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.379.1">a, ##ab]</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.381.1">Unlike BPE, WordPiece only stores the final token vocabulary. </span><span class="koboSpan" id="kobo.381.2">When it tokenizes a new word, it finds the longest matching subword in the vocabulary and splits the word on it. </span><span class="koboSpan" id="kobo.381.3">For example, let’s assume that we want to split the word </span><em class="italic"><span class="koboSpan" id="kobo.382.1">abcd</span></em><span class="koboSpan" id="kobo.383.1"> with a token vocabulary of </span><strong class="source-inline"><span class="koboSpan" id="kobo.384.1">[a, ##b, ##c, ##d, ab, ##cd, ##bcd]</span></strong><span class="koboSpan" id="kobo.385.1">. </span><span class="koboSpan" id="kobo.385.2">Following the new rule, WordPiece will first select the longest subword, </span><em class="italic"><span class="koboSpan" id="kobo.386.1">bcd</span></em><span class="koboSpan" id="kobo.387.1">, and it will tokenize </span><em class="italic"><span class="koboSpan" id="kobo.388.1">abcd</span></em><span class="koboSpan" id="kobo.389.1"> as </span><strong class="source-inline"><span class="koboSpan" id="kobo.390.1">[</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.391.1">a, ##bcd]</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.393.1">BPE and WordPiece</span><a id="_idIndexMarker829"/><span class="koboSpan" id="kobo.394.1"> are greedy algorithms – they will always merge tokens deterministically, based</span><a id="_idIndexMarker830"/><span class="koboSpan" id="kobo.395.1"> on frequency criteria. </span><span class="koboSpan" id="kobo.395.2">However, encoding the same text sequence with different tokens might be possible. </span><span class="koboSpan" id="kobo.395.3">This could act as regularization for a potential NLP algorithm. </span><span class="koboSpan" id="kobo.395.4">Next, we’ll introduce a tokenization technique that takes advantage </span><span class="No-Break"><span class="koboSpan" id="kobo.396.1">of this.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.397.1">Unigram</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.398.1">Unlike BPE</span><a id="_idIndexMarker831"/><span class="koboSpan" id="kobo.399.1"> and</span><a id="_idIndexMarker832"/><span class="koboSpan" id="kobo.400.1"> WordPiece, the </span><strong class="bold"><span class="koboSpan" id="kobo.401.1">Unigram</span></strong><span class="koboSpan" id="kobo.402.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.403.1">Subword Regularization: Improving Neural Network Translation Models with Multiple Subword Candidates</span></em><span class="koboSpan" id="kobo.404.1">, </span><a href="https://arxiv.org/abs/1804.10959"><span class="koboSpan" id="kobo.405.1">https://arxiv.org/abs/1804.10959</span></a><span class="koboSpan" id="kobo.406.1">) algorithm starts with a large base vocabulary and progressively tries to reduce it. </span><span class="koboSpan" id="kobo.406.2">The initial base vocabulary is a union of all unique characters and the most common substrings of the corpus. </span><span class="koboSpan" id="kobo.406.3">One way to find the most common substrings is with BPE. </span><span class="koboSpan" id="kobo.406.4">The algorithm assumes that each token, </span><span class="koboSpan" id="kobo.407.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/113.png" style="vertical-align:-0.340em;height:0.788em;width:0.668em"/></span><span class="koboSpan" id="kobo.408.1">, occurs independently (hence the Unigram name). </span><span class="koboSpan" id="kobo.408.2">Because of this assumption, the probability of a token, </span><span class="koboSpan" id="kobo.409.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/399.png" style="vertical-align:-0.340em;height:0.788em;width:0.619em"/></span><span class="koboSpan" id="kobo.410.1">, </span><span class="koboSpan" id="kobo.411.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/400.png" style="vertical-align:-0.390em;height:1.038em;width:1.855em"/></span><span class="koboSpan" id="kobo.412.1">, is just the number of its occurrences divided by the total size of the rest of the corpus. </span><span class="koboSpan" id="kobo.412.2">Then, the probability of a sequence of tokens with length </span><em class="italic"><span class="koboSpan" id="kobo.413.1">M</span></em><span class="koboSpan" id="kobo.414.1">, </span><span class="koboSpan" id="kobo.415.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mtext&gt;X&lt;/mml:mtext&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/401.png" style="vertical-align:-0.383em;height:1.032em;width:5.577em"/></span><span class="koboSpan" id="kobo.416.1">, is </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.418.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∏&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;M&lt;/mi&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;∀&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mrow&gt;&lt;munderover&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/mrow&gt;&lt;mrow /&gt;&lt;/munderover&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/402.png" style="vertical-align:-0.964em;height:2.383em;width:15.760em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.419.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.420.1">V</span></em><span class="koboSpan" id="kobo.421.1"> is the full </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">token vocabulary.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.423.1">Say that we have the same token sequence, </span><em class="italic"><span class="koboSpan" id="kobo.424.1">X</span></em><span class="koboSpan" id="kobo.425.1">, and multiple token segmentation candidates, </span><span class="koboSpan" id="kobo.426.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:mi&gt;S&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;X&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/403.png" style="vertical-align:-0.050em;height:0.748em;width:3.854em"/></span><span class="koboSpan" id="kobo.427.1">, </span><br/><span class="koboSpan" id="kobo.428.1">for that sequence. </span><span class="koboSpan" id="kobo.428.2">The most probable segmentation candidate, </span><em class="italic"><span class="koboSpan" id="kobo.429.1">x*</span></em><span class="koboSpan" id="kobo.430.1">, for </span><em class="italic"><span class="koboSpan" id="kobo.431.1">X</span></em><span class="koboSpan" id="kobo.432.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.433.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.434.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;/msup&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;r&lt;/mi&gt;&lt;mi&gt;g&lt;/mi&gt;&lt;mi&gt;m&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;/mfenced&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mo&gt;∈&lt;/mo&gt;&lt;mi&gt;S&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mi&gt;X&lt;/mi&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/404.png" style="vertical-align:-0.257em;height:0.977em;width:11.659em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.435.1">Let’s clarify this with an example. </span><span class="koboSpan" id="kobo.435.2">We’ll assume that our corpus consists of some (imaginary) words, </span><strong class="source-inline"><span class="koboSpan" id="kobo.436.1">{dab: 5, aab: 7, bun: 4}</span></strong><span class="koboSpan" id="kobo.437.1">, where the digits indicate the number of occurrences of that word in the text. </span><span class="koboSpan" id="kobo.437.2">Our initial token vocabulary is a union of all unique characters and all possible substrings (the numbers indicate frequency): </span><strong class="source-inline"><span class="koboSpan" id="kobo.438.1">{a: 19, b: 16, ab: 12, aa: 7, da: 5, d: 5, bu: 4, un: 4}</span></strong><span class="koboSpan" id="kobo.439.1">. </span><span class="koboSpan" id="kobo.439.2">The sum of all token frequencies is 19 + 16 + 12 + 7 + 5 + 5 + 4 + 4 = 72. </span><span class="koboSpan" id="kobo.439.3">Then, the independent probability for each token is </span><span class="koboSpan" id="kobo.440.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mn&gt;72&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/405.png" style="vertical-align:-0.390em;height:1.054em;width:7.940em"/></span><span class="koboSpan" id="kobo.441.1"> – for example, </span><span class="koboSpan" id="kobo.442.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;19&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mn&gt;72&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.264&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/406.png" style="vertical-align:-0.157em;height:0.821em;width:8.038em"/></span><span class="koboSpan" id="kobo.443.1">, </span><br/><span class="koboSpan" id="kobo.444.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;12&lt;/mn&gt;&lt;mo&gt;/&lt;/mo&gt;&lt;mn&gt;72&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.167&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/407.png" style="vertical-align:-0.204em;height:0.915em;width:8.750em"/></span><span class="koboSpan" id="kobo.445.1">, and </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">so on.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.447.1">Our extended vocabulary</span><a id="_idIndexMarker833"/><span class="koboSpan" id="kobo.448.1"> presents us with the possibility</span><a id="_idIndexMarker834"/><span class="koboSpan" id="kobo.449.1"> to tokenize each sequence (we’ll focus on words for simplicity) in multiple ways. </span><span class="koboSpan" id="kobo.449.2">For example, we can represent </span><em class="italic"><span class="koboSpan" id="kobo.450.1">dab</span></em><span class="koboSpan" id="kobo.451.1"> as either </span><strong class="source-inline"><span class="koboSpan" id="kobo.452.1">{d, a, b}</span></strong><span class="koboSpan" id="kobo.453.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.454.1">{da, b}</span></strong><span class="koboSpan" id="kobo.455.1">, or </span><strong class="source-inline"><span class="koboSpan" id="kobo.456.1">{d, ab}</span></strong><span class="koboSpan" id="kobo.457.1">. </span><span class="koboSpan" id="kobo.457.2">Here, the probabilities for each candidate are </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.458.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.459.1">(</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.460.1">{</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.461.1">d</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.462.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.463.1">a</span></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.464.1">,</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.465.1">b</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.466.1">}</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.467.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.468.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.469.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.470.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.471.1">d</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.472.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.473.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.474.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.475.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.476.1">a</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.477.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.478.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.479.1">P</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.480.1">(</span></span><span class="_-----MathTools-_Math_Variable"><span class="koboSpan" id="kobo.481.1">b</span></span><span class="_-----MathTools-_Math_Base"><span class="koboSpan" id="kobo.482.1">)</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.483.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.484.1">0.07</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.485.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.486.1">0.264</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Symbol"><span class="koboSpan" id="kobo.487.1">*</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.488.1">0.222</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.489.1">=</span></span><span class="_-----MathTools-_Math_Space"> </span><span class="_-----MathTools-_Math_Number"><span class="koboSpan" id="kobo.490.1">0.0041</span></span><span class="koboSpan" id="kobo.491.1">; </span><span class="koboSpan" id="kobo.492.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;{&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo&gt;}&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.07&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mn&gt;0.222&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.015&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/408.png" style="vertical-align:-0.204em;height:0.915em;width:12.560em"/></span><span class="koboSpan" id="kobo.493.1">; </span><span class="koboSpan" id="kobo.494.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mo&gt;{&lt;/mo&gt;&lt;mi&gt;d&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mi&gt;a&lt;/mi&gt;&lt;mi&gt;b&lt;/mi&gt;&lt;mo&gt;}&lt;/mo&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.07&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;*&lt;/mi&gt;&lt;mn&gt;0.167&lt;/mn&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;0.012&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/409.png" style="vertical-align:-0.204em;height:0.915em;width:12.671em"/></span><span class="koboSpan" id="kobo.495.1">. </span><br/><span class="koboSpan" id="kobo.496.1">The candidate with the highest probability is </span><em class="italic"><span class="koboSpan" id="kobo.497.1">x</span></em><span class="koboSpan" id="kobo.498.1">* = </span><strong class="source-inline"><span class="koboSpan" id="kobo.499.1">{da, b}</span></strong><span class="koboSpan" id="kobo.500.1">. </span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.501.1">With that, here’s how Unigram</span><a id="_idIndexMarker835"/><span class="koboSpan" id="kobo.502.1"> tokenization works step </span><span class="No-Break"><span class="koboSpan" id="kobo.503.1">by step:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.504.1">Start with the initial large base </span><span class="No-Break"><span class="koboSpan" id="kobo.505.1">vocabulary, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.506.1">V</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.508.1">Repeat the following steps until the size of |</span><em class="italic"><span class="koboSpan" id="kobo.509.1">V</span></em><span class="koboSpan" id="kobo.510.1">| reaches some minimum </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">threshold value:</span></span><ol><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.512.1">Find the </span><em class="italic"><span class="koboSpan" id="kobo.513.1">l</span></em><span class="koboSpan" id="kobo.514.1">-best tokenization</span><a id="_idIndexMarker836"/><span class="koboSpan" id="kobo.515.1"> candidates, </span><em class="italic"><span class="koboSpan" id="kobo.516.1">x</span></em><span class="koboSpan" id="kobo.517.1">*, for all words in the corpus with the help of the </span><strong class="bold"><span class="koboSpan" id="kobo.518.1">Viterbi</span></strong><span class="koboSpan" id="kobo.519.1"> algorithm (</span><a href="https://en.wikipedia.org/wiki/Viterbi_algorithm"><span class="koboSpan" id="kobo.520.1">https://en.wikipedia.org/wiki/Viterbi_algorithm</span></a><span class="koboSpan" id="kobo.521.1"> – using this algorithm is necessary because</span><a id="_idIndexMarker837"/><span class="koboSpan" id="kobo.522.1"> this is a computationally intensive task). </span><span class="koboSpan" id="kobo.522.2">Taking </span><em class="italic"><span class="koboSpan" id="kobo.523.1">l</span></em><span class="koboSpan" id="kobo.524.1"> candidates, instead of one, makes it possible to sample different token sequences over the same text. </span><span class="koboSpan" id="kobo.524.2">You can think of this as a data augmentation technique over the input data, which provides additional regularization to the NLP algorithm. </span><span class="koboSpan" id="kobo.524.3">Once we have a tokenized corpus in this way, we can estimate the probabilities, </span><span class="koboSpan" id="kobo.525.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;∈&lt;/mml:mo&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/410.png" style="vertical-align:-0.390em;height:1.038em;width:5.499em"/></span><span class="koboSpan" id="kobo.526.1">, for all tokens</span><a id="_idIndexMarker838"/><span class="koboSpan" id="kobo.527.1"> of the current token</span><a id="_idIndexMarker839"/><span class="koboSpan" id="kobo.528.1"> vocabulary, </span><em class="italic"><span class="koboSpan" id="kobo.529.1">V</span></em><span class="koboSpan" id="kobo.530.1">, with the help of an </span><strong class="bold"><span class="koboSpan" id="kobo.531.1">expectation-minimization</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.532.1">algorithm (</span></span><a href="https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm"><span class="No-Break"><span class="koboSpan" id="kobo.533.1">https://en.wikipedia.org/wiki/Expectation%E2%80%93maximization_algorithm</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.534.1">).</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.535.1">For each token, </span><span class="koboSpan" id="kobo.536.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/113.png" style="vertical-align:-0.340em;height:0.788em;width:0.678em"/></span><span class="koboSpan" id="kobo.537.1">, compute a special loss function, </span><span class="koboSpan" id="kobo.538.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/412.png" style="vertical-align:-0.340em;height:1.051em;width:1.757em"/></span><span class="koboSpan" id="kobo.539.1">, which determines how the likelihood of the corpus is reduced if we remove </span><span class="koboSpan" id="kobo.540.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/115.png" style="vertical-align:-0.340em;height:0.788em;width:0.624em"/></span><span class="koboSpan" id="kobo.541.1"> from the </span><span class="No-Break"><span class="koboSpan" id="kobo.542.1">token vocabulary.</span></span></li><li class="upper-roman" lang="en-GB"><span class="koboSpan" id="kobo.543.1">Sort the tokens by their </span><span class="koboSpan" id="kobo.544.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/414.png" style="vertical-align:-0.340em;height:1.051em;width:1.639em"/></span><span class="koboSpan" id="kobo.545.1"> and preserve only the top </span><em class="italic"><span class="koboSpan" id="kobo.546.1">n</span></em><span class="koboSpan" id="kobo.547.1"> % of the tokens (for example, </span><em class="italic"><span class="koboSpan" id="kobo.548.1">n = 80</span></em><span class="koboSpan" id="kobo.549.1">). </span><span class="koboSpan" id="kobo.549.2">Always preserve</span><a id="_idIndexMarker840"/><span class="koboSpan" id="kobo.550.1"> the individual characters to avoid </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">unknown tokens.</span></span></li></ol></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.552.1">This concludes our introduction</span><a id="_idIndexMarker841"/><span class="koboSpan" id="kobo.553.1"> to tokenization. </span><span class="koboSpan" id="kobo.553.2">Some of these techniques were developed alongside the transformer architecture and we’ll make the most use of them in the following chapters. </span><span class="koboSpan" id="kobo.553.3">But for now, let’s focus on another fundamental technique in the </span><span class="No-Break"><span class="koboSpan" id="kobo.554.1">NLP pipeline.</span></span></p>
<h2 id="_idParaDest-116" lang="en-GB"><a id="_idTextAnchor190"/><span class="koboSpan" id="kobo.555.1">Introducing word embeddings</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.556.1">Now that we’ve learned how to tokenize</span><a id="_idIndexMarker842"/><span class="koboSpan" id="kobo.557.1"> the text corpus, we can proceed to the next step in the NLP data processing pipeline. </span><span class="koboSpan" id="kobo.557.2">For the sake of clarity, we’ll assume that we’ve tokenized the corpus into words, rather than subwords or characters (in this section, </span><em class="italic"><span class="koboSpan" id="kobo.558.1">word</span></em><span class="koboSpan" id="kobo.559.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.560.1">token</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.561.1">are interchangeable).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.562.1">One way to feed the words of the sequence as input to the NLP algorithm is with one-hot encoding. </span><span class="koboSpan" id="kobo.562.2">Our input vector</span><a id="_idIndexMarker843"/><span class="koboSpan" id="kobo.563.1"> will have the same size as the number of tokens in the vocabulary and each token will have a unique one-hot encoded representation. </span><span class="koboSpan" id="kobo.563.2">However, this approach has a few drawbacks, </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">as follows:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.565.1">Sparse inputs</span></strong><span class="koboSpan" id="kobo.566.1">: The one-hot representation</span><a id="_idIndexMarker844"/><span class="koboSpan" id="kobo.567.1"> consists of mostly zeros and a single value. </span><span class="koboSpan" id="kobo.567.2">If our NLP algorithm is an NN (and it is), this type of input will activate only a small portion of its weights per word. </span><span class="koboSpan" id="kobo.567.3">Because of this, we’ll need a large training set to include a sufficient number of training samples of each word of </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">the vocabulary.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.569.1">Computational intensity</span></strong><span class="koboSpan" id="kobo.570.1">: The large size of the vocabulary will result in large input tensors, which require large NNs and more </span><span class="No-Break"><span class="koboSpan" id="kobo.571.1">computational resources.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.572.1">Impracticality</span></strong><span class="koboSpan" id="kobo.573.1">: Every time we add a new word to the vocabulary, we’ll increase its size. </span><span class="koboSpan" id="kobo.573.2">However, the size of the one-hot encoded input will also increase. </span><span class="koboSpan" id="kobo.573.3">Therefore, we’ll have to change the structure of our NN to accommodate the new size and we’ll perform </span><span class="No-Break"><span class="koboSpan" id="kobo.574.1">additional training.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.575.1">Lack of context</span></strong><span class="koboSpan" id="kobo.576.1">: Words such as </span><em class="italic"><span class="koboSpan" id="kobo.577.1">dog</span></em><span class="koboSpan" id="kobo.578.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.579.1">wolf</span></em><span class="koboSpan" id="kobo.580.1"> are semantically similar, but the one-hot representation lacks a way to convey </span><span class="No-Break"><span class="koboSpan" id="kobo.581.1">this similarity.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.582.1">In this section, we’ll try to solve</span><a id="_idIndexMarker845"/><span class="koboSpan" id="kobo.583.1"> these issues with the help of a lower-dimensional distributed representation of the words, known as </span><strong class="bold"><span class="koboSpan" id="kobo.584.1">word embeddings</span></strong><span class="koboSpan" id="kobo.585.1"> (</span><em class="italic"><span class="koboSpan" id="kobo.586.1">A Neural Probabilistic Language Model</span></em><span class="koboSpan" id="kobo.587.1">, </span><a href="http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf"><span class="koboSpan" id="kobo.588.1">http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf</span></a><span class="koboSpan" id="kobo.589.1">). </span><span class="koboSpan" id="kobo.589.2">The distributed representation is created by learning an embedding function that transforms the one-hot encoded words into a lower-dimensional space of word embeddings, </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer536">
<span class="koboSpan" id="kobo.591.1"><img alt="Figure 6.1 – Words -&gt; one-hot encoding -&gt; word embedding vectors" src="image/B19627_06_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.592.1">Figure 6.1 – Words -&gt; one-hot encoding -&gt; word embedding vectors</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.593.1">Words from the vocabulary</span><a id="_idIndexMarker846"/><span class="koboSpan" id="kobo.594.1"> with size </span><em class="italic"><span class="koboSpan" id="kobo.595.1">V</span></em><span class="koboSpan" id="kobo.596.1"> are transformed into one-hot encoding vectors of size </span><em class="italic"><span class="koboSpan" id="kobo.597.1">V</span></em><span class="koboSpan" id="kobo.598.1">. </span><span class="koboSpan" id="kobo.598.2">Then, an </span><strong class="bold"><span class="koboSpan" id="kobo.599.1">embedding function</span></strong><span class="koboSpan" id="kobo.600.1"> transforms this </span><em class="italic"><span class="koboSpan" id="kobo.601.1">V</span></em><span class="koboSpan" id="kobo.602.1">-dimensional space into a distributed representation (vector) of a </span><strong class="bold"><span class="koboSpan" id="kobo.603.1">fixed</span></strong><span class="koboSpan" id="kobo.604.1"> size, </span><em class="italic"><span class="koboSpan" id="kobo.605.1">D</span></em><span class="koboSpan" id="kobo.606.1"> (here, </span><em class="italic"><span class="koboSpan" id="kobo.607.1">D=4</span></em><span class="koboSpan" id="kobo.608.1">). </span><span class="koboSpan" id="kobo.608.2">This vector serves as input to the NLP algorithm. </span><span class="koboSpan" id="kobo.608.3">We can see that the fixed and smaller vector size solves the issues of sparsity, computational intensity, and impracticality we just described. </span><span class="koboSpan" id="kobo.608.4">Next, we’ll see how it solves the </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">context issue.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.610.1">The embedding function learns semantic information about the words. </span><span class="koboSpan" id="kobo.610.2">It maps each word in the vocabulary to a continuous-valued vector representation – that is, the word embedding. </span><span class="koboSpan" id="kobo.610.3">Each word corresponds to a point in this embedding space, and different dimensions correspond to the grammatical or semantic properties of these words. </span><span class="koboSpan" id="kobo.610.4">The concept of embedding space is similar to the latent space representation, which we first discussed in the context of diffusion models in </span><a href="B19627_05.xhtml#_idTextAnchor146"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.611.1">Chapter 5</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.612.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.613.1">The goal is to ensure that the words close to each other in the embedding space have similar meanings. </span><span class="koboSpan" id="kobo.613.2">By </span><em class="italic"><span class="koboSpan" id="kobo.614.1">close to each other</span></em><span class="koboSpan" id="kobo.615.1">, we mean a high value of the dot product (similarity) of their embedding vectors. </span><span class="koboSpan" id="kobo.615.2">In this way, the information that some words are semantically similar can be exploited by the ML algorithm. </span><span class="koboSpan" id="kobo.615.3">For example, it might learn that </span><em class="italic"><span class="koboSpan" id="kobo.616.1">fox</span></em><span class="koboSpan" id="kobo.617.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.618.1">cat</span></em><span class="koboSpan" id="kobo.619.1"> are semantically related and that both </span><em class="italic"><span class="koboSpan" id="kobo.620.1">the quick brown fox</span></em><span class="koboSpan" id="kobo.621.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.622.1">the quick brown cat</span></em><span class="koboSpan" id="kobo.623.1"> are valid phrases. </span><span class="koboSpan" id="kobo.623.2">A sequence of words can then be replaced with a sequence of embedding vectors that capture the characteristics of these words. </span><span class="koboSpan" id="kobo.623.3">We can use this sequence as a base</span><a id="_idIndexMarker847"/><span class="koboSpan" id="kobo.624.1"> for various NLP tasks. </span><span class="koboSpan" id="kobo.624.2">For example, a classifier trying to classify the sentiment of an article might be trained on previously learned word embeddings, instead of one-hot encoding vectors. </span><span class="koboSpan" id="kobo.624.3">In this way, the semantic information of the words becomes readily available for the </span><span class="No-Break"><span class="koboSpan" id="kobo.625.1">sentiment classifier.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.626.1">The mapping between one-hot representation and embedding vectors</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.627.1">Let’s assume that we have already computed</span><a id="_idIndexMarker848"/><span class="koboSpan" id="kobo.628.1"> the embedding vectors of each token. </span><span class="koboSpan" id="kobo.628.2">One way to implement the mapping between the one-hot representation and the actual embedding vector is with the help of a </span><em class="italic"><span class="koboSpan" id="kobo.629.1">V×D</span></em><span class="koboSpan" id="kobo.630.1">-shaped matrix, </span><span class="koboSpan" id="kobo.631.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/415.png" style="vertical-align:-0.339em;height:0.988em;width:2.174em"/></span><span class="koboSpan" id="kobo.632.1">. </span><span class="koboSpan" id="kobo.632.2">We can think of the matrix rows as a lookup table, where each row represents one word embedding vector. </span><span class="koboSpan" id="kobo.632.3">It works thanks to the one-hot encoded input word, which is a vector of all zeros, except for the index of the word itself. </span><span class="koboSpan" id="kobo.632.4">Because of this, the input word, </span><span class="koboSpan" id="kobo.633.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/416.png" style="vertical-align:-0.340em;height:0.788em;width:0.821em"/></span><span class="koboSpan" id="kobo.634.1">, will only activate its unique row (vector) of weights, </span><span class="koboSpan" id="kobo.635.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/417.png" style="vertical-align:-0.340em;height:0.781em;width:0.892em"/></span><span class="koboSpan" id="kobo.636.1">, in </span><span class="koboSpan" id="kobo.637.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/418.png" style="vertical-align:-0.339em;height:0.988em;width:2.150em"/></span><span class="koboSpan" id="kobo.638.1">. </span><span class="koboSpan" id="kobo.638.2">So, for each input sample (word), only the word’s embedding vector will participate. </span><span class="koboSpan" id="kobo.638.3">We can also think of </span><span class="koboSpan" id="kobo.639.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/415.png" style="vertical-align:-0.339em;height:0.988em;width:2.173em"/></span><span class="koboSpan" id="kobo.640.1"> as a weight matrix of a </span><strong class="bold"><span class="koboSpan" id="kobo.641.1">fully connected</span></strong><span class="koboSpan" id="kobo.642.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.643.1">FC</span></strong><span class="koboSpan" id="kobo.644.1">) NN layer. </span><span class="koboSpan" id="kobo.644.2">In this way, we can embed</span><a id="_idIndexMarker849"/><span class="koboSpan" id="kobo.645.1"> the embeddings (get it?) as the first NN layer – that is, the NN takes the one-hot encoded token as input and the embedding layer transforms it into a vector. </span><span class="koboSpan" id="kobo.645.2">Then, the rest of the NN uses the embedding vector instead of the one-hot representation. </span><span class="koboSpan" id="kobo.645.3">This is a standard implementation across</span><a id="_idIndexMarker850"/><span class="koboSpan" id="kobo.646.1"> all deep </span><span class="No-Break"><span class="koboSpan" id="kobo.647.1">learning libraries.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.648.1">The concept of word embeddings</span><a id="_idIndexMarker851"/><span class="koboSpan" id="kobo.649.1"> was first introduced more than 20 years ago but remains one of the central paradigms in NLP today. </span><strong class="bold"><span class="koboSpan" id="kobo.650.1">Large language models</span></strong><span class="koboSpan" id="kobo.651.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.652.1">LLMs</span></strong><span class="koboSpan" id="kobo.653.1">), such as ChatGPT, use improved</span><a id="_idIndexMarker852"/><span class="koboSpan" id="kobo.654.1"> versions of word embeddings, which we’ll discuss in </span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.655.1">Chapter 7</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.656.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.657.1">Now that we are familiar with embedding vectors, we’ll continue with the algorithm to obtain and </span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">compute them.</span></span></p>
<h2 id="_idParaDest-117" lang="en-GB"><a id="_idTextAnchor191"/><span class="koboSpan" id="kobo.659.1">Word2Vec</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.660.1">A lot of research</span><a id="_idIndexMarker853"/><span class="koboSpan" id="kobo.661.1"> has gone into creating better word embedding models, in particular by omitting to learn the probability function over sequences of words. </span><span class="koboSpan" id="kobo.661.2">One of the most popular ways to do this is with </span><strong class="bold"><span class="koboSpan" id="kobo.662.1">Word2Vec</span></strong><span class="koboSpan" id="kobo.663.1"> (</span><a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf"><span class="koboSpan" id="kobo.664.1">http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf</span></a><span class="koboSpan" id="kobo.665.1">, https://arxiv.org/abs/1301.3781, and https://arxiv.org/abs/1310.4546). </span><span class="koboSpan" id="kobo.665.2">It creates embedding vectors based on the context (surrounding words) of the word in focus. </span><span class="koboSpan" id="kobo.665.3">More specifically, the context is the </span><em class="italic"><span class="koboSpan" id="kobo.666.1">n</span></em><span class="koboSpan" id="kobo.667.1"> preceding and the </span><em class="italic"><span class="koboSpan" id="kobo.668.1">n</span></em><span class="koboSpan" id="kobo.669.1"> following words of the focus word. </span><span class="koboSpan" id="kobo.669.2">The following figure shows the context window as it slides across the text, surrounding different </span><span class="No-Break"><span class="koboSpan" id="kobo.670.1">focus words:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer542">
<span class="koboSpan" id="kobo.671.1"><img alt="Figure 6.2 – A Word2Vec sliding context window with n=2. The same type of context window applies to both CBOW and ﻿skip-gram" src="image/B19627_06_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.672.1">Figure 6.2 – A Word2Vec sliding context window with n=2. </span><span class="koboSpan" id="kobo.672.2">The same type of context window applies to both CBOW and skip-gram</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.673.1">Word2vec</span><a id="_idIndexMarker854"/><span class="koboSpan" id="kobo.674.1"> comes in two flavors: </span><strong class="bold"><span class="koboSpan" id="kobo.675.1">Continuous Bag of Words</span></strong><span class="koboSpan" id="kobo.676.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.677.1">CBOW</span></strong><span class="koboSpan" id="kobo.678.1">) and </span><strong class="bold"><span class="koboSpan" id="kobo.679.1">skip-gram</span></strong><span class="koboSpan" id="kobo.680.1">. </span><span class="koboSpan" id="kobo.680.2">We’ll start with CBOW and then we’ll continue </span><span class="No-Break"><span class="koboSpan" id="kobo.681.1">with skip-gram.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.682.1">CBOW</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.683.1">CBOW predicts</span><a id="_idIndexMarker855"/><span class="koboSpan" id="kobo.684.1"> the most likely word given</span><a id="_idIndexMarker856"/><span class="koboSpan" id="kobo.685.1"> its context (surrounding words). </span><span class="koboSpan" id="kobo.685.2">For example, given the sequence </span><em class="italic"><span class="koboSpan" id="kobo.686.1">the quick _____ fox jumps</span></em><span class="koboSpan" id="kobo.687.1">, the model will predict </span><em class="italic"><span class="koboSpan" id="kobo.688.1">brown</span></em><span class="koboSpan" id="kobo.689.1">. </span><span class="koboSpan" id="kobo.689.2">It takes all words within the context window with equal weights and doesn’t consider their order (hence the </span><em class="italic"><span class="koboSpan" id="kobo.690.1">bag</span></em><span class="koboSpan" id="kobo.691.1"> in the name). </span><span class="koboSpan" id="kobo.691.2">We can train the model with the help of the following simple NN with input, hidden, and </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">output layers:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer543">
<span class="koboSpan" id="kobo.693.1"><img alt="Figure 6.3 – A CBOW model NN" src="image/B19627_06_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.694.1">Figure 6.3 – A CBOW model NN</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.695.1">Here’s how the </span><span class="No-Break"><span class="koboSpan" id="kobo.696.1">model</span></span><span class="No-Break"><a id="_idIndexMarker857"/></span><span class="No-Break"><span class="koboSpan" id="kobo.697.1"> works:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.698.1">The input is the one-hot-encoded word representation (its length is equal to the vocabulary </span><span class="No-Break"><span class="koboSpan" id="kobo.699.1">size, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.700.1">V</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.701.1">).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.702.1">The embedding vectors are represented by the </span><em class="italic"><span class="koboSpan" id="kobo.703.1">input-to-hidden</span></em> <span class="No-Break"><span class="koboSpan" id="kobo.704.1">matrix, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.705.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/420.png" style="vertical-align:-0.339em;height:0.988em;width:2.173em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.706.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.707.1">The embedding vectors of all context words are averaged to produce the output of the hidden network layer (there is no </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">activation function).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.709.1">The hidden activations</span><a id="_idIndexMarker858"/><span class="koboSpan" id="kobo.710.1"> serve as input to the output </span><strong class="bold"><span class="koboSpan" id="kobo.711.1">Softmax</span></strong><span class="koboSpan" id="kobo.712.1"> layer of size </span><em class="italic"><span class="koboSpan" id="kobo.713.1">V</span></em><span class="koboSpan" id="kobo.714.1"> (with the hidden-to-output weight matrix, </span><span class="koboSpan" id="kobo.715.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/421.png" style="vertical-align:-0.339em;height:1.017em;width:2.134em"/></span><span class="koboSpan" id="kobo.716.1">), which predicts the most likely word to be found in the context (proximity) of the input words. </span><span class="koboSpan" id="kobo.716.2">The index with the highest activation represents</span><a id="_idIndexMarker859"/><span class="koboSpan" id="kobo.717.1"> the one-hot-encoded </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">related word.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.719.1">We’ll train the NN with gradient descent and backpropagation. </span><span class="koboSpan" id="kobo.719.2">The training set consists of (context and label) one-hot encoded pairs of words that appear close to each other in the text. </span><span class="koboSpan" id="kobo.719.3">For example, if part of the text is </span><strong class="source-inline"><span class="koboSpan" id="kobo.720.1">[the, quick, brown, fox, jumps]</span></strong><span class="koboSpan" id="kobo.721.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.722.1">n=2</span></em><span class="koboSpan" id="kobo.723.1">, the training tuples will include </span><strong class="source-inline"><span class="koboSpan" id="kobo.724.1">([quick, brown], the), ([the, brown, fox], quick)</span></strong><span class="koboSpan" id="kobo.725.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.726.1">([the, quick, fox jumps], brown)</span></strong><span class="koboSpan" id="kobo.727.1">, and so on. </span><span class="koboSpan" id="kobo.727.2">Since we are only interested in the embeddings, </span><span class="koboSpan" id="kobo.728.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/422.png" style="vertical-align:-0.339em;height:0.988em;width:2.189em"/></span><span class="koboSpan" id="kobo.729.1">, we’ll discard the output NN weights, </span><span class="koboSpan" id="kobo.730.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/423.png" style="vertical-align:-0.011em;height:0.688em;width:1.242em"/></span><span class="koboSpan" id="kobo.731.1">, when the training </span><span class="No-Break"><span class="koboSpan" id="kobo.732.1">is finished.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.733.1">CBOW will tell us which word is most likely to appear in a given context. </span><span class="koboSpan" id="kobo.733.2">This could be a problem for rare words. </span><span class="koboSpan" id="kobo.733.3">For example, given the context </span><em class="italic"><span class="koboSpan" id="kobo.734.1">The weather today is really _____</span></em><span class="koboSpan" id="kobo.735.1">, the model will predict the word </span><em class="italic"><span class="koboSpan" id="kobo.736.1">beautiful</span></em><span class="koboSpan" id="kobo.737.1"> rather than </span><em class="italic"><span class="koboSpan" id="kobo.738.1">fabulous</span></em><span class="koboSpan" id="kobo.739.1"> (hey, it’s just an example). </span><span class="koboSpan" id="kobo.739.2">CBOW is several times faster to train than the skip-gram and achieves slightly better accuracy for </span><span class="No-Break"><span class="koboSpan" id="kobo.740.1">frequent</span></span><span class="No-Break"><a id="_idIndexMarker860"/></span><span class="No-Break"><span class="koboSpan" id="kobo.741.1"> words.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.742.1">Skip-gram</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.743.1">The skip-gram model</span><a id="_idIndexMarker861"/><span class="koboSpan" id="kobo.744.1"> can predict the context</span><a id="_idIndexMarker862"/><span class="koboSpan" id="kobo.745.1"> of a given input word (the opposite of CBOW). </span><span class="koboSpan" id="kobo.745.2">For example, the word </span><em class="italic"><span class="koboSpan" id="kobo.746.1">brown</span></em><span class="koboSpan" id="kobo.747.1"> will predict the words </span><em class="italic"><span class="koboSpan" id="kobo.748.1">The quick fox jumps</span></em><span class="koboSpan" id="kobo.749.1">. </span><span class="koboSpan" id="kobo.749.2">Unlike CBOW, the input is a single one-hot encoded word vector. </span><span class="koboSpan" id="kobo.749.3">But how do we represent the context words in the output? </span><span class="koboSpan" id="kobo.749.4">Instead of trying to predict the whole context (all surrounding words) simultaneously, skip-gram transforms the context into multiple training pairs, such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.750.1">(fox, the)</span></strong><span class="koboSpan" id="kobo.751.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.752.1">(fox, quick)</span></strong><span class="koboSpan" id="kobo.753.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.754.1">(fox, brown)</span></strong><span class="koboSpan" id="kobo.755.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.756.1">(fox, jumps)</span></strong><span class="koboSpan" id="kobo.757.1">. </span><span class="koboSpan" id="kobo.757.2">Once again, we can train the model with a simple </span><span class="No-Break"><span class="koboSpan" id="kobo.758.1">single-layer NN:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer548">
<span class="koboSpan" id="kobo.759.1"><img alt="Figure 6.4 – A skip-gram model NN" src="image/B19627_06_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.760.1">Figure 6.4 – A skip-gram model NN</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.761.1">As with CBOW, the output</span><a id="_idIndexMarker863"/><span class="koboSpan" id="kobo.762.1"> is a softmax, which represents the one-hot-encoded most probable</span><a id="_idIndexMarker864"/><span class="koboSpan" id="kobo.763.1"> context word. </span><span class="koboSpan" id="kobo.763.2">The input-to-hidden weights, </span><span class="koboSpan" id="kobo.764.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/424.png" style="vertical-align:-0.339em;height:0.988em;width:2.260em"/></span><span class="koboSpan" id="kobo.765.1">, represent the word embeddings lookup table, and the hidden-to-output weights, </span><span class="koboSpan" id="kobo.766.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/425.png" style="vertical-align:-0.011em;height:0.688em;width:1.270em"/></span><span class="koboSpan" id="kobo.767.1">, are only relevant during training. </span><span class="koboSpan" id="kobo.767.2">The hidden layer doesn’t have an activation function (that is, it uses </span><span class="No-Break"><span class="koboSpan" id="kobo.768.1">linear activation).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.769.1">We’ll train the model with backpropagation (no surprises here). </span><span class="koboSpan" id="kobo.769.2">Given a sequence of words, </span><span class="koboSpan" id="kobo.770.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/426.png" style="vertical-align:-0.333em;height:0.781em;width:1.966em"/></span><span class="_-----MathTools-_Math_Operator"><span class="koboSpan" id="kobo.771.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/427.png" style="vertical-align:-0.333em;height:0.781em;width:1.190em"/></span></span><span class="koboSpan" id="kobo.772.1">, the objective of the skip-gram model is to maximize the average log probability, where </span><em class="italic"><span class="koboSpan" id="kobo.773.1">n</span></em><span class="koboSpan" id="kobo.774.1"> is the </span><span class="No-Break"><span class="koboSpan" id="kobo.775.1">window size:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.776.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;M&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;≠&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;g&lt;/mml:mi&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/428.png" style="vertical-align:-0.764em;height:2.259em;width:9.903em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.777.1">The model defines the probability, </span><span class="koboSpan" id="kobo.778.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/429.png" style="vertical-align:-0.390em;height:1.189em;width:4.560em"/></span><span class="koboSpan" id="kobo.779.1">, as the following </span><span class="No-Break"><span class="koboSpan" id="kobo.780.1">softmax formula:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.781.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;O&lt;/mi&gt;&lt;/msub&gt;&lt;mrow&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mo&gt;∑&lt;/mo&gt;&lt;mrow&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;mi&gt;V&lt;/mi&gt;&lt;/msubsup&gt;&lt;mrow&gt;&lt;mi&gt;e&lt;/mi&gt;&lt;mi&gt;x&lt;/mi&gt;&lt;mi&gt;p&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msubsup&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;′&lt;/mo&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;⊤&lt;/mi&gt;&lt;/mrow&gt;&lt;/msubsup&gt;&lt;msub&gt;&lt;mi mathvariant=&quot;bold&quot;&gt;v&lt;/mi&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;I&lt;/mi&gt;&lt;/msub&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/430.png" style="vertical-align:-1.049em;height:2.631em;width:10.961em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.782.1">In this example, </span><span class="koboSpan" id="kobo.783.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/431.png" style="vertical-align:-0.333em;height:0.781em;width:0.858em"/></span><span class="koboSpan" id="kobo.784.1"> and </span><span class="koboSpan" id="kobo.785.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/432.png" style="vertical-align:-0.342em;height:0.790em;width:1.077em"/></span><span class="koboSpan" id="kobo.786.1"> are the input and output words, and </span><strong class="bold"><span class="koboSpan" id="kobo.787.1">v</span></strong><span class="koboSpan" id="kobo.788.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.789.1">v</span></strong><span class="koboSpan" id="kobo.790.1">’ are the corresponding word vectors in the input and output weight matrices, </span><span class="koboSpan" id="kobo.791.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/420.png" style="vertical-align:-0.339em;height:0.988em;width:2.176em"/></span><span class="koboSpan" id="kobo.792.1"> and </span><span class="koboSpan" id="kobo.793.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/434.png" style="vertical-align:-0.339em;height:1.017em;width:2.177em"/></span><span class="koboSpan" id="kobo.794.1">, respectively (we keep the original notation of the paper). </span><span class="koboSpan" id="kobo.794.2">Since the NN doesn’t have a hidden activation function, its output value for one input/output word pair is simply the multiplication of the input word vector, </span><span class="koboSpan" id="kobo.795.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;I&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/435.png" style="vertical-align:-0.528em;height:0.969em;width:1.015em"/></span><span class="koboSpan" id="kobo.796.1">, and the output word vector, </span><span class="koboSpan" id="kobo.797.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;v&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;O&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/436.png" style="vertical-align:-0.533em;height:1.210em;width:1.143em"/></span><span class="koboSpan" id="kobo.798.1"> (hence the </span><span class="No-Break"><span class="koboSpan" id="kobo.799.1">transpose operation).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.800.1">The authors of the Word2Vec paper note that word representations cannot represent idiomatic phrases that are not compositions of individual words. </span><span class="koboSpan" id="kobo.800.2">For example, </span><em class="italic"><span class="koboSpan" id="kobo.801.1">New York Times</span></em><span class="koboSpan" id="kobo.802.1"> is a newspaper, and not just a natural combination of the meanings of </span><em class="italic"><span class="koboSpan" id="kobo.803.1">New</span></em><span class="koboSpan" id="kobo.804.1">, </span><em class="italic"><span class="koboSpan" id="kobo.805.1">York</span></em><span class="koboSpan" id="kobo.806.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.807.1">Times</span></em><span class="koboSpan" id="kobo.808.1">. </span><span class="koboSpan" id="kobo.808.2">To overcome this, the model can be extended to include whole phrases. </span><span class="koboSpan" id="kobo.808.3">However, this significantly increases the vocabulary size. </span><span class="koboSpan" id="kobo.808.4">And, as we can see from the preceding formula, the softmax denominator needs to compute the output vectors for all words of the vocabulary. </span><span class="koboSpan" id="kobo.808.5">Additionally, every weight of the </span><span class="koboSpan" id="kobo.809.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;D&lt;/mml:mi&gt;&lt;mml:mo&gt;×&lt;/mml:mo&gt;&lt;mml:mi&gt;V&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/434.png" style="vertical-align:-0.339em;height:1.017em;width:2.173em"/></span><span class="koboSpan" id="kobo.810.1"> matrix is updated on every training step, which slows </span><span class="No-Break"><span class="koboSpan" id="kobo.811.1">the training.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.812.1">To solve this, we can replace</span><a id="_idIndexMarker865"/><span class="koboSpan" id="kobo.813.1"> the softmax operation with the so-called </span><strong class="bold"><span class="koboSpan" id="kobo.814.1">negative sampling</span></strong><span class="koboSpan" id="kobo.815.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.816.1">NEG</span></strong><span class="koboSpan" id="kobo.817.1">). </span><span class="koboSpan" id="kobo.817.2">For each training sample, we’ll take the positive training pair (for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.818.1">(fox, brown)</span></strong><span class="koboSpan" id="kobo.819.1">), as well as </span><em class="italic"><span class="koboSpan" id="kobo.820.1">k</span></em><span class="koboSpan" id="kobo.821.1"> additional negative pairs (for example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.822.1">(fox, puzzle)</span></strong><span class="koboSpan" id="kobo.823.1">), where </span><em class="italic"><span class="koboSpan" id="kobo.824.1">k</span></em><span class="koboSpan" id="kobo.825.1"> is usually in the range of [5,20]. </span><span class="koboSpan" id="kobo.825.2">Instead of predicting the word that best matches the input word (softmax), we’ll simply predict whether the current pair of words is true or not. </span><span class="koboSpan" id="kobo.825.3">In effect, we convert the multinomial classification problem (classified as one of many classes) into a binary logistic regression (or binary classification) problem. </span><span class="koboSpan" id="kobo.825.4">By learning the distinction between positive and negative pairs, the classifier will eventually learn the word vectors in the same way, as with multinomial classification. </span><span class="koboSpan" id="kobo.825.5">In Word2Vec, the words for the negative pairs are drawn from a special distribution, which draws less frequent words more often, compared to more </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">frequent ones.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.827.1">Some of the most frequent</span><a id="_idIndexMarker866"/><span class="koboSpan" id="kobo.828.1"> words to occur carry less information value compared</span><a id="_idIndexMarker867"/><span class="koboSpan" id="kobo.829.1"> to the rare words. </span><span class="koboSpan" id="kobo.829.2">Examples of such words are the definite and indefinite articles </span><em class="italic"><span class="koboSpan" id="kobo.830.1">a</span></em><span class="koboSpan" id="kobo.831.1">, </span><em class="italic"><span class="koboSpan" id="kobo.832.1">an</span></em><span class="koboSpan" id="kobo.833.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.834.1">the</span></em><span class="koboSpan" id="kobo.835.1">. </span><span class="koboSpan" id="kobo.835.2">The model will benefit more from observing the pairs </span><em class="italic"><span class="koboSpan" id="kobo.836.1">London</span></em><span class="koboSpan" id="kobo.837.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.838.1">city</span></em><span class="koboSpan" id="kobo.839.1"> compared to </span><em class="italic"><span class="koboSpan" id="kobo.840.1">the</span></em><span class="koboSpan" id="kobo.841.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.842.1">city</span></em><span class="koboSpan" id="kobo.843.1"> because almost all words co-occur frequently with </span><em class="italic"><span class="koboSpan" id="kobo.844.1">the</span></em><span class="koboSpan" id="kobo.845.1">. </span><span class="koboSpan" id="kobo.845.2">The opposite is also true – the vector representations of frequent words do not change significantly after training on many examples. </span><span class="koboSpan" id="kobo.845.3">To counter the imbalance between the rare and frequent words, the authors of the paper propose a subsampling approach, where each word, </span><span class="koboSpan" id="kobo.846.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/114.png" style="vertical-align:-0.340em;height:0.788em;width:0.834em"/></span><span class="koboSpan" id="kobo.847.1">, of the training set is discarded with some probability, computed by the heuristic formula where </span><br/><span class="koboSpan" id="kobo.848.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/439.png" style="vertical-align:-0.390em;height:1.101em;width:1.784em"/></span><span class="koboSpan" id="kobo.849.1"> is the frequency of word </span><span class="koboSpan" id="kobo.850.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/15.png" style="vertical-align:-0.340em;height:0.788em;width:0.821em"/></span><span class="koboSpan" id="kobo.851.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.852.1">t</span></em><span class="koboSpan" id="kobo.853.1"> is a threshold (usually </span><span class="No-Break"><span class="koboSpan" id="kobo.854.1">around </span></span><span class="No-Break"><span class="koboSpan" id="kobo.855.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;/mml:math&gt;" src="image/441.png" style="vertical-align:-0.012em;height:0.708em;width:1.627em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.856.1">):</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.857.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot; display=&quot;block&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;msqrt&gt;&lt;mfrac&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;f&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/msqrt&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/442.png" style="vertical-align:-0.874em;height:1.979em;width:7.401em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.858.1">It aggressively subsamples words with a frequency greater than </span><em class="italic"><span class="koboSpan" id="kobo.859.1">t</span></em><span class="koboSpan" id="kobo.860.1"> but also preserves the ranking of </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">the frequencies.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.862.1">We can say that, in general, skip-gram</span><a id="_idIndexMarker868"/><span class="koboSpan" id="kobo.863.1"> performs better on rare words than CBOW, </span><a id="_idIndexMarker869"/><span class="koboSpan" id="kobo.864.1">but it takes longer </span><span class="No-Break"><span class="koboSpan" id="kobo.865.1">to train.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.866.1">Now that we’ve learned about embedding vectors, let’s learn how to </span><span class="No-Break"><span class="koboSpan" id="kobo.867.1">visualize them.</span></span></p>
<h2 id="_idParaDest-118" lang="en-GB"><a id="_idTextAnchor192"/><span class="koboSpan" id="kobo.868.1">Visualizing embedding vectors</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.869.1">A successful word embedding</span><a id="_idIndexMarker870"/><span class="koboSpan" id="kobo.870.1"> function will map semantically similar words to vectors with high dot product similarity in the embedding space. </span><span class="koboSpan" id="kobo.870.2">To illustrate this, we’ll implement the </span><span class="No-Break"><span class="koboSpan" id="kobo.871.1">following steps:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.872.1">Train a Word2Vec skip-gram model on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.873.1">text8</span></strong><span class="koboSpan" id="kobo.874.1"> dataset, which consists of the first 100,000,000 bytes of plain text from Wikipedia (</span><a href="http://mattmahoney.net/dc/textdata.html"><span class="koboSpan" id="kobo.875.1">http://mattmahoney.net/dc/textdata.html</span></a><span class="koboSpan" id="kobo.876.1">). </span><span class="koboSpan" id="kobo.876.2">Each embedding vector is 100-dimensional, which is the default value for this type </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">of model.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.878.1">Select a list of </span><em class="italic"><span class="koboSpan" id="kobo.879.1">seed</span></em><span class="koboSpan" id="kobo.880.1"> words. </span><span class="koboSpan" id="kobo.880.2">In this case, the words are </span><em class="italic"><span class="koboSpan" id="kobo.881.1">mother</span></em><span class="koboSpan" id="kobo.882.1">, </span><em class="italic"><span class="koboSpan" id="kobo.883.1">car</span></em><span class="koboSpan" id="kobo.884.1">, </span><em class="italic"><span class="koboSpan" id="kobo.885.1">tree</span></em><span class="koboSpan" id="kobo.886.1">,</span><em class="italic"><span class="koboSpan" id="kobo.887.1"> science</span></em><span class="koboSpan" id="kobo.888.1">, </span><em class="italic"><span class="koboSpan" id="kobo.889.1">building, elephant</span></em><span class="koboSpan" id="kobo.890.1">, </span><span class="No-Break"><span class="koboSpan" id="kobo.891.1">and </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.892.1">green</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.893.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.894.1">Compute the dot-product similarity between the Word2Vec embedding vector of each seed word and the embedding vectors of all other words in the vocabulary. </span><span class="koboSpan" id="kobo.894.2">Then, select a cluster of the top-</span><em class="italic"><span class="koboSpan" id="kobo.895.1">k</span></em><span class="koboSpan" id="kobo.896.1"> (in our case, </span><em class="italic"><span class="koboSpan" id="kobo.897.1">k=5</span></em><span class="koboSpan" id="kobo.898.1">) similar words (based on their dot-product similarity) for each </span><span class="No-Break"><span class="koboSpan" id="kobo.899.1">seed word.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.900.1">Visualize the similarity</span><a id="_idIndexMarker871"/><span class="koboSpan" id="kobo.901.1"> between the seed embeddings and the embeddings of their respective clusters of similar words in a 2D plot. </span><span class="koboSpan" id="kobo.901.2">Since the embeddings are 100-dimensional, we’ll use the t-SNE (</span><a href="https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding"><span class="koboSpan" id="kobo.902.1">https://en.wikipedia.org/wiki/T-distributed_stochastic_neighbor_embedding</span></a><span class="koboSpan" id="kobo.903.1">) dimensionality-reduction algorithm. </span><span class="koboSpan" id="kobo.903.2">It maps each high-dimensional embedding vector on a two- or three-dimensional point in a way where similar objects are modeled on nearby points and dissimilar objects are modeled on distant points with a high probability. </span><span class="koboSpan" id="kobo.903.3">We can see the result in the </span><span class="No-Break"><span class="koboSpan" id="kobo.904.1">following scatterplot:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer568">
<span class="koboSpan" id="kobo.905.1"><img alt="Figure 6.5 – t-SNE visualization of the seed words and their clusters of the most similar words" src="image/B19627_06_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.906.1">Figure 6.5 – t-SNE visualization of the seed words and their clusters of the most similar words</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.907.1">This graph proves that the obtained word vectors contain relevant information for </span><span class="No-Break"><span class="koboSpan" id="kobo.908.1">the words.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.909.1">Word2Vec (and similar models) create </span><strong class="bold"><span class="koboSpan" id="kobo.910.1">static</span></strong><span class="koboSpan" id="kobo.911.1"> (or </span><strong class="bold"><span class="koboSpan" id="kobo.912.1">context independent</span></strong><span class="koboSpan" id="kobo.913.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.914.1">embeddings</span></strong><span class="koboSpan" id="kobo.915.1">. </span><span class="koboSpan" id="kobo.915.2">Each word has a single embedding</span><a id="_idIndexMarker872"/><span class="koboSpan" id="kobo.916.1"> vector, based on all occurrences (that is, all contexts) of that word</span><a id="_idIndexMarker873"/><span class="koboSpan" id="kobo.917.1"> in the text corpus. </span><span class="koboSpan" id="kobo.917.2">This imposes some limitations. </span><span class="koboSpan" id="kobo.917.3">For example, </span><em class="italic"><span class="koboSpan" id="kobo.918.1">bank</span></em><span class="koboSpan" id="kobo.919.1"> has a different meaning in different contexts, such as </span><em class="italic"><span class="koboSpan" id="kobo.920.1">river bank</span></em><span class="koboSpan" id="kobo.921.1">, </span><em class="italic"><span class="koboSpan" id="kobo.922.1">savings bank</span></em><span class="koboSpan" id="kobo.923.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.924.1">bank holiday</span></em><span class="koboSpan" id="kobo.925.1">. </span><span class="koboSpan" id="kobo.925.2">Despite this, it is represented with a single embedding. </span><span class="koboSpan" id="kobo.925.3">In addition, the static embedding doesn’t take into account the word order in the context. </span><span class="koboSpan" id="kobo.925.4">For example, the expressions </span><em class="italic"><span class="koboSpan" id="kobo.926.1">I like apples, but I don’t like oranges</span></em><span class="koboSpan" id="kobo.927.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.928.1">I like oranges, but I don’t like apples</span></em><span class="koboSpan" id="kobo.929.1"> have opposite meanings, but Word2Vec interprets</span><a id="_idIndexMarker874"/><span class="koboSpan" id="kobo.930.1"> them as the same. </span><span class="koboSpan" id="kobo.930.2">We can</span><a id="_idIndexMarker875"/><span class="koboSpan" id="kobo.931.1"> solve these problems with the so-called </span><strong class="bold"><span class="koboSpan" id="kobo.932.1">dynamic</span></strong><span class="koboSpan" id="kobo.933.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.934.1">context dependent</span></strong><span class="koboSpan" id="kobo.935.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.936.1">embeddings</span></strong><span class="koboSpan" id="kobo.937.1">, which we’ll discuss in </span><a href="B19627_07.xhtml#_idTextAnchor202"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.938.1">Chapter 7</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.939.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.940.1">So far, we’ve focused</span><a id="_idIndexMarker876"/><span class="koboSpan" id="kobo.941.1"> on single words (or tokens). </span><span class="koboSpan" id="kobo.941.2">Next, we’ll expand our scope to </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">text sequences.</span></span></p>
<h2 id="_idParaDest-119" lang="en-GB"><a id="_idTextAnchor193"/><span class="koboSpan" id="kobo.943.1">Language modeling</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.944.1">A word-based </span><strong class="bold"><span class="koboSpan" id="kobo.945.1">language model</span></strong><span class="koboSpan" id="kobo.946.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.947.1">LM</span></strong><span class="koboSpan" id="kobo.948.1">) defines a probability distribution over</span><a id="_idIndexMarker877"/><span class="koboSpan" id="kobo.949.1"> sequences of </span><strong class="bold"><span class="koboSpan" id="kobo.950.1">tokens</span></strong><span class="koboSpan" id="kobo.951.1">. </span><span class="koboSpan" id="kobo.951.2">For this section, we’ll assume that the tokens</span><a id="_idIndexMarker878"/><span class="koboSpan" id="kobo.952.1"> are words. </span><span class="koboSpan" id="kobo.952.2">Given a sequence of words of length </span><em class="italic"><span class="koboSpan" id="kobo.953.1">m</span></em><span class="koboSpan" id="kobo.954.1"> (for example, a sentence), an LM assigns a probability, </span><span class="koboSpan" id="kobo.955.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/443.png" style="vertical-align:-0.390em;height:1.038em;width:4.272em"/></span><span class="koboSpan" id="kobo.956.1">, that the full sequence of words could exist. </span><span class="koboSpan" id="kobo.956.2">One application of these probabilities is a generative model to create new text – a word-based LM can compute the likelihood of the next word, given an existing sequence of words that precede it. </span><span class="koboSpan" id="kobo.956.3">Once we have this new word, we can append it to the existing sequence and predict yet another new word, and so on. </span><span class="koboSpan" id="kobo.956.4">In this way, we can generate new text sequences with arbitrary length. </span><span class="koboSpan" id="kobo.956.5">For example, given the sequence </span><em class="italic"><span class="koboSpan" id="kobo.957.1">the quick brown</span></em><span class="koboSpan" id="kobo.958.1">, the LM might predict </span><em class="italic"><span class="koboSpan" id="kobo.959.1">fox</span></em><span class="koboSpan" id="kobo.960.1"> as the next most likely word. </span><span class="koboSpan" id="kobo.960.2">Then, the sequence becomes </span><em class="italic"><span class="koboSpan" id="kobo.961.1">the quick brown fox</span></em><span class="koboSpan" id="kobo.962.1">, and we task the LM to predict the new most likely word based on the updated sequence. </span><span class="koboSpan" id="kobo.962.2">A model whose output depends on its previous</span><a id="_idIndexMarker879"/><span class="koboSpan" id="kobo.963.1"> values, as well as its stochastic (that is, with some randomness) output</span><a id="_idIndexMarker880"/><span class="koboSpan" id="kobo.964.1"> (new value), is called an </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.965.1">autoregressive model</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.967.1">Next, we’ll focus on the properties of the word sequence, rather than </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">the model.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.969.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.970.1">Even the most advanced LLMs, such as ChatGPT, are autoregressive models – they just predict the next word, one word at </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">a time.</span></span></p>
<h3 lang="en-GB"><span class="koboSpan" id="kobo.972.1">Understanding N-grams</span></h3>
<p lang="en-GB"><span class="koboSpan" id="kobo.973.1">The inference of the probability</span><a id="_idIndexMarker881"/><span class="koboSpan" id="kobo.974.1"> of a long sequence, say </span><span class="koboSpan" id="kobo.975.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/444.png" style="vertical-align:-0.340em;height:0.788em;width:2.998em"/></span><span class="koboSpan" id="kobo.976.1">, is typically</span><a id="_idIndexMarker882"/><span class="koboSpan" id="kobo.977.1"> infeasible. </span><span class="koboSpan" id="kobo.977.2">To understand why, let’s note that we can calculate the joint probability of </span><span class="koboSpan" id="kobo.978.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/445.png" style="vertical-align:-0.390em;height:1.038em;width:4.392em"/></span><span class="koboSpan" id="kobo.979.1"> with the chain rule of joint probability (</span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.980.1">Chapter 2</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.981.1">):</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.982.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/446.png" style="vertical-align:-0.390em;height:1.189em;width:24.289em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.983.1">The probability of the later words given the earlier words would be especially difficult to estimate from the data. </span><span class="koboSpan" id="kobo.983.2">That’s why this joint probability is typically approximated by an independence assumption that the </span><em class="italic"><span class="koboSpan" id="kobo.984.1">i</span></em><span class="koboSpan" id="kobo.985.1">-th word is only dependent on the </span><em class="italic"><span class="koboSpan" id="kobo.986.1">n-1</span></em><span class="koboSpan" id="kobo.987.1"> previous words. </span><span class="koboSpan" id="kobo.987.2">We’ll only model the joint probabilities of combinations of </span><em class="italic"><span class="koboSpan" id="kobo.988.1">n</span></em><span class="koboSpan" id="kobo.989.1"> sequential words, called </span><em class="italic"><span class="koboSpan" id="kobo.990.1">n</span></em><span class="koboSpan" id="kobo.991.1">-grams. </span><span class="koboSpan" id="kobo.991.2">For example, in the phrase </span><em class="italic"><span class="koboSpan" id="kobo.992.1">the quick brown fox</span></em><span class="koboSpan" id="kobo.993.1">, we have the </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">following </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.995.1">n</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.996.1">-grams:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.997.1">1-gram</span></strong><span class="koboSpan" id="kobo.998.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.999.1">unigram</span></strong><span class="koboSpan" id="kobo.1000.1">): </span><em class="italic"><span class="koboSpan" id="kobo.1001.1">the</span></em><span class="koboSpan" id="kobo.1002.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1003.1">quick</span></em><span class="koboSpan" id="kobo.1004.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1005.1">brown</span></em><span class="koboSpan" id="kobo.1006.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.1007.1">fox</span></em><span class="koboSpan" id="kobo.1008.1"> (this is where Unigram tokenization takes </span><span class="No-Break"><span class="koboSpan" id="kobo.1009.1">its name)</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1010.1">2-gram</span></strong><span class="koboSpan" id="kobo.1011.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1012.1">bigram</span></strong><span class="koboSpan" id="kobo.1013.1">): </span><em class="italic"><span class="koboSpan" id="kobo.1014.1">the quick</span></em><span class="koboSpan" id="kobo.1015.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1016.1">quick brown</span></em><span class="koboSpan" id="kobo.1017.1">, and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1018.1">brown fox</span></em></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1019.1">3-gram</span></strong><span class="koboSpan" id="kobo.1020.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1021.1">trigram</span></strong><span class="koboSpan" id="kobo.1022.1">): </span><em class="italic"><span class="koboSpan" id="kobo.1023.1">the quick brown</span></em><span class="koboSpan" id="kobo.1024.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1025.1">quick </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1026.1">brown fox</span></em></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1027.1">4-gram</span></strong><span class="koboSpan" id="kobo.1028.1">: </span><em class="italic"><span class="koboSpan" id="kobo.1029.1">the quick </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1030.1">brown fox</span></em></span></li>
</ul>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1031.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1032.1">The term </span><em class="italic"><span class="koboSpan" id="kobo.1033.1">n</span></em><span class="koboSpan" id="kobo.1034.1">-grams can refer to other types of sequences of length </span><em class="italic"><span class="koboSpan" id="kobo.1035.1">n</span></em><span class="koboSpan" id="kobo.1036.1">, such as </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1037.1">n</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1038.1"> characters.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1039.1">The inference of the joint distribution</span><a id="_idIndexMarker883"/><span class="koboSpan" id="kobo.1040.1"> is approximated with the help of </span><em class="italic"><span class="koboSpan" id="kobo.1041.1">n</span></em><span class="koboSpan" id="kobo.1042.1">-gram</span><a id="_idIndexMarker884"/><span class="koboSpan" id="kobo.1043.1"> models that split the joint distribution into multiple independent parts. </span><span class="koboSpan" id="kobo.1043.2">If we have a large corpus of text, we can find all the </span><em class="italic"><span class="koboSpan" id="kobo.1044.1">n</span></em><span class="koboSpan" id="kobo.1045.1">-grams up until a certain </span><em class="italic"><span class="koboSpan" id="kobo.1046.1">n</span></em><span class="koboSpan" id="kobo.1047.1"> (typically 2 to 4) and count the occurrence of each </span><em class="italic"><span class="koboSpan" id="kobo.1048.1">n</span></em><span class="koboSpan" id="kobo.1049.1">-gram in that corpus. </span><span class="koboSpan" id="kobo.1049.2">From these counts, we can estimate the probabilities of the last word of each </span><em class="italic"><span class="koboSpan" id="kobo.1050.1">n</span></em><span class="koboSpan" id="kobo.1051.1">-gram, given the previous </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1052.1">n-1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1053.1"> words:</span></span></p>
<ul>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.1054.1">Unigram: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1055.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;mml:mi&gt;u&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mtext&gt;total number of words in the corpus&lt;/mml:mtext&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;/mml:math&gt;" src="image/447.png" style="vertical-align:-0.351em;height:1.096em;width:11.800em"/></span></span></li>
<li lang="en-GB"><span class="No-Break"><span class="koboSpan" id="kobo.1056.1">Bigram: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/448.png" style="vertical-align:-0.390em;height:1.285em;width:8.831em"/></span></span></li>
<li lang="en-GB"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1058.1">n</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1059.1">-gram: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1060.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;mi&gt;P&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;|&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;mo&gt;=&lt;/mo&gt;&lt;mstyle scriptlevel=&quot;+1&quot;&gt;&lt;mfrac&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;mrow&gt;&lt;mi&gt;c&lt;/mi&gt;&lt;mi&gt;o&lt;/mi&gt;&lt;mi&gt;u&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mi&gt;t&lt;/mi&gt;&lt;mfenced open=&quot;(&quot; close=&quot;)&quot;&gt;&lt;mrow&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;/msub&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mo&gt;…&lt;/mo&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;msub&gt;&lt;mi&gt;w&lt;/mi&gt;&lt;mrow&gt;&lt;mi&gt;n&lt;/mi&gt;&lt;mo&gt;+&lt;/mo&gt;&lt;mi&gt;i&lt;/mi&gt;&lt;mo&gt;−&lt;/mo&gt;&lt;mn&gt;1&lt;/mn&gt;&lt;/mrow&gt;&lt;/msub&gt;&lt;/mrow&gt;&lt;/mfenced&gt;&lt;/mrow&gt;&lt;/mfrac&gt;&lt;/mstyle&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/449.png" style="vertical-align:-0.390em;height:1.285em;width:14.986em"/></span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1061.1">The independent assumption that the </span><em class="italic"><span class="koboSpan" id="kobo.1062.1">i</span></em><span class="koboSpan" id="kobo.1063.1">-th word is only dependent on the previous </span><em class="italic"><span class="koboSpan" id="kobo.1064.1">n-1 </span></em><span class="koboSpan" id="kobo.1065.1">words can now be used to approximate the </span><span class="No-Break"><span class="koboSpan" id="kobo.1066.1">joint distribution.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1067.1">For example, we can approximate the joint distribution for a unigram with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1068.1">following formula:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1069.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/450.png" style="vertical-align:-0.390em;height:1.038em;width:14.165em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1070.1">For a trigram, we can approximate the joint distribution with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1071.1">following formula:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1072.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mi&gt;P&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;w&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;m&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/451.png" style="vertical-align:-0.390em;height:1.189em;width:25.133em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1073.1">We can see that, based on the vocabulary size, the number of </span><em class="italic"><span class="koboSpan" id="kobo.1074.1">n</span></em><span class="koboSpan" id="kobo.1075.1">-grams grows exponentially with </span><em class="italic"><span class="koboSpan" id="kobo.1076.1">n</span></em><span class="koboSpan" id="kobo.1077.1">. </span><span class="koboSpan" id="kobo.1077.2">For example, if a small vocabulary contains 100 words, then the number of possible 5-grams would be </span><span class="koboSpan" id="kobo.1078.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;100&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;5&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;10,000,000,000&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/452.png" style="vertical-align:-0.117em;height:0.813em;width:9.177em"/></span><span class="koboSpan" id="kobo.1079.1"> different 5-grams. </span><span class="koboSpan" id="kobo.1079.2">In comparison, the entire works of Shakespeare contain around 30,000 different words, illustrating the infeasibility of using </span><em class="italic"><span class="koboSpan" id="kobo.1080.1">n</span></em><span class="koboSpan" id="kobo.1081.1">-grams with a large </span><em class="italic"><span class="koboSpan" id="kobo.1082.1">n</span></em><span class="koboSpan" id="kobo.1083.1">. </span><span class="koboSpan" id="kobo.1083.2">Not only is there the issue of storing all the probabilities, but we would also need a very large text corpus to create decent </span><em class="italic"><span class="koboSpan" id="kobo.1084.1">n</span></em><span class="koboSpan" id="kobo.1085.1">-gram probability estimations for larger values </span><span class="No-Break"><span class="koboSpan" id="kobo.1086.1">of </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1087.1">n</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1088.1">.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1089.1">The curse of dimensionality</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1090.1">When the number of possible input variables (words) increases, the number of different combinations of these input values</span><a id="_idIndexMarker885"/><span class="koboSpan" id="kobo.1091.1"> increases exponentially. </span><span class="koboSpan" id="kobo.1091.2">This problem is known as the curse of dimensionality. </span><span class="koboSpan" id="kobo.1091.3">It arises when the learning algorithm needs at least one example per relevant combination of values, which is the case in </span><em class="italic"><span class="koboSpan" id="kobo.1092.1">n</span></em><span class="koboSpan" id="kobo.1093.1">-gram modeling. </span><span class="koboSpan" id="kobo.1093.2">The larger our </span><em class="italic"><span class="koboSpan" id="kobo.1094.1">n</span></em><span class="koboSpan" id="kobo.1095.1">, the better we can approximate the original distribution and the more data we would need to make good estimations of the </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1096.1">n</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1097.1">-gram probabilities.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1098.1">But fret not, as the </span><em class="italic"><span class="koboSpan" id="kobo.1099.1">n</span></em><span class="koboSpan" id="kobo.1100.1">-gram LM gives us some important clues on how to proceed. </span><span class="koboSpan" id="kobo.1100.2">Its theoretical formulation is sound, but the curse of dimensionality makes it unfeasible. </span><span class="koboSpan" id="kobo.1100.3">In addition, the </span><em class="italic"><span class="koboSpan" id="kobo.1101.1">n</span></em><span class="koboSpan" id="kobo.1102.1">-gram model reinforces the importance of the word context, just as with Word2Vec. </span><span class="koboSpan" id="kobo.1102.2">In the next few sections, we’ll learn</span><a id="_idIndexMarker886"/><span class="koboSpan" id="kobo.1103.1"> how to simulate an </span><em class="italic"><span class="koboSpan" id="kobo.1104.1">n</span></em><span class="koboSpan" id="kobo.1105.1">-gram model probability distribution</span><a id="_idIndexMarker887"/><span class="koboSpan" id="kobo.1106.1"> with the help </span><span class="No-Break"><span class="koboSpan" id="kobo.1107.1">of NNs.</span></span></p>
<h1 id="_idParaDest-120" lang="en-GB"><a id="_idTextAnchor194"/><span class="koboSpan" id="kobo.1108.1">Introducing RNNs</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1109.1">An RNN is a type of NN</span><a id="_idIndexMarker888"/><span class="koboSpan" id="kobo.1110.1"> that can process sequential data with variable length. </span><span class="koboSpan" id="kobo.1110.2">Examples of such data include text sequences or the price of a stock at various moments in time. </span><span class="koboSpan" id="kobo.1110.3">By using the word </span><em class="italic"><span class="koboSpan" id="kobo.1111.1">sequential</span></em><span class="koboSpan" id="kobo.1112.1">, we imply that the sequence elements are related to each other and their order matters. </span><span class="koboSpan" id="kobo.1112.2">For example, if we take a book and randomly shuffle all the words in it, the text will lose its meaning, even though we’ll still know the </span><span class="No-Break"><span class="koboSpan" id="kobo.1113.1">individual words.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1114.1">RNNs get their name because they apply the same function over a sequence recurrently. </span><span class="koboSpan" id="kobo.1114.2">We can define an RNN as a </span><span class="No-Break"><span class="koboSpan" id="kobo.1115.1">recurrence relation:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1116.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/453.png" style="vertical-align:-0.390em;height:1.101em;width:5.408em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1117.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.1118.1">f</span></em><span class="koboSpan" id="kobo.1119.1"> is a differentiable</span><a id="_idIndexMarker889"/><span class="koboSpan" id="kobo.1120.1"> function, </span><span class="koboSpan" id="kobo.1121.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/454.png" style="vertical-align:-0.340em;height:0.793em;width:0.611em"/></span><span class="koboSpan" id="kobo.1122.1"> is a vector of values called internal RNN state (at step </span><em class="italic"><span class="koboSpan" id="kobo.1123.1">t</span></em><span class="koboSpan" id="kobo.1124.1">), and </span><span class="koboSpan" id="kobo.1125.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/352.png" style="vertical-align:-0.340em;height:0.781em;width:0.710em"/></span><span class="koboSpan" id="kobo.1126.1"> is the network input at step </span><em class="italic"><span class="koboSpan" id="kobo.1127.1">t</span></em><span class="koboSpan" id="kobo.1128.1">. </span><span class="koboSpan" id="kobo.1128.2">Unlike regular NNs, where the state only depends on the current input (and RNN weights), here, </span><span class="koboSpan" id="kobo.1129.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/454.png" style="vertical-align:-0.340em;height:0.793em;width:0.611em"/></span><span class="koboSpan" id="kobo.1130.1"> is a function of both the current input, as well as the previous state, </span><span class="koboSpan" id="kobo.1131.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/457.png" style="vertical-align:-0.340em;height:0.793em;width:1.229em"/></span><span class="koboSpan" id="kobo.1132.1">. </span><span class="koboSpan" id="kobo.1132.2">You can think of </span><span class="koboSpan" id="kobo.1133.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/458.png" style="vertical-align:-0.340em;height:0.793em;width:1.194em"/></span><span class="koboSpan" id="kobo.1134.1"> as the RNN’s summary of all previous inputs. </span><span class="koboSpan" id="kobo.1134.2">The recurrence relation defines how the state evolves step by step over the sequence via a feedback loop over previous states, as illustrated in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1135.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer585">
<span class="koboSpan" id="kobo.1136.1"><img alt="Figure 6.6 – An unfolded RNN" src="image/B19627_06_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1137.1">Figure 6.6 – An unfolded RNN</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1138.1">On the left, we have a visual illustration</span><a id="_idIndexMarker890"/><span class="koboSpan" id="kobo.1139.1"> of the RNN recurrence relation. </span><span class="koboSpan" id="kobo.1139.2">On the right, we have the RNN states recurrently unfolded over the sequence </span><em class="italic"><span class="koboSpan" id="kobo.1140.1">t-1</span></em><span class="koboSpan" id="kobo.1141.1">, </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1142.1">t</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1143.1">, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1144.1">t+1</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1145.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1146.1">The RNN has three sets of parameters (or weights), shared between </span><span class="No-Break"><span class="koboSpan" id="kobo.1147.1">all steps:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1148.1">U</span></strong><span class="koboSpan" id="kobo.1149.1">: Transforms the input, </span><span class="koboSpan" id="kobo.1150.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/352.png" style="vertical-align:-0.340em;height:0.781em;width:0.710em"/></span><span class="koboSpan" id="kobo.1151.1">, into the </span><span class="No-Break"><span class="koboSpan" id="kobo.1152.1">state, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1153.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/454.png" style="vertical-align:-0.340em;height:0.793em;width:0.611em"/></span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1154.1">W</span></strong><span class="koboSpan" id="kobo.1155.1">: Transforms the previous state, </span><span class="koboSpan" id="kobo.1156.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/457.png" style="vertical-align:-0.340em;height:0.793em;width:1.229em"/></span><span class="koboSpan" id="kobo.1157.1">, into the current </span><span class="No-Break"><span class="koboSpan" id="kobo.1158.1">state, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1159.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/454.png" style="vertical-align:-0.340em;height:0.793em;width:0.611em"/></span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1160.1">V</span></strong><span class="koboSpan" id="kobo.1161.1">: Maps the newly computed internal state, </span><span class="koboSpan" id="kobo.1162.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/454.png" style="vertical-align:-0.340em;height:0.793em;width:0.611em"/></span><span class="koboSpan" id="kobo.1163.1">, to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1164.1">output, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1165.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/464.png" style="vertical-align:-0.340em;height:0.781em;width:0.693em"/></span></span></li>
</ul>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1166.1">U</span></strong><span class="koboSpan" id="kobo.1167.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.1168.1">V</span></strong><span class="koboSpan" id="kobo.1169.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.1170.1">W</span></strong><span class="koboSpan" id="kobo.1171.1"> apply linear transformation over their respective inputs. </span><span class="koboSpan" id="kobo.1171.2">The most basic case of such a transformation is the familiar FC operation we know and love (therefore, </span><strong class="bold"><span class="koboSpan" id="kobo.1172.1">U</span></strong><span class="koboSpan" id="kobo.1173.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.1174.1">V</span></strong><span class="koboSpan" id="kobo.1175.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.1176.1">W</span></strong><span class="koboSpan" id="kobo.1177.1"> are weight matrices). </span><span class="koboSpan" id="kobo.1177.2">We can now define the internal state and the RNN output </span><span class="No-Break"><span class="koboSpan" id="kobo.1178.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1179.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/465.png" style="vertical-align:-0.390em;height:1.101em;width:8.093em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1180.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;V&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/466.png" style="vertical-align:-0.340em;height:0.989em;width:3.681em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1181.1">Here, </span><em class="italic"><span class="koboSpan" id="kobo.1182.1">f</span></em><span class="koboSpan" id="kobo.1183.1"> is the non-linear activation function (such as tanh, sigmoid, </span><span class="No-Break"><span class="koboSpan" id="kobo.1184.1">or ReLU).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1185.1">For example, in a word-level LM, the input, </span><em class="italic"><span class="koboSpan" id="kobo.1186.1">x</span></em><span class="koboSpan" id="kobo.1187.1">, will be a sequence of word embedding vectors (</span><span class="koboSpan" id="kobo.1188.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/467.png" style="vertical-align:-0.340em;height:0.781em;width:2.491em"/></span><span class="koboSpan" id="kobo.1189.1">). </span><br/><span class="koboSpan" id="kobo.1190.1">The state, </span><em class="italic"><span class="koboSpan" id="kobo.1191.1">s</span></em><span class="koboSpan" id="kobo.1192.1">, will be a sequence of state vectors (</span><span class="koboSpan" id="kobo.1193.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/468.png" style="vertical-align:-0.340em;height:0.793em;width:2.367em"/></span><span class="koboSpan" id="kobo.1194.1">). </span><span class="koboSpan" id="kobo.1194.2">Finally, the output, </span><em class="italic"><span class="koboSpan" id="kobo.1195.1">y</span></em><span class="koboSpan" id="kobo.1196.1">, will be a sequence of probability vectors (</span><span class="koboSpan" id="kobo.1197.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/469.png" style="vertical-align:-0.340em;height:0.781em;width:2.458em"/></span><span class="koboSpan" id="kobo.1198.1">) of the next words in </span><span class="No-Break"><span class="koboSpan" id="kobo.1199.1">the sequence.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1200.1">Note that in an RNN, each state is dependent on all previous computations via this recurrence relation. </span><span class="koboSpan" id="kobo.1200.2">An important implication of this is that RNNs have memory over time because the states, </span><span class="koboSpan" id="kobo.1201.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/454.png" style="vertical-align:-0.340em;height:0.793em;width:0.610em"/></span><span class="koboSpan" id="kobo.1202.1">, contain information based on the previous steps. </span><span class="koboSpan" id="kobo.1202.2">In theory, RNNs can remember information for an arbitrarily long period, but in practice, they are limited to looking back only a few steps. </span><span class="koboSpan" id="kobo.1202.3">We will address this issue in more detail in the </span><em class="italic"><span class="koboSpan" id="kobo.1203.1">Vanishing and exploding </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1204.1">gradients</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1205.1"> section.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1206.1">The RNN</span><a id="_idIndexMarker891"/><span class="koboSpan" id="kobo.1207.1"> we described is somewhat equivalent to a single-layer regular NN (with an additional recurrence relation). </span><span class="koboSpan" id="kobo.1207.2">But as with regular</span><a id="_idIndexMarker892"/><span class="koboSpan" id="kobo.1208.1"> NNs, we can stack multiple RNNs to form a </span><strong class="bold"><span class="koboSpan" id="kobo.1209.1">stacked RNN</span></strong><span class="koboSpan" id="kobo.1210.1">. </span><span class="koboSpan" id="kobo.1210.2">The cell state, </span><span class="koboSpan" id="kobo.1211.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/471.png" style="vertical-align:-0.340em;height:1.088em;width:0.606em"/></span><span class="koboSpan" id="kobo.1212.1">, of an RNN cell at level </span><em class="italic"><span class="koboSpan" id="kobo.1213.1">l</span></em><span class="koboSpan" id="kobo.1214.1"> at time </span><em class="italic"><span class="koboSpan" id="kobo.1215.1">t</span></em><span class="koboSpan" id="kobo.1216.1"> will take the output, </span><span class="koboSpan" id="kobo.1217.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/472.png" style="vertical-align:-0.340em;height:1.088em;width:1.266em"/></span><span class="koboSpan" id="kobo.1218.1">, of the RNN cell from level </span><em class="italic"><span class="koboSpan" id="kobo.1219.1">l-1</span></em><span class="koboSpan" id="kobo.1220.1"> and the previous cell state, </span><span class="koboSpan" id="kobo.1221.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/473.png" style="vertical-align:-0.340em;height:1.088em;width:1.229em"/></span><span class="koboSpan" id="kobo.1222.1">, of the cell at the same level </span><em class="italic"><span class="koboSpan" id="kobo.1223.1">l</span></em><span class="koboSpan" id="kobo.1224.1"> as </span><span class="No-Break"><span class="koboSpan" id="kobo.1225.1">the input:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1226.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;y&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;l&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/474.png" style="vertical-align:-0.390em;height:1.188em;width:6.219em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1227.1">In the following diagram, we can see an unfolded, </span><span class="No-Break"><span class="koboSpan" id="kobo.1228.1">stacked RNN:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer602">
<span class="koboSpan" id="kobo.1229.1"><img alt="Figure 6.7 – Stacked RNN" src="image/B19627_06_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1230.1">Figure 6.7 – Stacked RNN</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1231.1">Because RNNs</span><a id="_idIndexMarker893"/><span class="koboSpan" id="kobo.1232.1"> are not limited to processing fixed-size inputs, they expand the possibilities of what we can compute with NNs. </span><span class="koboSpan" id="kobo.1232.2">We can identify several types of tasks, based on the relationship between the input and </span><span class="No-Break"><span class="koboSpan" id="kobo.1233.1">output sizes:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1234.1">One-to-one</span></strong><span class="koboSpan" id="kobo.1235.1">: Non-sequential processing, such as feedforward</span><a id="_idIndexMarker894"/><span class="koboSpan" id="kobo.1236.1"> NNs and CNNs. </span><span class="koboSpan" id="kobo.1236.2">There isn’t much difference between a feedforward NN and applying an RNN to a single time step. </span><span class="koboSpan" id="kobo.1236.3">An example of one-to-one processing is </span><span class="No-Break"><span class="koboSpan" id="kobo.1237.1">image classification.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1238.1">One-to-many</span></strong><span class="koboSpan" id="kobo.1239.1">: This generates a sequence based </span><a id="_idIndexMarker895"/><span class="koboSpan" id="kobo.1240.1">on a single input – for example, caption generation from an image (</span><em class="italic"><span class="koboSpan" id="kobo.1241.1">Show and Tell: A Neural Image Caption </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1242.1">Generator</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1243.1">, </span></span><a href="https://arxiv.org/abs/1411.4555"><span class="No-Break"><span class="koboSpan" id="kobo.1244.1">https://arxiv.org/abs/1411.4555</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1245.1">).</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1246.1">Many-to-one</span></strong><span class="koboSpan" id="kobo.1247.1">: This outputs a single result</span><a id="_idIndexMarker896"/><span class="koboSpan" id="kobo.1248.1"> based on a sequence – for example, sentiment classification </span><span class="No-Break"><span class="koboSpan" id="kobo.1249.1">of text.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1250.1">Many-to-many indirect</span></strong><span class="koboSpan" id="kobo.1251.1">: A sequence is encoded into a state vector, after which</span><a id="_idIndexMarker897"/><span class="koboSpan" id="kobo.1252.1"> this state vector is decoded into a new sequence – for example, language translation (</span><em class="italic"><span class="koboSpan" id="kobo.1253.1">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</span></em><span class="koboSpan" id="kobo.1254.1">, </span><a href="https://arxiv.org/abs/1406.1078"><span class="koboSpan" id="kobo.1255.1">https://arxiv.org/abs/1406.1078</span></a><span class="koboSpan" id="kobo.1256.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1257.1">Sequence to Sequence Learning with Neural </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1258.1">Networks</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1259.1">, </span></span><a href="http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)"><span class="No-Break"><span class="koboSpan" id="kobo.1260.1">http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf)</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1261.1">.</span></span></li>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1262.1">Many-to-many direct</span></strong><span class="koboSpan" id="kobo.1263.1">: Outputs a result for each input step – for example, frame phoneme</span><a id="_idIndexMarker898"/><span class="koboSpan" id="kobo.1264.1"> labeling in </span><span class="No-Break"><span class="koboSpan" id="kobo.1265.1">speech recognition.</span></span></li>
</ul>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1266.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1267.1">The</span><a id="_idIndexMarker899"/><span class="koboSpan" id="kobo.1268.1"> many-to-many models</span><a id="_idIndexMarker900"/><span class="koboSpan" id="kobo.1269.1"> are often referred to as </span><strong class="bold"><span class="koboSpan" id="kobo.1270.1">sequence-to-sequence</span></strong><span class="koboSpan" id="kobo.1271.1"> (</span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1272.1">seq2seq</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1273.1">) models.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1274.1">The following is a graphical representation of the preceding </span><span class="No-Break"><span class="koboSpan" id="kobo.1275.1">input-output combinations:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer603">
<span class="koboSpan" id="kobo.1276.1"><img alt="Figure 6.8 – RNN input-output combinations, inspired by http://karpathy.github.io/2015/05/21/rnn-effectiveness/" src="image/B19627_06_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1277.1">Figure 6.8 – RNN input-output combinations, inspired by </span><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/"><span class="koboSpan" id="kobo.1278.1">http://karpathy.github.io/2015/05/21/rnn-effectiveness/</span></a></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1279.1">Now that we’ve introduced</span><a id="_idIndexMarker901"/><span class="koboSpan" id="kobo.1280.1"> RNNs, let’s improve our knowledge by implementing a simple </span><span class="No-Break"><span class="koboSpan" id="kobo.1281.1">RNN example.</span></span></p>
<h2 id="_idParaDest-121" lang="en-GB"><a id="_idTextAnchor195"/><span class="koboSpan" id="kobo.1282.1">RNN implementation and training</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1283.1">In the preceding section, we briefly</span><a id="_idIndexMarker902"/><span class="koboSpan" id="kobo.1284.1"> discussed what RNNs</span><a id="_idIndexMarker903"/><span class="koboSpan" id="kobo.1285.1"> are and what problems they can solve. </span><span class="koboSpan" id="kobo.1285.2">Let’s dive into the details of an RNN and how to train it with a very simple toy example: counting ones in </span><span class="No-Break"><span class="koboSpan" id="kobo.1286.1">a sequence.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1287.1">We’ll teach a basic RNN how to count the number of ones in the input and then output the result at the end of the sequence. </span><span class="koboSpan" id="kobo.1287.2">This is an example of a many-to-one relationship, which we defined in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1288.1">previous section.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1289.1">We’ll implement this example with Python (no DL libraries) and numpy. </span><span class="koboSpan" id="kobo.1289.2">An example of the input and output is </span><span class="No-Break"><span class="koboSpan" id="kobo.1290.1">as follows:</span></span></p>
<pre class="console" lang="en-GB"><span class="koboSpan" id="kobo.1291.1">
In: (0, 0, 0, 0, 1, 0, 1, 0, 1, 0)
Out: 3</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1292.1">The RNN we’ll use is illustrated in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1293.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer604">
<span class="koboSpan" id="kobo.1294.1"><img alt="Figure 6.9 – Basic RNN for counting ones in the input" src="image/B19627_06_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1295.1">Figure 6.9 – Basic RNN for counting ones in the input</span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1296.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1297.1">Since </span><span class="koboSpan" id="kobo.1298.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/475.png" style="vertical-align:-0.340em;height:0.788em;width:0.496em"/></span><span class="koboSpan" id="kobo.1299.1">, </span><span class="koboSpan" id="kobo.1300.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/476.png" style="vertical-align:-0.340em;height:0.788em;width:0.631em"/></span><span class="koboSpan" id="kobo.1301.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1302.1">U</span></em><span class="koboSpan" id="kobo.1303.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1304.1">W</span></em><span class="koboSpan" id="kobo.1305.1">, and </span><em class="italic"><span class="koboSpan" id="kobo.1306.1">y</span></em><span class="koboSpan" id="kobo.1307.1"> are scalar values (</span><strong class="bold"><span class="koboSpan" id="kobo.1308.1">x</span></strong><span class="koboSpan" id="kobo.1309.1"> remains a vector), we won’t use the matrix notation (bold capital letters) in the RNN implementation and training section and its subsections. </span><span class="koboSpan" id="kobo.1309.2">We’ll use italic notation instead. </span><span class="koboSpan" id="kobo.1309.3">In the code sections, we’ll denote them as variables. </span><span class="koboSpan" id="kobo.1309.4">However, note that the generic versions of these formulas use matrix and </span><span class="No-Break"><span class="koboSpan" id="kobo.1310.1">vector parameters.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1311.1">The RNN</span><a id="_idIndexMarker904"/><span class="koboSpan" id="kobo.1312.1"> will have only two parameters: an input</span><a id="_idIndexMarker905"/><span class="koboSpan" id="kobo.1313.1"> weight, </span><em class="italic"><span class="koboSpan" id="kobo.1314.1">U</span></em><span class="koboSpan" id="kobo.1315.1">, and a recurrence weight, </span><em class="italic"><span class="koboSpan" id="kobo.1316.1">W</span></em><span class="koboSpan" id="kobo.1317.1">. </span><span class="koboSpan" id="kobo.1317.2">The output weight, </span><em class="italic"><span class="koboSpan" id="kobo.1318.1">V</span></em><span class="koboSpan" id="kobo.1319.1">, is set to 1 so that we just read out the last state as the </span><span class="No-Break"><span class="koboSpan" id="kobo.1320.1">output, </span></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1321.1">y</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1322.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1323.1">First, let’s add some code so that our example can be executed. </span><span class="koboSpan" id="kobo.1323.2">We’ll import numpy and define our training set – inputs, </span><strong class="bold"><span class="koboSpan" id="kobo.1324.1">x</span></strong><span class="koboSpan" id="kobo.1325.1">, and labels, </span><em class="italic"><span class="koboSpan" id="kobo.1326.1">y</span></em><span class="koboSpan" id="kobo.1327.1">. </span><strong class="bold"><span class="koboSpan" id="kobo.1328.1">x</span></strong><span class="koboSpan" id="kobo.1329.1"> is two-dimensional since the first dimension represents the sample in the mini-batch. </span><em class="italic"><span class="koboSpan" id="kobo.1330.1">y</span></em><span class="koboSpan" id="kobo.1331.1"> is a single numerical value (it still has a batch dimension). </span><span class="koboSpan" id="kobo.1331.2">For the sake of simplicity, we’ll use a mini-batch with a </span><span class="No-Break"><span class="koboSpan" id="kobo.1332.1">single sample:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1333.1">
import numpy as np
# The first dimension represents the mini-batch
x = np.array([[0, 0, 0, 0, 1, 0, 1, 0, 1, 0]])
y = np.array([3])</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1334.1">The recurrence relation defined by this RNN is </span><span class="koboSpan" id="kobo.1335.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mi&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/477.png" style="vertical-align:-0.390em;height:1.101em;width:7.790em"/></span><span class="koboSpan" id="kobo.1336.1">. </span><span class="koboSpan" id="kobo.1336.2">Note that this is a linear model since we don’t apply a non-linear function in this formula. </span><span class="koboSpan" id="kobo.1336.3">We can implement a recurrence relationship in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1337.1">following way:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1338.1">
def step(s_t, x_t, U, W):
    return x_t * U + s_t * W</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1339.1">The states, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1340.1">s_t</span></strong><span class="koboSpan" id="kobo.1341.1">, and the weights, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1342.1">W</span></strong><span class="koboSpan" id="kobo.1343.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1344.1">U</span></strong><span class="koboSpan" id="kobo.1345.1">, are single scalar values. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1346.1">x_t</span></strong><span class="koboSpan" id="kobo.1347.1"> represents a single element of the input sequence (in our case, one </span><span class="No-Break"><span class="koboSpan" id="kobo.1348.1">or zero).</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1349.1">Note</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1350.1">One solution to this task is to just get the sum of the elements of the input sequence. </span><span class="koboSpan" id="kobo.1350.2">If we set </span><strong class="source-inline"><span class="koboSpan" id="kobo.1351.1">U=1</span></strong><span class="koboSpan" id="kobo.1352.1">, then whenever input is received, we will get its full value. </span><span class="koboSpan" id="kobo.1352.2">If we set </span><strong class="source-inline"><span class="koboSpan" id="kobo.1353.1">W=1</span></strong><span class="koboSpan" id="kobo.1354.1">, then the value we would accumulate would never decay. </span><span class="koboSpan" id="kobo.1354.2">So, for this example, we would get the desired output: 3. </span><span class="koboSpan" id="kobo.1354.3">Nevertheless, let’s use this simple example to explain the training and implementation of the RNN. </span><span class="koboSpan" id="kobo.1354.4">This will be interesting, as we will see in the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.1355.1">this section.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1356.1">We can think of an RNN</span><a id="_idIndexMarker906"/><span class="koboSpan" id="kobo.1357.1"> as a special type of regular NN </span><a id="_idIndexMarker907"/><span class="koboSpan" id="kobo.1358.1">by unfolding it through time for a certain number of time steps (as illustrated in the preceding diagram). </span><span class="koboSpan" id="kobo.1358.2">This regular NN has as many hidden layers as the size of the elements of the input sequence. </span><span class="koboSpan" id="kobo.1358.3">In other words, one hidden layer represents one step through time. </span><span class="koboSpan" id="kobo.1358.4">The only difference is that each layer has multiple inputs: the previous state, </span><span class="koboSpan" id="kobo.1359.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/478.png" style="vertical-align:-0.340em;height:0.788em;width:1.126em"/></span><span class="koboSpan" id="kobo.1360.1">, and the current input, </span><span class="koboSpan" id="kobo.1361.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/479.png" style="vertical-align:-0.340em;height:0.788em;width:0.640em"/></span><span class="koboSpan" id="kobo.1362.1">. </span><span class="koboSpan" id="kobo.1362.2">The parameters, </span><em class="italic"><span class="koboSpan" id="kobo.1363.1">U</span></em><span class="koboSpan" id="kobo.1364.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1365.1">W</span></em><span class="koboSpan" id="kobo.1366.1">, are shared between all of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1367.1">hidden layers.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1368.1">The forward pass unfolds the RNN along the sequence and builds a stack of states for each step. </span><span class="koboSpan" id="kobo.1368.2">In the following code block, we can see an implementation of the forward pass, which returns the activation, </span><em class="italic"><span class="koboSpan" id="kobo.1369.1">s</span></em><span class="koboSpan" id="kobo.1370.1">, for each recurrent step and each sample in </span><span class="No-Break"><span class="koboSpan" id="kobo.1371.1">the batch:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1372.1">
def forward(x, U, W):
    # Number of samples in the mini-batch
    number_of_samples = len(x)
    # Length of each sample
    sequence_length = len(x[0])
    # Initialize the state activation for each sample along the sequence
    s = np.zeros((number_of_samples, sequence_length + 1))
    # Update the states over the sequence
    for t in range(0, sequence_length):
        s[:, t + 1] = step(s[:, t], x[:, t], U, W)  # step function
    return s</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1373.1">Now that we have</span><a id="_idIndexMarker908"/><span class="koboSpan" id="kobo.1374.1"> the RNN</span><a id="_idIndexMarker909"/><span class="koboSpan" id="kobo.1375.1"> forward pass, let’s look at how to train our </span><span class="No-Break"><span class="koboSpan" id="kobo.1376.1">unfolded RNN.</span></span></p>
<h2 id="_idParaDest-122" lang="en-GB"><a id="_idTextAnchor196"/><span class="koboSpan" id="kobo.1377.1">Backpropagation through time</span></h2>
<p lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1378.1">Backpropagation through time</span></strong><span class="koboSpan" id="kobo.1379.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1380.1">BPTT</span></strong><span class="koboSpan" id="kobo.1381.1">) is the typical algorithm we use to train RNNs (</span><em class="italic"><span class="koboSpan" id="kobo.1382.1">Backpropagation Through Time: What It Does and How to Do It</span></em><span class="koboSpan" id="kobo.1383.1">, </span><a href="http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf"><span class="koboSpan" id="kobo.1384.1">http://axon.cs.byu.edu/~martinez/classes/678/Papers/Werbos_BPTT.pdf</span></a><span class="koboSpan" id="kobo.1385.1">). </span><span class="koboSpan" id="kobo.1385.2">As its name suggests, it’s an adaptation</span><a id="_idIndexMarker910"/><span class="koboSpan" id="kobo.1386.1"> of the backpropagation algorithm</span><a id="_idIndexMarker911"/><span class="koboSpan" id="kobo.1387.1"> we discussed in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1388.1">Chapter 2</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1389.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1390.1">Let’s assume that we’ll use the </span><strong class="bold"><span class="koboSpan" id="kobo.1391.1">mean squared error</span></strong><span class="koboSpan" id="kobo.1392.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1393.1">MSE</span></strong><span class="koboSpan" id="kobo.1394.1">) cost function. </span><span class="koboSpan" id="kobo.1394.2">Now that we also have our forward step</span><a id="_idIndexMarker912"/><span class="koboSpan" id="kobo.1395.1"> implementation, we can define how the gradient is propagated backward. </span><span class="koboSpan" id="kobo.1395.2">Since the unfolded RNN is equivalent to a regular feedforward NN, we can use the backpropagation chain rule we introduced in </span><a href="B19627_02.xhtml#_idTextAnchor047"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1396.1">Chapter 2</span></em></span></a><span class="No-Break"><span class="koboSpan" id="kobo.1397.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1398.1">Because the weights, </span><em class="italic"><span class="koboSpan" id="kobo.1399.1">W</span></em><span class="koboSpan" id="kobo.1400.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1401.1">U</span></em><span class="koboSpan" id="kobo.1402.1">, are shared across the layers, we’ll accumulate the error derivatives for each recurrent step, and in the end, we’ll update the weights with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1403.1">accumulated value.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1404.1">First, we need to get the gradient of the output, </span><span class="koboSpan" id="kobo.1405.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/480.png" style="vertical-align:-0.340em;height:0.788em;width:0.545em"/></span><span class="koboSpan" id="kobo.1406.1">, concerning the loss function, </span><em class="italic"><span class="koboSpan" id="kobo.1407.1">J</span></em><span class="koboSpan" id="kobo.1408.1">, </span><em class="italic"><span class="koboSpan" id="kobo.1409.1">∂J/∂s</span></em><span class="koboSpan" id="kobo.1410.1">. </span><span class="koboSpan" id="kobo.1410.2">Once we have it, we’ll propagate it backward through the stack of activities we built during the forward step. </span><span class="koboSpan" id="kobo.1410.3">This backward pass pops activities off of the stack to accumulate their error derivatives at each time step. </span><span class="koboSpan" id="kobo.1410.4">The recurrence relation that propagates this gradient through the RNN can be written as follows (</span><span class="No-Break"><span class="koboSpan" id="kobo.1411.1">chain rule):</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1412.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/481.png" style="vertical-align:-0.790em;height:2.107em;width:9.218em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1413.1">The gradients of the weights, </span><em class="italic"><span class="koboSpan" id="kobo.1414.1">U</span></em><span class="koboSpan" id="kobo.1415.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1416.1">W</span></em><span class="koboSpan" id="kobo.1417.1">, are accumulated </span><span class="No-Break"><span class="koboSpan" id="kobo.1418.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1419.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/482.png" style="vertical-align:-0.764em;height:2.074em;width:5.606em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1420.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∑&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/483.png" style="vertical-align:-0.764em;height:2.074em;width:6.298em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1421.1">Armed with this</span><a id="_idIndexMarker913"/><span class="koboSpan" id="kobo.1422.1"> knowledge, let’s implement</span><a id="_idIndexMarker914"/><span class="koboSpan" id="kobo.1423.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.1424.1">backward pass:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1425.1">Accumulate the gradients for </span><strong class="source-inline"><span class="koboSpan" id="kobo.1426.1">U</span></strong><span class="koboSpan" id="kobo.1427.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1428.1">W</span></strong><span class="koboSpan" id="kobo.1429.1"> in </span><strong class="source-inline"><span class="koboSpan" id="kobo.1430.1">gU</span></strong><span class="koboSpan" id="kobo.1431.1"> and </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1432.1">gW</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1433.1">, respectively:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1434.1">
def backward(x, s, y, W):
    sequence_length = len(x[0])
    # The network output is just the last activation of sequence
    s_t = s[:, -1]
    # Compute the gradient of the output w.r.t. </span><span class="koboSpan" id="kobo.1434.2">MSE cost function at final state
    gS = 2 * (s_t - y)
    # Set the gradient accumulations to 0
    gU, gW = 0, 0
    # Accumulate gradients backwards
    for k in range(sequence_length, 0, -1):
        # Compute the parameter gradients and accumulate the results.
</span><span class="koboSpan" id="kobo.1434.3">        gU += np.sum(gS * x[:, k - 1])
        gW += np.sum(gS * s[:, k - 1])
        # Compute the gradient at the output of the previous layer
        gS = gS * W
    return gU, gW</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1435.1">Use gradient descent to optimize</span><a id="_idIndexMarker915"/><span class="koboSpan" id="kobo.1436.1"> our RNN. </span><span class="koboSpan" id="kobo.1436.2">Compute the gradients (using MSE) with the help</span><a id="_idIndexMarker916"/><span class="koboSpan" id="kobo.1437.1"> of the backward function and use them to update the </span><span class="No-Break"><span class="koboSpan" id="kobo.1438.1">weights value:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1439.1">
def train(x, y, epochs, learning_rate=0.0005):
    # Set initial parameters
    weights = (-2, 0)  # (U, W)
    # Accumulate the losses and their respective weights
    losses, gradients_u, gradients_w = list(), list(), list()
    # Perform iterative gradient descent
    for i in range(epochs):
        # Perform forward and backward pass to get the gradients
        s = forward(x, weights[0], weights[1])
        # Compute the loss
        loss = (y[0] - s[-1, -1]) ** 2
        # Store the loss and weights values for later display
        losses.append(loss)
        gradients = backward(x, s, y, weights[1])
        gradients_u.append(gradients[0])
        gradients_w.append(gradients[1])
        # Update each parameter `p` by p = p - (gradient * learning_rate).
</span><span class="koboSpan" id="kobo.1439.2">        # `gp` is the gradient of parameter `p`
        weights = tuple((p - gp * learning_rate) for p, gp in zip(weights, gradients))
    return np.array(losses), np.array(gradients_u), np.array(gradients_w)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1440.1">Run the</span><a id="_idIndexMarker917"/><span class="koboSpan" id="kobo.1441.1"> training</span><a id="_idIndexMarker918"/><span class="koboSpan" id="kobo.1442.1"> for </span><span class="No-Break"><span class="koboSpan" id="kobo.1443.1">150 epochs:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1444.1">
losses, gradients_u, gradients_w = train(x, y,
     epochs=150)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1445.1">Finally, display the loss function and the gradients for each weight over the epochs. </span><span class="koboSpan" id="kobo.1445.2">We’ll do this with the help of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1446.1">plot_training</span></strong><span class="koboSpan" id="kobo.1447.1"> function, which is not implemented here but is available in the full example on GitHub. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1448.1">plot_training</span></strong><span class="koboSpan" id="kobo.1449.1"> produces the </span><span class="No-Break"><span class="koboSpan" id="kobo.1450.1">following graph:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer614">
<span class="koboSpan" id="kobo.1451.1"><img alt="Figure 6.10 – The RNN loss – uninterrupted line  – loss value; dashed lines – the weight gradients during training" src="image/B19627_06_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1452.1">Figure 6.10 – The RNN loss – uninterrupted line  – loss value; dashed lines – the weight gradients during training</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1453.1">Now that we’ve learned</span><a id="_idIndexMarker919"/><span class="koboSpan" id="kobo.1454.1"> about backpropagation</span><a id="_idIndexMarker920"/><span class="koboSpan" id="kobo.1455.1"> through time, let’s discuss how the familiar vanishing and exploding gradient problems </span><span class="No-Break"><span class="koboSpan" id="kobo.1456.1">affect it.</span></span></p>
<h2 id="_idParaDest-123" lang="en-GB"><a id="_idTextAnchor197"/><span class="koboSpan" id="kobo.1457.1">Vanishing and exploding gradients</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1458.1">The preceding</span><a id="_idIndexMarker921"/><span class="koboSpan" id="kobo.1459.1"> example</span><a id="_idIndexMarker922"/><span class="koboSpan" id="kobo.1460.1"> has an issue. </span><span class="koboSpan" id="kobo.1460.2">To illustrate</span><a id="_idIndexMarker923"/><span class="koboSpan" id="kobo.1461.1"> it, let’s run</span><a id="_idIndexMarker924"/><span class="koboSpan" id="kobo.1462.1"> the training process with a </span><span class="No-Break"><span class="koboSpan" id="kobo.1463.1">longer sequence:</span></span></p>
<pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1464.1">
x = np.array([[0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0]])
y = np.array([12])
losses, gradients_u, gradients_w = train(x, y, epochs=150)
plot_training(losses, gradients_u, gradients_w)</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1465.1">The output is </span><span class="No-Break"><span class="koboSpan" id="kobo.1466.1">as follows:</span></span></p>
<pre class="console" lang="en-GB"><span class="koboSpan" id="kobo.1467.1">
RuntimeWarning: overflow encountered in multiply
  return x * U + s * W
RuntimeWarning: invalid value encountered in multiply
  gU += np.sum(gS * x[:, k - 1])
RuntimeWarning: invalid value encountered in multiply
  gW += np.sum(gS * s[:, k - 1])</span></pre>
<p lang="en-GB"><span class="koboSpan" id="kobo.1468.1">The reason</span><a id="_idIndexMarker925"/><span class="koboSpan" id="kobo.1469.1"> for these warnings is that the final</span><a id="_idIndexMarker926"/><span class="koboSpan" id="kobo.1470.1"> parameters, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1471.1">U</span></strong><span class="koboSpan" id="kobo.1472.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1473.1">W</span></strong><span class="koboSpan" id="kobo.1474.1">, end up as </span><strong class="bold"><span class="koboSpan" id="kobo.1475.1">Not a Number</span></strong><span class="koboSpan" id="kobo.1476.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1477.1">NaN</span></strong><span class="koboSpan" id="kobo.1478.1">). </span><span class="koboSpan" id="kobo.1478.2">To display the gradients</span><a id="_idIndexMarker927"/><span class="koboSpan" id="kobo.1479.1"> properly, we’ll need</span><a id="_idIndexMarker928"/><span class="koboSpan" id="kobo.1480.1"> to change the scale of the gradient axis in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1481.1">plot_training</span></strong><span class="koboSpan" id="kobo.1482.1"> function</span><a id="_idIndexMarker929"/><span class="koboSpan" id="kobo.1483.1"> to produce the </span><span class="No-Break"><span class="koboSpan" id="kobo.1484.1">following result:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer615">
<span class="koboSpan" id="kobo.1485.1"><img alt="Figure 6.11 – Parameters and loss function during an exploding gradients scenario" src="image/B19627_06_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1486.1">Figure 6.11 – Parameters and loss function during an exploding gradients scenario</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1487.1">In the initial epochs, the gradients slowly increase, similar to the way they increased for the shorter sequence. </span><span class="koboSpan" id="kobo.1487.2">However, when they get to epoch 23 (the exact epoch is unimportant, though), the gradient becomes so large that it goes out of the range of the float variable and becomes NaN (as illustrated by the jump in the plot). </span><span class="koboSpan" id="kobo.1487.3">This problem is known as </span><strong class="bold"><span class="koboSpan" id="kobo.1488.1">exploding gradients</span></strong><span class="koboSpan" id="kobo.1489.1">. </span><span class="koboSpan" id="kobo.1489.2">We can stumble upon exploding gradients in a regular feedforward</span><a id="_idIndexMarker930"/><span class="koboSpan" id="kobo.1490.1"> NN, but it is especially pronounced in RNNs. </span><span class="koboSpan" id="kobo.1490.2">To understand why, let’s recall the recurrent gradient propagation chain rule for the two consecutive sequence steps we defined in the </span><em class="italic"><span class="koboSpan" id="kobo.1491.1">Backpropagation through </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1492.1">time</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1493.1"> section:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1494.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;J&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/481.png" style="vertical-align:-0.790em;height:2.107em;width:9.218em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1495.1">Depending on the sequence’s length, an unfolded RNN</span><a id="_idIndexMarker931"/><span class="koboSpan" id="kobo.1496.1"> can be much deeper</span><a id="_idIndexMarker932"/><span class="koboSpan" id="kobo.1497.1"> compared to a regular NN. </span><span class="koboSpan" id="kobo.1497.2">At the same </span><a id="_idIndexMarker933"/><span class="koboSpan" id="kobo.1498.1">time, the weights, </span><em class="italic"><span class="koboSpan" id="kobo.1499.1">W</span></em><span class="koboSpan" id="kobo.1500.1">, of an RNN are shared</span><a id="_idIndexMarker934"/><span class="koboSpan" id="kobo.1501.1"> across all of the steps. </span><span class="koboSpan" id="kobo.1501.2">Therefore, we can generalize this formula to compute the gradient between two non-consecutive steps of the sequence. </span><span class="koboSpan" id="kobo.1501.3">Because </span><em class="italic"><span class="koboSpan" id="kobo.1502.1">W</span></em><span class="koboSpan" id="kobo.1503.1"> is shared, the equation forms a </span><span class="No-Break"><span class="koboSpan" id="kobo.1504.1">geometric progression:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1505.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∏&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;s&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/485.png" style="vertical-align:-0.914em;height:2.426em;width:15.085em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1506.1">In our simple linear RNN, the gradient grows exponentially if </span><em class="italic"><span class="koboSpan" id="kobo.1507.1">|W|&gt;1</span></em><span class="koboSpan" id="kobo.1508.1"> (exploding gradient), where </span><em class="italic"><span class="koboSpan" id="kobo.1509.1">W</span></em><span class="koboSpan" id="kobo.1510.1"> is a single scalar weight – for example, 50 time steps over </span><em class="italic"><span class="koboSpan" id="kobo.1511.1">W=1.5</span></em><span class="koboSpan" id="kobo.1512.1"> is </span><span class="koboSpan" id="kobo.1513.1"><img alt="&lt;math xmlns=&quot;http://www.w3.org/1998/Math/MathML&quot;&gt;&lt;mrow&gt;&lt;mrow&gt;&lt;msup&gt;&lt;mi&gt;W&lt;/mi&gt;&lt;mrow&gt;&lt;mo&gt;(&lt;/mo&gt;&lt;mn&gt;50&lt;/mn&gt;&lt;mo&gt;)&lt;/mo&gt;&lt;/mrow&gt;&lt;/msup&gt;&lt;mo&gt;≈&lt;/mo&gt;&lt;mn&gt;637&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;﻿&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;621&lt;/mn&gt;&lt;mi mathvariant=&quot;normal&quot;&gt;﻿&lt;/mi&gt;&lt;mo&gt;,&lt;/mo&gt;&lt;mn&gt;500&lt;/mn&gt;&lt;/mrow&gt;&lt;/mrow&gt;&lt;/math&gt;" src="image/486.png" style="vertical-align:-0.117em;height:0.820em;width:8.152em"/></span><span class="koboSpan" id="kobo.1514.1">. </span><span class="koboSpan" id="kobo.1514.2">The gradient shrinks exponentially if </span><em class="italic"><span class="koboSpan" id="kobo.1515.1">|W|&lt;1</span></em><span class="koboSpan" id="kobo.1516.1"> (vanishing gradient), for example, 10 time steps over </span><em class="italic"><span class="koboSpan" id="kobo.1517.1">W=0.6</span></em><span class="koboSpan" id="kobo.1518.1"> is </span><span class="koboSpan" id="kobo.1519.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;(&lt;/mml:mo&gt;&lt;mml:mn&gt;10&lt;/mml:mn&gt;&lt;mml:mo&gt;)&lt;/mml:mo&gt;&lt;/mml:mrow&gt;&lt;/mml:msup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0.00097&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/487.png" style="vertical-align:-0.015em;height:0.718em;width:6.412em"/></span><span class="koboSpan" id="kobo.1520.1">. </span><span class="koboSpan" id="kobo.1520.2">If the weight parameter, </span><em class="italic"><span class="koboSpan" id="kobo.1521.1">W</span></em><span class="koboSpan" id="kobo.1522.1">, is a matrix instead of a scalar, this exploding </span><a id="_idIndexMarker935"/><span class="koboSpan" id="kobo.1523.1">or vanishing gradient is related to the largest eigenvalue, </span><em class="italic"><span class="koboSpan" id="kobo.1524.1">ρ</span></em><span class="koboSpan" id="kobo.1525.1">, of </span><em class="italic"><span class="koboSpan" id="kobo.1526.1">W</span></em><span class="koboSpan" id="kobo.1527.1"> (also known as a spectral radius). </span><span class="koboSpan" id="kobo.1527.2">It is sufficient for </span><em class="italic"><span class="koboSpan" id="kobo.1528.1">ρ&lt;1</span></em><span class="koboSpan" id="kobo.1529.1"> for the gradients to vanish, and it is necessary for </span><em class="italic"><span class="koboSpan" id="kobo.1530.1">ρ&gt;1</span></em><span class="koboSpan" id="kobo.1531.1"> for them </span><span class="No-Break"><span class="koboSpan" id="kobo.1532.1">to explode.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1533.1">The vanishing gradients problem, which we first mentioned in </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1534.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.1535.1">, has another more subtle effect in RNNs: the gradient decays exponentially over the number of steps to a point where it becomes extremely small in the earlier states. </span><span class="koboSpan" id="kobo.1535.2">In effect, they are overshadowed by the larger gradients from more recent time steps, and the RNN’s ability to retain the history of these earlier states vanishes. </span><span class="koboSpan" id="kobo.1535.3">This problem is harder to detect because the training will still work, and the NN will produce valid outputs (unlike with exploding gradients). </span><span class="koboSpan" id="kobo.1535.4">It just won’t be able to learn </span><span class="No-Break"><span class="koboSpan" id="kobo.1536.1">long-term dependencies.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1537.1">With that, we are familiar</span><a id="_idIndexMarker936"/><span class="koboSpan" id="kobo.1538.1"> with some of the problems</span><a id="_idIndexMarker937"/><span class="koboSpan" id="kobo.1539.1"> surrounding RNNs. </span><span class="koboSpan" id="kobo.1539.2">This knowledge will serve</span><a id="_idIndexMarker938"/><span class="koboSpan" id="kobo.1540.1"> us well because, in the next section, we’ll discuss how to solve these problems with</span><a id="_idIndexMarker939"/><span class="koboSpan" id="kobo.1541.1"> the help of a special type of </span><span class="No-Break"><span class="koboSpan" id="kobo.1542.1">RNN cell.</span></span></p>
<h2 id="_idParaDest-124" lang="en-GB"><a id="_idTextAnchor198"/><span class="koboSpan" id="kobo.1543.1">Long-short term memory</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1544.1">Hochreiter and Schmidhuber</span><a id="_idIndexMarker940"/><span class="koboSpan" id="kobo.1545.1"> studied the problems of vanishing</span><a id="_idIndexMarker941"/><span class="koboSpan" id="kobo.1546.1"> and exploding gradients extensively and came up with a solution called </span><strong class="bold"><span class="koboSpan" id="kobo.1547.1">long short-term memory</span></strong><span class="koboSpan" id="kobo.1548.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1549.1">LSTM</span></strong><span class="koboSpan" id="kobo.1550.1"> – </span><a href="https://www.bioinf.jku.at/publications/older/2604.pdf"><span class="koboSpan" id="kobo.1551.1">https://www.bioinf.jku.at/publications/older/2604.pdf</span></a><span class="koboSpan" id="kobo.1552.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1553.1">Learning to Forget: Continual Prediction with LSTM</span></em><span class="koboSpan" id="kobo.1554.1">, https://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.55.5709&amp;rep=rep1&amp;type=pdf). </span><span class="koboSpan" id="kobo.1554.2">LSTMs can handle long-term dependencies due to a specially crafted memory cell. </span><span class="koboSpan" id="kobo.1554.3">They work so well that most of the current accomplishments in training RNNs on a variety of problems are due to the use of LSTMs. </span><span class="koboSpan" id="kobo.1554.4">In this section, we’ll explore how this memory cell works and how it solves the vanishing </span><span class="No-Break"><span class="koboSpan" id="kobo.1555.1">gradients issue.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1556.1">The following is a diagram of an </span><span class="No-Break"><span class="koboSpan" id="kobo.1557.1">LSTM cell:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer620">
<span class="koboSpan" id="kobo.1558.1"><img alt="Figure 6.12 – LSTM cell (top); unfolded LSTM cell (bottom). Inspired by http://colah.github.io/posts/2015-08-Understanding-LSTMs/" src="image/B19627_06_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1559.1">Figure 6.12 – LSTM cell (top); unfolded LSTM cell (bottom). </span><span class="koboSpan" id="kobo.1559.2">Inspired by </span><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/"><span class="koboSpan" id="kobo.1560.1">http://colah.github.io/posts/2015-08-Understanding-LSTMs/</span></a></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1561.1">The key idea of LSTM</span><a id="_idIndexMarker942"/><span class="koboSpan" id="kobo.1562.1"> is the cell state, </span><span class="koboSpan" id="kobo.1563.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/488.png" style="vertical-align:-0.340em;height:0.793em;width:0.609em"/></span><span class="koboSpan" id="kobo.1564.1"> (in addition to the hidden RNN state, </span><span class="koboSpan" id="kobo.1565.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/489.png" style="vertical-align:-0.340em;height:1.042em;width:0.733em"/></span><span class="koboSpan" id="kobo.1566.1">), where</span><a id="_idIndexMarker943"/><span class="koboSpan" id="kobo.1567.1"> the information can only be explicitly written in or removed so that the state stays constant if there is no outside interference. </span><span class="koboSpan" id="kobo.1567.2">The cell state can only be modified by specific</span><a id="_idIndexMarker944"/><span class="koboSpan" id="kobo.1568.1"> gates, which</span><a id="_idIndexMarker945"/><span class="koboSpan" id="kobo.1569.1"> are a way to let</span><a id="_idIndexMarker946"/><span class="koboSpan" id="kobo.1570.1"> information pass through. </span><span class="koboSpan" id="kobo.1570.2">A typical LSTM is composed of three gates: a </span><strong class="bold"><span class="koboSpan" id="kobo.1571.1">forget gate</span></strong><span class="koboSpan" id="kobo.1572.1">, an </span><strong class="bold"><span class="koboSpan" id="kobo.1573.1">input gate</span></strong><span class="koboSpan" id="kobo.1574.1">, and an </span><strong class="bold"><span class="koboSpan" id="kobo.1575.1">output gate</span></strong><span class="koboSpan" id="kobo.1576.1">. </span><span class="koboSpan" id="kobo.1576.2">The cell state, input, and output are all vectors so that the LSTM can hold a combination of different information blocks at each </span><span class="No-Break"><span class="koboSpan" id="kobo.1577.1">time step.</span></span></p>
<p class="callout-heading" lang="en-GB"><span class="koboSpan" id="kobo.1578.1">LSTM notations</span></p>
<p class="callout" lang="en-GB"><span class="koboSpan" id="kobo.1579.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/490.png" style="vertical-align:-0.340em;height:0.781em;width:0.715em"/></span><span class="koboSpan" id="kobo.1580.1">, </span><span class="koboSpan" id="kobo.1581.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/491.png" style="vertical-align:-0.340em;height:0.793em;width:0.655em"/></span><span class="koboSpan" id="kobo.1582.1">, and </span><span class="koboSpan" id="kobo.1583.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/492.png" style="vertical-align:-0.340em;height:1.042em;width:0.783em"/></span><span class="koboSpan" id="kobo.1584.1"> are the LSTM’s input, cell</span><a id="_idIndexMarker947"/><span class="koboSpan" id="kobo.1585.1"> memory state, and output (or hidden state) vectors in moment </span><em class="italic"><span class="koboSpan" id="kobo.1586.1">t</span></em><span class="koboSpan" id="kobo.1587.1">. </span><span class="koboSpan" id="kobo.1588.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/493.png" style="vertical-align:-0.340em;height:1.018em;width:0.709em"/></span><span class="koboSpan" id="kobo.1589.1"> is the candidate cell state vector (more on that later). </span><span class="koboSpan" id="kobo.1589.2">The input, </span><span class="koboSpan" id="kobo.1590.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/494.png" style="vertical-align:-0.340em;height:0.781em;width:0.676em"/></span><span class="koboSpan" id="kobo.1591.1">, and the previous cell output, </span><span class="koboSpan" id="kobo.1592.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/495.png" style="vertical-align:-0.340em;height:1.042em;width:1.349em"/></span><span class="koboSpan" id="kobo.1593.1">, are connected to each gate and the candidate cell vector with sets of FC weights, </span><strong class="bold"><span class="koboSpan" id="kobo.1594.1">W</span></strong><span class="koboSpan" id="kobo.1595.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.1596.1">U</span></strong><span class="koboSpan" id="kobo.1597.1">, respectively. </span><span class="koboSpan" id="kobo.1598.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/496.png" style="vertical-align:-0.340em;height:1.042em;width:0.552em"/></span><span class="koboSpan" id="kobo.1599.1">, </span><span class="koboSpan" id="kobo.1600.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/497.png" style="vertical-align:-0.340em;height:0.996em;width:0.516em"/></span><span class="koboSpan" id="kobo.1601.1">, and </span><span class="koboSpan" id="kobo.1602.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/498.png" style="vertical-align:-0.340em;height:0.793em;width:0.774em"/></span><span class="koboSpan" id="kobo.1603.1"> are the forget, input, and output gates</span><a id="_idIndexMarker948"/><span class="koboSpan" id="kobo.1604.1"> of the LSTM cell (the gates use vector notation </span><span class="No-Break"><span class="koboSpan" id="kobo.1605.1">as well).</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1606.1">The gates are composed of FC</span><a id="_idIndexMarker949"/><span class="koboSpan" id="kobo.1607.1"> layers, sigmoid</span><a id="_idIndexMarker950"/><span class="koboSpan" id="kobo.1608.1"> activations, and element-wise multiplication (denoted with </span><span class="koboSpan" id="kobo.1609.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;/mml:math&gt;" src="image/499.png" style="vertical-align:-0.259em;height:1.021em;width:1.103em"/></span><span class="koboSpan" id="kobo.1610.1">). </span><span class="koboSpan" id="kobo.1610.2">Because the sigmoid only outputs values between 0 and 1, the multiplication can only reduce the value running through the gate. </span><span class="koboSpan" id="kobo.1610.3">Let’s discuss them </span><span class="No-Break"><span class="koboSpan" id="kobo.1611.1">in order:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1612.1">Forget gate</span></strong><span class="koboSpan" id="kobo.1613.1">, </span><span class="koboSpan" id="kobo.1614.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/500.png" style="vertical-align:-0.340em;height:1.042em;width:0.540em"/></span><span class="koboSpan" id="kobo.1615.1">: It decides whether we want to erase</span><a id="_idIndexMarker951"/><span class="koboSpan" id="kobo.1616.1"> parts of the existing cell state or not. </span><span class="koboSpan" id="kobo.1616.2">It bases its decision on the weighted vector sum of the output of the previous cell, </span><span class="koboSpan" id="kobo.1617.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/501.png" style="vertical-align:-0.340em;height:1.042em;width:1.386em"/></span><span class="koboSpan" id="kobo.1618.1">, and the current </span><span class="No-Break"><span class="koboSpan" id="kobo.1619.1">input, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1620.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/352.png" style="vertical-align:-0.340em;height:0.781em;width:0.710em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1621.1">:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1622.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/503.png" style="vertical-align:-0.533em;height:1.285em;width:9.073em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1623.1">From the preceding formula, we can see that the forget gate applies element-wise sigmoid activations to each element of the previous state vector, </span><span class="koboSpan" id="kobo.1624.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/504.png" style="vertical-align:-0.340em;height:0.793em;width:1.203em"/></span><span class="koboSpan" id="kobo.1625.1">: </span><span class="koboSpan" id="kobo.1626.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/505.png" style="vertical-align:-0.340em;height:1.102em;width:2.857em"/></span><span class="koboSpan" id="kobo.1627.1"> (note the circle-dot notation). </span><span class="koboSpan" id="kobo.1627.2">Since the operation is elementwise, the values of this vector are squashed in the [0, 1] range. </span><span class="koboSpan" id="kobo.1627.3">An output of 0 erases a specific </span><span class="koboSpan" id="kobo.1628.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;,&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/506.png" style="vertical-align:-0.483em;height:0.931em;width:1.416em"/></span><span class="koboSpan" id="kobo.1629.1"> cell block completely and an output of 1 allows the information in that cell block to pass through. </span><span class="koboSpan" id="kobo.1629.2">In this way, the LSTM can get rid of irrelevant information in its cell </span><span class="No-Break"><span class="koboSpan" id="kobo.1630.1">state vector.</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1631.1">Input gate</span></strong><span class="koboSpan" id="kobo.1632.1">, </span><span class="koboSpan" id="kobo.1633.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/507.png" style="vertical-align:-0.340em;height:0.996em;width:0.540em"/></span><span class="koboSpan" id="kobo.1634.1">: It decides what new information is going</span><a id="_idIndexMarker952"/><span class="koboSpan" id="kobo.1635.1"> to be added to the memory cell in a multi-step process. </span><span class="koboSpan" id="kobo.1635.2">The first step determines whether any information is going to be added. </span><span class="koboSpan" id="kobo.1635.3">As in the forget gate, its decision is based on </span><span class="koboSpan" id="kobo.1636.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/508.png" style="vertical-align:-0.340em;height:1.042em;width:1.454em"/></span><span class="koboSpan" id="kobo.1637.1"> and </span><span class="koboSpan" id="kobo.1638.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/352.png" style="vertical-align:-0.340em;height:0.781em;width:0.710em"/></span><span class="koboSpan" id="kobo.1639.1">: it outputs 0 or 1 through the sigmoid function for each cell of the candidate state vector. </span><span class="koboSpan" id="kobo.1639.2">An output of 0 means that no information is added to that cell block’s memory. </span><span class="koboSpan" id="kobo.1639.3">As a result, the LSTM can store specific pieces of information in its cell </span><span class="No-Break"><span class="koboSpan" id="kobo.1640.1">state vector:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1641.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/510.png" style="vertical-align:-0.390em;height:1.142em;width:8.918em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1642.1">In the next step of the input gate sequence, we compute the new candidate cell state, </span><span class="koboSpan" id="kobo.1643.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/511.png" style="vertical-align:-0.340em;height:1.018em;width:0.786em"/></span><span class="koboSpan" id="kobo.1644.1">. </span><span class="koboSpan" id="kobo.1644.2">It is based on the previous output, </span><span class="koboSpan" id="kobo.1645.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/512.png" style="vertical-align:-0.340em;height:1.042em;width:1.299em"/></span><span class="koboSpan" id="kobo.1646.1">, and the current input, </span><span class="koboSpan" id="kobo.1647.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/352.png" style="vertical-align:-0.340em;height:0.781em;width:0.710em"/></span><span class="koboSpan" id="kobo.1648.1">, and is transformed via a </span><span class="No-Break"><span class="koboSpan" id="kobo.1649.1">tanh function:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1650.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/514.png" style="vertical-align:-0.390em;height:1.142em;width:10.611em"/></span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1651.1">Then, we combine </span><span class="koboSpan" id="kobo.1652.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/515.png" style="vertical-align:-0.340em;height:1.018em;width:0.718em"/></span><span class="koboSpan" id="kobo.1653.1"> with the sigmoid outputs of the input gate via element-wise multiplication:  </span><span class="koboSpan" id="kobo.1654.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/516.png" style="vertical-align:-0.340em;height:1.102em;width:2.494em"/></span><span class="koboSpan" id="kobo.1655.1">.</span></p>
<p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1656.1">To recap, the forget and input gates decide what information to forget and include from the previous and candidate cell states, respectively. </span><span class="koboSpan" id="kobo.1656.2">The final version of the new cell</span><a id="_idIndexMarker953"/><span class="koboSpan" id="kobo.1657.1"> state, </span><span class="koboSpan" id="kobo.1658.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/517.png" style="vertical-align:-0.340em;height:0.793em;width:0.645em"/></span><span class="koboSpan" id="kobo.1659.1">, is just an element-wise sum between these </span><span class="No-Break"><span class="koboSpan" id="kobo.1660.1">two components:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1661.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⊕&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;b&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/518.png" style="vertical-align:-0.340em;height:1.102em;width:9.304em"/></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1662.1">Output gate</span></strong><span class="koboSpan" id="kobo.1663.1">, </span><span class="koboSpan" id="kobo.1664.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/519.png" style="vertical-align:-0.340em;height:0.793em;width:0.739em"/></span><span class="koboSpan" id="kobo.1665.1">: It decides what the total cell</span><a id="_idIndexMarker954"/><span class="koboSpan" id="kobo.1666.1"> output is going to be. </span><span class="koboSpan" id="kobo.1666.2">It takes </span><span class="koboSpan" id="kobo.1667.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/520.png" style="vertical-align:-0.340em;height:1.042em;width:1.373em"/></span><span class="koboSpan" id="kobo.1668.1"> and </span><span class="koboSpan" id="kobo.1669.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/352.png" style="vertical-align:-0.340em;height:0.781em;width:0.696em"/></span><span class="koboSpan" id="kobo.1670.1"> as inputs. </span><span class="koboSpan" id="kobo.1670.2">It outputs a value in the (0, 1) range (via the sigmoid function) for each block of the cell’s memory. </span><span class="koboSpan" id="kobo.1670.3">Like before, 0 means that the block doesn’t output any information and 1 means that the block can pass through as a cell’s output. </span><span class="koboSpan" id="kobo.1670.4">Therefore, the LSTM can output specific blocks of information from its cell </span><span class="No-Break"><span class="koboSpan" id="kobo.1671.1">state vector:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1672.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/522.png" style="vertical-align:-0.390em;height:1.142em;width:9.377em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1673.1">Finally, the LSTM</span><a id="_idIndexMarker955"/><span class="koboSpan" id="kobo.1674.1"> cell’s output is transferred</span><a id="_idIndexMarker956"/><span class="koboSpan" id="kobo.1675.1"> by a </span><span class="No-Break"><span class="koboSpan" id="kobo.1676.1">tanh function:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1677.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;t&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;a&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;n&lt;/mml:mi&gt;&lt;mml:mi mathvariant=&quot;normal&quot;&gt;h&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/523.png" style="vertical-align:-0.390em;height:1.152em;width:7.452em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1678.1">Because all these formulas are derivable, we can chain LSTM cells together, just like when we chain simple RNN states together and train the network via backpropagation </span><span class="No-Break"><span class="koboSpan" id="kobo.1679.1">through time.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1680.1">But how does the LSTM protect us from vanishing gradients? </span><span class="koboSpan" id="kobo.1680.2">Let’s start with the forward phase. </span><span class="koboSpan" id="kobo.1680.3">Notice that the cell state is copied identically from step to step if the forget gate is 1 and the input gate is </span><br/><span class="koboSpan" id="kobo.1681.1">0: </span><span class="koboSpan" id="kobo.1682.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⊕&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⊕&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/524.png" style="vertical-align:-0.340em;height:1.102em;width:20.218em"/></span><span class="koboSpan" id="kobo.1683.1">. </span><span class="koboSpan" id="kobo.1683.2">Only the forget gate can completely erase the cell’s memory. </span><span class="koboSpan" id="kobo.1683.3">As a result, the memory can remain unchanged over a long period. </span><span class="koboSpan" id="kobo.1683.4">Also, note that the input is a tanh activation that’s been added to the current cell’s memory. </span><span class="koboSpan" id="kobo.1683.5">This means that the cell’s memory doesn’t blow up and is </span><span class="No-Break"><span class="koboSpan" id="kobo.1684.1">quite stable.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1685.1">Let’s use an example</span><a id="_idIndexMarker957"/><span class="koboSpan" id="kobo.1686.1"> to demonstrate how an LSTM</span><a id="_idIndexMarker958"/><span class="koboSpan" id="kobo.1687.1"> cell is unfolded. </span><span class="koboSpan" id="kobo.1687.2">For the sake of simplicity, we’ll assume that it has one-dimensional (single scalar value) input, state, and output vectors. </span><span class="koboSpan" id="kobo.1687.3">Because the values are scalar, we won’t use vector notation for the rest of </span><span class="No-Break"><span class="koboSpan" id="kobo.1688.1">this example:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer658">
<span class="koboSpan" id="kobo.1689.1"><img alt="Figure 6.13 – Unrolling an LSTM through time" src="image/B19627_06_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1690.1">Figure 6.13 – Unrolling an LSTM through time</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1691.1">The process is </span><span class="No-Break"><span class="koboSpan" id="kobo.1692.1">as follows:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1693.1">First, we have a value of 3 as a candidate state. </span><span class="koboSpan" id="kobo.1693.2">The input gate is set to </span><span class="koboSpan" id="kobo.1694.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/525.png" style="vertical-align:-0.340em;height:1.051em;width:2.369em"/></span><span class="koboSpan" id="kobo.1695.1"> and the forget gate is set to </span><span class="koboSpan" id="kobo.1696.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/526.png" style="vertical-align:-0.340em;height:1.051em;width:2.464em"/></span><span class="koboSpan" id="kobo.1697.1">. </span><span class="koboSpan" id="kobo.1697.2">This means that the previous state, </span><span class="koboSpan" id="kobo.1698.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;/mml:math&gt;" src="image/527.png" style="vertical-align:-0.340em;height:0.988em;width:3.473em"/></span><span class="koboSpan" id="kobo.1699.1">, is erased and replaced with the new </span><span class="No-Break"><span class="koboSpan" id="kobo.1700.1">state, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1701.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:mi&gt;N&lt;/mml:mi&gt;&lt;mml:mo&gt;⊕&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/528.png" style="vertical-align:-0.340em;height:1.102em;width:10.014em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1702.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1703.1">For the next two time steps, the forget gate is set to 1, while the input gate is set to 0. </span><span class="koboSpan" id="kobo.1703.2">By doing this, all the information is kept throughout these steps and no new information is added because the input gate is set to </span><span class="No-Break"><span class="koboSpan" id="kobo.1704.1">0: </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1705.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;mml:mo&gt;⊕&lt;/mml:mo&gt;&lt;mml:mn&gt;0&lt;/mml:mn&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;3&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/529.png" style="vertical-align:-0.340em;height:1.102em;width:11.160em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1706.1">.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1707.1">Finally, the output gate is set to </span><span class="koboSpan" id="kobo.1708.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;o&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:math&gt;" src="image/530.png" style="vertical-align:-0.340em;height:0.974em;width:2.826em"/></span><span class="koboSpan" id="kobo.1709.1"> and 3 is output and remains unchanged. </span><span class="koboSpan" id="kobo.1709.2">We have successfully demonstrated how the internal state is stored across </span><span class="No-Break"><span class="koboSpan" id="kobo.1710.1">multiple steps.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1711.1">Next, let’s focus on the backward phase. </span><span class="koboSpan" id="kobo.1711.2">The cell state, </span><span class="koboSpan" id="kobo.1712.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/531.png" style="vertical-align:-0.340em;height:0.788em;width:0.517em"/></span><span class="koboSpan" id="kobo.1713.1">, can mitigate the vanishing/exploding gradients as well with the help of the forget gate, </span><span class="koboSpan" id="kobo.1714.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/532.png" style="vertical-align:-0.340em;height:1.051em;width:0.395em"/></span><span class="koboSpan" id="kobo.1715.1">. </span><span class="koboSpan" id="kobo.1715.2">Like the regular RNN, we can use the chain rule to compute the partial derivative, </span><span class="koboSpan" id="kobo.1716.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mtext&gt;/&lt;/mml:mtext&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/533.png" style="vertical-align:-0.340em;height:1.049em;width:3.205em"/></span><span class="koboSpan" id="kobo.1717.1">, for two consecutive steps. </span><span class="koboSpan" id="kobo.1717.2">Following the formula </span><span class="koboSpan" id="kobo.1718.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⊕&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;i&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/534.png" style="vertical-align:-0.340em;height:1.102em;width:8.344em"/></span> <br/><span class="koboSpan" id="kobo.1719.1">and without going into details, its partial derivative is </span><span class="No-Break"><span class="koboSpan" id="kobo.1720.1">as follows:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1721.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;≈&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/535.png" style="vertical-align:-0.790em;height:2.107em;width:3.484em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1722.1">We can also generalize this to </span><span class="No-Break"><span class="koboSpan" id="kobo.1723.1">non-consecutive steps:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1724.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;2&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;…&lt;/mml:mo&gt;&lt;mml:mfrac&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mo&gt;∂&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;c&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfrac&gt;&lt;mml:mo&gt;≈&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:munderover&gt;&lt;mml:mo stretchy=&quot;false&quot;&gt;∏&lt;/mml:mo&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;k&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:munderover&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mi&gt;j&lt;/mml:mi&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mrow&gt;&lt;/mml:math&gt;" src="image/536.png" style="vertical-align:-0.914em;height:2.377em;width:13.673em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1725.1">If the forget gate values are close to 1, gradient information can pass back through the network states almost unchanged. </span><span class="koboSpan" id="kobo.1725.2">This is because </span><span class="koboSpan" id="kobo.1726.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/532.png" style="vertical-align:-0.340em;height:1.051em;width:0.409em"/></span><span class="koboSpan" id="kobo.1727.1"> uses sigmoid activation and information flow is still subject to the vanishing gradient that’s specific to sigmoid activations. </span><span class="koboSpan" id="kobo.1727.2">But unlike the gradients </span><a id="_idIndexMarker959"/><span class="koboSpan" id="kobo.1728.1">in the regular RNN, </span><span class="koboSpan" id="kobo.1729.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;f&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/532.png" style="vertical-align:-0.340em;height:1.051em;width:0.399em"/></span><span class="koboSpan" id="kobo.1730.1"> has a different value at each time step. </span><span class="koboSpan" id="kobo.1730.2">Therefore, this is not a geometric</span><a id="_idIndexMarker960"/><span class="koboSpan" id="kobo.1731.1"> progression, and the vanishing gradient effect is </span><span class="No-Break"><span class="koboSpan" id="kobo.1732.1">less pronounced.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1733.1">Next, we’ll introduce a new type of lightweight RNN cell that still preserves the properties </span><span class="No-Break"><span class="koboSpan" id="kobo.1734.1">of LSTM.</span></span></p>
<h2 id="_idParaDest-125" lang="en-GB"><a id="_idTextAnchor199"/><span class="koboSpan" id="kobo.1735.1">Gated recurrent units</span></h2>
<p lang="en-GB"><span class="koboSpan" id="kobo.1736.1">A </span><strong class="bold"><span class="koboSpan" id="kobo.1737.1">gated recurrent unit</span></strong><span class="koboSpan" id="kobo.1738.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1739.1">GRU</span></strong><span class="koboSpan" id="kobo.1740.1">) is a type of recurrent block that was</span><a id="_idIndexMarker961"/><span class="koboSpan" id="kobo.1741.1"> introduced</span><a id="_idIndexMarker962"/><span class="koboSpan" id="kobo.1742.1"> in 2014 (</span><em class="italic"><span class="koboSpan" id="kobo.1743.1">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</span></em><span class="koboSpan" id="kobo.1744.1">, </span><a href="https://arxiv.org/abs/1406.1078"><span class="koboSpan" id="kobo.1745.1">https://arxiv.org/abs/1406.1078</span></a><span class="koboSpan" id="kobo.1746.1"> and </span><em class="italic"><span class="koboSpan" id="kobo.1747.1">Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling</span></em><span class="koboSpan" id="kobo.1748.1">, https://arxiv.org/abs/1412.3555) as an improvement over LSTM. </span><span class="koboSpan" id="kobo.1748.2">A GRU unit usually has similar or better performance than an LSTM, but it does so with fewer parameters </span><span class="No-Break"><span class="koboSpan" id="kobo.1749.1">and operations:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer673">
<span class="koboSpan" id="kobo.1750.1"><img alt="Figure 6.14 – A GRU cell diagram" src="image/B19627_06_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1751.1">Figure 6.14 – A GRU cell diagram</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1752.1">Similar to the classic RNN, a GRU</span><a id="_idIndexMarker963"/><span class="koboSpan" id="kobo.1753.1"> cell has a single hidden</span><a id="_idIndexMarker964"/><span class="koboSpan" id="kobo.1754.1"> state, </span><span class="koboSpan" id="kobo.1755.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/539.png" style="vertical-align:-0.340em;height:1.042em;width:0.747em"/></span><span class="koboSpan" id="kobo.1756.1">. </span><span class="koboSpan" id="kobo.1756.2">You can think of it as a combination of the hidden and cell states of an LSTM. </span><span class="koboSpan" id="kobo.1756.3">The GRU cell has </span><span class="No-Break"><span class="koboSpan" id="kobo.1757.1">two gates:</span></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1758.1">Update gate</span></strong><span class="koboSpan" id="kobo.1759.1">, </span><span class="koboSpan" id="kobo.1760.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/540.png" style="vertical-align:-0.340em;height:0.803em;width:0.696em"/></span><span class="koboSpan" id="kobo.1761.1">: Combines the input and forget LSTM gates. </span><span class="koboSpan" id="kobo.1761.2">It decides what information to discard</span><a id="_idIndexMarker965"/><span class="koboSpan" id="kobo.1762.1"> and what new information to include in its place based on the network input, </span><span class="koboSpan" id="kobo.1763.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/352.png" style="vertical-align:-0.340em;height:0.781em;width:0.710em"/></span><span class="koboSpan" id="kobo.1764.1">, and the previous hidden state, </span><span class="koboSpan" id="kobo.1765.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/542.png" style="vertical-align:-0.340em;height:1.042em;width:1.444em"/></span><span class="koboSpan" id="kobo.1766.1">. </span><span class="koboSpan" id="kobo.1766.2">By combining the two gates, we can ensure that the cell will forget information, but only when we are going to include new information in </span><span class="No-Break"><span class="koboSpan" id="kobo.1767.1">its place:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1768.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/543.png" style="vertical-align:-0.395em;height:1.147em;width:9.359em"/></span></p>
<ul>
<li lang="en-GB"><strong class="bold"><span class="koboSpan" id="kobo.1769.1">Reset gate</span></strong><span class="koboSpan" id="kobo.1770.1">, </span><span class="koboSpan" id="kobo.1771.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/544.png" style="vertical-align:-0.340em;height:0.794em;width:0.652em"/></span><span class="koboSpan" id="kobo.1772.1">: Uses the previous hidden</span><a id="_idIndexMarker966"/><span class="koboSpan" id="kobo.1773.1"> state, </span><span class="koboSpan" id="kobo.1774.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/545.png" style="vertical-align:-0.340em;height:1.042em;width:1.425em"/></span><span class="koboSpan" id="kobo.1775.1">, and the network input, </span><span class="koboSpan" id="kobo.1776.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/357.png" style="vertical-align:-0.340em;height:0.781em;width:0.732em"/></span><span class="koboSpan" id="kobo.1777.1">, to decide how much of the previous state to </span><span class="No-Break"><span class="koboSpan" id="kobo.1778.1">pass through:</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1779.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;σ&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/547.png" style="vertical-align:-0.390em;height:1.142em;width:9.135em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1780.1">Next, we have the candidate </span><span class="No-Break"><span class="koboSpan" id="kobo.1781.1">state, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1782.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/548.png" style="vertical-align:-0.340em;height:1.042em;width:0.889em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1783.1">:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1784.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mi&gt;a&lt;/mml:mi&gt;&lt;mml:mi&gt;n&lt;/mml:mi&gt;&lt;mml:mi&gt;h&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;W&lt;/mml:mi&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;x&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;+&lt;/mml:mo&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;U&lt;/mml:mi&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;r&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;/mml:math&gt;" src="image/549.png" style="vertical-align:-0.440em;height:1.302em;width:12.969em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1785.1">Finally, the GRU output, </span><span class="koboSpan" id="kobo.1786.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/550.png" style="vertical-align:-0.340em;height:1.042em;width:0.778em"/></span><span class="koboSpan" id="kobo.1787.1">, at time </span><em class="italic"><span class="koboSpan" id="kobo.1788.1">t</span></em><span class="koboSpan" id="kobo.1789.1"> is an element-wise sum between the previous output, </span><span class="koboSpan" id="kobo.1790.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/551.png" style="vertical-align:-0.340em;height:1.042em;width:1.396em"/></span><span class="koboSpan" id="kobo.1791.1">, and the candidate </span><span class="No-Break"><span class="koboSpan" id="kobo.1792.1">output, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1793.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/548.png" style="vertical-align:-0.340em;height:1.042em;width:0.889em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1794.1">:</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1795.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot; display=&quot;block&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;=&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;mml:mo&gt;⊕&lt;/mml:mo&gt;&lt;mml:mfenced separators=&quot;|&quot;&gt;&lt;mml:mrow&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;z&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:mrow&gt;&lt;/mml:mfenced&gt;&lt;mml:mo&gt;⨀&lt;/mml:mo&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/553.png" style="vertical-align:-0.390em;height:1.152em;width:11.965em"/></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1796.1">Since the update gate allows us to both forget and store data, it is directly applied to the previous output, </span><span class="koboSpan" id="kobo.1797.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;mml:mo&gt;-&lt;/mml:mo&gt;&lt;mml:mn&gt;1&lt;/mml:mn&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/551.png" style="vertical-align:-0.340em;height:1.042em;width:1.396em"/></span><span class="koboSpan" id="kobo.1798.1">, and applied over the candidate </span><span class="No-Break"><span class="koboSpan" id="kobo.1799.1">output, </span></span><span class="No-Break"><span class="koboSpan" id="kobo.1800.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msubsup&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;'&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msubsup&gt;&lt;/mml:math&gt;" src="image/548.png" style="vertical-align:-0.340em;height:1.042em;width:0.889em"/></span></span><span class="No-Break"><span class="koboSpan" id="kobo.1801.1">.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1802.1">We’ll conclude our introduction to RNNs by returning to the disclaimer at the start of this chapter – the practical limitations of RNNs. </span><span class="koboSpan" id="kobo.1802.2">We can solve one of them – the vanishing and exploding gradients – with the help of LSTM or GRU cells. </span><span class="koboSpan" id="kobo.1802.3">However, there are </span><span class="No-Break"><span class="koboSpan" id="kobo.1803.1">two others:</span></span></p>
<ul>
<li lang="en-GB"><span class="koboSpan" id="kobo.1804.1">The RNN’s internal state is updated after each element of the sequence – a new element requires all preceding elements to be processed in advance. </span><span class="koboSpan" id="kobo.1804.2">Therefore, the RNN sequence processing cannot be parallelized and RNNs cannot take advantage of the GPU </span><span class="No-Break"><span class="koboSpan" id="kobo.1805.1">parallelization capabilities.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1806.1">The information of all preceding sequence elements is summarized in a single hidden cell state. </span><span class="koboSpan" id="kobo.1806.2">The RNN doesn’t have direct access to the historical sequence elements and has to rely on the cell state instead. </span><span class="koboSpan" id="kobo.1806.3">In practice, this means that an RNN (even LSTM or GRU) can meaningfully process sequences with a maximum length of around </span><span class="No-Break"><span class="koboSpan" id="kobo.1807.1">100 elements.</span></span></li>
</ul>
<p lang="en-GB"><span class="koboSpan" id="kobo.1808.1">As we’ll see</span><a id="_idIndexMarker967"/><span class="koboSpan" id="kobo.1809.1"> in the next chapter, the transformer</span><a id="_idIndexMarker968"/><span class="koboSpan" id="kobo.1810.1"> architecture successfully solves both of these limitations. </span><span class="koboSpan" id="kobo.1810.2">But for now, let’s see how to use LSTMs </span><span class="No-Break"><span class="koboSpan" id="kobo.1811.1">in practice.</span></span></p>
<h1 id="_idParaDest-126" lang="en-GB"><a id="_idTextAnchor200"/><span class="koboSpan" id="kobo.1812.1">Implementing text classification</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1813.1">In this section, we’ll use LSTM</span><a id="_idIndexMarker969"/><span class="koboSpan" id="kobo.1814.1"> to implement a sentiment analysis example over the Large Movie Review Dataset (</span><strong class="bold"><span class="koboSpan" id="kobo.1815.1">IMDb</span></strong><span class="koboSpan" id="kobo.1816.1">, </span><a href="http://ai.stanford.edu/~amaas/data/sentiment/"><span class="koboSpan" id="kobo.1817.1">http://ai.stanford.edu/~amaas/data/sentiment/</span></a><span class="koboSpan" id="kobo.1818.1">), which consists of 25,000 training and 25,000 testing reviews of popular movies. </span><span class="koboSpan" id="kobo.1818.2">Each review has a binary label that indicates whether it is positive</span><a id="_idIndexMarker970"/><span class="koboSpan" id="kobo.1819.1"> or negative. </span><span class="koboSpan" id="kobo.1819.2">This type of problem is an example of a </span><strong class="bold"><span class="koboSpan" id="kobo.1820.1">many-to-one</span></strong><span class="koboSpan" id="kobo.1821.1"> relationship, which we defined in the </span><em class="italic"><span class="koboSpan" id="kobo.1822.1">Recurrent neural networks (</span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1823.1">RNNs)</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.1824.1"> section.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1825.1">The sentiment analysis model is displayed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.1826.1">following diagram:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer691">
<span class="koboSpan" id="kobo.1827.1"><img alt="Figure 6.15 – Sentiment analysis with word embeddings and LSTM" src="image/B19627_06_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1828.1">Figure 6.15 – Sentiment analysis with word embeddings and LSTM</span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1829.1">Let’s describe the model</span><a id="_idIndexMarker971"/><span class="koboSpan" id="kobo.1830.1"> components (these are valid for any text </span><span class="No-Break"><span class="koboSpan" id="kobo.1831.1">classification algorithm):</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1832.1">Each word of the sequence is replaced with its embedding vector. </span><span class="koboSpan" id="kobo.1832.2">These embeddings can be produced </span><span class="No-Break"><span class="koboSpan" id="kobo.1833.1">with word2vec.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1834.1">The word embedding is fed as input to the </span><span class="No-Break"><span class="koboSpan" id="kobo.1835.1">LSTM cell.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1836.1">The cell output, </span><span class="koboSpan" id="kobo.1837.1"><img alt="&lt;mml:math xmlns:mml=&quot;http://www.w3.org/1998/Math/MathML&quot; xmlns:m=&quot;http://schemas.openxmlformats.org/officeDocument/2006/math&quot;&gt;&lt;mml:msub&gt;&lt;mml:mrow&gt;&lt;mml:mi mathvariant=&quot;bold&quot;&gt;h&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;mml:mrow&gt;&lt;mml:mi&gt;t&lt;/mml:mi&gt;&lt;/mml:mrow&gt;&lt;/mml:msub&gt;&lt;/mml:math&gt;" src="image/556.png" style="vertical-align:-0.340em;height:1.042em;width:0.739em"/></span><span class="koboSpan" id="kobo.1838.1">, serves as input to an FC layer with two output units and softmax. </span><span class="koboSpan" id="kobo.1838.2">The softmax output represents the probability of the review being positive (1) or </span><span class="No-Break"><span class="koboSpan" id="kobo.1839.1">negative (0).</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1840.1">The network can be produced </span><span class="No-Break"><span class="koboSpan" id="kobo.1841.1">with Word2Vec.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1842.1">The output for the final element of the sequence is taken as a result of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1843.1">whole sequence.</span></span></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1844.1">To implement this example, we’ll use PyTorch and the TorchText package. </span><span class="koboSpan" id="kobo.1844.2">It consists of data processing utilities and popular datasets for natural language. </span><span class="koboSpan" id="kobo.1844.3">We’ll only include the interesting portions of the code, but the full example is available in this book’s GitHub repo. </span><span class="koboSpan" id="kobo.1844.4">With that, </span><span class="No-Break"><span class="koboSpan" id="kobo.1845.1">let’s start:</span></span></p>
<ol>
<li lang="en-GB"><span class="koboSpan" id="kobo.1846.1">Define the device (by default, this is GPU with a fallback </span><span class="No-Break"><span class="koboSpan" id="kobo.1847.1">on CPU):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1848.1">
import torch
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1849.1">Start the training and testing dataset pipeline. </span><span class="koboSpan" id="kobo.1849.2">First, define the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1850.1">basic_english</span></strong><span class="koboSpan" id="kobo.1851.1"> tokenizer, which splits the text on spaces (that is, </span><span class="No-Break"><span class="koboSpan" id="kobo.1852.1">word tokenization):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1853.1">
from torchtext.data.utils import get_tokenizer
tokenizer = get_tokenizer('basic_english')</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1854.1">Next, use </span><strong class="source-inline"><span class="koboSpan" id="kobo.1855.1">tokenizer</span></strong><span class="koboSpan" id="kobo.1856.1"> to build</span><a id="_idIndexMarker972"/><span class="koboSpan" id="kobo.1857.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.1858.1">token </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1859.1">vocabulary</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1860.1">:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1861.1">
from torchtext.datasets import IMDB
from torchtext.vocab import build_vocab_from_iterator
def yield_tokens(data_iter):
    for _, text in data_iter:
        yield tokenizer(text)
vocabulary = build_vocab_from_iterator(
    yield_tokens(IMDB(split='train')),
    specials=["&lt;unk&gt;"])
vocabulary.set_default_index(vocabulary["&lt;unk&gt;"])</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1862.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1863.1">IMDB(split='train')</span></strong><span class="koboSpan" id="kobo.1864.1"> provides an iterator of all movie reviews in the training set (each review is represented as a string). </span><span class="koboSpan" id="kobo.1864.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1865.1">yield_tokens(IMDB(split='train'))</span></strong><span class="koboSpan" id="kobo.1866.1"> generator iterates over all samples and splits them into words. </span><span class="koboSpan" id="kobo.1866.2">The result serves as input to </span><strong class="source-inline"><span class="koboSpan" id="kobo.1867.1">build_vocab_from_iterator</span></strong><span class="koboSpan" id="kobo.1868.1">, which iterates over the tokenized samples and builds the token </span><strong class="source-inline"><span class="koboSpan" id="kobo.1869.1">vocabulary</span></strong><span class="koboSpan" id="kobo.1870.1">. </span><span class="koboSpan" id="kobo.1870.2">Note that the vocabulary only includes training samples. </span><span class="koboSpan" id="kobo.1870.3">Therefore, any token that exists in the test set (but not the training one) will be replaced with the default unknown </span><strong class="source-inline"><span class="koboSpan" id="kobo.1871.1">&lt;</span></strong><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1872.1">unk&gt;</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1873.1"> token.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1874.1">Next, define the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1875.1">collate_batch</span></strong><span class="koboSpan" id="kobo.1876.1"> function, which takes a </span><strong class="source-inline"><span class="koboSpan" id="kobo.1877.1">batch</span></strong><span class="koboSpan" id="kobo.1878.1"> of tokenized samples with varying lengths, and concatenates them in a single long sequence </span><span class="No-Break"><span class="koboSpan" id="kobo.1879.1">of tokens:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1880.1">
def collate_batch(batch):
    labels, samples, offsets = [], [], [0]
    for (_label, _sample) in batch:
        labels.append(int(_label) - 1)
        processed_text = torch.tensor(
            vocabulary(tokenizer(_sample)),
            dtype=torch.int64)
        samples.append(processed_text)
        offsets.append(processed_text.size(0))
    labels = torch.tensor(
        labels,
        dtype=torch.int64)
    offsets = torch.tensor(
        offsets[:-1]).cumsum(dim=0)
    samples = torch.cat(samples)
    return labels, samples, offsets</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1881.1">Here, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1882.1">samples</span></strong><span class="koboSpan" id="kobo.1883.1"> list aggregates</span><a id="_idIndexMarker973"/><span class="koboSpan" id="kobo.1884.1"> all tokenized </span><strong class="source-inline"><span class="koboSpan" id="kobo.1885.1">_sample</span></strong><span class="koboSpan" id="kobo.1886.1"> instances of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1887.1">batch</span></strong><span class="koboSpan" id="kobo.1888.1">. </span><span class="koboSpan" id="kobo.1888.2">In the end, they are concatenated into a single list. </span><span class="koboSpan" id="kobo.1888.3">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.1889.1">offsets</span></strong><span class="koboSpan" id="kobo.1890.1"> list contains the offset from the start of each concatenated sample. </span><span class="koboSpan" id="kobo.1890.2">This information makes it possible to reverse-split the long </span><strong class="source-inline"><span class="koboSpan" id="kobo.1891.1">samples</span></strong><span class="koboSpan" id="kobo.1892.1"> sequence into separate items again. </span><span class="koboSpan" id="kobo.1892.2">The purpose of the function is to create a compressed </span><strong class="source-inline"><span class="koboSpan" id="kobo.1893.1">batch</span></strong><span class="koboSpan" id="kobo.1894.1"> representation. </span><span class="koboSpan" id="kobo.1894.2">This is necessary because of the varying length of each sample. </span><span class="koboSpan" id="kobo.1894.3">The alternative would be to pad all samples to match the length of the longest one so that they can fit in the batch tensor. </span><span class="koboSpan" id="kobo.1894.4">Fortunately, PyTorch provides us with the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1895.1">offsets</span></strong><span class="koboSpan" id="kobo.1896.1"> optimization to avoid this. </span><span class="koboSpan" id="kobo.1896.2">Once we feed the compressed batch to the RNN, it will automatically reverse it back into </span><span class="No-Break"><span class="koboSpan" id="kobo.1897.1">separate samples.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1898.1">Then, we define the </span><span class="No-Break"><span class="koboSpan" id="kobo.1899.1">LSTM</span></span><span class="No-Break"><a id="_idIndexMarker974"/></span><span class="No-Break"><span class="koboSpan" id="kobo.1900.1"> model:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1901.1">
class LSTMModel(torch.nn.Module):
    def __init__(self, vocab_size, embedding_size, hidden_size,
        num_classes):
        super().__init__()
        # Embedding field
        self.embedding = torch.nn.EmbeddingBag(
            num_embeddings=vocab_size,
            embedding_dim=embedding_size)
        # LSTM cell
        self.rnn = torch.nn.LSTM(
            input_size=embedding_size,
            hidden_size=hidden_size)
        # Fully connected output
        self.fc = torch.nn.Linear(
            hidden_size, num_classes)
    def forward(self, text_sequence, offsets):
        # Extract embedding vectors
        embeddings = self.embedding(
            text_sequence, offsets)
        h_t, c_t = self.rnn(embeddings)
        return self.fc(h_t)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1902.1">The model implements the scheme we introduced at the start of this section. </span><span class="koboSpan" id="kobo.1902.2">As its name suggests, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1903.1">embedding</span></strong><span class="koboSpan" id="kobo.1904.1"> property (an instance of </span><strong class="source-inline"><span class="koboSpan" id="kobo.1905.1">EmbeddingBag</span></strong><span class="koboSpan" id="kobo.1906.1">) maps the token (in our case, word) index to its embedding vector. </span><span class="koboSpan" id="kobo.1906.2">We can see that the constructor</span><a id="_idIndexMarker975"/><span class="koboSpan" id="kobo.1907.1"> takes the vocabulary size (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1908.1">num_embeddings</span></strong><span class="koboSpan" id="kobo.1909.1">) and the embedding vector size (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1910.1">embedding_dim</span></strong><span class="koboSpan" id="kobo.1911.1">). </span><span class="koboSpan" id="kobo.1911.2">In theory, we could initialize </span><strong class="source-inline"><span class="koboSpan" id="kobo.1912.1">EmbeddingBag</span></strong><span class="koboSpan" id="kobo.1913.1"> with pre-computed Word2Vec embedding vectors. </span><span class="koboSpan" id="kobo.1913.2">But in our case, we’ll simply use random initialization and let the model learn them as part of the training. </span><strong class="source-inline"><span class="koboSpan" id="kobo.1914.1">embedding</span></strong><span class="koboSpan" id="kobo.1915.1"> also takes care of the compressed batch representation (hence the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1916.1">offsets</span></strong><span class="koboSpan" id="kobo.1917.1"> parameter in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1918.1">forward</span></strong><span class="koboSpan" id="kobo.1919.1"> method). </span><span class="koboSpan" id="kobo.1919.2">The embedding’s output serves as input to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1920.1">rnn</span></strong><span class="koboSpan" id="kobo.1921.1"> LSTM cell, which, in turn, feeds the output </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1922.1">fc</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1923.1"> layer.</span></span></p></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1924.1">Define the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1925.1">train_model(model, cost_function, optimizer, data_loader)</span></strong><span class="koboSpan" id="kobo.1926.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1927.1">test_model(model, cost_function, data_loader)</span></strong><span class="koboSpan" id="kobo.1928.1"> functions. </span><span class="koboSpan" id="kobo.1928.2">These are almost the same functions that we first defined in </span><a href="B19627_03.xhtml#_idTextAnchor079"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.1929.1">Chapter 3</span></em></span></a><span class="koboSpan" id="kobo.1930.1">, so we won’t include them here. </span><span class="koboSpan" id="kobo.1930.2">However, they have been adapted to the compressed batch representation and the additional </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1931.1">offsets</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1932.1"> parameter.</span></span></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1933.1">Proceed with the experiment. </span><span class="koboSpan" id="kobo.1933.2">Instantiate the LSTM model, the cross-entropy cost function, and the </span><span class="No-Break"><span class="koboSpan" id="kobo.1934.1">Adam optimizer:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1935.1">
model = LSTMModel(
    vocab_size=len(vocabulary),
    embedding_size=64,
    hidden_size=64,
    num_classes=2)
cost_fn = torch.nn.CrossEntropyLoss()
optim = torch.optim.Adam(model.parameters())</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1936.1">Define </span><strong class="source-inline"><span class="koboSpan" id="kobo.1937.1">train_dataloader</span></strong><span class="koboSpan" id="kobo.1938.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1939.1">test_dataloader</span></strong><span class="koboSpan" id="kobo.1940.1">, and their respective datasets (use a mini-batch size </span><span class="No-Break"><span class="koboSpan" id="kobo.1941.1">of 64):</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1942.1">
from torchtext.data.functional import to_map_style_dataset
train_iter, test_iter = IMDB()
train_dataset = to_map_style_dataset(train_iter)
test_dataset = to_map_style_dataset(test_iter)
from torch.utils.data import DataLoader
train_dataloader = DataLoader(
    train_dataset, batch_size=64,
    shuffle=True, collate_fn=collate_batch)
test_dataloader = DataLoader(
    test_dataset, batch_size=64,
    shuffle=True, collate_fn=collate_batch)</span></pre></li>
<li lang="en-GB"><span class="koboSpan" id="kobo.1943.1"> Run the training for </span><span class="No-Break"><span class="koboSpan" id="kobo.1944.1">5 epochs:</span></span><pre class="source-code" lang="en-GB"><span class="koboSpan" id="kobo.1945.1">
for epoch in range(5):
    print(f'Epoch: {epoch + 1}')
    train_model(model, cost_fn, optim, train_dataloader)
    test_model(model, cost_fn, test_dataloader)</span></pre><p class="list-inset" lang="en-GB"><span class="koboSpan" id="kobo.1946.1">The model achieves a test accuracy in the realm </span><span class="No-Break"><span class="koboSpan" id="kobo.1947.1">of 87%.</span></span></p></li>
</ol>
<p lang="en-GB"><span class="koboSpan" id="kobo.1948.1">This concludes our small practical example</span><a id="_idIndexMarker976"/><span class="koboSpan" id="kobo.1949.1"> of LSTM text classification. </span><span class="koboSpan" id="kobo.1949.2">Coincidentally, it also concludes </span><span class="No-Break"><span class="koboSpan" id="kobo.1950.1">this chapter.</span></span></p>
<h1 id="_idParaDest-127" lang="en-GB"><a id="_idTextAnchor201"/><span class="koboSpan" id="kobo.1951.1">Summary</span></h1>
<p lang="en-GB"><span class="koboSpan" id="kobo.1952.1">In this chapter, we introduced two complementary topics – NLP and RNNs. </span><span class="koboSpan" id="kobo.1952.2">We discussed the tokenization technique and the most popular tokenization algorithms – BPE, WordPiece, and Unigram. </span><span class="koboSpan" id="kobo.1952.3">Then, we introduced the concept of word embedding vectors and the Word2Vec algorithm to produce them. </span><span class="koboSpan" id="kobo.1952.4">We also discussed the </span><em class="italic"><span class="koboSpan" id="kobo.1953.1">n</span></em><span class="koboSpan" id="kobo.1954.1">-gram LM, which provided us with a smooth transition to the topic of RNNs. </span><span class="koboSpan" id="kobo.1954.2">There, we implemented a basic RNN example and introduced two of the most advanced RNN architectures – LSTM and GRU. </span><span class="koboSpan" id="kobo.1954.3">Finally, we implemented a sentiment </span><span class="No-Break"><span class="koboSpan" id="kobo.1955.1">analysis model.</span></span></p>
<p lang="en-GB"><span class="koboSpan" id="kobo.1956.1">In the next chapter, we’ll supercharge our NLP potential by introducing the attention mechanism </span><span class="No-Break"><span class="koboSpan" id="kobo.1957.1">and transformers.</span></span></p>
</div>
</body></html>