<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer077">
			<h1 id="_idParaDest-84"><em class="italic"><a id="_idTextAnchor083"/>Chapter 7</em>: Multi-Step Deep Learning Inference Pipeline</h1>
			<p>Now that we have<a id="_idIndexMarker430"/> successfully run <strong class="bold">HPO</strong> (<strong class="bold">Hyperparameter Optimization</strong>) and produced a well-tuned DL model that meets the business requirements, it is time to move to the next step towards using this model for prediction. This is where the model inference pipeline comes into play, where the model is used for predicting or scoring real-world data in production, either in real time or batch mode. However, an inference pipeline usually does not just rely on a single model but needs preprocessing and postprocessing logic that is not necessarily seen during the model development stage. Examples of preprocessing steps include detecting the language locale (English or some other languages) before passing the input data to the model for scoring. Postprocessing could include enriching the predicted labels with additional metadata to meet the business application's requirements. There are also patterns of ML/DL inference pipelines that could even involve an ensemble of models to solve a real-world business problem. Many ML projects often underestimate the efforts needed to implement a production inference pipeline, which could result in degradation of the model's performance in production or in the worst case, failure of the entire project. Thus, it is important to learn how to recognize the pattern of different inference pipelines and implement them properly before we deploy the model into production. </p>
			<p>By the end of this chapter, you will be able to use MLflow to confidently implement preprocessing and postprocessing steps for a multi-step inference pipeline that is ready to be used in production in future chapters.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Understanding patterns of DL inference pipelines</li>
				<li>Understanding the MLflow Model Python Function API</li>
				<li>Implementing a custom MLflow Python model</li>
				<li>Implementing preprocessing and postprocessing steps in a DL inference pipeline</li>
				<li>Implementing an inference pipeline as a new entry point in the main ML project</li>
			</ul>
			<h1 id="_idParaDest-85"><a id="_idTextAnchor084"/>Technical requirements</h1>
			<p>The following are the technical requirements for this chapter:</p>
			<ul>
				<li>The GitHub code for this chapter: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter07">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter07</a></li>
				<li>A full-fledged local MLflow tracking server, as described in <a href="B18120_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a>, <em class="italic">Tracking Models, Parameters, and Metrics</em>.</li>
			</ul>
			<h1 id="_idParaDest-86"><a id="_idTextAnchor085"/>Understanding patterns of DL inference pipelines</h1>
			<p>As the model <a id="_idIndexMarker431"/>development enters the stage of implementing an<a id="_idIndexMarker432"/> inference pipeline for the upcoming production usage, it is important to understand that having a well-tuned and trained DL model is only half the success story for business AI strategy. The other half includes deploying, serving, monitoring, and continuously improving the model after it goes into production. Designing and implementing a DL inference pipeline is the initial step toward the second half of the story. While the model has been trained, tuned, and tested on curated offline datasets, now it needs to handle prediction in two ways:</p>
			<ul>
				<li><strong class="bold">Batch inference</strong>: This usually<a id="_idIndexMarker433"/> requires some scheduled or ad hoc execution of an inference pipeline for some offline batch of observational data. The turnaround time for producing prediction results is daily, weekly, or other schedules.</li>
				<li><strong class="bold">Online inference</strong>: This usually<a id="_idIndexMarker434"/> requires a web service for real-time execution of an inference pipeline that produces prediction results for input data in under a second or even less than 100 milliseconds depending on the user scenarios.</li>
			</ul>
			<p>Note that because the execution environment and data characteristics could be different from the offline training and testing environment, there will be additional preprocessing or postprocessing steps around the core model logic developed during the model training and tuning steps. While it should be emphasized that any sharable data preprocessing steps should be used in both the training pipeline and inference pipeline, it is unavoidable that some business logic will come into play, which will allow the inference pipeline to have additional preprocessing and postprocessing logic. For example, a very common step in a DL inference pipeline is to use caching to store and return prediction results based on a recently seen input so that an expensive model evaluation does not need to be invoked. This step is not needed for a training/testing pipeline during the model development stage.</p>
			<p>While the pattern for <a id="_idIndexMarker435"/>inference pipelines is still emerging, it is now<a id="_idIndexMarker436"/> commonly known that there are at least four patterns in a real-world production environment:</p>
			<ul>
				<li><strong class="bold">Multi-step pipeline</strong>: This <a id="_idIndexMarker437"/>is the most typical usage of the model in production, which includes a linear workflow of preprocessing steps before the model logic is invoked and some postprocessing steps after the model evaluation results are returned. While this is conceptually simple, the implementation can still be varied. We will see how we can do this efficiently in this chapter using MLflow.</li>
				<li><strong class="bold">Ensemble of models</strong>: This is a more complex scenario where multiple different models can be used. These could be the same types of models with different versions for A/B testing purposes or different types of models. For example, for a complex conversational AI chatbot scenario, an intent classification model of the user query to classify user intents into a specific category is required. Then a content relevance model is also required to retrieve relevant answers to present to the user based on the detected user intent. </li>
				<li><strong class="bold">Business logic and model</strong>: This usually involves additional business logic on how and where the input to the model should come from, such as querying from an enterprise database for user information and validation or retrieving precomputed additional features from a feature store before invoking a model. In addition, postprocessing business logic could also transform the prediction results into some application-specific logic and store the results in some backend storage. While this could be as simple as a linear multi-step pipeline, it can also quickly <a id="_idIndexMarker438"/>become a <strong class="bold">DAG</strong> (<strong class="bold">Directed Acyclic Graph</strong>) with<a id="_idIndexMarker439"/> multiple fan-in and fan-out parallel tasks before and after the model has been invoked.</li>
				<li><strong class="bold">Online learning</strong>: This is one of the most complex inference tasks in production where a model is constantly learning and updating its parameters such as reinforcement learning.</li>
			</ul>
			<p>While it is necessary to understand the big picture of the complexity of inference pipelines in production, the purpose of this chapter is to learn how we can create reusable building blocks of inference pipelines that could be used in multiple scenarios through the powerful and generic MLflow Model API, which can encapsulate preprocessing and postprocessing steps alongside a trained model. Interested readers are encouraged to learn more about the model pattern in production from this post (<a href="https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns">https://www.anyscale.com/blog/serving-ml-models-in-production-common-patterns</a>) and other references in the <em class="italic">Further reading</em> section.</p>
			<p>So, what's the MLflow Model API and how do you use that to implement preprocessing and <a id="_idIndexMarker440"/>postprocessing <a id="_idIndexMarker441"/>logic for a multi-step inference pipeline? Let's find out in the next section.</p>
			<p class="callout-heading">Multi-Step Inference Pipeline as an MLflow Model</p>
			<p class="callout">Previously, in <a href="B18120_03_ePub.xhtml#_idTextAnchor040"><em class="italic">Chapter 3</em></a>, <em class="italic">Tracking Models, Parameters, and Metrics</em>, we introduced the flexible loosely coupled multi-step pipeline implementation using MLflow <strong class="bold">MLproject</strong> so that we could execute and track a multi-step training pipeline explicitly in MLflow. However, during inference time, it is desirable to implement lightweight preprocessing and postprocessing logic alongside a trained model that's already logged in the model repository. The MLflow Model API provides a mechanism to wrap a trained model with preprocessing and postprocessing logic and then save the newly wrapped model as a new model that encapsulates the inference pipeline logic. This unifies the way to load an original model or an inference pipeline model using MLflow Model APIs. This is critical for flexible deployment using MLflow and opens doors for creative inference pipeline building.</p>
			<h2 id="_idParaDest-87"><a id="_idTextAnchor086"/>Understanding the MLflow Model Python Function API</h2>
			<p>The MLflow <a id="_idIndexMarker442"/>Model (<a href="https://www.mlflow.org/docs/latest/models.html#id25">https://www.mlflow.org/docs/latest/models.html#id25</a>) is one of the core components<a id="_idIndexMarker443"/> provided by <a id="_idIndexMarker444"/>MLflow to load, save, and log models in different flavors (for example, a <strong class="bold">scikit-learn</strong> or a <strong class="bold">PyTorch</strong> model flavor). A model flavor is an MLflow defined standard format that explicitly specifies a directory of arbitrary files and a description file<a id="_idIndexMarker445"/> called <strong class="bold">MLmodel</strong>. As a reminder and an example, <em class="italic">Figure 7.1</em> shows what we have saved after fine-tuning our example NLP sentiment classifier in the MLflow artifact store and the content of the <strong class="source-inline">MLmodel</strong> file:</p>
			<div>
				<div id="_idContainer068" class="IMG---Figure">
					<img src="Images/B18120_07_01.jpg" alt="Figure 7.1 – MLmodel content for a fine-tuned PyTorch model&#13;&#10;" width="1028" height="527"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.1 – MLmodel content for a fine-tuned PyTorch model</p>
			<p>As can be seen from <em class="italic">Figure 7.1</em>, the flavor of this model is PyTorch. There are also a few other metadata about the model, such as the conda environment, which defines the dependencies for running the model, and many others. Given this self-contained information, it should be enough to allow MLflow to load the model back using the <strong class="source-inline">mlflow.pytorch.load_model</strong> API as follows:</p>
			<p class="source-code">logged_model = f'runs:/{<strong class="bold">run_id</strong>}/model'</p>
			<p class="source-code">model = mlflow.pytorch.load_model(logged_model)</p>
			<p>This will allow loading the model that was logged by an MLflow run with <strong class="source-inline">run_id</strong> back to memory and doing inference. Now imagine we have the following scenario where we need to add some preprocessing logic to check the language type of the input text. This requires loading a language detector model (<a href="https://amitness.com/2019/07/identify-text-language-python/">https://amitness.com/2019/07/identify-text-language-python/</a>) such<a id="_idIndexMarker446"/> as the <strong class="bold">FastText</strong> language detector (<a href="https://fasttext.cc/">https://fasttext.cc/</a>), or<a id="_idIndexMarker447"/> Google's <strong class="bold">Compact Language Detector v3</strong> (<a href="https://pypi.org/project/gcld3/">https://pypi.org/project/gcld3/</a>). Additionally, we also want to check whether there is any cached prediction for the exact same input. If it exists, then we should just return the cached result without invoking the expensive model prediction part. This is very typical preprocessing logic. For postprocessing, a common scenario is to return the prediction along with some metadata about the model URIs so that we can debug any potential prediction issue in production. Given this preprocessing and postprocessing logic, the inference pipeline now looks like the following figure:</p>
			<div>
				<div id="_idContainer069" class="IMG---Figure">
					<img src="Images/B18120_07_02.jpg" alt="Figure 7.2 – Multi-step inference pipeline&#13;&#10;" width="772" height="76"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.2 – Multi-step inference pipeline</p>
			<p>As can be seen<a id="_idIndexMarker448"/> from <em class="italic">Figure 7.2</em>, these five steps include the following:</p>
			<ul>
				<li>One original fine-tuned model for prediction (a PyTorch DL model)</li>
				<li>One additional language detection model that was not part of our previous training pipeline</li>
				<li>Cache operations (check cache and store to cache) for improving response performance</li>
				<li>One response message composition step</li>
			</ul>
			<p>Rather than splitting these five steps into five different entry points in an <strong class="bold">ML project</strong> (recall that an entry point in an <strong class="bold">ML project</strong> can be arbitrary execution code in Python or other executables), it is much more elegant to compose this multi-step inference pipeline in a single entry point, since these steps are closely related to the model's prediction step. In addition, the advantage of encapsulating these closely related steps into a single inference pipeline is that we can save and load the inference pipeline as an MLmodel artifact. MLflow provides a generic way to implement this multi-step inference pipeline as a new Python model, without losing the flexibility of adding additional preprocessing and postprocessing capability if needed as shown in the following figure: </p>
			<div>
				<div id="_idContainer070" class="IMG---Figure">
					<img src="Images/B18120_07_03.jpg" alt=" Figure 7.3 – Encapsulate the multi-step preprocessing and postprocessing logic into &#13;&#10;a new MLflow Python model&#13;&#10;" width="1072" height="198"/>
				</div>
			</div>
			<p class="figure-caption"> Figure 7.3 – Encapsulate the multi-step preprocessing and postprocessing logic into a new MLflow Python model</p>
			<p>As can be seen from <em class="italic">Figure 7.3</em>, if we encapsulate the preprocessing and postprocessing logic into a new MLflow model called <strong class="source-inline">inference_pipeline_model</strong>, then we can load this entire inference pipeline as if it is just another model. This will also allow us to formalize the input and output format (called <strong class="bold">Model Signature</strong>) for <a id="_idIndexMarker449"/>the inference pipeline so that whoever wants to consume this inference pipeline will not need to guess what the format of the input and output is.</p>
			<p>The mechanism to implement this at a high level is as follows:</p>
			<ol>
				<li>First, create <a id="_idIndexMarker450"/>a custom <a id="_idIndexMarker451"/>MLflow <strong class="source-inline">pyfunc</strong> (Python function) model (<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models">https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#creating-custom-pyfunc-models</a>) to wrap the existing trained model. Specifically, we need to go beyond the built-in model flavors (<a href="https://www.mlflow.org/docs/latest/models.html#built-in-model-flavors">https://www.mlflow.org/docs/latest/models.html#built-in-model-flavors</a>) provided by MLflow and implement a new Python class that inherits from <strong class="source-inline">mlflow.pyfunc.PythonModel </strong>(<a href="https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel">https://www.mlflow.org/docs/latest/python_api/mlflow.pyfunc.html#mlflow.pyfunc.PythonModel</a>), defining <strong class="source-inline">predict()</strong> and, optionally, the <strong class="source-inline">load_context()</strong> methods in this new Python class. </li>
			</ol>
			<p>In addition, we <a id="_idIndexMarker452"/>can specify the <strong class="bold">Model Signature </strong>(<a href="https://mlflow.org/docs/latest/models.html#model-signature">https://mlflow.org/docs/latest/models.html#model-signature</a>) by defining the schema of a model's inputs and outputs. These schemas can be either column-based or tensor-based. It is highly recommended to implement these schemas for automatic input validation and model diagnosis in a production environment. </p>
			<ol>
				<li value="2">Then implement the preprocessing and postprocessing logic within this MLflow <strong class="source-inline">pyfunc</strong>. These could include caching, language detection, a response message, and any other logic that's needed.</li>
				<li>Finally, implement the entry point in the ML project for the inference pipeline so that we can invoke the inference pipeline as if it is a single model artifact. </li>
			</ol>
			<p>Now that we understand the fundamentals of MLflow's custom Python model to represent a multi-step inference <a id="_idIndexMarker453"/>pipeline, let's see how we can implement it for our NLP sentiment classification model with the preprocessing and postprocessing steps described in <em class="italic">Figure 7.3</em> in the following sections.</p>
			<h1 id="_idParaDest-88"><a id="_idTextAnchor087"/>Implementing a custom MLflow Python model </h1>
			<p>Let's first <a id="_idIndexMarker454"/>describe the steps to implement a custom MLflow Python model without any extra preprocessing and postprocessing logic:</p>
			<ol>
				<li value="1">First, make sure we have a trained DL model that's ready to be used for inference purposes. For the sake of learning in this chapter, we include the training pipeline <strong class="bold">MLproject</strong> in this chapter, so that we can easily produce a fine-tuned DL model. To run the training pipeline, make sure you have the virtual environment set up for this chapter by following the <strong class="source-inline">README</strong> file in this chapter's GitHub repository and <em class="italic">set up the environment variables</em> accordingly (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/README.md">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/README.md</a>). Then, in the command line, run the following command to generate a fine-tuned model in the local MLflow tracking server:<p class="source-code"><strong class="bold">mlflow run . --experiment-name dl_model_chapter07 -P pipeline_steps=download_data,fine_tuning_model</strong></p></li>
			</ol>
			<p>Once this is done, you will have a fine-tuned DL model logged in the MLflow tracking server. Now, we will use the logged model URI as the input for the inference pipeline since we will wrap it and save it as a new MLflow model. The logged model URI is something like the following, where the long random alphanumeric string is the <strong class="source-inline">run_id</strong> of the <strong class="source-inline">fine_tuning_model</strong> MLflow run, which you can find in the MLflow tracking server:</p>
			<p class="source-code">runs:/1290f813d8e74a249c86eeab9f6ed24e/model</p>
			<ol>
				<li value="2">Once you have a trained/fine-tuned model, we are ready to implement a new custom MLflow<a id="_idIndexMarker455"/> Python model as follows. You may want to check out the <strong class="bold">VS Code</strong> notebook <a id="_idIndexMarker456"/>for <strong class="source-inline">basic_custom_dl_model.py</strong> in the GitHub repo (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/notebooks/basic_custom_dl_model.py">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/notebooks/basic_custom_dl_model.py</a>) to follow through the steps outlined here: <p class="source-code">class <strong class="bold">InferencePipeline</strong>(<strong class="bold">mlflow.pyfunc.PythonModel</strong>):</p><p class="source-code">    def <strong class="bold">__init__</strong>(self, finetuned_model_uri):</p><p class="source-code">        self.finetuned_model_uri = finetuned_model_uri</p><p class="source-code">    def <strong class="bold">sentiment_classifier</strong>(self, row):</p><p class="source-code">        pred_label = self.finetuned_text_classifier.predict({row[0]})</p><p class="source-code">        return pred_label</p><p class="source-code">    def <strong class="bold">load_context</strong>(self, context):</p><p class="source-code">        self.finetuned_text_classifier = mlflow.pytorch.load_model(self.finetuned_model_uri)</p><p class="source-code">    def <strong class="bold">predict</strong>(self, context, model_input):</p><p class="source-code">        results = model_input.apply(</p><p class="source-code">                    self.sentiment_classifier, axis=1,</p><p class="source-code">                    result_type='broadcast')</p><p class="source-code">        return results</p></li>
			</ol>
			<p>Let's see what we have implemented. First, the <strong class="source-inline">InferencePipeline</strong> class inherits from the <strong class="source-inline">MLflow.pyfunc.PythonModel</strong> module, and implements four methods as follows:</p>
			<ul>
				<li><strong class="source-inline">predict</strong>: This is a method that's required by <strong class="source-inline">mlflow.pyfunc.PythonModel</strong>, which returns the prediction result. Here, the <strong class="source-inline">model_input</strong> parameter is a <strong class="source-inline">pandas</strong> DataFrame, which contains a column with input text that needs to be classified. We leverage the <strong class="source-inline">pandas</strong> DataFrame's <strong class="source-inline">apply</strong> method to run a <strong class="source-inline">sentiment_classifier</strong> method to score each row of the DataFrame's text and the<a id="_idIndexMarker457"/> result is a DataFrame with each row being the predicted label. Since our original fine-tuned model does not accept a <strong class="source-inline">pandas</strong> DataFrame as input (it accepts a list of text strings as input), we need to implement a new classifier as a wrapper to the original model. That's the <strong class="source-inline">sentiment_classifier</strong> method. The other <strong class="source-inline">context</strong> parameter is the MLflow context to describe where the model artifact is stored. Since we will pass an MLflow logged model URI, this <strong class="source-inline">context</strong> parameter is not used in our implementation, as the logged model URI contains everything MLflow needs to load a model.</li>
				<li><strong class="source-inline">sentiment_classifier</strong>: This is a wrapper method to allow each row of the input <strong class="source-inline">pandas</strong> DataFrame to be scored by calling the fine-tuned DL model's prediction function. Note that we are wrapping the first element of the row into a list so that the DL model can correctly use it as an input.</li>
				<li><strong class="source-inline">init</strong>: This is a standard Python constructor method. Here, we use it to pass in a previously fine-tuned DL model URI, <strong class="source-inline">finetuned_model_uri</strong>, so that we can load it in the <strong class="source-inline">load_context</strong> method. Note that we do not want to directly load the model in the <strong class="source-inline">init</strong> method since it will cause a serialization issue (if you want to try, you will find out serializing a DL model naively is not a fun experience). Since the fine-tuned DL model is already serialized and deserialized through the <strong class="source-inline">mlflow.pytorch</strong> APIs, we should not reinvent the wheel here. The recommended way is to load the model in the <strong class="source-inline">load_context</strong> method.</li>
				<li><strong class="source-inline">load_context</strong>: This method is called when loading an MLflow model with the <strong class="source-inline">mlflow.pyfunc.load_model</strong> API. This is executed immediately after the Python model is constructed. Here, we load the fine-tuned DL model by using the <strong class="source-inline">mlflow.pytorch.load_model</strong> API. Note that whatever models are loaded in this method can use their corresponding<a id="_idIndexMarker458"/> deserializing methods. This will open doors for loading other models such as a language detection model, which could contain native code (for example, C++ code) that cannot be serialized using Python serialization protocols. This is one of the nice features provided by the MLflow model API framework.</li>
			</ul>
			<ol>
				<li value="3">Now that we have an MLflow custom model that can accept a column-based input, we can also define the model signature as follows:<p class="source-code">input = json.dumps([{'name': 'text', 'type': 'string'}])</p><p class="source-code">output = json.dumps([{'name': 'text', 'type': 'string'}])</p><p class="source-code">signature = ModelSignature.from_dict({'inputs': input, 'outputs': output})</p></li>
			</ol>
			<p>This signature defines an input format with one named column called <strong class="source-inline">text</strong> with a datatype of <strong class="source-inline">string</strong>, and an output format with one named column called <strong class="source-inline">text</strong> with a datatype of <strong class="source-inline">string</strong>. The <strong class="source-inline">mlflow.models.ModelSignature</strong> class is used to create this <strong class="source-inline">signature</strong> object. This will be used when we log the new custom model in MLflow, as we will see in the next step.</p>
			<ol>
				<li value="4">Next, we can log this new custom model in MLflow as if this is a generic MLflow <strong class="source-inline">pyfunc</strong> model using the <strong class="source-inline">mlflow.pyfunc.log_model</strong> API as follows:<p class="source-code">MODEL_ARTIFACT_PATH = 'inference_pipeline_model'</p><p class="source-code">with mlflow.start_run() as dl_model_tracking_run:</p><p class="source-code">    finetuned_model_uri = 'runs:/1290f813d8e74a249c86eeab9f6ed24e/model'</p><p class="source-code">    inference_pipeline_uri = f'runs:/{dl_model_tracking_run.info.run_id}/{MODEL_ARTIFACT_PATH}'</p><p class="source-code">    mlflow.pyfunc.log_model(</p><p class="source-code">      artifact_path=MODEL_ARTIFACT_PATH,</p><p class="source-code">      conda_env=CONDA_ENV,</p><p class="source-code">      python_model=<strong class="bold">InferencePipeline</strong>(</p><p class="source-code">        finetuned_model_uri),</p><p class="source-code">      signature=signature)</p></li>
			</ol>
			<p>The preceding<a id="_idIndexMarker459"/> code will log a model in the MLflow tracking server with a top-level folder named <strong class="source-inline">inference_pipeline_model</strong>, since we define the <strong class="source-inline">MODEL_ARTIFACT_PATH</strong> variable with this string value and assign this value to the <strong class="source-inline">artifact_path</strong> parameter of the <strong class="source-inline">mlflow.pyfunc.log_model</strong> method. The other three parameters we assign are the following:</p>
			<ul>
				<li><strong class="source-inline">conda_env</strong>: This is to define the conda environment where this custom model will run. Here, we can pass the absolute path of the <strong class="source-inline">conda.yaml</strong> file in the root folder of this chapter defined by the <strong class="source-inline">CONDA_ENV</strong> variable (details of this variable can be found in the source code of this <strong class="source-inline">basic_custom_dl_model.py</strong> notebook on GitHub).</li>
				<li><strong class="source-inline">python_model</strong>: Here, we call the new <strong class="source-inline">InferencePipeline</strong> class we just implemented and pass in the parameter of <strong class="source-inline">finetuned_model_uri</strong>. This way, the inference pipeline will load the correct fine-tuned model for prediction purposes.</li>
				<li><strong class="source-inline">signature</strong>: We also pass the signature for both input and output we just defined and assign it to the signature parameter so that model input and output schema can be logged and enforced for validation purposes.</li>
			</ul>
			<p>As a reminder, make <a id="_idIndexMarker460"/>sure you replace the <strong class="source-inline">'runs:/1290f813d8e74a249c86eeab9f6ed24e/model'</strong> value for the <strong class="source-inline">finetuned_model_uri</strong> variable with your own fine-tuned model URI generated in <em class="italic">step 1</em> so that the code will correctly load the original fine-tuned model. </p>
			<ol>
				<li value="5">If you follow through the <strong class="bold">VS Code</strong> notebook for <strong class="source-inline">basic_custom_dl_model.py</strong> and run it cell by cell up to <em class="italic">step 4</em>, you should be able to find a newly logged model in the <strong class="bold">Artifacts</strong> section of the MLflow tracking server as shown in the following screenshot:</li>
			</ol>
			<div>
				<div id="_idContainer071" class="IMG---Figure">
					<img src="Images/B18120_07_04.jpg" alt="Figure 7.4 – Inference MLflow model with model schema and a root folder of inference_pipeline_model&#13;&#10;" width="1213" height="650"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.4 – Inference MLflow model with model schema and a root folder of inference_pipeline_model</p>
			<p>As can be seen<a id="_idIndexMarker461"/> from <em class="italic">Figure 7.4</em>, the root folder name (top left of the screenshot) is <strong class="source-inline">inference_pipeline_model</strong>, which is the <strong class="source-inline">artifact_path</strong> parameter's assigned value when calling <strong class="source-inline">mlflow.pyfunc.log_model</strong>. Note, if we do not specify the <strong class="source-inline">artifact_path</strong> parameter, by default it will be just <strong class="source-inline">model</strong>. You can confirm this by just looking at <em class="italic">Figure 7.1</em> earlier in this chapter. Also note that now there is a <strong class="bold">Model schema</strong> section as shown in <em class="italic">Figure 7.4</em>, which is new. This describes both the input and output format as we defined before. In fact, if we click the <strong class="source-inline">MLmodel</strong> file under the <strong class="source-inline">inference_pipeline_model</strong> folder, we can see the full content as follows:</p>
			<div>
				<div id="_idContainer072" class="IMG---Figure">
					<img src="Images/B18120_07_05.jpg" alt="Figure 7.5 – The content of inference_pipeline_model's MLmodel file&#13;&#10;" width="1249" height="654"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.5 – The content of inference_pipeline_model's MLmodel file</p>
			<p>As can be seen<a id="_idIndexMarker462"/> from <em class="italic">Figure 7.5</em>, the content of the <strong class="bold">MLmodel</strong> file now contains a <strong class="source-inline">signature</strong> section near the bottom, a new section compared with <em class="italic">Figure 7.1</em>. However, there are some more important differences in terms of the model flavor. The flavor of <strong class="source-inline">inference_pipeline_model</strong> is a generic <strong class="source-inline">mlflow.pyfunc.model</strong> model, not a <strong class="source-inline">mlflow.pytorch</strong> model anymore. In fact, if you compare <em class="italic">Figure 7.5</em> with <em class="italic">Figure 7.1</em>, which is our PyTorch fine-tuned DL model, there is a section about <strong class="source-inline">pytorch</strong> and its <strong class="source-inline">model_data</strong> and <strong class="source-inline">pytorch_version</strong>, which has now completely disappeared in <em class="italic">Figure 7.5</em>. For MLflow, it has no knowledge of the original model, which is a PyTorch model, but just a generic MLflow <strong class="source-inline">pyfunc</strong> model as the newly wrapped model. This is great news since now we only need one generic MLflow <strong class="source-inline">pyfunc</strong> API to load the model, regardless of how complex the wrapped model is and how many more preprocessing and postprocessing steps are inside this generic <strong class="source-inline">pyfunc</strong> model when we implement it in the next section. </p>
			<ol>
				<li value="6">We now can load <strong class="source-inline">inference_pipeline_model</strong> using the generic <strong class="source-inline">mlflow.pyfunc.load_model</strong> to load the model and do prediction with an input <strong class="source-inline">pandas</strong> DataFrame as follows:<p class="source-code">input = {"text":["what a disappointing movie","Great movie"]}</p><p class="source-code">input_df = pd.DataFrame(input)</p><p class="source-code">with mlflow.start_run():</p><p class="source-code">    loaded_model = \</p><p class="source-code">    mlflow.pyfunc.load_model(inference_pipeline_uri)</p><p class="source-code">    results = loaded_model.predict(input_df)</p></li>
			</ol>
			<p>Here, <strong class="source-inline">inference_pipeline_uri</strong> is the URI produced in <em class="italic">step 4</em> as the unique<a id="_idIndexMarker463"/> identifier for <strong class="source-inline">inference_pipeline_model</strong>. For example, an <strong class="source-inline">inference_pipeline_uri</strong> value could look as follows:</p>
			<p class="source-code">'runs:/6edf6013d2454f7f8a303431105f25f2/inference_pipeline_model'</p>
			<p>Once the model is loaded, we can just call the <strong class="source-inline">predict</strong> function to score the <strong class="source-inline">input_df</strong> DataFrame. This calls the <strong class="source-inline">predict</strong> function of our newly implemented <strong class="source-inline">InferencePipleine</strong> class, as described in <em class="italic">step 2</em>. The results will look something like the following:</p>
			<div>
				<div id="_idContainer073" class="IMG---Figure">
					<img src="Images/B18120_07_06.jpg" alt="Figure 7.6 – Output of the inference pipeline in a pandas DataFrame format&#13;&#10;" width="129" height="112"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.6 – Output of the inference pipeline in a pandas DataFrame format</p>
			<p>If you see the prediction results like in <em class="italic">Figure 7.6</em>, then you should feel proud that you have just implemented a working custom MLflow Python model that has enormous flexibility and power to enable us to implement preprocessing and postprocessing logic without changing any of the logging and loading model parts, as we will see in the next section.</p>
			<p class="callout-heading">Creating a New Flavor of MLflow Custom Model</p>
			<p class="callout">As shown in this <a id="_idIndexMarker464"/>chapter, we can build a wrapped MLflow custom model using an already trained model for inference purposes. It should be noted that it is also possible to build an entirely new flavor of MLflow custom model for training purposes. This is needed when you have a model that's not yet supported by the built-in MLflow model flavors. For example, if you want to train a brand new <strong class="bold">FastText</strong> model based on your own corpus <a id="_idIndexMarker465"/>but as of MLflow version 1.23.1, there is no <strong class="bold">FastText</strong> MLflow model flavor yet, then you can build a new <strong class="bold">FastText</strong> MLflow model flavor (see reference: <a href="mailto:https://medium.com/@pennyqxr/how-save-and-load-fasttext-model-in-mlflow-format-37e4d6017bf0">https://medium.com/@pennyqxr/how-save-and-load-fasttext-model-in-mlflow-format-37e4d6017bf0</a>). Interested readers can also find more references in the <em class="italic">Further reading</em> section at the end of this chapter.</p>
			<h1 id="_idParaDest-89"><a id="_idTextAnchor088"/>Implementing preprocessing and postprocessing steps in a DL inference pipeline</h1>
			<p>Now that<a id="_idIndexMarker466"/> we<a id="_idIndexMarker467"/> have a basic generic MLflow Python model that can do prediction on an input <strong class="source-inline">pandas</strong> DataFrame and produce output in another <strong class="source-inline">pandas</strong> DataFrame, we are ready to tackle the multi-step inference scenario described before. Note that while the initial implementation in the previous section might not look earth-shaking, this opens doors for implementing preprocessing and postprocessing logic that was not possible before while maintaining the capability of using the generic <strong class="source-inline">mlflow.pyfunc.log_model</strong> and <strong class="source-inline">mlflow.pyfunc.load_model</strong> to treat the entire inference pipeline as a generic <strong class="source-inline">pyfunc</strong> model, regardless of how complex the original DL model is and how many additional preprocessing and postprocessing steps there are. Let's see how we can do this in this section. You may want to check out the VS Code notebook for <strong class="source-inline">multistep_inference_model.py</strong> from GitHub (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/notebooks/multistep_inference_model.py">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/notebooks/multistep_inference_model.py</a>) to follow through the steps in this section. </p>
			<p>In <em class="italic">Figure 7.3</em>, we depicted two preprocessing steps prior to the model prediction, and two postprocessing steps after the model prediction. So where and how do we add the preprocessing and postprocessing logic while keeping this entire inference pipeline as a single MLflow model? It turns out the main changes will happen in the <strong class="source-inline">InferencePipeline</strong> class <a id="_idIndexMarker468"/>implemented<a id="_idIndexMarker469"/> in the previous section. Let's walk through the implementation and changes step by step in the following subsections.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor089"/>Implementing language detection preprocessing logic</h2>
			<p>Let's first<a id="_idIndexMarker470"/> implement <a id="_idIndexMarker471"/>the language detection preprocessing logic:</p>
			<ol>
				<li value="1">To detect the language type of the input text, we can use Google's <strong class="bold">Compact Language Detector v3</strong>. Note that this language detector is a neural network model that contains native code (the core implementation is in C++) with a Python binding so that we can use it in a Python environment (<a href="https://github.com/google/cld3">https://github.com/google/cld3</a>). As this model cannot be serialized by using Python serialization protocols such as pickle, it would be a major challenge to figure out how to package this in an MLflow <strong class="source-inline">pyfunc</strong> model. The good news is that MLflow's <strong class="source-inline">load_context</strong> method allows us to load this model without worrying about serialization and deserialization. We only need to add two lines of code in the <strong class="source-inline">load_context</strong> method in the <strong class="source-inline">InferencePipeline</strong> class as follows to load the language detector model:<p class="source-code">import gcld3</p><p class="source-code">self.detector = gcld3.NNetLanguageIdentifier(</p><p class="source-code">    min_num_bytes=0,</p><p class="source-code">    max_num_bytes=1000)</p></li>
			</ol>
			<p>The preceding two lines are added into the <strong class="source-inline">load_context</strong> method, along with the preexisting statement that loads the fine-tuned DL model for sentiment classification. This will allow the language detector to be loaded as soon as the initialization of the <strong class="source-inline">InferencePipeline</strong> class is done. This language detector will use up to the first 1,000 bytes of the input to determine the language type. Once this language detector is loaded, then we can use it to detect the language in a preprocessing method.</p>
			<ol>
				<li value="2">In a preprocessing method for language detection, we will accept each row of the input text, detect the language, and return the language type as a <strong class="source-inline">string</strong> as follows:<p class="source-code">def preprocessing_step_lang_detect(self, row):</p><p class="source-code">    language_detected = \</p><p class="source-code">    self.detector.FindLanguage(text=row[0])</p><p class="source-code">    if language_detected.language != 'en':</p><p class="source-code">        print("found Non-English Language text!")</p><p class="source-code">    return language_detected.language</p></li>
			</ol>
			<p>The<a id="_idIndexMarker472"/> implementation <a id="_idIndexMarker473"/>is straightforward. We also add a printout to see if we see any non-English text in the input to the console. If your business logic requires you to implement any preemptive actions when dealing with some specific language, then you can add more logic in this method. Here, we just return the language type detected.</p>
			<ol>
				<li value="3">Then, in the <strong class="source-inline">sentiment_classifier</strong> method that scores each row of the input, we can just add one line prior to the prediction to first detect the language as follows:<p class="source-code">language_detected = self.preprocessing_step_lang_detect(row)</p></li>
			</ol>
			<p>Later, we pass along the <strong class="source-inline">language_detected</strong> variable to the response as we will see in the postprocessing logic implementation.</p>
			<p>And that's all it takes to implement the language detection as a preprocessing step in the inference pipeline. </p>
			<p>Now let's see how to implement the other step: cache, which requires both preprocessing (detecting if there are any preexisting matched prediction results for the same input) and<a id="_idIndexMarker474"/> postprocessing (storing a key-value pair of input and prediction results in the cache).</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor090"/>Implementing caching preprocessing and postprocessing logic</h2>
			<p>Let's see<a id="_idIndexMarker475"/> how we can <a id="_idIndexMarker476"/>implement <a id="_idIndexMarker477"/>caching<a id="_idIndexMarker478"/> in the <strong class="source-inline">InferencePipeline</strong> class:</p>
			<ol>
				<li value="1">We can add a new statement to initialize the cache store in the <strong class="source-inline">init</strong> method, as this has no problem being serialized or deserialized:<p class="source-code">from cachetools import LRUCache</p><p class="source-code">self.cache = LRUCache(100)</p></li>
			</ol>
			<p>This will initialize a Least Recently Used cache with 100 objects stored.</p>
			<ol>
				<li value="2">Next, we will add a preprocessing method to detect if any input is in the cache:<p class="source-code">def preprocessing_step_cache(self, row):</p><p class="source-code">    if row[0] in self.cache:</p><p class="source-code">        print("found cached result")</p><p class="source-code">        return self.cache[row[0]]</p></li>
			</ol>
			<p>If it finds the exact input row as a key already in the cache, then it returns the cached value.</p>
			<ol>
				<li value="3">In the <strong class="source-inline">sentiment_classifier</strong> method, we can add the preprocessing step to check the cache and if it finds the cache, then it will immediately return the cached response without invoking the expensive DL model classifier:<p class="source-code">    cached_response = self.preprocessing_step_cache(row)</p><p class="source-code">    if cached_response is not None:</p><p class="source-code">        return cached_response</p></li>
			</ol>
			<p>This preprocessing step should be placed as the first step in the <strong class="source-inline">sentiment_classifier</strong> method, before doing language detection and model prediction. This can significantly speed up real-time prediction responses when there are many duplicated inputs.</p>
			<ol>
				<li value="4">Also in the <strong class="source-inline">sentiment_classifier</strong> method, we need to add a postprocessing step to store new input and prediction responses in the cache:<p class="source-code">self.cache[row[0]]=response</p></li>
			</ol>
			<p>That's it. We <a id="_idIndexMarker479"/>have<a id="_idIndexMarker480"/> successfully<a id="_idIndexMarker481"/> added <a id="_idIndexMarker482"/>caching as a preprocessing and postprocessing step in the <strong class="source-inline">InferencePipeline</strong> class.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor091"/>Implementing response composition postprocessing logic</h2>
			<p>Now let's see<a id="_idIndexMarker483"/> how <a id="_idIndexMarker484"/>we can implement the response composition logic as a postprocessing step after the original DL model prediction is invoked and the result is returned. Just returning a prediction label of <strong class="source-inline">positive</strong> or <strong class="source-inline">negative</strong> usually is not enough, as we would like to know which version of the model was used and what language was detected for debugging and diagnosis in the production environment. The response to the caller of the inference pipeline will no longer be a plain string, but rather a serialized JSON string. Follow these steps to implement this postprocessing logic:</p>
			<ol>
				<li value="1">In the <strong class="source-inline">init</strong> method of the <strong class="source-inline">InferencePipeline</strong> class, we need to add a new <strong class="source-inline">inference_pipeline_uri</strong> parameter, so that we can capture this generic MLflow <strong class="source-inline">pyfunc</strong> model's reference for provenance tracking purposes. Both the <strong class="source-inline">finetuned_model_uri</strong> and <strong class="source-inline">inference_pipeline_uri</strong> parameters will be part of the response's JSON object. The <strong class="source-inline">init</strong> method now looks like the following:<p class="source-code">def __init__(self, </p><p class="source-code">             finetuned_model_uri,</p><p class="source-code">             inference_pipeline_uri=None):</p><p class="source-code">    self.cache = LRUCache(100)</p><p class="source-code">    self.finetuned_model_uri = finetuned_model_uri</p><p class="source-code">    self.inference_pipeline_uri = inference_pipeline_uri</p></li>
				<li>In the <strong class="source-inline">sentiment_classifier</strong> method, add a new postprocessing statement to compose a new response based on the language detected, predicted<a id="_idIndexMarker485"/> label, and<a id="_idIndexMarker486"/> the model metadata including both <strong class="source-inline">finetuned_model_uri</strong> and <strong class="source-inline">inference_pipeline_uri</strong>:<p class="source-code">response = <strong class="bold">json.dumps</strong>({</p><p class="source-code">                'response': {</p><p class="source-code">                    'prediction_label': pred_label</p><p class="source-code">                },</p><p class="source-code">                'metadata': {</p><p class="source-code">                    'language_detected': language_detected,</p><p class="source-code">                },</p><p class="source-code">                'model_metadata': {</p><p class="source-code">                    'finetuned_model_uri': self.finetuned_model_uri,</p><p class="source-code">                    'inference_pipeline_model_uri': self.inference_pipeline_uri</p><p class="source-code">                },</p><p class="source-code">            })                    </p></li>
			</ol>
			<p>Note that we use <strong class="source-inline">json.dumps</strong> to encode a nested Python string object into a JSON formatted string, so that the caller can easily parse out the response using JSON tools.</p>
			<ol>
				<li value="3">In the <strong class="source-inline">mlflow.pyfunc.log_model</strong> statement, we need to add a new <strong class="source-inline">inference_pipeline_uri</strong> parameter when calling the <strong class="source-inline">InferencePipeline</strong> class:<p class="source-code">mlflow.pyfunc.log_model(</p><p class="source-code">  artifact_path=MODEL_ARTIFACT_PATH,</p><p class="source-code">  conda_env=CONDA_ENV,</p><p class="source-code">  python_model=InferencePipeline(finetuned_model_uri,</p><p class="source-code">  <strong class="bold">inference_pipeline_uri</strong>),</p><p class="source-code">  signature=signature)</p></li>
			</ol>
			<p>This will log a new inference pipeline model with all the additional processing logic we implemented. This completes the implementation of the multi-step inference pipeline depicted in <em class="italic">Figure 7.3</em>. </p>
			<p>Note that once the model is logged with all these new steps, to consume this new inference pipeline, that's to say, to load this model, requires zero code changes. We can load the newly<a id="_idIndexMarker487"/> logged <a id="_idIndexMarker488"/>model the same way as before: </p>
			<p class="source-code">loaded_model = mlflow.pyfunc.load_model(inference_pipeline_uri)</p>
			<p>If you have followed through the steps up until now, you should also run the VS Code notebook for <strong class="source-inline">multistep_inference_model.py</strong> cell by cell up to <em class="italic">step 3</em> described in this subsection. Now we can try to use this new multi-step inference pipeline to test it out. We can prepare a new set of input data where there are duplicates and a non-English text string as follows:</p>
			<p class="source-code">input = {"text":["what a disappointing movie", "Great movie", "Great movie", "很好看的电影"]}</p>
			<p class="source-code">input_df = pd.DataFrame(input)</p>
			<p>This input includes two duplicated entries (<strong class="source-inline">Great movie</strong>) and one Chinese text string (the last element in the input list, where the meaning of the Chinese text is the same as <strong class="source-inline">Great Movie</strong>). Now we can just load the model and call <strong class="source-inline">results = loaded_model.predict(input_df)</strong> as before. And during the execution of this predict statement, you should see the following two statements in the console output:</p>
			<p class="source-code">found cached result </p>
			<p class="source-code">found Non-English language text.</p>
			<p>This means that our caching and language detector works! </p>
			<p>We can also print out the results to double-check whether our multi-step pipeline works or not using the following code:</p>
			<p class="source-code">for i in range(results.size):</p>
			<p class="source-code">    print(results['text'][i])</p>
			<p>This will print out the full content for each row of the response. Here, we display the output for<a id="_idIndexMarker489"/> the<a id="_idIndexMarker490"/> last one (which has the Chinese text) as an example:</p>
			<div>
				<div id="_idContainer074" class="IMG---Figure">
					<img src="Images/B18120_07_07.jpg" alt="Figure 7.7 – JSON response for the Chinese text string input using the multi-step inference pipeline&#13;&#10;" width="920" height="305"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.7 – JSON response for the Chinese text string input using the multi-step inference pipeline</p>
			<p>As can be seen in <em class="italic">Figure 7.7</em>, <strong class="source-inline">prediction_label</strong> is included in the response (which is <strong class="source-inline">negative</strong>). Since we have been using <strong class="bold">TinyBERT</strong> for the English language only, this incorrect prediction is expected. If we switch to a multilingual pretrained language model <a id="_idIndexMarker491"/>such as <strong class="bold">bert-base-multilingual-uncased</strong> (<a href="https://huggingface.co/bert-base-multilingual-uncased">https://huggingface.co/bert-base-multilingual-uncased</a>) as the foundation model during model training and fine-tuning, then supporting inference for multiple languages is possible. In fact, the multilingual version of BERT supports 102 world languages. If we look at the <strong class="source-inline">language_detected</strong> field under the <strong class="source-inline">metadata</strong> section in the JSON response, we see the string <strong class="source-inline">"zh"</strong>, which represents the Chinese language. This is what the language detector produced in the preprocessing step. Additionally, the <strong class="source-inline">model_metadata</strong> section includes both the original <strong class="source-inline">finetuned_model_uri</strong> and <strong class="source-inline">inference_pipeline_model_uri</strong>. These are MLflow tracking server-specific URIs that we can use to uniquely trace and identify which fine-tuned model and inference pipeline was used for this prediction result. This is very important for provenance tracking and diagnosis analysis in the production environment. Comparing this complete JSON output with the earlier prediction label output in <em class="italic">Figure 7.6</em>, this has much richer contextual information for the consumer of the inference pipeline to use. </p>
			<p>If you see the JSON output in your notebook run like <em class="italic">Figure 7.7</em>, give yourself a round of applause, because <a id="_idIndexMarker492"/>you<a id="_idIndexMarker493"/> have just completed a big milestone in implementing a multi-step inference pipeline that can be reused and deployed into production for realistic business scenarios.</p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor092"/>Implementing an inference pipeline as a new entry point in the main MLproject</h1>
			<p>Now that<a id="_idIndexMarker494"/> we have<a id="_idIndexMarker495"/> successfully implemented a multi-step inference pipeline as a new custom MLflow model, we can go one step further by incorporating this as a new entry point in the main <strong class="bold">MLproject</strong> so that we can run the following entire pipeline end to end (<em class="italic">Figure 7.8</em>). Check out this chapter's code from GitHub to follow through and run the pipeline in your local environment.</p>
			<div>
				<div id="_idContainer075" class="IMG---Figure">
					<img src="Images/B18120_07_08.jpg" alt="Figure 7.8 – End-to-end pipeline using MLproject&#13;&#10;" width="1265" height="160"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.8 – End-to-end pipeline using MLproject</p>
			<p>We can add the new entry point <strong class="source-inline">inference_pipeline_model</strong> into the <strong class="source-inline">MLproject</strong> file. You can check out this file on the GitHub repository (<a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/MLproject">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/MLproject</a>):</p>
			<p class="source-code">inference_pipeline_model:</p>
			<p class="source-code">    parameters:</p>
			<p class="source-code">      finetuned_model_run_id: { type: str, default: None }</p>
			<p class="source-code">    command: "python pipeline/inference_pipeline_model.py --finetuned_model_run_id {finetuned_model_run_id}"</p>
			<p>This entry<a id="_idIndexMarker496"/> point <a id="_idIndexMarker497"/>or step can be invoked either standalone or as part of the entire pipeline depicted in <em class="italic">Figure 7.8</em>. As a reminder, make sure you have set up the environment variables as described in the <strong class="source-inline">README</strong> file of this chapter for the MLflow tracking server and backend storage URIs before you execute the MLflow <strong class="source-inline">run</strong> commands. This step logs and registers a new <strong class="source-inline">inference_pipeline_model</strong>, which itself contains multi-step preprocessing and postprocessing logic. The following command can be used to run this step at the root level of the <strong class="source-inline">chapter07</strong> folder, if you know the <strong class="source-inline">finetuned_model_run_id</strong>:</p>
			<p class="source-code">mlflow run . -e inference_pipeline_model  --experiment-name dl_model_chapter07 -P finetuned_model_run_id=07b900a96af04037a956c74ef691396e</p>
			<p>This will not only log a new <strong class="source-inline">inference_pipeline_model</strong> in the MLflow tracking server but will also register a new version of <strong class="source-inline">inference_pipeline_model</strong> in the MLflow model registry. You can find the registered <strong class="source-inline">inference_pipeline_model</strong> in your local MLflow server with the following link:</p>
			<p class="source-code">http://localhost/#/models/inference_pipeline_model/</p>
			<p>As an example, a registered <strong class="source-inline">inference_pipeline_model</strong> version 6 is shown in the following screenshot:</p>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="Images/B18120_07_09.jpg" alt="Figure 7.9 – A registered inference_pipeline_model at version 6&#13;&#10;" width="794" height="385"/>
				</div>
			</div>
			<p class="figure-caption">Figure 7.9 – A registered inference_pipeline_model at version 6</p>
			<p>You can also run <a id="_idIndexMarker498"/>the <a id="_idIndexMarker499"/>entire end-to-end pipeline depicted in <em class="italic">Figure 7.8</em> as follows:</p>
			<p class="source-code">mlflow run . --experiment-name dl_model_chapter07</p>
			<p>This will run all the steps in this end-to-end pipeline and finish with a logged and registered <strong class="source-inline">inference_pipeline_model</strong> in the model registry.</p>
			<p>The implementation of the Python code for <strong class="source-inline">inference_pipeline_model.py</strong>, which is executed when the entry point <strong class="source-inline">inference_pipeline_model</strong> is invoked, is basically copying the <strong class="source-inline">InferencePipeline</strong> class we implemented in the VS Code notebook for <strong class="source-inline">multistep_inference_model.py</strong> with a couple of small changes as follows:</p>
			<ul>
				<li>Adding a task function to be executed as a parameterized entry point for this step:<p class="source-code">def task(finetuned_model_run_id, pipeline_run_name):</p></li>
			</ul>
			<p>What this function does is starting a new MLflow run to log and register a new inference pipeline model.</p>
			<ul>
				<li>Turning on the model registration while logging as follows:<p class="source-code">mlflow.pyfunc.log_model(</p><p class="source-code">    artifact_path=MODEL_ARTIFACT_PATH,</p><p class="source-code">    conda_env=CONDA_ENV,          </p><p class="source-code">    python_model=InferencePipeline(</p><p class="source-code">        finetuned_model_uri, </p><p class="source-code">        inference_pipeline_uri),</p><p class="source-code">    signature=signature,</p><p class="source-code">    <strong class="bold">registered_model_name=MODEL_ARTIFACT_PATH</strong>)</p></li>
			</ul>
			<p>Note that we assign to <strong class="source-inline">registered_model_name</strong> the value of <strong class="source-inline">MODEL_ARTIFACT_PATH</strong>, which is <strong class="source-inline">inference_pipeline_model</strong>. This enables the model to be registered under this name in the MLflow model registry, as seen in <em class="italic">Figure 7.9</em>.</p>
			<p>The complete <a id="_idIndexMarker500"/>code <a id="_idIndexMarker501"/>for this new entry point can be found in the GitHub repository: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/pipeline/inference_pipeline_model.py">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/pipeline/inference_pipeline_model.py</a>.</p>
			<p>Note that we also need to add a new section in the <strong class="source-inline">main.py</strong> file to allow the <strong class="source-inline">inference_pipeline_model</strong> entry point to also be callable from within the <strong class="source-inline">main</strong> entry point. The implementation is straightforward, just like adding other steps previously as described in <a href="B18120_04_ePub.xhtml#_idTextAnchor050"><em class="italic">Chapter 4</em></a>, <em class="italic">Tracking Code and Data Versioning</em>. Interested readers should check out the <strong class="source-inline">main.py</strong> file from GitHub to take a look at the implementation: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/main.py">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter07/main.py</a>.</p>
			<p>This concludes the implementation of adding a new entry point in the <strong class="bold">MLproject</strong> so that we can run the <a id="_idIndexMarker502"/>multi-step<a id="_idIndexMarker503"/> inference pipeline creation, logging, and registering using the MLflow run command tool.</p>
			<h1 id="_idParaDest-94"><a id="_idTextAnchor093"/>Summary</h1>
			<p>In this chapter, we covered a very important topic on creating a multi-step inference pipeline using MLflow's custom Python model approach, namely <strong class="source-inline">mlflow.pyfunc.PythonModel</strong>. </p>
			<p>We discussed four patterns of inference workflow in production where usually a single trained model is not enough to complete the business application requirements. It is highly likely some preprocessing and postprocessing logic is not seen during the model training and development stage. That's why MLflow's <strong class="source-inline">pyfunc</strong> approach is an elegant approach to implementing a custom MLflow model that can wrap a trained DL model with additional preprocessing and postprocessing logic. </p>
			<p>We successfully implemented an inference pipeline model that wraps our DL sentiment classifier with language detection using Google's Compact Language Detector, caching, and additional model metadata in addition to the prediction label. We went one step further to incorporate the inference pipeline model creation step into the end-to-end model development workflow so that we can produce a registered inference pipeline model with one MLflow run command. </p>
			<p>The skills and lessons learned in this chapter will be critical for anyone who wants to implement a real-world inference pipeline using the MLflow <strong class="source-inline">pyfunc</strong> approach. This also opens doors for supporting flexible and powerful deployment into production scenarios, which we will cover in the next chapter.</p>
			<h1 id="_idParaDest-95"><a id="_idTextAnchor094"/>Further reading</h1>
			<ul>
				<li><em class="italic">MLflow Models</em> (MLflow documentation): <a href="https://www.mlflow.org/docs/latest/models.html#">https://www.mlflow.org/docs/latest/models.html#</a></li>
				<li><em class="italic">Implementing the statsmodels flavor in MLflow</em>: <a href="https://blog.stratio.com/implementing-the-statsmodels-flavor-in-mlflow/">https://blog.stratio.com/implementing-the-statsmodels-flavor-in-mlflow/</a></li>
				<li><em class="italic">InferLine: ML inference Pipeline Composition Framework</em>: <a href="https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-76.pdf">https://www2.eecs.berkeley.edu/Pubs/TechRpts/2018/EECS-2018-76.pdf</a></li>
				<li><em class="italic">Batch Inference vs Online Inference</em>: <a href="https://mlinproduction.com/batch-inference-vs-online-inference/">https://mlinproduction.com/batch-inference-vs-online-inference/</a></li>
				<li><em class="italic">Lessons from building a small MLOps pipeline</em>: <a href="https://www.nestorsag.com/blog/lessons-from-building-a-small-ml-ops-pipeline/">https://www.nestorsag.com/blog/lessons-from-building-a-small-ml-ops-pipeline/</a></li>
				<li><em class="italic">Text summarizer on Hugging Face with MLflow</em>: <a href="https://vishsubramanian.me/hugging-face-with-mlflow/">https://vishsubramanian.me/hugging-face-with-mlflow/</a></li>
			</ul>
		</div>
	</div></body></html>