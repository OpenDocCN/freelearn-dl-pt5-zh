- en: '*Chapter 10*: Machine Teaching'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第10章*：机器教学'
- en: 'The great excitement about **reinforcement learning** (**RL**) is, to a significant
    extent, due to its similarities to human learning: an RL agent learns from experience.
    This is also why many consider it as the path to artificial general intelligence.
    On the other hand, if you think about it, reducing human learning to just trial
    and error would be a gross underestimation. We don''t discover everything we know,
    in science, art, engineering, and so on, from scratch when we are born! Instead,
    we build on knowledge and intuition that have been accumulated over thousands
    of years! We transfer this knowledge among us through different, structured or
    unstructured, forms of **teaching**. This capability makes it possible for us
    to gain skills relatively quickly and advance common knowledge.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**强化学习**（**RL**）的巨大兴奋感，在很大程度上源于它与人类学习的相似性：RL智能体通过经验进行学习。这也是为什么许多人认为它是通向人工通用智能的路径。另一方面，如果你仔细想想，将人类学习仅仅归结为反复试验，实在是大大的低估了人类的学习过程。我们并非从出生开始就从零发现我们所知道的一切——无论是在科学、艺术、工程等领域！相反，我们建立在数千年来积累的知识和直觉之上！我们通过各种不同的、有结构的或无结构的**教学**形式将这些知识在我们之间传递。这种能力使得我们能够相对快速地获得技能并推动共识知识的进步。'
- en: 'When we think about it from this perspective, what we are doing with machine
    learning seems quite inefficient: we dump a bunch of raw data into algorithms,
    or expose them to an environment, in the case of RL, and train them with virtually
    no guidance. This is partly why machine learning requires so much data and fails
    at times.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，我们使用机器学习的方式似乎相当低效：我们将大量原始数据投入到算法中，或者让它们暴露于环境中（对于RL而言），几乎没有任何指导。这部分也是为什么机器学习需要如此大量的数据，并且有时会失败的原因。
- en: '**Machine teaching** (**MT**) is an emerging approach that shifts the focus
    to extracting knowledge from a teacher, rather than raw data, which guides the
    process of training machine learning algorithms. In turn, learning new skills
    and mappings is achieved more efficiently and with less data, time, and compute.
    In this chapter, we will introduce the components of MT for RL and some of its
    most important methods, such as reward function engineering, curriculum learning,
    demonstration learning, and action masking. At the end, we will also discuss the
    downsides and the future of MT. More concretely, we will cover the following topics
    in this chapter:'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器教学**（**MT**）是一种新兴的方式，它将重点转向从教师中提取知识，而不是仅仅依赖原始数据，从而指导训练机器学习算法的过程。反过来，学习新技能和映射的过程变得更加高效，且所需的数据、时间和计算资源更少。在本章中，我们将介绍MT在RL中的组成部分及其一些最重要的方法，例如奖励函数工程、课程学习、示范学习和动作屏蔽。最后，我们还将讨论MT的缺点与未来发展。具体而言，我们将在本章中涵盖以下内容：'
- en: Introduction to MT
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MT简介
- en: Engineering the reward function
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励函数的设计
- en: Curriculum learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 课程学习
- en: Warm starts and demonstration learning
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 热启动与示范学习
- en: Action masking
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动作屏蔽
- en: Concept networks
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 概念网络
- en: Downsides and the promises of MT
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MT的缺点与前景
- en: Technical requirements
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'All the code for the chapter can be found at the following GitHub URL:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的所有代码可以在以下GitHub链接找到：
- en: '[https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python)'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python](https://github.com/PacktPublishing/Mastering-Reinforcement-Learning-with-Python)'
- en: Introduction to MT
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: MT简介
- en: MT is the name of a general approach and collection of methods to efficiently
    transfer knowledge from a teacher – a subject matter expert – to a machine learning
    algorithm. With that, we aim to make the training much more efficient, and even
    feasible for tasks that would be impossible to achieve otherwise. Let's talk about
    what MT is in more detail, why we need it, and what its components are.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: MT是一种通用方法及其相关技术的集合，旨在高效地将知识从教师——即学科专家——转移到机器学习算法中。通过这种方式，我们旨在使训练过程更加高效，甚至使那些否则不可能完成的任务变得可行。接下来，我们将详细讨论MT是什么，为什么我们需要它，以及它的组成部分。
- en: Understanding the need for MT
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解MT的需求
- en: 'Did you know that the United States is expected to spend about 1.25 trillion
    dollars, around 5% of its gross domestic product, on education in 2021? This should
    speak to the existential significance of education to our society and civilization
    (and many would argue that we should spend more). We humans have built such a
    giant education system, which we expect people to spend many years in, because
    we don''t expect ourselves to be able to decipher the alphabet or math on our
    own. Not just that, we continuously learn from teachers around us, about how to
    use software, how to drive, how to cook, and so on. These teachers don''t have
    to be human teachers: books, blog posts, manuals, and course materials all distill
    valuable information for us so that we can learn, not just in school but throughout
    our lives.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 你知道吗？美国预计在2021年将花费约1.25万亿美元，约占其国内生产总值的5%，用于教育开支。这应该能反映教育对我们社会和文明的生死攸关的重要性（许多人会争辩说，我们应该投入更多）。我们人类建立了这样一个庞大的教育系统，我们期望人们在其中花费多年，因为我们不指望自己能够独立解码字母表或数学。不仅如此，我们不断从身边的老师那里学习，如何使用软件、如何开车、如何做饭，等等。这些老师不必是人类教师：书籍、博客文章、手册和课程材料都为我们提炼了有价值的信息，让我们能够学习，不仅是在学校，而是在整个生活过程中。
- en: I hope this convinces you of the importance of teaching. But if you found this
    example too populist and perhaps a bit irrelevant to RL, let's discuss how MT
    could specifically help in RL.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这能让你信服教学的重要性。但如果你觉得这个例子过于民粹化，或许与RL有些不相关，那么让我们来讨论机器翻译（MT）如何在RL中发挥特定作用。
- en: Feasibility of learning
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 学习的可行性
- en: Have you ever felt overwhelmed when trying to learn something on your own, without
    a (good) teacher? This is akin to an RL agent not figuring out a good policy for
    the problem at hand due to an overwhelming number of possible policies. One of
    the main obstacles in this process is the lack of proper feedback about their
    quality. You can also think of **hard exploration problems** with sparse rewards
    in the same context, a serious challenge in RL.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾在没有（好的）老师的情况下，尝试独自学习某些东西时感到力不从心？这就像一个RL代理由于可能的策略数量过多，无法为当前问题找到合适的策略。这个过程中主要的障碍之一是缺乏关于策略质量的适当反馈。你也可以将**困难的探索问题**与稀疏奖励相提并论，这是RL中的一个严重挑战。
- en: 'Consider the following example: an RL agent is trying to learn chess against
    a competitive opponent with a reward of +1 for winning, 0 for draw, and -1 for
    losing at the end of the game. The RL agent needs to stumble upon tens of "good
    moves," one after another, and among many alternative moves at each step, to be
    able to get its first 0 or +1 reward. Since this is a low likelihood, the training
    is likely to fail without a huge exploration budget. A teacher, on the other hand,
    might guide the exploration so that the RL agent knows at least a few ways to
    succeed, from which it can gradually improve upon the winning strategies.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下例子：一个RL代理正在与一个竞争对手对弈国际象棋，胜利奖励为+1，平局奖励为0，失败奖励为-1。RL代理需要不断“偶然”地找到数十个“好棋步”，一招接一招，在每一步的众多备选棋步中，才能获得第一次0或+1的奖励。由于这种情况的发生概率较低，如果没有巨大的探索预算，训练很可能会失败。另一方面，教师可以引导探索，使得RL代理至少知道几种成功的方式，从而逐渐改进其胜利策略。
- en: Info
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: When DeepMind created its AlphaStar agent to play StarCraft II, they used supervised
    learning to train the agent on past human game logs before going into RL-based
    training. Human players in some sense were the first teachers of the agent, and
    without them, the training would be impractical/too costly. To support this argument,
    you can take the example of the OpenAI Five agent trained to play Dota 2\. It
    took almost a year to train the agent.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 当DeepMind创建其AlphaStar代理来玩《星际争霸II》时，他们首先使用监督学习训练代理，基于过去的人类游戏日志，然后才进入基于强化学习（RL）的训练。在某种意义上，人类玩家是代理的第一批教师，没有他们，这种训练将变得不可行或成本过高。为了支持这一论点，可以以训练OpenAI
    Five代理玩《Dota 2》的例子为例。训练该代理花费了近一年的时间。
- en: 'The following figure shows the agent in action:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 以下图展示了代理在行动中的情形：
- en: '![Figure 10.1 – DeepMind''s AlphaStar agent in action (source: The AlphaStar
    Team, 2019)'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.1 – DeepMind的AlphaStar代理在行动中（来源：AlphaStar团队，2019）'
- en: '](img/B14160_10_1.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_10_1.jpg)'
- en: 'Figure 10.1 – DeepMind''s AlphaStar agent in action (source: The AlphaStar
    Team, 2019)'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 – DeepMind的AlphaStar代理在行动中（来源：AlphaStar团队，2019）
- en: In summary, having access to a teacher could make the learning feasible in a
    reasonable amount of time.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，有一个教师可以让学习在合理的时间内变得可行。
- en: Time, data, and compute efficiency
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 时间、数据和计算效率
- en: Let's say you have enough compute resources and can afford to try an enormous
    number of sequences of moves for the RL agent to discover winning strategies in
    an environment. Just because you can, doesn't mean that you should do it and waste
    all those resources. A teacher could help you to greatly reduce the training time,
    data, and compute. You can use the resources you saved to iterate on your ideas
    and come up with better agents.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你拥有足够的计算资源，并且能够尝试大量的动作序列，让RL代理在一个环境中发现获胜策略。仅仅因为你能做到，并不意味着你应该这样做并浪费所有这些资源。教师可以帮助你大大减少训练时间、数据和计算量。你可以利用节省下来的资源来反复改进你的想法，开发更好的代理人。
- en: Tip
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Do you have mentors in your life to help you with your career, education, marriage,
    and so on? Or do you read books about these topics? What is your motivation? You
    don't want to repeat the mistakes of others or reinvent what others already know
    just to waste your time, energy, and opportunities, do you? MT similarly helps
    your agent jumpstart its task.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否有导师帮助你在职业、教育、婚姻等方面的发展？或者你是否阅读过关于这些主题的书籍？你的动力是什么？你不想重复他人的错误，也不想重新发明别人已经知道的东西，浪费你的时间、精力和机会，对吧？MT以类似的方式帮助你的代理人快速开始任务。
- en: 'The benefit of MT goes beyond the feasibility of learning or its efficiency.
    Next, let''s talk about another aspect: the safety of your agent.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: MT的好处不仅仅在于学习的可行性或效率。接下来，我们来谈谈另一个方面：你代理人的安全性。
- en: Ensuring the safety of the agent
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 确保代理人的安全性
- en: A teacher is a subject matter expert on a topic. Therefore, a teacher usually
    has a pretty good idea about what actions under which conditions can get the agent
    in trouble. The teacher can inform the agent about these conditions by limiting
    the actions it could take to ensure its safety. For example, while training an
    RL agent for a self-driving car, it is natural to limit the speed of the car depending
    on the conditions of the road. This is especially needed if the training happens
    in the real world so that the agent does not blindly explore crazy actions to
    discover how to drive. Even when the training happens in a simulation, imposing
    these limitations will help with the efficient use of the exploration budget,
    related to the tip in the previous section.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 教师是某个主题的专家。因此，教师通常对在什么条件下采取哪些行动可能会让代理人陷入困境有一个相当清晰的认识。教师可以通过限制代理人可以采取的行动来告知代理人这些条件，从而确保其安全。例如，在为自动驾驶汽车训练强化学习（RL）代理时，根据路况限制汽车的速度是很自然的。如果训练发生在现实世界中，这一点尤为重要，以确保代理人不会盲目探索危险的行为来发现如何驾驶。即便训练发生在模拟环境中，施加这些限制也有助于高效使用探索预算，这与上一节中的提示相关。
- en: Democratizing machine learning
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习的民主化
- en: When teachers train students, they do not worry about the details of the biological
    mechanisms of learning, such as which chemicals are transferred between which
    brain cells. Those details are abstracted away from the teacher; neuroscientists
    and experts who study the brain put out research about effective teaching and
    learning techniques.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当教师训练学生时，他们不会担心学习的生物机制细节，比如哪些化学物质在什么脑细胞之间传递。这些细节被从教师的视野中抽象了出来；神经科学家和研究大脑的专家会发布关于有效教学和学习技术的研究。
- en: Just like how teachers don't have to be neuroscientists, subject matter experts
    don't have to be machine learning experts to train machine learning algorithms.
    The MT paradigm suggests abstracting the low-level details of machine learning
    away from the machine teacher by developing effective and intuitive teaching methods.
    With that, it would be much easier for subject matter experts to infuse their
    knowledge into machines. Eventually, this would lead to the democratization of
    machine learning and its much greater use in many applications.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 就像教师不需要是神经科学家一样，学科专家也不必是机器学习专家才能训练机器学习算法。MT范式通过开发有效且直观的教学方法，建议将机器学习的低层细节从机器教师中抽象出来。这样，学科专家将更容易将他们的知识注入到机器中。最终，这将导致机器学习的民主化，并使其在更多的应用中得到更广泛的使用。
- en: Data science, in general, requires combining business insights and expertise
    with mathematical tools and software to create value. When you want to apply RL
    to business problems, the situation is the same. This often requires either the
    data scientist to learn about the business or the subject matter expert to learn
    about data science, or people from both fields working together in a team. This
    poses a high bar for the adoption of (advanced) machine learning techniques in
    many settings, because it is rare for these two types of people to exist at the
    same time in the same place.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
- en: Info
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
- en: A McKinsey study shows that lack of analytical talent is a major barrier to
    unlocking the value in data and analytics. MT, its specific tools aside, is a
    paradigm to overcome these barriers by lowering the bar for entry for non-machine
    learning experts through the creation of intuitive tools to this end. To check
    out the study, visit [https://mck.co/2J3TFEj](https://mck.co/2J3TFEj).
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
- en: 'This vision we have just mentioned is aimed more for the long term as it requires
    a lot of research and abstractions on the machine learning side. The methods we
    will cover in this section will be pretty technical. For example, we will discuss
    the **action masking** method to limit the available actions for the agent depending
    on the state it is in, which will require coding and modifying the neural network
    outcomes. However, you can imagine an advanced MT tool listening to the teacher
    saying "don''t go over 40 miles per hour within the city limits," parsing that
    command, and implementing action masking under the hood for a self-driving car
    agent:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.2 – The future of MT?'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_10_2.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.2 – The future of MT?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
- en: Before closing this section and diving into the details of MT, let me put out
    a necessary disclaimer.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
- en: Disclaimer
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
- en: One of the most vocal proponents of the MT approach is Microsoft and its Autonomous
    Systems division. As of the time of writing this book, I am an employee of the
    Autonomous Systems organization of Microsoft, working toward the mission of using
    MT to create intelligent systems. However, my goal here is not to promote any
    Microsoft product or discourse, but to tell you about this emerging topic that
    I find important. In addition, I do not officially represent Microsoft in any
    capacity and my views on the topic may not necessarily align with the company's.
    If you are curious about Microsoft's view on MT, check out the blog post at [https://blogs.microsoft.com/ai/machine-teaching/](https://blogs.microsoft.com/ai/machine-teaching/)
    and the Autonomous Systems website at [https://www.microsoft.com/en-us/ai/autonomous-systems](https://www.microsoft.com/en-us/ai/autonomous-systems).
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
- en: Now, it is time to make the discussion more concrete. In the next section, let's
    look at the elements of MT.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the elements of MT
  id: totrans-50
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As MT is an emerging field, it is hard to formally define its elements. Still,
    let's look into some of the common components and themes used in it. We have already
    discussed who the machine teacher is, but let's start with that for the sake of
    completeness. Then, we will look into concepts, lessons, curriculum, training
    data, and feedback.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 由于机器教学是一个新兴领域，定义其元素是非常困难的。不过，让我们来看一下其中一些常见的组成部分和主题。我们已经讨论了机器教师是谁，但为了完整性，让我们从这点开始。接着，我们将探讨概念、课程、训练数据和反馈。
- en: Machine teacher
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器教师
- en: The **machine teacher**, or simply the **teacher**, is the subject matter expert
    of the problem at hand. In the absence of abstractions that decouple machine learning
    from teaching, this will be the data scientist – you – but this time with the
    explicit concern of guiding the training using your knowledge of the problem domain.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: '**机器教师**，或简称**教师**，是当前问题的主题专家。在没有将机器学习与教学解耦的抽象概念的情况下，教师通常是数据科学家——也就是你——但这次的角色是明确地利用你对问题领域的知识来指导训练。'
- en: Concept
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 概念
- en: 'The **concept** is a specific part of the skillset that is needed to solve
    the problem. Think about training a basketball player in real life. Training does
    not consist of only practice games but is divided into mastering individual skills
    as well. Some of these skills are as follows:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '**概念**是解决问题所需技能集的一个特定部分。想象一下在现实生活中训练一名篮球运动员。训练不仅仅是进行练习赛，还包括掌握个别技能。一些技能如下：'
- en: Shooting
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 投篮
- en: Passing
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 传球
- en: Dribbling
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运球
- en: Stopping and landing
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 停止与着陆
- en: 'Conventional training of an RL agent playing basketball would be through playing
    entire games, with which we would expect the agent to pick up these individual
    skills. MT suggests breaking the problem down into smaller concepts to learn,
    such as the skills we listed previously. This has several benefits:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的强化学习训练篮球智能体通常是通过进行整场比赛来实现，期望智能体通过比赛掌握这些个别技能。机器教学建议将问题拆解成更小的概念进行学习，比如我们之前列出的那些技能。这样做有几个好处：
- en: A monolithic task often comes with sparse rewards, which is challenging for
    an RL agent to learn from. For example, winning the basketball game would be +1
    and losing would be -1\. However, the machine teacher would know that winning
    a game would be possible through mastering individual skills. To train the agent
    on individual skills and concepts, there will be rewards assigned to them. This
    is helpful to get around the sparse reward issue and provide more frequent feedback
    to the agent in a manner that facilitates learning.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单一的任务往往伴随着稀疏的奖励，这对于强化学习智能体来说是很难从中学习的。例如，赢得篮球比赛的奖励是+1，输掉比赛则是-1。然而，机器教师会知道，通过掌握个别技能，赢得比赛是可能的。为了训练智能体掌握个别技能和概念，将会为这些技能和概念分配奖励。这有助于绕过稀疏奖励的问题，并为智能体提供更频繁的反馈，从而促进学习。
- en: The credit assignment problem is a serious challenge in RL, which is about the
    difficulty of attributing the reward in later stages to individual actions in
    the earlier ones. When the training is broken down into concepts, it is easier
    to see the concepts that the agent is not good at. To be specific, this does not
    solve the credit assignment problem in itself. It is still the teacher that determines
    whether mastering a particular concept is important. But once these concepts are
    defined by the teacher, it is easier to isolate what the agent is and isn't good
    at.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信用分配问题是强化学习中的一个严重挑战，它涉及到将后期阶段的奖励归因于早期阶段个别动作的难度。当训练被拆分成概念时，更容易看到智能体在哪些概念上不擅长。具体来说，这并不能解决信用分配问题。仍然是教师决定是否掌握某个特定概念很重要。但一旦这些概念被教师定义，就更容易隔离出智能体擅长和不擅长的部分。
- en: As a corollary to the preceding point, the teacher can allocate more of the
    training budget to concepts that need more training and/or are difficult to learn.
    This results in more efficient use of time and compute resources.
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为前述观点的推论，教师可以将更多的训练预算分配给那些需要更多训练和/或难以学习的概念。这样可以更高效地利用时间和计算资源。
- en: For all these reasons, a task that is impractical or costly to solve monolithically
    can be efficiently solved by breaking it down into concepts.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正因为如此，一个不切实际或代价高昂的任务，可以通过将其拆分成概念来高效地解决。
- en: Lessons and curriculum
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 课程和教学大纲
- en: Another important element in MT is called **curriculum learning**. While training
    the agent on a concept, exposing it to an expert-level difficulty may derail the
    training. Instead, what makes more sense is to start with some easy settings and
    increase the difficulty gradually. Each of these difficulty levels makes a separate
    **lesson**, and they, together with the success thresholds that define the transition
    criteria from one lesson to the next, comprise a **curriculum**.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 机器教学中的另一个重要元素是**课程学习**。在训练智能体某个概念时，暴露给它专家级难度可能会偏离训练目标。相反，更合理的做法是从一些简单的设置开始，逐步增加难度。这些难度级别分别构成一个**课程**，它们与定义从一个课程到下一个课程的过渡标准的成功阈值一起，组成了整个课程体系。
- en: Curriculum learning is one of the most important research areas in RL and we
    will elaborate more on it later. A curriculum may be designed by hand by the teacher,
    or an **auto-curriculum** algorithm can be used.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 课程学习是强化学习中最重要的研究领域之一，我们将在后续详细讲解。课程可以由教师手动设计，也可以使用**自动课程**算法来生成。
- en: Training material/data
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 训练材料/数据
- en: Related to the previous point, another aspect of MT is to engineer the data
    that the agent will learn from. For example, the machine teacher could seed the
    training with data that includes successful episodes while using off-policy methods,
    which can overcome hard exploration tasks. This data could be obtained from an
    existing non-RL controller or the teacher's actions. This approach is also called
    **demonstration learning**.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的观点相关，机器教学的另一个方面是设计智能体学习的数据。例如，机器教师可以通过使用非策略方法来为训练提供包含成功案例的数据，从而克服困难的探索任务。这些数据可以通过现有的非
    RL 控制器或教师的动作获取。这种方法也被称为**示范学习**。
- en: Info
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: Demonstration learning is a popular method to train RL agents, especially in
    robotics. An ICRA paper by Nair et al. shows robots how to pick and place objects
    to seed the training of an RL agent. Check out the video at [https://youtu.be/bb_acE1yzDo](https://youtu.be/bb_acE1yzDo).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 示范学习是一种流行的强化学习智能体训练方法，特别是在机器人领域。Nair 等人在 ICARA 论文中展示了如何通过示范机器人如何拾取和放置物体来为强化学习智能体的训练提供种子。请查看视频：[https://youtu.be/bb_acE1yzDo](https://youtu.be/bb_acE1yzDo)。
- en: Conversely, the teacher could steer the agent away from bad actions. An effective
    way of achieving this is through **action masking**, which limits the available
    action space given for observation to a desirable set of actions.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 反过来，教师可以引导智能体远离不良行为。一种有效的实现方式是通过**行动掩蔽**，即将观察到的可用动作空间限制为一组期望的动作。
- en: Another way of engineering the training data that the agent consumes is to monitor
    the performance of the agent, identify the parts of the state space that it needs
    more training in, and expose the agent to these states to improve the performance.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种设计智能体训练数据的方式是监控智能体的表现，识别其在状态空间中需要更多训练的部分，并将智能体暴露于这些状态中以提升其表现。
- en: Feedback
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反馈
- en: RL agents learn through feedback in the form of rewards. Engineering the reward
    function to make the learning easy – and even feasible in some cases that would
    have been infeasible otherwise – is one of the most important tasks of the machine
    teacher. This is usually an iterative process. It is common to revise the reward
    function many times during the course of a project to get the agent to learn the
    desired behavior. A futuristic MT tool could involve interacting with the agent
    through natural language to provide this feedback, which shapes the reward function
    used under the hood.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: RL 智能体通过奖励的反馈进行学习。设计奖励函数使得学习变得简单——甚至在某些情况下，原本不可能实现的情况也能变得可行——这是机器教师最重要的任务之一。通常这是一个迭代过程。在项目的过程中，奖励函数往往需要多次修订，以促使智能体学习期望的行为。未来的机器教学工具可能涉及通过自然语言与智能体互动，提供这种反馈，从而塑造背后使用的奖励函数。
- en: With this, we have introduced you to MT and its elements. Next, we will look
    into specific methods. Rather than going over an entire MT strategy for a sample
    problem, we will next focus on individual methods. You can use them as building
    blocks of your MT strategy depending on what your problem needs. We will start
    with the most common one, reward function engineering, which you might have already
    used before.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这些内容，我们已经向您介绍了机器教学及其元素。接下来，我们将探讨具体的方法。与其讨论一个样本问题的完整机器教学策略，不如专注于单个方法。根据您的问题需求，您可以将它们作为机器教学策略的构建模块。我们将从最常见的方法开始——奖励函数设计，您可能已经使用过它。
- en: Engineering the reward function
  id: totrans-77
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奖励函数设计
- en: Reward function engineering means crafting the reward dynamics of the environment
    in an RL problem so that it reflects the exact objective you have in mind for
    your agent. How you define your reward function might make the training easy,
    difficult, or even impossible for the agent. Therefore, in most RL projects, significant
    effort is dedicated to designing the reward. In this section, we will cover some
    specific cases where you will need to do it and how, then provide a specific example,
    and finally, discuss the challenges that come with engineering the reward function.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数工程是指在强化学习问题中设计环境的奖励动态，使其反映出你为智能体设定的确切目标。你如何定义奖励函数可能会使训练变得容易、困难，甚至是不可能的。因此，在大多数强化学习项目中，会投入大量精力来设计奖励。在本节中，我们将介绍一些需要设计奖励的具体案例以及如何设计奖励，接着提供一个具体的例子，最后讨论进行奖励函数设计时所面临的挑战。
- en: When to engineer the reward function
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 何时进行奖励函数设计
- en: Multiple times in this book, including in the previous section when we discussed
    concepts, we have mentioned how sparse rewards pose a problem for learning. One
    way of dealing with this is to **shape the reward** to make it non-sparse. The
    sparse reward case, therefore, is a common reason why we may want to do reward
    function engineering. Yet, it is not the only one. Not all environments/problems
    have a predefined reward for you like in an Atari game. In addition, in some cases,
    there are multiple objectives that you want your agent to achieve. For all these
    reasons, many real-life tasks require the machine teacher to specify the reward
    function based on their expertise. Let's look into these cases next.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中多次提到，包括在我们讨论概念的上一节中，我们讨论了稀疏奖励给学习带来的问题。解决这一问题的一种方法是**奖励塑形**，使奖励变得不再稀疏。因此，稀疏奖励是我们需要进行奖励函数设计的常见原因之一。然而，这并不是唯一的原因。并非所有环境/问题都像Atari游戏那样为你预定义了奖励。此外，在某些情况下，你可能希望你的智能体实现多个目标。由于这些原因，许多现实生活中的任务需要机器教师根据他们的专业知识来指定奖励函数。接下来，我们将探讨这些情况。
- en: Sparse rewards
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 稀疏奖励
- en: When the reward is sparse, meaning that the agent sees a change in the reward
    (from a constant 0 to positive/negative, from a constant negative to positive,
    and so on) with an unlikely sequence of random actions, the learning gets difficult.
    That is because the agent needs to stumble upon this sequence through random trial
    and error, which makes the problem exploration hard.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当奖励稀疏时，意味着智能体通过一系列不太可能的随机动作，看到奖励的变化（从恒定的0变为正数/负数，从恒定的负数变为正数，等等），学习就变得困难。这是因为智能体需要通过随机试错来偶然碰到这个序列，这使得问题的探索变得困难。
- en: 'Learning chess against a competitive player where the reward is +1 for winning,
    0 for draw, and -1 for losing at the very end is a good example of an environment
    with sparse rewards. A classic example used in RL benchmarks is Montezuma''s Revenge,
    an Atari game in which the player needs to collect equipment (keys, torch, and
    so on), open doors, and so on to be able to make any progress, which is very unlikely
    just by taking random actions:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 与一位竞争性玩家对弈时，奖励设定为赢得比赛为+1、平局为0、输掉比赛为-1，最后的这个例子就是一个稀疏奖励环境的典型例子。强化学习基准测试中常用的经典例子是《蒙特祖玛的复仇》，这是一款Atari游戏，玩家需要收集装备（钥匙、火把等），开门等，才能取得进展，而仅凭随机行动几乎不可能完成：
- en: '![Figure 10.3 – Montezuma''s Revenge'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '![图10.3 – 《蒙特祖玛的复仇》'
- en: '](img/B14160_10_3.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B14160_10_3.jpg)'
- en: Figure 10.3 – Montezuma's Revenge
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 – 《蒙特祖玛的复仇》
- en: In such hard exploration problems, a common strategy is **reward shaping**,
    which is to modify the reward to steer the agent toward high rewards. For example,
    a reward shaping strategy could be to give -0.1 reward to the agent learning chess
    if it loses the queen, and smaller penalties when other pieces are lost. With
    that, the machine teacher conveys their knowledge about the queen being an important
    piece in the game to the agent, although not losing the queen or any other pieces
    (except the king) in itself is not the game's objective.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这样的困难探索问题中，一个常见的策略是**奖励塑形**，即修改奖励，以引导智能体朝向高奖励方向。例如，奖励塑形策略可以是在学习国际象棋时，如果智能体丢掉了皇后，就给它-0.1的惩罚奖励，丢失其他棋子的惩罚也较小。通过这种方式，机器教师将关于皇后是游戏中重要棋子的知识传达给智能体，尽管丢失皇后或其他棋子（除国王外）本身并不是游戏的目标。
- en: We will talk more about reward shaping in detail later.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在后续部分更详细地讨论奖励塑形。
- en: Qualitative objectives
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 定性目标
- en: Let's say you are trying to teach a humanoid robot how to walk. Well, what is
    walking? How can you define it? How can you define it mathematically? What kind
    of walking gets a high reward? Is it just about moving forward or are there some
    elements of aesthetics? As you can see, it is not easy to put what you have in
    your mind about walking into mathematical expressions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在试图教一个类人机器人如何走路。那么，什么是走路？你如何定义走路？如何用数学定义走路？什么样的走路会得到高奖励？仅仅是前进就够了吗，还是其中有一些美学的元素？如你所见，把你脑海中的走路概念转化为数学表达式并不容易。
- en: 'In their famous work, researchers at DeepMind used the following reward function
    for the humanoid robot they trained to walk:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的著名研究中，DeepMind 的研究人员为他们训练的类人机器人走路使用了以下奖励函数：
- en: '![](img/Formula_10_001.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](img/Formula_10_001.png)'
- en: Here, ![](img/Formula_10_002.png) and ![](img/Formula_10_003.png) are velocities
    along the ![](img/Formula_10_004.png) and ![](img/Formula_10_005.png) axes, ![](img/Formula_10_006.png)
    is the position on the ![](img/Formula_10_007.png) axis, ![](img/Formula_10_008.png)
    is a cutoff for the velocity reward, and ![](img/Formula_05_281.png) is the control
    applied on the joints. As you can see, there are many arbitrary coefficients that
    are likely to differ for other kinds of robots. In fact, the paper uses three
    separate functions for three separate robot bodies.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，![](img/Formula_10_002.png) 和 ![](img/Formula_10_003.png) 是沿着 ![](img/Formula_10_004.png)
    和 ![](img/Formula_10_005.png) 轴的速度，![](img/Formula_10_006.png) 是在 ![](img/Formula_10_007.png)
    轴上的位置，![](img/Formula_10_008.png) 是速度奖励的截止值，而 ![](img/Formula_05_281.png) 是施加在关节上的控制。正如你所看到的，这里有许多任意系数，很可能对于其他种类的机器人会有所不同。实际上，这篇论文为三种不同的机器人身体使用了三个独立的函数。
- en: Info
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 信息
- en: 'If you are curious about how the robot walks after being trained with this
    reward, watch this video: [https://youtu.be/gn4nRCC9TwQ](https://youtu.be/gn4nRCC9TwQ).
    The way the robot walks is, how can I put it, a bit weird…'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对机器人在经过这种奖励训练后的走路方式感到好奇，可以观看这个视频：[https://youtu.be/gn4nRCC9TwQ](https://youtu.be/gn4nRCC9TwQ)。机器人的走路方式，怎么说呢，有点奇怪……
- en: So, in short, qualitative objectives require crafting a reward function to obtain
    the behavior intended.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，定性目标需要精心设计奖励函数，以实现预期的行为。
- en: Multi-objective tasks
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多目标任务
- en: A common situation in RL is to have multi-objective tasks. On the other hand,
    conventionally, RL algorithms optimize a scalar reward. As a result, when there
    are multiple objectives, they need to be reconciled into a single reward. This
    often results in mixing apples and oranges, and appropriately weighing them in
    the reward could be quite painful.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习中，一个常见的情况是任务是多目标的。另一方面，传统上，强化学习算法优化的是标量奖励。因此，当存在多个目标时，它们需要被调和成一个单一的奖励。这通常导致“把苹果和橙子混在一起”，而在奖励中恰当地加权这些目标可能相当困难。
- en: When the task objective is qualitative, it is often also multi-objective. For
    example, the task of driving a car includes elements of speed, safety, fuel efficiency,
    equipment wear and tear, comfort, and so on. You can guess that it is not easy
    to express what comfort means mathematically. But there are also many tasks in
    which multiple quantitative objectives need to be optimized concurrently. An example
    of this is to control an HVAC system to keep the room temperature as close to
    the specified setpoint as possible while minimizing the cost of energy. In this
    problem, it is the machine teacher's duty to balance these trade-offs.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 当任务目标是定性时，它通常也是多目标的。例如，驾驶汽车的任务包含了速度、安全性、燃油效率、设备磨损、舒适度等因素。你可以猜到，用数学表达舒适度是件不容易的事情。但也有许多任务需要同时优化多个定量目标。一个例子是控制暖通空调（HVAC）系统，使房间温度尽可能接近指定的设定点，同时最小化能源成本。在这个问题中，机器学习的任务是平衡这些权衡。
- en: It is very common for an RL task to involve one or more of the preceding situations.
    Then, engineering the reward function becomes a major challenge.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）任务中，涉及到前述一种或多种情况是非常常见的。接着，设计奖励函数成为了一个主要的挑战。
- en: After this much discussion, let's focus on reward shaping a little bit more.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在经过这么多讨论后，我们稍微更专注于奖励塑造（reward shaping）一点。
- en: Reward shaping
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 奖励塑造
- en: The idea behind reward shaping is to incentivize the agent to move toward success
    states and discourage it from reaching failure states using positive and negative
    rewards that are relatively smaller in magnitude with respect to the actual reward
    (and punishment). This will usually shorten the training time as the agent will
    not spend as much time trying to discover how to reach a success state. Here is
    a simple example to make our discussion more concrete.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
- en: Simple robot example
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose that a robot is moving on a horizontal axis with 0.01 step sizes. The
    goal is to reach +1 and avoid -1, which are the terminal states, as shown here:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.4 – Simple robot example with sparse rewards'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_10_4.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.4 – Simple robot example with sparse rewards
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: As you can imagine, it is very difficult for the robot to discover the trophy
    when we use sparse rewards, such as giving +1 for reaching the trophy and -1 for
    reaching the failure state. If there is a timeout for the task, let's say after
    200 steps, the episode is likely to end with that.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we can guide the robot by giving a reward that increases as
    it moves toward the trophy. A simple choice could be setting ![](img/Formula_10_010.png),
    where ![](img/Formula_10_011.png) is the position on the axis.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two potential problems with this reward function:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: As the robot moves right, the incremental relative benefit of moving even further
    right gets smaller. For example, going from ![](img/Formula_10_012.png) to ![](img/Formula_10_013.png)
    increases the step reward by 10% but going from ![](img/Formula_10_014.png) to
    ![](img/Formula_10_015.png) only increases it by 1.1%.
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Since the goal of the agent is to maximize the total cumulative reward, it is
    not in the best interest of the agent to reach the trophy since that will terminate
    the episode. Instead, the agent might choose to hang out at 0.99 forever (or until
    the time limit is reached).
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can address the first issue by shaping the reward in such a way that the
    agent gets increasing additional rewards for moving toward the success state.
    For example, we can set the reward to be ![](img/Formula_10_016.png) within the
    ![](img/Formula_10_017.png) range:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.5 – A sample reward shaping where'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_10_5.jpg)'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.5 – A sample reward shaping where ![](img/Formula_10_018.png)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
- en: With this, the amount of incentive accelerates as the robot gets closer to the
    trophy, encouraging the robot even further to go right. The situation is similar
    for the punishment for going to left.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: To address the latter, we should encourage the agent to finish the episode as
    soon as possible. We need to do that by punishing the agent for every time step
    it spends in the environment.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'This example shows two things: designing the reward function can get tricky
    even in such simple problems, and we need to consider the design of the reward
    function together with the terminal conditions we set.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: Before going into specific suggestions for reward shaping, let's also discuss
    how terminal conditions play a role in agent behavior.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: Terminal conditions
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Since the goal of the agent is to maximize the expected cumulative reward over
    an episode, how the episode ends will directly affect the agent's behavior. So,
    we can and should leverage a good set of terminal conditions to guide the agent.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: 'We can talk about several types of terminal conditions:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
- en: '**Positive terminal** indicates the agent has accomplished the task (or part
    of it, depending on how you define success). This terminal condition comes with
    a significant positive reward to encourage the agent to reach it.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Negative terminal** indicates a failure state and yields a significant negative
    reward. The agent will try to avoid these conditions.'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Neutral terminal** is neither success nor failure in itself, but it indicates
    that the agent has no path to success, and the episode is terminated with a zero
    reward in the last step. The machine teacher doesn''t want the agent to spend
    any time after that point in the environment but to reset back to the initial
    conditions. Although this does not directly punish the agent, it prevents it from
    collecting additional rewards (or penalties). So, it is implicit feedback to the
    agent.'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Time limit** bounds the number of time steps spent in the environment. It
    encourages the agent to seek high rewards within this budget, rather than wandering
    around forever. It works as feedback about what sequences of actions are rewarding
    in a reasonable amount of time and which ones are not.'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In some environments, terminal conditions are preset; but in most cases, the
    machine teacher has the flexibility to set them.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have all the components described, let's discuss some practical
    tips for reward shaping.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: Practical tips for reward shaping
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Here are some general guidelines you should keep in mind while designing the
    reward function:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: Keep the step reward between ![](img/Formula_10_019.png) and ![](img/Formula_10_020.png)
    for numerical stability, whenever possible.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Express your reward (and state) with terms that are generalizable to other versions
    of your problem. For example, rather than rewarding the agent for reaching a point,
    ![](img/Formula_10_021.png), you can incentivize reducing the distance to the
    target, based on ![](img/Formula_10_022.png).
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having a smooth reward function will provide the agent with feedback that is
    easy to follow.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The agent should be able to correlate the reward with its observations. In other
    words, observations must contain some information about what is leading to high
    or low rewards. Otherwise, there won't be much for the agent to base its decisions
    on.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The total incentive for getting close to the target states should not outweigh
    the actual reward of reaching the target state. Otherwise, the agent will prefer
    to focus on accumulating the incentives, rather than achieving the actual goal.
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you would like your agent to complete a task as soon as possible, assign
    a negative reward to each time step. The agent will try to finish the episode
    to avoid accumulating negative rewards.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the agent can collect more positive rewards by not reaching a terminal state,
    it will try to collect them before reaching a terminal condition. If the agent
    is likely to collect only negative rewards by staying inside an episode (such
    as when there is a penalty per time step), it will try to reach a terminal condition.
    The latter can result in suicidal behavior if the life is too painful for the
    agent, meaning that the agent can seek any terminal state, including natural ones
    or failure states, to avoid incurring excessive penalties by staying alive.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now, we will look at an example of reward shaping using OpenAI.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: Example – reward shaping for a mountain car
  id: totrans-141
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In OpenAI''s mountain car environment, the goal of the car is to reach the
    goal point on top of one of the hills, which is illustrated in *Figure 10.6*.
    The action space is to push the car to the left, push to the right, or apply no
    force:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 10.6 – Mountain car environment'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B14160_10_6.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: Figure 10.6 – Mountain car environment
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: Since the force we apply is not enough to climb the hill and reach the goal,
    the car needs to gradually accumulate potential energy by climbing in opposite
    directions. Figuring this out is non-trivial, because the car does not know what
    the goal is until it reaches it, which can be achieved after 100+ steps of correct
    actions. The only reward in the default environment is -1 per time step to encourage
    the car to reach the goal as quickly as possible to avoid accumulating negative
    rewards. The episode terminates after 200 steps.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
- en: We will use various MT techniques to train our agent throughout the chapter.
    To that end, we will have a custom training flow and a customized environment
    with which we can experiment with these methods. Let's get things set up first.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: Setting up the environment
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Our custom `MountainCar` environment wraps OpenAI''s `MountainCar-v0`, which
    looks like the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: Chapter10/custom_mcar.py
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you visit that file now, it may look complicated since it includes some add-ons
    we are yet to cover. For now, just know that this is the environment we will use.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use Ray/RLlib''s Ape-X DQN throughout the chapter to train our agents.
    Make sure that you have them installed, preferably within a virtual environment:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: With that, next, let's get a baseline performance by training an agent without
    any MT.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: Getting a baseline performance
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will use a single script for all of the training. We define a `STRATEGY`
    constant at the top of the script, which will control the strategy to be used
    in the training:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
- en: Chapter10/mcar_train.py
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: For each strategy, we will kick off five different training sessions of 2 million
    time steps each, so we set `NUM_TRIALS = 5` and `MAX_STEPS = 2e6`. At the end
    of each training session, we will evaluate the trained agent over `NUM_FINAL_EVAL_EPS
    = 20` episodes. Therefore, the result for each strategy will reflect the average
    length of 100 test episodes, where a lower number indicates better performance.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: 'For most of the strategies, you will see that we have two variants: with and
    without dueling networks enabled. When the dueling network is enabled, the agent
    achieves a near-optimal result (around 100 steps to reach the goal), so it becomes
    uninteresting for our case. Moreover, when we implement action masking later in
    the chapter, we won''t use dueling networks to avoid complexities in RLlib. Therefore,
    we will focus on the no-dueling network case in our example. Finally, note that
    the results of the experiments will be written to `results.csv`.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
- en: 'With that, let''s train our first agents. When I used no MT, in my case, I
    obtained the following average episode lengths:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The outcome is as follows:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let's see next whether reward shaping helps us here.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
- en: Solving the problem with a shaped reward
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Anyone looking at the mountain car problem could tell that we should encourage
    the car to go right, at least eventually. In this section, that is what we''ll
    do. The dip position of the car corresponds to an ![](img/Formula_10_023.png)
    position of -0.5\. We modify the reward function to give a quadratically increasing
    reward to the agent for going beyond this position toward the right. This happens
    inside the custom `MountainCar` environment:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Of course, feel free to try your own reward shaping here to gain a better idea.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: Applying a constant reward, such as -1, is an example of sparse reward, although
    the step reward is not 0\. That is because the agent does not get any feedback
    until the very end of the episode; none of its actions change the default reward
    for a long time.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: 'We can enable the custom (shaped) reward strategy with the following flag:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The outcome we get is something like the following:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This is obviously a significant gain, so kudos to our shaped reward function!
    Admittedly, though, it took me several iterations to figure out something that
    brings in significant improvement. This is because the behavior we encourage is
    more complex than just going right: we want the agent to go between left and right
    to speed up. This is a bit hard to capture in a reward function, and it could
    easily give you headaches.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: Reward function engineering in general can get quite tricky and time-consuming
    – so much so that this topic deserves a dedicated section to discuss, which we
    will turn to next.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Challenges with engineering the reward function
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The objective of RL is to find a policy that maximizes the expected cumulative
    reward the agent collects. We design and use very sophisticated algorithms to
    overcome this optimization challenge. In some problems, we use billions of training
    samples to this end and try to squeeze out a little extra reward. After all this
    hassle, it is not uncommon to observe that your agent obtains a great reward,
    but the behavior it exhibits is not exactly what you intended. In other words,
    the agent learns something different than what you want it to learn. If you run
    into such a situation, don't get too mad. That is because the agent's sole purpose
    is to maximize the reward you specified. If that reward does not exactly reflect
    the objective you had in mind, which is much more challenging than you may think,
    neither will the behavior of the agent.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Info
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
- en: 'A famous example of a misbehaving agent due to an incorrectly specified reward
    is OpenAI''s CoastRunners agent. In the game, the agent is expected to finish
    the boat race as quickly as possible while collecting rewards along the way. After
    training, the agent figured out a way of collecting higher rewards without having
    to finish the race, defeating the original purpose. You can read more about it
    on OpenAI''s blog: [https://openai.com/blog/faulty-reward-functions/](https://openai.com/blog/faulty-reward-functions/).'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: As a result, it is of tremendous importance that you specify a good reward function
    for your task, especially when it includes qualitative and/or complex objectives.
    Unfortunately, designing a good reward function is more of an art than science,
    and you will gain intuition through practice and trial and error.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
- en: 'Alex Irpan, a machine learning researcher at Google, beautifully expresses
    how important and challenging designing the reward function is: "I''ve taken to
    imagining deep RL as a demon that''s deliberately misinterpreting your reward
    and actively searching for the laziest possible local optima. It''s a bit ridiculous,
    but I''ve found it''s actually a productive mindset to have." (Irpan, 2018). François
    Chollet, the author of Keras, says "loss function engineering is probably going
    to be a job title in the future." ([https://youtu.be/Bo8MY4JpiXE?t=5270](https://youtu.be/Bo8MY4JpiXE?t=5270)).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
- en: Despite these challenges, the tricks we have just covered should give you a
    running start. The rest will come with experience.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
- en: With that, let's conclude our discussion on reward function engineering. This
    was a long yet necessary one. In the next section, we will discuss another topic,
    curriculum learning, which is important not just in MT but in RL as a whole.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
- en: Curriculum learning
  id: totrans-188
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When we learn a new skill, we start with the basics. Bouncing and dribbling
    are the first steps when learning basketball. Doing alley-oops is not something
    to try to teach in the first lesson. You need to gradually proceed to advanced
    lessons after getting comfortable with the earlier ones. This idea of following
    a curriculum, from the basics to advanced levels, is the basis of the whole education
    system. The question is whether machine learning models can benefit from the same
    approach. It turns out that they can!
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
- en: In the context of RL, when we create a curriculum, we similarly start with "easy"
    environment configurations for the agent. This way, the agent can get an idea
    about what success means early on, rather than spending a lot of time blindly
    exploring the environment with the hope of stumbling upon success. We then gradually
    increase the difficulty if we observe that the agent is exceeding a certain reward
    threshold. Each of these difficulty levels is considered a **lesson**. Curriculum
    learning has been shown to increase training efficiency and makes tasks that are
    infeasible to achieve feasible for the agent.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
- en: Designing lessons and transition criteria is a non-trivial undertaking. It requires
    significant thought and subject matter expertise. Although we follow manual curriculum
    design in this chapter, when we revisit the topic in [*Chapter 11*](B14160_11_Final_SK_ePub.xhtml#_idTextAnchor239),
    *Generalization and Partial Observability*, and [*Chapter 14*](B14160_14_Final_SK_ePub.xhtml#_idTextAnchor306),
    *Robot Learning*, we will discuss automatic curriculum generation methods.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
- en: In our mountain car example, we create lessons by modifying the initial conditions
    of the environment. Normally, as the episode beginnings, the environment randomizes
    the car position around the valley dip (![](img/Formula_10_024.png)) and sets
    the velocity (![](img/Formula_08_050.png)) to 0\. In our curriculum, the car will
    start in the first lesson close to the goal and with a high velocity toward the
    right. With that, it will easily reach the goal. We will make things gradually
    closer to the original difficulty as the curriculum progresses.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
- en: 'Specifically, here is how we define the lessons:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
- en: '**Lesson 0**: ![](img/Formula_10_026.png), ![](img/Formula_10_027.png)'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lesson 1**: ![](img/Formula_10_028.png), ![](img/Formula_10_029.png)'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lesson 2**: ![](img/Formula_10_030.png), ![](img/Formula_10_031.png)'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lesson 3**: ![](img/Formula_10_032.png), ![](img/Formula_10_033.png)'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Lesson 4 (final / original)**: ![](img/Formula_10_034.png), ![](img/Formula_10_035.png)'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'This is how it is set inside the environment:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We will let the agent proceed to the next lesson once it has been successful
    enough in the current one. We define this threshold as having an average episode
    length of less than 150 over 10 evaluation episodes. We set the lessons in training
    and the evaluation workers with the following functions:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'These are then used inside the training flow:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'So, this is how we implement a manual curriculum. Say you train the agent with
    this:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: You will see that the performance we get is near optimal!
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You just taught a machine something using a curriculum! Pretty cool, isn't it?
    Now it should feel like MT!
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will look at another interesting approach: MT using demonstrations.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
- en: Warm starts and demonstration learning
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A popular technique to demonstrate to the agent a way to success is to train
    it on data that is coming from a reasonably successful controller, such as humans.
    In RLlib, this can be done by saving the human play data from the mountain car
    environment:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: Chapter10/mcar_demo.py
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This data can then be fed to the training, which is implemented in `Chapter10/mcar_train.py`.
    When I tried it, RLlib got stuck with NaNs in multiple attempts when the training
    was seeded using this method. So, now that you know about it, we will leave the
    details of this to RLlib's documentation at [https://docs.ray.io/en/releases-1.0.1/rllib-offline.html](https://docs.ray.io/en/releases-1.0.1/rllib-offline.html)
    and not focus on it here.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: Action masking
  id: totrans-217
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'One final MT approach we will use is action masking. With that, we can prevent
    the agent from taking certain actions in certain steps based on conditions we
    define. For the mountain car, assume that we have this intuition of building momentum
    before trying to climb the hill. So, we want the agent to apply force to the left
    if the car is already moving left around the valley. So, for these conditions,
    we will mask all the actions except left:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In order to be able to use this masking, we need to build a custom model. For
    the masked actions, we push down all the logits to negative infinity:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, when using this model, we turn off dueling to avoid an overly complicated
    implementation. Also, we register our custom model:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In order to train your agent with this strategy, set the following:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The performance will be as follows:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This is definitely an improvement over the default case, yet it is behind the
    reward shaping and curriculum learning approaches. Having smarter masking conditions
    and adding dueling networks can further help with the performance.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
- en: This is the end of the MT techniques we use for the mountain car problem. Before
    we wrap up, let's check one more important topic in MT.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: Concept networks
  id: totrans-230
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: An important part of the MT approach is to divide the problem into concepts
    that correspond to different skills to facilitate learning. For example, for an
    autonomous car, training separate agents for cruising on a highway and passing
    a car could help with performance. In some problems, divisions between concepts
    are even more clear. In those cases, training a single agent for the entire problem
    will often result in better performance.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
- en: Before closing this chapter, let's talk about some of the potential downsides
    of the MT approach.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
- en: Downsides and the promises of MT
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: There are two potential downsides of the MT approach.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
- en: First, it is usually non-trivial to come up with good reward shaping, a good
    curriculum, a set of action masking conditions, and so on. This also in some ways
    defeats the purposes of learning from experience and not having to do feature
    engineering. On the other hand, whenever we are able to do so, feature engineering
    and MT could be immensely helpful for the agent to learn and increase its data
    efficiency.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
- en: Second, when we adopt a MT approach, it is possible to inject the bias of the
    teacher to the agent, which could prevent it from learning better strategies.
    The machine teacher needs to avoid such biases whenever possible.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
- en: Awesome job! We have reached the end of an exciting chapter here. Let's summarize
    what we have covered next.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we covered an emerging paradigm in artificial intelligence,
    MT, which is about effectively conveying the expertise of a subject matter expert
    (teacher) to machine learning algorithms. We discussed how this is similar to
    how humans are educated: by building on others'' knowledge, usually without reinventing
    it. The advantage of this approach is that it greatly increases data efficiency
    in machine learning, and, in some cases, makes learning possible that would have
    been impossible without a teacher. We discussed various methods in this paradigm,
    including reward function engineering, curriculum learning, demonstration learning,
    action masking, and concept networks. We observed how some of these methods have
    improved vanilla use of Ape-X DQN significantly. At the end, we also introduced
    potential downsides of this paradigm, namely the difficulty of designing the teaching
    process and tools, and possible bias introduced into learning. Despite these downsides,
    MT will become a standard part of an RL scientist''s toolbox in the near future.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will discuss generalization and partial observability,
    a key topic in RL. In doing so, we will visit curriculum learning again and see
    how it helps in creating robust agents.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
- en: See you on the other side!
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
- en: References
  id: totrans-242
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Bonsai. (2017). *Deep Reinforcement Learning Models: Tips & Tricks for Writing
    Reward Functions*. Medium. URL: [https://bit.ly/33eTjBv](https://bit.ly/33eTjBv)'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Weng, L. (2020). *Curriculum for Reinforcement Learning*. Lil''Log. URL: [https://bit.ly/39foJvE](https://bit.ly/39foJvE)'
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'OpenAI. (2016). *Faulty Reward Functions in the Wild*. OpenAI blog. URL: [https://openai.com/blog/faulty-reward-functions/](https://openai.com/blog/faulty-reward-functions/)'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Irpan, A. (2018). *Deep Reinforcement Learning Doesn''t Work Yet*. Sorta Insightful.
    URL: [https://www.alexirpan.com/2018/02/14/rl-hard.html](https://www.alexirpan.com/2018/02/14/rl-hard.html)'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Heess, N. et al. (2017). *Emergence of Locomotion Behaviours in Rich Environments*.
    arXiv.org. URL: [http://arxiv.org/abs/1707.02286](http://arxiv.org/abs/1707.02286)'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Bonsai. (2017). *Writing Great Reward Functions* – Bonsai. YouTube. URL: [https://youtu.be/0R3PnJEisqk](https://youtu.be/0R3PnJEisqk)'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Badnava, B. & Mozayani, N. (2019). *A New Potential-Based Reward Shaping for
    Reinforcement Learning Agent*. arXiv.org. URL: [http://arxiv.org/abs/1902.06239](http://arxiv.org/abs/1902.06239)'
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Microsoft Research. (2019). *Reward Machines: Structuring Reward Function Specifications
    and Reducing Sample Complexity*. YouTube. URL: [https://youtu.be/0wYeJAAnGl8](https://youtu.be/0wYeJAAnGl8)'
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'US Government Finances. (2020). URL: [https://www.usgovernmentspending.com/](https://www.usgovernmentspending.com/)'
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The AlphaStar team. (2019). *AlphaStar: Mastering the Real-Time Strategy Game
    StarCraft II*. DeepMind blog. URL: [https://bit.ly/39fpDIy](https://bit.ly/39fpDIy)'
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
