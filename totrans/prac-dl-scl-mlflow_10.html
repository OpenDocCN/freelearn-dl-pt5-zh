<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer067" class="Content">
			<h1 id="_idParaDest-83"><a id="_idTextAnchor082"/>Section 4 – Deploying a Deep Learning Pipeline at Scale</h1>
			<p>In this section, we will learn how to implement and deploy a multi-step inference pipeline for production usage. We will start with an overview of four patterns of inference workflows in production. We will then learn how to implement a multi-step inference pipeline with preprocessing and postprocessing steps around a fine-tuned <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) model using MLflow <strong class="bold">PyFunc</strong> APIs. With a ready-to-deploy MLflow PyFunc-compatible DL inference pipeline, we will learn about different deployment tools and hosting environments to decide which tool to use for a specific deployment scenario. We will then implement and deploy a batch inference pipeline using MLflow's Spark <strong class="bold">user-defined function</strong> (<strong class="bold">UDF</strong>). From there on, we will focus on deploying a web service using either MLflow's built-in model serving tool or Ray Serve's MLflow deployment plugin. Finally, we will show a complete step-by-step guide to deploying a DL inference pipeline to a managed AWS SageMaker instance for production usage.</p>
			<p>This section comprises the following chapters:</p>
			<ul>
				<li><a href="B18120_07_ePub.xhtml#_idTextAnchor083"><em class="italic">Chapter 7</em></a>, <em class="italic">Multi-Step Deep Learning Inference Pipeline</em></li>
				<li><a href="B18120_08_ePub.xhtml#_idTextAnchor095"><em class="italic">Chapter 8</em></a>, <em class="italic">Deploying a DL Inference Pipeline at Scale</em></li>
			</ul>
		</div>
	</div></body></html>