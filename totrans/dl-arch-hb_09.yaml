- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Exploring Unsupervised Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning works with data that does not have any labels. More broadly,
    unsupervised learning aims to uncover the intrinsic patterns hidden within the
    data. The most rigorous and expensive part of a supervised machine learning project
    is the labels required for a given data. In the real world, there is tons of unlabeled
    data available with tons of information that could be learned from. Frankly, it’s
    impossible to obtain labels for all of the data that exist in the world. Unsupervised
    learning is the key to unlocking the potential of the abundant unlabeled digital
    data we have today. Let’s explore a hypothetical situation below to understand
    this better.
  prefs: []
  type: TYPE_NORMAL
- en: Imagine that it costs 1 USD and 1 minute to obtain a label for a row of data
    for whatever use case it could be, and a single unit of information can be obtained
    through supervised learning. To get 10,000 units of information, 10,000 USD would
    need to be spent, and 10,000 minutes need to be contributed to obtain 10,000 pieces
    of labeled data. Both time and money are painful things to burn. However, for
    unsupervised learning, it costs 0 USD and 0 minutes to obtain 0.01 units of information
    through the same data without a label. Since the amount of data is not impeded
    by time or money, we can easily get 100 times more data than the 10,000 samples
    and get the same information that can be learned through a model. When money and
    time aren’t an issue, the amount of information your model can learn is endless,
    assuming that your unsupervised learning model has the capacity and ability to
    do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning provides a competitive edge to the unsupervised learning field,
    given the huge capacity and ability to learn complex information. If we can crack
    the code of unsupervised deep learning, it will set the stage for models that
    can come close to general intelligence one day! In this chapter, we will explore
    the notable components of unsupervised deep learning by taking a look at the following
    topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Exploring unsupervised deep learning applications
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating pretrained network weights for downstream tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating general representations through unsupervised deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring zero-shot learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring the dimensionality reduction component of unsupervised deep learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detecting anomalies in external data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter includes some practical implementations in the **Python** programming
    language. To complete it, you will need to have a computer with the following
    libraries installed:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pandas`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`CLIP`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`numpy`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pytorch`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pillow`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can find the code files for this chapter on GitHub at [https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_9](https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_9).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring unsupervised deep learning applications
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Today, practitioners have been able to leverage unsupervised deep learning
    to tap into their unlabeled data to achieve either one of the following use cases.
    These have been put in descending order in terms of their impact and usefulness:'
  prefs: []
  type: TYPE_NORMAL
- en: Creating pretrained network weights for downstream tasks
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating general representations that can be used as-is in downstream supervised
    tasks by predictive supervised models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Achieving one-shot and zero-shot learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Performing dimensionality reduction
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Detect anomalies in external data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Clustering the provided training data into groups
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To start, note that pure clustering is still a core application of unsupervised
    learning in general, but not for deep learning. **Clustering** is where unlabeled
    data is grouped into multiple arbitrary clusters or classes. This will be useful
    in use cases such as customer segmentation for targeted responses, or topic modeling
    to figure out trendy topics people are discussing on social media. In clustering,
    the relationship between the unlabeled data samples is leveraged to find groups
    of data that are close together. Some clustering techniques group this data by
    assuming a spherical distribution in each cluster, such as **K-means**. Some other
    clustering techniques are more adaptive and can find clusters of multiple distributions
    and sizes, such as **HDBSCAN**.
  prefs: []
  type: TYPE_NORMAL
- en: Deep learning methods have not been able to produce any significant improvement
    from non-deep learning clustering methods such as the simple *k*-means algorithm
    or the HDBSCAN algorithm. However, efforts have been made to utilize clustering
    itself to aid the unsupervised pretraining of neural network models. To realize
    it in a neural network model as a component, the clustering model has to be differentiable
    so that gradients can still be propagated to the entire network. These methods
    are not superior to non-deep learning techniques such as *k*-means or HDBSCAN
    and are simply a variation so that the concept of clustering can be realized in
    a neural network. An example application of clustering in unsupervised pretraining
    is **SwaV**, which will be introduced in the next section. However, for completeness,
    one example of a neural network-based clustering algorithm that is used traditionally
    is self-organizing maps, but the network itself is not considered a deep network.
  prefs: []
  type: TYPE_NORMAL
- en: In the next few sections, we will discover the other five applications more
    comprehensively, ordered by their impact and usefulness, as shown previously.
  prefs: []
  type: TYPE_NORMAL
- en: Creating pretrained network weights for downstream tasks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Also known as unsupervised transfer learning, this method is analogous to supervised
    transfer learning and naturally reaps the same benefits as described in the *Transfer
    learning* section in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125), *Exploring
    Supervised Deep Learning*. But as a recap, let’s go through an analogy. Imagine
    you’re a chef who has spent years learning how to cook a variety of dishes, from
    pasta and steak to desserts. One day, you’re asked to cook a new dish you’ve never
    tried before; let’s call it “Dish X.” Instead of starting from scratch, you use
    your prior knowledge and experience to simplify the process. You know how to chop
    vegetables, how to use the oven, and how to adjust the heat, so you don’t have
    to relearn all of these steps. You can focus your energy on learning the specific
    ingredients and techniques required for Dish X This is similar to how transfer
    learning works in machine learning, which applies to both unsupervised learning
    and supervised learning. A model that has already been trained on a related task
    can be used as a starting point, allowing the model to learn new tasks more quickly
    and effectively.
  prefs: []
  type: TYPE_NORMAL
- en: Besides being able to use the unlimited amount of unlabeled data available in
    the world, unsupervised transfer learning also bears another benefit. Labels in
    supervised learning often hold biases that the model will adopt. The biases acquired
    through learning can obstruct the acquisition of more generalized knowledge that
    would be more useful for downstream tasks, to varying extents. Other than biases
    in the labels, there are also situations where labels are wrong. Being unsupervised
    means that the model is stripped of any possibility of learning biases or errors
    from any labels. However, note that biases are more prominent in some datasets.
    A dataset with a complex task that has quality-related labels derived qualitatively
    from human judgment tends to have more biases compared to a simple task such as
    classifying whether a picture has a face or not.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let’s dive into the techniques. Unsupervised transfer learning has been
    rated as the most impactful and useful application due to contributions that were
    made in the NLP field. Transformers, introduced in [*Chapter 6*](B18187_06.xhtml#_idTextAnchor092),
    *Understanding Neural Network Transformers*, paved the way for the paradigm to
    pre-train your model with an unsupervised learning technique more commonly known
    as **self-supervised learning**. How this method is categorized here is a matter
    of perspective and not everybody would agree with it. Self-supervised learning
    leverages only the relationship of co-occurring data to pre-train a neural network
    with relational knowledge without labels. Seeing it this way, check out the following
    question, and decide your own answer.
  prefs: []
  type: TYPE_NORMAL
- en: Try it yourself
  prefs: []
  type: TYPE_NORMAL
- en: Traditional unsupervised learning data preprocessing techniques such as **Principal
    Component Analysis** (**PCA**) leverage the relationship of co-occurring data
    to build new features that can better represent impactful patterns. Do you see
    PCA as its own category with self-supervising, under the umbrella of supervised
    learning, or under the umbrella of unsupervised learning?
  prefs: []
  type: TYPE_NORMAL
- en: '*The author’s* *answer: unsupervised*!'
  prefs: []
  type: TYPE_NORMAL
- en: The unsupervised learning techniques that are used in transformers are masked
    language modeling and next-sentence prediction. These tasks help the model learn
    the relationships between words and sentences, allowing it to better understand
    the meaning and context of language data. A model that has been trained on masked
    language modeling and next-sentence prediction tasks can use its understanding
    of language to perform better on a variety of NLP downstream tasks. These are
    proven by the SoTA predictive performances on various datasets from leading transformers
    today, such as DeBERTa, as introduced in [*Chapter 6*](B18187_06.xhtml#_idTextAnchor092),
    *Understanding Neural* *Network Transformers*.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s briefly go through other examples of unsupervised pre-training:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A simple framework for contrastive learning of visual representations** (**SimCLR**):
    SimCLR utilizes a method called **contrastive learning** to pretrain convolutional
    neural networks. Contrastive learning is a key technique in unsupervised deep
    learning that helps neural networks learn representations of data by optimizing
    the distance between related features. The core idea behind contrastive learning
    is to bring features of similar data points closer together and push features
    of dissimilar data points further apart in the feature space, using a contrastive
    loss function. While there are various forms of contrastive loss today, the general
    idea is to minimize the distance between similar examples and maximize the distance
    between dissimilar examples. This distance can be measured in various ways, such
    as Euclidean distance or cosine distance. Although this method requires labels
    for learning and is technically a supervised learning loss, the features learned,
    along with the selection of similar and dissimilar samples for label-free samples,
    make this loss function a crucial technique in unsupervised deep learning. The
    simplest representation of such a contrastive loss is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For dissimilar samples:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: loss = − distance
  prefs: []
  type: TYPE_NORMAL
- en: 'For similar samples:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: loss = distance
  prefs: []
  type: TYPE_NORMAL
- en: SimCLR focuses on image data and uses crafted image augmentation techniques
    to generate image pairs that could optimize the network to produce closer features.
    Random cropping and random color distortions are the most general augmentations
    that can be useful setups for most image datasets to perform unsupervised pre-training
    with SimCLR.
  prefs: []
  type: TYPE_NORMAL
- en: '**Swapping assignments between multiple views of the same image** (**SwaV**):
    SwaV adopts a similar concept to SimCLR in utilizing image augmentations with
    convolutional neural networks. It also uses the concept of clustering and embeddings
    to optimize the model to produce features that make sure the two images are mapped
    to the same feature space.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The learning technique is executed as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: A pre-set amount of cluster number is determined, called *K*.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Featurize the two images with the same convolutional neural network.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The two sets of features will then be assigned independently to specific clusters
    by using an external technique called Optimal Transport Solver using an embedding
    layer that represents the representative features of the *K* clusters. Two features
    will always be assigned to different clusters.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: Dot products between the convolutional features and all the cluster embeddings
    are computed and a softmax operation is applied.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The assigned clusters for both image features will then be swapped, where cross-entropy
    between the swapped cluster assignments and the resulting values from the softmax
    operation will be used to optimize the weights of both the CNN and the embeddings
    layer.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The idea is to jointly learn the embedding weights and convolutional network
    weights that consistently categorize together multiple augmentations of the same
    image. The technique can be described as contrastive clustering. Both SwaV and
    SimCLR are competitively close in multiple downstream task performances.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '**SEER**: SEER is the combination of SwaV, an extremely high amount of unlabeled
    data for images at the billion scales instead of the more common million scale,
    and using high-capacity models to pre-train using random, uncurated, and unlabeled
    images. This allowed SEER to achieve SoTA downstream supervised task performance
    and outperformed both SimCLR and SwaV alone.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**UP-DETR, by the researchers from SCTU and Tencent Wechat AI**: This method
    pre-trains the transformer that has an encoder-decoder architecture with CNN features
    for image object detection tasks in an unsupervised way. UP-DETR managed to improve
    the performance of transformers on downstream supervised image object detection
    datasets. The interesting thing to remember here is that it structured the network
    in a way that allowed random image patches to be fed separately to the decoder
    to predict the bounding box of these patches on the original image. The original
    image is fed into the encoder part of the transformer and combined with the random
    image patches at the decoder part.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Wav2vec 2.0**: Wav2vec 2.0 showed how feasible it is to train a reliable
    speech recognition model with limited amounts of labeled data by leveraging self-supervised
    pretraining as a pretext task. It also uses contrastive, loss that is simply the
    cosine similarity between samples. The method uses CNNs as an audio feature extractor
    and quantizes the representations into a discrete array of values that can be
    trained before passing it into a transformer. The unsupervised tasks of masked
    speech modeling and contrastive loss are applied here. Let’s look at how these
    two methods can be combined:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A random location of the quantized latent speech representations is masked.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: The transformers output at the same location of the masked quantized latent
    speech representation will be used as the prediction of the missing masked quantized
    latent speech representation.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: A few parts of the non-masked quantized latent representations of the same sample
    will be used to compute the contrastive loss against the predicted missing masked
    quantized latent speech representation, which effectively enforces parts of the
    same audio sample to be in a similar latent domain.
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This process is demonstrated in *Figure 9**.1*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![](img/B18187_09_01.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_IMG
- en: 'To PD: This image is sent for redraw.'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 9.1 – Wav2vec 2.0 model structure
  prefs: []
  type: TYPE_NORMAL
- en: The methods we’ve introduced here were meant for a specific modality. However,
    with some effort methods, they can be adapted to other modalities to replicate
    performance. For example, augmentations were used for image-based methods along
    with contrastive learning. To adapt this method to text-based modality, using
    text augmentations that preserve the meaning, such as word replacement or back
    translation, should work well. The modalities involved in the unsupervised methods
    introduced here were images, text data, and audio data. These modalities were
    chosen due to their generalizability factor for downstream tasks. Other forms
    of modality, such as graph data or numerical data, are highly customizable to
    individual use cases where there is fundamentally no information that can be transferred
    to downstream tasks. Before you attempt to run an unsupervised deep learning method
    to create pre-trained weights, consider listing the information that can be transferred
    and evaluate qualitatively whether it makes sense to proceed.
  prefs: []
  type: TYPE_NORMAL
- en: But what if the pre-trained network weights are already capable of producing
    very generalizable features across different domains? Just like how CNNs trained
    on the ImageNet dataset in a supervised way can be used as feature extractors,
    there is no limiting the immediate usage of networks trained by unsupervised methods
    such as SwaV or Wav2vec 2.0 as feature extractors. Feel free to try it out yourself!
    However, a few unsupervised learning techniques use neural networks that are made
    to use their generated features instead of their weights directly. In the next
    section, we will discover exactly that.
  prefs: []
  type: TYPE_NORMAL
- en: Creating general representations through unsupervised deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The representations that are learned through unsupervised deep learning can
    be directly used as-is in downstream supervised tasks by predictive supervised
    models or consumed directly by end users. There are a handful of generally impactful
    unsupervised methods that utilize neural networks that are meant to be used primarily
    as feature extractors. Let’s take a look at a couple of unsupervised feature extractors:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Unsupervised pre-trained word tokenizers**: These are used heavily by variants
    of the transformers architecture and were introduced in [*Chapter 8*](B18187_08.xhtml#_idTextAnchor125),
    *Exploring Supervised Deep Learning,* in the *Representing text data for supervised
    deep* *learning* section.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Unsupervised pre-trained word embeddings**: These methods leverage unsupervised
    learning and attempt to perform language modeling, similar to masked language
    modeling in transformers. However, word embeddings-based methods have been overtaken
    by transformer-based pretraining with sub-word-based text tokenization in terms
    of metric performance. The word embeddings method still stays relevant today due
    to the runtime efficiency it has over transformer-based methods. Note that not
    every project has the GPU available that is needed to run a big transformer in
    a reasonable runtime. Some projects only have access to CPU processing, and word
    embeddings provide the perfect inference runtime versus metric performance tradeoff.
    Additionally, word embeddings are a natural solution for some use cases that require
    words as a result, such as word-to-word translation from one language into another
    language, or even finding synonyms or antonyms. Examples of methods that produce
    pre-trained word embeddings are **fastText** and **word2vec**. *Figure 9**.2*
    exemplifies the architecture of word embedding methods:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.2 – The word embeddings architecture with a two-layer MLP and trainable
    embeddings](img/B18187_09_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.2 – The word embeddings architecture with a two-layer MLP and trainable
    embeddings
  prefs: []
  type: TYPE_NORMAL
- en: The task is either to predict the middle word based on the summed embeddings
    of the surrounding words or to predict the surrounding words based on the embeddings
    of the current word. *Figure 9**.2* shows the former case. After training the
    word embeddings with the MLP with a dictionary of N words, the MLP is then dumped,
    and the word embeddings are saved as a dictionary for simple look-up utilization
    during inference. FastText differs from word2vec by using not words but subwords
    to generate embeddings and thus can better handle missing words. A word embedding
    for FastText is produced by summing up embeddings of subwords that form the full
    word. Go to [https://github.com/facebookresearch/fastText](https://github.com/facebookresearch/fastText)
    to learn how to use word embeddings that have pre-trained for 157 languages, or
    how to pre-train FastText embeddings on your custom dataset!
  prefs: []
  type: TYPE_NORMAL
- en: '**Autoencoders**: Autoencoders are encoder-decoder architectures that can be
    trained to denoise data and reduce the dimensionality of the data while optimizing
    it to be reconstructible. They are generally trained to extract useful and core
    information by limiting the number of features in the bottleneck section of the
    architecture right after the encoder and right before the decoder. Go back to
    *Chapter 5*, *Understanding Autoencoders*, to find out more!'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Contrastive Language-Image Pretraining** (**CLIP**): CLIP is a method that
    trains a text language transformer encoder and a CNN image encoder with contrastive
    learning. It provides a dataset consisting of around 400 million image text pairs
    constructed by combining multiple publicly available datasets. This method produces
    a powerful image and text feature encoder that can be used independently. This
    method became a main part of the current SoTA of text-to-image methods by assisting
    during training to optimize to generate an image that has CLIP-encoded image embeddings
    close to the CLIP-encoded text embeddings. *Figure 9**.3* shows the CLIP architecture:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 9.3 – CLIP architecture](img/B18187_09_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.3 – CLIP architecture
  prefs: []
  type: TYPE_NORMAL
- en: 'A small implementation detail here is that the outputs of the image encoder
    and text encoder are fed separately to a linear layer so that the number of features
    matches up for contrastive learning loss to be computed. Specifically, for CLIP,
    the contrastive loss is applied in the following way:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Notice that cross-entropy is still applied after applying pairwise cosine similarity
    between the image and text embeddings, which exemplifies the many variations of
    contrastive loss, where the core workhorse still comes down to using the distance
    metric. Since CLIP leverages co-occurring data, it does not belong to the self-supervised
    learning sub-category.
  prefs: []
  type: TYPE_NORMAL
- en: '**Generative models**: Let’s look at some examples of these models:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Transformer models**. Examples include GPT-3 and ChatGPT. Both are transformer
    models that are trained with masked language modeling and next-sentence prediction
    tasks. The difference with ChatGPT is that it is fine-tuned using reinforcement
    learning through human feedback and training. Both generate new text data by predicting
    in an autoregressive manner.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text to image generators**. Examples include DALL.E 2 and Stable Diffusion.
    Both methods utilize the diffusion model. At a high level, the Stable Diffusion
    method slowly generates an extremely high-quality image from a base image full
    of noise.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Generative adversarial networks** (**GANs**): GANs utilize two neural network
    components during training, called **discriminators** and **generators**. The
    discriminator is a classifier that is meant to discern fake from real images.
    The generator and discriminator are trained iteratively until a point where the
    discriminator can’t discern that the image generated by the generator is fake.
    GANs can generate good-quality images but are generally surpassed by diffusion-based
    models as they produce much higher-quality images.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: It’s important to note that the methods to learn and create feature representations
    with unsupervised learning are not limited to those based on neural networks.
    PCA and TF-IDF, which are considered data pre-processing techniques, also belong
    in this category. However, the key difference between them and deep learning-based
    methods is that the latter require more training time but offer better generalization
    capabilities.
  prefs: []
  type: TYPE_NORMAL
- en: The key in unsupervised representation learning is leveraging relationships
    between co-occurring data. The techniques that were introduced in the last two
    sections have public repositories that can be utilized immediately. For most of
    them, there are already pre-trained weights that you can leverage out of the box
    to either fine-tune further on downstream supervised tasks or use as plain feature
    extractors. In the next section, we will explore a special type of utilization
    of CLIP called zero-shot learning.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring zero-shot learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Zero-shot learning** is a paradigm that involves utilizing a trained machine
    learning model to tackle new tasks without training and learning directly on the
    new task. The method implements transfer learning at its core but instead of requiring
    additional learning in the downstream task, no learning is done. The method that
    we will be using to realize zero-shot learning here is CLIP as a base and thus
    is an extension of an unsupervised learning method.'
  prefs: []
  type: TYPE_NORMAL
- en: CLIP can be used to perform zero-shot learning on a wide variety of downstream
    tasks. To recap, CLIP is pre-trained with the task of image-text retrieval. So
    long as CLIP is applied to downstream tasks without any additional learning process,
    it can be considered as zero-shot learning. The tested use cases include tasks
    such as object character recognition, action recognition in videos, geo-localization
    based on images, and many types of fine-grained image object classification. Additionally,
    there are basic ways people have been testing and giving demos on zero-shot learning
    for object detection.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will implement a non-documented zero-shot application of
    CLIP, which is image object counting. Counting means the model will be performing
    regression. Let’s start the implementation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s import all the necessary libraries. We will be using the open
    source `clip` library from [https://github.com/openai/CLIP](https://github.com/openai/CLIP)
    to utilize a pretrained version of CLIP using a visual transformer model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will load the pretrained CLIP model in either CPU mode or GPU mode
    if the CUDA toolkit is installed:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The model variable we loaded here is a class that contains the individual methods
    to encode images and text with the encoder model loaded with pretrained weights.
    Additionally, the preprocess variable is a method that needs to be executed on
    the input before it’s fed to the image encoder. It performs normalization on the
    data that was used during training.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To predict with this model more conveniently, we will create a helper method
    with the following structure.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Remember that CLIP is trained to generate the image encoder and text encoder
    outputs in a way that they’re mapped into the same feature space. Matching text
    descriptions to an image will produce text and image features that are close together
    in distance. Since the metric we used is cosine similarity, the higher the similarity
    value, the lower the distance. The main technique to realize zero-shot learning
    from CLIP is to think of multiple descriptions that represent a certain label
    and choose the label that has the highest similarity score against an image. The
    similarity score will then be normalized against all the other labels to obtain
    a probability score. Additionally, `top_k` is used to control how many top highest
    similarity score text description indices and scores to return. We will come back
    to how to design the descriptions objectively for zero-shot learning later, once
    we’ve defined the code that will belong in the predict method we defined previously.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The first part of this method will be to preprocess the provided single image
    input and multiple text descriptions, called `clip_labels`. The image will be
    preprocessed according to the provided preprocessor, while the multiple text descriptions
    will be tokenized according to the sub-word tokenizer used in the text transformer
    provided by the `clip` library:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will encode the preprocessed image and text inputs using the image
    encoder and text encoder, respectively:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In PyTorch, remember that we need to make sure no gradients are computed during
    inference mode as we are not training the model and don’t want that extra computation
    or RAM wastage.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now that the image and text features with the same column dimensions have been
    extracted, we will compute the cosine similarity score of the provided image input
    features against all the text description label features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In addition to the similarity score, the similarity among all the text descriptions
    is normalized using Softmax so that the scores will all add up to one. View this
    as a way to compare the similarity scores against other samples. This will essentially
    convert distance scores into a multiclass prediction setting where the provided
    text descriptions as labels are all the possible classes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Next, we will extract the `top_k` highest similarity score and return their
    similarity score probability and indices to indicate which label the score belongs
    to:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that the method is complete, we can load an image using Pillow, create
    some descriptions, and feed it into the method to perform zero-shot learning!
    In this tutorial, we will work on object counting. This can range from counting
    how many cars to properly account for parking availability to counting how many
    people to account for personnel that are required to service the people. In this
    tutorial, we will count the number of paper clips as a pet project. Note that
    this can easily be extended to other counting datasets and projects. We will use
    the dataset at [https://www.kaggle.com/datasets/jeffheaton/count-the-paperclips?resource=download](https://www.kaggle.com/datasets/jeffheaton/count-the-paperclips?resource=download)
    to achieve this. Be sure to download the dataset in the same folder where the
    code exists. Let’s load up a simple version with fewer paper clips than 2:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will get the following output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 9.4 – Example paper clip counting from easy (A), medium (B), to hard
    (C)](img/B18187_09_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 9.4 – Example paper clip counting from easy (A), medium (B), to hard
    (C)
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 9**.4 B* and *Figure 9**.4 C* are harder examples that we will explore
    after we go through predictions for *Figure* *9**.4 A*.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we need multiple sets of text descriptions to account for all the possible
    counts of paper clips. A simple description that just uses numbers as text is
    not descriptive enough to even get close to a good similarity score. Instead,
    let’s add some additional text along with the number, as shown in the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this code, we use all numbers from 0 to 99 with the same surrounding text.
    More effort can be put into designing more variations of the pretext needed. It
    is also possible to utilize multiple pretexts with the same raw labels. The more
    descriptive the text is of the image, the more likely there will be a description
    that produces the closest features to the image.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let’s run a prediction on this example and see how it performs:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It accurately predicted two paper clips!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s go through two more harder examples from *Figure 9**.4 B* and *Figure*
    *9**.4 C*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This produces an error of 1 since the image contains three paper clips. Predicting
    on *Figure 9**.4 C* produces the following results:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This produces an error of two since the image contains 16 paper clips. Not bad
    at all!
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Now, let’s evaluate the mean error on a partitioned validation dataset of 1,000
    examples:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This results in a `23.8122` average count error. All of a sudden, this doesn’t
    look that usable. In some of the examples, the model couldn’t properly count clips
    that were partially occluded, even when in the description it was specified that
    there would be some partially occluded. It might make sense to add a description
    stating that the clips are in different sizes, or even that it is on top of a
    piece of lined paper. Try it for yourself and do some experiments!
  prefs: []
  type: TYPE_NORMAL
- en: Training a ridge regressor model with a pretrained SqueezeNet featurizer on
    this dataset separately and validated on the same validation partition in *step
    12* results in a root mean squared error of `1.9102`. This shows that zero-shot
    learning by itself is not as reliable as a supervised model for this use case,
    but it might be able to discern and predict nicely on simpler images. In the paper,
    the authors emphasized that CLIP-based zero-shot learning can work well and achieve
    performance close to supervised learning techniques in some specific use cases
    and datasets. However, in most use cases and datasets, CLIP-based zero-shot learning
    still falls way behind proper supervised learning methods. This tutorial is an
    example of where it could work, but it doesn’t work so reliably that it can be
    utilized in the real world. However, it does show promise in the base unsupervised
    method that CLIP was trained in. A few more years and some more research down
    the line, and we’ll be sure to see an even better performance that will likely
    be generally the same supervised learning or better!
  prefs: []
  type: TYPE_NORMAL
- en: Next, let’s explore the dimensionality reduction component of unsupervised learning.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the dimensionality reduction component of unsupervised deep learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Dimensionality reduction is a technique that can be useful in cases where a
    faster runtime is needed to train and perform inference on your model or when
    the model has a hard time learning from too much data. The most well-known unsupervised
    deep learning method for dimensionality reduction is based on autoencoders, which
    we discussed in [*Chapter 5*](B18187_05.xhtml#_idTextAnchor085), *Understanding
    Autoencoders*. A typical autoencoder network is trained to reproduce the input
    data as an unsupervised learning method. This is done through the encoder-decoder
    structure. At inference time, using only the encoder will allow you to perform
    dimensionality reduction as the outputs of the encoder will contain the most compact
    representation, which can fully reconstruct the original input data. Autoencoders
    can support different modalities, with one modality at any one time, which makes
    it a very versatile unsupervised dimensionality reduction method.
  prefs: []
  type: TYPE_NORMAL
- en: Other examples include unsupervised methods to create word embeddings that use
    shallow neural networks, such as FastText or Word2vec. The number of unique words
    that exist in a language is huge. Even if this gets scaled down to the total amount
    of unique words in the training data, this can balloon up to 100,000 words easily.
    One simple way to encode the words is by using one-hot-encoding or TF-IDF. Both
    methods produce 100,000 column features when the dataset contains 100,000 unique
    words. This will easily blow up the RAM requirements and can make or break the
    potential of a solution. However, word embeddings can be tuned to have the desired
    amount of feature columns as you can choose the embedding size during pretraining
    with the language model you’re using.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, let’s learn how to detect anomalies in external data.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting anomalies in external data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Anomaly detection** is also considered to be an important application of
    unsupervised learning in general. Anomaly detection can be used in cases where
    you want to perform any kind of filtering of your existing data, called **outlier
    detection**, and also act as a real-time detector during the inference stage given
    new external data, known as **novelty detection**. Here are some examples of end
    user use cases of anomaly detection:'
  prefs: []
  type: TYPE_NORMAL
- en: Removing noise in your dataset that will be fed into a supervised feature learning
    process to enable more stable learning.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Removing defective products in the production line. This can range from the
    manufacturing production of semiconductor wafers to egg production.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Fraud prevention by detecting anomalous transactions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scam detection through SMS, email, or direct messenger platforms.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Anomaly detection is a two-class or binary problem. This means that an alternative
    way people approach these example use cases listed is to use supervised learning
    and collect a bunch of negative samples and positive samples. The supervised learning
    approach can work well when a good-quality dataset containing negative/anomalous
    data has been collected and labeled because usually, there will already be a good
    amount of positive/normal data. Go back to the *Preparing data* section of [*Chapter
    1*](B18187_01.xhtml#_idTextAnchor015), *Deep Learning Life Cycle*, to recap what
    it means to have quality data for machine learning! However, in reality, we can’t
    possibly capture all the possible variations and types of negative samples that
    can occur. Anomaly detection-specific algorithms are built to handle anomalies
    or negative samples more generally and can extrapolate well to unseen examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'One of the more widely known deep learning methods to achieve anomaly detection
    on external data that’s not used for training is the autoencoder model. The model
    performs something called a **one-class classifier**, analogous to the traditional
    one-class support vector machines. This means that autoencoders can be trained
    to reconstruct the original data that’s fed into it to achieve either or a combination
    of four tasks – dimensionality reduction, noise remover, general featurizer, and
    anomaly detection. Depending on the objective at hand, the autoencoder should
    be trained differently. For dimensionality reduction, noise remover, and general
    featurizer, the autoencoder should be trained normally as introduced. For anomaly
    detection, the following things need to be done to ensure it achieves the desired
    behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: The data without anomalies is to be used as the training data. If you know there
    might be anomalies, use a simple outlier detection algorithm such as the local
    outlier factor, and remove outliers in the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Use a combination of anomalous and non-anomalous data to form the validation
    and holdout partition. If such categorization is not known beforehand, use an
    outlier-based anomaly detection algorithm from *step 1* to label the data.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *step 2* is somehow not possible, train and overfit the training data using
    the mean squared error loss. This is so that you can make sure the model will
    only be able to reconstruct the data that has the same characteristics as the
    training data. Overfitting can be done by training and making sure the training
    reconstruction loss becomes lower each epoch.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: If *step 2* is possible, train, validate and evaluate with the given cross-validation
    partitioning strategy and follow the tips introduced in the *Training supervised
    deep learning models effectively* section in the previous chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The predict function of the autoencoder for testing, validation, and inference
    can be set based on how well the model can reproduce the input data indicated
    through the mean squared error. A threshold will need to be used as a cut-off
    mechanism to determine which data is anomalous or not. The higher the mean squared
    error, the more probable it is that the provided data is anomalous. Since anomaly
    detection is a binary classification problem, once labels are available, perform
    performance analysis by using ROC curves, confusion metrics, and the log loss
    error using different mean squared error thresholds. This will be introduced more
    comprehensively in the next chapter.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: In anomaly detection in general, note that anomalies are a vague description
    of what it means to be an anomaly. Different algorithms encode their definition
    of what an anomaly is given a dataset, its feature space, and its feature distribution.
    When an algorithm does not work well for your predefined case of anomalies, it
    does not mean that the algorithm is not a good model in general. The autoencoder
    approach won’t always be the best method that captures your intuition of anomalies.
    When it comes to unsupervised anomaly detection learning, make sure you train
    a bunch of models with proper hyperparameter settings and analyze individually
    which models match your intuition of what makes data anomalous.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Deep learning has made significant contributions to the field of unsupervised
    learning, leading to the development of several innovative methods. But by far
    the most impactful method is unsupervised pretraining, which leverages the abundance
    of free data available on the internet today to improve the model performance
    of the downstream supervised tasks and create generalizable representations. With
    deeper research and time, unsupervised learning will aid in closing the gap toward
    general artificial intelligence. Overall, deep learning has been a valuable tool
    in the unsupervised learning domain, helping practitioners make the most of the
    large amounts of free data available on the internet today.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will dive into the first chapter of the second part
    of this book, which is meant to introduce methods that provide insights about
    a trained deep learning model.
  prefs: []
  type: TYPE_NORMAL
- en: Part 2 – Multimodal Model Insights
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this part of the book, we delve into the fascinating world of multimodal
    model insights, taking you on a comprehensive journey through various aspects
    of evaluating, interpreting, and securing deep learning models. This part offers
    a comprehensive understanding of various facets of model assessment and enhancement
    while emphasizing the importance of responsible and effective AI deployment in
    real-world applications. Throughout these chapters, you will explore methods for
    evaluating and understanding model predictions, interpreting neural networks,
    and addressing ethical and security concerns, such as bias, fairness, and adversarial
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: By the end of this part, you will have a solid understanding of the importance
    of model evaluation, interpretation, and security, enabling you to create robust,
    reliable, and equitable deep learning systems and solutions that not only excel
    in performance but also consider ethical implications and potential vulnerabilities
    while standing the test of time for real-world applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'This part contains the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 10*, *Exploring Model Evaluation Methods*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapter 11*, *Explaining Neural Network Predictions*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapter 12*, *Interpreting Neural Networks*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapter 13*, *Exploring Bias and Fairness*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Chapter 14*, *Analyzing Adversarial Performance*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
