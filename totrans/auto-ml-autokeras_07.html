<html><head></head><body>
		<div id="_idContainer068">
			<h1 id="_idParaDest-76"><em class="italic"><a id="_idTextAnchor077"/>Chapter 5</em>: Text Classification and Regression Using AutoKeras</h1>
			<p>In this chapter, we will focus on the use of AutoKeras to work with text (a sequence of words).</p>
			<p>In the previous chapter, we saw that there was a specialized type of network suitable for image processing, called a <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>). In this chapter, we will see what <a id="_idTextAnchor078"/><strong class="bold">recurrent neural networks</strong> (<strong class="bold">RNNs</strong>) are and how they work. An RNN is a type of neural network that is very suited to working with text. </p>
			<p>We will also use a classifier and a regressor to solve text-based tasks. By the end of the chapter, you will have learned how to use AutoKeras to solve a wide variety of problems that are text-based, such as extracting emotions from tweets, detecting spam in emails, and so on.</p>
			<p>In this chapter, we will cover the following topics:</p>
			<ul>
				<li>Working with text data</li>
				<li>Understanding RNNs—what are these neural networks and how do they work?</li>
				<li>One-dimensional CNNs (Conv1D)</li>
				<li>Creating an email spam detector</li>
				<li>Predicting news popularity in social media</li>
			</ul>
			<h1 id="_idParaDest-77"><a id="_idTextAnchor079"/>Technical requirements </h1>
			<p>All coding examples in this book are available as Jupyter notebooks that can be downloaded from the following link: <a href="https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras">https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras</a>.</p>
			<p>As code cells can be executed, each notebook can be self-installable, by adding a code snippet with the requirements you need. For this reason, at the beginning of each notebook there is a code cell for environmental setup, which installs AutoKeras and its dependencies.</p>
			<p>So, to run the coding examples, you only need a computer with Ubuntu Linux as the operating system and can install the Jupyter Notebook with the following command line:</p>
			<p class="source-code">$ apt-get install python3-pip jupyter-notebook</p>
			<p>Alternatively, you can also run these notebooks using Google Colaboratory, in which case you will only need a web browser. For further details, see the <em class="italic">AutoKeras with Google Colaboratory</em> section in <a href="B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with AutoKeras</em>. Furthermore, in the <em class="italic">Installing AutoKeras</em> section, you will also find other installation options.</p>
			<h1 id="_idParaDest-78"><a id="_idTextAnchor080"/>Working with text data</h1>
			<p>AutoKeras <a id="_idIndexMarker238"/>allows us to quickly and easily create high-performance models for solving text-based tasks.  </p>
			<p>Text is an excellent source of information to feed DL models, and there is a multitude of sources that are text-based, such as social media, chats, emails, articles, books, and countless tasks to automate based on text, such as the following:</p>
			<ul>
				<li><strong class="bold">Translation</strong>: Convert source text in one language to text in another language.</li>
				<li><strong class="bold">Conversational bots</strong>: Simulate human conversation using ML models.</li>
				<li><strong class="bold">Sentiment analysis</strong>: Classification of emotions by analyzing text data.</li>
				<li><strong class="bold">Spam classifiers</strong>: Email classification using machine learning models.</li>
				<li><strong class="bold">Document summarizers</strong>: Generate summaries of documents automatically.</li>
				<li><strong class="bold">Text generators</strong>: Generate text from scratch automatically.</li>
			</ul>
			<p>As with other types of data, AutoKeras will do all the preprocessing so that we can pass the text directly to our model, but before starting with the practical examples, let's take a look at what it does under the hood. </p>
			<h2 id="_idParaDest-79"><a id="_idTextAnchor081"/>Tokenization</h2>
			<p>As we already know, neural networks take vectors of numbers as input, so the text must be converted to numerical Tensors in a <a id="_idIndexMarker239"/>process called <strong class="bold">vectorization</strong>. Before that, however, we must cut the text into segments.</p>
			<p>This text segmentation<a id="_idIndexMarker240"/> can be done with the help of different units, such as the following:</p>
			<ul>
				<li><strong class="bold">Word</strong>: Divide the text by words.</li>
				<li><strong class="bold">Character</strong>: Fragment the text into characters.</li>
				<li><strong class="bold">N-gram</strong>: Extract N-grams of words or characters. N-grams are overlapping groupings of multiple consecutive words or characters.</li>
			</ul>
			<p>The units mentioned previously are<a id="_idIndexMarker241"/> called <strong class="bold">tokens</strong>, and the process of dividing the text into said tokens is <a id="_idIndexMarker242"/>called <strong class="bold">tokenization</strong>. This is a necessary step to convert the text to tensors in the vectorization process, which we explain next.</p>
			<h2 id="_idParaDest-80"><a id="_idTextAnchor082"/>Vectorization</h2>
			<p>Once the text is <a id="_idIndexMarker243"/>tokenized, vectorization is performed. This process transforms each word/character/N-gram into a vector.</p>
			<p>All text vectorization processes consist of the following steps:</p>
			<ol>
				<li>Applying some tokenization scheme </li>
				<li>Associating numeric vectors with the generated tokens </li>
			</ol>
			<p>These vectors, packed into <a id="_idIndexMarker244"/>sequence tensors, feed into <strong class="bold">deep neural networks</strong> (<strong class="bold">DNNs</strong>).</p>
			<p>There are multiple ways to associate a token with a vector. Let's see two of the most important ones, as follows:</p>
			<ul>
				<li><strong class="bold">One-hot token encoding</strong> is the <a id="_idIndexMarker245"/>simplest way to associate a token with a vector. If we have used words for tokenization, one-hot encoding consists of associating a unique integer index with each word and then converting this integer index <em class="italic">i</em> into a binary vector of size <em class="italic">N</em> (the size of the vocabulary) so that the value of input <em class="italic">i</em> is <strong class="source-inline">1</strong> and all the remaining values of the vector are zeros.</li>
				<li><strong class="bold">Token embedding</strong> is another<a id="_idIndexMarker246"/> form of token-vector association that is widely used and is more powerful than one-hot encoding. While the vectors obtained by one-hot encoding are binary (one input with value <em class="italic">1</em> and the rest of values <em class="italic">0</em>) and large (they must have the same length as the number of words in the vocabulary), the embeddings of words are low-dimensional floating-point vectors.<p>The word vectors obtained by one-hot encoding are static (the position in the array determines the word and these values never change), while the word-embedding vectors are dynamic (they are learned from the data), in such a way that their values are modified during learning, in the same way that they are with the weights of the neural network layers.</p><p>It is this dynamism that allows it to store more information in a much smaller size, as you can see in the following screenshot:</p></li>
			</ul>
			<div>
				<div id="_idContainer058" class="IMG---Figure">
					<img src="image/B16953_05_01.jpg" alt="Figure 5.1 – One-hot encoding versus embedding comparison"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.1 – One-hot encoding versus embedding comparison</p>
			<p>Just as convolutional networks were the most appropriate choice for image-based tasks, when we talk about word processing, the most optimal type of network is an RNN. Let's see what this consists of in the following section.</p>
			<h1 id="_idParaDest-81"><a id="_idTextAnchor083"/>Understanding RNNs</h1>
			<p>A common<a id="_idIndexMarker247"/> feature of all the neural networks seen so far is that they don't have a memory. Networks formed by either fully connected layers or convolutional layers process each input independently so that it is isolated from the other layers. However, in RNNs, "the past" is taken into account, and this is done using its previous output as the<a id="_idIndexMarker248"/> state; so, an RNN layer will have two inputs, one is which is the standard input of the current vector, and the other being the output of the previous vector, as seen in the following diagram:</p>
			<div>
				<div id="_idContainer059" class="IMG---Figure">
					<img src="image/B16953_05_02.jpg" alt="Figure 5.2 – RNN loop unfolded"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.2 – RNN loop unfolded</p>
			<p>The RNN implements this memory feature with an internal loop over the entire sequence of elements. Let's explain it with some pseudocode, as follows:</p>
			<p class="source-code">state = 0</p>
			<p class="source-code">for input in input_sequence:</p>
			<p class="source-code">     output = f(input, state)</p>
			<p class="source-code">     state = output</p>
			<p>There are several types of RNN architectures with much more complex systems than the one presented here, but this is beyond the scope of the book. Understanding the concepts explained here is enough, since both the configuration and the choice of architecture to be used will be handled by AutoKeras.</p>
			<h1 id="_idParaDest-82"><a id="_idTextAnchor084"/>One-dimensional CNNs (Conv1D)</h1>
			<p>Another architecture to <a id="_idIndexMarker249"/>take into account when working with texts is one-dimensional CNNs (Conv1D). The principle on which they are based is similar to the 2D CNN that we saw in the previous chapter, <a href="B16953_04_Final_PG_ePub.xhtml#_idTextAnchor063"><em class="italic">Chapter 4</em></a>, <em class="italic">Image Classification and Regression Using AutoKeras</em>. These neural networks manage to learn patterns in text through filters, in the same way as they did with images in the previous chapter.</p>
			<p>An example of a one-dimensional CNN is shown in the following diagram:</p>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B16953_05_03.jpg" alt="Figure 5.3 – 1D convolution over text sequences"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.3 – 1D convolution over text sequences</p>
			<p>It is good to know that if the chronological order of the elements in the sequence is important for the prediction, the RNNs are much more effective, thus one-dimensional CNNs are often combined with the RNNs to create high-performance models. The exhaustive search performed by AutoKeras takes both into account to find the best model.</p>
			<p>Now, let's put the learned concepts into practice with some practical examples.</p>
			<h1 id="_idParaDest-83"><a id="_idTextAnchor085"/>Creating an email spam detector</h1>
			<p>The model we are going to<a id="_idIndexMarker250"/> create will detect spam emails from an <strong class="source-inline">emails</strong> dataset. This is a little dataset of 5,572 emails, labeled with a <strong class="source-inline">spam</strong> column. </p>
			<p>The notebook with the complete source code can be found at the following link:</p>
			<p><a href="https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb">https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb</a></p>
			<p>Let's now have a look at the relevant cells of the notebook in detail, as follows:</p>
			<ul>
				<li><strong class="bold">Installing AutoKeras</strong>: As we commented in other examples, the following snippet at the top of the notebook is responsible for installing AutoKeras and its dependencies, using the <strong class="source-inline">pip</strong> package manager:<p class="source-code">!pip3 install autokeras</p></li>
				<li><strong class="bold">Importing needed packages</strong>: The following lines load <strong class="source-inline">tensorflow</strong>, <strong class="source-inline">pandas</strong>, <strong class="source-inline">numpy</strong>, and <strong class="source-inline">autokeras</strong> as needed dependencies for this project:<p class="source-code">import tensorflow as tf</p><p class="source-code">import pandas as pd </p><p class="source-code">import numpy as np</p><p class="source-code">import autokeras as ak</p><p class="source-code">from sklearn import model_selection</p></li>
				<li><strong class="bold">Creating datasets</strong>: First, we load and preprocess the <strong class="source-inline">emails</strong> spam dataset from our GitHub repository, as follows:<p class="source-code">emails_dataset = pd.read_csv("https://raw.githubusercontent.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/main/spam.csv", encoding="latin-1")</p><p>We now prepare our dataset by renaming the relevant columns and removing unnecessary ones, as follows:</p><p class="source-code">emails_dataset.drop(['Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4'], axis = 1, inplace = True)</p><p class="source-code">emails_dataset.rename(columns = {'v1': 'spam', 'v2': 'message'}, inplace = True)</p><p class="source-code">emails_dataset['spam'] = emails_dataset['spam'].map({'ham': 0, 'spam': 1})</p><p class="source-code">emails_dataset.head()</p><p>Here is the <a id="_idIndexMarker251"/>output of the preceding code:</p></li>
			</ul>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B16953_05_04.jpg" alt="Figure 5.4 – Notebook output of dataset preview"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.4 – Notebook output of dataset preview</p>
			<p>Let's now split the dataset into <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong> datasets, as follows:</p>
			<p class="source-code">x_train, x_test, y_train, y_test = model_selection.train_test_split(emails_dataset.message.to_numpy(), emails_dataset.spam.to_numpy())</p>
			<p>We are ready to create the spam classifier.</p>
			<h2 id="_idParaDest-84"><a id="_idTextAnchor086"/>Creating the spam predictor</h2>
			<p>Now, we <a id="_idIndexMarker252"/>will use the AutoKeras <strong class="source-inline">TextClassifier</strong> class to find the best classification model. Just for this little example, we set <strong class="source-inline">max_trials</strong> (the maximum number of different Keras models to try) to 2, and we do not set the <strong class="source-inline">epochs</strong> parameter but rather define an <strong class="source-inline">EarlyStopping</strong> callback of <strong class="source-inline">2</strong> epochs, such that the training process stops if the loss of validation does not improve in two consecutive epochs. The code is shown in the following snippet:</p>
			<p class="source-code">clf = ak.TextClassifier(max_trials=2)</p>
			<p class="source-code">cbs = [tf.keras.callbacks.EarlyStopping(patience=2)]</p>
			<p>Let's run the training to search for the optimal classifier for the training dataset, as follows: </p>
			<p class="source-code">clf.fit(x_train, y_train, callbacks=cbs)</p>
			<p>Here is the output of the preceding code:</p>
			<div>
				<div id="_idContainer062" class="IMG---Figure">
					<img src="image/B16953_05_05.jpg" alt="Figure 5.5 – Notebook output of text classifier training&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.5 – Notebook output of text classifier training</p>
			<p>The previous output shows that the accuracy with the training dataset is increasing. </p>
			<p>As we can see, we achieved a loss value of <strong class="source-inline">0.080</strong> in the validation set. It's a really good number just for one minute of training. We have limited the search to two architectures (<strong class="source-inline">max_trials = 2</strong>). Increasing this number would give us a more accurate model, although it would also take longer to finish.</p>
			<h2 id="_idParaDest-85"><a id="_idTextAnchor087"/>Evaluating the model</h2>
			<p>It's time to<a id="_idIndexMarker253"/> evaluate the best model with the testing dataset. We can do this by running the following command:</p>
			<p class="source-code">clf.evaluate(x_test, y_test)</p>
			<p>Here is the output of the preceding command:</p>
			<p class="source-code">44/44 [==============================] - 0s 4ms/step - loss: 0.0491 - accuracy: 0.9849</p>
			<p class="source-code">[0.04908078908920288, 0.9849246144294739]</p>
			<p>As we can see, <strong class="source-inline">0.9849</strong> as prediction accuracy in the test set is a really good final prediction score for the time invested.</p>
			<h2 id="_idParaDest-86"><a id="_idTextAnchor088"/>Visualizing the model</h2>
			<p>Now, we<a id="_idIndexMarker254"/> can see a little summary of the architecture of the best generated model. We can do this by running the following code:</p>
			<p class="source-code">model = clf.export_model()</p>
			<p class="source-code">model.summary()</p>
			<p>Here is the output of the preceding code:</p>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/B16953_05_06.jpg" alt="Figure 5.6 – Best model architecture summary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.6 – Best model architecture summary</p>
			<p>As we can<a id="_idIndexMarker255"/> see here, AutoKeras has chosen a convolution model (Conv1D) to do the task. As we explained in <a href="B16953_04_Final_PG_ePub.xhtml#_idTextAnchor063"><em class="italic">Chapter 4</em></a>, <em class="italic">Image Classification and Regression Using AutoKeras</em>, this kind of architecture works great when the order of the elements in the sequence is not important for the prediction, as in this case.</p>
			<p>Here is a visual representation of the architecture:</p>
			<div>
				<div id="_idContainer064" class="IMG---Figure">
					<img src="image/B16953_05_07.jpg" alt="Figure 5.7 – Best model architecture visualization&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.7 – Best model architecture visualization</p>
			<p>As you already know, generating the models and choosing the best one is a task done by AutoKeras automatically, but let's briefly explain these blocks. </p>
			<p>Each block<a id="_idIndexMarker256"/> represents a layer, and the output of each is connected to the input of the next, except the first block (whose input is the text) and the last block (whose output is the predicted number). The blocks before the Conv1D block are all data-preprocessing blocks to vectorize the text-generating embeddings to feed this Conv1D block, as well as reduce the dimension of the filters through the max pooling layer. Notice that AutoKeras has also added several dropout blocks to reduce overfitting.</p>
			<p>In the next section, we are going to resolve a text regression problem with a practical example: we are going to create a news popularity predictor.</p>
			<h1 id="_idParaDest-87"><a id="_idTextAnchor089"/>Predicting news popularity in social media</h1>
			<p>In this section, we will <a id="_idIndexMarker257"/>create a model that will find out the popularity score for an article on social media platforms, based on its text. For this, we will train the model with a <em class="italic">News Popularity</em> dataset collected between 2015 and 2016 (<a href="https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms">https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms</a>).</p>
			<p>As we want to approximate a score (number of likes), we will use a text regressor for this task.</p>
			<p>In the next screenshot, you can see some samples taken from this dataset:</p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/B16953_05_08.jpg" alt="Figure 5.8 – A few samples from the News Popularity dataset&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.8 – A few samples from the News Popularity dataset</p>
			<p>This notebook with the complete source code can be found at <a href="https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb">https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb</a>.</p>
			<p>We will now <a id="_idIndexMarker258"/>explain the relevant code cells of the notebook in detail, as follows:</p>
			<ul>
				<li><strong class="bold">Getting the articles dataset</strong>: Before training, we have to download the dataset that contains the text of each article, as well as the popularity score. Here is the code to do this:<p class="source-code">news_df = pd.read_csv("https://archive.ics.uci.edu/ml/machine-learning-databases/00432/Data/News_Final.csv")</p></li>
				<li><strong class="bold">Data preprocessing</strong>: As we want to estimate the popularity score (number) based on its title and headline, we will use a regression model. But first, we have to prepare the text data in a suitable format, which we can do with the following code:<p class="source-code">text_inputs = np.array(news_df.Title+ ". " + news_df.Headline).astype("str")</p><p>In the previous code, we have merged the <strong class="source-inline">Title</strong> and <strong class="source-inline">Headline</strong> text columns to feed our regression model.</p><p>Now, we extract the popularity score of each article on LinkedIn, to be used as labels. We have decided to use only the LinkedIn scores to simplify the example. The code is shown in the following snippet:</p><p class="source-code">media_success_outputs = news_df.LinkedIn.to_numpy(dtype="int")</p><p>Next, we will create the train and test datasets, as follows:</p></li>
				<li><strong class="bold">Creating the datasets</strong>: We split the dataset in a <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong> set using the <strong class="source-inline">sklearn</strong> function, as follows:<p class="source-code">from sklearn.model_selection import train_test_split</p><p class="source-code">x_train, x_test, y_train, y_test = train_test_split(text_inputs, media_success_outputs, test_size = 0.2, random_state = 10)</p></li>
			</ul>
			<p>Once we have created our datasets we are ready to feed our model, but first, we have to create it.</p>
			<h2 id="_idParaDest-88"><a id="_idTextAnchor090"/>Creating a text regressor</h2>
			<p>Because we<a id="_idIndexMarker259"/> want to predict a popularity score from a set of text sentences, and this score is a scalar value, we are going to use AutoKeras <strong class="source-inline">TextRegressor</strong>. For this example, we set <strong class="source-inline">max_trials</strong> to <strong class="source-inline">2</strong>, and we do not set the <strong class="source-inline">epochs</strong> parameter but rather define an <strong class="source-inline">EarlyStopping</strong> callback of <strong class="source-inline">2</strong> epochs of patience, such that the training process stops if the validation loss does not decrease in two consecutive epochs. The code can be seen in the following snippet:</p>
			<p class="source-code">reg = ak.ImageRegressor(max_trials=2)</p>
			<p class="source-code">cbs = [tf.keras.callbacks.EarlyStopping(patience=2)]</p>
			<p>Let's run the training to search for the optimal regressor for the training dataset, as follows:</p>
			<p class="source-code">reg.fit(x_train, y_train, callbacks=cbs)</p>
			<p>Here is the output of the preceding code:</p>
			<div>
				<div id="_idContainer066" class="IMG---Figure">
					<img src="image/B16953_05_09.jpg" alt="Figure 5.9 – Notebook output of the training of our news popularity predictor"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.9 – Notebook output of the training of our news popularity predictor</p>
			<p>As we can see<a id="_idIndexMarker260"/> from the previous output, after a few minutes we have a model with <strong class="source-inline">14726</strong> as the best validation loss (<strong class="bold">mean squared error</strong>, or <strong class="bold">MSE</strong>). This means that every prediction is failing at an <a id="_idIndexMarker261"/>average of <strong class="source-inline">121</strong> (square root of <strong class="source-inline">14726</strong>) in the final score, which is not a bad result for the time invested. Let's see how it's working with the test set.</p>
			<h2 id="_idParaDest-89"><a id="_idTextAnchor091"/>Evaluating the model</h2>
			<p>Time to evaluate the best <a id="_idIndexMarker262"/>model with the testing dataset. We do this by running the following code:</p>
			<p class="source-code">reg.evaluate(x_test, y_test)</p>
			<p>Here is the output of the preceding code:</p>
			<p class="source-code">583/583 [==============================] - 3s 5ms/step - loss: 13944.2070 - mean_squared_error: 13944.2070</p>
			<p class="source-code"> </p>
			<p class="source-code">[13944.20703125, 13944.20703125]</p>
			<p>As we can see, <strong class="source-inline">13944</strong> is a really good prediction score for the time invested. If we run AutoKeras with <a id="_idIndexMarker263"/>more trials, we will get better results.</p>
			<h2 id="_idParaDest-90"><a id="_idTextAnchor092"/>Visualizing the model</h2>
			<p>Now, it's time to take a<a id="_idIndexMarker264"/> look at what we have under the hood. We'll run the following code:</p>
			<p class="source-code">model = reg.export_model()</p>
			<p class="source-code">model.summary()</p>
			<p>Here is the output of the preceding code:</p>
			<div>
				<div id="_idContainer067" class="IMG---Figure">
					<img src="image/B16953_05_10.jpg" alt="Figure 5.10 – Best model architecture summary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 5.10 – Best model architecture summary</p>
			<p>As in the<a id="_idIndexMarker265"/> previous classification example, AutoKeras has chosen a convolution model (Conv1D) to do the task. As we explained before, this is a less time-consuming architecture than RNN and is most suitable when the order of the elements in the sequence is not important for the prediction.</p>
			<h2 id="_idParaDest-91"><a id="_idTextAnchor093"/>Improving the model performance</h2>
			<p>As we did in previous <a id="_idIndexMarker266"/>examples, if we need more precision in less time, we can fine-tune our model using an advanced AutoKeras feature that allows you to customize your search space.</p>
			<p>By using <strong class="source-inline">AutoModel</strong> with <strong class="source-inline">TextBlock</strong> instead of <strong class="source-inline">TextRegressor</strong>, we can create high-level configurations, such as <strong class="source-inline">block_type</strong> for the type of neural network to look for; or, if your text source has a larger vocabulary (number of distinct words), you may need to create a custom pipeline in AutoKeras to increase the <strong class="source-inline">max_tokens</strong> parameter.</p>
			<p>See the following example for more details:</p>
			<p class="source-code">cbs = [tf.keras.callbacks.EarlyStopping(patience=2)]</p>
			<p class="source-code">input_node = ak.TextInput()</p>
			<p class="source-code">output_node = ak.TextToIntSequence(max_tokens=20000)(input_node)</p>
			<p class="source-code">output_node = ak.TextBlock(block_type='ngram')(input_node)</p>
			<p class="source-code">output_node = ak.RegressionHead()(output_node)</p>
			<p class="source-code">automodel = ak.AutoModel(inputs=input_node, outputs=output_node, objective='val_mean_squared_error', max_trials=2)</p>
			<p class="source-code">automodel.fit(x_train, y_train, callbacks=cbs)</p>
			<p>In the previous code block, we have done the following with the settings:</p>
			<ul>
				<li>The <strong class="source-inline">EarlyStopping</strong> block will stop the training if the validation loss doesn't decrease in two consecutive epochs.</li>
				<li>The <strong class="source-inline">max_token</strong> parameter is set to <strong class="source-inline">20000</strong> because our text source has a larger vocabulary (number of distinct words). </li>
				<li>With <strong class="source-inline">TextBlock(block_type="ngram"</strong>, we are telling AutoKeras to only scan models using N-gram embeddings.</li>
			</ul>
			<p>You can also <a id="_idIndexMarker267"/>not specify any of these arguments, in which case these different options would be tuned automatically.</p>
			<h2 id="_idParaDest-92"><a id="_idTextAnchor094"/>Evaluating the model with the test set</h2>
			<p>After training, it is<a id="_idIndexMarker268"/> time to measure the actual prediction of our model using the reserved test dataset. In this way, we can rule out that the good results obtained with the training set are due to overfitting. Run the following code to do this:</p>
			<p class="source-code">automodel.evaluate(x_test, y_test)</p>
			<p>Here is the output of the preceding code:</p>
			<p class="source-code">583/583 [==============================] - 6s 9ms/step - loss: 13508.9316 - mean_squared_error: 13508.9316</p>
			<p class="source-code">[13508.931640625, 13508.931640625]</p>
			<p>The performance is slightly better than in the model without fine-tuning, but training it for a longer time surely improves it.</p>
			<h1 id="_idParaDest-93"><a id="_idTextAnchor095"/>Summary</h1>
			<p>In this chapter, we have learned how neural networks work with text data, what recurrent neural networks are and how they work.</p>
			<p>We've also put the concept of neural network into practice, using the power of AutoKeras, by implementing a spam predictor and a news popularity regressor, in just a few lines of code.</p>
			<p>Now that we have learned how to work with text, we are ready to move on to the next chapter, where you will learn how to work with structured data by implementing classification and regression models using AutoKeras. </p>
		</div>
	</body></html>