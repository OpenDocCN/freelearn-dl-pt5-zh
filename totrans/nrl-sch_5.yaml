- en: '5'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Multiple Search Modalities
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: With the benefits of deep learning and artificial intelligence, we can encode
    any kind of data into **vectors**. This allows us to create a search system that
    uses any kind of data as a query and returns any kind of data as a search result.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will introduce the rising topic of the **multimodal search
    problem**. You will see different data modalities and how to work with them. You
    will see how text, images, and audio documents can be transformed into vectors,
    and how to implement search systems independently of the data modality. You will
    also see the differences between the concepts of **multimodality** and **cross-modality**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: How to represent documents of different data types
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to encode multimodal documents
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Cross-modal and multimodal searches
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: By the end of this chapter, you will have a solid understanding of how cross-modal
    and multimodal searches work, and how easy it is to process data of different
    modalities in Jina.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This chapter has the following technical requirements:'
  prefs: []
  type: TYPE_NORMAL
- en: A laptop with a minimum of 4 GB of RAM (8 GB or more is preferred)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Python installed with *3.7*, *3.8*, or *3.9* on a Unix-like operating system,
    such as macOS or Ubuntu
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introducing multimodal documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Over the last decade, various types of data, such as **texts**, **images**,
    and **audio**, have been growing rapidly on the internet. Commonly, different
    types of data are associated with one piece of content. For example, images often
    also have textual tags and captions to describe the content. Therefore, the content
    has two modalities: image and text. A movie clip with subtitles has three modalities:
    image, audio, and text.'
  prefs: []
  type: TYPE_NORMAL
- en: Jina is a **data-type-agnostic framework**, letting you work with any type of
    data and develop cross-modal and multimodal search systems. To better understand
    what this implies, it makes sense to first show how to represent documents of
    different data types, and then show how to represent multimodal documents in Jina.
  prefs: []
  type: TYPE_NORMAL
- en: Text document
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To represent a textual document in Jina is quite easy. You can do it simply
    by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'In some cases, one document can include thousands of words. But a long document
    with thousands of words is hard to search; some finer granularity would be nice.
    You can do this by segmenting a long document into smaller *chunks*. For example,
    let’s segment this simple document by using the `!` mark:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This creates five subdocuments under the original document and stores them
    under `.chunks`. To see that more clearly, you can visualize it via `d.display()`,
    whose output is shown in the following figure:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.1 – An example of a text document with chunk-level subdocuments
    ](img/Figure_5.01_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – An example of a text document with chunk-level subdocuments
  prefs: []
  type: TYPE_NORMAL
- en: 'You can also print out each subdocument’s text attributes by using the `.texts`
    sugar attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: That’s all you need to know about representing textual documents in Jina!
  prefs: []
  type: TYPE_NORMAL
- en: Image document
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Compared to textual data, image data is more universal and easier to comprehend.
    Image data is often just an `ndim=2` or `ndim=3` and `dtype=uint8`. Each element
    in that ndarray represents a pixel value between `0` and `255` on a certain channel
    at a certain position. For example, a colored JPG image of 256x300 can be represented
    as a `[256, 300, 3]` ndarray. You may ask why `3` is in the last dimension. It
    is because it represents the `R`, `G`, and `B` channels of each pixel. Some images
    have a different number of channels. For example, a PNG with a transparent background
    has four channels, where the extra channel represents opacity. A grayscale image
    has only one channel, which represents the luminance (a measure representing the
    proportions of black and white).
  prefs: []
  type: TYPE_NORMAL
- en: 'In Jina, you can load image data by specifying the image URI and then convert
    it into `.tensor` using the Document API. As an example, we will use the following
    code to load a PNG apple image (as shown in *Figure 5.2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – An example PNG image located in the apple.png local file ](img/Figure_5.02_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – An example PNG image located in the apple.png local file
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'You will get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Now the image content is converted into a document’s `.tensor` field, which
    can then be used for further processing. Some help functions can be used to process
    the image data. You can resize it (that is, downsample/upsample) and normalize
    it. You can switch the channel axis of `.tensor` to meet certain requirements
    of some framework, and finally, you can chain all these processing steps together
    in one line. For example, the image can be normalized and the color axis should
    be placed first, not last. You can perform such image transformations with the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also dump `.tensor` back to a PNG image file by using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that the channel axis is now switched to `0` because of the processing
    steps we just conducted. Finally, you will get the resulting image shown in *Figure
    5.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – The resulting image after resizing and normalizing ](img/Figure_5.03_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – The resulting image after resizing and normalizing
  prefs: []
  type: TYPE_NORMAL
- en: Audio document
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As an important format for storing information, digital audio data can be a
    soundbite, music, a ringtone, or background noise. It often comes in `.wav` and
    `.mp3` formats, where the sound waves are digitized by sampling them at discrete
    intervals. To load a `.wav` file as a document in Jina, you can simply use the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'You will see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'As shown in the preceding example, the data from the `.wav` file is converted
    to a one-dimension (mono) ndarray, in which each element is generally expected
    to lie in the range [-1.0, +1.0] scale. You are by no means restricted to using
    Jina-native methods for audio processing. Here are some command-line tools, programs,
    and libraries that you can use for more advanced handling of audio data:'
  prefs: []
  type: TYPE_NORMAL
- en: '**FFmpeg** ([https://ffmpeg.org/](https://ffmpeg.org/)): This is a free, open
    source project for handling multimedia files and streams.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pydub** ([https://github.com/jiaaro/pydub](https://github.com/jiaaro/pydub)):
    This manipulates audio with a simple and easy-to-use high-level interface.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Librosa** ([https://librosa.github.io/librosa/](https://librosa.github.io/librosa/)):
    This is a Python package for music and audio analysis.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Multimodal document
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: So far, you have learned how to represent different data modalities in Jina.
    However, in the real world, data often comes in a form that combines multiple
    modalities, such as video, which typically includes at least *image* and *audio*,
    as well as *text* in the form of subtitles. Now, it is very interesting to know
    how to represent multimodal data.
  prefs: []
  type: TYPE_NORMAL
- en: A Jina document can be nested vertically via chunks. It is intuitive to put
    data of different modalities into subdocuments in chunks. For example, you can
    create a fashion product document that has two modalities, including a dress image
    and a product description.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – An example of a fashion product document with two modalities
    ](img/Figure_5.04_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – An example of a fashion product document with two modalities
  prefs: []
  type: TYPE_NORMAL
- en: 'You can do it simply by using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the example fashion product (as shown in *Figure 5.4*) is represented
    as a Jina document, which has two chunk-level documents representing the product’s
    description and dress image, respectively. You can also use `fashion_doc.display()`
    to produce the visualization, as shown in *Figure 5.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – An example of a fashion product document with two chunk-level
    documents ](img/Figure_5.05_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.5 – An example of a fashion product document with two chunk-level documents
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: You may think that different modalities correspond to different kinds of data
    (images and text in this case). However, this is not accurate. For example, you
    can do a cross-modal search by searching for images from different perspectives
    or searching for matching titles for given paragraph text. Therefore, we can consider
    that a modality is related to a given data distribution from which the data may
    come.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned how to represent a single piece of text, image, and
    audio data, as well as representing multimodal data as a Jina document. In the
    following section, we will show how to get the embedding of each document.
  prefs: []
  type: TYPE_NORMAL
- en: How to encode multimodal documents
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After defining the document for different types of data, the next step is to
    encode the documents into vector embeddings using a model. Formally, embedding
    was a multi-dimension of a document (often a `[1, D]` vector), which was designed
    to contain the content information of a document. With current advances in the
    performance of all the deep learning methods, even general-purpose models (for
    example, CNN models trained on ImageNet) can be used to extract meaningful feature
    vectors. In the following sections, we will show how to encode embedding for documents
    of different modalities.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding text documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To convert textual documents into vectors, we can use the pretrained Bert model
    ([https://www.sbert.net/docs/pretrained_models.xhtml](https://www.sbert.net/docs/pretrained_models.xhtml))
    provided by Sentence Transformer ([https://www.sbert.net/](https://www.sbert.net/)),
    as shown in the following example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: As a result, each document in the input `DocumentArray` will have an embedding
    with a 384-dimensional dense vector space after `.encode(...)` has been completed.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding image documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For encoding image documents, we can use a pretrained model from Pytorch for
    embedding. As an example, we will use the **ResNet50** network ([https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385))
    for object classification on images provided by **torchvision** ([https://pytorch.org/vision/stable/models.xhtml](https://pytorch.org/vision/stable/models.xhtml)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: In this way, we’ve successfully encoded an image document into its feature vector
    representation. The feature vector generated is the output activations of the
    neural network (a vector of 1,000 components).
  prefs: []
  type: TYPE_NORMAL
- en: Important Note
  prefs: []
  type: TYPE_NORMAL
- en: You might have noticed that in the preceding example, we use `.embed()` for
    embeddings. Usually, when `DocumentArray` has `.tensors` set, you can use this
    API for encoding documents. You can specify `.embed(..., device='cuda')` when
    working with a GPU. The device name identifier depends on the model framework
    that you are using.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding audio documents
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To encode the sound clips into vectors, we chose the **VGGish** model ([https://arxiv.org/abs/1609.09430](https://arxiv.org/abs/1609.09430))
    from Google Research. We will use the pretrained model from **torchvggish** ([https://github.com/harritaylor/torchvggish](https://github.com/harritaylor/torchvggish))
    to get the feature embeddings for audio data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The returned embeddings for each sound clip are a matrix of the size *K* x 128,
    where *K* is the number of examples in the log mel spectrogram and roughly corresponds
    to the length of audio in seconds. Therefore, each 4-second audio clip in the
    chunks is represented by four 128-dimensional vectors.
  prefs: []
  type: TYPE_NORMAL
- en: We have now learned how to encode embeddings for different modalities of documents.
    In the following section, we will show you how to search for data by using multiple
    modalities. This can be useful when trying to find data that is not easily represented
    in a single modality. For example, you might use an image search to find data
    that is textual in nature.
  prefs: []
  type: TYPE_NORMAL
- en: Cross-modal and multimodal searches
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to work with multimodal data, we can describe **cross-modal**
    and **multimodal** searches. Before that, I would like to first describe the **unimodal**
    (single-modality) search. In general, unimodal search means processing a single
    modality of data at both index and query time. For example, in an image search
    retrieval, the returned search results are also images based on the given image
    query.
  prefs: []
  type: TYPE_NORMAL
- en: 'So far, we already know how to encode document content into feature vectors
    to create embeddings. In the index, each document with the content of an image,
    text, or audio can be represented as embedding vectors and stored in an indexe.
    In the query, the query document can also be represented as an embedding, which
    can then be used to identify similar documents via some similarity scores such
    as cosine, Euclidean distance, and so on. *Figure 5.6* illustrates the unified
    matching view of the search problem:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – An illustration of the unified matching view for the search
    problem ](img/Figure_5.06_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.6 – An illustration of the unified matching view for the search problem
  prefs: []
  type: TYPE_NORMAL
- en: 'More formally, searching can be considered as constructing a matching model
    that calculates the matching degree between input query documents and documents
    in the search. With this unified matching view, the matching models for **unimodal**,
    **multimodal**, and **cross-modal** searches bear even more resemblance to each
    other in terms of architecture and methodology, as reflected in the techniques:
    embedding the inputs (queries and documents) as distributed representations, combining
    neural network components to represent the relations between data of different
    modalities, and training the model parameters in an end-to-end manner.'
  prefs: []
  type: TYPE_NORMAL
- en: Cross-modal search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a unimodal search, the search is designed to deal with a single data type,
    making it less flexible and more fragile regarding the input of different data
    types. Beyond unimodal search, **cross-modal search** aims to take one type of
    data as the query to retrieve relevant data of another type, such as image-text,
    video-text, and audio-text cross-modal searches. For example, as shown in *Figure
    5.7*, we can devise a text-to-image search system that retrieves images based
    on short text descriptions as queries:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – A cross-modal search system to look for images from captions
    ](img/Figure_5.07_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.7 – A cross-modal search system to look for images from captions
  prefs: []
  type: TYPE_NORMAL
- en: More recently, cross-modal search has attracted considerable attention due to
    the rapid growth of multimodal data. As multimodal data grows, it becomes difficult
    for users to search for information of interest effectively and efficiently. So
    far, there have been various search methods proposed for searching multimodal
    data. However, these search techniques are mostly single modality-based, which
    converts cross-modal search into keyword-based search. This can be expensive because
    you need a person to write those keywords, and also, information about multimodal
    content is not always available. We need to look for another solution! **C****ross-modal**
    search aims to identify relevant data across different modalities. The main challenge
    in cross-modal search is how to measure the content similarity between different
    modalities of data. Various methods have been proposed to deal with such a problem.
    One common way is to generate feature vectors from different modalities in the
    same latent space, such that newly generated features can be applied in the computation
    of distance metrics.
  prefs: []
  type: TYPE_NORMAL
- en: To achieve this goal, usually, two very common architectures for deep metric
    learning (Siamese and triplet networks) can be utilized. They both share the idea
    that different subnetworks (which may or may not share weights) receive different
    inputs at the same time (positive and negative pairs for Siamese networks, and
    positive, negative, and anchor documents for triplets), and try to project their
    own feature vectors onto a common latent space where the contrastive loss is computed
    and its error propagated to all the subnetworks.
  prefs: []
  type: TYPE_NORMAL
- en: Positive pairs are pairs of objects (images, text, or any document) that are
    semantically related and expected to remain close in the projection space. On
    the other hand, negative pairs are pairs of documents that should be apart.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – The schema of the deep metric learning process with a triplet
    network and anchor ](img/Figure_5.08_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.8 – The schema of the deep metric learning process with a triplet network
    and anchor
  prefs: []
  type: TYPE_NORMAL
- en: As shown in *Figure 5.8*, an example of a cross-modal search between an image
    and text, the subnetwork used to extract image features is a **ResNet50** architecture
    with weights pretrained on ImageNet, while for the text embedding, the output
    of a hidden layer from a pretrained **Bert** model is used. And recently, a new
    deep metric learning pretrained model, **Contrastive Language-Image Pretraining**
    (**CLIP**), was proposed, which is a neural network trained on a variety of image-text
    pairs. It is trained to learn visual concepts from natural language with the help
    of text snippets and image pairs from the internet. It can perform zero-shot learning
    by encoding text labels and images in the same semantic space and creating a standard
    embedding for both modalities. With the CLIP-style model, both images and query
    texts can be mapped into the same latent space, so that they can be compared using
    a similarity measure.
  prefs: []
  type: TYPE_NORMAL
- en: Multimodal search
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compared to unimodal and cross-modal searches, **multimodal search** aims to
    enable multimodal data as the query input. The search queries can be composed
    of a combination of text input, image input, and other modalities of input. It
    is intuitive to combine different modalities of information for improving search
    performance. Imagine an e-commerce search scenario that takes two types of query
    information: an image and a text. For example, if you were searching for pants,
    the image would be a picture of pants, and the text would be something like “tight”
    and “blue.” In this case, the search query is composed of two modalities (text
    and image). We can refer to this search scenario as a multimodal search.'
  prefs: []
  type: TYPE_NORMAL
- en: 'To allow for a multimodal search, two approaches are widely used in practice
    to fuse multiple modalities in the search: early fusion (which fuses features
    from multiple modalities as query input) and late fusion (which fuses the search
    results from different modalities at the very end).'
  prefs: []
  type: TYPE_NORMAL
- en: Specifically, the early-fusion method fuses the features extracted from data
    of different modalities. As shown in *Figure 5.9*, features of two different modalities
    (image and text) resulting from different models are fed into a fusion operator.
    To combine features simply, we can use the feature concatenation as the fusion
    operator to produce the feature for multimodal data. Another fusion option involves
    the projection of different modalities into a common embedding space. And then,
    we can directly add the features from the data of different modalities.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Early fusion, the fusion of multimodal features as the query
    input ](img/Figure_5.09_B17488.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Early fusion, the fusion of multimodal features as the query input
  prefs: []
  type: TYPE_NORMAL
- en: 'After combining the features, we can use the same method devised for unimodal
    search to resolve the multimodal search problem. This feature fusion approach
    suffers from one significant limitation: it would be hard to define the right
    balance between the importance of various modalities in the context of a user
    query. To overcome this limitation, an end-to-end neural network can be trained
    to model the joint multimodal space. However, modeling this joint multimodal space
    requires a complex training strategy and thoroughly annotated datasets.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In practice, to address the abovementioned shortcomings, we can simply use
    the late-fusion approach to separate search per modality, and then fuse the search
    results from different modalities, for example, with a linear combination of the
    retrieval scores of all modalities per document. While late fusion has been proven
    to be robust, it has a few issues: appropriate weights of modalities are not a
    trivial problem, and there is a primary modality issue. For example, in a text-image
    multimodal search, when the results are assessed by the user based on visual similarity
    only, the influence of textual scores may worsen the visual quality of the end
    results.'
  prefs: []
  type: TYPE_NORMAL
- en: The main difference between these two search modes is that for cross-modal,
    there is a direct mapping between a single document and a vector in embedding
    space, while for multimodal, this does not hold true, since two or more documents
    might be combined into a single vector.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter describes the concept of multimodal data, and cross-modal and multimodal
    search problems. First, we introduced multimodal data and how to represent it
    in Jina. Then, we learned how to use a deep neural network to get the vector features
    from data of different modalities. Finally, we introduced cross-modal and multimodal
    search systems. This unlocks a lot of powerful search patterns and makes it easier
    to understand how to implement cross-modal and multimodal search applications
    with Jina.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will introduce some basic practical examples to explain
    how to use Jina to implement search applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: How to Use Jina for Neural Search'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this part, you will use all the knowledge learned so far and you will see
    step-by-step guides on how to build a search system for different modalities,
    either for text, images, audio, or cross- and multi-modality. The following chapters
    are included in this part:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 6*](B17488_06.xhtml#_idTextAnchor085), *Basic Practical Examples
    with Jina*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 7*](B17488_07.xhtml#_idTextAnchor101), *Exploring Advanced Use Cases
    of Jina*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
