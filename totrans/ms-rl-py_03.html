<html><head></head><body>
		<div id="_idContainer122">
			<h1 id="_idParaDest-39"><em class="italic"><a id="_idTextAnchor038"/>Chapter 2</em>: Multi-Armed Bandits</h1>
			<p>When you log on to your favorite social media app, chances are that you'll see one of the many versions of the app that are tested at that time. When you visit a website, the ads displayed to you are tailored to your profile. In many online shopping platforms, the prices are determined dynamically. Do you know what all of these have in common? They are often modeled as <strong class="bold">multi-armed bandit</strong> (<strong class="bold">MAB</strong>) problems to identify optimal decisions. A MAB problem is a form of <strong class="bold">reinforcement</strong> <strong class="bold">learning</strong> (<strong class="bold">RL</strong>), where the agent makes decisions in a problem horizon that consists of a single step. Therefore, the goal is to maximize only the immediate reward, and there are no consequences considered for any subsequent steps. While this is a simplification over multi-step RL, the agent must still deal with a fundamental trade-off of RL: the exploration of new actions that could possibly lead to higher rewards versus exploitation of the actions that are known to be decent. A wide range of business problems, such as the ones mentioned previously, involve optimizing this exploration-exploitation trade-off. Throughout the next two chapters, you will understand the implications of this trade-off – which will be a recurring theme in almost all RL methods – and learn how to effectively address it. </p>
			<p>In this chapter, we lay the groundwork by solving MAB problems that don't take into account the "context" in which the actions are taken, such as the profile of the user visiting the website/app of interest, the time of day, and so on. To that end, we cover four fundamental exploration strategies. In the next chapter, we are going the extend these strategies to solve <strong class="bold">contextual MABs</strong>. In both chapters, we use online advertising, an important application of bandit problems, as our running case study.</p>
			<p>So, let's get started! Here is what we will specifically cover in this chapter:</p>
			<ul>
				<li>Exploration-exploitation trade-off</li>
				<li>What is a MAB?</li>
				<li>Case study – online advertising</li>
				<li>A/B/n testing as an exploration strategy</li>
				<li><img src="image/Formula_02_000.png" alt=""/>-greedy actions for exploration</li>
				<li>Action selection using upper confidence bounds</li>
				<li>Thompson (posterior) sampling</li>
			</ul>
			<h1 id="_idParaDest-40"><a id="_idTextAnchor039"/>Exploration-exploitation trade-off</h1>
			<p>As <a id="_idIndexMarker084"/>we mentioned earlier, RL is all about learning from experience without a supervisor labeling correct actions for the agent. The agent observes the consequences of its actions, identifies what actions are leading to the highest rewards in each situation, and learns from this experience. Now, think about something you have learned from your own experience – for example, how to study for a test. Chances are you explored different methods until you discovered what works best for you. Maybe you studied regularly for your tests first, but then you tested whether studying the last night before the test could work well enough – and maybe it does for certain types of tests. The point is that you had to <strong class="bold">explore</strong> to find the method(s) that maximizes your "reward," which is a function of your test score, time spent on leisure activities, your anxiety levels before and during the test, and so on. In fact, exploration is essential for any learning that is based on experience. Otherwise, we may never discover better ways of doing things or a way that works at all! On the other hand, we cannot always be trying new ways. It would be silly to not exploit what we have already learned! So, there is a <em class="italic">trade-off between exploration and exploitation</em>, and this trade-off is at the very center of RL. It is crucial to balance this trade-off for efficient learning.</p>
			<p>If the exploration-exploitation trade-off is a challenge across all RL problems, why do we specifically bring it up in the context of MAB? This is for two main reasons:</p>
			<ul>
				<li>MAB is one-step RL. Therefore, it allows us to study various exploration strategies in isolation from the complexities of multi-step RL, and potentially prove how good they are theoretically.</li>
				<li>While in multi-step RL we often train the agent offline (and in a simulation) and use its policy online, in MAB problems, the agent is often trained and used (almost always) online. Therefore, inefficient exploration costs more than just computer time: it actually burns real money through bad actions. Therefore, it becomes absolutely crucial to balance exploration and exploitation effectively in MAB problems.</li>
			</ul>
			<p>With this in mind, now it is time to define what a MAB problem is, and then see an example. </p>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor040"/>What is a MAB?</h1>
			<p>A MAB problem is all <a id="_idIndexMarker085"/>about identifying the best action<a id="_idIndexMarker086"/> among a set of actions available to an agent through trial and error, such as figuring out the best look for a website among some alternatives, or the best ad banner to run for a product. We will focus on the more common variant of MABs where there are <img src="image/Formula_02_001.png" alt=""/> discrete <a id="_idIndexMarker087"/>actions available to the agent, also known as a <img src="image/Formula_02_002.png" alt=""/><strong class="bold">-armed bandit problem</strong>. </p>
			<p>Let's define the problem in more detail through the example it got its name from.</p>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor041"/>Problem definition</h2>
			<p>The MAB<a id="_idIndexMarker088"/> problem is named after the case of a gambler who needs to choose a slot machine (bandit) to play from a row of machines: </p>
			<ul>
				<li>When the lever of a machine is pulled, it gives a random reward coming from a probability distribution specific to that machine. </li>
				<li>Although the machines look identical, their reward probability distributions are different. </li>
			</ul>
			<p>The gambler is trying to maximize their total reward. So, in each turn, they need to decide whether to play the machine that has given the highest average reward so far, or to try another machine. Initially, the gambler has no knowledge of the machines' reward distributions.</p>
			<p>Clearly, the gambler needs to find a balance between exploiting the one that has been the best so far and exploring the alternatives. Why is that needed? Well, because the rewards are stochastic. A machine that won't give the highest average reward in the long term may have looked like the best just by chance!</p>
			<div>
				<div id="_idContainer019" class="IMG---Figure">
					<img src="image/B14160_02_01.jpg" alt="Figure 2.1 – MAB problems involve identifying the best lever to pull among multiple options&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.1 – MAB problems involve identifying the best lever to pull among multiple options</p>
			<p>So, to summarize what a <a id="_idIndexMarker089"/>MAB problem looks like, we can state the following:</p>
			<ul>
				<li>The agent takes sequential actions. After each action, a reward is received.</li>
				<li>An action affects only the immediate reward, not the subsequent rewards.</li>
				<li>There is no "state" in the system that changes with the actions that the agent takes.</li>
				<li>There is no input that the agent uses to base its decisions on. That will come later in the next chapter when we discuss contextual bandits.</li>
			</ul>
			<p>So far, so good! Let's better understand this by actually coding an example.</p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor042"/>Experimenting with a simple MAB problem</h2>
			<p>In this section, you will <a id="_idIndexMarker090"/>experience through an example how tricky it could be to solve even a simple MAB problem. We will create some virtual slot machines and try to maximize the total reward by identifying the luckiest machine. This code is available in <strong class="source-inline">Chapter02/Multi-armed bandits.ipynb</strong> on the GitHub repo.</p>
			<h3>Setting up the virtual environment</h3>
			<p>Before we start, we <a id="_idIndexMarker091"/>suggest you create a virtual environment for the exercise using <strong class="source-inline">virtualenv</strong> or using Conda commands. In a folder that you would like to place the virtual environment files in, execute the following commands in your terminal:</p>
			<p class="source-code">virtualenv rlenv</p>
			<p class="source-code">source rlenv/bin/activate</p>
			<p class="source-code">pip install pandas==0.25.3</p>
			<p class="source-code">pip install plotly==4.10.0</p>
			<p class="source-code">pip install cufflinks==0.17.3</p>
			<p class="source-code">pip install jupyter</p>
			<p class="source-code">ipython kernel install --name «rlenv» –user</p>
			<p class="source-code">jupyter notebook</p>
			<p>This will open a browser tab with a Jupyter notebook. Find the <strong class="source-inline">.ipynb</strong> file you get from the repo, open it, and set your kernel to be the <strong class="source-inline">rlenv</strong> environment we just created.</p>
			<h3>The bandit exercise</h3>
			<p>Let's get<a id="_idIndexMarker092"/> started with the exercise: </p>
			<ol>
				<li>First, let's create a class for a single slot machine that gives a reward from a normal (Gaussian) distribution with respect to a given mean and standard deviation:<p class="source-code">import numpy as np # Class for a single slot machine. Rewards are Gaussian.class GaussianBandit(object):    def __init__(self, mean=0, stdev=1):        self.mean = mean         self.stdev = stdev         def pull_lever(self):        reward = np.random.normal(self.mean, self.stdev)        return np.round(reward, 1)</p></li>
				<li>Next, we <a id="_idIndexMarker093"/>create a class that will simulate the game:<p class="source-code">class GaussianBanditGame(object):</p><p class="source-code">    def __init__(self, bandits):</p><p class="source-code">        self.bandits = bandits</p><p class="source-code">        np.random.shuffle(self.bandits)</p><p class="source-code">        self.reset_game()</p><p class="source-code">    </p><p class="source-code">    def play(self, choice):</p><p class="source-code">        reward = self.bandits[choice - 1].pull_lever()</p><p class="source-code">        self.rewards.append(reward)</p><p class="source-code">        self.total_reward += reward</p><p class="source-code">        self.n_played += 1</p><p class="source-code">        return reward</p><p class="source-code">    </p><p class="source-code">    def user_play(self):</p><p class="source-code">        self.reset_game()</p><p class="source-code">        print("Game started. " + </p><p class="source-code">              "Enter 0 as input to end the game.")</p><p class="source-code">        while True:</p><p class="source-code">            print(f"\n -- Round {self.n_played}")</p><p class="source-code">            choice = int(input(f"Choose a machine " + </p><p class="source-code">                     f"from 1 to {len(self.bandits)}: "))</p><p class="source-code">            if choice in range(1, len(self.bandits) + 1):</p><p class="source-code">                reward = self.play(choice)</p><p class="source-code">                print(f"Machine {choice} gave " + </p><p class="source-code">                      f"a reward of {reward}.")</p><p class="source-code">                avg_rew = self.total_reward/self.n_played</p><p class="source-code">                print(f"Your average reward " +</p><p class="source-code">                      f"so far is {avg_rew}.")</p><p class="source-code">            else:</p><p class="source-code">                break</p><p class="source-code">        print("Game has ended.")</p><p class="source-code">        if self.n_played &gt; 0:</p><p class="source-code">            print(f"Total reward is {self.total_reward}" + </p><p class="source-code">                  f" after {self.n_played} round(s).")</p><p class="source-code">            avg_rew = self.total_reward/self.n_played</p><p class="source-code">            print(f"Average reward is {avg_rew}.")              </p><p class="source-code">            </p><p class="source-code">    def reset_game(self):</p><p class="source-code">        self.rewards = []</p><p class="source-code">        self.total_reward = 0</p><p class="source-code">        self.n_played = 0</p><p>A game instance receives a list of slot machines as inputs. It then shuffles the order of the slot machines so that you won't recognize which machine gives the highest average reward. In each step, you will choose a machine and aim to get the highest reward.</p></li>
				<li>Then, we create <a id="_idIndexMarker094"/>some slot machines and a game instance:<p class="source-code">slotA = GaussianBandit(5, 3)slotB = GaussianBandit(6, 2)slotC = GaussianBandit(1, 5)game = GaussianBanditGame([slotA, slotB, slotC])</p></li>
				<li>Now, start playing the game by calling the <strong class="source-inline">user_play()</strong> method of the game object:<p class="source-code">game.user_play()</p><p>The output will look like the following:</p><p class="source-code"><strong class="bold">Game started. Enter 0 as input to end the game. </strong></p><p class="source-code"><strong class="bold">-- Round 0</strong></p><p class="source-code"><strong class="bold">Choose a machine from 1 to 3:</strong></p></li>
				<li>As you enter your choice, you will observe the reward you got in that round. We don't know anything about the machines, so let's start with 1:<p class="source-code"><strong class="bold">Choose a machine from 1 to 3: 1</strong></p><p class="source-code"><strong class="bold">Machine 1 gave a reward of 8.4.</strong></p><p class="source-code"><strong class="bold">Your average reward so far is 8.4.</strong></p><p>It looks like we started pretty well! You might think that this reward is closest to what we could expect from the <strong class="source-inline">slotB</strong> machine, so there is no reason to try something else and lose money! </p></li>
				<li>Let's play the same machine for a couple more rounds:<p class="source-code">-- Round 1</p><p class="source-code">Choose a machine from 1 to 3: 1</p><p class="source-code">Machine 1 gave a reward of 4.9.</p><p class="source-code">Your average reward so far is 6.65.</p><p class="source-code"> -- Round 2</p><p class="source-code">Choose a machine from 1 to 3: 1</p><p class="source-code">Machine 1 gave a reward of -2.8.</p><p class="source-code">Your average reward so far is 3.5.</p><p>Snap! This in fact looks like the worst machine! It is very unlikely for <strong class="source-inline">slotA</strong> or <strong class="source-inline">slotB</strong> machines to give a reward of <strong class="source-inline">-2.8</strong>. </p></li>
				<li>Let's check what we<a id="_idIndexMarker095"/> have as the first machine in the game (remember that the first machine would correspond to index 0 in the <strong class="source-inline">bandits</strong> list) by looking at its mean value parameter. Executing <strong class="source-inline">game.bandits[0].mean</strong> gives us <strong class="source-inline">1</strong> as the output!</li>
			</ol>
			<p>Indeed, we thought we had chosen the best machine although it was the worst! Why did that happen though? Well, again, the rewards are stochastic. Depending on the variance of the reward distribution, a particular reward could be wildly different from the average reward we could expect from that machine. For this reason, it is not quite possible to know which lever to pull before we experience enough rounds of the game. In fact, with only a few samples, our observations could be quite misleading as just happened. In addition, if you play the game yourself, you will realize that it is quite difficult to differentiate between <strong class="source-inline">slotA</strong> and <strong class="source-inline">slotB</strong>, because their reward distributions are similar. You might be thinking, "is this a big deal?". Well, it kind of is, if the difference corresponds to significant money and resources, as is the case in many real-world applications. </p>
			<p>Next, we will introduce such an application, online advertising, which is going to be our running example throughout this chapter and the next. </p>
			<h1 id="_idParaDest-44"><a id="_idTextAnchor043"/>Case study – online advertising</h1>
			<p>Consider a company<a id="_idIndexMarker096"/> that wants to advertise a product on various websites through digital banners, aiming to attract visitors to the product landing page. Among multiple alternatives, the advertiser company wants to find out which banner is the most effective and has the maximum <strong class="bold">click-through rate</strong> (<strong class="bold">CTR</strong>), which is defined as the total<a id="_idIndexMarker097"/> number of clicks an ad receives divided by the total number of impressions (number of times it is shown). </p>
			<p>Every time a banner is about to be shown on a website, it is the advertiser's algorithm that chooses the banner (for example, through an API provided by the advertiser to the website) and observes whether the impression has resulted in a click or not. This is a great use case for a MAB model, which could boost clicks and product sales. What we want the MAB model to do is to identify the ad that performs the best as early as possible, display it more, and write off the ad(s) that is (are) a clear loser(s) as early as possible.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">The probability of observing a click or no click after an impression, a binary outcome, can be modeled using the Bernoulli distribution. It has a single parameter, <img src="image/Formula_02_003.png" alt=""/>, which is the probability of receiving a click, or more generally, observing a 1 as opposed to a 0. Note that this is a discrete probability distribution, whereas the normal distribution we used earlier is a continuous one.</p>
			<p>In the previous example, we had rewards coming from a normal distribution. In the online ad case, we have a binary outcome. For each ad version, there is a different probability of click (CTR), which the advertiser does not know but is trying to discover. So, the rewards will come from different Bernoulli distributions for each ad. Let's code these to use with our algorithms later:</p>
			<ol>
				<li value="1">We start by creating a class to model the ad behavior:<p class="source-code">class BernoulliBandit(object):</p><p class="source-code">    def __init__(self, p):</p><p class="source-code">        self.p = p</p><p class="source-code">    def display_ad(self):</p><p class="source-code">        reward = np.random.binomial(n=1, p=self.p)</p><p class="source-code">        return reward</p></li>
				<li>Now, let's<a id="_idIndexMarker098"/> create five different ads (banners) with the corresponding CTRs we arbitrarily pick:<p class="source-code">adA = BernoulliBandit(0.004)</p><p class="source-code">adB = BernoulliBandit(0.016)</p><p class="source-code">adC = BernoulliBandit(0.02)</p><p class="source-code">adD = BernoulliBandit(0.028)</p><p class="source-code">adE = BernoulliBandit(0.031)</p><p class="source-code">ads = [adA, adB, adC, adD, adE]</p></li>
			</ol>
			<p>So far, so good. Now, it is time to implement some exploration strategies to maximize the CTR of the ad campaign!</p>
			<h1 id="_idParaDest-45"><a id="_idTextAnchor044"/>A/B/n testing</h1>
			<p>One of the most common <a id="_idIndexMarker099"/>exploration strategies is what is called <strong class="bold">A/B testing</strong>, which is a method to determine which one of the two alternatives (of online products, pages, ads, and so on) performs better. In this type of testing, the users are randomly split into two groups to try different alternatives. At the end of the testing period, the results are compared to choose the best alternative, which is then used in production for the rest of the problem horizon. In our case, we have more than two ad versions. So, we will implement<a id="_idIndexMarker100"/> what is called <strong class="bold">A/B/n testing</strong>. </p>
			<p>We will use A/B/n testing as our baseline strategy for comparison with the more advanced methods that we will introduce afterward. Before going into the implementation, we need to define some notation that we will use throughout the chapter.</p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor045"/>Notation</h2>
			<p>Throughout the implementations of <a id="_idIndexMarker101"/>various algorithms, we will need to keep track of some quantities related to a particular action (the ad chosen for display), <img src="image/Formula_02_004.png" alt=""/>. Now, we define some notation for those quantities. Initially, we drop <img src="image/Formula_02_0041.png" alt=""/> from our notation for brevity, but at the end of this section, we will put it back:</p>
			<ul>
				<li>First, we denote the reward (that is, 1 for a click, 0 for no click) received after selecting the action, <img src="image/Formula_02_0042.png" alt=""/>, for the <img src="image/Formula_02_007.png" alt=""/> time by <img src="image/Formula_02_008.png" alt=""/>. </li>
				<li>The average reward observed before the <img src="image/Formula_02_009.png" alt=""/> selection of this same action is defined as follows:</li>
			</ul>
			<div>
				<div id="_idContainer027" class="IMG---Figure">
					<img src="image/Formula_02_010.jpg" alt=""/>
				</div>
			</div>
			<p>This estimates the expected value of the reward that this action yields, <img src="image/Formula_02_011.png" alt=""/>, after <img src="image/Formula_02_012.png" alt=""/> observations. </p>
			<ul>
				<li>This is <a id="_idIndexMarker102"/>also called <a id="_idIndexMarker103"/>the <strong class="bold">action value</strong> of <img src="image/Formula_02_013.png" alt=""/>. Here, this is <img src="image/Formula_02_014.png" alt=""/> estimates of the action value after selecting this action <img src="image/Formula_02_0121.png" alt=""/> times.</li>
				<li>Now, we need a bit of simple algebra and we will have a very convenient formula to update the action values:<div id="_idContainer033" class="IMG---Figure"><img src="image/Formula_02_016.jpg" alt=""/></div></li>
			</ul>
			<p class="figure-caption">
 
</p>
			<div>
				<div id="_idContainer034" class="IMG---Figure">
					<img src="image/Formula_02_017.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer035" class="IMG---Figure">
					<img src="image/Formula_02_018.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer036" class="IMG---Figure">
					<img src="image/Formula_02_019.jpg" alt=""/>
				</div>
			</div>
			<div>
				<div id="_idContainer037" class="IMG---Figure">
					<img src="image/Formula_02_020.jpg" alt=""/>
				</div>
			</div>
			<p class="figure-caption">
</p>
			<div>
				<div id="_idContainer038" class="IMG---Figure">
					<img src="image/Formula_02_021.jpg" alt=""/>
				</div>
			</div>
			<ul>
				<li>Remember that <img src="image/Formula_02_014.png" alt=""/> is our estimate for the action value of <img src="image/Formula_02_0131.png" alt=""/> before we take it for the <img src="image/Formula_02_0091.png" alt=""/> time. When <a id="_idIndexMarker104"/>we observe the reward, <img src="image/Formula_02_025.png" alt=""/>, it gives us another signal for the action value. We don't want to discard our previous observations, but we also want to update our estimate to reflect the new signal. <p>So, we adjust our current estimate, <img src="image/Formula_02_0141.png" alt=""/>, in the direction of the <strong class="bold">error</strong> that we calculate based on the latest observed reward, <img src="image/Formula_02_027.png" alt=""/>, with a <strong class="bold">step size</strong> of <img src="image/Formula_02_028.png" alt=""/> and obtain a new estimate, <img src="image/Formula_02_029.png" alt=""/>. This means, for example, if the latest observed reward is greater than our current estimate, we revise the action value estimate upward.</p></li>
				<li>For convenience, we define <img src="image/Formula_02_030.png" alt=""/>.</li>
				<li>Notice that the rate at which we adjust our estimate will get smaller as we make more observations due to the <img src="image/Formula_02_0281.png" alt=""/> term. So, we put less weight on the most recent observations and our estimate for the action value for a particular action will settle down over time.</li>
				<li>However, this might be a<a id="_idIndexMarker105"/> disadvantage if the environment is not stationary but is changing over time. In those cases, we would want to use a step size that does not diminish over time, such as a fixed step size of <img src="image/Formula_02_032.png" alt=""/>.</li>
				<li>Note that this step size must be smaller than 1 for the estimate to converge (and larger than 0 for a proper update).</li>
				<li>Using a fixed value for <img src="image/Formula_02_033.png" alt=""/> will make the weights of the older observations decrease exponentially as we take action <img src="image/Formula_02_0331.png" alt=""/> more and more.</li>
			</ul>
			<p>Let's bring <img src="image/Formula_02_033.png" alt=""/> back to the notation, so we can obtain our formula to update the action values:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/Formula_02_036.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_02_0332.png" alt=""/> is a number between 0 and 1. For stationary problems, we usually set  <img src="image/Formula_02_038.png" alt=""/>, where <img src="image/Formula_02_039.png" alt=""/> is the number of times the action <img src="image/Formula_02_0132.png" alt=""/> has been taken up to that point (which was denoted by <img src="image/Formula_02_041.png" alt=""/> initially). In stationary problems, this will help action values converge quicker, due to the diminishing <img src="image/Formula_02_042.png" alt=""/> term, rather than chasing after noisy observations.</p>
			<p>That's all we need. Without further ado, let's implement an A/B/n test.</p>
			<h2 id="_idParaDest-47"><a id="_idTextAnchor046"/>Application to the online advertising scenario</h2>
			<p>In our example, we have<a id="_idIndexMarker106"/> five different ad versions, which we randomly show to the users with equal probabilities. Let's implement this in Python:</p>
			<ol>
				<li value="1">We start with creating the variables to keep track of the rewards in the experiment:<p class="source-code">n_test = 10000</p><p class="source-code">n_prod = 90000</p><p class="source-code">n_ads = len(ads)</p><p class="source-code">Q = np.zeros(n_ads)  # Q, action values</p><p class="source-code">N = np.zeros(n_ads)  # N, total impressions</p><p class="source-code">total_reward = 0</p><p class="source-code">avg_rewards = []  # Save average rewards over time</p></li>
				<li>Now, let's run the A/B/n test:<p class="source-code">for i in range(n_test):</p><p class="source-code">    ad_chosen = np.random.randint(n_ads)</p><p class="source-code">    R = ads[ad_chosen].display_ad() # Observe reward</p><p class="source-code">    N[ad_chosen] += 1</p><p class="source-code">    Q[ad_chosen] += (1 / N[ad_chosen]) * (R - Q[ad_chosen])</p><p class="source-code">    total_reward += R</p><p class="source-code">    avg_reward_so_far = total_reward / (i + 1)</p><p class="source-code">    avg_rewards.append(avg_reward_so_far)</p><p>Remember that we randomly select an ad to display during the test and observe whether it gets a click. We update the counter, the action value estimate, and the average reward observed so far.</p></li>
				<li>At the end of the test <a id="_idIndexMarker107"/>period, we choose the winner as the ad that has achieved the highest action value:<p class="source-code">best_ad_index = np.argmax(Q)</p></li>
				<li>We display the winner using a <strong class="source-inline">print</strong> statement:<p class="source-code">print("The best performing ad is {}".format(chr(ord('A') + best_ad_index)))</p></li>
				<li>The outcome is the following:<p class="source-code"><strong class="bold">The best performing ad is D.</strong></p><p>In this case, the A/B/n test has identified D as the best performing ad, which is not exactly correct. Apparently, the test period was not long enough. </p></li>
				<li>Let's run the best ad identified in the A/B/n test in production:<p class="source-code">ad_chosen = best_ad_index</p><p class="source-code">for i in range(n_prod):</p><p class="source-code">    R = ads[ad_chosen].display_ad()</p><p class="source-code">    total_reward += R</p><p class="source-code">    avg_reward_so_far = total_reward / (n_test + i + 1)</p><p class="source-code">    avg_rewards.append(avg_reward_so_far)</p><p>At this stage, we don't explore any other actions. So, the incorrect selection of the ad D will have its impact throughout the production period. We continue to record the average reward observed so far to later visualize the ad campaign performance.</p><p>Now, time to <a id="_idIndexMarker108"/>visualize the results:</p></li>
				<li>Let's create a <strong class="source-inline">pandas</strong> DataFrame to record the results from the A/B/n test:<p class="source-code">import pandas as pd</p><p class="source-code">df_reward_comparison = pd.DataFrame(avg_rewards, columns=['A/B/n'])</p></li>
				<li>To display the progress of the average rewards, we use Plotly with Cufflinks:<p class="source-code">import cufflinks as cf</p><p class="source-code">import plotly.offline</p><p class="source-code">cf.go_offline()</p><p class="source-code">cf.set_config_file(world_readable=True, theme="white")</p><p class="source-code">df_reward_comparison['A/B/n'].iplot(title="A/B/n Test Avg. Reward: {:.4f}"</p><p class="source-code">                                   .format(avg_reward_so_far),</p><p class="source-code">                                    xTitle='Impressions', </p><p class="source-code">                                    yTitle='Avg. Reward')</p><p>This results in the following output:</p></li>
			</ol>
			<div>
				<div id="_idContainer060" class="IMG---Figure">
					<img src="image/B14160_02_02.jpg" alt="Figure 2.2 – A/B/n test rewards&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.2 – A/B/n test rewards</p>
			<p>You can see from <em class="italic">Figure 2.2</em> that after the exploration ends, the average reward is approaching 2.8%, which<a id="_idIndexMarker109"/> is the expected CTR for the ad D. On the other hand, due to the exploration during the first 10k impressions, in which we tried several bad alternatives, the CTR after 100k impressions ended up being 2.71%. We could have achieved a higher CTR if the A/B/n test had identified ad E as the best alternative.</p>
			<p>That's it! We have just implemented an A/B/n test. Overall, the test was able to identify one of the best ads for us, although not the best. Next, we discuss the pros and cons of A/B/n testing.</p>
			<h2 id="_idParaDest-48"><a id="_idTextAnchor047"/>Advantages and disadvantages of A/B/n testing</h2>
			<p>Now, let's <a id="_idIndexMarker110"/>qualitatively evaluate this method and discuss its shortcomings:</p>
			<ul>
				<li><strong class="bold">A/B/n testing is inefficient as it does not modify the experiment dynamically by learning from the observations</strong>. Instead, it explores in a fixed time budget with pre-determined probabilities of trying the alternatives. It fails to benefit from the early observations in the test by writing off/promoting an alternative even though it is obviously underperforming/outperforming the others. </li>
				<li><strong class="bold">It is unable to correct a decision once it's made</strong>. If, for some reason, the test period identifies an alternative as the best incorrectly (mostly because of a not-sufficiently long test duration), this selection remains fixed during the production period. So, there is no way to correct the decision for the rest of the deployment horizon.</li>
				<li><strong class="bold">It is unable to adapt to changes in a dynamic environment</strong>. Related to the previous note, this approach<a id="_idIndexMarker111"/> is especially problematic for environments that are not stationary. So, if the underlying reward distributions change over time, plain A/B/n testing has no way of detecting such changes after the selection is fixed.</li>
				<li><strong class="bold">The length of the test period is a hyperparameter to tune, affecting the efficiency of the test</strong>. If this period is chosen to be shorter than needed, an incorrect alternative could be declared the best because of the noise in the observations. If the test period is chosen to be too long, too much money gets wasted in exploration.</li>
				<li><strong class="bold">A/B/n testing is simple</strong>. Despite all these shortcomings, it is intuitive and easy to implement, and therefore widely used in practice.</li>
			</ul>
			<p>So, the vanilla A/B/n testing is a rather naive approach to MAB. Next, let's look into some other more advanced approaches that will overcome some of the shortcomings of A/B/n testing, starting with ε-greedy.</p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor048"/>ε-greedy actions</h1>
			<p>An easy-to-implement, effective, and widely used approach to the exploration-exploitation problem<a id="_idIndexMarker112"/> is what is called <strong class="bold">ε-greedy</strong> actions. This approach suggests, most of the time, greedily taking the action that is the best according to the rewards observed by that point in the experiment (that is, with 1-ε probability); but once in a while (that is, with ε probability), take a random action regardless of the action performances. Here, ε is a number between 0 and 1, usually closer to zero (for example, 0.1) to "exploit" in most decisions. This way, the method allows continuous exploration of the alternative actions throughout the experiment.</p>
			<h2 id="_idParaDest-50"><a id="_idTextAnchor049"/>Application to the online advertising scenario</h2>
			<p>Now, let's<a id="_idIndexMarker113"/> implement the ε-greedy actions to the online advertising scenario that we have:</p>
			<ol>
				<li value="1">We start with initializing the necessary variables for the experiment, which will keep track of the action value estimates, the number of times each ad has been displayed, and the moving average for the reward:<p class="source-code">eps = 0.1</p><p class="source-code">n_prod = 100000</p><p class="source-code">n_ads = len(ads)</p><p class="source-code">Q = np.zeros(n_ads)</p><p class="source-code">N = np.zeros(n_ads)</p><p class="source-code">total_reward = 0</p><p class="source-code">avg_rewards = []</p><p>Note that we choose 0.1 for ε, but this is a somewhat arbitrary choice. Different ε values will lead to different performances, so this should be treated as a hyperparameter to be tuned. A more sophisticated approach would be to start with a high ε value and gradually reduce it. We'll talk about this a bit more later.</p></li>
				<li>Next, we run the experiment. Pay attention to how we select a random action with ε probability, and the best action otherwise. We update our action value estimates according<a id="_idIndexMarker114"/> to the rule we previously described:<p class="source-code">ad_chosen = np.random.randint(n_ads)</p><p class="source-code">for i in range(n_prod):</p><p class="source-code">    R = ads[ad_chosen].display_ad()</p><p class="source-code">    N[ad_chosen] += 1</p><p class="source-code">    Q[ad_chosen] += (1 / N[ad_chosen]) * (R - Q[ad_chosen])</p><p class="source-code">    total_reward += R</p><p class="source-code">    avg_reward_so_far = total_reward / (i + 1)</p><p class="source-code">    avg_rewards.append(avg_reward_so_far)</p><p class="source-code">    # Select the next ad to display</p><p class="source-code">    if np.random.uniform() &lt;= eps:</p><p class="source-code">        ad_chosen = np.random.randint(n_ads)</p><p class="source-code">    else:</p><p class="source-code">        ad_chosen = np.argmax(Q)</p><p class="source-code">df_reward_comparison['e-greedy: {}'.format(eps)] = avg_rewards</p></li>
				<li>Run <em class="italic">steps 1 and 2</em> for different ε values, namely 0.01, 0.05, 0.1, and 0.2. Then, compare how the ε selection affects the performance, as follows: <p class="source-code">greedy_list = ['e-greedy: 0.01', 'e-greedy: 0.05', 'e-greedy: 0.1', 'e-greedy: 0.2']</p><p class="source-code">df_reward_comparison[greedy_list].iplot(title="ε-Greedy Actions",</p><p class="source-code"> dash=['solid', 'dash', 'dashdot', 'dot'],</p><p class="source-code"> xTitle='Impressions', </p><p class="source-code"> yTitle='Avg. Reward')</p><p>This results in the following output:</p></li>
			</ol>
			<div>
				<div id="_idContainer061" class="IMG---Figure">
					<img src="image/B14160_02_03.jpg" alt="Figure 2.3 – Exploration using ε-greedy actions&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.3 – Exploration using ε-greedy actions</p>
			<p>The best rewards are<a id="_idIndexMarker115"/> given by ε=0.05 and ε=0.1 as 2.97%. It turns out the exploration with the other two ε values were either too low or too high. In addition, all of the ε-greedy policies gave better results than the A/B/n test, particularly because the A/B/n test happened to make an incorrect choice in that specific case.</p>
			<h2 id="_idParaDest-51"><a id="_idTextAnchor050"/>Advantages and disadvantages of ε-greedy actions</h2>
			<p>Let's talk about the<a id="_idIndexMarker116"/> pros and cons of using ε-greedy actions:</p>
			<ul>
				<li><strong class="bold">ε-greedy actions and A/B/n tests are similarly inefficient and static in allocating the exploration budget</strong>. The ε-greedy approach, too, fails to write off actions that are clearly bad and continues to allocate the same exploration budget to each alternative. For example, halfway through the experiment, it is pretty clear that ad A is performing pretty poorly. It would have been more efficient to use the exploration budget to try to differentiate between the rest of the alternatives to identify the best. On a related note, if a particular action is under-explored/over-explored at any point, the exploration budget is not adjusted accordingly.</li>
				<li><strong class="bold">With ε-greedy actions, exploration is continuous, unlike in A/B/n testing</strong>. This means if the environment is not stationary, the ε-greedy approach has the potential to<a id="_idIndexMarker117"/> pick up the changes and modify its selection<a id="_idIndexMarker118"/> of the best alternative. In stationary environments, though, we can expect the A/B/n testing and the ε-greedy approach to perform similarly since they are very similar in nature, except when they do the exploration. </li>
				<li><strong class="bold">The ε-greedy actions approach could be made more efficient by dynamically changing the ε value</strong>. For example, you could start with a high ε value to explore more at the beginning and gradually decrease it to exploit more later. This way, there is still continuous exploration, but not as much as at the beginning when there was no knowledge of the environment.</li>
				<li><strong class="bold">The ε-greedy actions approach could be made more dynamic by increasing the importance of more recent observations</strong>. In the standard version, the <img src="image/Formula_02_043.png" alt=""/> values in the preceding are calculated as simple averages. Remember that, in dynamic environments, we could instead use the following formula:</li>
			</ul>
			<div>
				<div id="_idContainer063" class="IMG---Figure">
					<img src="image/Formula_02_044.jpg" alt=""/>
				</div>
			</div>
			<p>This would exponentially diminish the weights of the older observations and enable the approach to detect the changes in the environment more easily.</p>
			<ul>
				<li><strong class="bold">Modifying the ε-greedy actions approach introduces new hyperparameters, which need to be tuned</strong>. Both of the previous suggestions – gradually <a id="_idIndexMarker119"/>diminishing ε and using exponential smoothing for <em class="italic">Q</em> – come with additional hyperparameters, and it may not be obvious what values to set these to. Moreover, incorrect selection of these hyperparameters may lead to worse results than what the standard version would yield.</li>
			</ul>
			<p>So far, so good! We have used ε-greedy actions to optimize our online advertising campaign and obtained better results than A/B/n testing. We have also discussed how we can modify this approach to use in a broader set of environments. However, ε-greedy selection of the actions is still too static, and we can do better. Now, let's look into another approach, upper confidence bounds, which dynamically adjusts the exploration of the actions.</p>
			<h1 id="_idParaDest-52"><a id="_idTextAnchor051"/>Action selection using upper confidence bounds</h1>
			<p><strong class="bold">Upper confidence bounds</strong> (<strong class="bold">UCB</strong>) is a <a id="_idIndexMarker120"/>simple yet effective solution to the exploration-exploitation trade-off. The idea is that at each time step, we select the action that has the highest potential for reward. The potential of the action is calculated as the sum of the action value estimate and a measure of the uncertainty of this estimate. This sum is what we call the UCB. So, an action is selected either because our estimate for the action value is high, or the action has not been explored enough (that is, as many times as the other ones) and there is high uncertainty about its value, or both. </p>
			<p>More formally, we select the action to take at time <img src="image/Formula_02_045.png" alt=""/> using the following formula: </p>
			<div>
				<div id="_idContainer065" class="IMG---Figure">
					<img src="image/Formula_02_046.jpg" alt=""/>
				</div>
			</div>
			<p>Let's unpack this a little bit:</p>
			<ul>
				<li>Now, we have used a notation that is slightly different from what we introduced earlier. <img src="image/Formula_02_047.png" alt=""/> and <img src="image/Formula_02_048.png" alt=""/> have essentially the same meanings as before. This formula looks at the variable values, which may have been updated a while ago, at the time of decision-making, <img src="image/Formula_02_0451.png" alt=""/>, whereas the earlier formula described how to update them.</li>
				<li>In this equation, the <a id="_idIndexMarker121"/>square root term is a measure of the uncertainty for the estimate of the action value of <img src="image/Formula_02_0332.png" alt=""/>. </li>
				<li>The more we select <img src="image/Formula_02_0332.png" alt=""/>, the less uncertainty we have about its value, and so is the <img src="image/Formula_02_052.png" alt=""/> term in the denominator. </li>
				<li>As time passes, however, the uncertainty grows due to the <img src="image/Formula_02_053.png" alt=""/> term (which makes sense especially if the environment is not stationary), and more exploration is encouraged. </li>
				<li>On the other hand, the emphasis on uncertainty during decision making is controlled by a hyperparameter, <img src="image/Formula_02_054_1.png" alt=""/>. This obviously requires tuning, and a bad selection could diminish the value in the method.</li>
			</ul>
			<p>Now, it is time to see UCB in action.</p>
			<h2 id="_idParaDest-53"><a id="_idTextAnchor052"/>Application to the online advertising scenario</h2>
			<p>Follow along to <a id="_idIndexMarker122"/>implement the UCB method to optimize the ad display:  </p>
			<ol>
				<li value="1">As usual, let's initialize the necessary variables first:<p class="source-code">c = 0.1</p><p class="source-code">n_prod = 100000</p><p class="source-code">n_ads = len(ads)</p><p class="source-code">ad_indices = np.array(range(n_ads))</p><p class="source-code">Q = np.zeros(n_ads)</p><p class="source-code">N = np.zeros(n_ads)</p><p class="source-code">total_reward = 0</p><p class="source-code">avg_rewards = []</p></li>
				<li>Now, implement the main loop to use UCB for action selection:<p class="source-code">for t in range(1, n_prod + 1):</p><p class="source-code">    if any(N==0):</p><p class="source-code">        ad_chosen = np.random.choice(ad_indices[N==0])</p><p class="source-code">    else:</p><p class="source-code">        uncertainty = np.sqrt(np.log(t) / N)</p><p class="source-code">        ad_chosen = np.argmax(Q + c * uncertainty)  </p><p class="source-code">    R = ads[ad_chosen].display_ad()</p><p class="source-code">    N[ad_chosen] += 1</p><p class="source-code">    Q[ad_chosen] += (1 / N[ad_chosen]) * (R - Q[ad_chosen])</p><p class="source-code">    total_reward += R</p><p class="source-code">    avg_reward_so_far = total_reward / t</p><p class="source-code">    avg_rewards.append(avg_reward_so_far)</p><p class="source-code">df_reward_comparison['UCB, c={}'.format(c)] = avg_rewards</p><p>Note that we select the action in each time step with the highest UCB. If an action has not been selected yet, it has the highest UCB. We break the ties randomly if there are multiple such actions.</p></li>
				<li>As mentioned <a id="_idIndexMarker123"/>before, different <img src="image/Formula_02_054.png" alt=""/> selections will lead to different levels of performance. Run <em class="italic">steps 1 and 2</em> with different selections of the <img src="image/Formula_02_0541.png" alt=""/> hyperparameter. Then, compare the results, as follows:<p class="source-code">ucb_list = [‹UCB, c=0.1›, ‹UCB, c=1›, ‹UCB, c=10›]</p><p class="source-code">best_reward = df_reward_comparison.loc[t-1,ucb_list].max()</p><p class="source-code">df_reward_comparison[ucb_list].iplot(title=»Action Selection using UCB. Best avg. reward: {:.4f}»</p><p class="source-code">                                    .format(best_reward),</p><p class="source-code">                                    dash = [‹solid›, ‹dash›, ‹dashdot›],</p><p class="source-code">                                    xTitle=›Impressions›, </p><p class="source-code">                                    yTitle=›Avg. Reward›)</p><p>This results in the following output:</p></li>
			</ol>
			<div>
				<div id="_idContainer076" class="IMG---Figure">
					<img src="image/B14160_02_04.jpg" alt="Figure 2.4 – Exploration using UCB&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.4 – Exploration using UCB</p>
			<p>In this case, using UCB for exploration, after some hyperparameter tuning, gave a better result (3.07% CTR) than ε-greedy exploration and A/B/n testing! Of course, the elephant in the room is how to do this hyperparameter tuning. Interestingly, this is itself a MAB problem! First, you have to <a id="_idIndexMarker124"/>form a set of plausible <img src="image/Formula_02_0541.png" alt=""/> values and choose the best one using one of the methods we described so far.</p>
			<p class="callout-heading">Tip</p>
			<p class="callout">Try hyperparameters in a logarithmic scale, such as [0.01, 0.1, 1, 10], rather than a linear scale, such as [0.08, 0.1, 0.12, 0.14]. The former allows exploring different orders of magnitude, where we could see significant jumps in performance. A search on a linear scale could be used after identifying the right order of magnitude.</p>
			<p>To make things less complicated, you can use an A/B/n test to choose <img src="image/Formula_02_0541.png" alt=""/>. This might look like an infinite loop – you form a MAB to solve a MAB, which itself may have a hyperparameter to tune <a id="_idIndexMarker125"/>and so on. Fortunately, once you identify a good <img src="image/Formula_02_0542.png" alt=""/> value that works for your problem type (for example, online advertising), you can usually use the same value over and over again in later experiments as far as the reward scale remains similar (for example, around 1–3% CTR for online ads).</p>
			<h2 id="_idParaDest-54"><a id="_idTextAnchor053"/>Advantages and disadvantages of using UCBs</h2>
			<p>Finally, let's <a id="_idIndexMarker126"/>discuss some of the pros and cons of the UCB approach:</p>
			<ul>
				<li><strong class="bold">UCB is a set-and-forget approach</strong>. It systematically and dynamically allocates the budget to alternatives that need exploration. If there are changes in the environment – for example, if the reward structure changes because one of the <a id="_idIndexMarker127"/>ads gets more popular for some reason – the method will adapt its selection of the actions accordingly.  </li>
				<li><strong class="bold">UCB can be further optimized for dynamic environments, potentially at the expense of introducing additional hyperparameters</strong>. The formula we provided for UCB is a common one, but it can be improved – for example, by using exponential smoothing to calculate the <img src="image/Formula_02_060.png" alt=""/> values. There are also more effective estimations of the uncertainty component in literature. These modifications, though, could potentially make the method more complicated.</li>
				<li><strong class="bold">UCB could be hard to tune</strong>. It is somewhat easier to make the call and say, "I want to explore 10% of the time, and exploit for the rest" for the ε-greedy approach than saying, "I want my <img src="image/Formula_02_0543.png" alt=""/> to be 0.729" for the UCB approach, especially if you are trying these methods on a brand-new problem. When not tuned, a UCB implementation<a id="_idIndexMarker128"/> could give unexpectedly bad results.</li>
			</ul>
			<p>There you go! You have now implemented multiple approaches to the online advertising problem, and using the UCB approach has particularly equipped you to manage the exploration effectively in potentially non-stationary environments. Next, we will cover another very powerful approach, Thompson sampling, which will be a great addition to your arsenal.</p>
			<h1 id="_idParaDest-55"><a id="_idTextAnchor054"/>Thompson (posterior) sampling</h1>
			<p>The goal in MAB problems is <a id="_idIndexMarker129"/>to estimate the parameter(s) of the reward distribution for each arm (that is, the ad to display, in the preceding example). In addition, measuring our uncertainty about our estimate is a good way to guide the exploration strategy. This problem very much fits into the Bayesian inference framework, which is what Thompson sampling leverages. Bayesian inference starts with a prior probability distribution – an initial idea, for the parameter <img src="image/Formula_02_062.png" alt=""/> – and updates this prior distribution as data becomes available. Here, <img src="image/Formula_02_062.png" alt=""/> refers to the mean and variance for a normal distribution, and to the probability of observing a 1 for Bernoulli distribution. So, the Bayesian approach treats the parameter as a random variable given the data. </p>
			<p>The formula for this is given by the following:</p>
			<div>
				<div id="_idContainer084" class="IMG---Figure">
					<img src="image/Formula_02_064.jpg" alt=""/>
				</div>
			</div>
			<p>In <a id="_idIndexMarker130"/>this formula, <img src="image/Formula_02_065.png" alt=""/> is the <strong class="bold">prior distribution</strong> of <img src="image/Formula_02_062.png" alt=""/>, which represents the current hypothesis on its distribution. <img src="image/Formula_02_067.png" alt=""/> represents the data, with which we <a id="_idIndexMarker131"/>obtain a <strong class="bold">posterior distribution</strong>, <img src="image/Formula_02_068.png" alt=""/>. This is our updated hypothesis on the distribution of the <a id="_idIndexMarker132"/>parameter given the data we observe. <img src="image/Formula_02_069.png" alt=""/> is called the <strong class="bold">likelihood</strong> (of observing the data <img src="image/Formula_02_0671.png" alt=""/> given the parameter) and <img src="image/Formula_02_071.png" alt=""/> is <a id="_idIndexMarker133"/>called the <strong class="bold">evidence</strong>.</p>
			<p>Next, let's look into how we can implement Thompson sampling for cases with 0–1 type of outcome, such as what we have in the online advertising scenario.</p>
			<h2 id="_idParaDest-56"><a id="_idTextAnchor055"/>Application to the online advertising scenario</h2>
			<p>In our<a id="_idIndexMarker134"/> example, for a given ad <img src="image/Formula_02_072.png" alt=""/>, observing a click is a Bernoulli random variable with parameter <img src="image/Formula_02_073.png" alt=""/>, which we are trying to estimate. Since <img src="image/Formula_02_073.png" alt=""/> is essentially the probability that the ad <img src="image/Formula_02_075.png" alt=""/> is clicked when displayed, equivalently, the CTR is between 0 and 1. Note that many problems other than online advertising have such a binary outcome. Therefore, our discussion and the formulas here can be extended to such other cases. </p>
			<h3>Details of Thompson sampling</h3>
			<p>For now, let's see <a id="_idIndexMarker135"/>how we can use a Bayesian approach to our problem:</p>
			<ul>
				<li>Initially, we don't have any reason to believe that the parameter is high or low for a given ad. Therefore, it makes sense to assume that <img src="image/Formula_02_0731.png" alt=""/> has a uniform distribution over <img src="image/Formula_02_077.png" alt=""/>. </li>
				<li>Assume that we display the ad <img src="image/Formula_02_0721.png" alt=""/> and it results in a click. We take this as a signal to update the probability distribution for <img src="image/Formula_02_0731.png" alt=""/> so that the expected value shifts a little bit toward 1. </li>
				<li>As we collect more and more data, we should also see the variance estimate for the parameter shrink. Well, this is exactly how we want to balance exploration and exploitation. We did something similar when we used UCB: we used our estimate of a parameter together with the associated uncertainty around the estimate to guide the exploration. Thompson sampling does exactly the same, using Bayesian inference. </li>
				<li>This method tells us to take a sample from the posterior distribution of the parameter, <img src="image/Formula_02_080.png" alt=""/>. If the expected value of <img src="image/Formula_02_0732.png" alt=""/> is high, we are likely to get samples closer to 1. If the variance is high because ad <img src="image/Formula_02_0722.png" alt=""/> has not been selected many times by that point, our samples will also have high variance, which will lead to exploration. At a given time step, we take one sample for each ad and select the greatest sample to determine the ad to display.</li>
			</ul>
			<p>In our example, the <a id="_idIndexMarker136"/>likelihood (chance of an impression resulting in a click) is Bernoulli distribution, to which we will apply the logic we described previously. Here is what is really going on in less technical terms: </p>
			<ul>
				<li>We want to understand what the CTR is for each ad. We have estimates, but we are unsure about them, so we associate a probability distribution with each CTR.</li>
				<li>We update the probability distributions for CTRs as new data come in.</li>
				<li>When it is time to select an ad, we make a guess about the CTR for each ad – that is, sample <img src="image/Formula_02_083.png" alt=""/>s. We then pick the ad for which we happened to guess the highest CTR.</li>
				<li>If the probability distribution for the CTR of an ad has a high variance, it means we are very uncertain about it. This will cause us to make wild guesses about that particular ad and select it more often, until the variance reduces – that is, we become more certain about it.</li>
			</ul>
			<p>Now, let's talk about the update rules for the Bernoulli distribution. It is okay if you don't fully grasp the terms here. The preceding explanations should tell you about what is going on:</p>
			<ul>
				<li>A common choice to use for the prior is beta distribution. If you think for a moment, parameter <img src="image/Formula_02_084.png" alt=""/> takes values within <img src="image/Formula_02_085.png" alt=""/>. So, we need to use a probability distribution with the same support to model <img src="image/Formula_02_086.png" alt=""/>, which beta distribution has. </li>
				<li>In addition, if we use beta distribution for the prior and plug it in the Bayes formula with a Bernoulli likelihood, the posterior also becomes a beta distribution. This way, we can use the posterior as the prior for the next update when we observe new data. </li>
				<li>Having the <a id="_idIndexMarker137"/>posterior in the same distribution family with the prior is such a convenience that it even has a special name: they are called <strong class="bold">conjugate distributions</strong>, and the prior is <a id="_idIndexMarker138"/>called a <strong class="bold">conjugate prior</strong> for the<a id="_idIndexMarker139"/> likelihood function. Beta distribution is a conjugate prior for the Bernoulli distribution. Depending on your choice of modeling the likelihood, it is possible to find a conjugate before implementing Thompson sampling.</li>
			</ul>
			<p>Without further ado, let's implement Thompson sampling for our online advertising example. </p>
			<h3>Implementation</h3>
			<p>The <a id="_idIndexMarker140"/>beta distribution for the prior of the ad <img src="image/Formula_02_087.png" alt=""/> is given by the following:</p>
			<div>
				<div id="_idContainer108" class="IMG---Figure">
					<img src="image/Formula_02_088.jpg" alt=""/>
				</div>
			</div>
			<p>Here, <img src="image/Formula_02_089.png" alt=""/> and <img src="image/Formula_02_090.png" alt=""/> are the parameters characterizing the beta distribution, and <img src="image/Formula_02_091.png" alt=""/> is the gamma function. Don't let this formula scare you! It is actually pretty easy to implement. To initialize the prior, we use <img src="image/Formula_02_092.png" alt=""/>, which makes <img src="image/Formula_02_093.png" alt=""/> uniformly distributed over <img src="image/Formula_02_094.png" alt=""/>. Once we observe a reward, <img src="image/Formula_02_095.png" alt=""/>, after selecting the ad <img src="image/Formula_02_096.png" alt=""/>, we obtain the posterior distribution, as follows:</p>
			<p>			<img src="image/Formula_02_097.png" alt=""/>s<img src="image/Formula_02_098.png" alt=""/></p>
			<p>Now, let's do<a id="_idIndexMarker141"/> this in Python:</p>
			<ol>
				<li value="1">First, initialize the variables that we will need:<p class="source-code">n_prod = 100000</p><p class="source-code">n_ads = len(ads)</p><p class="source-code">alphas = np.ones(n_ads)</p><p class="source-code">betas = np.ones(n_ads)</p><p class="source-code">total_reward = 0</p><p class="source-code">avg_rewards = []</p></li>
				<li>Now, initialize the main loop with Bayesian updates:<p class="source-code">for i in range(n_prod):</p><p class="source-code">    theta_samples = [np.random.beta(alphas[k], betas[k]) for k in range(n_ads)]</p><p class="source-code">    ad_chosen = np.argmax(theta_samples)</p><p class="source-code">    R = ads[ad_chosen].display_ad()</p><p class="source-code">    alphas[ad_chosen] += R</p><p class="source-code">    betas[ad_chosen] += 1 - R</p><p class="source-code">    total_reward += R</p><p class="source-code">    avg_reward_so_far = total_reward / (i + 1)</p><p class="source-code">    avg_rewards.append(avg_reward_so_far)</p><p class="source-code">df_reward_comparison['Thompson Sampling'] = avg_rewards</p><p>We sample <img src="image/Formula_02_099.png" alt=""/> for each <img src="image/Formula_02_100.png" alt=""/> value from their corresponding posteriors and display the ad that corresponds to the greatest sampled parameter. Once we observe the reward, we make the posterior the prior and update it according to the preceding rule to obtain the new posterior.</p></li>
				<li>Then, display<a id="_idIndexMarker142"/> the results:<p class="source-code">df_reward_comparison['Thompson Sampling'].iplot(title="Thompson Sampling Avg. Reward: {:.4f}"</p><p class="source-code">                                   .format(avg_reward_so_far),</p><p class="source-code">                                    xTitle='Impressions', </p><p class="source-code">                                    yTitle='Avg. Reward')</p><p>This results in the following output:</p></li>
			</ol>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B14160_02_05.jpg" alt="Figure 2.5 – Exploration using Thompson sampling&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 2.5 – Exploration using Thompson sampling</p>
			<p>Thompson sampling has<a id="_idIndexMarker143"/> given a performance that is similar to ε-greedy and UCB approaches, right at 3% CTR.</p>
			<h2 id="_idParaDest-57"><a id="_idTextAnchor056"/>Advantages and disadvantages of Thompson sampling</h2>
			<p>Thompson<a id="_idIndexMarker144"/> sampling is a very competitive approach with one major advantage over the ε-greedy and UCB approaches: <em class="italic">Thompson sampling did not require us to do any hyperparameter tuning</em>. This, in practice, has the following benefits:</p>
			<ul>
				<li><strong class="bold">Saves significant time</strong> that would have been spent on hyperparameter tuning</li>
				<li><strong class="bold">Saves significant money</strong> that would have been burned by ineffective exploration and incorrect selection of hyperparameters in other approaches.</li>
			</ul>
			<p>In addition, Thompson<a id="_idIndexMarker145"/> sampling is shown to be a very competitive choice in many benchmarks in literature, and it has gotten increasingly popular over the last few years.</p>
			<p>Awesome job! Now that Thompson sampling is in your toolkit, along with the other methods, you are set to go out and solve real-world MAB problems! </p>
			<h1 id="_idParaDest-58"><a id="_idTextAnchor057"/>Summary</h1>
			<p>In this chapter, we covered MAB problems, which is one-step RL with many practical business applications. Despite its apparent simplicity, it is tricky to balance the exploration and exploitation in MAB problems, and any improvements in managing this trade-off come with savings in cost and increases in revenue. We have introduced four approaches to this end: A/B/n testing, ε-greedy actions, action selection using UCB, and Thompson sampling. We implemented these approaches in an online advertising scenario and discussed their advantages and disadvantages.</p>
			<p>So far, while making decisions, we have not considered any information about the situation in the environment. For example, we have not used any information about the users (for example, location, age, previous behavior, and so on) in the online advertising scenario that could be available to our decision-making algorithm. In the next chapter, you will learn about a more advanced form of MABs, namely contextual bandits, which can use to come up with better decisions.</p>
			<h1 id="_idParaDest-59"><a id="_idTextAnchor058"/>References</h1>
			<ul>
				<li>Chapelle, O., &amp; Li, L. (2011). An Empirical Evaluation of Thompson Sampling. <em class="italic">Advances in Neural Information Processing Systems 24</em>, (pp. 2249-2257)</li>
				<li>Marmerola, G. D. (2017, November 28). <em class="italic">Thompson Sampling for Contextual bandits</em>. Retrieved from Guilherme's blog: <a href="https://gdmarmerola.github.io/ts-for-contextual-bandits/">https://gdmarmerola.github.io/ts-for-contextual-bandits/</a></li>
				<li>Russo, D., Van Roy, B., Kazerouni, A., Osband, I., &amp; Wen, Z. (2018). <em class="italic">A Tutorial on Thompson Sampling. Foundations and Trends in Machine Learning</em>, (pp. 1-96)</li>
			</ul>
		</div>
	</body></html>