<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Training Magenta Models</h1>
                </header>
            
            <article>
                
<p class="mce-root">In this chapter, we'll use the prepared data from the previous chapter to train some of the RNN and VAE networks. Machine learning training is a finicky process involving a lot of tuning, experimentation, and back and forth between your data and your model. We'll learn to tune hyperparameters, such as batch size, learning rate, and network size, to optimize network performance and training time. We'll also show common training problems such as overfitting and models not converging. Once a model's training is complete, we'll show how to use the trained model to generate new sequences. Finally, we'll show how to use Google Cloud Platform to train models faster on the cloud.</p>
<p class="mce-root">The following topics will be covered in this chapter:</p>
<ul>
<li>Choosing the model and configuration</li>
<li>Training and tuning a model</li>
<li>Using Google Cloud Platform</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll use the following tools:</p>
<ul>
<li>A <strong>command line</strong> or <strong>Bash</strong> to launch Magenta from the Terminal</li>
<li><strong>Python</strong> and its libraries to write specific training configuration for a model</li>
<li><strong>Magenta</strong> and <strong>Magenta GPU</strong> to train our models</li>
<li><strong>TensorBoard</strong> to verify the training metrics</li>
<li><strong>Google Cloud Platform</strong> to offload the training in the cloud</li>
</ul>
<p class="mce-root"/>
<p>In Magenta, we'll make the use of the <strong>Drums RNN</strong>, <strong>Melody RNN</strong>, and <strong>MusicVAE</strong> models for training. We'll be explaining the training for those models, but if you feel like you need more information, the model's README in Magenta's source code (<a href="https://github.com/tensorflow/magenta/tree/master/magenta/models">github.com/tensorflow/magenta/tree/master/magenta/models</a>) is a good place to start. You can also take a look at Magenta's code, which is well documented. We have also provided additional content in the last section, <em>Further reading</em>.</p>
<p>The code for this chapter is in this book's GitHub repository in the <kbd>Chapter07</kbd><span> </span><span>folder</span><span>, located at</span> <a href="https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter07">github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter07</a><span>. The examples and code snippets will assume you are located in the chapter folder. For this chapter, you should</span> <span><span>go to </span></span><kbd>cd Chapter07</kbd><span> before you start.</span></p>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/2OcaY5p">http://bit.ly/2OcaY5p</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Choosing the model and configuration</h1>
                </header>
            
            <article>
                
<p>In <a href="1ca56e24-b4d2-40de-b4cf-ae6bbb3c0eef.xhtml">Chapter 6</a>, <em>Data Preparation for Training</em>, we looked at how to build a dataset. The datasets we produced were symbolic ones composed of MIDI files containing specific instruments, such as percussion or piano, and from specific genres, such as dance music and jazz music. </p>
<p>We also looked at how to prepare a dataset, which corresponds to the action of preparing the input formats (MIDI, MusicXML, or ABCNotation) into a format that can be fed to the network. That format is specific to a Magenta model, meaning the preparation will be different for the Drums RNN and MusicVAE models, even if both models can train on percussion data.</p>
<p>The first step before starting the training is to choose the proper model and configuration for our use case. Remember, a model in Magenta defines a deep neural network architecture, and each network type has its advantages and disadvantages. Let's have a look at how to choose a model, configure it, and train it from scratch.</p>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Comparing music generation use cases</h1>
                </header>
            
            <article>
                
<p>Let's take the example of training a percussion model. If we want to train a model that generates rhythmic percussion, we can either choose the Drums RNN model or the MusicVAE model:</p>
<ul>
<li>The first model, Drums RNN, will be more efficient at <strong>generating longer sequences</strong> that keep the global musical structure because the model can learn long-term dependencies using the attention mechanism (refer to <a href="b60deee5-c58f-45eb-88a2-23718802e580.xhtml">Chapter 2</a>, <em>Generating Drum Sequences with Drums RNN</em>, for a refresher on that).</li>
<li>The second model, MusicVAE, won't be able to do that but will be able to sample from the latent space and <strong>interpolate between sequences</strong> (refer to <a href="838da33e-26a9-4701-bfd3-5014dfff4146.xhtml">Chapter 4</a>, <em>Latent Space Interpolation with MusicVAE</em>, for a refresher on that).</li>
</ul>
<p>Depending on your use case, you might want to train one or the other or both, but keep in mind their strengths and weaknesses.</p>
<p>If we take the example of training a melody model, we can use a monophonic model, such as Melody RNN or MusicVAE (with the same restrictions as previously mentioned) if we want the resulting generation to be monophonic. We can also use a polyphonic model, such as Polyphony RNN if we want the generation to be polyphonic.</p>
<p>Sometimes, we know what model to use, but the configuration doesn't fit our use case. Let's look at how to create a new configuration.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a new configuration</h1>
                </header>
            
            <article>
                
<p>We'll take the example of a bass dataset we would like to train using the Music VAE model. Looking at the <kbd>configs</kbd> module in the <kbd>magenta.models.music_vae</kbd> module, we find the <kbd>cat-mel_2bar_small</kbd> configuration, which is close to what we want to achieve, but when the dataset is converted, the notes that don't correspond to a melody program (defined as 0 to 32 in Magenta) are thrown out.</p>
<div class="packt_tip">You can find this code in the <kbd>chapter_07_example_01.py</kbd> file <span>in the source code of this chapter. There are more comments and content in the source code, so you should go check it out.</span></div>
<p class="mce-root"/>
<p class="mce-root"/>
<p>To achieve that, we'll create a new configuration called <kbd>cat-bass_2bar_small</kbd> and we'll change the valid programs to <kbd>bass</kbd> programs:</p>
<ol>
<li>First, let's create a new <kbd>Config</kbd> instance with the following content:</li>
</ol>
<pre style="padding-left: 60px"><span>from </span>magenta.common <span>import </span>merge_hparams<br/><span>from </span>magenta.models.music_vae <span>import </span>Config<br/><span>from </span>magenta.models.music_vae <span>import </span>MusicVAE<br/><span>from </span>magenta.models.music_vae <span>import </span>lstm_models<br/><span>from </span>magenta.models.music_vae.data <span>import </span>BASS_PROGRAMS<br/><span>from </span>magenta.models.music_vae.data <span>import </span>NoteSequenceAugmenter<br/><span>from </span>magenta.models.music_vae.data <span>import </span>OneHotMelodyConverter<br/><span>from </span>tensorflow.contrib.training <span>import </span>HParams<br/><br/><strong><span>cat_bass_2bar_small</span></strong> = Config(<br/>  <span>model</span>=MusicVAE(lstm_models.BidirectionalLstmEncoder(),<br/>                 lstm_models.CategoricalLstmDecoder()),<br/>  <span>hparams</span>=merge_hparams(<br/>    lstm_models.get_default_hparams(),<br/>    HParams(<br/>      <span>batch_size</span>=<span>512</span>,<br/>      <span>max_seq_len</span>=<span>32</span>,<br/>      <span>z_size</span>=<span>256</span>,<br/>      <span>enc_rnn_size</span>=[<span>512</span>],<br/>      <span>dec_rnn_size</span>=[<span>256</span>, <span>256</span>],<br/>      <span>free_bits</span>=<span>0</span>,<br/>      <span>max_beta</span>=<span>0.2</span>,<br/>      <span>beta_rate</span>=<span>0.99999</span>,<br/>      <span>sampling_schedule</span>=<span>"inverse_sigmoid"</span>,<br/>      <span>sampling_rate</span>=<span>1000</span>,<br/>    )),<br/>  <span>note_sequence_augmenter</span>=NoteSequenceAugmenter(<span>transpose_range</span>=(-<span>5</span>, <span>5</span>)),<br/>  <span>data_converter</span>=OneHotMelodyConverter(<br/>    <span>valid_programs</span>=<strong>BASS_PROGRAMS</strong>,<br/>    <span>skip_polyphony</span>=<span>False</span>,<br/>    <span>max_bars</span>=<span>100</span>,<br/>    <span>slice_bars</span>=<span>2</span>,<br/>    <span>steps_per_quarter</span>=<span>4</span>),<br/>  <span>train_examples_path</span>=<span>None</span>,<br/>  <span>eval_examples_path</span>=<span>None</span>,<br/>)</pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The only part we've changed here is the <kbd>valid_programs=BASS_PROGRAMS</kbd> argument in <kbd>OneHotMelodyConverter</kbd>, but we could have changed other elements, such as <kbd>NoteSequenceAugmenter</kbd> that we talked about in the previous chapter. Hyperparameters can be changed using the <kbd>hparams</kbd> flag, but we can also define them in a configuration if we want to define default values for a model.</p>
<ol start="2">
<li>To use the new configuration, we can call the <kbd>run</kbd> method of the <kbd>music_vae_train</kbd> module:</li>
</ol>
<pre style="padding-left: 60px"><span>import tensorflow as tf<br/>from </span>magenta.models.music_vae.configs <span>import </span>CONFIG_MAP<br/>from magenta.models.music_vae.music_vae_train import run<br/><br/>def main(unused_argv):<br/>  <strong>CONFIG_MAP["cat-bass_2bar_small"] = cat_bass_2bar_small</strong><br/>  <strong>run(CONFIG_MAP)</strong><br/><br/>if __name__ == "__main__":<br/>  tf.app.run(main)</pre>
<p style="padding-left: 60px">Here, we import the whole configuration map and add our new configuration before calling the <kbd>run</kbd> method, so that we can still pass other configurations in the <kbd>--config</kbd> flag.</p>
<ol start="3">
<li>We can then call this code the same way as we would call the <kbd>music_vae_train</kbd> command:</li>
</ol>
<pre style="padding-left: 60px"><strong>python chapter_07_example_01.py --config="cat-bass_2bar_small" [FLAGS]</strong></pre>
<p>Here, <kbd>FLAGS</kbd> are the training flags we need to pass, such as <kbd>--run_dir</kbd> and <kbd>--sequence_example_file</kbd>.</p>
<p>Other models, such as the Drums RNN or Melody RNN models, will be configured in the same manner. Refer to the next section for examples on how to do that.</p>
<p>Now that we know how to choose a model and a configuration (or create a new one), let's look at how to start and configure the training.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training and tuning a model</h1>
                </header>
            
            <article>
                
<p>Training a machine model is an empirical and iterative approach, where we first prepare the data and the configuration, then train the model, fail, and restart again. Getting models to train on the first try is rare, but we'll persevere through hardship together.</p>
<p>When launching a <strong>training phase</strong>, we'll be looking at specific metrics to verify that our model is training properly and converging. We'll also be launching an <strong>evaluation phase</strong>, which executes on a separate, smaller dataset, to verify that the model can properly generalize on data that it hasn't seen yet.</p>
<div class="packt_infobox">The <strong>evaluation</strong> dataset is often called the <strong>validation</strong> dataset in machine learning in general, but we'll keep the term evaluation since it is used in Magenta.<br/>
<br/>
The validation dataset is different than the <strong>test</strong> dataset, which is an external dataset, often curated by hand, and contains hard examples, giving a final test to measure the network performance. The test dataset is often used to compare different models' performance. We won't be looking at test datasets here.</div>
<p>We'll break down and explain each step of the process. Let's start by looking at some best practices and conventions we'll be using for this chapter.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Organizing datasets and training data</h1>
                </header>
            
            <article>
                
<p>Because of the iterative nature of training, we'll get to produce many datasets and many <strong>training runs</strong>. The best way to proceed is to keep both separate, for example, in two folders named <kbd>datasets</kbd> and <kbd>training</kbd>.</p>
<p>In the <kbd>datasets</kbd> folder, we can copy what we produced from the previous chapter in separate folders, for example, <kbd>dance_drums</kbd>, <kbd>jazz_drums</kbd>, <kbd>piano_drums</kbd>, and so on, with the folders containing the MIDI files:</p>
<ul>
<li>We keep the <kbd>notesequences.tfrecords</kbd> file in the proper dataset folder since it is produced only once per dataset.</li>
<li>We keep the <kbd>sequence_examples</kbd> folder outside of this folder because it is model dependent, meaning we'll be regenerating this folder for each model, for example, once for Drums RNN and once for MusicVAE (even if we use the same data).</li>
</ul>
<p class="mce-root"/>
<p>In the <kbd>training</kbd> folder, we'll be creating a new folder for each model and dataset, for example, <kbd>drums_rnn_dance_drums</kbd>:</p>
<ul>
<li>We'll be executing the <kbd>MODEL_create_dataset</kbd> command (if available), creating the <kbd>sequence_examples</kbd> directory (if any) for the model.</li>
<li>Then, we'll be launching multiple training runs, with proper naming, for example, <kbd>run1_with_dropout</kbd>, or other configuration we might want to use:</li>
</ul>
<p class="CDPAlignCenter CDPAlign"><img src="assets/c44e2218-c64f-46ff-861b-3fcaea388f2a.png"/></p>
<p>Having a single training folder with multiple runs is useful because we can load multiple training runs in TensorBoard and compare how each model has performed.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training on a CPU or a GPU</h1>
                </header>
            
            <article>
                
<p>Training is a computationally extensive activity. Training a simple model, such as the Drums RNN model, on an entry-level GPU (for example, an RTX 2060) will take around 5 hours. Training on the CPU takes a lot more time since the operations needed in network training (namely, vector arithmetic) are optimized to be executed in parallel on a GPU. To use a GPU, we also need to have properly installed the <kbd>magenta-gpu</kbd> package, as well as the CUDA libraries (see <a href="c5602f6c-c094-42f2-936f-98746cf04a49.xhtml">Chapter 1</a>, <em>Introduction on Magenta and Generative Art</em>, for more information on that).</p>
<p class="mce-root"/>
<p>If you don't have a GPU, don't despair, you can follow this chapter anyway. You can do the data preparation steps and launch the training on a small network (see the first training example later on how to do that). Then, let the network train for a while, and if you see encouraging results, follow the steps in the last section, <em>Using Google Cloud Platform</em>, to restart the training on a faster machine. This will enable you to test the commands and datasets locally and then offload the bulk of the work to GCP. Even if you have a GPU, this might be a good way to proceed, especially if you want to train multiple models at the same time, or test different hyperparameters at the same time.</p>
<p>The following sections and commands still apply to both CPU and GPU training, as well as GCP.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training RNN models</h1>
                </header>
            
            <article>
                
<p>We now have all of the elements to start training a model. Let's take a simple example, we'll use the <kbd>dance_drums</kbd> dataset from the previous chapter and train the Drums RNN model.</p>
<div class="packt_tip">You can find this code in the <kbd>README.md</kbd> file <span>in the source code of this chapter. Since most of the code snippets in this chapter are command line, we are not providing example files for each example.</span></div>
<p>From the previous chapter, we should now have a <kbd>datasets/dance_drums</kbd> folder ready with the MIDI files. We've already executed the <kbd>convert_dir_to_note_sequences</kbd> command, which produces a <kbd>notesequences.tfrecord</kbd> file.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating the dataset and launching the training</h1>
                </header>
            
            <article>
                
<p>We'll now create the dataset (an operation we've already done in the previous chapter, but we show it again here as a refresher) and launch the training.</p>
<ol>
<li>First, let's create the sequence examples. In the <kbd>training</kbd> folder, create and change directory to a new folder called <kbd>drums_rnn_dance_drums</kbd>, and execute (replacing <kbd>PATH_TO_NOTE_SEQUENCES</kbd> with the proper file):</li>
</ol>
<pre style="padding-left: 60px"><span><strong>drums_rnn_create_dataset --config="drum_kit" --input="PATH_TO_NOTE_SEQUENCES" --output_dir="sequence_examples" --eval_ratio=0.10</strong><br/></span></pre>
<p style="padding-left: 60px">This should create a <kbd>sequence_examples</kbd> folder containing two files, a training set and an evaluation set for the drum sequences.</p>
<p class="mce-root"/>
<div class="packt_tip">Ideally, the <kbd>drums_rnn_create_dataset</kbd> command should be called only once for all of the training runs. Since we are going to tune the hyperparameters between each run, and that the hyperparameters are sensible to the training data, changing the training and evaluation dataset while tuning the model is not a good idea.</div>
<ol start="2">
<li>We'll now start the training using a small network:</li>
</ol>
<pre style="padding-left: 60px"><span><strong>drums_rnn_train --config="drum_kit" --run_dir="logdir/run1_small" --sequence_example_file="sequence_examples/training_drum_tracks.tfrecord" --hparams="batch_size=64,rnn_layer_sizes=[64,64]" --num_training_steps=20000</strong><br/></span></pre>
<div class="packt_tip">On Windows, the<span> </span><kbd>--run_dir</kbd><span> </span>flag should use a backslash. For this example and all of the following examples, instead of writing<span> </span><kbd>--run_dir="logdir/run1_small"</kbd>, use<span> </span><kbd>--run_dir="logdir\run1_small"</kbd>.</div>
<p style="padding-left: 60px">We use an output directory named <kbd>run1_small</kbd>, so we can remember later what run it is, and an input file named <kbd>training_drum_tracks.tfrecord</kbd>. The hyperparameters are a batch size of 64 and a two-layer RNN network of 64 units for each layer, the number of elements in the list defining the number of layers. For a 3 layers RNN network, use [64, 64, 64].</p>
<p style="padding-left: 60px">You should see in the Terminal the complete list of hyperparameters and their values, which are taken from the configuration when not overridden by a flag:</p>
<pre style="padding-left: 60px"><strong>INFO:tensorflow:hparams = {'batch_size': 64, 'rnn_layer_sizes': [64, 64], 'dropout_keep_prob': 0.5, 'attn_length': 32, 'clip_norm': 3, 'learning_rate': 0.001, 'residual_connections': False, 'use_cudnn': False}</strong></pre>
<p style="padding-left: 60px">We'll soon see how hyperparameters affect training in the following sections. If you are using a GPU, make sure TensorFlow can use your GPU by checking this output:</p>
<pre style="padding-left: 60px">2019-11-20 01:56:12.058398: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1304] <strong>Created TensorFlow device</strong> (/job:localhost/replica:0/task:0/device:GPU:0 with 5089 MB memory) -&gt; physical GPU (device: 0, name: GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<ol start="3">
<li>Now, the network will start training, and you should see outputs like this:</li>
</ol>
<pre style="padding-left: 60px">INFO:tensorflow:Accuracy = 0.27782458, Global Step = 10, Loss = 5.4186254, Perplexity = 225.56882 (16.487 sec)<br/>INFO:tensorflow:global_step/sec: 0.586516</pre>
<p>We'll be talking about the different metrics soon. When we launched the training, we've used the <kbd>--num_training_steps=20000</kbd> flag, meaning the network will stop its training after reaching 20,000 global steps. We won't be talking about epoch here, which consists of a full cycle through the training data since we only handle steps in Magenta. The model should converge before that, but giving an upper bound is good so that it doesn't execute for too long for no reason.</p>
<p>If you want to have an approximation on the time the training will take to reach 20,000 steps, you can use the <kbd>global_step</kbd>/<kbd>sec</kbd> output. For the previous output, our job should finish in approximately 9 hours, but this is an upper bound, so chances are we can stop it before.</p>
<p>Now that the training is launched, we can launch the evaluation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Launching the evaluation</h1>
                </header>
            
            <article>
                
<p>The evaluation job executes on the evaluation dataset, which is a smaller (we previously used a <kbd>--eval_ratio=0.10</kbd> flag, meaning 10%) and separate dataset from the training set. The evaluation job evaluates the model and computes the loss function, without updating any of the weights in the network. Therefore, the evaluation process is fast and can be executed at the same time as the training job on the CPU.</p>
<p>To launch the evaluation, we use the same command, using the <kbd>--eval</kbd> flag. If you are using a GPU, you'll need to deactivate the GPU for that execution, using the <kbd>CUDA_VISIBLE_DEVICES=""</kbd> environment variable, because the previous TensorFlow process takes all of the available memory.</p>
<div class="packt_tip">On Windows, don't forget to use a backslash in the <kbd>--run_dir</kbd> flag. Also on Windows, use the <kbd>set</kbd> command to set an environment variable for the current command-line session. On Linux and macOS, you can set the environment variable for a single command by prefixing the variable value before the command.</div>
<p class="mce-root"/>
<p>On Windows, use the following:</p>
<pre><span>set CUDA_VISIBLE_DEVICES=""<br/>drums_rnn_train --config="drum_kit" --run_dir="logdir\run1_small" --sequence_example_file="sequence_examples/<strong>eval_drum_tracks.tfrecord</strong>" --hparams="batch_size=64,rnn_layer_sizes=[64,64]" --num_training_steps=20000 -<strong>-eval</strong><br/></span></pre>
<p>On Linux and macOS, use the following:</p>
<pre><span>CUDA_VISIBLE_DEVICES="" drums_rnn_train --config="drum_kit" --run_dir="logdir/run1_small" --sequence_example_file="sequence_examples/<strong>eval_drum_tracks.tfrecord</strong>" --hparams="batch_size=64,rnn_layer_sizes=[64,64]" --num_training_steps=20000 <strong>--eval</strong></span></pre>
<p>For this command, the provided network size needs to correspond to the training network size. If you used <kbd>rnn_layer_sizes=[64,64]</kbd> in the training command, then you need to use the same here. The two flags we've changed from the previous command are the <kbd>--eval</kbd> and <kbd>--sequence_example_file</kbd> flags.</p>
<p>The evaluation will execute when a new checkpoint (which happens approximately every 40 steps) is added in the running directory. When that happens, you'll see outputs similar to this:</p>
<pre><strong>Starting evaluation at 2019-11-25-23:38:24</strong><br/><strong>INFO:tensorflow:Accuracy = 0.0, Global Step = 35, Loss = 0.0, Perplexity = 1.0</strong><br/><strong>INFO:tensorflow:Evaluation [1/3]</strong><br/><strong>INFO:tensorflow:Evaluation [2/3]</strong><br/><strong>INFO:tensorflow:Evaluation [3/3]</strong><br/><strong>Finished evaluation at 2019-11-25-23:38:30</strong></pre>
<p>The evaluation job will stop automatically when the training job hasn't produced a checkpoint for a while.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Looking at TensorBoard</h1>
                </header>
            
            <article>
                
<p>During and after training, we can launch TensorBoard, which helps to visualize the network metrics. We'll be using TensorBoard to tune the hyperparameters and iterate with the data preparation phase.</p>
<p>To launch TensorBoard, use the following command:</p>
<pre><strong>tensorboard --logdir=logdir</strong></pre>
<p class="mce-root">Notice that we pass the parent output directory, meaning we'll have access to all of the contained runs (currently, there's only one). We can find the URL of TensorBoard in the console. Once opened, the page will look like this:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/805823b5-0ced-4f1f-903e-2d12343a48e9.png"/></p>
<p>This is the result of our trained model after 20,000 steps. On the left, we have the training and evaluation jobs, which can be toggled. On the right, different metrics are shown in the screenshot, with the a<span class="module__title__link">bscissa</span> being the global step count, which goes to 20,000 steps. The two most interesting metrics for us are <strong>loss</strong> and <strong>accuracy</strong>. We want the <strong>loss to go down</strong>, both for the training and evaluation sets, and the <strong>accuracy to go up</strong>.</p>
<p>We notice that this model has converged, meaning we have a successful training at hand, but we need to verify that the resulting model is good, by looking at the loss metric. Let's have a look at the loss function, comparing the training loss and the evaluation loss:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4bc7a1a3-a4af-4278-bff6-ca8ab4d71cc1.png"/></p>
<p>We can see here that the model is slightly overfitting the training data. You can find the model optimum by taking the <strong>evaluation loss curve at its lowest point</strong> before it starts going up. On the left of that point, the model is underfitting the training data; on the right, the model is overfitting the data. The difference between both curves is called the generalization gap.</p>
<p>Let's explain underfitting and overfitting before continuing on to other examples.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Explaining underfitting and overfitting</h1>
                </header>
            
            <article>
                
<p>Understanding underfitting and overfitting and how to prevent them is important for proper network training. When a model is too simple and cannot learn from the training dataset, we say the model is <strong>underfitting</strong>. On the other hand, when a model is learning properly from the training dataset but cannot generalize to data outside of it, we say the model is <strong>overfitting</strong>.</p>
<p>We show underfitting, optimal solution, and overfitting in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/ebaab65a-ee21-416e-a484-ce3e621229e8.png" style="width:55.92em;height:22.42em;"/></p>
<p>On the left, we show underfitting, meaning the model didn't learn on the dataset. In the middle, we show an optimal solution, and on the right, we show overfitting, where the resulting model is overly complex for the given data, meaning the model won't generalize to other datasets.</p>
<p>Remember that the objective of a neural network is to perform well on the training dataset as well as <strong>new data that it has never seen</strong>, which will be used to make predictions. Achieving that is hard since it requires a proper combination of quality and quantity of data, as well as proper network size and learning parameters.</p>
<p>Let's have a look at how to fix those issues.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fixing underfitting</h1>
                </header>
            
            <article>
                
<p>Underfitting is easy to address and can be solved by <strong>increasing the capacity of the model</strong>—basically, by adding more layers and units. By increasing the model's capacity, the network can learn more types of functions for mapping inputs to outputs.</p>
<p>For our previous example, we can increase the capacity of the model by adding more units (neurons) in each of the two layers:</p>
<pre><span>drums_rnn_train --config="drum_kit" --run_dir="logdir/run1_small" --sequence_example_file="sequence_examples/training_drum_tracks.tfrecord" --hparams="batch_size=64,<strong>rnn_layer_sizes=[128,128]</strong>" --num_training_steps=20000</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>We also need to train long enough, which might be hardware dependent, since if we are training on slow hardware, it might take a lot of time to reach the optimal point.</p>
<p>See the section, <em>Defining proper network size and hyperparameters</em>, for more information on network size.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fixing overfitting</h1>
                </header>
            
            <article>
                
<p>Overfitting, on the other hand, is harder to address since it comes from a variety of factors. The two most common causes are as follows:</p>
<ul>
<li>When a model overfits, it is because it has the capacity of overfitting the training dataset. By keeping the same training dataset, you can <strong>reduce the network capacity</strong>, so that the network won't have the necessary resources to overfit the training data anymore. To reduce the network capacity, use the <kbd>rnn_layer_sizes</kbd> hyperparameter as in the previous example.</li>
<li>By keeping the same network capacity, you can <strong>augment the training dataset size</strong> so that, with more data, the network might not have the capacity to overfit it anymore. The added data needs to be varied enough to fix overfitting and might not always work. To augment the training dataset size, go back to <a href="1ca56e24-b4d2-40de-b4cf-ae6bbb3c0eef.xhtml">Chapter 6</a>, <em>Data Preparation for Training</em>, and add content to the dataset. Be aware that augmenting the dataset with data that isn't diverse enough won't help to resolve overfitting.</li>
</ul>
<p>There is a relationship between the training dataset and the network size: <strong>the bigger and more diverse the dataset, the bigger the network has to be</strong>. A bigger network won't necessarily produce better results if the training dataset isn't big enough or qualitative enough.</p>
<p>There are other ways of fixing overfitting that can be used in Magenta models:</p>
<ul>
<li><strong>Early stopping</strong> of the training phase at the optimal point is one way since all of the training after that point makes the resulting network worse. To use early stopping, see the next section, <em>Using a specific checkpoint to implement early stop</em>.</li>
<li>Using regularization techniques such as <strong>dropout</strong>, which randomly and temporarily drops a unit/neuron out of the network, is another way. To use dropout, use the <kbd>dropout_keep_prob</kbd> hyperparameter.</li>
</ul>
<p>Regularization techniques are a class of approach that aims at constraining the size of the weights in a neural network and is widely used as a way to prevent overfitting.</p>
<p class="mce-root"/>
<p>As you might have noticed by now, there's a relationship between our dataset and our model that needs to be taken into account when tuning the training phase. Let's have a more detailed look into network size and hyperparameters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Defining network size and hyperparameters</h1>
                </header>
            
            <article>
                
<p>Defining a proper network size is a trial and error process, but a good starting point, if your hardware is good enough, are the values in the configuration of the model you want to use.</p>
<p>Let's take an example using the <kbd>attention_rnn</kbd> configuration of the Melody RNN model, using <kbd>batch_size=128</kbd>, <kbd>rnn_layer_sizes=[128, 128]</kbd>, <kbd>dropout_keep_prob=0.5</kbd>, <kbd>learning_rate=0.001</kbd>, and <kbd>clip_norm=3</kbd>:</p>
<ol>
<li>If the model is overfitting, we can try the following:<br/>
<ol>
<li>Using more dropout, for example, <kbd>dropout_keep_prob=0.4</kbd> and lower values</li>
<li>Adding more data</li>
<li>Reducing the network size using <kbd>rnn_layer_sizes=[64, 64]</kbd></li>
</ol>
</li>
<li>If the model is converging and not overfitting, we can try using a bigger model, <kbd>rnn_layer_sizes=[256, 256]</kbd>. If we have good data, using a bigger model will yield better results, so we want to optimize that.</li>
</ol>
<div class="packt_tip">When changing something, we need to make sure we are making a single modification and then testing the result before making any other change. Changing multiple parameters at the same time will prevent us from knowing the direct impact of each one.</div>
<p>Sometimes, when increasing the network size, we might stumble into a model that doesn't converge, meaning the loss function starts increasing again, which will result in a training error. That can be fixed by changing <kbd>learning_rate</kbd> or <kbd>clip_norm</kbd>. See the next section, <em>Fixing a model not converging</em>, for more information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Determining the batch size</h1>
                </header>
            
            <article>
                
<p>We haven't talked about <kbd>batch_size</kbd> yet. The batch size is the amount of data the network will handle at once. A bigger batch size may improve the training time by making the model parameters converge faster. It also should remove some computational overhead from transferring a larger chunk of data to the GPU memory at once.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>A rule of thumb is that when you increase the batch size, you'll also need to <strong>increase the learning rate</strong>. Since more data is taken into account at the same time, the model's weight can get updated using a bigger ratio.</p>
<p>Increasing the batch size might improve training time, but it might as well <strong>decrease the performance of the model</strong>, so using a <span>too big</span><span> </span><span>batch size might not be a good idea. Overall, the batch size is often a trade-off between execution time and the resulting quality of the model.</span></p>
<p>We provide more information on this in the last section, <em>Further reading</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fixing out of memory errors</h1>
                </header>
            
            <article>
                
<p>Sometimes, when using a batch size or network size too big, you might end up with the following error:</p>
<pre><strong>[tensorflow/stream_executor/cuda/cuda_driver.cc:890] failed to alloc 8589934592 bytes on host: CUDA_ERROR_OUT_OF_MEMORY: out of memory</strong></pre>
<p>Reduce the batch size and network size until the out of memory error disappears. Sometimes, the error is not fatal, in which case it will negatively impact the training performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fixing a wrong network size</h1>
                </header>
            
            <article>
                
<p>When using an existing run directory, either from continuing a previous training, starting an evaluation job, or starting a generation job, we need to provide the same network size as when it was first launched.</p>
<p>If the training run was first started using <kbd>rnn_layer_sizes=[256,256,256]</kbd> and then restarted using <kbd>rnn_layer_sizes=[128,128,128]</kbd>, we'll end up with the following error:</p>
<pre><strong>Invalid argument: Assign requires shapes of both tensors to match. lhs shape= [128] rhs shape= [256]</strong></pre>
<p>In this case, we'll need to use the network size that was first used when we started the training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fixing a model not converging</h1>
                </header>
            
            <article>
                
<p>A model that converges is defined by a <strong>decreasing loss function</strong> on both the training and evaluation sets. If our loss function goes up at some point, the model is unstable and isn't properly converging. There are many reasons as to why a model might not converge.</p>
<p class="mce-root"/>
<p>Let's take a simple example (this example uses the <kbd>jazz_drums</kbd> dataset we created in the previous chapter):</p>
<pre><strong><span>drums_rnn_train --config="drum_kit" --run_dir="logdir/run1_diverge" --sequence_example_file="sequence_examples/training_drum_tracks.tfrecord" --hparams="batch_size=128,rnn_layer_sizes=[128,128,128]" --num_training_steps=20000</span></strong></pre>
<p>When a model is diverging, we might get an error at some point:</p>
<pre>E1112 20:03:08.279203 10460 basic_session_run_hooks.py:760] Model diverged with loss = NaN.<br/>...<br/>  File "c:\users\magenta\appdata\local\programs\python\python35\lib\site-packages\tensorflow\python\training\basic_session_run_hooks.py", line 761, in after_run<br/>    raise NanLossDuringTrainingError<br/><strong>tensorflow.python.training.basic_session_run_hooks.NanLossDuringTrainingError: NaN loss during training.</strong></pre>
<p>The resulting TensorBoard graph will show the loss function going up. Let's fix the problem by using a smaller learning rate. The default learning rate value that was used in the previous command is <kbd>learning_rate=0.001</kbd>, so we'll make that 10 times smaller:</p>
<pre><span>drums_rnn_train --config="drum_kit" --run_dir="logdir/run2_learning_rate" --sequence_example_file="sequence_examples/training_drum_tracks.tfrecord" --hparams="<strong>learning_rate=0.0001</strong>,batch_size=128,rnn_layer_sizes=[128,128,128]" --num_training_steps=20000</span></pre>
<p><span> </span>Here is the resulting TensorBoard graph with both runs:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d4808a0e-2910-49a0-944b-25004a1a3f21.png"/></p>
<p class="mce-root">You can see that <kbd>run1_diverge</kbd> has a loss function that goes up, and <kbd>run2_learning_rate</kbd> is training properly.</p>
<p>There are many ways of fixing a diverging model, but since the problem is dependent on the data and the network size, you'll have to test various methods:</p>
<ul>
<li>Try <strong>reducing the learning rate</strong>, like in our previous example. Learning rate decay (available in the Music VAE model), where the learning rate is gradually reduced, can also help.</li>
<li>Try <strong>changing the network size</strong>. In this example, using a network size of <kbd>rnn_layer_sizes=[256,256,256]</kbd> will also fix the problem.</li>
<li>Try <strong>decreasing the gradient clipping</strong>. In our previous example, the gradient clipping default value is <kbd>clip_norm=3</kbd>, so you will want to decrease the <kbd>clip_norm</kbd> hyperparameter, for example, to <kbd>clip_norm=2</kbd>. Remember, default hyperparameter values are in the configurations for each model (see the previous section, <em>Creating a new configuration</em>, for more information on this).</li>
</ul>
<div class="packt_tip">Sometimes, fixing a diverging model will make another problem arise. For example, fixing the problem using a bigger network size might result in the network overfitting. Make sure you test multiple solutions so that the chosen one is best.</div>
<p>Most often than not, the NaN loss during training error is caused by the exploding gradients problem we've already talked about in <a href="48023567-4100-492a-a28e-53b18a63e01e.xhtml">Chapter 4</a>, <em>Generating Polyphonic Melodies</em>. This problem is common in RNNs, and even if LSTM cells helps a lot to make the model converge, the exploding gradient problem can still occur, given the accumulation of gradients unrolled over hundreds of input time steps.</p>
<p>During training, the loss is calculated on the training examples, and then its derivative is backpropagated through the network, updating the weights by a fraction of the propagated error, this fraction being the learning rate. When the weights get updated over and over by large values, they tend to explode, or overflow, which is why using a smaller learning rate might fix the problem.</p>
<p>Gradient clipping has a similar effect but is useful if we prefer not changing the learning rate. By using gradient clipping, we can rescale, or clip to a maximum value, the gradient vector (the error derivative) that will be backpropagated through the network. The parameter we have available in Magenta is <kbd>clip_norm</kbd>, which is used by TensorFlow as <kbd>tf.clip_by_norm(t, clip_norm)</kbd>. By decreasing the parameter's value, we effectively normalize the error gradient so that the norm is equal or less than the provided value.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Fixing not enough training data</h1>
                </header>
            
            <article>
                
<p>Let's now train the Melody RNN model using our previous chapter's jazz piano dataset:</p>
<ol>
<li>We first create the dataset using the <kbd>attention_rnn</kbd> configuration. In the <kbd>training</kbd> folder, create and change the directory to a new folder called <kbd>melody_rnn_jazz_piano</kbd>, and execute (replacing <kbd>PATH_TO_NOTE_SEQUENCES</kbd> with the proper file, which should be in your <kbd>datasets</kbd> folder):</li>
</ol>
<pre style="padding-left: 60px"><strong><span>melody_rnn_create_dataset --config="attention_rnn" --input="PATH_TO_NOTE_SEQUENCES" --output_dir="sequence_examples" --eval_ratio=0.10</span></strong></pre>
<ol start="2">
<li>We then train the model using the following:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>melody_rnn_train --config="attention_rnn" --run_dir="logdir/run1_few_data" --sequence_example_file="sequence_examples/training_melodies.tfrecord" --hparams="batch_size=128,rnn_layer_sizes=[128,128]" --num_training_steps=20000</span></strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>When checked in TensorFlow, we can look at the <kbd>run1_few_data</kbd> run:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/974cbacb-aadb-44c5-afc9-130d1afdc52c.png"/></p>
<p>In the <strong>loss</strong> diagram, the first two lines at the top are the train and evaluation metrics for the run, <kbd>run1_few_data</kbd>. The evaluation loss goes up, meaning the model is overfitting really fast. This is because we don't have a lot of data (659 outputs to be exact).</p>
<p>Fixing this problem requires us to go back to preparing the data. For the <kbd>run2_more_data</kbd> run, in the loss diagram, the two lowest curves show us that the problem is fixed. To get more data, we came back to the pipeline from the previous chapter, <kbd>melody_rnn_pipeline_example.py</kbd>, and changed <kbd>ignore_polyphonic_notes=False</kbd> to <kbd>True</kbd> in the <kbd>MelodyExtractor</kbd> pipeline. This means that, instead of throwing out polyphonic melodies, the pipeline converts them into a monophonic one, keeping the highest note. The conversion method is in the <kbd>melodies_lib</kbd> module, so if we want to change that behavior, we'll have to write our own pipeline.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Because this change modifies the musical content of our dataset, we'll need to carefully listen to the generated results, to verify that the trained model outputs interesting samples. Here is a generated sample from the <kbd>run2_more_data</kbd> trained model:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/2bd8188d-e618-42bd-8cca-e7a60ff35b15.png"/></p>
<p>This example is a good example of the necessity of going back and forth between the data preparation step and the training step. See the next section, <em>Generating sequences from a trained model</em>, for more information on how to generate sequences from a trained model.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring attention and other hyperparameters</h1>
                </header>
            
            <article>
                
<p>The Melody RNN model uses attention during training to look at previous steps (see <a href="48023567-4100-492a-a28e-53b18a63e01e.xhtml">Chapter 3</a>, <em>Generating Polyphonic Melodies</em>, for a refresher on that). You can use the <kbd>attn_length</kbd> <span>hyperparameter</span><span> </span><span>to configure the length of the attention spawn.</span></p>
<p>Each model has its own configuration. Make sure you check them out in their respective configuration files to see what parameter you can tune during training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating sequences from a trained model</h1>
                </header>
            
            <article>
                
<p>Now that we have a trained model, <kbd>run1_small</kbd>, and we understand that it slightly overfits, let's try and generate a sequence out of it to see how it goes. To generate a sequence when the network has finished training or during training, we can use the model's <kbd>generate</kbd> command.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Let's call the <kbd>drums_rnn_generate</kbd> method using the following parameters (if you are launching this command during training and you are using a GPU, don't forget to use <kbd>CUDA_VISIBLE_DEVICES=""</kbd>):</p>
<pre><strong><span>drums_rnn_generate --config="drum_kit" --run_dir="logdir/run1_small" --hparams="rnn_layer_sizes=[64,64]" --output_dir="generated/generated1_small"</span></strong></pre>
<p>The <kbd>generate</kbd> command will take the latest checkpoint in the run directory and use it to generate a sequence. We need to use the same network size as for the training phase.</p>
<p>Here is a generated sequence from our training phase:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/be288cd0-828d-4787-87b0-3c039172c22f.png"/></p>
<p>The model should have generated 10 sequences; we should listen to hear what it sounds like. Congratulations! You've heard the first notes of your own generated model.</p>
<p>The advantage of using the <kbd>dance</kbd> dataset is that it is easy to verify that the training model is generating what we expected: in the resulting 10 sequences, each of them mostly has a bass drum on beat. Now, we need to ask ourselves whether the resulting generation is diverse and interesting. If not, we should go back to prepare a new dataset and iterate on it.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using a specific checkpoint to implement early stops</h1>
                </header>
            
            <article>
                
<p>We previously talked about early stopping, which is the action of stopping training at the optimum, instead of letting the model degrade further, as a way to prevent overfitting. There are multiple ways of doing that, such as coding a stop condition that checks whether the evaluation loss function starts going up, but the easiest approach is to keep only the checkpoint that is nearest to the optimum after the training phase is over.</p>
<p>Going back to our previous example, we find the evaluation loss curve minimum to be at around 7,000 steps. In the <kbd>logdir/run1_small</kbd> directory, we find that we have a checkpoint near our optimum that we can use: <kbd>model.<strong>ckpt-6745</strong>.data-00000.</kbd></p>
<p class="mce-root"/>
<p>To use that checkpoint, we need to use the <kbd>--checkpoint_file</kbd> flag instead of the <kbd>--run_dir</kbd> flag:</p>
<pre>drums_rnn_generate --config="drum_kit" <strong>--checkpoint_file="logdir/run1_small/train/model.ckpt-6745"</strong> --hparams="batch_size=64,rnn_layer_sizes=[64,64]" --output_dir="generated/generated1_small"</pre>
<p>Notice we don't pass the full filename (only <kbd>model.ckpt-6745</kbd> and not <kbd>model.ckpt-6745.data-00000-of-00001</kbd>) because TensorFlow expects only the first part. The command should generate 10 new elements using that checkpoint.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Packaging and distributing the result using bundles</h1>
                </header>
            
            <article>
                
<p>When we are satisfied with our trained model, we can package it using Magenta's bundle for distribution. Remember that bundles are available for RNN models only, but we'll provide ways of bundling other models such as Music VAE when we get there. Follow these steps:</p>
<ol>
<li>To package a bundle, we call the generate command using the <kbd>--bundle_file</kbd> and <kbd>--save_generator_bundle</kbd> flags:</li>
</ol>
<pre style="padding-left: 60px">drums_rnn_generate --config="drum_kit" --run_dir="logdir/run1_small" --hparams="batch_size=64,rnn_layer_sizes=[64,64]" <strong>--bundle_file="drums_rnn_dance_small.mag" --save_generator_bundle</strong></pre>
<p style="padding-left: 60px">This will save the bundle in the <kbd>drums_rnn_dance_small.mag</kbd> file using the latest checkpoint. We can also use the <kbd>--checkpoint_file</kbd> flag from the previous command if we need another checkpoint that is not the latest.</p>
<ol start="2">
<li>We can now use the bundle as follows:</li>
</ol>
<pre style="padding-left: 60px">drums_rnn_generate --config="drum_kit" <strong>--bundle_file="drums_rnn_dance_small.mag"</strong> --output_dir="generated/generated1_small"</pre>
<p>Notice how the hyperparameters are left out—this is because they are configured in the bundle file. This also means the bundle hyperparameters overrides the ones configured in the <kbd>drum_kit</kbd> configuration.</p>
<p>Now that we have our first model trained, tuned, and packaged, we'll have a look at training other models.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training MusicVAE</h1>
                </header>
            
            <article>
                
<p>Let's now train the MusicVAE model so that we can compare the sampling with an RNN generation. One thing that differs for the MusicVAE training is that the data preparation step (the <kbd>create dataset</kbd> command) doesn't exist because the data conversion is made right before the model starts training. We'll manually create the dataset using a pipeline and then start the training.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Splitting the dataset into evaluation and training sets</h1>
                </header>
            
            <article>
                
<p>Since there is no dataset creation command but we still need to split the dataset into training and evaluation data, we're going to write a pipeline to do that.</p>
<div class="mce-root packt_tip">You can find this code in the <kbd>chapter_07_example_02.py</kbd> file <span>in the source code of this chapter. There are more comments and content in the source code, so you should go check it out.</span></div>
<p>We'll also convert the note sequences into tensors, which will help us to validate the data before we start the training:</p>
<ol>
<li>First, let's write the <kbd>partition</kbd> method that will split the dataset into training and evaluation data:</li>
</ol>
<pre style="padding-left: 60px"><span>from magenta.music.protobuf.music_pb2 import NoteSequence<br/>from magenta.pipelines.dag_pipeline import DAGPipeline<br/>from magenta.pipelines.dag_pipeline import DagInput<br/>from magenta.pipelines.dag_pipeline import DagOutput<br/>from magenta.pipelines.pipeline import run_pipeline_serial<br/>from magenta.pipelines.pipeline import tf_record_iterator<br/>from magenta.pipelines.pipelines_common import RandomPartition<br/><br/>def </span>partition(config: <span>str</span>, input: <span>str</span>, output_dir: <span>str</span>, eval_ratio: <span>int</span>):<br/>  modes = [<span>"eval"</span>, <span>"train"</span>]<br/>  partitioner = <strong>RandomPartition</strong>(NoteSequence, modes, [eval_ratio])<br/>  dag = {partitioner: DagInput(NoteSequence)}<br/>  <span>for </span>mode <span>in </span>modes:<br/>    validator = <strong>TensorValidator</strong>(NoteSequence, <span>f"</span><span>{</span>mode<span>}</span><span>_TensorValidator"</span>, config)<br/>    dag[validator] = partitioner[<span>f"</span><span>{</span>mode<span>}</span><span>"</span>]<br/>    dag[DagOutput(<span>f"</span><span>{</span>mode<span>}</span><span>"</span>)] = validator<br/><br/>  pipeline = DAGPipeline(dag)<br/>  run_pipeline_serial(<br/>    pipeline, tf_record_iterator(input, pipeline.input_type), output_dir)</pre>
<p style="padding-left: 60px">We've already seen similar code in the previous chapter; we are actually reusing the <kbd>RandomPartition</kbd> class we've already covered, which will split the input into two sets using a given ratio.</p>
<ol start="2">
<li>Then, let's write the <kbd>TensorValidator</kbd> class:</li>
</ol>
<pre style="padding-left: 60px"><span>from magenta.models.music_vae.configs import CONFIG_MAP<br/>from magenta.pipelines.pipeline import Pipeline<br/><br/>class </span>TensorValidator(Pipeline):<br/><br/>  <span>def </span><span>__init__</span>(<span>self</span>, type_, name, config):<br/>    <span>super</span>(TensorValidator, <span>self</span>).<span>__init__</span>(type_, type_, name)<br/>    <span>self</span>._model = CONFIG_MAP[config]<br/>    <strong><span>self</span>._data_converter = <span>self</span>._model.data_converter</strong><br/><br/>  <span>def </span>transform(<span>self</span>, note_sequence):<br/>    tensors = <span>self</span>._data_converter.to_tensors(note_sequence)<br/>    <span>if not </span><strong>tensors.lengths</strong>:<span><br/></span><span>      </span>path = <span>str</span>(note_sequence).split(<span>'</span><span>\n</span><span>'</span>)[<span>0</span>:<span>2</span>]<br/>      <span>print</span>(<span>f"Empty tensor for </span><span>{</span>path<span>}</span><span>"</span>)<br/>      <span>return </span>[]<br/>    <span>return </span>[note_sequence]</pre>
<p style="padding-left: 60px">What is interesting here is that we are using the configuration to find the data converter (drums conversion, melody conversion, and so on) and then converting into tensors, a step that will be done before the model starts training. This step validates our input data and can help us to make statistics about the number of "valid" tensors as well as how much data we have. Unfortunately, the fact that we don't have a "create dataset" command makes it harder for us to know exactly what kind of data will be fed to the network, which is why this class is useful.</p>
<ol start="3">
<li>Finally, we'll call the <kbd>partition</kbd> method and declare some flags for the command line:</li>
</ol>
<pre style="padding-left: 60px"><span>import argparse<br/><br/>parser = argparse.ArgumentParser()<br/>parser.add_argument("--config", type=str, required=True)<br/>parser.add_argument("--input", type=str, required=True)<br/>parser.add_argument("--output_dir", type=str, required=True)<br/>parser.add_argument("--eval_ratio", type=float, default=0.1)<br/><br/>def </span>main():<br/>  args = parser.parse_args()<br/>  <strong>partition</strong>(args.config, args.input, args.output_dir, args.eval_ratio)<br/><br/><span>if </span>__name__ == <span>"__main__"</span>:<br/>  main()</pre>
<ol start="4">
<li>Now, let's create a new training directory and then call our new Python script (replacing <kbd>PATH_TO_PYTHON_SCRIPT</kbd> and <kbd>PATH_TO_DATASET_TFRECORDS</kbd> with proper values):</li>
</ol>
<pre style="padding-left: 60px"><strong>python PATH_TO_PYTHON_SCRIPT --config="cat-drums_2bar_small" --input="PATH_TO_DATASET_TFRECORDS" --output_dir="sequence_examples"</strong></pre>
<p>This will create a <kbd>sequence_examples</kbd> directory containing the <kbd>eval.tfrecords</kbd> and <kbd>train.tfrecords</kbd> datasets.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Launching the training and evaluation</h1>
                </header>
            
            <article>
                
<p>Now that we have validated our data and split it into two datasets, we can start the training.</p>
<p>Launching the training, evaluation, and TensorBoard is similar to the previous sections:</p>
<pre># Start the training job<br/>music_vae_train --config="cat-drums_2bar_small" --run_dir="logdir/run1" --mode="train" --examples_path="sequence_examples/<strong>train.tfrecord</strong>"<br/><br/># Start the evaluation job<br/>music_vae_train --config="cat-drums_2bar_small" --run_dir="logdir/run1" --mode="eval" --examples_path="sequence_examples/<strong>eval.tfrecord</strong>"<br/><br/># Start the TensorBoard<br/>tensorboard --logdir=logdir</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<p>Like for the previous models, you can pass hyperparameters using the <kbd>--hparams=FLAGS</kbd> flag. Here, we are using the "small" configuration since MusicVAE models can get big in terms of size pretty fast. A small model is enough for good performance, for example, the Magenta pre-trained drum model uses the <kbd>cat-drums_2bar_small</kbd> configuration.</p>
<p>When training Music VAE, we'll also want to tune the following two hyperparameters: <kbd>free_bits</kbd> and <kbd>max_beta</kbd>. By increasing <kbd>free_bits</kbd> or decreasing <kbd>max_beta</kbd>, we are decreasing the effect of the KL loss, resulting in a model that is better at reconstruction, with potentially worse random samples. See the previous chapter, <a href="838da33e-26a9-4701-bfd3-5014dfff4146.xhtml">Chapter 4</a>, <em>Latent Space Interpolation with MusicVAE</em>, if you don't remember how <strong>Kulback-Leibler</strong> (<strong>KL</strong>) divergence affects the model performance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Distributing a trained model</h1>
                </header>
            
            <article>
                
<p>Unfortunately, for MusicVAE, we cannot create a Magenta bundle. The simplest way to distribute a TensorFlow checkpoint is to copy the checkpoint files and zip them for the transfer:</p>
<ol>
<li>First, let's copy the corresponding files (replace <kbd>STEP</kbd> with the checkpoint step you want to keep):</li>
</ol>
<pre style="padding-left: 60px"><strong>mkdir "trained/cat-drums_2bar_small"</strong><br/><strong>cp logdir/run1/train/model.ckpt-STEP* "trained/cat-drums_2bar_small"</strong></pre>
<p style="padding-left: 60px">You should now have three files in the <kbd>complete/cat-drums_2bar_small</kbd> directory. Remember, TensorFlow checkpoints should be loaded using their prefix, for example, <kbd>model.ckpt-157780</kbd>.</p>
<ol start="2">
<li>Enter the following to use the checkpoint in a generation (replace <kbd>STEP</kbd> with the checkpoint you want to use):</li>
</ol>
<pre style="padding-left: 60px">music_vae_generate --config="cat-drums_2bar_small" --checkpoint_file="trained/cat-drums_2bar_small/<strong>model.ckpt-STEP</strong>"</pre>
<p>Remember that checkpoints do not contain information about the changes you've made to the hyperparameters (unlike Magenta bundles), so you'll need to pass the same hyperparameters you've used during training each time you use it.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<div class="packt_tip">Some hyperparameters can be changed. For example, there is no sense in using a batch size of 512 when you are sampling (unless you are sampling 512 sequences at once, perhaps), but it could be the value you've used for training.<br/>
<br/>
You will want to keep everything related to the TensorFlow graph, which means the network size and anything related to encoding and decoding.</div>
<p>Creating a configuration for this specific training is probably the easiest way of keeping track of the hyperparameters that were used.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Training other models</h1>
                </header>
            
            <article>
                
<p>We won't be training every model here, but training other models should be fairly similar to the one we've shown. The same model tuning, regarding overfitting, for example, can be applied to other models. Refer to the README files of the model you want to train for more information.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using Google Cloud Platform</h1>
                </header>
            
            <article>
                
<p>Using a cloud computing provider is useful to offload computing to faster machines. It can also be used if we want to make multiple runs at the same time. For example, we could try fixing exploding gradients by launching two runs: one with a lower learning rate and one with a lower gradient clipping. We could spawn two different VMs, each training its own model, and see which performs better.</p>
<p>We are going to use Google Cloud Platform (GCP), but other cloud providers, such as Amazon AWS or Microsoft Azure, will also work. We'll go through the different steps needed to train a Melody RNN model on the piano jazz dataset from the previous chapter, including the GCP account configuration and VM instance creation.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating and configuring an account</h1>
                </header>
            
            <article>
                
<p>First, head to <a href="https://console.cloud.google.com">console.cloud.google.com</a> and create a new Google account (or use an existing one). Once in GCP, follow these steps:</p>
<ol>
<li>If this is your first time logging in, you will need to create a new project, which you can call <kbd>Magenta</kbd>. If not, find your current project at the top of the screen, and create a new one if you want.</li>
</ol>
<ol start="2">
<li>Then, we'll need to set up quotas, since when a new account is created, it cannot create VM instances with GPUs. On the left, go to <strong>IAM &amp; Admin</strong> &gt; <strong>Quotas</strong>, and find the <strong>GPUs (all regions)</strong> quotas by searching <kbd>GPU</kbd> in the <strong>Metrics</strong> field. Click on the checkbox, then <strong>Edit</strong>, and change the quota to another value, like 5. The quota modification will take a bit of time to get validated.</li>
<li>Finally, we'll need to set up billing. On the left, go to <strong>Billing</strong> and then follow the instructions to add a <strong>Billing Account</strong>.</li>
</ol>
<p>Our account being properly setup, we can now create a new VM instance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing an SSH key (optional)</h1>
                </header>
            
            <article>
                
<p>Using an SSH key is useful to connect to the VM instance from a local Terminal. This is an optional step since, in GCP, you can connect to the VM instance using a Terminal in a browser, which works pretty well, but with really slow upload and download speeds.</p>
<p>If you already have an SSH key ready, you can skip this step. If you are unsure, check the file at <kbd>~/.ssh/id_rsa.pub</kbd>.</p>
<p>On Linux and macOS, you can generate a new SSH key by entering the following in a Terminal:</p>
<pre><strong>ssh-keygen</strong></pre>
<p>This will save a key in <kbd>~/.ssh/id_rsa.pub</kbd>. On Windows, the easiest way to do this is to install Git Bash (<a href="https://git-scm.com/download/win">git-scm.com/download/win</a>), which contains two commands we'll use—<kbd>ssh-keygen</kbd> and <kbd>scp</kbd>, which we'll be using in the next sections.</p>
<p>Once generated, the public key looks like this:</p>
<pre><strong>ssh-rsa AAAA... user@host</strong></pre>
<p>The <kbd>user</kbd> part before the host is important since it will be the user to provide to GCP when you log in.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Creating a VM instance from a Tensforflow image</h1>
                </header>
            
            <article>
                
<p>Now, we return to GCP then, on the menu on the left, we go to <strong>Compute Engine</strong>:</p>
<ol>
<li>Once in <strong>Compute</strong> <strong>Engine</strong>, choose <strong>I</strong><strong>mages</strong> from the left menu.</li>
<li>In the <strong>Filter Images</strong> search, type <kbd>TensorFlow</kbd> and find the most recent image. At the time of writing, the image is called <strong>c3-deeplearning-tf-1-15-cu100-20191112</strong>.</li>
<li>Choose the image and then choose <strong>Create instance</strong>; you will see the create instance screen:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/155d37d9-c178-45eb-9bb5-ccfcc9e69c28.png"/></p>
<p style="padding-left: 60px">We'll now fill the information as in the preceding diagram:</p>
<ol start="1">
<li>In <strong>Name</strong>, use <kbd>Magenta</kbd>.</li>
<li>In <strong>Region</strong> and <strong>Zone</strong>, find a place that is near you.</li>
<li>In <strong>Machine</strong> <strong>Type</strong>, use at least <strong>n1-standard-4</strong>, a 4 core CPU with 15 GB of RAM.</li>
<li>In <strong>CPU platform and GPU</strong>, click on <strong>Add GPU</strong> and choose a <strong>GPU type</strong>, using at least an <strong>NVIDIA Tesla K80</strong>.</li>
</ol>
<p class="mce-root"/>
<div class="packt_tip">Depending on the chosen region and the current availability, you'll have different GPUs available. The NVIDIA Tesla K80 GPUs have an average computing power (0.45 global step/second on Melody RNN) and the P100 GPUs are almost twice as powerful (0.75 global step/second on Melody RNN). As a comparison, an entry-level gaming GPU such as the RTX 2060 makes 0.6 global step/second on Melody RNN.</div>
<p style="padding-left: 60px">Now, let's go on to the disk content:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/95410c03-38bb-410c-a776-1663a11e0a93.png"/></p>
<p class="mce-root"/>
<p>We will initialize the instance using the following steps:</p>
<ol start="1">
<li>The boot disk should be already filled with a <strong>Deep Learning Image</strong> of 50 GB.</li>
<li>After expanding the <strong>Management, security, disks, networking, sole tenancy</strong> section, in the <strong>Security</strong> tab, paste your public SSH key (if you have one). The resulting user (in this case, <strong>user</strong>) will be shown on the left.</li>
<li>Check the resulting price in the upper-right corner, which is <strong>about $1.269 hourly</strong> for this machine. Note that we are billed for the machine uptime. If we don't use the machine, we won't be billed for it, so we'll need to close it when we are finished.</li>
<li>Click on <strong>Create</strong>.</li>
</ol>
<p>In the <strong>VM instances</strong> menu on the left, you should see the newly created VM.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Initializing the VM</h1>
                </header>
            
            <article>
                
<p>Now that we have a new VM to work with, we need to install all of the required software. We can refer to <a href="c5602f6c-c094-42f2-936f-98746cf04a49.xhtml">Chapter 1</a>, <em>Introduction on Magenta and Generative Art</em>, for the detailed installation instructions, but we'll also put the main commands here.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing the NVIDIA CUDA drivers</h1>
                </header>
            
            <article>
                
<p>Fortunately, the VM image we are using does a lot of the installation for us. Let's first connect and install the required drivers:</p>
<ol>
<li>First, we need to connect to the VM. If we don't have an SSH key, we use the <strong>SSH</strong> button on the right of the VM instance, which should start a new browser Terminal. If we have an SSH key, we can use the following command, on Linux, macOS, and Windows (using Git Bash):</li>
</ol>
<pre style="padding-left: 60px"><strong>ssh USER@IP</strong></pre>
<p style="padding-left: 60px">Here, we have to replace <kbd>USER</kbd> with the user in our SSH key and <kbd>IP</kbd> with the <span class="packt_screen">external IP</span> shown in the VM instance page.</p>
<ol start="2">
<li>The VM will greet us with the following message, which we answer with <kbd>y</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong>This VM requires Nvidia drivers to function correctly. Installation takes ~1 minute.</strong><br/><strong><span>Would you like to install the Nvidia driver? [y/n] y</span></strong></pre>
<p class="mce-root"/>
<p style="padding-left: 60px">The NVIDIA drivers (CUDA drivers and cuDNN), in the proper versions for TensorFlow, should get installed. Unfortunately, there is a problem with the cuDNN versions, so we'll have to manually reinstall it.</p>
<ol start="3">
<li>Download the latest <span class="packt_screen">cuDNN a.b.c for CUDA 10.0</span> at <a href="https://developer.nvidia.com/rdp/cudnn-download">developer.nvidia.com/rdp/cudnn-download</a> (download the full cuDNN Library) on your local machine.</li>
<li>Transfer the cuDNN archive to the VM instance. If we don't have an SSH key, we will use the interface to transfer the file (in the upper-right corner). If we have an SSH key, we use our Terminal:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>scp PATH_TO_CUDNN_ARCHIVE USER@IP:</span></strong></pre>
<p style="padding-left: 60px">We'll have to replace <kbd>PATH_TO_CUDNN_ARCHIVE</kbd>, <kbd>USER</kbd>, and <kbd>IP</kbd> with proper values. The archive should now be in our home directory in the VM instance.</p>
<ol start="5">
<li>Now, log in to the machine using SSH (we don't have to do this if we are using the browser Terminal):</li>
</ol>
<pre style="padding-left: 60px"><strong><span>ssh USER@IP</span></strong></pre>
<p style="padding-left: 60px">We'll have to replace <kbd>USER</kbd> and <kbd>IP</kbd> with proper values.</p>
<ol start="6">
<li>Extract the archive:</li>
</ol>
<pre style="padding-left: 60px"><span># On the VM instance<br/><strong>tar -xzvf CUDNN_ARCHIVE_NAME</strong></span></pre>
<p style="padding-left: 60px">We'll have to replace <kbd>CUDNN_ARCHIVE_NAME</kbd> with the archive name.</p>
<ol start="7">
<li>Now let's override the current cuDNN installation with the new versions:</li>
</ol>
<pre style="padding-left: 60px"><span># On the VM instance</span><span><br/><strong>sudo cp cuda/include/cudnn.h /usr/local/cuda/include</strong><br/><strong>sudo cp cuda/lib64/libcudnn* /usr/local/cuda/lib64</strong><br/><strong>sudo chmod a+r /usr/local/cuda/include/cudnn.h /usr/local/cuda/lib64/libcudnn*</strong></span></pre>
<p>Now that we have the CUDA drivers with the proper versions up and running, let's install the prerequisite software and Magenta GPU.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Installing Magenta GPU</h1>
                </header>
            
            <article>
                
<p>Some packages are not installed by default on this image, so we'll have to install them manually:</p>
<ol>
<li>First, we'll install some audio dependencies:</li>
</ol>
<pre style="padding-left: 60px"><span># On the VM instance<br/><strong>sudo apt install libasound2-dev libsndfile-dev</strong></span></pre>
<ol start="2">
<li>Then, we'll install Miniconda (<a href="https://docs.conda.io/en/latest/miniconda.html">docs.conda.io/en/latest/miniconda.html</a>) and create a new environment:</li>
</ol>
<pre style="padding-left: 60px"><span># On the VM instance<br/><strong>conda create -n magenta python=3.6</strong><br/><strong>conda activate magenta</strong></span></pre>
<ol start="3">
<li>Finally, let's install the Magenta GPU:</li>
</ol>
<pre style="padding-left: 60px"><span># On the VM instance<br/><strong>pip install magenta-gpu</strong></span></pre>
<p>We are now good to go, so let's start a training job on our new VM instance.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Launching the training</h1>
                </header>
            
            <article>
                
<p>To start the training, we'll need to first transfer the dataset to the VM instance:</p>
<ol>
<li>To transfer a dataset to the VM instance, first zip it, and then use the Terminal browser interface to upload it if you aren't using an SSH key. If you are using an SSH key, you can use <kbd>scp</kbd> to do that:</li>
</ol>
<pre style="padding-left: 60px"><strong>scp PATH_TO_DATASET USER@IP:</strong></pre>
<p style="padding-left: 60px">We'll have to replace <kbd>PATH_TO_DATASET</kbd>, <kbd>USER</kbd>, and <kbd>IP</kbd> with proper values. The archive should now be in our home directory in the VM instance.</p>
<ol start="2">
<li>Once uploaded, we unzip the archive, and then we start the training process as we would on a local machine, for example, for the Melody RNN model:</li>
</ol>
<pre style="padding-left: 60px"><span># On the VM instance<br/><strong>melody_rnn_create_dataset --config="attention_rnn" --input="PATH_TO_NOTE_SEQUENCE_TFRECORDS" --output_dir="sequence_examples" --eval_ratio=0.10</strong><br/><br/></span><span># On the VM instance<br/><strong>melody_rnn_train --config="attention_rnn" --run_dir="logdir/run1" --sequence_example_file="sequence_examples/training_melodies.tfrecord" --hparams="batch_size=128,rnn_layer_sizes=[128,128]" --num_training_steps=20000</strong><br/></span></pre>
<p style="padding-left: 60px">We'll have to replace <kbd>PATH_TO_NOTE_SEQUENCE_TFRECORDS</kbd> with a proper value.</p>
<ol start="3">
<li>To start the evaluation, we have to start a new Terminal; SSH again on the VM instance, and then we can launch the evaluation:</li>
</ol>
<pre style="padding-left: 60px"><span># On the VM instance</span><span><br/><strong>CUDA_VISIBLE_DEVICES="" melody_rnn_train --config="attention_rnn" --run_dir="logdir/run1" --sequence_example_file="sequence_examples/eval_melodies.tfrecord" --hparams="batch_size=62,rnn_layer_sizes=[128,128]" --num_training_steps=20000 --eval</strong></span></pre>
<ol start="4">
<li>We can also see TensorBoard by using connection-forwarding in SSH. In a new Terminal, use the following command to connect to the VM instance:</li>
</ol>
<pre style="padding-left: 60px"><strong>ssh -L 16006:127.0.0.1:6006 USER@IP</strong></pre>
<p style="padding-left: 60px">We'll have to replace <kbd>USER</kbd> and <kbd>IP</kbd> with proper values. This command redirects the local port <kbd>16006</kbd> to port <kbd>6006</kbd> in the VM instance.</p>
<ol start="5">
<li>Then, in the previous Terminal, we can launch TensorBoard:</li>
</ol>
<pre style="padding-left: 60px"># On the VM instance<br/><strong>tensorboard --logdir=logdir</strong></pre>
<p style="padding-left: 60px">The TensorBoard can be opened locally by using the <kbd>127.0.0.1:16006</kbd> address in the browser. Once the training is finished, we can zip the training folder on the VM instance, and then get it back using <kbd>scp</kbd> or the browser Terminal.</p>
<ol start="6">
<li>Once this is finished, don't forget to <strong>stop the running instance</strong>. Remember, we are billed on usage on GCP; we don't want to keep our VM instance running without a reason.</li>
</ol>
<p>At the time of writing, the cost of training a Melody RNN model for 20,000 steps on a P100 GPU is around $5.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we used the datasets that we prepared in the previous chapter to train the Magenta models on different instruments and genres. We first compared different models and configurations for specific use cases and then showed how to create a new one if necessary.</p>
<p>Then, we launched different training runs and looked at how to tune the model for better training. We showed how to launch the training and evaluation jobs and how to check the resulting metrics on the TensorBoard. Then, we saw a case of overfitting and explained how to fix overfitting and underfitting. We also showed how to define the proper network size and hyperparameters, by looking at problems such as incorrect batch size, memory errors, a model not converging, and not having enough training data. Using our newly trained model, we've generated sequences and showed how to package and handle checkpoints.</p>
<p>Finally, we've shown how to use GCP to train our model in the cloud on powerful machines. We've introduced how to create a Magenta VM instance and how to launch training, evaluation, and TensorBoard on it.</p>
<p>This chapter marks the end of the training content for this book. In the next chapters, we'll be looking at parts that are outside the core features of Magenta, such as Magenta.js, and making Magenta interact with a <strong>Digital Audio Workstation</strong> (<strong>DAW</strong>).</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>Write a new configuration for the Drums RNN model that uses an attention length of 64 and that encodes only the snares and bass drums, but inverted.</li>
<li>We have a model that underfits: what does this mean and how do we fix the problem?</li>
<li>We have a model that overfits: what <span>does this mean</span> and how do we fix the problem?</li>
<li>What is a technique that makes sure that, for a given training run, we stop at the optimum?</li>
<li>Why might increasing batch size make the model worse in terms of performance? Will it make it worse in terms of efficiency or training time?</li>
</ol>
<ol start="6">
<li>What is a good network size?</li>
<li>Does limiting the value of the error derivative before backpropagation help or worsen the problem of exploding gradients? What is another solution to that problem?</li>
<li>Why is using a cloud provider useful to train our models?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>How to Choose Loss Functions When Training Deep Learning Neural Networks</strong>: A very thorough article on how to write a loss function (<a href="https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/">machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/</a>)</li>
<li><strong>How to Avoid Overfitting in Deep Learning Neural Networks</strong>: An article on underfitting and overfitting and its solutions (<a href="https://machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/">machinelearningmastery.com/introduction-to-regularization-to-reduce-overfitting-and-improve-generalization-error/</a>)</li>
<li><strong>How to Avoid Exploding Gradients With Gradient Clipping</strong>: An article on exploding gradients and how to fix it using gradient clipping (<a href="https://machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/">machinelearningmastery.com/how-to-avoid-exploding-gradients-in-neural-networks-with-gradient-clipping/</a>)</li>
<li><strong>Use Weight Regularization to Reduce Overfitting of Deep Learning Models</strong>: An article on weight regularization, which helps reduce overfitting (<a href="https://machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/">machinelearningmastery.com/weight-regularization-to-reduce-overfitting-of-deep-learning-models/</a>)</li>
<li><strong>A Gentle Introduction to Dropout for Regularizing Deep Neural Networks</strong>: An article on dropout, which helps to reduce overfitting (<a href="https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/">machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/</a>)</li>
<li><strong>A Gentle Introduction to Early Stopping to Avoid Overtraining Neural Networks</strong>: An article on early stopping, which helps to reduce overfitting (<a href="https://machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/">machinelearningmastery.com/early-stopping-to-avoid-overtraining-neural-network-models/</a>)</li>
<li><strong>On Large-Batch Training for Deep Learning: Generalization Gap and Sharp Minima</strong>: A paper (2017) explaining why a larger batch size might lead to a less efficient network (<a href="https://arxiv.org/abs/1609.04836">arxiv.org/abs/1609.04836</a>)</li>
</ul>


            </article>

            
        </section>
    </body></html>