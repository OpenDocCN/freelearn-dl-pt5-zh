- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rule-Based Techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Rule-based techniques** are a very important and useful tool in **natural
    language processing** (**NLP**). Rules are used to examine text and decide how
    it should be analyzed in an all-or-none fashion, as opposed to the statistical
    techniques we will be reviewing in later chapters. In this chapter, we will discuss
    how to apply rule-based techniques to NLP. We will look at examples such as regular
    expressions, syntactic parsing, and semantic role assignment. We will primarily
    use the NLTK and spaCy libraries, which we have seen in earlier chapters.'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Rule-based techniques
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Why use rules?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring regular expressions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Sentence-level analysis – syntactic parsing and semantic role assignment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Rule-based techniques
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Rule-based techniques in NLP are, as the name suggests, based on rules written
    by human developers, as opposed to machine-learned models derived from data. Rule-based
    techniques were, for many years, the most common approach to NLP, but as we saw
    in [*Chapter 7*](B19005_07.xhtml#_idTextAnchor144), rule-based approaches have
    largely been superseded by numerical, machine-learned approaches for the overall
    design of most NLP applications. There are many reasons for this; for example,
    since rules are written by humans, it is possible that they might not cover all
    situations if the human developer has overlooked something.
  prefs: []
  type: TYPE_NORMAL
- en: However, for practical applications, rules can be very useful, either by themselves
    or, more likely, along with machine-learned models.
  prefs: []
  type: TYPE_NORMAL
- en: The next section will discuss the motivations for using rules in NLP applications.
  prefs: []
  type: TYPE_NORMAL
- en: Why use rules?
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Rules are a useful approach in one or more of the following situations:'
  prefs: []
  type: TYPE_NORMAL
- en: The application you are developing has a requirement to analyze fixed expressions
    that include thousands or even millions of variants when it would be extremely
    difficult to provide enough data to learn a machine model. These kinds of expressions
    include numbers, monetary amounts, dates, and addresses, for example. It is hard
    for systems to learn models when the data is so diverse. Moreover, it is usually
    not necessary, as rules to analyze these expressions are not difficult to write
    because their formats are very structured. For both of these reasons, a rule-based
    approach is a simpler solution for recognizing fixed expressions.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Very little training data is available for the application, and creating new
    data would be expensive. For example, annotating new data might require very specialized
    expertise. Although there are now techniques (such as few-shot or zero-shot learning)
    that can adapt large pre-trained models to cover specialized domains, if the domain-specific
    data is very different from the original training data in terms of syntax or vocabulary,
    it will be difficult for the adaptation process to work well. Medical reports
    and air traffic control messages are examples of these kinds of data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There are existing, well-tested rules and libraries available that can easily
    be used in new applications, such as the Python `datetime` package for recognizing
    dates and times.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal is to bootstrap a machine-learned model by preliminary annotation of
    a corpus. Corpus annotation is needed in preparation for using that corpus as
    training data for a machine learning model, or for using the corpus as a *gold
    standard* in NLP system evaluation. In this process, the data is first annotated
    by applying some hand-written rules. Then, the resulting annotated corpus is usually
    reviewed and corrected by human annotators, since it is likely to contain errors.
    Even though the corpus requires a review process, an initial annotation by a rule-based
    system will save time over manual annotation from scratch.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The application needs to find named entities from a fixed, known set.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The results have to be very precise – for example, grammar checking, proofreading,
    language learning, and authorship studies.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A quick prototype is needed in order to test downstream processing, and the
    data collection and model training stages needed for machine learning would take
    too much time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We will start by looking at regular expressions, a very common technique for
    analyzing text that contains well-understood patterns.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring regular expressions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Regular expressions** are a widely used rule-based technique that is often
    used for recognizing fixed expressions. By **fixed expressions**, we mean words
    and phrases that are formed according to their own internal rules, which are largely
    different from the normal patterns of the language.'
  prefs: []
  type: TYPE_NORMAL
- en: One type of fixed expression is *monetary amounts*. There are only a few variations
    in formats for monetary amounts – the number of decimal places, the symbol for
    the type of currency, and whether the numbers are separated by commas or periods.
    The application might only have a requirement to recognize specific currencies,
    which would simplify the rules further. Other common fixed expressions include
    *dates*, *times*, *telephone numbers*, *addresses*, *email addresses*, *measurements*,
    and *numbers*. Regular expressions in NLP are most frequently used in preprocessing
    text that will be further analyzed with other techniques.
  prefs: []
  type: TYPE_NORMAL
- en: Different programming languages have slightly different formats for regular
    expressions. We will be using the Python formats defined at [https://docs.python.org/3/library/re.html](https://docs.python.org/3/library/re.html)
    and available in the Python `re` library. We will not define regular expression
    syntax here because there are many resources on the web that describe regular
    expression syntax, including the Python documentation, and we don’t need to duplicate
    that. You might find the information at [https://www.h2kinfosys.com/blog/nltk-regular-expressions/](https://www.h2kinfosys.com/blog/nltk-regular-expressions/)
    and [https://python.gotrained.com/nltk-regex/](https://python.gotrained.com/nltk-regex/)
    useful for getting into the details of regular expressions in NLTK.
  prefs: []
  type: TYPE_NORMAL
- en: We will start by going over the basics of operating on strings with regular
    expressions, followed by some tips for making it easier to apply and debug regular
    expressions.
  prefs: []
  type: TYPE_NORMAL
- en: Recognizing, parsing, and replacing strings with regular expressions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The simplest use of regular expressions is to simply note that a match occurred.
    What we want to do after a fixed expression has been matched depends on the goal
    of the application. In some applications, all we want to do is recognize that
    a fixed expression has occurred or did not occur. This can be useful, for example,
    in validating user input in web forms, so that users can correct invalid address
    formats. The following code shows how to recognize a US address with regular expressions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: In other cases, we might want to parse the expression and assign meanings to
    the components – for example, in a date, it could be useful to recognize a month,
    a day, and a year. In still other cases, we might want to replace the expression
    with another expression, delete it, or normalize it so that all occurrences of
    the expression are in the same form. We could even want to do a combination of
    these operations. For example, if the application is classification, it is likely
    that we only need to know whether or not the regular expression occurred; that
    is, we don’t need its content. In that case, we can replace the expression with
    a `class` token such as `DATE`, so that a sentence such as *We received the package
    on August 2, 2022* becomes *We received the package on DATE*. Replacing the whole
    expression with a `class` token can also be used for redacting sensitive text
    such as social security numbers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding code shows how to use regular expressions to match patterns in
    text and shows the code to confirm a match. However, this example just shows how
    to confirm that a match exists. We might want to do other things, such as replace
    the address with a class label, or label the matched portion of a string. The
    following code shows how to use the regular expression `sub` method to substitute
    a class label for an address:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Another useful operation is to label the whole expression with a semantic label
    such as `address`, as shown in the following code. This code shows how to add
    a label to the address. This enables us to identify US addresses in text and do
    tasks such as counting them or extracting all the addresses from texts:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'The result of running the preceding code is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Finally, regular expressions are useful if we want to remove the whole match
    from the text – for example, to remove HTML markup.
  prefs: []
  type: TYPE_NORMAL
- en: General tips for using regular expressions
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Regular expressions can easily become very complex and difficult to modify and
    debug. They can also easily fail to recognize some examples of what they’re supposed
    to recognize and falsely recognize what they’re not supposed to recognize. While
    it is tempting to try to match the regular expression so that it recognizes exactly
    what it is supposed to recognize and nothing else, this can make the expression
    so complicated that it is difficult to understand. Sometimes, it can be better
    to miss a few edge cases to keep the expression simple.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we find that an existing regular expression is failing to find some expressions
    that we want to capture, or incorrectly finding expressions that we don’t want
    to capture, it can sometimes be difficult to revise the existing expression without
    breaking some things that used to work. Here are a few tips that can make regular
    expressions easier to work with:'
  prefs: []
  type: TYPE_NORMAL
- en: Write down what you want the regular expression to match first (such as *any
    two consecutive uppercase alphabetic characters*). This will be helpful in both
    clarifying what you’re trying to do as well as in helping catch any cases that
    you might have overlooked.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Break complex expressions into components and test each component independently
    before putting them together. Besides helping with debugging, the component expressions
    can potentially be reused in other complex expressions. We saw this in the first
    code block in the previous section with components such as `street_name_re`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use existing tested regular expressions for common expressions, for example,
    the Python `datetime` package (see [https://docs.python.org/3/library/datetime.html](https://docs.python.org/3/library/datetime.html)),
    before trying to write your own regular expressions. They have been well tested
    over many years by many developers.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The next two sections cover specific ways to analyze two of the most important
    aspects of natural language: words and sentences.'
  prefs: []
  type: TYPE_NORMAL
- en: The next section will start this topic by talking about analyzing individual
    words.
  prefs: []
  type: TYPE_NORMAL
- en: Word-level analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section will discuss two approaches to analyzing words. The first one,
    lemmatization, involves breaking words down into their components in order reduce
    the variation in texts. The second one discusses some ideas for making use of
    hierarchically organized semantic information about the meanings of words in the
    form of ontologies.
  prefs: []
  type: TYPE_NORMAL
- en: Lemmatization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In our earlier discussion of preprocessing text in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107),
    we went over the task of **lemmatization** (and the related task of stemming)
    as a tool for regularizing text documents so that there is less variation in the
    documents we are analyzing. As we discussed, the process of lemmatization converts
    each word in the text to its root word, discarding information such as plural
    endings like *-s* in English. Lemmatization also requires a dictionary, because
    the dictionary supplies the root words for the words being lemmatized. We used
    Princeton University’s **WordNet** ([https://wordnet.princeton.edu/](https://wordnet.princeton.edu/))
    as a dictionary when we covered lemmatization in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107).
  prefs: []
  type: TYPE_NORMAL
- en: We will use WordNet’s semantic information about the relationships among words
    in the next section, where we discuss ontologies and their applications.
  prefs: []
  type: TYPE_NORMAL
- en: Ontologies
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`entity`.'
  prefs: []
  type: TYPE_NORMAL
- en: The ontology in *Figure 3**.2* is part of the **WordNet** ontology for English
    and a number of other languages. These hierarchical relationships are sometimes
    called **is a** relationships. For example, *an airplane is a vehicle*. In this
    example, we say that *vehicle* is a **superordinate** term and *airplane* is a
    **subordinate** term. WordNet uses some of its own terminology. In WordNet terminology,
    **hypernym** is the same thing as a superordinate term, and **hyponym** is the
    same thing as a subordinate term.
  prefs: []
  type: TYPE_NORMAL
- en: WordNet also includes many other semantic relationships, such as synonymy and
    *part-whole*. For example, we can find out that a wing is part of an airplane
    from WordNet. In addition, WordNet also includes part-of-speech information, and
    you will recall that we used this part-of-speech information in [*Chapter 5*](B19005_05.xhtml#_idTextAnchor107)
    for part-of-speech tagging texts in preparation for lemmatization.
  prefs: []
  type: TYPE_NORMAL
- en: There are other ontologies besides WordNet, and you can even construct your
    own ontologies with tools such as Stanford University’s Protégé ([https://protege.stanford.edu/](https://protege.stanford.edu/)).
    However, WordNet is a good way to get started.
  prefs: []
  type: TYPE_NORMAL
- en: 'How can we make use of ontologies such as WordNet in NLP applications? Here
    are a few ideas:'
  prefs: []
  type: TYPE_NORMAL
- en: Develop a writing tool that helps authors find synonyms, antonyms, and definitions
    of words they would like to use.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Count the number of mentions of different categories of words in texts. For
    example, you might be interested in finding all mentions of vehicles. Even if
    the text actually says *car* or *boat*, you could tell that the text mentions
    a vehicle by looking for words with *vehicle* as their hypernym.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Generate additional examples of training data for machine learning by substituting
    different words with the same superordinate term in different sentence patterns.
    For example, suppose we have a chatbot that provides advice about cooking. It
    would probably get questions such as *How can I tell whether a pepper is ripe?*,
    or *Can I freeze tomatoes?* There are hundreds of types of food that could be
    substituted for *pepper* and *tomatoes* in those questions. It would be very tedious
    to create training examples for all of them. To avoid this, you could find all
    of the different types of vegetables in WordNet and generate training data by
    putting them into sentence templates to create new sentences.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s see an example of the previous strategy.
  prefs: []
  type: TYPE_NORMAL
- en: 'You probably recall from earlier mentions of WordNet that it is included in
    NLTK, so we can import it and ask for the list of senses of *vegetable* (**synsets**)
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll then see that there are two *senses*, or meanings, of *vegetable*, and
    we can ask for their definitions in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The format of the sense names such as `vegetable.n.01` should be interpreted
    as `word` and `part-of-speech` (*n* means *noun* here), followed by the word’s
    order in the list of senses. We print the definitions of each of the two senses
    so that we can see what the WordNet senses mean. The resulting definitions are
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The first sense refers to the part that we eat, and the second sense refers
    to the plants whose parts we eat. If we are interested in cooking, we probably
    want the first sense of *vegetable* as *food*. Let’s get the list of all the vegetables
    in the first sense, using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The code goes through the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Collect all the different types of the first sense of *vegetable* (the hyponyms)
    and store them in the `word_list` variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Iterate through the list of words, collect the lemma for each word, and store
    the lemmas in the `simple_names` variable.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print the words.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'We can then generate some sample data by filling in a text template with each
    word, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code shows the first few sentences that we can generate from the
    text frame and the list of vegetables. Of course, in a real application, we would
    want to have multiple text frames to get a good variety of sentences.
  prefs: []
  type: TYPE_NORMAL
- en: At the beginning of this section, we listed a few ways to apply ontologies in
    NLP applications; you can probably come up with more if you think of different
    ways that you could make use of the meanings of words to solve problems in natural
    language applications.
  prefs: []
  type: TYPE_NORMAL
- en: However, words don’t occur in isolation; they are combined with other words
    to create sentences with richer and more complex meanings. The next section will
    move on from analyzing words to the analysis of entire sentences, which we will
    analyze both syntactically and semantically.
  prefs: []
  type: TYPE_NORMAL
- en: Sentence-level analysis
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Sentences can be analyzed in terms of their **syntax** (the structural relationships
    among parts of the sentence) or their **semantics** (the relationships among the
    meanings of the parts of the sentence). We’ll look at both of these types of analysis
    next. Recognizing syntactic relationships is useful on its own for applications
    such as grammar checking (does the subject of the sentence agree with the verb?
    Is the correct form of the verb being used?), while recognizing semantic relationships
    on their own is useful for applications such as finding the components of a request
    in chatbots. Recognizing both syntactic and semantic relationships together is
    an alternative to statistical methods in almost any NLP application.
  prefs: []
  type: TYPE_NORMAL
- en: Syntactic analysis
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The syntax of sentences and phrases can be analyzed in a process called `parse`
    package, which includes a variety of parsing algorithms that you can explore.
    For our examples in this section, we will use the chart parser in the `nltk.parse.ChartParser`
    class, which is a common and basic approach.
  prefs: []
  type: TYPE_NORMAL
- en: Context-free grammars and parsing
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: A very common way to define the rules for syntactic parsing is **context-free
    grammars** (**CFGs**). CFGs can be used in chart parsing as well as many other
    parsing algorithms. You may be familiar with this format because it is widely
    used in computer science for defining formal languages, such as programming languages.
    CFGs consist of a set of *rules*. Each rule consists of a **left-hand side** (**LHS**)
    and a **right-hand side** (**RHS**), typically separated by a symbol, such as
    an arrow. The rule is interpreted to mean that the single symbol on the LHS is
    made up of the components of the RHS.
  prefs: []
  type: TYPE_NORMAL
- en: For example, the context-free rule `S -> NP VP` states that a sentence (`S`)
    consists of a noun phrase (**NP**), followed by a **verb phrase** (**VP**). An
    NP can consist of a **determiner** (**Det**) such as *an*, *my*, or *the*, followed
    by one or two **nouns** (**Ns**) such as *elephant*, possibly followed by a **prepositional
    phrase** (**PP**), or just a **pronoun** (**Pro**), and so on. Every rule must
    be in turn defined with another rule, until the rules end in words (or, more generally,
    *terminal symbols*), which do not appear on the LHS of any rule.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following shows the code for creating a CFG for a few rules of English.
    These are *constituency rules*, which show how the parts of the sentence are related
    to each other. There is another commonly used format, *dependencies*, which shows
    how the words are related to each other, which we will not explore in this book
    because the constituency rules are sufficient to illustrate the basic concepts
    of syntactic grammar and syntactic parsing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: This grammar is only able to parse a few sentences, such as *the children watched
    the movie in the family room*. For example, it would not be able to parse a sentence
    *the children slept* because, in this grammar, the VP has to include an object
    or prepositional phrase in addition to the verb. A full CFG for English would
    be much larger and more complex than the one in the preceding code. It’s also
    worth pointing out that the NLTK rules can be annotated with probabilities that
    indicate the likelihood of each alternative on the RHS.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, *rule 4* (`Pro ![](img/011.png) ''I'' |''you''|''we''`) in the
    preceding code could have probabilities for the relative likelihoods of *I*, *you*,
    and *we*. In practice, this will result in more accurate parses, but it does not
    affect the examples we’ll show in this chapter. *Table 8.1* summarizes some CFG
    terminology:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Symbol** | **Meaning** | **Example** |'
  prefs: []
  type: TYPE_TB
- en: '| S | Sentence | The children watched the movie |'
  prefs: []
  type: TYPE_TB
- en: '| NP | Noun phrase | The children |'
  prefs: []
  type: TYPE_TB
- en: '| VP | Verb phrase | Watched the movie |'
  prefs: []
  type: TYPE_TB
- en: '| PP | Prepositional phrase | In the family room |'
  prefs: []
  type: TYPE_TB
- en: '| Pro | Pronoun | I, we, you, they, he, she, it |'
  prefs: []
  type: TYPE_TB
- en: '| Det | Determiner or article | The, a |'
  prefs: []
  type: TYPE_TB
- en: '| V | Verb | Watched, saw |'
  prefs: []
  type: TYPE_TB
- en: '| N | Noun | Children, movie, elephant, family, room |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 – Meanings of CFG terms for the grammar in the CFG code block
  prefs: []
  type: TYPE_NORMAL
- en: '*Table 8.2* summarizes some syntactic conventions used in NLTK CFGs:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Symbol** | **Meaning** |'
  prefs: []
  type: TYPE_TB
- en: '| -> | Separates the LHS from the RHS |'
  prefs: []
  type: TYPE_TB
- en: '| &#124; | Separates alternate possibilities for RHSs that expand the LHS |'
  prefs: []
  type: TYPE_TB
- en: '| Single quotes | Indicates a word; that is, a terminal symbol |'
  prefs: []
  type: TYPE_TB
- en: '| Initial capitalization | Indicates a syntactic category; that is, a non-terminal
    symbol, which is expected to be defined by additional rules |'
  prefs: []
  type: TYPE_TB
- en: Table 8.2 – CFG syntax
  prefs: []
  type: TYPE_NORMAL
- en: 'We can parse and visualize the sentence *the children watched the movie in
    the family room* with the grammar in the previous code block using the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'We can view the results of parsing in different ways – for example, as a bracketed
    text format, as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Notice that the parse directly reflects the grammar: the overall result is
    called *S*, because it came from the first rule in the grammar, `S -> NP VP`.
    Similarly, *NP* and *VP* are connected directly to *S*, and their child nodes
    are listed in parentheses after them.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding format is useful for possible later stages of processing that
    may need to be computer-readable; however, it is a little bit difficult to read.
    *Figure 8**.1* shows a conventional tree diagram of this parse, which is easier
    to view. As in the case of the preceding text parse, you can see that it aligns
    directly with the grammar. The words, or terminal symbols, all appear at the bottom,
    or *leaves*, of the tree:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – Constituency tree for “the children watched the movie in the
    family room”](img/B19005_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – Constituency tree for “the children watched the movie in the family
    room”
  prefs: []
  type: TYPE_NORMAL
- en: You can try parsing other sentences with this grammar, and you can also try
    modifying the grammar. For example, try adding a grammar rule that will enable
    the grammar to parse sentences with verbs that are not followed by NPs or PPs,
    such as *the* *children slept*.
  prefs: []
  type: TYPE_NORMAL
- en: Semantic analysis and slot filling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The previous sections on regular expressions and syntactic analysis dealt only
    with the structure of sentences, not their meaning. A syntactic grammar like the
    one shown in the preceding section can parse nonsense sentences such as *the movie
    watched the children in the room room* as long as they match the grammar. We can
    see this in *Figure 8**.2*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.2 – Constituency tree for “the movie watched the children in the
    room room”](img/B19005_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – Constituency tree for “the movie watched the children in the room
    room”
  prefs: []
  type: TYPE_NORMAL
- en: In most applications, however, we don’t only want to find the syntactic structure
    of sentences; we also want to extract part or all of their meanings. The process
    of extracting meaning is called **semantic analysis**. The particular sense of
    *meaning* will vary depending on the application. For example, in many of the
    applications we’ve worked on so far in this book, the only meaning that needed
    to be derived from a document was its overall classification. This was the case
    in the movie review data – the only meaning that we wanted to get from the document
    was the positive or negative sentiment. The statistical methods that we’ve looked
    at in previous chapters are very good at doing this kind of coarse-grained processing.
  prefs: []
  type: TYPE_NORMAL
- en: However, there are other applications in which it’s necessary to get more detailed
    information about the relationships between items in the sentence. While there
    are machine learning techniques for getting fine-grained information (and we will
    be discussing them in *Chapters 9*, *10*, and *11*), they work best with large
    amounts of data. If there is less data available, sometimes rule-based processing
    can be more effective.
  prefs: []
  type: TYPE_NORMAL
- en: Basic slot filling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: For the next examples, we will be looking in detail at a technique often used
    in interactive applications, **slot filling**. This is a common technique used
    in voicebots and chatbots, although it is also used in non-interactive applications
    such as *information extraction*.
  prefs: []
  type: TYPE_NORMAL
- en: As an example, consider a chatbot application that helps a user find a restaurant.
    The application is designed to expect the user to offer some search criteria,
    such as the type of cuisine, the atmosphere, and the location. These criteria
    are the *slots* in the application. For example, the user might say, *I’d like
    to find a nice Italian restaurant near here*. The overall user goal, **Restaurant
    Search**, is the *intent*. We will be focusing on identifying slots in this chapter
    but will discuss intents in more detail in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The design for this application is shown in *Figure 8**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Slots for a restaurant search application](img/B19005_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Slots for a restaurant search application
  prefs: []
  type: TYPE_NORMAL
- en: In processing the user’s utterance, the system has to identify what slots the
    user has specified, and it has to extract their values. This is all the information
    the system needs to help the user find a restaurant, so anything else in the sentence
    is normally ignored. This can lead to errors if the part of the sentence that
    is ignored is actually relevant, but in most cases, the approach works. This leads
    to a useful processing strategy in many applications, where the system only looks
    for information that is relevant to its task. This is in contrast to the syntactic
    parsing process that we reviewed previously, which required the system to analyze
    the entire sentence.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the rule-based matcher in spaCy to create an application that analyzes
    a user’s utterance to find values for these slots. The basic approach is to define
    patterns for the system to find words that specify a slot and to define corresponding
    tags that label the values with their slot names. The following code shows how
    to find some of the slots shown in *Figure 8**.3* in sentences (we won’t show
    the code for all of the slots in order to keep the example short):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code starts by importing spaCy and the information that we need
    for processing text in the English language. The rule processor is called `ruler`
    and is added as a stage in the NLP pipeline. We then define three cuisines (a
    real application would likely have many more) and label them `CUISINE`. Similarly,
    we define patterns for recognizing price ranges, atmospheres, and locations. These
    rules state that if the user’s sentence contains a specific word or phrase, such
    as `near here`, the `LOCATION` slot should be filled with that word or phrase.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to add the patterns to the rule processor (`ruler`) and then
    run the NLP processor on a sample sentence, *can you recommend a casual italian
    restaurant within walking distance?*. This process applies the rules to the document,
    which results in a set of labeled slots (which are called `doc.ents`. By printing
    the slots and values, we can see that the processor found three slots, `ATMOSPHERE`,
    `CUISINE`, and `LOCATION`, with values of `casual`, `Italian`, and `walking distance`,
    respectively. By trying other sentences, we can confirm the following important
    characteristics of this approach to slot filling:'
  prefs: []
  type: TYPE_NORMAL
- en: Parts of the sentence that don’t match a pattern, such as *can you recommend*,
    are ignored. This also means that the non-matching parts of the sentence can be
    nonsense, or they could actually be important to the meaning, but when they’re
    ignored, the system can potentially make a mistake. For example, if the utterance
    were *can you recommend a casual non-Italian restaurant within walking distance*,
    the system would incorrectly think that the user wanted to find an Italian restaurant
    by using these rules. Additional rules can be written to take these kinds of cases
    into account, but in many applications, we just want to accept some inaccuracies
    as the price of keeping the application simple. This has to be considered on an
    application-by-application basis.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Slots and values will be recognized wherever they occur in the sentences; they
    don’t have to occur in any particular order.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a particular slot doesn’t occur in the sentence, that doesn’t cause any problems.
    It will just be left out of the resulting list of entities.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If a slot occurs more than once, all occurrences will be recognized.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The names of the slot labels are up to the developer; they don’t have to be
    specific values. For example, we could have said `TYPE_OF_FOOD` instead of `CUISINE`,
    and the processing would be the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can use the spaCy visualizer, `displacy`, to get a clearer visualization
    of the result using the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the result in *Figure 8**.4*, where the text and its slots and values
    are highlighted:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Slot visualization with displacy](img/B19005_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Slot visualization with displacy
  prefs: []
  type: TYPE_NORMAL
- en: Because our slots are custom (that is, not built into spaCy), to show colored
    slots, we have to define colors for the different slots (or `ents`) and assign
    the colors to the slots. We can then visualize the slots and values using a different
    color for each slot. The colors are defined in the `colors` variable in the preceding
    code. We can assign any colors to the slots that we find useful. The colors don’t
    need to be different, but it is normally most helpful if they are, and if they
    are fairly distinctive. The values of the colors in this example are hexadecimal
    codes, which have standard color interpretations. [https://www.color-hex.com/](https://www.color-hex.com/)
    is a useful website that shows the hexadecimal values for many colors.
  prefs: []
  type: TYPE_NORMAL
- en: Using the spaCy id attribute
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You have probably noticed that some of the slot values we have defined in this
    example mean the same thing – for example, `close by` and `near here`. If the
    slots and values are passed on to a later stage in processing such as database
    lookup, that later stage will have to have code for handling both `close by` and
    `near here`, even though the database lookup will be the same. This will complicate
    the application, so we would like to avoid it. spaCy provides another attribute
    of `ent`, `ent_id_`, for this purpose. This `id` attribute can be assigned in
    the patterns that find the slots, along with the label and pattern. This is accomplished
    by specifying an `id` attribute in the pattern declarations, which is a modification
    of the location patterns in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'If we print the slots, values, and IDs that result from *can you recommend
    a casual italian restaurant close by*, the result is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that the ID of `close by` is `nearby`, based on the `close`
    `by` pattern.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we can see that the first three location patterns, which
    have similar meanings, have all been assigned the ID `nearby`. With this ID, the
    next stage in processing only needs to receive the `ent_id_` value, so it only
    has to handle `nearby`, and it doesn’t have to have additional cases for `close
    by` and `near me`.
  prefs: []
  type: TYPE_NORMAL
- en: Note that in this example, the results for the `CUISINE` and `ATMOSPHERE` slots
    have empty IDs, because these were not defined in the `CUISINE` and `ATMOSPHERE`
    patterns. It is nevertheless good practice to define IDs for all patterns, if
    there are any IDs, in order to keep the results uniform.
  prefs: []
  type: TYPE_NORMAL
- en: Also note that these patterns reflect some design decisions about what phrases
    are synonymous and, therefore, should have the same ID, and which phrases are
    not synonymous and should have different IDs.
  prefs: []
  type: TYPE_NORMAL
- en: In the preceding code, we can see that `short walk` doesn’t have the same ID
    as `near me`, for example. The design decision that was made here was to consider
    `short walk` and `near me` to have different meanings, and therefore to require
    different handling in the later stages of the application. Making decisions about
    which values are and are not synonymous will depend on the application and how
    rich the information available in the backend application is.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have described several useful rule-based approaches to NLP. *Table 8.3*
    summarizes these rule-based techniques by listing three important properties of
    rule-based techniques:'
  prefs: []
  type: TYPE_NORMAL
- en: The format of the rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The types of processing that apply the rules to text
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How the results are represented
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B19005_08_Table_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Table 8.3 – Formats, processing, and results for rule-based techniques
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we’ve learned several important skills that make use of rules
    for processing natural language.
  prefs: []
  type: TYPE_NORMAL
- en: We’ve learned how to apply regular expressions to identify fixed-format expressions
    such as numbers, dates, and addresses. We’ve also learned about the uses of rule-based
    Python tools such as the NLTK syntactic parsing libraries for analyzing the syntactic
    structure of sentences and how to apply them. Finally, we’ve learned about rule-based
    tools for semantics analysis such as spaCy’s `entity_ruler` for analyzing the
    slot-value semantics of sentences.
  prefs: []
  type: TYPE_NORMAL
- en: The next chapter, [*Chapter 9*](B19005_09.xhtml#_idTextAnchor173), will begin
    the discussion on machine learning by introducing statistical techniques such
    as classification with Naïve Bayes and **term frequency-inverse document frequency**
    (**TF-IDF**), **support vector machines** (**SVMs**), and conditional random fields.
    In contrast to the rule-based approaches we have discussed in this chapter, statistical
    approaches are based on models that are learned from training data and then applied
    to new, previously unseen data. Unlike the all-or-none rule-based systems, statistical
    systems are based on probabilities.
  prefs: []
  type: TYPE_NORMAL
- en: As we explore these techniques, we’ll also consider how they can be combined
    with the rule-based techniques discussed in this chapter to create even more powerful
    and effective systems.
  prefs: []
  type: TYPE_NORMAL
