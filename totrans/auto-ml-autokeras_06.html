<html><head></head><body>
		<div id="_idContainer057">
			<h1 id="_idParaDest-62"><em class="italic"><a id="_idTextAnchor063"/>Chapter 4</em>: Image Classification and Regression Using AutoKeras</h1>
			<p>In this chapter, we will focus on the use of AutoKeras applied to images. In <a href="B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with AutoKeras</em>, we got our first contact with <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) applied to images, by creating two models (a classifier and a regressor) that recognized handwritten digits. We will now create more complex and powerful image recognizers, examine how they work, and see how to fine-tune them to improve their performance.</p>
			<p>After reading this chapter, you will be able to create your own image models and apply them, to solve a wide range of problems in the real world.</p>
			<p>As we discussed in <a href="B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with AutoKeras</em>, the most suitable models for recognizing images use a type of neural network called a <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>). For the two examples that we will see in this chapter, AutoKeras will also choose CNNs for the creation of its models. So, let's see in a little more detail what these types of neural networks are and how they work.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Understanding CNNs—what are these neural networks and how do they work?</li>
				<li>Creating a CIFAR-10 image classifier</li>
				<li>Creating and fine-tuning a powerful image classifier</li>
				<li>Creating an image regressor to find out the age of people</li>
				<li>Creating and fine-tuning a powerful image regressor</li>
			</ul>
			<h1 id="_idParaDest-63"><a id="_idTextAnchor064"/>Technical requirements </h1>
			<p>All coding examples in this book are available as Jupyter Notebooks that can be downloaded from the following link: <a href="https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras">https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras</a>.</p>
			<p>As code cells can be executed, each Notebook can be self-installable by adding a code snippet with the requirements you need. For this reason, at the beginning of each notebook there is a code cell for environmental setup, which installs AutoKeras and its dependencies.</p>
			<p>So, to run the coding examples, you only need a computer with Ubuntu/Linux as the operating system and you can install the Jupyter Notebook with this command line:</p>
			<p class="source-code">$ apt-get install python3-pip jupyter-notebook</p>
			<p>Alternatively, you can also run these notebooks using Google Colaboratory, in which case you will only need a web browser—see the <em class="italic">AutoKeras with Google Colaboratory</em> section in <a href="B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with AutoKeras</em>, for more details. Furthermore, in the <em class="italic">Installing AutoKeras</em> section, you will also find other installation options. Let's get started by understanding CNNs in detail.</p>
			<h3>Understanding CNNs</h3>
			<p>A CNN is a type of neural network, inspired <a id="_idIndexMarker190"/>by the functioning of neurons in the visual cortex of a biological brain.</p>
			<p>These types of networks perform very well in solving computer vision problems such as image classification, object detection, segmentation, and so on.</p>
			<p>The following screenshot shows how a CNN recognizes a cat:</p>
			<div>
				<div id="_idContainer041" class="IMG---Figure">
					<img src="image/B16953_04_01.jpg" alt="Figure 4.1 – How a CNN recognizes a cat&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1 – How a CNN recognizes a cat</p>
			<p>But why do these CNNs work so well, compared to a classical fully connected model? To answer this, let's dive into what the convolutional and pooling layers do.</p>
			<h3>Convolutional layer</h3>
			<p>The key building block<a id="_idIndexMarker191"/> in a CNN is the convolutional layer, which uses a window (kernel) to scan an image and perform transformations on it to detect patterns.</p>
			<p>A kernel is nothing more than a simple neural network fed by the pixel matrix of the scanned window that outputs a vector of numbers, which we will use as filters.</p>
			<p>Let's imagine a convolutional layer with many <a id="_idIndexMarker192"/>small square templates (called kernels) that go through an image and look for patterns. When the square of the input image matches the kernel pattern, the kernel returns a positive value; otherwise, it returns <strong class="source-inline">0</strong> or less.</p>
			<p>The following screenshot shows how a convolutional layer processes an image:</p>
			<div>
				<div id="_idContainer042" class="IMG---Figure">
					<img src="image/B16953_04_02.jpg" alt="Figure 4.2 – How a convolutional layer processes an image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2 – How a convolutional layer processes an image</p>
			<p>Once we have the filters, we have to reduce their dimensions using a pooling operation, which is explained next.</p>
			<h3>Pooling layer</h3>
			<p>The function <a id="_idIndexMarker193"/>of the pooling layer is to progressively reduce the size of the input features matrix to reduce the number of parameters and calculations in the network. The most common form of pooling is max pooling, which performs downscaling by applying a maximum filter to non-overlapping subregions of the input features matrix.</p>
			<p>The following screenshot provides an example of max pooling:</p>
			<p class="figure-caption"> </p>
			<div>
				<div id="_idContainer043" class="IMG---Figure">
					<img src="image/B16953_04_03.jpg" alt="Figure 4.3 – Max pooling example"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3 – Max pooling example</p>
			<p>In the preceding <a id="_idIndexMarker194"/>screenshot, we can see an example of a max pooling operation on a features matrix. In the case of an image, this matrix would be made up of the pixel values of the image.</p>
			<p>Applying this operation reduces the computational cost by reducing the number of features to process, and it also helps prevent overfitting. Next, we will see how the convolutional and pooling layers are combined in a CNN.</p>
			<h3>CNN structure</h3>
			<p>Usually, a CNN is made<a id="_idIndexMarker195"/> up of a series of convolutional layers, followed by a pooling layer (downscaling). This combination is repeated several times, as we can see in the following screenshot example:</p>
			<div>
				<div id="_idContainer044" class="IMG---Figure">
					<img src="image/B16953_04_04.jpg" alt="Figure 4.4 – Example of a CNN pipeline"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4 – Example of a CNN pipeline</p>
			<p>In this process, the first layer detects simple features such as the outlines of an image, and the second layer begins to detect higher-level features. In the intermediate layers, it is already capable of detecting more complex shapes, such as the nose or eyes. In the final layers, it is usually able to differentiate human faces.</p>
			<p>This seemingly<a id="_idIndexMarker196"/> simple repetition process is extremely powerful, detecting features of a slightly higher order than its predecessor at each step and generating astonishing predictions.</p>
			<h2 id="_idParaDest-64"><a id="_idTextAnchor065"/>Surpassing classical neural networks</h2>
			<p>A classical neural <a id="_idIndexMarker197"/>network uses fully connected (dense) layers as the main feature transformation operations, whereas a CNN uses convolution and pooling layers (Conv2D).</p>
			<p>The main differences between a fully connected layer and a convolutional layer are outlined here:  </p>
			<ul>
				<li>Fully connected layers learn global <a id="_idIndexMarker198"/>patterns in their input feature space (for example, in the case of a digit from the <strong class="bold">Modified National Institute of Standards and Technology</strong> (<strong class="bold">MNIST</strong>) dataset, seen in the example from <a href="B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with AutoKeras</em>, the input feature space would be all the pixels from the image). </li>
				<li>On the other hand, the convolution layers learn local patterns—in the case of images, patterns found in small two-dimensional windows that run through the image.</li>
			</ul>
			<p>In the following screenshot, we can see how these little windows detect local patterns such as lines, edges, and so on:</p>
			<div>
				<div id="_idContainer045" class="IMG---Figure">
					<img src="image/B16953_04_05.jpg" alt="Figure 4.5 – Visual representation of pattern extraction by a convolutional network"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5 – Visual representation of pattern extraction by a convolutional network</p>
			<p>The convolution <a id="_idIndexMarker199"/>operation performs a transformation of the input image through a window (2D matrix) that scans it, generating a new image with <a id="_idIndexMarker200"/>different features. Each of these generated images is called a <strong class="bold">filter</strong>, and each filter contains different patterns extracted from the original image (edges, axes, straight lines, and so on). </p>
			<p>The set of filters created in each intermediate layer of a CNN is called a feature map, which is a matrix of numbers of dimensions <em class="italic">r x c x n</em>, where <em class="italic">r</em> and <em class="italic">c</em> are rows and columns and <em class="italic">n</em> is the number of filters.</p>
			<p>Basically, these feature maps are the parameters that CNNs learn.</p>
			<p>As we were already able to see when viewing the architecture of the MNIST classifier from <a href="B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with AutoKeras</em>, CNNs stack several convolutional layers (Conv2D) combined with pooling layers (MaxPooling2D). The task of the latter consists of reducing the dimensions of the filters, keeping the most relevant values. This helps clean up noise and reduces training time for the model.</p>
			<p>Now, it's time to implement some practical examples. Let's start with an image classifier for a well-known <a id="_idIndexMarker201"/>dataset.</p>
			<h1 id="_idParaDest-65"><a id="_idTextAnchor066"/>Creating a CIFAR-10 image classifier</h1>
			<p>The model we are going to create will classify<a id="_idIndexMarker202"/> images from a dataset called <strong class="bold">Canadian Institute for Advanced Research, 10 classes</strong> (<strong class="bold">CIFAR-10</strong>). It contains 60,000 <strong class="source-inline">32x32</strong> <strong class="bold">red, green, blue</strong> (<strong class="bold">RGB</strong>) colored images, classified into 10 different classes. It is a collection of images that is commonly <a id="_idIndexMarker203"/>used to train ML and computer vision algorithms.</p>
			<p>Here are the classes in the dataset: </p>
			<ul>
				<li><strong class="source-inline">airplane </strong></li>
				<li><strong class="source-inline">automobile </strong></li>
				<li><strong class="source-inline">bird </strong></li>
				<li><strong class="source-inline">cat </strong></li>
				<li><strong class="source-inline">deer </strong></li>
				<li><strong class="source-inline">dog </strong></li>
				<li><strong class="source-inline">frog </strong></li>
				<li><strong class="source-inline">horse </strong></li>
				<li><strong class="source-inline">ship</strong></li>
				<li><strong class="source-inline">truck </strong></li>
			</ul>
			<p>In the next screenshot, you can see some random image samples found in the CIFAR-10 dataset:</p>
			<div>
				<div id="_idContainer046" class="IMG---Figure">
					<img src="image/B16953_04_06.jpg" alt="Figure 4.6 – CIFAR-10 image samples&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6 – CIFAR-10 image samples</p>
			<p>This a problem considered already solved. It is relatively easy to achieve a classification accuracy close to 80%. For better performance, we must use deep learning CNNs with which a<a id="_idIndexMarker204"/> classification precision greater than 90% can be achieved in the test dataset. Let's see how to implement it with AutoKeras.</p>
			<p>This is a classification task, so we can use the <strong class="source-inline">ImageClassifier</strong> class. This class generates and tests different models and hyperparameters, returning an optimal classifier to categorize each image with its corresponding class.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The notebook with the complete source code can be found at <a href="https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_Cifar10.ipynb">https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_Cifar10.ipynb</a>.</p>
			<p>Let's now have a look at the relevant cells of the notebook in detail, as follows:</p>
			<ul>
				<li><strong class="bold">Installing AutoKeras</strong>: The <a id="_idIndexMarker205"/>following snippet at the top of the notebook is responsible for installing AutoKeras and its dependencies using the <strong class="source-inline">pip</strong> package manager:<p class="source-code">!pip3 install autokeras</p></li>
				<li><strong class="bold">Importing needed packages</strong>: Load AutoKeras and some more used packages—such as <strong class="source-inline">matplotlib</strong>, a Python plotting library that we will use to plot some digit representations, and CIFAR-10, which contains the categorized images dataset. The code to import the packages is shown here:<p class="source-code">import autokeras as ak</p><p class="source-code">import matplotlib.pyplot as plt</p><p class="source-code">from tensorflow.keras.datasets import cifar10</p></li>
				<li><strong class="bold">Getting the CIFAR-10 dataset</strong>: We have to first load the CIFAR-10 dataset in memory and have a quick look at the dataset shape, as follows:<p class="source-code">(x_train, y_train), (x_test, y_test) = cifar10.load_data()</p><p class="source-code">print(x_train.shape)</p><p class="source-code">print(x_test.shape)</p><p>Here is the output of the preceding code:</p><p class="source-code">Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz</p><p class="source-code">170500096/170498071 [==============================] - 11s 0us/step</p><p class="source-code">(50000, 32, 32, 3)</p><p class="source-code">(10000, 32, 32, 3)</p></li>
			</ul>
			<p>Although it is a well-known machine learning dataset, it is always important to ensure that the data is distributed evenly, to avoid surprises. This can be easily done using <strong class="source-inline">numpy</strong> functions, as shown in the following code block:</p>
			<p class="source-code">import numpy as np </p>
			<p class="source-code">train_histogram = np.histogram(y_train)</p>
			<p class="source-code">test_histogram = np.histogram(y_test)</p>
			<p class="source-code">_, axs = plt.subplots(1, 2)</p>
			<p class="source-code">axs[0].set_xticks(range(10))</p>
			<p class="source-code">axs[0].bar(range(10), train_histogram[0])</p>
			<p class="source-code">axs[1].set_xticks(range(10))</p>
			<p class="source-code">axs[1].bar(range(10), test_histogram[0])</p>
			<p class="source-code">plt.show()</p>
			<p>The <a id="_idIndexMarker206"/>samples are perfectly balanced, as you can see in the following screenshot:</p>
			<div>
				<div id="_idContainer047" class="IMG---Figure">
					<img src="image/B16953_04_07.jpg" alt="Figure 4.7 – Train and test dataset histograms&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7 – Train and test dataset histograms</p>
			<p>Now that we are sure that our dataset is correct, it's time to create our image classifier.</p>
			<h1 id="_idParaDest-66"><a id="_idTextAnchor067"/>Creating and fine-tuning a powerful image classifier</h1>
			<p>We will now use <a id="_idIndexMarker207"/>the AutoKeras <strong class="source-inline">ImageClassifier</strong> class to find the best classification model. Just for this little example, we set <strong class="source-inline">max_trials</strong> (the maximum<a id="_idIndexMarker208"/> number of different Keras models to try) to <strong class="source-inline">2</strong>, and we do not set the <strong class="source-inline">epochs</strong> parameter so that it will use an adaptive number of epochs automatically. For real use, it is recommended to set a large number of trials. The code is shown here:</p>
			<p class="source-code">clf = ak.ImageClassifier(max_trials=2)</p>
			<p>Let's run the training to search for the optimal classifier for the CIFAR-10 training dataset, as follows:</p>
			<p class="source-code">clf.fit(x_train, y_train)</p>
			<p>Here is the output:</p>
			<div>
				<div id="_idContainer048" class="IMG---Figure">
					<img src="image/B16953_04_08.jpg" alt="Figure 4.8 – Notebook output of image classifier training&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8 – Notebook output of image classifier training</p>
			<p>The previous output <a id="_idIndexMarker209"/>shows that the accuracy of the training dataset is increasing. </p>
			<p>As it has to process thousands of <a id="_idIndexMarker210"/>color images, the models that <a id="_idIndexMarker211"/>AutoKeras will generate will be more expensive to train, so this process will take hours, even using <strong class="bold">graphics processing units</strong> (<strong class="bold">GPUs</strong>). We have limited the search to five architectures (<strong class="source-inline">max_trials = 5</strong>). Increasing this number would give us a more accurate model, although it would also take longer to finish.</p>
			<h2 id="_idParaDest-67"><a id="_idTextAnchor068"/>Improving the model performance</h2>
			<p>If we need more <a id="_idIndexMarker212"/>precision in less time, we can fine-tune our model using an advanced AutoKeras feature that allows you to customize your search space.</p>
			<p>By using <strong class="source-inline">AutoModel</strong> with <strong class="source-inline">ImageBlock</strong> instead of <strong class="source-inline">ImageClassifier</strong>, we can create high-level configurations, such as <strong class="source-inline">block_type</strong> for the type of neural network to look for. We can also perform data normalization or data augmentation.</p>
			<p>If we have knowledge of deep learning and have faced this problem before, we can design a suitable architecture such as an <strong class="source-inline">EfficientNet</strong>-based image classifier, for instance, which is a deep residual learning architecture for image recognition. </p>
			<p>See the following example for more details:</p>
			<p class="source-code">input_node = ak.ImageInput()</p>
			<p class="source-code">output_node = ak.ImageBlock(</p>
			<p class="source-code">             block_type="efficient",</p>
			<p class="source-code">             augment=False)(input_node)</p>
			<p class="source-code">output_node = ak.ClassificationHead()(output_node)</p>
			<p class="source-code">clf = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=2)</p>
			<p class="source-code">clf.fit(x_train, y_train)</p>
			<p>In the<a id="_idIndexMarker213"/> preceding code block, we have<a id="_idIndexMarker214"/> done the following with the settings:</p>
			<ul>
				<li>With <strong class="source-inline">block_type = "efficient"</strong>, AutoKeras will only explore <strong class="source-inline">EfficientNet</strong> architectures. </li>
				<li>Initializing <strong class="source-inline">augment = True</strong> means we want to do data augmentation, a technique to create new artificial images from the originals. Upon activating it, AutoKeras will perform a series of transformations in the original image, as translations, zooms, rotations, or flips.</li>
			</ul>
			<p>You can also not <a id="_idIndexMarker215"/>specify these arguments, in which case these different options would be tuned automatically.</p>
			<p>You can see more details about the <strong class="source-inline">EfficientNet</strong> function here: </p>
			<ul>
				<li><a href="https://keras.io/api/applications/efficientnet/">https://keras.io/api/applications/efficientnet/</a></li>
				<li><a href="https://keras.io/api/applications/resnet/">https://keras.io/api/applications/resnet/</a></li>
			</ul>
			<h2 id="_idParaDest-68"><a id="_idTextAnchor069"/>Evaluating the model with the test set</h2>
			<p>After training, it is time<a id="_idIndexMarker216"/> to measure the actual prediction of our model using the reserved test dataset. In this way, we can contrast the good results obtained from the training set with a dataset never seen before. To do this, we run the following code:</p>
			<p class="source-code">metrics = clf.evaluate(x_test, y_test)</p>
			<p class="source-code">print(metrics)</p>
			<p>Here is the output:</p>
			<p class="source-code">313/313 [==============================] - 34s 104ms/step - loss: 0.5260 - accuracy: 0.8445</p>
			<p class="source-code">[0.525996744632721, 0.8445000052452087]</p>
			<p>We can see here that prediction accuracy has a margin to improve using our test dataset (84.4%), although it's a pretty decent score for just a few hours of training; but just increasing the trials, we have achieved 98% of precision training for the first model (<strong class="source-inline">ImageClassifier</strong>) and running during one day in Google Colaboratory.</p>
			<p>Once we have created and trained our classifier model, let's see how it predicts on a subset of test samples. To do this, we run the following code:</p>
			<p class="source-code">import matplotlib.pyplot as plt</p>
			<p class="source-code">labelNames = ["airplane", "automobile", "bird", "cat", "deer", "dog", "frog", "horse", "ship", "truck"]</p>
			<p class="source-code">fig = plt.figure(figsize=[18,6])</p>
			<p class="source-code">for i in range(len(predicted_y)):</p>
			<p class="source-code">    ax = fig.add_subplot(2, 5, i+1)</p>
			<p class="source-code">    ax.set_axis_off()</p>
			<p class="source-code">    ax.set_title('Prediced: %s, Real: %s' % (labelNames[int(predicted_y[i])],labelNames[int(y_test[i])]))</p>
			<p class="source-code">    img = x_test[i]</p>
			<p class="source-code">    ax.imshow(img)</p>
			<p class="source-code">plt.show()</p>
			<p>Here is the<a id="_idIndexMarker217"/> output:</p>
			<div>
				<div id="_idContainer049" class="IMG---Figure">
					<img src="image/B16953_04_09.jpg" alt="Figure 4.9 – Samples with their predicted and true labels"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9 – Samples with their predicted and true labels</p>
			<p>We can see that all the predicted samples match their true value, so our classifier has predicted correctly. Now, let's take a look inside the classifier to understand how it is working.</p>
			<h2 id="_idParaDest-69"><a id="_idTextAnchor070"/>Visualizing the model</h2>
			<p>Now, we can see a little<a id="_idIndexMarker218"/> summary with the architecture of the best generated model found (the one with 98% accuracy), and we will explain the reason why its performance is so good. Run the following code to see the summary:</p>
			<p class="source-code">model = clf.export_model()</p>
			<p class="source-code">model.summary()</p>
			<p>Here is the output:</p>
			<div>
				<div id="_idContainer050" class="IMG---Figure">
					<img src="image/B16953_04_10.jpg" alt="Figure 4.10 – Best model architecture summary"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10 – Best model architecture summary</p>
			<p>The key layer here is the <strong class="source-inline">efficientnetb7</strong> layer, which implements a cutting-edge architecture created by Google. Today, <strong class="source-inline">EfficientNet</strong> models are the best choice for image classification because this is a recent architecture that not only focuses on improving accuracy <a id="_idIndexMarker219"/>but also on the efficiency of the models so that they achieve higher precision and better efficiency over existing convolutional network-based architectures, reducing<a id="_idIndexMarker220"/> parameter sizes and <strong class="bold">floating-point operations per second</strong> (<strong class="bold">FLOPS</strong>) by an order of magnitude. However, we didn't need to know anything about it because AutoKeras automatically chose it for us.</p>
			<p>Let's see how the blocks are connected to each other in a more visual way, as follows:</p>
			<div>
				<div id="_idContainer051" class="IMG---Figure">
					<img src="image/B16953_04_11.jpg" alt="Figure 4.11 – Best model architecture visualization"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11 – Best model architecture visualization</p>
			<p>As we explained in <a href="B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029"><em class="italic">Chapter 2</em></a>, <em class="italic">Getting Started with AutoKeras</em>, each block represents a layer, and the output of each is connected to the input of the next, except the first block (whose input is the image) and the last block (whose output is the prediction). The blocks before the <strong class="source-inline">efficientnetb7</strong> layer are all data-preprocessing blocks and they are in charge of adapting the image to a suitable format for this <strong class="source-inline">EfficientNet</strong> block, as well as generating extra images through augmentation techniques.</p>
			<p>Now is the time to<a id="_idIndexMarker221"/> tackle a non-classification problem. In the following practical example, we are going to create a human-age predictor based on a set of celebrity data—a fun tool that could make anyone blush.</p>
			<h1 id="_idParaDest-70"><a id="_idTextAnchor071"/>Creating an image regressor to find out the age of people</h1>
			<p>In this section, we will<a id="_idIndexMarker222"/> create a model that will find out the age of a person from an image of their face. For this, we will train the model with a dataset of faces<a id="_idIndexMarker223"/> extracted from images of celebrities in <strong class="bold">Internet Movie Database</strong> (<strong class="bold">IMDb</strong>).</p>
			<p>As we want to approximate the age, we will use an image regressor for this task.</p>
			<p>In the next screenshot, you can see some samples taken from this dataset of celebrity faces:</p>
			<div>
				<div id="_idContainer052" class="IMG---Figure">
					<img src="image/B16953_04_12.jpg" alt="Figure 4.12 – A few image samples from IMDb faces dataset"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.12 – A few image samples from IMDb faces dataset</p>
			<p>This notebook with the complete source code can be found here: <a href="https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_CelebrityAgeDetector.ipynb">https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter04/Chapter4_CelebrityAgeDetector.ipynb</a>.</p>
			<p>We will now explain the relevant code cells of the notebook in detail, as follows:</p>
			<ul>
				<li><strong class="bold">Installing AutoKeras</strong>: As with the previous example, the first code cell at the top of the notebook is responsible for installing AutoKeras and its dependencies, using the <strong class="source-inline">pip</strong> package manager. The code is shown here:<p class="source-code">!pip3 install autokeras</p></li>
				<li><strong class="bold">Importing needed packages</strong>: We now load AutoKeras and some more used packages, such as matplotlib, a Python plotting library that we will use to plot some picture samples and the categories distribution. The code to do this is shown here:<p class="source-code">import autokeras as ak</p><p class="source-code">import matplotlib.pyplot as plt</p></li>
				<li><strong class="bold">Getting the IMDb faces dataset</strong>: Before training, we have to download the IMDb cropped faces dataset that contains the images of each face, as well as metadata with<a id="_idIndexMarker224"/> the age tags.<p>The following command lines are idempotent—they download and extract data only if it does not already exist:</p><p class="source-code">!wget -nc https://data.vision.ee.ethz.ch/cvl/rrothe/imdb-wiki/static/imdb_crop.tar</p><p class="source-code">!tar --no-overwrite-dir -xf imdb_crop.tar</p><p>Here is the output of the preceding code:</p><p class="source-code">Resolving data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)... 129.132.52.162</p><p class="source-code">Connecting to data.vision.ee.ethz.ch (data.vision.ee.ethz.ch)|129.132.52.162|:443... connected.</p><p class="source-code">HTTP request sent, awaiting response... 200 OK</p><p class="source-code">Length: 7012157440 (6.5G) [application/x-tar]</p><p class="source-code">Saving to: 'imdb_crop.tar'</p><p class="source-code">imdb_crop.tar       100%[===================&gt;]   6.53G  27.7MB/s    in 3m 59s  </p><p class="source-code">2020-12-20 00:05:48 (28.0 MB/s) - 'imdb_crop.tar' saved [7012157440/7012157440]</p></li>
				<li><strong class="bold">Preprocessing the dataset</strong>: Before feeding our model with this dataset, we have some issues to resolve, as follows:<p>a. The metadata parameters are in a <strong class="source-inline">MatLab</strong> file.</p><p>b. The age is not in the params—it has to be calculated.</p><p>c. The images are<a id="_idIndexMarker225"/> not homogeneous—they have different dimensions and colors.</p><p>To resolve these issues, we have created the following utility functions:</p><p>a. <strong class="source-inline">imdb_meta_to_df(matlab_filename)</strong>: This converts the IMDb MatLab file to a pandas DataFrame and calculates the age. </p><p>b. <strong class="source-inline">normalize_dataset(df_train_set)</strong>: This returns a tuple of normalized images (resized to <strong class="source-inline">128x128</strong> and converted to grayscale) and ages converted to integers.</p></li>
			</ul>
			<p>In the notebook, you will find more details about how these functions are working.</p>
			<p>Let's now see how to use them, as follows:</p>
			<p class="source-code">df = imdb_meta_to_df("imdb_crop/imdb.mat")</p>
			<p>In the previous code snippet, we used the <strong class="source-inline">imdb_meta_to_df</strong> function to convert the <strong class="source-inline">imdb</strong> metadata information stored in a MatLab file to a Pandas DataFrame.</p>
			<p>The DataFrame contains a lot of images; to make the training faster, we will use only a part of them to create the datasets, as follows:</p>
			<p class="source-code">train_set = df.sample(10000)</p>
			<p class="source-code">test_set = df.sample(1000)</p>
			<p>Now, we create the final datasets with normalized images and ages, as follows:</p>
			<p class="source-code">train_imgs, train_ages = normalize_dataset(train_set)</p>
			<p class="source-code">test_imgs, test_ages = normalize_dataset(test_set)</p>
			<p>Once all the images are<a id="_idIndexMarker226"/> the same size (128×128) and the same color (grayscale) and we have the labels and the estimated age, we are ready to feed our model, but first we have to create it.</p>
			<h1 id="_idParaDest-71"><a id="_idTextAnchor072"/>Creating and fine-tuning a powerful image regressor</h1>
			<p>Because we want to predict <a id="_idIndexMarker227"/>age, and this is a scalar value, we are going to use AutoKeras <strong class="source-inline">ImageRegressor</strong> as an age predictor. We set <strong class="source-inline">max_trials</strong> (the maximum number of different Keras models to try) to <strong class="source-inline">10</strong>, and we do not set the <strong class="source-inline">epochs</strong> parameter so<a id="_idIndexMarker228"/> that it will use an adaptive number of epochs automatically. For real use, it is recommended to set a large number of trials. The code is shown here:</p>
			<p class="source-code">reg = ak.ImageRegressor(max_trials=10)</p>
			<p>Let's run the training model to search for the optimal regressor for the training dataset, as follows:</p>
			<p class="source-code">reg.fit(train_imgs, train_ages)</p>
			<p>Here is the output of the preceding code:</p>
			<div>
				<div id="_idContainer053" class="IMG---Figure">
					<img src="image/B16953_04_13.jpg" alt="Figure 4.13 – Notebook output of our age predictor training&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.13 – Notebook output of our age predictor training</p>
			<p>The previous output shows that the loss for the training dataset is decreasing.</p>
			<p>This training process has taken 1 hour in Colaboratory. We have limited the search to 10 architectures (<strong class="source-inline">max_trials = 10</strong>) and restricted the number of images to 10,000. Increasing these numbers <a id="_idIndexMarker229"/>would give us a more accurate model, although it would<a id="_idIndexMarker230"/> also take longer to finish.</p>
			<h2 id="_idParaDest-72"><a id="_idTextAnchor073"/>Improving the model performance</h2>
			<p>If we need more precision in<a id="_idIndexMarker231"/> less time, we can fine-tune our model using an advanced AutoKeras feature that allows you to customize your search space.</p>
			<p>As we did earlier in the regressor example, we can use <strong class="source-inline">AutoModel</strong> with <strong class="source-inline">ImageBlock</strong> instead of <strong class="source-inline">ImageRegressor</strong> so that we can implement high-level configurations, such as define a specific architecture neural network to search using <strong class="source-inline">block_type</strong>. We can also perform data preprocessing operations, such as normalization or augmentation.</p>
			<p>As we did in the previous image classifier example, we can design a suitable architecture as an <strong class="source-inline">EfficientNet</strong>-based image regressor, for instance, which is a deep residual learning architecture for image recognition.</p>
			<p>See the following example for more details:</p>
			<p class="source-code">input_node = ak.ImageInput()</p>
			<p class="source-code">output_node = ak.Normalization()(input_node)</p>
			<p class="source-code">output_node = ak.ImageAugmentation()(output_node)</p>
			<p class="source-code">output_node = ak.ImageBlock(block_type="efficient")(input_node)</p>
			<p class="source-code">output_node = ak.RegressionHead()(output_node)</p>
			<p class="source-code">reg = ak.AutoModel(inputs=input_node, outputs=output_node, max_trials=20)</p>
			<p class="source-code">reg.fit(train_imgs, train_ages)</p>
			<p>In the previous code, we have done the following with the settings:</p>
			<ul>
				<li>The <strong class="source-inline">Normalization</strong> block will transform all image values in the range between 0 and 255 to floats between 0 and 1.</li>
				<li>The shape has been set (60000, 28 * 28) with values between 0 and 1.</li>
				<li>With <strong class="source-inline">ImageBlock(block_type="efficient"</strong>, we are telling AutoKeras to only scan <strong class="source-inline">EfficientNet</strong> architectures.</li>
				<li>The <strong class="source-inline">ImageAugmentation</strong> block performs data augmentation, a technique to create new artificial images from the originals. </li>
			</ul>
			<p>You can also not<a id="_idIndexMarker232"/> specify any of these arguments, in which case these different options would be tuned automatically.</p>
			<p>You can see more details about the <strong class="source-inline">EfficientNet</strong> function here:  </p>
			<p><a href="https://keras.io/api/applications/efficientnet/">https://keras.io/api/applications/efficientnet/</a></p>
			<h2 id="_idParaDest-73"><a id="_idTextAnchor074"/>Evaluating the model with the test set</h2>
			<p>After training, it's time to<a id="_idIndexMarker233"/> measure the actual prediction of our model using the reserved test dataset. In this way, we can rule out that the good results obtained with the training set are due to overfitting. The code to do this is shown here:</p>
			<p class="source-code">print(reg.evaluate(test_imgs, test_ages))</p>
			<p>Here is the output of the preceding code:</p>
			<p class="source-code">32/32 [==============================] - 2s 51ms/step - loss: 165.3358 - mean_squared_error: 165.3358</p>
			<p class="source-code">[165.33575439453125, 165.33575439453125]</p>
			<p>This error still has a lot of margin to improve, but let's have a look at how it's predicting over a subset of test samples, as follows:</p>
			<p class="source-code">fig = plt.figure(figsize=[20,100])</p>
			<p class="source-code">for i, v in enumerate(predicted_y[0:80]):</p>
			<p class="source-code">    ax = fig.add_subplot(20, 5, i+1)</p>
			<p class="source-code">    ax.set_axis_off()</p>
			<p class="source-code">    ax.set_title('Prediced: %s, Real: %s' % (predicted_y[i][0], test_ages[i]))</p>
			<p class="source-code">    img = test_imgs[i]</p>
			<p class="source-code">    ax.imshow(img)</p>
			<p class="source-code">plt.show()</p>
			<p>Here is the output <a id="_idIndexMarker234"/>of the preceding code:</p>
			<div>
				<div id="_idContainer054" class="IMG---Figure">
					<img src="image/B16953_04_14.jpg" alt="Figure 4.14 – Samples with their predicted and true labels&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.14 – Samples with their predicted and true labels</p>
			<p>We can see that some predicted samples are near to the real age of the person but others aren't, so investing in more training hours and fine-tuning will make it predict better. Let's take a look inside the classifier to understand how it is working.</p>
			<h2 id="_idParaDest-74"><a id="_idTextAnchor075"/>Visualizing the model</h2>
			<p>We can now see a little <a id="_idIndexMarker235"/>summary with the architecture of the best generated model found by running the following code:</p>
			<p class="source-code">model = clf.export_model()</p>
			<p class="source-code">model.summary()</p>
			<p>Here is the output of the preceding code:</p>
			<div>
				<div id="_idContainer055" class="IMG---Figure">
					<img src="image/B16953_04_15.jpg" alt="Figure 4.15 – Best model architecture summary&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.15 – Best model architecture summary</p>
			<p>The key layers<a id="_idIndexMarker236"/> here are the convolution and pooling blocks, as we explained at the beginning of this chapter. These layers learn local patterns from the image that help to perform the predictions. Here is a visual representation of this:</p>
			<div>
				<div id="_idContainer056" class="IMG---Figure">
					<img src="image/B16953_04_16.jpg" alt="Figure 4.16 – Best model architecture visualization"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.16 – Best model architecture visualization</p>
			<p>First, there are some data preprocessing blocks that normalize the images and do data augmentation; then, there are several stacked convolution and pooling blocks; then, a dropout block to<a id="_idIndexMarker237"/> do the regularization (a technique to reduce overfitting based on dropping random neurons while training, to reduce the correlation between the closest neurons); and finally, we see the regression block, to convert the output to a scalar (the age).</p>
			<h1 id="_idParaDest-75"><a id="_idTextAnchor076"/>Summary</h1>
			<p>In this chapter, we have learned how convolutional networks work, how to implement an image classifier, and how to fine-tune it to improve its accuracy. We have also learned how to implement an image regressor and fine-tune it to improve its performance.</p>
			<p>Now that we have learned how to work with images, we are ready to move on to the next chapter, where you will learn how to work with text by implementing classification and regression models using AutoKeras. </p>
		</div>
	</body></html>