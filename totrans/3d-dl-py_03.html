<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer060">
<h1 class="chapter-number" id="_idParaDest-30" lang="en-GB"><a id="_idTextAnchor030"/>2</h1>
<h1 id="_idParaDest-31" lang="en-GB"><a id="_idTextAnchor031"/>Introducing 3D Computer Vision and Geometry</h1>
<p lang="en-GB">In this chapter, we will learn about some basic concepts of 3D computer vision and geometry that will be especially useful for later chapters in this book. We will start by discussing what rendering, rasterization, and shading are. We will go through different lighting models and shading models, such as point light sources, directional light sources, ambient lighting, diffusion, highlights, and shininess. We will go through a coding example for rendering a mesh model using different lighting models <span class="No-Break" lang="">and parameters.</span></p>
<p lang="en-GB">We will then learn how to use PyTorch for solving optimization problems. Particularly, we will go through stochastic gradient descent over heterogeneous mini-batches, which becomes possible by using PyTorch3D. We will also learn about different formats for mini-batches in PyTorch3D, including the list, padded, and packed formats, and learn how to convert between the <span class="No-Break" lang="">different formats.</span></p>
<p lang="en-GB">In the last part of the chapter, we will discuss some frequently used rotation representations and how to convert between these representations. </p>
<p lang="en-GB">In this chapter, we’re going to cover the <span class="No-Break" lang="">following topics:</span></p>
<ul>
<li lang="en-GB">Exploring the basic concepts of rendering, rasterization, <span class="No-Break" lang="">and shading</span></li>
<li lang="en-GB">Understanding the Lambertian shading and Phong <span class="No-Break" lang="">shading models</span></li>
<li lang="en-GB">How to define a PyTorch tensor and optimize the tensor using <span class="No-Break" lang="">an optimizer</span></li>
<li lang="en-GB">How to define a mini-batch and heterogeneous mini-batch and packed and <span class="No-Break" lang="">padded tensors</span></li>
<li lang="en-GB">Rotations and different ways to <span class="No-Break" lang="">describe rotations</span></li>
<li lang="en-GB">Exponential mapping and log mapping in the <span class="No-Break" lang="">SE(3) space</span></li>
</ul>
<h1 id="_idParaDest-32" lang="en-GB"><a id="_idTextAnchor032"/>Technical requirements</h1>
<p lang="en-GB">To run the example code snippets in this book, the readers need to have a computer, ideally with a GPU. However, running the code snippets only with CPUs is <span class="No-Break" lang="">not impossible.</span></p>
<p lang="en-GB">The recommended computer configuration includes <span class="No-Break" lang="">the following:</span></p>
<ul>
<li lang="en-GB">A modern GPU – for example, the Nvidia GTX series or RTX series with at least 8 GB <span class="No-Break" lang="">of memory</span></li>
<li lang="en-GB"><span class="No-Break" lang="">Python 3</span></li>
<li lang="en-GB">PyTorch library and <span class="No-Break" lang="">PyTorch3D libraries</span></li>
</ul>
<p lang="en-GB">The code snippets with this chapter can be found <span class="No-Break" lang="">at </span><a href="https://github.com/PacktPublishing/3D-Deep-Learning-with-Python"><span class="No-Break" lang="">https://github.com/PacktPublishing/3D-Deep-Learning-with-Python.</span></a></p>
<h1 id="_idParaDest-33" lang="en-GB"><a id="_idTextAnchor033"/>Exploring the basic concepts of rendering, rasterization, and shading</h1>
<p lang="en-GB"><strong class="bold" lang="">Rendering</strong> is a process that takes 3D data models <a id="_idIndexMarker074"/><a id="_idIndexMarker075"/>of the world around our camera as input and output images. It is an approximation to the physical process where images are formed in our camera in the real world. Typically, the 3D data models are meshes. In this case, rendering is usually done using <span class="No-Break" lang="">ray tracing:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer014">
<img alt="Figure 2.1: Rendering by ray tracing (rays are generated from camera origins and go through the image pixels for finding relevant mesh faces) " height="783" src="image/B18217_02_001.jpg" width="1200"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1: Rendering by ray tracing (rays are generated from camera origins and go through the image pixels for finding relevant mesh faces)</p>
<p lang="en-GB">An example of ray tracing processing is shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 2</em></span><em class="italic" lang="">.1</em>. In the example, the world model contains one 3D sphere, which is represented by a mesh model. To form the image of the 3D sphere, for each image pixel, we generate one ray, starting from the camera origin and going through the image pixel. If one ray intersects with one mesh face, then we know the mesh face can project its color to the image pixel. We also need to trace the depth of each intersection because a face with a smaller depth would occlude faces with larger depths.   </p>
<p lang="en-GB">Thus, the process of <a id="_idIndexMarker076"/><a id="_idIndexMarker077"/>rendering can usually be divided into two stages – rasterization and shading. The ray tracing process is a typical rasterization process – that is, the process of finding relevant geometric objects for each image pixel. Shading is the process of taking the outputs of the rasterization and computing the pixel value for each image pixel. </p>
<p lang="en-GB">The <strong class="source-inline" lang="">pytorch3d.renderer.mesh.rasterize_meshes.rasterize_meshes</strong> function in PyTorch3D usually computes the following four things for each <span class="No-Break" lang="">image pixel:</span></p>
<ul>
<li lang="en-GB"><strong class="source-inline" lang="">pix_to_face</strong> is a list of face indices that the ray <span class="No-Break" lang="">may intersect.</span></li>
<li lang="en-GB"><strong class="source-inline" lang="">zbuf</strong> is a list of depth values of <span class="No-Break" lang="">these faces.</span></li>
<li lang="en-GB"><strong class="source-inline" lang="">bary_coords</strong> is a list of barycentric coordinates of the intersection point of each face and <span class="No-Break" lang="">the ray.</span></li>
<li lang="en-GB"><strong class="source-inline" lang="">pix_dists</strong> is a list of signed distances between pixels (<em class="italic" lang="">x</em> and <em class="italic" lang="">y</em>) and the nearest point on all the faces where the ray intersects. The values of this list can take negative values since it contains signed distances. </li>
</ul>
<p lang="en-GB">Note that usually, one face with the smallest depth would occlude all the mesh faces with larger depths. Thus, if all we need is the rendered image, then all we need in this list is the face with the smallest depth. However, with the more advanced setting of differentiable rendering (which we will cover in later chapters of this book), the pixel colors are usually fused from multiple mesh faces. </p>
<h2 id="_idParaDest-34" lang="en-GB"><a id="_idTextAnchor034"/>Understanding barycentric coordinates</h2>
<p lang="en-GB">For each point coplanar with a mesh<a id="_idIndexMarker078"/><a id="_idIndexMarker079"/> face, the coordinates of the point can always be written as a linear combination of the coordinates of the three vertices of the mesh face. For example, as shown in the following diagram, the point p can be written as <img alt="" height="37" src="image/01.png" width="294"/>, where <em class="italic" lang="">A</em>, <em class="italic" lang="">B</em>, and <em class="italic" lang="">C</em> are the coordinates of the three vertices of the mesh face. Thus, we can represent each such point with the coefficients u, v, and w. This representation is called the barycentric coordinates of the point. For point lays within the mesh face triangle, <img alt="" height="34" src="image/02.png" width="256"/> and all u,v,w are positive numbers. Since barycentric coordinates define any point inside a face as a function of face vertices, we can use the same coefficients to interpolate other properties across the whole face as a function of the properties defined at the vertices of the face. For example, we can use it for shading as shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 2</em></span><span class="No-Break" lang=""><em class="italic" lang="">.2</em></span><span class="No-Break" lang="">:</span></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><img alt="Figure 2.2: Definition of the barycentric coordinate system " height="838" src="image/B18217_02_002.png" width="900"/></p>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2: Definition of the barycentric coordinate system</p>
<p lang="en-GB">Once we have a list of the <strong class="source-inline" lang="">pix_to_face</strong>, <strong class="source-inline" lang="">zbuf</strong>, <strong class="source-inline" lang="">bary_coords</strong>, and <strong class="source-inline" lang="">dists</strong> values, a shading process would mimic the physical process <a id="_idIndexMarker080"/><a id="_idIndexMarker081"/>of image formation as in the real world. Thus, we are going to discuss several physical models for <span class="No-Break" lang="">color formation.</span></p>
<h2 id="_idParaDest-35" lang="en-GB"><a id="_idTextAnchor035"/>Light source models</h2>
<p lang="en-GB">Light propagation in the real world can be a sophisticated process. Several approximations of light sources are usually used in shading to<a id="_idIndexMarker082"/><a id="_idIndexMarker083"/> reduce <span class="No-Break" lang="">computational costs:</span></p>
<ul>
<li lang="en-GB">The first assumption is ambient lighting, where we assume that there is some background light radiation after sufficient reflections, such that they usually come from all directions with almost the same amplitude at all <span class="No-Break" lang="">image pixels.</span></li>
<li lang="en-GB">Another assumption that we usually use is that some light sources can be considered point light sources. A point light source radiates lights from one single point and the radiations at all directions have the same color and amplitude.    </li>
<li lang="en-GB">A third assumption that we usually use is that some light sources can be modeled as directional light sources. In such a case, the light directions from the light source are identical at all the 3D spatial locations. Directional lighting is a good approximation model for cases where the light sources are far away from the rendered objects – for example, sunlight. </li>
</ul>
<h2 id="_idParaDest-36" lang="en-GB"><a id="_idTextAnchor036"/>Understanding the Lambertian shading model</h2>
<p lang="en-GB">The first physical model that we will <a id="_idIndexMarker084"/><a id="_idIndexMarker085"/>discuss is Lambert’s cosine law. Lambertian surfaces are types of objects that are not shiny at all, such as paper, unfinished wood, and <span class="No-Break" lang="">unpolished stones:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<img alt="Figure 2.3: Light diffusion on Lambertian surfaces " height="347" src="image/B18217_02_003.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3: Light diffusion on Lambertian surfaces</p>
<p lang="en-GB"><span class="No-Break" lang=""><em class="italic" lang="">Figure 2</em></span><em class="italic" lang="">.3</em> shows an example of how lights diffuse on a Lambertian surface. One basic idea of the Lambertian cosine law is that for Lambertian surfaces, the amplitude of the reflected light does not depend on the viewer’s angle, but only depends on the angle <img alt="" height="35" src="image/03.png" width="22"/> between the surface normal and the direction of the incident light. More precisely, the intensity of the reflected light <img alt="" height="26" src="image/04.png" width="22"/> is <span class="No-Break" lang="">as follows:</span></p>
<p lang="en-GB"><img alt="" height="47" src="image/05.png" width="269"/></p>
<p lang="en-GB">Here, <img alt="" height="35" src="image/06.png" width="38"/> is the material’s reflected coefficient and  <img alt="" height="34" src="image/07.png" width="31"/> is the amplitude of the incident light. If we further consider the ambient light, the amplitude of the reflected light is <span class="No-Break" lang="">as follows:</span></p>
<p lang="en-GB"><img alt="" height="47" src="image/08.png" width="391"/></p>
<p lang="en-GB">Here, <img alt="" height="32" src="image/09.png" width="38"/> is the amplitude of the <span class="No-Break" lang="">ambient light.</span></p>
<h2 id="_idParaDest-37" lang="en-GB"><a id="_idTextAnchor037"/>Understanding the Phong lighting model</h2>
<p lang="en-GB">For shiny surfaces, such as polished tile <a id="_idIndexMarker086"/><a id="_idIndexMarker087"/>floors and glossy paint, the reflected light also contains a highlight component. The Phong lighting model is a frequently used model for these <span class="No-Break" lang="">glossy components:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer026">
<img alt="Figure 2.4: The Phong lighting model " height="400" src="image/B18217_02_004.jpg" width="861"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4: The Phong lighting model</p>
<p lang="en-GB">An example of the Phong lighting model is shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 2</em></span><em class="italic" lang="">.4</em>. One basic principle of the Phong lighting model is that the shiny light component should be strongest in the direction of reflection of the incoming light. The component would become weaker as the angle <img alt="" height="29" src="image/10.png" width="25"/> between the direction of reflection and the viewing angle becomes larger.  </p>
<p lang="en-GB">More precisely, the amplitude of the shiny light component <img alt="" height="25" src="image/101.png" width="22"/> is equal to <span class="No-Break" lang="">the following:</span></p>
<p lang="en-GB"><img alt="" height="54" src="image/11.png" width="350"/></p>
<p lang="en-GB">Here, the exponent <img alt="" height="29" src="image/12.png" width="25"/> is a parameter of the model for controlling the speed at which the shiny components attenuate when the viewing angle is away from the direction <span class="No-Break" lang="">of reflection.</span></p>
<p lang="en-GB">Finally, if we consider all three major components – ambient lighting, diffusion, and highlights – the final equation for the amplitude of light is <span class="No-Break" lang="">as follows:</span></p>
<p lang="en-GB"><img alt="" height="56" src="image/13.png" width="681"/></p>
<p lang="en-GB">Note that the preceding equation applies to <a id="_idIndexMarker088"/><a id="_idIndexMarker089"/>each color component. In other words, we will have one of these equations for each color channel (red, green, and blue) with a distinct set of <img alt="" height="37" src="image/14.png" width="138"/> <span class="No-Break" lang="">values:</span></p>
<p lang="en-GB">Now, we have learned about the basic concepts of rendering, rasterization, and rendering. We have also learned about the different light source models and shading models. We are ready to perform some coding exercises to use these light sources and shading models. </p>
<h1 id="_idParaDest-38" lang="en-GB"><a id="_idTextAnchor038"/>Coding exercises for 3D rendering</h1>
<p lang="en-GB">In this section, we will look at a concrete <a id="_idIndexMarker090"/><a id="_idIndexMarker091"/>coding exercise using PyTorch3D for rendering a mesh model. We are going to learn how to define a camera model and how to define a light source in PyTorch3D. We will also learn how to change the incoming light components and material properties so that more realistic images can be rendered by controlling the three light components (ambient, diffusion, <span class="No-Break" lang="">and glossy):</span></p>
<ol>
<li lang="en-GB">First, we need to import all the Python modules that <span class="No-Break" lang="">we need:</span><p class="source-code" lang="en-GB">import open3d</p><p class="source-code" lang="en-GB">import os</p><p class="source-code" lang="en-GB">import sys</p><p class="source-code" lang="en-GB">import torch</p><p class="source-code" lang="en-GB">import matplotlib.pyplot as plt</p><p class="source-code" lang="en-GB">from pytorch3d.io import load_objs_as_meshes</p><p class="source-code" lang="en-GB">from pytorch3d.renderer import (</p><p class="source-code" lang="en-GB">    look_at_view_transform,</p><p class="source-code" lang="en-GB">    PerspectiveCameras,</p><p class="source-code" lang="en-GB">    PerspectiveCameras,</p><p class="source-code" lang="en-GB">    PointLights,</p><p class="source-code" lang="en-GB">    Materials,</p><p class="source-code" lang="en-GB">    RasterizationSettings,</p><p class="source-code" lang="en-GB">    MeshRenderer,</p><p class="source-code" lang="en-GB">    MeshRasterizer</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">from pytorch3d.renderer.mesh.shader import HardPhongShader</p><p class="source-code" lang="en-GB">sys.path.append(os.path.abspath(''))</p></li>
<li lang="en-GB">Then, we need to load the mesh that <a id="_idIndexMarker092"/><a id="_idIndexMarker093"/>we are going to use. The <strong class="source-inline" lang="">cow.obj</strong> file contains a mesh model for a toy <span class="No-Break" lang="">cow object:</span><p class="source-code" lang="en-GB">DATA_DIR = "./data"</p><p class="source-code" lang="en-GB">obj_filename = os.path.join(DATA_DIR, "cow_mesh/cow.obj")</p><p class="source-code" lang="en-GB">device = torch.device('cuda')</p><p class="source-code" lang="en-GB">mesh = load_objs_as_meshes([obj_filename], device=device)</p></li>
<li lang="en-GB">We will define the cameras and light sources next. We use the <strong class="source-inline" lang="">look_at_view_transform</strong> function to map easy-to-understand parameters, such as the distance from the camera, elevation angle, and azimuth angle to obtain the rotation (R) and translation (T) matrices. The <strong class="source-inline" lang="">R</strong> and <strong class="source-inline" lang="">T</strong> variables define where we are going to place our camera. The <strong class="source-inline" lang="">lights</strong> variable is a point light source placed at <strong class="source-inline" lang="">[0.0, 0.0, -3.0]</strong> as <span class="No-Break" lang="">its location:</span><p class="source-code" lang="en-GB">R, T = look_at_view_transform(2.7, 0, 180)</p><p class="source-code" lang="en-GB">cameras = PerspectiveCameras(device=device, R=R, T=T)</p><p class="source-code" lang="en-GB">lights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])</p></li>
<li lang="en-GB">We will define a <strong class="source-inline" lang="">renderer</strong> variable of the <strong class="source-inline" lang="">MeshRenderer</strong> type. A <strong class="source-inline" lang="">renderer</strong> variable is a callable object, which can take a mesh as input and output the rendered images. Note that the renderer takes two inputs in its initialization – one rasterizer and one shader. PyTorch3D has defined several different types of rasterizers and shaders. Here, we are going to use <strong class="source-inline" lang="">MeshRasterizer</strong> and <strong class="source-inline" lang="">HardPhongShader</strong>. Note that we can also specify the setting of the rasterizer. <strong class="source-inline" lang="">image_size</strong> is equal to <strong class="source-inline" lang="">512</strong> here, which implies the rendered images would be 512 x 512 pixels. <strong class="source-inline" lang="">blur_radius</strong> is set to <strong class="source-inline" lang="">0</strong> and <strong class="source-inline" lang="">faces_per_pixel</strong> is set to <strong class="source-inline" lang="">1</strong>. The <strong class="source-inline" lang="">blur_radius</strong> and <strong class="source-inline" lang="">faces_per_pixel</strong> settings are the most useful for differentiable rendering, where <strong class="source-inline" lang="">blur_radius</strong> should be greater than <strong class="source-inline" lang="">0</strong> and <strong class="source-inline" lang="">faces_per_pixel</strong> should be greater <span class="No-Break" lang="">than </span><span class="No-Break" lang=""><strong class="source-inline" lang="">1</strong></span><span class="No-Break" lang="">:</span><p class="source-code" lang="en-GB">raster_settings = RasterizationSettings(</p><p class="source-code" lang="en-GB">    image_size=512,</p><p class="source-code" lang="en-GB">    blur_radius=0.0,</p><p class="source-code" lang="en-GB">    faces_per_pixel=1,</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">renderer = MeshRenderer(</p><p class="source-code" lang="en-GB">    rasterizer=MeshRasterizer(</p><p class="source-code" lang="en-GB">        cameras=cameras,</p><p class="source-code" lang="en-GB">        raster_settings=raster_settings</p><p class="source-code" lang="en-GB">    ),</p><p class="source-code" lang="en-GB">    shader = HardPhongShader(</p><p class="source-code" lang="en-GB">        device = device,</p><p class="source-code" lang="en-GB">        cameras = cameras,</p><p class="source-code" lang="en-GB">        lights = lights</p><p class="source-code" lang="en-GB">    )</p><p class="source-code" lang="en-GB">)</p></li>
<li lang="en-GB">We are therefore ready to run our <a id="_idIndexMarker094"/><a id="_idIndexMarker095"/>first rendering results by calling the renderer and passing the mesh model. The rendered image is shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 2</em></span><span class="No-Break" lang=""><em class="italic" lang="">.5</em></span><span class="No-Break" lang="">:</span><p class="source-code" lang="en-GB">images = renderer(mesh)</p><p class="source-code" lang="en-GB">plt.figure(figsize=(10, 10))</p><p class="source-code" lang="en-GB">plt.imshow(images[0, ..., :3].cpu().numpy())</p><p class="source-code" lang="en-GB">plt.axis("off")</p><p class="source-code" lang="en-GB">plt.savefig('light_at_front.png')</p><p class="source-code" lang="en-GB">plt.show()</p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer033">
<img alt="Figure 2.5: The rendered image when the light source is placed in front " height="303" src="image/B18217_02_005.jpg" width="342"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5: The rendered image when the light source is placed in front</p>
<ol>
<li lang="en-GB" value="6">Next, we will change the location of the<a id="_idIndexMarker096"/><a id="_idIndexMarker097"/> light source to the back of the mesh and see what will happen. The rendered image is shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 2</em></span><em class="italic" lang="">.6</em>. In this case, the light from the point light source cannot intersect with any mesh faces that are facing us. Thus, all the colors that we can observe here are due to <span class="No-Break" lang="">ambient light:</span><p class="source-code" lang="en-GB">lights.location = torch.tensor([0.0, 0.0, +1.0], device=device)[None]</p><p class="source-code" lang="en-GB">images = renderer(mesh, lights=lights, )</p><p class="source-code" lang="en-GB">plt.figure(figsize=(10, 10))</p><p class="source-code" lang="en-GB">plt.imshow(images[0, ..., :3].cpu().numpy())</p><p class="source-code" lang="en-GB">plt.axis("off")</p><p class="source-code" lang="en-GB">plt.savefig('light_at_back.png')</p><p class="source-code" lang="en-GB">plt.show()</p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer034">
<img alt="Figure 2.6: The rendered image when the light source is placed behind the toy cow " height="288" src="image/B18217_02_006.jpg" width="323"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.6: The rendered image when the light source is placed behind the toy cow</p>
<ol>
<li lang="en-GB" value="7">In the next experiment, we are going to define a <strong class="source-inline" lang="">materials</strong> data structure. Here, we change the configuration so that the ambient components are close to 0 (indeed, being <strong class="source-inline" lang="">0.01</strong>). Because the point light source is behind the object and the ambient light is also turned off, the rendered object does not reflect any light now. The rendered image is shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 2</em></span><span class="No-Break" lang=""><em class="italic" lang="">.7:</em></span><p class="source-code" lang="en-GB">materials = Materials(</p><p class="source-code" lang="en-GB">    device=device,</p><p class="source-code" lang="en-GB">    specular_color=[[0.0, 1.0, 0.0]],</p><p class="source-code" lang="en-GB">    shininess=10.0,</p><p class="source-code" lang="en-GB">    ambient_color=((0.01, 0.01, 0.01),),</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">images = renderer(mesh, lights=lights, materials = materials)</p><p class="source-code" lang="en-GB">plt.figure(figsize=(10, 10))</p><p class="source-code" lang="en-GB">plt.imshow(images[0, ..., :3].cpu().numpy())</p><p class="source-code" lang="en-GB">plt.axis("off")</p><p class="source-code" lang="en-GB">plt.savefig('dark.png')</p><p class="source-code" lang="en-GB">plt.show()</p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer035">
<img alt="Figure 2.7: The rendered image without ambient light and the point light source behind the toy cow " height="296" src="image/B18217_02_007.jpg" width="312"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.7: The rendered image without ambient light and the point light source behind the toy cow</p>
<ol>
<li lang="en-GB" value="8">In the next experiment, we will rotate the camera again and redefine the light source location so that the light can <a id="_idIndexMarker098"/><a id="_idIndexMarker099"/>shine on the cow’s face. Note that when we define the material, we set <strong class="source-inline" lang="">shininess</strong> to <strong class="source-inline" lang="">10.0</strong>. This <strong class="source-inline" lang="">shininess</strong> parameter is precisely the <strong class="source-inline" lang="">p</strong> parameter in the Phong lighting model. <strong class="source-inline" lang="">specular_color</strong> is <strong class="source-inline" lang="">[0.0, 1.0, 0.0]</strong>, which implies that the surface is shiny mainly in the green component. The rendered results are shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 2</em></span><span class="No-Break" lang=""><em class="italic" lang="">.8</em></span><span class="No-Break" lang="">:</span><p class="source-code" lang="en-GB">R, T = look_at_view_transform(dist=2.7, elev=10, azim=-150)</p><p class="source-code" lang="en-GB">cameras = PerspectiveCameras(device=device, R=R, T=T)</p><p class="source-code" lang="en-GB">lights.location = torch.tensor([[2.0, 2.0, -2.0]], device=device)</p><p class="source-code" lang="en-GB">materials = Materials(</p><p class="source-code" lang="en-GB">    device=device,</p><p class="source-code" lang="en-GB">    specular_color=[[0.0, 1.0, 0.0]],</p><p class="source-code" lang="en-GB">    shininess=10.0</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">images = renderer(mesh, lights=lights, materials=materials, cameras=cameras)</p><p class="source-code" lang="en-GB">plt.figure(figsize=(10, 10))</p><p class="source-code" lang="en-GB">plt.imshow(images[0, ..., :3].cpu().numpy())</p><p class="source-code" lang="en-GB">plt.axis("off")</p><p class="source-code" lang="en-GB">plt.savefig('green.png')</p><p class="source-code" lang="en-GB">plt.show()</p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer036">
<img alt="Figure 2.8: The rendered image with specular lighting components " height="488" src="image/B18217_02_008.jpg" width="496"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.8: The rendered image with specular lighting components</p>
<ol>
<li lang="en-GB" value="9">In the next experiment, we are going to change <strong class="source-inline" lang="">specular_color</strong> to <strong class="source-inline" lang="">red</strong> and increase the <strong class="source-inline" lang="">shininess</strong> value. The results are shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 2</em></span><span class="No-Break" lang=""><em class="italic" lang="">.9</em></span><span class="No-Break" lang="">:</span><p class="source-code" lang="en-GB">materials = Materials(</p><p class="source-code" lang="en-GB">    device=device,</p><p class="source-code" lang="en-GB">    specular_color=[[1.0, 0.0, 0.0]],</p><p class="source-code" lang="en-GB">    shininess=20.0</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">images = renderer(mesh, lights=lights, materials=materials, cameras=cameras)</p><p class="source-code" lang="en-GB">plt.figure(figsize=(10, 10))</p><p class="source-code" lang="en-GB">plt.imshow(images[0, ..., :3].cpu().numpy())</p><p class="source-code" lang="en-GB">plt.savefig('red.png')</p><p class="source-code" lang="en-GB">plt.axis("off")</p><p class="source-code" lang="en-GB">plt.show()</p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer037">
<img alt="Figure 2.9: The rendered image with a red specular color " height="694" src="image/B18217_02_009.jpg" width="716"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.9: The rendered image with a red specular color</p>
<ol>
<li lang="en-GB" value="10">Finally, we turn off the shininess<a id="_idIndexMarker100"/><a id="_idIndexMarker101"/> and the results are shown in <span class="No-Break" lang=""><em class="italic" lang="">Figure 2</em></span><span class="No-Break" lang=""><em class="italic" lang="">.10</em></span><span class="No-Break" lang="">:</span><p class="source-code" lang="en-GB">materials = Materials(</p><p class="source-code" lang="en-GB">    device=device,</p><p class="source-code" lang="en-GB">    specular_color=[[0.0, 0.0, 0.0]],</p><p class="source-code" lang="en-GB">    shininess=0.0</p><p class="source-code" lang="en-GB">)</p><p class="source-code" lang="en-GB">images = renderer(mesh, lights=lights, materials=materials, cameras=cameras)</p><p class="source-code" lang="en-GB">plt.figure(figsize=(10, 10))</p><p class="source-code" lang="en-GB">plt.imshow(images[0, ..., :3].cpu().numpy())</p><p class="source-code" lang="en-GB">plt.savefig('blue.png')</p><p class="source-code" lang="en-GB">plt.axis("off")</p><p class="source-code" lang="en-GB">plt.show()</p></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer038">
<img alt="Figure 2.10: The rendered image without specular components " height="655" src="image/B18217_02_010.jpg" width="663"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.10: The rendered image without specular components</p>
<p lang="en-GB">In the first part of<a id="_idIndexMarker102"/> this chapter, we mainly discussed rendering and shading, which are super important for 3D computer vision. Next, we will discuss another very important topic for 3D deep learning, which is the heterogeneous batch issue for optimization. </p>
<h1 id="_idParaDest-39" lang="en-GB"><a id="_idTextAnchor039"/>Using PyTorch3D heterogeneous batches and PyTorch optimizers</h1>
<p lang="en-GB">In this section, we are <a id="_idIndexMarker103"/>going to learn how to use the PyTorch optimizer on<a id="_idIndexMarker104"/> PyTorch3D heterogeneous mini-batches. In deep learning, we are usually given a list of data examples, such as the following ones – <img alt="" height="48" src="image/15.png" width="659"/>.. Here, <img alt="" height="35" src="image/16.png" width="34"/> are the observations and <img alt="" height="30" src="image/17.png" width="34"/> are the prediction values. For example, <img alt="" height="34" src="image/18.png" width="34"/> may be some images and <img alt="" height="33" src="image/20.png" width="34"/> the ground-truth classification results – for example, “cat” or “dog”. A deep neural network is then trained so that the outputs of the neural networks are as close to <img alt="" height="32" src="image/21.png" width="34"/> as possible. Usually, a loss function between the neural network outputs and <img alt="" height="31" src="image/22.png" width="34"/> is defined so that the loss function values decrease as the neural network outputs become closer <span class="No-Break" lang="">to <img alt="" height="31" src="image/22.png" width="34"/>.</span></p>
<p lang="en-GB">Thus, training a deep learning network is usually done by minimizing the loss function that is evaluated on all training data examples, <img alt="" height="34" src="image/23.png" width="34"/> and<img alt="" height="32" src="image/24.png" width="34"/>. A straightforward method used in many optimization algorithms is computing the gradients first, as shown in the following equation, and then modifying the parameters of the neural network along the direction of the negative gradient. In the equation, <em class="italic" lang="">f</em> represents the neural network that takes <img alt="" height="31" src="image/25.png" width="34"/> as its input and has parameters Ɵ; loss is the loss function between the neural network outputs and the <span class="No-Break" lang="">ground-truth prediction<img alt="" height="30" src="image/26.png" width="34"/>:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer051">
<img alt="" height="107" src="image/27.jpg" width="450"/>
</div>
</div>
<p lang="en-GB">However, computing<a id="_idIndexMarker105"/> this gradient is expensive, as the computational cost is proportional to the size of the training dataset. In reality, a <strong class="bold" lang="">Stochastic Gradient Descent</strong> (<strong class="bold" lang="">SGD</strong>) algorithm is <a id="_idIndexMarker106"/>used instead of the original gradient descent algorithm. In the<a id="_idIndexMarker107"/> SGD algorithm, the descent direction is computed as in the <span class="No-Break" lang="">following equation:</span></p>
<p lang="en-GB"><img alt="" height="100" src="image/28.png" width="428"/></p>
<p lang="en-GB">In the equation, the so-called mini-batch D is a small subset of all the training data examples. The mini-batch D is randomly sampled from the whole training data examples in each iteration. The SGD algorithm has a much lower computational cost than the gradient descent algorithm. Due to the law of large numbers, the computed descent directions in SGD are approximately close to the gradient descent directions. It is also widely believed that SGD introduces certain implicit regularization, which may contribute to the nice generalization properties of deep learning. The method for choosing the size of the mini-batch is an important hyperparameter that needs to be considered carefully. Nevertheless, the SGD algorithm and its variants have been the methods of choice for training deep <span class="No-Break" lang="">learning models.</span></p>
<p lang="en-GB">For many data types, such as images, the data can easily be made homogeneous. We can form a mini-batch of images all with the same widths, heights, and channels. For example, a mini-batch of eight images with three channels (the three colors red, green, and blue), a height of 256, and a width of 256 can be made into a PyTorch tensor with the dimensions 8 x 3 x 256 x 256. Usually, the first dimension of the tensor represents the data sample indices within the mini-batch. Usually, computations on this kind of homogeneous data can be done efficiently using GPUs. </p>
<p lang="en-GB">On the other hand, 3D <a id="_idIndexMarker108"/>data is usually heterogeneous. For example, meshes<a id="_idIndexMarker109"/> within one mini-batch may contain different numbers of vertices and faces. Processing this heterogeneous data on GPUs efficiently is not a trivial issue. Coding for the heterogeneous mini-batch processing can also be tedious. Luckily, PyTorch3D has the capacity to handle heterogeneous mini-batches very efficiently. We will go over a coding exercise involving these PyTorch3D capacities in the <span class="No-Break" lang="">next section.</span></p>
<h2 id="_idParaDest-40" lang="en-GB"><a id="_idTextAnchor040"/>A coding exercise for a heterogeneous mini-batch </h2>
<p lang="en-GB">In this section, we are <a id="_idIndexMarker110"/>going to learn how to use the PyTorch optimizer and PyTorch3D heterogeneous mini-batch capacities by looking at a toy example. In this example, we will consider a problem where a depth camera is placed at an unknown location and we want to estimate the unknown location using the sensing results of the camera. To simplify the problem, we assume that the orientation of the camera is known and the only unknown is the <span class="No-Break" lang="">3D displacement.</span></p>
<p lang="en-GB">More specifically, we assume that the camera observes three objects in the scene and we know the ground-truth mesh models of the three objects. Let us look at the code using PyTorch and PyTorch3D to solve the problem <span class="No-Break" lang="">as follows:</span></p>
<ol>
<li lang="en-GB" value="1">In the first step, we are going to import all the packages that we are going <span class="No-Break" lang="">to use:</span><p class="source-code" lang="en-GB">import open3d</p><p class="source-code" lang="en-GB">import os</p><p class="source-code" lang="en-GB">import torch</p><p class="source-code" lang="en-GB">from pytorch3d.io import load_objs_as_meshes</p><p class="source-code" lang="en-GB">from pytorch3d.structures.meshes import join_meshes_as_batch</p><p class="source-code" lang="en-GB">from pytorch3d.ops import sample_points_from_meshes</p><p class="source-code" lang="en-GB">from pytorch3d.loss import chamfer_distance</p><p class="source-code" lang="en-GB">import numpy as np</p></li>
<li lang="en-GB">In the next step, we will define a <strong class="source-inline" lang="">torch</strong> device using either a CPU <span class="No-Break" lang="">or CUDA:</span><p class="source-code" lang="en-GB">if torch.cuda.is_available():</p><p class="source-code" lang="en-GB">    device = torch.device("cuda:0")</p><p class="source-code" lang="en-GB">else:</p><p class="source-code" lang="en-GB">    device = torch.device("cpu")</p><p class="source-code" lang="en-GB">    print("WARNING: CPU only, this will be slow!")</p></li>
<li lang="en-GB">The mesh models that you are going to use in this toy example are included in the code repository and <a id="_idIndexMarker111"/>are under the <strong class="source-inline" lang="">data</strong> subfolder. We are going to use three mesh models contained in the <strong class="source-inline" lang="">cube.obj</strong>, <strong class="source-inline" lang="">diamond.obj</strong> and <strong class="source-inline" lang="">dodecahedron.obj</strong> files. In the following code snippet, we are using the <strong class="source-inline" lang="">Open3D</strong> library to load these mesh models and <span class="No-Break" lang="">visualize them:</span><p class="source-code" lang="en-GB">mesh_names = ['cube.obj', 'diamond.obj', 'dodecahedron.obj']</p><p class="source-code" lang="en-GB">data_path = './data'</p><p class="source-code" lang="en-GB">for mesh_name in mesh_names:</p><p class="source-code" lang="en-GB">    mesh = open3d.io.read_triangle_mesh(os.path.join(data_path, mesh_name))</p><p class="source-code" lang="en-GB">    open3d.visualization.draw_geometries([mesh],</p><p class="source-code" lang="en-GB">                                  mesh_show_wireframe = True,</p><p class="source-code" lang="en-GB">                                  mesh_show_back_face = True,</p><p class="source-code" lang="en-GB">                                     )</p></li>
<li lang="en-GB">Next, we are going to use PyTorch3D to load the same meshes and build a list of meshes, which is the <span class="No-Break" lang=""><strong class="source-inline" lang="">mesh_list</strong></span><span class="No-Break" lang=""> variable:</span><p class="source-code" lang="en-GB">mesh_list = list()</p><p class="source-code" lang="en-GB">device = torch.device('cuda')</p><p class="source-code" lang="en-GB">for mesh_name in mesh_names:</p><p class="source-code" lang="en-GB">    mesh = load_objs_as_meshes([os.path.join(data_path, mesh_name)], device=device)</p><p class="source-code" lang="en-GB">    mesh_list.append(mesh)</p></li>
<li lang="en-GB">Finally, we can create<a id="_idIndexMarker112"/> a PyTorch3D mini-batch of meshes by using the <strong class="source-inline" lang="">join_meshes_as_batch</strong> PyTorch3D function. The function takes a list of meshes and returns a mini-batch <span class="No-Break" lang="">of meshes:</span><p class="source-code" lang="en-GB">mesh_batch = join_meshes_as_batch(mesh_list, include_textures = False)</p></li>
</ol>
<p lang="en-GB">In each PyTorch3D mini-batch, there are three ways to represent vertices <span class="No-Break" lang="">and faces:</span></p>
<ul>
<li lang="en-GB"><strong class="bold" lang="">List format</strong>: The vertices<a id="_idIndexMarker113"/> are represented by a list of tensors where each tensor represents the vertices or faces of one mesh within <span class="No-Break" lang="">the mini-batch.</span></li>
<li lang="en-GB"><strong class="bold" lang="">Padded format</strong>: All the<a id="_idIndexMarker114"/> vertices are represented by one tensor and the data of the smaller meshes are zero-padded so that all the meshes now have the same numbers of vertices <span class="No-Break" lang="">and faces.</span></li>
<li lang="en-GB"><strong class="bold" lang="">Packed format</strong>: All the <a id="_idIndexMarker115"/>vertices or faces are packed into one tensor. For each vertex or face, which mesh it belongs to is tracked internally. </li>
</ul>
<p lang="en-GB">The three representations all have their pros and cons. Nevertheless, the formats can be converted between each other efficiently by using the PyTorch3D API.  </p>
<ol>
<li lang="en-GB" value="6">The next code snippet shows an example of how to return vertices and faces in a list format from <span class="No-Break" lang="">a mini-batch:</span><p class="source-code" lang="en-GB">vertex_list = mesh_batch.verts_list()</p><p class="source-code" lang="en-GB">print('vertex_list = ', vertex_list)</p><p class="source-code" lang="en-GB">face_list = mesh_batch.faces_list()</p><p class="source-code" lang="en-GB">print('face_list = ', face_list)</p></li>
<li lang="en-GB">To return vertices and faces in the padded format, we can use the following <span class="No-Break" lang="">PyTorch3D API:</span><p class="source-code" lang="en-GB">vertex_padded = mesh_batch.verts_padded()</p><p class="source-code" lang="en-GB">print('vertex_padded = ', vertex_padded)</p><p class="source-code" lang="en-GB">face_padded = mesh_batch.faces_padded()</p><p class="source-code" lang="en-GB">print('face_padded = ', face_padded)</p></li>
<li lang="en-GB">To get vertices and <a id="_idIndexMarker116"/>faces in the packed format, we can use the following <span class="No-Break" lang="">code snippet:</span><p class="source-code" lang="en-GB">vertex_packed = mesh_batch.verts_packed()</p><p class="source-code" lang="en-GB">print('vertex_packed = ', vertex_packed)</p><p class="source-code" lang="en-GB">face_packed = mesh_batch.faces_packed()</p><p class="source-code" lang="en-GB">print('face_packed = ', face_packed)</p><p class="source-code" lang="en-GB">num_vertices = vertex_packed.shape[0]</p><p class="source-code" lang="en-GB">print('num_vertices = ', num_vertices)</p></li>
<li lang="en-GB">In this coding example, we consider the <strong class="source-inline" lang="">mesh_batch</strong> variable as the ground-truth mesh model for the three objects. We will then simulate a noisy and displaced version of the three meshes. In the first step, we want to clone the ground truth mesh models: <p class="source-code" lang="en-GB">mesh_batch_noisy = mesh_batch.clone()</p></li>
<li lang="en-GB">We then define a <strong class="source-inline" lang="">motion_gt</strong> variable to represent the displacement between the camera location and <span class="No-Break" lang="">the origin:</span><p class="source-code" lang="en-GB">motion_gt = np.array([3, 4, 5])</p><p class="source-code" lang="en-GB">motion_gt = torch.as_tensor(motion_gt)</p><p class="source-code" lang="en-GB">print('motion ground truth = ', motion_gt)</p><p class="source-code" lang="en-GB">motion_gt = motion_gt[None, :]</p><p class="source-code" lang="en-GB">motion_gt = motion_gt.to(device)</p></li>
<li lang="en-GB">To simulate the noisy depth camera observations, we generate some random Gaussian noise with a mean equal to <strong class="source-inline" lang="">motion_gt</strong>. The noises are added to <strong class="source-inline" lang="">mesh_batch_noisy</strong> using the <strong class="source-inline" lang="">offset_verts</strong> <span class="No-Break" lang="">PyTorch3D function:</span><p class="source-code" lang="en-GB">noise = (0.1**0.5)*torch.randn(mesh_batch_noisy.verts_packed().shape).to(device)</p><p class="source-code" lang="en-GB">motion_gt = np.array([3, 4, 5])</p><p class="source-code" lang="en-GB">motion_gt = torch.as_tensor(motion_gt)</p><p class="source-code" lang="en-GB">noise = noise + motion_gt</p><p class="source-code" lang="en-GB">mesh_batch_noisy = mesh_batch_noisy.offset_verts(noise).detach()</p></li>
<li lang="en-GB">To estimate the<a id="_idIndexMarker117"/> unknown displacement between the camera and the origin, we will formulate an optimization problem. First, we will define the <strong class="source-inline" lang="">motion_estimate</strong> optimization variable. The <strong class="source-inline" lang="">torch.zeros</strong> function will create an all zero PyTorch tensor. Note that we set <strong class="source-inline" lang="">requires_grad</strong> to <strong class="source-inline" lang="">true</strong>. What that means is that when we run gradient backpropagation from the <strong class="source-inline" lang="">loss</strong> function, we want the gradient for this variable to be automatically computed by PyTorch <span class="No-Break" lang="">for us:</span><p class="source-code" lang="en-GB">motion_estimate = torch.zeros(motion_gt.shape, device = device, requires_grad=True)</p></li>
<li lang="en-GB">Next, we are going to define a PyTorch optimizer with a learning rate of <strong class="source-inline" lang="">0.1</strong>. By passing a list of variables to the optimizer, we specify the optimization variables for this optimization problem. Here, the optimization variable is the <span class="No-Break" lang=""><strong class="source-inline" lang="">motion_estimate </strong></span><span class="No-Break" lang="">variable:</span><p class="source-code" lang="en-GB">optimizer = torch.optim.SGD([motion_estimate], lr=0.1, momentum=0.9)</p></li>
<li lang="en-GB">The major optimization procedure is then shown as follows. Basically, we run the stochastic gradient descent for 200 iterations. The resulting <strong class="source-inline" lang="">motion_estimate</strong> should be very close to the ground truth after the <span class="No-Break" lang="">200 iterations.</span></li>
</ol>
<p lang="en-GB">Each optimization iteration can be divided into the following <span class="No-Break" lang="">four steps:</span></p>
<ol>
<li lang="en-GB">In the first step, <strong class="source-inline" lang="">optimizer.zero_grad()</strong> resets all the gradient values from the values computed in the last iteration <span class="No-Break" lang="">to zero.</span></li>
<li lang="en-GB">In the second step, we compute the <strong class="source-inline" lang="">loss</strong> function. Note that PyTorch retains a dynamic computational graph. In other words, all the computation procedures toward the <strong class="source-inline" lang="">loss</strong> function are recorded and will be used in <span class="No-Break" lang="">the backpropagation.</span></li>
<li lang="en-GB">In the third step, <strong class="source-inline" lang="">loss.backward()</strong> computes all the gradients from the <strong class="source-inline" lang="">loss</strong> function to the optimization variables in the <span class="No-Break" lang="">PyTorch optimizer.</span></li>
<li lang="en-GB">In the fourth <a id="_idIndexMarker118"/>and final step, <strong class="source-inline" lang="">optimizer.step</strong> moves all the optimization variables one step in the direction of decreasing the <span class="No-Break" lang=""><strong class="source-inline" lang="">loss<a id="_idTextAnchor041"/></strong></span><span class="No-Break" lang=""> function.</span></li>
</ol>
<p lang="en-GB">In the process of computing the <strong class="source-inline" lang="">loss</strong> function, we randomly sample 5,000 points from the two meshes and compute their Chamfer distances. The Chamfer distance is a distance between two sets of points. We will have a more detailed discussion of this distance function in <span class="No-Break" lang="">later chapters:</span></p>
<pre class="source-code" lang="en-GB">for i in range(0, 200):</pre>
<pre class="source-code" lang="en-GB">    optimizer.zero_grad()</pre>
<pre class="source-code" lang="en-GB">    current_mesh_batch = mesh_batch.offset_verts(motion_estimate.repeat(num_vertices,1))</pre>
<pre class="source-code" lang="en-GB">    sample_trg = sample_points_from_meshes(current_mesh_batch, 5000)</pre>
<pre class="source-code" lang="en-GB">    sample_src = sample_points_from_meshes(mesh_batch_noisy, 5000)</pre>
<pre class="source-code" lang="en-GB">    loss, _ = chamfer_distance(sample_trg, sample_src)</pre>
<pre class="source-code" lang="en-GB">    loss.backward()</pre>
<pre class="source-code" lang="en-GB">    optimizer.step()</pre>
<pre class="source-code" lang="en-GB">    print('i = ', i, ', motion_estimation = ', motion_estimate)</pre>
<p lang="en-GB">We can check that the optimization process here would converge to the <strong class="source-inline" lang="">[3,4,5]</strong> ground-truth location very quickly. </p>
<p lang="en-GB">In this coding exercise, we<a id="_idIndexMarker119"/> learned how to use heterogenous mini-batches in PyTorch3D. Next, we will discuss another important concept in 3D <span class="No-Break" lang="">computer vision.</span></p>
<h1 id="_idParaDest-41" lang="en-GB"><a id="_idTextAnchor042"/>Understanding transformations and rotations</h1>
<p lang="en-GB">In 3D deep learning and computer vision, we <a id="_idIndexMarker120"/>usually need to work with 3D transformations, such as <a id="_idIndexMarker121"/>rotations and 3D rigid motions. PyTorch3D provides a high-level encapsulation of these transformations in its <strong class="source-inline" lang="">pytorch3d.transforms.Transform3d</strong> class. One advantage of the <strong class="source-inline" lang="">Transform3d</strong> class is that it is mini-batch based. Thus, as frequently needed in 3D deep learning, it is possible to apply a mini-batch of transformations on a mini-batch of meshes only within several lines of code. Another advantage of <strong class="source-inline" lang="">Transform3d</strong> is that gradient backpropagation can straightforwardly pass <span class="No-Break" lang="">through </span><span class="No-Break" lang=""><strong class="source-inline" lang="">Transform3d</strong></span><span class="No-Break" lang="">.</span></p>
<p lang="en-GB">PyTorch3D also provides many lower-level APIs for computations in the Lie groups SO(3) and SE(3). Here, SO(3) denotes the special orthogonal group in 3D and SE(3) denotes the special Euclidean group in 3D. Informally speaking, SO(3) denotes the set of all the rotation transformations and SE(3) denotes the set of all the rigid transformations in 3D. Many low-level APIs on SE(3) and SO(3) are provided <span class="No-Break" lang="">in PyTorch3D.</span></p>
<p lang="en-GB">In 3D computer vision, multiple representations exist for rotations. One representation is the rotation <span class="No-Break" lang="">matrices <img alt="" height="33" src="image/29.png" width="50"/>.</span></p>
<p lang="en-GB">In this equation, <em class="italic" lang="">x</em> is a 3D vector and <em class="italic" lang="">R</em> is a 3 x 3 matrix. To be a rotation matrix, <em class="italic" lang="">R</em> needs to be an orthogonal matrix and has a determinant of +1. Thus, not all 3 x 3 matrices can be a rotation matrix. The degree of freedom for rotation matrices <span class="No-Break" lang="">is 3.</span></p>
<p lang="en-GB">A 3D rotation can also be<a id="_idIndexMarker122"/> represented by a 3D vector <em class="italic" lang="">v,</em> where the direction of <em class="italic" lang="">v</em> is the rotation axis. That is, the rotation would keep <em class="italic" lang="">v</em> fixed and rotate all the other things around <em class="italic" lang="">v</em>. It is also conventional to use the amplitude of <em class="italic" lang="">v</em> to represent the angle of rotation. </p>
<p lang="en-GB">There are various <a id="_idIndexMarker123"/>mathematical connections between the two representations of rotation. If we consider a constant speed rotation around the axis v, then the rotation matrices become a matrix-valued function of the time <em class="italic" lang="">t</em>, <em class="italic" lang="">R(t)</em>. In this case, the gradient of <em class="italic" lang="">R(t)</em> is always a skew-symmetric matrix, in the form shown in the <span class="No-Break" lang="">following equation:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer054">
<img alt="" height="217" src="image/30.jpg" width="450"/>
</div>
</div>
<p lang="en-GB">Here, the <span class="No-Break" lang="">following applies:</span></p>
<p lang="en-GB"><img alt="" height="56" src="image/31.png" width="390"/></p>
<p lang="en-GB">As we can see from these two equations, the skew-symmetric matrix of the gradient is uniquely determined by the vector <em class="italic" lang="">v</em> and vice versa. This mapping from the vector <em class="italic" lang="">v</em> to its skew-symmetric matrix form is <a id="_idIndexMarker124"/>usually called the <span class="No-Break" lang="">hat operator.</span></p>
<p lang="en-GB">A closed-form formula from the skew-symmetric matrix gradient to the rotation matrix exists as follows. The mapping is called <a id="_idIndexMarker125"/>the exponential map <span class="No-Break" lang="">for <img alt="" height="44" src="image/32.png" width="109"/>:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer057">
<img alt="" height="158" src="image/33.jpg" width="450"/>
</div>
</div>
<p lang="en-GB">Certainly, the inverse mapping of the exponential map <span class="No-Break" lang="">also exists:</span></p>
<p lang="en-GB"><img alt="" height="46" src="image/34.png" width="206"/></p>
<p lang="en-GB">The <a id="_idIndexMarker126"/>mapping is called the logarithmic map <span class="No-Break" lang="">for <img alt="" height="44" src="image/35.png" width="109"/>.</span></p>
<p lang="en-GB">All the hat, inverse hat, exponential, and logarithmic <a id="_idIndexMarker127"/>operations have already been implemented in PyTorch3D. PyTorch3D <a id="_idIndexMarker128"/>also implements many other frequently used 3D operations, such as quaternion operations and Euler angles.  </p>
<h2 id="_idParaDest-42" lang="en-GB"><a id="_idTextAnchor043"/>A coding exercise for transformation and rotation</h2>
<p lang="en-GB">In this section, we will go through a<a id="_idIndexMarker129"/> coding exercise on how to use some of PyTorch3D’s<a id="_idIndexMarker130"/> <span class="No-Break" lang="">low-level APIs:</span></p>
<ol>
<li lang="en-GB" value="1">We begin by importing the <span class="No-Break" lang="">necessary packages:</span><p class="source-code" lang="en-GB">import torch</p><p class="source-code" lang="en-GB">from pytorch3d.transforms.so3 import so3_exp_map, so3_log_map, hat_inv, hat</p></li>
<li lang="en-GB">We then define a PyTorch device using either a CPU <span class="No-Break" lang="">or CUDA:</span><p class="source-code" lang="en-GB">if torch.cuda.is_available():</p><p class="source-code" lang="en-GB">    device = torch.device("cuda:0")</p><p class="source-code" lang="en-GB">else:</p><p class="source-code" lang="en-GB">    device = torch.device("cpu")</p><p class="source-code" lang="en-GB">    print("WARNING: CPU only, this will be slow!")</p></li>
<li lang="en-GB">Next, we will define a mini-batch of four rotations. Here, each rotation is represented by one 3D vector. The direction of the vector represents the rotation axis and the amplitude of the vector represents the angle <span class="No-Break" lang="">of rotation:</span><p class="source-code" lang="en-GB">log_rot = torch.zeros([4, 3], device = device)</p><p class="source-code" lang="en-GB">log_rot[0, 0] = 0.001</p><p class="source-code" lang="en-GB">log_rot[0, 1] = 0.0001</p><p class="source-code" lang="en-GB">log_rot[0, 2] = 0.0002</p><p class="source-code" lang="en-GB">log_rot[1, 0] = 0.0001</p><p class="source-code" lang="en-GB">log_rot[1, 1] = 0.001</p><p class="source-code" lang="en-GB">log_rot[1, 2] = 0.0002</p><p class="source-code" lang="en-GB">log_rot[2, 0] = 0.0001</p><p class="source-code" lang="en-GB">log_rot[2, 1] = 0.0002</p><p class="source-code" lang="en-GB">log_rot[2, 2] = 0.001</p><p class="source-code" lang="en-GB">log_rot[3, 0] = 0.001</p><p class="source-code" lang="en-GB">log_rot[3, 1] = 0.002</p><p class="source-code" lang="en-GB">log_rot[3, 2] = 0.003</p></li>
<li lang="en-GB">The shape of <strong class="source-inline" lang="">log_rot</strong> is <strong class="source-inline" lang="">[4, 3]</strong>, where <strong class="source-inline" lang="">4</strong> is the batch size and each rotation is represented by a 3D <a id="_idIndexMarker131"/>vector. We can use the hat operator in PyTorch3D to<a id="_idIndexMarker132"/> convert them into the 3 x 3 skew-symmetric matrix representation <span class="No-Break" lang="">as follows:</span><p class="source-code" lang="en-GB">log_rot_hat = hat(log_rot)</p><p class="source-code" lang="en-GB">print('log_rot_hat shape = ', log_rot_hat.shape)</p><p class="source-code" lang="en-GB">print('log_rot_hat = ', log_rot_hat)</p></li>
<li lang="en-GB">The backward conversion from the skew-symmetric matrix form to the 3D vector form is also possible using the <span class="No-Break" lang=""><strong class="source-inline" lang="">hat_inv</strong></span><span class="No-Break" lang=""> operator:</span><p class="source-code" lang="en-GB">log_rot_copy = hat_inv(log_rot_hat)</p><p class="source-code" lang="en-GB">print('log_rot_copy shape = ', log_rot_copy.shape)</p><p class="source-code" lang="en-GB">print('log_rot_copy = ', log_rot_copy)</p></li>
<li lang="en-GB">From the gradient matrix, we can compute the rotation matrix by using the PyTorch3D <span class="No-Break" lang=""><strong class="source-inline" lang="">so3_exp_map</strong></span><span class="No-Break" lang=""> function:</span><p class="source-code" lang="en-GB">rotation_matrices = so3_exp_map(log_rot)</p><p class="source-code" lang="en-GB">print('rotation_matrices = ', rotation_matrices)</p></li>
<li lang="en-GB">The inverse conversation is <strong class="source-inline" lang="">so3_log_map</strong>, which would map the rotation matrix back to the gradient <span class="No-Break" lang="">matrix again:</span><p class="source-code" lang="en-GB">log_rot_again = so3_log_map(rotation_matrices)</p><p class="source-code" lang="en-GB">print('log_rot_again = ', log_rot_again)</p></li>
</ol>
<p lang="en-GB">These coding <a id="_idIndexMarker133"/>exercises show the most frequently used PyTorch3D APIs for<a id="_idIndexMarker134"/> transformations and rotations. These APIs can be very useful for real-world 3D computer <span class="No-Break" lang="">vision projects.</span></p>
<h1 id="_idParaDest-43" lang="en-GB"><a id="_idTextAnchor044"/>Summary</h1>
<p lang="en-GB">In this chapter, we learned about the basic concepts of rendering, rasterization, and shading, including light source models, the Lambertian shading model, and the Phong lighting model. We learned how to implement rendering, rasterization, and shading using PyTorch3D. We also learned how to change the parameters in the rendering process, such as ambient lighting, shininess, and specular colors, and how these parameters would affect the rendering results. </p>
<p lang="en-GB">We then learned how to use the PyTorch optimizer. We went through a coding example, where the PyTorch optimizer was used on a PyTorch3D mini-batch. In the last part of the chapter, we learned how to use the PyTorch3D APIs for converting between the different representations or rotations <span class="No-Break" lang="">and transformations.</span></p>
<p lang="en-GB">In the next chapter, we will learn some more advanced techniques for using deformable mesh models for fitting real-world <span class="No-Break" lang="">3D data.</span></p>
</div>
</div>

<div id="sbo-rt-content"><div>
<div class="Basic-Graphics-Frame" id="_idContainer061">
</div>
</div>
<div class="Content" id="_idContainer062">
<h1 id="_idParaDest-44" lang="en-GB"><a id="_idTextAnchor045"/>PART 2: 3D Deep Learning Using PyTorch3D</h1>
<p lang="en-GB">This part will cover some basic 3D computer vision processing using PyTorch3D. Implementing these 3D computer vision algorithms may become easier by using PyTorch3D. The readers will get a lot of hands-on experience working with meshes, point clouds, and fitting <span class="No-Break" lang="">from images.</span></p>
<p lang="en-GB">This part includes the <span class="No-Break" lang="">following chapters:</span></p>
<ul>
<li lang="en-GB"><a href="B18217_03.xhtml#_idTextAnchor046"><em class="italic" lang="">Chapter 3</em></a>, <em class="italic" lang="">Fitting Deformable Mesh Models to Raw Point Clouds</em></li>
<li lang="en-GB"><a href="B18217_04.xhtml#_idTextAnchor059"><em class="italic" lang="">Chapter 4</em></a>, <em class="italic" lang="">Learning Object Pose Detection and Tracking by Differentiable Rendering</em></li>
<li lang="en-GB"><a href="B18217_05.xhtml#_idTextAnchor070"><em class="italic" lang="">Chapter 5</em></a>, <em class="italic" lang="">Understanding Differentiable Volumetric Rendering</em></li>
<li lang="en-GB"><a href="B18217_06.xhtml#_idTextAnchor081"><em class="italic" lang="">Chapter 6</em></a>, <em class="italic" lang="">Exploring Neural Radiance Fields (NeRF)</em></li>
</ul>
</div>
<div>
<div id="_idContainer063">
</div>
</div>
</div></body></html>