<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Reinforcement Learning for Gaming</h1>
                </header>
            
            <article>
                
<p>In this chapter, we will learn about reinforcement learning. As the name suggests, with this method, optimal strategies are discovered through reinforcing or rewarding certain behavior and penalizing other behavior. The basic <span><span>idea </span></span>for this type of machine learning is to use an agent that performs actions towards a goal <span>in an environment</span><span>. We will explore this machine learning technique by using the</span> <kbd>ReinforcementLearning</kbd> <span>package in R to compute a policy for the agent to win a game of tic-tac-toe.</span></p>
<p>While this may seem like a simple game, it is a good environment for investigating reinforcement learning. We will learn how to structure input data for reinforcement learning, which is the same format for tic-tac-toe as for more complex games. We will learn how to compute a policy using the input data to provide the agent with the optimal strategy for the environment. We will also look at the hyperparameters available with this type of machine learning and the effect of adjusting these values.</p>
<p>Throughout this chapter, we will complete the following tasks:</p>
<ul>
<li>Understanding the concept of reinforcement learning</li>
<li>Preparing and preprocessing data </li>
<li>Configuring a reinforcement learning agent</li>
<li>Tuning hyperparameters</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p><span>You can find the code files of this chapter at</span> GitHub link at <a href="https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R">https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the concept of reinforcement learning</h1>
                </header>
            
            <article>
                
<p>Reinforcement learning is the last of the three most broad categories of machine learning. We have already studied supervised learning and unsupervised learning. Reinforcement learning is the third broad category and differs from the other two types in significant ways. Reinforcement learning neither trains on labeled data nor adds labels to data. Instead, it seeks to find an optimal solution for an agent to receive the highest reward.</p>
<p>The environment is the space where the agent completes its task. In our case, the environment will be the 3 x 3 grid used to play the game tic-tac-toe. The agent performs tasks within the environment. In this case, the agent places the X's or O's on the grid. The environment also contains rewards and penalties—that is, the agent needs to be rewarded for certain actions and penalized for others. In tic-tac-toe, if a player places marks (X or O) in three consecutive spaces either horizontally, vertically, or diagonally, then they win and conversely the other player loses. This is the simple reward and penalty structure for this game. The policy is the strategy that dictates which actions the agent should take to lead to the greatest probability for success given any set of previous actions.</p>
<p>To determine the optimal policy, we will be using Q-learning. The Q in Q-learning stands for quality. It involves developing a quality matrix to determine the best course of action. This involves using the Bellman equation. The interior of the equation calculates the reward value plus the discounted maximum value of future moves minus the current quality score. This calculated value is then multiplied by the learning rate and added to the current quality score. Later, we will see how to write this equation using R.</p>
<p class="mce-root">In this chapter, we are using Q-learning; however, there are other ways to perform reinforcement learning. Another popular algorithm is called <strong>actor–critic</strong> and it differs from Q-learning in significant ways. The following paragraph is a comparison of the two to better show the different approaches to pursuing the same type of machine learning.</p>
<p>Q-learning computes a value function, so it requires a finite set of actions, such as the game tic-tac-toe. Actor–critic works with a continuous environment and seeks to optimize the policy without a value function like Q-learning does. Instead, actor–critic has two models. One of them, the actor, performs actions while the other, the critic, calculates the value function. This takes place for each action, and over numerous iterations, the actor learns the best set of actions. While Q-learning works well for solving a game like tic-tac-toe, which has a finite space and set of moves, actor–critic works well for environments that are not constrained or that change dynamically.</p>
<p>In this section, we quickly reviewed the different methods for performing reinforcement learning. Next, we will begin to implement Q-learning on our tic-tac-toe data.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Preparing and processing data</h1>
                </header>
            
            <article>
                
<p>For our first task, we will use the tic-tac-toe dataset from the <kbd>ReinforcementLearning</kbd> package. In this case, the dataset is built for us; however, we will investigate how it is made to understand how to get data into the proper format for reinforcement learning:</p>
<ol>
<li>First, let's load the tic-tac-toe data. To load the dataset, we first load the <kbd>ReinforcementLearning</kbd> library and then call the <kbd>data</kbd> function with <kbd>"tictactoe"</kbd> as the argument. We load our data by running the following code:</li>
</ol>
<pre style="padding-left: 60px">library(ReinforcementLearning)<br/><br/>data("tictactoe")</pre>
<p style="padding-left: 60px">After running these lines, you will see the data object in the data <strong>Environment</strong> pane. Its current type is <kbd>&lt;Promise&gt;</kbd>; however, we will change that in the next step to see what is contained in this object. For now, your <span class="packt_screen">Environment</span> pane will look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f4b61246-8a29-4e82-a853-0dc2adb682da.png" style="width:27.67em;height:8.00em;"/></p>
<ol start="2">
<li>Now, let's look at the first few rows to evaluate what the dataset contains. We will use the <kbd>head</kbd> function to print the first few rows to the console and this will also convert the object in our <strong>Environment</strong> pane from <kbd>&lt;Promise&gt;</kbd> to an object that we can interact with and explore. We print the first five rows to our console using the following code:</li>
</ol>
<pre style="padding-left: 60px">head(tictactoe, 5)</pre>
<p style="padding-left: 60px"> </p>
<p style="padding-left: 60px">After running the code, your console will look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/62c086d5-bbd8-491a-8bc5-8ce1426fa4c8.png" style="width:20.17em;height:9.58em;"/></p>
<p style="padding-left: 60px">In addition, the object in the <span class="packt_screen">Environment</span> pane will now look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/288e4e5f-2255-4d45-817c-fd07e7fd2681.png" style="width:38.33em;height:13.58em;"/></p>
<p style="padding-left: 60px">As we look at these images, we can see the way that this data is set up. In order to conduct reinforcement learning, we need our data to be in the format where one column is the current state, another is the action, and then the subsequent state, and lastly the reward. Let's take the first row and explain exactly what the value means.<br/>
<br/>
The <kbd>State</kbd> is <kbd>"........."</kbd>. The dots indicate that the space on the 3 x 3 grid is blank, so this character string represents a blank tic-tac-toe board. The <kbd>Action</kbd> is <kbd>"c7"</kbd>. This means that the agent who is playing as X will place an X in the seventh spot, which is the bottom-left corner. The <kbd>NextState</kbd> is <kbd>"......X.B"</kbd>, which means that in this scenario, for this row, the opponent has placed an O in the bottom-right corner. The <kbd>Reward</kbd> is <kbd>0</kbd> because the game has not ended and the value of <kbd>0</kbd> for <kbd>Reward</kbd> indicates a neutral state where the game will continue. A row like this will exist for every possible combination of values for <kbd>State</kbd>, <kbd>Action</kbd>, <kbd>NextState</kbd>, and <kbd>Reward</kbd><strong>.</strong></p>
<ol start="3">
<li>Using only the top five rows, we can see that all possible moves are nonterminal, which is to say that the game continues after the move. Let's now look at the moves that lead to the conclusion of the game:</li>
</ol>
<pre style="padding-left: 60px">tictactoe %&gt;%<br/>  dplyr::filter(Reward == 1) %&gt;%<br/>  head()<br/><br/><br/>tictactoe %&gt;%<br/>  dplyr::filter(Reward == -1) %&gt;%<br/>  head()</pre>
<p style="padding-left: 60px">After running the preceding code, we will see the following rows printed to our console for actions that lead to victory:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/92c76c14-d5f1-48d0-a0ef-be0d36d4f1ea.png" style="width:20.75em;height:9.67em;"/></p>
<p style="padding-left: 60px">We will also see these rows printed to our console for moves that lead to defeat:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d8480855-a816-4849-aec6-24c5b36fc69f.png" style="width:20.50em;height:9.58em;"/></p>
<p style="padding-left: 60px">Let's look at the first row from the subset that leads to victory. In this case, the agent already has an X in the top-right corner and the center of the game board. Here, the agent places an X in the bottom-left corner, and this results in three consecutive X's along a diagonal, which means the agent has won the game, which we see reflected in the <kbd>Reward</kbd> column.</p>
<ol start="4">
<li>Next, let's look at a given state and see all possible moves:</li>
</ol>
<pre style="padding-left: 60px">tictactoe %&gt;%<br/> dplyr::filter(State == 'XB..X.XBB') %&gt;%<br/>  dplyr::distinct()</pre>
<p style="padding-left: 60px">Subsetting our data this way, we start from a given state and see all possible options. The printout to your console will look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1418b1c2-3821-4406-a80b-14830fc71ab7.png" style="width:20.50em;height:6.92em;"/></p>
<p style="padding-left: 60px">In this case, there are only three spaces left on the game board. We can see that two moves lead to victory for the agent. If the agent selects the other space on the game board, then there are two remaining spaces on the board, and we can see that, regardless of which one the opponent chooses, the game continues.</p>
<p>From this investigation, we can see how to prepare a dataset for reinforcement learning. Even though this one was done for us, we could see just how we would make one ourselves. If we wanted to code our tic-tac-toe board differently, we could use the values from the game Number Scramble. Number Scramble is isomorphic to tic-tac-toe, but involves choosing numbers rather than placing marks on a grid; however, the number values match perfectly to the grid, so the values can be swapped. The game of Number Scramble involves selecting numbers between 1 and 15 between two players, where no number can be selected twice and the winner is the first to select numbers that sum up to 15. With this in mind, we could rewrite the first row that we looked at like this:</p>
<pre>State &lt;- '0,0'<br/>Action &lt;- '4'<br/>NextState &lt;- '4,8'<br/>Reward &lt;- 0<br/><br/>numberscramble &lt;- tibble::tibble(<br/>  State = State,<br/>  Action = Action,<br/>  NextState = NextState,<br/>  Reward = Reward<br/>)<br/><br/>numberscramble</pre>
<p>After running this, we would get the following printed to our console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5a61c3b8-8679-4458-9820-fef283216fc5.png" style="width:16.00em;height:5.00em;"/></p>
<p>From this, we can see that the values for <kbd>State</kbd>, <kbd>Action</kbd>, and <kbd>NextState</kbd> can be encoded in any way that we like as long as a consistent convention is used so that the reinforcement learning process can traverse from state to state to discover the optimal path to reward.</p>
<p>Now that we know how to set up our data, let's move on to looking at exactly how our agent will find the best way to reach the reward.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the reinforcement agent</h1>
                </header>
            
            <article>
                
<p>Let's go into the details of what is happening to configure a reinforcement agent using Q-learning. The goal of Q-learning is to create a state–action matrix where a value is assigned for all state–action combinations—that is, if our agent is at any given state, then the values provided determine the action the agent will take to obtain maximum value. We are going to enable the computation of the best policy for our agent by creating a value matrix that provides a calculated value for every possible move:</p>
<ol>
<li>To start, we need a set of state and action pairs that all have a value of 0. As a best practice, we will use hashing here, which is a more efficient alternative to large lists for scaling up to more complex environments. To begin, we will load the hash library and then we will use a <kbd>for</kbd> loop to populate the hash environment. The <kbd>for</kbd> loop starts by getting every unique state from the data, and for every unique state, it then appends every unique action to create all possible state–action pairs and assigns all pairs a value of 0. We generate this hash environment that will hold the values calculated during the Q-learning phase by running the following code:</li>
</ol>
<pre style="padding-left: 60px">library(hash)<br/><br/>Q &lt;- hash()<br/><br/>for (i in unique(tictactoe$State)[!unique(tictactoe$State) %in% names(Q)]) {<br/> Q[[i]] &lt;- hash(unique(tictactoe$Action), rep(0, length(unique(tictactoe$Action))))<br/>}</pre>
<p style="padding-left: 60px">After running the code, we will see that our <strong>Environment</strong> pane now looks like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f802eb4e-3660-45a5-8529-8aadb4fac82b.png" style="width:31.42em;height:12.50em;"/></p>
<p style="padding-left: 60px">We have a hash environment, <kbd>Q</kbd>, that contains every state-action pair.</p>
<ol start="2">
<li>The next step is to define the hyperparameters. For now, we will use the default values; however, we will soon tune these to see the impact. We set the hyperparameters to their default values by running the following code:</li>
</ol>
<pre style="padding-left: 60px">control = list(<br/> alpha = 0.1, <br/> gamma = 0.1, <br/> epsilon = 0.1<br/> )</pre>
<p style="padding-left: 60px">After running the code, we can now see that we have a list with our hyperparameter values in our <span class="packt_screen">Environment</span> pane, which now looks like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/a3bfbff9-c5ef-4fb1-8820-875de3044620.png" style="width:26.92em;height:16.58em;"/></p>
<ol start="3">
<li>Next, we begin to populate our Q matrix. This again takes place within a <kbd>for</kbd> loop; however, we will look at one isolated iteration. We start by taking a row and moving the elements from this row to discrete data objects using the following code:</li>
</ol>
<pre style="padding-left: 60px">  d &lt;- tictactoe[1, ]<br/>  state &lt;- d$State<br/>  action &lt;- d$Action<br/>  reward &lt;- d$Reward<br/>  nextState &lt;- d$NextState</pre>
<p style="padding-left: 60px">After running the code, we can see the changes to our <strong>Environment</strong> pane, which now contains the discrete elements from the first row. The <span class="packt_screen">Environment</span> pane will look like the following:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/9884af2f-f684-429c-8ce8-29ea8628f8c5.png" style="width:23.50em;height:24.42em;"/></p>
<ol start="4">
<li>Next, we get a value for the current Q-learning score if there is one. If there isn't a value, then <kbd>0</kbd> is stored as the current value. We set this initial quality value score by running the following code:</li>
</ol>
<pre style="padding-left: 60px">  currentQ &lt;- Q[[state]][[action]]<br/>  if (has.key(nextState,Q)) {<br/>    maxNextQ &lt;- max(values(Q[[nextState]]))<br/>  } else {<br/>    maxNextQ &lt;- 0<br/>  }</pre>
<p style="padding-left: 60px">After running this code, we now have a value for <kbd>currentQ</kbd>, which is <kbd>0</kbd> in this case because all values in Q for the state <kbd>'......X.B'</kbd> are <kbd>0</kbd>, as we have set all values to <kbd>0</kbd>; however, in the next step, we will begin to update the Q values.</p>
<ol start="5">
<li>Lastly, we update the Q value by using the Bellman equation. This is also called <strong>temporal difference</strong> <strong>learning</strong>. We write out this step for computing values with this equation for R using the following code:</li>
</ol>
<pre style="padding-left: 60px">  ## Bellman equation<br/>  Q[[state]][[action]] &lt;- currentQ + control$alpha *<br/>    (reward + control$gamma * maxNextQ - currentQ)<br/><br/>q_value &lt;- Q[[tictactoe$State[1]]][[tictactoe$Action[1]]]</pre>
<p style="padding-left: 60px">After running the following code, we can pull out the updated value for this state–action pair; we can see it in the field labeled <kbd>q_value</kbd>. Your <strong>Environment</strong> pane will look like the following screenshot:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7ad7e1e0-875a-4992-b1fe-3f3bed94b04e.png" style="width:27.92em;height:32.08em;"/></p>
<p style="padding-left: 60px">We note here that the <kbd>q_value</kbd> is still <kbd>0</kbd>. Why is this the case? If we look at our equation, we will see that the reward is part of the equation and our reward is <kbd>0</kbd>, which makes the entire calculated value <kbd>0</kbd>. As a result, we will not begin to see updated Q values until our code encounters a row with a nonzero reward.</p>
<ol start="6">
<li>We can now put all of these steps together and run them over every row to create our Q matrix. We create the matrix of values that we will use to select the policy for optimal strategy by running the following code, which wraps all the previous code together in a <kbd>for</kbd> loop:</li>
</ol>
<pre style="padding-left: 60px">for (i in 1:nrow(tictactoe) {<br/>  d &lt;- tictactoe[i, ]<br/>  state &lt;- d$State<br/>  action &lt;- d$Action<br/>  reward &lt;- d$Reward<br/>  nextState &lt;- d$NextState<br/>  <br/>  currentQ &lt;- Q[[state]][[action]]<br/>  if (has.key(nextState,Q)) {<br/>    maxNextQ &lt;- max(values(Q[[nextState]]))<br/>  } else {<br/>    maxNextQ &lt;- 0<br/>  }<br/>  ## Bellman equation<br/>  Q[[state]][[action]] &lt;- currentQ + control$alpha *<br/>    (reward + control$gamma * maxNextQ - currentQ)<br/>}<br/><br/>Q[[tictactoe$State[234543]]][[tictactoe$Action[234543]]]</pre>
<p>After looping through all the rows, we see that some state–action pairs do now have a value in the Q matrix. Running the following code, we will see the following value printed to our console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/e2db8f77-2cb1-483f-84bc-8455f72227c4.png" style="width:34.58em;height:4.00em;"/></p>
<p>At this point, we have now created our matrix for Q-learning. In this case, we are storing the value in a hash environment with values for every key-value pairing; however, this is equivalent to storing the values in a matrix—it is just more efficient for scaling up later. Now that we have these values, we can compute a policy for our agent that will provide the best path to a reward; however, before we compute this policy, we will make one last set of modifications, and that is to tune the hyperparameters that we set earlier to their default values.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Tuning hyperparameters</h1>
                </header>
            
            <article>
                
<p>We have now defined our environment and iterated over all possible actions and results from any given state to calculate the quality value of every move and stored these values in our Q object. At this point, we can now begin to tune the options for this model to see how it impacts performance.</p>
<p>If we recall, there are three parameters for reinforcement learning, and these are alpha, gamma, and epsilon. The following list describes the role of each parameter and the impact of adjusting their value:</p>
<ul>
<li><strong>Alpha</strong>: The alpha rate for reinforcement learning is the same as the learning rate for many other machine learning models. It is the constant value used to control how quickly probabilities are updated as calculations are made based on exploring rewards for the agent taking certain actions.</li>
<li><strong>Gamma</strong>: Adjusting gamma adjusts how much the model values future rewards. When gamma is set to <kbd>1</kbd>, then all rewards current and future are valued equally. This means that a reward that is several steps away is worth as much as a reward that is earned at the next step. Practically, this is almost never what we want since we want future rewards to be more valuable since it takes more effort to earn them. By contrast, setting gamma to <kbd>0</kbd> means that only rewards from the next action will have any value. Future rewards have no value at all. Again, aside from special cases, this is not desirable. When adjusting gamma, you have to seek the balance in weighting among future rewards that leads the agent to make an optimal selection of actions.</li>
<li><strong>Epsilon</strong>: The epsilon parameter is used to introduce randomness when selecting future actions. Setting epsilon to <kbd>0</kbd> is referred to as greedy learning. In this case, the agent will always take the path with the highest probability of success; however, in this case, as with other machine learning, it is easy for an agent to get lost in some local minima and never discover an optimal strategy. By introducing some randomness, different actions will be pursued over different iterations. Adjusting this value optimizes the exploration to exploit balance. We want the model to exploit what has been learned to choose the best future action; however, we also want the model to explore and continue learning.</li>
</ul>
<p>Using what we know about these hyperparameters, let's see how the values change as we make adjustments to these parameters:</p>
<ol>
<li>First, we will make adjustments to the value of <kbd>alpha</kbd>. As noted, the <kbd>alpha</kbd> value is the learning rate value, which we may be familiar with from learning about other machine learning topics. It is just the constant value that controls how quickly the algorithm makes adjustments. Currently, we have an <kbd>alpha</kbd> rate set at <kbd>0.1</kbd>; however, let's set our <kbd>alpha</kbd> rate at <kbd>0.5</kbd>. This is higher than we would usually ever want it to be in practice and is used here simply to explore the impact of changing these values. We will need to reset Q to all zeroes and restart the learning process to see what happens. The following code block takes everything we just did previously and runs it all again with the one adjustment to <kbd>alpha</kbd>. We tune the <kbd>alpha</kbd> value and see the effect by running the following code:</li>
</ol>
<pre style="padding-left: 60px">Q &lt;- hash()<br/><br/>for (i in unique(tictactoe$State)[!unique(tictactoe$State) %in% names(Q)]) {<br/>  Q[[i]] &lt;- hash(unique(tictactoe$Action), rep(0, length(unique(tictactoe$Action))))<br/>}<br/><br/>control = list(<br/>  alpha = 0.5, <br/>  gamma = 0.1,  <br/>  epsilon = 0.1<br/>)<br/><br/>for (i in 1:nrow(tictactoe)) {<br/>  d &lt;- tictactoe[i, ]<br/>  state &lt;- d$State<br/>  action &lt;- d$Action<br/>  reward &lt;- d$Reward<br/>  nextState &lt;- d$NextState<br/>  <br/>  currentQ &lt;- Q[[state]][[action]]<br/>  if (has.key(nextState,Q)) {<br/>    maxNextQ &lt;- max(values(Q[[nextState]]))<br/>  } else {<br/>   maxNextQ &lt;- 0<br/>  }<br/>  ## Bellman equation<br/>  Q[[state]][[action]] &lt;- currentQ + control$alpha *<br/>    (reward + control$gamma * maxNextQ - currentQ)</pre>
<pre style="padding-left: 60px">}<br/><br/>Q[[tictactoe$State[234543]]][[tictactoe$Action[234543]]]</pre>
<p style="padding-left: 60px">We can see from this tuning that we get a different value for the Q value at <kbd>234543</kbd>. You will see the following printed out to your console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/1c9cb469-4878-4e9c-bb0f-64e28f731d82.png" style="width:33.67em;height:2.75em;"/></p>
<p style="padding-left: 60px">As we might have expected, since we increased the value for alpha, we, as a result, ended up with a larger value for the Q value at the same point that we looked at previously. In other words, our algorithm learned faster and the quality value received a greater amount of weight.</p>
<ol start="2">
<li>Next, let's tune the value for <kbd>gamma</kbd>. If we recall, adjusting this value for gamma will alter how much the agent values future rewards. Our value is currently set at <kbd>0.1</kbd>, which means that future rewards do have value, but the level at which they are valued is relatively small. Let's boost this up to <kbd>0.9</kbd> and see what happens. We go through the same operation as we did when we adjusted alpha. We start by resetting the Q hash environment so that all state–action pairs have a value of <kbd>0</kbd> and then we repopulate this hash environment by looping through all options, applying the Bellman equation by making our own changes to the <kbd>gamma</kbd> value. We assess what happens when we change the <kbd>gamma</kbd> value by running the following code:</li>
</ol>
<pre style="padding-left: 60px">library(hash)<br/><br/>Q &lt;- hash()<br/><br/>for (i in unique(tictactoe$State)[!unique(tictactoe$State) %in% names(Q)]) {<br/>  Q[[i]] &lt;- hash(unique(tictactoe$Action), rep(0, length(unique(tictactoe$Action))))<br/>}<br/><br/>control = list(<br/>  alpha = 0.1, <br/>  gamma = 0.9,  <br/>  epsilon = 0.1<br/>)<br/><br/>for (i in 1:nrow(tictactoe)) {<br/>  d &lt;- tictactoe[i, ]<br/>  state &lt;- d$State<br/>  action &lt;- d$Action<br/>  reward &lt;- d$Reward<br/>  nextState &lt;- d$NextState<br/>  <br/>  currentQ &lt;- Q[[state]][[action]]<br/>  if (has.key(nextState,Q)) {<br/>    maxNextQ &lt;- max(values(Q[[nextState]]))<br/>  } else {<br/>    maxNextQ &lt;- 0<br/>  }<br/>  ## Bellman equation<br/>  Q[[state]][[action]] &lt;- currentQ + control$alpha *<br/>    (reward + control$gamma * maxNextQ - currentQ)<br/>}<br/><br/>Q[[tictactoe$State[234543]]][[tictactoe$Action[234543]]]</pre>
<p style="padding-left: 60px">After running this code, you will see the following code printed on your console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/d7316a87-4159-4839-b6f9-001edd3a9989.png" style="width:39.08em;height:5.08em;"/></p>
<p style="padding-left: 60px">From this, we can make the following observations:</p>
<ul>
<li style="padding-left: 60px">We can see that our value increased quite significantly</li>
<li style="padding-left: 60px">From this state and taking this action, there is value, but when considering future rewards there is even more value</li>
<li style="padding-left: 60px">However, with a game like tic-tac-toe, we need to consider that there are never many steps between any state and a reward; however, we can see that from this state and this action, there will be a good probability of getting a reward</li>
</ul>
<ol start="3">
<li>For our final adjustment, we will adjust the <kbd>epsilon</kbd>. The value of <kbd>epsilon</kbd> relies on how much previous knowledge is used compared with how much exploration is done to gather knowledge. For this adjustment, we will go back to using the function from the <kbd>ReinforcementLearning</kbd> package, since it relies on not only looping through the Bellman equation, but also storing these values for reference over multiple iterations. To adjust <kbd>epsilon</kbd>, we use the following code:</li>
</ol>
<pre style="padding-left: 60px"># Define control object<br/>control &lt;- list(<br/>alpha = 0.1, <br/>gamma = 0.1, <br/>epsilon = 0.9<br/>)<br/><br/># Perform reinforcement learning<br/>model &lt;- ReinforcementLearning(data = tictactoe, <br/>                               s = "State", <br/>                               a = "Action", <br/>                               r = "Reward", <br/>                               s_new = "NextState", <br/>                               iter = 5,<br/>                               control = control)<br/><br/>model$Q_hash[[tictactoe$State[234543]]][[tictactoe$Action[234543]]]</pre>
<p style="padding-left: 60px">After running this code, we will see that our Q value has changed. You will see the following value printed to the console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/8db9a1a2-7374-447f-bbb0-f2e1d57862bb.png" style="width:40.83em;height:4.17em;"/></p>
<p style="padding-left: 60px"><span>From this, we can make the following observations:</span></p>
<ul>
<li style="padding-left: 60px">Our value is similar to the value that we had when we were using our default values for parameters, but slightly larger</li>
<li style="padding-left: 60px">In this case, we have introduced a relatively large amount of randomness to force our agent to continue exploring; as a result, we can see that we have not lost so much value with this randomness, and this action retains a similar value even when it leads to different sets of subsequent actions</li>
</ul>
<ol start="4">
<li>After tuning the parameters to the desired settings, we can now view the policy for a given state. First, let's take a look at the model object in our <strong>Environment</strong> pane. Your <strong>Environment</strong> pane will look like the following:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/62c571c6-90d6-41df-8db1-3223277e7e82.png" style="width:40.17em;height:33.42em;"/></p>
<p>Let's look more in-depth at every element in the model object:</p>
<ul>
<li><kbd>Q_hash</kbd>: The hash environment, just like we created earlier, which includes every state–action pair, along with a Q value</li>
<li><kbd>Q</kbd>: A named matrix that contains the same data as the hash environment, except in the form of a named matrix</li>
<li><kbd>States</kbd>: The named rows from our matrix</li>
<li><kbd>Actions</kbd>: The named columns from our matrix</li>
<li><kbd>Policy</kbd>: A named vector that contains the optimal action that the agent should take from any state</li>
<li><kbd>Reward</kbd> and <kbd>RewardSequence</kbd>: These are the number of rows from the dataset that lead to a reward less than the number that lead to a penalty</li>
</ul>
<p>We can use the values from here to see the value of all actions at any given state and judge which is the best move to make. Let's start with a brand new game and see which move has the most value. We can see the value of every action from this state and note which action is best by running the following code:</p>
<pre>sort(model$Q['.........',1:9], decreasing = TRUE)<br/><br/>model$Policy['.........']</pre>
<p>After running this code, we will see the following printed to the console:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/7aa1b28b-40f0-4a49-a323-f6d0eff91e52.png" style="width:47.75em;height:14.00em;"/></p>
<p>We can see that our first line lists every possible action along with their respective values in descending order. We can see that from this named vector, the move <kbd>"c5"</kbd>, which is a mark at the center of the grid, has the highest value. Consequently, when we view the policy for our agent when it is at that state, we see that it is <kbd>"c5"</kbd>. In this way, we can now use the results of reinforcement learning to choose the optimal move from any given state:</p>
<ul>
<li>We just adjusted all the parameters to note the effects of changing these variables</li>
<li>Then, in the last step, we saw how to select the best policy based on the grid being in any state</li>
<li>Through trying every possible combination of actions, we calculated the value of moves based on immediate and future rewards</li>
<li>We decided to weigh the Q value by adjusting our parameters and decided upon a method to solve games</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we have coded a reinforcement-learning system using Q-learning. We defined our environment or playing surface and then looked at the dataset containing every possible combination of states, actions, and <span><span>future </span></span>states. Using the dataset, we calculated the value of every state–action pair, which we stored in a hash environment and also as a matrix. We then used this matrix of values as the basis of our policy, which selects the move with the most value.</p>
<p>In our next chapter, we will expand on Q-learning by adding neural networks to create deep Q-learning networks.</p>


            </article>

            
        </section>
    </body></html>