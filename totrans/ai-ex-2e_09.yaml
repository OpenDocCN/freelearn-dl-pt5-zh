- en: '9'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Abstract Image Classification with Convolutional Neural Networks (CNNs)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The invention of **convolutional neural networks** (**CNNs**) applied to vision
    represents by far one of the most innovative achievements in the history of applied
    mathematics. With their multiple layers (visible and hidden), CNNs have brought
    artificial intelligence from machine learning to deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: In *Chapter 8*, *Solving the XOR Problem with a Feedforward Neural Network*,
    we saw that *f*(*x*, *w*) is the building block of any neural network. A function
    *f* will transform an input *x* with weights *w* to produce an output. This output
    can be used as such or fed into another layer. In this chapter, we will generalize
    this principle and introduce several layers. At the same time, we will use datasets
    with images. We will have a dataset for training and a dataset for validation
    to confirm that our model works.
  prefs: []
  type: TYPE_NORMAL
- en: 'A CNN relies on two basic tools of linear algebra: kernels and functions, applying
    them to convolutions as described in this chapter. These tools have been used
    in mathematics for decades.'
  prefs: []
  type: TYPE_NORMAL
- en: However, it took the incredible imagination of Yann LeCun, Yoshua Bengio, and
    others—who built a mathematical model of several layers—to solve real-life problems
    with CNNs.
  prefs: []
  type: TYPE_NORMAL
- en: This chapter describes the marvels of CNNs, one of the pillars of **artificial
    neural networks** (**ANNs**). A CNN will be built from scratch, trained, and saved.
    The classification model described will detect production failures on a food-processing
    production line. Image detection will go beyond object recognition and produce
    abstract results in the form of concepts.
  prefs: []
  type: TYPE_NORMAL
- en: A Python TensorFlow 2 program will be built layer by layer and trained. Additional
    sample programs will illustrate key functions.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: The differences between 1D, 2D, and 3D CNNs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Adding layers to a convolutional neural network
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernels and filters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Shaping images
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The ReLU activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Kernel initialization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Flattening
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Dense layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cross-entropy loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Adam optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Saving the model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Visualizing the PNG of a model
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We'll begin by introducing CNNs and defining what they are.
  prefs: []
  type: TYPE_NORMAL
- en: Introducing CNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This section describes the basic components of a CNN. `CNN_SRATEGY_MODEL.py`
    will illustrate the basic CNN components used to build a model for abstract image
    detection. For machines, as for humans, concepts are the building blocks of cognition.
    CNNs constitute one of the pillars of deep learning (multiple layers and neurons).
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, TensorFlow 2 with Python will be running using Keras libraries
    that are now part of TensorFlow. If you do not have Python or do not wish to follow
    the programming exercises, the chapter is self-contained, with graphs and explanations.
  prefs: []
  type: TYPE_NORMAL
- en: Defining a CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A convolutional neural network processes information, such as an image, for
    example, and makes sense out of it.
  prefs: []
  type: TYPE_NORMAL
- en: For example, imagine you have to represent the sun with an ordinary pencil and
    a piece of paper. It is a sunny day, and the sun is shining very brightly—too
    brightly. You put on a special pair of very dense sunglasses. Now you can look
    at the sun for a few seconds. You have just applied a color reduction filter,
    one of the first operations of a convolutional network.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, you try to draw the sun. You draw a circle and put some gray in the middle.
    You have just applied an edge filter. Finally, you go over the circle several
    times to make it easy to recognize, progressively reducing what you saw into a
    representation of it. Now, with the circle, some gray in the middle, and a few
    lines of rays around it, anybody can see you drew the sun. You smile; you did
    it! You took a color image of the sun and made a mathematical representation of
    it as a circle, which would probably look something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_01.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.1: Mathematical representation of a circle'
  prefs: []
  type: TYPE_NORMAL
- en: You just went through the basic processes of a convolutional network.
  prefs: []
  type: TYPE_NORMAL
- en: The word **convolutional** means that you transformed the sun you were looking
    at into a drawing, area by area. But, you did not look at the whole sky at once.
    You made many eye movements to capture the sun, area by area, and you did the
    same when drawing. If you made a mathematical representation of the way you transformed
    each area from your vision to your paper abstraction, it would be a kernel. You
    can see that the convolutional operation converts an object into a more abstract
    representation. This is not limited to images but can apply to any type of data
    (words, sounds and video) we want to draw patterns from.
  prefs: []
  type: TYPE_NORMAL
- en: With that concept in mind, the following graph shows the successive mathematical
    steps to follow in this chapter's model for a machine to process an image just
    as you did. A convolutional network is a succession of steps that will transform
    what you see into a classification status.
  prefs: []
  type: TYPE_NORMAL
- en: In the graph, each box represents a layer. Each layer has an input that comes
    from the previous layer. Each layer will then transform the input and then produce
    an output that will become the input of the next layer. At each layer, the key
    features that are necessary to classify the images will be isolated.
  prefs: []
  type: TYPE_NORMAL
- en: In your example, it would serve to find out whether your drawing represents
    the sun or not. This falls under a binary classification model (yes or no, or
    1 or 0).
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_02.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.2: Architecture of a CNN'
  prefs: []
  type: TYPE_NORMAL
- en: Notice that the size of the outputs diminishes progressively until the outputs
    reach 1, the binary classification status that will return (1 or 0). These successive
    steps, or layers, represent what you did when you went from observing the sun
    to drawing it. In the end, if we draw poorly and nobody recognizes the sun, it
    means that we'll have to go back to step 1 and change some parameters (weights
    in this case). That way, we train to represent the sun better until somebody says,
    "Yes, it is a sun!" That is probability = 1\. Another person may say that it is
    not a sun (probability = 0). In that case, more training would be required.
  prefs: []
  type: TYPE_NORMAL
- en: If you carry out this experiment of drawing the sun, you will notice that, as
    a human, you transform one area at a time with your eye and pencil. You repeat
    the way you do it in each area. The mathematical repetition you perform is your
    **kernel**. Using a kernel per area is the fastest way to draw. For us humans,
    in fact, it is the only way we can draw. A CNN is based on this process.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we looked at some key aspects of a CNN model, using the analogy
    of representing the sun as a drawing. This is just one way to start a convolutional
    neural network, and there are hundreds of different ways to do so. However, once you
    understand one model, you will have the understanding necessary to implement other
    variations.
  prefs: []
  type: TYPE_NORMAL
- en: In the following section, we'll see how to initialize and build our own CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Initializing the CNN
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '`CNN_SRATEGY_MODEL.py` builds the CNN using TensorFlow 2\. TensorFlow 2 has
    made tremendous improvements in terms of development. The Keras datasets, layers,
    and models are now part of the TensorFlow instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: The CNN only requires two lines of headers to build the layers! In TensorFlow
    2, for each layer, we simply have to call `layers.<add your layer here>` and that's
    it!
  prefs: []
  type: TYPE_NORMAL
- en: 'The model used is a Keras `sequential()` called from the TensorFlow `from tensorflow.keras`
    instance:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: And that's it. We have just started to build our own CNN in just a few lines
    of code. TensorFlow 2 has simplified the whole process of creating a CNN, making
    it an easy, intuitive process, as we will see throughout this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Let's begin to build upon the foundations of our CNN in the following section
    and add a convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: Adding a 2D convolution layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this chapter, we will be using a two-dimensional model as our example. Two-dimensional
    relationships can be real-life images and also many other objects, as described
    in this chapter. This chapter describes a two-dimensional network, although others
    exist:'
  prefs: []
  type: TYPE_NORMAL
- en: A one-dimensional CNN mostly describes a temporal mode, for example, a sequence
    of sounds (phonemes = parts of words), words, numbers, and any other type of sequence.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A volumetric module is a 3D convolution, such as recognizing a cube or a video.
    For example, for a self-driving car, it is critical to recognize the difference
    between a 2D picture of a person in an advertisement near a road and a real 3D
    image of a pedestrian that is starting to cross the same road!
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, a spatial 2D convolution module will be applied to images of
    different kinds. The main program, `CNN_STRATEGY_MODEL.py`, will describe how
    to build and save a model.
  prefs: []
  type: TYPE_NORMAL
- en: '`classifier.add` will add a layer to the model. The name **classifier** does
    not represent a function but simply the arbitrary name that was given to this
    model in this particular program. The model will end up with *n* layers. Look
    at the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This line of code contains a lot of information: the filters (applied with
    kernels), the input shape, and an activation function. The function contains many
    more options. Once you understand these in-depth, you can implement other options
    one by one, as you deem necessary, for each project you have to work on.'
  prefs: []
  type: TYPE_NORMAL
- en: Kernel
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Just to get started, intuitively, let's take another everyday model. This model
    is a bit more mathematical and closer to a CNN's kernel representation. Imagine
    a floor of very small square tiles in an office building. You would like each
    floor tile to be converted from dirty to clean, for example.
  prefs: []
  type: TYPE_NORMAL
- en: You can imagine a cleaning machine capable of converting 3×3 small tiles (pixels)
    one at a time from dirty to clean. You would laugh if you saw somebody come with one
    enormous cleaning machine to clean all of the 32×32 tiles (pixels) at the same
    time. You know it would be very bulky, slow, and difficult to use, intuitively.
    On top of that, you would need one big machine per surface size! Not only is a
    kernel an efficient way to filter, but a kernel convolution is also a time-saving
    resource process. The small cleaning machine is the kernel (dirty-to-clean filter),
    which will save you time performing the convolution (going over all of the tiles
    to clean a 3×3 area), transforming the floor from dirty to clean.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, 32 different filters have been added with 3×3 sized kernels:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: The use of kernels as filters is the core of a convolutional network. `(32,
    (3,3))` means `(number of filters, (size of kernels))`.
  prefs: []
  type: TYPE_NORMAL
- en: An intuitive approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: To understand a kernel intuitively, keep the sun and cleaning tiles examples
    in mind. In this section, a photograph of a cat will show how kernels work.
  prefs: []
  type: TYPE_NORMAL
- en: 'In a model analyzing cats, the initial photograph would look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_03.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.3: Cat photograph for model analysis'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the first run of this layer, even with no training, an untrained kernel
    would transform the photograph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_04.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.4: Cat photograph transformation'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first layer has already begun isolating the features of the cat. The edges
    have begun to appear: the cat''s body, ears, nose, and eyes. In itself, this first
    filter (one of 32) with a size 3×3 kernel—in the first layer and with no training—already
    produces effective results. The size of a kernel can vary according to your needs.
    A 3×3 kernel will require a larger number of weights than a 1×1 kernel, for example.
    A 1×1 kernel will have only one weight, which restricts the size of the features
    to represent. The rule is that the smaller the kernel, the fewer weights we have
    to find. It will also perform a feature reduction. When the size of the kernel
    increases, the number of weights and features to find increases as well as the
    number of features represented.'
  prefs: []
  type: TYPE_NORMAL
- en: Each subsequent layer will make the features stand out much better, with smaller
    and smaller matrices and vectors, until the program obtains a clear mathematical
    representation.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an intuitive view of how a filter works, let's explore a developer's
    approach.
  prefs: []
  type: TYPE_NORMAL
- en: The developers' approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Developers like to see the result first to decide how to approach a problem.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s take a quick, tangible shortcut to understand kernels through `Edge_detection_Kernel.py`
    with an edge detection kernel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: The kernel is a 3×3 matrix, like the cat example. But the values are preset,
    and not trained with weights. There is no learning here; only a matrix needs to
    be applied. The major difference with a CNN is that it will learn how to optimize
    kernels itself through weights and biases.
  prefs: []
  type: TYPE_NORMAL
- en: '`img.bmp` is loaded, and the 3×3 matrix is applied to the pixels of the loaded
    image, area by area:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'The image before the convolution applying the kernel is the letter **A** (letter
    recognition):'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_05.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5: The letter "A"'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now the convolution transforms the image, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'The edges of A now appear clearly in white, as shown in the following graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_06.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6: The white edges of A are visible'
  prefs: []
  type: TYPE_NORMAL
- en: The original image on top displayed a very thick A. The preceding graph displays
    a thin, identifiable A feature through thin edges that a neural network can classify
    within a few mathematical operations. The first layers of a convolutional network
    train to find the right weights to generate the right kernel automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have an intuitive and practical developer's view of a filter, let's
    add some mathematics to our approach.
  prefs: []
  type: TYPE_NORMAL
- en: A mathematical approach
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'The initial image has a set of values you can display, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'The code will print a numerical output of the image, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The convolution filter is applied using `filter.convolve`, a mathematical function,
    to transform the image and filter it.
  prefs: []
  type: TYPE_NORMAL
- en: 'The convolution filter function uses several variables:'
  prefs: []
  type: TYPE_NORMAL
- en: The spatial index for the 3×3 kernel to apply; in this case, it must know how
    to access the data. This is performed through a spatial index, *j*, which manages
    data in grids. Databases also use spatial indexes to access data. The axes of
    those grids determine the density of a spatial index. Kernels and the image are
    convolved using *j* over *W*, the weights kernel.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*W* is the weights kernel.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*I* is the input image.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*k* is the coordinate of the center of *W*. The default value is 0 in this
    case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'These variables then enter the `filter.convolve` function as represented by
    the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_001.png)'
  prefs: []
  type: TYPE_IMG
- en: 'A CNN relies on kernels. Take all the time you need to explore convolutions
    through the three dimensions required to master AI: an intuitive approach, development
    testing, and mathematical representation.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have a mathematical idea on how a convolutional filter works, let's
    determine the shape and the activation function to the convolutional layer.
  prefs: []
  type: TYPE_NORMAL
- en: Shape
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: '`input_shape` defines the size of the image, which is 64×64 pixels (height×width),
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '`3` indicates the number of channels. In this case, `3` indicates the three
    parameters of an RGB color. Each channel can have a given value of 0 to 255.'
  prefs: []
  type: TYPE_NORMAL
- en: ReLU
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Activation functions provide useful ways to influence the transformation of
    weighted data calculations. Their output will change the course of classification,
    a prediction, or whatever goal the network was built for. This model applies a
    **rectified linear unit** (**ReLU**), as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'ReLU activation functions apply variations of the following function to an
    input value:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x*) = max{0, *x*}'
  prefs: []
  type: TYPE_NORMAL
- en: The function returns 0 for negative values; it returns positive values as *x*;
    it returns 0 for 0 values. Half of the domain of the function will return zeros.
    This means that when you provide positive values, the derivative will always be
    1\. ReLU avoids the squashing effect of the logistic sigmoid function, for example.
    However, the decision to use one activation function rather than another will
    depend on the goal of each ANN model.
  prefs: []
  type: TYPE_NORMAL
- en: In mathematical terms, a **rectified linear unit (ReLU)** function will take
    all the negative values and apply 0 to them. And all the positive values remain
    unchanged.
  prefs: []
  type: TYPE_NORMAL
- en: The `ReLU.py` program provides some functions, including a NumPy function, to
    test how ReLU works.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can enter test values or use the ones in the source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`nx` expects a negative value, and `px` expects a positive value for testing
    purposes for the `relu(x)` and `lrelu(x)` functions. Use the `f(x)` function if
    you wish to include zeros in your testing session.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `relu(x)` function will calculate the ReLU value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the program will return the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: The result of a negative value becomes 0, and a positive value remains unchanged.
    The derivative or slope is thus always 1, which is practical in many cases and
    provides good visibility when debugging a CNN or any other ANN.
  prefs: []
  type: TYPE_NORMAL
- en: 'The NumPy function, defined as follows, will provide the same results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: Through trial and error, ANN research has come up with several variations of
    ReLU.
  prefs: []
  type: TYPE_NORMAL
- en: One important example occurs when many input values are negative. ReLU will
    constantly produce zeros, making gradient descent difficult, if not impossible.
  prefs: []
  type: TYPE_NORMAL
- en: 'A clever solution was found using a leaky ReLU. A leaky ReLU does not return
    0 for a negative value but a small value you can choose, 0.1 instead of 0, for
    example. See the following equation:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(*x*) = max{0.1, *x*}'
  prefs: []
  type: TYPE_NORMAL
- en: The leaky ReLU fixes the problem of "dying" neurons. Suppose you have a layer
    that keeps returning negative values when activating neurons. The ReLU activation
    will always return 0 in this case. That means that these neurons are "dead." They
    will never be activated. To avoid these "dying" neurons, a leaky ReLU provides
    the small positive value seen previously (0.1) that makes sure that a neuron does
    not "die."
  prefs: []
  type: TYPE_NORMAL
- en: 'Now gradient descent will work fine. In the sample code, the function is implemented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: Although many other variations of ReLU exist, with this in mind, you have an
    idea of what it does.
  prefs: []
  type: TYPE_NORMAL
- en: 'Enter some values of your own, and the program will display the results, as
    shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'The results will display the ReLU results as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: We have processed a large representation of the input image. We now need to
    reduce the size of our representation to obtain a better, more abstract representation.
    By pooling some of the pixels we will also reduce the calculations of the subsequent
    layers.
  prefs: []
  type: TYPE_NORMAL
- en: Pooling
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'A CNN contains hidden layers. The input is visible. Then as the layers work
    to transform the data, "hidden" work goes on. The output layer is visible again.
    Let''s continue to explore the "hidden" layers! Pooling reduces the size of an
    input representation, in this case, an image. Max pooling consists of applying
    a max pooling window to a layer of the image:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This `pool_size` 2×2 window will first find the maximum value of the 2×2 matrix
    at the top left of the image matrix. This first maximum value is 4\. It is thus
    the first value of the pooling window on the right.
  prefs: []
  type: TYPE_NORMAL
- en: 'Then, the max pooling window hops over 2 squares and finds that 5 is the highest
    value. 5 is written in the max pooling window. The hop action is called a **stride**.
    A stride value of 2 will avoid overlapping, although some CNN models have strides
    that overlap. It all depends on your goal. Look at the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_07.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.7: Pooling example'
  prefs: []
  type: TYPE_NORMAL
- en: 'The output size has now gone from a 62×62×32 (number of filters) to a 31×31×32,
    as shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_08.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.8: Output size changes (pooling)'
  prefs: []
  type: TYPE_NORMAL
- en: Other pooling methods exist, such as average pooling, which uses the average
    of the pooling window and not the maximum value. This depends on the model and
    shows the hard work that needs to be put in to train a model.
  prefs: []
  type: TYPE_NORMAL
- en: Next convolution and pooling layer
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The next two layers of the CNN repeat the same method as the first two described
    previously, and it is implemented as follows in the source code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'These two layers have drastically downsized the input to 14×14×32, as shown
    in this diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_09.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9: Convolution and pooling layer'
  prefs: []
  type: TYPE_NORMAL
- en: It is possible to insert a padding layer on a CNN. As we shrink our image layer
    by layer, the filters in a convolutional network will impact the center pixels
    more than the outer pixels. Suppose you start drawing on a piece of paper. You
    tend to fill the center of the paper and avoid the edges. The edges of the piece
    of paper contain less information. If you decide to apply padding to the edges,
    the image will be more complete. In a neural network, padding has the same function.
    It makes sure the edges are taken into account by adding values. Padding can be
    implemented before or after pooling, for example. We will implement an example
    of padding in *Chapter 13*, *Visualizing Networks with TensorFlow 2.x and TensorBoard*.
  prefs: []
  type: TYPE_NORMAL
- en: The next layer can apply flattening to the output of the pooling of this section,
    as we'll see in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Flattening
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The flattening layer takes the output of the max pooling layer and transforms
    the vector of size *x* * *y* * *z* into a flattened vector, as shown in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In this case, the layer vector will be 14 × 14 × 32 = 6,272, as shown in the
    following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_10.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10: Flattening layer'
  prefs: []
  type: TYPE_NORMAL
- en: This operation creates a standard layer with 6,272 very practical connections
    for the dense operations that follow. After flattening has been carried out, a
    fully connected dense network can be implemented.
  prefs: []
  type: TYPE_NORMAL
- en: Dense layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Dense layers are fully connected. Full connections are possible through the
    size reductions calculated so far, as shown before.
  prefs: []
  type: TYPE_NORMAL
- en: 'The successive layers in this sequential model have brought the size of the
    image down enough to use dense layers to finish the job. `dense_1` comes first,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_11.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.11: Dense layer'
  prefs: []
  type: TYPE_NORMAL
- en: The flattening layer produced a 14×14×32 size 6,272 layer with a weight for
    each input. If it had not gone through the previous layers, the flattening would
    have produced a much larger layer, slowing feature extractions down. The result
    would produce nothing effective.
  prefs: []
  type: TYPE_NORMAL
- en: 'With the main features extracted by the filters through successive layers and
    size reduction, the dense operations will lead directly to a prediction using
    ReLU on the dense operation and then the logistic sigmoid function to produce
    the final result:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have the dense layer, let's explore the dense layer's activation
    functions.
  prefs: []
  type: TYPE_NORMAL
- en: Dense activation functions
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The ReLU activation function can be applied to a dense layer as in other layers.
  prefs: []
  type: TYPE_NORMAL
- en: 'The domain of the ReLU activation function is applied to the result of the
    first dense operation. The ReLU activation function will output the initial input
    for values >=0 and will output 0 for values <0:'
  prefs: []
  type: TYPE_NORMAL
- en: '*f*(input_value) = max{0, input_value)'
  prefs: []
  type: TYPE_NORMAL
- en: The logistic activation function is applied to the second dense operation, as
    described in *Chapter 2*, *Building a Reward Matrix – Designing Your Datasets*.
  prefs: []
  type: TYPE_NORMAL
- en: 'It will produce a value between 0 and 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '*LS*(*x*)={0,1}'
  prefs: []
  type: TYPE_NORMAL
- en: We have now built the last dense layer after the *LS* activation function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The last dense layer is of size 1 and will classify the initial input—an image
    in this case:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_12.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12: Dense layer 2'
  prefs: []
  type: TYPE_NORMAL
- en: The layers of the model have now been added. Training can begin.
  prefs: []
  type: TYPE_NORMAL
- en: Training a CNN model
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Training a CNN model involves four phases: compiling the model, loading the
    training data, loading the test data, and running the model through epochs of
    loss evaluation and parameter-updating cycles.'
  prefs: []
  type: TYPE_NORMAL
- en: In this section, the choice of theme for the training dataset will be an example
    from the food-processing industry. The idea here is not only to recognize an object
    but to form a concept. We will explore concept learning neural networks further
    in *Chapter 10*, *Conceptual Representation Learning*. For the moment, let's train
    our model.
  prefs: []
  type: TYPE_NORMAL
- en: The goal
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The primary goal of this model consists of detecting production efficiency flaws
    on a food-processing conveyor belt. The use of CIFAR-10 (images) and MNIST (a handwritten
    digit database) proves useful to understand and train some models. However, in
    this example, the goal is not to recognize objects but a concept.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following image shows a section of the conveyor belt that contains an acceptable level
    of products, in this case, portions of chocolate cake:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_13.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13: Portions of chocolate cake example'
  prefs: []
  type: TYPE_NORMAL
- en: The images can represent anything from chocolate cakes, cars on a road or any
    other type of object. The main point is to detect when there are "gaps" or "holes"
    in the rows of objects. To train the CNN, I used images containing objects I sometimes
    drew just to train the system to "see" gaps.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, sometimes production slows down, and the output goes down to an alert
    level, as shown in the following photograph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_14.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.14: Portions of chocolate cake example'
  prefs: []
  type: TYPE_NORMAL
- en: The alert-level image shows a gap that will slow down the packaging section
    of the factory dramatically. There are three lines of objects in the preceding
    image. On line one, you can see five little objects (here pieces of cake), on
    line two, only three. On line three, you can only see two objects. There are thus
    objects missing on lines two and three. This constitutes a gap in this case that
    is a real problem on production lines. The level of the number of acceptable objects
    in a frame is a parameter.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have our goal, let's begin compiling the model.
  prefs: []
  type: TYPE_NORMAL
- en: Compiling the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Compiling a TensorFlow 2 model requires a minimum of two options: a loss function and
    an optimizer. You evaluate how much you are losing and then optimize your parameters,
    just as in real life. A metric option has been added to measure the performance
    of the model. With a metric, you can analyze your losses and optimize your situation,
    as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Let's take a look at some specific aspects of model compiling, starting with
    the loss function.
  prefs: []
  type: TYPE_NORMAL
- en: The loss function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The loss function provides information on how far the state of the model *y*[1]
    (weights and biases) is from its target state *y*.
  prefs: []
  type: TYPE_NORMAL
- en: A description of the quadratic loss function precedes that of the binary cross-entropy functions
    applied to the case study model in this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The quadratic loss function
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Let''s refresh the concept of gradient descent. Imagine you are on a hill and
    want to walk down that hill. Your goal is to get to *y*, the bottom of the hill.
    Presently, you are at location *a*. Google Maps shows you that you still have
    to go a certain distance:'
  prefs: []
  type: TYPE_NORMAL
- en: '*y* – *a*'
  prefs: []
  type: TYPE_NORMAL
- en: That formula is great for the moment. But now suppose you are almost at the
    bottom of the hill, and the person walking in front of you has dropped a coin.
    You have to slow down now, and Google Maps is not helping much because at this zoom
    level.
  prefs: []
  type: TYPE_NORMAL
- en: 'You must then zoom into smaller distances with a quadratic objective (or cost)
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '*O* = (*y* – *a*)²'
  prefs: []
  type: TYPE_NORMAL
- en: 'To make it more comfortable to analyze, *O* is divided by 2, producing a standard
    quadratic cost function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_002.png)'
  prefs: []
  type: TYPE_IMG
- en: '*y* is the goal. *a* is the result of the operation of applying the weights,
    biases, and finally, the activation functions.'
  prefs: []
  type: TYPE_NORMAL
- en: With the derivatives of the results, the weights and biases can be updated.
    In our hill example, if you move one meter (*y*) per step (*x*), that is much
    more than moving 0.5 meters (*y*) per step. Depending on your position on the
    hill, you can see that you cannot apply a constant learning rate (conceptually,
    the length of your step); you adapt it just like Adam, the optimizer, does.
  prefs: []
  type: TYPE_NORMAL
- en: Binary cross-entropy
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: Cross-entropy comes in handy when the learning slows down. In the hill example,
    it slowed down at the bottom. But, remember, a path can lead you sideways, meaning
    you are momentarily stuck at a given height. Cross-entropy solves that by being
    able to function well with very small values (steps on the hill).
  prefs: []
  type: TYPE_NORMAL
- en: 'Suppose you have the following structure:'
  prefs: []
  type: TYPE_NORMAL
- en: Inputs = {*x*[1], *x*[2], …, *x*[n]}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights = {*w*[1], *w*[2], …, *w*[n]}
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A bias (or sometimes more) is *b*
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An activation function (ReLU, logistic sigmoid, or other)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Before the activation, *z* represents the sum of the classical operations:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_003.png)'
  prefs: []
  type: TYPE_IMG
- en: Now the activation function is applied to *z* to obtain the present output of
    the model.
  prefs: []
  type: TYPE_NORMAL
- en: '*y*[1] = *act*(*z*)'
  prefs: []
  type: TYPE_NORMAL
- en: 'With this in mind, the cross-entropy loss formula can be explained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/B15438_09_004.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In this function:'
  prefs: []
  type: TYPE_NORMAL
- en: '*n* is the total number of items of the input training, with multiclass data.
    The choice of the logarithm base (2, *e*, 10) will produce different effects.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y* is the output goal.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*y*[1] is the present value, as described previously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This loss function is always positive; the values have a minus sign in front
    of them, and the function starts with a minus. The output produces small numbers
    that tend to zero as the system progresses.
  prefs: []
  type: TYPE_NORMAL
- en: The loss function uses this basic concept with more mathematical inputs to update
    the parameters.
  prefs: []
  type: TYPE_NORMAL
- en: A binary cross-entropy loss function is a binomial function that will produce
    a probability output of 0 or 1 and not a value between 0 and 1 as in standard
    cross-entropy. In the binomial classification model, the output will be 0 or 1.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the sum ![](img/B15438_09_005.png) is not necessary when *M*
    (number of classes) = 2\. The binary cross-entropy loss function is then as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Loss = –*y* log *y*[1] + (1 – *y*) log (1 – *y*[1])
  prefs: []
  type: TYPE_NORMAL
- en: The whole concept of this loss function method is for the CNN network to provide
    information for the optimizer to adapt the weights accordingly and automatically.
  prefs: []
  type: TYPE_NORMAL
- en: The Adam optimizer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the hill example, you first walked with big strides down the hill using momentum
    (larger strides because you are going in the right direction). Then, you had to
    take smaller steps to find the object. You adapted your estimation of your moment
    to your need; hence, the name **adaptive moment estimation** (**Adam**).
  prefs: []
  type: TYPE_NORMAL
- en: Adam constantly compares mean past gradients to present gradients. In the hill
    example, it compares how fast you were going.
  prefs: []
  type: TYPE_NORMAL
- en: The Adam optimizer represents an alternative to the classical gradient descent
    method. Adam goes further by applying its optimizer to random (stochastic) mini-batches
    of the dataset. This approach is an efficient version of stochastic gradient descent.
  prefs: []
  type: TYPE_NORMAL
- en: Then, with even more inventiveness, Adam adds **root-mean-square deviation**
    (**RMSprop**) to the process by applying per-parameter learning weights. It analyzes
    how fast the means of the weights are changing (such as the gradients in our hill
    slope example) and adapts the learning weights.
  prefs: []
  type: TYPE_NORMAL
- en: Metrics
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Metrics are there to measure the performance of your model during the training
    process. The metric function behaves like a loss function. However, it is not
    used to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the `accuracy` parameter was this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: Here, a value that descends toward 0 shows whether the training is on the right
    track and moves up to 1 when the training requires Adam function optimizing to set
    the training on track again.
  prefs: []
  type: TYPE_NORMAL
- en: With this, we have compiled our model. We can now consider our training dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The training dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The training dataset is available on GitHub. The dataset contains the image
    shown previously for the food-processing conveyor belt example. I created a training
    dataset with a repetition of a few images that I used to illustrate the architecture
    of a CNN simply. In a real-life project, it will take careful designing with a
    trial-and-error approach to create a proper dataset that represents all of the
    cases that the CNN will face.
  prefs: []
  type: TYPE_NORMAL
- en: The class `A` directory contains the acceptable level images of a production
    line that is producing acceptable levels of products. The class `B` directory
    contains the alert-level images of a production line that is producing unacceptable
    levels of products.
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of images in the dataset is limited because of the following:'
  prefs: []
  type: TYPE_NORMAL
- en: For experimental training purposes, the images produced good results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The training-testing phase runs faster to study the program
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The goal of the model is to detect the alert levels, an abstract conceptual
    application of a CNN.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Data augmentation increases the size of the dataset by generating distorted
    versions of the images provided.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `ImageDataGenerator` function generates batches of all images found in
    tensor formats. It will perform data augmentation by distorting the images (shear
    range, for example). Data augmentation is a fast way to use the images you have
    and create more virtual images through distortions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'The code description is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`rescale` will rescale the input image if not `0` (or `None`). In this case,
    the data is multiplied by 1/255 before applying any other operation.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`shear_range` will displace each value in the same direction, determined in
    this case by the `0.2`. It will slightly distort the image at one point, giving
    some more virtual images to train.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`zoom_range` is the value of zoom.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`horizontal_flip` is set to `True`. This is a Boolean that randomly flips inputs horizontally.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ImageDataGenerator` provides many more options for real-time data augmentation,
    such as rotation range, height shift, and others.'
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Loading the data goes through the `train_datagen` preprocessing image function
    (described previously) and is implemented in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'The flow in this program uses the following options:'
  prefs: []
  type: TYPE_NORMAL
- en: '`flow_from_directory` sets the directory + `''training_set''` to the path where
    the two binary classes to train are stored.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`target_size` will be resized to that dimension. In this case, it is 64×64.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`batch_size` is the size of the batches of data. The default value is 32, and
    it''s set to `10` in this case.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`class_mode` determines the label arrays returned: `None` or `''categorical''`
    will be 2D one-hot encoded labels. In this case, `''binary''` returns 1D binary
    labels.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Having looked at the training dataset, let's move on to the testing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: The testing dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The testing dataset flow follows the same structure as the training dataset
    flow described previously. However, for testing purposes, the task can be made
    easier or more difficult, depending on the choice of the model. To make the task
    more difficult, add images with defects or noise. This will force the system to
    train more and the project team to do more hard work to fine-tune the model. I
    chose to use a small dataset to illustrate the architecture of a CNN. In a real-life
    project choosing the right data that contains all of the cases that the CNN will
    face takes time and a trial-and-error approach.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation provides an efficient way of producing distorted images without adding
    images to the dataset. Both methods, among many others, can be applied at the
    same time when necessary.
  prefs: []
  type: TYPE_NORMAL
- en: Data augmentation on the testing dataset
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In this model, the data only goes through rescaling. Many other options could
    be added to complicate the training task to avoid overfitting, for example, or
    simply because the dataset is small:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Building datasets is one of the most difficult tasks in an artificial intelligence
    project. Data augmentation can be a solution if the results are efficient. If
    not, other techniques must be used. One technique is to gather very large datasets
    when that is possible and then use data augmentation just to distort the data
    a bit for training purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Loading the data
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Loading the testing data remains limited to what is necessary for this model.
    Other options can fine-tune the task at hand:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Never underestimate dataset fine-tuning. Sometimes, this phase can last weeks
    before finding the right dataset and arguments.
  prefs: []
  type: TYPE_NORMAL
- en: Once the data is loaded, the CNN classifier is ready to be trained. Let's now
    see how this is done.
  prefs: []
  type: TYPE_NORMAL
- en: Training with the classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The classifier has been built and can be run:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: You will notice that, in this chapter, we have a directory for the training
    data and a separate directory for the test data. In *Chapter 5*, *How to Use Decision
    Trees to Enhance K-Means Clustering*, we split the datasets into training subsets
    and testing subsets. This can be applied to a CNN's dataset as well. This is a
    decision you will need to make, depending on the situation.
  prefs: []
  type: TYPE_NORMAL
- en: For example, sometimes, the test set will be more difficult than the training
    set, which justifies a separate directory. In other cases, splitting the data
    can be the most efficient method.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `fit_generator` function, which fits the model generated batch by batch,
    contains the main hyperparameters to run the training session through the following
    arguments in this model. The hyperparameter settings determine the behavior of
    the training algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '`training_set` is the training set flow described previously.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`steps_per_epoch` is the total number of steps (batches of samples) to yield
    from the generator. The variable used in the following code is `estep`.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`epochs` is the variable of the total number of iterations made on the data
    input. The variable used is `ep` in the preceding code.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validation_data=test_set` is the testing data flow.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`validation_steps=vs` is used with the generator and defines the number of
    batches of samples to test as defined by `vs` in the following code at the beginning
    of the program:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'While the training runs, measurements are displayed: loss, accuracy, epochs,
    information on the structure of the layers, and the steps calculated by the algorithm.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of the loss and accuracy data displayed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have built and trained the model, we need to save it. Saving the
    model will avoid having to train the model each time we wish to use it.
  prefs: []
  type: TYPE_NORMAL
- en: Saving the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: By saving the model, we will not have to train it again every time to use it.
    We will only go back to training when it's required to fine-tune it.
  prefs: []
  type: TYPE_NORMAL
- en: 'TensorFlow 2 provides a method to save the structure of the model and the weights
    in a single line of code and a single serialized file:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model3.h5` saved in the following code, contains serialized data with the
    model structure and weights. It contains the parameters and options of each layer.
    This information is very useful to fine-tune the model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: The model has been built, trained, and saved.
  prefs: []
  type: TYPE_NORMAL
- en: Next steps
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The model has been built and trained. In *Chapter 10*, *Conceptual Representation
    Learning*, we will explore how to load and run it with no training.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Building and training a CNN will only succeed with hard work, choosing the model,
    the right datasets, and hyperparameters. The model must contain convolutions,
    pooling, flattening, dense layers, activation functions, and optimizing parameters
    (weights and biases) to form solid building blocks to train and use a model.
  prefs: []
  type: TYPE_NORMAL
- en: Training a CNN to solve a real-life problem can help sell AI to a manager or
    a sales prospect. In this case, using the model to help a food-processing factory
    solve a conveyor belt productivity problem takes AI a step further into everyday
    corporate life.
  prefs: []
  type: TYPE_NORMAL
- en: A CNN that recognizes abstract concepts within an image takes deep learning
    one step closer to powerful machine thinking. A machine that can detect objects
    in an image and extract concepts from the results represents the true final level
    of AI.
  prefs: []
  type: TYPE_NORMAL
- en: Once the training is over, saving the model provides a practical way to use
    it by loading it and applying it to new images to classify them. This chapter
    concluded after we had trained and saved the model.
  prefs: []
  type: TYPE_NORMAL
- en: '*Chapter 10*, *Conceptual Representation Learning*, will dive deeper into how
    to design symbolic neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A CNN can only process images. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A kernel is a preset matrix used for convolutions. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Does pooling have a pooling matrix, or is it random?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The size of the dataset always has to be large. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Finding a dataset is not a problem with all the available image banks on the
    web. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Once a CNN is built, training it does not take much time. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A trained CNN model applies to only one type of image. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: A quadratic loss function is not very efficient compared to a cross-entropy
    function. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The performance of a deep learning CNN does not present a real issue with modern
    CPUs and GPUs. (Yes | No)
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading and references
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'TensorFlow 2: [https://www.tensorflow.org/beta](https://www.tensorflow.org/beta)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
