- en: '7'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Operationalizing Deep Learning Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In [*Chapter 1*](B17519_01.xhtml#_idTextAnchor013), *Introducing Deep Learning
    with Amazon SageMaker*, we discussed how SageMaker integrates with CloudWatch
    Logs and Metrics to provide visibility into your training process by collecting
    training logs and metrics. However, **deep learning** (**DL**) training jobs are
    prone to multiple types of specific issues related to model architecture and training
    configuration. Specialized tools are required to monitor, detect, and react to
    these issues. Since many training jobs run for hours and days on large amounts
    of compute instances, the cost of errors is high.
  prefs: []
  type: TYPE_NORMAL
- en: 'When running DL training jobs, you need to be aware of two types of issues:'
  prefs: []
  type: TYPE_NORMAL
- en: Issues with model and training configuration, which prevent the model from efficient
    learning during training. Examples of such issues include vanishing and exploding
    gradients, overfitting and underfitting, not decreasing loss, and others. The
    process of finding such errors is known as **debugging**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Suboptimal model and training configuration, which doesn’t allow you to fully
    utilize available hardware resources. For instance, let’s say that the batch size
    is smaller than the optimal value and the GPU resources are underutilized. This
    leads to a slower than possible training speed. We call the process of finding
    such issues **profiling**.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this chapter, we will review the available open source and SageMaker capabilities
    for training, debugging, and profiling. We will start with the popular open source
    tool for training monitoring and debugging called **TensorBoard** and review how
    it can be integrated with SageMaker’s training infrastructure. Then, we will compare
    it to the proprietary **SageMaker Debugger**, which provides advanced capabilities
    to help you automatically detect various types of issues and manage your training
    job accordingly. You will develop practical experience in using both tools.
  prefs: []
  type: TYPE_NORMAL
- en: Another type of problem you typically need to solve when operationalizing your
    DL models is establishing an efficient way to find optimal combinations of model
    hyperparameters. This process is known as **hyperparameter tuning**. It is especially
    relevant in the initial stages of model development and adoption when you need
    to establish a production-ready model baseline. SageMaker provides an automated
    way to tune your model using a feature called **Automatic Model Tuning**.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we will discuss how to reduce the cost of your training job and model
    tuning jobs by using **EC2 Spot Instances**.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Debugging training jobs
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Profiling your DL training
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Hyperparameter optimization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using EC2 Spot Instances
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: After reading this chapter, you will be able to establish profiling and debugging
    procedures for your large-scale DL training to minimize unnecessary costs and
    time to train. You will also know how to organize hyperparameter tuning and optimize
    it for cost using Spot Instances.
  prefs: []
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will provide code samples so that you can develop practical
    skills. The full code examples are available here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'To follow along with this code, you will need the following:'
  prefs: []
  type: TYPE_NORMAL
- en: An AWS account and IAM user with permission to manage Amazon SageMaker resources.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Have a SageMaker notebook, SageMaker Studio notebook, or local SageMaker compatible
    environment established.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to GPU training instances in your AWS account. Each example in this chapter
    will provide recommended instance types to use. You may need to increase your
    compute quota for **SageMaker Training Job** to have GPU instances enabled. In
    this case, please follow the instructions at [https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml](https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You must install the required Python libraries by running `pip install -r requirements.txt`.
    The file that contains the required libraries is located at the root of `chapter7`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Debugging training jobs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To effectively monitor and debug DL training jobs, we need to have access to
    the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: Scalar values such as accuracy and loss, which we use to measure the quality
    of the training process
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tensor values such as weights, biases, and gradients, which represent the internal
    state of the model and its optimizers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both TensorBoard and SageMaker Debugger allow you to collect tensors and scalars,
    so both can be used to debug the model and training processes. However, unlike
    TensorBoard, which is primarily used for training visualizations, SageMaker Debugger
    provides functionality to react to changes in model states in near-real time.
    For example, it allows us to stop training jobs earlier if training loss hasn’t
    decreased for a certain period.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will dive deep into how to use TensorBoard and SageMaker
    Debugger. We will review the features of both solutions in detail and then develop
    practical experiences of using both solutions to debug your training script.
  prefs: []
  type: TYPE_NORMAL
- en: Please note that we will use the same examples for both debugging and profiling
    tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorBoard with SageMaker
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'TensorBoard is an open source tool developed originally for the TensorFlow
    framework, but it is now available for other DL frameworks, including PyTorch.
    TensorBoard supports the following features for visualizing and inspecting the
    training process:'
  prefs: []
  type: TYPE_NORMAL
- en: Tracking scalar values (loss, accuracy, and others) over time.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capturing tensors such as weights, biases, and gradients and how they change
    over time. This can be useful for visualizing weights and biases and verifying
    that they are changing expectedly.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Experiment tracking via a dashboard of hyperparameters.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Projecting high-dimensional embeddings to a lower-dimensionality space.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Capturing images, audio, and text data.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Additionally, TensorBoard comes with native profiling capabilities for TensorFlow
    programs. Profiling is also available for PyTorch via an add-on.
  prefs: []
  type: TYPE_NORMAL
- en: Debugging PyTorch training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s review how TensorBoard can help you to get insights into your training
    process and debug it using a practical example. We will use a pre-trained ResNet
    model from the PyTorch model zoo and train it to recognize two classes: bees and
    ants.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We provide code highlights in this section. The full training code is available
    here: [https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/1_TensorBoard_PyTorch.ipynb](https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/1_TensorBoard_PyTorch.ipynb).'
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the training script
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To use TensorBoard, we need to make minimal changes to our training script.
    Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we must import and initialize TensorBoard’s `SummaryWriter` object.
    Here, we are using the S3 location to write TensorBoard summaries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must capture some training artifacts that won’t change during training
    – in our case, the model graph. Note that we need to execute the model’s forward
    path on the sample data to do so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In our training loop, we capture the scalars and tensors that we wish to inspect.
    We use the epoch number as the time dimension. Let’s say that in our case, we
    wish to capture the following data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How accuracy and loss change every epoch for training and validation datasets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Distribution of gradients and weights on the first convolutional and last fully
    connected layers during the training phase
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The training hyperparameters and how they impact performance
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'To capture these parameters, we must add the following code to our training
    loop:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s review our training job configuration with debugging enabled.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring the training process
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To start the SageMaker training job, we need to provide the S3 location where
    TensorBoard summaries will be written. We can do this by setting the `tb-s3-url`
    hyperparameter, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the training job has started, you can start your TensorBoard locally by
    running the following command in the terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Note the following when using TensorBoard in cloud development environments:'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using a SageMaker notebook instance, then TensorBoard will be available
    here: `https://YOUR_NOTEBOOK_DOMAIN/proxy/6006/`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If you are using SageMaker Studio, then TensorBoard will available here: `https://<YOUR_STUDIO_DOMAIN>/jupyter/default/proxy/6006/`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The TensorBoard data will be updated in near-real time as the training job
    progresses. Let’s review our training process in TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: 'On the **Scalar** and **Time Series** tabs, you can find changes in scalar
    values over time. We use the epoch index as an indicator of time. *Figure 7.1*
    shows the training and validation accuracies at every epoch:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.1 – Accuracies over time in TensorBoard ](img/B17519_07_001.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Accuracies over time in TensorBoard
  prefs: []
  type: TYPE_NORMAL
- en: On the **Graph** tab, you can see visual representations of the model and how
    data flows from inputs to outputs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `0`, which indicates that our model is learning and, hence, the absolute
    gradient values are decreasing:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.2 – Histogram of model weights in TensorBoard ](img/B17519_07_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.2 – Histogram of model weights in TensorBoard
  prefs: []
  type: TYPE_NORMAL
- en: The **HParam** tab allows us to capture and compare hyperparameters side by
    side. This can be useful for tracking experiments during hyperparameter searches
    to identify optimal model and training job configuration.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we understand how to use TensorBoard to visualize the training process,
    let’s see how we can use TensorBoard to profile our training job.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling PyTorch training
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: TensorBoard provides out-of-the-box profiling capabilities for TensorFlow programs
    (including Keras). To profile PyTorch programs in TensorBoard, you can use the
    open source **torch_tb_profiler** plugin.
  prefs: []
  type: TYPE_NORMAL
- en: 'When profiling the training process, we are usually interested in the following:'
  prefs: []
  type: TYPE_NORMAL
- en: How efficiently we utilize our resources (GPU and CPU) over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: What operations (DL operators, data loading, memory transfer, and so on) utilize
    what resources
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of distributed training, how efficient the communication is between
    nodes and individual training devices
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to improve the overall resource utilization and increase training efficiency
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both the TensorFlow and PyTorch plugins for TensorBoard provide such capabilities
    for profiling. Let’s review how profiling works for the same task we did for debugging.
  prefs: []
  type: TYPE_NORMAL
- en: Modifying the training script
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To profile applications using `torch_tb_profiler`, we need to make minimal
    modifications to our training code. Specifically, we need to wrap our training
    loop with the plugin context manager, as shown in the following code block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: The parameters of the context manager that are passed at initialization time
    define what profiling data must be gathered and at what intervals. At the time
    of writing this book, the `torch_db_profiler` plugin doesn’t support writing to
    the S3 location. Hence, we must write the profiling data to the local output directory
    stored in the `"SM_OUTPUT_DATA_DIR"` environment variable. After training is done,
    SageMaker automatically archives and stores the content of this directory to the
    S3 location.
  prefs: []
  type: TYPE_NORMAL
- en: Using TensorBoard Profiler
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'To review the output of TensorBoard Profiler, we need to download the data
    to the local environment:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by getting a path to the profiler data. For this, we can use
    the training job estimator instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, in your notebook or terminal window, you can run the following commands
    to unarchive the profiler data and start TensorBoard:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Upon starting TensorBoard, you should be automatically redirected to the profiler
    summary. From there, you have access to several views that contain profiling information:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Overview** tab provides a general summary of the device(s) used for training,
    their utilization over time, and the breakdown of operations. In our case, for
    example, the majority of the time is spent performing kernels that comprise forward
    and backward model passes. This is generally a good indicator that we utilize
    our GPU resources on the training model:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.3 – The Overview tab of TensorBoard Profiler ](img/B17519_07_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – The Overview tab of TensorBoard Profiler
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Operators** tab gives you an idea of how much time specific operators
    consume (such as convolution or batch normalization). In the following screenshot,
    we can see, for instance, that the backward pass on convolution layers takes most
    of the GPU time:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.4 – The Operators tab of TensorBoard Profiler ](img/B17519_07_004.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – The Operators tab of TensorBoard Profiler
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Kernel** tab breaks down the time spent performing on specific GPU kernels.
    For instance, in the following diagram, you can see that various **Single-Precision
    General Matrix Multiply** (**SGEMM**) kernels take most of the time:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.5 – The Kernel tab of TensorBoard Profiler ](img/B17519_07_005.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – The Kernel tab of TensorBoard Profiler
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Trace** tab shows the timeline of the profiled operators and GPU kernels,
    as well as the handoff between CPU and GPU devices (for instance, transferring
    data inputs from CPU to GPU):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/B17519_07_006.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – The Trace tab of TensorBoard Profiler
  prefs: []
  type: TYPE_NORMAL
- en: 'The **Memory** tab provides memory utilization over time for a given device.
    In the following chart, for instance, you can see allocated memory (that is, memory
    used to store tensors) and total reserved memory:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.7 – The Memory tab of TensorBoard Profiler ](img/B17519_07_007.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.7 – The Memory tab of TensorBoard Profiler
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, TensorBoard is a great tool for monitoring, debugging, and profiling
    your training script. TensorBoard, when used with plugins, supports both TensorFlow
    and PyTorch frameworks. However, one of the drawbacks of TensorBoard is that it
    doesn’t provide any ways to react to suboptimal conditions, such as underutilization
    of GPU devices or slow or no convergences of your model during training. To stop
    training jobs earlier when such conditions happen, you will need to further instrumentalize
    your code via callbacks and custom logic.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker Debugger addresses these limitations by providing a generic mechanism
    to detect common training problems and take mitigation actions.
  prefs: []
  type: TYPE_NORMAL
- en: Monitoring training with SageMaker Debugger
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: SageMaker Debugger is a comprehensive SageMaker capability that allows you to
    automatically monitor, debug, and profile DL training jobs running on SageMaker.
    SageMaker Debugger provides you with insights into your DL training by capturing
    the internal state of your training loop and instances metrics in near-real time.
    Debugger also allows you to automatically detect common issues happening during
    training and take appropriate actions when issues are detected. This allows you
    to automatically find issues in complex DL training jobs earlier and react accordingly.
    Additionally, SageMaker Debugger supports writing custom rules for scenarios not
    covered by built-in rules.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker has several key components:'
  prefs: []
  type: TYPE_NORMAL
- en: The open source `smedebug` library (https://github.com/awslabs/sagemaker-debugger),
    which integrates with DL frameworks and Linux instances to persist debugging and
    profiling data to Amazon S3, as well as to retrieve and analyze it once the training
    job has been started
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The SageMaker Python SDK, which allows you to configure the `smedebug` library
    with no or minimal code changes in your training script
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Automatically provisioned processing jobs to validate output tensors and profiling
    data against rules
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: SageMaker Debugger supports TensorFlow, PyTorch, and MXNet DL frameworks. The
    `smedebug` library is installed by default in SageMaker DL containers, so you
    can start using SageMaker Debugger without having to make any modifications to
    your training script. You can also install the `smdebug` library in a custom Docker
    container and use all the features of SageMaker Debugger.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Please note that there are minor differences in the `smedebug` APIs for different
    DL frameworks.
  prefs: []
  type: TYPE_NORMAL
- en: The `smedebug` library provides a rich API for configuring, saving, and analyzing
    captured tensors. It uses a `hook` object to capture tensors and scalars by injecting
    them into the training process. `hook` allows you to group tensors and scalars
    into logical tensor `Trial` object allows you to query the stored tensors of a
    given training job for further analysis. You can run tensor queries in real time
    without waiting for the training job to be fully complete. SageMaker Debugger
    also supports emitting TensorBoard-compatible summary logs for easy visualization
    of output tensors and scalars.
  prefs: []
  type: TYPE_NORMAL
- en: Using SageMaker Debugger
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Let’s apply these concepts to a practical task. We will instrumentalize the
    ResNet model and fine-tune it for a binary classification task. The full code
    is available here: https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/2_SMDebugger_PyTorch.ipynb.'
  prefs: []
  type: TYPE_NORMAL
- en: Code instrumentalization
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The `smedebug` library requires minimal changes to capture tensors and scalars.
    First, you need to initiate the `hook` object outside of your training loop, as
    well as after model and optimizer initialization:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: Note that we are using `.create_from_json_file()` to create our `hook` object.
    This method instantiates `hook` based on the hook configuration you provide in
    the SageMaker training object. Since we are adding both the `model` and `criterion`
    objects to `hook`, we should expect to see both model parameters (weights, biases,
    and others), as well as loss scalar.
  prefs: []
  type: TYPE_NORMAL
- en: 'Inside our training loop, the only modification we need to make is to differentiate
    between the training and validation phases by switching between `smedebug.modes.Train`
    and `smedebug.modes.Eval`. This will allow `smedebug` to segregate the tensors
    that are captured in the training and evaluation phases:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: Now, let’s review how to configure `hook`, rules, actions, and tensor collections
    when running a SageMaker training job.
  prefs: []
  type: TYPE_NORMAL
- en: Training job configuration
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'As part of the SageMaker Python SDK, AWS provides the `sagemaker.debugger`
    library for Debugger configuration. Let’s take a look:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will start by importing some Debugger entities:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must define automatic actions and a set of rules. Here, we are using
    Debugger’s built-in rules to detect some common DL training issues. Note that
    we can assign different actions to different rules. In our case, we want to stop
    our training job immediately when the rule is triggered:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must configure the collection of tensors and how they will be persisted.
    Here, we will define that we want to persist the weights and losses collection.
    For weights, we will also save a histogram that can be further visualized in TensorBoard.
    We will also set a saving interval for the training and evaluation phases:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, we are ready to pass these objects to the SageMaker `Estimator` object:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now, we are ready to start training the job using the `fit()` method. In the
    next section, we will learn how to retrieve and analyze SageMaker Debugger outputs.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing the Debugger results
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'SageMaker Debugger provides functionality to retrieve and analyze collected
    tensors from training jobs as part of the `smedebug` library. In the following
    steps, we will highlight some key APIs:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code block, we are creating a new trial object using the S3
    path where the tensors were persisted:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, let’s output all the available tensors by running the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'You should be able to see multiple collections with many tensors, including
    biases, weights, losses, and gradients. Let’s access specific numeric values.
    Running the following command will return a list of associated scalar values:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Using a simple plotting function (refer to the sources for its implementation),
    we can visualize loss for the training and evaluation phases. Running the following
    command will result in a 2D loss chart. Similarly, you can access and process
    tensors:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following figure visualizes the training and validation losses:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 7.8 – Training and validation losses ](img/B17519_07_008.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Training and validation losses
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let’s review if any rules were triggered during our training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This outputs all configured rules; their statuses are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, in our case, no rules were triggered, and our job was completed.
    You can experiment with rule settings. For instance, you can reset weights on
    one of the model layers. This will result in triggering the `PoorWeightInitiailization`
    rule and the training process being stopped.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, let’s visually inspect the saved tensors using TensorBoard. For this,
    we simply need to start TensorBoard using the S3 path we supplied to the `Estimator`
    object earlier:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Feel free to explore TensorBoard on your own. You should expect to find histograms
    of weights.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this section, we reviewed SageMaker Debugger’s key capabilities and learned
    how to use them. You may have already observed some benefits of SageMaker Debugger
    over TensorBoard:'
  prefs: []
  type: TYPE_NORMAL
- en: Zero or minimal effort in instrumentalizing your code for SageMaker Debugger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A rich API to process and analyze output tensors
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A large number of built-in rules and actions with the ability to create custom
    rules and actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: TensorBoard functionality is supported out of the box
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With these capabilities, SageMaker Debugger allows you to improve the quality
    of your training jobs, accelerate experimentation, and reduce unnecessary costs.
  prefs: []
  type: TYPE_NORMAL
- en: Additionally, SageMaker Debugger provides profiling capabilities. We’ll review
    them next.
  prefs: []
  type: TYPE_NORMAL
- en: Profiling your DL training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: SageMaker Debugger allows you to collect various types of advanced metrics from
    your training instances. Once these metrics have been collected, SageMaker generates
    detailed metrics visualizations, detects resource bottlenecks, and provides recommendations
    on how instance utilization can be improved.
  prefs: []
  type: TYPE_NORMAL
- en: 'SageMaker Debugger collects two types of metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '**System metrics**: These are the resource utilization metrics of training
    instances such as CPU, GPU, network, and I/O.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Framework metrics**: These are collected at the DL framework level. This
    includes metrics collected by native framework profiles (such as PyTorch profiler
    or TensorFlow Profiler), data loader metrics, and Python profiling metrics.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As in the case of debugging, you can define rules that will be automatically
    evaluated against collected metrics. If a rule is triggered, you can define one
    or several actions that will be taken. For example, you can send an email if the
    training job has GPU utilization below a certain threshold.
  prefs: []
  type: TYPE_NORMAL
- en: It’s time to profile our training code with SageMaker Debugger. You can find
    the full code in the *Profiling DL Training* section at https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter7/2_SMDebugger_PyTorch.ipynb.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the training job for profiling
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We will start by defining what system and framework metrics we want to collect.
    For instance, we can provide a custom configuration for the framework, data loader,
    and Python. Note that system profiling is enabled by default:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we must provide the profiling config to the SageMaker training job configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: Note that we set `num-data-workers` to `8`, while `ml.p2.xlarge` has only 4
    CPU cores. Usually, it’s recommended to have the number of data workers equal
    to the number of CPUs. Let’s see if SageMaker Debugger will be able to detect
    this suboptimal configuration.
  prefs: []
  type: TYPE_NORMAL
- en: Reviewing profiling outcomes
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'You can start monitoring profiling outcomes in near-real time. We will use
    the `semdebug.profiler` API to process profiling outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'Once the data is available, we can retrieve and visualize it. Running the following
    code will chart the CPU, GPU, and GPU memory utilization from system metrics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Similarly, you can visualize other collected metrics. SageMaker Debugger also
    generates a detailed profiling report that aggregates all visualizations, insights,
    and recommendations in one place. Once your training job has finished, you can
    download the profile report and all collected data by running the following command
    in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Once all the assets have been downloaded, open the `profiler-report.xhtml` file
    in your browser and review the generated information. Alternatively, you can open
    `profiler-report.ipynb`, which provides the same insights in the form of an executable
    Jupyter notebook.
  prefs: []
  type: TYPE_NORMAL
- en: 'The report covers the following aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: System usage statistics
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Framework metrics summary
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary of rules and their status
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training loop analysis and recommendations for optimizations
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note that, in the *Dataloading analysis* section, you should see a recommendation
    to decrease the number of data workers according to our expectations.
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, SageMaker Debugger provides extensive profiling capabilities,
    including a recommendation to improve and automate rule validation with minimal
    development efforts. Similar to other Debugger capabilities, profiling is free
    of charge, so long as you are using built-in rules.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter optimization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A SageMaker Automatic Model Tuning job allows you to run multiple training jobs
    with a unique combination of hyperparameters in parallel. In other words, a single
    tuning job creates multiple SageMaker training jobs. Hyperparameter tuning allows
    you to speed up your model development and optimization by trying many combinations
    of hyperparameters in parallel and iteratively moving toward more optimal combinations.
    However, it doesn’t guarantee that your model performance will always improve.
    For instance, if the chosen model architecture is not optimal for the task at
    hand or your dataset is too small for the chosen model, you are unlikely to see
    any improvements when running hyperparameter optimizations.
  prefs: []
  type: TYPE_NORMAL
- en: 'When designing for your tuning job, you need to consider several key parameters
    of your tuning job, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Search algorithm** (or **strategy**): This defines how SageMaker chooses
    the next combination of hyperparameters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Hyperparameters with ranges**: The SageMaker search algorithm will pick hyperparameter
    values within user-defined ranges.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Objective metric**: This will be used to compare a combination of hyperparameters
    and define the best candidate. SageMaker doesn’t restrict you from choosing any
    arbitrary target metric.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'SageMaker supports two search strategies: **Bayesian** and **Random**. Random
    search selects the next combination of hyperparameters randomly within defined
    ranges. While it’s a simple strategy, it is considered a relatively efficient
    one. Because the next hyperparameter combination doesn’t depend on previously
    tried or currently running combinations, you can have a large number of training
    jobs running in parallel. Bayesian search selects the next combination of hyperparameters
    based on the outcomes of previous training jobs. Under the hood, SageMaker trains
    a regression model for this, which takes the results of previous jobs as input
    (hyperparameters and resulting target metrics) and outputs the candidate hyperparameter
    combination. Note that the Bayesian model may not converge. In such cases, it
    makes sense to review identified hyperparameter ranges.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Choosing hyperparameters and their ranges significantly impacts your tuning
    job performance. SageMaker supports several types of hyperparameters – categorical,
    continuous, and integer. You can combine different types of hyperparameters. For
    instance, the following code defines the model architecture as a categorical hyperparameter,
    the learning rate scheduler step is defined as an integer parameter, and the learning
    rate is defined as a continuous parameter (in other words, a float type):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: Note that for numeric hyperparameters, we also define `"Logarithmic"` scaling
    type for the learning rate parameter since its range spans multiple orders of
    magnitude. For the scheduler step size, we choose the `"Linear"` scaling type
    since its range is narrow.
  prefs: []
  type: TYPE_NORMAL
- en: 'You also need to define the objective metric for your hyperparameter tuning
    job. The objective metric is defined similarly to other metrics via a Regex pattern.
    Note that you need to ensure that your training script outputs your objective
    metric in the `stdout/stderr` streams. Follow these steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we are defining four metrics that will be captured by
    SageMaker and then choosing `val_accuracy` as our objective metric to optimize
    for:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we must define the parameters of the training job. Note that the hyperparameters
    that are provided as part of the training job configuration will be static and
    won’t be changed as part of the tuning job:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we must combine our objective metric, metric definitions, and hyperparameter
    ranges in the `HyperParameterTuner` object, which will orchestrate the creation
    of child training jobs and track the overall status of your tuning. Additionally,
    we must provide the total max number of training jobs and the number of concurrent
    training jobs. These parameters will have an impact on how quickly the tuning
    job will run and its total cost:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Also, pay attention to the `objective_type` parameter, which defines whether
    the tuning job will try to maximize or minimize the objective metric. Since we
    chose `accuracy` as our objective metric, we want to maximize it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the tuner object has been instantiated, you can use the `.fit()` method
    to start training:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Once the job is completed, you can analyze the outcomes of the tuning job.
    For this, you can navigate to the AWS console and inspect them visually. Alternatively,
    you can export tuner job results and statistics to a pandas DataFrame for further
    analysis, as shown here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Using this method, you can perform more advanced analytics, such as defining
    the correlation between various hyperparameters and objective metrics. This type
    of analysis, for instance, may uncover cases where your hyperparameter ranges
    need to be modified to further improve the target metric.
  prefs: []
  type: TYPE_NORMAL
- en: Using EC2 Spot Instances
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Running large training and model tuning jobs can be very expensive. One approach
    to minimize costs is to use EC2 Spot Instances from a pool of unused compute resources
    in a chosen AWS region. Thus, Spot Instances are considerably cheaper than regular
    on-demand instances (up to 90%). However, Spot Instances can be stopped with short
    notice if the spot capacity of the chosen instance type is exhausted in a given
    AWS region.
  prefs: []
  type: TYPE_NORMAL
- en: SageMaker simplifies the provisioning of Spot Instances for training jobs and
    fully handles interruption and training job restarts when the spot capacity is
    available again. When the training job is interrupted and then restarted, we want
    to continue our training process rather than starting from scratch. To support
    this, your training script needs to be modified so that it can save and restart
    the training job.
  prefs: []
  type: TYPE_NORMAL
- en: 'To support spot training, your training script needs the following modifications:'
  prefs: []
  type: TYPE_NORMAL
- en: 'When loading the model for the first time, check if there is a model copy already
    available in the `/opt/ml/checkpoints` path. If the checkpointed model is available,
    this means that we trained this model previously. To continue training, we need
    to load the checkpointed model and proceed with training. If the checkpointed
    model is not available, we proceed with regular model loading:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.9 – Uploading the checkpoint artifacts to S3 storage ](img/B17519_07_009.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Uploading the checkpoint artifacts to S3 storage
  prefs: []
  type: TYPE_NORMAL
- en: 'In your training script, you need to specify the checkpoint handler (refer
    to the DL framework documentation for this) and store your model checkpoints in
    the designated directory – that is, `/opt/ml/checkpoints`. In the case of Spot
    Instance interruption, SageMaker will automatically copy the content of this directory
    to S3\. When the Spot Instance is available again, SageMaker will copy your checkpoints
    from S3 back to the `/opt/ml/checkpoints` directory:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 7.10 – Restoring the checkpoint artifacts from S3 storage ](img/B17519_07_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Restoring the checkpoint artifacts from S3 storage
  prefs: []
  type: TYPE_NORMAL
- en: When using Spot Instances, please be aware that using spot training may result
    in longer and unpredictable training times. Each Spot Instance interruption will
    result in additional startup time during restart. The amount of available spot
    capacity depends on the instance type and AWS region. GPU-based instance types
    in certain AWS regions may have very limited spot capacity. Note that spot capacity
    constantly fluctuates. You can use the **Amazon Spot Instance advisor** feature
    to determine available spot capacity for different EC2 instances, the chance of
    interruption, and cost savings compared to regular on-demand instances.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter concludes *Part 2* of this book. In this and the two previous chapters,
    we discussed how to build and optimize large-scale training jobs. First, we reviewed
    the available specialized hardware for DL training and how to choose optimal instance
    types. Then, we discussed how to engineer distributed training using open source
    and Amazon proprietary solutions. In this chapter, we discussed how to efficiently
    operationalize your model training. We reviewed different issues that may occur
    during training and how to detect and mitigate them. We also discussed how to
    manage and optimize hyperparameter tuning.
  prefs: []
  type: TYPE_NORMAL
- en: In *Part 3*, *Serving Deep Learning Models*, we will dive deep into DL inference
    on Amazon SageMaker. We will discuss what hardware is available for inference
    and how to engineer your inference server. Then, we will review the operational
    aspects of model serving. In the next chapter, [*Chapter 8*](B17519_08.xhtml#_idTextAnchor121),
    *Considering Hardware for Inference*, we will review the available hardware accelerators
    suitable for inference workloads, discuss selection criteria, and explain how
    you can optimize your model for inference on specific hardware accelerators using
    model compilers and SageMaker Neo.
  prefs: []
  type: TYPE_NORMAL
- en: 'Part 3: Serving Deep Learning Models'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus on hosting trained models on Amazon SageMaker.
    We will review available software and hardware options and provide recommendations
    on what to choose and when.
  prefs: []
  type: TYPE_NORMAL
- en: 'This section comprises the following chapters:'
  prefs: []
  type: TYPE_NORMAL
- en: '[*Chapter 8*](B17519_08.xhtml#_idTextAnchor121), *Considering Hardware for
    Inference*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 9*](B17519_09.xhtml#_idTextAnchor137), *Implementing Model Servers*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[*Chapter 10*](B17519_10.xhtml#_idTextAnchor154), *Operationalizing Inference
    Workloads*'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
