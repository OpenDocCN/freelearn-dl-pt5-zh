<html><head></head><body>
<div id="_idContainer223">
<h1 class="c apter-number" id="_idParaDest-189"><a id="_idTextAnchor190"/><span class="koboSpan" id="kobo.1.1">9</span></h1>
<h1 id="_idParaDest-190"><a id="_idTextAnchor191"/><span class="koboSpan" id="kobo.2.1">Improving Inference Performance with MXNet</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In previous chapters, we leveraged MXNetâ€™s capabilities to solve </span><strong class="bold"><span class="koboSpan" id="kobo.4.1">computer vision</span></strong><span class="koboSpan" id="kobo.5.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.6.1">natural language processing tasks</span></strong><span class="koboSpan" id="kobo.7.1">. </span><span class="koboSpan" id="kobo.7.2">In those chapters, the focus was on obtaining the maximum performance out of </span><strong class="bold"><span class="koboSpan" id="kobo.8.1">pre-trained models</span></strong><span class="koboSpan" id="kobo.9.1">, leveraging the </span><strong class="bold"><span class="koboSpan" id="kobo.10.1">Model Zoo</span></strong><span class="koboSpan" id="kobo.11.1"> API from GluonCV and GluonNLP. </span><span class="koboSpan" id="kobo.11.2">We trained these models using different approaches from scratch, including </span><strong class="bold"><span class="koboSpan" id="kobo.12.1">transfer learning</span></strong><span class="koboSpan" id="kobo.13.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.14.1">fine-tuning</span></strong><span class="koboSpan" id="kobo.15.1">. </span><span class="koboSpan" id="kobo.15.2">In the previous chapter, we explored how some advanced techniques can be leveraged to optimize the training process. </span><span class="koboSpan" id="kobo.15.3">Finally, in this chapter, we will focus on improving the performance of the inference process itself, accelerating how we can obtain results from our models with several topics related to </span><strong class="bold"><span class="koboSpan" id="kobo.16.1">edge </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.17.1">AI computing</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.19.1">To achieve the objective of optimizing the performance of our inference pipeline, MXNet contains different features. </span><span class="koboSpan" id="kobo.19.2">We have already briefly discussed some of those features, such as the concept of </span><strong class="bold"><span class="koboSpan" id="kobo.20.1">Automatic Mixed Precision</span></strong><span class="koboSpan" id="kobo.21.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.22.1">AMP</span></strong><span class="koboSpan" id="kobo.23.1">), which was introduced in the previous chapter to increase the training performance and can also be used to increase the inference performance. </span><span class="koboSpan" id="kobo.23.2">We will revisit it in this chapter, along with other features, such as </span><strong class="bold"><span class="koboSpan" id="kobo.24.1">hybridization</span></strong><span class="koboSpan" id="kobo.25.1">. </span><span class="koboSpan" id="kobo.25.2">Moreover, we will further optimize how to use data types efficiently, leveraging the speed-ups associated with using the </span><strong class="bold"><span class="koboSpan" id="kobo.26.1">INT8</span></strong><span class="koboSpan" id="kobo.27.1"> data type </span><span class="No-Break"><span class="koboSpan" id="kobo.28.1">with </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.29.1">quantization</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.30.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.31.1">Moreover, we will explore how our models work in terms of operations, understanding how they work internally with the help of the </span><strong class="bold"><span class="koboSpan" id="kobo.32.1">MXNet profiler</span></strong><span class="koboSpan" id="kobo.33.1">. </span><span class="koboSpan" id="kobo.33.2">We will then take a step forward with the help of MXNet GluonCV Model Zoo and learn how to export our models to </span><strong class="bold"><span class="koboSpan" id="kobo.34.1">ONNX</span></strong><span class="koboSpan" id="kobo.35.1">, which allows us to use our models in different frameworks, such as deploying our models on NVIDIA hardware platforms, such as the </span><strong class="bold"><span class="koboSpan" id="kobo.36.1">NVIDIA Jetson</span></strong><span class="koboSpan" id="kobo.37.1"> family </span><span class="No-Break"><span class="koboSpan" id="kobo.38.1">of products.</span></span></p>
<p><span class="koboSpan" id="kobo.39.1">Finally, we will apply all these techniques together, taking as examples problems already explored in the book. </span><span class="koboSpan" id="kobo.39.2">For our computer vision task, we will choose image segmentation, and for our natural language processing task, we will choose translating text from English </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">to German.</span></span></p>
<p><span class="koboSpan" id="kobo.41.1">Specifically, this chapter contains the </span><span class="No-Break"><span class="koboSpan" id="kobo.42.1">following recipes:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.43.1">Introducing inference optimization features</span></li>
<li><span class="koboSpan" id="kobo.44.1">Optimizing inference for image segmentation</span></li>
<li><span class="koboSpan" id="kobo.45.1">Optimizing inference when translating text from English to German</span></li>
</ul>
<h1 id="_idParaDest-191"><a id="_idTextAnchor192"/><span class="koboSpan" id="kobo.46.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.47.1">Apart from the technical requirements specified in the </span><em class="italic"><span class="koboSpan" id="kobo.48.1">Preface</span></em><span class="koboSpan" id="kobo.49.1">, the </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">following apply:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.51.1">Ensure that you have completed </span><em class="italic"><span class="koboSpan" id="kobo.52.1">Recipe 1</span></em><span class="koboSpan" id="kobo.53.1">, </span><em class="italic"><span class="koboSpan" id="kobo.54.1">Installing MXNet</span></em><span class="koboSpan" id="kobo.55.1">, from </span><a href="B16591_01.xhtml#_idTextAnchor016"><em class="italic"><span class="koboSpan" id="kobo.56.1">Chapter 1</span></em></a><span class="koboSpan" id="kobo.57.1">, </span><em class="italic"><span class="koboSpan" id="kobo.58.1">Up and Running with MXNet</span></em><span class="koboSpan" id="kobo.59.1">.</span></li>
<li><span class="koboSpan" id="kobo.60.1">Ensure that you have completed </span><a href="B16591_05.xhtml#_idTextAnchor098"><em class="italic"><span class="koboSpan" id="kobo.61.1">Chapter 5</span></em></a><span class="koboSpan" id="kobo.62.1">, </span><em class="italic"><span class="koboSpan" id="kobo.63.1">Analyzing Images with Computer Vision</span></em><span class="koboSpan" id="kobo.64.1">, and </span><a href="B16591_06.xhtml#_idTextAnchor121"><em class="italic"><span class="koboSpan" id="kobo.65.1">Chapter 6</span></em></a><span class="koboSpan" id="kobo.66.1">, </span><em class="italic"><span class="koboSpan" id="kobo.67.1">Understanding Text with Natural Language Processing</span></em><span class="koboSpan" id="kobo.68.1">.</span></li>
<li><span class="koboSpan" id="kobo.69.1">Ensure that you have completed </span><a href="B16591_07.xhtml#_idTextAnchor148"><em class="italic"><span class="koboSpan" id="kobo.70.1">Chapter 7</span></em></a><span class="koboSpan" id="kobo.71.1">, </span><em class="italic"><span class="koboSpan" id="kobo.72.1">Optimizing Models with Transfer Learning and Fine-Tuning</span></em><span class="koboSpan" id="kobo.73.1">.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.74.1">The code for this chapter can be found at the following GitHub </span><span class="No-Break"><span class="koboSpan" id="kobo.75.1">URL: </span></span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch09"><span class="No-Break"><span class="koboSpan" id="kobo.76.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch09</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.77.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.78.1">Furthermore, you can access each recipe directly from Google Colab, for example, for the first recipe of this </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">chapter: </span></span><a href="https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch09/9_1_Introducing_inference_optimization_features.ipynb"><span class="No-Break"><span class="koboSpan" id="kobo.80.1">https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch09/9_1_Introducing_inference_optimization_features.ipynb</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.81.1">.</span></span></p>
<h1 id="_idParaDest-192"><a id="_idTextAnchor193"/><span class="koboSpan" id="kobo.82.1">Introducing inference optimization features</span></h1>
<p><span class="koboSpan" id="kobo.83.1">In the previous chapters, we</span><a id="_idIndexMarker1042"/><span class="koboSpan" id="kobo.84.1"> have seen how we can leverage MXNet, GluonCV, and GluonNLP to retrieve pre-trained models in certain datasets (such as ImageNet, MS COCO, or IWSLT2015) and use them for our specific tasks and datasets. </span><span class="koboSpan" id="kobo.84.2">Furthermore, we used transfer learning and fine-tuning techniques to improve the algorithmic performance of </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">those tasks/datasets.</span></span></p>
<p><span class="koboSpan" id="kobo.86.1">In this recipe, we will introduce (and revisit) several concepts and features that will optimize our inference loops to improve our runtime performance, and we will analyze the </span><span class="No-Break"><span class="koboSpan" id="kobo.87.1">trade-offs involved.</span></span></p>
<h2 id="_idParaDest-193"><a id="_idTextAnchor194"/><span class="koboSpan" id="kobo.88.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.89.1">As in previous</span><a id="_idIndexMarker1043"/><span class="koboSpan" id="kobo.90.1"> chapters, in this recipe, we will be using some matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.91.1">at all.</span></span></p>
<h2 id="_idParaDest-194"><a id="_idTextAnchor195"/><span class="koboSpan" id="kobo.92.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.93.1">In this recipe, we will be carrying out the </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">following steps:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.95.1">Hybridizing </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">our models</span></span></li>
<li><span class="koboSpan" id="kobo.97.1">Applying float16 and AMP </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">for inference</span></span></li>
<li><span class="koboSpan" id="kobo.99.1">Applying quantization by </span><span class="No-Break"><span class="koboSpan" id="kobo.100.1">using INT8</span></span></li>
<li><span class="koboSpan" id="kobo.101.1">Profiling </span><span class="No-Break"><span class="koboSpan" id="kobo.102.1">our models</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.103.1">Letâ€™s dive into each of </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">these steps.</span></span></p>
<h3><span class="koboSpan" id="kobo.105.1">Hybridizing our models</span></h3>
<p><span class="koboSpan" id="kobo.106.1">In the initial chapters where we </span><a id="_idIndexMarker1044"/><span class="koboSpan" id="kobo.107.1">were exploring the features of MXNet, we focused on </span><strong class="bold"><span class="koboSpan" id="kobo.108.1">imperative programming</span></strong><span class="koboSpan" id="kobo.109.1">. </span><span class="koboSpan" id="kobo.109.2">If you </span><a id="_idIndexMarker1045"/><span class="koboSpan" id="kobo.110.1">have coded in the past with languages such as Java, C/C++, or Python, it is very likely you used imperative programming. </span><span class="koboSpan" id="kobo.110.2">It is the usual way of coding, as it is </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">more flexible.</span></span></p>
<p><span class="koboSpan" id="kobo.112.1">With imperative programming, a step-by-step sequential execution of the statements set in the code is expected. </span><span class="koboSpan" id="kobo.112.2">For example, typically in our evaluation paths, we run these statements step-by-step inside </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">a loop:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.114.1">Load new samples from our </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">data loader.</span></span></li>
<li><span class="koboSpan" id="kobo.116.1">Transform the input and expected output so that it can be consumed by our model and our </span><span class="No-Break"><span class="koboSpan" id="kobo.117.1">metrics computations.</span></span></li>
<li><span class="koboSpan" id="kobo.118.1">Pass the input through the model to compute </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">the output.</span></span></li>
<li><span class="koboSpan" id="kobo.120.1">Compare the model output with the expected output and update the </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">corresponding metrics.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.122.1">In this </span><a id="_idIndexMarker1046"/><span class="koboSpan" id="kobo.123.1">programming paradigm, each of the statements is executed in sequence, and the output can be checked or debugged for each step if we wait for its completion (as MXNet uses </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.124.1">lazy evaluation</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">).</span></span></p>
<p><span class="koboSpan" id="kobo.126.1">With a different programming paradigm, called </span><strong class="bold"><span class="koboSpan" id="kobo.127.1">symbolic programming</span></strong><span class="koboSpan" id="kobo.128.1">, symbols are used instead, which</span><a id="_idIndexMarker1047"/><span class="koboSpan" id="kobo.129.1"> are basically abstractions for operations, and no actual computation happens until a defined point (typically known as the compile step). </span><span class="koboSpan" id="kobo.129.2">This is especially useful</span><a id="_idIndexMarker1048"/><span class="koboSpan" id="kobo.130.1"> for </span><strong class="bold"><span class="koboSpan" id="kobo.131.1">deep learning</span></strong><span class="koboSpan" id="kobo.132.1">, as all models can be defined as graphs, use this graph as a symbol, optimize the operation paths in the underlying graph, and only run the optimized computation </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">when needed.</span></span></p>
<p><span class="koboSpan" id="kobo.134.1">However, as the computation hasnâ€™t happened yet, the output for each step cannot be checked or debugged, making the finding and fixing of issues much more difficult. </span><span class="koboSpan" id="kobo.134.2">On the other hand, due to the capabilities of graph optimization, symbolic programming requires less memory and </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">is faster.</span></span></p>
<p><span class="koboSpan" id="kobo.136.1">Thankfully, with MXNet, we can leverage the best of both worlds. </span><span class="koboSpan" id="kobo.136.2">We can define our model with imperative programming, test it, debug it, and fix it with the usual mechanisms (</span><em class="italic"><span class="koboSpan" id="kobo.137.1">print</span></em><span class="koboSpan" id="kobo.138.1"> statements, tests, debugging, and so on). </span><span class="koboSpan" id="kobo.138.2">When we are ready for optimization, we just need to call the </span><strong class="source-inline"><span class="koboSpan" id="kobo.139.1">hybridize</span></strong><span class="koboSpan" id="kobo.140.1"> function, and it will take care of everything under the hood, working with our graph in symbolic programming. </span><span class="koboSpan" id="kobo.140.2">This approach is called hybrid programming and is one of the best advantages of MXNet. </span><span class="koboSpan" id="kobo.140.3">Moreover, there is no hardware limitation for this feature, and it can be used for both CPU and </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">GPU computations.</span></span></p>
<p><span class="koboSpan" id="kobo.142.1">As a toy example, we can run some experiments with the inference of a model and compare the different results for different configurations. </span><span class="koboSpan" id="kobo.142.2">Specifically, these are the configurations we </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">will test:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.144.1">CPU:</span><ul><li><span class="koboSpan" id="kobo.145.1">With imperative execution</span></li><li><span class="koboSpan" id="kobo.146.1">With symbolic execution and default parameters</span></li><li><span class="koboSpan" id="kobo.147.1">With symbolic execution with a specific backend</span></li><li><span class="koboSpan" id="kobo.148.1">With symbolic execution, specific backend, and static allocation of memory</span></li><li><span class="koboSpan" id="kobo.149.1">With symbolic execution, specific backend, static allocation of memory, and invariant input shapes</span></li></ul></li>
<li><span class="koboSpan" id="kobo.150.1">GPU:</span><ul><li><span class="koboSpan" id="kobo.151.1">With imperative execution</span></li><li><span class="koboSpan" id="kobo.152.1">With symbolic execution and default parameters</span></li><li><span class="koboSpan" id="kobo.153.1">With symbolic execution and static allocation of memory</span></li><li><span class="koboSpan" id="kobo.154.1">With symbolic execution, static allocation of memory and invariant input shapes</span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.155.1">Please note that in </span><a id="_idIndexMarker1049"/><span class="koboSpan" id="kobo.156.1">order to verify the computation time properly, we are adding calls to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.157.1">mx.nd.waitall()</span></strong><span class="koboSpan" id="kobo.158.1"> function. </span><span class="koboSpan" id="kobo.158.2">The method chosen is to </span><a id="_idIndexMarker1050"/><span class="koboSpan" id="kobo.159.1">use the </span><strong class="bold"><span class="koboSpan" id="kobo.160.1">ADE20K</span></strong><span class="koboSpan" id="kobo.161.1"> validation split (dataset available from MXNet GluonCV) and </span><a id="_idIndexMarker1051"/><span class="koboSpan" id="kobo.162.1">process it with a </span><strong class="bold"><span class="koboSpan" id="kobo.163.1">DeepLabv3</span></strong><span class="koboSpan" id="kobo.164.1"> model. </span><span class="koboSpan" id="kobo.164.2">We will be using a batch size </span><span class="No-Break"><span class="koboSpan" id="kobo.165.1">of 4:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.166.1">For the initial CPU computing configuration, with imperative execution, the processing of the dataset by the model took the </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">following time:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.168.1">
Time (s): 115.22693085670471</span></pre></li> <li><span class="koboSpan" id="kobo.169.1">For the second CPU computing configuration, we just need to leverage the MXNet hybrid programming model and transform our model with </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">the following:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.171.1">
deeplab_pt_cpu_hybrid.hybridize()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.172.1">With this configuration, the processing of the dataset by the model took the following time:</span></p><pre class="source-code"><span class="koboSpan" id="kobo.173.1">Time (s): 64.75840330123901</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.174.1">As we can see, the optimizations performed reduced to almost half the computation time.</span></p></li> <li><span class="koboSpan" id="kobo.175.1">For the third CPU computing configuration, we just need to slightly modify our hybridization call to define a specific backend. </span><span class="koboSpan" id="kobo.175.2">We will leverage our Intel CPU architecture, use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.176.1">MKLDNN</span></strong><span class="koboSpan" id="kobo.177.1"> backend, and transform our model with </span><span class="No-Break"><span class="koboSpan" id="kobo.178.1">the following:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.179.1">
deeplab_pt_cpu_hybrid.hybridize(backend = "MKLDNN")</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.180.1">With this </span><a id="_idIndexMarker1052"/><span class="koboSpan" id="kobo.181.1">configuration, the processing of the dataset by the model took the following time:</span></p><pre class="source-code"><span class="koboSpan" id="kobo.182.1">Time (s): 55.860424757003784</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.183.1">As we can see, the specific backend further reduced the computation time by ~20%.</span></p></li> <li><span class="koboSpan" id="kobo.184.1">For the fourth CPU computing configuration, we just need to slightly modify our hybridization call to define that we want to use the static memory allocation. </span><span class="koboSpan" id="kobo.184.2">We can update our call with </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">the following:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.186.1">
deeplab_pt_cpu_hybrid.hybridize(backend = "MKLDNN", static_alloc=True)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.187.1">With this configuration, the processing of the dataset by the model took the following time:</span></p><pre class="source-code"><span class="koboSpan" id="kobo.188.1">Time (s): 53.905478715896606</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.189.1">As we can see, the static memory allocation allowed us to reduce the computation time by another ~4%.</span></p></li> <li><span class="koboSpan" id="kobo.190.1">For the fifth CPU computing configuration, we just need to slightly modify our hybridization call to define that we want to leverage our invariant input shapes (we have already preprocessed our data to have the same input shape, </span><strong class="source-inline"><span class="koboSpan" id="kobo.191.1">480x480</span></strong><span class="koboSpan" id="kobo.192.1">). </span><span class="koboSpan" id="kobo.192.2">We can update our call with </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">the following:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.194.1">
deeplab_pt_cpu_hybrid.hybridize(backend = "MKLDNN", static_alloc=True, static_shape=True)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.195.1">With this configuration, the processing of the dataset by the model took the following time:</span></p><pre class="source-code"><span class="koboSpan" id="kobo.196.1">Time (s): 52.464826822280884</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.197.1">As we can see, the invariant input shape constraint allowed us to reduce the computation time by another ~2%.</span></p></li> <li><span class="koboSpan" id="kobo.198.1">For the initial GPU computing configuration, with imperative execution, the processing of the dataset by the model took the </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">following time:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.200.1">
Time (s): 13.315197944641113</span></pre></li> <li><span class="koboSpan" id="kobo.201.1">For the</span><a id="_idIndexMarker1053"/><span class="koboSpan" id="kobo.202.1"> second GPU computing configuration, we just need to leverage the MXNet hybrid programming model and transform our model with </span><span class="No-Break"><span class="koboSpan" id="kobo.203.1">the following:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.204.1">
deeplab_pt_gpu_hybrid.hybridize()</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.205.1">With this configuration, the processing of the dataset by the model took the following time:</span></p><pre class="source-code"><span class="koboSpan" id="kobo.206.1">Time (s): 12.873461246490479</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.207.1">As we can see, when the optimizations are performed in the GPU, they yield almost no improvement in the computation time as GPUs are already optimized internally for these types of computations.</span></p></li> <li><span class="koboSpan" id="kobo.208.1">For GPU computing, there are no specific backends to be selected. </span><span class="koboSpan" id="kobo.208.2">Therefore, for the third GPU computing configuration, we just need to slightly modify our hybridization call to define that we want to use static memory allocation. </span><span class="koboSpan" id="kobo.208.3">We can update our call with </span><span class="No-Break"><span class="koboSpan" id="kobo.209.1">the following:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.210.1">
deeplab_pt_gpu_hybrid.hybridize(static_alloc=True)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.211.1">With this configuration, the processing of the dataset by the model took the following time:</span></p><pre class="source-code"><span class="koboSpan" id="kobo.212.1">Time (s): 12.752988815307617</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.213.1">As we can see, the static memory allocation produced another negligible improvement on the GPU.</span></p></li> <li><span class="koboSpan" id="kobo.214.1">For the fourth GPU computing configuration, we just need to slightly modify our hybridization call to define that we want to leverage our invariant input shapes (we already preprocessed our data to have the same input shape, 480x480). </span><span class="koboSpan" id="kobo.214.2">We can update our call with </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">the following:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.216.1">
deeplab_pt_gpu_hybrid.hybridize(static_alloc=True, static_shape=True)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.217.1">With this configuration, the processing of the dataset by the model took the following:</span></p><pre class="source-code"><span class="koboSpan" id="kobo.218.1">Time (s): 12.583650827407837</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.219.1">As we can see, the invariant input shape constraint produced another negligible improvement on the GPU.</span></p></li> </ol>
<p><span class="koboSpan" id="kobo.220.1">The results show </span><a id="_idIndexMarker1054"/><span class="koboSpan" id="kobo.221.1">that when using the CPU, we can reduce the inference time to half the original time, which is a significant improvement. </span><span class="koboSpan" id="kobo.221.2">With the GPU, the improvements are negligible due to the </span><span class="No-Break"><span class="koboSpan" id="kobo.222.1">internal optimizations.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.223.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.224.1">Please note in the code how we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.225.1">mx.nd.waitall()</span></strong><span class="koboSpan" id="kobo.226.1"> function to verify that all computations have been strictly completed before computing the time these operations took.</span></p>
<h3><span class="koboSpan" id="kobo.227.1">Applying float16 and AMP for inference</span></h3>
<p><span class="koboSpan" id="kobo.228.1">In the</span><a id="_idIndexMarker1055"/><span class="koboSpan" id="kobo.229.1"> previous chapter, </span><a href="B16591_08.xhtml#_idTextAnchor172"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.230.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.231.1">, </span><em class="italic"><span class="koboSpan" id="kobo.232.1">Improving Training Performance with MXNet</span></em><span class="koboSpan" id="kobo.233.1">, we introduced the </span><strong class="source-inline"><span class="koboSpan" id="kobo.234.1">float16</span></strong><span class="koboSpan" id="kobo.235.1"> data type and AMP optimization, an extremely simple way to leverage this half-precision data type only when it was </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">most useful.</span></span></p>
<p><span class="koboSpan" id="kobo.237.1">In </span><em class="italic"><span class="koboSpan" id="kobo.238.1">Recipe 1</span></em><span class="koboSpan" id="kobo.239.1">, </span><em class="italic"><span class="koboSpan" id="kobo.240.1">Introducing training optimization features</span></em><span class="koboSpan" id="kobo.241.1">, from the previous chapter, we compared single-precision (</span><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">float32</span></strong><span class="koboSpan" id="kobo.243.1">) and half-precision (</span><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">float16</span></strong><span class="koboSpan" id="kobo.245.1">) data types, understanding their characteristics and memory/speed trade-offs. </span><span class="koboSpan" id="kobo.245.2">You are encouraged to review the recipe if you havenâ€™t already done so as it is very relevant to </span><span class="No-Break"><span class="koboSpan" id="kobo.246.1">this topic.</span></span></p>
<p><span class="koboSpan" id="kobo.247.1">As most concepts were introduced previously, in this section, we will focus on how to apply AMP to the inference process. </span><span class="koboSpan" id="kobo.247.2">As usual, MXNet provides a very simple interface for this operation, just requiring a call to the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.248.1">amp.convert_hybrid_block()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.249.1"> function.</span></span></p>
<p><span class="koboSpan" id="kobo.250.1">This </span><a id="_idIndexMarker1056"/><span class="koboSpan" id="kobo.251.1">optimization</span><a id="_idIndexMarker1057"/><span class="koboSpan" id="kobo.252.1"> can be applied to both CPU and GPU environments, so letâ€™s run </span><span class="No-Break"><span class="koboSpan" id="kobo.253.1">these experiments.</span></span></p>
<p><span class="koboSpan" id="kobo.254.1">To modify our CPU model to use AMP, we just need the following line </span><span class="No-Break"><span class="koboSpan" id="kobo.255.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.256.1">
deeplab_pt_cpu_hybrid_amp = amp.convert_hybrid_block(deeplab_pt_cpu_hybrid, ctx=mx.cpu())</span></pre> <p><span class="koboSpan" id="kobo.257.1">With this modified model, the processing of the dataset took the </span><span class="No-Break"><span class="koboSpan" id="kobo.258.1">following time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.259.1">
Time (s): 56.16465926170349</span></pre> <p><span class="koboSpan" id="kobo.260.1">As we can see, AMP produced negligible improvements on the CPU. </span><span class="koboSpan" id="kobo.260.2">This is due to the largest gains being achieved on the backward pass required for training, but not necessary during inference. </span><span class="koboSpan" id="kobo.260.3">Furthermore, CPUs do not typically have specific circuitry to work directly with float16, limiting </span><span class="No-Break"><span class="koboSpan" id="kobo.261.1">the improvements.</span></span></p>
<p><span class="koboSpan" id="kobo.262.1">To modify the GPU model to use AMP, we just need the following line </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.264.1">
deeplab_pt_gpu_hybrid_amp = amp.convert_hybrid_block(deeplab_pt_gpu_hybrid, ctx=mx.gpu())</span></pre> <p><span class="koboSpan" id="kobo.265.1">With this modified model, the processing of the dataset took the </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">following time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.267.1">
Time (s): 3.371366024017334</span></pre> <p><span class="koboSpan" id="kobo.268.1">As we can see, AMP produced excellent results on the GPU, reducing the inference time to almost ~25%. </span><span class="koboSpan" id="kobo.268.2">This is due to GPUs having specific circuitry to work directly with float16, improving the </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">results massively.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.270.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.271.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.272.1">amp.convert_hybrid_block()</span></strong><span class="koboSpan" id="kobo.273.1"> function accepts different parameters. </span><span class="koboSpan" id="kobo.273.2">You are encouraged to try different options (such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.274.1">cast_optional_params</span></strong><span class="koboSpan" id="kobo.275.1">) to find the optimal configuration.</span></p>
<h3><span class="koboSpan" id="kobo.276.1">Applying quantization by using Int8</span></h3>
<p><span class="koboSpan" id="kobo.277.1">In the</span><a id="_idIndexMarker1058"/><span class="koboSpan" id="kobo.278.1"> previous sections, we saw how to optimize our inference loops by using different approaches optimizing how to use the CPU and GPU for maximum performance, given a model. </span><span class="koboSpan" id="kobo.278.2">We also explored</span><a id="_idIndexMarker1059"/><span class="koboSpan" id="kobo.279.1"> how to leverage single-precision (float32) and half-precision (float16) data types. </span><span class="koboSpan" id="kobo.279.2">In this section, we will explore how our data inputs, our model parameters, and the different arithmetic calculations among them can be optimized with a new data </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">type, Int8.</span></span></p>
<p><span class="koboSpan" id="kobo.281.1">This data type modification has larger implications than a change in precision. </span><span class="koboSpan" id="kobo.281.2">We are also modifying the underlying representation from a floating-point number to an integer, which yields a reduction in both memory and computing requirements. </span><span class="koboSpan" id="kobo.281.3">Letâ€™s analyze this </span><span class="No-Break"><span class="koboSpan" id="kobo.282.1">data type.</span></span></p>
<p><span class="koboSpan" id="kobo.283.1">Int8 indicates two things: that it is a data type that only supports integer numbers (no floating radix point), and that the amount of bits used to store a single number in this format is 8 bits. </span><span class="koboSpan" id="kobo.283.2">The most important features of this format are </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.285.1">Capability of representing integer numbers from -128 to 127, or from 0 to 255 (depending on whether it is of the signed or unsigned type)</span></li>
<li><span class="koboSpan" id="kobo.286.1">Constant precision (each consecutive number differs by exactly 1)</span></li>
</ul>
<p><span class="koboSpan" id="kobo.287.1">To explain the core idea behind </span><strong class="source-inline"><span class="koboSpan" id="kobo.288.1">Int8</span></strong><span class="koboSpan" id="kobo.289.1"> quantization, and also to show the loss of precision, we can display the approximated value of the number 1/3 (one third-th) in both formats (Float32 and Int8) with this </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">code excerpt:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.291.1">
a = mx.nd.array([1/3], dtype=mx.np.float32)
 int_value = 85
scaling_factor = 255
b = int_value / scaling_factor
print("1/3 as 0.333... </span><span class="koboSpan" id="kobo.291.2">(Float32): {0:.30f}".format(a.asscalar())))
print("1/3 as 85/255Â Â Â (Int8)Â Â Â : {0:.30f}".format(b))</span></pre> <p><span class="koboSpan" id="kobo.292.1">This yields </span><span class="No-Break"><span class="koboSpan" id="kobo.293.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.294.1">
1/3 as 0.333... </span><span class="koboSpan" id="kobo.294.2">(Float32): 0.333333343267440795898437500000
1/3 as 85/255Â Â Â (Int8)Â Â Â : 0.333333333333333314829616256247</span></pre> <p><span class="koboSpan" id="kobo.295.1">As we</span><a id="_idIndexMarker1060"/><span class="koboSpan" id="kobo.296.1"> can see, none of the representations are exact, with </span><strong class="source-inline"><span class="koboSpan" id="kobo.297.1">float32</span></strong><span class="koboSpan" id="kobo.298.1"> yielding a very high precision as expected. </span><span class="koboSpan" id="kobo.298.2">With </span><strong class="source-inline"><span class="koboSpan" id="kobo.299.1">Int8</span></strong><span class="koboSpan" id="kobo.300.1">, we </span><a id="_idIndexMarker1061"/><span class="koboSpan" id="kobo.301.1">did a small shortcut; we </span><a id="_idIndexMarker1062"/><span class="koboSpan" id="kobo.302.1">used two 8-bit integers, </span><strong class="source-inline"><span class="koboSpan" id="kobo.303.1">85</span></strong><span class="koboSpan" id="kobo.304.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.305.1">255</span></strong><span class="koboSpan" id="kobo.306.1">, and used one as the scaling factor. </span><span class="koboSpan" id="kobo.306.2">This scaling factor is typically applied for several sets of numbers at the same time. </span><span class="koboSpan" id="kobo.306.3">It can be the same scaling factor for the whole model (unlikely), per layer, and so on. </span><span class="koboSpan" id="kobo.306.4">The scaling factor does not need to be represented in </span><strong class="source-inline"><span class="koboSpan" id="kobo.307.1">Int8</span></strong><span class="koboSpan" id="kobo.308.1">; it can be </span><span class="No-Break"><span class="koboSpan" id="kobo.309.1">a </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.310.1">float32</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.312.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.313.1">For this particular example, the chosen Int8 representation is more exact to the intended number, but this is a coincidence. </span><span class="koboSpan" id="kobo.313.2">In common scenarios, there is a loss of precision that translates to a loss of performance.</span></p>
<p><span class="koboSpan" id="kobo.314.1">To minimize this loss of performance, typically quantization tuning techniques ask for a </span><strong class="bold"><span class="koboSpan" id="kobo.315.1">calibration dataset</span></strong><span class="koboSpan" id="kobo.316.1">. </span><span class="koboSpan" id="kobo.316.2">This </span><a id="_idIndexMarker1063"/><span class="koboSpan" id="kobo.317.1">dataset is then used to compute the parameters that minimize the mentioned </span><span class="No-Break"><span class="koboSpan" id="kobo.318.1">performance loss.</span></span></p>
<p><span class="koboSpan" id="kobo.319.1">In addition to using a calibration dataset, there are several techniques to optimize the computation of the most accurate Int8 values, and MXNet provides a very simple API to facilitate the optimization of our networks. </span><span class="koboSpan" id="kobo.319.2">With a simple call to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.320.1">mx.contrib.quantization.quantize_net_v2()</span></strong><span class="koboSpan" id="kobo.321.1"> function, we will update our network </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">to Int8.</span></span></p>
<p><span class="koboSpan" id="kobo.323.1">Specifically, for our experiments, this is the call that </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">we used:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.325.1">
deeplab_pt_cpu_q_hybrid = quantization.quantize_net_v2(
deeplab_pt_cpu,
 quantized_dtype='auto',
 exclude_layers=None,
 exclude_layers_match=None,
 calib_data=ade20k_cal_loader_gpu_cpu,
 calib_mode='entropy',
 logger=logger,
 ctx=mx.cpu())</span></pre> <p class="callout- eading"><span class="koboSpan" id="kobo.326.1">Important Note</span></p>
<p class="callout"><strong class="source-inline"><span class="koboSpan" id="kobo.327.1">Int8</span></strong><span class="koboSpan" id="kobo.328.1"> quantization is a nuanced process, and tailoring it for a specific application requires in-depth analysis and some trial-and-error experiments. </span><span class="koboSpan" id="kobo.328.2">For more information regarding the parameters involved, you are encouraged to read the function documentation: </span><a href="https://github.com/apache/mxnet/blob/v1.9.1/python/mxnet/contrib/quantization.py#L825"><span class="koboSpan" id="kobo.329.1">https://github.com/apache/mxnet/blob/v1.9.1/python/mxnet/contrib/quantization.py#L825</span></a><span class="koboSpan" id="kobo.330.1">.</span></p>
<p><span class="koboSpan" id="kobo.331.1">With this</span><a id="_idIndexMarker1064"/><span class="koboSpan" id="kobo.332.1"> modified CPU-based model, the processing of the dataset took the </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">following time:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.334.1">
Time (s): 36.10324692726135</span></pre> <p><span class="koboSpan" id="kobo.335.1">As we can see, </span><strong class="source-inline"><span class="koboSpan" id="kobo.336.1">Int8</span></strong><span class="koboSpan" id="kobo.337.1"> produced a strong improvement in the CPU, yielding almost another ~50% reduction </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">in runtime.</span></span></p>
<p><span class="koboSpan" id="kobo.339.1">Unfortunately, for GPUs, this feature cannot be introduced. </span><span class="koboSpan" id="kobo.339.2">Although recent GPUs have dedicated </span><strong class="source-inline"><span class="koboSpan" id="kobo.340.1">Int8</span></strong><span class="koboSpan" id="kobo.341.1"> circuitry, this is quite a new development and MXNet does not support these </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">operators yet.</span></span></p>
<h3><span class="koboSpan" id="kobo.343.1">Profiling our models</span></h3>
<p><span class="koboSpan" id="kobo.344.1">In </span><a id="_idIndexMarker1065"/><span class="koboSpan" id="kobo.345.1">this recipe, we have seen how to use different techniques to optimize our inference loops. </span><span class="koboSpan" id="kobo.345.2">However, sometimes, even after introducing these optimization techniques, our models might not reach the runtime performance we are targeting. </span><span class="koboSpan" id="kobo.345.3">This could be due to a number </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">of reasons:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.347.1">Architecture is not optimal for edge computing.</span></li>
<li><span class="koboSpan" id="kobo.348.1">Operators have not been optimized adequately.</span></li>
<li><span class="koboSpan" id="kobo.349.1">Data transfers among components.</span></li>
<li><span class="koboSpan" id="kobo.350.1">Memory leaks.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.351.1">In order to verify how our model is working internally, to check where to optimize further and/or investigate the possible reasons why our models might not be performing well, MXNet provides us with a tool for low-level analysis called the </span><span class="No-Break"><span class="koboSpan" id="kobo.352.1">MXNet profiler.</span></span></p>
<p><span class="koboSpan" id="kobo.353.1">The MXNet </span><a id="_idIndexMarker1066"/><span class="koboSpan" id="kobo.354.1">profiler runs in the background and records all operations and data transfers happening on our models in real time. </span><span class="koboSpan" id="kobo.354.2">It is also very lightweight, consuming a minimal amount of resources. </span><span class="koboSpan" id="kobo.354.3">Best of all, it is extremely easy to configure </span><span class="No-Break"><span class="koboSpan" id="kobo.355.1">and use.</span></span></p>
<p><span class="koboSpan" id="kobo.356.1">In order to profile a set of statements, we need to take </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">two steps:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.358.1">Configure </span><span class="No-Break"><span class="koboSpan" id="kobo.359.1">the profiler.</span></span></li>
<li><span class="koboSpan" id="kobo.360.1">Start and stop the profiler before and after the statements to </span><span class="No-Break"><span class="koboSpan" id="kobo.361.1">be profiled.</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.362.1">To configure the profiler, we just need one line of code, </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.364.1">
mx.profiler.set_config(
profile_all=True,
 aggregate_stats=True,
 continuous_dump=True,
 filename='profile_output_cpu.json')</span></pre> <p><span class="koboSpan" id="kobo.365.1">To start and stop the profiler, we need to add the following lines at the beginning and end of the statements to </span><span class="No-Break"><span class="koboSpan" id="kobo.366.1">be analyzed:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.367.1">
mx.profiler.set_state('run')
[... </span><span class="koboSpan" id="kobo.367.2">code statements to analyze ...]
# Wait until all operations have completed
mx.nd.waitall()
# Stop recording
mx.profiler.set_state('stop')
# Log results
mx.profiler.dump()</span></pre> <p><span class="koboSpan" id="kobo.368.1">Please note how we need three statements to stop recording: finalize all instructions, stop recording, and dump the information of the file configured in the </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">first step.</span></span></p>
<p class="callout- eading"><span class="koboSpan" id="kobo.370.1">Important Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.371.1">Please note in the code how we used the </span><strong class="source-inline"><span class="koboSpan" id="kobo.372.1">mx.nd.waitall()</span></strong><span class="koboSpan" id="kobo.373.1"> function to verify that all computations have been strictly completed before computing the time these operations took.</span></p>
<p><span class="koboSpan" id="kobo.374.1">The</span><a id="_idIndexMarker1067"/><span class="koboSpan" id="kobo.375.1"> instructions described previously generate a JSON file, which can then be analyzed with tracing applications. </span><span class="koboSpan" id="kobo.375.2">I recommend the Tracing app included with Google Chrome as it is very easy to use and access. </span><span class="koboSpan" id="kobo.375.3">Simply type the following in the address </span><span class="No-Break"><span class="koboSpan" id="kobo.376.1">bar: </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.377.1">chrome://tracing</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.378.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.379.1">In order to verify the functionality of the MXNet profiler, letâ€™s take the example of </span><strong class="bold"><span class="koboSpan" id="kobo.380.1">ResNet</span></strong><span class="koboSpan" id="kobo.381.1"> architectures, which</span><a id="_idIndexMarker1068"/><span class="koboSpan" id="kobo.382.1"> are used extensively, such as in our image segmentation task, being the backbone of the DeepLabv3 network </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">we use.</span></span></p>
<p><span class="koboSpan" id="kobo.384.1">The typical architecture of a ResNet network is </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">the following:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer204">
<span class="koboSpan" id="kobo.386.1"><img alt="Figure 9.1 â€“ ResNet50 model architecture" src="image/B16591_09_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.387.1">Figure 9.1 â€“ ResNet50 model architecture</span></p>
<p><span class="koboSpan" id="kobo.388.1">Please note that in </span><em class="italic"><span class="koboSpan" id="kobo.389.1">Figure 9.1</span></em><span class="koboSpan" id="kobo.390.1">, the initial steps (stage 1) are convolution, batch normalization, </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">and activation.</span></span></p>
<p><span class="koboSpan" id="kobo.392.1">From our profiled model, the Google Chrome Tracing app provides the </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">following screen:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer205">
<span class="koboSpan" id="kobo.394.1"><img alt="Figure 9.2 â€“ Profiling ResNet" src="image/B16591_09_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.395.1">Figure 9.2 â€“ Profiling ResNet</span></p>
<p><span class="koboSpan" id="kobo.396.1">In </span><em class="italic"><span class="koboSpan" id="kobo.397.1">Figure 9.2</span></em><span class="koboSpan" id="kobo.398.1">, we </span><a id="_idIndexMarker1069"/><span class="koboSpan" id="kobo.399.1">can see the general execution</span><a id="_idIndexMarker1070"/><span class="koboSpan" id="kobo.400.1"> of the model. </span><span class="koboSpan" id="kobo.400.2">Zooming on the earlier layers, we can see it </span><span class="No-Break"><span class="koboSpan" id="kobo.401.1">as follows:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer206">
<span class="koboSpan" id="kobo.402.1"><img alt="Figure 9.3 â€“ Profiling ResNet: Zoom" src="image/B16591_09_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.403.1">Figure 9.3 â€“ Profiling ResNet: Zoom</span></p>
<p><span class="koboSpan" id="kobo.404.1">In </span><em class="italic"><span class="koboSpan" id="kobo.405.1">Figure 9.3</span></em><span class="koboSpan" id="kobo.406.1">, we can see how the stage 1 steps of convolution, batch normalization, and activation are clearly displayed. </span><span class="koboSpan" id="kobo.406.2">We can also now very clearly see how the batch normalization operation takes about 4x longer than the convolution and activation steps, potentially indicating an avenue </span><span class="No-Break"><span class="koboSpan" id="kobo.407.1">of improvement.</span></span></p>
<h2 id="_idParaDest-195"><a id="_idTextAnchor196"/><span class="koboSpan" id="kobo.408.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.409.1">In this recipe, we have </span><a id="_idIndexMarker1071"/><span class="koboSpan" id="kobo.410.1">taken a deeper look into how MXNet and Gluon can help us optimize our inference loops. </span><span class="koboSpan" id="kobo.410.2">We have leveraged our hardware (CPUs and GPUs) by addressing each of the steps in the </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">inference loop:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.412.1">Reused the work done for data loading in the previous chapter</span></li>
<li><span class="koboSpan" id="kobo.413.1">Optimized graph computation via hybridization</span></li>
<li><span class="koboSpan" id="kobo.414.1">Analyzed different data types and combined the accuracy and precision of float32 with the speed-ups of </span><strong class="source-inline"><span class="koboSpan" id="kobo.415.1">float16</span></strong><span class="koboSpan" id="kobo.416.1"> (leveraging the specific circuitry of GPUs) where possible, using AMP</span></li>
<li><span class="koboSpan" id="kobo.417.1">Taken </span><a id="_idIndexMarker1072"/><span class="koboSpan" id="kobo.418.1">another step forward by using Int8 quantization</span></li>
<li><span class="koboSpan" id="kobo.419.1">Analyzed low-level performance using the MXNet profiler</span></li>
</ul>
<p><span class="koboSpan" id="kobo.420.1">We compared</span><a id="_idIndexMarker1073"/><span class="koboSpan" id="kobo.421.1"> each of these features by running several experiments and comparing the performance before and after a specific optimization, emphasizing potential trade-offs that have to be taken into account when using these optimizations. </span><span class="koboSpan" id="kobo.421.2">To summarize, these were </span><span class="No-Break"><span class="koboSpan" id="kobo.422.1">the results:</span></span></p>
<table class="No-Table-Style _idGenTablePara-1" id="table001-2">
<colgroup>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.423.1">Feature</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.424.1">Result on </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.425.1">CPU (ms)</span></strong></span></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold"><span class="koboSpan" id="kobo.426.1">Result on </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.427.1">GPU (ms)</span></strong></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.428.1">Standard</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.429.1">115</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.430.1">13.3</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.431.1">Hybridize / </span><span class="No-Break"><span class="koboSpan" id="kobo.432.1">Default</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.433.1">65</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.434.1">12.9</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.435.1">Hybridize / </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">MKLDNN</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.437.1">56</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.438.1">N/A</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.439.1">Hybridize / MKLDNN + </span><span class="No-Break"><span class="koboSpan" id="kobo.440.1">Static Alloc</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.441.1">54</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.442.1">12.8</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="koboSpan" id="kobo.443.1">Hybridize / MKLDNN + Static Alloc + </span><span class="No-Break"><span class="koboSpan" id="kobo.444.1">Invariant Shape</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.445.1">52</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.446.1">12.6</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.447.1">AMP</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.448.1">54</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.449.1">3.5</span></span></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.450.1">Int8 Quantization</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.451.1">36</span></span></p>
</td>
<td class="No-Table-Style">
<p><span class="No-Break"><span class="koboSpan" id="kobo.452.1">N/A</span></span></p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.453.1">Table 9.1 â€“ Summary of features and results for the CPU and GPU</span></p>
<p><span class="koboSpan" id="kobo.454.1">In the</span><a id="_idIndexMarker1074"/><span class="koboSpan" id="kobo.455.1"> next </span><a id="_idIndexMarker1075"/><span class="koboSpan" id="kobo.456.1">recipes, we will apply all these optimization techniques concurrently for the best cases for the CPU (MKL-DNN + static allocation + invariant shape + Int8 quantization) and GPU (static allocation + invariant shape + automatic mixed precision) to optimize two familiar tasks: image segmentation and </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">text translation.</span></span></p>
<h2 id="_idParaDest-196"><a id="_idTextAnchor197"/><span class="koboSpan" id="kobo.458.1">Thereâ€™s moreâ€¦</span></h2>
<p><span class="koboSpan" id="kobo.459.1">All the optimization features shown in this recipe have been thoroughly described in the literature. </span><span class="koboSpan" id="kobo.459.2">In this section, we share some introductory links to start understanding each of the features </span><span class="No-Break"><span class="koboSpan" id="kobo.460.1">in depth:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.461.1">Hybridization</span></strong><span class="koboSpan" id="kobo.462.1">: </span><a href="https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html"><span class="koboSpan" id="kobo.463.1">https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html</span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.464.1">Automatic Mixed Precision (AMP)</span></strong><span class="koboSpan" id="kobo.465.1">: </span><a href="https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html"><span class="koboSpan" id="kobo.466.1">https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html</span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.467.1">Int8 quantization</span></strong><span class="koboSpan" id="kobo.468.1">: </span><a href="https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html"><span class="koboSpan" id="kobo.469.1">https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html</span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.470.1">MXNet profiler</span></strong><span class="koboSpan" id="kobo.471.1">: </span><a href="https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html"><span class="koboSpan" id="kobo.472.1">https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html</span></a></li>
</ul>
<h1 id="_idParaDest-197"><a id="_idTextAnchor198"/><span class="koboSpan" id="kobo.473.1">Optimizing inference for image segmentation</span></h1>
<p><span class="koboSpan" id="kobo.474.1">In the</span><a id="_idIndexMarker1076"/><span class="koboSpan" id="kobo.475.1"> previous recipe, we saw how we can leverage MXNet and Gluon to optimize the inference of our models, applying </span><a id="_idIndexMarker1077"/><span class="koboSpan" id="kobo.476.1">different techniques, such as improving the runtime performance using hybridization; how using half-precision (float16) in combination with AMP can strongly reduce our inference times; and how to take advantage of further optimizations with data types such as </span><span class="No-Break"><span class="koboSpan" id="kobo.477.1">Int8 quantization.</span></span></p>
<p><span class="koboSpan" id="kobo.478.1">Now, we can revisit a problem we have been working with throughout the book, image segmentation. </span><span class="koboSpan" id="kobo.478.2">We have worked with this task in recipes from previous chapters. </span><span class="koboSpan" id="kobo.478.3">In </span><em class="italic"><span class="koboSpan" id="kobo.479.1">Recipe 4</span></em><span class="koboSpan" id="kobo.480.1">, </span><em class="italic"><span class="koboSpan" id="kobo.481.1">Segmenting objects semantically with MXNet Model Zoo â€“ PSPNet and DeepLabv3</span></em><span class="koboSpan" id="kobo.482.1">, from </span><a href="B16591_05.xhtml#_idTextAnchor098"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.483.1">Chapter 5</span></em></span></a><span class="koboSpan" id="kobo.484.1">, </span><em class="italic"><span class="koboSpan" id="kobo.485.1">Analyzing Images with Computer Vision</span></em><span class="koboSpan" id="kobo.486.1">, we introduced the task and the datasets that we will be using in</span><a id="_idIndexMarker1078"/><span class="koboSpan" id="kobo.487.1"> this recipe, </span><em class="italic"><span class="koboSpan" id="kobo.488.1">MS COCO and Penn-Fudan Pedestrian</span></em><span class="koboSpan" id="kobo.489.1">, and learned </span><a id="_idIndexMarker1079"/><span class="koboSpan" id="kobo.490.1">how to use pre-trained models from GluonCV </span><span class="No-Break"><span class="koboSpan" id="kobo.491.1">Model Zoo.</span></span></p>
<p><span class="koboSpan" id="kobo.492.1">Furthermore, in </span><em class="italic"><span class="koboSpan" id="kobo.493.1">Recipe 3</span></em><span class="koboSpan" id="kobo.494.1">, </span><em class="italic"><span class="koboSpan" id="kobo.495.1">Improving performance for segmenting images</span></em><span class="koboSpan" id="kobo.496.1">, from </span><a href="B16591_07.xhtml#_idTextAnchor148"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.497.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.498.1">, </span><em class="italic"><span class="koboSpan" id="kobo.499.1">Optimizing Models with Transfer Learning and Fine-Tuning</span></em><span class="koboSpan" id="kobo.500.1">, we compared the different approaches that we could take when dealing with a target dataset, training our models from scratch </span><a id="_idIndexMarker1080"/><span class="koboSpan" id="kobo.501.1">or leveraging past knowledge from pre-trained models and adjust them for our task, using the different modalities of transfer learning and fine-tuning. </span><span class="koboSpan" id="kobo.501.2">Lastly, in </span><em class="italic"><span class="koboSpan" id="kobo.502.1">Recipe 2</span></em><span class="koboSpan" id="kobo.503.1">, </span><em class="italic"><span class="koboSpan" id="kobo.504.1">Optimizing training for image segmentation</span></em><span class="koboSpan" id="kobo.505.1">, from </span><a href="B16591_08.xhtml#_idTextAnchor172"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.506.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.507.1">, </span><em class="italic"><span class="koboSpan" id="kobo.508.1">Improving Training Performance with MXNet</span></em><span class="koboSpan" id="kobo.509.1">, we applied different techniques to improve the runtime performance of our </span><span class="No-Break"><span class="koboSpan" id="kobo.510.1">training loops.</span></span></p>
<p><span class="koboSpan" id="kobo.511.1">Therefore, in</span><a id="_idIndexMarker1081"/><span class="koboSpan" id="kobo.512.1"> this recipe, we will apply all the introduced optimization techniques for the specific task of optimizing the inference of an image </span><span class="No-Break"><span class="koboSpan" id="kobo.513.1">segmentation model.</span></span></p>
<h2 id="_idParaDest-198"><a id="_idTextAnchor199"/><span class="koboSpan" id="kobo.514.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.515.1">As in previous chapters, in this recipe, we will be using some matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.516.1">at all.</span></span></p>
<h2 id="_idParaDest-199"><a id="_idTextAnchor200"/><span class="koboSpan" id="kobo.517.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.518.1">In this recipe, we will be using the </span><span class="No-Break"><span class="koboSpan" id="kobo.519.1">following steps:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.520.1">Applying inference </span><span class="No-Break"><span class="koboSpan" id="kobo.521.1">optimization techniques</span></span></li>
<li><span class="koboSpan" id="kobo.522.1">Visualizing and profiling </span><span class="No-Break"><span class="koboSpan" id="kobo.523.1">our models</span></span></li>
<li><span class="koboSpan" id="kobo.524.1">Exporting our models to ONNX </span><span class="No-Break"><span class="koboSpan" id="kobo.525.1">and TensorRT</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.526.1">Letâ€™s dive into each of </span><span class="No-Break"><span class="koboSpan" id="kobo.527.1">these steps.</span></span></p>
<h3><span class="koboSpan" id="kobo.528.1">Applying inference optimization techniques</span></h3>
<p><span class="koboSpan" id="kobo.529.1">In </span><em class="italic"><span class="koboSpan" id="kobo.530.1">Recipe 1</span></em><span class="koboSpan" id="kobo.531.1">, </span><em class="italic"><span class="koboSpan" id="kobo.532.1">Introducing inference optimization features</span></em><span class="koboSpan" id="kobo.533.1">, at the beginning of this chapter, we </span><a id="_idIndexMarker1082"/><span class="koboSpan" id="kobo.534.1">showed how different optimization techniques could improve the performance of the different steps we take in the inference of a machine learning model, including hybridization, AMP, and </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">Int8 quantization.</span></span></p>
<p><span class="koboSpan" id="kobo.536.1">In this section, we will show how, with MXNet and Gluon, just with a few lines of code, we can easily apply each and every technique we've introduced and verify the results of </span><span class="No-Break"><span class="koboSpan" id="kobo.537.1">each technique.</span></span></p>
<p><span class="koboSpan" id="kobo.538.1">Without applying these optimization techniques, as a baseline, these are the quantitative results obtained with </span><span class="No-Break"><span class="koboSpan" id="kobo.539.1">the CPU:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.540.1">
PixAcc:Â Â 0.9602144097222223
mIoUÂ Â :Â Â 0.4742364603465315
Time (s): 27.573920726776123</span></pre> <p><span class="koboSpan" id="kobo.541.1">We can </span><a id="_idIndexMarker1083"/><span class="koboSpan" id="kobo.542.1">display an image for </span><span class="No-Break"><span class="koboSpan" id="kobo.543.1">qualitative results:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer207">
<span class="koboSpan" id="kobo.544.1"><img alt="Figure 9.4 â€“ Qualitative results: CPU baseline" src="image/B16591_09_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.545.1">Figure 9.4 â€“ Qualitative results: CPU baseline</span></p>
<p><span class="koboSpan" id="kobo.546.1">As expected from the quantitative metrics, </span><em class="italic"><span class="koboSpan" id="kobo.547.1">Figure 9.4</span></em><span class="koboSpan" id="kobo.548.1"> shows excellent results </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">as well.</span></span></p>
<p><span class="koboSpan" id="kobo.550.1">As concluded in the previous recipe, for maximum performance on the CPU, the best approach is </span><span class="No-Break"><span class="koboSpan" id="kobo.551.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.552.1">Use hybridization: Using the Intel MKL-DNN backend, combined with static memory allocation and invariant input shapes.</span></li>
<li><span class="koboSpan" id="kobo.553.1">Do not use AMP.</span></li>
<li><span class="koboSpan" id="kobo.554.1">Use Int8 quantization.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.555.1">Letâ€™s apply each of these techniques for our current specific task, </span><span class="No-Break"><span class="koboSpan" id="kobo.556.1">image segmentation.</span></span></p>
<p><span class="koboSpan" id="kobo.557.1">For hybridization, we just need one line of code (which includes the necessary parameters for the Intel </span><strong class="source-inline"><span class="koboSpan" id="kobo.558.1">MKLDNN</span></strong><span class="koboSpan" id="kobo.559.1"> backend, combined with static memory allocation and invariant </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">input shapes):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.561.1">
deeplab_pt_cpu_q_hybrid.hybridize(backend="MKLDNN", static_alloc=True, static_shape=True)</span></pre> <p><span class="koboSpan" id="kobo.562.1">We do not</span><a id="_idIndexMarker1084"/><span class="koboSpan" id="kobo.563.1"> need to add an AMP step as it was shown not to add benefits to </span><span class="No-Break"><span class="koboSpan" id="kobo.564.1">CPU-based workloads.</span></span></p>
<p><span class="koboSpan" id="kobo.565.1">For </span><strong class="source-inline"><span class="koboSpan" id="kobo.566.1">Int8</span></strong><span class="koboSpan" id="kobo.567.1"> quantization, we need two separate steps. </span><span class="koboSpan" id="kobo.567.2">On one hand, we need to define the calibration dataset. </span><span class="koboSpan" id="kobo.567.3">This can be achieved with a small number </span><span class="No-Break"><span class="koboSpan" id="kobo.568.1">of lines:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.569.1">
# Dataset Loading &amp; Transforming
# Limit to 10 samples (last ones)
 max_samples = 10
samples = range(0, max_samples)
 p_cal_cpu_pre = mx.gluon.data.SimpleDataset([(pedestrian_val_dataset[-i][0], pedestrian_val_dataset[-i][1]) for i in tqdm(samples)])
 p_cal_gpu_cpu = p_cal_cpu_pre.transform_first(input_transform_fn_gpu_cpu, lazy=False)
# DataLoader for Calibration
# For CPU, Pre-processed in GPU, copied back to CPU memory space)
 num_workers = 0
batch_size = 4
p_cal_loader_gpu_cpu = mx.gluon.data.DataLoader(
Â Â Â Â p_cal_gpu_cpu,Â Â Â Â batch_size=batch_size,
Â Â Â Â num_workers=num_workers,
Â Â Â Â last_batch="discard")</span></pre> <p><span class="koboSpan" id="kobo.570.1">Then, to apply </span><strong class="source-inline"><span class="koboSpan" id="kobo.571.1">Int8</span></strong><span class="koboSpan" id="kobo.572.1"> quantization, optimized using the calibration dataset, just another line of code </span><span class="No-Break"><span class="koboSpan" id="kobo.573.1">is required:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.574.1">
deeplab_pt_cpu_q_hybrid = quantization.quantize_net_v2(
Â Â Â Â deeplab_pt_cpu,
Â Â Â Â quantized_dtype='auto',
Â Â Â Â exclude_layers=None,
Â Â Â Â exclude_layers_match=None,
Â Â Â Â calib_data=p_cal_loader_gpu_cpu,
Â Â Â Â calib_mode='entropy',
Â Â Â Â logger=logger,
Â Â Â Â ctx=mx.cpu())</span></pre> <p><span class="koboSpan" id="kobo.575.1">Applying these</span><a id="_idIndexMarker1085"/><span class="koboSpan" id="kobo.576.1"> optimization techniques, these are the quantitative results obtained for an optimized </span><span class="No-Break"><span class="koboSpan" id="kobo.577.1">CPU inference:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.578.1">
PixAcc:Â Â 0.9595597222222222
mIoUÂ Â :Â Â 0.47379941937958425
Time (s): 8.355125904083252</span></pre> <p><span class="koboSpan" id="kobo.579.1">As we can see, the differences in performance (</span><strong class="source-inline"><span class="koboSpan" id="kobo.580.1">0.959</span></strong><span class="koboSpan" id="kobo.581.1"> versus </span><strong class="source-inline"><span class="koboSpan" id="kobo.582.1">0.960</span></strong><span class="koboSpan" id="kobo.583.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.584.1">0.473</span></strong><span class="koboSpan" id="kobo.585.1"> versus </span><strong class="source-inline"><span class="koboSpan" id="kobo.586.1">0.474</span></strong><span class="koboSpan" id="kobo.587.1">) are negligible. </span><span class="koboSpan" id="kobo.587.2">However, with these inference optimization techniques, we have been able to reduce the inference runtime by 4x (8.4 seconds versus 27.6 seconds), which is an </span><span class="No-Break"><span class="koboSpan" id="kobo.588.1">impressive result.</span></span></p>
<p><span class="koboSpan" id="kobo.589.1">We can also display an image for </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">qualitative results:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer208">
<span class="koboSpan" id="kobo.591.1"><img alt="Figure 9.5 â€“ Qualitative results: CPU-optimized inference" src="image/B16591_09_5.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.592.1">Figure 9.5 â€“ Qualitative results: CPU-optimized inference</span></p>
<p><span class="koboSpan" id="kobo.593.1">As expected </span><a id="_idIndexMarker1086"/><span class="koboSpan" id="kobo.594.1">from the quantitative metrics, </span><em class="italic"><span class="koboSpan" id="kobo.595.1">Figure 9.5</span></em><span class="koboSpan" id="kobo.596.1"> shows excellent results as well, with negligible differences (</span><span class="No-Break"><span class="koboSpan" id="kobo.597.1">if any).</span></span></p>
<p><span class="koboSpan" id="kobo.598.1">What about GPU-based inference? </span><span class="koboSpan" id="kobo.598.2">Letâ€™s follow the </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">same steps:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.600.1">Without applying these optimization techniques, as a baseline, these are the quantitative results obtained with </span><span class="No-Break"><span class="koboSpan" id="kobo.601.1">the GPU:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.602.1">
PixAcc:Â Â 0.9602144097222223
mIoUÂ Â :Â Â 0.4742364603465315
Time (s): 13.068315982818604</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.603.1">As expected, there is no change relative to algorithmic performance from the CPU baseline. </span><span class="koboSpan" id="kobo.603.2">The runtime inference is indeed twice as fast in the GPU (</span><strong class="source-inline"><span class="koboSpan" id="kobo.604.1">13.1</span></strong><span class="koboSpan" id="kobo.605.1"> seconds versus </span><strong class="source-inline"><span class="koboSpan" id="kobo.606.1">27.6</span></strong><span class="koboSpan" id="kobo.607.1"> seconds).</span></p></li> <li><span class="koboSpan" id="kobo.608.1">We can display an image for </span><span class="No-Break"><span class="koboSpan" id="kobo.609.1">qualitative results:</span></span></li>
</ol>
<div>
<div class="IMG---Figure" id="_idContainer209">
<span class="koboSpan" id="kobo.610.1"><img alt="Figure 9.6 â€“ Qualitative results: GPU baseline" src="image/B16591_09_6.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.611.1">Figure 9.6 â€“ Qualitative results: GPU baseline</span></p>
<p><span class="koboSpan" id="kobo.612.1">As expected from</span><a id="_idIndexMarker1087"/><span class="koboSpan" id="kobo.613.1"> the quantitative metrics, </span><em class="italic"><span class="koboSpan" id="kobo.614.1">Figure 9.6</span></em><span class="koboSpan" id="kobo.615.1"> shows excellent results </span><span class="No-Break"><span class="koboSpan" id="kobo.616.1">as well.</span></span></p>
<p><span class="koboSpan" id="kobo.617.1">As concluded in the previous recipe, for maximum performance on the GPU, the best approach is </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.619.1">Use hybridization</span></strong><span class="koboSpan" id="kobo.620.1">: Using</span><a id="_idIndexMarker1088"/><span class="koboSpan" id="kobo.621.1"> static memory allocation and invariant input shapes. </span><span class="koboSpan" id="kobo.621.2">Do not use the Intel MKL-DNN backend.</span></li>
<li><span class="koboSpan" id="kobo.622.1">Use AMP</span><strong class="bold"><span class="koboSpan" id="kobo.623.1">.</span></strong></li>
<li><span class="koboSpan" id="kobo.624.1">Do not</span><a id="_idIndexMarker1089"/><span class="koboSpan" id="kobo.625.1"> use Int8 quantization (not supported).</span></li>
</ul>
<p><span class="koboSpan" id="kobo.626.1">Letâ€™s apply each of these techniques to our current specific task, </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">image segmentation.</span></span></p>
<p><span class="koboSpan" id="kobo.628.1">For hybridization, we just need one line of code (which includes the necessary parameters for static memory allocation and invariant </span><span class="No-Break"><span class="koboSpan" id="kobo.629.1">input shapes):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.630.1">
deeplab_pt_gpu_hybrid.hybridize(static_alloc=True, static_shape=True)</span></pre> <p><span class="koboSpan" id="kobo.631.1">For AMP, we need to follow two simple steps, a forward pass and the conversion of the model, </span><span class="No-Break"><span class="koboSpan" id="kobo.632.1">as follows:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.633.1">
deeplab_pt_gpu_hybrid(single_sample_gpu);
deeplab_pt_gpu_hybrid_amp = amp.convert_hybrid_block(deeplab_pt_gpu_hybrid, ctx=mx.gpu())</span></pre> <p><span class="koboSpan" id="kobo.634.1">No further steps </span><span class="No-Break"><span class="koboSpan" id="kobo.635.1">are required.</span></span></p>
<p><span class="koboSpan" id="kobo.636.1">By applying these optimization techniques, these are the quantitative results obtained for an optimized </span><span class="No-Break"><span class="koboSpan" id="kobo.637.1">GPU inference:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.638.1">
PixAcc:Â Â 0.9602565972222222
mIoUÂ Â :Â Â 0.4742640561133744
Time (s): 0.8551054000854492</span></pre> <p><span class="koboSpan" id="kobo.639.1">As we can see, the</span><a id="_idIndexMarker1090"/><span class="koboSpan" id="kobo.640.1"> differences in performance (</span><strong class="source-inline"><span class="koboSpan" id="kobo.641.1">0.960</span></strong><span class="koboSpan" id="kobo.642.1"> versus </span><strong class="source-inline"><span class="koboSpan" id="kobo.643.1">0.960</span></strong><span class="koboSpan" id="kobo.644.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.645.1">0.474</span></strong><span class="koboSpan" id="kobo.646.1"> versus </span><strong class="source-inline"><span class="koboSpan" id="kobo.647.1">0.474</span></strong><span class="koboSpan" id="kobo.648.1">) are non-existent. </span><span class="koboSpan" id="kobo.648.2">Furthermore, with these inference optimization techniques, we have been able to reduce the inference runtime by 15x (0.85 seconds versus 13.1 seconds), which is an </span><span class="No-Break"><span class="koboSpan" id="kobo.649.1">impressive result.</span></span></p>
<p><span class="koboSpan" id="kobo.650.1">We can also display an image for </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">qualitative results:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer210">
<span class="koboSpan" id="kobo.652.1"><img alt="Figure 9.7 â€“ Qualitative results: GPU-optimized inference" src="image/B16591_09_7.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.653.1">Figure 9.7 â€“ Qualitative results: GPU-optimized inference</span></p>
<p><span class="koboSpan" id="kobo.654.1">As expected from the quantitative metrics, </span><em class="italic"><span class="koboSpan" id="kobo.655.1">Figure 9.7</span></em><span class="koboSpan" id="kobo.656.1"> shows excellent results as well, with negligible differences (if any) from the results in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.657.1">Figure 9.6</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.658.1">.</span></span></p>
<h3><span class="koboSpan" id="kobo.659.1">Visualizing and profiling our models</span></h3>
<p><span class="koboSpan" id="kobo.660.1">In the</span><a id="_idIndexMarker1091"/><span class="koboSpan" id="kobo.661.1"> previous sections, we saw the different techniques that we could apply to optimize our inference</span><a id="_idIndexMarker1092"/><span class="koboSpan" id="kobo.662.1"> loops, and the results these techniques achieved. </span><span class="koboSpan" id="kobo.662.2">However, how exactly do these techniques work? </span><span class="koboSpan" id="kobo.662.3">Why are </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">they faster?</span></span></p>
<p><span class="koboSpan" id="kobo.664.1">We are going to use two tools that MXNet provides for exactly </span><span class="No-Break"><span class="koboSpan" id="kobo.665.1">this purpose:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.666.1">Model visualization</span></strong></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.667.1">Model profiling</span></strong></li>
</ul>
<p><span class="koboSpan" id="kobo.668.1">Model visualization </span><a id="_idIndexMarker1093"/><span class="koboSpan" id="kobo.669.1">provides us with an intuitive way to see how the different layers interact with each other. </span><span class="koboSpan" id="kobo.669.2">This is particularly interesting for networks that </span><a id="_idIndexMarker1094"/><span class="koboSpan" id="kobo.670.1">use ResNet backbones (such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.671.1">DeepLabv3</span></strong><span class="koboSpan" id="kobo.672.1">, which we use for image segmentation in this recipe) because of the </span><strong class="bold"><span class="koboSpan" id="kobo.673.1">residuals</span></strong><span class="koboSpan" id="kobo.674.1"> being transferred </span><span class="No-Break"><span class="koboSpan" id="kobo.675.1">through layers.</span></span></p>
<p><span class="koboSpan" id="kobo.676.1">Visualizing our model architecture with MXNet is very easy. </span><span class="koboSpan" id="kobo.676.2">When working with symbolic models, just one line of code is necessary. </span><span class="koboSpan" id="kobo.676.3">In our case, as we work with Gluon models, these are the lines of </span><span class="No-Break"><span class="koboSpan" id="kobo.677.1">code necessary:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.678.1">
deeplab_pt_cpu_q_hybrid.export('deeplab_pt_cpu_q_hybrid_sym')
 sym, arg_params, aux_params = mx.model.load_checkpoint('deeplab_pt_cpu_q_hybrid_sym', 0)
 mx.visualization.plot_network(sym)</span></pre> <p><span class="koboSpan" id="kobo.679.1">As shown in the previous recipe, ResNet-based networks are composed of ResNet blocks, which include the convolution, batch normalization, and </span><span class="No-Break"><span class="koboSpan" id="kobo.680.1">activation steps.</span></span></p>
<p><span class="koboSpan" id="kobo.681.1">For our CPU-optimized model (hybridized and </span><strong class="source-inline"><span class="koboSpan" id="kobo.682.1">Int8</span></strong><span class="koboSpan" id="kobo.683.1">-quantized), this is what some of the connections among those blocks </span><span class="No-Break"><span class="koboSpan" id="kobo.684.1">look like:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer211">
<span class="koboSpan" id="kobo.685.1"><img alt="Figure 9.8 â€“ GraphViz of ResNet blocks (CPU-optimized)" src="image/B16591_09_8.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.686.1">Figure 9.8 â€“ GraphViz of ResNet blocks (CPU-optimized)</span></p>
<p><span class="koboSpan" id="kobo.687.1">As we </span><a id="_idIndexMarker1095"/><span class="koboSpan" id="kobo.688.1">can </span><a id="_idIndexMarker1096"/><span class="koboSpan" id="kobo.689.1">see in </span><em class="italic"><span class="koboSpan" id="kobo.690.1">Figure 9.8</span></em><span class="koboSpan" id="kobo.691.1">, there are no individual blocks for each of the expected ResNet block operations; they are all part of single blocks that perform all computations. </span><span class="koboSpan" id="kobo.691.2">This combination of operations is aptly called operator fusion, where as many operations as possible are fused together, instead of computing an operation and then the next one (with the typical data transfers occurring). </span><span class="koboSpan" id="kobo.691.3">The largest benefit is that fused operations can happen in the same memory space. </span><span class="koboSpan" id="kobo.691.4">This is one of the optimizations performed by hybridization, as once the graph for the network is finished, it is quite straightforward to find the operations that are candidates to </span><span class="No-Break"><span class="koboSpan" id="kobo.692.1">be fused.</span></span></p>
<p><span class="koboSpan" id="kobo.693.1">OK, so the </span><a id="_idIndexMarker1097"/><span class="koboSpan" id="kobo.694.1">model visualization tells us those optimizations will happen, but how can we verify they are actually happening? </span><span class="koboSpan" id="kobo.694.2">This is what model profiling is good at, and can also help us understand issues happening </span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">during runtime.</span></span></p>
<p><span class="koboSpan" id="kobo.696.1">As </span><a id="_idIndexMarker1098"/><span class="koboSpan" id="kobo.697.1">mentioned in the recipe, </span><em class="italic"><span class="koboSpan" id="kobo.698.1">Introducing inference optimization features</span></em><span class="koboSpan" id="kobo.699.1">, and the section Profiling our models, the output of model profiling is a JSON file that can be visualized with tools such as the Google Chrome Tracing app. </span><span class="koboSpan" id="kobo.699.2">For a non-optimized CPU workload, our </span><strong class="source-inline"><span class="koboSpan" id="kobo.700.1">DeepLabv3</span></strong><span class="koboSpan" id="kobo.701.1"> model shows the following </span><span class="No-Break"><span class="koboSpan" id="kobo.702.1">timing profile:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer212">
<span class="koboSpan" id="kobo.703.1"><img alt="Figure 9.9 â€“ Profiling DeepLabv3: Non-optimized CPU workload" src="image/B16591_09_9.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.704.1">Figure 9.9 â€“ Profiling DeepLabv3: Non-optimized CPU workload</span></p>
<p><span class="koboSpan" id="kobo.705.1">In </span><em class="italic"><span class="koboSpan" id="kobo.706.1">Figure 9.9</span></em><span class="koboSpan" id="kobo.707.1">, we can see the </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">following characteristics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.709.1">Almost all of the tasks are handled by a single process.</span></li>
<li><span class="koboSpan" id="kobo.710.1">Around 80 ms into the operation, all tasks have been sent to be dispatched, and the control is returned for operations to continue (lazy evaluation and </span><strong class="source-inline"><span class="koboSpan" id="kobo.711.1">mx.nd.waitall</span></strong><span class="koboSpan" id="kobo.712.1">).</span></li>
<li><span class="koboSpan" id="kobo.713.1">All tasks have been sent to be dispatched around 80ms into the operation, and the control is returned for operations to continue (lazy evaluation and mx.nd.waitall).</span></li>
<li><span class="koboSpan" id="kobo.714.1">All operations are atomic and executed individually.</span></li>
<li><span class="koboSpan" id="kobo.715.1">The full operation takes around 800 ms.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.716.1">For a CPU-optimized workload, our DeepLabv3 model shows the following </span><span class="No-Break"><span class="koboSpan" id="kobo.717.1">timing profile:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer213">
<span class="koboSpan" id="kobo.718.1"><img alt="Figure 9.10 â€“ Profiling DeepLabv3: Optimized CPU workload" src="image/B16591_09_10.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.719.1">Figure 9.10 â€“ Profiling DeepLabv3: Optimized CPU workload</span></p>
<p><span class="koboSpan" id="kobo.720.1">In </span><em class="italic"><span class="koboSpan" id="kobo.721.1">Figure 9.10</span></em><span class="koboSpan" id="kobo.722.1">, we can see</span><a id="_idIndexMarker1099"/><span class="koboSpan" id="kobo.723.1"> the </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">following characteristics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.725.1">Almost all of the</span><a id="_idIndexMarker1100"/><span class="koboSpan" id="kobo.726.1"> tasks are handled by a single process, similar to the non-optimized counterpart.</span></li>
<li><span class="koboSpan" id="kobo.727.1">Around 5 ms into the operation, all tasks have been sent to be dispatched, and the control is returned for operations to continue (lazy evaluation and </span><strong class="source-inline"><span class="koboSpan" id="kobo.728.1">mx.nd.waitall</span></strong><span class="koboSpan" id="kobo.729.1">), much faster than the non-optimized counterpart.</span></li>
<li><span class="koboSpan" id="kobo.730.1">Memory is used in a synchronous/structured way, in stark contrast to the non-optimized counterpart.</span></li>
<li><span class="koboSpan" id="kobo.731.1">All operations are fused together, again in stark contrast to the non-optimized counterpart.</span></li>
<li><span class="koboSpan" id="kobo.732.1">The full operation takes around 370 ms.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.733.1">In summary, for CPU-based optimizations, we can clearly see </span><span class="No-Break"><span class="koboSpan" id="kobo.734.1">the effects:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.735.1">Hybridization has fused all operators together, basically executing almost the full workload in a single operation.</span></li>
<li><span class="koboSpan" id="kobo.736.1">The MKL-DNN backend and Int8 quantization have improved those operations with accelerated operators.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.737.1">For our GPU-optimized model (hybridized and AMP-ed), this is what some of the connections among ResNet blocks </span><span class="No-Break"><span class="koboSpan" id="kobo.738.1">look like:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer214">
<span class="koboSpan" id="kobo.739.1"><img alt="Figure 9.11 â€“ GraphViz of ResNet blocks (GPU-optimized)" src="image/B16591_09_11.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.740.1">Figure 9.11 â€“ GraphViz of ResNet blocks (GPU-optimized)</span></p>
<p><span class="koboSpan" id="kobo.741.1">As we</span><a id="_idIndexMarker1101"/><span class="koboSpan" id="kobo.742.1"> can see in </span><em class="italic"><span class="koboSpan" id="kobo.743.1">Figure 9.11</span></em><span class="koboSpan" id="kobo.744.1">, this is a very different visualization than the CPU-optimized one as all the individual blocks </span><a id="_idIndexMarker1102"/><span class="koboSpan" id="kobo.745.1">of the expected ResNet block operations can be identified. </span><span class="koboSpan" id="kobo.745.2">As we saw in the first recipe of this chapter, hybridization had a very limited effect </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">on GPUs.</span></span></p>
<p><span class="koboSpan" id="kobo.747.1">So, where do the accelerations come from? </span><span class="koboSpan" id="kobo.747.2">Letâ€™s get some help from </span><span class="No-Break"><span class="koboSpan" id="kobo.748.1">model profiling.</span></span></p>
<p><span class="koboSpan" id="kobo.749.1">For a non-optimized GPU workload, our DeepLabv3 model shows the following </span><span class="No-Break"><span class="koboSpan" id="kobo.750.1">timing profile:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer215">
<span class="koboSpan" id="kobo.751.1"><img alt="Figure 9.12 â€“ Profiling DeepLabv3: Non-optimized GPU workload" src="image/B16591_09_12.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.752.1">Figure 9.12 â€“ Profiling DeepLabv3: Non-optimized GPU workload</span></p>
<p><span class="koboSpan" id="kobo.753.1">In </span><em class="italic"><span class="koboSpan" id="kobo.754.1">Figure 9.12</span></em><span class="koboSpan" id="kobo.755.1">, we can</span><a id="_idIndexMarker1103"/><span class="koboSpan" id="kobo.756.1"> see the </span><span class="No-Break"><span class="koboSpan" id="kobo.757.1">following characteristics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.758.1">Almost all of the</span><a id="_idIndexMarker1104"/><span class="koboSpan" id="kobo.759.1"> tasks are handled by two GPU processes.</span></li>
<li><span class="koboSpan" id="kobo.760.1">Around 40 ms into the operation, all tasks have been sent to be dispatched, and the control is returned for operations to continue (lazy evaluation and </span><strong class="source-inline"><span class="koboSpan" id="kobo.761.1">mx.nd.waitall</span></strong><span class="koboSpan" id="kobo.762.1">).</span></li>
<li><span class="koboSpan" id="kobo.763.1">Asynchronous/unstructured usage of memory.</span></li>
<li><span class="koboSpan" id="kobo.764.1">All operations are atomic and executed individually.</span></li>
<li><span class="koboSpan" id="kobo.765.1">The full operation takes around 150 ms.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.766.1">For a GPU-optimized workload, our DeepLabv3 model shows the following </span><span class="No-Break"><span class="koboSpan" id="kobo.767.1">timing profile:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer216">
<span class="koboSpan" id="kobo.768.1"><img alt="Figure 9.13 â€“ Profiling DeepLabv3: Optimized GPU workload" src="image/B16591_09_13.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.769.1">Figure 9.13 â€“ Profiling DeepLabv3: Optimized GPU workload</span></p>
<p><span class="koboSpan" id="kobo.770.1">In </span><em class="italic"><span class="koboSpan" id="kobo.771.1">Figure 9.13</span></em><span class="koboSpan" id="kobo.772.1">, we can see the </span><span class="No-Break"><span class="koboSpan" id="kobo.773.1">following characteristics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.774.1">Almost all of </span><a id="_idIndexMarker1105"/><span class="koboSpan" id="kobo.775.1">the tasks are handled by two processes, similar to the non-optimized counterpart.</span></li>
<li><span class="koboSpan" id="kobo.776.1">Around 4 ms</span><a id="_idIndexMarker1106"/><span class="koboSpan" id="kobo.777.1"> into the operation, all tasks have been sent to be dispatched, and the control is returned for operations to continue (lazy evaluation and </span><strong class="source-inline"><span class="koboSpan" id="kobo.778.1">mx.nd.waitall</span></strong><span class="koboSpan" id="kobo.779.1">), much faster than the non-optimized counter-part.</span></li>
<li><span class="koboSpan" id="kobo.780.1">Synchronous/structured usage of memory, in stark contrast to the non-optimized counterpart.</span></li>
<li><span class="koboSpan" id="kobo.781.1">All operations are atomic and executed individually; similar to the non-optimized counterpart, they are just much faster. </span><span class="koboSpan" id="kobo.781.2">For example, large convolution operations take ~1 ms in the GPU non-optimized case, whereas they take one-third of that time (~0.34 ms) in the GPU-optimized case.</span></li>
<li><span class="koboSpan" id="kobo.782.1">The full operation takes around 55 ms (one-third of the non-optimized time).</span></li>
</ul>
<p><span class="koboSpan" id="kobo.783.1">In summary, for GPU-based optimizations, we can clearly see </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">the effects:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.785.1">Hybridization, as expected, has no effect and no operator fusion can be identified.</span></li>
<li><span class="koboSpan" id="kobo.786.1">AMP makes operations run much faster if the GPU has float16-dedicated circuitry.</span></li>
</ul>
<h3><span class="koboSpan" id="kobo.787.1">Exporting our models to ONNX and TensorRT</span></h3>
<p><span class="koboSpan" id="kobo.788.1">MXNet and GluonCV</span><a id="_idIndexMarker1107"/><span class="koboSpan" id="kobo.789.1"> also provide tools to export our models externally. </span><span class="koboSpan" id="kobo.789.2">This makes the most sense for optimizing runtime computation times (inference) </span><span class="No-Break"><span class="koboSpan" id="kobo.790.1">might need:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.791.1">Specific algorithms that MXNet/GluonCV might not support</span></li>
<li><span class="koboSpan" id="kobo.792.1">Deployment and optimizations on specific hardware platforms</span></li>
</ul>
<p><span class="koboSpan" id="kobo.793.1">In this section, we are going to study one example of </span><span class="No-Break"><span class="koboSpan" id="kobo.794.1">each category.</span></span></p>
<p><span class="koboSpan" id="kobo.795.1">For specific algorithms, we are going to export our models in ONNX format. </span><strong class="bold"><span class="koboSpan" id="kobo.796.1">ONNX</span></strong><span class="koboSpan" id="kobo.797.1"> stands for </span><strong class="bold"><span class="koboSpan" id="kobo.798.1">Open Neural Network eXchange</span></strong><span class="koboSpan" id="kobo.799.1"> and is an open format that describes how deep</span><a id="_idIndexMarker1108"/><span class="koboSpan" id="kobo.800.1"> learning models can be stored and shared. </span><span class="koboSpan" id="kobo.800.2">This is extremely useful to leverage specific tools for highly specialized tasks. </span><span class="koboSpan" id="kobo.800.3">For example, </span><strong class="bold"><span class="koboSpan" id="kobo.801.1">ONNX Runtime</span></strong><span class="koboSpan" id="kobo.802.1"> has</span><a id="_idIndexMarker1109"/><span class="koboSpan" id="kobo.803.1"> really powerful inference tools, including quantization (for example, ONNX Runtime has support for GPU-based INT8 quantization). </span><span class="koboSpan" id="kobo.803.2">Therefore, we can export our model in ONNX format and start working directly with </span><span class="No-Break"><span class="koboSpan" id="kobo.804.1">ONNX Runtime.</span></span></p>
<p><span class="koboSpan" id="kobo.805.1">As usual, MXNet will allow us to accomplish this with just a few lines of code. </span><span class="koboSpan" id="kobo.805.2">We will need to carry out two steps. </span><span class="koboSpan" id="kobo.805.3">Firstly, we need to transform our model from Gluon to symbolic format (hybridizing and </span><span class="No-Break"><span class="koboSpan" id="kobo.806.1">then exporting):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.807.1">
deeplab_pt_gpu_hybrid.hybridize(static_alloc=True, static_shape=True)
 deeplab_pt_gpu_hybrid(single_sample_gpu)
 # Need to be exported externally for the symbols to be loaded
deeplab_pt_gpu_hybrid_filename = "deeplab_resnet101_coco_pt_gpu_hybrid"
deeplab_pt_gpu_hybrid.export(deeplab_pt_gpu_hybrid_filename)</span></pre> <p><span class="koboSpan" id="kobo.808.1">Next, we can transform the symbolic model </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">into ONNX:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.810.1">
# Files exported
sym_filename = deeplab_pt_gpu_hybrid_filename + "-symbol.json"
params_filename = deeplab_pt_gpu_hybrid_filename + "-0000.params"
in_shapes = [single_sample_gpu.shape]
 in_types = [mx.np.float32]
 onnx_model_path = mx.onnx.export_model(
Â Â Â Â sym_filename,
Â Â Â Â params_filename,
Â Â Â Â in_shapes,
Â Â Â Â in_types,
Â Â Â Â onnx_file_name)</span></pre> <p><span class="koboSpan" id="kobo.811.1">ONNX also </span><a id="_idIndexMarker1110"/><span class="koboSpan" id="kobo.812.1">provides a checker to verify our model has been exported correctly. </span><span class="koboSpan" id="kobo.812.2">This can be done with the following lines </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.814.1">
# Model Verification
import onnx
# Load the ONNX model
onnx_model = onnx.load_model(onnx_model_path)
 # Check the ONNX graph
onnx.checker.check_graph(onnx_model.graph)</span></pre> <p><span class="koboSpan" id="kobo.815.1">And thatâ€™s it! </span><span class="koboSpan" id="kobo.815.2">Following these instructions, we will have our ONNX model stored in a file (in our example, </span><strong class="source-inline"><span class="koboSpan" id="kobo.816.1">'deeplab_resnet101_coco_pt_gpu_hybrid.onnx'</span></strong><span class="koboSpan" id="kobo.817.1">), ready to be used with any tool that accepts ONNX models </span><span class="No-Break"><span class="koboSpan" id="kobo.818.1">as input.</span></span></p>
<p><span class="koboSpan" id="kobo.819.1">On the other hand, sometimes we would like to deploy and/or optimize our models on specific hardware platforms, such as the NVIDIA family of products (for example, Nvidia Jetson platforms). </span><span class="koboSpan" id="kobo.819.2">Specifically, Nvidia works with a specific machine learning framework designed to run inference on their own hardware. </span><span class="koboSpan" id="kobo.819.3">This framework</span><a id="_idIndexMarker1111"/><span class="koboSpan" id="kobo.820.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.821.1">called </span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.822.1">TensorRT</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.823.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.824.1">Although</span><a id="_idIndexMarker1112"/><span class="koboSpan" id="kobo.825.1"> MXNet features direct TensorRT integration, itâ€™s not enabled by default, requiring building MXNet directly from the source, with specific parameters enabling TensorRT integration. </span><span class="koboSpan" id="kobo.825.2">Much more straightforwardly, we can leverage our recently described ONNX export to generate a </span><span class="No-Break"><span class="koboSpan" id="kobo.826.1">TensorRT-capable model.</span></span></p>
<p><span class="koboSpan" id="kobo.827.1">To achieve this, it is enough to write a few lines </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.829.1">
import tensorrt as trt
trt_file_name = "deeplab_resnet101_coco_pt_gpu_hybrid.trt"
TRT_LOGGER = trt.Logger(trt.Logger.INFO)
 builder = trt.Builder(TRT_LOGGER)
 config = builder.create_builder_config()
explicit_batch = 1 &lt;&lt; (int) (trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH)
 deeplab_pt_gpu_hybrid_trt = builder.create_network(explicit_batch)
with open(onnx_file_name, 'rb') as model:
Â Â Â Â with trt.OnnxParser(deeplab_pt_gpu_hybrid_trt, TRT_LOGGER) as parser:
Â Â Â Â Â Â Â Â assert parser.parse(model.read()) == True
Â Â Â Â deeplab_pt_gpu_hybrid_engine_serialized = builder.build_serialized_network(deeplab_pt_gpu_hybrid_trt, config=config)
with open(trt_file_name, 'wb') as f:
Â Â Â Â f.write(bytearray(deeplab_pt_gpu_hybrid_engine_serialized))</span></pre> <p><span class="koboSpan" id="kobo.830.1">With this, we will write a serialized </span><span class="No-Break"><span class="koboSpan" id="kobo.831.1">TensorRT-capable model.</span></span></p>
<p><span class="koboSpan" id="kobo.832.1">We can verify the model</span><a id="_idIndexMarker1113"/><span class="koboSpan" id="kobo.833.1"> can be read by deserializing and reading it. </span><span class="koboSpan" id="kobo.833.2">We can do so with the following lines </span><span class="No-Break"><span class="koboSpan" id="kobo.834.1">of code:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.835.1">
# Check it can be read back
runtime = trt.Runtime(TRT_LOGGER)
 with open(trt_file_name, 'rb') as f:
Â Â Â Â deeplab_pt_gpu_hybrid_engine_deserialized = runtime.deserialize_cuda_engine(f.read())</span></pre> <p><span class="koboSpan" id="kobo.836.1">And we are</span><a id="_idIndexMarker1114"/><span class="koboSpan" id="kobo.837.1"> done! </span><span class="koboSpan" id="kobo.837.2">In this section, we have been able to successfully write ONNX and TensorRT </span><span class="No-Break"><span class="koboSpan" id="kobo.838.1">models. </span><span class="koboSpan" id="kobo.838.2">Congratulations!</span></span></p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor201"/><span class="koboSpan" id="kobo.839.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.840.1">In this </span><a id="_idIndexMarker1115"/><span class="koboSpan" id="kobo.841.1">recipe, we have applied the different inference optimization techniques seen in the first recipe of this chapter, leveraging our hardware (CPUs and GPUs) to optimize our model runtime performance by doing </span><span class="No-Break"><span class="koboSpan" id="kobo.842.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.843.1">Hybridizing the model</span></li>
<li><span class="koboSpan" id="kobo.844.1">Leveraging AMP</span></li>
<li><span class="koboSpan" id="kobo.845.1">Quantizing with the INT8 data type for accelerated inference</span></li>
</ul>
<p><span class="koboSpan" id="kobo.846.1">Moreover, we have learned how to use model visualizations (powered by GraphViz) and the MXNet profiler and have used these tools to analyze the inference optimizations from a </span><span class="No-Break"><span class="koboSpan" id="kobo.847.1">low-level perspective.</span></span></p>
<p><span class="koboSpan" id="kobo.848.1">Finally, we have learned how to export our models for specific scenarios and purposes, using the ONNX and </span><span class="No-Break"><span class="koboSpan" id="kobo.849.1">TensorRT libraries.</span></span></p>
<h2 id="_idParaDest-201"><a id="_idTextAnchor202"/><span class="koboSpan" id="kobo.850.1">Thereâ€™s moreâ€¦</span></h2>
<p><span class="koboSpan" id="kobo.851.1">In this recipe, we have presented the inference optimization problem from a post-training perspective. </span><span class="koboSpan" id="kobo.851.2">We were given a (pre-)trained model and tried to squeeze as much performance as we could </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">from it.</span></span></p>
<p><span class="koboSpan" id="kobo.853.1">However, there is another avenue that can be explored, which starts thinking about maximizing inference performance from a machine learning model design perspective. </span><span class="koboSpan" id="kobo.853.2">This is known</span><a id="_idIndexMarker1116"/><span class="koboSpan" id="kobo.854.1"> as </span><strong class="bold"><span class="koboSpan" id="kobo.855.1">model compression</span></strong><span class="koboSpan" id="kobo.856.1"> and is an active area of research, with lots of improvements published periodically. </span><span class="koboSpan" id="kobo.856.2">Recently active research topics include </span><span class="No-Break"><span class="koboSpan" id="kobo.857.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.858.1">Knowledge distillation</span></strong><span class="koboSpan" id="kobo.859.1">: </span><a href="https://arxiv.org/pdf/1503.02531.pdf"><span class="koboSpan" id="kobo.860.1">https://arxiv.org/pdf/1503.02531.pdf</span></a><a href="https://arxiv.org/pdf/1503.02531.pdf "/></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.861.1">Pruning</span></strong><span class="koboSpan" id="kobo.862.1">: </span><a href="https://arxiv.org/pdf/1510.00149.pdf"><span class="koboSpan" id="kobo.863.1">https://arxiv.org/pdf/1510.00149.pdf</span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.864.1">Quantization-aware training</span></strong><span class="koboSpan" id="kobo.865.1">: </span><a href="https://arxiv.org/pdf/1712.05877.pdf"><span class="koboSpan" id="kobo.866.1">https://arxiv.org/pdf/1712.05877.pdf</span></a></li>
</ul>
<h1 id="_idParaDest-202"><a id="_idTextAnchor203"/><span class="koboSpan" id="kobo.867.1">Optimizing inference when translating text from English to German</span></h1>
<p><span class="koboSpan" id="kobo.868.1">In the</span><a id="_idIndexMarker1117"/><span class="koboSpan" id="kobo.869.1"> initial recipe, we saw how we can leverage MXNet and Gluon to optimize the inference of our models, applying different techniques: improving the runtime performance using hybridization; how using half-precision (float16) in combination with AMP can strongly reduce our inference times; and how to take advantage of further optimizations with data types such as </span><span class="No-Break"><span class="koboSpan" id="kobo.870.1">Int8 quantization.</span></span></p>
<p><span class="koboSpan" id="kobo.871.1">Now, we can revisit a problem we have been working with throughout the book: translating English to German. </span><span class="koboSpan" id="kobo.871.2">We have worked with translation tasks in recipes from previous chapters. </span><span class="koboSpan" id="kobo.871.3">In </span><em class="italic"><span class="koboSpan" id="kobo.872.1">Recipe 4</span></em><span class="koboSpan" id="kobo.873.1">, </span><em class="italic"><span class="koboSpan" id="kobo.874.1">Translating text from Vietnamese to English</span></em><span class="koboSpan" id="kobo.875.1">, from </span><a href="B16591_06.xhtml#_idTextAnchor121"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.876.1">Chapter 6</span></em></span></a><span class="koboSpan" id="kobo.877.1">, </span><em class="italic"><span class="koboSpan" id="kobo.878.1">Understanding Text with Natural Language Processing</span></em><span class="koboSpan" id="kobo.879.1">, we introduced the task of translating text, while also learning how to use pre-trained models from GluonCV </span><span class="No-Break"><span class="koboSpan" id="kobo.880.1">Model Zoo.</span></span></p>
<p><span class="koboSpan" id="kobo.881.1">Furthermore, in </span><em class="italic"><span class="koboSpan" id="kobo.882.1">Recipe 4</span></em><span class="koboSpan" id="kobo.883.1">, </span><em class="italic"><span class="koboSpan" id="kobo.884.1">Improving performance for translating English to German</span></em><span class="koboSpan" id="kobo.885.1">, from </span><a href="B16591_07.xhtml#_idTextAnchor148"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.886.1">Chapter 7</span></em></span></a><span class="koboSpan" id="kobo.887.1">, </span><em class="italic"><span class="koboSpan" id="kobo.888.1">Optimizing Models with Transfer Learning and Fine-Tuning</span></em><span class="koboSpan" id="kobo.889.1">, we introduced the datasets that we will be using in this</span><a id="_idIndexMarker1118"/><span class="koboSpan" id="kobo.890.1"> recipe: </span><strong class="bold"><span class="koboSpan" id="kobo.891.1">WMT 2014</span></strong><span class="koboSpan" id="kobo.892.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.893.1">WMT 2016</span></strong><span class="koboSpan" id="kobo.894.1">. </span><span class="koboSpan" id="kobo.894.2">We</span><a id="_idIndexMarker1119"/><span class="koboSpan" id="kobo.895.1"> also compared the different approaches that we could take when dealing with a target dataset: training our models from scratch or leveraging past knowledge from pre-trained models and adjusting them for our task, using the different modalities of transfer learning and fine-tuning. </span><span class="koboSpan" id="kobo.895.2">Lastly, in </span><em class="italic"><span class="koboSpan" id="kobo.896.1">Recipe 3</span></em><span class="koboSpan" id="kobo.897.1">, </span><em class="italic"><span class="koboSpan" id="kobo.898.1">Optimizing training for translating English to German</span></em><span class="koboSpan" id="kobo.899.1">, from </span><a href="B16591_08.xhtml#_idTextAnchor172"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.900.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.901.1">, </span><em class="italic"><span class="koboSpan" id="kobo.902.1">Improving Training Performance with MXNet</span></em><span class="koboSpan" id="kobo.903.1">, we applied different techniques to improve the runtime performance of our </span><span class="No-Break"><span class="koboSpan" id="kobo.904.1">training loops.</span></span></p>
<p><span class="koboSpan" id="kobo.905.1">Therefore, in </span><a id="_idIndexMarker1120"/><span class="koboSpan" id="kobo.906.1">this recipe, we will apply all the introduced optimization techniques for the specific task of optimizing the inference for translating English </span><span class="No-Break"><span class="koboSpan" id="kobo.907.1">to German.</span></span></p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor204"/><span class="koboSpan" id="kobo.908.1">Getting ready</span></h2>
<p><span class="koboSpan" id="kobo.909.1">As in previous chapters, in this recipe, we will be using some matrix operations and linear algebra, but it will not be hard </span><span class="No-Break"><span class="koboSpan" id="kobo.910.1">at all.</span></span></p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor205"/><span class="koboSpan" id="kobo.911.1">How to do it...</span></h2>
<p><span class="koboSpan" id="kobo.912.1">In this recipe, we will be carrying out the </span><span class="No-Break"><span class="koboSpan" id="kobo.913.1">following steps:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.914.1">Applying inference </span><span class="No-Break"><span class="koboSpan" id="kobo.915.1">optimization techniques</span></span></li>
<li><span class="koboSpan" id="kobo.916.1">Profiling </span><span class="No-Break"><span class="koboSpan" id="kobo.917.1">our models</span></span></li>
<li><span class="koboSpan" id="kobo.918.1">Exporting </span><span class="No-Break"><span class="koboSpan" id="kobo.919.1">our models</span></span></li>
</ol>
<p><span class="koboSpan" id="kobo.920.1">Letâ€™s dive into each of </span><span class="No-Break"><span class="koboSpan" id="kobo.921.1">these steps.</span></span></p>
<h3><span class="koboSpan" id="kobo.922.1">Applying inference optimization techniques</span></h3>
<p><span class="koboSpan" id="kobo.923.1">In </span><em class="italic"><span class="koboSpan" id="kobo.924.1">Recipe 1</span></em><span class="koboSpan" id="kobo.925.1">, </span><em class="italic"><span class="koboSpan" id="kobo.926.1">Introducing inference optimization features</span></em><span class="koboSpan" id="kobo.927.1">, at the beginning of this chapter, we </span><a id="_idIndexMarker1121"/><span class="koboSpan" id="kobo.928.1">showed how different optimization techniques could improve the performance of the different steps we take in the inference of a machine learning model, including hybridization, AMP, and </span><span class="No-Break"><span class="koboSpan" id="kobo.929.1">Int8 quantization.</span></span></p>
<p><span class="koboSpan" id="kobo.930.1">In this section, we will show how, with MXNet and Gluon, just with a few lines of code, we can easily apply each and every technique introduced and verify the results of </span><span class="No-Break"><span class="koboSpan" id="kobo.931.1">each technique.</span></span></p>
<p><span class="koboSpan" id="kobo.932.1">Without applying these optimization techniques, as a baseline, these are the quantitative results obtained with </span><span class="No-Break"><span class="koboSpan" id="kobo.933.1">the CPU:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.934.1">
WMT16 test loss: 1.53; test bleu score: 26.40
Time (s): 373.5446252822876</span></pre> <p><span class="koboSpan" id="kobo.935.1">From a </span><a id="_idIndexMarker1122"/><span class="koboSpan" id="kobo.936.1">qualitative point of view, we can also check how well our model is performing with a sentence example. </span><span class="koboSpan" id="kobo.936.2">In our case, we chose </span><em class="italic"><span class="koboSpan" id="kobo.937.1">I learn new things every day</span></em><span class="koboSpan" id="kobo.938.1">, and the output obtained is </span><span class="No-Break"><span class="koboSpan" id="kobo.939.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.940.1">
Qualitative Evaluation: Translating from English to German
Expected translation:
 Ich lerne neue Dinge.
 </span><span class="koboSpan" id="kobo.940.2">In English:
 I learn new things every day.
 </span><span class="koboSpan" id="kobo.940.3">The German translation is:
 Ich lerne neue Dinge, die in jedem Fall auftreten.</span></pre> <p><span class="koboSpan" id="kobo.941.1">The German sentence obtained in the output (</span><em class="italic"><span class="koboSpan" id="kobo.942.1">Ich lerne neue Dinge, die in jedem Fall auftreten</span></em><span class="koboSpan" id="kobo.943.1">) means </span><em class="italic"><span class="koboSpan" id="kobo.944.1">I learn new things that arise in every case</span></em><span class="koboSpan" id="kobo.945.1">, and therefore, as can be seen from the results, the text has been almost perfectly translated from English </span><span class="No-Break"><span class="koboSpan" id="kobo.946.1">to German.</span></span></p>
<p><span class="koboSpan" id="kobo.947.1">As concluded in the previous recipe, for maximum performance on the CPU, the best approach is </span><span class="No-Break"><span class="koboSpan" id="kobo.948.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.949.1">Use hybridization: Using the Intel MKL-DNN backend, combined with static memory allocation and invariant input shapes.</span></li>
<li><span class="koboSpan" id="kobo.950.1">Do not use AMP.</span></li>
<li><span class="koboSpan" id="kobo.951.1">Use Int8 quantization.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.952.1">Unfortunately, we wonâ€™t be able to use Int8 quantization, as this is not supported for </span><span class="No-Break"><span class="koboSpan" id="kobo.953.1">GluonNLP models.</span></span></p>
<p><span class="koboSpan" id="kobo.954.1">Letâ€™s apply each of these techniques for our current specific task, translating from English </span><span class="No-Break"><span class="koboSpan" id="kobo.955.1">to German.</span></span></p>
<p><span class="koboSpan" id="kobo.956.1">For </span><a id="_idIndexMarker1123"/><span class="koboSpan" id="kobo.957.1">hybridization, we just need a couple of lines of code (which include the necessary parameters for the Intel MKL-DNN backend, combined with static memory allocation and invariant input shapes, and the hybridization of the loss function </span><span class="No-Break"><span class="koboSpan" id="kobo.958.1">as well):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.959.1">
wmt_transformer_pt_cpu_hybrid.hybridize(backend="MKLDNN", static_alloc=True, static_shape=True)
loss_function = nlp.loss.MaskedSoftmaxCELoss()
loss_function.hybridize(backend="MKLDNN", static_alloc=True, static_shape=True)</span></pre> <p><span class="koboSpan" id="kobo.960.1">We do not need to add any steps connected to AMP as it was shown not to add benefits to CPU-based workloads. </span><span class="koboSpan" id="kobo.960.2">Similarly, GluonNLP does not support Int8 quantization, and therefore, we donâ€™t need to make any further changes to </span><span class="No-Break"><span class="koboSpan" id="kobo.961.1">our code.</span></span></p>
<p><span class="koboSpan" id="kobo.962.1">Applying these optimization techniques, these are the quantitative results obtained for an optimized </span><span class="No-Break"><span class="koboSpan" id="kobo.963.1">CPU inference:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.964.1">
WMT16 test loss: 1.53; test bleu score: 26.40
Time (s): 312.5660226345062</span></pre> <p><span class="koboSpan" id="kobo.965.1">As we can see, the differences in performance (1.53 versus 1.53 for the loss and 26.40 versus 26.40 for the BLEU score) are negligible. </span><span class="koboSpan" id="kobo.965.2">However, with these inference optimization techniques, we have been able to reduce the inference runtime by 20% (313 seconds versus 374 seconds), which is a very </span><span class="No-Break"><span class="koboSpan" id="kobo.966.1">good result.</span></span></p>
<p><span class="koboSpan" id="kobo.967.1">From a qualitative point of view, we can also check how well our model is performing with a sentence example. </span><span class="koboSpan" id="kobo.967.2">In our case, we chose </span><em class="italic"><span class="koboSpan" id="kobo.968.1">I learn new things every day</span></em><span class="koboSpan" id="kobo.969.1">, and the output obtained is </span><span class="No-Break"><span class="koboSpan" id="kobo.970.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.971.1">
Qualitative Evaluation: Translating from English to German
Expected translation:
 Ich lerne neue Dinge.
 </span><span class="koboSpan" id="kobo.971.2">In English:
 I learn new things every day.
 </span><span class="koboSpan" id="kobo.971.3">The German translation is:
 Ich lerne neue Dinge, die in jedem Fall auftreten.</span></pre> <p><span class="koboSpan" id="kobo.972.1">The </span><a id="_idIndexMarker1124"/><span class="koboSpan" id="kobo.973.1">German sentence obtained in the output (</span><em class="italic"><span class="koboSpan" id="kobo.974.1">Ich lerne neue Dinge, die in jedem Fall auftreten</span></em><span class="koboSpan" id="kobo.975.1">) means </span><em class="italic"><span class="koboSpan" id="kobo.976.1">I learn new things that arise in every case</span></em><span class="koboSpan" id="kobo.977.1">, and therefore, as can be seen from the results, the text has been almost perfectly translated from English to German. </span><span class="koboSpan" id="kobo.977.2">Moreover, the results are equivalent to the non-optimized case (</span><span class="No-Break"><span class="koboSpan" id="kobo.978.1">as expected).</span></span></p>
<p><span class="koboSpan" id="kobo.979.1">What about GPU-based inference? </span><span class="koboSpan" id="kobo.979.2">Letâ€™s follow the </span><span class="No-Break"><span class="koboSpan" id="kobo.980.1">same steps:</span></span></p>
<ol>
<li value="1"><span class="koboSpan" id="kobo.981.1">Without applying these optimization techniques, as a baseline, these are the quantitative results obtained with </span><span class="No-Break"><span class="koboSpan" id="kobo.982.1">the GPU:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.983.1">
WMT16 test loss: 1.53; test bleu score: 26.40
Time (s): 61.67868137359619</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.984.1">As expected, there is no change relative to algorithmic performance from the CPU baseline. </span><span class="koboSpan" id="kobo.984.2">Runtime inference is indeed six times as fast in the GPU (61.7 seconds versus 374 seconds).</span></p></li> <li><span class="koboSpan" id="kobo.985.1">From a qualitative point of view, we can also check how well our model is performing with a sentence example. </span><span class="koboSpan" id="kobo.985.2">In our case, we chose </span><em class="italic"><span class="koboSpan" id="kobo.986.1">I learn new things every day</span></em><span class="koboSpan" id="kobo.987.1">, and the output obtained is </span><span class="No-Break"><span class="koboSpan" id="kobo.988.1">the following:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.989.1">
Qualitative Evaluation: Translating from English to German
Expected translation:
 Ich lerne neue Dinge.
 </span><span class="koboSpan" id="kobo.989.2">In English:
 I learn new things every day.
 </span><span class="koboSpan" id="kobo.989.3">The German translation is:
 Ich lerne neue Dinge, die in jedem Fall auftreten.</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.990.1">The German sentence obtained in the output (</span><em class="italic"><span class="koboSpan" id="kobo.991.1">Ich lerne neue Dinge, die in jedem Fall auftreten</span></em><span class="koboSpan" id="kobo.992.1">) means </span><em class="italic"><span class="koboSpan" id="kobo.993.1">I learn new things that arise in every case</span></em><span class="koboSpan" id="kobo.994.1">, and therefore, as can be seen from the results, the text has been almost perfectly translated from English to German (and is equivalent to both CPU cases).</span></p></li> </ol>
<p><span class="koboSpan" id="kobo.995.1">As </span><a id="_idIndexMarker1125"/><span class="koboSpan" id="kobo.996.1">concluded in the previous recipe, for maximum performance on the GPU, the best approach is </span><span class="No-Break"><span class="koboSpan" id="kobo.997.1">the following:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.998.1">Use hybridization: Using static memory allocation and invariant input shapes. </span><span class="koboSpan" id="kobo.998.2">Do not use the Intel MKL-DNN backend.</span></li>
<li><span class="koboSpan" id="kobo.999.1">Use AMP.</span></li>
<li><span class="koboSpan" id="kobo.1000.1">Do not use Int8 quantization (not supported).</span></li>
</ul>
<p><span class="koboSpan" id="kobo.1001.1">Unfortunately, we wonâ€™t be able to use AMP, as this is not supported for </span><span class="No-Break"><span class="koboSpan" id="kobo.1002.1">GluonNLP models.</span></span></p>
<p><span class="koboSpan" id="kobo.1003.1">Letâ€™s apply each of these techniques for our current specific task, translating from English </span><span class="No-Break"><span class="koboSpan" id="kobo.1004.1">to German.</span></span></p>
<p><span class="koboSpan" id="kobo.1005.1">For hybridization, we just need a couple of lines of code (which include the necessary parameters for static memory allocation and invariant input shapes, and the hybridization of the loss function </span><span class="No-Break"><span class="koboSpan" id="kobo.1006.1">as well):</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1007.1">
wmt_transformer_pt_gpu_hybrid.hybridize(static_alloc=True, static_shape=True)
loss_function = nlp.loss.MaskedSoftmaxCELoss()
loss_function.hybridize(static_alloc=True, static_shape=True)</span></pre> <p><span class="koboSpan" id="kobo.1008.1">We do not need to add any steps connected to AMP or Int8 quantization, as GluonNLP does not support these features. </span><span class="koboSpan" id="kobo.1008.2">Therefore, no further steps </span><span class="No-Break"><span class="koboSpan" id="kobo.1009.1">are required.</span></span></p>
<p><span class="koboSpan" id="kobo.1010.1">By applying these optimization techniques, these are the quantitative results obtained for an optimized </span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">GPU inference:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1012.1">
WMT16 test loss: 1.53; test bleu score: 26.40
Time (s): 56.29795598983765</span></pre> <p><span class="koboSpan" id="kobo.1013.1">As we can see, the differences in performance (1.53 versus 1.53 for the loss and 26.40 versus 26.40 for the BLEU score) are negligible. </span><span class="koboSpan" id="kobo.1013.2">However, with these inference optimization techniques, we have been able to reduce the inference runtime by 10% (56.3 seconds versus 61.7 seconds), which is a very </span><span class="No-Break"><span class="koboSpan" id="kobo.1014.1">good result.</span></span></p>
<p><span class="koboSpan" id="kobo.1015.1">From a </span><a id="_idIndexMarker1126"/><span class="koboSpan" id="kobo.1016.1">qualitative point of view, we can also check how well our model is performing with a sentence example. </span><span class="koboSpan" id="kobo.1016.2">In our case, we chose </span><em class="italic"><span class="koboSpan" id="kobo.1017.1">I learn new things every day</span></em><span class="koboSpan" id="kobo.1018.1">, and the output obtained is </span><span class="No-Break"><span class="koboSpan" id="kobo.1019.1">the following:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1020.1">
Qualitative Evaluation: Translating from English to German
Expected translation:
 Ich lerne neue Dinge.
 </span><span class="koboSpan" id="kobo.1020.2">In English:
 I learn new things every day.
 </span><span class="koboSpan" id="kobo.1020.3">The German translation is:
 Ich lerne neue Dinge, die in jedem Fall auftreten.</span></pre> <p><span class="koboSpan" id="kobo.1021.1">The German sentence obtained in the output (</span><em class="italic"><span class="koboSpan" id="kobo.1022.1">Ich lerne neue Dinge, die in jedem Fall auftreten</span></em><span class="koboSpan" id="kobo.1023.1">) means </span><em class="italic"><span class="koboSpan" id="kobo.1024.1">I learn new things that arise in every case</span></em><span class="koboSpan" id="kobo.1025.1">, and therefore, as can be seen from the results, the text has been almost perfectly translated from English to German. </span><span class="koboSpan" id="kobo.1025.2">Moreover, the results are equivalent to the non-optimized case (</span><span class="No-Break"><span class="koboSpan" id="kobo.1026.1">as expected).</span></span></p>
<h3><span class="koboSpan" id="kobo.1027.1">Profiling our models</span></h3>
<p><span class="koboSpan" id="kobo.1028.1">In the previous</span><a id="_idIndexMarker1127"/><span class="koboSpan" id="kobo.1029.1"> sections, we saw the different techniques that we could apply to optimize our inference loops, and the results these techniques achieved. </span><span class="koboSpan" id="kobo.1029.2">However, how exactly do these techniques work? </span><span class="koboSpan" id="kobo.1029.3">Why are </span><span class="No-Break"><span class="koboSpan" id="kobo.1030.1">they faster?</span></span></p>
<p><span class="koboSpan" id="kobo.1031.1">In this section, we are going to use the MXNet profiler, which can help us understand issues happening </span><span class="No-Break"><span class="koboSpan" id="kobo.1032.1">during runtime.</span></span></p>
<p><span class="koboSpan" id="kobo.1033.1">As mentioned in the initial section, the output of model profiling is a JSON file that can be visualized with tools such as the Google Chrome Tracing app. </span><span class="koboSpan" id="kobo.1033.2">For a non-optimized CPU workload, our transformer model shows the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1034.1">timing profile:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer217">
<span class="koboSpan" id="kobo.1035.1"><img alt="Figure 9.14 â€“ Profiling Transformer: Non-optimized CPU workload" src="image/B16591_09_14.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1036.1">Figure 9.14 â€“ Profiling Transformer: Non-optimized CPU workload</span></p>
<p><span class="koboSpan" id="kobo.1037.1">In </span><em class="italic"><span class="koboSpan" id="kobo.1038.1">Figure 9.14</span></em><span class="koboSpan" id="kobo.1039.1">, we </span><a id="_idIndexMarker1128"/><span class="koboSpan" id="kobo.1040.1">can see the </span><span class="No-Break"><span class="koboSpan" id="kobo.1041.1">following characteristics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1042.1">Almost all of the tasks are handled by two processes.</span></li>
<li><span class="koboSpan" id="kobo.1043.1">There is almost no waiting time (lazy evaluation and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1044.1">mx.nd.waitall</span></strong><span class="koboSpan" id="kobo.1045.1">).</span></li>
<li><span class="koboSpan" id="kobo.1046.1">Synchronous/structured usage of memory.</span></li>
<li><span class="koboSpan" id="kobo.1047.1">All operations are atomic and executed individually.</span></li>
<li><span class="koboSpan" id="kobo.1048.1">The full operation takes around 1,200 ms.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.1049.1">For a CPU-optimized workload, our Transformer model shows the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1050.1">timing profile:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer218">
<span class="koboSpan" id="kobo.1051.1"><img alt="Figure 9.15 â€“ Profiling Transformer: Optimized CPU workload" src="image/B16591_09_15.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1052.1">Figure 9.15 â€“ Profiling Transformer: Optimized CPU workload</span></p>
<p><span class="koboSpan" id="kobo.1053.1">In </span><em class="italic"><span class="koboSpan" id="kobo.1054.1">Figure 9.15</span></em><span class="koboSpan" id="kobo.1055.1">, we </span><a id="_idIndexMarker1129"/><span class="koboSpan" id="kobo.1056.1">can see the </span><span class="No-Break"><span class="koboSpan" id="kobo.1057.1">following characteristics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1058.1">Almost all of the tasks are handled by two processes, similar to the non-optimized counterpart.</span></li>
<li><span class="koboSpan" id="kobo.1059.1">There is almost no waiting time (lazy evaluation and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1060.1">mx.nd.waitall</span></strong><span class="koboSpan" id="kobo.1061.1">), similar to the non-optimized counterpart.</span></li>
<li><span class="koboSpan" id="kobo.1062.1">Memory is used in a more asynchronous/structured way, in comparison to the non-optimized counterpart.</span></li>
<li><span class="koboSpan" id="kobo.1063.1">Some operations are fused together. </span><span class="koboSpan" id="kobo.1063.2">Although the visualizations are not very clear, operator fusion (hybridization) seems to be working, with most of the time spent on fused operations.</span></li>
<li><span class="koboSpan" id="kobo.1064.1">The full operation takes around 720 ms.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.1065.1">Letâ€™s take a zoomed look into one of the operator </span><span class="No-Break"><span class="koboSpan" id="kobo.1066.1">fusion steps:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer219">
<span class="koboSpan" id="kobo.1067.1"><img alt="Figure 9.16 â€“ Profiling Transformer: Optimized CPU workload (zoom into OperatorFusion)" src="image/B16591_09_16.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1068.1">Figure 9.16 â€“ Profiling Transformer: Optimized CPU workload (zoom into OperatorFusion)</span></p>
<p><span class="koboSpan" id="kobo.1069.1">In </span><em class="italic"><span class="koboSpan" id="kobo.1070.1">Figure 9.16</span></em><span class="koboSpan" id="kobo.1071.1"> we can see how operator fusion has fused together several different operations, including embeddings, layer normalization, fully connected layers, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">MKL-DNN-accelerated layers.</span></span></p>
<p><span class="koboSpan" id="kobo.1073.1">In summary, for </span><a id="_idIndexMarker1130"/><span class="koboSpan" id="kobo.1074.1">CPU-based optimizations, we can clearly see </span><span class="No-Break"><span class="koboSpan" id="kobo.1075.1">the effects:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1076.1">Hybridization has fused most of the operators together, although the visualization is difficult to see, and this happens many times.</span></li>
<li><span class="koboSpan" id="kobo.1077.1">The MKL-DNN backend has improved those operations with accelerated operators.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.1078.1">Letâ€™s discuss the GPU </span><span class="No-Break"><span class="koboSpan" id="kobo.1079.1">case now.</span></span></p>
<p><span class="koboSpan" id="kobo.1080.1">For a non-optimized GPU workload, our Transformer model shows the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1081.1">timing profile:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer220">
<span class="koboSpan" id="kobo.1082.1"><img alt="Figure 9.17 â€“ Profiling Transformer: Non-optimized GPU workload" src="image/B16591_09_17.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1083.1">Figure 9.17 â€“ Profiling Transformer: Non-optimized GPU workload</span></p>
<p><span class="koboSpan" id="kobo.1084.1">In </span><em class="italic"><span class="koboSpan" id="kobo.1085.1">Figure 9.17</span></em><span class="koboSpan" id="kobo.1086.1">, we can see the </span><span class="No-Break"><span class="koboSpan" id="kobo.1087.1">following characteristics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1088.1">Tasks are mostly handled by several (three) GPU processes.</span></li>
<li><span class="koboSpan" id="kobo.1089.1">There is almost no waiting time (lazy evaluation and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1090.1">mx.nd.waitall</span></strong><span class="koboSpan" id="kobo.1091.1">).</span></li>
<li><span class="koboSpan" id="kobo.1092.1">Memory is gradually increasing.</span></li>
<li><span class="koboSpan" id="kobo.1093.1">All operations are atomic and executed individually.</span></li>
<li><span class="koboSpan" id="kobo.1094.1">Several copies from/to CPU, which do not seem to degrade performance.</span></li>
<li><span class="koboSpan" id="kobo.1095.1">The full operation takes around 580 ms.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.1096.1">For a </span><a id="_idIndexMarker1131"/><span class="koboSpan" id="kobo.1097.1">GPU-optimized workload, our Transformer model shows the following </span><span class="No-Break"><span class="koboSpan" id="kobo.1098.1">timing profile:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer221">
<span class="koboSpan" id="kobo.1099.1"><img alt="Figure 9.18 â€“ Profiling Transformer: Optimized GPU workload" src="image/B16591_09_18.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1100.1">Figure 9.18 â€“ Profiling Transformer: Optimized GPU workload</span></p>
<p><span class="koboSpan" id="kobo.1101.1">In </span><em class="italic"><span class="koboSpan" id="kobo.1102.1">Figure 9.18</span></em><span class="koboSpan" id="kobo.1103.1">, we can see the </span><span class="No-Break"><span class="koboSpan" id="kobo.1104.1">following characteristics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.1105.1">Almost all of the tasks are handled by three processes, similar to the non-optimized counterpart.</span></li>
<li><span class="koboSpan" id="kobo.1106.1">There is almost no waiting time (lazy evaluation and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1107.1">mx.nd.waitall</span></strong><span class="koboSpan" id="kobo.1108.1">), similar to the non-optimized counterpart.</span></li>
<li><span class="koboSpan" id="kobo.1109.1">More asynchronous/unstructured usage of memory, in comparison to the non-optimized counterpart.</span></li>
<li><span class="koboSpan" id="kobo.1110.1">Some operations are fused together. </span><span class="koboSpan" id="kobo.1110.2">Although the visualizations are not very clear, operator fusion (hybridization) seems to be working, spending most of the time in fused operations.</span></li>
<li><span class="koboSpan" id="kobo.1111.1">Data copy operations from/to CPU do not seem to degrade performance, although there are several.</span></li>
<li><span class="koboSpan" id="kobo.1112.1">The full operation takes around 260 ms.</span></li>
</ul>
<p><span class="koboSpan" id="kobo.1113.1">Letâ€™s take a </span><a id="_idIndexMarker1132"/><span class="koboSpan" id="kobo.1114.1">zoomed look into one of the operator </span><span class="No-Break"><span class="koboSpan" id="kobo.1115.1">fusion steps:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer222">
<span class="koboSpan" id="kobo.1116.1"><img alt="Figure 9.19 â€“ Profiling Transformer: Optimized GPU workload (zoom into OperatorFusion)" src="image/B16591_09_19.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.1117.1">Figure 9.19 â€“ Profiling Transformer: Optimized GPU workload (zoom into OperatorFusion)</span></p>
<p><span class="koboSpan" id="kobo.1118.1">In </span><em class="italic"><span class="koboSpan" id="kobo.1119.1">Figure 9.19</span></em><span class="koboSpan" id="kobo.1120.1">, we can see how operator fusion has fused together several different operations, including embeddings, layer normalization, and fully </span><span class="No-Break"><span class="koboSpan" id="kobo.1121.1">connected layers.</span></span></p>
<p><span class="koboSpan" id="kobo.1122.1">In summary, for GPU-based optimizations, we can clearly see the effect of hybridization, where all operations have been fused together, although the visualization is difficult to interpret, and this happens </span><span class="No-Break"><span class="koboSpan" id="kobo.1123.1">many times.</span></span></p>
<h3><span class="koboSpan" id="kobo.1124.1">Exporting our models</span></h3>
<p><span class="koboSpan" id="kobo.1125.1">MXNet and </span><a id="_idIndexMarker1133"/><span class="koboSpan" id="kobo.1126.1">GluonNLP also provide tools to export our models. </span><span class="koboSpan" id="kobo.1126.2">However, these tools are mostly for internal usage of MXNet/Gluon. </span><span class="koboSpan" id="kobo.1126.3">The reason for this is that GluonNLP mostly deals </span><a id="_idIndexMarker1134"/><span class="koboSpan" id="kobo.1127.1">with </span><strong class="bold"><span class="koboSpan" id="kobo.1128.1">Large Language Models</span></strong><span class="koboSpan" id="kobo.1129.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.1130.1">LLMs</span></strong><span class="koboSpan" id="kobo.1131.1">) and these workloads are not typically optimized for inference (edge AI computing). </span><span class="koboSpan" id="kobo.1131.2">Therefore, the only method available for model export is the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1132.1">save()</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1133.1"> function.</span></span></p>
<p><span class="koboSpan" id="kobo.1134.1">This function can be </span><span class="No-Break"><span class="koboSpan" id="kobo.1135.1">easily called:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1136.1">
wmt_transformer_pt_gpu_hybrid.save('transformer_pt_gpu_hybrid')</span></pre> <p><span class="koboSpan" id="kobo.1137.1">We can verify the files associated with the model, the parameters (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1138.1">.params</span></strong><span class="koboSpan" id="kobo.1139.1"> extension), and the architecture (the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1140.1">.json</span></strong><span class="koboSpan" id="kobo.1141.1"> extension) have been saved with </span><span class="No-Break"><span class="koboSpan" id="kobo.1142.1">these commands:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.1143.1">
assert os.path.exists("transformer_pt_gpu_hybrid-model.params")
assert os.path.exists("transformer_pt_gpu_hybrid-model.json")</span></pre> <p><span class="koboSpan" id="kobo.1144.1">And we </span><a id="_idIndexMarker1135"/><span class="koboSpan" id="kobo.1145.1">are done! </span><span class="koboSpan" id="kobo.1145.2">In this section, we have been able to successfully export our Transformer </span><span class="No-Break"><span class="koboSpan" id="kobo.1146.1">model. </span><span class="koboSpan" id="kobo.1146.2">Congratulations!</span></span></p>
<h2 id="_idParaDest-205"><a id="_idTextAnchor206"/><span class="koboSpan" id="kobo.1147.1">How it works...</span></h2>
<p><span class="koboSpan" id="kobo.1148.1">In this recipe, we</span><a id="_idIndexMarker1136"/><span class="koboSpan" id="kobo.1149.1"> have applied the different inference optimization techniques seen in the first recipe of this chapter, leveraging our hardware (CPUs and GPUs) to optimize our model runtime performance by hybridizing </span><span class="No-Break"><span class="koboSpan" id="kobo.1150.1">the model.</span></span></p>
<p><span class="koboSpan" id="kobo.1151.1">Moreover, we have learned how to use the MXNet profiler to analyze the inference optimizations from a </span><span class="No-Break"><span class="koboSpan" id="kobo.1152.1">low-level perspective.</span></span></p>
<p><span class="koboSpan" id="kobo.1153.1">Finally, we have learned how to export our models using internal </span><span class="No-Break"><span class="koboSpan" id="kobo.1154.1">MXNet libraries.</span></span></p>
<h2 id="_idParaDest-206"><a id="_idTextAnchor207"/><span class="koboSpan" id="kobo.1155.1">Thereâ€™s moreâ€¦</span></h2>
<p><span class="koboSpan" id="kobo.1156.1">In this recipe, we presented the inference optimization problem from a post-training perspective. </span><span class="koboSpan" id="kobo.1156.2">We were given a (pre-)trained model and we tried to squeeze as much performance as we could </span><span class="No-Break"><span class="koboSpan" id="kobo.1157.1">from it.</span></span></p>
<p><span class="koboSpan" id="kobo.1158.1">However, there is another avenue that can be explored, which starts thinking about maximizing inference performance from a machine learning model design perspective. </span><span class="koboSpan" id="kobo.1158.2">Several improvements to how LLMs can be used without large compute workloads have been published, such as </span><span class="No-Break"><span class="koboSpan" id="kobo.1159.1">the following:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1160.1">Low Ranking Adaptation (LORA)</span></strong><span class="koboSpan" id="kobo.1161.1">: </span><a href="https://arxiv.org/pdf/2012.13255.pdf"><span class="koboSpan" id="kobo.1162.1">https://arxiv.org/pdf/2012.13255.pdf</span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1163.1">LORA meets pruning</span></strong><span class="koboSpan" id="kobo.1164.1">: </span><a href="https://arxiv.org/pdf/2305.18403.pdf"><span class="koboSpan" id="kobo.1165.1">https://arxiv.org/pdf/2305.18403.pdf</span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1166.1">GPT4All (quantization)</span></strong><span class="koboSpan" id="kobo.1167.1">: </span><a href="https://gpt4all.io"><span class="koboSpan" id="kobo.1168.1">https://gpt4all.io</span></a></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1169.1">Int4 quantization</span></strong><span class="koboSpan" id="kobo.1170.1">: </span><a href="https://arxiv.org/pdf/2301.12017.pdf"><span class="koboSpan" id="kobo.1171.1">https://arxiv.org/pdf/2301.12017.pdf</span></a></li>
</ul>
</div>
</body></html>