["```py\nlibrary(ReinforcementLearning)\n\ndata(\"tictactoe\")\n```", "```py\nhead(tictactoe, 5)\n```", "```py\ntictactoe %>%\n  dplyr::filter(Reward == 1) %>%\n  head()\n\ntictactoe %>%\n  dplyr::filter(Reward == -1) %>%\n  head()\n```", "```py\ntictactoe %>%\n dplyr::filter(State == 'XB..X.XBB') %>%\n  dplyr::distinct()\n```", "```py\nState <- '0,0'\nAction <- '4'\nNextState <- '4,8'\nReward <- 0\n\nnumberscramble <- tibble::tibble(\n  State = State,\n  Action = Action,\n  NextState = NextState,\n  Reward = Reward\n)\n\nnumberscramble\n```", "```py\nlibrary(hash)\n\nQ <- hash()\n\nfor (i in unique(tictactoe$State)[!unique(tictactoe$State) %in% names(Q)]) {\n Q[[i]] <- hash(unique(tictactoe$Action), rep(0, length(unique(tictactoe$Action))))\n}\n```", "```py\ncontrol = list(\n alpha = 0.1, \n gamma = 0.1, \n epsilon = 0.1\n )\n```", "```py\n  d <- tictactoe[1, ]\n  state <- d$State\n  action <- d$Action\n  reward <- d$Reward\n  nextState <- d$NextState\n```", "```py\n  currentQ <- Q[[state]][[action]]\n  if (has.key(nextState,Q)) {\n    maxNextQ <- max(values(Q[[nextState]]))\n  } else {\n    maxNextQ <- 0\n  }\n```", "```py\n  ## Bellman equation\n  Q[[state]][[action]] <- currentQ + control$alpha *\n    (reward + control$gamma * maxNextQ - currentQ)\n\nq_value <- Q[[tictactoe$State[1]]][[tictactoe$Action[1]]]\n```", "```py\nfor (i in 1:nrow(tictactoe) {\n  d <- tictactoe[i, ]\n  state <- d$State\n  action <- d$Action\n  reward <- d$Reward\n  nextState <- d$NextState\n\n  currentQ <- Q[[state]][[action]]\n  if (has.key(nextState,Q)) {\n    maxNextQ <- max(values(Q[[nextState]]))\n  } else {\n    maxNextQ <- 0\n  }\n  ## Bellman equation\n  Q[[state]][[action]] <- currentQ + control$alpha *\n    (reward + control$gamma * maxNextQ - currentQ)\n}\n\nQ[[tictactoe$State[234543]]][[tictactoe$Action[234543]]]\n```", "```py\nQ <- hash()\n\nfor (i in unique(tictactoe$State)[!unique(tictactoe$State) %in% names(Q)]) {\n  Q[[i]] <- hash(unique(tictactoe$Action), rep(0, length(unique(tictactoe$Action))))\n}\n\ncontrol = list(\n  alpha = 0.5, \n  gamma = 0.1,  \n  epsilon = 0.1\n)\n\nfor (i in 1:nrow(tictactoe)) {\n  d <- tictactoe[i, ]\n  state <- d$State\n  action <- d$Action\n  reward <- d$Reward\n  nextState <- d$NextState\n\n  currentQ <- Q[[state]][[action]]\n  if (has.key(nextState,Q)) {\n    maxNextQ <- max(values(Q[[nextState]]))\n  } else {\n   maxNextQ <- 0\n  }\n  ## Bellman equation\n  Q[[state]][[action]] <- currentQ + control$alpha *\n    (reward + control$gamma * maxNextQ - currentQ)\n```", "```py\n}\n\nQ[[tictactoe$State[234543]]][[tictactoe$Action[234543]]]\n```", "```py\nlibrary(hash)\n\nQ <- hash()\n\nfor (i in unique(tictactoe$State)[!unique(tictactoe$State) %in% names(Q)]) {\n  Q[[i]] <- hash(unique(tictactoe$Action), rep(0, length(unique(tictactoe$Action))))\n}\n\ncontrol = list(\n  alpha = 0.1, \n  gamma = 0.9,  \n  epsilon = 0.1\n)\n\nfor (i in 1:nrow(tictactoe)) {\n  d <- tictactoe[i, ]\n  state <- d$State\n  action <- d$Action\n  reward <- d$Reward\n  nextState <- d$NextState\n\n  currentQ <- Q[[state]][[action]]\n  if (has.key(nextState,Q)) {\n    maxNextQ <- max(values(Q[[nextState]]))\n  } else {\n    maxNextQ <- 0\n  }\n  ## Bellman equation\n  Q[[state]][[action]] <- currentQ + control$alpha *\n    (reward + control$gamma * maxNextQ - currentQ)\n}\n\nQ[[tictactoe$State[234543]]][[tictactoe$Action[234543]]]\n```", "```py\n# Define control object\ncontrol <- list(\nalpha = 0.1, \ngamma = 0.1, \nepsilon = 0.9\n)\n\n# Perform reinforcement learning\nmodel <- ReinforcementLearning(data = tictactoe, \n                               s = \"State\", \n                               a = \"Action\", \n                               r = \"Reward\", \n                               s_new = \"NextState\", \n                               iter = 5,\n                               control = control)\n\nmodel$Q_hash[[tictactoe$State[234543]]][[tictactoe$Action[234543]]]\n```", "```py\nsort(model$Q['.........',1:9], decreasing = TRUE)\n\nmodel$Policy['.........']\n```"]