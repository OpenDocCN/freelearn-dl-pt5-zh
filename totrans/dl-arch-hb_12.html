<html><head></head><body>
<div id="_idContainer115">
<h1 class="chapter-number" id="_idParaDest-177"><a id="_idTextAnchor184"/><span class="koboSpan" id="kobo.1.1">12</span></h1>
<h1 id="_idParaDest-178"><a id="_idTextAnchor185"/><span class="koboSpan" id="kobo.2.1">Interpreting Neural Networks</span></h1>
<p><span class="koboSpan" id="kobo.3.1">When trying to comprehend the reasons behind a model’s prediction, local per-sample feature importance can be a valuable tool. </span><span class="koboSpan" id="kobo.3.2">This method enables you to focus your analysis on a smaller part of the input data, resulting in a more targeted understanding of key features that contributed to the model’s output. </span><span class="koboSpan" id="kobo.3.3">However, it is often still unclear which patterns the models are using to identify highly important features. </span><span class="koboSpan" id="kobo.3.4">This issue can be somewhat circumvented by reviewing more prediction explanations from targeted samples meant to strategically discern the actual reason for the prediction, which will also be introduced practically later in this chapter. </span><span class="koboSpan" id="kobo.3.5">However, this method is limited to the available number of samples you must validate your model against, and it can sometimes still be difficult to pinpoint the pattern </span><span class="No-Break"><span class="koboSpan" id="kobo.4.1">used concretely.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.5.1">Deep neural networks</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.7.1">DNNs</span></strong><span class="koboSpan" id="kobo.8.1">) learn low- to high-level </span><a id="_idIndexMarker882"/><a id="_idIndexMarker883"/><span class="koboSpan" id="kobo.9.1">features that help the prediction layer discern the right label under the hood. </span><span class="koboSpan" id="kobo.9.2">When we use local feature importance-based explanations on the input, we can’t know for sure which low-, medium-, or high-level patterns contributed to the importance of the input data. </span><span class="koboSpan" id="kobo.9.3">For images, this would range from low-level features, such as simple shapes, to medium-level features, such as the silhouette shape of a human body, all the way to a combination of patterns that build up to become a human face or everyday objects. </span><span class="koboSpan" id="kobo.9.4">For text, this would range from low-level features such as word embeddings, which represent the meaning of a word, to medium-level features such as the semantic roles of words in a sentence that enable the meaning of the text to be represented properly such as sentence embeddings, all the way to high-level features we are more familiar with, such as topics and sentiment. </span><span class="koboSpan" id="kobo.9.5">Of course, these are merely theoretical assumptions on what we think NNs </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">are learning.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">In this chapter, we will explore a method that can help to clear all the ambiguity in the features learned by a deep neural network, which is to visualize the patterns an NN detects directly through input optimization. </span><span class="koboSpan" id="kobo.11.2">By visualizing the patterns learned directly in combination with the filtering of activations, we can shed light on the actual reasons a deep neural network makes its predictions. </span><span class="koboSpan" id="kobo.11.3">Specifically, the following topics will </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">be discussed:</span></span></p>
<ul>
<li><span class="No-Break"><span class="koboSpan" id="kobo.13.1">Interpreting neurons</span></span></li>
<li><span class="koboSpan" id="kobo.14.1">Finding neurons </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">to interpret</span></span></li>
<li><span class="koboSpan" id="kobo.16.1">Interpreting learned </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">image patterns</span></span></li>
<li><span class="koboSpan" id="kobo.18.1">Discovering the counterfactual </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">explanation strategy</span></span></li>
</ul>
<h1 id="_idParaDest-179"><a id="_idTextAnchor186"/><span class="koboSpan" id="kobo.20.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.21.1">This chapter includes practical implementation in the Python programming language. </span><span class="koboSpan" id="kobo.21.2">To complete it, you will need to have a computer with the following </span><span class="No-Break"><span class="koboSpan" id="kobo.22.1">libraries installed:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.23.1">torchvision</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.24.1">torch</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.25.1">torch-lucent==0.1.8</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.26.1">matplotlib==3.3.0</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.27.1">captum</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.28.1">pillow</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.29.1">numpy</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.30.1">The code files are present on </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_12"><span class="No-Break"><span class="koboSpan" id="kobo.32.1">https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_12</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.33.1">.</span></span></p>
<h1 id="_idParaDest-180"><a id="_idTextAnchor187"/><span class="koboSpan" id="kobo.34.1">Interpreting neurons</span></h1>
<p><span class="koboSpan" id="kobo.35.1">Neurons in NN layers produce features that </span><a id="_idIndexMarker884"/><a id="_idIndexMarker885"/><span class="koboSpan" id="kobo.36.1">will be consumed by subsequent layers. </span><span class="koboSpan" id="kobo.36.2">The features or activations produced are simply an indicator of how prominent a learned pattern is in the input data. </span><span class="koboSpan" id="kobo.36.3">But have you ever wondered what the patterns are? </span><span class="koboSpan" id="kobo.36.4">Decoding the actual patterns learned by the NN can further improve the transparency needed to achieve the goals mentioned in the </span><em class="italic"><span class="koboSpan" id="kobo.37.1">Exploring the value of prediction explanations</span></em><span class="koboSpan" id="kobo.38.1"> section of </span><a href="B18187_11.xhtml#_idTextAnchor172"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.39.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.40.1">, </span><em class="italic"><span class="koboSpan" id="kobo.41.1">Explaining Neural </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.42.1">Network Predictions</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.43.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.44.1">Data is composed of many complicated patterns combined into a single sample. </span><span class="koboSpan" id="kobo.44.2">Traditionally, to discern what a neuron is detecting, much input data has to be evaluated and compared against other data so that a qualitative conclusion can be made by humans, which is both time-consuming and hard to get right. </span><span class="koboSpan" id="kobo.44.3">This method allows us to pinpoint the actual pattern that causes a high activation value visually, without the disturbance of other highly </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">correlated patterns.</span></span></p>
<p><span class="koboSpan" id="kobo.46.1">More formally, feature visualization by optimization </span><a id="_idIndexMarker886"/><a id="_idIndexMarker887"/><span class="koboSpan" id="kobo.47.1">can be useful in the following </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">use cases:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.49.1">Understanding the patterns associated with confusing labels without help from a </span><span class="No-Break"><span class="koboSpan" id="kobo.50.1">domain expert:</span></span><ul><li><span class="koboSpan" id="kobo.51.1">This is more prevalent in real-world audio data where the sound of the label in the real data can often be mixed together with lots </span><span class="No-Break"><span class="koboSpan" id="kobo.52.1">of noises</span></span></li><li><span class="koboSpan" id="kobo.53.1">This can also happen in </span><span class="No-Break"><span class="koboSpan" id="kobo.54.1">image data</span></span></li></ul></li>
<li><span class="koboSpan" id="kobo.55.1">It is not straightforward or possible to obtain real data to test any hypothesis on what the NN learned that can’t be proven with gradient-based feature attribution techniques on </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">available data</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.57.1">The core of the neuron interpretation technique is </span><strong class="bold"><span class="koboSpan" id="kobo.58.1">neural input optimization</span></strong><span class="koboSpan" id="kobo.59.1">, which is a process that modifies the input data of the NN to </span><a id="_idIndexMarker888"/><a id="_idIndexMarker889"/><span class="koboSpan" id="kobo.60.1">activate highly on the chosen neuron. </span><span class="koboSpan" id="kobo.60.2">Remember that during the training process, we optimize the weights of the NNs toward reducing the loss value. </span><span class="koboSpan" id="kobo.60.3">In this technique, we randomly initialize an input and optimize the input data to activate highly on chosen neurons, effectively treating the input data as NN weights. </span><span class="koboSpan" id="kobo.60.4">Gradients can be naturally computed to the input data stage, making it possible to update the input data according to the computed gradients after applying a learning rate. </span><span class="koboSpan" id="kobo.60.5">This technique </span><a id="_idIndexMarker890"/><a id="_idIndexMarker891"/><span class="koboSpan" id="kobo.61.1">also allows you to jointly optimize multiple neurons to activate highly and obtain an image that shows the patterns of how two different neurons </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">can coexist.</span></span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.63.1">Figure 12</span></em></span><em class="italic"><span class="koboSpan" id="kobo.64.1">.1</span></em><span class="koboSpan" id="kobo.65.1"> showcases the idea of low-level to </span><a id="_idIndexMarker892"/><a id="_idIndexMarker893"/><span class="koboSpan" id="kobo.66.1">medium-level and high-level patterns in </span><strong class="bold"><span class="koboSpan" id="kobo.67.1">convolutional NNs</span></strong><span class="koboSpan" id="kobo.68.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.69.1">CNNs</span></strong><span class="koboSpan" id="kobo.70.1">) with examples of optimized image input data using random low-, mid-, and high-level convolutional filters in the ImageNet pre-trained </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.71.1">efficientnet-b0</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.72.1"> model:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer111">
<span class="koboSpan" id="kobo.73.1"><img alt="Figure 12.1 – Example of optimized images from random filters in the efficientnet-b0 model" src="image/B18187_12_1.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.74.1">Figure 12.1 – Example of optimized images from random filters in the efficientnet-b0 model</span></p>
<p><span class="koboSpan" id="kobo.75.1">If you look at the high-level filter patterns, the first optimized image on a random filter looks somewhat like flowers, and the second image looks like </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">leaf patterns.</span></span></p>
<p><span class="koboSpan" id="kobo.77.1">However, a main caveat with this technique is that the resulting optimized input data may not represent all the real-life variations of a pattern associated with a neuron. </span><span class="koboSpan" id="kobo.77.2">Even for a dynamic input data variable such as an image, which can be optimized to present the pattern in diverse ways, the resulting optimized input can still miss out on some representations of the pattern. </span><span class="koboSpan" id="kobo.77.3">A good approach to tackle this caveat is to first obtain the initial optimized input data variant and then execute subsequent optimizations and ensure the optimized input data will be different from the initial variant. </span><span class="koboSpan" id="kobo.77.4">This can be done by jointly optimizing an additional component – the negative cosine similarity between the initial optimized input data and the current input data</span><a id="_idIndexMarker894"/><a id="_idIndexMarker895"/><span class="koboSpan" id="kobo.78.1"> being optimized. </span><span class="koboSpan" id="kobo.78.2">This technique helps to generate diverse input data examples. </span><span class="koboSpan" id="kobo.78.3">But before you can optimize the input data and attempt to interpret a neuron, you need a strategy to choose the best neurons to optimize input data against, which will be discussed in the </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">next section.</span></span></p>
<h1 id="_idParaDest-181"><a id="_idTextAnchor188"/><span class="koboSpan" id="kobo.80.1">Finding neurons to interpret</span></h1>
<p><span class="koboSpan" id="kobo.81.1">With millions and billions of neurons in today’s SoTA architectures, it’s impossible to interpret every single neuron, and, frankly, a waste of time. </span><span class="koboSpan" id="kobo.81.2">The choice of the neuron to explain should depend on your goal. </span><span class="koboSpan" id="kobo.81.3">The following list shows some of the </span><a id="_idIndexMarker896"/><a id="_idIndexMarker897"/><span class="koboSpan" id="kobo.82.1">different goals and associated methods for choosing </span><span class="No-Break"><span class="koboSpan" id="kobo.83.1">suitable neurons:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.84.1">Finding out what a certain prediction label or class pattern looks like</span></strong><span class="koboSpan" id="kobo.85.1">: In this case, you should simply choose a neuron specific to the prediction of the target label or class. </span><span class="koboSpan" id="kobo.85.2">This is usually done to understand whether the model captured the desired patterns of the class well, or whether it learned irrelevant features. </span><span class="koboSpan" id="kobo.85.3">This can also be useful in multilabel scenarios where multiple labels always only exist together, and you want to decouple the labels to understand the input patterns associated with a single </span><span class="No-Break"><span class="koboSpan" id="kobo.86.1">label better.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.87.1">Wanting to understand the latent reasons why a specific label can be predicted in your dataset, or for all labels in general</span></strong><span class="koboSpan" id="kobo.88.1">: In this case, you should choose the top most impactful neurons from the latent intermediate layers from a global neuron importance score. </span><span class="koboSpan" id="kobo.88.2">Global neuron importance can be obtained by aggregating the results of the integrated gradients method (introduced in </span><a href="B18187_11.xhtml#_idTextAnchor172"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.89.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.90.1">, </span><em class="italic"><span class="koboSpan" id="kobo.91.1">Explaining Neural Network Predictions</span></em><span class="koboSpan" id="kobo.92.1">) applied to your validation dataset for all neurons. </span><span class="koboSpan" id="kobo.92.2">The importance values for all neurons can then be ranked, and the top neurons can </span><span class="No-Break"><span class="koboSpan" id="kobo.93.1">be picked.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.94.1">Finding out the breakdown reasons why a prediction was made on top of saliency-based explanation techniques</span></strong><span class="koboSpan" id="kobo.95.1">: In this case, you should choose the neuron that has the highest activation value and highest importance score. </span><span class="koboSpan" id="kobo.95.2">A neuron that is activated highly does not necessarily mean that it is important for a certain prediction. </span><span class="koboSpan" id="kobo.95.3">Additionally, a neuron that is important does not mean that the neuron is activated. </span><span class="koboSpan" id="kobo.95.4">Using both the integrated gradients’ importance value and the activation values to obtain the most important neuron will help to make sure the neurons you care about are chosen. </span><span class="koboSpan" id="kobo.95.5">Additionally, you can further filter out more neurons if you have a focus area based on the initial input data saliency map by only choosing neurons that affect the chosen </span><span class="No-Break"><span class="koboSpan" id="kobo.96.1">focus area.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.97.1">Understanding the interactions between multiple labels or classes</span></strong><span class="koboSpan" id="kobo.98.1">: In scenarios where the relationships between multiple labels or classes are important, you can choose neurons that capture these interactions. </span><span class="koboSpan" id="kobo.98.2">Identify neurons that are highly activated and have high importance scores when multiple labels or classes are predicted together. </span><span class="koboSpan" id="kobo.98.3">Analyzing these neurons can help you understand how the model captures the relationships between different labels or classes and may reveal potential areas </span><span class="No-Break"><span class="koboSpan" id="kobo.99.1">for improvement.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.100.1">Investigating the robustness of the model to adversarial attacks</span></strong><span class="koboSpan" id="kobo.101.1">: In this case, you should choose neurons that are sensitive to adversarial perturbations in the input data. </span><span class="koboSpan" id="kobo.101.2">You can generate adversarial examples, with more info on how to do so in </span><a href="B18187_14.xhtml#_idTextAnchor206"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.102.1">Chapter 14</span></em></span></a><span class="koboSpan" id="kobo.103.1">, </span><em class="italic"><span class="koboSpan" id="kobo.104.1">Analyzing Adversarial Performance</span></em><span class="koboSpan" id="kobo.105.1">, and then compute the neuron importance scores using techniques such as integrated gradients. </span><span class="koboSpan" id="kobo.105.2">By visualizing neurons </span><a id="_idIndexMarker898"/><a id="_idIndexMarker899"/><span class="koboSpan" id="kobo.106.1">that are most affected by adversarial perturbations, you can gain insights into the model’s vulnerabilities and explore </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">potential defenses.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.108.1">Exploring the hierarchical structure of learned features</span></strong><span class="koboSpan" id="kobo.109.1">: In this case, you should choose neurons from different layers of the NN to understand how the model learns hierarchical features. </span><span class="koboSpan" id="kobo.109.2">Select neurons from early layers to investigate low-level features, and from deeper layers to investigate high-level features. </span><span class="koboSpan" id="kobo.109.3">You can also select multiple neurons to co-optimize the input data for high activation to understand how multiple-neuron learned patterns can exist in the same input data. </span><span class="koboSpan" id="kobo.109.4">Visualizing these neurons can help you understand the model’s internal representation of the data and how it builds increasingly complex features. </span><span class="koboSpan" id="kobo.109.5">This can provide insights into the model’s learning process and potential areas </span><span class="No-Break"><span class="koboSpan" id="kobo.110.1">for improvement.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.111.1">Analyzing the model’s generalization capabilities across different datasets</span></strong><span class="koboSpan" id="kobo.112.1">: To understand how well the model generalizes to new data, you should choose neurons that are consistently important across different datasets. </span><span class="koboSpan" id="kobo.112.2">Calculate the neuron importance scores using techniques such as integrated gradients for different datasets, and identify neurons that maintain high importance scores across all datasets. </span><span class="koboSpan" id="kobo.112.3">By visualizing these neurons, you can gain insights into the model’s generalization capabilities and identify potential areas </span><span class="No-Break"><span class="koboSpan" id="kobo.113.1">for improvement.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.114.1">Now that we’ve established the method to choose a neuron for interpretation, let’s start with a practical exploration of interpreting neurons with image </span><span class="No-Break"><span class="koboSpan" id="kobo.115.1">input data!</span></span></p>
<h1 id="_idParaDest-182"><a id="_idTextAnchor189"/><span class="koboSpan" id="kobo.116.1">Interpreting learned image patterns</span></h1>
<p><span class="koboSpan" id="kobo.117.1">Interpreting NNs that take in image data</span><a id="_idIndexMarker900"/><a id="_idIndexMarker901"/><span class="koboSpan" id="kobo.118.1"> enables a new paradigm in interpretation, which is the capability to visualize exactly what a neuron is detecting. </span><span class="koboSpan" id="kobo.118.2">In the case of audio input data, interpreting NNs would allow us to audibly represent what a neuron is detecting, similar to how we visualize patterns in image data! </span><span class="koboSpan" id="kobo.118.3">Choose neurons you want to understand based on your goal and visualize the patterns it is detecting through iterative optimizing on image data to activate highly for </span><span class="No-Break"><span class="koboSpan" id="kobo.119.1">that neuron.</span></span></p>
<p><span class="koboSpan" id="kobo.120.1">Practically, however, optimizing image data based on a neuron has an issue where the resulting image often produces high-frequency patterns that are perceived to be noisy, uninterpretable, and unaesthetic. </span><span class="koboSpan" id="kobo.120.2">High-frequency patterns are defined to be pixels that are high in intensity and change quickly from one to the next. </span><span class="koboSpan" id="kobo.120.3">This is largely due to the mostly unconstrained range of values that a pixel can be represented by, and pixels in isolation are not the semantic units we care about. </span><span class="koboSpan" id="kobo.120.4">Zooming in on the resulting image might make it more interpretable, but the interpretation effectiveness is diminished with the need to perform human evaluation and </span><span class="No-Break"><span class="koboSpan" id="kobo.121.1">extra work.</span></span></p>
<p><span class="koboSpan" id="kobo.122.1">This issue can be effectively mitigated</span><a id="_idIndexMarker902"/><a id="_idIndexMarker903"/><span class="koboSpan" id="kobo.123.1"> practically through the </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">following techniques:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.125.1">Frequency penalization – example techniques are </span><span class="No-Break"><span class="koboSpan" id="kobo.126.1">as follows:</span></span><ul><li><span class="koboSpan" id="kobo.127.1">Randomly blurring the image during optimization using a bilateral filter, which has the benefit of also preserving </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">edge patterns</span></span></li><li><span class="koboSpan" id="kobo.129.1">Penalizing variation between neighboring pixels conservatively in </span><span class="No-Break"><span class="koboSpan" id="kobo.130.1">the optimization</span></span></li></ul></li>
<li><span class="No-Break"><span class="koboSpan" id="kobo.131.1">Image augmentations</span></span></li>
<li><span class="koboSpan" id="kobo.132.1">Image preprocessing – example techniques are </span><span class="No-Break"><span class="koboSpan" id="kobo.133.1">as follows:</span></span><ul><li><span class="No-Break"><span class="koboSpan" id="kobo.134.1">Data decorrelation</span></span></li><li><span class="koboSpan" id="kobo.135.1">Fast </span><span class="No-Break"><span class="koboSpan" id="kobo.136.1">Fourier transform</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.137.1">Let’s continue our journey here by going through a practical tutorial using a pre-trained 121-layer densenet model on the </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">ImageNet dataset.</span></span></p>
<h2 id="_idParaDest-183"><a id="_idTextAnchor190"/><span class="koboSpan" id="kobo.139.1">Explaining predictions with image input data and integrated gradients</span></h2>
<p><span class="koboSpan" id="kobo.140.1">In this section, we will explorepredictions, explaining withpredictions, explaining with a </span><a id="_idIndexMarker904"/><span class="koboSpan" id="kobo.141.1">practical tutorial on explaining predictions from a CNN model that takes in image input data with integrated gradients, providing some insight into the reasons the model made its prediction. </span><span class="koboSpan" id="kobo.141.2">In this tutorial, we will discover what answers we need that are missing from prediction explanations, which will set us up </span><a id="_idIndexMarker905"/><a id="_idIndexMarker906"/><span class="koboSpan" id="kobo.142.1">for interpreting the CNN model. </span><span class="koboSpan" id="kobo.142.2">We’ll proceed </span><span class="No-Break"><span class="koboSpan" id="kobo.143.1">as follows:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.144.1">We will be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.145.1">lucent</span></strong><span class="koboSpan" id="kobo.146.1"> library for this</span><a id="_idIndexMarker907"/><a id="_idIndexMarker908"/><span class="koboSpan" id="kobo.147.1"> tutorial, which provides methods to interpret NNs through feature visualization by optimization. </span><span class="koboSpan" id="kobo.147.2">Additionally, we will be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.148.1">torch</span></strong><span class="koboSpan" id="kobo.149.1"> library for the densenet model. </span><span class="koboSpan" id="kobo.149.2">We will also be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.150.1">captum</span></strong><span class="koboSpan" id="kobo.151.1"> library to use the integrated gradients method. </span><span class="koboSpan" id="kobo.151.2">Let’s start by importing all the </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">necessary libraries:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.153.1">
import glob
import numpy as np
import torch
import torch.nn as nn
import torchvision.transforms as transforms
from PIL import Image
from captum.attr import IntegratedGradients
from captum.attr import NoiseTunnel
from captum.attr import visualization as viz
from lucent.optvis import render, param, objectives
from lucent.optvis.objectives import diversity</span></pre></li> <li><span class="koboSpan" id="kobo.154.1">Next, we will define the model </span><a id="_idIndexMarker909"/><a id="_idIndexMarker910"/><span class="koboSpan" id="kobo.155.1">class for the pre-trained densenet model that we </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">will use:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.157.1">
class CNN(nn.Module):
  def __init__(self, num_classes, model='resnet50'):
    super(CNN, self).__init__()
    self.num_classes = num_classes
    self.chosen_model = model
    if self.chosen_model=='densenet121':
      self.model = models.densenet121(pretrained=True)
      self.classifier = nn.Sequential(
        nn.Dropout(p=0.1),
        nn.Linear(self.model.classifier.in_features, 256, bias=False),
        nn.ReLU(),
        nn.BatchNorm1d(256),
        nn.Linear(256, 128, bias=False),
        nn.ReLU(),
        nn.BatchNorm1d(128),
        nn.Linear(128, self.num_classes, bias=False),
        nn.BatchNorm1d(self.num_classes),
      )
      self.model.classifier = self.classifier
      model_parameters = filter(lambda p: p.requires_grad, self.model.parameters())
      params = sum([np.prod(p.size()) for p in model_parameters])
  def forward(self, x):
    return self.model(x)</span></pre></li> <li><span class="koboSpan" id="kobo.158.1">We will be using the defined model </span><a id="_idIndexMarker911"/><a id="_idIndexMarker912"/><span class="koboSpan" id="kobo.159.1">class to load weights pre-trained on an image dataset called </span><strong class="source-inline"><span class="koboSpan" id="kobo.160.1">HAM10000</span></strong><span class="koboSpan" id="kobo.161.1"> with seven different skin </span><span class="No-Break"><span class="koboSpan" id="kobo.162.1">lesion classes:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.163.1">
checkpoint = torch.load('0.8228 checkpoint.pt', map_location=torch.device('cpu'))
model = checkpoint['model']</span></pre></li> <li><span class="koboSpan" id="kobo.164.1">The seventh index of the prediction layer of the pre-trained model is trained to predict melanoma, which is a type of skin cancer. </span><span class="koboSpan" id="kobo.164.2">Let’s take a look at a</span><a id="_idIndexMarker913"/><a id="_idIndexMarker914"/><span class="koboSpan" id="kobo.165.1"> few examples of melanoma from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.166.1">ISIC-2017</span></strong><span class="koboSpan" id="kobo.167.1"> dataset and see what exactly the pre-trained model is focusing on when predicting these images. </span><span class="koboSpan" id="kobo.167.2">We will be using the integrated gradients method from the </span><strong class="source-inline"><span class="koboSpan" id="kobo.168.1">captum</span></strong><span class="koboSpan" id="kobo.169.1"> library. </span><span class="koboSpan" id="kobo.169.2">First, let’s define the preprocessing needed to allow model inferencing, which converts a </span><strong class="source-inline"><span class="koboSpan" id="kobo.170.1">numpy</span></strong><span class="koboSpan" id="kobo.171.1"> image array into </span><strong class="source-inline"><span class="koboSpan" id="kobo.172.1">torch</span></strong><span class="koboSpan" id="kobo.173.1"> tensors, resizes the image into the pre-trained image size, and </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">normalizes it:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.175.1">
resize_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize((224, 224)),
])
norm_transform = transforms.Normalize(
    mean</span><a id="_idTextAnchor191"/><span class="koboSpan" id="kobo.176.1">=[0.485, 0.456, 0.406],
    std=[0.229, 0.224, 0.225])</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.177.1">The mean and standard deviation values are derived directly from the ImageNet dataset and were used to pre-train the model, as prior to pre-training on the </span><strong class="source-inline"><span class="koboSpan" id="kobo.178.1">HAM10000</span></strong><span class="koboSpan" id="kobo.179.1"> dataset, it was pre-trained on ImageNet. </span><span class="koboSpan" id="kobo.179.2">Additionally, the </span><strong class="source-inline"><span class="koboSpan" id="kobo.180.1">224</span></strong><span class="koboSpan" id="kobo.181.1"> dimension is also adopted from the ImageNet pre-trained settings. </span><span class="koboSpan" id="kobo.181.2">As we need the intermediate result after resizing separately, we defined the logic for resizing and </span><span class="No-Break"><span class="koboSpan" id="kobo.182.1">normalization separately.</span></span></p></li> <li><span class="koboSpan" id="kobo.183.1">Next, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.184.1">glob</span></strong><span class="koboSpan" id="kobo.185.1"> library to </span><a id="_idIndexMarker915"/><a id="_idIndexMarker916"/><span class="koboSpan" id="kobo.186.1">load all the melanoma images in the provided </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">dataset folder:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.188.1">
all_melanoma_images = glob.glob("lesions_augmented_data/Data/test/Melanoma/*.jpg")</span></pre></li> <li><span class="koboSpan" id="kobo.189.1">We will be using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.190.1">captum</span></strong><span class="koboSpan" id="kobo.191.1"> library implementation of integrated gradients and noise tunneling to smooth out the resulting </span><a id="_idIndexMarker917"/><a id="_idIndexMarker918"/><span class="koboSpan" id="kobo.192.1">attribution noise. </span><span class="koboSpan" id="kobo.192.2">Let’s define the instances needed to execute these components, along with defining the prediction class index for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.193.1">Melanoma</span></strong><span class="koboSpan" id="kobo.194.1"> target class that we are </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">interested in:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.196.1">
integrated_gradients = IntegratedGradients(model)
noise_tunnel_applyer = NoiseTunnel(integrated_gradients)
prediction_label_index = 6</span></pre></li> <li><span class="koboSpan" id="kobo.197.1">We can now loop through the first six</span><a id="_idIndexMarker919"/><a id="_idIndexMarker920"/><span class="koboSpan" id="kobo.198.1"> images, apply the preprocessing, apply the integrated gradients method from </span><strong class="source-inline"><span class="koboSpan" id="kobo.199.1">captum</span></strong><span class="koboSpan" id="kobo.200.1">, and finally, visualize the original image and the obtained input importance </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">heat map:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.202.1">
for melanoma_image in all_melanoma_images[:6]:
  pil_image = Image.open(melanoma_image)
  img = np.array(pil_image)
  transformed_image = resize_transform(img)
  input =  norm_transform(transformed_image).unsqueeze(0)
  attributions_ig_nt =  noise_tunnel_applyer.attribute(
    input, nt_samples=10, nt_type='smoothgrad_sq',
    target= prediction_label_index)
  _ = viz.visualize_image_attr_multiple(   np.transpose(attributions_ig_nt.squeeze().cpu().detach().numpy(), (1,2,0)),
np.transpose(transformed_image.squeeze().cpu().detach().numpy(), (1,2,0)),
      ["original_image", "heat_map"],
      ["all", "positive"],
      cmap='turbo',
      show_colorbar=True)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.203.1">This will show visualizations</span><a id="_idIndexMarker921"/><a id="_idIndexMarker922"/><span class="koboSpan" id="kobo.204.1"> presented in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.205.1">Figure 12</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.206.1">.2</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.207.1">:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer112">
<span class="koboSpan" id="kobo.208.1"><img alt="Figure 12.2 – Six real images of melanoma from the ISIC-2017 dataset along with the gradient-based attribution of the model" src="image/B18187_12_2.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.209.1">Figure 12.2 – Six real images of melanoma from the ISIC-2017 dataset along with the gradient-based attribution of the model</span></p>
<p><span class="koboSpan" id="kobo.210.1">The model seems to be mainly focusing on the darker spots in the first five examples, but the model still considers the surrounding skin, although</span><a id="_idIndexMarker923"/><a id="_idIndexMarker924"/><span class="koboSpan" id="kobo.211.1"> with much less focus. </span><span class="koboSpan" id="kobo.211.2">This might be a signal that the model can depend on the surrounding skin slightly to predict </span><a id="_idIndexMarker925"/><a id="_idIndexMarker926"/><span class="koboSpan" id="kobo.212.1">melanoma. </span><span class="koboSpan" id="kobo.212.2">But this begs the question: Is the model identifying the darkness of the skin for melanoma, or is it identifying some sort of pattern under the hood, or is it both? </span><span class="koboSpan" id="kobo.212.3">For the last example, the model seems to be all over the place and not really focusing on the dark spots. </span><span class="koboSpan" id="kobo.212.4">This could mean that the skin has patterns that are related to melanoma that are not necessarily darker in color. </span><span class="koboSpan" id="kobo.212.5">There are a few more questions that can’t really be answered through these examples, which are </span><span class="No-Break"><span class="koboSpan" id="kobo.213.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.214.1">Is the model dependent on the color of the skin to predict melanoma? </span><span class="koboSpan" id="kobo.214.2">Or is it really about </span><span class="No-Break"><span class="koboSpan" id="kobo.215.1">the pattern?</span></span></li>
<li><span class="koboSpan" id="kobo.216.1">What exactly are the patterns the </span><a id="_idIndexMarker927"/><a id="_idIndexMarker928"/><span class="koboSpan" id="kobo.217.1">model </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">is detecting?</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.219.1">To answer these questions, we will use the </span><strong class="source-inline"><span class="koboSpan" id="kobo.220.1">lucent</span></strong><span class="koboSpan" id="kobo.221.1"> library to visualize the</span><a id="_idIndexMarker929"/><a id="_idIndexMarker930"/><span class="koboSpan" id="kobo.222.1"> patterns learned to predict </span><span class="No-Break"><span class="koboSpan" id="kobo.223.1">melanoma confidently.</span></span></p>
<h2 id="_idParaDest-184"><a id="_idTextAnchor192"/><span class="koboSpan" id="kobo.224.1">Practically visualizing neurons with image input data</span></h2>
<p><span class="koboSpan" id="kobo.225.1">In this section, we will continue with the </span><a id="_idIndexMarker931"/><a id="_idIndexMarker932"/><span class="koboSpan" id="kobo.226.1">previous tutorial to further explore how to practically visualize neurons with image input data using optimization techniques to gain insights into the patterns and behaviors learned by the CNN model. </span><span class="koboSpan" id="kobo.226.2">This process involves choosing neurons to interpret, optimizing image data for those neurons, and applying</span><a id="_idIndexMarker933"/><span class="koboSpan" id="kobo.227.1"> regularization techniques to generate visually interpretable patterns. </span><span class="koboSpan" id="kobo.227.2">By visualizing the patterns learned by the model, we can gain a better understanding of the model’s predictions and answer questions that may not be apparent from traditional feature </span><span class="No-Break"><span class="koboSpan" id="kobo.228.1">importance methods.</span></span></p>
<p><span class="koboSpan" id="kobo.229.1">By following the steps outlined in this tutorial, you can visualize the patterns learned by your deep neural network, gaining a deeper understanding of the model’s predictions and the features that contribute to those predictions. </span><span class="koboSpan" id="kobo.229.2">This can help to answer questions about the model’s dependence on certain features, such as the color of the skin or the shape of the melanoma, and provide valuable insights into the patterns and behaviors of the model. </span><span class="koboSpan" id="kobo.229.3">Let’s </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">get started:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.231.1">First, let’s define the necessary variables. </span><span class="koboSpan" id="kobo.231.2">We want to visualize image patterns for the </span><strong class="source-inline"><span class="koboSpan" id="kobo.232.1">Melanoma</span></strong><span class="koboSpan" id="kobo.233.1"> class, which is at the sixth prediction layer index, so we must define the parameter we want to optimize </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.235.1">
param_to_optimize = "classifier_8:6"</span></pre></li> <li><span class="koboSpan" id="kobo.236.1">Next, for the first iteration, we will be utilizing the </span><strong class="bold"><span class="koboSpan" id="kobo.237.1">Compositional Pattern-Producing Network</span></strong><span class="koboSpan" id="kobo.238.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.239.1">CPPN</span></strong><span class="koboSpan" id="kobo.240.1">) method to initialize and </span><a id="_idIndexMarker934"/><a id="_idIndexMarker935"/><span class="koboSpan" id="kobo.241.1">generate an input image, instead of directly generating a random image input. </span><span class="koboSpan" id="kobo.241.2">Note that this is a method specifically for images and has been proven to generate more aesthetically pleasing images. </span><span class="koboSpan" id="kobo.241.3">In </span><strong class="source-inline"><span class="koboSpan" id="kobo.242.1">lucent</span></strong><span class="koboSpan" id="kobo.243.1"> specifically, CPPN consists of several convolutional layers with a compositional activation function consisting of element-wise tangents, squaring, division, and concatenation. </span><span class="koboSpan" id="kobo.243.2">This means that instead of optimizing the image directly, we optimize the parameters of the CPPN convolutional network that generates the input image for the main network. </span><span class="koboSpan" id="kobo.243.3">Backpropagation can be executed all the way through the generated input image till the first layer of the CPPN network. </span><span class="koboSpan" id="kobo.243.4">The initial input image is a fixed image comprising a circular region in the center of the image, with the values at the center being close to zero and gradually increasing toward the edges of the circle. </span><span class="koboSpan" id="kobo.243.5">However, with CPPN, the learning rate usually needs to be lower to converge properly. </span><span class="koboSpan" id="kobo.243.6">Let’s define the CPPN configuration with a </span><strong class="source-inline"><span class="koboSpan" id="kobo.244.1">224</span></strong><span class="koboSpan" id="kobo.245.1"> image size and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.246.1">Adam</span></strong><span class="koboSpan" id="kobo.247.1"> optimizer with a lower-than-typical </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">learning rate:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.249.1">
cppn_param_f = lambda: param.cppn(224)
cppn_opt = lambda params: torch.optim.Adam(params, 4e-3)</span></pre></li> <li><span class="koboSpan" id="kobo.250.1">Finally, let’s utilize the defined variables and visualize the patterns captured by the melanoma part of the prediction layer using a </span><span class="No-Break"><span class="koboSpan" id="kobo.251.1">GPU-configured model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.252.1">
model.cuda()
images = render.render_vis(
    model, param_to_optimize, cppn_param_f, cppn_opt,
    thresholds=np.linspace(0, 10000, 100, dtype=int).tolist(),
    verbose=True,
    show_inline=True
)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.253.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.254.1">thresholds</span></strong><span class="koboSpan" id="kobo.255.1"> list component controls the number of optimization steps taken, along with the intermediate step number to </span><a id="_idIndexMarker936"/><a id="_idIndexMarker937"/><span class="koboSpan" id="kobo.256.1">visualize the optimized image. </span><span class="koboSpan" id="kobo.256.2">Additionally, a hidden component built into the </span><strong class="source-inline"><span class="koboSpan" id="kobo.257.1">render_vis</span></strong><span class="koboSpan" id="kobo.258.1"> method is the </span><strong class="source-inline"><span class="koboSpan" id="kobo.259.1">transforms</span></strong><span class="koboSpan" id="kobo.260.1"> component. </span><span class="koboSpan" id="kobo.260.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.261.1">transforms</span></strong><span class="koboSpan" id="kobo.262.1"> component adds minimal augmentations such as padding, jitter, random scaling, and random rotating to reduce any random noise in the image being optimized. </span><span class="koboSpan" id="kobo.262.2">The result from the previous code is shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.263.1">Figure 12</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.264.1">.3</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.265.1">:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer113">
<span class="koboSpan" id="kobo.266.1"><img alt="Figure 12.3 – Progress of optimizing the CPPN to generate an image that activates highly for the melanoma class prediction" src="image/B18187_12_3.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.267.1">Figure 12.3 – Progress of optimizing the CPPN to generate an image that activates highly for the melanoma class prediction</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.268.1">A pretty good image of what seems to be the actual melanoma was able to be generated from </span><span class="No-Break"><span class="koboSpan" id="kobo.269.1">the process.</span></span></p>
<ol>
<li value="4"><span class="koboSpan" id="kobo.270.1">Let’s find out the melanoma</span><a id="_idIndexMarker938"/><span class="koboSpan" id="kobo.271.1"> probability of this image. </span><span class="koboSpan" id="kobo.271.2">We can do</span><a id="_idIndexMarker939"/><span class="koboSpan" id="kobo.272.1"> this by defining the preprocessing methods needed to perform inference with </span><span class="No-Break"><span class="koboSpan" id="kobo.273.1">this model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.274.1">
inference_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize((224, 224)),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
      std=[0.229, 0.224, 0.225]),
])</span></pre></li> <li><span class="koboSpan" id="kobo.275.1">Here, we predict the skin lesion classes of the final optimized image from the process while disabling </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">gradient computation:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.277.1">
def inference_lesion_model(img):
    transformed_image = inference_transform(img).unsqueeze(0)
    with torch.no_grad():
        output = torch.nn.functional.softmax(model(transformed_image), dim=1)
    return output
output = inference_lesion_model(images[</span><a id="_idTextAnchor193"/><span class="koboSpan" id="kobo.278.1">-1].squeeze())</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.279.1">This results in a 100% probability of the image being predicted </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">as melanoma!</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.281.1">However, the image alone does not make it apparent</span><a id="_idIndexMarker940"/><span class="koboSpan" id="kobo.282.1"> that all the diverse sets of images can be recognized</span><a id="_idIndexMarker941"/><span class="koboSpan" id="kobo.283.1"> as melanoma. </span><span class="koboSpan" id="kobo.283.2">Some classes can be sufficiently represented in a single image, but some labels can’t really be represented in a single picture. </span><span class="koboSpan" id="kobo.283.3">Take a background class, for example: it is impossible to put every single background visual in a single image. </span><span class="koboSpan" id="kobo.283.4">Here are some specific questions that could be useful to answer based on </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">the result:</span></span></p><ul><li><span class="koboSpan" id="kobo.285.1">Does the color of the </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">skin matter?</span></span></li><li><span class="koboSpan" id="kobo.287.1">Does the shape of the melanoma matter, as the final generated image seems to have similar </span><span class="No-Break"><span class="koboSpan" id="kobo.288.1">melanoma patterns?</span></span></li><li><span class="koboSpan" id="kobo.289.1">Does the color of the melanoma patch matter? </span><span class="koboSpan" id="kobo.289.2">There are green patches with a similar pattern to the </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">red patch.</span></span></li></ul></li> <li><span class="koboSpan" id="kobo.291.1">This is where the loss used to ensure diversity mentioned earlier can help to provide more insight. </span><span class="koboSpan" id="kobo.291.2">Now, let’s utilize the diversity objective with the original melanoma prediction layer index objective and optimize a batch of four input images concurrently. </span><span class="koboSpan" id="kobo.291.3">The batch mode functionality is not supported for CPPN in </span><strong class="source-inline"><span class="koboSpan" id="kobo.292.1">lucent</span></strong><span class="koboSpan" id="kobo.293.1"> and is only supported for the basic input image initialization </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.294.1">param</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.295.1"> module:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.296.1">
obj = objectives.channel("classifier_8", 6) - 1e2 * diversity("input")
batch_param_f = lambda: param.image(224, fft=True, decorrelate=True, batch=4)
batch_images = render.render_vis(
    model, obj,
    batch_param_f,
    thresholds=np.linspace(0, 500, 50, dtype=int).tolist(),
    show_inline=True,
    verbose=True,
)</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.297.1">Two additional points on the image initialization method are </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">as follows:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.299.1">fft</span></strong><span class="koboSpan" id="kobo.300.1"> stands for Fast </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">Fourier transform</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.302.1">decorrelate</span></strong><span class="koboSpan" id="kobo.303.1"> applies SVD to the </span><span class="No-Break"><span class="koboSpan" id="kobo.304.1">image input</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.305.1">Both techniques here are acknowledged</span><a id="_idIndexMarker942"/><span class="koboSpan" id="kobo.306.1"> in research to allow faster</span><a id="_idIndexMarker943"/><span class="koboSpan" id="kobo.307.1"> convergence, reduce high-frequency images, and generate </span><span class="No-Break"><span class="koboSpan" id="kobo.308.1">better-looking images.</span></span></p><p class="list-inset"><span class="koboSpan" id="kobo.309.1">The results are shown in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.310.1">Figure 12</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.311.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.312.1">:</span></span></p></li> </ol>
<div>
<div class="IMG---Figure" id="_idContainer114">
<span class="koboSpan" id="kobo.313.1"><img alt="Figure 12.4 – Four jointly optimized images to activate highly on the melanoma neuron" src="image/B18187_12_4.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.314.1">Figure 12.4 – Four jointly optimized images to activate highly on the melanoma neuron</span></p>
<p class="list-inset"><span class="koboSpan" id="kobo.315.1">These are very funky-looking images. </span><span class="koboSpan" id="kobo.315.2">The non-black color portions are probably simulating the skin. </span><span class="koboSpan" id="kobo.315.3">Let’s see the probability of the melanoma class for each of these images </span><span class="No-Break"><span class="koboSpan" id="kobo.316.1">to verify:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.317.1">
outputs = []
 for img_idx in range(4):
     img = batch_images[-1][img_idx]
    output = inference_lesion_model(img)
    outputs.append((output[0][6], output.argmax()))</span></pre> <p class="list-inset"><span class="koboSpan" id="kobo.318.1">This will result in the </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">following array:</span></span></p>
<pre class="source-code"><span class="koboSpan" id="kobo.320.1">
 [(tensor(0.9946, device='cuda:0'), tensor(6, device='cuda:0')),
 (tensor(0.9899, device='cuda:0'), tensor(6, device='cuda:0')),
 (tensor(0.7015, device='cuda:0'), tensor(6, device='cuda:0')),
 (tensor(0.9939, device='cuda:0'), tensor(6, device='cuda:0'))]</span></pre> <p class="list-inset"><span class="koboSpan" id="kobo.321.1">They all have high probabilities</span><a id="_idIndexMarker944"/><span class="koboSpan" id="kobo.322.1"> for melanoma! </span><span class="koboSpan" id="kobo.322.2">From this, the following</span><a id="_idIndexMarker945"/><span class="koboSpan" id="kobo.323.1"> conclusion can </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">be argued:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.325.1">The model does not depend a lot on the color of the skin to detect melanoma. </span><span class="koboSpan" id="kobo.325.2">The most that skin color can provide will likely be in the range of around a 3% </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">probability boost.</span></span></li>
<li><span class="koboSpan" id="kobo.327.1">The model depends on the underlying lower-level pattern mostly to </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">detect melanoma.</span></span></li>
<li><span class="koboSpan" id="kobo.329.1">The model doesn’t depend a lot on the color of the melanoma patch. </span><span class="koboSpan" id="kobo.329.2">The color of the melanoma in the first generated image and real images was reddish. </span><span class="koboSpan" id="kobo.329.3">The color of the melanoma patch in the batch-generated images </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">was black.</span></span></li>
<li><span class="koboSpan" id="kobo.331.1">The model can detect smaller melanoma signals from the real </span><span class="No-Break"><span class="koboSpan" id="kobo.332.1">images used.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.333.1">The results here are exemplary</span><a id="_idIndexMarker946"/><span class="koboSpan" id="kobo.334.1"> of how complementary each insight technique</span><a id="_idIndexMarker947"/><span class="koboSpan" id="kobo.335.1"> is to the other. </span><span class="koboSpan" id="kobo.335.2">We will end this topic with some useful notes about the pattern visualization of neurons through optimization techniques </span><span class="No-Break"><span class="koboSpan" id="kobo.336.1">in general:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.337.1">Some problems are harder to converge than others, and some just don’t converge at all. </span><span class="koboSpan" id="kobo.337.2">Be ready to experiment with multiple settings to see whether you can get a resulting input that can activate highly on your chosen neuron, channel, or entire layer. </span><span class="koboSpan" id="kobo.337.3">You can even choose multiple neurons to see how </span><span class="No-Break"><span class="koboSpan" id="kobo.338.1">they interact!</span></span></li>
<li><span class="koboSpan" id="kobo.339.1">The loss can turn out to be extremely negative, and the more the input converges, the more negative it gets. </span><span class="koboSpan" id="kobo.339.2">This is good, as the loss is defined as the negative of the resulting </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">activation value.</span></span></li>
<li><span class="koboSpan" id="kobo.341.1">Regularization techniques are the key to allowing reasonable inputs to be generated </span><span class="No-Break"><span class="koboSpan" id="kobo.342.1">through optimization.</span></span></li>
<li><span class="koboSpan" id="kobo.343.1">Use both real data and diverse optimized data to understand the patterns your model learned to detect. </span><span class="koboSpan" id="kobo.343.2">One optimized piece of data usually can’t represent the entire range of patterns a neuron </span><span class="No-Break"><span class="koboSpan" id="kobo.344.1">can detect.</span></span></li>
<li><span class="koboSpan" id="kobo.345.1">In this tutorial, we used the final classification layer, which made it easier to find samples that activate highly toward the chosen neuron. </span><span class="koboSpan" id="kobo.345.2">If an intermediate neuron is chosen, be sure to find the set of data with the highest activations for the </span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">chosen neuron.</span></span></li>
<li><span class="koboSpan" id="kobo.347.1">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.348.1">lucent</span></strong><span class="koboSpan" id="kobo.349.1"> library for </span><strong class="source-inline"><span class="koboSpan" id="kobo.350.1">pytorch</span></strong><span class="koboSpan" id="kobo.351.1">-based models and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.352.1">lucid</span></strong><span class="koboSpan" id="kobo.353.1"> library for TensorFlow-based models are focused on image visualizations but can both be adapted to other input variable types such as text. </span><span class="koboSpan" id="kobo.353.2">However, not much research has been done there to figure out good regularization techniques for other variable types to allow </span><span class="No-Break"><span class="koboSpan" id="kobo.354.1">faster convergence.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.355.1">Overall, the visualization of neurons</span><a id="_idIndexMarker948"/><span class="koboSpan" id="kobo.356.1"> through optimization techniques can provide valuable insights into the patterns and behaviors of </span><strong class="bold"><span class="koboSpan" id="kobo.357.1">machine learning</span></strong><span class="koboSpan" id="kobo.358.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.359.1">ML</span></strong><span class="koboSpan" id="kobo.360.1">) models, but it requires experimentation and careful consideration of the inputs and regularization techniques used. </span><span class="koboSpan" id="kobo.360.2">As a bonus here, with knowledge of how to execute prediction explanations and NN</span><a id="_idIndexMarker949"/><span class="koboSpan" id="kobo.361.1"> interpretation, we will discover a useful way to make explanations more effective</span><a id="_idIndexMarker950"/><span class="koboSpan" id="kobo.362.1"> in general with a method called </span><span class="No-Break"><span class="koboSpan" id="kobo.363.1">counterfactual explanations.</span></span></p>
<h1 id="_idParaDest-185"><a id="_idTextAnchor194"/><span class="koboSpan" id="kobo.364.1">Discovering the counterfactual explanation strategy</span></h1>
<p><span class="koboSpan" id="kobo.365.1">Counterfactual explanation</span><a id="_idIndexMarker951"/><span class="koboSpan" id="kobo.366.1"> or reasoning is a method of understanding and explaining anything in general by considering alternative and counterfactual scenarios or “what-if” situations. </span><span class="koboSpan" id="kobo.366.2">In the context of prediction explanations, it involves identifying changes in the input data that would lead to a different outcome. </span><span class="koboSpan" id="kobo.366.3">Ideally, the minimal changes should be identified. </span><span class="koboSpan" id="kobo.366.4">In the context of NN interpretation, it involves visualizing the opposite of the target label or intermediate latent features. </span><span class="koboSpan" id="kobo.366.5">This approach makes sense to use because it closely aligns with how humans naturally explain events and assess causality, which ultimately allows us to comprehend the underlying decision-making process of the </span><span class="No-Break"><span class="koboSpan" id="kobo.367.1">model better.</span></span></p>
<p><span class="koboSpan" id="kobo.368.1">Humans tend to think in terms of cause and effect, and we often explore alternative possibilities to make sense of events or decisions. </span><span class="koboSpan" id="kobo.368.2">For example, when trying to understand why a certain decision was made, we may ask questions such as, “What would have happened if we had chosen a different option?” </span><span class="koboSpan" id="kobo.368.3">or “What factors led to this outcome?”. This kind of reasoning helps us identify the key elements that influenced the decision and allows us to learn from the experience. </span><span class="koboSpan" id="kobo.368.4">Counterfactual explanations for ML models follow a similar thought process. </span><span class="koboSpan" id="kobo.368.5">By presenting alternative input instances that would have resulted in a different prediction, counterfactual explanations help us understand which features of the input data are most critical in the model’s decision-making process. </span><span class="koboSpan" id="kobo.368.6">This kind of explanation allows users to grasp the model’s rationale more intuitively and can also help improve their trust in the </span><span class="No-Break"><span class="koboSpan" id="kobo.369.1">model’s predictions.</span></span></p>
<p><span class="koboSpan" id="kobo.370.1">Counterfactual reasoning complements feature</span><a id="_idIndexMarker952"/><span class="koboSpan" id="kobo.371.1"> importance and neuron visualization techniques. </span><span class="koboSpan" id="kobo.371.2">Together, these methods can provide a more comprehensive understanding of how the model arrives at its decisions. </span><span class="koboSpan" id="kobo.371.3">This, in turn, can help users better assess the reliability of the model and make more informed decisions based on </span><span class="No-Break"><span class="koboSpan" id="kobo.372.1">its predictions.</span></span></p>
<h1 id="_idParaDest-186"><a id="_idTextAnchor195"/><span class="koboSpan" id="kobo.373.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.374.1">NN interpretation is a form of a model understanding process that is different from explaining the predictions made by a model. </span><span class="koboSpan" id="kobo.374.2">Both manual discovery of real images and optimizing synthetic images to activate highly for the chosen neuron to interpret are techniques that can be applied together to understand the NN. </span><span class="koboSpan" id="kobo.374.3">Practically, the interpretation of NNs will be useful when you have goals to reveal the appearance of a particular prediction label or class pattern, gain insight into the factors contributing to the prediction of a specific label in your dataset or all labels in general, and gain a detailed breakdown of the reasons behind </span><span class="No-Break"><span class="koboSpan" id="kobo.375.1">a prediction.</span></span></p>
<p><span class="koboSpan" id="kobo.376.1">There might be hiccups when trying to apply the technique in your use case, so don’t be afraid to experiment with the parameters and components introduced in this chapter in your goal to interpret </span><span class="No-Break"><span class="koboSpan" id="kobo.377.1">your NN.</span></span></p>
<p><span class="koboSpan" id="kobo.378.1">We will explore a different facet of insights that you can obtain from your data and your model in the next chapter, which is about bias </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">and fairness.</span></span></p>
</div>
</body></html>