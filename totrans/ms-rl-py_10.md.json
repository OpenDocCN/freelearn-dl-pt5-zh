["```py\n$ virtualenv mbenv\n$ source mbenv/bin/activate\n$ pip install ray[rllib]==1.0.1\n$ pip install tensorflow==2.3.1\n$ pip install cma==3.0.3\n$ pip install gym[box2d]\n```", "```py\n    from math import sin, pi\n    def black_box_projectile(theta, v0=10, g=9.81):\n        assert theta >= 0\n        assert theta <= 90\n        return (v0 ** 2) * sin(2 * pi * theta / 180) / g\n    ```", "```py\n    import random\n    def random_shooting(n, min_a=0, max_a=90):\n        return [random.uniform(min_a, max_a) for i in range(n)]\n    ```", "```py\n    import numpy as np\n    def pick_elites(actions, M_elites):\n        actions = np.array(actions)\n        assert M_elites <= len(actions)\n        assert M_elites > 0\n        results = np.array([black_box_projectile(a)\n                            for a in actions])\n        sorted_ix = np.argsort(results)[-M_elites:][::-1]\n        return actions[sorted_ix], results[sorted_ix]\n    ```", "```py\n    n = 20\n    actions_to_try = random_shooting(n)\n    best_action, best_result = pick_elites(actions_to_try, 1)\n    ```", "```py\nfrom scipy.stats import norm\nN, M_elites, iterations = 5, 2, 3\nactions_to_try = random_shooting(N)\nelite_acts, _ = pick_elites(actions_to_try, M_elites)\nfor r in range(iterations - 1):\n    mu, std = norm.fit(elite_acts)\n    actions_to_try = np.clip(norm.rvs(mu, std, N), 0, 90)\n    elite_acts, elite_results = pick_elites(actions_to_try, \n                                                    M_elites)            \nbest_action, _ = norm.fit(elite_acts)\n```", "```py\n--iteration: 1\nactions_to_try: [29.97 3.56 57.8 5.83 74.15]\nelites: [57.8  29.97]\n--iteration: 2\nfitted normal mu: 43.89, std: 13.92\nactions_to_try: [26.03 52.85 36.69 54.67 25.06]\nelites: [52.85 36.69]\n--iteration: 3\nfitted normal mu: 44.77, std: 8.08\nactions_to_try: [46.48 34.31 56.56 45.33 48.31]\nelites: [45.33 46.48]\nThe best action: 45.91\n```", "```py\n@ray.remote\ndef rollout(env, dist, args):\n    if dist == \"Bernoulli\":\n        actions = np.random.binomial(**args)\n    else:\n        raise ValueError(\"Unknown distribution\")\n    sampled_reward = 0\n    for a in actions:\n        obs, reward, done, info = env.step(a)\n        sampled_reward += reward\n        if done:\n            break\n    return actions, sampled_reward\n```", "```py\n        def cross_ent_optimizer(self):\n            n_elites = int(np.ceil(self.num_parallel * \\\n                                       self.elite_frac))\n            if self.dist == \"Bernoulli\":\n                p = [0.5] * self.look_ahead\n    ```", "```py\n                for i in range(self.opt_iters):\n                    futures = []\n                    for j in range(self.num_parallel):\n                        args = {\"n\": 1, \"p\": p, \n                                \"size\": self.look_ahead}\n                        fid = \\\n                           rollout.remote(copy.deepcopy(self.env), \n                                                  self.dist, args)\n                        futures.append(fid)\n                    results = [tuple(ray.get(id)) \n                                           for id in futures]\n                    sampled_rewards = [r for _, r in results]\n                    elite_ix = \\\n                         np.argsort(sampled_rewards)[-n_elites:]\n                    elite_actions = np.array([a for a, \n                                        _ in results])[elite_ix]\n                    p = np.mean(elite_actions, axis=0)\n    ```", "```py\n                actions = np.random.binomial(n=1, p=p,\n                                             size=self.look_ahead)\n    ```", "```py\n    def cma_es_optimizer(self):\n        es = cma.CMAEvolutionStrategy([0] \\\n                                      * self.n_tot_actions, 1)\n        while (not es.stop()) and \\\n                es.result.iterations <= self.opt_iter:\n            X = es.ask()  # get list of new solutions\n            futures = [\n                rollout.remote(self.env, x,\n                               self.n_actions, \n                               self.look_ahead)\n                for x in X\n            ]\n            costs = [-ray.get(id) for id in futures]\n            es.tell(X, costs)  # feed values\n            es.disp()\n        actions = [\n            es.result.xbest[i * self.n_actions : \\\n                            (i + 1) * self.n_actions]\n            for i in range(self.look_ahead)\n        ]\n        return actions\n```", "```py\n(7_w,15)-aCMA-ES (mu_w=4.5,w_1=34%) in dimension 40 (seed=836442, Mon Nov 30 05:46:55 2020)\nIterat #Fevals   function value  axis ratio  sigma  min&max std  t[m:s]\n    1     15 7.467667956594279e-01 1.0e+00 9.44e-01  9e-01  9e-01 0:00.0\n    2     30 8.050216186274498e-01 1.1e+00 9.22e-01  9e-01  9e-01 0:00.1\n    3     45 7.222612141709712e-01 1.1e+00 9.02e-01  9e-01  9e-01 0:00.1\n   71   1065 9.341667377266198e-01 1.8e+00 9.23e-01  9e-01  1e+00 0:03.1\n  100   1500 8.486571756945928e-01 1.8e+00 7.04e-01  7e-01  8e-01 0:04.3\nEpisode 0, reward: -121.5869865603307\n```"]