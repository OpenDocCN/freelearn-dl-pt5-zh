<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer207">
<h1 class="chapter-number" id="_idParaDest-185"><a id="_idTextAnchor185"/>7</h1>
<h1 id="_idParaDest-186"><a id="_idTextAnchor186"/>Deep Learning Regularization</h1>
<p>In this chapter, we will cover several tricks and techniques to regularize neural networks. We will reuse the L2 regularization technique, as we did in linear models, for example. But there are other techniques not yet presented in this book, such as early stopping and dropout, which will be covered in <span class="No-Break">this chapter.</span></p>
<p>In this chapter, we’ll look at the <span class="No-Break">following recipes:</span></p>
<ul>
<li>Regularizing a neural network with <span class="No-Break">L2 regularization</span></li>
<li>Regularizing a neural network with <span class="No-Break">early stopping</span></li>
<li>Regularization with <span class="No-Break">network architecture</span></li>
<li>Regularizing <span class="No-Break">with dropout</span></li>
</ul>
<h1 id="_idParaDest-187"><a id="_idTextAnchor187"/>Technical requirements</h1>
<p>In this chapter, we will train neural networks on various tasks. This will require us to use the <span class="No-Break">following libraries:</span></p>
<ul>
<li><span class="No-Break">NumPy</span></li>
<li><span class="No-Break">Scikit-learn</span></li>
<li><span class="No-Break">Matplotlib</span></li>
<li><span class="No-Break">PyTorch</span></li>
<li><span class="No-Break">torchvision</span></li>
</ul>
<h1 id="_idParaDest-188"><a id="_idTextAnchor188"/>Regularizing a neural network with L2 regularization</h1>
<p>Just like a linear model, whether it be a linear regression<a id="_idIndexMarker384"/> or a logistic regression, neural<a id="_idIndexMarker385"/> networks have weights. And so, just like a linear model, L2 penalization can be used on those weights to regularize the neural network. In this recipe, we will apply L2 penalization to a neural network on the MNIST handwritten <span class="No-Break">digits dataset.</span></p>
<p>As a reminder, when training a neural network on this task in <a href="B19629_06.xhtml#_idTextAnchor162"><span class="No-Break"><em class="italic">Chapter 6</em></span></a>, there was a small overfitting after 20 epochs, and the results were an accuracy of 97% on the train set and 95% on the test set. Let’s try to reduce this overfitting by adding L2 regularization in <span class="No-Break">this recipe.</span></p>
<h2 id="_idParaDest-189"><a id="_idTextAnchor189"/>Getting ready</h2>
<p>Just like for linear models, L2 regularization is just adding a new L2 term to the loss. Given the weights W=w1,w2,..., the added term to the loss would be <img alt="" height="26" src="image/Formula_07_001.png" width="115"/>. The consequence of this added term to the loss is that the weights are more constrained and must stay close to zero to keep the loss small. As a result, it adds bias to the model and then can help <span class="No-Break">regularize it.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">This notation for the weights is simplified here. Actually, there are weights <img alt="" height="28" src="image/Formula_07_002.png" width="43"/> for each unit <strong class="source-inline">i</strong>, each feature <strong class="source-inline">j</strong>, and each layer <strong class="source-inline">l</strong>. But in the end, the L2 term remains the sum of all the <span class="No-Break">squared weights.</span></p>
<p>For this recipe, only three libraries <span class="No-Break">are needed:</span></p>
<ul>
<li><strong class="source-inline">matplotlib</strong> <span class="No-Break">for plots</span></li>
<li><strong class="source-inline">pytorch</strong> for <span class="No-Break">deep learning</span></li>
<li><strong class="source-inline">torchvision</strong> for the <span class="No-Break">MNIST dataset</span></li>
</ul>
<p>These can be installed with <strong class="source-inline">pip install matplotlib </strong><span class="No-Break"><strong class="source-inline">torch torchvision</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-190"><a id="_idTextAnchor190"/>How to do it...</h2>
<p>In this recipe, we reuse<a id="_idIndexMarker386"/> the exact same code as in the previous<a id="_idIndexMarker387"/> chapter when training a multiclass classification model on the MNIST dataset. The only difference will be at <em class="italic">step 6</em> – feel free to jump there <span class="No-Break">if needed.</span></p>
<p>The input data is the MNIST handwritten dataset: grayscale images of 28x28 pixels. The data will thus need to be rescaled and flattened before being able to train a custom <span class="No-Break">neural network:</span></p>
<ol>
<li>Import the required libraries. As in previous recipes, we import several useful <strong class="source-inline">torch</strong> modules <span class="No-Break">and functions:</span><ul><li><span class="No-Break"><strong class="source-inline">torch</strong></span></li><li><strong class="source-inline">torch.nn</strong> containing required classes for building a <span class="No-Break">neural network</span></li><li><strong class="source-inline">torch.nn.functional</strong> for activation functions such <span class="No-Break">as ReLU</span></li><li><strong class="source-inline">DataLoader</strong> for handling <span class="No-Break">the data</span></li></ul></li>
</ol>
<p>And we have some imports <span class="No-Break">from </span><span class="No-Break"><strong class="source-inline">torchvision</strong></span><span class="No-Break">:</span></p>
<ul>
<li><strong class="source-inline">MNIST</strong> for loading <span class="No-Break">the dataset</span></li>
<li><strong class="source-inline">transforms</strong> for transforming the dataset – both rescaling and flattening <span class="No-Break">the data:</span><pre class="source-code">
import torch</pre><pre class="source-code">
import torch.nn as nn</pre><pre class="source-code">
import torch.nn.functional as F</pre><pre class="source-code">
from torch.utils.data import DataLoader</pre><pre class="source-code">
from torchvision.datasets import MNIST</pre><pre class="source-code">
import torchvision.transforms as transforms</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre></li>
</ul>
<ol>
<li value="2">Instantiate the transformations. The <strong class="source-inline">Compose</strong> class is used here to compose <span class="No-Break">three transformations:</span><ul><li><strong class="source-inline">transforms.ToTensor()</strong>: Convert the input image in to <span class="No-Break"><strong class="source-inline">torch.Tensor</strong></span><span class="No-Break"> format</span></li><li><strong class="source-inline">transforms.Normalize()</strong>: Normalize the image<a id="_idIndexMarker388"/> with the mean value<a id="_idIndexMarker389"/> and standard deviation. Will subtract the mean (i.e., <strong class="source-inline">0.1307</strong>) and then divide it by the standard deviation (i.e., <strong class="source-inline">0.3081</strong>) for each <span class="No-Break">pixel value.</span></li><li><strong class="source-inline">transforms.Lambda(torch.flatten)</strong>: Flatten the 2D tensor in to a <span class="No-Break">1D tensor:</span></li></ul></li>
</ol>
<p>Here is <span class="No-Break">the code:</span></p>
<pre class="source-code">
transform = transforms.Compose([transforms.ToTensor(),
    transforms.Normalize((0.1307), (0.3081)),
    transforms.Lambda(torch.flatten)])</pre>
<p class="callout-heading">Note</p>
<p class="callout">Images are commonly normalized with a mean and standard deviation of 0.5. We normalize with those specific values because the dataset is made with specific images, but 0.5 would work <span class="No-Break">fine too.</span></p>
<ol>
<li value="3">Load the train and test sets, as well as the train and test data loaders. Using the <strong class="source-inline">MNIST</strong> class, we both get the train and test sets using the <strong class="source-inline">train=True</strong> and <strong class="source-inline">train=False</strong> parameters, respectively. We apply the previously defined transformations directly while loading the data with the <strong class="source-inline">MNIST</strong> class too. Then we instantiate the data loaders with a batch size <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">64</strong></span><span class="No-Break">:</span><pre class="source-code">
trainset = MNIST('./data', train=True, download=True,</pre><pre class="source-code">
    transform=transform)</pre><pre class="source-code">
train_dataloader = DataLoader(trainset, batch_size=64,</pre><pre class="source-code">
    shuffle=True)</pre><pre class="source-code">
testset = MNIST('./data', train=False, download=True,</pre><pre class="source-code">
    transform=transform)</pre><pre class="source-code">
test_dataloader = DataLoader(testset, batch_size=64,</pre><pre class="source-code">
    shuffle=True)</pre></li>
<li>Define the neural network. We define<a id="_idIndexMarker390"/> here, by default, a neural<a id="_idIndexMarker391"/> network made of 2 hidden layers of 24 units. The output layer has 10 units since there are 10 classes (digits between 0 and 9). Finally, the <strong class="source-inline">softmax</strong> function is applied to the output layer, allowing the sum of the 10 units to be strictly equal <span class="No-Break">to </span><span class="No-Break"><strong class="source-inline">1</strong></span><span class="No-Break">:</span><pre class="source-code">
class Net(nn.Module):</pre><pre class="source-code">
    def __init__(self, input_shape: int,</pre><pre class="source-code">
    hidden_units: int = 24):</pre><pre class="source-code">
        super(Net, self).__init__()</pre><pre class="source-code">
        self.hidden_units = hidden_units</pre><pre class="source-code">
        self.fc1 = nn.Linear(input_shape,</pre><pre class="source-code">
            self.hidden_units)</pre><pre class="source-code">
        self.fc2 = nn.Linear(self.hidden_units,</pre><pre class="source-code">
            self.hidden_units)</pre><pre class="source-code">
        self.output = nn.Linear(self.hidden_units, 10)</pre><pre class="source-code">
    def forward(self,</pre><pre class="source-code">
        x: torch.Tensor) -&gt; torch.Tensor:</pre><pre class="source-code">
            x = self.fc1(x)</pre><pre class="source-code">
            x = F.relu(x)</pre><pre class="source-code">
            x = self.fc2(x)</pre><pre class="source-code">
            x = F.relu(x)</pre><pre class="source-code">
            output = torch.softmax(self.output(x), dim=1)</pre><pre class="source-code">
            return output</pre></li>
<li>To check the code, we instantiate <a id="_idIndexMarker392"/>the model with the right<a id="_idIndexMarker393"/> input shape of <strong class="source-inline">784</strong> (28x28 pixels) and check the forward propagation works properly on a given <span class="No-Break">random tensor:</span><pre class="source-code">
# Instantiate the model</pre><pre class="source-code">
net = Net(784)</pre><pre class="source-code">
# Generate randomly one random 28x28 image as a 784 values tensor</pre><pre class="source-code">
random_data = torch.rand((1, 784))</pre><pre class="source-code">
result = net(random_data)</pre><pre class="source-code">
print('Resulting output tensor:', result)</pre><pre class="source-code">
print('Sum of the output tensor:', result.sum())</pre></li>
</ol>
<p>The code output would be something like the following (only the sum must be equal to 1; other numbers may <span class="No-Break">be different):</span></p>
<pre class="source-code">
<strong class="bold">Resulting output tensor: tensor([[0.0882, 0.1141, 0.0846, 0.0874, 0.1124, 0.0912, 0.1103, 0.0972, 0.1097,</strong>
<strong class="bold">         0.1048]], grad_fn=&lt;SoftmaxBackward0&gt;)</strong>
<strong class="bold">Sum of the output tensor: tensor(1.0000, grad_fn=&lt;SumBackward0&gt;)</strong></pre>
<ol>
<li value="6">Define the loss function as the cross-entropy loss, available as <strong class="source-inline">nn.CrossEntropyLoss()</strong> in <strong class="source-inline">pytorch</strong>, and the optimizer as <strong class="source-inline">Adam</strong>. Here we set another<a id="_idIndexMarker394"/> parameter to the <strong class="source-inline">Adam</strong> optimizer: <strong class="source-inline">weight_decay=0.001</strong>. This parameter is the strength <a id="_idIndexMarker395"/>of the L2 penalization. By default, <strong class="source-inline">weight_decay</strong> is <strong class="source-inline">0</strong>, meaning there is no L2 penalization. A higher value means a higher regularization, just like in linear models <span class="No-Break">in scikit-learn:</span><pre class="source-code">
criterion = nn.CrossEntropyLoss()</pre><pre class="source-code">
optimizer = torch.optim.Adam(net.parameters(),</pre><pre class="source-code">
    lr=0.001, weight_decay=0.001)</pre></li>
<li>Instantiate the <strong class="source-inline">epoch_step</strong> helper function allowing to compute forward and backward propagation (for the training set only) as well as the loss <span class="No-Break">and accuracy:</span><pre class="source-code">
def epoch_step(net, dataloader, training_set: bool):</pre><pre class="source-code">
    running_loss = 0.</pre><pre class="source-code">
    correct = 0.</pre><pre class="source-code">
    for i, data in enumerate(dataloader, 0):</pre><pre class="source-code">
        # Get the inputs: data is a list of [inputs, labels]</pre><pre class="source-code">
        inputs, labels = data</pre><pre class="source-code">
        if training_set:</pre><pre class="source-code">
            # Zero the parameter gradients</pre><pre class="source-code">
            optimizer.zero_grad()</pre><pre class="source-code">
        # Forward + backward + optimize</pre><pre class="source-code">
        outputs = net(inputs)</pre><pre class="source-code">
        loss = criterion(outputs, labels)</pre><pre class="source-code">
        if training_set:</pre><pre class="source-code">
            loss.backward()</pre><pre class="source-code">
            optimizer.step()</pre><pre class="source-code">
        # Add correct predictions for this batch</pre><pre class="source-code">
        correct += (outputs.argmax(</pre><pre class="source-code">
            dim=1) == labels).float().sum()</pre><pre class="source-code">
        # Compute loss for this batch</pre><pre class="source-code">
        running_loss += loss.item()</pre><pre class="source-code">
    return running_loss, correct</pre></li>
<li>We can finally train the neural network on 20 epochs and compute the loss and accuracy for <span class="No-Break">each epoch.</span></li>
</ol>
<p>Since we both train<a id="_idIndexMarker396"/> on the train set and evaluate<a id="_idIndexMarker397"/> on the test set, the model is switched to <strong class="source-inline">train</strong> mode with <strong class="source-inline">model.train()</strong> before training, whereas before evaluating on the test set, it is switched to <strong class="source-inline">eval</strong> mode <span class="No-Break">with </span><span class="No-Break"><strong class="source-inline">model.eval()</strong></span><span class="No-Break">:</span></p>
<pre class="source-code">
# Create empty lists to store the losses and accuracies
train_losses = []
test_losses = []
train_accuracy = []
test_accuracy = []
# Loop over the dataset 20 times for 20 epochs
for epoch in range(20):
    ## Train the model on the training set
    running_train_loss, correct = epoch_step(net,
        dataloader=train_dataloader,
        training_set=True)
    # Compute and store loss and accuracy for this epoch
    train_epoch_loss = running_train_loss / len(
        train_dataloader)
    train_losses.append(train_epoch_loss)
    train_epoch_accuracy = correct / len(trainset)
    train_accuracy.append(train_epoch_accuracy)
    ## Evaluate the model on the test set
    #running_test_loss = 0.
    #correct = 0.
    net.eval()
    with torch.no_grad():
        running_test_loss, correct = epoch_step(net,
            dataloader=test_dataloader,
            training_set=False)
        test_epoch_loss = running_test_loss / len(
            test_dataloader)
        test_losses.append(test_epoch_loss)
        test_epoch_accuracy = correct / len(testset)
        test_accuracy.append(test_epoch_accuracy)
    # Print stats
    print(f'[epoch {epoch + 1}] Training: loss={
        train_epoch_loss:.3f}accuracy={
            train_epoch_accuracy:.3f} |\
            \t Test: loss={test_epoch_loss:.3f}
            accuracy={test_epoch_accuracy:.3f}')
print('Finished Training')</pre>
<p>On the last epoch, the output should look like <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">[epoch 20] Training: loss=1.505 accuracy=0.964 |   Test: loss=1.505 accuracy=0.962</strong>
<strong class="bold">Finished Training</strong></pre>
<ol>
<li value="9">For visualization purposes, we can plot<a id="_idIndexMarker398"/> the loss for both the train<a id="_idIndexMarker399"/> and test sets as a function of <span class="No-Break">the epoch:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('loss (CE)')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer192">
<img alt="Figure 7.1 – Cross-entropy loss as a function of the epoch; output from the previous code" height="408" src="image/B19629_07_01.jpg" width="555"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.1 – Cross-entropy loss as a function of the epoch; output from the previous code</p>
<p>We can notice that the loss seems<a id="_idIndexMarker400"/> to be almost <a id="_idIndexMarker401"/>the same for both the training and test set, with no clear divergence. In the previous attempts without L2 penalization, the losses were further apart from each other, meaning we effectively regularized <span class="No-Break">the model.</span></p>
<ol>
<li value="10">Showing related results, we can do it with <span class="No-Break">accuracy too:</span><pre class="source-code">
plt.plot(train_accuracy, label='train')</pre><pre class="source-code">
plt.plot(test_accuracy, label='test')</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('Accuracy')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer193">
<img alt="Figure 7.2 – Accuracy as a function of the epoch; output from the previous code" height="408" src="image/B19629_07_02.jpg" width="554"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.2 – Accuracy as a function of the epoch; output from the previous code</p>
<p>At the end, the accuracy is about 96% for both the train<a id="_idIndexMarker402"/> set and the test set, with<a id="_idIndexMarker403"/> no <span class="No-Break">significant overfitting.</span></p>
<h2 id="_idParaDest-191"><a id="_idTextAnchor191"/>There’s more...</h2>
<p>Even if L2 regularization is a quite common technique to regularize linear models such as linear regression and logistic regression, it is not usually the first choice with deep learning. Other methods such as early stopping or dropout are <span class="No-Break">usually preferred.</span></p>
<p>On another note, in this recipe, we keep mentioning only the train and test sets. But to optimize the <strong class="source-inline">weight_decay</strong> hyperparameter properly, it is required to use a validation set; otherwise, the results will be biased. We have simplified this recipe by having only two sets to keep <span class="No-Break">it concise.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Generally speaking, in deep learning, any other hyperparameter optimization, such as the number of layers, number of units, activation functions, and so on must be optimized for the validation set too, not just for the <span class="No-Break">test set.</span></p>
<h2 id="_idParaDest-192"><a id="_idTextAnchor192"/>See also</h2>
<p>It may seem strange to adjust the L2 penalization through the optimizer of the model rather than directly in the loss function, and indeed <span class="No-Break">it is.</span></p>
<p>Of course, it would be possible to manually add an L2 penalization, but it would probably be suboptimal. See this PyTorch thread for more about this design choice, as well as an example of adding L1 <span class="No-Break">penalization: </span><a href="https://discuss.pytorch.org/t/simple-l2-regularization/139"><span class="No-Break">https://discuss.pytorch.org/t/simple-l2-regularization/139</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-193"><a id="_idTextAnchor193"/>Regularizing a neural network with early stopping</h1>
<p>Early stopping is a commonly<a id="_idIndexMarker404"/> employed approach in deep learning to prevent the overfitting of models. The concept is straightforward yet effective: if the model is overfitting due to prolonged training epochs, we terminate the training prematurely to prevent overfitting. We can utilize this technique on the breast <span class="No-Break">cancer dataset.</span></p>
<h2 id="_idParaDest-194"><a id="_idTextAnchor194"/>Getting ready</h2>
<p>In a perfect world, there is no need for regularization. What that means is that for both the train and validation sets, the losses are almost perfectly equal, for any number of epochs, as in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.3</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer194">
<img alt="Figure 7.3 – Example with no overfitting of train and valid losses as a function of the number of epochs" height="853" src="image/B19629_07_03.jpg" width="1093"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.3 – Example with no overfitting of train and valid losses as a function of the number of epochs</p>
<p>But it’s not always that perfect. In practice, it may happen that the neural network is learning more and more about the data distribution of the train set at every epoch, at the cost of the generalization to new data. This case is depicted by the example in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.4</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer195">
<img alt="Figure 7.4 – Example with overfitting of train and valid losses as a function of the number of epochs" height="853" src="image/B19629_07_04.jpg" width="1093"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.4 – Example with overfitting of train and valid losses as a function of the number of epochs</p>
<p>When dealing with such<a id="_idIndexMarker405"/> a scenario, a natural solution would be to halt<a id="_idIndexMarker406"/> the training process once the <strong class="bold">valid</strong> loss of the model stops decreasing. Once the validation loss of the model stops decreasing, continuing to train the model for additional epochs may cause it to become better at memorizing the training data, rather than improving its ability<a id="_idIndexMarker407"/> to make accurate predictions on new, unseen data. This technique is <strong class="bold">called early stopping</strong> and allows to prevent a model <span class="No-Break">from overfitting.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer196">
<img alt="Figure 7.5 – As soon as the valid loss stops decreasing, we can stop the learning and consider the model fully trained; this is early stopping" height="853" src="image/B19629_07_05.jpg" width="1093"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.5 – As soon as the valid loss stops decreasing, we can stop the learning and consider the model fully trained; this is early stopping</p>
<p>Since this recipe will be applied to the breast cancer dataset, scikit-learn must be installed, along with <strong class="source-inline">torch</strong> for the models and <strong class="source-inline">matplotlib</strong> for visualization. These libraries can<a id="_idIndexMarker408"/> be installed with <strong class="source-inline">pip install sklearn </strong><span class="No-Break"><strong class="source-inline">torch matplotlib</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-195"><a id="_idTextAnchor195"/>How to do it...</h2>
<p>In this recipe, we will first train a neural network on the breast cancer dataset and visualize the overfitting effect amplifying with the number of epochs. Then, we will implement early stopping, <span class="No-Break">to regularize.</span></p>
<h3>Regular training</h3>
<p>Since the breast cancer dataset<a id="_idIndexMarker409"/> is rather small, instead of splitting the dataset into train, valid, and test sets, we will consider only the train and <span class="No-Break">valid sets:</span></p>
<ol>
<li>Import the needed libraries from <strong class="source-inline">scikit-learn</strong>, <strong class="source-inline">matplotlib</strong>, <span class="No-Break">and </span><span class="No-Break"><strong class="source-inline">torch</strong></span><span class="No-Break">:</span><ul><li><strong class="source-inline">load_breast_cancer</strong> to load <span class="No-Break">the dataset</span></li><li><strong class="source-inline">train_test_split</strong> to split the data into training and <span class="No-Break">validation sets</span></li><li><strong class="source-inline">StandardScaler</strong> to rescale the <span class="No-Break">quantitative data</span></li><li><strong class="source-inline">accuracy_score</strong> to evaluate <span class="No-Break">the model</span></li><li><strong class="source-inline">matplotlib</strong> <span class="No-Break">for display</span></li><li><span class="No-Break"><strong class="source-inline">torch</strong></span><span class="No-Break"> itself</span></li><li><strong class="source-inline">torch.nn</strong> containing required classes for building a <span class="No-Break">neural network</span></li><li><strong class="source-inline">torch.nn.functional</strong> for activation functions such <span class="No-Break">as ReLU</span></li><li><strong class="source-inline">Dataset</strong> and <strong class="source-inline">DataLoader</strong> for handling <span class="No-Break">the data</span></li></ul></li>
</ol>
<p>Here is the code <span class="No-Break">for it:</span></p>
<pre class="source-code">
import numpy as np
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader</pre>
<ol>
<li value="2">Load the features <a id="_idIndexMarker410"/>and labels with the <span class="No-Break"><strong class="source-inline">load_breast_cancer</strong></span><span class="No-Break"> function:</span><pre class="source-code">
X, y = load_breast_cancer(return_X_y=True)</pre></li>
<li>Split the data into training and validation sets, specifying the random state for reproducibility, and convert the features and labels in to <strong class="source-inline">float32</strong> for later compatibility <span class="No-Break">with PyTorch:</span><pre class="source-code">
X_train, X_val, y_train, y_val = train_test_split(</pre><pre class="source-code">
    X.astype(np.float32), y.astype(np.float32),</pre><pre class="source-code">
    test_size=0.2, random_state=0)</pre></li>
<li>Create the <strong class="source-inline">Dataset</strong> class for handling the data. We are simply reusing the class implemented in the <span class="No-Break">previous chapter:</span><pre class="source-code">
class BreastCancerDataset(Dataset):</pre><pre class="source-code">
    def __init__(self, X: np.array, y: np.array,</pre><pre class="source-code">
        x_scaler: StandardScaler = None):</pre><pre class="source-code">
            if x_scaler is None:</pre><pre class="source-code">
                self.x_scaler = StandardScaler()</pre><pre class="source-code">
                X = self.x_scaler.fit_transform(X)</pre><pre class="source-code">
            else:</pre><pre class="source-code">
                self.x_scaler = x_scaler</pre><pre class="source-code">
                X = self.x_scaler.transform(X)</pre><pre class="source-code">
            self.X = torch.from_numpy(X)</pre><pre class="source-code">
            self.y = torch.from_numpy(y)</pre><pre class="source-code">
    def __len__(self) -&gt; int:</pre><pre class="source-code">
        return len(self.X)</pre><pre class="source-code">
    def __getitem__(self, idx: int) -&gt; tuple[torch.Tensor]:</pre><pre class="source-code">
        return self.X[idx], self.y[idx]</pre></li>
<li>Instantiate the training<a id="_idIndexMarker411"/> and validation sets and loaders for PyTorch. Notice that we provide the training scaler when instantiating the validation dataset to make sure the scaler used with both datasets is the one fitted on the <span class="No-Break">training set:</span><pre class="source-code">
training_data = BreastCancerDataset(X_train, y_train)</pre><pre class="source-code">
val_data = BreastCancerDataset(X_val, y_val,</pre><pre class="source-code">
    training_data.x_scaler)</pre><pre class="source-code">
train_dataloader = DataLoader(training_data,</pre><pre class="source-code">
    batch_size=64, shuffle=True)</pre><pre class="source-code">
val_dataloader = DataLoader(val_data, batch_size=64,</pre><pre class="source-code">
    shuffle=True)</pre></li>
<li>Define the neural network architecture – 2 hidden layers of 36 units and an output layer with 1 unit with a sigmoid activation function since it’s a binary <span class="No-Break">classification task:</span><pre class="source-code">
class Net(nn.Module):</pre><pre class="source-code">
    def __init__(self, input_shape: int,</pre><pre class="source-code">
        hidden_units: int = 36):</pre><pre class="source-code">
            super(Net, self).__init__()</pre><pre class="source-code">
            self.hidden_units = hidden_units</pre><pre class="source-code">
            self.fc1 = nn.Linear(input_shape,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.fc2 = nn.Linear(self.hidden_units,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.output = nn.Linear(self.hidden_units,</pre><pre class="source-code">
                1)</pre><pre class="source-code">
    def forward(self, x: torch.Tensor) -&gt;</pre><pre class="source-code">
        torch.Tensor:</pre><pre class="source-code">
            x = self.fc1(x)</pre><pre class="source-code">
            x = F.relu(x)</pre><pre class="source-code">
            x = self.fc2(x)</pre><pre class="source-code">
            x = F.relu(x)</pre><pre class="source-code">
            output = torch.sigmoid(self.output(x))</pre><pre class="source-code">
            return output</pre></li>
<li>Instantiate the model<a id="_idIndexMarker412"/> with the expected input shape (the number of features). Optionally, we can check the forward propagation works properly on a given <span class="No-Break">random tensor:</span><pre class="source-code">
# Instantiate the model</pre><pre class="source-code">
net = Net(X_train.shape[1])</pre><pre class="source-code">
# Generate randomly one random 28x28 image as a 784 values tensor</pre><pre class="source-code">
random_data = torch.rand((1, X_train.shape[1]))</pre><pre class="source-code">
result = net(random_data)</pre><pre class="source-code">
print('Resulting output tensor:', result)</pre></li>
</ol>
<p>The output of this code<a id="_idIndexMarker413"/> is the following (the value itself may change, but will be between 0 and 1 since it’s a sigmoid activation function on the <span class="No-Break">last layer):</span></p>
<pre class="source-code">
<strong class="bold">Resulting output tensor: tensor([[0.5674]], grad_fn=&lt;SigmoidBackward0&gt;)</strong></pre>
<ol>
<li value="8">Define the loss function as the binary cross entropy loss since this is a binary classification task. Instantiate the <span class="No-Break">optimizer too:</span><pre class="source-code">
criterion = nn.BCELoss()</pre><pre class="source-code">
optimizer = torch.optim.Adam(net.parameters(),</pre><pre class="source-code">
    lr=0.001)</pre></li>
<li>Implement a helper function, <strong class="source-inline">epoch_step</strong>, that computes forward propagation, backpropagation (for the training set), loss, and accuracy for <span class="No-Break">one epoch:</span><pre class="source-code">
def epoch_step(net, dataloader, training_set: bool):</pre><pre class="source-code">
    running_loss = 0.</pre><pre class="source-code">
    correct = 0.</pre><pre class="source-code">
    for i, data in enumerate(dataloader, 0):</pre><pre class="source-code">
        # Get the inputs: data is a list of [inputs, labels]</pre><pre class="source-code">
        inputs, labels = data</pre><pre class="source-code">
        labels = labels.unsqueeze(1)</pre><pre class="source-code">
        if training_set:</pre><pre class="source-code">
            # Zero the parameter gradients</pre><pre class="source-code">
            optimizer.zero_grad()</pre><pre class="source-code">
        # Forward + backward + optimize</pre><pre class="source-code">
        outputs = net(inputs)</pre><pre class="source-code">
        loss = criterion(outputs, labels)</pre><pre class="source-code">
        if training_set:</pre><pre class="source-code">
            loss.backward()</pre><pre class="source-code">
            optimizer.step()</pre><pre class="source-code">
        # Add correct predictions for this batch</pre><pre class="source-code">
        correct += ((</pre><pre class="source-code">
            outputs &gt; 0.5) == labels).float().sum()</pre><pre class="source-code">
        # Compute loss for this batch</pre><pre class="source-code">
        running_loss += loss.item()</pre><pre class="source-code">
    return running_loss, correct</pre></li>
<li>Let’s now implement the <strong class="source-inline">train_model</strong> function allowing us to train a model, with or without<a id="_idIndexMarker414"/> patience. This function stores each epoch and then returns <span class="No-Break">the following:</span><ul><li>The loss and accuracy for the <span class="No-Break">train set</span></li><li>The loss and accuracy for the <span class="No-Break">valid set</span></li></ul></li>
</ol>
<p>Here is the code for <span class="No-Break">the model:</span></p>
<pre class="source-code">
def train_model(net, train_dataloader, val_dataloader, criterion, optimizer, epochs, patience=None):
    # Create empty lists to store the losses and accuracies
    train_losses = []
    val_losses = []
    train_accuracy = []
    val_accuracy = []
    best_val_loss = np.inf
    best_val_loss_epoch = 0
    # Loop over the dataset 20 times for 20 epochs
    for epoch in range(500):
        ## If the best epoch was more than the patience, just stop training
        if patience is not None and epoch - best_val_loss_epoch &gt; patience:
            break
        ## Train the model on the training set
        net.train()
        running_train_loss, correct = epoch_step(net,
            dataloader=train_dataloader,
            training_set=True)
        # Compute and store loss and accuracy for this epoch
        train_epoch_loss = running_train_loss / len(
            train_dataloader)
        train_losses.append(train_epoch_loss)
        train_epoch_accuracy = correct / len(training_data)
        train_accuracy.append(train_epoch_accuracy)
        ## Evaluate the model on the val set
        net.eval()
        with torch.no_grad():
            running_val_loss, correct = epoch_step(
                net, dataloader=val_dataloader,
                training_set=False)
            val_epoch_loss = running_val_loss / len(
                val_dataloader)
            val_losses.append(val_epoch_loss)
            val_epoch_accuracy = correct / len(val_data)
            val_accuracy.append(val_epoch_accuracy)
            # If the loss is better than the current best, update it
            if best_val_loss &gt;= val_epoch_loss:
                best_val_loss = val_epoch_loss
                best_val_loss_epoch = epoch + 1
        # Print stats
        print(f'[epoch {epoch + 1}] Training: loss={
            train_epoch_loss:.3f} accuracy={
            train_epoch_accuracy:.3f} |\
                \t Valid: loss={val_epoch_loss:.3f}
                accuracy={val_epoch_accuracy:.3f}')
    return train_losses, val_losses, train_accuracy,
        val_accuracy</pre>
<p>Let’s now train the neural network<a id="_idIndexMarker415"/> on 500 epochs reusing the previously implemented <strong class="source-inline">train_model</strong> function. Here is the code <span class="No-Break">for it:</span></p>
<pre class="source-code">
train_losses, val_losses, train_accuracy,
    val_accuracy = train_model(
        net, train_dataloader, val_dataloader,
        criterion, optimizer, epochs=500
)</pre>
<p>After 500 epochs, the code output will be something like <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">[epoch 500] Training: loss=0.000 accuracy=1.000 |</strong>   <strong class="bold">Validation: loss=0.099 accuracy=0.965</strong></pre>
<ol>
<li value="11">We can now plot the loss for both training and validation sets, as a function of the epoch, and visualize the overfitting effect increasing with the number <span class="No-Break">of epochs:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(val_losses, label='valid')</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('loss (CE)')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer197">
<img alt="Figure 7.6 – Cross-entropy loss as a function of the epoch. (despite a few bumps, the training loss keeps decreasing)" height="413" src="image/B19629_07_06.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.6 – Cross-entropy loss as a function of the epoch. (despite a few bumps, the training loss keeps decreasing)</p>
<p>We indeed have a training<a id="_idIndexMarker416"/> loss that keeps decreasing overall, even reaching a value of zero. On the other hand, the valid loss starts decreasing to reach a minimum somewhere around epoch 100 and then increases slowly over <span class="No-Break">the epochs.</span></p>
<p>We can implement early stopping to avoid this situation in <span class="No-Break">several ways:</span></p>
<ul>
<li>After the first training, we could retrain the model up to 100 epochs (or any identified optimal validation loss), hopefully having the same results. This would be a waste of <span class="No-Break">CPU time.</span></li>
<li>We could save the model at every epoch, and then pick the best one afterward. This solution is sometimes implemented but can be a waste of storage memory, especially for <span class="No-Break">large models.</span></li>
<li>We could automatically stop the training after a given number of epochs not improving validation loss. The minimum number of steps without validation loss improvement is usually <span class="No-Break">called patience.</span></li>
</ul>
<p>Let’s now implement<a id="_idIndexMarker417"/> the <span class="No-Break">latter solution.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Using patience is risky too: a too-small patience may get the model stuck in a local minimum, while a too-large patience may miss the actual optimal epoch by stopping <span class="No-Break">too late.</span></p>
<h3>Training with patience and early stopping</h3>
<p>Let’s now retrain a model<a id="_idIndexMarker418"/> using early stopping. We first<a id="_idIndexMarker419"/> instantiate a fresh model to avoid training an already <span class="No-Break">t</span><span class="No-Break">rained model:</span></p>
<ol>
<li>Instantiate a fresh model as well as a fresh optimizer. No need to test it, nor to instantiate the loss again if you are using the same notebook kernel. If you want to run this code separately, <em class="italic">steps 1</em> to <em class="italic">8</em> of the previous recipe must <span class="No-Break">be reused:</span><pre class="source-code">
# Instantiate a fresh model</pre><pre class="source-code">
net = Net(X_train.shape[1])</pre><pre class="source-code">
optimizer = torch.optim.Adam(net.parameters(), lr=0.001)</pre></li>
<li>We now train this model with a patience of <strong class="source-inline">30</strong>. After 30 epochs without improving the <strong class="source-inline">val</strong> loss, the training will <span class="No-Break">just stop:</span><pre class="source-code">
train_losses, val_losses, train_accuracy,</pre><pre class="source-code">
    val_accuracy = train_model(</pre><pre class="source-code">
        net, train_dataloader, val_dataloader,</pre><pre class="source-code">
        criterion, optimizer, patience=30, epochs=500</pre><pre class="source-code">
)</pre></li>
</ol>
<p>The code output will be something like the following (the total number of epochs before reaching the early stopping <span class="No-Break">may vary):</span></p>
<pre class="source-code">
<strong class="bold">[epoch 134] Training: loss=0.004 accuracy=1.000 |</strong>   <strong class="bold">Valid: loss=0.108 accuracy=0.982</strong></pre>
<p>The training<a id="_idIndexMarker420"/> stopped after about 100 epochs (the result may vary since the results are not deterministic by default), with a validation<a id="_idIndexMarker421"/> accuracy of about 98%, far better than the 96% that we got after <span class="No-Break">500 epochs.</span></p>
<ol>
<li value="3">Let’s plot the train and validation losses again as a function of the number <span class="No-Break">of epochs:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(val_losses, label='validation')</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('loss (BCE)')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer198">
<img alt="Figure 7.7 – Cross-entropy loss as a function of the epoch" height="413" src="image/B19629_07_07.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.7 – Cross-entropy loss as a function of the epoch</p>
<p>As we can see, the validation loss<a id="_idIndexMarker422"/> is already overfitting but did not have time to grow too much, preventing<a id="_idIndexMarker423"/> <span class="No-Break">further overfitting.</span></p>
<h2 id="_idParaDest-196"><a id="_idTextAnchor196"/>There’s more...</h2>
<p>As explained earlier in this recipe, for proper evaluation, it would be necessary to compute the accuracy (or any selected evaluation metric) on a separate test set. Indeed, stopping the training based on the validation set and evaluating the model on this same dataset is a biased approach, and may artificially improve <span class="No-Break">the evaluation.</span></p>
<h1 id="_idParaDest-197"><a id="_idTextAnchor197"/>Regularization with network architecture</h1>
<p>In this recipe, we will explore<a id="_idIndexMarker424"/> a less popular, but still sometimes useful, regularization method: adapting the neural network architecture. After reviewing why to use this method and when, we will apply it to the California housing dataset, a <span class="No-Break">regression task.</span></p>
<h2 id="_idParaDest-198"><a id="_idTextAnchor198"/>Getting ready</h2>
<p>Sometimes, the best way to regularize <a id="_idIndexMarker425"/>is not to use any fancy techniques but only common sense. In many cases, it happens that the neural network used is just too large for the input task and dataset. An easy rule of thumb is to have a quick look at the number of parameters in the network (e.g., weights and biases) and compare it to the number of data points: if the ratio is above 1 (i.e., there are more parameters than data points), there is a risk of <span class="No-Break">severe overfitting.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">If transfer learning is used, this rule of thumb no longer applies since the network has been trained on a presumably large <span class="No-Break">enough dataset.</span></p>
<p>If we take a step back and go back to linear models such as linear regression, it is well known that having too many correlated features can deteriorate the model’s performance. It can be the same for neural networks: having too many free parameters will do no good to the performances. So, depending on the task, it is not always required to have dozens of layers; just a few may be enough to get the best performances and avoid overfitting. Let’s check that in practice on the <span class="No-Break">California dataset.</span></p>
<p>To do so, the libraries needed are scikit-learn, Matplotlib, and PyTorch. They can be installed with <strong class="source-inline">pip install sklearn </strong><span class="No-Break"><strong class="source-inline">matplotlib torch</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-199"><a id="_idTextAnchor199"/>How to do it...</h2>
<p>This will be a two-step recipe: first, we will train a large model (compared to the dataset) on the data, to expose the effect of the network on overfitting. Then, we will train another, more adapted model on this same data, hopefully fixing the <span class="No-Break">overfitting issue.</span></p>
<h3>Training a large model</h3>
<p>Here are the steps<a id="_idIndexMarker426"/> to train <span class="No-Break">a model:</span></p>
<ol>
<li>The following imports are <span class="No-Break">needed first:</span><ul><li><strong class="source-inline">fetch_california_housing</strong> to load <span class="No-Break">the dataset</span></li><li><strong class="source-inline">train_test_split</strong> to split the data into training and <span class="No-Break">test sets</span></li><li><strong class="source-inline">StandardScaler</strong> to rescale <span class="No-Break">the features</span></li><li><strong class="source-inline">r2_score</strong> to evaluate the model at <span class="No-Break">the end</span></li><li><strong class="source-inline">matplotlib</strong> to display <span class="No-Break">the loss</span></li><li><strong class="source-inline">torch</strong> itself for some functions at the lower level of <span class="No-Break">the library</span></li><li><strong class="source-inline">torch.nn</strong>, which has many useful classes for building a <span class="No-Break">neural network</span></li><li><strong class="source-inline">torch.nn.functional</strong> for some <span class="No-Break">useful functions</span></li><li><strong class="source-inline">Dataset</strong> and <strong class="source-inline">DataLoader</strong> for handling the <span class="No-Break">data operations</span></li></ul></li>
</ol>
<p>The following <a id="_idIndexMarker427"/>is the code for these <span class="No-Break"><strong class="source-inline">import</strong></span><span class="No-Break"> statements:</span></p>
<pre class="source-code">
import numpy as np
from sklearn.datasets
import fetch_california_housing
from sklearn.model_selection
import train_test_split
from sklearn.preprocessing
import StandardScaler
from sklearn.metrics
import r2_score
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader</pre>
<ol>
<li value="2">Load the data using the <span class="No-Break"><strong class="source-inline">fetch_california_housing</strong></span><span class="No-Break"> function:</span><pre class="source-code">
X, y = fetch_california_housing(return_X_y=True)</pre></li>
<li>Split the data into training and test sets with a ratio of 80%/20%, using the <strong class="source-inline">train_test_split</strong> function. Set a random state for reproducibility. For <strong class="source-inline">pytorch</strong>, the data is converted in to <span class="No-Break"><strong class="source-inline">float32</strong></span><span class="No-Break"> variables:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    X.astype(np.float32), y.astype(np.float32),</pre><pre class="source-code">
    test_size=0.2, random_state=0)</pre></li>
<li>Rescale the data<a id="_idIndexMarker428"/> using the <span class="No-Break">standard scaler:</span><pre class="source-code">
scaler = StandardScaler()</pre><pre class="source-code">
X_train = scaler.fit_transform(X_train)</pre><pre class="source-code">
X_test = scaler.transform(X_test)</pre></li>
<li>Create the <strong class="source-inline">CaliforniaDataset</strong> class, allowing to handle the data. The only transformation here is the conversion from a <strong class="source-inline">numpy</strong> array to a <span class="No-Break"><strong class="source-inline">torch</strong></span><span class="No-Break"> tensor:</span><pre class="source-code">
class CaliforniaDataset(Dataset):</pre><pre class="source-code">
    def __init__(self, X: np.array, y: np.array):</pre><pre class="source-code">
        self.X = torch.from_numpy(X)</pre><pre class="source-code">
        self.y = torch.from_numpy(y)</pre><pre class="source-code">
    def __len__(self) -&gt; int:</pre><pre class="source-code">
        return len(self.X)</pre><pre class="source-code">
    def __getitem__(self, idx: int) -&gt;</pre><pre class="source-code">
        tuple[torch.Tensor]: return self.X[idx], self.y[idx]</pre></li>
<li>Instantiate the datasets for the train and test sets and the data loaders. We define here a batch size of <strong class="source-inline">64</strong> but this can <span class="No-Break">be modified:</span><pre class="source-code">
# Instantiate datasets</pre><pre class="source-code">
training_data = CaliforniaDataset(X_train, y_train)</pre><pre class="source-code">
test_data = CaliforniaDataset(X_test, y_test)</pre><pre class="source-code">
# Instantiate data loaders</pre><pre class="source-code">
train_dataloader = DataLoader(training_data,</pre><pre class="source-code">
    batch_size=64, shuffle=True)</pre><pre class="source-code">
test_dataloader = DataLoader(test_data, batch_size=64,</pre><pre class="source-code">
    shuffle=True)</pre></li>
<li>Create the neural network<a id="_idIndexMarker429"/> architecture. We create a large model here on purpose considering the dataset – 5 hidden layers of <span class="No-Break">128 units:</span><pre class="source-code">
class Net(nn.Module):</pre><pre class="source-code">
    def __init__(self, input_shape: int,</pre><pre class="source-code">
        hidden_units: int = 128):</pre><pre class="source-code">
            super(Net, self).__init__()</pre><pre class="source-code">
            self.hidden_units = hidden_units</pre><pre class="source-code">
            self.fc1 = nn.Linear(input_shape,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.fc2 = nn.Linear(self.hidden_units,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.fc3 = nn.Linear(self.hidden_units,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.fc4 = nn.Linear(self.hidden_units,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.fc5 = nn.Linear(self.hidden_units,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.output = nn.Linear(self.hidden_units, 1)</pre><pre class="source-code">
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:</pre><pre class="source-code">
        x = self.fc1(x)</pre><pre class="source-code">
        x = F.relu(x)</pre><pre class="source-code">
        x = self.fc2(x)</pre><pre class="source-code">
        x = F.relu(x)</pre><pre class="source-code">
        x = self.fc3(x)</pre><pre class="source-code">
        x = F.relu(x)</pre><pre class="source-code">
        x = self.fc4(x)</pre><pre class="source-code">
        x = F.relu(x)</pre><pre class="source-code">
        x = self.fc5(x)</pre><pre class="source-code">
        x = F.relu(x)</pre><pre class="source-code">
        output = self.output(x)</pre><pre class="source-code">
        return output</pre></li>
<li>Instantiate the model<a id="_idIndexMarker430"/> with the given input shape (the number of features). Optionally, we can check the network is correctly created using an input tensor of the expected shape (so here is the number <span class="No-Break">of features):</span><pre class="source-code">
# Instantiate the network</pre><pre class="source-code">
net = Net(X_train.shape[1])</pre><pre class="source-code">
# Generate one random sample of 8 features</pre><pre class="source-code">
random_data = torch.rand((1, X_train.shape[1]))</pre><pre class="source-code">
# Compute the forward propagation</pre><pre class="source-code">
print(net(random_data))</pre><pre class="source-code">
<strong class="bold">tensor([[0.0674]], grad_fn=&lt;AddmmBackward0&gt;)</strong></pre></li>
<li>Instantiate the loss to be a mean squared error loss since this is a regression task, and define the optimizer to be <strong class="source-inline">Adam</strong>, with a learning rate <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">0.001</strong></span><span class="No-Break">:</span><pre class="source-code">
criterion = nn.MSELoss()</pre><pre class="source-code">
optimizer = torch.optim.Adam(net.parameters(), lr=0.001)</pre></li>
<li>Finally, train the neural network <a id="_idIndexMarker431"/>on 500 epochs by using the <strong class="source-inline">train_model</strong> function. The implementation of this function is similar to previous ones and can be found in the GitHub repository. Again, we purposely chose a large number of epochs; otherwise, the overfitting could be compensated by early stopping. We also store the train and test losses for each epoch, for visualization purposes <span class="No-Break">and information:</span><pre class="source-code">
train_losses, test_losses = train_model(net,</pre><pre class="source-code">
    train_dataloader, test_dataloader, criterion,</pre><pre class="source-code">
    optimizer, 500)</pre></li>
</ol>
<p>After 500 epochs, the final output lines will be like <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">[epoch 500] Training: loss=0.013 | Test: loss=0.312</strong>
<strong class="bold">Finished Training</strong></pre>
<ol>
<li value="11">Plot the loss for both the train and test set as a function of <span class="No-Break">the epoch:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('loss (MSE)')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer199">
<img alt="Figure 7.8 – Mean squared error loss as a function of the epoch (note the clear divergence between the train and test losses)" height="413" src="image/B19629_07_08.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.8 – Mean squared error loss as a function of the epoch (note the clear divergence between the train and test losses)</p>
<p>We can notice that the train<a id="_idIndexMarker432"/> loss keeps decreasing over and over, while the test loss soon reaches a plateau before increasing again. This is a clear sign of overfitting. Let’s confirm there is overfitting by computing <span class="No-Break">the R2-scores.</span></p>
<ol>
<li value="12">Finally, let’s evaluate the model on both the training and test sets with <span class="No-Break">the R2-score:</span><pre class="source-code">
# Compute the predictions with the trained neural network</pre><pre class="source-code">
y_train_pred = net(</pre><pre class="source-code">
    torch.tensor((X_train))).detach().numpy()</pre><pre class="source-code">
y_test_pred = net(</pre><pre class="source-code">
    torch.tensor((X_test))).detach().numpy()</pre><pre class="source-code">
# Compute the R2-score</pre><pre class="source-code">
print('R2-score on training set:', r2_score(y_train,</pre><pre class="source-code">
     y_train_pred))</pre><pre class="source-code">
print('R2-score on test set:', r2_score(y_test,</pre><pre class="source-code">
    y_test_pred))</pre></li>
</ol>
<p>This code will output values such as <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">R2-score on training set: 0.9922777453770203</strong>
<strong class="bold">R2-score on test set: 0.7610035849523354</strong></pre>
<p>As expected, we are facing<a id="_idIndexMarker433"/> a clear overfitting here, with an almost perfect R2-score on the train set, and an R2-score of about 0.76 on the <span class="No-Break">test set.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">This may look like an exaggerated example, but it is fairly easy to choose an architecture that is way too large for the task <span class="No-Break">and dataset.</span></p>
<h3>Regularizing with a smaller network</h3>
<p>Let’s now train a more reasonable<a id="_idIndexMarker434"/> model and see how this impacts overfitting, even with the same number of epochs. The goal is not only to decrease overfitting but also to get better performances on the <span class="No-Break">test set.</span></p>
<p>If you are using the same kernel, there is no need to redo the first steps. Otherwise, <em class="italic">steps 1</em> to <em class="italic">6</em> must <span class="No-Break">be redone:</span></p>
<ol>
<li>Define the neural network. This time, we only have two hidden layers of 16 units each, so this is much smaller <span class="No-Break">than earlier:</span><pre class="source-code">
class Net(nn.Module):</pre><pre class="source-code">
    def __init__(self, input_shape: int,</pre><pre class="source-code">
        hidden_units: int = 16):</pre><pre class="source-code">
            super(Net, self).__init__()</pre><pre class="source-code">
        self.hidden_units = hidden_units</pre><pre class="source-code">
        self.fc1 = nn.Linear(input_shape,</pre><pre class="source-code">
            self.hidden_units)</pre><pre class="source-code">
        self.fc2 = nn.Linear(self.hidden_units,</pre><pre class="source-code">
            self.hidden_units)</pre><pre class="source-code">
        self.output = nn.Linear(self.hidden_units, 1)</pre><pre class="source-code">
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:</pre><pre class="source-code">
        x = self.fc1(x)</pre><pre class="source-code">
        x = F.relu(x)</pre><pre class="source-code">
        x = self.fc2(x)</pre><pre class="source-code">
        x = F.relu(x)</pre><pre class="source-code">
        output = self.output(x)</pre><pre class="source-code">
        return output</pre></li>
<li>Instantiate the network <a id="_idIndexMarker435"/>with the expected number of input features and <span class="No-Break">the optimizer:</span><pre class="source-code">
# Instantiate the network</pre><pre class="source-code">
net = Net(X_train.shape[1])</pre><pre class="source-code">
optimizer = torch.optim.Adam(net.parameters(),</pre><pre class="source-code">
    lr=0.001)</pre></li>
<li>Train the neural network over 500 epochs so that we have results that we can compare to the previous ones. We will reuse the <strong class="source-inline">train_model</strong> function already used earlier in <span class="No-Break">this recipe:</span><pre class="source-code">
train_losses, test_losses = train_model(net,</pre><pre class="source-code">
    train_dataloader, test_dataloader, criterion,</pre><pre class="source-code">
    optimizer, 500)</pre><pre class="source-code">
<strong class="bold">[epoch 500] Training: loss=0.248 | Test: loss=0.273</strong></pre><pre class="source-code">
<strong class="bold">Finished Training</strong></pre></li>
<li>Plot the loss as a function<a id="_idIndexMarker436"/> of the epoch for the train and <span class="No-Break">test sets:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('loss (MSE)')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer200">
<img alt="Figure 7.9 – Mean squared error loss as a function of the epoch (note the train and test sets almost overlapping)" height="413" src="image/B19629_07_09.jpg" width="551"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.9 – Mean squared error loss as a function of the epoch (note the train and test sets almost overlapping)</p>
<p>As we can see, this time, even with many epochs, there is no strong overfitting: the train and test losses remain<a id="_idIndexMarker437"/> close to each other no matter the number (except for a few noise bumps), even if a small amount of overfitting seems to appear <span class="No-Break">over time.</span></p>
<ol>
<li value="5">Let’s again evaluate the model with the R2-score on the training and <span class="No-Break">test sets:</span><pre class="source-code">
# Compute the predictions with the trained neural network</pre><pre class="source-code">
y_train_pred = net(</pre><pre class="source-code">
    torch.tensor((X_train))).detach().numpy()</pre><pre class="source-code">
y_test_pred = net(</pre><pre class="source-code">
    torch.tensor((X_test))).detach().numpy()</pre><pre class="source-code">
# Compute the R2-score</pre><pre class="source-code">
print('R2-score on training set:', r2_score(y_train,</pre><pre class="source-code">
    y_train_pred))</pre><pre class="source-code">
print('R2-score on test set:', r2_score(y_test,</pre><pre class="source-code">
    y_test_pred))</pre></li>
</ol>
<p>Here is the typical output of <span class="No-Break">this code:</span></p>
<pre class="source-code">
<strong class="bold">R2-score on training set: 0.8161885562733123</strong>
<strong class="bold">R2-score on test set: 0.7906037325658601</strong></pre>
<p>While the R2-score on the training set decreased from 0.99 to 0.81, the score on the test set increased from 0.76 to 0.79, effectively improving<a id="_idIndexMarker438"/> the performance of <span class="No-Break">the model.</span></p>
<p>Even if it was a rather extreme example, the general idea <span class="No-Break">remains true.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">Early stopping could work well too in this case. The two techniques (early stopping and downsizing the network) are not mutually exclusive and can work <span class="No-Break">well together.</span></p>
<h2 id="_idParaDest-200"><a id="_idTextAnchor200"/>There’s more...</h2>
<p>The model complexity can arguably be computed using the number of parameters. Even if it’s not a direct measure, it remains a <span class="No-Break">good indicator.</span></p>
<p>For example, the first neural network used in this recipe, with 10 hidden layers of 128 units, had 67,329 trainable parameters. On the other side, the second neural network, with only 2 hidden layers of 16 units, had only 433 <span class="No-Break">trainable parameters.</span></p>
<p>The number of parameters in a fully connected neural network is based on the number of units and the number of layers: both units and layers do not have to  be the same on the number of <span class="No-Break">parameters though.</span></p>
<p>To compute the number of trainable parameters in the torch network’s net, we can use the following <span class="No-Break">code snippet:</span></p>
<pre class="source-code">
sum(p.numel() for p in net.parameters() if p.requires_grad)</pre>
<p>To get an idea, let’s take again three examples of neural networks<a id="_idIndexMarker439"/> with the same number of neurons, but with a different number of layers. Let’s assume they all have 10 input features and 1 unit <span class="No-Break">output layer:</span></p>
<ul>
<li>A neural network with 1 hidden layer of 100 units: <span class="No-Break">1,201 parameters</span></li>
<li>A neural network with 2 hidden layers of 50 units: <span class="No-Break">3,151 parameters</span></li>
<li>A neural network with 10 hidden layers of 10 units: <span class="No-Break">1,111 parameters</span></li>
</ul>
<p>So, there is a trade-off between the number of layers and the number of units per layer to get the most complex neural network for a given number <span class="No-Break">of neurons.</span></p>
<h1 id="_idParaDest-201"><a id="_idTextAnchor201"/>Regularizing with dropout</h1>
<p>A widely used method for regularizing<a id="_idIndexMarker440"/> is dropout. Dropout is just randomly setting some neurons’ activations to zero during the training phase. Let’s first review how this works and then apply it to a multiclass classification task, the <strong class="source-inline">sklearn</strong> digits dataset, which is kind of an older and smaller version of the <span class="No-Break">MNIST dataset.</span></p>
<h2 id="_idParaDest-202"><a id="_idTextAnchor202"/>Getting ready</h2>
<p>Dropout is a widely adopted regularization approach in deep learning, due to its simplicity and effectiveness. The technique is easy to understand, yet can yield <span class="No-Break">powerful results.</span></p>
<p>The principle is simple – during training, we randomly ignore some units by setting their activations to zero, as represented in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.10</em></span><span class="No-Break">:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer201">
<img alt="Figure 7.10 – On the left, a standard neural network with its connections, and, on the right, the same neural network with dropout, having, on average, 50% of its neurons ignored at training" height="285" src="image/B19629_07_10.jpg" width="530"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.10 – On the left, a standard neural network with its connections, and, on the right, the same neural network with dropout, having, on average, 50% of its neurons ignored at training</p>
<p>Dropout adds one hyperparameter<a id="_idIndexMarker441"/> though: the dropout probability. For a 0% probability, there is no dropout. For a 50% probability, about 50% of the neurons will be randomly selected to be ignored. For a 100% probability, well, there is nothing left to learn. The ignored neurons are not always the same: for each new batch size, a new set of units is randomly selected to <span class="No-Break">be ignored.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">The remaining activations are consequently scaled to keep a consistent global input for any unit. In practice, for a dropout probability of 1/2, all the neurons that are not ignored are scaled by a factor of 2 (i.e., their activations are multiplied <span class="No-Break">by 2).</span></p>
<p>Certainly, when evaluating or inferring on new data, dropout is deactivated, causing all neurons to <span class="No-Break">be activated.</span></p>
<p>But what is the point of doing that? Why would randomly ignoring some neurons help? A formal explanation is beyond the scope of this book, but at least we can provide some intuition. The idea is to avoid confusing the neural network with too much information. As a human, having too much information can hurt more than it helps: sometimes, having less information allows you to make better decisions, preventing you from being flooded by it. This is the idea of dropout: instead of giving the network all the information at once, it is gently trained with less information by turning off a few neurons randomly for a short amount of time. Hopefully, this will help the network make better decisions in <span class="No-Break">the end.</span></p>
<p>In this recipe, this will be run<a id="_idIndexMarker442"/> on the <strong class="source-inline">digits</strong> dataset of scikit-learn, which is just a link to the <em class="italic">Optical Recognition of Handwritten Digits</em> dataset. A small subset of these images is represented in <span class="No-Break"><em class="italic">Figure 7</em></span><span class="No-Break"><em class="italic">.11</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer202">
<img alt="Figure 7.11 – A sample of images from the dataset and their labels: each image is composed of 8x8 pixels" height="614" src="image/B19629_07_11.jpg" width="658"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.11 – A sample of images from the dataset and their labels: each image is composed of 8x8 pixels</p>
<p>Each image is an 8x8-pixel picture of a handwritten digit. Thus, the dataset is made up of 10 classes, 1 for <span class="No-Break">each digit.</span></p>
<p>To run the code of this recipe, the required libraries are <strong class="source-inline">sklearn</strong>, <strong class="source-inline">matplotlib</strong>, and <strong class="source-inline">torch</strong>. They can be installed with <strong class="source-inline">pip install sklearn </strong><span class="No-Break"><strong class="source-inline">matplotlib torch</strong></span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-203"><a id="_idTextAnchor203"/>How to do it...</h2>
<p>This recipe will comprise <span class="No-Break">two steps:</span></p>
<ol>
<li>First, we will train a neural network without dropout with a rather large model, considering <span class="No-Break">the data.</span></li>
<li>Then, we will train the same neural network with dropout, hopefully, to improve the <span class="No-Break">model’s performance.</span></li>
</ol>
<p>We will use the same data for both configurations, the same batch size, and the same number of epochs, so that we can compare <span class="No-Break">the results.</span></p>
<h3>Without dropout</h3>
<p>Here are the steps<a id="_idIndexMarker443"/> to regularize <span class="No-Break">without dropout:</span></p>
<ol>
<li>The following imports must <span class="No-Break">be loaded:</span><ul><li><strong class="source-inline">load_digits</strong> from <strong class="source-inline">sklearn</strong> to load <span class="No-Break">the dataset</span></li><li><strong class="source-inline">train_test_split</strong> from <strong class="source-inline">sklearn</strong> to split <span class="No-Break">the dataset</span></li><li><strong class="source-inline">torch</strong>, <strong class="source-inline">torch.nn</strong>, and <strong class="source-inline">torch.nn.functional</strong> for the <span class="No-Break">neural network</span></li><li><strong class="source-inline">Dataset</strong> and <strong class="source-inline">DataLoader</strong> from <strong class="source-inline">torch</strong> for the dataset loading <span class="No-Break">in </span><span class="No-Break"><strong class="source-inline">torch</strong></span></li><li><strong class="source-inline">matplotlib</strong> for the visualization of <span class="No-Break">the loss</span></li></ul></li>
</ol>
<p>Here is the code for the <span class="No-Break"><strong class="source-inline">import</strong></span><span class="No-Break"> statements:</span></p>
<pre class="source-code">
import numpy as np
from sklearn.datasets import load_digits
from sklearn.model_selection import train_test_split
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
import matplotlib.pyplot as plt</pre>
<ol>
<li value="2">Load the data. The dataset is made of 1,797 samples, and the images are already flattened to 64 values between 0 and 16 for the <span class="No-Break">8x8 pixels:</span><pre class="source-code">
X, y = load_digits(return_X_y=True)</pre></li>
<li>Split the data into training and test sets, with 80% in the training set and 20% in the test set. The features are converted in to <strong class="source-inline">float32</strong>, while the labels are converted into <strong class="source-inline">int64</strong> to avoid <strong class="source-inline">torch</strong> <span class="No-Break">errors later:</span><pre class="source-code">
X_train, X_test, y_train, y_test = train_test_split(</pre><pre class="source-code">
    X.astype(np.float32), y.astype(np.int64),</pre><pre class="source-code">
    test_size=0.2, random_state=0)</pre></li>
<li>Create the <strong class="source-inline">DigitsDataset</strong> class for PyTorch. The only transformation to the features, besides<a id="_idIndexMarker444"/> converting them into <strong class="source-inline">torch</strong> tensors, is to divide the values by 255 to have a range of features in <strong class="source-inline">[</strong><span class="No-Break"><strong class="source-inline">0, 1]</strong></span><span class="No-Break">:</span><pre class="source-code">
class DigitsDataset(Dataset):</pre><pre class="source-code">
    def __init__(self, X: np.array, y: np.array):</pre><pre class="source-code">
        self.X = torch.from_numpy(X/255)</pre><pre class="source-code">
        self.y = torch.from_numpy(y)</pre><pre class="source-code">
    def __len__(self) -&gt; int:</pre><pre class="source-code">
        return len(self.X)</pre><pre class="source-code">
    def __getitem__(self, idx: int) -&gt; tuple[torch.Tensor]:</pre><pre class="source-code">
        return self.X[idx], self.y[idx]</pre></li>
<li>Instantiate the datasets for the train and test sets and the data loaders with a batch size <span class="No-Break">of </span><span class="No-Break"><strong class="source-inline">64</strong></span><span class="No-Break">:</span><pre class="source-code">
# Instantiate datasets</pre><pre class="source-code">
training_data = DigitsDataset(X_train, y_train)</pre><pre class="source-code">
test_data = DigitsDataset(X_test, y_test)</pre><pre class="source-code">
# Instantiate data loaders</pre><pre class="source-code">
train_dataloader = DataLoader(training_data,</pre><pre class="source-code">
    batch_size=64, shuffle=True)</pre><pre class="source-code">
test_dataloader = DataLoader(test_data, batch_size=64,</pre><pre class="source-code">
    shuffle=True)</pre></li>
<li>Define the neural network architecture – here, there are 3 hidden layers of 128 units (by default) and a dropout<a id="_idIndexMarker445"/> probability set to 25% applied to all the <span class="No-Break">hidden layers:</span><pre class="source-code">
class Net(nn.Module):</pre><pre class="source-code">
    def __init__(self, input_shape: int,</pre><pre class="source-code">
        hidden_units: int = 128,</pre><pre class="source-code">
        dropout: float = 0.25):</pre><pre class="source-code">
            super(Net, self).__init__()</pre><pre class="source-code">
            self.hidden_units = hidden_units</pre><pre class="source-code">
            self.fc1 = nn.Linear(input_shape,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.fc2 = nn.Linear(self.hidden_units,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.fc3 = nn.Linear(self.hidden_units,</pre><pre class="source-code">
                self.hidden_units)</pre><pre class="source-code">
            self.dropout = nn.Dropout(p=dropout)</pre><pre class="source-code">
            self.output = nn.Linear(self.hidden_units, 10)</pre><pre class="source-code">
    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:</pre><pre class="source-code">
        x = self.fc1(x)</pre><pre class="source-code">
        x = F.relu(x)</pre><pre class="source-code">
        x = self.dropout(x)</pre><pre class="source-code">
        x = self.fc2(x)</pre><pre class="source-code">
        x = F.relu(x)</pre><pre class="source-code">
        x = self.dropout(x)</pre><pre class="source-code">
        x = self.fc3(x)</pre><pre class="source-code">
        x = F.relu(x)</pre><pre class="source-code">
        x = self.dropout(x)</pre><pre class="source-code">
        output = torch.softmax(self.output(x), dim=1)</pre><pre class="source-code">
        return output</pre></li>
</ol>
<p>Here, dropout is added in <span class="No-Break">two steps:</span></p>
<ul>
<li>Instantiate an <strong class="source-inline">nn.Dropout(p=dropout)</strong> class in the constructor, having the provided <span class="No-Break">dropout probability</span></li>
<li>Apply the dropout layer (defined in the constructor) after the activation function for each hidden layer with <strong class="source-inline">x = </strong><span class="No-Break"><strong class="source-inline">self.dropout(x)</strong></span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">In the case of a ReLU activation function, setting the dropout before or after the activation function won’t change the output. For other activation functions such as the sigmoid, though, this makes <span class="No-Break">a difference.</span></p>
<ol>
<li value="7">Instantiate the model <a id="_idIndexMarker446"/>with the right input shape of <strong class="source-inline">64</strong> (8x8 pixels) and a dropout of <strong class="source-inline">0</strong> since we want to check the results without dropout first. Check the forward propagation works properly on a given <span class="No-Break">random tensor:</span><pre class="source-code">
# Instantiate the model</pre><pre class="source-code">
net = Net(X_train.shape[1], dropout=0)</pre><pre class="source-code">
# Generate randomly one random 28x28 image as a 784 values tensor</pre><pre class="source-code">
random_data = torch.rand((1, 64))</pre><pre class="source-code">
result = net(random_data)</pre><pre class="source-code">
print('Resulting output tensor:', result)</pre><pre class="source-code">
print('Sum of the output tensor:', result.sum())</pre></li>
</ol>
<p>The output of this code should look like <span class="No-Break">the following:</span></p>
<pre class="source-code">
<strong class="bold">Resulting output tensor: tensor([[0.0964, 0.0908, 0.1043, 0.1083, 0.0927, 0.1047, 0.0949, 0.0991, 0.1012,</strong>
<strong class="bold">         0.1076]], grad_fn=&lt;SoftmaxBackward0&gt;)</strong>
<strong class="bold">Sum of the output tensor: tensor(1., grad_fn=&lt;SumBackward0&gt;)</strong></pre>
<ol>
<li value="8">Define the loss function<a id="_idIndexMarker447"/> as the cross-entropy loss and the optimizer <span class="No-Break">as </span><span class="No-Break"><strong class="source-inline">Adam</strong></span><span class="No-Break">:</span><pre class="source-code">
criterion = nn.CrossEntropyLoss()</pre><pre class="source-code">
optimizer = torch.optim.Adam(net.parameters(), lr=0.001)</pre></li>
<li>Train the neural network on 500 epochs using the <strong class="source-inline">train_model</strong> function available in the GitHub repository. For each epoch, we store and compute the loss and the accuracy for both the training and <span class="No-Break">test sets:</span><pre class="source-code">
train_losses, test_losses, train_accuracy,</pre><pre class="source-code">
    test_accuracy = train_model(</pre><pre class="source-code">
        net, train_dataloader, test_dataloader,</pre><pre class="source-code">
        criterion, optimizer, epochs=500</pre><pre class="source-code">
)</pre></li>
</ol>
<p>After 500 epochs, you should get an output <span class="No-Break">like this:</span></p>
<pre class="source-code">
<strong class="bold">[epoch 500] Training: loss=1.475 accuracy=0.985 |       Test: loss=1.513 accuracy=0.947</strong>
<strong class="bold">Finished Training</strong></pre>
<ol>
<li value="10">Plot the cross-entropy<a id="_idIndexMarker448"/> loss for both the training and test sets as a function of the <span class="No-Break">epoch number:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('loss (CE)')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer203">
<img alt="Figure 7.12 – Cross-entropy loss as a function of the epoch (note the slight divergence between train and test sets)" height="413" src="image/B19629_07_12.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.12 – Cross-entropy loss as a function of the epoch (note the slight divergence between train and test sets)</p>
<ol>
<li value="11">Plotting the accuracy will show the <span class="No-Break">equivalent results:</span><pre class="source-code">
plt.plot(train_accuracy, label='train')</pre><pre class="source-code">
plt.plot(test_accuracy, label='test')</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('Accuracy')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer204">
<img alt="Figure 7.13 – Accuracy as a function of the epoch; we can again notice the overfitting" height="413" src="image/B19629_07_13.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.13 – Accuracy as a function of the epoch; we can again notice the overfitting</p>
<p>The final accuracy<a id="_idIndexMarker449"/> is about 98% on the train set and only about 95% on the test set, showing overfitting. Let’s try now to add dropout to reduce <span class="No-Break">this overfitting.</span></p>
<h3>With dropout</h3>
<p>In this part, we will simply<a id="_idIndexMarker450"/> restart from <em class="italic">step 7</em>, but with dropout, and then compare <span class="No-Break">the results:</span></p>
<ol>
<li>Instantiate the model with an input share of <strong class="source-inline">64</strong> and a dropout probability of 25%. A probability of 25% means that during the training, in each of the hidden layers, about 32 randomly selected neurons will be ignored. Instantiate a fresh optimizer, still <span class="No-Break">using </span><span class="No-Break"><strong class="source-inline">Adam</strong></span><span class="No-Break">:</span><pre class="source-code">
# Instantiate the model</pre><pre class="source-code">
net = Net(X_train.shape[1], dropout=0.25)</pre><pre class="source-code">
optimizer = torch.optim.Adam(net.parameters(), lr=0.001)</pre></li>
<li>Train the neural network again for 500 epochs, while storing the train and test loss <span class="No-Break">and accuracy:</span><pre class="source-code">
train_losses, test_losses, train_accuracy, test_accuracy = train_model(</pre><pre class="source-code">
    net, train_dataloader, test_dataloader, criterion,</pre><pre class="source-code">
        optimizer, epochs=500</pre><pre class="source-code">
)</pre><pre class="source-code">
<strong class="bold">[epoch 500] Training: loss=1.472 accuracy=0.990 |       Test: loss=1.488 accuracy=0.975</strong></pre><pre class="source-code">
<strong class="bold">Finished Training</strong></pre></li>
<li>Plot the train<a id="_idIndexMarker451"/> and test losses again as a function of <span class="No-Break">the epoch:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('loss (CE)')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer205">
<img alt="Figure 7.14 – Cross-entropy loss as a function of the epoch, with reduced divergence thanks to dropout" height="413" src="image/B19629_07_14.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.14 – Cross-entropy loss as a function of the epoch, with reduced divergence thanks to dropout</p>
<p>We face a different behavior here than seen previously. The train and test losses do not seem to grow<a id="_idIndexMarker452"/> apart too much with the epochs. During the initial 100 epochs, the test loss is marginally lower than the train loss, but afterward, the train loss decreases further, indicating slight overfitting of <span class="No-Break">the model.</span></p>
<ol>
<li value="4">Finally, plot the train and test accuracy as a function of <span class="No-Break">the epoch:</span><pre class="source-code">
plt.plot(train_accuracy, label='train')</pre><pre class="source-code">
plt.plot(test_accuracy, label='test')</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('Accuracy')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here is the plot <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer206">
<img alt="Figure 7.15 – Accuracy as a function of the epoch (the overfitting is largely reduced thanks to dropout)" height="413" src="image/B19629_07_15.jpg" width="543"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 7.15 – Accuracy as a function of the epoch (the overfitting is largely reduced thanks to dropout)</p>
<p>We have a train accuracy of 99% against the 98% seen previously. More interestingly, the test accuracy<a id="_idIndexMarker453"/> climbed to 97%, from 95% previously, effectively regularizing and reducing <span class="No-Break">the overfitting.</span></p>
<h2 id="_idParaDest-204"><a id="_idTextAnchor204"/>There’s more...</h2>
<p>Although dropout is not always foolproof, it has been demonstrated to be an effective regularization technique, particularly when training large networks on small datasets. More about this can be found in the publication <em class="italic">Improving neural networks by preventing co-adaptation of feature detectors</em>, by Hinton et al. This publication can be found here on <span class="No-Break"><strong class="source-inline">arxiv</strong></span><span class="No-Break">: </span><a href="https://arxiv.org/abs/1207.0580"><span class="No-Break">https://arxiv.org/abs/1207.0580</span></a><span class="No-Break">.</span></p>
<h2 id="_idParaDest-205"><a id="_idTextAnchor205"/>See also</h2>
<ul>
<li>The official location<a id="_idIndexMarker454"/> of the <strong class="source-inline">digits</strong> <span class="No-Break">dataset: </span><a href="https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits"><span class="No-Break">https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits</span></a></li>
<li>The PyTorch documentation<a id="_idIndexMarker455"/> about <span class="No-Break">dropout: </span><a href="https://pytorch.org/docs/stable/generated/torch.nn.Dropout.xhtml"><span class="No-Break">https://pytorch.org/docs/stable/generated/torch.nn.Dropout.xhtml</span></a></li>
</ul>
</div>
</div></body></html>