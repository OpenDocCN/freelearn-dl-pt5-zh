["```py\nkaggle datasets download -d karakaggle/kaggle-cat-vs-dog-dataset --unzip\n```", "```py\nfrom glob import glob\nimport os\ncats_folder = 'kagglecatsanddogs_3367a/PetImages/Cat/'\ndogs_folder = 'kagglecatsanddogs_3367a/PetImages/Dog/'\ncats_paths = sorted(glob(cats_folder + '*.jpg'))\ndogs_paths = sorted(glob(dogs_folder + '*.jpg'))\ntrain_ratio = 0.8\nos.mkdir(cats_folder + 'train')\nos.mkdir(cats_folder + 'test')\nos.mkdir(dogs_folder + 'train')\nos.mkdir(dogs_folder + 'test')\nfor i in range(len(cats_paths)):\n    if i <= train_ratio * len(cats_paths):\n        os.rename(cats_paths[i], cats_folder + 'train/' + cats_paths[i].split('/')[-1])\n    else:\n        os.rename(cats_paths[i], cats_folder + 'test/' + cats_paths[i].split('/')[-1])\nfor i in range(len(dogs_paths)):\n    if i <= train_ratio * len(dogs_paths):\n        os.rename(dogs_paths[i], dogs_folder + 'train/' + dogs_paths[i].split('/')[-1])\n    else:\n        os.rename(dogs_paths[i], dogs_folder + 'test/' + dogs_paths[i].split('/')[-1])\n```", "```py\nkagglecatsanddogs_3367a\n└── PetImages\n    ├── Cat\n    │   ├── train: 9993 images\n    │   └── test: 2497 images\n    └── Dog\n        ├── train: 9976 images\n        └── test: 2493 images\n```", "```py\npip install matplotlib numpy torch torchvision albumentations\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom torchvision.models import mobilenet_v3_small\nfrom glob import glob\nfrom PIL import Image\n```", "```py\nclass DogsAndCats(Dataset) :\n    def __init__(self, cats_folder: str,\n        dogs_folder: str, transform, augment = None):\n            self.cats_path = sorted(glob(\n                f'{cats_folder}/*.jpg'))\n            self.dogs_path = sorted(glob(\n                f'{dogs_folder}/*.jpg'))\n            self.images_path = self.cats_path + self.dogs_path\n            self.labels = [0.]*len(\n            self.cats_path) + [1.]*len(self.dogs_path)\n            self.transform = transform\n            self.augment = augment\n    def __len__(self):\n        return len(self.images_path)\n    def __getitem__(self, idx):\n        image = Image.open(self.images_path[\n            idx]).convert('RGB')\n        if self.augment is not None:\n            image = self.augment(\n                image=np.array(image))[\"image\"]\n        return self.transform(image),\n        torch.tensor(self.labels[idx],\n            dtype=torch.float32)\n```", "```py\ntransform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize((224, 224), antialias=True),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5,\n        0.5)),\n])\n```", "```py\n    batch_size = 64\n    ```", "```py\n    device = torch.device(\n    ```", "```py\n        'cuda' if torch.cuda.is_available() else 'cpu')\n    ```", "```py\n    epochs = 20\n    ```", "```py\n    trainset = DogsAndCats(\n    ```", "```py\n        'kagglecatsanddogs_3367a/PetImages/Cat/train/',\n    ```", "```py\n        'kagglecatsanddogs_3367a/PetImages/Dog/train/',\n    ```", "```py\n        transform=transform\n    ```", "```py\n    )\n    ```", "```py\n    train_dataloader = DataLoader(trainset,\n    ```", "```py\n        batch_size=batch_size, shuffle=True)\n    ```", "```py\n    testset = DogsAndCats(\n    ```", "```py\n        'kagglecatsanddogs_3367a/PetImages/Cat/test/',\n    ```", "```py\n        'kagglecatsanddogs_3367a/PetImages/Dog/test/',\n    ```", "```py\n        transform=transform\n    ```", "```py\n    )\n    ```", "```py\n    test_dataloader = DataLoader(testset,\n    ```", "```py\n        batch_size=batch_size, shuffle=True)\n    ```", "```py\n    def display_images(dataloader, classes = ['cat', 'dog']):\n    ```", "```py\n        plt.figure(figsize=(14, 10))\n    ```", "```py\n        images, labels = next(iter(dataloader))\n    ```", "```py\n        for idx in range(8):\n    ```", "```py\n            plt.subplot(2, 4, idx+1)\n    ```", "```py\n            plt.imshow(images[idx].permute(\n    ```", "```py\n                1, 2, 0).numpy() * 0.5 + 0.5)\n    ```", "```py\n            plt.title(classes[int(labels[idx].item())])\n    ```", "```py\n            plt.axis('off')\n    ```", "```py\n    display_images(train_dataloader)\n    ```", "```py\n    class Classifier(nn.Module):\n    ```", "```py\n        def __init__(self):\n    ```", "```py\n            super(Classifier, self).__init__()\n    ```", "```py\n            self.mobilenet = mobilenet_v3_small()\n    ```", "```py\n            self.output_layer = nn.Linear(1000, 1)\n    ```", "```py\n        def forward(self, x):\n    ```", "```py\n            x = self.mobilenet(x)\n    ```", "```py\n            x = nn.Sigmoid()(self.output_layer(x))\n    ```", "```py\n            return x\n    ```", "```py\n    model = Classifier()\n    ```", "```py\n    model = model.to(device)\n    ```", "```py\n    criterion = nn.BCELoss()\n    ```", "```py\n    optimizer = torch.optim.Adam(model.parameters(),\n    ```", "```py\n        lr=0.001)\n    ```", "```py\n    train_losses, test_losses, train_accuracy, \n    ```", "```py\n    test_accuracy = train_model(\n    ```", "```py\n        epochs, model, criterion, optimizer, device,\n    ```", "```py\n        train_dataloader, test_dataloader, trainset,\n    ```", "```py\n        testset\n    ```", "```py\n    )\n    ```", "```py\n    plt.figure(figsize=(10, 10))\n    ```", "```py\n    plt.subplot(2, 1, 1)\n    ```", "```py\n    plt.plot(train_losses, label='train')\n    ```", "```py\n    plt.plot(test_losses, label='test')\n    ```", "```py\n    plt.ylabel('BCE Loss')\n    ```", "```py\n    plt.legend()\n    ```", "```py\n    plt.subplot(2, 1, 2)\n    ```", "```py\n    plt.plot(train_accuracy, label='train')\n    ```", "```py\n    plt.plot(test_accuracy, label='test')\n    ```", "```py\n    plt.xlabel('Epoch')\n    ```", "```py\n    plt.ylabel('Accuracy')\n    ```", "```py\n    plt.legend()\n    ```", "```py\n    plt.show()\n    ```", "```py\nimport albumentations as A\naugment = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.Rotate(p=0.5),\n    A.RandomBrightnessContrast(p=0.2),\n])\n```", "```py\n    augmented_trainset = DogsAndCats(\n    ```", "```py\n        'kagglecatsanddogs_3367a/PetImages/Cat/train/',\n    ```", "```py\n        'kagglecatsanddogs_3367a/PetImages/Dog/train/',\n    ```", "```py\n        transform=transform,\n    ```", "```py\n        augment=augment,\n    ```", "```py\n    )\n    ```", "```py\n    augmented_train_dataloader = DataLoader(\n    ```", "```py\n        augmented_trainset, batch_size=batch_size,\n    ```", "```py\n        shuffle=True)\n    ```", "```py\n    display_images(augmented_train_dataloader)\n    ```", "```py\n    model = Classifier()\n    ```", "```py\n    model = model.to(device)\n    ```", "```py\n    optimizer = torch.optim.Adam(model.parameters(),\n    ```", "```py\n        lr=0.001)\n    ```", "```py\n    train_losses, test_losses, train_accuracy, \n    ```", "```py\n    test_accuracy = train_model(\n    ```", "```py\n        epochs, model, criterion, optimizer, device,\n    ```", "```py\n        augmented_train_dataloader, test_dataloader,\n    ```", "```py\n        trainset, testset\n    ```", "```py\n    )\n    ```", "```py\n    plt.figure(figsize=(10, 10))\n    ```", "```py\n    plt.subplot(2, 1, 1)\n    ```", "```py\n    plt.plot(train_losses, label='train')\n    ```", "```py\n    plt.plot(test_losses, label='test')\n    ```", "```py\n    plt.ylabel('BCE Loss')\n    ```", "```py\n    plt.legend()\n    ```", "```py\n    plt.subplot(2, 1, 2)\n    ```", "```py\n    plt.plot(train_accuracy, label='train')\n    ```", "```py\n    plt.plot(test_accuracy, label='test')\n    ```", "```py\n    plt.xlabel('epoch')\n    ```", "```py\n    plt.ylabel('Accuracy')\n    ```", "```py\n    plt.legend()\n    ```", "```py\n    plt.show()\n    ```", "```py\nkaggle datasets download -d vincentv/qr-detection-yolo --unzip\n```", "```py\nQR-detection-yolo\n├── train\n│   ├── images: 9750 images\n│   └── labels: 9750 text files\n├── test\n│   ├── images: 683 images\n│   └── labels: 683 text files\n└── background_images: 44 images\n```", "```py\npip install albumentations opencv-python matplotlib numpy ultralytics.\n```", "```py\nfrom glob import glob\nimport os\nimport albumentations as A\nimport cv2\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom ultralytics import YOLO\n```", "```py\n    def read_labels(labels_path):\n    ```", "```py\n        res = []\n    ```", "```py\n        with open(labels_path, 'r') as file:\n    ```", "```py\n            lines = file.readlines()\n    ```", "```py\n            for line in lines:\n    ```", "```py\n                cls,xc,yc,w,h = line.strip().split(' ')\n    ```", "```py\n                res.append([int(float(cls)), float(xc),\n    ```", "```py\n                    float(yc), float(w), float(h)])\n    ```", "```py\n            file.close()\n    ```", "```py\n        return res\n    ```", "```py\n    def plot_labels(images_folder, labels_folder,\n    ```", "```py\n        classes):\n    ```", "```py\n            images_path = sorted(glob(\n    ```", "```py\n                images_folder + '/*.jpg'))\n    ```", "```py\n         labels_path = sorted(glob(\n    ```", "```py\n                labels_folder + '/*.txt'))\n    ```", "```py\n        plt.figure(figsize=(10, 6))\n    ```", "```py\n        for i in range(8):\n    ```", "```py\n            idx = np.random.randint(len(images_path))\n    ```", "```py\n            image = plt.imread(images_path[idx])\n    ```", "```py\n            labels = read_labels(labels_path[idx])\n    ```", "```py\n            for cls, xc, yc, w, h in labels:\n    ```", "```py\n                xc = int(xc*image.shape[1])\n    ```", "```py\n                yc = int(yc*image.shape[0])\n    ```", "```py\n                w = int(w*image.shape[1])\n    ```", "```py\n                h = int(h*image.shape[0])\n    ```", "```py\n                cv2.rectangle(image,\n    ```", "```py\n                    (xc - w//2, yc - h//2),\n    ```", "```py\n                    (xc + w//2 ,yc + h//2), (255,0,0), 2)\n    ```", "```py\n                cv2.putText(image, f'{classes[int(cls)]}',\n    ```", "```py\n                    (xc-w//2, yc - h//2 - 10),\n    ```", "```py\n                    cv2.FONT_HERSHEY_SIMPLEX, 0.5,\n    ```", "```py\n                    (1.,0.,0.), 1)\n    ```", "```py\n            plt.subplot(2, 4, i + 1)\n    ```", "```py\n            plt.imshow(image)\n    ```", "```py\n            plt.axis('off')\n    ```", "```py\n    plot_labels('QR-detection-yolo/train/images/',\n    ```", "```py\n        'QR-detection-yolo/train/labels/', 'QR Code')\n    ```", "```py\n    plot_labels('QR-detection-yolo/test/images/',\n    ```", "```py\n        'QR-detection-yolo/test/labels/', 'QR Code')\n    ```", "```py\ndef generate_synthetic_background_image_with_tag(\n    n_images_to_generate: int,\n    output_path: str,\n    raw_tags_folder: str,\n    background_images_path: str,\n    labels_path: str,\n    background_proba: float = 0.8,\n):\n    \"\"\"Generate images with random tag and synthetic background.\n    Parameters\n    ----------\n    n_images_to_generate : int\n        The number of images to generate.\n    output_path : str\n        The output directory path where to store the generated images.\n        If the path does not exist, the directory is created.\n    raw_tags_folder : str\n        Path to the folder containing the raw QR codes.\n    background_images_path : str\n        Path to the folder containing the background images.\n    labels_path : str\n        Path to the folder containing the labels.\n        Files must be in the same order as the ones in the raw_tags_folder.\n    background_proba : float (optional, default=0.8)\n        Probability to use a background image when generating a new sample.\n    \"\"\"\n```", "```py\n    generate_synthetic_background_image_with_tag(\n    ```", "```py\n        n_images_to_generate=3000,\n    ```", "```py\n        output_path='QR-detection-yolo/generated_qr_code_images/',\n    ```", "```py\n        raw_tags_folder='QR-detection-yolo/train/images/',\n    ```", "```py\n        background_images_path='QR-detection-yolo/background_images/',\n    ```", "```py\n        labels_path='QR-detection-yolo/train/labels/'\n    ```", "```py\n    )\n    ```", "```py\nplot_labels(\n    'QR-detection-yolo/generated_qr_code_images/images/',\n    'QR-detection-yolo/generated_qr_code_images/labels/',\n    'QR Code'\n)\n```", "```py\n    # Create a new YOLO model with pretrained weights\n    ```", "```py\n    model = YOLO('yolov8n.pt')\n    ```", "```py\n    train: ../../QR-detection-yolo/generated_qr_code_images/images\n    ```", "```py\n    val: ../../QR-detection-yolo/test/images\n    ```", "```py\n    nc: 1\n    ```", "```py\n    names: ['QR_CODE']\n    ```", "```py\n    # Train the model for 50 epochs\n    ```", "```py\n    model.train(data='data_qr_generated.yaml', epochs=50,\n    ```", "```py\n        lr0=0.001, name='generated_qrcode')\n    ```", "```py\n    def plot_results_random_images(test_images, model, classes=['QR_code']):\n    ```", "```py\n        images = glob(test_images + '/*.jpg')\n    ```", "```py\n        plt.figure(figsize=(14, 10))\n    ```", "```py\n        for i in range(8):\n    ```", "```py\n            idx = np.random.randint(len(images))\n    ```", "```py\n            result = model.predict(images[idx])\n    ```", "```py\n            image = result[0].orig_img.copy()\n    ```", "```py\n            raw_res = result[0].boxes.data\n    ```", "```py\n            for detection in raw_res:\n    ```", "```py\n                x1, y1, x2, y2, p,\n    ```", "```py\n                    cls = detection.cpu().tolist()\n    ```", "```py\n                cv2.rectangle(image, (int(x1), int(y1)),\n    ```", "```py\n                    (int(x2), int(y2)), (255,0,0), 2)\n    ```", "```py\n                cv2.putText(image, f'{classes[int(cls)]}',\n    ```", "```py\n                    (int(x1), int(y1) - 10),\n    ```", "```py\n                        cv2.FONT_HERSHEY_SIMPLEX, 1,\n    ```", "```py\n                        (255,0,0), 2)\n    ```", "```py\n            plt.subplot(2, 4, i + 1)\n    ```", "```py\n            plt.axis('off')\n    ```", "```py\n            plt.imshow(image)\n    ```", "```py\n    # Load the best weights\n    ```", "```py\n    model = YOLO(\n    ```", "```py\n        'runs/detect/generated_qrcode/weights/best.pt')\n    ```", "```py\n    # Plot the results\n    ```", "```py\n    Plot_results_random_images(\n    ```", "```py\n        'QR-detection-yolo/test/images/', model)\n    ```", "```py\n    plt.figure(figsize=(10, 8))\n    ```", "```py\n    plt.imshow(plt.imread(\n    ```", "```py\n        'runs/detect/generated_qrcode/results.png'))\n    ```", "```py\n    import openai\n    ```", "```py\n    import urllib\n    ```", "```py\n    from PIL import Image\n    ```", "```py\n    import matplotlib.pyplot as plt\n    ```", "```py\n    openai.api_key = 'xx-xxx'\n    ```", "```py\n    def get_mask_to_complete(image_path, label_path, output_filename, margin: int = 100):\n    ```", "```py\n        image = plt.imread(image_path)\n    ```", "```py\n        labels = read_labels(label_path)\n    ```", "```py\n        output_mask = np.zeros(image.shape[:2])\n    ```", "```py\n        for cls, xc, yc, w, h in labels:\n    ```", "```py\n            xc = int(xc*image.shape[1])\n    ```", "```py\n            yc = int(yc*image.shape[0])\n    ```", "```py\n            w = int(w*image.shape[1])\n    ```", "```py\n            h = int(h*image.shape[0])\n    ```", "```py\n            output_mask[yc-h//2-margin:yc+h//2+margin,\n    ```", "```py\n                xc-w//2-margin:xc+w//2+margin] = 255\n    ```", "```py\n        output_mask = np.concatenate([image,\n    ```", "```py\n            np.expand_dims(output_mask, -1)],\n    ```", "```py\n                axis=-1).astype(np.uint8)\n    ```", "```py\n        # Save the images\n    ```", "```py\n        output_mask_filename = output_filename.split('.')[0] + '_mask.png'\n    ```", "```py\n        plt.imsave(output_filename, image)\n    ```", "```py\n        plt.imsave(output_mask_filename, output_mask)\n    ```", "```py\n        return output_mask_filename\n    ```", "```py\n    output_image_filename = 'image_edit.png'\n    ```", "```py\n    mask_filename = get_mask_to_complete(\n    ```", "```py\n        'QR-detection-yolo/generated_qr_code_images/images/synthetic_image_0.jpg',\n    ```", "```py\n        'QR-detection-yolo/generated_qr_code_images/labels/synthetic_image_0.txt',\n    ```", "```py\n        output_image_filename\n    ```", "```py\n    )\n    ```", "```py\n    # Display the masked image and the original image side by side\n    ```", "```py\n    plt.figure(figsize=(12, 10))\n    ```", "```py\n    plt.subplot(1, 2, 1)\n    ```", "```py\n    plt.imshow(plt.imread(output_image_filename))\n    ```", "```py\n    plt.subplot(1, 2, 2)\n    ```", "```py\n    plt.imshow(plt.imread(mask_filename))\n    ```", "```py\n# Query openAI API to generate image\nresponse = openai.Image.create_edit(\n    image=open(output_image_filename, 'rb'),\n    mask=open(mask_filename, 'rb'),\n    prompt=\"A store in background\",\n    n=1,\n    size=\"512x512\"\n)\n# Download and display the generated image\nplt.figure(figsize=(12, 10))\nimage_url = response['data'][0]['url']\nplt.subplot(1, 2, 1)\nplt.imshow(plt.imread(output_image_filename))\nplt.subplot(1, 2, 2)\nplt.imshow(np.array(Image.open(urllib.request.urlopen(\n    image_url))))\n```", "```py\n# Query to create variation of a given image\nresponse = openai.Image.create_variation(\n    image=open(output_image_filename, \"rb\"),\n    n=1,\n    size=\"512x512\"\n)\n# Download and display the generated image\nplt.figure(figsize=(12, 10))\nimage_url = response['data'][0]['url']\nplt.subplot(1, 2, 1)\nplt.imshow(plt.imread(output_image_filename))\nplt.subplot(1, 2, 2)\nplt.imshow(np.array(Image.open(urllib.request.urlopen(\n    image_url))))\n```", "```py\nkaggle datasets download -d vincentv/qr-detection-yolo --unzip\n```", "```py\nanime-style-transfer\n├── train\n│   ├── images: 820 images\n│   └── labels: 820 images\n└── test\n     ├── images: 93 images\n     └── labels: 93 images\n```", "```py\npip install matplotlib numpy torch torchvision segmentation-models-pytorch albumentations tqdm\n```", "```py\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Dataset\nimport torchvision.transforms as transforms\nfrom torchvision.models import vgg16, VGG16_Weights\nfrom glob import glob\nimport segmentation_models_pytorch as smp\nfrom torch.optim.lr_scheduler import ExponentialLR\nimport albumentations as A\nimport tqdm\n```", "```py\n    class AnimeStyleDataset(Dataset):\n    ```", "```py\n        def __init__(self, input_path: str,\n    ```", "```py\n            output_path: str, transform, augment = None):\n    ```", "```py\n                self.input_paths = sorted(glob(\n    ```", "```py\n                    f'{input_path}/*.png'))\n    ```", "```py\n                self.output_paths = sorted(glob(\n    ```", "```py\n                    f'{output_path}/*.png'))\n    ```", "```py\n                self.transform = transform\n    ```", "```py\n                self.augment = augment\n    ```", "```py\n        def __len__(self):\n    ```", "```py\n            return len(self.input_paths)\n    ```", "```py\n        def __getitem__(self, idx):\n    ```", "```py\n            input_img = plt.imread(self.input_paths[idx])\n    ```", "```py\n            output_img = plt.imread(\n    ```", "```py\n                self.output_paths[idx])\n    ```", "```py\n            if self.augment:\n    ```", "```py\n                augmented = self.augment(image=input_img)\n    ```", "```py\n                input_img = augmented['image']\n    ```", "```py\n                output_img = A.ReplayCompose.replay(\n    ```", "```py\n                    augmented['replay'],\n    ```", "```py\n                    image=output_img)['image']\n    ```", "```py\n            return self.transform(input_img),\n    ```", "```py\n                self.transform(output_img)\n    ```", "```py\naugment = A.ReplayCompose([\n    A.Resize(512, 512),\n    A.HorizontalFlip(p=0.5),\n    A.ShiftScaleRotate(shift_limit=0.05,\n        scale_limit=0.05, rotate_limit=15, p=0.5),\n    A.RandomBrightnessContrast(p=0.5),\n    A.RandomCropFromBorders(0.2, 0.2, 0.2, 0.2, p=0.5)\n])\n```", "```py\n    batch_size = 12\n    ```", "```py\n    device = torch.device(\n    ```", "```py\n        'cuda' if torch.cuda.is_available() else 'cpu')\n    ```", "```py\n    mean = (0.485, 0.456, 0.406)\n    ```", "```py\n    std = (0.229, 0.224, 0.225)\n    ```", "```py\n    transform = transforms.Compose([\n    ```", "```py\n        transforms.ToTensor(),\n    ```", "```py\n        transforms.Resize((512, 512), antialias=True),\n    ```", "```py\n        transforms.Normalize(mean, std),\n    ```", "```py\n    ])\n    ```", "```py\n    trainset = AnimeStyleDataset(\n    ```", "```py\n        'anime-style-transfer/train/images/',\n    ```", "```py\n        'anime-style-transfer/train/labels/',\n    ```", "```py\n        transform=transform,\n    ```", "```py\n        augment=augment,\n    ```", "```py\n    )\n    ```", "```py\n    train_dataloader = DataLoader(trainset,\n    ```", "```py\n    batch_size=batch_size, shuffle=True)\n    ```", "```py\n    testset = AnimeStyleDataset(\n    ```", "```py\n        'anime-style-transfer/test/images/',\n    ```", "```py\n        'anime-style-transfer/test/labels/',\n    ```", "```py\n        transform=transform,\n    ```", "```py\n    )\n    ```", "```py\n    test_dataloader = DataLoader(testset,\n    ```", "```py\n        batch_size=batch_size, shuffle=True)\n    ```", "```py\n    def unnormalize(x, mean, std):\n    ```", "```py\n        x = np.asarray(x, dtype=np.float32)\n    ```", "```py\n        for dim in range(3):\n    ```", "```py\n            x[:, :, dim] = (x[:, :, dim] * std[dim]) + mean[dim]\n    ```", "```py\n        return x\n    ```", "```py\n    plt.figure(figsize=(12, 6))\n    ```", "```py\n    images, labels = next(iter(train_dataloader))\n    ```", "```py\n    for idx in range(4):\n    ```", "```py\n        plt.subplot(2, 4, idx*2+1)\n    ```", "```py\n        plt.imshow(unnormalize(images[idx].permute(\n    ```", "```py\n            1, 2, 0).numpy(), mean, std))\n    ```", "```py\n        plt.axis('off')\n    ```", "```py\n        plt.subplot(2, 4, idx*2+2)\n    ```", "```py\n        plt.imshow(unnormalize(labels[idx].permute(\n    ```", "```py\n            1, 2, 0).numpy(), mean, std))\n    ```", "```py\n        plt.axis('off')\n    ```", "```py\n    model = smp.UnetPlusPlus(\n    ```", "```py\n        encoder_name='timm-mobilenetv3_large_100',\n    ```", "```py\n        encoder_weights='imagenet',\n    ```", "```py\n        in_channels=3,\n    ```", "```py\n        classes=3,\n    ```", "```py\n        )\n    ```", "```py\n    model = model.to(device)\n    ```", "```py\n    class VGGPerceptualLoss(torch.nn.Module):\n    ```", "```py\n        def __init__(self):\n    ```", "```py\n            super(VGGPerceptualLoss, self).__init__()\n    ```", "```py\n            blocks = []\n    ```", "```py\n            blocks.append(vgg16(weights=VGG16_Weights.DEFAULT).features[:4].eval())\n    ```", "```py\n            blocks.append(vgg16(weights=VGG16_Weights.DEFAULT).features[4:9].eval())\n    ```", "```py\n            blocks.append(vgg16(weights=VGG16_Weights.DEFAULT).features[9:16].eval())\n    ```", "```py\n            blocks.append(vgg16(weights=VGG16_Weights.DEFAULT).features[16:23].eval())\n    ```", "```py\n            for block in blocks:\n    ```", "```py\n                block = block.to(device)\n    ```", "```py\n                for param in block.parameters():\n    ```", "```py\n                    param.requires_grad = False\n    ```", "```py\n                self.blocks = torch.nn.ModuleList(blocks)\n    ```", "```py\n                self.transform = torch.nn.functional.interpolate\n    ```", "```py\n        def forward(self, input, target):\n    ```", "```py\n            input = self.transform(input, mode='bilinear',\n    ```", "```py\n                size=(224, 224), align_corners=False)\n    ```", "```py\n                target = self.transform(target,\n    ```", "```py\n                    mode='bilinear', size=(224, 224),\n    ```", "```py\n                    align_corners=False)\n    ```", "```py\n                loss = 0.0\n    ```", "```py\n                x = input\n    ```", "```py\n                y = target\n    ```", "```py\n                for i, block in enumerate(self.blocks):\n    ```", "```py\n                    x = block(x)\n    ```", "```py\n                    y = block(y)\n    ```", "```py\n                    loss += torch.nn.functional.l1_loss(\n    ```", "```py\n                        x, y)\n    ```", "```py\n                    act_x = x.reshape(x.shape[0],\n    ```", "```py\n                        x.shape[1], -1)\n    ```", "```py\n                    act_y = y.reshape(y.shape[0],\n    ```", "```py\n                        y.shape[1], -1)\n    ```", "```py\n                    gram_x = act_x @ act_x.permute(\n    ```", "```py\n                        0, 2, 1)\n    ```", "```py\n                    gram_y = act_y @ act_y.permute(\n    ```", "```py\n                        0, 2, 1)\n    ```", "```py\n                    loss += torch.nn.functional.l1_loss(\n    ```", "```py\n                        gram_x, gram_y)\n    ```", "```py\n            return loss\n    ```", "```py\n    optimizer = torch.optim.Adam(model.parameters(),\n    ```", "```py\n        lr=0.001)\n    ```", "```py\n    scheduler = ExponentialLR(optimizer, gamma=0.995)\n    ```", "```py\n    vgg_loss = VGGPerceptualLoss()\n    ```", "```py\n    content_loss_weight=1.\n    ```", "```py\n    style_loss_weight=5e-4\n    ```", "```py\n    train_losses, test_losses = train_style_transfer(\n    ```", "```py\n        model,\n    ```", "```py\n        train_dataloader,\n    ```", "```py\n        test_dataloader,\n    ```", "```py\n        vgg_loss,\n    ```", "```py\n        content_loss_weight,\n    ```", "```py\n        style_loss_weight,\n    ```", "```py\n        device,\n    ```", "```py\n        epochs=50,\n    ```", "```py\n    )\n    ```", "```py\n    plt.plot(train_losses, label='train')\n    ```", "```py\n    plt.plot(test_losses, label='test')\n    ```", "```py\n    plt.ylabel('Loss')\n    ```", "```py\n    plt.xlabel('Epoch')\n    ```", "```py\n    plt.legend()\n    ```", "```py\n    plt.show()\n    ```", "```py\n    images, labels = next(iter(test_dataloader))\n    ```", "```py\n    with torch.no_grad():\n    ```", "```py\n        outputs = model(images.to(device)).cpu()\n    ```", "```py\n    plt.figure(figsize=(12, 6))\n    ```", "```py\n    for idx in range(4):\n    ```", "```py\n        plt.subplot(2, 4, idx*2+1)\n    ```", "```py\n        plt.imshow(unnormalize(images[idx].permute(\n    ```", "```py\n            1, 2, 0).numpy(), mean, std))\n    ```", "```py\n        plt.axis('off')\n    ```", "```py\n        plt.subplot(2, 4, idx*2+2)\n    ```", "```py\n        plt.imshow(unnormalize(outputs[idx].permute(\n    ```", "```py\n            1, 2, 0).numpy(), mean, std).clip(0, 1))\n    ```", "```py\n        plt.axis('off')\n    ```", "```py\ngit clone git@github.com:lllyasviel/ControlNet.git\ncd ControlNet\nconda env create -f environment.yaml\nconda activate control\n```", "```py\nwget https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth -P models/\n```"]