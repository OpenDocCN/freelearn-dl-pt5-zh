- en: '9'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Improving Inference Performance with MXNet
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用MXNet提升推理性能
- en: In previous chapters, we leveraged MXNet’s capabilities to solve **computer
    vision** and **natural language processing tasks**. In those chapters, the focus
    was on obtaining the maximum performance out of **pre-trained models**, leveraging
    the **Model Zoo** API from GluonCV and GluonNLP. We trained these models using
    different approaches from scratch, including **transfer learning** and **fine-tuning**.
    In the previous chapter, we explored how some advanced techniques can be leveraged
    to optimize the training process. Finally, in this chapter, we will focus on improving
    the performance of the inference process itself, accelerating how we can obtain
    results from our models with several topics related to **edge** **AI computing**.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们利用MXNet的功能解决了**计算机视觉**和**自然语言处理任务**。这些章节的重点是从**预训练模型**中获得最大性能，利用GluonCV和GluonNLP的**模型库**API。我们使用从头开始的不同方法训练这些模型，包括**迁移学习**和**微调**。在上一章中，我们探索了如何利用一些高级技术优化训练过程。最后，在本章中，我们将重点提高推理过程本身的性能，加速从我们的模型中获得结果，并讨论与**边缘****AI计算**相关的多个主题。
- en: To achieve the objective of optimizing the performance of our inference pipeline,
    MXNet contains different features. We have already briefly discussed some of those
    features, such as the concept of **Automatic Mixed Precision** (**AMP**), which
    was introduced in the previous chapter to increase the training performance and
    can also be used to increase the inference performance. We will revisit it in
    this chapter, along with other features, such as **hybridization**. Moreover,
    we will further optimize how to use data types efficiently, leveraging the speed-ups
    associated with using the **INT8** data type with **quantization**.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现优化推理管道性能的目标，MXNet包含了不同的功能。我们已经简要讨论过其中的一些功能，例如在前一章中介绍的**自动混合精度**（**AMP**），它可以提高训练性能，同时也可以用来提升推理性能。在本章中，我们将重新讨论这一点，以及其他功能，如**混合化**。此外，我们还将进一步优化如何有效利用数据类型，借助**量化**中的**INT8**数据类型加速推理过程。
- en: Moreover, we will explore how our models work in terms of operations, understanding
    how they work internally with the help of the **MXNet profiler**. We will then
    take a step forward with the help of MXNet GluonCV Model Zoo and learn how to
    export our models to **ONNX**, which allows us to use our models in different
    frameworks, such as deploying our models on NVIDIA hardware platforms, such as
    the **NVIDIA Jetson** family of products.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们将探索我们的模型在操作方面的工作原理，了解它们如何在**MXNet分析器**的帮助下内部运行。然后，我们将借助MXNet GluonCV模型库，进一步学习如何将我们的模型导出为**ONNX**格式，使用该格式，我们可以将模型应用于不同的框架，例如将我们的模型部署到NVIDIA硬件平台上，如**NVIDIA
    Jetson**系列产品。
- en: Finally, we will apply all these techniques together, taking as examples problems
    already explored in the book. For our computer vision task, we will choose image
    segmentation, and for our natural language processing task, we will choose translating
    text from English to German.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将结合应用所有这些技术，选择书中已经探讨过的问题作为例子。对于计算机视觉任务，我们将选择图像分割；对于自然语言处理任务，我们将选择将英文文本翻译成德文。
- en: 'Specifically, this chapter contains the following recipes:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章具体包含以下食谱：
- en: Introducing inference optimization features
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍推理优化功能
- en: Optimizing inference for image segmentation
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化图像分割的推理
- en: Optimizing inference when translating text from English to German
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化将英文文本翻译为德文时的推理
- en: Technical requirements
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'Apart from the technical requirements specified in the *Preface*, the following
    apply:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 除了《前言》中指定的技术要求外，以下内容适用：
- en: Ensure that you have completed *Recipe 1*, *Installing MXNet*, from [*Chapter
    1*](B16591_01.xhtml#_idTextAnchor016), *Up and Running with MXNet*.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了[*第1章*](B16591_01.xhtml#_idTextAnchor016)中的*食谱1*，*安装MXNet*。
- en: Ensure that you have completed [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098),
    *Analyzing Images with Computer Vision*, and [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121),
    *Understanding Text with Natural Language Processing*.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了[*第5章*](B16591_05.xhtml#_idTextAnchor098)，*使用计算机视觉分析图像*，以及[*第6章*](B16591_06.xhtml#_idTextAnchor121)，*利用自然语言处理理解文本*。
- en: Ensure that you have completed [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148),
    *Optimizing Models with Transfer Learning and Fine-Tuning*.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保你已经完成了[*第7章*](B16591_07.xhtml#_idTextAnchor148)，*通过迁移学习与微调优化模型*。
- en: 'The code for this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch09](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch09).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下GitHub网址找到：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch09](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/tree/main/ch09)。
- en: 'Furthermore, you can access each recipe directly from Google Colab, for example,
    for the first recipe of this chapter: [https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch09/9_1_Introducing_inference_optimization_features.ipynb](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch09/9_1_Introducing_inference_optimization_features.ipynb).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你可以直接从Google Colab访问每个配方，例如，本章第一个配方：[https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch09/9_1_Introducing_inference_optimization_features.ipynb](https://github.com/PacktPublishing/Deep-Learning-with-MXNet-Cookbook/blob/main/ch09/9_1_Introducing_inference_optimization_features.ipynb)。
- en: Introducing inference optimization features
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入推理优化功能
- en: In the previous chapters, we have seen how we can leverage MXNet, GluonCV, and
    GluonNLP to retrieve pre-trained models in certain datasets (such as ImageNet,
    MS COCO, or IWSLT2015) and use them for our specific tasks and datasets. Furthermore,
    we used transfer learning and fine-tuning techniques to improve the algorithmic
    performance of those tasks/datasets.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的章节中，我们已经看到如何利用MXNet、GluonCV和GluonNLP从特定数据集（如ImageNet、MS COCO或IWSLT2015）中获取预训练模型，并将其应用于我们的特定任务和数据集。此外，我们还使用了迁移学习和微调技术来提高这些任务/数据集的算法性能。
- en: In this recipe, we will introduce (and revisit) several concepts and features
    that will optimize our inference loops to improve our runtime performance, and
    we will analyze the trade-offs involved.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将介绍（并重温）几个概念和功能，这些内容将优化我们的推理循环，以提高运行时性能，同时分析其中的权衡。
- en: Getting ready
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 做好准备
- en: As in previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前的章节一样，在这个配方中，我们将使用一些矩阵运算和线性代数，但这完全不难。
- en: How to do it...
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will be carrying out the following steps:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们将执行以下步骤：
- en: Hybridizing our models
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 混合我们的模型
- en: Applying float16 and AMP for inference
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用float16和AMP进行推理
- en: Applying quantization by using INT8
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用INT8进行量化
- en: Profiling our models
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对我们的模型进行性能分析
- en: Let’s dive into each of these steps.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解每个步骤。
- en: Hybridizing our models
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 混合我们的模型
- en: In the initial chapters where we were exploring the features of MXNet, we focused
    on **imperative programming**. If you have coded in the past with languages such
    as Java, C/C++, or Python, it is very likely you used imperative programming.
    It is the usual way of coding, as it is more flexible.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在最初的章节中，我们在探索MXNet的特性时，重点介绍了**命令式编程**。如果你以前用过Java、C/C++或Python等语言编程，那么你很可能使用过命令式编程。这是一种常见的编码方式，因为它更灵活。
- en: 'With imperative programming, a step-by-step sequential execution of the statements
    set in the code is expected. For example, typically in our evaluation paths, we
    run these statements step-by-step inside a loop:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在命令式编程中，通常期待代码中的语句按顺序逐步执行。例如，在我们的评估路径中，通常会逐步执行这些语句，通常是在一个循环内部：
- en: Load new samples from our data loader.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从数据加载器中加载新样本。
- en: Transform the input and expected output so that it can be consumed by our model
    and our metrics computations.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换输入和预期输出，以便它们可以被我们的模型和指标计算所使用。
- en: Pass the input through the model to compute the output.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将输入传递给模型以计算输出。
- en: Compare the model output with the expected output and update the corresponding
    metrics.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型输出与预期输出进行比较，并更新相应的指标。
- en: In this programming paradigm, each of the statements is executed in sequence,
    and the output can be checked or debugged for each step if we wait for its completion
    (as MXNet uses **lazy evaluation**).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种编程范式中，每个语句按顺序执行，输出可以在每一步完成后进行检查或调试（因为MXNet使用**惰性求值**）。
- en: With a different programming paradigm, called **symbolic programming**, symbols
    are used instead, which are basically abstractions for operations, and no actual
    computation happens until a defined point (typically known as the compile step).
    This is especially useful for **deep learning**, as all models can be defined
    as graphs, use this graph as a symbol, optimize the operation paths in the underlying
    graph, and only run the optimized computation when needed.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用不同的编程范式，称为**符号编程**，其中使用的是符号，符号本质上是操作的抽象，直到定义的某一点（通常称为编译步骤）才会进行实际计算。这对于**深度学习**尤其有用，因为所有模型都可以定义为图，使用这个图作为符号，优化底层图中的操作路径，并仅在需要时运行优化后的计算。
- en: However, as the computation hasn’t happened yet, the output for each step cannot
    be checked or debugged, making the finding and fixing of issues much more difficult.
    On the other hand, due to the capabilities of graph optimization, symbolic programming
    requires less memory and is faster.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，由于计算尚未发生，因此每个步骤的输出无法检查或调试，这使得查找和修复问题变得更加困难。另一方面，由于图优化的能力，符号编程需要更少的内存且速度更快。
- en: Thankfully, with MXNet, we can leverage the best of both worlds. We can define
    our model with imperative programming, test it, debug it, and fix it with the
    usual mechanisms (*print* statements, tests, debugging, and so on). When we are
    ready for optimization, we just need to call the `hybridize` function, and it
    will take care of everything under the hood, working with our graph in symbolic
    programming. This approach is called hybrid programming and is one of the best
    advantages of MXNet. Moreover, there is no hardware limitation for this feature,
    and it can be used for both CPU and GPU computations.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，使用MXNet，我们可以充分利用两者的优势。我们可以使用命令式编程定义模型，进行测试、调试和修复（通过*print*语句、测试、调试等机制）。当我们准备好进行优化时，我们只需要调用`hybridize`函数，它会处理底层的一切工作，与我们的图形在符号编程中一起工作。这种方法被称为混合编程，是MXNet的最佳优势之一。此外，这个特性没有硬件限制，可以用于CPU和GPU计算。
- en: 'As a toy example, we can run some experiments with the inference of a model
    and compare the different results for different configurations. Specifically,
    these are the configurations we will test:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个玩具示例，我们可以进行一些实验，通过推理模型并比较不同配置的不同结果。具体来说，这些是我们将测试的配置：
- en: 'CPU:'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU：
- en: With imperative execution
  id: totrans-42
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用命令式执行
- en: With symbolic execution and default parameters
  id: totrans-43
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行和默认参数
- en: With symbolic execution with a specific backend
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行和特定后端
- en: With symbolic execution, specific backend, and static allocation of memory
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行、特定后端和静态内存分配
- en: With symbolic execution, specific backend, static allocation of memory, and
    invariant input shapes
  id: totrans-46
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行、特定后端、静态内存分配和不变输入形状
- en: 'GPU:'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GPU：
- en: With imperative execution
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用命令式执行
- en: With symbolic execution and default parameters
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行和默认参数
- en: With symbolic execution and static allocation of memory
  id: totrans-50
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行和静态内存分配
- en: With symbolic execution, static allocation of memory and invariant input shapes
  id: totrans-51
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用符号执行、静态内存分配和不变输入形状
- en: 'Please note that in order to verify the computation time properly, we are adding
    calls to the `mx.nd.waitall()` function. The method chosen is to use the **ADE20K**
    validation split (dataset available from MXNet GluonCV) and process it with a
    **DeepLabv3** model. We will be using a batch size of 4:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，为了正确验证计算时间，我们添加了对`mx.nd.waitall()`函数的调用。选择的方法是使用**ADE20K**验证集（数据集可通过MXNet
    GluonCV获得），并使用**DeepLabv3**模型进行处理。我们将使用批量大小为4：
- en: 'For the initial CPU computing configuration, with imperative execution, the
    processing of the dataset by the model took the following time:'
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于初始的CPU计算配置，使用命令式执行时，模型对数据集的处理时间如下：
- en: '[PRE0]'
  id: totrans-54
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'For the second CPU computing configuration, we just need to leverage the MXNet
    hybrid programming model and transform our model with the following:'
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第二种CPU计算配置，我们只需利用MXNet混合编程模型并使用以下方式转换我们的模型：
- en: '[PRE1]'
  id: totrans-56
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As we can see, the optimizations performed reduced to almost half the computation
    time.
  id: totrans-57
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所看到的，所做的优化将计算时间减少了近一半。
- en: 'For the third CPU computing configuration, we just need to slightly modify
    our hybridization call to define a specific backend. We will leverage our Intel
    CPU architecture, use the `MKLDNN` backend, and transform our model with the following:'
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第三种CPU计算配置，我们只需稍微修改我们的混合化调用以定义特定的后端。我们将利用我们的Intel CPU架构，使用`MKLDNN`后端，并使用以下方式转换我们的模型：
- en: '[PRE2]'
  id: totrans-59
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As we can see, the specific backend further reduced the computation time by
    ~20%.
  id: totrans-60
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，特定的后端进一步将计算时间减少了约20%。
- en: 'For the fourth CPU computing configuration, we just need to slightly modify
    our hybridization call to define that we want to use the static memory allocation.
    We can update our call with the following:'
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第四个CPU计算配置，我们只需要稍微修改我们的混合化调用，定义我们希望使用静态内存分配。我们可以使用以下方式更新我们的调用：
- en: '[PRE3]'
  id: totrans-62
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we can see, the static memory allocation allowed us to reduce the computation
    time by another ~4%.
  id: totrans-63
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，静态内存分配使我们能够将计算时间再减少约4%。
- en: 'For the fifth CPU computing configuration, we just need to slightly modify
    our hybridization call to define that we want to leverage our invariant input
    shapes (we have already preprocessed our data to have the same input shape, `480x480`).
    We can update our call with the following:'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第五个CPU计算配置，我们只需要稍微修改我们的混合化调用，定义我们希望利用不变的输入形状（我们已经预处理了数据，使其具有相同的输入形状，`480x480`）。我们可以使用以下方式更新我们的调用：
- en: '[PRE4]'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE4]'
- en: As we can see, the invariant input shape constraint allowed us to reduce the
    computation time by another ~2%.
  id: totrans-66
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，不变输入形状约束使我们能够将计算时间再减少约2%。
- en: 'For the initial GPU computing configuration, with imperative execution, the
    processing of the dataset by the model took the following time:'
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于初始的GPU计算配置，采用命令式执行，模型处理数据集的时间如下：
- en: '[PRE5]'
  id: totrans-68
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'For the second GPU computing configuration, we just need to leverage the MXNet
    hybrid programming model and transform our model with the following:'
  id: totrans-69
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第二个GPU计算配置，我们只需要利用MXNet混合编程模型，并用以下方式转换我们的模型：
- en: '[PRE6]'
  id: totrans-70
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As we can see, when the optimizations are performed in the GPU, they yield almost
    no improvement in the computation time as GPUs are already optimized internally
    for these types of computations.
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，当在GPU上执行优化时，由于GPU已经针对这些类型的计算进行了内部优化，因此几乎没有提高计算时间。
- en: 'For GPU computing, there are no specific backends to be selected. Therefore,
    for the third GPU computing configuration, we just need to slightly modify our
    hybridization call to define that we want to use static memory allocation. We
    can update our call with the following:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于GPU计算，没有需要选择的特定后端。因此，对于第三个GPU计算配置，我们只需要稍微修改我们的混合化调用，定义我们希望使用静态内存分配。我们可以使用以下方式更新我们的调用：
- en: '[PRE7]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As we can see, the static memory allocation produced another negligible improvement
    on the GPU.
  id: totrans-74
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，静态内存分配在GPU上带来了另一个微不足道的改进。
- en: 'For the fourth GPU computing configuration, we just need to slightly modify
    our hybridization call to define that we want to leverage our invariant input
    shapes (we already preprocessed our data to have the same input shape, 480x480).
    We can update our call with the following:'
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于第四个GPU计算配置，我们只需要稍微修改我们的混合化调用，定义我们希望利用不变的输入形状（我们已经预处理了数据，使其具有相同的输入形状，480x480）。我们可以使用以下方式更新我们的调用：
- en: '[PRE8]'
  id: totrans-76
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE8]'
- en: As we can see, the invariant input shape constraint produced another negligible
    improvement on the GPU.
  id: totrans-77
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 正如我们所见，不变输入形状约束在GPU上带来了另一个微不足道的改进。
- en: The results show that when using the CPU, we can reduce the inference time to
    half the original time, which is a significant improvement. With the GPU, the
    improvements are negligible due to the internal optimizations.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 结果显示，当使用CPU时，我们可以将推理时间减少到原始时间的一半，这是一个显著的改进。使用GPU时，由于内部优化，改进几乎可以忽略不计。
- en: Important Note
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: Please note in the code how we used the `mx.nd.waitall()` function to verify
    that all computations have been strictly completed before computing the time these
    operations took.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在代码中我们是如何使用`mx.nd.waitall()`函数来验证所有计算是否已经严格完成，然后才计算这些操作所花费的时间。
- en: Applying float16 and AMP for inference
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用float16和AMP进行推理
- en: In the previous chapter, [*Chapter 8*](B16591_08.xhtml#_idTextAnchor172), *Improving
    Training Performance with MXNet*, we introduced the `float16` data type and AMP
    optimization, an extremely simple way to leverage this half-precision data type
    only when it was most useful.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章，[*第8章*](B16591_08.xhtml#_idTextAnchor172)，*使用MXNet提升训练性能*，我们介绍了`float16`数据类型和AMP优化，这是一种极为简单的方式，仅在最有用时才使用这一半精度数据类型。
- en: In *Recipe 1*, *Introducing training optimization features*, from the previous
    chapter, we compared single-precision (`float32`) and half-precision (`float16`)
    data types, understanding their characteristics and memory/speed trade-offs. You
    are encouraged to review the recipe if you haven’t already done so as it is very
    relevant to this topic.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在*食谱1*，*介绍训练优化特性*，上一章中，我们比较了单精度（`float32`）和半精度（`float16`）数据类型，理解它们的特性和内存/速度折衷。如果你还没有复习这个食谱，建议你回顾一下，因为它与本主题非常相关。
- en: As most concepts were introduced previously, in this section, we will focus
    on how to apply AMP to the inference process. As usual, MXNet provides a very
    simple interface for this operation, just requiring a call to the `amp.convert_hybrid_block()`
    function.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 正如大多数概念之前已介绍的那样，本节将重点讨论如何将AMP应用于推理过程。像往常一样，MXNet为此操作提供了一个非常简单的接口，只需调用`amp.convert_hybrid_block()`函数即可。
- en: This optimization can be applied to both CPU and GPU environments, so let’s
    run these experiments.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 该优化可以应用于CPU和GPU环境，因此让我们来运行这些实验。
- en: 'To modify our CPU model to use AMP, we just need the following line of code:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要修改我们的CPU模型以使用AMP，我们只需要以下一行代码：
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'With this modified model, the processing of the dataset took the following
    time:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个修改后的模型，处理数据集的时间如下：
- en: '[PRE10]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As we can see, AMP produced negligible improvements on the CPU. This is due
    to the largest gains being achieved on the backward pass required for training,
    but not necessary during inference. Furthermore, CPUs do not typically have specific
    circuitry to work directly with float16, limiting the improvements.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，AMP在CPU上几乎没有产生改善。这是因为最大的收益出现在训练时所需的反向传递过程中，但在推理过程中并不需要。此外，CPU通常没有专门的电路来直接处理float16，这限制了改进效果。
- en: 'To modify the GPU model to use AMP, we just need the following line of code:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要修改GPU模型以使用AMP，我们只需要以下一行代码：
- en: '[PRE11]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'With this modified model, the processing of the dataset took the following
    time:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个修改后的模型，处理数据集的时间如下：
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As we can see, AMP produced excellent results on the GPU, reducing the inference
    time to almost ~25%. This is due to GPUs having specific circuitry to work directly
    with float16, improving the results massively.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，AMP在GPU上产生了优秀的结果，将推理时间减少了大约~25%。这是因为GPU具有专门的电路来直接处理float16，极大地改善了结果。
- en: Important Note
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: The `amp.convert_hybrid_block()` function accepts different parameters. You
    are encouraged to try different options (such as `cast_optional_params`) to find
    the optimal configuration.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '`amp.convert_hybrid_block()`函数接受不同的参数。鼓励你尝试不同的选项（如`cast_optional_params`）以找到最佳配置。'
- en: Applying quantization by using Int8
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用Int8进行量化
- en: In the previous sections, we saw how to optimize our inference loops by using
    different approaches optimizing how to use the CPU and GPU for maximum performance,
    given a model. We also explored how to leverage single-precision (float32) and
    half-precision (float16) data types. In this section, we will explore how our
    data inputs, our model parameters, and the different arithmetic calculations among
    them can be optimized with a new data type, Int8.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们看到如何通过使用不同的方法来优化推理循环，优化如何使用CPU和GPU以获得最大性能，给定一个模型。我们还探讨了如何利用单精度（float32）和半精度（float16）数据类型。在本节中，我们将探讨如何通过一种新的数据类型Int8来优化我们的数据输入、模型参数以及它们之间的不同算术运算。
- en: This data type modification has larger implications than a change in precision.
    We are also modifying the underlying representation from a floating-point number
    to an integer, which yields a reduction in both memory and computing requirements.
    Let’s analyze this data type.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据类型的修改比精度变化更有深远的影响。我们还将底层表示从浮点数修改为整数，这样可以减少内存和计算要求。让我们分析一下这种数据类型。
- en: 'Int8 indicates two things: that it is a data type that only supports integer
    numbers (no floating radix point), and that the amount of bits used to store a
    single number in this format is 8 bits. The most important features of this format
    are the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: Int8表示两件事：它是一种仅支持整数数字（没有浮动小数点）的数据类型，并且在这种格式下存储单个数字所用的位数是8位。此格式的最重要特性如下：
- en: Capability of representing integer numbers from -128 to 127, or from 0 to 255
    (depending on whether it is of the signed or unsigned type)
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能表示从-128到127，或从0到255的整数（取决于它是有符号还是无符号类型）
- en: Constant precision (each consecutive number differs by exactly 1)
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常数精度（每个连续的数字相差恰好1）
- en: 'To explain the core idea behind `Int8` quantization, and also to show the loss
    of precision, we can display the approximated value of the number 1/3 (one third-th)
    in both formats (Float32 and Int8) with this code excerpt:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释`Int8`量化的核心思想，并展示精度损失，我们可以用以下代码片段显示数字1/3（即三分之一）在`Float32`和`Int8`两种格式下的近似值：
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This yields the following:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下结果：
- en: '[PRE14]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As we can see, none of the representations are exact, with `float32` yielding
    a very high precision as expected. With `Int8`, we did a small shortcut; we used
    two 8-bit integers, `85` and `255`, and used one as the scaling factor. This scaling
    factor is typically applied for several sets of numbers at the same time. It can
    be the same scaling factor for the whole model (unlikely), per layer, and so on.
    The scaling factor does not need to be represented in `Int8`; it can be a `float32`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，所有表示方式都不是完全精确的，`float32`表现出了非常高的精度，符合预期。使用`Int8`时，我们做了一个小的简化；我们使用了两个8位整数，`85`和`255`，并用其中一个作为缩放因子。这个缩放因子通常会同时应用于多个数字集。它可以是整个模型的相同缩放因子（不太可能），也可以是每层的缩放因子，等等。这个缩放因子不需要以`Int8`表示，它可以是`float32`。
- en: Important Note
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: For this particular example, the chosen Int8 representation is more exact to
    the intended number, but this is a coincidence. In common scenarios, there is
    a loss of precision that translates to a loss of performance.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的例子，选择的`Int8`表示比目标数值更精确，但这只是巧合。在常见的场景中，存在精度损失，进而导致性能损失。
- en: To minimize this loss of performance, typically quantization tuning techniques
    ask for a **calibration dataset**. This dataset is then used to compute the parameters
    that minimize the mentioned performance loss.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化性能损失，通常量化调优技术会要求一个**校准数据集**。然后，使用该数据集来计算减少性能损失的参数。
- en: In addition to using a calibration dataset, there are several techniques to
    optimize the computation of the most accurate Int8 values, and MXNet provides
    a very simple API to facilitate the optimization of our networks. With a simple
    call to the `mx.contrib.quantization.quantize_net_v2()` function, we will update
    our network to Int8.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使用校准数据集外，还有一些技术可以优化最准确的`Int8`值的计算，而MXNet提供了一个非常简单的API来促进我们网络的优化。通过简单调用`mx.contrib.quantization.quantize_net_v2()`函数，我们将更新我们的网络为`Int8`。
- en: 'Specifically, for our experiments, this is the call that we used:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 特别是，对于我们的实验，这就是我们使用的调用：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Important Note
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示
- en: '`Int8` quantization is a nuanced process, and tailoring it for a specific application
    requires in-depth analysis and some trial-and-error experiments. For more information
    regarding the parameters involved, you are encouraged to read the function documentation:
    [https://github.com/apache/mxnet/blob/v1.9.1/python/mxnet/contrib/quantization.py#L825](https://github.com/apache/mxnet/blob/v1.9.1/python/mxnet/contrib/quantization.py#L825).'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: '`Int8`量化是一个复杂的过程，针对特定应用的定制需要深入的分析和一些反复试验。关于涉及的参数，建议阅读以下函数文档：[https://github.com/apache/mxnet/blob/v1.9.1/python/mxnet/contrib/quantization.py#L825](https://github.com/apache/mxnet/blob/v1.9.1/python/mxnet/contrib/quantization.py#L825)。'
- en: 'With this modified CPU-based model, the processing of the dataset took the
    following time:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个修改后的基于CPU的模型，处理数据集所需的时间如下：
- en: '[PRE16]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: As we can see, `Int8` produced a strong improvement in the CPU, yielding almost
    another ~50% reduction in runtime.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，`Int8`在CPU上产生了显著的提升，几乎减少了约50%的运行时间。
- en: Unfortunately, for GPUs, this feature cannot be introduced. Although recent
    GPUs have dedicated `Int8` circuitry, this is quite a new development and MXNet
    does not support these operators yet.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，对于GPU，这个特性无法引入。尽管最近的GPU有专门的`Int8`电路，但这还是一个比较新的发展，MXNet尚不支持这些操作符。
- en: Profiling our models
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 对我们的模型进行分析
- en: 'In this recipe, we have seen how to use different techniques to optimize our
    inference loops. However, sometimes, even after introducing these optimization
    techniques, our models might not reach the runtime performance we are targeting.
    This could be due to a number of reasons:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个配方中，我们已经看到如何使用不同的技术来优化推理循环。然而，有时即使引入了这些优化技术，我们的模型仍可能无法达到我们预期的运行时性能。这可能是由于以下几个原因：
- en: Architecture is not optimal for edge computing.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构并不适合边缘计算。
- en: Operators have not been optimized adequately.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 操作符尚未得到充分优化。
- en: Data transfers among components.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组件之间的数据传输。
- en: Memory leaks.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存泄漏。
- en: In order to verify how our model is working internally, to check where to optimize
    further and/or investigate the possible reasons why our models might not be performing
    well, MXNet provides us with a tool for low-level analysis called the MXNet profiler.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证我们的模型内部是如何工作的，检查需要进一步优化的地方和/或调查模型性能不佳的可能原因，MXNet 提供了一种低级分析工具，称为 MXNet 性能分析器。
- en: The MXNet profiler runs in the background and records all operations and data
    transfers happening on our models in real time. It is also very lightweight, consuming
    a minimal amount of resources. Best of all, it is extremely easy to configure
    and use.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: MXNet 性能分析器在后台运行，实时记录模型中发生的所有操作和数据传输。它也非常轻量，占用的资源非常少。最重要的是，它极易配置和使用。
- en: 'In order to profile a set of statements, we need to take two steps:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析一组语句，我们需要采取两个步骤：
- en: Configure the profiler.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置性能分析器。
- en: Start and stop the profiler before and after the statements to be profiled.
  id: totrans-131
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在要进行性能分析的语句之前和之后启动与停止性能分析器。
- en: 'To configure the profiler, we just need one line of code, as follows:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要配置性能分析器，我们只需要一行代码，如下所示：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To start and stop the profiler, we need to add the following lines at the beginning
    and end of the statements to be analyzed:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动和停止性能分析器，我们需要在要分析的语句的开始和结束处添加以下几行代码：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Please note how we need three statements to stop recording: finalize all instructions,
    stop recording, and dump the information of the file configured in the first step.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们需要三条语句来停止录制：完成所有指令、停止录制，并转储在第一步中配置的文件的信息。
- en: Important Note
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 重要说明
- en: Please note in the code how we used the `mx.nd.waitall()` function to verify
    that all computations have been strictly completed before computing the time these
    operations took.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意代码中我们如何使用 `mx.nd.waitall()` 函数来验证所有计算已严格完成，然后再计算这些操作所花费的时间。
- en: 'The instructions described previously generate a JSON file, which can then
    be analyzed with tracing applications. I recommend the Tracing app included with
    Google Chrome as it is very easy to use and access. Simply type the following
    in the address bar: `chrome://tracing`.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述的指令会生成一个 JSON 文件，随后可以通过追踪应用程序进行分析。我推荐使用 Google Chrome 中包含的 Tracing 应用程序，因为它非常易于使用和访问。只需在地址栏中输入以下内容：`chrome://tracing`。
- en: In order to verify the functionality of the MXNet profiler, let’s take the example
    of **ResNet** architectures, which are used extensively, such as in our image
    segmentation task, being the backbone of the DeepLabv3 network we use.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证 MXNet 性能分析器的功能，我们以广泛使用的**ResNet**架构为例，这些架构在我们的图像分割任务中被广泛应用，作为我们所使用的 DeepLabv3
    网络的骨干。
- en: 'The typical architecture of a ResNet network is the following:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet 网络的典型架构如下：
- en: '![Figure 9.1 – ResNet50 model architecture](img/B16591_09_1.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.1 – ResNet50 模型架构](img/B16591_09_1.jpg)'
- en: Figure 9.1 – ResNet50 model architecture
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.1 – ResNet50 模型架构
- en: Please note that in *Figure 9.1*, the initial steps (stage 1) are convolution,
    batch normalization, and activation.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在*图 9.1*中，初始步骤（阶段 1）是卷积、批量归一化和激活。
- en: 'From our profiled model, the Google Chrome Tracing app provides the following
    screen:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们分析过的模型中，Google Chrome Tracing 应用程序提供了以下屏幕：
- en: '![Figure 9.2 – Profiling ResNet](img/B16591_09_2.jpg)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.2 – 性能分析 ResNet](img/B16591_09_2.jpg)'
- en: Figure 9.2 – Profiling ResNet
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 – 性能分析 ResNet
- en: 'In *Figure 9.2*, we can see the general execution of the model. Zooming on
    the earlier layers, we can see it as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.2*中，我们可以看到模型的一般执行情况。放大早期层，我们可以看到如下：
- en: '![Figure 9.3 – Profiling ResNet: Zoom](img/B16591_09_3.jpg)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.3 – 性能分析 ResNet: 放大](img/B16591_09_3.jpg)'
- en: 'Figure 9.3 – Profiling ResNet: Zoom'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '图 9.3 – 性能分析 ResNet: 放大'
- en: In *Figure 9.3*, we can see how the stage 1 steps of convolution, batch normalization,
    and activation are clearly displayed. We can also now very clearly see how the
    batch normalization operation takes about 4x longer than the convolution and activation
    steps, potentially indicating an avenue of improvement.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在*图 9.3*中，我们可以看到阶段 1 的卷积、批量归一化和激活步骤被清晰地展示出来。我们还可以非常清楚地看到，批量归一化操作所需的时间大约是卷积和激活步骤的
    4 倍，这可能表明了一个优化的方向。
- en: How it works...
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 它的工作原理……
- en: 'In this recipe, we have taken a deeper look into how MXNet and Gluon can help
    us optimize our inference loops. We have leveraged our hardware (CPUs and GPUs)
    by addressing each of the steps in the inference loop:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们深入探讨了 MXNet 和 Gluon 如何帮助我们优化推理循环。我们通过解决推理循环中的每一个步骤，充分利用了我们的硬件（CPU 和 GPU）：
- en: Reused the work done for data loading in the previous chapter
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重用了上一章中数据加载的工作
- en: Optimized graph computation via hybridization
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过混合化优化图计算
- en: Analyzed different data types and combined the accuracy and precision of float32
    with the speed-ups of `float16` (leveraging the specific circuitry of GPUs) where
    possible, using AMP
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析了不同的数据类型，并在可能的情况下，使用AMP结合`float16`的加速与`float32`的精度，利用GPU的特定电路
- en: Taken another step forward by using Int8 quantization
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用Int8量化迈出了进一步的步伐
- en: Analyzed low-level performance using the MXNet profiler
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MXNet分析器分析了低级性能
- en: 'We compared each of these features by running several experiments and comparing
    the performance before and after a specific optimization, emphasizing potential
    trade-offs that have to be taken into account when using these optimizations.
    To summarize, these were the results:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过运行多个实验，比较了每个特性的效果，比较了在特定优化前后的性能，强调了使用这些优化时需要考虑的潜在权衡。总结来说，结果如下：
- en: '| **Feature** | **Result on** **CPU (ms)** | **Result on** **GPU (ms)** |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| **特性** | **CPU上的结果（毫秒）** | **GPU上的结果（毫秒）** |'
- en: '| Standard | 115 | 13.3 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 标准 | 115 | 13.3 |'
- en: '| Hybridize / Default | 65 | 12.9 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 混合化 / 默认 | 65 | 12.9 |'
- en: '| Hybridize / MKLDNN | 56 | N/A |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 混合化 / MKLDNN | 56 | N/A |'
- en: '| Hybridize / MKLDNN + Static Alloc | 54 | 12.8 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 混合化 / MKLDNN + 静态分配 | 54 | 12.8 |'
- en: '| Hybridize / MKLDNN + Static Alloc + Invariant Shape | 52 | 12.6 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 混合化 / MKLDNN + 静态分配 + 不变形状 | 52 | 12.6 |'
- en: '| AMP | 54 | 3.5 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| AMP | 54 | 3.5 |'
- en: '| Int8 Quantization | 36 | N/A |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| Int8量化 | 36 | N/A |'
- en: Table 9.1 – Summary of features and results for the CPU and GPU
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.1 – CPU和GPU的特性及结果汇总
- en: 'In the next recipes, we will apply all these optimization techniques concurrently
    for the best cases for the CPU (MKL-DNN + static allocation + invariant shape
    + Int8 quantization) and GPU (static allocation + invariant shape + automatic
    mixed precision) to optimize two familiar tasks: image segmentation and text translation.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的食谱中，我们将同时应用所有这些优化技术，针对CPU（MKL-DNN + 静态分配 + 不变形状 + Int8量化）和GPU（静态分配 + 不变形状
    + 自动混合精度）优化两项常见任务：图像分割和文本翻译。
- en: There’s more…
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 还有更多…
- en: 'All the optimization features shown in this recipe have been thoroughly described
    in the literature. In this section, we share some introductory links to start
    understanding each of the features in depth:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 本食谱中展示的所有优化特性都在文献中进行了详细描述。在本节中，我们分享了一些入门链接，以便深入理解每个特性：
- en: '**Hybridization**: [https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**混合化**：[https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
- en: '**Automatic Mixed Precision (AMP)**: [https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动混合精度（AMP）**：[https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
- en: '**Int8 quantization**: [https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Int8量化**：[https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
- en: '**MXNet profiler**: [https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**MXNet分析器**：[https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html](https://mxnet.apache.org/versions/1.9.1/api/python/docs/tutorials/packages/gluon/blocks/hybridize.html)'
- en: Optimizing inference for image segmentation
  id: totrans-176
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 优化图像分割的推理
- en: In the previous recipe, we saw how we can leverage MXNet and Gluon to optimize
    the inference of our models, applying different techniques, such as improving
    the runtime performance using hybridization; how using half-precision (float16)
    in combination with AMP can strongly reduce our inference times; and how to take
    advantage of further optimizations with data types such as Int8 quantization.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前的食谱中，我们展示了如何利用 MXNet 和 Gluon 来优化模型推理，应用了不同的技术，例如使用混合化提高运行时性能；如何结合 AMP 使用半精度（float16）显著减少推理时间；以及如何利用
    Int8 量化等数据类型进一步优化。
- en: Now, we can revisit a problem we have been working with throughout the book,
    image segmentation. We have worked with this task in recipes from previous chapters.
    In *Recipe 4*, *Segmenting objects semantically with MXNet Model Zoo – PSPNet
    and DeepLabv3*, from [*Chapter 5*](B16591_05.xhtml#_idTextAnchor098), *Analyzing
    Images with Computer Vision*, we introduced the task and the datasets that we
    will be using in this recipe, *MS COCO and Penn-Fudan Pedestrian*, and learned
    how to use pre-trained models from GluonCV Model Zoo.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以重新审视本书中一直在处理的一个问题：图像分割。我们在前几章的食谱中已经处理过这个任务。在*食谱 4*，《使用 MXNet Model Zoo
    进行语义图像分割—PSPNet 和 DeepLabv3》中，来自[*第 5 章*](B16591_05.xhtml#_idTextAnchor098)《使用计算机视觉分析图像》，我们介绍了这个任务以及我们将在本食谱中使用的数据集，*MS
    COCO 和 Penn-Fudan Pedestrian*，并学习了如何使用来自 GluonCV Model Zoo 的预训练模型。
- en: Furthermore, in *Recipe 3*, *Improving performance for segmenting images*, from
    [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148), *Optimizing Models with Transfer
    Learning and Fine-Tuning*, we compared the different approaches that we could
    take when dealing with a target dataset, training our models from scratch or leveraging
    past knowledge from pre-trained models and adjust them for our task, using the
    different modalities of transfer learning and fine-tuning. Lastly, in *Recipe
    2*, *Optimizing training for image segmentation*, from [*Chapter 8*](B16591_08.xhtml#_idTextAnchor172),
    *Improving Training Performance with MXNet*, we applied different techniques to
    improve the runtime performance of our training loops.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，在*食谱 3*，《提升图像分割性能》中，来自[*第 7 章*](B16591_07.xhtml#_idTextAnchor148)《通过迁移学习和微调优化模型》中，我们比较了处理目标数据集时可以采取的不同方法，是否从头开始训练我们的模型，或者利用预训练模型的先验知识并针对我们的任务进行调整，使用不同的迁移学习和微调方式。最后，在*食谱
    2*，《优化图像分割训练》中，来自[*第 8 章*](B16591_08.xhtml#_idTextAnchor172)《通过 MXNet 提升训练性能》中，我们应用了不同的技术来提升训练循环的运行时性能。
- en: Therefore, in this recipe, we will apply all the introduced optimization techniques
    for the specific task of optimizing the inference of an image segmentation model.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在本食谱中，我们将应用所有介绍的优化技术，专注于优化图像分割模型的推理任务。
- en: Getting ready
  id: totrans-181
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 准备工作
- en: As in previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前的章节一样，在本食谱中，我们将使用一些矩阵运算和线性代数，但这绝对不难。
- en: How to do it...
  id: totrans-183
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何操作...
- en: 'In this recipe, we will be using the following steps:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个食谱中，我们将使用以下步骤：
- en: Applying inference optimization techniques
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用推理优化技术
- en: Visualizing and profiling our models
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化和分析我们的模型
- en: Exporting our models to ONNX and TensorRT
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将我们的模型导出到 ONNX 和 TensorRT
- en: Let’s dive into each of these steps.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨每个步骤。
- en: Applying inference optimization techniques
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 应用推理优化技术
- en: In *Recipe 1*, *Introducing inference optimization features*, at the beginning
    of this chapter, we showed how different optimization techniques could improve
    the performance of the different steps we take in the inference of a machine learning
    model, including hybridization, AMP, and Int8 quantization.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在*食谱 1*，《介绍推理优化功能》中，我们展示了不同的优化技术如何提高推理过程中各个步骤的性能，包括混合化、AMP 和 Int8 量化。
- en: In this section, we will show how, with MXNet and Gluon, just with a few lines
    of code, we can easily apply each and every technique we've introduced and verify
    the results of each technique.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一部分，我们将展示如何仅通过几行代码，在 MXNet 和 Gluon 中轻松应用我们介绍的每一项技术，并验证每项技术的结果。
- en: 'Without applying these optimization techniques, as a baseline, these are the
    quantitative results obtained with the CPU:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 如果不应用这些优化技术，作为基准，以下是使用 CPU 获取的定量结果：
- en: '[PRE19]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We can display an image for qualitative results:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以显示图像以获取定性结果：
- en: '![Figure 9.4 – Qualitative results: CPU baseline](img/B16591_09_4.jpg)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.4 – 定性结果：CPU 基准](img/B16591_09_4.jpg)'
- en: 'Figure 9.4 – Qualitative results: CPU baseline'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
- en: As expected from the quantitative metrics, *Figure 9.4* shows excellent results
    as well.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
- en: 'As concluded in the previous recipe, for maximum performance on the CPU, the
    best approach is the following:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
- en: 'Use hybridization: Using the Intel MKL-DNN backend, combined with static memory
    allocation and invariant input shapes.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not use AMP.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Int8 quantization.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s apply each of these techniques for our current specific task, image segmentation.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
- en: 'For hybridization, we just need one line of code (which includes the necessary
    parameters for the Intel `MKLDNN` backend, combined with static memory allocation
    and invariant input shapes):'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We do not need to add an AMP step as it was shown not to add benefits to CPU-based
    workloads.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
- en: 'For `Int8` quantization, we need two separate steps. On one hand, we need to
    define the calibration dataset. This can be achieved with a small number of lines:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Then, to apply `Int8` quantization, optimized using the calibration dataset,
    just another line of code is required:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Applying these optimization techniques, these are the quantitative results
    obtained for an optimized CPU inference:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As we can see, the differences in performance (`0.959` versus `0.960` and `0.473`
    versus `0.474`) are negligible. However, with these inference optimization techniques,
    we have been able to reduce the inference runtime by 4x (8.4 seconds versus 27.6
    seconds), which is an impressive result.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also display an image for qualitative results:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.5 – Qualitative results: CPU-optimized inference](img/B16591_09_5.jpg)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.5 – Qualitative results: CPU-optimized inference'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
- en: As expected from the quantitative metrics, *Figure 9.5* shows excellent results
    as well, with negligible differences (if any).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
- en: 'What about GPU-based inference? Let’s follow the same steps:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
- en: 'Without applying these optimization techniques, as a baseline, these are the
    quantitative results obtained with the GPU:'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-219
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: As expected, there is no change relative to algorithmic performance from the
    CPU baseline. The runtime inference is indeed twice as fast in the GPU (`13.1`
    seconds versus `27.6` seconds).
  id: totrans-220
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We can display an image for qualitative results:'
  id: totrans-221
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 9.6 – Qualitative results: GPU baseline](img/B16591_09_6.jpg)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.6 – Qualitative results: GPU baseline'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
- en: As expected from the quantitative metrics, *Figure 9.6* shows excellent results
    as well.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
- en: 'As concluded in the previous recipe, for maximum performance on the GPU, the
    best approach is the following:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
- en: '**Use hybridization**: Using static memory allocation and invariant input shapes.
    Do not use the Intel MKL-DNN backend.'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use AMP**.**
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not use Int8 quantization (not supported).
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s apply each of these techniques to our current specific task, image segmentation.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
- en: 'For hybridization, we just need one line of code (which includes the necessary
    parameters for static memory allocation and invariant input shapes):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'For AMP, we need to follow two simple steps, a forward pass and the conversion
    of the model, as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于AMP，我们需要遵循两个简单的步骤，即前向传播和模型的转换，如下所示：
- en: '[PRE26]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: No further steps are required.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 不需要进一步的步骤。
- en: 'By applying these optimization techniques, these are the quantitative results
    obtained for an optimized GPU inference:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 通过应用这些优化技术，我们获得了优化GPU推断的量化结果：
- en: '[PRE27]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As we can see, the differences in performance (`0.960` versus `0.960` and `0.474`
    versus `0.474`) are non-existent. Furthermore, with these inference optimization
    techniques, we have been able to reduce the inference runtime by 15x (0.85 seconds
    versus 13.1 seconds), which is an impressive result.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所看到的，性能上的差异（`0.960`与`0.960`，以及`0.474`与`0.474`）是不存在的。此外，通过这些推断优化技术，我们已经成功将推断运行时间缩短了15倍（0.85秒与13.1秒），这是一个令人印象深刻的成果。
- en: 'We can also display an image for qualitative results:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以显示一张图片来展示定性结果：
- en: '![Figure 9.7 – Qualitative results: GPU-optimized inference](img/B16591_09_7.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7 – 定性结果：GPU优化推断](img/B16591_09_7.jpg)'
- en: 'Figure 9.7 – Qualitative results: GPU-optimized inference'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 – 定性结果：GPU优化推断
- en: As expected from the quantitative metrics, *Figure 9.7* shows excellent results
    as well, with negligible differences (if any) from the results in *Figure 9.6*.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 正如定量指标所预期的那样，*图9.7*也显示了出色的结果，与*图9.6*中的结果几乎没有（如果有的话）可忽略的差异。
- en: Visualizing and profiling our models
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化和剖析我们的模型
- en: In the previous sections, we saw the different techniques that we could apply
    to optimize our inference loops, and the results these techniques achieved. However,
    how exactly do these techniques work? Why are they faster?
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几节中，我们看到了可以应用于优化推断循环的不同技术，以及这些技术所取得的结果。但是，这些技术究竟是如何工作的？为什么它们更快？
- en: 'We are going to use two tools that MXNet provides for exactly this purpose:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用MXNet提供的两个工具来达到这个目的：
- en: '**Model visualization**'
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型可视化**'
- en: '**Model profiling**'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型剖析**'
- en: Model visualization provides us with an intuitive way to see how the different
    layers interact with each other. This is particularly interesting for networks
    that use ResNet backbones (such as `DeepLabv3`, which we use for image segmentation
    in this recipe) because of the **residuals** being transferred through layers.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可视化为我们提供了一种直观的方式来看待不同层之间的交互。对于使用ResNet骨干（例如我们在本文档中用于图像分割的`DeepLabv3`），这尤为重要，因为**残差**通过层进行传递。
- en: 'Visualizing our model architecture with MXNet is very easy. When working with
    symbolic models, just one line of code is necessary. In our case, as we work with
    Gluon models, these are the lines of code necessary:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 用MXNet可视化我们的模型架构非常简单。当使用符号模型时，只需一行代码即可。在我们的情况下，由于我们使用Gluon模型，需要以下几行代码：
- en: '[PRE28]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: As shown in the previous recipe, ResNet-based networks are composed of ResNet
    blocks, which include the convolution, batch normalization, and activation steps.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前一篇文章所示，基于ResNet的网络由ResNet块组成，其中包括卷积、批量归一化和激活步骤。
- en: 'For our CPU-optimized model (hybridized and `Int8`-quantized), this is what
    some of the connections among those blocks look like:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的CPU优化模型（混合化和`Int8`量化），以下是这些块之间部分连接的样子：
- en: '![Figure 9.8 – GraphViz of ResNet blocks (CPU-optimized)](img/B16591_09_8.jpg)'
  id: totrans-252
  prefs: []
  type: TYPE_IMG
  zh: '![图9.8 – ResNet块的GraphViz（CPU优化）](img/B16591_09_8.jpg)'
- en: Figure 9.8 – GraphViz of ResNet blocks (CPU-optimized)
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 – ResNet块的GraphViz（CPU优化）
- en: As we can see in *Figure 9.8*, there are no individual blocks for each of the
    expected ResNet block operations; they are all part of single blocks that perform
    all computations. This combination of operations is aptly called operator fusion,
    where as many operations as possible are fused together, instead of computing
    an operation and then the next one (with the typical data transfers occurring).
    The largest benefit is that fused operations can happen in the same memory space.
    This is one of the optimizations performed by hybridization, as once the graph
    for the network is finished, it is quite straightforward to find the operations
    that are candidates to be fused.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在*图9.8*中所看到的，预期的ResNet块操作没有单独的块；它们都是执行所有计算的单个块的一部分。这些操作的组合称为运算符融合，其中尽可能多的操作被融合在一起，而不是计算一个操作，然后是下一个操作（通常发生数据传输）。最大的好处在于融合的操作可以在相同的内存空间中进行。这是混合化执行的优化之一，因为一旦网络的图形完成，很容易找到候选融合操作。
- en: OK, so the model visualization tells us those optimizations will happen, but
    how can we verify they are actually happening? This is what model profiling is
    good at, and can also help us understand issues happening during runtime.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the recipe, *Introducing inference optimization features*,
    and the section Profiling our models, the output of model profiling is a JSON
    file that can be visualized with tools such as the Google Chrome Tracing app.
    For a non-optimized CPU workload, our `DeepLabv3` model shows the following timing
    profile:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.9 – Profiling DeepLabv3: Non-optimized CPU workload](img/B16591_09_9.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.9 – Profiling DeepLabv3: Non-optimized CPU workload'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 9.9*, we can see the following characteristics:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
- en: Almost all of the tasks are handled by a single process.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Around 80 ms into the operation, all tasks have been sent to be dispatched,
    and the control is returned for operations to continue (lazy evaluation and `mx.nd.waitall`).
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All tasks have been sent to be dispatched around 80ms into the operation, and
    the control is returned for operations to continue (lazy evaluation and mx.nd.waitall).
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All operations are atomic and executed individually.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full operation takes around 800 ms.
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a CPU-optimized workload, our DeepLabv3 model shows the following timing
    profile:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.10 – Profiling DeepLabv3: Optimized CPU workload](img/B16591_09_10.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.10 – Profiling DeepLabv3: Optimized CPU workload'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 9.10*, we can see the following characteristics:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
- en: Almost all of the tasks are handled by a single process, similar to the non-optimized
    counterpart.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Around 5 ms into the operation, all tasks have been sent to be dispatched, and
    the control is returned for operations to continue (lazy evaluation and `mx.nd.waitall`),
    much faster than the non-optimized counterpart.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory is used in a synchronous/structured way, in stark contrast to the non-optimized
    counterpart.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All operations are fused together, again in stark contrast to the non-optimized
    counterpart.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full operation takes around 370 ms.
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In summary, for CPU-based optimizations, we can clearly see the effects:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
- en: Hybridization has fused all operators together, basically executing almost the
    full workload in a single operation.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MKL-DNN backend and Int8 quantization have improved those operations with
    accelerated operators.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our GPU-optimized model (hybridized and AMP-ed), this is what some of the
    connections among ResNet blocks look like:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.11 – GraphViz of ResNet blocks (GPU-optimized)](img/B16591_09_11.jpg)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
- en: Figure 9.11 – GraphViz of ResNet blocks (GPU-optimized)
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
- en: As we can see in *Figure 9.11*, this is a very different visualization than
    the CPU-optimized one as all the individual blocks of the expected ResNet block
    operations can be identified. As we saw in the first recipe of this chapter, hybridization
    had a very limited effect on GPUs.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
- en: So, where do the accelerations come from? Let’s get some help from model profiling.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
- en: 'For a non-optimized GPU workload, our DeepLabv3 model shows the following timing
    profile:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.12 – Profiling DeepLabv3: Non-optimized GPU workload](img/B16591_09_12.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.12 – Profiling DeepLabv3: Non-optimized GPU workload'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 9.12*, we can see the following characteristics:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
- en: Almost all of the tasks are handled by two GPU processes.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Around 40 ms into the operation, all tasks have been sent to be dispatched,
    and the control is returned for operations to continue (lazy evaluation and `mx.nd.waitall`).
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Asynchronous/unstructured usage of memory.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All operations are atomic and executed individually.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full operation takes around 150 ms.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a GPU-optimized workload, our DeepLabv3 model shows the following timing
    profile:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.13 – Profiling DeepLabv3: Optimized GPU workload](img/B16591_09_13.jpg)'
  id: totrans-292
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.13 – Profiling DeepLabv3: Optimized GPU workload'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 9.13*, we can see the following characteristics:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
- en: Almost all of the tasks are handled by two processes, similar to the non-optimized
    counterpart.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Around 4 ms into the operation, all tasks have been sent to be dispatched, and
    the control is returned for operations to continue (lazy evaluation and `mx.nd.waitall`),
    much faster than the non-optimized counter-part.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronous/structured usage of memory, in stark contrast to the non-optimized
    counterpart.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All operations are atomic and executed individually; similar to the non-optimized
    counterpart, they are just much faster. For example, large convolution operations
    take ~1 ms in the GPU non-optimized case, whereas they take one-third of that
    time (~0.34 ms) in the GPU-optimized case.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full operation takes around 55 ms (one-third of the non-optimized time).
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In summary, for GPU-based optimizations, we can clearly see the effects:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
- en: Hybridization, as expected, has no effect and no operator fusion can be identified.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: AMP makes operations run much faster if the GPU has float16-dedicated circuitry.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exporting our models to ONNX and TensorRT
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'MXNet and GluonCV also provide tools to export our models externally. This
    makes the most sense for optimizing runtime computation times (inference) might
    need:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
- en: Specific algorithms that MXNet/GluonCV might not support
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deployment and optimizations on specific hardware platforms
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this section, we are going to study one example of each category.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
- en: For specific algorithms, we are going to export our models in ONNX format. **ONNX**
    stands for **Open Neural Network eXchange** and is an open format that describes
    how deep learning models can be stored and shared. This is extremely useful to
    leverage specific tools for highly specialized tasks. For example, **ONNX Runtime**
    has really powerful inference tools, including quantization (for example, ONNX
    Runtime has support for GPU-based INT8 quantization). Therefore, we can export
    our model in ONNX format and start working directly with ONNX Runtime.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, MXNet will allow us to accomplish this with just a few lines of code.
    We will need to carry out two steps. Firstly, we need to transform our model from
    Gluon to symbolic format (hybridizing and then exporting):'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Next, we can transform the symbolic model into ONNX:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'ONNX also provides a checker to verify our model has been exported correctly.
    This can be done with the following lines of code:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-314
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: And that’s it! Following these instructions, we will have our ONNX model stored
    in a file (in our example, `'deeplab_resnet101_coco_pt_gpu_hybrid.onnx'`), ready
    to be used with any tool that accepts ONNX models as input.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, sometimes we would like to deploy and/or optimize our models
    on specific hardware platforms, such as the NVIDIA family of products (for example,
    Nvidia Jetson platforms). Specifically, Nvidia works with a specific machine learning
    framework designed to run inference on their own hardware. This framework is called
    **TensorRT**.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
- en: Although MXNet features direct TensorRT integration, it’s not enabled by default,
    requiring building MXNet directly from the source, with specific parameters enabling
    TensorRT integration. Much more straightforwardly, we can leverage our recently
    described ONNX export to generate a TensorRT-capable model.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
- en: 'To achieve this, it is enough to write a few lines of code:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: With this, we will write a serialized TensorRT-capable model.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
- en: 'We can verify the model can be read by deserializing and reading it. We can
    do so with the following lines of code:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: And we are done! In this section, we have been able to successfully write ONNX
    and TensorRT models. Congratulations!
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-324
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we have applied the different inference optimization techniques
    seen in the first recipe of this chapter, leveraging our hardware (CPUs and GPUs)
    to optimize our model runtime performance by doing the following:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
- en: Hybridizing the model
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Leveraging AMP
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Quantizing with the INT8 data type for accelerated inference
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Moreover, we have learned how to use model visualizations (powered by GraphViz)
    and the MXNet profiler and have used these tools to analyze the inference optimizations
    from a low-level perspective.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have learned how to export our models for specific scenarios and
    purposes, using the ONNX and TensorRT libraries.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-331
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we have presented the inference optimization problem from a
    post-training perspective. We were given a (pre-)trained model and tried to squeeze
    as much performance as we could from it.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is another avenue that can be explored, which starts thinking
    about maximizing inference performance from a machine learning model design perspective.
    This is known as **model compression** and is an active area of research, with
    lots of improvements published periodically. Recently active research topics include
    the following:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
- en: '**Knowledge distillation**: [https://arxiv.org/pdf/1503.02531.pdf](https://arxiv.org/pdf/1503.02531.pdf)'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Pruning**: [https://arxiv.org/pdf/1510.00149.pdf](https://arxiv.org/pdf/1510.00149.pdf)'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Quantization-aware training**: [https://arxiv.org/pdf/1712.05877.pdf](https://arxiv.org/pdf/1712.05877.pdf)'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizing inference when translating text from English to German
  id: totrans-337
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In the initial recipe, we saw how we can leverage MXNet and Gluon to optimize
    the inference of our models, applying different techniques: improving the runtime
    performance using hybridization; how using half-precision (float16) in combination
    with AMP can strongly reduce our inference times; and how to take advantage of
    further optimizations with data types such as Int8 quantization.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can revisit a problem we have been working with throughout the book:
    translating English to German. We have worked with translation tasks in recipes
    from previous chapters. In *Recipe 4*, *Translating text from Vietnamese to English*,
    from [*Chapter 6*](B16591_06.xhtml#_idTextAnchor121), *Understanding Text with
    Natural Language Processing*, we introduced the task of translating text, while
    also learning how to use pre-trained models from GluonCV Model Zoo.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, in *Recipe 4*, *Improving performance for translating English
    to German*, from [*Chapter 7*](B16591_07.xhtml#_idTextAnchor148), *Optimizing
    Models with Transfer Learning and Fine-Tuning*, we introduced the datasets that
    we will be using in this recipe: **WMT 2014** and **WMT 2016**. We also compared
    the different approaches that we could take when dealing with a target dataset:
    training our models from scratch or leveraging past knowledge from pre-trained
    models and adjusting them for our task, using the different modalities of transfer
    learning and fine-tuning. Lastly, in *Recipe 3*, *Optimizing training for translating
    English to German*, from [*Chapter 8*](B16591_08.xhtml#_idTextAnchor172), *Improving
    Training Performance with MXNet*, we applied different techniques to improve the
    runtime performance of our training loops.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
- en: Therefore, in this recipe, we will apply all the introduced optimization techniques
    for the specific task of optimizing the inference for translating English to German.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  id: totrans-342
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As in previous chapters, in this recipe, we will be using some matrix operations
    and linear algebra, but it will not be hard at all.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
- en: How to do it...
  id: totrans-344
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will be carrying out the following steps:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
- en: Applying inference optimization techniques
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Profiling our models
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Exporting our models
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Let’s dive into each of these steps.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
- en: Applying inference optimization techniques
  id: totrans-350
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Recipe 1*, *Introducing inference optimization features*, at the beginning
    of this chapter, we showed how different optimization techniques could improve
    the performance of the different steps we take in the inference of a machine learning
    model, including hybridization, AMP, and Int8 quantization.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we will show how, with MXNet and Gluon, just with a few lines
    of code, we can easily apply each and every technique introduced and verify the
    results of each technique.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
- en: 'Without applying these optimization techniques, as a baseline, these are the
    quantitative results obtained with the CPU:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'From a qualitative point of view, we can also check how well our model is performing
    with a sentence example. In our case, we chose *I learn new things every day*,
    and the output obtained is the following:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: The German sentence obtained in the output (*Ich lerne neue Dinge, die in jedem
    Fall auftreten*) means *I learn new things that arise in every case*, and therefore,
    as can be seen from the results, the text has been almost perfectly translated
    from English to German.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
- en: 'As concluded in the previous recipe, for maximum performance on the CPU, the
    best approach is the following:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
- en: 'Use hybridization: Using the Intel MKL-DNN backend, combined with static memory
    allocation and invariant input shapes.'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not use AMP.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use Int8 quantization.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, we won’t be able to use Int8 quantization, as this is not supported
    for GluonNLP models.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
- en: Let’s apply each of these techniques for our current specific task, translating
    from English to German.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
- en: 'For hybridization, we just need a couple of lines of code (which include the
    necessary parameters for the Intel MKL-DNN backend, combined with static memory
    allocation and invariant input shapes, and the hybridization of the loss function
    as well):'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: We do not need to add any steps connected to AMP as it was shown not to add
    benefits to CPU-based workloads. Similarly, GluonNLP does not support Int8 quantization,
    and therefore, we don’t need to make any further changes to our code.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
- en: 'Applying these optimization techniques, these are the quantitative results
    obtained for an optimized CPU inference:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As we can see, the differences in performance (1.53 versus 1.53 for the loss
    and 26.40 versus 26.40 for the BLEU score) are negligible. However, with these
    inference optimization techniques, we have been able to reduce the inference runtime
    by 20% (313 seconds versus 374 seconds), which is a very good result.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
- en: 'From a qualitative point of view, we can also check how well our model is performing
    with a sentence example. In our case, we chose *I learn new things every day*,
    and the output obtained is the following:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The German sentence obtained in the output (*Ich lerne neue Dinge, die in jedem
    Fall auftreten*) means *I learn new things that arise in every case*, and therefore,
    as can be seen from the results, the text has been almost perfectly translated
    from English to German. Moreover, the results are equivalent to the non-optimized
    case (as expected).
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
- en: 'What about GPU-based inference? Let’s follow the same steps:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
- en: 'Without applying these optimization techniques, as a baseline, these are the
    quantitative results obtained with the GPU:'
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE39]'
  id: totrans-375
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: As expected, there is no change relative to algorithmic performance from the
    CPU baseline. Runtime inference is indeed six times as fast in the GPU (61.7 seconds
    versus 374 seconds).
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'From a qualitative point of view, we can also check how well our model is performing
    with a sentence example. In our case, we chose *I learn new things every day*,
    and the output obtained is the following:'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The German sentence obtained in the output (*Ich lerne neue Dinge, die in jedem
    Fall auftreten*) means *I learn new things that arise in every case*, and therefore,
    as can be seen from the results, the text has been almost perfectly translated
    from English to German (and is equivalent to both CPU cases).
  id: totrans-379
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'As concluded in the previous recipe, for maximum performance on the GPU, the
    best approach is the following:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
- en: 'Use hybridization: Using static memory allocation and invariant input shapes.
    Do not use the Intel MKL-DNN backend.'
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Use AMP.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Do not use Int8 quantization (not supported).
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unfortunately, we won’t be able to use AMP, as this is not supported for GluonNLP
    models.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
- en: Let’s apply each of these techniques for our current specific task, translating
    from English to German.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
- en: 'For hybridization, we just need a couple of lines of code (which include the
    necessary parameters for static memory allocation and invariant input shapes,
    and the hybridization of the loss function as well):'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We do not need to add any steps connected to AMP or Int8 quantization, as GluonNLP
    does not support these features. Therefore, no further steps are required.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
- en: 'By applying these optimization techniques, these are the quantitative results
    obtained for an optimized GPU inference:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: As we can see, the differences in performance (1.53 versus 1.53 for the loss
    and 26.40 versus 26.40 for the BLEU score) are negligible. However, with these
    inference optimization techniques, we have been able to reduce the inference runtime
    by 10% (56.3 seconds versus 61.7 seconds), which is a very good result.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
- en: 'From a qualitative point of view, we can also check how well our model is performing
    with a sentence example. In our case, we chose *I learn new things every day*,
    and the output obtained is the following:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: The German sentence obtained in the output (*Ich lerne neue Dinge, die in jedem
    Fall auftreten*) means *I learn new things that arise in every case*, and therefore,
    as can be seen from the results, the text has been almost perfectly translated
    from English to German. Moreover, the results are equivalent to the non-optimized
    case (as expected).
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
- en: Profiling our models
  id: totrans-395
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In the previous sections, we saw the different techniques that we could apply
    to optimize our inference loops, and the results these techniques achieved. However,
    how exactly do these techniques work? Why are they faster?
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we are going to use the MXNet profiler, which can help us understand
    issues happening during runtime.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
- en: 'As mentioned in the initial section, the output of model profiling is a JSON
    file that can be visualized with tools such as the Google Chrome Tracing app.
    For a non-optimized CPU workload, our transformer model shows the following timing
    profile:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.14 – Profiling Transformer: Non-optimized CPU workload](img/B16591_09_14.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.14 – Profiling Transformer: Non-optimized CPU workload'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 9.14*, we can see the following characteristics:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
- en: Almost all of the tasks are handled by two processes.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is almost no waiting time (lazy evaluation and `mx.nd.waitall`).
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Synchronous/structured usage of memory.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All operations are atomic and executed individually.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full operation takes around 1,200 ms.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a CPU-optimized workload, our Transformer model shows the following timing
    profile:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.15 – Profiling Transformer: Optimized CPU workload](img/B16591_09_15.jpg)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.15 – Profiling Transformer: Optimized CPU workload'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 9.15*, we can see the following characteristics:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
- en: Almost all of the tasks are handled by two processes, similar to the non-optimized
    counterpart.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is almost no waiting time (lazy evaluation and `mx.nd.waitall`), similar
    to the non-optimized counterpart.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory is used in a more asynchronous/structured way, in comparison to the non-optimized
    counterpart.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some operations are fused together. Although the visualizations are not very
    clear, operator fusion (hybridization) seems to be working, with most of the time
    spent on fused operations.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full operation takes around 720 ms.
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a zoomed look into one of the operator fusion steps:'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.16 – Profiling Transformer: Optimized CPU workload (zoom into OperatorFusion)](img/B16591_09_16.jpg)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.16 – Profiling Transformer: Optimized CPU workload (zoom into OperatorFusion)'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 9.16* we can see how operator fusion has fused together several different
    operations, including embeddings, layer normalization, fully connected layers,
    and MKL-DNN-accelerated layers.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, for CPU-based optimizations, we can clearly see the effects:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
- en: Hybridization has fused most of the operators together, although the visualization
    is difficult to see, and this happens many times.
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The MKL-DNN backend has improved those operations with accelerated operators.
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let’s discuss the GPU case now.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
- en: 'For a non-optimized GPU workload, our Transformer model shows the following
    timing profile:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.17 – Profiling Transformer: Non-optimized GPU workload](img/B16591_09_17.jpg)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.17 – Profiling Transformer: Non-optimized GPU workload'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 9.17*, we can see the following characteristics:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
- en: Tasks are mostly handled by several (three) GPU processes.
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is almost no waiting time (lazy evaluation and `mx.nd.waitall`).
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Memory is gradually increasing.
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All operations are atomic and executed individually.
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Several copies from/to CPU, which do not seem to degrade performance.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full operation takes around 580 ms.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For a GPU-optimized workload, our Transformer model shows the following timing
    profile:'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.18 – Profiling Transformer: Optimized GPU workload](img/B16591_09_18.jpg)'
  id: totrans-435
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.18 – Profiling Transformer: Optimized GPU workload'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 9.18*, we can see the following characteristics:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
- en: Almost all of the tasks are handled by three processes, similar to the non-optimized
    counterpart.
  id: totrans-438
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: There is almost no waiting time (lazy evaluation and `mx.nd.waitall`), similar
    to the non-optimized counterpart.
  id: totrans-439
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: More asynchronous/unstructured usage of memory, in comparison to the non-optimized
    counterpart.
  id: totrans-440
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Some operations are fused together. Although the visualizations are not very
    clear, operator fusion (hybridization) seems to be working, spending most of the
    time in fused operations.
  id: totrans-441
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Data copy operations from/to CPU do not seem to degrade performance, although
    there are several.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The full operation takes around 260 ms.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s take a zoomed look into one of the operator fusion steps:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 9.19 – Profiling Transformer: Optimized GPU workload (zoom into OperatorFusion)](img/B16591_09_19.jpg)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
- en: 'Figure 9.19 – Profiling Transformer: Optimized GPU workload (zoom into OperatorFusion)'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 9.19*, we can see how operator fusion has fused together several
    different operations, including embeddings, layer normalization, and fully connected
    layers.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
- en: In summary, for GPU-based optimizations, we can clearly see the effect of hybridization,
    where all operations have been fused together, although the visualization is difficult
    to interpret, and this happens many times.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
- en: Exporting our models
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: MXNet and GluonNLP also provide tools to export our models. However, these tools
    are mostly for internal usage of MXNet/Gluon. The reason for this is that GluonNLP
    mostly deals with `save()` function.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
- en: 'This function can be easily called:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  id: totrans-452
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We can verify the files associated with the model, the parameters (the `.params`
    extension), and the architecture (the `.json` extension) have been saved with
    these commands:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: And we are done! In this section, we have been able to successfully export our
    Transformer model. Congratulations!
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
- en: How it works...
  id: totrans-456
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we have applied the different inference optimization techniques
    seen in the first recipe of this chapter, leveraging our hardware (CPUs and GPUs)
    to optimize our model runtime performance by hybridizing the model.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
- en: Moreover, we have learned how to use the MXNet profiler to analyze the inference
    optimizations from a low-level perspective.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we have learned how to export our models using internal MXNet libraries.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  id: totrans-460
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we presented the inference optimization problem from a post-training
    perspective. We were given a (pre-)trained model and we tried to squeeze as much
    performance as we could from it.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
- en: 'However, there is another avenue that can be explored, which starts thinking
    about maximizing inference performance from a machine learning model design perspective.
    Several improvements to how LLMs can be used without large compute workloads have
    been published, such as the following:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
- en: '**Low Ranking Adaptation (LORA)**: [https://arxiv.org/pdf/2012.13255.pdf](https://arxiv.org/pdf/2012.13255.pdf)'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LORA meets pruning**: [https://arxiv.org/pdf/2305.18403.pdf](https://arxiv.org/pdf/2305.18403.pdf)'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GPT4All (quantization)**: [https://gpt4all.io](https://gpt4all.io)'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Int4 quantization**: [https://arxiv.org/pdf/2301.12017.pdf](https://arxiv.org/pdf/2301.12017.pdf)'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
