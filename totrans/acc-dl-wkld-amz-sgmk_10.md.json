["```py\n    import torch\n    from transformers import DistilBertTokenizer, DistilBertForSequenceClassification\n    en_tokenizer = DistilBertTokenizer.from_pretrained(EN_MODEL)\n    en_model = DistilBertForSequenceClassification.from_pretrained(EN_MODEL)\n    en_model_path = \"models/english_sentiment\"\n    os.makedirs(en_model_path, exist_ok=True)\n    en_model.save_pretrained(save_directory=en_model_path)\n    en_tokenizer.save_pretrained(save_directory=en_model_path)\n    ```", "```py\n('models/english_sentiment/tokenizer_config.json',\n 'models/english_sentiment/special_tokens_map.json',\n 'models/english_sentiment/vocab.txt',\n 'models/english_sentiment/added_tokens.json')\n```", "```py\n    def model_fn(model_dir):\n        tokenizer = DistilBertTokenizer.from_pretrained(model_dir)\n        model = DistilBertForSequenceClassification.from_pretrained(model_dir)\n        return model, tokenizer\n    def input_fn(serialized_input_data, content_type=JSON_CONTENT_TYPE):\n        if content_type == JSON_CONTENT_TYPE:\n            input_data = json.loads(serialized_input_data)\n            return input_data\n        else:\n            Exception(\"Requested unsupported ContentType in Accept: \" + content_type)\n    def predict_fn(input_data, model_tokenizer_tuple):\n        model, tokenizer = model_tokenizer_tuple\n        inputs = tokenizer(input_data, return_tensors=\"pt\")\n        with torch.no_grad():\n            logits = model(**inputs).logits\n        predicted_class_id = logits.argmax().item()\n        predictions = model.config.id2label[predicted_class_id]\n        return predictions\n    def output_fn(prediction_output, accept=JSON_CONTENT_TYPE):\n        if accept == JSON_CONTENT_TYPE:\n            return json.dumps(prediction_output), accept\n        raise Exception(\"Requested unsupported ContentType in Accept: \" + accept)\n    ```", "```py\n    model.tar.gz/\n                 |- model.pth # and any other model artifacts\n                 |- code/\n                         |- inference.py\n                         |- requirements.txt # optional\n    ```", "```py\nen_model_data = sagemaker_session.upload_data('models/english_sentiment.tar.gz', bucket=bucket,key_prefix=prefix)\nger_model_data = sagemaker_session.upload_data('models/german_sentiment.tar.gz', bucket=bucket,key_prefix=prefix)\n```", "```py\n    from sagemaker import image_uris\n    HF_VERSION = '4.17.0'\n    PT_VERSION = 'pytorch1.10.2'\n    pt_container_uri = image_uris.retrieve(framework='huggingface',\n                                    region=region,\n                                    version=HF_VERSION,\n                                    image_scope='inference',\n                                    base_framework_version=PT_VERSION,\n                                    instance_type='ml.c5.xlarge')\n    ```", "```py\n    container  = {\n        'Image': pt_container_uri,\n        'ContainerHostname': 'MultiModel',\n        'Mode': 'MultiModel',\n        'ModelDataUrl': mm_data_path,\n        'Environment': {\n        'SAGEMAKER_PROGRAM':'inference.py',\n        'SAGEMAKER_SUBMIT_DIRECTORY':mm_data_path\n        }\n    }\n    ```", "```py\n    unique_id = datetime.datetime.now().strftime(\"%Y-%m-%d%H-%M-%S\")\n    model_name = f\"mme-sentiment-model-{unique_id}\"\n    create_model_response = sm_client.create_model(\n        ModelName=model_name,\n        PrimaryContainer=container,\n        ExecutionRoleArn=role,\n    )\n    ```", "```py\n    ger_response = runtime_sm_client.invoke_endpoint(\n        EndpointName=endpoint_name,\n        ContentType=\"application/json\",\n        Accept=\"application/json\",\n        TargetModel=\"german_sentiment.tar.gz\",\n        Body=json.dumps(ger_input),\n    )\n    ```", "```py\n    model.tar.gz/\n                 |--[model_version_number]/\n                                           |--variables\n                                           |--saved_model.pb\n                code/\n                    |--inference.py\n                    |--requirements.txt # optional\n    ```", "```py\n    model_name = f\"mce-nlp-model-{unique_id}\"\n    create_model_response = sm_client.create_model(\n        ModelName=model_name,\n        Containers=[tensorflow_container, pytorch_container],\n        InferenceExecutionConfig={\"Mode\": \"Direct\"},\n        ExecutionRoleArn=role,\n    )\n    ```", "```py\n    tf_response = runtime_sm_client.invoke_endpoint(\n        EndpointName=endpoint_name,\n        ContentType=\"application/json\",\n        Accept=\"application/json\",\n        TargetContainerHostname=\"tensorflow-distilbert-qa\",\n        Body=json.dumps(qa_inputs),\n    )\n    ```", "```py\n    from sagemaker.session import production_variant\n    variant1 = production_variant(\n        model_name=model1_name,\n        instance_type=\"ml.c5.4xlarge\",\n        initial_instance_count=1,\n        variant_name=\"Variant1\",\n        initial_weight=1,\n    )\n    variant2 = production_variant(\n        model_name=model2_name,\n        instance_type=\"ml.c5.4xlarge\",\n        initial_instance_count=1,\n        variant_name=\"Variant2\",\n        initial_weight=1,\n    )\n    ```", "```py\n    from datetime import datetime\n    endpoint_name = f\"ab-testing-{datetime.now():%Y-%m-%d-%H-%M-%S}\"\n    sagemaker_session.endpoint_from_production_variants(\n        name=endpoint_name, production_variants=[variant1, variant2]))\n    ```", "```py\n    results = {\"Variant1\": 0, \"Variant2\": 0, \"total_count\": 0}\n    for i in range(20):\n        response = sm_runtime_client.invoke_endpoint(EndpointName=endpoint_name, ContentType=\"application/json\", Body=json.dumps(data))\n        results[response['InvokedProductionVariant']] += 1\n        results[\"total_count\"] += 1\n    ```", "```py\n    sm_client.update_endpoint_weights_and_capacities(\n        EndpointName=endpoint_name,\n        DesiredWeightsAndCapacities=[\n            {\"DesiredWeight\": 10, \"VariantName\": \"Variant1\"},\n            {\"DesiredWeight\": 90, \"VariantName\": \"Variant2\"},])\n    ```", "```py\n    sm_runtime_client.invoke_endpoint(EndpointName=endpoint_name, TargetVariant=\"Variant2\", ContentType=\"application/json\", Body=json.dumps(data))\n    ```", "```py\n    From sagemaker import image_uris\n    HF_VERSION = '4.17.0'\n    PT_VERSION = 'pytorch1.10.2'\n    hf_container_uri = image_uris.retrieve(framework='huggingface',\n                              region=region,\n                                    version=HF_VERSION,\n                                    image_scope='inference',\n                                    base_framework_version=PT_VERSION,\n                                    instance_type='ml.c5.xlarge')\n    ```", "```py\n    Hub = {\n        'HF_MODEL_ID':'distilbert-base-uncased-distilled-squad',\n        'HF_TASK':'question-answering'\n    }\n    huggingface_model = HuggingFaceModel(\n       env=hub,  \n       role= role, \n       transformers_version=HF_VERSION,\n       pytorch_version=PT_VERSION,     \n       image_uri=hf_container_uri,     \n    )\n    ```", "```py\n    Serverless_config = ServerlessInferenceConfig(\n        memory_size_in_mb=4096, max_concurrency=10,\n    )\n    predictor = huggingface_model.deploy(\n        serverless_inference_config=serverless_config\n    )\n    ```", "```py\nimport boto3 \nas_client = boto3.client('application-autoscaling') \nresource_id=f\"endpoint/{predictor.endpoint_name}/variant/AllTraffic\"\npolicy_name = f'Request-ScalingPolicy-{predictor.endpoint_name}'\nscalable_dimension = 'sagemaker:variant:DesiredInstanceCount'\n# scaling configuration\nresponse = as_client.register_scalable_target(\n    ServiceNamespace='sagemaker', #\n    ResourceId=resource_id,\n    ScalableDimension='sagemaker:variant:DesiredInstance Count', \n    MinCapacity=1,\n    MaxCapacity=4\n)\n```", "```py\n    response = as_client.put_scaling_policy(\n        PolicyName=policy_name,\n        ServiceNamespace='sagemaker',\n        ResourceId=resource_id,\n        ScalableDimension=scalable_dimension,\n        PolicyType='TargetTrackingScaling',\n        TargetTrackingScalingPolicyConfiguration={\n            'TargetValue': 10.0, # Threshold\n            'PredefinedMetricSpecification': {\n                'PredefinedMetricType': 'SageMakerVariantInvocationsPerInstance',\n            },\n            'ScaleInCooldown': 300, # duration until scale in\n            'ScaleOutCooldown': 60 # duration between scale out\n        }\n    )\n    ```", "```py\n    locust -f ../utils/load_testing/locustfile.py --headless -u 20 -r 1 --run-time 5m\n    ```", "```py\n      ...\n      \"ProductionVariants\": [\n        {\n          \"VariantName\": \"AllTraffic\",\n          ...\n          ],\n          \"CurrentWeight\": 1,\n          \"DesiredWeight\": 1,\n          \"CurrentInstanceCount\": 1,\n          \"DesiredInstanceCount\": 4\n        }\n      ]\n      'EndpointStatus': 'Updating'\n      ...\n    ```"]