<html><head></head><body><div id="sbo-rt-content"><div id="_idContainer093">
			<h1 id="_idParaDest-113"><em class="italic"><a id="_idTextAnchor112"/>Chapter 9</em>: Fundamentals of Deep Learning Explainability</h1>
			<p>Explainability is providing selective human-understandable explanations for a decision provided by an automated system. In the context of this book, during the full life cycle of <strong class="bold">deep learning</strong> (<strong class="bold">DL</strong>) development, explainability should be emphasized as a first-class artifact, along with the other three pillars: data, code, and model. This is because different stakeholders and regulators, model developers, and final consumers of the model output may have different needs to understand how the data is used and why the model produces certain predictions or classifications. Without such understanding, it will be difficult to gain the trust of the consumers of the model output or to diagnose what could have gone wrong when model output results drift. This also means that explainability tools should be employed not only for explaining prediction results from a deployed model in production or during offline experimentation, but also for understanding the data characteristics and differences between the datasets used in offline model training and the ones encountered in online model operation. </p>
			<p>In addition, in many highly regulated industries, such as autonomous driving, medical diagnosis, banking, and finance, there is also a legal mandate that demands the <strong class="bold">right to explanation</strong> (<a href="https://academic.oup.com/idpl/article/7/4/233/4762325">https://academic.oup.com/idpl/article/7/4/233/4762325</a>) for any individual to get an explanation for an output of the algorithm. Finally, a recent survey showed that over 82% of CEOs believe that AI-based decisions must be explainable to be trusted as enterprises accelerate their investment in developing and deploying AI-based initiatives (<a href="https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explainable-ai-now-ga-help-you-interpret-your-machine-learning-models">https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explainable-ai-now-ga-help-you-interpret-your-machine-learning-models</a>). Therefore, it is important to learn the fundamentals of explainability and the related tools so that we know when to use what tools for what audience to provide a relevant, accurate, and consistent explanation. </p>
			<p>By the end of this chapter, you will be confident to know what a good explanation is and what tools exist for different explainability purposes and will gain hands-on experience in using two explainability toolboxes for explaining DL sentiment classification models.</p>
			<p>In this chapter, we're going to cover the following main topics:</p>
			<ul>
				<li>Understanding the categories and audience of explainability</li>
				<li>Exploring the SHAP Explainability toolbox</li>
				<li>Exploring the Transformers Interpret toolbox</li>
			</ul>
			<h1 id="_idParaDest-114"><a id="_idTextAnchor113"/>Technical requirements</h1>
			<p>The following requirements are necessary to complete the learning in this chapter:</p>
			<ul>
				<li>SHAP Python library: <a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a></li>
				<li>Transformers Interpret Python library: <a href="https://github.com/cdpierse/transformers-interpret">https://github.com/cdpierse/transformers-interpret</a></li>
				<li>Captum Python library: <a href="https://github.com/pytorch/captum">https://github.com/pytorch/captum</a></li>
				<li>Code from the GitHub repository for this chapter: <a href="https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter09">https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter09</a></li>
			</ul>
			<h1 id="_idParaDest-115"><a id="_idTextAnchor114"/>Understanding the categories and audience of explainability</h1>
			<p>As this <a id="_idIndexMarker572"/>chapter's<a id="_idIndexMarker573"/> opening texts imply, explainability for a DL system becomes increasingly critical, sometimes even mandatory, in highly regulated industries such as financial, legal, governmental, and medical application domains. An example lawsuit partially due to the lack of ML explainability is the <a id="_idIndexMarker574"/>case of <strong class="bold">B2C2 v Quoine</strong> (<a href="https://www.scl.org/articles/12130-explainable-machine-learning-how-can-you-determine-what-a-party-knew-or-intended-when-a-decision-was-made-by-machine-learning">https://www.scl.org/articles/12130-explainable-machine-learning-how-can-you-determine-what-a-party-knew-or-intended-when-a-decision-was-made-by-machine-learning</a>), where automated AI trading algorithms mistakenly placed an order with 250 times the market price for bitcoin trading. The recent successful applications of DL models in production stimulate active and abundant research and development in the explainability area due to the need to understand why and how a DL model works. You may have heard<a id="_idIndexMarker575"/> of the term <strong class="bold">explainable artificial intelligence</strong> (<strong class="bold">XAI</strong>), which was started by the <strong class="bold">US Defense Advanced Research Projects Agency</strong> (<strong class="bold">DARPA</strong>) in 2015 for its XAI program with the goal of<a id="_idIndexMarker576"/> enabling end users to better understand, trust, and effectively manage AI systems (<a href="https://onlinelibrary.wiley.com/doi/epdf/10.1002/ail2.61">https://onlinelibrary.wiley.com/doi/epdf/10.1002/ail2.61</a>). However, the concept of explainability goes way back to the early days of expert systems in the 1980s or even earlier (<a href="https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1391">https://wires.onlinelibrary.wiley.com/doi/full/10.1002/widm.1391</a>), and the recent surge of attention on the topic of explainability just highlights how important it is. </p>
			<p>So, what's an explanation? It turns out that this is still an active research topic in the ML/DL/AI community. From a practical purpose, a precise definition of explanation depends on who wants the explanations for what purpose at what time across the ML/DL/AI life cycle (<a href="https://dl.acm.org/doi/abs/10.1145/3461778.3462131">https://dl.acm.org/doi/abs/10.1145/3461778.3462131</a>). So, explainability can be defined as <em class="italic">the capability to provide an audience-appropriate, human-understandable interpretation of why and how a model provides certain predictions</em>. This may also include the data explainability aspect, where and how the data was <a id="_idIndexMarker577"/>used <a id="_idIndexMarker578"/>through provenance tracking, what the data characteristics are, or whether it has changed due to unexpected events. For example, sales and marketing emails changed due to an unexpected COVID outbreak (<a href="https://www.validity.com/resource-center/disruption-in-email/">https://www.validity.com/resource-center/disruption-in-email/</a>). Such data changes will unexpectedly change the distribution of model prediction results. We need to take into account such data changes when explaining the model drift. This means the complexity of the explanations needs to be tailored and selective to the receiving audience without overwhelming information. For example, a complex explanation with many technical jargons such as <em class="italic">activation</em> might not work as well as a simple text summary with business-friendly terms. This<a id="_idIndexMarker579"/> further shows that explainability is also a <strong class="bold">Human-Computer Interface/Interaction</strong> (<strong class="bold">HCI</strong>) topic.</p>
			<p>To get the big picture of what the explainability categories and corresponding audiences look like, we consider the eight dimensions of explanations shown in <em class="italic">Figure 9.1</em>:</p>
			<div>
				<div id="_idContainer086" class="IMG---Figure">
					<img src="Images/B18120_09_001.jpg" alt="Figure 9.1 – Eight dimensions to understand explainability &#13;&#10;" width="772" height="700"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.1 – Eight dimensions to understand explainability </p>
			<p>As can be seen from <em class="italic">Figure 9.1</em>, the complexity of explainability can be understood from eight dimensions. This is not necessarily an exhaustive categorization, but rather a guide to understanding different perspectives from HCI, the full life cycle of AI/ML/DL, and<a id="_idIndexMarker580"/> different <a id="_idIndexMarker581"/>technical approaches. In the following discussion, we will highlight the dimensions and their inter-relationships that are most relevant to DL applications, since the focus of this chapter is on DL explainability.</p>
			<h2 id="_idParaDest-116"><a id="_idTextAnchor115"/>Audience: who needs to know</h2>
			<p>As pointed out recently by a study (<a href="https://dl.acm.org/doi/abs/10.1145/3461778.3462131">https://dl.acm.org/doi/abs/10.1145/3461778.3462131</a>), it is important to understand who needs to know what kind of explanations at what stage across an AI project life cycle. This will also affect the explanation output formats. An earlier study (<a href="https://arxiv.org/pdf/1702.08608.pdf">https://arxiv.org/pdf/1702.08608.pdf</a>) also points out that depending on whether a domain expert is involved in a real application task (for example, a medical doctor in a diagnosis of cancer), the cost of validating an explanation could also be high since it requires an actual human in a real work environment. </p>
			<p>For current practical DL projects, we need to tailor our methods and presentations of explanations depending on the target audience, such as data scientists, ML engineers, business <a id="_idIndexMarker582"/>stakeholders, <strong class="bold">User Experience (UX)</strong> designers, or end users, as there is no one-size-fits-all approach.</p>
			<h2 id="_idParaDest-117"><strong class="bold"><a id="_idTextAnchor116"/>Stage: when to provide an explanation in the DL life cycle</strong></h2>
			<p>A <strong class="bold">stage</strong> usually refers to when the explanations can be provided during the model development life cycle. For a model such as a decision tree, since it is a white-box model, we say we can <a id="_idIndexMarker583"/>provide <strong class="bold">ante-hoc</strong> explainability. However, currently, most DL models are mostly treated as black-box models even though self-explaining DL models are being gradually developed with ante-hoc explainability (<a href="https://arxiv.org/abs/2108.11761">https://arxiv.org/abs/2108.11761</a>). Therefore, for current practical DL <a id="_idIndexMarker584"/>applications, <strong class="bold">post-hoc</strong> explainability is needed. In addition, when the model development stages are in training, validation, or production, the explainability scope can be global, cohort, or local, even using the same post-hoc explainability tools (<a href="https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f">https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f</a>).</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor117"/>Scope: which prediction needs explanation</h2>
			<p><strong class="bold">Scope</strong> refers<a id="_idIndexMarker585"/> to<a id="_idIndexMarker586"/> whether we can provide the explanation for all predictions, a subset of the predictions, or just one specific prediction, even if we use the same post-hoc tool for a black-box DL model. The most common global explainability is to describe <strong class="bold">feature importance</strong> and allow users to know which feature is the most impactful one for the overall model performance. Local <a id="_idIndexMarker587"/>explainability is about <strong class="bold">feature attribution</strong> for a specific prediction instance. The difference between feature attribution and feature importance is that feature attribution not only quantifies the ranking and magnitude of the feature impact, but also the direction of the impact (for example, whether a feature is positively or negatively affecting the prediction). </p>
			<p>Many of the post-hoc tools for DL models are very good at local explainability. Cohort explainability is useful for identifying potential model bias for some specific groups such as age or race groups. For a DL model, if we want to have a global explanation, we often need to use a surrogate model such as a decision tree model to emulate the behavior of a DL model (<a href="https://towardsdatascience.com/explainable-ai-xai-methods-part-5-global-surrogate-models-9c228d27e13a">https://towardsdatascience.com/explainable-ai-xai-methods-part-5-global-surrogate-models-9c228d27e13a</a>). However, this approach does not always work well as it is very difficult to know whether the surrogate model is approximating the predictions of the original black-box model well enough. So, in practice, local <a id="_idIndexMarker588"/>explainability tools for DL models are often used, such as <strong class="bold">SHapley Additive exPlanations</strong> (<strong class="bold">SHAP</strong>), which we will explain in the method dimension.</p>
			<h2 id="_idParaDest-119"><strong class="bold"><a id="_idTextAnchor118"/>Input data format: what is the format of the input data</strong></h2>
			<p><strong class="bold">Input data format</strong> refers to what kind of input data we are dealing with when developing and using the model. While a simple model might only focus on a single type of input data format such as text, many complex models might require using a mix of structured tabular data plus unstructured data such as images or texts. In addition, there is also a separate need to understand the input data hidden bias (during model training and validation) or drifting (during production). As such, this is quite a complex topic. The data explanation can also be used for monitoring data outliers and drifting during production. This is applicable to all types of ML/DL models.</p>
			<h2 id="_idParaDest-120"><strong class="bold"><a id="_idTextAnchor119"/>Output data format: what is the format of the output explanation</strong></h2>
			<p><strong class="bold">Output explanation format</strong> refers to how we present the explanations to our target audience. Often, an image explanation might be a bar chart showing the feature importance with the top few features and their scores, or a saliency map <a id="_idIndexMarker589"/>that <a id="_idIndexMarker590"/>highlights the spatial support of a particular class in each image for image-related ML problems. For a textual output, it could be an English sentence to say why a credit application is rejected because of a few factors that are understandable to the<a id="_idIndexMarker591"/> applicants. <strong class="bold">Natural language processing (NLP)</strong> model explainability could also be through interactive exploration that uses salience maps, attention, and other rich visualization (see <a id="_idIndexMarker592"/>examples in Google's <strong class="bold">Language Interpretability Tool</strong> (<strong class="bold">LIT</strong>): <a href="https://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html">https://ai.googleblog.com/2020/11/the-language-interpretability-tool-lit.html</a>). As there is no silver bullet for explainability of these complex output formats, it is critical to meet the needs, experiences, and expectations of the audience that asks for the explanation.</p>
			<h2 id="_idParaDest-121"><strong class="bold"><a id="_idTextAnchor120"/>Problem type: what is the machine learning problem type</strong></h2>
			<p><strong class="bold">Problem type</strong> refers to all kinds of ML/AI problems broadly, but for practical purposes, current commercially successful problems are mostly around classification, regression, and clustering. Reinforcement learning and recommendation systems also see increasingly successful adoption in the industry. DL models are now often used in all these types of problems or are at least being evaluated as a potential candidate model.</p>
			<h2 id="_idParaDest-122"><strong class="bold"><a id="_idTextAnchor121"/>Objectives type: what is the motivation or goal to explain</strong></h2>
			<p><strong class="bold">Objectives type</strong> refers to the motivation of using explainability in AI/ML projects. It has been argued that the number one objective of explainability is to gain trust by providing a sufficient understanding of the AI system behavior and uncovering vulnerabilities, biases, and flaws of the system. An additional motivation is to infer the causal relationship from the input and output prediction. Other objectives include improving the model accuracy through a better understanding of the inner workings of the AI/ML systems, and justifying the model behavior and decisions through transparent explanations when potentially <a id="_idIndexMarker593"/>severe consequences are involved. It is even <a id="_idIndexMarker594"/>possible to reveal unknown insights and rules that are based on explanations (https://www.tandfonline.com/doi/full/10.1080/10580530.2020.1849465). Overall, it is very desirable to break the black box so that when being used in a real production system, the AI/ML models and systems can be used with confidence.</p>
			<h2 id="_idParaDest-123"><a id="_idTextAnchor122"/>Method type: what is the specific post-hoc explanation method used</h2>
			<p><strong class="bold">Method type (post-hoc)</strong> refers to post-hoc methods that are very relevant to the DL models. There are two major categories of post-hoc methods: perturbation-based and gradient-based. Recent work has started to unify these two approaches, although it is not yet widely applicable for practical usage ( <a href="https://teamcore.seas.harvard.edu/publications/towards-unification-and-robustness-perturbation-and-gradient-based">https://teamcore.seas.harvard.edu/publications/towards-unification-and-robustness-perturbation-and-gradient-based</a>). The following is a brief discussion on these two types of methods:</p>
			<ul>
				<li>Perturbation-based methods leverage perturbations of individual instances to construct interpretable local approximations using linear models to explain the<a id="_idIndexMarker595"/> predictions. The most popular perturbation-based methods include <strong class="bold">Local Interpretable Model-Agnostic Explanations</strong> (<strong class="bold">LIME</strong>), (<a href="https://arxiv.org/pdf/1602.04938.pdf">https://arxiv.org/pdf/1602.04938.pdf</a>), SHAP, and variants of LIME and SHAP such as BayesLIME and BayesSHAP, TreeSHAP, and many more (<a href="https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df">https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df</a>). LIME can be used for tabular, image, and textual input data and is model agnostic. That's to say, LIME can be used for any type of classifiers (tree-based or DL models) regardless of the algorithms being used. SHAP uses principles from cooperative game theory to identify the contribution of different features to the prediction in order to quantify the impact of each feature. SHAP produces a so-called shapely value, which is the average of all the marginal contributions to all possible coalitions or combinations of different features. It works well for many types of models, including DL models, although the computational time could be much faster for tree-based models such as XGBoost or LightGBM (<a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>). </li>
				<li>Gradient-based <a id="_idIndexMarker596"/>methods, such<a id="_idIndexMarker597"/> as SmoothGrad (<a href="https://arxiv.org/abs/1706.03825">https://arxiv.org/abs/1706.03825</a>) and Integrated Gradients (<a href="https://towardsdatascience.com/understanding-deep-learning-models-with-integrated-gradients-24ddce643dbf">https://towardsdatascience.com/understanding-deep-learning-models-with-integrated-gradients-24ddce643dbf</a>), leverages gradients computed with respect to input dimensions of individual instances to explain model predictions. They can be applied to both image and textual input data, although sometimes, textual input could suffer a manipulation or adversary attack (<a href="https://towardsdatascience.com/limitations-of-integrated-gradients-for-feature-attribution-ca2a50e7d269">https://towardsdatascience.com/limitations-of-integrated-gradients-for-feature-attribution-ca2a50e7d269</a>), which will change the feature importance undesirably. </li>
			</ul>
			<p>Note that there are additional types of methods such as counterfactual (https://christophm.github.io/interpretable-ml-book/counterfactual.html) or prototype-based methods (<a href="https://christophm.github.io/interpretable-ml-book/proto.html">https://christophm.github.io/interpretable-ml-book/proto.html</a>), which we will not cover in this book.</p>
			<p>Having discussed the many dimensions of explainability, it is important to know that XAI is still an emerging area (<a href="https://fairlyaccountable.org/aaai-2021-tutorial/doc/AAAI_slides_final.pdf">https://fairlyaccountable.org/aaai-2021-tutorial/doc/AAAI_slides_final.pdf</a>) and it is sometimes even difficult to find agreement among different explainability methods when applying to the same dataset or models (see a recent study on the topic of disagreement problems in explainable ML from the practitioners' perspective: <a href="https://arxiv.org/abs/2202.01602">https://arxiv.org/abs/2202.01602</a>). In the end, it does require some experimentation to find out which explainability provides the human validated <a id="_idIndexMarker598"/>explanations <a id="_idIndexMarker599"/>that are meeting the requirements for a specific prediction task in the real world. </p>
			<p>In the next two sections of this chapter, we will focus on providing some hands-on experiments using some popular and emerging toolkits to learn how to do explainability.</p>
			<h1 id="_idParaDest-124"><a id="_idTextAnchor123"/>Exploring the SHAP Explainability toolbox</h1>
			<p>For our learning <a id="_idIndexMarker600"/>purpose, let's review some popular explainability toolboxes while experimenting with some examples. Based on the number of GitHub stars (16,000 as of April 2022, <a href="https://github.com/slundberg/shap">https://github.com/slundberg/shap</a>), SHAP is the most widely used and integrated open source model explainability toolbox. It is also the foundation explanation tool that is integrated with MLflow. Here, we would like to run a small experiment to get some hands-on experience on how this works. Let's use a sentimental analysis NLP model to explore how SHAP can be used for explaining the model behavior:</p>
			<ol>
				<li>Set up the virtual environment on your local environment after checking out this chapter's code from GitHub. Running the following command will create a new virtual environment called <strong class="source-inline">dl-explain</strong>:<p class="source-code"><strong class="bold">conda env create -f conda.yaml</strong></p></li>
			</ol>
			<p>This will install SHAP and its related dependencies such as <strong class="source-inline">matplotlib</strong> in this virtual environment. Once this virtual environment is created, activate this virtual environment by running the following command:</p>
			<p class="source-code"><strong class="bold">conda activate dl-explain</strong></p>
			<p>Now, we are ready to run the experiment with SHAP.</p>
			<ol>
				<li value="2">You can check out the <strong class="source-inline">shap_explain.ipynb</strong> notebook to follow through with the exploration. The first step in this notebook is to import the relevant Python libraries:<p class="source-code">import transformers</p><p class="source-code">import shap</p><p class="source-code">from shap.plots import *</p></li>
			</ol>
			<p>These imports<a id="_idIndexMarker601"/> will allow us to use the Hugging Face transformers pipeline API to get a pre-trained NLP model and SHAP functions. </p>
			<ol>
				<li value="3">We then create <strong class="source-inline">dl_model</strong> using the transformers pipeline API for <strong class="source-inline">sentiment_analysis</strong>. Note this is a pretrained pipeline so we can use this without additional finetuning. The default transformer model used in this pipeline is <strong class="source-inline">distilbert-base-uncased-finetuned-sst-2-english</strong> (https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english):<p class="source-code">dl_model = transformers.pipeline(</p><p class="source-code">    'sentiment-analysis', return_all_scores=True)</p></li>
			</ol>
			<p>This will produce a model ready to predict positive or negative sentiment for an input sentence.</p>
			<ol>
				<li value="4">Try this <strong class="source-inline">dl_model</strong> with two input sentences and see whether the output makes sense:<p class="source-code">dl_model(</p><p class="source-code">    ["What a great movie! ...if you have no taste.", </p><p class="source-code">     "Not a good movie to spend time on."])</p></li>
			</ol>
			<p>This will produce an output of the labels and probability scores for each sentence as follows:</p>
			<p class="source-code">[[{'label': 'NEGATIVE', 'score': 0.00014734962314832956}, {'label': 'POSITIVE', 'score': 0.9998526573181152}], [{'label': 'NEGATIVE', 'score': 0.9997993111610413}, {'label': 'POSITIVE', 'score': 0.00020068213052581996}]]</p>
			<p>It seems that the first sentence was predicted with a high probability to be <strong class="source-inline">POSITIVE</strong>, and the second sentence was predicted with a high probability to be <strong class="source-inline">NEGATIVE</strong>. Now, if we take a deep look at the first sentence, we may think the model prediction was incorrect, as there is a subtle negative emotion in the second part of the sentence (<strong class="source-inline">no taste</strong>). So, we want to know why the model made such a prediction. This is where model explainability comes into play.</p>
			<ol>
				<li value="5">Now, let's use<a id="_idIndexMarker602"/> the SHAP API, <strong class="source-inline">shap.Explainer</strong>, to get the Shapley values for the two sentences we are interested in explaining:<p class="source-code">explainer = shap.Explainer(dl_model) </p><p class="source-code">shap_values = explainer(["What a great movie! ...if you have no taste.", "Not a good movie to spend time on."])</p></li>
				<li>Once we have <strong class="source-inline">shap_values</strong>, we can visualize the Shapley values using different visualization techniques. The first one is to use <strong class="source-inline">shap.plot.text</strong> to visualize the first sentence's Shapley values when the prediction label is <strong class="source-inline">POSITIVE</strong>:<p class="source-code">shap.plots.text(shap_values[0, :, "POSITIVE"])</p></li>
			</ol>
			<p>This will produce the plot as follows:</p>
			<div>
				<div id="_idContainer087" class="IMG---Figure">
					<img src="Images/B18120_09_002.jpg" alt="Figure 9.2 – SHAP visualization for sentence 1 with a positive prediction &#13;&#10;" width="1263" height="216"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.2 – SHAP visualization for sentence 1 with a positive prediction </p>
			<p>As can be seen in <em class="italic">Figure 9.2</em>, the word <strong class="source-inline">great</strong> has a very large SHAP value that dominates the influence of the final prediction, while the word <strong class="source-inline">no</strong> has less effect on the final prediction. This results in the final prediction result of <strong class="source-inline">POSITIVE</strong>. So, what about the second sentence with a <strong class="source-inline">NEGATIVE</strong> prediction? Running the following command will produce a similar plot:</p>
			<p class="source-code">shap.plots.text(shap_values[1, :, "NEGATIVE"])</p>
			<p>This command<a id="_idIndexMarker603"/> creates the following plot:</p>
			<div>
				<div id="_idContainer088" class="IMG---Figure">
					<img src="Images/B18120_09_003.jpg" alt="Figure 9.3 – SHAP visualization for sentence 2 with a negative prediction &#13;&#10;" width="1265" height="216"/>
				</div>
			</div>
			<p class="figure-caption"> </p>
			<p class="figure-caption">Figure 9.3 – SHAP visualization for sentence 2 with a negative prediction </p>
			<p>As can be seen from <em class="italic">Figure 9.3</em>, the word <strong class="source-inline">Not</strong> has a strong influence on the final prediction, while the word <strong class="source-inline">good</strong> has a very small influence, resulting in the final prediction of a <strong class="source-inline">NEGATIVE</strong> sentiment. This makes a lot of sense, which is a good explanation of the model's behavior.</p>
			<ol>
				<li value="7">We can also visualize <strong class="source-inline">shap_values</strong> using different plots. A common one is the bar plot, which plots the feature contribution to the final prediction. Running the following command will produce a plot for the first sentence:<p class="source-code">bar(shap_values[0, :,'POSITIVE'])</p></li>
			</ol>
			<p>This will produce a bar chart as follows:</p>
			<div>
				<div id="_idContainer089" class="IMG---Figure">
					<img src="Images/B18120_09_004.jpg" alt="Figure 9.4 – SHAP bar chart for sentence 1 with a positive prediction &#13;&#10;" width="625" height="401"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.4 – SHAP bar chart for sentence 1 with a positive prediction </p>
			<p>As can be seen <a id="_idIndexMarker604"/>from <em class="italic">Figure 9.4</em>, the chart ranks the most important features from top to bottom, where the top ones with a positive influence on the final prediction are plotted on the positive side of the <em class="italic">x</em> axis, while the negative contribution is plotted on the negative side of the <em class="italic">x</em> axis. The <em class="italic">x</em> axis is the value of each token or word's SHAP value with a sign (+ or -). This clearly shows the word <strong class="source-inline">great</strong> is a strong positive factor that impacts the final prediction, while <strong class="source-inline">have no taste</strong> has some negative effect but not enough to change the direction of the final prediction.</p>
			<p>Similarly, we can plot a bar chart for the second sentence as follows:</p>
			<p class="source-code">bar(shap_values[1, :,'NEGATIVE'])</p>
			<p>This will produce the following bar chart:</p>
			<div>
				<div id="_idContainer090" class="IMG---Figure">
					<img src="Images/B18120_09_005.jpg" alt="Figure 9.5 – SHAP bar chart for sentence 2 with a negative prediction &#13;&#10;" width="661" height="401"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.5 – SHAP bar chart for sentence 2 with a negative prediction </p>
			<p>As can be seen from <em class="italic">Figure 9.5</em>, the word <strong class="source-inline">Not</strong> has a strong contribution to the final prediction, while the word <strong class="source-inline">good</strong> is second. These two words have the opposite effect on the final prediction, but apparently, the word <strong class="source-inline">Not</strong> is much stronger and has a much larger SHAP value. </p>
			<p>If you have followed along with this example and seen the SHAP charts in your notebook, congratulations! This <a id="_idIndexMarker605"/>means you have successfully run the SHAP Explainability tool to explain the DL transformer model for the NLP text sentiment analysis.</p>
			<p>Let's further explore another popular explainability tool to see how they perform different explanations.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor124"/>Exploring the Transformers Interpret toolbox</h1>
			<p>As we already<a id="_idIndexMarker606"/> reviewed in the first section of this chapter, there are two major methods: perturbation-based and gradient-based post-hoc explainability tools. SHAP belongs to the perturbation-based family. Now, let's look at a gradient-based toolbox called <strong class="bold">Transformers Interpret</strong> (<a href="https://github.com/cdpierse/transformers-interpret">https://github.com/cdpierse/transformers-interpret</a>). This<a id="_idIndexMarker607"/> is a relatively new tool, but it is built on top of a unified model interpretability and understanding library for PyTorch <a id="_idIndexMarker608"/>called <strong class="bold">Captum</strong> (<a href="https://github.com/pytorch/captum">https://github.com/pytorch/captum</a>), which <a id="_idIndexMarker609"/>provides a unified API to use either perturbation or gradient-based tools (<a href="https://arxiv.org/abs/2009.07896">https://arxiv.org/abs/2009.07896</a>). Transformers Interpret further simplifies the API of Captum so that we can quickly explore gradient-based explainability methods to get some hands-on experience.</p>
			<p>To get started, first make sure you already have the <strong class="source-inline">dl-explain</strong> virtual environment set up and activated, as described in the previous section. Then, we can use the same Hugging Face transformer sentiment analysis model to explore some NLP sentiment classification examples. Then, we can perform the following steps to learn how to use Transformers <a id="_idIndexMarker610"/>Interpret to do the model explanation. You may want to check out the <strong class="source-inline">gradient_explain.ipynb</strong> notebook to follow the instructions:</p>
			<ol>
				<li value="1">Import relevant packages into the notebook as follows:<p class="source-code">from transformers import AutoModelForSequenceClassification, AutoTokenizer</p><p class="source-code">from transformers_interpret import SequenceClassificationExplainer</p></li>
			</ol>
			<p>This will use Hugging Face's transformer model and tokenizer, as well as the explainability function from <strong class="source-inline">transformers_interpret</strong>.</p>
			<ol>
				<li value="2">Create the model and the tokenizer using the same pre-trained model as previous section, which is the <strong class="source-inline">distilbert-base-uncased-finetuned-sst-2-english</strong> model:<p class="source-code">model_name = "distilbert-base-uncased-finetuned-sst-2-english"</p><p class="source-code">model = AutoModelForSequenceClassification.from_pretrained(model_name)</p><p class="source-code">tokenizer = AutoTokenizer.from_pretrained(model_name)</p></li>
			</ol>
			<p>Now that we have the model and tokenizer, we can create an explainability variable using the <strong class="source-inline">SequenceClassificationExplainer</strong> API.</p>
			<ol>
				<li value="3">Create an explainer and give an example sentence to get the <strong class="source-inline">word</strong> attribution from the explainer:<p class="source-code">cls_explainer = SequenceClassificationExplainer(model, tokenizer)</p><p class="source-code">word_attributions = cls_explainer("Not a good movie to spend time on.")</p></li>
				<li>We can also get the prediction label before we check the <strong class="source-inline">word</strong> attributions by running the following command:<p class="source-code">cls_explainer.predicted_class_name</p></li>
			</ol>
			<p>This will produce a result of <strong class="source-inline">Negative</strong>, which means the prediction is a negative sentiment. So, let's see how the explainer provides an explanation for this prediction.</p>
			<ol>
				<li value="5">We can just<a id="_idIndexMarker611"/> display the <strong class="source-inline">word_attributions</strong> value, or we can visualize it. The value of <strong class="source-inline">word_attributions</strong> is as follows:</li>
			</ol>
			<div>
				<div id="_idContainer091" class="IMG---Figure">
					<img src="Images/B18120_09_006.jpg" alt="Figure 9.6 – Layered integrated gradient word attribution values with a negative prediction &#13;&#10;" width="301" height="300"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.6 – Layered integrated gradient word attribution values with a negative prediction </p>
			<p>As can be seen from <em class="italic">Figure 9.6</em>, using the layered integrated gradient method, which is the current explainer's default method implemented in the Transformers Interpret library, the word <strong class="source-inline">not</strong> contributed positively to the final prediction result, which is a negative sentiment. This makes sense. Notice that several other words, such as <strong class="source-inline">to spend time on</strong>, also have a strong positive influence on the final prediction. Given the cross-attention mechanism, it seems the model is trying to extract <strong class="source-inline">not to spend time on</strong> as the main attribution to the final prediction. Note we can also visualize these <strong class="source-inline">word</strong> attributions as follows:</p>
			<p class="source-code">cls_explainer.visualize("distilbert_viz.html")</p>
			<p>This will <a id="_idIndexMarker612"/>produce the follow plot:</p>
			<div>
				<div id="_idContainer092" class="IMG---Figure">
					<img src="Images/B18120_09_007.jpg" alt="Figure 9.7 – Layered integrated gradient word attribution values with a negative prediction &#13;&#10;" width="882" height="100"/>
				</div>
			</div>
			<p class="figure-caption">Figure 9.7 – Layered integrated gradient word attribution values with a negative prediction </p>
			<p>As can be seen in <em class="italic">Figure 9.7</em>, it highlights the word importance of <strong class="source-inline">not to spend time on</strong> to positively impact the final negative prediction.</p>
			<p>Now that we have experimented with both perturbation and gradient-based explainability methods, we have successfully completed our hands-on exploration of using the <a id="_idIndexMarker613"/>explainability tool for post-hoc local explanation. </p>
			<p>Next, we will summarize what we learned in this chapter.</p>
			<h1 id="_idParaDest-126"><a id="_idTextAnchor125"/>Summary</h1>
			<p>In this chapter, we reviewed explainability in AI/ML through an eight-dimension categorization. Although this is not necessarily a comprehensive or exhaustive overview, this does give us a big picture of who to explain to, different stages and scopes to explain, various kinds of input and output formats of the explanation, common ML problems and objectives types, and finally, different post-hoc explainability methods. We then provided two concrete exercises to explore the SHAP and Transformers Interpret toolboxes, which can provide perturbation and gradient-based feature attribution explanations for NLP text sentiment DL models. </p>
			<p>This gives us a solid foundation for using explainability tools for DL models. However, given the active development of XAI, this is only the beginning of using XAI in DL models. Additional explainability toolboxes such as TruLens (<a href="https://github.com/truera/trulens">https://github.com/truera/trulens</a>), Alibi (<a href="https://github.com/SeldonIO/alibi">https://github.com/SeldonIO/alibi</a>), Microsoft Responsible AI Toolbox (<a href="https://github.com/microsoft/responsible-ai-toolbox">https://github.com/microsoft/responsible-ai-toolbox</a>), and IBM AI Explainability 360 Toolkit (<a href="https://github.com/Trusted-AI/AIX360">https://github.com/Trusted-AI/AIX360</a>) are all in active development and worthy of investigation and future learning. Additional links are also provided in the <em class="italic">Further reading</em> section to help you continue to learn this topic.</p>
			<p>Now that we know the fundamentals of explainability, in the next chapter, we will learn how to implement explainability in the MLflow framework so that we can provide a unified way to support explanation within the MLflow framework.</p>
			<h1 id="_idParaDest-127"><a id="_idTextAnchor126"/>Further reading</h1>
			<ul>
				<li><em class="italic">New frontiers in Explainable AI</em>: <a href="https://towardsdatascience.com/new-frontiers-in-explainable-ai-af43bba18348">https://towardsdatascience.com/new-frontiers-in-explainable-ai-af43bba18348</a></li>
				<li><em class="italic">Towards a Rigorous Science of Interpretable Machine Learning</em>: <a href="https://arxiv.org/pdf/1702.08608.pdf">https://arxiv.org/pdf/1702.08608.pdf</a></li>
				<li><em class="italic">The Toolkit Approach to Trustworthy AI</em>: <a href="https://opendatascience.com/the-toolkit-approach-to-trustworthy-ai/">https://opendatascience.com/the-toolkit-approach-to-trustworthy-ai/</a></li>
				<li><em class="italic">A Framework for Learning Ante-hoc Explainable Models via Concepts</em>: <a href="https://arxiv.org/abs/2108.11761">https://arxiv.org/abs/2108.11761</a></li>
				<li><em class="italic">Demystifying Post-hoc Explainability for ML models</em>: <a href="https://spectra.mathpix.com/article/2021.09.00007/demystify-post-hoc-explainability">https://spectra.mathpix.com/article/2021.09.00007/demystify-post-hoc-explainability</a></li>
				<li><em class="italic">A Look Into Global, Cohort and Local Model Explainability</em>: <a href="https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f">https://towardsdatascience.com/a-look-into-global-cohort-and-local-model-explainability-973bd449969f</a></li>
				<li><em class="italic">What Are the Prevailing Explainability Methods? </em><a href="https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df">https://towardsdatascience.com/what-are-the-prevailing-explainability-methods-3bc1a44f94df</a></li>
				<li><em class="italic">Explainable Artificial Intelligence: Objectives, Stakeholders, and Future Research Opportunities</em>: <a href="https://www.tandfonline.com/doi/full/10.1080/10580530.2020.1849465">https://www.tandfonline.com/doi/full/10.1080/10580530.2020.1849465</a></li>
			</ul>
		</div>
	</div></body></html>