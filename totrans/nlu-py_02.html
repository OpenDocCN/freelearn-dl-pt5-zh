<html><head></head><body>
		<div id="_idContainer016">
			<h1 id="_idParaDest-40" class="chapter-number"><a id="_idTextAnchor044"/>2</h1>
			<h1 id="_idParaDest-41"><a id="_idTextAnchor045"/>Identifying Practical Natural Language Understanding Problems</h1>
			<p>In this chapter, you will learn how to identify <strong class="bold">natural language understanding</strong> (<strong class="bold">NLU</strong>) problems that are a good fit for today’s technology. That means they will not be too difficult for the state-of-the-art NLU approaches but neither can they be addressed by simple, non-NLU approaches. Practical NLU problems also require sufficient training data. Without sufficient training data, the resulting NLU system will perform poorly. The benefits of an NLU system also must justify its development and maintenance costs. While many of these considerations are things that project managers should think about, they also apply to students who are looking for class projects or <span class="No-Break">thesis topics.</span></p>
			<p>Before starting a project that involves NLU, the first question to ask is whether the goals of the project are a good fit for the current state of the art in NLU. Is NLU the right technology for solving the problem that you wish to address? How does the difficulty of the problem compare to the NLU state of <span class="No-Break">the art?</span></p>
			<p>Starting out, it’s also important to decide what <em class="italic">solving the problem</em> means. Problems can be solved to different degrees. If the application is a class project, demo, or proof of concept, the solution does not have to be as accurate as a deployed solution that’s designed for the robust processing of thousands of user inputs a day. Similarly, if the problem is a cutting-edge research question, any improvement over the current state of the art is valuable, even if the problem isn’t completely solved by the work done in the project. How complete the solution has to be is a question that everyone needs to decide as they think about the problem that they want <span class="No-Break">to address.</span></p>
			<p>The project manager, or whoever is responsible for making the technical decisions about what technologies to use, should decide what level of accuracy they would find acceptable when the project is completed, keeping in mind that 100% accuracy is unlikely to be achievable in any natural language <span class="No-Break">technology application.</span></p>
			<p>This chapter will get into the details of identifying problems where NLU is applicable. Follow the principles discussed in this chapter, and you will be rewarded with a quality, working system that solves a real problem for <span class="No-Break">its users.</span></p>
			<p>The following topics are covered in <span class="No-Break">this chapter:</span></p>
			<ul>
				<li>Identifying problems that are the appropriate level of difficulty for <span class="No-Break">the technology</span></li>
				<li>Looking at difficult <span class="No-Break">NLU applications</span></li>
				<li>Looking at applications that don’t <span class="No-Break">need NLP</span></li>
				<li><span class="No-Break">Training data</span></li>
				<li><span class="No-Break">Application data</span></li>
				<li>Taking development costs <span class="No-Break">into account</span></li>
				<li>Taking maintenance costs <span class="No-Break">into account</span></li>
				<li>A flowchart for deciding on <span class="No-Break">NLU applications</span></li>
			</ul>
			<h2 id="_idParaDest-42"><a id="_idTextAnchor046"/>Identifying problems that are the appropriate level of difficulty for the technology</h2>
			<p class="callout-heading">Note</p>
			<p class="callout">This chapter is focused on technical considerations. Questions such as whether a market exists for a proposed application, or how to decide whether customers will find it appealing, are important questions, but they are outside of the scope of <span class="No-Break">this book.</span></p>
			<p>Here are some kinds<a id="_idIndexMarker076"/> of problems that are a good fit for the state of <span class="No-Break">the art.</span></p>
			<p>Today’s NLU is very good at handling problems based on specific, concrete topics, such as <span class="No-Break">these examples:</span></p>
			<ul>
				<li><strong class="bold">Classifying customers’ product reviews into positive and negative reviews</strong>: Online sellers typically offer buyers a chance to review products they have bought, which is helpful for other prospective buyers as well as for sellers. But large online retailers with<a id="_idIndexMarker077"/> thousands of products are then faced with the problem of what to do with the information from thousands of reviews. It’s impossible for human tabulators to read all the incoming reviews, so an automated product review classification system would be <span class="No-Break">very helpful.</span></li>
				<li><strong class="bold">Answering basic banking questions about account balances or recent transactions</strong>: Banks and other financial institutions have large contact centers that handle customer questions. Often, the most common reasons for calling are simple questions about account balances, which can be answered with a database lookup based on account numbers and account types. An automated system can handle these by asking callers for their account numbers and the kind of information <span class="No-Break">they need.</span></li>
				<li><strong class="bold">Making simple stock trades</strong>: Buying and selling stock can become very complex, but in many cases, users simply want to buy or sell a certain number of shares of a specific company. This kind of transaction only needs a few pieces of information, such as an account number, the company, the number of shares, and whether to buy <span class="No-Break">or sell.</span></li>
				<li><strong class="bold">Package tracking</strong>: Package tracking needs only a tracking number to tell users the status of their shipments. While web-based package tracking is common, sometimes, people don’t have access to the web. With a natural language application, users can track packages with just a <span class="No-Break">phone call.</span></li>
				<li><strong class="bold">Routing customers’ questions to the right customer service agent</strong>: Many customers have questions that can only be answered by a human customer service agent. For those customers, an NLU system can still be helpful by directing the callers to the call center agents in the right department. It can ask the customer the reason for their call, classify the request, and then automatically route their call to the expert or department that handles <span class="No-Break">that topic.</span><a id="_idTextAnchor047"/></li>
				<li><strong class="bold">Providing information about weather forecasts, sports scores, and historical facts</strong>: These kinds of applications are characterized by requests that have a few well-defined parameters. For sports scores, this would be a team name and possibly the date of a game. For weather forecasts, the parameters include the location and timeframe for <span class="No-Break">the forecast.</span></li>
			</ul>
			<p>All of these applications are characterized by having unambiguous, correct answers. In addition, the user’s language that the system is expected to understand is not too complex. These would all be suitable topics for an <span class="No-Break">NLU project.</span></p>
			<p>Let’s illustrate what makes these applications suitable for today’s technology by going into more detail on providing information about weather forecasts, sports scores, and <span class="No-Break">historical facts.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em> shows a sample architecture for an application that can provide weather forecasts for different cities. Processing starts when the user asks, <em class="italic">What is the weather forecast for tomorrow in New York City?</em> Note that the user is making a single, short request, for specific information – the weather forecast, for a particular date, in a particular location. The NLU system needs to detect the intent (weather forecast), the entities’ <em class="italic">location</em>, and the <em class="italic">date</em>. These should all be easy to find – the entities are very dissimilar, and the <em class="italic">weather forecast</em> intent is not likely to be confused with any other intents. This makes it straightforward for the NLU system to convert the user’s question to a structured message that could be interpreted by a weather information web service, as shown at the top of the <span class="No-Break">following figure:</span></p>
			<div>
				<div id="_idContainer014" class="IMG---Figure">
					<img src="image/B19005_02_01.jpg" alt="Figure 2.1 – A practical NLU application"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – A practical NLU application</p>
			<p>Despite the fact<a id="_idIndexMarker078"/> that the information<a id="_idIndexMarker079"/> being requested is not very complex, there are many ways to ask about it, which means that it’s not very practical to just make a list of possible user queries. <em class="italic">Table 2.1</em> illustrates a few of the many ways to make <span class="No-Break">this request:</span></p>
			<table id="table001-1" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Some paraphrases of “What is the weather forecast for tomorrow in New <span class="No-Break">York City?”</span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>What will the weather be like tomorrow in <span class="No-Break">New York?</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>What’s tomorrow’s weather for <span class="No-Break">New York?</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>I want the New York City weather forecast <span class="No-Break">for tomorrow.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>The weather tomorrow in New <span class="No-Break">York, please.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>New York weather forecast <span class="No-Break">for tomorrow.</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>Tomorrow’s weather forecast for New <span class="No-Break">York City.</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><a id="_idTextAnchor048"/>Table 2.1 – Paraphrases for a weather request</p>
			<p>Another aspect of this application<a id="_idIndexMarker080"/> that makes it a good candidate for NLU is that the information<a id="_idIndexMarker081"/> the user is asking about (weather forecasts) is available from multiple easily-accessible, cloud-based web services, with <strong class="bold">application programming interfaces</strong> (<strong class="bold">APIs</strong>) that are usually well documented. This makes it easy for developers to send queries to the web services and get back the information that the user requested in a structured form. This information can then be presented to the user. Developers have choices about how they want to present the information – for example, text, graphics, or a combination of text <span class="No-Break">and graphics.</span></p>
			<p>In <span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.1</em>, we can see that the developer<a id="_idIndexMarker082"/> has chosen to present the information via natural language, and consequently, a <strong class="bold">natural language generation</strong> (<strong class="bold">NLG</strong>) component is used to generate the natural language output from a form. Other presentation options would be to show graphics, such as a picture of the sun partially covered by a cloud, or to simply show a form with the information received from the weather information web service. However, only the NLG option is a good fit for a spoken or voice-only interface such as a smart speaker since, with a voice-only interface, there is no way to <span class="No-Break">display graphics.</span></p>
			<p>The biggest benefit of NLU for an application<a id="_idIndexMarker083"/> such as weather forecasting is that NLU can handle the many possible ways that the user might ask this question with the same intent, as shown in <span class="No-Break"><em class="italic">Table 2.1</em></span><span class="No-Break">.</span></p>
			<p><em class="italic">Table 2.1</em> shows some paraphrases of a weather forecast request. These are just a few examples of possible ways to ask for a weather forecast. It is often surprising how many different ways there are to make even a simple request. If we could make a list of all the options, even if it was a very long list, NLU wouldn’t <span class="No-Break">be necessary.</span></p>
			<p>We could theoretically just list all the possibilities and map them to the structured queries. However, it’s actually very difficult to anticipate all the possible ways that someone would ask even a simple question about the weather. If a user happens to phrase their query in a way that the developer hasn’t included in their list, the system will fail to respond. This can be very confusing to users because users won’t understand why this query failed when similar queries worked. An NLU system will be able to cope with many more <span class="No-Break">query variations.</span></p>
			<p>As we’ve seen in this section, applications that have clear and easily identifiable intents and entities and that have definite answers that can be obtained from web resources, have a good chance of success with today’s <span class="No-Break">NLU technology.</span></p>
			<p>Now, let’s turn to applications<a id="_idIndexMarker084"/> that are unlikely to be successful because they require capabilities that are beyond the state of <span class="No-Break">the art.</span></p>
			<h2 id="_idParaDest-43"><a id="_idTextAnchor049"/>Looking at difficult applications of NLU</h2>
			<p>How can we tell whether<a id="_idIndexMarker085"/> the problem is too hard for the state of the art? First of all, we can ask what it means for a problem to be <em class="italic">too hard</em>. Here are some consequences of trying to use NLU for an application that is beyond the state of <span class="No-Break">the art:</span></p>
			<ul>
				<li>The system will be unable to reliably understand <span class="No-Break">user queries</span></li>
				<li>Answers will contain errors because the system has misunderstood <span class="No-Break">user queries</span></li>
				<li>The system will have to say <em class="italic">I don’t know</em> or <em class="italic">I can’t do that</em> so frequently that users become frustrated and decide not to use the <span class="No-Break">application anymore</span></li>
			</ul>
			<p>It’s important to keep in mind that the state of the art is rapidly improving. Remarkable progress has been made recently as cloud-based <strong class="bold">large language models</strong> (<strong class="bold">LLMs</strong>) such as ChatGPT have become<a id="_idIndexMarker086"/> available. Some applications that might<a id="_idIndexMarker087"/> be very hard now will not always be <span class="No-Break">too hard.</span></p>
			<p>Let’s look at a few of the characteristics of today’s difficult <span class="No-Break">NLU problems.</span></p>
			<h3>Applications that require the system to use judgment or common sense</h3>
			<p>Unlike the weather example<a id="_idIndexMarker088"/> in the previous section, applications that require judgment are applications where there isn’t a single correct answer, or even a few reasonable alternatives. These could include applications where the user is asking for advice that depends on many, often complex, considerations. Here are <span class="No-Break">some examples:</span></p>
			<ul>
				<li>Should I <span class="No-Break">learn Python?</span></li>
				<li>Should I get a <span class="No-Break">COVID vaccine?</span></li>
				<li>Should I buy an <span class="No-Break">electric car?</span></li>
				<li>Is this a good time to buy <span class="No-Break">a house?</span></li>
			</ul>
			<p>To answer the first question, the system needs specific knowledge about the user (whether the user already has a programming background or what they want to do with their new programming skills). LLM-based systems, such as ChatGPT, will respond to these kinds of questions in a general way – for example, by providing generic considerations about buying a house – but they can’t give advice that’s specific to the user, because they don’t know anything about <span class="No-Break">the user.</span></p>
			<p>Applications in which the system is asked for a subjective opinion are also very difficult to handle well, such as <span class="No-Break">these examples:</span></p>
			<ul>
				<li>What is the best movie of <span class="No-Break">all time?</span></li>
				<li>Who was the most talented <span class="No-Break">20th-century actor?</span></li>
				<li>What is a good way to cook chicken that doesn’t take more than half <span class="No-Break">an hour?</span></li>
			</ul>
			<p>To fully answer these kinds of queries requires the system to have a lot of general knowledge, such as actors who had careers in the 20th century. A system could respond to subjective questions by giving a random answer – just pick a movie at random and say that that movie is the best of all time. However, a randomly picked movie is not necessarily going to even be good, let alone the best movie of <span class="No-Break">all time.</span></p>
			<p>In that case, if there’s a follow-up question, the system won’t be able to explain or defend its opinion. So, if you asked a system <em class="italic">Should I buy an electric car</em>, and it said <em class="italic">Yes</em>, it wouldn’t be able to explain why it said yes. In fact, it’s probably too difficult for many of today’s systems to even realize that they’re being asked a subjective question. As in the case of questions that require the knowledge of the user to give a good answer, LLM-based systems<a id="_idIndexMarker089"/> will give generic answers to subjective questions, but they will admit that they aren’t able to deal <span class="No-Break">with subjectivity.</span></p>
			<h3>Applications that require dealing with hypotheticals, possibilities, and counterfactuals</h3>
			<p>Another difficult area<a id="_idIndexMarker090"/> is dealing with information<a id="_idIndexMarker091"/> that isn’t true or is possibly<a id="_idIndexMarker092"/> not true. When the user asks about something that might happen, if the circumstances are right, the user is asking about a hypothetical or a possibility. Today’s state-of-the-art systems are good at providing specific, concrete information, but the technology is not good at reasoning about possibilities. Here are <span class="No-Break">some examples:</span></p>
			<ul>
				<li>If I have a budget of $15,000, how big of a patio should I be able to build, assuming I’m willing to do some of the <span class="No-Break">work myself?</span></li>
				<li>If I have six people, how many pizzas should <span class="No-Break">I get?</span></li>
				<li>If there’s no rain in the forecast tomorrow, remind me to water <span class="No-Break">my plants.</span></li>
			</ul>
			<p>Similarly, systems aren’t very good at reasoning about things that aren’t true. For example, consider the sentence, <em class="italic">I’d like to find a nearby Asian restaurant, but not Japanese</em>. To answer this question<a id="_idIndexMarker093"/> correctly, the system has to find Asian restaurants, and it has to understand<a id="_idIndexMarker094"/> that it should exclude Japanese<a id="_idIndexMarker095"/> restaurants, which are nevertheless Asian, from <span class="No-Break">the list.</span></p>
			<h3>Applications that require combining information from a language with information from various sensors</h3>
			<p>Some very interesting applications<a id="_idIndexMarker096"/> could involve integrating information<a id="_idIndexMarker097"/> from language and cameras or microphones. These are called <strong class="bold">multimodal</strong> applications because they integrate multiple modalities such as speech, images, and non-speech audio such <span class="No-Break">as music:</span></p>
			<ul>
				<li>Is this cake done? (holding camera up <span class="No-Break">to cake)</span></li>
				<li>What is this noise that my car is making? (holding microphone up <span class="No-Break">to engine)</span></li>
			</ul>
			<p>These applications are currently beyond the state of the art of today’s commercial natural language technology, although they could be appropriate for an exploratory research project. They are also currently outside of the capabilities of LLMs, which can only understand <span class="No-Break">text input.</span></p>
			<h3>Applications that integrate broad general or expert knowledge</h3>
			<p>When users interact<a id="_idIndexMarker098"/> with an NLU system, they have goals that they want to accomplish. In many cases, the system has some kind of knowledge or expertise that the user doesn’t have, and the user wants to take advantage of that expertise. But where does that expertise come from? Providing systems with large amounts of knowledge is difficult. There are existing web APIs for simple information such as sports scores and weather. Systems such as Wolfram Alpha can also answer more complicated questions, such as <span class="No-Break">scientific facts.</span></p>
			<p>On the other hand, answering questions that require the use of expert knowledge, such as medical information, is more difficult, as there’s no easily accessible source of this kind of knowledge. In addition, existing sources of information might be inconsistent. One obvious<a id="_idIndexMarker099"/> source of large amounts of knowledge is the <strong class="bold">World Wide Web</strong> (<strong class="bold">WWW</strong>), which is the major source of knowledge of LLM. However, knowledge available on the WWW can be wrong, inconsistent, or not applicable to a particular situation, so it has to be used <span class="No-Break">with caution.</span></p>
			<p>These are a few examples of difficult topics for today’s natural <span class="No-Break">language technology:</span></p>
			<ul>
				<li><strong class="bold">Answer complex technical questions</strong>: A statement like <em class="italic">I can’t connect to the internet</em> requires the system to have a lot of information about internet connectivity as well as how to debug connectivity problems. It would also have to have access to other time-sensitive information such as whether there are global internet outages in the <span class="No-Break">user’s area.</span></li>
				<li><strong class="bold">Answer questions that require an understanding of human relationships</strong>: <em class="italic">My friend won’t talk to me since I started dating her boyfriend; what should I do?</em> A system would have to understand a lot about dating, and probably dating in a specific culture as well, in order to give a good answer to a question <span class="No-Break">like this.</span></li>
				<li><strong class="bold">Read a book and tell me whether I would like that book</strong>: Today’s systems would have a hard time even reading and understanding an entire book since long texts like books contain very complex information. In addition to just reading a book, for a system to tell me whether I would like it requires a lot of information about me and my <span class="No-Break">reading interests.</span></li>
				<li><strong class="bold">Read an article from a medical journal and tell me whether the findings apply to me</strong>: Answering questions like this would require a tremendous amount of information about the user’s health and medical history, as well as the ability to understand medical language and interpret the results of <span class="No-Break">medical studies.</span></li>
				<li><strong class="bold">Understand jokes</strong>: Understanding jokes often requires considerable cultural knowledge. Think about what knowledge a system would need to be able to understand the traditional joke, <em class="italic">Why did the chicken cross the road? To get to the other side</em>. This is funny because the question leads the user to believe that the chicken has an interesting<a id="_idIndexMarker100"/> reason for crossing the road, but its reason turns out to be extremely obvious. Not only would it be very hard for a system to be able to understand why this particular joke is funny, but this is only one joke—just being able to understand this joke wouldn’t help a system understand any <span class="No-Break">other jokes.</span></li>
				<li><strong class="bold">Interpret figures of speech</strong>: <em class="italic">I could eat a horse</em> doesn’t mean that you want to eat a horse, it just means that you’re very hungry. A system would have to realize that this is a figure of speech because horses are very large and no one could actually eat a horse in one sitting, no matter how hungry they are. On the other hand, <em class="italic">I could eat a pizza</em> is not a figure of speech and probably just means that the user would like to order <span class="No-Break">a pizza.</span></li>
				<li><strong class="bold">Understand irony and sarcasm</strong>: If a book review contains a sentence like <em class="italic">The author is a real genius</em>, the review writer might mean that the author is literally a genius, but not necessarily. This could be intended sarcastically to mean that the author is not a genius at all. If this sentence is followed by <em class="italic">My three-year-old could have written a better book</em>, we can tell that the first sentence was intended to be taken as sarcasm. NLU systems can’t understand sarcasm. They also don’t know that three-year-olds are unlikely to be able to write good books, and so the writer of the review is claiming that the book is worse than one authored by a three-year-old, and so it is a <span class="No-Break">bad book.</span></li>
				<li><strong class="bold">Be able to make use of complex knowledge</strong>: As an example of complex knowledge, consider the utterance, <em class="italic">My cake is as flat as a pancake; what went wrong?</em> To answer this question, the system has to understand that a cake shouldn’t be flat but that pancakes are normally flat. It also has to understand that we’re talking about a cake that has been baked, as unbaked cakes are typically flat. Once the system has figured all this out, it also has to understand the process of baking enough to give advice about why the cake <span class="No-Break">is flat.</span></li>
			</ul>
			<p>One general property shared <a id="_idIndexMarker101"/>by many of these difficult types of applications is that there often isn’t any one data source where the answers can be obtained. That is, there aren’t any backend data sources that developers can just query to answer a question like <em class="italic">Is this a good time to buy an electric car?</em> This is in contrast to the earlier weather forecast example, where developers can go to a single backend <span class="No-Break">data source.</span></p>
			<p>Rather than trying to find a single backend data source, one strategy might be to do a web search for the question. But as anyone who’s done a web search knows, there will be millions of search results (nearly 2 billion for <em class="italic">Is this a good time to buy an electric car?</em>), and what’s worse, the answers are not likely to be consistent with each other. Some pages will assert that it is a good time to buy an electric car, and others will assert that it is not. So, the strategy of using a web search to answer questions without a good data source will probably<a id="_idIndexMarker102"/> not work. However, being able to integrate information from across the web is a strength of LLMs, so if the information is available on the web, an LLM such as ChatGPT will be able to <span class="No-Break">find it.</span></p>
			<h3>Applications where users often don’t have a clear idea of what they want</h3>
			<p>Users don’t always state their<a id="_idIndexMarker103"/> intentions very clearly. As an example, consider a tourist who’s visiting an unfamiliar town. Perhaps the town provides a service that tourists can call to find out about public transportation options. If a tourist asks a question like <em class="italic">What train should I take to get from the Marriott Hotel to 123 Market Street?</em>, a literal answer might be <em class="italic">You can’t take the train from the Marriott Hotel to 123 Market Street</em>. Or the user might be offered a circuitous route that takes <span class="No-Break">six hours.</span></p>
			<p>A human agent could figure out that the caller’s actual goal is probably to get from the Marriott Hotel to 123 Market Street, and the reference to the train was just the caller’s guess that the train would be a good way to do that. In that case, a human agent could say something like <em class="italic">There isn’t really a good train route between those two locations; would you like some other ideas about how to get between them?</em> This would be natural for a human agent but very difficult for an automated system, because the system would need to be able to reason about what the user’s real <span class="No-Break">goal is.</span></p>
			<h3>Applications that require understanding multiple languages</h3>
			<p>As discussed in <a href="B19005_01.xhtml#_idTextAnchor016"><span class="No-Break"><em class="italic">Chapter 1</em></span></a>, language technology is better for some languages<a id="_idIndexMarker104"/> than others. If a system has to be able to communicate with users (by speech or text) in different languages, then language models for each language have to be developed. Processing for some languages will be more accurate than processing for other languages, and for some languages, processing might not be good enough at all. At the current state of the art, NLP technology for major European, Middle Eastern, and Asian languages should be able to handle <span class="No-Break">most applications.</span></p>
			<p>In some applications, the system has to be prepared to speak different languages depending on what the user says. To do this, the system has to be able to tell the different languages apart just by their sounds or words. This<a id="_idIndexMarker105"/> technology is called <strong class="bold">language identification</strong>. Identifying commonly spoken languages is not difficult but, again, this is not the case for less <span class="No-Break">common languages.</span></p>
			<p>In the case of languages with very little training data, such as languages with fewer than one million speakers, the language may not have been studied well enough for natural language applications to be developed for <span class="No-Break">that language.</span></p>
			<p>Even more difficult than understanding multiple languages is handling cases where two or more languages are mixed in the same sentence. This often happens when several different languages are spoken in the same area, and people can assume that anyone they talk with can understand all the local languages. Mixing languages<a id="_idIndexMarker106"/> in the same sentence is called <strong class="bold">code-switching</strong>. Processing sentences with code-switching is even more difficult than processing several languages in the same application because the system has to be prepared for any word in any of the languages it knows at any point in the sentence. This is a difficult problem for <span class="No-Break">today’s technology.</span></p>
			<p>In the preceding discussion, we’ve reviewed<a id="_idIndexMarker107"/> many factors that make applications too difficult for today’s state of the art in NLP. Let’s now look at applications that are <span class="No-Break">too easy.</span></p>
			<h2 id="_idParaDest-44"><a id="_idTextAnchor050"/>Looking at applications that don’t need NLP</h2>
			<p>Turning from applications<a id="_idIndexMarker108"/> that are too difficult, we can also look at applications<a id="_idIndexMarker109"/> that are too easy – that is, applications where simpler solutions than NLP will work, and where NLP is overkill. These are applications where the complexity of the problem doesn’t justify the complexity of building and managing a natural <span class="No-Break">language system.</span></p>
			<p>Natural language is characterized by unpredictable inputs and an indirect mapping of words to meanings. Different words can have the same meaning, and different meanings can be expressed by the same words, depending on the context. If there is a simple one-to-one mapping between inputs and meanings, NLP <span class="No-Break">isn’t necessary.</span></p>
			<h3>Text that can be analyzed with regular expressions</h3>
			<p>The first case where NLU<a id="_idIndexMarker110"/> isn’t necessary is when<a id="_idIndexMarker111"/> the possible inputs consist of a limited<a id="_idIndexMarker112"/> set of options, such as cities, states, or countries. Internally, such inputs can be represented as lists, and can be analyzed via table lookup. Even if there are synonyms for certain inputs (<em class="italic">UK</em> for the <em class="italic">United Kingdom</em>, for example), the synonyms can be included in the lists <span class="No-Break">as well.</span></p>
			<p>A slightly more complicated, but still simple, input is when every input to the system is composed according to easily stated, unvarying rules. NLP is not necessary in those cases because the input is predictable. Good examples of these kinds of simple expressions are telephone numbers, which have fixed, predictable formats, or dates, which are more varied, but still limited. In addition to these generic expressions, in specific applications, there is often a requirement to analyze expressions such as product IDs or serial numbers. These types of inputs can be analyzed with regular expressions. Regular expressions are rules that describe patterns of characters (alphabetical, numerical, or special characters). For example, the <strong class="source-inline">^\d{5}(-\d{4})?$</strong> regular expression matches US zip codes, either containing five digits (<strong class="source-inline">12345</strong>) or containing five digits followed by a hyphen, and then four more <span class="No-Break">digits (</span><span class="No-Break"><strong class="source-inline">12345-1234</strong></span><span class="No-Break">).</span></p>
			<p>If all of the inputs in an application are these kinds of fixed phrases, regular expressions can do the job without requiring full-scale NLP. If the entire problem can be solved with regular expressions, then NLP isn’t needed. If only part of the problem can be solved with regular expressions, but part of it needs NLP, regular expressions can be combined with natural language techniques. For example, if the text includes formatted numbers such as phone numbers, zip codes, or dates, regular expressions can be used to just analyze those numbers. Python has excellent libraries for handling regular expressions if regular expressions are needed in an application. We will discuss combining NLP and regular expressions in <a href="B19005_08.xhtml#_idTextAnchor159"><span class="No-Break"><em class="italic">Chapter 8</em></span></a> and <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a><span class="No-Break">.</span></p>
			<h3>Recognizing inputs from a known list of words</h3>
			<p>If the only available inputs<a id="_idIndexMarker113"/> are from a fixed set<a id="_idIndexMarker114"/> of possibilities, then NLP isn’t needed. For example, if the input can only be a US state, then the application can just look for the names of states. Things can get a little more complicated if the inputs include words from a fixed<a id="_idIndexMarker115"/> set of possibilities, but there are surrounding words. This is called <strong class="bold">keyword spotting</strong>. This can happen if the desired response is from a fixed set of words, such as the name of one of 50 states, and the users sometimes add something – for example, the user says <em class="italic">I live in Arizona</em> in response to a system question like <em class="italic">Where do </em><span class="No-Break"><em class="italic">you live?</em></span></p>
			<p>NLP is probably not needed for this – the system just has to be able to ignore the irrelevant words (<em class="italic">I live in</em>, in this example). Regular expressions<a id="_idIndexMarker116"/> can be written to ignore irrelevant words by using <strong class="bold">wildcard</strong> characters. Python regular expressions use <strong class="source-inline">*</strong> to match any number of characters, including zero. Python uses <strong class="source-inline">+</strong> to match at least <a id="_idIndexMarker117"/>one character. So, a regular expression for spotting the keyword <strong class="source-inline">Arizona</strong> in Python would just <span class="No-Break">be </span><span class="No-Break"><strong class="source-inline">*Arizona*</strong></span><span class="No-Break">.</span></p>
			<h3>Using graphical interfaces</h3>
			<p>Most applications rely on a <strong class="bold">graphical user interface</strong>, where the user interacts with the application<a id="_idIndexMarker118"/> by selecting choices from menus<a id="_idIndexMarker119"/> and clicking buttons. These conventional interfaces are easier to build than NLU-based interfaces and are perfectly suitable for many applications. When is an NLU-based interface a <span class="No-Break">better choice?</span></p>
			<p>NLU is a better<a id="_idIndexMarker120"/> choice as the information<a id="_idIndexMarker121"/> that the user has to supply becomes more detailed. When this happens, a graphical interface has to rely on deeper and deeper levels of menus, requiring users to navigate through menu after menu until they find the information they need or until the application has collected enough information to answer their questions. This is especially a problem with mobile interfaces, where the amount of information that can fit on the screen is much less than the amount of information that fits on a laptop or desktop computer, which means that the menus need to have deeper levels. On the other hand, an NLU input allows the user to state their goal once, without having to navigate through <span class="No-Break">multiple menus.</span></p>
			<p>Another problem that graphical interfaces with deep menus have is that the terminology used in the menus does not always match the users’ mental models of their goals. These mismatches can lead users down the wrong path. They might not realize their mistake until several levels farther down in the menu tree. When that happens, the user has to start all <span class="No-Break">over again.</span></p>
			<p>The contrast between graphical and NLP applications can easily be seen on websites and applications that include both a conventional graphical interface and an NLP chatbot. In those interfaces, the user can choose between menu-based navigation and interacting with the chatbot. A good example is the Microsoft Word 2016 interface. Word is a very complex application with a rich set of capabilities. Making an intuitive graphical interface for an application that is this complex is difficult, and it can be hard for users to find the information they’re <span class="No-Break">looking for.</span></p>
			<p>To address this, Microsoft provides both graphical and NLP interfaces to Word functionality. At the top of the page of a Word document, there are choices including <strong class="bold">Home</strong>, <strong class="bold">Insert</strong>, <strong class="bold">Draw</strong>, and <strong class="bold">Layout</strong>. Clicking on any of these changes the ribbon to offer many more choices, often opening up more and more levels of menus. This is the graphical approach. But Word also offers a <strong class="bold">Tell me</strong> option as one of the top-level menu options. If the user selects <strong class="bold">Tell me</strong>, they can type natural language questions about how to do things in Word. For example, typing <strong class="source-inline">How do I add an equation</strong> will provide a list of several different ways to add an equation to a Word document. This is much quicker and more direct than looking through <span class="No-Break">nested menus.</span></p>
			<p>Developers should consider adding NLU functionality to graphical applications when menu levels get more than three or so levels deep, especially if each menu level has <span class="No-Break">many choices.</span></p>
			<p>So far, we’ve looked at many factors<a id="_idIndexMarker122"/> that make an application<a id="_idIndexMarker123"/> more or less suited for NLP technology. The next considerations are related to the development process – the availability of data and the development process itself, which we discuss in the <span class="No-Break">next sections.</span></p>
			<h3>Ensuring that sufficient data is available</h3>
			<p>Having determined<a id="_idIndexMarker124"/> whether the problem is suitable<a id="_idIndexMarker125"/> for NLU, we can turn to the next question – what kinds of data are available for addressing this problem? Is there existing data? If not, what would be involved in obtaining the kind of data that’s needed to solve <span class="No-Break">the problem?</span></p>
			<p>We will look at two kinds of data. First, we will consider <em class="italic">training data</em>, or examples of the kinds of language that will be used by users of NLU systems, and we will look at sources of training data. The second kind of data that we will discuss is <em class="italic">application data</em>. The information in this section will enable you to determine whether you have enough training data and how much work it will take to format it properly to be used in the NLU system <span class="No-Break">development process.</span></p>
			<p>Application data is the information that the system will use to answer users’ questions. As we will see, it can come from publicly available sources or from internal databases. For application data, we will see<a id="_idIndexMarker126"/> that it is important to ensure that the data is available, reliable, and can be obtained <a id="_idIndexMarker127"/>without <span class="No-Break">excessive cost.</span></p>
			<h2 id="_idParaDest-45"><a id="_idTextAnchor051"/>Training data</h2>
			<p>Natural language applications<a id="_idIndexMarker128"/> are nearly all trained based on examples of the kinds of inputs they’re expected to process. That means that sufficient training data needs to be available in order for any natural language application to be successful. Not having enough training data means that when the application is deployed, there will be inputs that can’t be processed because the system hasn’t been exposed to any similar inputs during the development phase. This doesn’t mean that the system needs to see every possible input during training. This is nearly impossible, especially if the intended inputs are long or complex documents such as <span class="No-Break">product reviews.</span></p>
			<p>It is extremely unlikely that the same review will occur more than once. Rather, the training process is designed so that documents that are semantically similar will be analyzed in the same way, even if the exact words and phrasings <span class="No-Break">are different.</span></p>
			<p>Machine learning algorithms such as those we’ll be learning about in <a href="B19005_09.xhtml#_idTextAnchor173"><span class="No-Break"><em class="italic">Chapter 9</em></span></a> and <a href="B19005_10.xhtml#_idTextAnchor184"><span class="No-Break"><em class="italic">Chapter 10</em></span></a>, require fairly large amounts of data. The more different categories or intents that have to be distinguished, the more data is required. Most practical applications will need thousands of <span class="No-Break">training examples.</span></p>
			<p>In addition to examples, normally the training data also has to include the <em class="italic">right answer</em> or how the trained system is expected to analyze the data. The<a id="_idIndexMarker129"/> technical term for the <em class="italic">right answer</em> is <strong class="bold">annotation</strong>. Annotations can also be referred to as the <strong class="bold">ground truth</strong> or <strong class="bold">gold standard</strong>. For example, if the application is designed to determine whether<a id="_idIndexMarker130"/> a product review<a id="_idIndexMarker131"/> is positive or negative, annotations (provided by human judges) assign a positive or negative label to a set of reviews that will be used as training and <span class="No-Break">test data.</span></p>
			<p><em class="italic">Table 2.2</em> shows examples of positive and negative product reviews and their annotations. An accurate system for classifying product reviews would probably need to be based on several thousand product reviews. In some cases, as in the examples in <em class="italic">Table 2.2</em>, the task of annotation doesn’t require any special expertise; almost anyone with a reasonable command of English can decide whether a product review is positive or negative. This means that simple annotation tasks can be <span class="No-Break">inexpensively crowdsourced.</span></p>
			<p>On the other hand, some<a id="_idIndexMarker132"/> annotations<a id="_idIndexMarker133"/> have to be done by subject matter experts. For example, annotating data from an interactive troubleshooting dialog for a complex software product would probably need to be done by someone with expertise in that product. This would make the annotation process much more expensive and might not even be possible if the necessary experts <span class="No-Break">aren’t available:</span></p>
			<table id="table002" class="No-Table-Style _idGenTablePara-1">
				<colgroup>
					<col/>
					<col/>
				</colgroup>
				<thead>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p><span class="No-Break" lang="en-US" xml:lang="en-US">Text</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break" lang="en-US" xml:lang="en-US">Annotation</span></p>
						</td>
					</tr>
				</thead>
				<tbody>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>I was very disappointed with this product. It was flimsy, overpriced, and the paint <span class="No-Break">flaked off.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Negative</span></p>
						</td>
					</tr>
					<tr class="No-Table-Style">
						<td class="No-Table-Style">
							<p>This product met my every expectation. It is well made, looks great, and the price is right. I have no reservations about recommending it <span class="No-Break">to anyone.</span></p>
						</td>
						<td class="No-Table-Style">
							<p><span class="No-Break">Positive</span></p>
						</td>
					</tr>
				</tbody>
			</table>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 2.2 – Examples of positive and negative annotations of product reviews</p>
			<p>Although data annotation can be difficult and expensive, not all NLU algorithms require annotated data. In particular, unsupervised learning, which we will cover in <a href="B19005_12.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>, is based on unannotated data. In <a href="B19005_12.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic">Chapter 12</em></span></a>, we will also discuss the limits of <span class="No-Break">unannotated data.</span></p>
			<p>The full set of training<a id="_idIndexMarker134"/> examples<a id="_idIndexMarker135"/> for an application is called a <strong class="bold">corpus</strong>, or <strong class="bold">dataset</strong>. It is essential to have sufficient training data in order for the application to be accurate. The training data does not have to be available all at once – development can begin before the data collection is complete, and additional data can be added as development progresses. This can lead to problems with consistency if annotators forget the criteria that they used to annotate <span class="No-Break">earlier data.</span></p>
			<p>Where does data come from? Python NLP libraries contain several toy datasets that can be used to test system setup or algorithms, or that can be used in student projects where there’s no plan to put a system into production. In addition, larger datasets can also<a id="_idIndexMarker136"/> be obtained<a id="_idIndexMarker137"/> from organizations such as Hugging Face (https://huggingface.co/) or the Linguistic Data <span class="No-Break">Consortium (</span><span class="No-Break">https://www.ldc.upenn.edu/</span><span class="No-Break">).</span></p>
			<p>For enterprise applications, preexisting data from an earlier application that was performed by human agents can be very helpful. Examples of this could include transcripts of customer service calls <span class="No-Break">with agents.</span></p>
			<p>Another good source of data is the text fields of databases. For example, this is probably where you would expect to find product reviews for an organization’s products. In many cases, text fields of databases are accompanied by another field with a manual classification that identifies, for example, whether the review is positive or negative. This manual classification is, in effect, an annotation that can be used in the training process to create a system that can automatically classify <span class="No-Break">product reviews.</span></p>
			<p>Finally, new data can also be collected specifically to support an application. This can be time-consuming and expensive, but sometimes it’s the only way to get the appropriate data. Data collection can be a complex topic in itself, especially when the data is collected to support interactive dialogs with <span class="No-Break">human users.</span></p>
			<p>Data, including data<a id="_idIndexMarker138"/> collection, is discussed in more detail in <a href="B19005_05.xhtml#_idTextAnchor107"><span class="No-Break"><em class="italic">Chapter 5</em></span></a><span class="No-Break">.</span></p>
			<h2 id="_idParaDest-46"><a id="_idTextAnchor052"/>Application data</h2>
			<p>In addition to the data<a id="_idIndexMarker139"/> required to train the natural language application, it’s important to take into account any costs associated with accessing the information that the system will <span class="No-Break">be providing.</span></p>
			<p>Many third-party web services provide APIs that can be accessed by developers to obtain free or paid information. There are some websites that provide general information<a id="_idIndexMarker140"/> about available<a id="_idIndexMarker141"/> public APIs, such as <strong class="bold">APIsList</strong> (https://apislist.com/). This site lists APIs that can deliver data on topics ranging over hundreds of categories including weather, social networks, mapping, government, travel, and many more. Many APIs require payments, either as a subscription or per transaction, so it’s important<a id="_idIndexMarker142"/> to consider these potential costs when selecting <span class="No-Break">an application.</span></p>
			<h1 id="_idParaDest-47"><a id="_idTextAnchor053"/>Taking development costs into account</h1>
			<p>After making sure that data<a id="_idIndexMarker143"/> is available, and that the data is (or can be) annotated<a id="_idIndexMarker144"/> with the required intents, entities, and classification categories, the next consideration for deciding whether NLP is a good fit for an application is the cost of developing the application itself. Some technically feasible applications can nevertheless be impractical because they would be too costly, risky, or time-consuming <span class="No-Break">to develop.</span></p>
			<p>Development costs include determining the most effective machine learning approaches to a specific problem. This can take significant time and involve some trial and error as models need to be trained and retrained in the process of exploring different algorithms. Identifying the most promising algorithms is also likely to require NLP data scientists, who may be in short supply. Developers have to ask the question of whether the cost of development is consistent with the benefits that will be realized by the <span class="No-Break">final application.</span></p>
			<p>For low-volume applications, it should also be kept in mind that the cost of developing and deploying an NLP solution can exceed the cost of employing humans to perform the same tasks. This is particularly true if some humans will still be needed for more complex tasks, even if an NLP solution is implemented and is doing part of <span class="No-Break">the work</span></p>
			<h1 id="_idParaDest-48"><a id="_idTextAnchor054"/>Taking maintenance costs into account</h1>
			<p>The final consideration for natural language<a id="_idIndexMarker145"/> applications, especially deployed applications, is the cost<a id="_idIndexMarker146"/> of maintenance. This is easy to overlook because NLU applications have several maintenance considerations that don’t apply to most traditional applications. Specifically, the type of language used in some applications changes over time. This is expected since it reflects changes in the things that the users are talking about. In customer service applications, for example, product names, store locations, and services change, sometimes very quickly. The new vocabulary that customers use to ask about this information changes as well. This means that new words have to be added to the system, and machine learning models have to <span class="No-Break">be retrained.</span></p>
			<p>Similarly, applications<a id="_idIndexMarker147"/> that provide<a id="_idIndexMarker148"/> rapidly changing information need to be kept up to date on an ongoing basis. As an example, the word <em class="italic">COVID-19</em> was introduced in early 2020 – no one had ever heard it before, but now it is universally familiar. Since medical information about COVID-19 changes rapidly, a chatbot designed to provide COVID-19 information will have to be very carefully maintained in order to ensure that it’s up to date and is not providing incorrect or even <span class="No-Break">harmful information.</span></p>
			<p>In order to keep applications up to date with the users’ topics, three tasks that are specific to natural language applications need to be <span class="No-Break">planned for:</span></p>
			<ul>
				<li>Developers need to be assigned to keep the application up to date as new information (such as new products or new product categories) is added to <span class="No-Break">the system.</span></li>
				<li>Frequent review of platform-provided logging of user inputs should be done. User inputs that are not handled correctly must be analyzed to determine the correct way of handling them. Are the users asking about new topics (intents)? Then new intents have to be added. Are they talking about existing topics in different ways? If that’s the case, new training examples need to be added to the <span class="No-Break">existing intents.</span></li>
				<li>When issues are discovered and user inputs are not being handled correctly, the system needs to be modified. The simplest type of modification is adding new vocabulary, but in some cases, more structural changes are necessary. For example, it may be that an existing intent has to be split into multiple intents, which means that all the training data for the original intent has to <span class="No-Break">be reviewed.</span></li>
			</ul>
			<p>The number of developers required to keep the application updated depends on <span class="No-Break">several considerations:</span></p>
			<ul>
				<li><strong class="bold">The number of user inputs</strong>: If the system gets hundreds or thousands of failed inputs per day, developers need to be assigned to review these and add information to the system so that it can handle <span class="No-Break">these inputs</span></li>
				<li><strong class="bold">The complexity of the application</strong>: If the application includes hundreds of intents and entities, it will take more developers to keep it up to date and ensure that any new information stays consistent with <span class="No-Break">old information</span></li>
				<li><strong class="bold">The volatility of the information provided by the application</strong>: If the application is one where new words, new products, and new services are continually being added, the system will require more frequent changes to stay up <span class="No-Break">to date</span></li>
			</ul>
			<p>These costs are in addition<a id="_idIndexMarker149"/> to any costs for<a id="_idIndexMarker150"/> hardware or cloud services that are not specific to natural <span class="No-Break">language applications.</span></p>
			<h1 id="_idParaDest-49"><a id="_idTextAnchor055"/>A flowchart for deciding on NLU applications</h1>
			<p>This chapter has covered<a id="_idIndexMarker151"/> many considerations<a id="_idIndexMarker152"/> that should be taken into account in deciding on an <span class="No-Break">NLP application.</span></p>
			<p><span class="No-Break"><em class="italic">Figure 2</em></span><em class="italic">.2</em> summarizes these considerations as a flowchart of the process for evaluating a potential <span class="No-Break">NLU application.</span></p>
			<div>
				<div id="_idContainer015" class="IMG---Figure">
					<img src="image/B19005_02_02.jpg" alt="Figure 2.2 – Steps in evaluating an NLU project"/>
				</div>
			</div>
			<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Steps in evaluating an NLU project</p>
			<p>Starting at the top, the process starts by asking whether the problem is too hard or too easy for the current state of the art, using the criteria discussed earlier. If it’s either too hard or too easy, we should look for another application, or look at cutting back or expanding the scope of the application to make it a better fit for NLP technology. For example, the application might be redesigned to handle <span class="No-Break">fewer languages.</span></p>
			<p>If the problem seems to be a good fit for the state of the art, the next steps are to ensure that the appropriate data is available, and if not, whether data can be collected. Once data is available, the next thing<a id="_idIndexMarker153"/> to look at is to see whether the costs of development and maintenance<a id="_idIndexMarker154"/> are reasonable. If everything looks good, work on the application <span class="No-Break">can proceed.</span></p>
			<h1 id="_idParaDest-50"><a id="_idTextAnchor056"/>Summary</h1>
			<p><a id="_idTextAnchor057"/>In this chapter, we covered the topic of selecting NLP applications that have a good chance of success with current NLP technology. Successful applications generally have input with specific, objective answers, have training data available, and handle (at most) a <span class="No-Break">few languages.</span></p>
			<p>Specifically, this chapter addressed a number of important questions. We learned how to identify problems that are the appropriate level of difficulty for the current state of the art of NLU technology. We also learned how to ensure that sufficient data is available for system development and how to estimate the costs of development <span class="No-Break">and maintenance.</span></p>
			<p>Learning how to evaluate the feasibility of different types of NLP applications as discussed in this chapter will be extremely valuable as you move forward with your NLP projects. Selecting an application that is too ambitious will result in frustration and a failed project, whereas selecting an application that is too easy for the state of the art will lead to wasted time and an unnecessarily <span class="No-Break">complex system.</span></p>
			<p>We have achieved our goal of learning how to evaluate the feasibility of NLP projects in terms of important criteria such as technical feasibility as well as the practical considerations of data availability and <span class="No-Break">maintenance costs.</span></p>
			<p>In the next chapter, we will look at the major approaches to NLP and the advantages and disadvantages of each approach. These approaches include rule-based systems, in which human experts write rules that describe how the system should analyze inputs, and machine learning, where the system is trained to analyze inputs by processing many examples of inputs and how they should <span class="No-Break">be analyzed.</span></p>
		</div>
	

		<div id="_idContainer017" class="Content">
			<h1 id="_idParaDest-51"><a id="_idTextAnchor058"/>Part 2:Developing and Testing Natural Language Understanding Systems</h1>
			<p>After completing this section, you will be able to decide what techniques are applicable to address a problem with natural language understanding technologies and implement a system using Python and Python libraries such as NLTK, spaCy, and Keras, and <span class="No-Break">evaluate it.</span></p>
			<p>This part comprises the <span class="No-Break">following chapters:</span></p>
			<ul>
				<li><a href="B19005_03.xhtml#_idTextAnchor059"><em class="italic">Chapter 3</em></a>, <em class="italic">Approaches to Natural Language Understanding – Rule-Based Systems, Machine Learning, and Deep Learning</em></li>
				<li><a href="B19005_04.xhtml#_idTextAnchor085"><em class="italic">Chapter 4</em></a>, <em class="italic">Selecting Libraries and Tools for Natural Language Understanding</em></li>
				<li><a href="B19005_05.xhtml#_idTextAnchor107"><em class="italic">Chapter 5</em></a>, <em class="italic">Natural Language Data – Finding and Preparing Data</em></li>
				<li><a href="B19005_06.xhtml#_idTextAnchor134"><em class="italic">Chapter 6</em></a>, <em class="italic">Exploring and Visualizing Data</em></li>
				<li><a href="B19005_07.xhtml#_idTextAnchor144"><em class="italic">Chapter 7</em></a>, <em class="italic">Selecting Approaches and Representing Data</em></li>
				<li><a href="B19005_08.xhtml#_idTextAnchor159"><em class="italic">Chapter 8</em></a>, <em class="italic">Rule-Based Techniques</em></li>
				<li><a href="B19005_09.xhtml#_idTextAnchor173"><em class="italic">Chapter 9</em></a>, <em class="italic">Machine Learning Part 1 – Statistical Machine Learning</em></li>
				<li><a href="B19005_10.xhtml#_idTextAnchor184"><em class="italic">Chapter 10</em></a>, <em class="italic">Machine Learning Part 2 – Neural Networks and Deep Learning Techniques</em></li>
				<li><a href="B19005_11.xhtml#_idTextAnchor193"><em class="italic">Chapter 11</em></a>, <em class="italic">Machine Learning Part 3 – Transformers and Large Language Models</em></li>
				<li><a href="B19005_12.xhtml#_idTextAnchor217"><em class="italic">Chapter 12</em></a>, <em class="italic">Applying Unsupervised Learning Approaches</em></li>
				<li><a href="B19005_13.xhtml#_idTextAnchor226"><em class="italic">Chapter 13</em></a>, <em class="italic">How Well Does It Work? – Evaluation</em></li>
			</ul>
		</div>
		<div>
			<div id="_idContainer018" class="Basic-Graphics-Frame">
			</div>
		</div>
		<div>
			<div id="_idContainer019">
			</div>
		</div>
	</body></html>