<html><head></head><body>
<div id="_idContainer074">
<h1 class="chapter-number" id="_idParaDest-92"><a id="_idTextAnchor092"/><span class="koboSpan" id="kobo.1.1">6</span></h1>
<h1 id="_idParaDest-93"><a id="_idTextAnchor093"/><span class="koboSpan" id="kobo.2.1">Understanding Neural Network Transformers</span></h1>
<p><span class="koboSpan" id="kobo.3.1">Not to be confused with the electrical devices</span><a id="_idIndexMarker382"/><span class="koboSpan" id="kobo.4.1"> that are also called transformers, neural network transformers are the jack-of-all-trades variant of NNs. </span><span class="koboSpan" id="kobo.4.2">Transformers are capable of processing and capturing patterns from data of any modality, including sequential data such as text data and time-series data, image data, audio data, and </span><span class="No-Break"><span class="koboSpan" id="kobo.5.1">video data.</span></span></p>
<p><span class="koboSpan" id="kobo.6.1">The transformer architecture</span><a id="_idIndexMarker383"/><span class="koboSpan" id="kobo.7.1"> was introduced in 2017 with the motive of replacing RNN-based sequence-to-sequence architectures and primarily focusing on the machine translation use case of converting text data from one language to another language. </span><span class="koboSpan" id="kobo.7.2">The results performed better than the baseline RNN-based model and proved that we don’t need inherent inductive biases on the sequential nature of the data that the RNNs employ. </span><span class="koboSpan" id="kobo.7.3">Transformers then became the root of a family of neural network architectures and branched off to model variants that are capable of capturing patterns in other data modalities while continually raising the bar of the performance of the original text sequence data-based tasks. </span><span class="koboSpan" id="kobo.7.4">This showed that we don’t really need to have inherent inductive bias built into a model in general, regardless of the data modality, and instead, we can allow the model to </span><em class="italic"><span class="koboSpan" id="kobo.8.1">learn</span></em><span class="koboSpan" id="kobo.9.1"> these inherent structures and patterns. </span><span class="koboSpan" id="kobo.9.2">Do you have sequential data such as video, text, or audio? </span><span class="koboSpan" id="kobo.9.3">Let the neural network learn its sequential nature. </span><span class="koboSpan" id="kobo.9.4">Do you have image data? </span><span class="koboSpan" id="kobo.9.5">Let the neural network learn the spatial and depth relationships between the pixels. </span><span class="koboSpan" id="kobo.9.6">You get </span><span class="No-Break"><span class="koboSpan" id="kobo.10.1">the picture.</span></span></p>
<p><span class="koboSpan" id="kobo.11.1">Before we explore a more in-depth overview of transformers, take a breather to check out the topics that will be covered in </span><span class="No-Break"><span class="koboSpan" id="kobo.12.1">this chapter:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.13.1">Exploring neural </span><span class="No-Break"><span class="koboSpan" id="kobo.14.1">network transformers</span></span></li>
<li><span class="koboSpan" id="kobo.15.1">Decoding the original transformer </span><span class="No-Break"><span class="koboSpan" id="kobo.16.1">architecture holistically</span></span></li>
<li><span class="koboSpan" id="kobo.17.1">Uncovering transformer improvements using only </span><span class="No-Break"><span class="koboSpan" id="kobo.18.1">the encoder</span></span></li>
<li><span class="koboSpan" id="kobo.19.1">Uncovering transformer improvements using only </span><span class="No-Break"><span class="koboSpan" id="kobo.20.1">the decoder</span></span></li>
</ul>
<h1 id="_idParaDest-94"><a id="_idTextAnchor094"/><span class="koboSpan" id="kobo.21.1">Exploring neural network transformers</span></h1>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.22.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.23.1">.1</span></em><span class="koboSpan" id="kobo.24.1"> provides an overview </span><a id="_idIndexMarker384"/><span class="koboSpan" id="kobo.25.1">of the impact transformers have had, thanks to the plethora of transformer </span><span class="No-Break"><span class="koboSpan" id="kobo.26.1">model variants.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<span class="koboSpan" id="kobo.27.1"><img alt="Figure 6.1 – Transformers’ different modality and model branches" src="image/B18187_06_001.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.28.1">Figure 6.1 – Transformers’ different modality and model branches</span></p>
<p><span class="koboSpan" id="kobo.29.1">The transformer does not have </span><a id="_idIndexMarker385"/><span class="koboSpan" id="kobo.30.1">inherent inductive bias structurally designed into its architecture. </span><span class="koboSpan" id="kobo.30.2">Inductive bias refers to the pre-assumptions made by a learning algorithm on the data. </span><span class="koboSpan" id="kobo.30.3">This bias can be built into the model architecture or the learning process, and it helps to guide the model toward learning specific patterns or structures in the data. </span><span class="koboSpan" id="kobo.30.4">Traditional models, such as RNNs, incorporate inductive bias through their design, for instance, by assuming that data has a sequential structure and that the order of elements is important. </span><span class="koboSpan" id="kobo.30.5">Another example is CNN models, which are specifically designed for processing grid-like data, such as images, by incorporating inductive bias in the form of local connectivity and translation invariance through the use of convolutional layers and pooling layers. </span><span class="koboSpan" id="kobo.30.6">In this context, the model architecture itself enforces certain constraints on the patterns that can </span><span class="No-Break"><span class="koboSpan" id="kobo.31.1">be learned.</span></span></p>
<p><span class="koboSpan" id="kobo.32.1">Transformers were designed with the idea that we should allow the model to decide how and where to focus, based on all the input data provided. </span><span class="koboSpan" id="kobo.32.2">This is executed technically using an aggregate of multiple mechanisms that each decide where to focus in different ways. </span><span class="koboSpan" id="kobo.32.3">Thus, transformers depend entirely on the information provided by the input data to determine any form of inductive</span><a id="_idIndexMarker386"/><span class="koboSpan" id="kobo.33.1"> bias your data has, if any. </span><span class="koboSpan" id="kobo.33.2">These focusing components are formally called attention layers. </span><span class="koboSpan" id="kobo.33.3">The improvements on top of transformers that form the branches you see in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.34.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.35.1">.1</span></em><span class="koboSpan" id="kobo.36.1"> do not deviate far from the base structure of transformers. </span><span class="koboSpan" id="kobo.36.2">Usually, the improvements to add different modalities are done by data setup variations to adapt the input data structure to the structure of the transformer along with the different prediction output details specific to variations in the target </span><span class="No-Break"><span class="koboSpan" id="kobo.37.1">task application.</span></span></p>
<p><span class="koboSpan" id="kobo.38.1">At the moment, transformers</span><a id="_idIndexMarker387"/><span class="koboSpan" id="kobo.39.1"> are the architecture family with the biggest capacity for learning and identifying highly complex patterns that exist in the real world. </span><span class="koboSpan" id="kobo.39.2">To add to that, for a period of almost a year from mid-2022 to 2023, transformers demonstrated that their informational capacity is bounded only by the hardware resource capacity. </span><span class="koboSpan" id="kobo.39.3">Currently, one of the largest transformer models, scaled up many times from the base</span><a id="_idIndexMarker388"/><span class="koboSpan" id="kobo.40.1"> transformer, with an astronomical </span><em class="italic"><span class="koboSpan" id="kobo.41.1">540 billion</span></em><span class="koboSpan" id="kobo.42.1"> parameters, is a model variant called </span><strong class="bold"><span class="koboSpan" id="kobo.43.1">PaLM</span></strong><span class="koboSpan" id="kobo.44.1"> provided by Google. </span><span class="koboSpan" id="kobo.44.2">Additionally, it is rumored that the GPT-4 multimodal text generation model by OpenAI, which is not available as an open sourced model but as a service, consists of either multiple models or a single model that adds up more than a trillion parameters. </span><span class="koboSpan" id="kobo.44.3">These huge models usually take months of training to allow the model to achieve peak performance, along with the need to have highly performant and </span><span class="No-Break"><span class="koboSpan" id="kobo.45.1">state-of-the-art GPUs.</span></span></p>
<p><span class="koboSpan" id="kobo.46.1">You might wonder why we would want to train such a big model and wonder whether the value it provides</span><a id="_idIndexMarker389"/><span class="koboSpan" id="kobo.47.1"> is worth it. </span><span class="koboSpan" id="kobo.47.2">Let’s evaluate one of the transformer models, called </span><strong class="bold"><span class="koboSpan" id="kobo.48.1">GPT-3</span></strong><span class="koboSpan" id="kobo.49.1">, developed by OpenAI. </span><span class="koboSpan" id="kobo.49.2">GPT-3 is a type of language model; it takes in input text and outputs text-based predictions based on what it thinks is the most appropriate and useful response. </span><span class="koboSpan" id="kobo.49.3">Now, this spans many different tasks in the NLP space that conventionally would have</span><a id="_idIndexMarker390"/><span class="koboSpan" id="kobo.50.1"> been accomplished by individual models for each task, making it a </span><strong class="bold"><span class="koboSpan" id="kobo.51.1">task-agnostic model</span></strong><span class="koboSpan" id="kobo.52.1">. </span><span class="koboSpan" id="kobo.52.2">The tasks that can be accomplished are machine language translation, reading comprehension, reasoning, arithmetic processing, and in general, demonstrating a wide variety of language understanding capabilities, providing results in different formats, depending on the query input text. </span><span class="koboSpan" id="kobo.52.3">For example, end user applications include generating code in any specified languages capable of achieving the described objectives, writing fiction with a specified theme, obtaining any requested information on any publicly known person, summarizing customer feedback, in general or with a focus on certain topics, such as “what’s frustrating the customer,” and adding realism to the conversations of characters in a virtual world. </span><span class="koboSpan" id="kobo.52.4">To date, there are more than 300 applications of GPT-3 being used in a variety of categories and industries, a number that will seem small but will pave the way for more innovation and adoption of the immense capabilities </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">of NNs.</span></span></p>
<p><span class="koboSpan" id="kobo.54.1">Transformers have gone through multiple years of gradual improvements through rigorous research but have not deviated a lot from the base architecture. </span><span class="koboSpan" id="kobo.54.2">This means that understanding the original architecture is key to understanding all the latest and greatest improved transformers such as GPT-3. </span><span class="koboSpan" id="kobo.54.3">With that, let’s dive into the original transformer architecture from 2017 and discuss which components have been changed or adapted in the past five years</span><a id="_idIndexMarker391"/><span class="koboSpan" id="kobo.55.1"> of research that gave birth to new </span><span class="No-Break"><span class="koboSpan" id="kobo.56.1">model architectures.</span></span></p>
<h1 id="_idParaDest-95"><a id="_idTextAnchor095"/><span class="koboSpan" id="kobo.57.1">Decoding the original transformer architecture holistically</span></h1>
<p><span class="koboSpan" id="kobo.58.1">Before we look into the structure</span><a id="_idIndexMarker392"/><span class="koboSpan" id="kobo.59.1"> of the model, let’s talk about the basic intent </span><span class="No-Break"><span class="koboSpan" id="kobo.60.1">of transformers.</span></span></p>
<p><span class="koboSpan" id="kobo.61.1">As we covered in the previous chapter, transformers are also a family of architectures that utilize the concept of encoder and decoder. </span><span class="koboSpan" id="kobo.61.2">The encoder encodes data into what is known as the code and the decoder decodes the code into a data format that looks similar to raw, unprocessed data. </span><span class="koboSpan" id="kobo.61.3">The very first transformer used both the encoder and decoder concepts to build the entire architecture and demonstrated its application in text generation. </span><span class="koboSpan" id="kobo.61.4">The subsequent adaptations and improvements either used only the encoder or only the decoder to achieve different tasks. </span><span class="koboSpan" id="kobo.61.5">In a transformer, however, the encoder’s goal is not to compress the data to achieve a smaller and more compact representation of the data, but instead mainly to serve as a feature extractor. </span><span class="koboSpan" id="kobo.61.6">Additionally, the decoder’s goal for transformers is not to reproduce the </span><span class="No-Break"><span class="koboSpan" id="kobo.62.1">same input.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.63.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.64.1">It is possible to build an actual autoencoder structure with the transformer component instead of using CNN or RNN components, but this will not be covered in </span><span class="No-Break"><span class="koboSpan" id="kobo.65.1">this book.</span></span></p>
<p><span class="koboSpan" id="kobo.66.1">The original transformer fixed the data/weight dimensions in the encoder and decoder to a unified single size so that residual connections could be made, using a dimension </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">of 512.</span></span></p>
<p><span class="koboSpan" id="kobo.68.1">Transformers also utilize</span><a id="_idIndexMarker393"/><span class="koboSpan" id="kobo.69.1"> logical block structures to define their novelties, which allows you to scale their size easily. </span><span class="koboSpan" id="kobo.69.2">The core operation of the transformer is the mechanism to identify which part of the entire input data to focus on for each input data unit in an input data</span><a id="_idIndexMarker394"/><span class="koboSpan" id="kobo.70.1"> sample. </span><span class="koboSpan" id="kobo.70.2">The focusing mechanism is achieved technically by a type of neural network layer called the </span><strong class="bold"><span class="koboSpan" id="kobo.71.1">attention layer</span></strong><span class="koboSpan" id="kobo.72.1">. </span><span class="koboSpan" id="kobo.72.2">There are many types of attention variants that will not be covered in </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">this book.</span></span></p>
<p><span class="koboSpan" id="kobo.74.1">Transformers utilize the simplest</span><a id="_idIndexMarker395"/><span class="koboSpan" id="kobo.75.1"> variant of attention that utilizes the </span><strong class="source-inline"><span class="koboSpan" id="kobo.76.1">softmax</span></strong><span class="koboSpan" id="kobo.77.1"> activation to achieve </span><strong class="bold"><span class="koboSpan" id="kobo.78.1">dot-product</span></strong><span class="koboSpan" id="kobo.79.1">-based attention. </span><span class="koboSpan" id="kobo.79.2">Since </span><strong class="source-inline"><span class="koboSpan" id="kobo.80.1">softmax</span></strong><span class="koboSpan" id="kobo.81.1"> forces values to add up to </span><strong class="source-inline"><span class="koboSpan" id="kobo.82.1">1.0</span></strong><span class="koboSpan" id="kobo.83.1">, there is usually one strongest focal point for each layer. </span><span class="koboSpan" id="kobo.83.2">Take this as a form of gate similar to the gates in RNN, for which you can refer to </span><a href="B18187_04.xhtml#_idTextAnchor068"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.84.1">Chapter 4</span></em></span></a><span class="koboSpan" id="kobo.85.1">, </span><em class="italic"><span class="koboSpan" id="kobo.86.1">Understanding Recurrent Neural Networks</span></em><span class="koboSpan" id="kobo.87.1">, again for the full context on what an RNN is and how it functions. </span><span class="koboSpan" id="kobo.87.2">Transformers use multiple attention layers where</span><a id="_idIndexMarker396"/><span class="koboSpan" id="kobo.88.1"> each input will be focused on in multiple ways, and the aggregated name to simplify referencing and scaling is called the </span><strong class="bold"><span class="koboSpan" id="kobo.89.1">multi-head </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.90.1">attention</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.91.1"> layer.</span></span></p>
<p><span class="koboSpan" id="kobo.92.1">The inputs and outputs of transformers are in the form</span><a id="_idIndexMarker397"/><span class="koboSpan" id="kobo.93.1"> of tokens. </span><span class="koboSpan" id="kobo.93.2">Tokens are a way to refer to the individual units of input data that can be passed into and out of a transformer. </span><span class="koboSpan" id="kobo.93.3">These tokens can be sequential or non-sequential, grouped or non-grouped, although the structure of transformers is not explicitly designed with these assumptions. </span><span class="koboSpan" id="kobo.93.4">Through research and advancements since they were devised, tokens for text can be words or sub-words, tokens for images can be image patches, tokens for audio can be sequential time windows of audio data, for example, one-second windows, and tokens for videos can be single image frames or a group of image frames. </span><span class="koboSpan" id="kobo.93.5">As the model does not have an inherent inductive bias toward any type of data modalities, the positional or sequential identification of the data is encoded into the data explicitly by tokens. </span><span class="koboSpan" id="kobo.93.6">These can range from incremental discrete integer positions (1, 2, 3 ...) to continuous floating-point positions in between discrete integer values (1.2, 1.4556, 2.42325 ...), from absolute positions to relative positions, and finally, single-valued positions for each token or embedding that can be learned. </span><span class="koboSpan" id="kobo.93.7">The original transformer used a mapping function that maps absolute integer positions to relative floating-point positions that consider the data dimensions of the tokens and the intended unified data dimension size, but the state-of-the-art models commonly adopt instead embeddings that can be learned. </span><span class="koboSpan" id="kobo.93.8">Embeddings are a look-up table to encode any discrete categorical data into a higher-dimensional and more complex representation of the original category that can accurately discern categories among each other. </span><span class="koboSpan" id="kobo.93.9">Note that the actual input data token itself is also usually applied with embeddings depending on the input data (image frames or patches usually don’t use embeddings as they are already high in dimensionality, but text tokens are categorical and </span><span class="No-Break"><span class="koboSpan" id="kobo.94.1">use embeddings).</span></span></p>
<p><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.95.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.96.1">.2</span></em><span class="koboSpan" id="kobo.97.1"> shows a high-level overview of </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">the architecture:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<span class="koboSpan" id="kobo.99.1"><img alt="Figure 6.2 – Transformers viewed with input and output visualizations with both the encoder and decoder of the task to translate an English sentence to its Mandarin counterpart" src="image/B18187_06_002.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.100.1">Figure 6.2 – Transformers viewed with input and output visualizations with both the encoder and decoder of the task to translate an English sentence to its Mandarin counterpart</span></p>
<p><span class="koboSpan" id="kobo.101.1">The encoder and decoder have somewhat</span><a id="_idIndexMarker398"/><span class="koboSpan" id="kobo.102.1"> similar structures, with the exception that the decoder has an extra middle </span><strong class="bold"><span class="koboSpan" id="kobo.103.1">multi-head attention</span></strong><span class="koboSpan" id="kobo.104.1"> layer that connects to the output of the encoder and a masked</span><a id="_idIndexMarker399"/><span class="koboSpan" id="kobo.105.1"> version of the multi-head attention block. </span><span class="koboSpan" id="kobo.105.2">The purpose of masking is to prevent the use of irrelevant data, which will be apparent in the next paragraph of the input text. </span><span class="koboSpan" id="kobo.105.3">The encoder’s purpose in the original transformer is to transfer the input data information to the middle multi-head attention layer for multiple blocks of the decoder. </span><span class="koboSpan" id="kobo.105.4">The decoder’s purpose is to finally produce the output</span><a id="_idIndexMarker400"/><span class="koboSpan" id="kobo.106.1"> that gets fed into a token-wise fully connected layer (formally known as the pointwise linear layer) for supervised target predictions. </span><span class="koboSpan" id="kobo.106.2">Predictions can be for regression or classification-based targets in which the latter employs a </span><strong class="source-inline"><span class="koboSpan" id="kobo.107.1">softmax</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.108.1">activation function.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.109.1">Note</span></p>
<p class="callout"><span class="koboSpan" id="kobo.110.1">The decoder also takes in other data, aside from the </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">encoder outputs.</span></span></p>
<p><span class="koboSpan" id="kobo.112.1">To achieve data generation</span><a id="_idIndexMarker401"/><span class="koboSpan" id="kobo.113.1"> without a constraint on the number of outputs, the transformer was designed into an autoregressive model. </span><span class="koboSpan" id="kobo.113.2">An autoregressive model uses the result of the first prediction as input for the second prediction and is subsequently used as the input for the next predictions. </span><span class="koboSpan" id="kobo.113.3">The following figure shows an example of this prediction process with text-based data that aims to accomplish machine translation from English </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">to Mandarin.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer067">
<span class="koboSpan" id="kobo.115.1"><img alt="Figure 6.3 – Autoregressive workflow for transformers" src="image/B18187_06_003.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.116.1">Figure 6.3 – Autoregressive workflow for transformers</span></p>
<p><span class="koboSpan" id="kobo.117.1">Each forward pass of the transformer will predict a single output. </span><span class="koboSpan" id="kobo.117.2">An end-of-sentence token prediction is factored in at the transformer output to signal that the prediction is done. </span><span class="koboSpan" id="kobo.117.3">During the training process, however, it is not necessary to perform this autoregressive loop; instead, masks are generated at random positions to nullify all the future tokens from the target tokens to prevent the model from copying the single target token directly and taking in future token information. </span><span class="koboSpan" id="kobo.117.4">Plainly speaking, the masking mechanism is not used during the prediction or </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">inference stage.</span></span></p>
<p><span class="koboSpan" id="kobo.119.1">The multi-head attention layer</span><a id="_idIndexMarker402"/><span class="koboSpan" id="kobo.120.1"> has a few other key operations besides the actual attention mechanism. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.121.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.122.1">.4</span></em><span class="koboSpan" id="kobo.123.1"> shows the unraveled multi-head attention layer along with the previously mentioned attention mechanism in the scaled dot-product </span><span class="No-Break"><span class="koboSpan" id="kobo.124.1">attention layer.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer068">
<span class="koboSpan" id="kobo.125.1"><img alt="Figure 6.4 – Multi-head attention layer with the scaled dot-product attention structure" src="image/B18187_06_004.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.126.1">Figure 6.4 – Multi-head attention layer with the scaled dot-product attention structure</span></p>
<p><span class="koboSpan" id="kobo.127.1">Take the multiple attention layer as multiple humans contributing to different focusing patterns on the same data. </span><strong class="bold"><span class="koboSpan" id="kobo.128.1">Q</span></strong><span class="koboSpan" id="kobo.129.1">, </span><strong class="bold"><span class="koboSpan" id="kobo.130.1">K</span></strong><span class="koboSpan" id="kobo.131.1">, and </span><strong class="bold"><span class="koboSpan" id="kobo.132.1">V</span></strong><span class="koboSpan" id="kobo.133.1"> represent the query, key, and value respectively where the query represents the individual tokens, and the key and value are just the entire lists of tokens. </span><span class="koboSpan" id="kobo.133.2">The linear layers</span><a id="_idIndexMarker403"/><span class="koboSpan" id="kobo.134.1"> are the only place where the weights live in the multi-head attention layer where each query, key, and value component has its own linear layer. </span><span class="koboSpan" id="kobo.134.2">The scaled dot-product attention layer is where the mask is used to nullify the future tokens during training. </span><span class="koboSpan" id="kobo.134.3">One unique part of this layer is that it is scaled by the square root of the linear layer output dimensions to prevent </span><strong class="source-inline"><span class="koboSpan" id="kobo.135.1">softmax</span></strong><span class="koboSpan" id="kobo.136.1"> from focusing on unimportant areas. </span><span class="koboSpan" id="kobo.136.2">The attention mechanism is meant to find the degree of relevance of each token to the entire set of input tokens, in other words, to find out what tokens interacted with what tokens and figure out how much they depend on each other. </span><span class="koboSpan" id="kobo.136.3">Recall that the model’s dimension has to stay a fixed sized throughout the architecture to ensure values can be added together without additional processing. </span><span class="koboSpan" id="kobo.136.4">Since the output of the multiple attention heads will be concatenated instead of added, the dimensions of the linear layer have to be evenly distributed among the heads. </span><span class="koboSpan" id="kobo.136.5">With 8 heads, for example, and a data/weight dimension of 512, the linear layer dimension for each query, key, and value for each head would then be 512/8=64 neurons. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.137.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.138.1">.5</span></em><span class="koboSpan" id="kobo.139.1"> shows a simple example of the outputs and their shapes in the workflow with the number of fixed data/weight dimensions per token being 3, using the same input text data as </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.140.1">Figure 6</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.141.1">.4</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.142.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer069">
<span class="koboSpan" id="kobo.143.1"><img alt="Figure 6.5 – Part operation of the multi-head attention before concatenating using the query of the word “I” in the context of “I love you” with a linear layer with an output dimension of 3" src="image/B18187_06_005.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.144.1">Figure 6.5 – Part operation of the multi-head attention before concatenating using the query of the word “I” in the context of “I love you” with a linear layer with an output dimension of 3</span></p>
<p><span class="koboSpan" id="kobo.145.1">The same operation will be done</span><a id="_idIndexMarker404"/><span class="koboSpan" id="kobo.146.1"> for the other two query words, “love” and “you.” </span><span class="koboSpan" id="kobo.146.2">An operation from a single head focuses on the input data in one way while the other heads focus on the input data in a different way. </span><span class="koboSpan" id="kobo.146.3">In the encoder-decoder layer, the query comes from the decoder layer while the keys and value come from the encoder layer. </span><span class="koboSpan" id="kobo.146.4">This allows the decoder to choose where to focus in the given input text to produce the next output. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.147.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.148.1">.6</span></em><span class="koboSpan" id="kobo.149.1"> shows a good way to think about the output of multi-head attention </span><a id="_idIndexMarker405"/><span class="koboSpan" id="kobo.150.1">with a four-headed multi-head attention focusing on different parts of </span><span class="No-Break"><span class="koboSpan" id="kobo.151.1">the text.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer070">
<span class="koboSpan" id="kobo.152.1"><img alt="Figure 6.6 – Multi-head attention example output with four heads" src="image/B18187_06_006.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.153.1">Figure 6.6 – Multi-head attention example output with four heads</span></p>
<p><span class="koboSpan" id="kobo.154.1">The regularization mechanisms part of the transformer has the same purpose as described in other previous architectures such as CNNs, RNNs, and MLPs, and won’t be discussed again here. </span><span class="koboSpan" id="kobo.154.2">This sums up all the components of the transformer. </span><span class="koboSpan" id="kobo.154.3">This base architecture allows us to achieve better data generation results compared to older architectures such as sequence-to-sequence or autoencoders. </span><span class="koboSpan" id="kobo.154.4">Although the original architecture focused on applications for text data, the concept can and has been adapted to handle other types of data modalities such as image, video, and audio data. </span><span class="koboSpan" id="kobo.154.5">Recall that the improvements and adaptations that came after the original model used only the encoder or only</span><a id="_idIndexMarker406"/><span class="koboSpan" id="kobo.155.1"> the decoder. </span><span class="koboSpan" id="kobo.155.2">In the next two topics, we will dive into the two different </span><span class="No-Break"><span class="koboSpan" id="kobo.156.1">concepts separately.</span></span></p>
<h1 id="_idParaDest-96"><a id="_idTextAnchor096"/><span class="koboSpan" id="kobo.157.1">Uncovering transformer improvements using only the encoder</span></h1>
<p><span class="koboSpan" id="kobo.158.1">The first type of architectural advancements</span><a id="_idIndexMarker407"/><span class="koboSpan" id="kobo.159.1"> based on transformers we will discuss are transformers that utilize</span><a id="_idIndexMarker408"/><span class="koboSpan" id="kobo.160.1"> only the encoder part of the original transformer using the same multi-head attention layer. </span><span class="koboSpan" id="kobo.160.2">The encoder-only line of transformers is adopted generally because there is no masked multi-head attention layer since the next token prediction training setup is not used. </span><span class="koboSpan" id="kobo.160.3">In this line of improvements, training goals and setups vary across different data modalities and vary slightly for sequential improvements under the same data modality. </span><span class="koboSpan" id="kobo.160.4">However, one concept that stays pretty much constant across different data modalities is the fact that a semi-supervised learning method is used. </span><span class="koboSpan" id="kobo.160.5">In the case of transformers, this means that a form of unsupervised learning is executed first and then the straightforward supervised learning method is executed next. </span><span class="koboSpan" id="kobo.160.6">Unsupervised learning offers transformers a way to initialize</span><a id="_idIndexMarker409"/><span class="koboSpan" id="kobo.161.1"> their state based on a wider understanding of the nature of the data. </span><span class="koboSpan" id="kobo.161.2">This process is also known as </span><strong class="bold"><span class="koboSpan" id="kobo.162.1">pre-training</span></strong><span class="koboSpan" id="kobo.163.1"> a model, which is just one form of unsupervised learning. </span><span class="koboSpan" id="kobo.163.2">Unsupervised learning will be discussed more extensively in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.164.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.165.1">, </span><em class="italic"><span class="koboSpan" id="kobo.166.1">Exploring Unsupervised </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.167.1">Deep Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.168.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.169.1">Some improvements are specific to the nature of the data modality, which mostly consists of text-specific improvements and involves a change in the type of task to optimize. </span><span class="koboSpan" id="kobo.169.2">Other improvements are general transformer architectural improvements that can generalize to different data modalities. </span><span class="koboSpan" id="kobo.169.3">We will start by focusing on the general architectural improvements before diving into the improvements specifically crafted for text to date, which might or might not be adaptable to other data modalities. </span><span class="koboSpan" id="kobo.169.4">To start off, we will go through the base architecture of an encoder-only structure </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">called BERT.</span></span></p>
<p><span class="koboSpan" id="kobo.171.1">The </span><strong class="bold"><span class="koboSpan" id="kobo.172.1">Bidirectional Encoder Representations from Transformers</span></strong><span class="koboSpan" id="kobo.173.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.174.1">BERT</span></strong><span class="koboSpan" id="kobo.175.1">) architecture, introduced in 2018, is simply a decoder-only transformer</span><a id="_idIndexMarker410"/><span class="koboSpan" id="kobo.176.1"> that utilizes the same multi-head attention layer and introduces a pre-training task that can be generalized to other data modalities, but was built for text data. </span><span class="koboSpan" id="kobo.176.2">This task was called </span><strong class="bold"><span class="koboSpan" id="kobo.177.1">masked language modeling</span></strong><span class="koboSpan" id="kobo.178.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.179.1">MLM</span></strong><span class="koboSpan" id="kobo.180.1">) where the objective is to predict the randomly masked input tokens and only learn</span><a id="_idIndexMarker411"/><span class="koboSpan" id="kobo.181.1"> from these token outputs. </span><span class="koboSpan" id="kobo.181.2">Note that the encoder of the transformer outputs token-based outputs and thus token classification can be achieved by simply adding a token-wise </span><strong class="source-inline"><span class="koboSpan" id="kobo.182.1">softmax</span></strong><span class="koboSpan" id="kobo.183.1"> layer. </span><span class="koboSpan" id="kobo.183.2">This specific task was proven to be highly effective as a pretraining method that allows downstream subsequent supervised tasks to achieve better performance. </span><span class="koboSpan" id="kobo.183.3">Since the tokens are masked out in different positions from the original sequence, the architecture is said to be bidirectional, as information from the future tokens can be attended to predict tokens in the past in addition to the past tokens. </span><span class="koboSpan" id="kobo.183.4">15% of tokens were masked, for example, and from that percentage, 10% were randomly replaced with random tokens to act as noise, making the model similar</span><a id="_idIndexMarker412"/><span class="koboSpan" id="kobo.184.1"> to a denoising autoencoder</span><a id="_idIndexMarker413"/><span class="koboSpan" id="kobo.185.1"> and allowing it to be robust to noise. </span><span class="koboSpan" id="kobo.185.2">Standard autoregressive structures are naturally uni- and forward-directional and can’t take advantage of future tokens. </span><span class="koboSpan" id="kobo.185.3">BERT also introduced ways to encode multiple sentences in a single input representation. </span><span class="koboSpan" id="kobo.185.4">This is achieved by adding a special separator token that has its own learned embeddings to signify a separation between text along with an extra segment embedding</span><a id="_idIndexMarker414"/><span class="koboSpan" id="kobo.186.1"> that signals to the transformer which sentence number the token is part of. </span><span class="koboSpan" id="kobo.186.2">This allows another task called </span><strong class="bold"><span class="koboSpan" id="kobo.187.1">next sentence prediction</span></strong><span class="koboSpan" id="kobo.188.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.189.1">NSP</span></strong><span class="koboSpan" id="kobo.190.1">) to be optimized along with the MLM objectives for pre-training purposes. </span><span class="koboSpan" id="kobo.190.2">This is achieved by reserving the first token to be a special class token, shown as </span><strong class="bold"><span class="koboSpan" id="kobo.191.1">CLS</span></strong><span class="koboSpan" id="kobo.192.1"> in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.193.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.194.1">.7</span></em><span class="koboSpan" id="kobo.195.1">, and the first output token to be used as the prediction for the next sentence, where positive examples are two consecutive sentences from a single document and negative examples are created by pairing sentences from different documents. </span><span class="koboSpan" id="kobo.195.2">The pre-training mechanism is a way to allow the model to learn generalizable patterns of the nature of the data, and these two tasks proved to improve the performance of downstream supervised learning tasks. </span><span class="koboSpan" id="kobo.195.3">Downstream supervised learning simply adds a global fully connected layer across all the tokens along with target-specific layers such as </span><strong class="source-inline"><span class="koboSpan" id="kobo.196.1">softmax</span></strong><span class="koboSpan" id="kobo.197.1">. </span><span class="koboSpan" id="kobo.197.2">The three embeddings (positional embeddings, segment embeddings, and token embeddings) are added together before passing the result into the transformer. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.198.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.199.1">.7</span></em><span class="koboSpan" id="kobo.200.1"> depicts </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">this</span></span><span class="No-Break"><a id="_idIndexMarker415"/></span><span class="No-Break"><span class="koboSpan" id="kobo.202.1"> architecture.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer071">
<span class="koboSpan" id="kobo.203.1"><img alt="Figure 6.7 – BERT architecture" src="image/B18187_06_007.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.204.1">Figure 6.7 – BERT architecture</span></p>
<p><span class="koboSpan" id="kobo.205.1">The architecture focused on ways to represent extra input and methods to increase model understanding of the nature of the data modality through multiple-task learning and exceeded prior performance on multiple downstream </span><span class="No-Break"><span class="koboSpan" id="kobo.206.1">text-based tasks.</span></span></p>
<p><span class="koboSpan" id="kobo.207.1">Now that we have introduced the base encoder-only model, we are ready to explore the different categories of advancements specifically for an encoder-only transformer. </span><span class="koboSpan" id="kobo.207.2">Improvements will be explored in terms of advancement categories instead of by model this time, as the different models combine many advancements, which makes it hard to compare actual advancements. </span><span class="koboSpan" id="kobo.207.3">The three types of advancements are </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">as follows:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.209.1">Improved data-modality-based tasks used </span><span class="No-Break"><span class="koboSpan" id="kobo.210.1">for pre-training</span></span></li>
<li><span class="koboSpan" id="kobo.211.1">Architectural improvements in terms of compactness, speed, </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">and efficiency</span></span></li>
<li><span class="koboSpan" id="kobo.213.1">Core/functional </span><span class="No-Break"><span class="koboSpan" id="kobo.214.1">architectural improvements</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.215.1">We will skip the details</span><a id="_idIndexMarker416"/><span class="koboSpan" id="kobo.216.1"> of data-based improvements</span><a id="_idIndexMarker417"/><span class="koboSpan" id="kobo.217.1"> such as using multilingual data to build a multilingual BERT or the fact that sub-word-based tokens are used to reduce vocabulary and increase vocabulary reuse </span><span class="No-Break"><span class="koboSpan" id="kobo.218.1">between words.</span></span></p>
<h2 id="_idParaDest-97"><a id="_idTextAnchor097"/><span class="koboSpan" id="kobo.219.1">Improving the encoder only pre-training tasks and objectives</span></h2>
<p><span class="koboSpan" id="kobo.220.1">Recall that MLM and NSP</span><a id="_idIndexMarker418"/><span class="koboSpan" id="kobo.221.1"> are used in the base BERT model and that MLM was mentioned to be a robust task for the understanding of text language but can be easily adapted to tokens from other data modalities. </span><span class="koboSpan" id="kobo.221.2">NSP, however, has been proven to be unstable and doesn’t concretely help models improve performance</span><a id="_idIndexMarker419"/><span class="koboSpan" id="kobo.222.1"> in every case. </span><span class="koboSpan" id="kobo.222.2">One improvement in this direction is to use </span><strong class="bold"><span class="koboSpan" id="kobo.223.1">sentence order prediction</span></strong><span class="koboSpan" id="kobo.224.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.225.1">SOP</span></strong><span class="koboSpan" id="kobo.226.1">). </span><span class="koboSpan" id="kobo.226.2">SOP demonstrated higher consistency in improving the downstream supervised task performance compared to simply utilizing the inverted sequence of positive consecutive sentences from the same document as a negative sentence. </span><span class="koboSpan" id="kobo.226.3">Conceptually, it learns the cohesion between sentences instead of trying to simultaneously predict whether two sentences are from</span><a id="_idIndexMarker420"/><span class="koboSpan" id="kobo.227.1"> the same topic or not. </span><span class="koboSpan" id="kobo.227.2">This method was introduced in the </span><strong class="bold"><span class="koboSpan" id="kobo.228.1">ALBERT</span></strong><span class="koboSpan" id="kobo.229.1"> model in 2020, which mainly focused on </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">transformer efficiency.</span></span></p>
<p><span class="koboSpan" id="kobo.231.1">Another notable</span><a id="_idIndexMarker421"/><span class="koboSpan" id="kobo.232.1"> improvement</span><a id="_idIndexMarker422"/><span class="koboSpan" id="kobo.233.1"> is called </span><strong class="bold"><span class="koboSpan" id="kobo.234.1">replaced token detection</span></strong><span class="koboSpan" id="kobo.235.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.236.1">RTD</span></strong><span class="koboSpan" id="kobo.237.1">). </span><span class="koboSpan" id="kobo.237.2">RTD, from the </span><strong class="bold"><span class="koboSpan" id="kobo.238.1">ELECTRA</span></strong><span class="koboSpan" id="kobo.239.1"> model, predicts whether a token is replaced by a random token or not and requires that all token positions are learned from, compared to the MLM process of only learning from the masked token positions. </span><span class="koboSpan" id="kobo.239.2">This was introduced as an improvement on the </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">MLM</span></span><span class="No-Break"><a id="_idIndexMarker423"/></span><span class="No-Break"><span class="koboSpan" id="kobo.241.1"> objective.</span></span></p>
<p><span class="koboSpan" id="kobo.242.1">Next, let’s dive into architectural improvements in terms of compactness, speed, </span><span class="No-Break"><span class="koboSpan" id="kobo.243.1">and efficiency.</span></span></p>
<h2 id="_idParaDest-98"><a id="_idTextAnchor098"/><span class="koboSpan" id="kobo.244.1">Improving the encoder-only transformer’s architectural compactness and efficiency</span></h2>
<p><span class="koboSpan" id="kobo.245.1">Transformers have proved themselves</span><a id="_idIndexMarker424"/><span class="koboSpan" id="kobo.246.1"> to hold an immense structural capacity to take in information that grows along with the model. </span><span class="koboSpan" id="kobo.246.2">Some research on transformers has been about increasing and scaling the transformer model to the level of hundreds of billions of parameters in both decoder-only and encoder-only models. </span><span class="koboSpan" id="kobo.246.3">The results achieved using these huge models are astounding, but they are unsuitable for practical use due to the need to have state-of-the-art GPUs and machines that are not readily available or affordable for most people in the world. </span><span class="koboSpan" id="kobo.246.4">This is where the line of architectural compactness and efficiency improvements can help to level the field. </span><span class="koboSpan" id="kobo.246.5">Since we are mostly limited by the hardware resource that we can use, if the model can be efficiently reduced in size architecturally while maintaining the same performance, this would allow a more performant model when you scale it up to the limits of your </span><span class="No-Break"><span class="koboSpan" id="kobo.247.1">hardware resource.</span></span></p>
<p><span class="koboSpan" id="kobo.248.1">One of the notable improvements in this line comes again from the ALBERT model, a lite BERT model. </span><span class="koboSpan" id="kobo.248.2">A key improvement made here was to factorize the embeddings layer in the same way as convolutional layers were factorized to make depthwise convolutions. </span><span class="koboSpan" id="kobo.248.3">A smaller embedding dimension is used with a fully connected layer that maps the small dimensional embedding to the same desired dimensions. </span><span class="koboSpan" id="kobo.248.4">As embeddings are essential weights and contribute to the number of parameters, factorizing the embeddings layer allows for a transformer model with fewer parameters. </span><span class="koboSpan" id="kobo.248.5">This allows BERT to be more performant at a lower number of parameters. </span><span class="koboSpan" id="kobo.248.6">Note that some improvements add model parallelization and use clever memory management, but these will not be covered extensively here. </span><span class="koboSpan" id="kobo.248.7">However, since there are some methods that speed up the model in general for both inference</span><a id="_idIndexMarker425"/><span class="koboSpan" id="kobo.249.1"> and training stages, they will be discussed and covered in </span><a href="B18187_15.xhtml#_idTextAnchor217"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.250.1">Chapter 15</span></em></span></a><span class="koboSpan" id="kobo.251.1">, </span><em class="italic"><span class="koboSpan" id="kobo.252.1">Deploying Deep Learning Models in Production</span></em><span class="koboSpan" id="kobo.253.1">, which is all </span><span class="No-Break"><span class="koboSpan" id="kobo.254.1">about deployment.</span></span></p>
<h2 id="_idParaDest-99"><a id="_idTextAnchor099"/><span class="koboSpan" id="kobo.255.1">Improving the encoder-only transformers’ core functional architecture</span></h2>
<p><span class="koboSpan" id="kobo.256.1">The most notable improvement made in this line is from the model called </span><strong class="bold"><span class="koboSpan" id="kobo.257.1">Decoding-enhanced BERT with Disentangled Attention</span></strong> <a id="_idTextAnchor100"/><span class="koboSpan" id="kobo.258.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.259.1">DeBERTa</span></strong><span class="koboSpan" id="kobo.260.1">), introduced in 2021, which is considered to be the current</span><a id="_idIndexMarker426"/><span class="koboSpan" id="kobo.261.1"> architectural SOTA. </span><span class="koboSpan" id="kobo.261.2">The main idea here is that two separate positional</span><a id="_idIndexMarker427"/><span class="koboSpan" id="kobo.262.1"> encoded embeddings are used, which are relative positions and absolute positions. </span><span class="koboSpan" id="kobo.262.2">These positional embeddings, however, are not added along with the input token embeddings but instead are treated as separate queries and keys to be fed into the layers of the transformer, with their own fully connected layers. </span><span class="koboSpan" id="kobo.262.3">This allows three explicit attention maps to be obtained, specifically, content-to-position attention, content-to-content attention, and position-to-content attention. </span><span class="koboSpan" id="kobo.262.4">The different attention maps are then summed together. </span><span class="koboSpan" id="kobo.262.5">The relative positions are specifically used by each intermediate multi-head attention layer’s internal scaled dot-product attention layer by acting either as the key or query, depending on whether it is being attended to by the content or attending to </span><span class="No-Break"><span class="koboSpan" id="kobo.263.1">the content.</span></span></p>
<p><span class="koboSpan" id="kobo.264.1">This allows the raw relative positional information to be explicitly considered in every attention layer instead of possibly being forgotten after going through many layers. </span><span class="koboSpan" id="kobo.264.2">We are essentially applying a form of skip connections for relative position data. </span><span class="koboSpan" id="kobo.264.3">Relative encoding allows the model to learn more generalizable representations by conceptually allowing pattern identifiers to be reused at each relative position. </span><span class="koboSpan" id="kobo.264.4">This solution is similar to how the same convolutional filters are used at different partitions of a single-image data instead of using one fully connected layer on an entire image, which doesn’t allow the reuse of pattern identifiers across the entire input space. </span><span class="koboSpan" id="kobo.264.5">As for the usage of absolute positions, it was only added due to the nature of text data where the absolute position is needed to discern the absolute importance of different things, such as police holding more power than most people and trucks being bigger than motorcycles. </span><span class="koboSpan" id="kobo.264.6">Absolute positions, however, are only added to the last few layers once, using a mechanism called </span><strong class="bold"><span class="koboSpan" id="kobo.265.1">enhanced mask decoder</span></strong><span class="koboSpan" id="kobo.266.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.267.1">EMD</span></strong><span class="koboSpan" id="kobo.268.1">). </span><span class="koboSpan" id="kobo.268.2">EMD applies absolute positional embeddings as the query</span><a id="_idIndexMarker428"/><span class="koboSpan" id="kobo.269.1"> component of the scaled dot-product attention. </span><span class="koboSpan" id="kobo.269.2">There are no concrete reasons why it is only applied in the final few layers</span><a id="_idIndexMarker429"/><span class="koboSpan" id="kobo.270.1"> and it can probably be extended to be applied in </span><span class="No-Break"><span class="koboSpan" id="kobo.271.1">every layer.</span></span></p>
<p class="callout-heading"><span class="koboSpan" id="kobo.272.1">Bucketing</span></p>
<p class="callout"><span class="koboSpan" id="kobo.273.1">A bucketing mechanism</span><a id="_idIndexMarker430"/><span class="koboSpan" id="kobo.274.1"> to group multiple positions into a single number is used to reduce the number of embeddings and thus the number of parameters. </span><span class="koboSpan" id="kobo.274.2">Consider this as grouped position information that again increases the compactness of </span><span class="No-Break"><span class="koboSpan" id="kobo.275.1">the model.</span></span></p>
<h2 id="_idParaDest-100"><a id="_idTextAnchor101"/><span class="koboSpan" id="kobo.276.1">Uncovering encoder-only transformers’ adaptations to other data modalities</span></h2>
<p><span class="koboSpan" id="kobo.277.1">Transformers are capable</span><a id="_idIndexMarker431"/><span class="koboSpan" id="kobo.278.1"> of handling other data modalities. </span><span class="koboSpan" id="kobo.278.2">First, the input data just needs to be structured properly in a sequence format. </span><span class="koboSpan" id="kobo.278.3">After that, since pre-training is at the core of a transformer, suitable task objectives need to be crafted for the encoder-only transformers to achieve a pre-trained state that captures an understanding of the specific data modality. </span><span class="koboSpan" id="kobo.278.4">Even though the introduction to transformers using text is focused on an unsupervised pre-training method, it really can be either supervised, unsupervised, or semi-supervised, where the key is to set the weights of the model to be in a state that the subsequent supervised downstream task </span><span class="No-Break"><span class="koboSpan" id="kobo.279.1">can leverage.</span></span></p>
<p><span class="koboSpan" id="kobo.280.1">For images, </span><strong class="bold"><span class="koboSpan" id="kobo.281.1">Vision Transformer</span></strong><span class="koboSpan" id="kobo.282.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.283.1">ViT</span></strong><span class="koboSpan" id="kobo.284.1">), introduced in 2021, forms the base of an image-based transformer</span><a id="_idIndexMarker432"/><span class="koboSpan" id="kobo.285.1"> and has proved that transformers can achieve competitive performance for image-based tasks when compared to convolutional-based models. </span><span class="koboSpan" id="kobo.285.2">One of the most problematic issues for transformers to handle images is that images are big in size and attending to the entire large input space bloats up the model size of transformers very quickly. </span><span class="koboSpan" id="kobo.285.3">ViT solves this by splitting images into systematic patches where each patch would subsequently be fed into a single fully connected layer that reduces the dimension of each image patch substantially. </span><span class="koboSpan" id="kobo.285.4">The patch data with reduced dimensions will be the token embeddings for the transformer. </span><span class="koboSpan" id="kobo.285.5">The patch conversion is an essential process to represent an image input in a format that a transformer can process. </span><span class="koboSpan" id="kobo.285.6">Additionally, this patch-based mechanism introduces some form of image domain inductive bias, but the rest of the architecture remains practically free of inductive bias for image data. </span><span class="koboSpan" id="kobo.285.7">As transformers are almost always coupled with a pre-training method for performance improvement reasons, ViT adopted a similar style of pre-training as the CNN model families, using supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.286.1">classification pre-training.</span></span></p>
<p><span class="koboSpan" id="kobo.287.1">For audio, </span><strong class="bold"><span class="koboSpan" id="kobo.288.1">wav2vec 2.0</span></strong><span class="koboSpan" id="kobo.289.1">, created in 2020, utilized similar concepts</span><a id="_idIndexMarker433"/><span class="koboSpan" id="kobo.290.1"> to its predecessor, </span><strong class="bold"><span class="koboSpan" id="kobo.291.1">wav2vec</span></strong><span class="koboSpan" id="kobo.292.1">, but instead utilized a decoder-only transformer</span><a id="_idIndexMarker434"/><span class="koboSpan" id="kobo.293.1"> as part of the architecture. </span><span class="koboSpan" id="kobo.293.2">A brief overview is that wav2vec 2.0 first encodes audio of a specific time window length using 1D convolutions into a lower-dimensional space as a feature vector. </span><span class="koboSpan" id="kobo.293.3">The encoder feature vectors for each window are then fed into the decoder transformer and act as tokens. </span><span class="koboSpan" id="kobo.293.4">The unsupervised pre-training method is applied here, similarly to MLM from BERT, where a certain portion of the input encoded feature vectors is masked, and the task is to predict the masked encoded feature vector. </span><span class="koboSpan" id="kobo.293.5">After pre-training, the model is then fine-tuned on downstream tasks, using a linear layer right after the transformer outputs for tasks such as speech recognition. </span><span class="koboSpan" id="kobo.293.6">wav2vec 2.0 achieved a new SOTA on a downstream speech recognition dataset and proved that audio works extremely well </span><span class="No-Break"><span class="koboSpan" id="kobo.294.1">with transformers.</span></span></p>
<p><strong class="bold"><span class="koboSpan" id="kobo.295.1">VideoBERT</span></strong><span class="koboSpan" id="kobo.296.1">, created in 2019 by Google, not only demonstrates </span><a id="_idIndexMarker435"/><span class="koboSpan" id="kobo.297.1">methods to model video data for transformers but also the capability of the transformer model to learn joint representations in a multimodal way using both video and text. </span><span class="koboSpan" id="kobo.297.2">Note that a video by itself is also inherently multimodal since video data contains multiple image frames as well as an audio data component. </span><span class="koboSpan" id="kobo.297.3">In VideoBERT, a cooking video sentence pair dataset with clear annotations for the starting and ending time stamps of each cooking action is used to pre-train the model. </span><span class="koboSpan" id="kobo.297.4">The model takes in the sequence of image frames by encoding a preset number of frames into a feature vector using a pre-trained 3D convolutional model and treats the feature vector as a token. </span><span class="koboSpan" id="kobo.297.5">These encoded video tokens are then paired with the text data and use a similar objective to MLM to predict masked tokens of videos and text tokens. </span><span class="koboSpan" id="kobo.297.6">The model then can either be used as a featurizer for video and text-related data or similarly be fine-tuned with an additional linear layer on the outputs of the </span><span class="No-Break"><span class="koboSpan" id="kobo.298.1">transformer model.</span></span></p>
<p><span class="koboSpan" id="kobo.299.1">The preceding summaries of different approaches to different data modalities are meant to serve as a reminder that transformers can be applied to more than just text data and not as an introduction to the current state of the art for these modalities. </span><span class="koboSpan" id="kobo.299.2">A key takeaway here is that the different data modalities need to be arranged into a sequence of token formats with reduced dimensions and suitable task objectives need to be crafted for the encoder-only transformers to achieve a pre-trained state that captures an understanding of the main nature of the data modality. </span><span class="koboSpan" id="kobo.299.3">Moreover, the same pre-training method from BERT can be easily adapted to other data modalities. </span><span class="koboSpan" id="kobo.299.4">So instead of naming it “masked language</span><a id="_idIndexMarker436"/><span class="koboSpan" id="kobo.300.1"> modeling,” maybe it would be better to name it “masked token modeling.” </span><span class="koboSpan" id="kobo.300.2">Next, let’s uncover the transformer’s improvements using only </span><span class="No-Break"><span class="koboSpan" id="kobo.301.1">the decoder.</span></span></p>
<h1 id="_idParaDest-101"><a id="_idTextAnchor102"/><span class="koboSpan" id="kobo.302.1">Uncovering transformer improvements using only the decoder</span></h1>
<p><span class="koboSpan" id="kobo.303.1">Recall that the decoder block</span><a id="_idIndexMarker437"/><span class="koboSpan" id="kobo.304.1"> of the transformer focuses on an autoregressive structure. </span><span class="koboSpan" id="kobo.304.2">For the decoder-only transformer</span><a id="_idIndexMarker438"/><span class="koboSpan" id="kobo.305.1"> line of models, the task of predicting tokens autoregressively remains the same. </span><span class="koboSpan" id="kobo.305.2">With the removal of the encoder, the architecture has to adapt its input to accept more than one sentence, similar to what BERT does. </span><span class="koboSpan" id="kobo.305.3">Starting, ending, and separator tokens are used to encode input data sequentially. </span><span class="koboSpan" id="kobo.305.4">Masking is still performed to prevent the model from depending on the current token to predict future tokens from the input data during predictions, which is similar to the original transformer along with </span><span class="No-Break"><span class="koboSpan" id="kobo.306.1">positional embeddings.</span></span></p>
<h2 id="_idParaDest-102"><a id="_idTextAnchor103"/><span class="koboSpan" id="kobo.307.1">Diving into the GPT model family</span></h2>
<p><span class="koboSpan" id="kobo.308.1">All these architectural</span><a id="_idIndexMarker439"/><span class="koboSpan" id="kobo.309.1"> concepts were introduced by the GPT model in 2018, which is short for </span><strong class="bold"><span class="koboSpan" id="kobo.310.1">generative pre-training</span></strong><span class="koboSpan" id="kobo.311.1">. </span><span class="koboSpan" id="kobo.311.2">As the name suggests, GPT also adopts unsupervised</span><a id="_idIndexMarker440"/><span class="koboSpan" id="kobo.312.1"> pre-training as the initial stage and subsequently moves into the supervised fine-tuning stage for any downstream task. </span><span class="koboSpan" id="kobo.312.2">GPT is focused on text data, but the same concepts can be applied to other data modalities. </span><span class="koboSpan" id="kobo.312.3">GPT uses the basic language modeling task for pre-training to predict the next token given the prior tokens as context with a fixed window to limit the number of context tokens taken as input. </span><span class="koboSpan" id="kobo.312.4">After the weights are pre-trained, it is subsequently fine-tuned to the objectives defined by any of the supervised task types. </span><span class="koboSpan" id="kobo.312.5">GPT-1 utilized the same language modeling task objective as an auxiliary side objective along with the main supervised task as an attempt to boost the performance on downstream tasks and showed that it boosts performance only on larger datasets. </span><span class="koboSpan" id="kobo.312.6">Furthermore, the model showcases first-hand the generalizability of using language modeling as a pre-training task to other tasks without</span><a id="_idIndexMarker441"/><span class="koboSpan" id="kobo.313.1"> actually fine-tuning it and still achieving good scores. </span><span class="koboSpan" id="kobo.313.2">This line of behavior is termed </span><strong class="bold"><span class="koboSpan" id="kobo.314.1">zero-shot</span></strong><span class="koboSpan" id="kobo.315.1"> and will be discussed more extensively in </span><a href="B18187_09.xhtml#_idTextAnchor149"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.316.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.317.1">, </span><em class="italic"><span class="koboSpan" id="kobo.318.1">Exploring Unsupervised Deep Learning</span></em><span class="koboSpan" id="kobo.319.1">. </span><span class="koboSpan" id="kobo.319.2">The model was pre-trained on 7,000 unpublished books and had 117 million parameters with 12 repeated </span><span class="No-Break"><span class="koboSpan" id="kobo.320.1">decoder blocks.</span></span></p>
<p><span class="koboSpan" id="kobo.321.1">It’s important to clarify that the GPT model’s ability to perform without fine-tuning (few-shot or zero-shot learning) doesn’t negate the benefits that fine-tuning can provide in domain-specific tasks. </span><span class="koboSpan" id="kobo.321.2">While the GPT line of models has shown impressive capabilities in various tasks, fine-tuning may still be advantageous for specific applications, leading to improved performance and better adaptation to the unique requirements of a </span><span class="No-Break"><span class="koboSpan" id="kobo.322.1">given task.</span></span></p>
<p><span class="koboSpan" id="kobo.323.1">Proceeding to the next advancement, GPT-2 had no concrete architectural changes as the main idea but emphasized the capability of zero-shot learning with the concept of task conditioning along with scaling the GPT-1 model by 10 times in the number of layers, embedding dimensions, the context window size, and vocabulary size. </span><span class="koboSpan" id="kobo.323.2">Task conditioning is the idea that you tell the model what kind of outputs to generate instead of having a fixed, single, known prediction output that is determined by the type of task you train it for. </span><span class="koboSpan" id="kobo.323.3">To allow true flexibility similar to humans, they didn’t add any new architecture to accommodate the task conditioning idea but instead decided that the textual input for the transformer could be directly used as the task conditioning method. </span><span class="koboSpan" id="kobo.323.4">For example, free-form text model input such as “I love you = </span><span class="koboSpan" id="kobo.324.1"><img alt="" role="presentation" src="image/01.png"/></span><span class="koboSpan" id="kobo.325.1">, I love deep learning=” naturally directs the transformer to perform a translation from English to Mandarin. </span><span class="koboSpan" id="kobo.325.2">This can also be done differently with an example that specifies both the task and the context such as “(translate to mandarin), I love deep learning =”. </span><span class="koboSpan" id="kobo.325.3">GPT-2 achieves this by pre-training with the language modeling objective on a wide variety of textual domains such as news articles, fiction books, Wikipedia, and blogs, essentially a much larger dataset compared to the training dataset in GPT-1. </span><span class="koboSpan" id="kobo.325.4">This work exemplifies the huge potential of true AI generalization in general even though at the moment only text and image data</span><a id="_idIndexMarker442"/><span class="koboSpan" id="kobo.326.1"> have been utilized. </span><span class="koboSpan" id="kobo.326.2">For image data, this is done in a model called </span><strong class="bold"><span class="koboSpan" id="kobo.327.1">DALL-E</span></strong><span class="koboSpan" id="kobo.328.1">, which is capable of generating images according to the provided </span><span class="No-Break"><span class="koboSpan" id="kobo.329.1">input text.</span></span></p>
<p><span class="koboSpan" id="kobo.330.1">Finally, the GPT-3 model</span><a id="_idIndexMarker443"/><span class="koboSpan" id="kobo.331.1"> was introduced in 2020. </span><span class="koboSpan" id="kobo.331.2">It is similar to the previous advancement, with the main changes being increasing the size of the model in all components and characterizing different types of text examples with different ways of specifying the task conditions. </span><span class="koboSpan" id="kobo.331.3">We won’t go into this too much here. </span><span class="koboSpan" id="kobo.331.4">Although GPT-2 performed well on zero-shot settings without fine-tuning for the downstream tasks, only by fine-tuning could the model exceed previous benchmarks on multiple text language datasets. </span><span class="koboSpan" id="kobo.331.5">The sheer size of GPT-3 completely removed the need for fine-tuning by surpassing previous benchmarks without any form of fine-tuning. </span><span class="koboSpan" id="kobo.331.6">This model, however, requires specialized machines to train and is not readily accessible to individuals or small organizations. </span><span class="koboSpan" id="kobo.331.7">It is currently part of OpenAI’s API offering. </span><span class="koboSpan" id="kobo.331.8">GPT-3 demonstrates a wide range of applicability to many different tasks without any fine-tuning, where some of the most notable uses are code generation for any programming language, storytelling, and producing amazingly human-seeming chatbots. </span><span class="koboSpan" id="kobo.331.9">The three improvements pushed the performance limitations on many datasets with language-based tasks to higher levels, and serve as an example of the generalizability component of machine learning, as well as the immense learning potential of transformers, limited only by the amount of hardware </span><a id="_idIndexMarker444"/><span class="koboSpan" id="kobo.332.1">resources </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">you have.</span></span></p>
<p><span class="koboSpan" id="kobo.334.1">Next, we will discuss a slightly different form of a decoder model that is worth mentioning, called the </span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.335.1">XLNet</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.336.1"> model.</span></span></p>
<h2 id="_idParaDest-103"><a id="_idTextAnchor104"/><span class="koboSpan" id="kobo.337.1">Diving into the XLNet model</span></h2>
<p><span class="koboSpan" id="kobo.338.1">If you and a friend were </span><a id="_idIndexMarker445"/><span class="koboSpan" id="kobo.339.1">given the prompt “_ _ is a city,” would you both separately predict “New York” to fill in the blanks? </span><span class="koboSpan" id="kobo.339.2">This is one of the flaws of MLM objectives of the encoder-only transformers as they learn by predicting on multiple tokens at once. </span><span class="koboSpan" id="kobo.339.3">Standard autoregressive models, however, are not susceptible to this issue due to their autoregressive nature of only predicting the future and only one token at a time. </span><span class="koboSpan" id="kobo.339.4">Encoder-only transformers, however, have a bidirectional sequence support that leads to improved performance. </span><span class="koboSpan" id="kobo.339.5">Mainly, XLNet works on the idea of making an autoregressive model attend to data in a bidirectional way like encoder-only models, while maintaining the benefits of an autoregressive model of predicting a </span><span class="No-Break"><span class="koboSpan" id="kobo.340.1">single output.</span></span></p>
<p><span class="koboSpan" id="kobo.341.1">The way XLNet achieves the best of both worlds is by conditioning the decoder-only transformer during MLM pre-training. </span><span class="koboSpan" id="kobo.341.2">The idea is to pre-train the model on all permutations of the token’s sequence order by using the masking mechanism. </span><span class="koboSpan" id="kobo.341.3">Standard autoregressive models use the order of “1-2-3-4” sequentially to predict the fifth token during pre-training, but in XLNet, the order can be changed into “3-4-1-5” to predict the second token. </span><span class="koboSpan" id="kobo.341.4">XLNet also introduced another extra query linear layer hidden state along with an additional single fixed learnable embedding that will be traversed through the attention layer along with each token embedding, as an extra</span><a id="_idIndexMarker446"/><span class="koboSpan" id="kobo.342.1"> stream along with the original token-based content stream. </span><span class="koboSpan" id="kobo.342.2">The extra path is called the query stream. </span><span class="koboSpan" id="kobo.342.3">The entire process</span><a id="_idIndexMarker447"/><span class="koboSpan" id="kobo.343.1"> is called the masked two-stream attention layer, depicted in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.344.1">Figure 6</span></em></span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.345.1">.8</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.346.1">.</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer073">
<span class="koboSpan" id="kobo.347.1"><img alt="Figure 6.8 – XLNet workflow" src="image/B18187_06_008.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.348.1">Figure 6.8 – XLNet workflow</span></p>
<p><span class="koboSpan" id="kobo.349.1">The forward propagation mechanism </span><a id="_idIndexMarker448"/><span class="koboSpan" id="kobo.350.1">of the content-based stream for any given position of a sequence ordering has the capability to attend to the content of its own position and the positions before it while the query-based stream for any given position of a sequence ordering only has the capability to attend to the content of positions before it. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.351.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.352.1">.8 </span></em><span class="koboSpan" id="kobo.353.1">(</span><strong class="bold"><span class="koboSpan" id="kobo.354.1">a</span></strong><span class="koboSpan" id="kobo.355.1">) shows how the attention operation for the content stream for the actual position 1 in the model can attend to all tokens in other positions due to the ordering of “3-2-4-1” where position 1 is permuted to be the last token. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.356.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.357.1">.8</span></em><span class="koboSpan" id="kobo.358.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.359.1">b</span></strong><span class="koboSpan" id="kobo.360.1">) shows how the token content at position 1 is not used to update the additional query stream path data denoted as </span><strong class="bold"><span class="koboSpan" id="kobo.361.1">g</span></strong><span class="koboSpan" id="kobo.362.1"> and that the </span><strong class="bold"><span class="koboSpan" id="kobo.363.1">g</span></strong><span class="koboSpan" id="kobo.364.1"> component acts as the query to obtain the next </span><strong class="bold"><span class="koboSpan" id="kobo.365.1">g</span></strong><span class="koboSpan" id="kobo.366.1"> as the output of the attention layer. </span><span class="koboSpan" id="kobo.366.2">If the fourth position of the model needs to be updated, only the second and third position content will be attended to in the content stream, as the first position is the future token according to the permuted sequence order. </span><span class="koboSpan" id="kobo.366.3">The masking mechanisms are applied dynamically based on this concept to prevent each position of </span><strong class="bold"><span class="koboSpan" id="kobo.367.1">h</span></strong><span class="koboSpan" id="kobo.368.1"> and </span><strong class="bold"><span class="koboSpan" id="kobo.369.1">g</span></strong><span class="koboSpan" id="kobo.370.1"> from attending to the content positions before it is defined from the permuted sequence order. </span><span class="koboSpan" id="kobo.370.2">Note the </span><strong class="bold"><span class="koboSpan" id="kobo.371.1">w</span></strong><span class="koboSpan" id="kobo.372.1"> component in </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.373.1">Figure 6</span></em></span><em class="italic"><span class="koboSpan" id="kobo.374.1">.8</span></em><span class="koboSpan" id="kobo.375.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.376.1">c</span></strong><span class="koboSpan" id="kobo.377.1">) denotes the single-weight embedding shared across all positions for the query stream. </span><span class="koboSpan" id="kobo.377.2">During pre-training, the query stream acts as the path for the final classification output for the MLM objective where the final </span><strong class="bold"><span class="koboSpan" id="kobo.378.1">h</span></strong><span class="koboSpan" id="kobo.379.1"> components are ignored. </span><span class="koboSpan" id="kobo.379.2">When passed on to the fine-tuning stage, however, the query stream components and query-based linear layer hidden states are discarded or </span><span class="No-Break"><span class="koboSpan" id="kobo.380.1">ignored completely.</span></span></p>
<p><span class="koboSpan" id="kobo.381.1">The novel way of enabling the bidirectional context of the data while leveraging the benefits of a single-word prediction-based language modeling objective during pre-training led to improved performance</span><a id="_idIndexMarker449"/><span class="koboSpan" id="kobo.382.1"> on downstream supervised </span><span class="No-Break"><span class="koboSpan" id="kobo.383.1">learning tasks.</span></span></p>
<h2 id="_idParaDest-104"><a id="_idTextAnchor105"/><span class="koboSpan" id="kobo.384.1">Discussing additional advancements for a decoder-only transformer model</span></h2>
<p><span class="koboSpan" id="kobo.385.1">Since the advent of GPT, there have</span><a id="_idIndexMarker450"/><span class="koboSpan" id="kobo.386.1"> not been any highly impactful changes to the base decoder-only architecture. </span><span class="koboSpan" id="kobo.386.2">Most of the work focuses on </span><span class="No-Break"><span class="koboSpan" id="kobo.387.1">three things:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.388.1">Applying transformers to different </span><span class="No-Break"><span class="koboSpan" id="kobo.389.1">problem types</span></span></li>
<li><span class="koboSpan" id="kobo.390.1">Scaling up the model size to hundreds of billions </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">of parameters</span></span></li>
<li><span class="koboSpan" id="kobo.392.1">Focusing on engineering solutions so that a huge model is feasible to be trained on the available </span><span class="No-Break"><span class="koboSpan" id="kobo.393.1">hardware resources</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.394.1">One of the most notable works utilizing decoder-only transformers for other problem types is the DALL-E model from the OpenAI team. </span><span class="koboSpan" id="kobo.394.2">Transformers were used in both the first and second versions of DALL-E to autoregressively predict a compact image embedding given a text input with some other implementation-specific inputs. </span><span class="koboSpan" id="kobo.394.3">These image embeddings are then transformed into actual image embeddings through </span><span class="No-Break"><span class="koboSpan" id="kobo.395.1">other mechanisms.</span></span></p>
<p><span class="koboSpan" id="kobo.396.1">For model scaling, GPT-3 showed that the model</span><a id="_idIndexMarker451"/><span class="koboSpan" id="kobo.397.1"> capacity can still be increased along with the performance. </span><span class="koboSpan" id="kobo.397.2">First to mention is </span><strong class="bold"><span class="koboSpan" id="kobo.398.1">Megatron</span></strong><span class="koboSpan" id="kobo.399.1">, which focused on making the transformer architecture more efficient with engineering strategies made to achieve parallelism and capable of being trained with 512 GPUs, making it 5.6 times larger than GPT-2. </span><span class="koboSpan" id="kobo.399.2">GPT-3 outsized</span><a id="_idIndexMarker452"/><span class="koboSpan" id="kobo.400.1"> Megatron, but another notable model called </span><strong class="bold"><span class="koboSpan" id="kobo.401.1">BLOOM</span></strong><span class="koboSpan" id="kobo.402.1"> scaled it to 176B using Megatron as a base to match the GPT-3 model size of 175B. </span><span class="koboSpan" id="kobo.402.2">What’s most notable is not that it is bigger than GPT-3 but the fact that BLOOM gives birth to a new paradigm – open source – which means that the collective work and contributions of the community of researchers and institutions can also train highly useful and performant models on a massive scale. </span><span class="koboSpan" id="kobo.402.3">These huge models in the past have always been exclusive to big corporations due to their immense hardware resources. </span><span class="koboSpan" id="kobo.402.4">Other than that, another model, subsequently</span><a id="_idIndexMarker453"/><span class="koboSpan" id="kobo.403.1"> released in 2022, that scaled up GPT to 540 billion parameters is </span><strong class="bold"><span class="koboSpan" id="kobo.404.1">PaLM</span></strong><span class="koboSpan" id="kobo.405.1">, which exceeded the performance of every other smaller GPT variant model. </span><strong class="bold"><span class="koboSpan" id="kobo.406.1">PaLM 2</span></strong><span class="koboSpan" id="kobo.407.1"> outperforms its predecessor by offering enhanced performance across</span><a id="_idIndexMarker454"/><span class="koboSpan" id="kobo.408.1"> various tasks, including English and multilingual understanding, reasoning, and code generation. </span><span class="koboSpan" id="kobo.408.2">It achieves this by optimizing computing scaling laws, utilizing diverse multilingual datasets, and implementing architectural improvements. </span><span class="koboSpan" id="kobo.408.3">PaLM 2 excels in multilingual proficiency, classification, question-answering, and translation tasks, demonstrating its broad applicability. </span><span class="koboSpan" id="kobo.408.4">The model also incorporates control tokens to mitigate toxicity and provides guidelines for responsible development </span><span class="No-Break"><span class="koboSpan" id="kobo.409.1">and deployment.</span></span></p>
<p><span class="koboSpan" id="kobo.410.1">When selecting a transformer model for a specific task, it’s crucial to take a balanced approach, considering several factors in addition to performance metrics. </span><span class="koboSpan" id="kobo.410.2">Model size and resource requirements play a significant role in determining the feasibility and applicability of the chosen model. </span><span class="koboSpan" id="kobo.410.3">While larger models such as GPT-3 have shown impressive capabilities, their resource demands may not be suitable for all situations, especially for individuals or small organizations with limited </span><span class="No-Break"><span class="koboSpan" id="kobo.411.1">computational resources.</span></span></p>
<p><span class="koboSpan" id="kobo.412.1">It’s essential to weigh the trade-offs between model size, performance, and resource requirements when choosing a transformer for a particular task. </span><span class="koboSpan" id="kobo.412.2">In some cases, smaller models may provide adequate performance while being more efficient and environmentally friendly. </span><span class="koboSpan" id="kobo.412.3">Additionally, fine-tuning can often improve the performance of a model in domain-specific tasks, even if it is not as large as some of the most prominent models such as GPT-3. </span><span class="koboSpan" id="kobo.412.4">Ultimately, the best choice depends on the unique requirements and constraints</span><a id="_idIndexMarker455"/><span class="koboSpan" id="kobo.413.1"> of the specific task, as well as the resources available for model training </span><span class="No-Break"><span class="koboSpan" id="kobo.414.1">and deployment.</span></span></p>
<h1 id="_idParaDest-105"><a id="_idTextAnchor106"/><span class="koboSpan" id="kobo.415.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.416.1">Transformers are versatile NNs capable of capturing relationships of any data modality without explicit data-specific biases in the architecture. </span><span class="koboSpan" id="kobo.416.2">Instead of a neural network architecture capable of ingesting different data modalities directly, careful considerations of the data input structure along with crafting proper training task objectives are needed to successfully build a performant transformer. </span><span class="koboSpan" id="kobo.416.3">The benefits of pre-training still hold true even for the current SOTA architecture. </span><span class="koboSpan" id="kobo.416.4">The act of pre-training is part of a concept called transfer learning, which will be covered more extensively in the supervised and unsupervised learning chapters. </span><span class="koboSpan" id="kobo.416.5">Transformers can currently perform both data generation and supervised learning tasks in general with more and more research experimenting with using transformers in unexplored niche tasks and data modalities. </span><span class="koboSpan" id="kobo.416.6">Look forward to more deep learning innovations in the coming years with transformers being at the forefront of </span><span class="No-Break"><span class="koboSpan" id="kobo.417.1">the advancement.</span></span></p>
<p><span class="koboSpan" id="kobo.418.1">By now, you have gained the knowledge needed to appropriately choose and design a neural network architecture according to your data and requirements. </span><span class="koboSpan" id="kobo.418.2">Most of the architectures and concepts presented in </span><em class="italic"><span class="koboSpan" id="kobo.419.1">Chapters 2</span></em><span class="koboSpan" id="kobo.420.1"> to </span><em class="italic"><span class="koboSpan" id="kobo.421.1">6</span></em><span class="koboSpan" id="kobo.422.1"> are tricky to implement from scratch but with the help of open source work such as </span><a href="https://github.com/rwightman/pytorch-image-models"><span class="koboSpan" id="kobo.423.1">https://github.com/rwightman/pytorch-image-models</span></a><span class="koboSpan" id="kobo.424.1"> and the help of many deep learning frameworks, using a model is a matter of importing libraries and adding a few lines of code. </span><span class="koboSpan" id="kobo.424.2">Truthfully, understanding what goes into each of the architectures under the hood is not actually needed to utilize these models, due to the ease of adopting publicly available work complete with pre-trained weights on big datasets. </span><span class="koboSpan" id="kobo.424.3">More often than not, understanding architectural concepts such as CNNs at a high level and knowing some bits and pieces about training a model is really all it takes today to benefit practically from these mostly ready-off-the-shelf architectures/models. </span><span class="koboSpan" id="kobo.424.4">However, understanding the implementation details of these architectures is essential and will prove to be beneficial when it comes to the </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">following cases:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.426.1">When things fail or don’t work as expected (the model does not converge to an optimum solution or diverges, model errors with the prepared input with unexpected data shapes, and </span><span class="No-Break"><span class="koboSpan" id="kobo.427.1">so on).</span></span></li>
<li><span class="koboSpan" id="kobo.428.1">When it’s necessary to choose a more appropriate architecture based on your dataset, runtime requirements, or </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">performance requirements.</span></span></li>
<li><span class="koboSpan" id="kobo.430.1">When you are inventing a shiny new neural </span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">network layer.</span></span></li>
<li><span class="koboSpan" id="kobo.432.1">When you want to decode what the neural network actually learns and when you want to understand why the neural network makes its predictions. </span><span class="koboSpan" id="kobo.432.2">This will be explored in more depth in </span><a href="B18187_11.xhtml#_idTextAnchor172"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.433.1">Chapter 11</span></em></span></a><span class="koboSpan" id="kobo.434.1">, </span><em class="italic"><span class="koboSpan" id="kobo.435.1">Explaining Neural Network Predictions</span></em><span class="koboSpan" id="kobo.436.1">, and </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.437.1">Chapter 12</span></em></span><span class="koboSpan" id="kobo.438.1">, </span><em class="italic"><span class="koboSpan" id="kobo.439.1">Interpreting </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.440.1">Neural Networks</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">.</span></span></li>
<li><span class="koboSpan" id="kobo.442.1">To adopt concepts from one architecture domain to another domain. </span><span class="koboSpan" id="kobo.442.2">For example, skip connections from DenseNet and ResNet can be easily transferable </span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">to MLPs.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.444.1">Since you’ve read to the end of this chapter, which completes the neural network architecture specific content, give yourself a pat on the back. </span><span class="koboSpan" id="kobo.444.2">You now have knowledge about models that deal with images (CNNs), time-series or sequence data (RNNs), models that deal with tabular data (MLPs), and models that are a jack of all trades (transformers). </span><span class="koboSpan" id="kobo.444.3">The performance of architectures, however, is closely coupled with the data type, the data structure, the data preprocessing method, the weights learning and optimizing method, the loss function, and the optimization task. </span><span class="koboSpan" id="kobo.444.4">More details about these components will be decoded in </span><a href="B18187_08.xhtml#_idTextAnchor125"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.445.1">Chapter 8</span></em></span></a><span class="koboSpan" id="kobo.446.1">, </span><em class="italic"><span class="koboSpan" id="kobo.447.1">Exploring Supervised Deep Learning</span></em><span class="koboSpan" id="kobo.448.1">, and </span><a href="B18187_09.xhtml#_idTextAnchor149"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.449.1">Chapter 9</span></em></span></a><span class="koboSpan" id="kobo.450.1">, </span><em class="italic"><span class="koboSpan" id="kobo.451.1">Exploring Unsupervised </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.452.1">Deep Learning</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.453.1">.</span></span></p>
<p><span class="koboSpan" id="kobo.454.1">In the next chapter, we will explore an emerging method to design neural network models in an automated way called neural </span><span class="No-Break"><span class="koboSpan" id="kobo.455.1">architecture search.</span></span></p>
</div>
</body></html>