- en: '8'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularization with Recurrent Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will work with **Recurrent Neural Networks** (**RNNs**).
    As we will see, they are well suited for **Natural Language Processing** (**NLP**)
    tasks, even if they also apply well to time series tasks. After learning how to
    train RNNs, we will apply several regularization methods, such as using dropout
    and the sequence maximum length. This will allow you to gain foundational knowledge
    that can be applied to NLP or time series-related tasks. This will also give you
    the necessary knowledge to understand more advanced techniques covered in the
    next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll cover the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Training an RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a **Gated Recurrent** **Unit** (**GRU**)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing with dropout
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing with a maximum sequence length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we will train RNNs on various tasks using the following libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pandas
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: PyTorch
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training an RNN
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In NLP, input data is commonly textual data. Since a text is usually nothing
    but a sequence of words, using RNNs is sometimes a good solution. Indeed, RNNs,
    unlike fully connected networks, consider data’s sequential information.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will train an RNN on tweets to predict whether they are positive,
    negative, or neutral.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In NLP, we usually manipulate textual data, which is unstructured. To handle
    it properly, this is usually a multi-step process – first, convert the text into
    numbers, and then only train a model on those numbers.
  prefs: []
  type: TYPE_NORMAL
- en: There are several ways to convert text into numbers. In this recipe, we will
    use a simple approach called `['the', 'dog', 'is', 'out']`. There is usually one
    more step in the tokenization process – once the sentence is converted into a
    list of words, it must be converted to a number. Each word is assigned to a number
    so that the sentence “*The dog is out*” could be tokenized as `[3, 198,` `50,
    3027]`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: This is a quite simplistic explanation of tokenization. Check the *See also*
    subsection for more resources.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will train an RNN on tweets for a multiclass classification
    task. However, how does RNN work? An RNN takes as input a sequence of features,
    as represented in *Figure 8**.1*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.1 – A representation of an RNN. At the bottom level is the input
    features, in the middle is the hidden layers, and at the top level is the output
    layer](img/B19629_08_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.1 – A representation of an RNN. At the bottom level is the input features,
    in the middle is the hidden layers, and at the top level is the output layer
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 8**.1*, the hidden layer of an RNN has two inputs and two outputs:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Inputs**: The features at the current step, ![](img/Formula_08_001.png),
    and the hidden state of the previous step, ![](img/Formula_08_002.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Outputs**: The hidden state, ![](img/Formula_08_003.png) (fed to the next
    step), and this step’s activation output ![](img/Formula_08_004.png)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the case of a one-layer RNN, the activation function is simply the output
    ![](img/Formula_08_005.png).
  prefs: []
  type: TYPE_NORMAL
- en: Going back to our example, the input features are the tokens. So, at each sequence
    step, one or more layers of neural networks take as input both the features that
    are at this sequence step and the hidden state of the previous step.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: RNNs can be used in other contexts, such as forecasting, where the input features
    can be both quantitative and qualitative features.
  prefs: []
  type: TYPE_NORMAL
- en: 'RNNs also have several sets of weights. As represented in *Figure 8**.2*, there
    are three sets of weights:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_08_006.png): The weights applied to the hidden state of the
    previous step, for the current hidden state computation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_08_007.png): The weights applied to the input features, for
    the current hidden state computation'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_08_008.png): The weights applied to the current hidden state,
    for the current output'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 8.2 – A representation of an RNN with different sets of weights](img/B19629_08_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.2 – A representation of an RNN with different sets of weights
  prefs: []
  type: TYPE_NORMAL
- en: 'Overall, considering all of this, the computation of the hidden state and the
    activation output can be computed as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_08_009.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_08_010.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, *g* is the activation function, and ![](img/Formula_08_011.png) and ![](img/Formula_08_012.png)
    are biases. We use *softmax* here for the output computation, assuming it is a
    multiclass classification, but any activation function is possible depending on
    the task.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the loss can be computed easily, as for any other machine-learning
    task (for example, for a classification task, a cross-entropy loss can be computed
    between the ground truth and the neural network’s output). Backpropagation on
    such neural networks, called **backpropagation through time**, is beyond the scope
    of this book.
  prefs: []
  type: TYPE_NORMAL
- en: 'On a practical side, for this recipe, we will need a Kaggle dataset. To get
    this dataset, once the Kaggle API has been set, the following command lines can
    be used to get the dataset in the current working directory:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: This line should download a `.zip` file and unzip its content, and then a file
    named `Tweets.csv` should be available. You can move or copy this file to the
    current working directory.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, the following libraries must be installed: `pandas`, `numpy`, `scikit-learn`,
    `matplotlib`, `torch`, and `transformers`. They can be installed with the following
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will use an RNN to perform the classification of tweets
    into three classes – negative, neutral, and positive. As explained in the previous
    section, this will be a multi-step process – first, a tokenization of the tweet’s
    texts, and then just model training:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`torch` and some related modules and classes for the neural network'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` and `LabelEncoder` from scikit-learn for preprocessing'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AutoTokenizer` from Transformers to tokenize the tweets'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` to load the dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` for visualization:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE5]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE7]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data from the `.csv` file with pandas:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '|  | `airline_sentiment` | `Text` |'
  prefs: []
  type: TYPE_TB
- en: '| 0 | Neutral | `@VirginAmerica What @``dhepburn said.` |'
  prefs: []
  type: TYPE_TB
- en: '| 1 | Positive | `@VirginAmerica plus you''ve added` `commercials t...` |'
  prefs: []
  type: TYPE_TB
- en: '| 2 | Neutral | `@VirginAmerica I didn''t today... Must mean` `I n...` |'
  prefs: []
  type: TYPE_TB
- en: '| 3 | Negative | `@VirginAmerica it''s really aggressive` `to blast...` |'
  prefs: []
  type: TYPE_TB
- en: '| 4 | Negative | `@VirginAmerica and it''s a really big` `bad thing...` |'
  prefs: []
  type: TYPE_TB
- en: Table 8.1 – Output with the data classified
  prefs: []
  type: TYPE_NORMAL
- en: The data we will use is made of labels from the `airline_sentiment` column (either
    negative, neutral, or positive) and their associated raw tweets texts from the
    `text` column.
  prefs: []
  type: TYPE_NORMAL
- en: '3. Split the data into train and test sets, using the `train_test_split` function,
    with a test size of 20% and a specified random state for reproducibility:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '4. Implement the `TextClassificationDataset` dataset class, handling the data.
    At instance creation, this class will so the following:'
  prefs: []
  type: TYPE_NORMAL
- en: Instantiate `AutoTokenizer` from Transformers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenize the tweets with that previously instantiated tokenizer and store the
    results
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Encode the labels and store them:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE25]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE26]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE27]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Several options are specified with this tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: It is instantiated with the `'bert-base-uncased'` tokenizer, a tokenizer used
    for BERT models
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tokenization is made, with a maximum length provided as a constructor argument
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The padding is set to `True`, meaning that if a tweet has less than the maximum
    length, it will be filled with zeros to match that length
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The truncation is set to `True`, meaning that if a tweet has more than the maximum
    length, the remaining tokens will be ignored
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The return tensor is specified as `'pt'` so that it returns a PyTorch tensor
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: See the *There’s more…* subsection for more details about what the tokenizer
    does.
  prefs: []
  type: TYPE_NORMAL
- en: '5. Instantiate the `TextClassificationDataset` objects for the train and test
    sets, as well as the related data loaders. We specify here a maximum number of
    words of `24` and a batch size of `64`. This means that each tweet will be converted
    into a sequence of exactly 24 tokens:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '6. Implement the RNN model:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'The RNN model defined here can be described in several steps:'
  prefs: []
  type: TYPE_NORMAL
- en: An embedding that takes the tokens as input, with the input the size of the
    vocabulary and the output the size of the given embedding dimension
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Three layers of RNN that take as input the embedding output, with the given
    number of layers, a hidden size, and a ReLU activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, an embedding that takes the tokens as input, with the input the size
    of the vocabulary and the output the size of the given embedding dimension; note
    that the output is computed only for the last sequence step (that is, `output[:,
    -1]`), and a softmax activation function is applied
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: The output is not necessarily computed only for the last sequence step. Depending
    on the task, it can be useful to output a value at each step alike (for example,
    forecasting) or only a final value (for example, classification task).
  prefs: []
  type: TYPE_NORMAL
- en: '7. Instantiate and test the model. The vocabulary size is given by the tokenizer
    and the output size of three is defined by the task; there are three classes (negative,
    neutral, positive). The other arguments are hyperparameters; here, the following
    values are chosen:'
  prefs: []
  type: TYPE_NORMAL
- en: An embedding dimension of `64`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A hidden dimension of `64`
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Other values can, of course, be tested:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'The code will output the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '8. Instantiate the optimizer; here, we will use an Adam optimizer, with a learning
    rate of `0.001`. The loss is the cross-entropy loss, since this is a multiclass
    classification task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 9. Let’s define two helper functions to train the model.
  prefs: []
  type: TYPE_NORMAL
- en: '`epoch_step_tweet` will compute the loss and accuracy for one epoch, as well
    as update the weights for the training set:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '`train_tweet_classification` will use loop over the epochs and use `epoch_step_tweet`
    to compute and store the loss and accuracy:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '10. Reusing the helper functions, we can now train the model on 20 epochs.
    Here, we will compute and store the accuracy and the loss for both the train and
    test sets at each epoch, to plot them afterward:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'After 20 epochs, the output should be something like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '11. Plot the loss as a function of the epoch number, for both the train and
    test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the resulting graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.3 – Cross-entropy loss as a function of the epoch](img/B19629_08_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.3 – Cross-entropy loss as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: We can see some overfitting as early as the fifth epoch, since the train loss
    keeps decreasing while the test loss reaches a plateau.
  prefs: []
  type: TYPE_NORMAL
- en: '12. Similarly, plot the accuracy as a function of the epoch number of both
    the train and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'We then get this graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.4 – Accuracy as a function of the epoch](img/B19629_08_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.4 – Accuracy as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: After 20 epochs, the accuracy of the train set is about 82%, but it is only
    about 74% on the test set, meaning there might be room for improvement with proper
    regularization.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, we used the `HuggingFace` tokenizer, but what does it actually
    do? Let’s have a look at a text example to fully understand what it is.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s define a brand-new tokenizer with the `AutoTokenizer` class, specifying
    the BERT tokenizer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: There are many tokenizers that have different methods and, thus, different outputs
    for the same given text. `'bert-base-uncased'` is quite a common one, but many
    others can be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now apply this tokenizer to a given text, using the `tokenize` method,
    to see what the output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'The code output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: So, the tokenization can be described as splitting a sentence into smaller chunks.
    Other tokenizers can have different chunks (or tokens) at the end, but the process
    remains essentially the same.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we just apply the tokenization in this same sentence, we can get the
    token numbers with `''input_ids''`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'The code output is now the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that the `3180` and `3989` tokens are present twice. Indeed, the word `regularization`
    (tokenized as two separate tokens) is present twice.
  prefs: []
  type: TYPE_NORMAL
- en: For a given tokenizer, the vocabulary size is just the number of existing tokens.
    This is stored in the `vocab_size` attribute. In this case, the vocabulary size
    is `30522`.
  prefs: []
  type: TYPE_NORMAL
- en: Tip
  prefs: []
  type: TYPE_NORMAL
- en: If you’re curious, you can also directly have a look at the whole vocabulary,
    stored in the `.vocab` attribute as a dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'This is great content about tokenizers by HuggingFace: [https://huggingface.co/docs/transformers/tokenizer_summary](https://huggingface.co/docs/transformers/tokenizer_summary%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation about `AutoTokenizer`: [https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/auto#transformers.AutoTokenizer](https://huggingface.co/docs/transformers/v4.27.2/en/model_doc/auto#transformers.AutoTokenizer%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation about RNNs: [https://pytorch.org/docs/stable/generated/torch.nn.RNN.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.RNN.xhtml%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a GRU
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will keep exploring RNNs with the **GRU** – what it is, how
    it works, and how to train such a model.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One of the main limitations of RNNs is the memory of the network throughout
    their steps. GRUs try to overcome this limit by adding a memory gate.
  prefs: []
  type: TYPE_NORMAL
- en: If we take a step back and describe an RNN cell with a simple diagram, it could
    look like *Figure 8**.5*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.5 – A diagram of an RNN cell](img/B19629_08_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.5 – A diagram of an RNN cell
  prefs: []
  type: TYPE_NORMAL
- en: So basically, at each step *t*, there are both a hidden state ![](img/Formula_08_013.png)
    and a set of features ![](img/Formula_08_014.png). They are concatenated, then
    weights are applied and an activation function g resulting in a new hidden state
    ![](img/Formula_08_015.png). Optionally, an output ![](img/Formula_08_016.png)
    is computed from this hidden state, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: But what if this step of features ![](img/Formula_08_017.png) is not relevant?
    Or what if it would be useful for the network to just remember fully this hidden
    state ![](img/Formula_08_018.png) from time to time? This is exactly what a GRU
    does, by adding a new set of parameters through what is called a gate.
  prefs: []
  type: TYPE_NORMAL
- en: A **gate** is learned through backpropagation too, using a new set of weights,
    and allows a network to learn more complex patterns, as well as remember relevant
    past information.
  prefs: []
  type: TYPE_NORMAL
- en: 'A GRU is made up of two gates:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_08_019.png): the update gate, responsible for learning whether
    to update the hidden state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_08_020.png): the relevance gate, responsible for learning how
    relevant the hidden state is'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the end, a simplified diagram of the GRU unit looks like the one shown in
    *Figure 8**.6*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.6 – A diagram of a GRU cell. The relevance gate is omitted for clarity](img/B19629_08_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.6 – A diagram of a GRU cell. The relevance gate is omitted for clarity
  prefs: []
  type: TYPE_NORMAL
- en: 'The forward computation is now slightly more complicated than for a simple
    RNN cell, and it can be described with the following set of formulas:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_08_021.png)'
  prefs: []
  type: TYPE_IMG
- en: '![](img/Formula_08_022.png)'
  prefs: []
  type: TYPE_IMG
- en: 'These equations can be simply described in a few words. Compared to a simple
    RNN, there are three major differences:'
  prefs: []
  type: TYPE_NORMAL
- en: Two gates, ![](img/Formula_08_023.png) and ![](img/Formula_08_024.png), are
    computed with associated weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The relevance gate ![](img/Formula_08_025.png) is used to compute the intermediate
    hidden state ![](img/Formula_08_026.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The final hidden state ![](img/Formula_08_027.png) is a linear combination of
    the previous hidden state and the current intermediate hidden state, with the
    update gate ![](img/Formula_08_028.png) as the weight
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The major trick here is the use of the update gate, which can be interpreted
    in extreme cases as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: If ![](img/Formula_08_029.png) is only made of ones, the previous hidden state
    is forgotten
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If ![](img/Formula_08_030.png)is only made of zeros, the new hidden state is
    not taken into account
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Although the concepts can be quite complex at first, the GRU is fortunately
    super easy to use with PyTorch, as we will see in this recipe.
  prefs: []
  type: TYPE_NORMAL
- en: 'To run the code in this recipe, we will use the IMDb dataset – a dataset containing
    movie reviews, and positive or negative labels. The task is to guess the polarity
    of the review (positive or negative) based on the text. It can be downloaded with
    the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also need the following libraries: `pandas`, `numpy`, `scikit-learn`,
    `matplotlib`, `torch`, and `transformers`. They can be installed with the following
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will train a GRU on the same IMDb dataset for a binary classification
    task. As we will see, the code to train a GRU is almost the same as that to train
    a simple RNN:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`torch` and some related modules and classes for the neural network'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`train_test_split` and `LabelEncoder` from scikit-learn for preprocessing'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`AutoTokenizer` from Transformers to tokenize the reviews'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pandas` to load the dataset'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`matplotlib` for visualization:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE49]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE51]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE53]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE55]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the data from the `.csv` file with pandas. This is a 50,000-row dataset,
    with textual reviews and labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'Split the data into train and test sets, using the `train_test_split` function,
    with a test size of 20% and a specified random state for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the `TextClassificationDataset` dataset class, handling the data.
    At instance creation, this class will do the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Instantiate `AutoTokenizer` from the transformers library, using the `bert-base-uncased`
    tokenizer
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Tokenize the tweets with the previously instantiated tokenizer, along with the
    provided maximum length, padding, and truncation
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Encode the labels and store them:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE66]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE67]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE68]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE69]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE70]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE71]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE74]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE75]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE77]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE78]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE80]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the `TextClassificationDataset` objects for the train and test
    sets, as well as the related data loaders, with a maximum number of words of 64
    and a batch size of 64\. This means that each movie review will be converted as
    a sequence of exactly 64 tokens:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the GRU classifier model. It is made up of the following elements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An embedding layer (taking a zero vector as the first input)
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Three layers of GRU
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A fully connected layer on the last sequence step, with a sigmoid activation
    function, since it’s a binary classification:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE93]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE97]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE98]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE99]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE100]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE102]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE104]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE105]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE107]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE111]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE112]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE114]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE116]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE117]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE118]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the GRU model, with an embedding dimension and a hidden dimension
    of 32:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE122]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE123]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE124]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE125]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE126]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE127]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE128]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE129]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE130]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE131]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE132]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE133]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE134]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE135]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code output is the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE136]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the optimizer as an Adam optimizer, with a learning rate of `0.001`.
    The loss is defined as the binary cross-entropy loss because this is a binary
    classification task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE137]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE138]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Let’s now implement two helper functions.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`epoch_step_IMDB` updates the weights on the train set and computes the binary
    cross-entropy loss and accuracy for a given epoch:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE139]'
  prefs: []
  type: TYPE_PRE
- en: '`train_IMDB_classification` loops over the epochs, trains a model, and stores
    the accuracy and loss for the train and test sets:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE140]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model over 20 epochs, reusing the functions we just implemented.
    Compute and store the accuracy and the loss for both the train and test sets at
    each epoch, for visualization purposes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE141]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE142]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE143]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After 20 epochs, the results should be close to the following code output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE144]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the loss as a function of the epoch number, for both the train and test
    sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE145]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE146]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE147]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE148]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We then get this graph:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.7 – A binary cross-entropy loss as a function of the epoch](img/B19629_08_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.7 – A binary cross-entropy loss as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the loss is clearly diverging for the test set, meaning after
    only a few epochs, there is already overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'Similarly, plot the accuracy as a function of the epoch number of both the
    train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE149]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE150]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE151]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE152]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the graph obtained:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.8 – Accuracy as a function of the epoch](img/B19629_08_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.8 – Accuracy as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: As with the loss, we can see we face overfitting with an accuracy close to 100%
    on the train set, but it is only about a maximum of 77% on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: On a side note, if you try this recipe and the previous one yourself, you may
    find the GRU much more stable in the results, while the RNN in the previous recipe
    may sometimes have a hard time properly converging.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: When working with sequential data such as text, time series, and audio, RNNs
    are commonly used. While simple RNNs are not so frequently used because of their
    limitations, GRUs are usually a better choice. Besides simple RNNs and GRUs, another
    type of cell is frequently used – **long short-term memory** cells, better known
    as **LSTMs**.
  prefs: []
  type: TYPE_NORMAL
- en: The cell of an LSTM is even more complex than the one of a GRU. While a GRU
    cell has a hidden state and two gates, an LSTM cell has two types of hidden states
    (the hidden state and the cell state) and three gates. Let’s now have a quick
    look.
  prefs: []
  type: TYPE_NORMAL
- en: 'The cell state of an LSTM is described in *Figure 8**.9*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.9 – A diagram of an LSTM cell, assuming the LSTM activation function
    is a tanh and the output layer activation function is a softmax](img/B19629_08_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.9 – A diagram of an LSTM cell, assuming the LSTM activation function
    is a tanh and the output layer activation function is a softmax
  prefs: []
  type: TYPE_NORMAL
- en: 'Without getting into all the computational details of the LSTM, from *Figure
    8**.9* we can see there are three gates, computed with their own set of weights,
    based on both the previous hidden state ![](img/Formula_08_031.png) and the current
    features ![](img/Formula_08_032.png), just like for a GRU, with a sigmoid activation
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: The forget gate ![](img/Formula_08_033.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The update gate ![](img/Formula_08_034.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output gate ![](img/Formula_08_035.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There are also two states, computed at each step:'
  prefs: []
  type: TYPE_NORMAL
- en: A cell state ![](img/Formula_08_036.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A hidden state ![](img/Formula_08_037.png)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, the intermediary state ![](img/Formula_08_038.png) is computed with its
    own set of weights, just like a gate, with a free activation function.
  prefs: []
  type: TYPE_NORMAL
- en: Having more gates and states, LSTMs have more parameters than GRUs and, thus,
    usually need more data to be properly trained. However, they are proven to be
    very effective with long sequences, such as long texts.
  prefs: []
  type: TYPE_NORMAL
- en: 'Using PyTorch, the code for training an LSTM is not much different from the
    code to train a GRU. In this recipe, the only piece of code that needs to be changed
    would be the model implementation, replaced, for example, with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE153]'
  prefs: []
  type: TYPE_PRE
- en: 'The main differences with `GRUClassifier` implemented earlier are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In `init`: Of course, using `nn.LSTM` instead of `nn.GRU`, since we now want
    an LSTM-based classifier'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In `forward`: We now initialize two zero vectors, `h0` and `c0`, which are
    fed to the LSTM'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The output of the LSTM is now made of the output and both the hidden and cell
    states
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Besides that, it can be trained the same way as a GRU, with the same code.
  prefs: []
  type: TYPE_NORMAL
- en: On a comparative note, let’s compute the number of parameters in this LSTM,
    and let’s compare it to the number of parameters in an “equivalent” RNN and GRU
    (that is, the same hidden dimension, the same number of layers, and so on).
  prefs: []
  type: TYPE_NORMAL
- en: 'The number of parameters in this LSTM can be computed with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE154]'
  prefs: []
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Note that we do not take into account the embedding part, since we omit the
    first layer.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the number of parameters for each type of model:'
  prefs: []
  type: TYPE_NORMAL
- en: '**RNN**: 6,369'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**GRU**: 19,041'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LSTM**: 25,377'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A rule of thumb to explain this is the number of gates. Compared to a simple
    RNN, a GRU has two additional gates requiring their own weights, hence a total
    number of parameters multiplied by 3\. The same logic applies to the LSTM having
    three gates.
  prefs: []
  type: TYPE_NORMAL
- en: Overall, the more parameters a model contains, the more data it needs to be
    trained robustly, which is why a GRU is a good trade-off and usually a good first
    choice.
  prefs: []
  type: TYPE_NORMAL
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: Up to now, we only assumed GRUs (and RNNs in general) can go from left to right
    – from the start of a sentence to the end of a sentence. Just because that’s what
    we humans usually do when we read, it doesn’t mean it’s necessarily the most optimal
    way for a neural network to learn. It is possible to use RNNs in both directions,
    known as `bidirectional=True` to the model definition, such as `nn.GRU(bidirectional=True)`.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The official documentation about GRUs: [https://pytorch.org/docs/stable/generated/torch.nn.GRU.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.GRU.xhtml%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The official documentation about LSTMs: [https://pytorch.org/docs/stable/generated/torch.nn.LSTM.xhtml](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.xhtml%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A somewhat out-of-date but great post about LSTMs’ effectiveness by Andrej
    Karpathy: [https://karpathy.github.io/2015/05/21/rnn-effectiveness/](https://karpathy.github.io/2015/05/21/rnn-effectiveness/%0D)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing with dropout
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will add dropout to a GRU to add regularization to the IMDb
    classification dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Just like fully connected neural networks, recurrent neural networks such as
    GRUs and LSTMs can be trained with dropout. As a reminder, dropout is just randomly
    setting some unit’s activation to zero during training. As a result, it allows
    a network to have less information at once and to hopefully generalize better.
  prefs: []
  type: TYPE_NORMAL
- en: We will improve upon the results of the GRU training recipe, by using dropout
    on the same task – the IMDb dataset binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'If not already done, the dataset can be downloaded using the Kaggle API with
    the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE155]'
  prefs: []
  type: TYPE_PRE
- en: 'The required libraries can be installed with the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE156]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We will train a GRU on the IMDb dataset, just like in the *Training a GRU*
    recipe. Since the five first steps of *Training a GRU* (from the imports to the
    `DataLoaders` instantiation) are common to this recipe, let’s just assume they
    have been run and start directly with the model class implementation. Implement
    the GRU classifier model. It is made of the following elements:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: An embedding layer (taking a zero vector as the first input), on which dropout
    is applied in the forward
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: Three layers of GRU, with dropout directly provided as an argument to the GRU
    constructor
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A fully connected layer on the last sequence step, with a sigmoid activation
    function, with no dropout:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE157]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE158]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE159]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE160]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE161]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE162]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE163]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE164]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE165]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE166]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE167]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE168]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE169]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE170]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE171]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE172]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE173]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE174]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE175]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE176]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE177]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE178]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE179]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE180]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE181]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE182]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE183]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE184]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE185]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: Important note
  prefs: []
  type: TYPE_NORMAL
- en: It is not mandatory to apply dropout to the embedding, nor is it always useful.
    In this case, since the embedding is a large part of the model, applying dropout
    only to the GRU layers won’t have a significant impact on performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the GRU model, with an embedding dimension and a hidden dimension
    of `32`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE186]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE187]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE188]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE189]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE190]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE191]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE192]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE193]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE194]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE195]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the optimizer as an Adam optimizer, with a learning rate of `0.001`.
    The loss is defined as the binary cross-entropy loss, since this is a binary classification
    task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE196]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model over 20 epochs by reusing the helper function implemented in
    the previous recipe. For each epoch, we compute and store the accuracy and the
    loss for both the train and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE197]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE198]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE199]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE200]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The last epoch output should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE201]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the loss as a function of the epoch number, for both the train and test
    sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE202]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE203]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE204]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE205]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.10 – Binary cross-entropy loss as a function of the epoch](img/B19629_08_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.10 – Binary cross-entropy loss as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: We can see that we are still overfitting, but it’s a bit less dramatic than
    without dropout.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, plot the accuracy as a function of the epoch number of both the train
    and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE206]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE207]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE208]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE209]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.11 – Accuracy as a function of the epoch](img/B19629_08_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.11 – Accuracy as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: Note the effect of dropout on the train accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: While the accuracy did not spectacularly improve, it has increased from 77%
    to 79% with dropout. Also, the difference between the train and test losses is
    smaller than it was without dropout, allowing us to improve generalization.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike dropout, other methods that are useful to regularize fully connected
    neural networks can be used with GRUs and other RNN-based architectures.
  prefs: []
  type: TYPE_NORMAL
- en: For example, since we have rather substantial overfitting here, with a train
    loss having a steep decrease, it might be interesting to test smaller architectures,
    with fewer parameters to be learned.
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing with the maximum sequence length
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will regularize by playing with the maximum sequence length,
    on the IMDB dataset, using a GRU-based neural network.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Up to now, we have not played much with the maximum length of the sequence,
    but it is sometimes one of the most important hyperparameters to tune.
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, depending on the input dataset, the optimal maximum length can be quite
    different:'
  prefs: []
  type: TYPE_NORMAL
- en: A tweet is short, so having a maximum number of tokens of hundreds does not
    make sense most of the time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A product or movie review can be significantly longer, and sometimes, the reviewer
    writes a lot of pros and cons about the product/movie, before getting to the final
    conclusion – in such cases, a larger maximum length may help
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this recipe, we will train a GRU on the IMDb dataset, containing movie reviews
    and associated labels (either positive or negative); this dataset contains some
    very lengthy texts. So, we will significantly increase the maximum number of words,
    and see how it impacts the final accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'If not already done, you can download the dataset, assuming you have a Kaggle
    API installed, running the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE210]'
  prefs: []
  type: TYPE_PRE
- en: 'The following libraries are needed: `pandas`, `numpy`, `scikit-learn`, `matplotlib`,
    `torch`, and `transformers`. They can be installed with the following command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE211]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Here are the steps to perform this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: This recipe will be mostly the same as the *Training a GRU* recipe on the IMDb
    dataset; it will only change the maximum length of the sequence. Since the most
    significant differences are the sequence length value and the results, we will
    assume the four first steps of *Training a GRU* (from the imports to the dataset
    implementation) have been run, and we will reuse some of the code. Instantiate
    the `TextClassificationDataset` objects for the train and test sets (reusing the
    class implemented in *Training a GRU*), as well as the related data loaders.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'This time, we chose a maximum number of words of `256`, significantly higher
    than the earlier value of `64`. We will keep a batch size of `64`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE212]'
  prefs: []
  type: TYPE_PRE
- en: 'Instantiate the GRU model by reusing the `GRUClassifier` class implemented
    in *Training a GRU*, with an embedding dimension and a hidden dimension of `32`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE213]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE214]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE215]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE216]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE217]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE218]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE219]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE220]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE221]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE222]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE223]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE224]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the optimizer as an Adam optimizer, with a learning rate of `0.001`.
    The loss is defined as the binary cross-entropy loss, since this is a binary classification
    task:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE225]'
  prefs: []
  type: TYPE_PRE
- en: 'Train the model over 20 epochs reusing the `train_IMDB_classification` helper
    function implemented in the *Training a GRU* recipe; store the accuracy and the
    loss for both the train and test sets for each epoch:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE226]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE227]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE228]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE229]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'After 20 epochs, the output looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE230]'
  prefs: []
  type: TYPE_PRE
- en: 'Plot the loss as a function of the epoch number for both the train and test
    sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE231]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE232]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE233]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE234]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.12 – Binary cross-entropy loss as a function of the epoch](img/B19629_08_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.12 – Binary cross-entropy loss as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: We can see that there is overfitting after only a few epochs.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, plot the accuracy as a function of the epoch number of both the train
    and test sets:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE235]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE236]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE237]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE238]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This is what we get:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.13 – Accuracy as a function of the epoch](img/B19629_08_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.13 – Accuracy as a function of the epoch
  prefs: []
  type: TYPE_NORMAL
- en: The test accuracy reaches a maximum after a few epochs and then slowly decreases.
  prefs: []
  type: TYPE_NORMAL
- en: Although there is still a large overfitting effect, compared to the training
    with a maximum length of 64, the test accuracy went from a maximum of 77% to a
    maximum of 87%, a significant improvement.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Instead of blindly choosing the maximum number of tokens, it might be interesting
    to first quickly analyze the length distribution.
  prefs: []
  type: TYPE_NORMAL
- en: 'For relatively small datasets, it’s easy to compute the length of all samples;
    let’s do it with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE239]'
  prefs: []
  type: TYPE_PRE
- en: 'We can now plot the distribution of the review lengths with a histogram, using
    a log scale:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE240]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 8.14 – A histogram of the review length of the IMDb dataset, in a
    log scale](img/B19629_08_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 8.14 – A histogram of the review length of the IMDb dataset, in a log
    scale
  prefs: []
  type: TYPE_NORMAL
- en: 'We can see a peak of around 300 tokens in length, and almost no review has
    more than 1,500 tokens. As we can see from the histogram, most of the reviews
    seem to have a length of a couple of hundred tokens. We can also compute the average
    and median length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE241]'
  prefs: []
  type: TYPE_PRE
- en: 'The computed average and median values are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE242]'
  prefs: []
  type: TYPE_PRE
- en: As a result, the average length is about 309, and the median length is 231\.
    According to this information, if the computational power allows it and depending
    on the task, choosing a maximum length of 256 seems to be a good first choice.
  prefs: []
  type: TYPE_NORMAL
