<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer067">
<h1 class="chapter-number" id="_idParaDest-140"><a id="_idTextAnchor137"/>9</h1>
<h1 id="_idParaDest-141"><a id="_idTextAnchor138"/>Implementing Model Servers</h1>
<p>In <a href="B17519_08.xhtml#_idTextAnchor121"><em class="italic">Chapter 8</em></a>, <em class="italic">Considering Hardware for Inference</em>, we discussed hardware options and optimizations for serving DL models that are available to you as part of the Amazon SageMaker platform. In this chapter, we will focus on another important aspect of engineering inference workloads – choosing and configuring model servers.</p>
<p>Model servers, similar to application servers for regular applications, provide a runtime context to serve your DL models. You, as a developer, deploy trained models to the model server, which exposes the deployed models as REST or gRPC endpoints. The end users of your DL models then send inference requests to established endpoints and receive a response with predictions. The model server can serve multiple end users simultaneously. It also provides configurable mechanisms to optimize inference latency and throughput to meet specific SLAs. </p>
<p>In <a href="B17519_01.xhtml#_idTextAnchor013"><em class="italic">Chapter 1</em></a>, <em class="italic">Introducing Deep Learning with Amazon SageMaker</em>, we discussed that Amazon SageMaker Managed Hosting has several mechanisms to deploy models: real-time inference endpoints, batch transform jobs, and asynchronous inference. In all these cases, you will need to select a model server to manage inference runtime and model deployment. However, model server configuration for these use cases will likely be different, since they have different inference traffic profiles and latency/throughput requirements.</p>
<p>Amazon SageMaker provides several model server solutions as part of its DL Inference Containers. In this chapter, we will focus on three popular model servers designed to productionalize DL inference workloads: <strong class="bold">TensorFlow Serving</strong> (<strong class="bold">TFS</strong>), <strong class="bold">PyTorch</strong> <strong class="bold">TorchServe</strong> (<strong class="bold">PTS</strong>), and <strong class="bold">NVIDIA Triton</strong>. </p>
<p>In this chapter, we will cover the following topics:</p>
<ul>
<li>Using TFS</li>
<li>Using PTS</li>
<li>Using NVIDIA Triton</li>
</ul>
<p>After reading this chapter, you will know how to deploy your TensorFlow and PyTorch models and configure your model servers for your inference requirements. We will also discuss the functional limitations of using model servers as part of SageMaker Managed Hosting.</p>
<h1 id="_idParaDest-142"><a id="_idTextAnchor139"/>Technical requirements</h1>
<p>In this chapter, we will provide code samples so that you can develop practical skills. The full code examples are available here: <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter9/">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter9/</a>.</p>
<p>To follow along with this code, you will need the following:</p>
<ul>
<li>An AWS account and IAM user with permission to manage Amazon SageMaker resources.</li>
<li>Have a SageMaker Notebook, SageMaker Studio Notebook, or local SageMaker-compatible environment established.</li>
<li>Access to GPU training instances in your AWS account. Each example in this chapter will provide the recommended instance types for you to use. You may need to increase your compute quota for <em class="italic">SageMaker Training Job</em> to have GPU instances enabled. In this case, please follow the instructions at <a href="https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml">https://docs.aws.amazon.com/sagemaker/latest/dg/regions-quotas.xhtml</a>.</li>
<li>You must install the required Python libraries by running <strong class="source-inline">pip install -r requirements.txt</strong>. The file that contains the required libraries can be found in the <strong class="source-inline">chapter9</strong> directory.</li>
<li>In this chapter, we will provide examples of compiling models for inference, which requires access to specific accelerator types. Please review the instance recommendations as part of the model server examples.</li>
</ul>
<h1 id="_idParaDest-143"><a id="_idTextAnchor140"/>Using TFS</h1>
<p><strong class="bold">TFS</strong> is a native <a id="_idIndexMarker722"/>model server for <a id="_idIndexMarker723"/>TensorFlow 1, TensorFlow 2, and Keras models. It is designed to provide a flexible and high-performance runtime environment with an extensive management API and operational features (such as logging and metrics). AWS provides TFS as part of TensorFlow inference<a id="_idIndexMarker724"/> containers (<a href="https://github.com/aws/deep-learning-containers/tree/master/tensorflow/inference/docker">https://github.com/aws/deep-learning-containers/tree/master/tensorflow/inference/docker</a>). </p>
<h2 id="_idParaDest-144"><a id="_idTextAnchor141"/>Reviewing TFS concepts</h2>
<p>TFS has a<a id="_idIndexMarker725"/> concept known as <strong class="bold">servable</strong> that <a id="_idIndexMarker726"/>encapsulates all model and code assets required for inference. To prepare servable for TFS serving, you need to package the trained model into <strong class="bold">SavedModel</strong> format. A <a id="_idIndexMarker727"/>SavedModel contains a complete TensorFlow program, including trained parameters and computation. It does not require the original model building code to run, which makes it useful for sharing or deploying across the TFS ecosystem (for example, using TFLite, TensorFlow.js, or TFS). You can package more than one model as well as specific model lookups or embeddings in a single servable. </p>
<p>TFS loads and exposes your servable via REST or gRPC endpoints. The Server API defines a list of endpoints to perform classification and regression inference. Additionally, each servable has an<a id="_idIndexMarker728"/> associated <strong class="bold">signature</strong> that defines the input and output tensors for your model, as well as the model type (regression or classification). Many common models have standard signatures that depend on the type of task (for example, image classification, object detection, text classification, and so on). TFS allows you to have custom signatures as well.</p>
<h2 id="_idParaDest-145"><a id="_idTextAnchor142"/>Integrating TFS with SageMaker</h2>
<p>Amazon<a id="_idIndexMarker729"/> SageMaker <a id="_idIndexMarker730"/>provides a managed hosting environment where you can manage your inference endpoints with uniform management and invocation APIs, regardless of the underlying model server. This approach sets certain limitations on native model server functionality. In this section, we will review how SageMaker integrates with TFS and the limitations you should be aware of.</p>
<p>When deploying TFS on SageMaker, you will not have access to TFS’s native management API to manage your servable life cycle (loading and unloading models, promoting the model version, and more). Also, you will not have direct access to the TFS Serving API. Instead, you will need to call your SageMaker endpoint using the standard SageMaker invocation interface. Then, the SageMaker HTTP server (a part of the DL TFS container) translates your requests into TFS format and passes them to the TFS Serving APIs. Note that you can provide custom pre-processing, prediction, and post-processing logic in your inference script. SageMaker supports both the REST and gRPC serving APIs. The following diagram shows this TFS integration:</p>
<div>
<div class="IMG---Figure" id="_idContainer064">
<img alt="Figure 9.1 – TFS integration with SageMaker Managed Hosting " height="304" src="image/B17519_09_001.jpg" width="783"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.1 – TFS integration with SageMaker Managed Hosting</p>
<p>There are several<a id="_idIndexMarker731"/> things <a id="_idIndexMarker732"/>to keep in mind when working with TFS on SageMaker:</p>
<ul>
<li>As mentioned previously, SageMaker doesn’t allow you to access the TFS Management API. However, it does allow to you provide the configuration of TFS via environmental variables.</li>
<li>SageMaker supports hosting multiple models with TFS. For this, you need to prepare separate servables for each model and then create a multi-model archive. </li>
<li>You can use REST headers and a request body to specify which models TFS should use to serve specific requests. For instance, the following request tells TFS to use <strong class="source-inline">model2</strong> to serve this request:<p class="source-code">aws sagemaker-runtime invoke-endpoint \</p><p class="source-code">    --endpoint-name my-endpoint \</p><p class="source-code">    --content-type 'application/json' \</p><p class="source-code">    --body '{"instances": [1.0, 2.0, 5.0]}' \</p><p class="source-code">    --custom-attributes 'tfs-model-name=other_model</p></li>
</ul>
<p>SageMaker supports default TFS input and output formats for your inference requests. Additionally, SageMaker also supports application/JSON, text/CSV, and application/JSON Lines.</p>
<p>Note that once you deploy the endpoint with the TFS model server, you won’t be able to directly change the TFS configuration or served models. For this, you will need to use the SageMaker Management API to create a new endpoint or endpoint variant with the desired configuration. We<a id="_idIndexMarker733"/> will <a id="_idIndexMarker734"/>discuss managing SageMaker inference resources in production in <a href="B17519_10.xhtml#_idTextAnchor154"><em class="italic">Chapter 10</em></a>, <em class="italic">Operationalizing Inference Workloads</em>.</p>
<h2 id="_idParaDest-146"><a id="_idTextAnchor143"/>Optimizing TFS </h2>
<p>TFS provides a set of mechanisms to optimize your model serving based on your requirements, the<a id="_idIndexMarker735"/> runtime environment, and available hardware resources. It implies that TFS tuning is use-case-specific and typically requires testing and benchmarking to achieve desired performance. In this section, we will review several mechanisms that you can use to tune TFS performance.</p>
<h3>Using TFS batching</h3>
<p>TFS supports <a id="_idIndexMarker736"/>automatic batching, where you can put several inference requests in a single batch. This can improve your server throughput, especially when using GPU instances (remember, GPUs are very good for parallel computations). How you configure batching will be different depending on the type of hardware device. TFS supports different batching schedules for different servables. </p>
<p>To configure TFS batching on SageMaker, you can use the following environment variables:</p>
<ul>
<li><strong class="source-inline">SAGEMAKER_TFS_ENABLE_BATCHING</strong> to enable the TFS batching feature. This defaults to <strong class="source-inline">false</strong>, which means that batching is not enabled.</li>
<li><strong class="source-inline">SAGEMAKER_TFS_MAX_BATCH_SIZE</strong> defines the maximum size of the batch. This defaults to <strong class="source-inline">8</strong>.</li>
<li><strong class="source-inline">SAGEMAKER_TFS_BATCH_TIMEOUT_MICROS</strong> defines how long to wait to accumulate a full batch in microseconds. This defaults to <strong class="source-inline">1000</strong>.</li>
<li><strong class="source-inline">SAGEMAKER_TFS_NUM_BATCH_THREADS</strong> sets how many batches to process simultaneously. This defaults to the number of instance CPUs.</li>
<li><strong class="source-inline">SAGEMAKER_TFS_MAX_ENQUEUED_BATCHES</strong> defines how many batches can be enqueued at the same time.</li>
</ul>
<p>You can review the<a id="_idIndexMarker737"/> detailed documentation on the TFS batching feature here: <a href="https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md">https://github.com/tensorflow/serving/blob/master/tensorflow_serving/batching/README.md</a>.</p>
<h3>Using the gRPC serving API </h3>
<p>As discussed<a id="_idIndexMarker738"/> earlier, TFS supports two<a id="_idIndexMarker739"/> types of APIs: REST and gRPC. While both APIs have the same functionality, the gRPC API typically has better performance due to the use of HTTP/2 network protocol and more efficient payload representations via the ProtoBuf format.</p>
<p>While the SageMaker Invocation API only supports the REST API, you can still use gRPC for inter-container communication between SageMaker’s HTTP frontend server and TFS (refer to <em class="italic">Figure 9.1</em> for an illustration of this). Note that in this case, you will need to provide some code to translate the SageMaker payload into gRPC format and send it to TFS. However, even in this case, AWS reports a decrease in the overall latency by at least 75% for image classification tasks. Refer to this article for details: <a href="https://aws.amazon.com/blogs/machine-learning/reduce-compuer-vision-inference-latency-using-grpc-with-tensorflow-serving-on-amazon-sagemaker/">https://aws.amazon.com/blogs/machine-learning/reduce-compuer-vision-inference-latency-using-grpc-with-tensorflow-serving-on-amazon-sagemaker/</a>. The performance benefits will vary based on the model and payload size. </p>
<h3>Configuring resource utilization with TFS</h3>
<p>TFS provides<a id="_idIndexMarker740"/> the following parameters for configuring hardware resources allocation:</p>
<ul>
<li><strong class="source-inline">SAGEMAKER_TFS_INSTANCE_COUNT</strong> defines how many instances of the TFS serving process will be spawned. Changing this parameter may increase your CPU and GPU utilization and ultimately improve your latency/throughput characteristics.</li>
<li><strong class="source-inline">SAGEMAKER_TFS_FRACTIONAL_GPU_MEM_MARGIN</strong> defines the fraction of GPU memory available to initialize the CUDA/cuDNN library. The remaining memory will be distributed equally between TFS processes.</li>
<li><strong class="source-inline">SAGEMAKER_TFS_INTER_OP_PARALLELISM</strong> determines how many threads are used when running independent non-blocking compute operations in your model graphs.</li>
<li><strong class="source-inline">SAGEMAKER_TFS_INTRA_OP_PARALLELISM</strong> determines how many threads are <a id="_idIndexMarker741"/>used when running operations that can be parallelized interally. </li>
</ul>
<p>Now, let’s review how we can use TFS on SageMaker using a practical example.</p>
<h2 id="_idParaDest-147"><a id="_idTextAnchor144"/>Implementing TFS serving</h2>
<p>In this example, we<a id="_idIndexMarker742"/> will take one of the pre-trained models from TensorFlow Hub, convert it into <strong class="bold">SavedModel</strong> format, and then package it with the custom inference for deployment on SageMaker. We will review how we can use both the REST and gRPC APIs and how to define the TFS configuration when it’s deployed on SageMaker Managed Hosting. For this task, we will use the popular EfficientNetV2 model architecture to classify images.</p>
<p>The full code is available here: <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter9/1_TensorFlow_Serving.ipynb">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter9/1_TensorFlow_Serving.ipynb</a>.</p>
<h3>Preparing the training model</h3>
<p>We will start by loading the model artifacts from TensorFlow Hub. You can read about the EfficientNetV2 model on its model page here: <a href="https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2">https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2</a>. To download the model, we can use the TensorFlow Hub API, as shown in the following code block:</p>
<pre class="source-code">
import tensorflow as tf
import tensorflow_hub as hub
model_handle = "<a href="https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2">https://tfhub.dev/google/imagenet/efficientnet_v2_imagenet1k_s/classification/2</a>"
classifier = hub.load(model_handle)</pre>
<p>This model expects a dense 4D tensor of the <strong class="source-inline">float32</strong> dtype with a shape of <strong class="source-inline">[batch, height, weight, color]</strong>, where <strong class="source-inline">height</strong> and <strong class="source-inline">weight</strong> have a fixed length of <strong class="source-inline">384</strong>, and <strong class="source-inline">color</strong> has a length of <strong class="source-inline">3</strong>. <strong class="source-inline">batch</strong> can be variable.</p>
<p>To test the model locally, you need to convert the image (or a batch of images) into the expected 4D tensor, run it through the model, and apply the <strong class="source-inline">softmax</strong> function to get the label probabilities, as shown here:</p>
<pre class="source-code">
probabilities = tf.nn.softmax(classifier(image)).numpy()</pre>
<p>Now that we have <a id="_idIndexMarker743"/>performed smoke testing on the model, we need to package it in SageMaker/TFS-compatible formats.</p>
<h3>Packaging the model artifacts</h3>
<p>As discussed<a id="_idIndexMarker744"/> earlier, TFS expects your model to be converted into SavedModel format. Additionally, SageMaker expects the model artifact to be packaged into a <strong class="source-inline">tar.gz</strong> archive with the following structure:</p>
<pre class="source-code">
    model1
        |--[model_version_number]
            |--variables
            |--saved_model.pb
    model2
        |--[model_version_number]
            |--assets
            |--variables
            |--saved_model.pb
    code
        |--inference.py
        |--requirements.txt</pre>
<p>The following code creates the appropriate directory structure and exports the trained model in SavedModel format:</p>
<pre class="source-code">
model_name = "efficientnetv2-s"
model_dir = f"./{model_name}/1"
code_dir = f"./{model_name}/code"
os.makedirs(model_dir, exist_ok=False)
os.makedirs(code_dir, exist_ok=False)
tf.saved_model.save(classifier, model_dir)</pre>
<p>Note that in our example, we will only use a single version of a single model. Next, we need to prepare<a id="_idIndexMarker745"/> an inference script for preprocessing, running predictions, and postprocessing between the SageMaker HTTP frontend and the TFS server.</p>
<h3>Developing the inference code</h3>
<p>SageMaker expects <a id="_idIndexMarker746"/>your processing code to be named <strong class="source-inline">inference.py</strong> and placed in the <strong class="source-inline">/code</strong> directory in the model archive. Our inference code needs to implement either the <strong class="source-inline">input_handler()</strong> and <strong class="source-inline">output_handler()</strong> functions or a single <strong class="source-inline">handler()</strong> function. In our case, we have chosen to implement a single <strong class="source-inline">handler()</strong> method to process incoming requests and send it to the appropriate TFS API:</p>
<pre class="source-code">
def handler(data, context):
    if context.request_content_type == "application/json":
        instance = json.loads(data.read().decode("utf-8"))
    else:
        raise ValueError(
            415,
            'Unsupported content type "{}"'.format(
                context.request_content_type or "Unknown"
            ),
        )
    if USE_GRPC:
        prediction = _predict_using_grpc(context, instance)
    else:
        inst_json = json.dumps({"instances": instance})
        response = requests.post(context.rest_uri, data=inst_json)
        if response.status_code != 200:
            raise Exception(response.content.decode("utf-8"))
        prediction = response.content
    response_content_type = context.accept_header
    return prediction, response_content_type</pre>
<p>As you can see, depending on<a id="_idIndexMarker747"/> whether we want to use the gRCP API or the REST API, the processing and prediction code will be slightly different. Note that the <strong class="source-inline">context</strong> <strong class="source-inline">namedtuple</strong> object provides necessary details about the TFS configuration, such as the endpoint path and ports, model name and version, and more. </p>
<p>If we choose to use the TFS REST API, we need to convert the incoming request into the expected TFS format, serialize it into JSON, and then generate a POST request.</p>
<p>To use the gRPC API, we will need to convert the incoming REST payload into a <strong class="source-inline">protobuf</strong> object. For this, we will use the following helper function:</p>
<pre class="source-code">
from tensorflow_serving.apis import predict_pb2
from tensorflow_serving.apis import prediction_service_pb2_grpc
def _predict_using_grpc(context, instance):
    grpc_request = predict_pb2.PredictRequest()
    grpc_request.model_spec.name = "model"
    grpc_request.model_spec.signature_name = "serving_default"
    options = [
        ("grpc.max_send_message_length", MAX_GRPC_MESSAGE_LENGTH),
        ("grpc.max_receive_message_length", MAX_GRPC_MESSAGE_LENGTH),
    ]
    channel = grpc.insecure_channel(f"0.0.0.0:{context.grpc_port}", options=options)
    stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)
grpc_request.inputs["input_1"].CopyFrom(tf.make_tensor_proto(instance))
    result = stub.Predict(grpc_request, 10)
    output_shape = [dim.size for dim in result.outputs["output_1"].tensor_shape.dim]
    np_result = np.array(result.outputs["output_1"].float_val).reshape(output_shape)
    return json.dumps({"predictions": np_result.tolist()})</pre>
<p>Here, we use<a id="_idIndexMarker748"/> the <strong class="source-inline">prediction_service_pb2()</strong> and <strong class="source-inline">predict_pb2()</strong> TFS methods to communicate with the gRPC API.  Here, the <strong class="source-inline">stub</strong> object converts parameters during the RPC. The <strong class="source-inline">grpc_request</strong> object defines what TFS API to invoke and call the parameters.</p>
<p>To choose what TFS API to call, we implemented a simple mechanism that allows you to provide the <strong class="source-inline">USE_GRPC</strong> environment variable via a SageMaker Model object:</p>
<pre class="source-code">
USE_GRPC = True if os.getenv("USE_GRPC").lower() == "true" else False </pre>
<p>Once we have our <strong class="source-inline">inference.py</strong> code ready, we can add it to the model package and create a <strong class="source-inline">tar.gz</strong> model archive. This can be done by running the following Bash code from a Jupyter notebook:</p>
<pre class="source-code">
! cp 1_src/inference.py $code_dir
! cp 1_src/requirements.txt $code_dir
! tar -C "$PWD" -czf model.tar.gz  efficientnetv2-s/</pre>
<p>Now, our model has <a id="_idIndexMarker749"/>been packaged according to TFS and SageMaker requirements and we are ready to deploy it. </p>
<h3>Deploying the TFS model</h3>
<p>To deploy the TFS<a id="_idIndexMarker750"/> model, follow these steps:</p>
<ol>
<li>We will start by uploading our model archive to Amazon S3 so that SageMaker can download it to the serving container at deployment time. We can use a SageMaker <strong class="source-inline">Session()</strong> object to do this:<p class="source-code">import sagemaker</p><p class="source-code">from sagemaker import get_execution_role</p><p class="source-code">sagemaker_session = sagemaker.Session()</p><p class="source-code">role = get_execution_role() </p><p class="source-code">bucket = sagemaker_session.default_bucket()</p><p class="source-code">prefix = 'tf-serving'</p><p class="source-code">s3_path = 's3://{}/{}'.format(bucket, prefix)</p><p class="source-code">model_data = sagemaker_session.upload_data('model.tar.gz',</p><p class="source-code">                                           bucket,</p><p class="source-code">                                           os.path.join(prefix, 'model'))</p></li>
<li>Then, we can use the SageMaker SDK TensorFlowModel object to configure the TFS environment. Note that we are providing the TFS configuration via the <strong class="source-inline">env</strong> dictionary:<p class="source-code">from sagemaker.tensorflow import TensorFlowModel</p><p class="source-code">env = {</p><p class="source-code">       "SAGEMAKER_TFS_ENABLE_BATCHING":"true",</p><p class="source-code">       "SAGEMAKER_TFS_MAX_BATCH_SIZE":"4",</p><p class="source-code">       "SAGEMAKER_TFS_BATCH_TIMEOUT_MICROS":"100000",</p><p class="source-code">       "SAGEMAKER_TFS_NUM_BATCH_THREADS":"6",</p><p class="source-code">       "SAGEMAKER_TFS_MAX_ENQUEUED_BATCHES":"6",</p><p class="source-code">       "USE_GRPC":"true" # to switch between TFS REST and gRCP API</p><p class="source-code">       }</p><p class="source-code">tensorflow_serving_model = TensorFlowModel(model_data=model_data,</p><p class="source-code">                                 name="efficientnetv2-1",</p><p class="source-code">                                 role=role,</p><p class="source-code">                                 framework_version='2.8',</p><p class="source-code">                                 env=env,</p><p class="source-code">sagemaker_session=sagemaker_session)</p></li>
</ol>
<p>Once the model has <a id="_idIndexMarker751"/>been configured, we are ready to deploy the endpoint. Here, we will use one of the GPU instances, but you can experiment with CPU instances as well. </p>
<p>Before we can run predictions, we need to convert the image (or several images) into a 4D TFS tensor and then convert it into a NumPy <strong class="source-inline">ndarray</strong> that the <strong class="source-inline">.predict()</strong> method knows how to serialize into the application/JSON content type. A sample method to process images into TFS format has been provided in the sample notebook. </p>
<p>In the following code, we are running predictions and then mapping the resulting softmax scores to labels:</p>
<pre class="source-code">
response_remote = predictor.predict(image.numpy())
probabilities = np.array(response_remote['predictions'])
top_5 = tf.argsort(probabilities, axis=-1, direction="DESCENDING")[0][:5].numpy()
np_classes = np.array(classes)
# Some models include an additional 'background' class in the predictions, so
# we must account for this when reading the class labels.
includes_background_class = probabilities.shape[1] == 1001
for i, item in enumerate(top_5):
  class_index = item if includes_background_class else item + 1
  line = f'({i+1}) {class_index:4} - {classes[class_index]}: {probabilities[0][top_5][i]}'
  print(line)</pre>
<p>After running this <a id="_idIndexMarker752"/>code, you should have an output that contains labels and their normalized probabilities.</p>
<p>In this section, we reviewed how to use the TFS model server on Amazon SageMaker. TFS is a highly configurable production-grade model server that should be considered a great candidate when it comes to hosting TensorFlow models. We also discussed some implementation specifics of Sagemaker/TFS integration that should be accounted for when engineering your model server. Once you have your TensorFlow model(s) running on SageMaker, it’s recommended to perform benchmarking and tune the TFS configuration based on your specific use case requirements.</p>
<p>In the next section, we will review the native model server for PyTorch models – TorchServe.</p>
<h1 id="_idParaDest-148"><a id="_idTextAnchor145"/>Using PTS </h1>
<p><strong class="bold">PTS</strong> is a <a id="_idIndexMarker753"/>native model server for PyTorch<a id="_idIndexMarker754"/> models. PTS was developed in collaboration between Meta and AWS to provide a production-ready model server for the PyTorch ecosystem. It allows you to serve and manage multiple models and serve requests via REST or gRPC endpoints. PTS supports serving TorchScripted models for better inference performance. It also comes with utilities to collect logs and metrics and optimization tweaks. SageMaker supports PTS as part of PyTorch inference containers (<a href="https://github.com/aws/deep-learning-containers/tree/master/pytorch/inference/docker">https://github.com/aws/deep-learning-containers/tree/master/pytorch/inference/docker</a>).</p>
<h2 id="_idParaDest-149"><a id="_idTextAnchor146"/>Integration with SageMaker </h2>
<p>PTS is a default <a id="_idIndexMarker755"/>model <a id="_idIndexMarker756"/>server for PyTorch models on Amazon SageMaker. Similar to TFS, SageMaker doesn’t expose native PTS APIs to end users for model management and inference. The following diagram shows how to integrate SageMaker and PTS:</p>
<div>
<div class="IMG---Figure" id="_idContainer065">
<img alt="Figure 9.2 – PTS architecture on SageMaker " height="724" src="image/B17519_09_002.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.2 – PTS architecture on SageMaker</p>
<p>Let’s highlight these integration details:</p>
<ul>
<li>SageMaker supports a limited number of PTS configs out of the box. If you need to have more flexibility with your PTS configuration, you may need to extend the SageMaker PyTorch Inference container. Alternatively, you can package the PTS configs as part of your model package and provide the path to it via the <strong class="source-inline">TS_CONFIG_FILE</strong> environment variable. However, with the latter approach, you won’t be able to manipulate all the settings (for example, the JVM config).</li>
<li>PTS requires you to package model artifacts and handler code into a MAR archive. SageMaker has slightly different requirements regarding the model archive, which we will discuss in the following code example.</li>
<li>SageMaker supports hosting multiple models at the same time. For this, you need to set the <strong class="source-inline">ENABLE_MULTI_MODEL</strong> environment variable to <strong class="source-inline">true</strong> and package your models into a single archive.</li>
</ul>
<p>SageMaker<a id="_idIndexMarker757"/> provides a <a id="_idIndexMarker758"/>mechanism to configure PTS via endpoint environmental variables. Let’s review the available config parameters.</p>
<h2 id="_idParaDest-150"><a id="_idTextAnchor147"/>Optimizing PTS on SageMaker</h2>
<p>PTS supports <a id="_idIndexMarker759"/>two primary <a id="_idIndexMarker760"/>mechanisms for performance optimization: server-side batching and spawning multiple model threads. These settings can be configured via the following environmental variables:</p>
<ul>
<li><strong class="source-inline">SAGEMAKER_TS_BATCH_SIZE</strong> to set the maximum size of server-side batches.</li>
<li><strong class="source-inline">SAGEMAKER_TS_MAX_BATCH_DELAY</strong> to set the maximum delay that the server will wait to complete the batch in microseconds.</li>
<li><strong class="source-inline">SAGEMAKER_TS_RESPONSE_TIMEOUT</strong> sets the time delay for a timeout in seconds if an inference response is not available. </li>
<li><strong class="source-inline">SAGEMAKER_TS_MIN_WORKERS</strong> and <strong class="source-inline">SAGEMAKER_TS_MAX_WORKERS</strong> configure the minimum and the maximum number of model worker threads on CPU or GPU devices, respectively. You can read some of the considerations on setting up these in the PyTorch documentation at <a href="https://github.com/pytorch/serve/blob/master/docs/performance_guide.md">https://github.com/pytorch/serve/blob/master/docs/performance_guide.md</a>.</li>
</ul>
<p>Additionally, PTS supports inference profiling using the PyTorch TensorBoard plugin, which we discussed in <a href="B17519_07.xhtml#_idTextAnchor110"><em class="italic">Chapter 7</em></a>, <em class="italic">Operationalizing Deep Learning Training</em>. This plugin allows <a id="_idIndexMarker761"/>you to profile your <a id="_idIndexMarker762"/>PyTorch inference code and identify potential bottlenecks.</p>
<h2 id="_idParaDest-151"><a id="_idTextAnchor148"/>Serving models with PTS</h2>
<p>Let’s review<a id="_idIndexMarker763"/> how to deploy PyTorch models <a id="_idIndexMarker764"/>using PTS on SageMaker. We will use the Distilbert model that has been trained on the Q&amp;A NLP task from HuggingFace Models. The sample code is available here: <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter9/2_PyTorch_Torchserve.ipynb">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter9/2_PyTorch_Torchserve.ipynb</a>.</p>
<h3>Packaging the model for PTS on SageMaker</h3>
<p>When using<a id="_idIndexMarker765"/> the PTS model server<a id="_idIndexMarker766"/> on SageMaker, you may <a id="_idIndexMarker767"/>choose to use one of two options:</p>
<ul>
<li>Deploy your model using the <strong class="source-inline">PyTorchModel</strong> class from the Python SageMaker SDK. In this case, your model archive needs to provide only the necessary model artifacts (for example, model weights, lookups, tokenizers, and so on). As part of the <strong class="source-inline">PyTorchModel</strong> object configuration, you will provide your inference code and other dependencies, and SageMaker will automatically package it for PTS.</li>
<li>You can also package your model along with the inference code in a single archive. While this approach requires some additional work, it allows you to create a model package and deploy models without using the SageMaker SDK. SageMaker expects the following directory structure in this case:<p class="source-code">model.tar.gz/</p><p class="source-code">|- model_weights.pth</p><p class="source-code">|- other_model_artifacts</p><p class="source-code">|- code/</p><p class="source-code">  |- inference.py</p><p class="source-code">  |- requirements.txt  # optional</p></li>
</ul>
<p>In this example, we will use the first option: </p>
<ol>
<li value="1">The following Bash script will download the required HuggingFace model artifacts and <a id="_idIndexMarker768"/>package<a id="_idIndexMarker769"/> them <a id="_idIndexMarker770"/>into a single <strong class="source-inline">tar.gz archive</strong>:<p class="source-code">mkdir distilbert-base-uncased-distilled-squad</p><p class="source-code">wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/pytorch_model.bin -P distilbert-base-uncased-distilled-squad</p><p class="source-code">wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer.json -P distilbert-base-uncased-distilled-squad</p><p class="source-code">wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/tokenizer_config.json -P distilbert-base-uncased-distilled-squad</p><p class="source-code">wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/vocab.txt -P distilbert-base-uncased-distilled-squada</p><p class="source-code">wget https://huggingface.co/distilbert-base-uncased-distilled-squad/resolve/main/config.json -P distilbert-base-uncased-distilled-squad</p><p class="source-code">tar -C "$PWD" -czf distilbert-base-uncased-distilled-squad.tar.gz  distilbert-base-uncased-distilled-squad/</p></li>
<li>Then, we need to upload the model archive to Amazon S3 using the following code:<p class="source-code">import sagemaker</p><p class="source-code">from sagemaker import get_execution_role</p><p class="source-code">sagemaker_session = sagemaker.Session()</p><p class="source-code">role = get_execution_role()</p><p class="source-code">bucket = sagemaker_session.default_bucket()</p><p class="source-code">prefix = 'torchserve'</p><p class="source-code">s3_path = 's3://{}/{}'.format(bucket, prefix)</p><p class="source-code">model_data = sagemaker_session.upload_data('distilbert-base-uncased-distilled-squad.tar.gz',bucket,os.path.join(prefix, 'model-artifacts'))</p></li>
</ol>
<p>Next, we need to prepare some code to load models from the uploaded model artifacts and perform<a id="_idIndexMarker771"/> inference and data <a id="_idIndexMarker772"/>processing. This code is <a id="_idIndexMarker773"/>called the <strong class="bold">inference handler</strong> in PTS terminology.</p>
<h3>Preparing the inference handler</h3>
<p>SageMaker<a id="_idIndexMarker774"/> requires you to provide <a id="_idIndexMarker775"/>some code to load the model and run predictions so that you can preprocess incoming inference requests and post-process the response. To perform these operations, you need to implement the <strong class="source-inline">model_fn()</strong>, <strong class="source-inline">predict_fn()</strong>, <strong class="source-inline">input_fn()</strong>, and <strong class="source-inline">output_fn()</strong> methods. You can find implementations of the inference handler using the HuggingFace Pipeline API here: <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter9/2_src/pipeline_predictor.py">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter9/2_src/pipeline_predictor.py</a>.</p>
<h3>Deploying the model to a SageMaker endpoint</h3>
<p>Deploying the <a id="_idIndexMarker776"/>model on <a id="_idIndexMarker777"/>PTS using the SageMaker<a id="_idIndexMarker778"/> SDK is straightforward. To configure PTS, we can use the <strong class="source-inline">"env"</strong> dictionary to set the appropriate environment variables in the serving container. Note that here, we explicitly reference the inference code via the <strong class="source-inline">"entry_point"</strong> parameter. Follow these steps:</p>
<ol>
<li value="1">As a prerequisite, you can add any other dependencies (for example, custom libraries or <strong class="source-inline">requirements.txt</strong>) to the <strong class="source-inline">"source_dir"</strong> location. The SageMaker SDK will automatically merge these assets with the model data into the MAR archive required by PTS:<p class="source-code">from sagemaker.pytorch import PyTorchModel</p><p class="source-code">env = {</p><p class="source-code">    "SAGEMAKER_TS_BATCH_SIZE": "2",</p><p class="source-code">    "SAGEMAKER_TS_MAX_BATCH_DELAY": "1000",</p><p class="source-code">    "SAGEMAKER_TS_RESPONSE_TIMEOUT" : "120",</p><p class="source-code">    "SAGEMAKER_TS_MIN_WORKERS" : "1",</p><p class="source-code">    "SAGEMAKER_TS_MAX_WORKERS" : "2"</p><p class="source-code">    }</p><p class="source-code">model = PyTorchModel(model_data=model_data,</p><p class="source-code">                   role=role, </p><p class="source-code">                   entry_point='pipeline_predictor.py',</p><p class="source-code">                   source_dir='2_src',</p><p class="source-code">                   framework_version='1.9.0',</p><p class="source-code">                   py_version='py38',</p><p class="source-code">                   env=env,</p><p class="source-code">                   sagemaker_session=sagemaker_session)</p></li>
<li>Now, we <a id="_idIndexMarker779"/>can <a id="_idIndexMarker780"/>define<a id="_idIndexMarker781"/> the endpoint configuration and supported serializers and deserializers for the request/response pair:<p class="source-code">from sagemaker.serializers import JSONSerializer</p><p class="source-code">from sagemaker.deserializers import JSONDeserializer</p><p class="source-code">remote_predictor = model.deploy(initial_instance_count=1, instance_type="ml.g4dn.4xlarge", serializer=JSONSerializer(), deserializer=JSONDeserializer())</p></li>
<li>Now, we can run prediction by calling the <strong class="source-inline">.predict()</strong> method:<p class="source-code">remote_predictor.predict(data)</p></li>
<li>We can also confirm that our PTS configurations have been applied properly. For this, you can open your SageMaker endpoint log stream and search for a log line, as shown here:<p class="source-code">Model config: </p><p class="source-code">{ "model": { "1.0": { "defaultVersion": true, "marName": "model.mar", "minWorkers": 1, "maxWorkers": 2, "batchSize": 3, "maxBatchDelay": 100000, "responseTimeout": 120 } } }</p></li>
</ol>
<p>In this section, we discussed how PTS can be used to serve PyTorch models. In real production systems, you will probably prefer to convert your model into TorchScript format and further experiment with batching and worker scaling options to optimize your specific<a id="_idIndexMarker782"/> use <a id="_idIndexMarker783"/>case <a id="_idIndexMarker784"/>requirements. </p>
<p>In the next section, we will review a feature-rich framework-agnostic model server called NVIDIA Triton.</p>
<h1 id="_idParaDest-152"><a id="_idTextAnchor149"/>Using NVIDIA Triton </h1>
<p><strong class="bold">NVIDIA Triton</strong> is an <a id="_idIndexMarker785"/>open source model server developed by NVIDIA. It supports multiple DL frameworks (such as TensorFlow, PyTorch, ONNX, Python, and OpenVINO), as well various hardware platforms and runtime environments (NVIDIA GPUs, x86 and ARM CPUs, and AWS Inferentia). Triton can be used for inference in cloud and data center environments and edge or mobile devices. Triton is optimized for performance and scalability on various CPU and GPU platforms. NVIDIA provides a specialized utility for performance analysis and model analysis to improve Triton’s performance. </p>
<h2 id="_idParaDest-153"><a id="_idTextAnchor150"/>Integration with SageMaker</h2>
<p>You can use<a id="_idIndexMarker786"/> Triton model servers by utilizing a pre-built SageMaker DL container with it. Note that SageMaker Triton containers are not open source. You can find the latest list of Triton containers here: <a href="https://github.com/aws/deep-learning-containers/blob/master/available_images.md#nvidia-triton-inference-containers-sm-support-only">https://github.com/aws/deep-learning-containers/blob/master/available_images.md#nvidia-triton-inference-containers-sm-support-only</a>.</p>
<p>SageMaker doesn’t require you to provide inference custom code when deploying models on Triton. However, you will need to provide a Triton <strong class="source-inline">config.pbtxt</strong> file for each model you intend to serve. This config specifies the API contract for the inference request/response pair and other parameters on how the model needs to be served. You can review the possible configuration parameters by reading the official Triton documentation: <a href="https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md">https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md</a>. </p>
<p>Also, note that, unlike TFS and PTS, at the time of writing, SageMaker doesn’t support hosting multiple independent models on Triton. However, you can still have multiple versions of the same model or organize several models into a pipeline.</p>
<h2 id="_idParaDest-154"><a id="_idTextAnchor151"/>Optimizing Triton inference </h2>
<p>Triton provides<a id="_idIndexMarker787"/> several utilities to improve your performance:</p>
<ul>
<li><strong class="bold">Model Analyzer</strong> allows <a id="_idIndexMarker788"/>you to understand the GPU memory utilization of your models so that you can understand how to run multiple models on a single GPU</li>
<li><strong class="bold">Performance Analyzer</strong> allows you to analyze your Triton inference and throughput</li>
</ul>
<p>You won’t be able to run Performance Analyzer directly against SageMaker Triton Endpoint since the SageMaker inference API doesn’t match the Triton inference API. To bypass this limitation, you can run the Triton container locally on an instance of SageMaker Notebook with the target hardware accelerator and run an analysis against it.</p>
<p>Triton provides the <a id="_idIndexMarker789"/>following optimization features:</p>
<ul>
<li><strong class="bold">Dynamic batching</strong>: This puts multiple inference requests into a batch to increase Triton throughput. This feature is similar to the batching we discussed for TFS and PTS model servers.</li>
<li><strong class="bold">Model instances</strong>: This specifies how many copies of each model will be available for inference. By default, a single instance of the model is loaded. Having more than one copy of the model typically results in better latency/throughout as it allows you to overlap memory transfer operations (for example, CPU to/from GPU) with inference compute. Having multiple instances also allows you to use all the available GPU resources more efficiently.</li>
</ul>
<p>Both parameters can be configured via the <strong class="source-inline">config.pbtxt</strong> file. Let’s gain some practical experience in using Triton on SageMaker.</p>
<h2 id="_idParaDest-155"><a id="_idTextAnchor152"/>Serving models with Triton on SageMaker</h2>
<p>In this<a id="_idIndexMarker790"/> example, we will deploy the image classification PyTorch<a id="_idIndexMarker791"/> ResNet50 model using Triton. Our target hardware accelerator will be <strong class="source-inline">ml.g4dn</strong> instances. First, we need to compile the model to the TensorRT runtime; then, the compiled model will be packaged and deployed to the Triton model server. The sample code is available here: <a href="https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter9/3_NVIDIA_Triton_Server.ipynb">https://github.com/PacktPublishing/Accelerate-Deep-Learning-Workloads-with-Amazon-SageMaker/blob/main/chapter9/3_NVIDIA_Triton_Server.ipynb</a>.</p>
<p>Note that the model compilation process described in the following subsection is specific to the PyTorch framework. If you choose to use the TensorFlow model, your model compilation and configuration will be different. You can refer to the Triton TensorFlow backend repository for details: <a href="https://github.com/triton-inference-server/tensorflow_backend">https://github.com/triton-inference-server/tensorflow_backend</a>.</p>
<h3>Compiling the model for Triton</h3>
<p>There are several <a id="_idIndexMarker792"/>ways you can compile your eager PyTorch model into TensorRT format, such as by converting your PyTorch model into ONNX format. Another way is to use the PyTorch JIT compiler to convert your eager model into TorchScript format natively. Recently, the PyTorch and NVIDIA teams have implemented an optimized way to compile your PyTorch model into a TensorRT runtime using <a id="_idIndexMarker793"/>the <strong class="bold">Torch-TensorRT compiler</strong>. This approach has several advantages as it allows you to use TensorRT-specific optimizations such as the GP16 and INT8 reduced precision types and NVIDIA GPU weight sparsity:</p>
<div>
<div class="IMG---Figure" id="_idContainer066">
<img alt="Figure 9.3 – Compiling the PyTorch model using TensorRT-Torch " height="473" src="image/B17519_09_003.jpg" width="1640"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 9.3 – Compiling the PyTorch model using TensorRT-Torch</p>
<p>To compile the PyTorch <a id="_idIndexMarker794"/>model using TensorRT-Torch, we need two components:</p>
<ul>
<li>A runtime environment for compilation. It’s highly recommended to use NVIDIA’s latest PyTorch containers for this purpose. Note that you will need to run this container on an instance with an NVIDIA GPU available. For instance, you can run this sample on a SageMaker Notebook whose type is <strong class="source-inline">g4dn</strong>.</li>
<li>Compilation code. This code will be executed inside the NVIDIA PyTorch Docker container. </li>
</ul>
<p>Now, let’s review the compilation code:</p>
<ol>
<li value="1">We will start by loading the model from PyTorch Hub, setting it to evaluation mode, and placing it on the GPU device:<p class="source-code">import torch</p><p class="source-code">import torch_tensorrt</p><p class="source-code">import os</p><p class="source-code">torch.hub._validate_not_a_forked_repo = lambda a, b, c: True</p><p class="source-code">MODEL_NAME = "resnet50"</p><p class="source-code">MODEL_VERSION = "1"</p><p class="source-code">device = "cuda" if torch.cuda.is_available() else "cpu"</p><p class="source-code"># load model</p><p class="source-code">model = (torch.hub.load("pytorch/vision:v0.10.0", MODEL_NAME, pretrained=True).eval().to(device))</p></li>
<li>Next, we will compile it using the TensorRT-Torch compiler. As part of the compiler configuration, we will specify the expected inputs and target precision. Note that since we plan to use dynamic batching for our model, we will provide several input <a id="_idIndexMarker795"/>shapes with different values for the batch dimensions:<p class="source-code"># Compile with Torch TensorRT;</p><p class="source-code">trt_model = torch_tensorrt.compile(</p><p class="source-code">    model,</p><p class="source-code">    inputs=[</p><p class="source-code">        torch_tensorrt.Input(</p><p class="source-code">            min_shape=(1, 3, 224, 224),</p><p class="source-code">            opt_shape=(8, 3, 224, 224),</p><p class="source-code">            max_shape=(16, 3, 224, 224),</p><p class="source-code">            dtype=torch.float32,</p><p class="source-code">        )</p><p class="source-code">    ],</p><p class="source-code">    enabled_precisions={ torch.float32 },</p><p class="source-code">)</p></li>
<li>Finally, we will save our model to disk:<p class="source-code"># Save the model</p><p class="source-code">model_dir = os.path.join(os.getcwd(), "3_src", MODEL_NAME, MODEL_VERSION)</p><p class="source-code">os.makedirs(model_dir, exist_ok=True)</p><p class="source-code">print(model_dir)</p><p class="source-code">torch.jit.save(trt_model, os.path.join(model_dir, "model.pt"))</p></li>
<li>To execute this script, you need to start a Docker container with the <strong class="source-inline">docker run --gpus all --ipc=host --ulimit memlock=-1 --ulimit stack=67108864 -it --rm -v $PWD/chapter9/3_src:/workspace/3_src nvcr.io/nvidia/pytorch:22.05-py3</strong> command.</li>
<li>Your console session will open inside a container, where you can execute the compilation script by running the <strong class="source-inline">python 3_src/compile_tensorrt.py</strong> command.</li>
</ol>
<p>The <a id="_idIndexMarker796"/>resulting <strong class="source-inline">model.pt</strong> file will be available outside of the Docker container in the <strong class="source-inline">3_src</strong> directory.</p>
<h3>Preparing the model config</h3>
<p>Previously, we <a id="_idIndexMarker797"/>mentioned that Triton uses a configuration file with a specific convention to define model signatures and runtime configuration. The following code is for a <strong class="source-inline">config.pbtxt</strong> file that we can use to host the ResNet50 model. Here, we define batching parameters (the max batch size and dynamic batching config), input and output signatures, as well as model copies and the target hardware environment (via the <strong class="source-inline">instance_group</strong> object):</p>
<pre class="source-code">
name: "resnet50"
platform: "pytorch_libtorch"
max_batch_size : 128
input [
  {
    name: "input__0"
    data_type: TYPE_FP32
    dims: [ 3, 224, 224 ]
  }
]
output [
  {
    name: "output__0"
    data_type: TYPE_FP32
    dims: [ 1, 1000 ,1, 1]
  }
]
dynamic_batching {
   preferred_batch_size: 128
   max_queue_delay_microseconds: 1000
 }
instance_group {
  count: 1
  kind: KIND_GPU
}</pre>
<p>Refer to the <a id="_idIndexMarker798"/>Triton configuration for more details: <a href="https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md">https://github.com/triton-inference-server/server/blob/main/docs/user_guide/model_configuration.md</a>.</p>
<h3>Packaging the model artifacts</h3>
<p>To deploy the <a id="_idIndexMarker799"/>compiled model with its configuration, we need to bundle everything into a single <strong class="source-inline">tar.gz</strong> archive and upload it to Amazon S3. The following code shows the directory structure within the model archive:</p>
<pre class="source-code">
resnet50
|- 1
  |- model.pt
|- config.pbtxt</pre>
<p>Once the model <a id="_idIndexMarker800"/>package has been uploaded to Amazon S3, we can deploy our Triton endpoint.</p>
<h3>Deploying the Triton endpoint</h3>
<p>The Triton<a id="_idIndexMarker801"/> inference container is not supported by the SageMaker Python SDK. Hence, we will need to use the boto3 SageMaker client to deploy the model. Follow these steps:</p>
<ol>
<li value="1">First, we need to identify the correct Triton image. Use the following code to find the Triton container URI based on your version of the Triton server (we used <strong class="source-inline">22.05</strong> for both model compilation and serving) and your AWS region:<p class="source-code">account_id_map = {</p><p class="source-code"> # &lt;REDACTED_FOR_BREVITY&gt;</p><p class="source-code">}</p><p class="source-code">region = boto3.Session().region_name</p><p class="source-code">if region not in account_id_map.keys():</p><p class="source-code">    raise("UNSUPPORTED REGION")</p><p class="source-code">base = "amazonaws.com.cn" if region.startswith("cn-") else "amazonaws.com"</p><p class="source-code">triton_image_uri = "{account_id}.dkr.ecr.{region}.{base}/sagemaker-tritonserver:22.05-py3".format(</p><p class="source-code">    account_id=account_id_map[region], region=region, base=base)</p></li>
<li>Next, we can create the model, which defines the model data and serving container, as well as other parameters, such as environment variables:<p class="source-code">unique_id = time.strftime("%Y-%m-%d-%H-%M-%S", time.gmtime())</p><p class="source-code">sm_model_name = "triton-resnet50-" + unique_id</p><p class="source-code">container = {</p><p class="source-code">    "Image": triton_image_uri,</p><p class="source-code">    "ModelDataUrl": model_data,</p><p class="source-code">    "Environment": {"SAGEMAKER_TRITON_DEFAULT_MODEL_NAME": "resnet50"},</p><p class="source-code">}</p><p class="source-code">create_model_response = sm_client.create_model(</p><p class="source-code">    ModelName=sm_model_name, ExecutionRoleArn=role, PrimaryContainer=container</p><p class="source-code">)</p></li>
<li>After that, we<a id="_idIndexMarker802"/> can define the endpoint configuration:<p class="source-code">endpoint_config_name = "triton-resnet50-" + unique_id</p><p class="source-code">create_endpoint_config_response = sm_client.create_endpoint_config(</p><p class="source-code">    EndpointConfigName=endpoint_config_name,</p><p class="source-code">    ProductionVariants=[</p><p class="source-code">        {</p><p class="source-code">            "InstanceType": "ml.g4dn.4xlarge",</p><p class="source-code">            "InitialVariantWeight": 1,</p><p class="source-code">            "InitialInstanceCount": 1,</p><p class="source-code">            "ModelName": sm_model_name,</p><p class="source-code">            "VariantName": "AllTraffic",</p><p class="source-code">        }</p><p class="source-code">    ],)</p></li>
<li>Now, we are ready to deploy our endpoint:<p class="source-code">endpoint_name = "triton-resnet50-" + unique_id</p><p class="source-code">create_endpoint_response = sm_client.create_endpoint(</p><p class="source-code">    EndpointName=endpoint_name, EndpointConfigName=endpoint_config_name)</p></li>
</ol>
<p>Once the endpoint has been deployed, you can check SageMaker’s endpoint logs to confirm that<a id="_idIndexMarker803"/> the Triton server has started and that the model was successfully loaded. </p>
<h3>Running inference </h3>
<p>To run inference, we <a id="_idIndexMarker804"/>must construct a payload according to the model signature defined in <strong class="source-inline">config.pbtxt</strong>. Take a look at the following inference call. The response will follow a defined output signature as well:</p>
<pre class="source-code">
payload = {
    "inputs": [
        {
            "name": "input__0",
            "shape": [1, 3, 224, 224],
            "datatype": "FP32",
            "data": get_sample_image(),
        }
    ]
}
response = runtime_sm_client.invoke_endpoint(   EndpointName=endpoint_name, ContentType="application/octet-stream", Body=json.dumps(payload))
predictions = json.loads(response["Body"].read().decode("utf8"))</pre>
<p>This section described the basic functionality of the Triton model server and how to use it on Amazon SageMaker. It’s recommended that you refer to the Triton documentation to learn advanced features and optimization techniques. Keep in mind that depending on your chosen model format and DL framework, your model configuration will be different. You can review the AWS detailed benchmarking for the Triton server for the BERT model at <a href="https://aws.amazon.com/blogs/machine-learning/achieve-hyperscale-performance-for-model-serving-using-nvidia-triton-inference-server-on-amazon-sagemaker/">https://aws.amazon.com/blogs/machine-learning/achieve-hyperscale-performance-for-model-serving-using-nvidia-triton-inference-server-on-amazon-sagemaker/</a>. These benchmarks provide a good starting point for experimenting<a id="_idIndexMarker805"/> with and tuning Triton-hosted models.</p>
<h1 id="_idParaDest-156"><a id="_idTextAnchor153"/>Summary</h1>
<p>In this chapter, we discussed how to use popular model servers – TensorFlow Serving, PyTorch TorchServe, and NVIDIA Triton – on Amazon SageMaker. Each model server provides rich functionality to deploy and tune your model inference. The choice of a specific model server may be driven by the DL framework, target hardware and runtime environments, and other preferences. NVIDIA Triton supports multiple model formats, target hardware platforms, and runtimes. At the same time, TensorFlow Serving and TorchServe provide native integration with their respective DL frameworks. Regardless of which model server you choose, to ensure optimal utilization of compute resources and inference performance, it’s recommended to plan how you load test and benchmark your model with various server configurations.</p>
<p>In the next chapter, <a href="B17519_10.xhtml#_idTextAnchor154"><em class="italic">Chapter 10</em></a>, <em class="italic">Operationalizing Inference Workloads</em>, we will discuss how to move and manage inference workloads in production environments. We will review SageMaker’s capabilities for optimizing your inference workload costs, perform A/B testing, scale in and out endpoint resources based on inference traffic patterns, and advanced deployment patterns such as multi-model and multi-container endpoints.</p>
</div>
</div></body></html>