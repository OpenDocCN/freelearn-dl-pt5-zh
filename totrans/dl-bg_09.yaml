- en: Autoencoders
  prefs: []
  type: TYPE_NORMAL
- en: This chapter introduces the autoencoder model by explaining the relationship
    between encoding and decoding layers. We will be showcasing a model that belongs
    to the unsupervised learning family. This chapter also introduces a loss function
    commonly associated with the autoencoder model, and it also applies it to the
    dimensionality reduction of MNIST data and its visualization in an autoencoder-induced
    latent space.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Introduction to unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Encoding and decoding layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Applications in dimensionality reduction and visualization
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Ethical implications of unsupervised learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Introduction to unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'As machine learning has progressed over the last few years, I have come across
    many ways to categorize the different types of learning. Recently, at the NeurIPS
    2018 conference in Montreal, Canada, Dr. Alex Graves shared information about
    the different types of learning, shown in *Figure 7.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5bee9eca-50c2-4a2f-ad2d-2d7f4dd7de2e.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.1 – Different types of learning
  prefs: []
  type: TYPE_NORMAL
- en: Such efforts at categorization are very useful today when there are many learning
    algorithms being studied and improved. The first row depicts *active* learning,
    which means that there is a sense of interaction between the learning algorithm
    and the data. For example, in reinforcement learning and active learning operating
    over *labeled data*, the reward policies can inform what type of data the model
    will read in the following iterations. However, traditional supervised learning,
    which is what we have studied so far, involves no interaction with the data source
    and instead assumes that the dataset is fixed and that its dimensionality and
    shape will not change; these non-interactive approaches are known as *passive*
    learning.
  prefs: []
  type: TYPE_NORMAL
- en: The second column in the table in the *Figure 7.1* represents a special kind
    of learning algorithm that requires *no labels* to learn from data. Other algorithms
    require you to have a dataset that has data ![](img/52ff04b1-d1b4-4173-846d-eb4fc6caaa71.png) that
    is associated with a label ![](img/2e108673-48ce-4bf6-886c-961518e1492f.png);
    that is: ![](img/16a7ebb1-2910-4ea4-be45-11490e9bdd33.png). However, unsupervised
    algorithms have no need for labels to "do things" with data.
  prefs: []
  type: TYPE_NORMAL
- en: You can think of labels as a **teacher**. A teacher tells the learner that **x** corresponds
    to [![](img/35b87990-2cba-4534-a272-8a62f182d1e0.png)] and the learner attempts
    to learn the relationship between [![](img/af098bbe-7b09-4ffa-a98f-1537109b18f5.png)] and [![](img/35b87990-2cba-4534-a272-8a62f182d1e0.png)] iteratively
    by trial and error, adjusting its *beliefs* (parameters) until it gets it right.
    However, if there is no teacher, the learner does not know anything about the
    label ![](img/4f8dc03e-cf59-479f-89d9-7e277b089165.png) and therefore learns *something*
    about ![](img/cd1d240b-75a4-4b5b-b53e-e6f9147a5425.png) by itself, provided some
    boundaries, and it forms its own beliefs about ![](img/af098bbe-7b09-4ffa-a98f-1537109b18f5.png) without
    ever knowing anything about ![](img/35b87990-2cba-4534-a272-8a62f182d1e0.png).
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapters, we will study **unsupervised learning**, which is
    the type of learning that assumes that the data we have will not change in its
    shape or form and will remain consistent during the learning process and also
    during its deployment. These algorithms are guided by something other than labels,
    for example, a unique loss function for data compression. On the other hand, there
    are other algorithms that have an exploration mechanism or a specific motivation
    to learn from data in an interactive way, and these algorithms are **active learning**
    algorithms. We will not discuss these algorithms in this book since this book
    is intended to be introductory and for absolute beginners. However, we will discuss
    at length some of the most robust *unsupervised *deep learning models.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will begin by learning about **autoencoders**. An autoencoder has the sole
    purpose of taking input data into a neural network that is composed of two parts:
    an **encoder** and a **decoder**. The encoder portion has the mission of encoding
    the input data, usually into a lower-dimensional space, thereby compressing or
    encoding the input data. The decoder portion of the model is in charge of taking
    the encoded (or compressed) latent representation of the input data and then reconstructing
    it back to its original shape and to its original values without losing any data.
    That is, in an ideal autoencoder, the input is equal to the output. Let''s discuss
    this in more detail in the following sections.'
  prefs: []
  type: TYPE_NORMAL
- en: Encoding and decoding layers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The autoencoder can be broken down into two major components that serve specific
    purposes during an unsupervised learning process. The left side of *Figure 7.2*
    shows an autoencoder that is implemented using fully connected (dense) layers.
    It receives as input some vector ![](img/51804466-39c7-495b-a3c7-6b5a0e8462fe.png) and
    then it goes into six hidden layers; the first three, with 6, 4, and 2 neurons,
    respectively, are meant to compress the input data down to two dimensions, since
    the output of two neurons is two scalar values. This first set of layers is known
    as the **encoder**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f308db9c-1cf1-42e1-b1eb-7c8bfea595b9.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.2 – Two representations of an autoencoder. Left: full and descriptive
    model. Right: compact and abstracted model representation'
  prefs: []
  type: TYPE_NORMAL
- en: The second set of neurons is meant to reconstruct the input data back to its
    original dimensionality and values ![](img/9d520487-cfc5-48c1-aa44-2e0a69486ce5.png) using
    three layers with 4, 6, and 8 neurons, respectively; this group of layers is known
    as the **decoder**.
  prefs: []
  type: TYPE_NORMAL
- en: Note that the last layer of the autoencoder *must have* the same number of neurons
    as the number of dimensions of the input vector. Otherwise, the reconstruction
    would not match the input data.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the autoencoder shown on the left in *Figure 7.2* acts as a compression
    network, in the sense that after training the model to achieve good reconstruction,
    if we disconnect the decoder, we end up with a neural network that encodes the
    input data into two dimensions (or any dimensions we choose, for that matter).
    This presents a unique advantage over supervised models: in a supervised model,
    we teach a network to look for a pattern that will permit an association with
    a given target label; however, in unsupervised learning (or in this autoencoder,
    for example), the network does not look for a specific pattern but rather learns
    to use the input space in any way that preserves the most representative and most
    important information of the input data, so as to allow good reconstruction in
    the decoder.'
  prefs: []
  type: TYPE_NORMAL
- en: Think of a neural network and an autoencoder that takes input images of cats
    and dogs; a traditional neural network can be trained to distinguish between dogs
    and cats and it is tasked with finding important patterns in the images of dogs
    and cats so as to tell the difference between them; however, an autoencoder will
    train to learn the most important patterns, the most representative of all patterns,
    so as to preserve that information and allow good reconstruction regardless of
    the label. In a way, the traditional supervised neural network is biased to see
    the world in terms of cats and dogs, while the autoencoder is free to learn from
    the world regardless of cats or dogs.
  prefs: []
  type: TYPE_NORMAL
- en: The diagram on the right of *Figure 7.2* depicts an alternative representation
    of an autoencoder that is more abstract and compact. This type of representation
    is useful when describing a relatively deep autoencoder, when the number of layers
    is large to the point of it being difficult to represent all the neurons and all
    the layers one by one (as in the left side of the *Figure 7.2*). We will use those
    trapezoidal shapes to denote that there is an encoder/decoder; we note also that
    this abstraction will allow us the freedom to use other types of layers and not
    only dense (fully connected) layers. The diagram on the right of *Figure 7.2*
    depicts an autoencoder that takes an image as input, then encodes the input into
    a *d-*dimensional space, and then reconstructs the *latent* vector back to the
    input (image) space.
  prefs: []
  type: TYPE_NORMAL
- en: A **latent space**is a space where the learned lower-dimensional patterns are
    mapped. It is also known as the *learned representation space*. Ideally, this
    latent space is rich in important information about the input data and has fewer
    dimensions than the input data without any loss of information.
  prefs: []
  type: TYPE_NORMAL
- en: Let's now go ahead and implement each of the autoencoder parts based on the
    simple model on the left in *Figure 7.2*.
  prefs: []
  type: TYPE_NORMAL
- en: Encoding layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The TensorFlow and Keras libraries that we will be using are `Input` and `Dense`
    from `tensorflow.keras.layers` and `Model` from `tensorflow.keras.models`. We
    will be using the `keras` functional approach as opposed to *sequential* modeling.
    Import the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The `Input` layer will be used to describe the dimensionality of the input
    vector, which in our case will be `8`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, considering all of our activation functions as `sigmoid` just for the
    sake of this example, we can define the pipeline of the encoder layers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: The `Dense` class constructor receives the number of neurons and the activation
    function as parameters and at the end of the definition (on the right side), we
    must include what the input to the layer is, and this is assigned a name on the
    left side. Thus, in the line `elayer1 = Dense(6, activation='sigmoid')(inpt_vec)`,
    the name assigned to the layer is `elayer1`, then `6` is the number of neurons,
    `activation='sigmoid'` assigns a `sigmoid` activation function to the dense layer,
    and `inpt_vec` is the input to this layer in the pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the preceding three lines of code, we have defined the layers of the encoder,
    and the `encoder` variable points to the object that can output the latent variable
    if we make it a model and call `predict()` on it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: In this line of code, `latent_ncdr` contains the model that can map the input
    data to the latent space once it is trained. But before we do that, let's go ahead
    and define the layers of the decoder in a similar way.
  prefs: []
  type: TYPE_NORMAL
- en: Decoding layers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We can define the decoder layers as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Note that in the preceding code, the number of neurons usually goes in increasing
    order until the last layer that matches the input dimension. In this case, 4,
    6, and 8 are defined as `inpt_dim`. Similarly, the `decoder` variable points to
    the object that can output the reconstructed input if we make it a model and call `predict()` on
    it.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have separated the encoder and decoder intentionally here, simply to show
    that we could have the ability to access the different components of the network
    if we choose to do so. However, we should probably also define the autoencoder
    as a whole, from input to output, by using the `Model` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: This is exactly what we meant earlier when we said "if we make it a model and
    call `predict()` on it." This declaration is making a model that takes as input
    the input vector defined in `inpt_vec` and retrieves the output from the `decoder` layer.
    Then, we can use this as a model object that has a few nice functions in Keras
    that will allow us to pass input, read output, train, and do other things that
    we will discuss in the upcoming sections. For now, since we have defined our model,
    and before we can train it, we should define what the objective of the training
    will be, that is*, *what our loss function will be.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Our loss function has to be in terms of the goal of the autoencoder. This goal
    is to reconstruct the input perfectly. That means that our input ![](img/5609bfb0-1712-46de-89cc-f6748f485e75.png) and
    our reconstruction ![](img/20073a1c-ab85-4c68-9710-7dc75e3838d9.png) have to be
    identical in an ideal autoencoder. This implies that the absolute difference must
    be zero:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/46e56da7-bf75-4217-8bda-4ebfce996ffa.png)'
  prefs: []
  type: TYPE_IMG
- en: 'However, this may not be realistic, and it is not in terms of a function that
    we can easily differentiate. For this, we can come back to the classic mean squared
    error function, which is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/c7136505-52c9-429a-8d05-5d958e6983f1.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We want to make ![](img/cf4788bf-0dbb-4180-9e74-0c6f8c31b7b0.png), ideally,
    or at best minimize it as much as possible. We interpret this loss function as
    minimizing the average of the squared differences between the input and its reconstruction.
    If we use a standard backprop strategy, say, some type of standard gradient descent
    technique, we can compilethe model and prepare it for training as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The `compile()` method prepares the model for training. The loss function defined
    previously is given as a parameter, `loss='mean_squared_error'`, and the optimization
    technique chosen here is known as **stochastic gradient descent** (**SGD**)*,*
    `optimizer='sgd'`. For more information on SGD, please see Amari, S. I. (1993).
  prefs: []
  type: TYPE_NORMAL
- en: Learning and testing
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Since this is an introductory example of a simple autoencoder, we will train
    only with one data point and begin the learning process. We also want to show
    the encoded version and the reconstructed version.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will use the number 39 in binary as eight digits, which corresponds to 00100111\.
    We will declare this as our input vector as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then perform the training as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `fit()` method performs the training. Its first two arguments are the input
    data and the desired target output; in the case of the autoencoder, they are both
    `x`. The number of epochs is specified as `epochs=10000`, since the model can
    produce a decent output at this point, and we set the verbosity to zero since
    we do not need to visualize every epoch, using `verbose=0`.
  prefs: []
  type: TYPE_NORMAL
- en: In Google Colab or Jupyter Notebook, it is not a good idea to visualize more
    than 1,000 epochs on the screen at a time. The web browser might become unresponsive
    to the JavaScript code in charge of displaying all these epochs. Beware.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `predict()` method in the latent encoder model, `latent_ncdr`, and in the
    `autoencoder` model produce the output at the specified layers. If we retrieve
    `encdd`, we can see the latent representation of the input, and if we retrieve `x_hat`,
    we can see the reconstruction. We can even calculate the mean squared error manually
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: The numbers here will vary due to the unsupervised nature of the learning algorithm.
    The first output vector can be any real numbers. The second output vector will
    likely have real numbers close to zero and close to one, resembling the original
    binary vector, but the exact values will vary every single time.
  prefs: []
  type: TYPE_NORMAL
- en: The first vector of two elements is the latent representation, [0.55, 0.43];
    this may not mean much to us at this point, but it will be very important to us
    in terms of data compression. It means that we are able to represent eight digits
    using two.
  prefs: []
  type: TYPE_NORMAL
- en: Although this is a toy example and representing a binary number with two digits
    is not very exciting, the theory behind this is that we could take any eight floating-point
    digits in the range [0, 1] and compress them down to two digits in the same range.
  prefs: []
  type: TYPE_NORMAL
- en: 'The second vector displayed shows evidence of a good reconstruction: something
    that should be a zero is a 0.08 and something that should be a one is a 0.91\.
    The **mean squared error** (**MSE**) as calculated manually yields a 0.007, which,
    although not zero, is small enough to be good.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can visualize the decay of the MSE throughout the training phase using the
    information stored in the `hist` object defined during the invocation of `fit()`.
    This object contains the information of the loss function value across epochs
    and allows us to visualize the process with the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'This produces what you see in *Figure 7.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/2207d736-a57d-4b78-b766-5d47cdb5e6b3.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.3 – Reconstruction loss across epochs of training of an autoencoder
    as described in terms of the MSE
  prefs: []
  type: TYPE_NORMAL
- en: 'Okay, once again, this was a toy example with one data point. We would never
    do this in *real life*. To show how bad an idea this is, we can take the binary
    string we used to train the model and invert every single bit, which gives 11011000
    (or 216 in decimal). If we gave this to the autoencoder, we would expect a *good*
    reconstruction, but let''s see what happens if we try to do that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: Once again, the numbers here will vary due to the unsupervised nature of the
    learning algorithm. If your results are different from what you see here (I'm
    sure they are), that is not a problem.
  prefs: []
  type: TYPE_NORMAL
- en: If you compare these results with the ones from before, you will notice that
    the latent representation is not that different, and the reconstructed output
    does not at all match the given input. It is evident that the model **memorized**
    the input on which it was trained. This is evident when we calculate the MSE and
    we obtain a value of 0.84, which is large compared to the one previously obtained.
  prefs: []
  type: TYPE_NORMAL
- en: The solution to this is, of course, adding more data. But this concludes the
    toy example of building an autoencoder. What really changes after this is the
    type and amount of data, the number of layers, and the types of layers. In the
    next section, we will look at the application of a simple autoencoder in dimensionality
    reduction problems.
  prefs: []
  type: TYPE_NORMAL
- en: Applications in dimensionality reduction and visualization
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Among some of the most interesting applications of autoencoders is dimensionality
    reduction [Wang, Y., et al. (2016)]. Given that we live in a time where data storage
    is easily accessible and affordable, large amounts of data are currently stored
    everywhere. However, not everything is relevant information. Consider, for example,
    a database of video recordings of a home security camera that always faces one
    direction. Chances are that there is a lot of repeated data in every video frame
    or image and very little of the data gathered will be useful. We would need a
    strategy to look at what is really important in those images. Images, by their
    nature, have a lot of redundant information, and there is usually correlation
    among image regions, which makes autoencoders very useful in compressing the information
    in images (Petscharnig, S., et al. (2017)).
  prefs: []
  type: TYPE_NORMAL
- en: To demonstrate the applicability of autoencoders in dimensionality reduction
    for images, we will use the well-known MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST data preparation
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For details about MNIST, please go to [Chapter 3](8300fba9-620e-4bc3-8d81-3b02c5043a0d.xhtml),
    *Preparing Data*. Here we will only mention that the MNIST data will be scaled
    to the range [0, 1]. We also need to convert all images into vectors by reshaping
    the 28 by 28 digit images into a 784-dimensional vector. This can be achieved
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: We will use `x_train` to train the autoencoder and `x_test` to test the generalization
    capability of the autoencoder to both encode and decode MNIST digits. For visualization
    purposes, we will need `y_test`, but `y_train` can be ignored since we do not
    need labels in unsupervised machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.4* depicts the first eight samples in `x_test`. These samples will
    be used across a few experiments to show the capabilities of different autoencoder
    models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/6d1cc115-5ef0-473a-a4a5-f470dc712f1f.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.4 – Test MNIST digits to be used for comparison
  prefs: []
  type: TYPE_NORMAL
- en: Autoencoders for MNIST
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We can design a few experiments with different numbers of layers to see how
    the autoencoder changes its performance for MNIST. We can start with an autoencoder
    with four layers, always using a latent dimension of two. This is done to facilitate
    visualizing the MNIST digits in a two-dimensional space induced by the autoencoders.
  prefs: []
  type: TYPE_NORMAL
- en: 'Based on the previously defined autoencoder, we can propose the following four-layer
    base autoencoder:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'This will be the base of the subsequent models. There are a few things that
    are highlighted that are new and need to be introduced properly. The first important
    thing is a new activation function called the **hyperbolic tangent**. This activation
    function is defined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/52343b1b-860d-4414-b38c-dcd47901d881.png)'
  prefs: []
  type: TYPE_IMG
- en: 'The corresponding first derivative is relatively simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/787eef00-1a67-4308-a579-98b187cf7ab2.png)'
  prefs: []
  type: TYPE_IMG
- en: Besides having a nice and easily calculable derivative, the hyperbolic tangent
    activation function has a nice output range of [-1, 1]. This allows a neutral
    range that is not necessarily constrained to the sigmoid range [0, 1]. For visualization
    purposes, sometimes it is interesting to visualize in the hyperbolic tangent range,
    but it is not necessary to do so.
  prefs: []
  type: TYPE_NORMAL
- en: 'Another new element we have introduced is the loss function called **binary
    cross-entropy**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b277ace3-8e19-46d5-969d-3c8eb2469cac.png)'
  prefs: []
  type: TYPE_IMG
- en: In general, binary cross-entropy uses information, which are theoretical ideas
    to calculate the error between the target data ![](img/64dc5f89-e650-4e23-a54d-ee7b42d906f2.png) and
    the reconstructed (or predicted) data ![](img/491f7fd1-ce72-4085-a1f5-9de08fee77d6.png).
    In a way, it measures the amount of entropy, or surprise, between the target and
    the prediction. For example, in an ideal autoencoder, it is not surprising that
    the target ![](img/d759bd24-540c-47ec-a39e-533ee26397b9.png) is equal to its reconstruction ![](img/1383cd76-dc04-4c7e-8437-9e74de440010.png),
    and the loss should be zero. However, if the target ![](img/131b4632-5c63-4ab0-856c-21ef58000492.png) is
    not equal to ![](img/c67111eb-15ba-4682-9dae-c6468ad25fe8.png), that would be
    surprising and would yield a high loss.
  prefs: []
  type: TYPE_NORMAL
- en: For a more complete discussion on autoencoders using cross-entropy loss, see
    (Creswell, A., et. al. (2017)).
  prefs: []
  type: TYPE_NORMAL
- en: A new optimizer called **Adam** has also been introduced (Kingma, D. P., et.
    al. (2014)). It is an algorithm for stochastic optimization that uses an adaptive
    learning rate that has proven to be very fast in some deep learning applications.
    Speed is a nice property when we are dealing with deep learning models and large
    datasets. Time is of the essence, and Adam provides a nice approach that has become
    popular.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, the last new thing we added was on the `fit()` method. You should have
    noticed that there are two new parameters: `shuffle=True`, which allows the shuffling
    of data during the training process; and `validation_data=( , )`, which specifies
    a tuple of data that is used to monitor the loss using validation data, or data
    that the model has never seen, and will never use, for training.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is all the new things we have introduced. The next step is to explain
    the autoencoder architectures we will try in our experiments. Please see *Figure
    7.5* for a reference regarding the experiments we will perform:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9ded95c1-d8fa-45fe-aa65-6c97a457ad43.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.5 – Different autoencoder configurations to demonstrate the difference
    in the quality of the latent representations
  prefs: []
  type: TYPE_NORMAL
- en: In the figure, you will notice that we are using the abstracted representation
    of an autoencoder, and on the right side of the *Figure 7.5* are the different
    layers that each autoencoder architecture will use. The first architecture shown
    corresponds to the code shown in this section. That is, the code shows an autoencoder
    with encoding layers of 392, 28, 10, and 2 neurons, respectively; while the decoding
    layers contain 10, 28, 392, and 784 neurons, respectively. The next model on the
    right contains the same layers except for removing the pair of layers corresponding
    to 392 neurons, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: The last autoencoder model only contains two layers, one encoding (two neurons)
    and one decoding (784 neurons). At this point, you should be able to modify the
    Python code to remove the necessary layers and replicate the models depicted in
    *Figure 7.5*. The next step is to train the models in *Figure 7.5* and visualize
    the quality of the output.
  prefs: []
  type: TYPE_NORMAL
- en: Training and visualization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The execution of `autoencoder.fit()` for 100 epochs produces a viable model
    that can easily encode into two dimensions as specified. Looking closely into
    the loss function during training, we can observe that it converges properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1eaad2b2-ec72-436a-a4d6-7551d485dc5a.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.6 – Loss function monitoring during the training of a four-layer autoencoder
  prefs: []
  type: TYPE_NORMAL
- en: 'Once the model has been trained successfully, we can retrieve the encoded representation
    using the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'We are using the test set, `x_test`. This encoding will encode into two dimensions,
    as specified, and will produce a latent representation in the range [-1, 1], as
    specified. Similarly, we can always take the test set and use the autoencoder
    to compress it and reconstruct it to see how similar the input is to the reconstruction.
    We do so with this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Before we look into the latent representations learned from MNIST, we can look
    into the reconstruction quality as a way of assessing the quality of the learned
    model. *Figure 7.7* shows the reconstruction results (in `x_hat`) using *Figure
    7.4* as a reference to the input provided to each model. The figure is broken
    down into four parts, each part corresponding to the models described in *Figure
    7.5*: a) the model with eight layers, b) the model with six layers, c) the model
    with four layers, and d) the model with two layers:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/ba164838-ce99-4c8b-9915-f1d7c9312e19.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 7.7 – Autoencoder reconstruction for the models in Figure 7.5: a) the
    model with eight layers, b) the model with six layers, c) the model with four layers,
    and d) the model with two layers'
  prefs: []
  type: TYPE_NORMAL
- en: 'From *Figure 7.7.a*, we can see that the model with eight layers (392, 28,
    10, 2, 10, 28, 392, 784) is capable of producing generally good reconstructions
    with the exception of the numbers 4 and 9\. It is evident that both digits are
    closely related (visually) and the autoencoder has some difficulty distinguishing
    clearly between the two digits. To further explore this observation, we can visualize
    test data in the latent space (in `encdd`), which is depicted in *Figure 7.8*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/0d99e2d5-f02f-416d-bd0a-38ec29e7ef4d.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.8 – Four-layer encoder using MNIST test data
  prefs: []
  type: TYPE_NORMAL
- en: The overlap between digits four and nine is evident in the latent space produced
    by the autoencoder. However, most of the other groups of digits have relatively
    clear separate clusters. *Figure 7.8* also explains the natural closeness of other
    numbers that look like each other; for example, one and seven appear to be close
    to each other, as well as zero and six, and so do three and eight. However, numbers
    that do not look alike are in opposite sections of the latent space – for example,
    zero and one.
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 7.9* depicts the three-layer autoencoder that removed the layer with
    392 neurons and left a 28, 10, 2 neuron architecture. Clearly, the quality of
    the latent space is significantly reduced, although some of the major structure
    is consistent. That is, zero and one are on opposite sides, and other numbers
    that look alike are closer together; the overlap is greater in comparison to *Figure
    7.8*. The quality of this three-layer autoencoder is consistently lower, as shown
    in *Figure 7.7.b*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/4a3f6fa6-12e8-472d-9309-baa2396e9346.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.9 – Three-layer encoder using MNIST test data
  prefs: []
  type: TYPE_NORMAL
- en: 'In *Figure 7.10*, we can observe the results of the two-layer autoencoder with
    10 and 2 neurons, which again has a greater digit overlap than the previous autoencoders;
    this is also evident in the poor reconstruction shown in *Figure 7.7.c*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/b60d2d57-df05-46f3-ac6a-8fbded4dfb34.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.10 – Two-layer encoder using MNIST test data
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, *Figure 7.11* shows the latent space of the one-layer autoencoder.
    Clearly, this is a terrible idea. Just consider what we are asking the autoencoder
    to do: we are asking just two neurons to find a way to look at an entire image
    of a digit and find a way (learning weights [![](img/e2c8f2cb-9a89-4d71-9848-c755d1cd2c58.png)])
    to map all images to two dimensions. It is just not possible to do that. Logically,
    if we only have one layer, we would want at least 10 neurons to adequately model
    each of the 10 digits in MNIST:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/73bc6359-d02c-4f03-b66e-0f8a11f34068.png)'
  prefs: []
  type: TYPE_IMG
- en: Figure 7.11 – One-layer encoder using MNIST test data – a bad idea
  prefs: []
  type: TYPE_NORMAL
- en: Close observation of *Figure 7.11* also makes it clear that the scale of the
    axes varies just slightly; this can be interpreted as the encoder not being able
    to separate into different regions of the latent space all the digits of MNIST.
    In practice, please do not use autoencoders with a few layers with a few neurons,
    unless the dimensionality of the input space is already very low. Autoencoders
    might be more successful in deep configurations, as shown in this experiment.
    Learn more about deep autoencoders in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Ethical implications of unsupervised learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Unsupervised learning, such as what we see happening in the autoencoder we have
    been exploring so far, is not magical. It is well established and has very rigorous
    boundaries that are known and pre-defined. It does not have the capability of
    learning new things outside the limitations given by the data. Remember, unsupervised
    learning is **passive** learning as explained in the introductory section of this
    chapter.
  prefs: []
  type: TYPE_NORMAL
- en: However, even the most robust of unsupervised learning models have ethical risks
    associated with them. One of the major problems is that they create difficulties
    when dealing with outliers or data that may contain edge cases. For example, say
    that there is a large amount of data for IT recruitment, which includes years
    of experience, current salary, and programming languages that a candidate knows.
    If the data mostly contains data about candidates with the same programming language
    experience, and only a few know Python, then those candidates that know the Python
    language might be placed into a boundary or a region that might be difficult to
    visualize clearly, because the model has learned that since Python is an infrequent
    language, it may not be relevant in terms of data compression, dimensionality
    reduction, or data visualization. Furthermore, consider what would happen if 5
    years later, you used that same model despite there being newer programming languages
    that were not known about during training 5 years ago. The model may or may not
    map such information properly for visualization or data compression applications.
  prefs: []
  type: TYPE_NORMAL
- en: You must be very careful about what data is used to train an autoencoder, and
    having a variety of cases is important for the reliability of any model. If there
    is not enough diversity in the data, the autoencoder will be biased toward learning
    only from one input space. Imagine that you train an autoencoder on images of
    the 10 MNIST digits from earlier – you would not expect the autoencoder to perform
    properly on images of cats; that would be a mistake and would likely produce unwanted
    results. When using, for example, images of people, you must make sure that there
    is enough variety and diversity in the training data to produce proper training
    and a robust model that does not perform incorrectly for images of people that
    were not considered part of the training data.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: This chapter showed that autoencoders are very simple models that can be used
    to encode and decode data for different purposes, such as data compression, data
    visualization, and simply finding latent spaces where only important features
    are preserved. We showed that the number of neurons and the number of layers in
    the autoencoder are important for the success of the model. Deeper (more layers)
    and wider (more neurons) traits are often ingredients for good models, even if
    that leads to slower training times.
  prefs: []
  type: TYPE_NORMAL
- en: 'At this point, you should know the difference between supervised and unsupervised
    learning in terms of passive learning. You should also feel comfortable implementing
    the two basic components of an autoencoder: the encoder and the decoder. Similarly,
    you should be able to modify the architecture of an autoencoder to fine-tune it
    to achieve better performance. Taking the example we discussed in this chapter,
    you should be able to apply an autoencoder to a dimensionality reduction problem
    or to a data visualization problem. Also, you should be considering the risks
    and responsibilities associated with unsupervised learning algorithms when it
    comes to the data used to train them.'
  prefs: []
  type: TYPE_NORMAL
- en: '[Chapter 8](6677b8b1-806c-4c39-8c1e-371e83501acf.xhtml), *Deep Autoencoders,* will
    continue with deeper and wider autoencoder architectures that go beyond the introduction
    we covered in this chapter. The next chapter will introduce the idea of deep belief
    networks and the significance of this type of deep unsupervised learning. It will
    explain such concepts by introducing deep autoencoders and contrasting them with
    shallow autoencoders. The chapter will also give important advice on optimizing
    the number of neurons and layers to maximize performance.'
  prefs: []
  type: TYPE_NORMAL
- en: Questions and answers
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**Is overfitting a bad thing for an autoencoder? **'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Actually, no. You want the autoencoder to overfit! That is, you want it to exactly
    replicate the input data in the output. However, there is a caveat. Your dataset
    must be really large in comparison to the size of the model; otherwise, the memorization
    of the data will prevent the model from generalizing to unseen data.
  prefs: []
  type: TYPE_NORMAL
- en: '**Why did we use two neurons in the encoder''s last layer?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For visualization purposes only. The two-dimensional latent space produced by
    the two neurons allows us to easily visualize the data in the latent space. In
    the next chapter, we will use other configurations that do not necessarily have
    a two-dimensional latent space.
  prefs: []
  type: TYPE_NORMAL
- en: '**What is so cool about autoencoders again?**'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: They are simple neural models that learn without a teacher (unsupervised). They
    are not biased toward learning specific labels (classes). They learn about the
    world of data through iterative observations, aiming to learn the most representative
    and relevant features. They can be used as feature extraction models, but we will
    discuss more about that in future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: References
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Amari, S. I. (1993). Backpropagation and stochastic gradient descent method. *Neurocomputing*,
    5(4-5), 185-196.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Wang, Y., Yao, H., & Zhao, S. (2016). Auto-encoder based dimensionality reduction.
    *Neurocomputing*, 184, 232-242.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Petscharnig, S., Lux, M., & Chatzichristofis, S. (2017). Dimensionality reduction
    for image features using deep learning and autoencoders. In *Proceedings of the
    15th International Workshop on Content-Based Multimedia Indexing* (p. 23). ACM.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creswell, A., Arulkumaran, K., & Bharath, A. A. (2017). On denoising autoencoders
    trained to minimize binary cross-entropy. *arXiv preprint* arXiv:1708.08487.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Kingma, D. P., & Ba, J. (2014). Adam: A method for stochastic optimization.
    arXiv preprint arXiv:1412.6980.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
