- en: Reinforcement Learning for Gaming
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will learn about reinforcement learning. As the name suggests,
    with this method, optimal strategies are discovered through reinforcing or rewarding
    certain behavior and penalizing other behavior. The basic idea for this type of
    machine learning is to use an agent that performs actions towards a goal in an
    environment. We will explore this machine learning technique by using the `ReinforcementLearning`
    package in R to compute a policy for the agent to win a game of tic-tac-toe.
  prefs: []
  type: TYPE_NORMAL
- en: While this may seem like a simple game, it is a good environment for investigating
    reinforcement learning. We will learn how to structure input data for reinforcement
    learning, which is the same format for tic-tac-toe as for more complex games.
    We will learn how to compute a policy using the input data to provide the agent
    with the optimal strategy for the environment. We will also look at the hyperparameters
    available with this type of machine learning and the effect of adjusting these
    values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Throughout this chapter, we will complete the following tasks:'
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the concept of reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing and preprocessing data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Configuring a reinforcement learning agent
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files of this chapter at GitHub link at [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R).
  prefs: []
  type: TYPE_NORMAL
- en: Understanding the concept of reinforcement learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Reinforcement learning is the last of the three most broad categories of machine
    learning. We have already studied supervised learning and unsupervised learning.
    Reinforcement learning is the third broad category and differs from the other
    two types in significant ways. Reinforcement learning neither trains on labeled
    data nor adds labels to data. Instead, it seeks to find an optimal solution for
    an agent to receive the highest reward.
  prefs: []
  type: TYPE_NORMAL
- en: The environment is the space where the agent completes its task. In our case,
    the environment will be the 3 x 3 grid used to play the game tic-tac-toe. The
    agent performs tasks within the environment. In this case, the agent places the
    X's or O's on the grid. The environment also contains rewards and penalties—that
    is, the agent needs to be rewarded for certain actions and penalized for others.
    In tic-tac-toe, if a player places marks (X or O) in three consecutive spaces
    either horizontally, vertically, or diagonally, then they win and conversely the
    other player loses. This is the simple reward and penalty structure for this game.
    The policy is the strategy that dictates which actions the agent should take to
    lead to the greatest probability for success given any set of previous actions.
  prefs: []
  type: TYPE_NORMAL
- en: To determine the optimal policy, we will be using Q-learning. The Q in Q-learning
    stands for quality. It involves developing a quality matrix to determine the best
    course of action. This involves using the Bellman equation. The interior of the
    equation calculates the reward value plus the discounted maximum value of future
    moves minus the current quality score. This calculated value is then multiplied
    by the learning rate and added to the current quality score. Later, we will see
    how to write this equation using R.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we are using Q-learning; however, there are other ways to perform
    reinforcement learning. Another popular algorithm is called **actor–critic** and
    it differs from Q-learning in significant ways. The following paragraph is a comparison
    of the two to better show the different approaches to pursuing the same type of
    machine learning.
  prefs: []
  type: TYPE_NORMAL
- en: Q-learning computes a value function, so it requires a finite set of actions,
    such as the game tic-tac-toe. Actor–critic works with a continuous environment
    and seeks to optimize the policy without a value function like Q-learning does.
    Instead, actor–critic has two models. One of them, the actor, performs actions
    while the other, the critic, calculates the value function. This takes place for
    each action, and over numerous iterations, the actor learns the best set of actions.
    While Q-learning works well for solving a game like tic-tac-toe, which has a finite
    space and set of moves, actor–critic works well for environments that are not
    constrained or that change dynamically.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we quickly reviewed the different methods for performing reinforcement
    learning. Next, we will begin to implement Q-learning on our tic-tac-toe data.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing and processing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For our first task, we will use the tic-tac-toe dataset from the `ReinforcementLearning`
    package. In this case, the dataset is built for us; however, we will investigate
    how it is made to understand how to get data into the proper format for reinforcement
    learning:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s load the tic-tac-toe data. To load the dataset, we first load
    the `ReinforcementLearning` library and then call the `data` function with `"tictactoe"` as
    the argument. We load our data by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'After running these lines, you will see the data object in the data **Environment**
    pane. Its current type is `<Promise>`; however, we will change that in the next
    step to see what is contained in this object. For now, your Environment pane will
    look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f4b61246-8a29-4e82-a853-0dc2adb682da.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Now, let''s look at the first few rows to evaluate what the dataset contains.
    We will use the `head` function to print the first few rows to the console and
    this will also convert the object in our **Environment** pane from `<Promise>`
    to an object that we can interact with and explore. We print the first five rows
    to our console using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the code, your console will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/62c086d5-bbd8-491a-8bc5-8ce1426fa4c8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'In addition, the object in the Environment pane will now look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/288e4e5f-2255-4d45-817c-fd07e7fd2681.png)'
  prefs: []
  type: TYPE_IMG
- en: As we look at these images, we can see the way that this data is set up. In
    order to conduct reinforcement learning, we need our data to be in the format
    where one column is the current state, another is the action, and then the subsequent
    state, and lastly the reward. Let's take the first row and explain exactly what
    the value means.
  prefs: []
  type: TYPE_NORMAL
- en: The `State` is `"........."`. The dots indicate that the space on the 3 x 3
    grid is blank, so this character string represents a blank tic-tac-toe board.
    The `Action` is `"c7"`. This means that the agent who is playing as X will place
    an X in the seventh spot, which is the bottom-left corner. The `NextState` is
    `"......X.B"`, which means that in this scenario, for this row, the opponent has
    placed an O in the bottom-right corner. The `Reward` is `0` because the game has
    not ended and the value of `0` for `Reward` indicates a neutral state where the
    game will continue. A row like this will exist for every possible combination
    of values for `State`, `Action`, `NextState`, and `Reward`**.**
  prefs: []
  type: TYPE_NORMAL
- en: 'Using only the top five rows, we can see that all possible moves are nonterminal,
    which is to say that the game continues after the move. Let''s now look at the
    moves that lead to the conclusion of the game:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the preceding code, we will see the following rows printed to
    our console for actions that lead to victory:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/92c76c14-d5f1-48d0-a0ef-be0d36d4f1ea.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We will also see these rows printed to our console for moves that lead to defeat:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d8480855-a816-4849-aec6-24c5b36fc69f.png)'
  prefs: []
  type: TYPE_IMG
- en: Let's look at the first row from the subset that leads to victory. In this case,
    the agent already has an X in the top-right corner and the center of the game
    board. Here, the agent places an X in the bottom-left corner, and this results
    in three consecutive X's along a diagonal, which means the agent has won the game,
    which we see reflected in the `Reward` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s look at a given state and see all possible moves:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Subsetting our data this way, we start from a given state and see all possible
    options. The printout to your console will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1418b1c2-3821-4406-a80b-14830fc71ab7.png)'
  prefs: []
  type: TYPE_IMG
- en: In this case, there are only three spaces left on the game board. We can see
    that two moves lead to victory for the agent. If the agent selects the other space
    on the game board, then there are two remaining spaces on the board, and we can
    see that, regardless of which one the opponent chooses, the game continues.
  prefs: []
  type: TYPE_NORMAL
- en: 'From this investigation, we can see how to prepare a dataset for reinforcement
    learning. Even though this one was done for us, we could see just how we would
    make one ourselves. If we wanted to code our tic-tac-toe board differently, we
    could use the values from the game Number Scramble. Number Scramble is isomorphic
    to tic-tac-toe, but involves choosing numbers rather than placing marks on a grid;
    however, the number values match perfectly to the grid, so the values can be swapped.
    The game of Number Scramble involves selecting numbers between 1 and 15 between
    two players, where no number can be selected twice and the winner is the first
    to select numbers that sum up to 15\. With this in mind, we could rewrite the
    first row that we looked at like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this, we would get the following printed to our console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/5a61c3b8-8679-4458-9820-fef283216fc5.png)'
  prefs: []
  type: TYPE_IMG
- en: From this, we can see that the values for `State`, `Action`, and `NextState`
    can be encoded in any way that we like as long as a consistent convention is used
    so that the reinforcement learning process can traverse from state to state to
    discover the optimal path to reward.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to set up our data, let's move on to looking at exactly
    how our agent will find the best way to reach the reward.
  prefs: []
  type: TYPE_NORMAL
- en: Configuring the reinforcement agent
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s go into the details of what is happening to configure a reinforcement
    agent using Q-learning. The goal of Q-learning is to create a state–action matrix
    where a value is assigned for all state–action combinations—that is, if our agent
    is at any given state, then the values provided determine the action the agent
    will take to obtain maximum value. We are going to enable the computation of the
    best policy for our agent by creating a value matrix that provides a calculated
    value for every possible move:'
  prefs: []
  type: TYPE_NORMAL
- en: 'To start, we need a set of state and action pairs that all have a value of
    0\. As a best practice, we will use hashing here, which is a more efficient alternative
    to large lists for scaling up to more complex environments. To begin, we will
    load the hash library and then we will use a `for` loop to populate the hash environment.
    The `for` loop starts by getting every unique state from the data, and for every
    unique state, it then appends every unique action to create all possible state–action
    pairs and assigns all pairs a value of 0\. We generate this hash environment that
    will hold the values calculated during the Q-learning phase by running the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the code, we will see that our **Environment** pane now looks
    like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f802eb4e-3660-45a5-8529-8aadb4fac82b.png)'
  prefs: []
  type: TYPE_IMG
- en: We have a hash environment, `Q`, that contains every state-action pair.
  prefs: []
  type: TYPE_NORMAL
- en: 'The next step is to define the hyperparameters. For now, we will use the default
    values; however, we will soon tune these to see the impact. We set the hyperparameters
    to their default values by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the code, we can now see that we have a list with our hyperparameter
    values in our Environment pane, which now looks like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a3bfbff9-c5ef-4fb1-8820-875de3044620.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we begin to populate our Q matrix. This again takes place within a `for`
    loop; however, we will look at one isolated iteration. We start by taking a row
    and moving the elements from this row to discrete data objects using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the code, we can see the changes to our **Environment** pane,
    which now contains the discrete elements from the first row. The Environment pane
    will look like the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/9884af2f-f684-429c-8ce8-29ea8628f8c5.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Next, we get a value for the current Q-learning score if there is one. If there
    isn''t a value, then `0` is stored as the current value. We set this initial quality
    value score by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: After running this code, we now have a value for `currentQ`, which is `0` in
    this case because all values in Q for the state `'......X.B'` are `0`, as we have
    set all values to `0`; however, in the next step, we will begin to update the
    Q values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Lastly, we update the Q value by using the Bellman equation. This is also called
    **temporal difference** **learning**. We write out this step for computing values
    with this equation for R using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the following code, we can pull out the updated value for this
    state–action pair; we can see it in the field labeled `q_value`. Your **Environment**
    pane will look like the following screenshot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7ad7e1e0-875a-4992-b1fe-3f3bed94b04e.png)'
  prefs: []
  type: TYPE_IMG
- en: We note here that the `q_value` is still `0`. Why is this the case? If we look
    at our equation, we will see that the reward is part of the equation and our reward
    is `0`, which makes the entire calculated value `0`. As a result, we will not
    begin to see updated Q values until our code encounters a row with a nonzero reward.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can now put all of these steps together and run them over every row to create
    our Q matrix. We create the matrix of values that we will use to select the policy
    for optimal strategy by running the following code, which wraps all the previous
    code together in a `for` loop:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'After looping through all the rows, we see that some state–action pairs do
    now have a value in the Q matrix. Running the following code, we will see the
    following value printed to our console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/e2db8f77-2cb1-483f-84bc-8455f72227c4.png)'
  prefs: []
  type: TYPE_IMG
- en: At this point, we have now created our matrix for Q-learning. In this case,
    we are storing the value in a hash environment with values for every key-value
    pairing; however, this is equivalent to storing the values in a matrix—it is just
    more efficient for scaling up later. Now that we have these values, we can compute
    a policy for our agent that will provide the best path to a reward; however, before
    we compute this policy, we will make one last set of modifications, and that is
    to tune the hyperparameters that we set earlier to their default values.
  prefs: []
  type: TYPE_NORMAL
- en: Tuning hyperparameters
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We have now defined our environment and iterated over all possible actions and
    results from any given state to calculate the quality value of every move and
    stored these values in our Q object. At this point, we can now begin to tune the
    options for this model to see how it impacts performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'If we recall, there are three parameters for reinforcement learning, and these
    are alpha, gamma, and epsilon. The following list describes the role of each parameter
    and the impact of adjusting their value:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Alpha**: The alpha rate for reinforcement learning is the same as the learning
    rate for many other machine learning models. It is the constant value used to
    control how quickly probabilities are updated as calculations are made based on
    exploring rewards for the agent taking certain actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Gamma**: Adjusting gamma adjusts how much the model values future rewards.
    When gamma is set to `1`, then all rewards current and future are valued equally.
    This means that a reward that is several steps away is worth as much as a reward
    that is earned at the next step. Practically, this is almost never what we want
    since we want future rewards to be more valuable since it takes more effort to
    earn them. By contrast, setting gamma to `0` means that only rewards from the
    next action will have any value. Future rewards have no value at all. Again, aside
    from special cases, this is not desirable. When adjusting gamma, you have to seek
    the balance in weighting among future rewards that leads the agent to make an
    optimal selection of actions.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Epsilon**: The epsilon parameter is used to introduce randomness when selecting
    future actions. Setting epsilon to `0` is referred to as greedy learning. In this
    case, the agent will always take the path with the highest probability of success;
    however, in this case, as with other machine learning, it is easy for an agent
    to get lost in some local minima and never discover an optimal strategy. By introducing
    some randomness, different actions will be pursued over different iterations.
    Adjusting this value optimizes the exploration to exploit balance. We want the
    model to exploit what has been learned to choose the best future action; however,
    we also want the model to explore and continue learning.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Using what we know about these hyperparameters, let''s see how the values change
    as we make adjustments to these parameters:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will make adjustments to the value of `alpha`. As noted, the `alpha`
    value is the learning rate value, which we may be familiar with from learning
    about other machine learning topics. It is just the constant value that controls
    how quickly the algorithm makes adjustments. Currently, we have an `alpha` rate
    set at `0.1`; however, let''s set our `alpha` rate at `0.5`. This is higher than
    we would usually ever want it to be in practice and is used here simply to explore
    the impact of changing these values. We will need to reset Q to all zeroes and
    restart the learning process to see what happens. The following code block takes
    everything we just did previously and runs it all again with the one adjustment
    to `alpha`. We tune the `alpha` value and see the effect by running the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see from this tuning that we get a different value for the Q value at
    `234543`. You will see the following printed out to your console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/1c9cb469-4878-4e9c-bb0f-64e28f731d82.png)'
  prefs: []
  type: TYPE_IMG
- en: As we might have expected, since we increased the value for alpha, we, as a
    result, ended up with a larger value for the Q value at the same point that we
    looked at previously. In other words, our algorithm learned faster and the quality
    value received a greater amount of weight.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let''s tune the value for `gamma`. If we recall, adjusting this value
    for gamma will alter how much the agent values future rewards. Our value is currently
    set at `0.1`, which means that future rewards do have value, but the level at
    which they are valued is relatively small. Let''s boost this up to `0.9` and see
    what happens. We go through the same operation as we did when we adjusted alpha.
    We start by resetting the Q hash environment so that all state–action pairs have
    a value of `0` and then we repopulate this hash environment by looping through
    all options, applying the Bellman equation by making our own changes to the `gamma`
    value. We assess what happens when we change the `gamma` value by running the
    following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, you will see the following code printed on your console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/d7316a87-4159-4839-b6f9-001edd3a9989.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From this, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: We can see that our value increased quite significantly
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: From this state and taking this action, there is value, but when considering
    future rewards there is even more value
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, with a game like tic-tac-toe, we need to consider that there are never
    many steps between any state and a reward; however, we can see that from this
    state and this action, there will be a good probability of getting a reward
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'For our final adjustment, we will adjust the `epsilon`. The value of `epsilon`
    relies on how much previous knowledge is used compared with how much exploration
    is done to gather knowledge. For this adjustment, we will go back to using the
    function from the `ReinforcementLearning` package, since it relies on not only
    looping through the Bellman equation, but also storing these values for reference
    over multiple iterations. To adjust `epsilon`, we use the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, we will see that our Q value has changed. You will
    see the following value printed to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/8db9a1a2-7374-447f-bbb0-f2e1d57862bb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'From this, we can make the following observations:'
  prefs: []
  type: TYPE_NORMAL
- en: Our value is similar to the value that we had when we were using our default
    values for parameters, but slightly larger
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In this case, we have introduced a relatively large amount of randomness to
    force our agent to continue exploring; as a result, we can see that we have not
    lost so much value with this randomness, and this action retains a similar value
    even when it leads to different sets of subsequent actions
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'After tuning the parameters to the desired settings, we can now view the policy
    for a given state. First, let''s take a look at the model object in our **Environment**
    pane. Your **Environment** pane will look like the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![](img/62c571c6-90d6-41df-8db1-3223277e7e82.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s look more in-depth at every element in the model object:'
  prefs: []
  type: TYPE_NORMAL
- en: '`Q_hash`: The hash environment, just like we created earlier, which includes
    every state–action pair, along with a Q value'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Q`: A named matrix that contains the same data as the hash environment, except
    in the form of a named matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`States`: The named rows from our matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Actions`: The named columns from our matrix'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Policy`: A named vector that contains the optimal action that the agent should
    take from any state'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Reward` and `RewardSequence`: These are the number of rows from the dataset
    that lead to a reward less than the number that lead to a penalty'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'We can use the values from here to see the value of all actions at any given
    state and judge which is the best move to make. Let''s start with a brand new
    game and see which move has the most value. We can see the value of every action
    from this state and note which action is best by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'After running this code, we will see the following printed to the console:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/7aa1b28b-40f0-4a49-a323-f6d0eff91e52.png)'
  prefs: []
  type: TYPE_IMG
- en: 'We can see that our first line lists every possible action along with their
    respective values in descending order. We can see that from this named vector,
    the move `"c5"`, which is a mark at the center of the grid, has the highest value.
    Consequently, when we view the policy for our agent when it is at that state,
    we see that it is `"c5"`. In this way, we can now use the results of reinforcement
    learning to choose the optimal move from any given state:'
  prefs: []
  type: TYPE_NORMAL
- en: We just adjusted all the parameters to note the effects of changing these variables
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, in the last step, we saw how to select the best policy based on the grid
    being in any state
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Through trying every possible combination of actions, we calculated the value
    of moves based on immediate and future rewards
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We decided to weigh the Q value by adjusting our parameters and decided upon
    a method to solve games
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have coded a reinforcement-learning system using Q-learning.
    We defined our environment or playing surface and then looked at the dataset containing
    every possible combination of states, actions, and future states. Using the dataset,
    we calculated the value of every state–action pair, which we stored in a hash
    environment and also as a matrix. We then used this matrix of values as the basis
    of our policy, which selects the move with the most value.
  prefs: []
  type: TYPE_NORMAL
- en: In our next chapter, we will expand on Q-learning by adding neural networks
    to create deep Q-learning networks.
  prefs: []
  type: TYPE_NORMAL
