<html><head></head><body>
        <section>

                            <header>
                    <h1 class="header-title">Generating Drum Sequences with the Drums RNN</h1>
                </header>
            
            <article>
                
<p><span>In this chapter, you'll learn what many consider the foundation of music—percussion. We'll show the importance of <strong>Recurrent Neural Networks</strong> (<strong>RNNs</strong>) for music generation. You'll then learn how to use the Drums RNN model using a pre-trained drum kit model, by calling it in the command-line window and directly in Python, to generate drum sequences. We'll introduce the different model parameters, including the model's MIDI encoding, and show how to interpret the output of the model.</span></p>
<p>The following topics will be covered in this chapter:</p>
<ul>
<li>The significance of RNNs in music generation</li>
<li>Using the Drums RNN in the command line</li>
<li>Using the Drums RNN in Python</li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Technical requirements</h1>
                </header>
            
            <article>
                
<p>In this chapter, we'll use the following tools:</p>
<ul>
<li>The <strong>command line</strong> or <strong>bash</strong> to launch Magenta from the Terminal</li>
<li><strong>Python</strong> and its libraries to write music generation code using Magenta</li>
<li><strong>Magenta</strong> to generate music in MIDI</li>
<li><strong>MuseScore</strong> or <strong>FluidSynth</strong> to listen to the generated MIDI</li>
</ul>
<p class="mce-root"/>
<p>In Magenta, we'll make the use of the <strong>Drums RNN</strong> model. We'll be explaining this model in depth, but if you feel like you need more information, the model's README in Magenta's source code (<a href="https://github.com/tensorflow/magenta/tree/master/magenta/models/drums_rnn">github.com/tensorflow/magenta/tree/master/magenta/models/drums_rnn</a>) is a good place to start. You can also take a look at Magenta's code on GitHub, which is well documented. We also provide additional content in the last section, <em>Further reading</em>.</p>
<p>The code for this chapter is in this book's GitHub repository <span>in the </span><kbd>Chapter02</kbd> <span>folder, located at</span> <a href="https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter02">github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter02</a><span>. For this chapter, you should run </span><kbd>cd Chapter02</kbd> <span>in the command-line window before you start.</span></p>
<p>Check out the following video to see the Code in Action:<br/>
<a href="http://bit.ly/37G0mmW">http://bit.ly/37G0mmW</a></p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">The significance of RNNs in music generation</h1>
                </header>
            
            <article>
                
<p>Specific neural network architectures are designed for specific problems. It doesn't mean that one architecture is better than another one—it just means it is better at a specific task.</p>
<p>In this section, we'll be looking at our specific problem, generating music, and see why RNNs are well suited for the task. We'll be building our knowledge of neural network architectures for music throughout this book, by introducing specific concepts in each chapter.</p>
<p>For music generation, we are looking at two specific problems that RNNs solve—operating on sequences in terms of input and output and keeping an internal state of past events. Let's have a look at those properties.</p>
<div class="packt_infobox">Musical score prediction is analogous to generating music. By predicting the next notes from an input sequence, you can iteratively generate a new sequence by choosing a prediction at each iteration. This process is described in the <em>Understanding the generation algorithm</em> section in this chapter.</div>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Operating on a sequence of vectors</h1>
                </header>
            
            <article>
                
<p>In many neural net architectures, <strong>input size</strong> and <strong>output size</strong> are fixed. Take a <strong>Convolutional Neural Network</strong> (<strong>CNN</strong>), for example. This neural net can be used for image classification, with the input being an array of pixels representing the image and the output the prediction for each element of a set of classes (for example, "cat," "dog," and so on). Notice the input and output are of fixed size.</p>
<p>What is nice about RNNs is that input and output size can be of arbitrary lengths. For a music score prediction network, an input could be an arbitrary length sequence of notes, and the output could be a sequence of predicted notes from that input.</p>
<p>This is possible in an RNN because it works on a <strong>sequence</strong> of vectors. There are many ways of representing RNN types:</p>
<ul>
<li><strong>One</strong>-<strong>to</strong>-<strong>one</strong>: This is where there's fixed input and output; an example is image classification.</li>
<li><strong>One</strong>-<strong>to</strong>-<strong>many</strong>: This is where there's fixed input to sequence output; an example is image captioning, where the network will generate a text based on the image content.</li>
<li><strong>Many</strong>-<strong>to</strong>-<strong>one</strong>: Here, there's sequence input to fixed output; an example is sentiment analysis, where the network will output a single word (sentiment) describing an input sentence.</li>
<li><strong>Many</strong>-<strong>to</strong>-<strong>many</strong>: Here, there's sequence input to sequence output; an example is language translation, where the network will output a full sentence in a language from a full sentence in another language.</li>
</ul>
<p class="mce-root"/>
<p>A classic way of representing an RNN is shown in the following diagram. On the left side of the diagram, you have a compact representation of the network—the hidden layer outputs the feeds into itself. On the right side, you have the detailed representation of the same network—at each step, the hidden layer takes an input and the previous state and produces an output:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/96ac9fbc-545c-46e4-abe0-5de6187303a3.png"/></p>
<p>The bottom row of the diagram shows the input vectors, the middle row of the diagram shows the hidden layers, and the upper row of the diagram shows the output layer. This representation shows how well an RNN can represent many-to-many inputs and outputs for the following:</p>
<ul>
<li>A sequence of vectors for the input: <em>{ ..., x(t - 1), x(t), x(t + 1), ... }</em></li>
<li>A sequence of vectors for the output: <em>{ ..., y(t - 1), y(t), y(t + 1), ... }</em></li>
</ul>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Remember the past to better predict the future</h1>
                </header>
            
            <article>
                
<p>As we saw in the previous section, in RNNs, the input vector is combined with its state vector to produce the output, which is then used to update the state vector for the next step. This is different than feed-forward neural networks such as CNNs, where the network feeds information from the input to the output and only in that direction, meaning the output is a function of only its input, not previous events.</p>
<p class="mce-root"/>
<p>Let's look at how we define a simple RNN. We implement a single operation, the <kbd>step</kbd> operation, that takes an input vector, <kbd>x</kbd>, and returns an output vector, <kbd>y</kbd>. Each time the step operation is called, the RNN needs to update its state, the hidden vector, <kbd>h</kbd>.</p>
<p>What is important to note here, is that we can <strong>stack</strong> as many RNNs as we want by taking the output of an RNN and feeding it in the next RNN, just like in the previous diagram. For example, we could go <kbd>y1 = rnn1.step(x1)</kbd>, <kbd>y2 = rnn2.step(y1)</kbd>, and so on.</p>
<p>When training the RNN, during the forward pass, we need to update the state, calculate the output vector, and update the loss. But how do we update the state? Let's see the steps we need to follow:</p>
<ol>
<li>First, we do the matrix multiplication of the hidden state matrix (<kbd>Whh</kbd>) with the previous hidden state (<kbd>hs[t-1]</kbd>), <kbd>np.dot(Whh, hs[t<span class="pl-k">-</span><span class="pl-c1">1</span>])</kbd>.</li>
<li>Then, we sum it with the matrix multiplication of the current input matrix (<kbd>Wxh</kbd>) and the input vector (<kbd>xs[t]</kbd>), <kbd>np.dot(Wxh, xs[t])</kbd>.</li>
<li>Finally, we use the <kbd>tanh</kbd> activation function on the resulting matrix to squash the activations between -1 and 1.</li>
</ol>
<p>We do that at every step, meaning that, at every step of the training, the network has an <strong>up-to-date</strong> context in regards to the sequence it is handling.</p>
<p>To understand how an RNN handles sequential data, such as a note sequence, let's take the example of an RNN training on broken chords, which are chords broken down as a series of notes. We have the input data "A", "C", "E", and "G", which is encoded as a vector, for example <em>[1, 0, 0, 0]</em> for the first note (which corresponds to <kbd>x(t - 1)</kbd> in the previous diagram), <em>[0, 1, 0, 0]</em> for the second note (<kbd>x(t)</kbd> in the previous diagram), and so on.</p>
<p>During the first step, with the first input vector, the RNN outputs, for example, a confidence of the next note being 0.5 for "A", 1.8 for "C", -2.5 for "E", and 3.1 for "G". Because our training data tells us that the correct next note is "C", we want to increase the confidence score of 1.8, and decrease the other scores. Similarly, for each of the 4 steps (for the 4 input notes), we have a correct note to predict. Remember, at each step, the RNN uses both the hidden vector and the input vector to make a prediction. During backpropagation, the parameters are nudged in the proper direction by a small amount, and by repeating this enough times, we get predictions that match the training data.</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p>During inference, if the network first receives an input of "C", it won't necessarily predict "E" because it hasn't seen "A" yet, which doesn't match the example chord that was used to train the model. The RNN prediction is based on its <strong>recurrent connection</strong>, which keeps track of the context, and doesn't rely on the input alone.</p>
<p>To sample from a trained RNN, we feed a note into the network, which outputs the distribution for the next note. By <strong>sampling the distribution</strong>, we get a probable next note that we can then feed back to the network. We can repeat the process until we have a long enough sequence. This generation process is described in more detail in the following section, <em>Understanding the generation algorithm</em>.</p>
<p>During backpropagation, we saw that we update the parameters going backward in the network. Imagine the network is learning a long sequence of notes: how far can the gradients be backpropagated in the network so that the link between a note far in the sequence and a note at the beginning still holds? Well, it turns out that this is a difficult problem for vanilla RNNs. One answer to that is <strong>Lo</strong><strong>ng</strong>-<strong>Short Term Memory</strong> (<strong>LSTM</strong>) cell, which uses a different mechanism for keeping the current state.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the right terminology for RNNs</h1>
                </header>
            
            <article>
                
<p>Now that we understand RNNs, we can say that most RNNs are using LSTM cells. Those RNNs are sometimes called <strong>LSTM networks</strong>, but more often than not, they are just called RNNs. Unfortunately, the two terms are often used interchangeably. In Magenta, all of the RNNs are LSTMs but aren't named as such. This is the case of the Drums RNN model we're looking at in this chapter and all of the models we're going to use in the next chapters.</p>
<p>We'll be explaining LSTMs in <a href="48023567-4100-492a-a28e-53b18a63e01e.xhtml">Chapter 3</a>, <em>Generating Polyphonic Melodies</em>. For now, just remember that what we saw in the previous section still holds, but the hidden state update is more complex than what we described.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Drums RNN on the command line</h1>
                </header>
            
            <article>
                
<p>Now that we understand how RNNs make for powerful tools of music generation, we'll use the Drums RNN model to do just that. The pre-trained models in Magenta are a good way of starting music generation straightaway. For the Drums RNN model, we'll be using the <kbd>drum_kit</kbd> pre-trained bundle, which was trained on thousands of percussion MIDI files.</p>
<p class="mce-root"/>
<p>This section will provide insight into the usage of Magenta on the command line. We'll be primarily using Python code to call Magenta, but using the command line has some advantages:</p>
<ul>
<li>It is simple to use and useful for quick use cases.</li>
<li>It doesn't require writing any code or having any programming knowledge.</li>
<li>It encapsulates parameters in helpful commands and flags.</li>
</ul>
<p>In this section, we'll use the Drums RNN model in the command line and learn to configure the generation though flags. We'll explain how the generation algorithm works and look at its parameters and output.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Magenta's command-line utilities</h1>
                </header>
            
            <article>
                
<p>Magenta comes with multiple command-line utilities. These command-line utilities are Python scripts that can be called directly from the command line as console entry points and are installed in your Conda environment when you install Magenta (look in the <kbd>bin</kbd> folder of your Magenta environment or the <kbd>scripts</kbd> folder if using Windows). The complete list of command-line utilities is located in Magenta's source code, in <kbd>setup.py</kbd>, under <kbd>CONSOLE_SCRIPTS</kbd>.</p>
<div class="packt_tip">You can always checkout Magenta's source code and have a look at it. It might seem intimidating at first, but the source code is well documented and provides invaluable insight into the inner workings of the software. Using Git, execute <kbd>git clone https://github.com/tensorflow/magenta</kbd> in a Terminal and then open the repository in your favorite IDE. Another advantage of having the source code is to have a look at certain files that are not packaged with the app.</div>
<p>For the Drums RNN model that we are going to use, we have three command-line utilities (like much of the models):</p>
<ul>
<li><kbd>drums_rnn_create_dataset</kbd> will help to create a dataset for the training command. We'll be looking into this command in <a href="1ca56e24-b4d2-40de-b4cf-ae6bbb3c0eef.xhtml">Chapter 6</a>, <em>Data Preparation for Training</em>.</li>
<li><kbd>drums_rnn_generate</kbd> will be used in this chapter to generate a musical score.</li>
<li><kbd>drums_rnn_train</kbd> will train the model on an input dataset. We'll be looking into this command in <a href="6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml">Chapter 7</a>, <em>Training Magenta Models</em>.</li>
</ul>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating a simple drum sequence</h1>
                </header>
            
            <article>
                
<p>In the previous chapter, we generated a simple MIDI file to test our installation. We'll take that example and change it a bit.</p>
<p>Before starting, go back on a terminal to the main book's folder and then change directory to <kbd>Chapter02</kbd>. Make sure you are in your Magenta environment. If not, use <kbd>conda activate magenta</kbd> to do so:</p>
<ol>
<li>First, we download the Drums RNN bundle file, <kbd>drum_kit_rnn.mag</kbd>, in the <kbd>bundles</kbd> folder. You only need to do this once:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>&gt; curl --output bundles/drum_kit_rnn.mag http://download.magenta.tensorflow.org/models/drum_kit_rnn.mag</span></strong></pre>
<p style="padding-left: 60px">A bundle file is a file containing the model checkpoint and metadata. This is a pre-trained model that contains the weights from the training phase, which will be used to initialize the RNN network. We'll be seeing this format in detail in <a href="6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml">Chapter 7</a>, <em>Training Magenta Models</em>.</p>
<ol start="2">
<li>Then, we can use the bundle to generate MIDI files in the output directory with <kbd>--output-dir</kbd>:</li>
</ol>
<pre style="padding-left: 60px"><strong><span>&gt; drums_rnn_generate --bundle_file=bundles/drum_kit_rnn.mag --output_dir output</span></strong></pre>
<ol start="3">
<li>Open one of the generated files in the <kbd>output</kbd> folder in MuseScore or Visual MIDI. For the latter, you need to convert the MIDI file into a plot rendered in an HTML file, which you can then open in a browser. To convert the MIDI file into a plot, use the following command:</li>
</ol>
<pre style="padding-left: 60px"><span># Replace GENERATED by the name of the file<br/><strong>&gt; visual_midi "output/GENERATED.mid"</strong></span></pre>
<ol start="4">
<li>Then, open the <kbd>output/GENERATED.html</kbd> <span>HTML </span>file, which contains the plot:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/58c6a8a4-dff4-4e91-857c-d132239500ae.png"/></p>
<ol start="5">
<li>To listen to the generated MIDI, use your software synthesizer or MuseScore. For the software synthesizer, refer to the following command depending on your platform and replace <kbd>PATH_TO_SF2</kbd> and <kbd>PATH_TO_MIDI</kbd> with the proper values:
<ul>
<li>Linux: <kbd>fluidsynth -a pulseaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>macOS: <kbd>fluidsynth -a coreaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>Windows: <kbd>fluidsynth -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
</ul>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the model's parameters</h1>
                </header>
            
            <article>
                
<p>From the screenshot in the last section, you can already see that the model used some default configurations to generate the score: the number of steps to generate, the tempo, and so on. Now, let's see what other flags are possible. To see what kind of flags the model takes, use the <kbd>--helpfull</kbd> flag:</p>
<pre><span>&gt; <strong>drums_rnn_generate --helpfull</strong><br/><br/>    USAGE: drums_rnn_generate [flags]<br/>    ...<br/><br/>magenta.models.drums_rnn.drums_rnn_config_flags:<br/>    ...<br/><br/>magenta.models.drums_rnn.drums_rnn_generate:<br/>    ...</span></pre>
<p class="mce-root"/>
<p>You will see a lot of possible flags showing. The sections that are of interest for us are <kbd>drums_rnn_config_flags</kbd> and <kbd>drums_rnn_generate</kbd>, which are flags specific for the Drums RNN model.</p>
<p>The following subsections will explain the most important ones. Because most also apply to other models, you'll be able to apply what you learn to the next chapters as well. We'll explain other model-specific flags of the later chapters as we go.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Changing the output size</h1>
                </header>
            
            <article>
                
<p>A simple flag to change the number of generated samples is <kbd>--num_outputs</kbd>:</p>
<pre><span>--num_outputs: The number of drum tracks to generate. One MIDI file will be created for each. (default: '10')</span></pre>
<p>You can also use the <kbd>--num_steps</kbd> flag to change the size of the generated samples:</p>
<pre><span>--num_steps: The total number of steps the generated drum tracks should be, priming drum track length + generated steps. Each step is a 16th of a bar. (default: '128')</span></pre>
<p>The last example we generated was 128 steps long because we generated it using the default value. By looking at the previous screenshot, you can count the vertical bar lines, which counts to 8 bars. This is because, with 128 steps at 16 steps per bar, you get <em>128/16 = 8</em> bars. If you want a 1 bar generation, you'll be asking for 16 steps, for example. You can see a single step as a single note slot, in the sense that generators will generate one note per step maximum. It is a convenient way of dividing time.</p>
<div class="packt_tip packt_infobox">We'll be using the term <strong>bar</strong> in this book, which is more popular in British English, but readers might be used to the word <strong>measure</strong>, which is more popular in American English. There are some differences in their usage, and depending on the context, one or the other might be used more often. However, both words mainly have the same significance.<br/>
<br/>
The main reason to stick with bar throughout this book is to follow Magenta's code convention, where bar is used more consistently than measure.</div>
<p>We can show the steps by zooming on the last two bars of the previous example:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/565596cb-9b46-4e66-8c78-e8144626129c.png"/></p>
<p>You can see in this diagram that there are 2 bars, each of 2 seconds (see the next section for information on the tempo), with a different background for each bar. You can also see that there are 16 steps per bar; we've marked one of those steps with a different background. A step can contain multiple notes if the model is polyphonic, like the Drums RNN model. Depending on the model, a note can spawn multiple steps, which is not the case here. For this model, the note will always start and stop exactly on the step start and end since the model outputs quantized sequences.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Changing the tempo</h1>
                </header>
            
            <article>
                
<p>The tempo is the speed at which the score is played. Be aware that it won't change the number of notes or the generation length—it will only put the information in the generated MIDI so that the MIDI player will later be able to play it at the right speed.</p>
<p>The tempo in Magenta is expressed in <strong><span>Quarter-notes Per Minute</span></strong> (<strong><span>QPM</span></strong>). A <strong>quarter</strong> is a bar separated into four—if you have 16 steps in a bar, then a quarter contains 4 steps. So, if your tempo is 120 QPM, then you have <em>120 quarters/60 seconds = 2</em> quarters per second. That means you play 1 bar per 2 seconds (see the previous diagram for an example of that).</p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<div class="packt_tip">QPM is a measure of tempo similar but not to be confused with <strong>BPM</strong> (<strong>Beats</strong> <strong>Per</strong> <strong>Minute</strong>) since, in the latter, the meaning of a beat might change for some time signature. Also, the concept of a beat might change depending on the listener. QPM is well defined and used in the MIDI and MusicXML format.</div>
<p>To change the tempo, use the <kbd>--qpm</kbd> flag:</p>
<pre>--qpm: The quarters per minute to play generated output at. If a primer MIDI is given, the qpm from that will override this flag. (default: '120')</pre>
<p class="mce-root">In the following diagram, we've generated a drum file at 150 QPM using <kbd>--qpm 150</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/60e478fe-9897-49f5-9793-c3b2488378a3.png"/></p>
<p><span>You can see the bars are not aligned with 2, 4, and more seconds anymore. This is because, at 120 QPM, a bar is exactly 2 seconds long, but it is now slightly less. Our generated sample still has </span><kbd>--num_steps 128</kbd><span> but now has a duration of 12.8 seconds (and still 8 bars) because we still have the same amounts of steps—they are just played faster.</span></p>
<p><span>To find the duration in seconds of a sequence for a specific QPM such as 150, we first calculate the length of a step in seconds, by taking the number of seconds in a minute (60), dividing by the QPM (150), and dividing by the number of steps per quarter (4). This gives us 0.1 seconds per step. For 128 steps, the sequence is 12.8 seconds long.</span></p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Changing the model type</h1>
                </header>
            
            <article>
                
<p>The <kbd>--config</kbd> flag changes the configuration of the model. With each configuration in Magenta comes a pre-trained model. In this chapter, we are using the <kbd>drum_kit_rnn.mag</kbd> pre-trained model (or bundle) for the <kbd>drum_kit</kbd> configuration. The chosen pre-trained model must match the configuration it was trained with:</p>
<pre><span>--config: Which config to use. Must be one of 'one_drum' or 'drum_kit'. (default: 'drum_kit')</span></pre>
<p>It won't be useful for us now, but it will come in handy in <a href="48023567-4100-492a-a28e-53b18a63e01e.xhtml">Chapter 3</a>, <em>Generating Polyphonic Melodies</em>. This also changes the mapping of the drums, where the resulting encoded vector is different in both cases. We'll be talking about vector encoding in the next section when we look at the Python code.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Priming the model with Led Zeppelin</h1>
                </header>
            
            <article>
                
<p>A primer sequence can be given to the model to <strong>prepare</strong> it before the generation. This is used extensively with Magenta and is really useful if you want the model to generate something that is inspired by your primer. You can either prime the model with a hardcoded sequence or directly from a MIDI file. The priming sequence is fed to the model before the generation starts.</p>
<p>The string representation of the <kbd>--primer_drums</kbd><span>flag</span><span> </span><span>reads as follows: you enter a list of tuples, each tuple corresponding to a step, with each tuple containing the MIDI notes being played at the same time. In this example, on the first step, both MIDI notes, 36 and 42, are played at the same time, followed by 3 steps of silence, then MIDI note 42 is played alone in its own step:</span></p>
<pre><span>--primer_drums: A string representation of a Python list of tuples containing drum pitch values. For example: "[(36,42),(),(),(),(42,),(),(),()]". If specified, this drum track will be used as the priming drum track. If a priming drum track is not specified, drum tracks will be generated from scratch. (default: ''</span></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<div class="packt_infobox">As you might remember from the previous chapter, a MIDI note also have velocity information, which is not given here. This is not necessary since the Drums RNN doesn't support velocity. Each generated note will have a default value of 100 for velocity (on a maximum value of 127).<br/>
<br/>
Some models in Magenta support velocity as we'll see in the next chapters. Since the velocities have to be encoded in the input vectors that are fed to the network during training, it is a design choice to include them or not. We'll also talk about encoding in the next chapters.</div>
<p>To give a primer corresponding to a bar, you'll have to provide 16 tuples, because there are 16 steps per bar. The previous primer is half a bar long.</p>
<p>You can also provide the path to a MIDI file with the <kbd>--primer_midi</kbd> flag:</p>
<pre><span>--primer_midi: The path to a MIDI file containing a drum track that will  be used as a priming drum track. If a primer drum track is not specified, drum tracks will be generated from scratch. (default: '')</span></pre>
<p>A primer MIDI file gives the tempo and will override your <kbd>--qpm</kbd> flag if you also provide it.</p>
<p>When initializing the model with a primer, you also get the primer in the resulting output sequence. That means <kbd>--num_steps</kbd> needs to be bigger than the primer's length or else Magenta won't have space left to generate. For example, this command will output an error because the number of steps is not high enough:</p>
<pre><strong>&gt; drums_rnn_generate --bundle_file=bundles/drum_kit_rnn.mag --output_dir=output --primer_drums="[(36,),(36,),(36,),(36,)]" --num_steps=4</strong></pre>
<p>This results in the following output:</p>
<pre><strong>CRITICAL - Priming sequence is longer than the total number of steps requested: Priming sequence length: 0.625, Generation length requested: 0.62</strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Let's generate something based on a small drum part of Jon Bonham's (Led Zeppelin) <em>When The Levee Breaks</em> track. Here's a two-bar primer:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/da0a6c06-1fda-47b1-8e2f-d8281188d5eb.png"/></p>
<p>Then, we generate some MIDI files by setting the primer, the temperature, and the proper number of steps. Remember, the number of steps is the total number of steps, primer included:</p>
<pre><strong><span>drums_rnn_generate --bundle_file bundles/drum_kit_rnn.mag --output_dir output --num_steps 46 --primer_midi primers/When_The_Levee_Breaks_Led_Zeppelin.mid --temperature 1.1</span></strong></pre>
<p>We get an interesting sequence shown in the following diagram:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/bbe135e5-4b35-4670-9db3-ce2ed58dfa05.png"/></p>
<p class="mce-root"/>
<p>You can still find the primer in the first 3 seconds or so, then we notice that the model kept the musical structure of the track, but improvised over it, adding a few kick drums, hit hats, and snares here and there. We'll be looking at the MIDI mapping of percussion sequences, including the mapping of each pitch to their corresponding instrument, in the next section, <em>Mapping MIDI notes to the real world</em>.</p>
<p>Now, we verify whether Magenta knows how to count: you have 16 steps of primer, 1 step of silence, then 29 steps of generation for a total of 46, which is what we asked for. The step of silence comes from the way Magenta calculates the start of the generation. We'll see in the Python code how to handle that in a better way.</p>
<p>We also notice that the length of the notes in the primer are different in the generated score. You can see the same primer notes are present, but not with the same duration. This is because Magenta will <strong>quantize the primer before feeding it to the model</strong> and will generate quantized sequences. This depends on the model. <strong>Quantization</strong> is the process of moving the note's beginning and end so that they fall directly on some subdivisions of bars. In this case, Magenta moved the notes' end so that they fall on the closest step.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Configuring the generation algorithm</h1>
                </header>
            
            <article>
                
<p>The <kbd>--temperature</kbd> flag is important because it changes how random the generated sequence is:</p>
<pre><span>--temperature: The randomness of the generated drum tracks. 1.0 uses the unaltered softmax probabilities, greater than 1.0 makes tracks more random, less than 1.0 makes tracks less random. (default: '1.0')</span></pre>
<p>Let's try to generate a drum track with more randomness using <kbd>--temperature 1.5</kbd>:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="assets/f9851eb7-dbee-49c1-a35b-be6a58a5465e.png"/></p>
<p>This is pretty wild! Remember a temperature of 1.5 is high, so you might have a more coherent sample with a more conservative value, like 1.1, for example.</p>
<p>Now, to generate a track with less randomness, use <kbd>--temperature 0.9</kbd>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4bd4aa61-7b48-408a-bdfb-c9d0d678ec3e.png"/></p>
<p class="mce-root"/>
<p>You can clearly see the generation is more conservative here. Choosing the temperature is up to taste and depends on what you are trying to achieve. Try different temperature values and see what fits best with the music you are trying to generate. Also, some models might sound better with wilder temperature values than others.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Other Magenta and TensorFlow flags</h1>
                </header>
            
            <article>
                
<p>There are other flags that we haven't talked about, such as the configuration of the hyperparameters of the model with <kbd>--hparams</kbd>, but we'll be looking into this when we train our own model in <a href="6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml">Chapter 7</a>, <em>Training Magenta Models</em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Understanding the generation algorithm</h1>
                </header>
            
            <article>
                
<p>In the previous sections, we introduced how the generation algorithm works—by predicting at each generation step what the next note in the sequence<span> </span><span>is</span><span>, we can iteratively generate a full score. The resulting prediction depends on what the model has learned during the training phase. This section will delve deeper into the generation algorithm by showing it in action on an example being executed step by step.</span></p>
<p> </p>
<p>We'll also be explaining the parameters that modify the generation's execution:</p>
<pre><span>--beam_size: The beam size to use for beam search when generating drum tracks. (default: '1')<br/>--branch_factor: The branch factor to use for beam search when generating drum tracks. (default: '1')<br/>--steps_per_iteration: The number of steps to take per beam search iteration. (default: '1')</span></pre>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating the sequence branches and steps</h1>
                </header>
            
            <article>
                
<p>Let's use this command to launch the generation:</p>
<pre><strong><span>drums_rnn_generate --bundle_file=bundles/drum_kit_rnn.mag --output_dir=output </span><span>--temperature 1.1 </span><span>--beam_size 1 --branch_factor 2 --steps_per_iteration 1 --num_steps 64</span></strong></pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p>Magenta will do the following operations:</p>
<ol>
<li>It converts the primer sequence into a format that the model understands (this is called <strong>encoding</strong>—check the <em>Encoding percussion events as classes</em> section).</li>
<li>It uses that encoded primer to initialize the model state.</li>
<li>It loops until all of the steps (<kbd>--num_steps 64</kbd><span>) have been generated:</span>
<ol>
<li>It loops to generate <em>N</em> branches (<kbd>--branch_factor 2</kbd><span>):</span>
<ol>
<li>It generates <em>X</em> steps (<kbd>--steps_per_iterations 1</kbd><span>) by running the model with its current state using the</span> <strong>temperature</strong> <span>(</span><kbd>--temperature 1.1</kbd><span>). This returns the predicted sequence as well as the resulting</span> <strong>softmax probabilities</strong><span>. The softmax probabilities are the actual probability scores for each class (the encoded notes) at the final layer of the network.</span></li>
<li>It calculates the <strong>negative log-likelihood</strong> <span>of the resulting sequence, which is a scoring evaluation of the entire sequence from the softmax probabilities.</span></li>
<li>It updates the model state for the next iteration.</li>
</ol>
</li>
<li>It prunes the generated branches to best <em>K</em> branches (<kbd>--beam_size 1</kbd>) by using the calculated score.</li>
</ol>
</li>
</ol>
<p>This diagram shows the generation process with a final sequence of <strong>36</strong>, <strong>38</strong>, and <strong>42</strong>:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/3a0ea9af-e3a2-469e-81c8-09ab5cb8656a.png"/></p>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root">In the diagram, the <strong>S</strong> value denotes the calculated score of the entire sequence (see <em>steps 3.1.2</em> and <em>3.2</em>). The beam search algorithm shown here is linear in complexity on the output sequence length (which is the depth of the tree), so it is pretty fast. The default value of <kbd>--beam_size 1</kbd> is useful since the algorithm becomes a best-first search algorithm, where you don't actually do a breadth-first search since you are keeping only the best candidate.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Making sense of the randomness</h1>
                </header>
            
            <article>
                
<p>When we launch a generation that uses the beam search, Magenta shows the resulting log-likelihood of the whole sequence:</p>
<pre><strong><span>Beam search yields sequence with log-likelihood: -16.006279</span></strong></pre>
<p>What happens with <kbd>--temperature 1.25</kbd> instead of 1.1? The log-likelihood will be smaller (further from zero) since the generation is more random:</p>
<pre><strong><span>Beam search yields sequence with log-likelihood: -57.161125</span></strong></pre>
<p>What if we generate only 1 branch with <kbd>--branch_factor 1</kbd> but keep the same temperature at 1.25? The log-likelihood will be smaller:</p>
<pre><strong><span>Beam search yields sequence with log-likelihood: -140.033295</span></strong></pre>
<p>Why is the log-likelihood smaller? Because we've reduced the branch factor, the algorithm will generate fewer branches per iteration, meaning, at each iteration, it will have fewer branches to choose its best from, resulting in a globally more random sequence.</p>
<p>Let's now use what we've learned about the Drums RNN model and create a small Python application using those concepts.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Using the Drums RNN in Python</h1>
                </header>
            
            <article>
                
<p>In the previous section, we've seen how much we can already do on the command line with the Drums RNN model. In this section, you'll get to create a small application that will use that model to generate music in Python.</p>
<p>Using Magenta in Python is a bit difficult because of the following reasons:</p>
<ul>
<li>It requires you to write code and understand Magenta's architecture.</li>
<li>It requires more boilerplate code and is less straightforward.</li>
</ul>
<p class="mce-root"/>
<p class="mce-root"/>
<p>But it also has advantages that we think are important:</p>
<ul>
<li>You have more freedom in the usage of the models.</li>
<li>You can create new models and modify existing ones.</li>
<li>You can go beyond generating single sequences.</li>
</ul>
<p>The last point is important for us because we'll be building a small music application that generates music autonomously. Calling Magenta's scripts on the command line is convenient, but you cannot build an app using only this. You'll be starting this in the last section of this chapter, <em>Creating a music generation application</em>, and building on it in the next chapters.</p>
<p>Let's dive into the code by recreating what we've done on the command line and then building from there.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Generating a drum sequence using Python</h1>
                </header>
            
            <article>
                
<p>We are going to generate a MIDI file from a primer in Python, much like we've done in the previous section. </p>
<div class="packt_tip">You can follow this example in the <kbd>chapter_02_example_01.py</kbd> file <span>in the source code of this chapter. There are more comments and content in the source code, so you should go check it out.</span></div>
<ol>
<li>Let's start by downloading the bundle. There are a lot of useful tools in the <kbd>magenta.music</kbd> package, and we'll be using it in many examples:</li>
</ol>
<pre style="padding-left: 60px"><span>import </span>os<span><br/>import </span>magenta.music <span>as </span>mm<br/><br/>mm.notebook_utils.download_bundle(<span>"drum_kit_rnn.mag"</span>, <span>"bundles"</span>)<br/>bundle = mm.sequence_generator_bundle.read_bundle_file(<br/> os.path.join(<span>"bundles"</span>, <span>"drum_kit_rnn.mag"</span>))</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="2">
<li>We then use the drums generator to initialize the generator class with the <kbd>drum_kit</kbd> configuration. We are importing the Drums RNN models from its own package, and we'll do the same for each model:</li>
</ol>
<pre style="padding-left: 60px"><span>from </span>magenta.models.drums_rnn <span>import </span>drums_rnn_sequence_generator<br/><br/>generator_map = drums_rnn_sequence_generator.get_generator_map()<br/>generator = generator_map[<span>"drum_kit"</span>](<span>checkpoint</span>=<span>None</span>, <span>bundle</span>=bundle)<br/>generator.initialize()</pre>
<ol start="3">
<li>By declaring the tempo, we can also calculate the length of a bar in seconds. We need this because the generation start and end is given in seconds to Magenta.</li>
</ol>
<p style="padding-left: 60px">We first calculate the seconds per step, which is equal to the number of seconds in a minute, divided by the quarter per minute (the tempo), divided by the number of steps per quarter. This last value is dependent on the generator, but it is mostly equal to 4:</p>
<pre style="padding-left: 60px"><span>from </span>magenta.music <span>import </span>constants<span><br/><br/></span>qpm = <span>120<br/></span>seconds_per_step = <span>60.0 </span>/ qpm / generator.steps_per_quarter</pre>
<ol start="4">
<li>Then, we calculate the seconds per bar, which is equal to the number of steps per bar multiplied by the seconds per step we previously calculated. The number of steps per bar changes depending on the time signature, but for now, we'll just put the default value, which is 16, for 4/4 music sampled at 4 steps per quarter note:</li>
</ol>
<pre style="padding-left: 60px">num_steps_per_bar = constants.DEFAULT_STEPS_PER_BAR<br/>seconds_per_bar = num_steps_per_bar * seconds_per_step<br/><br/>print("Seconds per step: " + str(seconds_per_step))<br/>print("Seconds per bar: " + str(seconds_per_bar))</pre>
<ol start="5">
<li>We are now ready to initialize our primer sequence. We'll use a small jazz drum sequence of 1 bar for the primer (you can check it out in this book's source code in the <kbd>Chapter02</kbd> folder, <kbd>primers/Jazz_Drum_Basic_1_bar.mid</kbd>), so we'll need a list of 16 steps. We'll be explaining the primer definition in the next section.</li>
</ol>
<p class="mce-root"/>
<p style="padding-left: 60px">We convert that primer drum track into a primer sequence using the QPM we've already defined:</p>
<pre style="padding-left: 60px">primer_drums = mm.DrumTrack(<br/> [frozenset(pitches) for pitches in<br/>   [(38, 51),     (), (36,),    (),<br/>    (38, 44, 51), (), (36,),    (),<br/>    (),           (), (38,),    (),<br/>    (38, 44),     (), (36, 51), (),]])<br/>primer_sequence = primer_drums.to_sequence(qpm=qpm)</pre>
<ol start="6">
<li>We can calculate the time of the primer in seconds, which is only the seconds per bar value since the primer is 1 bar:</li>
</ol>
<pre style="padding-left: 60px">primer_start_time = 0<br/>primer_end_time = primer_start_time + seconds_per_bar</pre>
<ol start="7">
<li>We now calculate the start and end time of the generator section. First, we define the number of generation bars, which is 3, then we start the generation at the end of the primer and extend it for a three-bars duration in seconds:</li>
</ol>
<pre style="padding-left: 60px">num_bars = 3<br/>generation_start_time = primer_end_time<br/>generation_end_time = generation_start_time + (seconds_per_bar * num_bars)<br/><br/>print("Primer start and end: [" + str(primer_start_time) + ", " <br/> + str(primer_end_time) + "]")<br/>print("Generation start and end: [" + str(generation_start_time) + ", " <br/> + str(generation_end_time) + "]")</pre>
<ol start="8">
<li>We can now configure our generator options with the start and end times. The generation options also take the temperature, which we'll set to 1.1 for a bit of randomness. The generator interface is common for all models:</li>
</ol>
<pre style="padding-left: 60px"><span>from </span>magenta.protobuf <span>import </span>generator_pb2<br/><br/>generator_options = generator_pb2.GeneratorOptions()<br/>generator_options.args[<span>'temperature'</span>].float_value = 1.1<br/>generator_options.generate_sections.add(<br/> <span>start_time</span>=generation_start_time,<br/> <span>end_time</span>=generation_end_time)</pre>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mce-root"/>
<p class="mceNonEditable"/>
<ol start="9">
<li>It is time to generate! You can now call the generate method on the generator with the primer sequence as input. The return value of this method is a <kbd>NoteSequence</kbd> instance:</li>
</ol>
<pre style="padding-left: 60px">sequence = generator.generate(primer_sequence, generator_options)</pre>
<ol start="10">
<li>There are many utilities to then convert the resulting <kbd>NoteSequence</kbd> instance into other formats such as PrettyMidi. We'll now convert the result, and write the file and the plot to disk:</li>
</ol>
<pre style="padding-left: 60px"><span>from </span>visual_midi <span>import </span>Plotter<br/><br/># Write the resulting midi file to the output directory<br/>midi_file = os.path.join(<span>"output"</span>, <span>"out.mid"</span>)<br/>mm.midi_io.note_sequence_to_midi_file(sequence, midi_file)<br/><span>print</span>(<span>"Generated midi file: " </span>+ <span>str</span>(os.path.abspath(midi_file)))<br/><br/># Write the resulting plot file to the output directory<br/><span>from </span>visual_midi <span>import </span>Plotter<br/>plot_file = os.path.join(<span>"output"</span>, <span>"out.html"</span>)<br/><span>print</span>(<span>"Generated plot file: " </span>+ <span>str</span>(os.path.abspath(plot_file)))<br/>pretty_midi = mm.midi_io.note_sequence_to_pretty_midi(sequence)<br/>plotter = Plotter()<br/>plotter.show(pretty_midi, plot_file)</pre>
<ol start="11">
<li>Let's open the <kbd>output/out.html</kbd> file:</li>
</ol>
<p class="CDPAlignCenter CDPAlign"><img src="assets/4b72ea6f-16e5-4ec6-a2d6-7e8534517d9a.png"/></p>
<p class="mce-root"/>
<p style="padding-left: 60px">Notice that your primer at the beginning should be the same (because it is hardcoded), but your 3 generated bars should be different than these.</p>
<ol start="12">
<li>To listen to the generated MIDI, use your software synthesizer or MuseScore. For the software synth, refer to the following command depending on your platform and replace <kbd>PATH_TO_SF2</kbd> and <kbd>PATH_TO_MIDI</kbd> with the proper values:
<ul>
<li>Linux: <kbd>fluidsynth -a pulseaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>macOS: <kbd>fluidsynth -a coreaudio -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
<li>Windows: <kbd>fluidsynth -g 1 -n -i PATH_TO_SF2 PATH_TO_MIDI</kbd></li>
</ul>
</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Packaging checkpoints as bundle files</h1>
                </header>
            
            <article>
                
<p>In the last example, we saw the usage of a bundle. In Magenta, a bundle is a convenient way of packaging a <strong>TensorFlow checkpoint</strong> and metadata information into a single file. A checkpoint is used in TensorFlow to save the model state that occurs during training, making it easy to reload the model's state at a later time.</p>
<p>Another nice usage of a bundle is that it defines a <strong>common interface</strong> for multiple generators. You can check the <kbd>generator.proto</kbd> file in the Magenta's source code, in the <kbd>magenta/protobuf</kbd> folder, which defines that interface, including the <span>generator</span><span> </span><kbd>id</kbd> <span>and</span> <kbd>description</kbd><span>, as well as generator options such as </span><kbd>generate_sections</kbd> <span>that we'll be using to provide the generation length in many examples.</span></p>
<p>This common interface covers many models, including all of the models of Chapter 2 and Chapter 3. Unfortunately, bundles aren't used in Chapter 4 for the MusicVAE models, but we'll see more of them in <a href="6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml">Chapter 7</a>, <em><span>Training Magenta Models</span></em>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encoding MIDI using Protobuf in NoteSequence</h1>
                </header>
            
            <article>
                
<p>In the last example, we saw the usage of a class named <strong><kbd>NoteSequence</kbd></strong>, which is an important part of Magenta, since every model working on the score will use it to represent a sequence of MIDI notes. <kbd>NoteSequence</kbd> and <kbd>GeneratorOptions</kbd> are Protobuf (Protocol Buffers), a language-neutral, platform-neutral extensible mechanism for serializing structured data. In Magenta's source code, in the <kbd>magenta/protobuf/music.proto</kbd> <span>file, </span><span>you can see</span><span> </span><span>the message definition of </span><kbd>NoteSequence</kbd><span>.</span></p>
<p class="mce-root"/>
<p>The definition of <kbd>NoteSequence</kbd> is based on a MIDI message content, so you have the following:</p>
<ul>
<li>A list of <kbd>TimeSignature</kbd> changes: By default, 4/4 is assumed per MIDI standard.</li>
<li>A list of <kbd>KeySignature</kbd> changes: By default, C Major is assumed per MIDI standard.</li>
<li>A list of <kbd>Tempo</kbd> changes: By default, 120 QPM is assumed per MIDI standard.</li>
<li>A list of <kbd>Note</kbd> changes.</li>
</ul>
<p>There's also much more including annotations, quantization information, pitch bend, and control changes, but we won't be looking into that in this book.</p>
<p>The <kbd>Note</kbd> list is one we'll be mainly using, with the <kbd>pitch</kbd> (which is the MIDI note, based on the MIDI tuning standard), <kbd>start_time</kbd>, and <kbd>end_time</kbd> properties, representing a note.</p>
<p>Converting into and from <kbd>NoteSequence</kbd> is important. In the previous example, we've used the following functions:</p>
<ul>
<li><kbd>magenta.music.midi_io.note_sequence_to_midi_file</kbd>: This is for converting from a note sequence into a MIDI file. You can also convert into <kbd>PrettyMIDI</kbd>, a useful format to edit MIDI in memory.</li>
<li><kbd>magenta.music.midi_io.midi_file_to_note_sequence</kbd>: <span>This is for c</span>onverting from a MIDI file into a note sequence; this would have been useful in our previous example. Instead of hardcoding the primer in the Python code, we could have used <kbd>midi_file_to_note_sequence("primers/Jazz_Drum_Basic_1_bar.mid")</kbd>.</li>
</ul>
<p>Another important point about <kbd>NoteSequence</kbd> is that it doesn't explicitly define a start and end; it just assumes it starts at the start of the first note and ends at the end of the last note. In other words, a sequence starting or ending with silence cannot be defined.<span class="underline"><br/></span></p>
<p>In the following diagram, the sequence is expected to be of 2 bars, which is 32 steps, but stops at the 31<sup>st</sup> step, meaning the last note end time is 3.875 seconds, not 4 seconds:</p>
<p class="mce-root"/>
<p class="CDPAlignCenter CDPAlign"><img src="assets/5599fd77-4d46-4b29-bdc1-23b29dcbec34.png"/></p>
<p>Concatenating this sequence with another might yield unexpected resulting sequence length. Fortunately, methods that handle note sequences have options to make this work properly.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Mapping MIDI notes to the real world</h1>
                </header>
            
            <article>
                
<p>In the previous example, we've shown the following 1-bar primer but we haven't explained what those pitches correspond to:</p>
<pre>[(<span>38</span>, 51),     (), (36),     (),<br/> (<span>38</span>, <span>44, 51</span>), (), (36),     (),<br/> (),           (), (38),     (),<br/> (<span>38, 44</span>),     (), (36, 51), (),]</pre>
<p>We won't be going into detail in the MIDI specification since it is pretty big (you can check it out at <a href="https://www.midi.org/specifications">www.midi.org/specifications</a>), but we'll look at the parts that concern this book. Two specifications are interesting to us:</p>
<ul>
<li>The <strong>MIDI specification</strong> that defines the low-level protocol of communication and encoding between the different instruments
<ul>
<li>The <strong>General MIDI specification</strong> (<strong>GM</strong>) that defines a higher-level protocol, defining requirements for instruments to be compliant and specifying instrument sounds</li>
</ul>
</li>
</ul>
<div class="packt_infobox">January 2019 marks the first major update of the MIDI specification since its standardization in 1983 with the release of MIDI 2.0 specification. That's after more than 25 years of usage by millions of devices and users.<br/>
<br/>
MIDI 2.0 introduces higher resolution values with 16 bits of precision instead of 7 and the addition of the MIDI Capability Inquiry, enabling better integration between tools. The new version of MIDI is completely backward compatible with the old version.</div>
<p>The instrument sounds definition is interesting for us and we'll be looking into the <strong>GM 1 Sound Set</strong> specification, which defines the sound that should be played for each MIDI note. In GM 1 Sound Set, each MIDI Program Change (PC#) corresponds to a specific instrument in the synthesizer. For example, PC# 1 is <strong>Acoustic Grand Piano</strong> and PC# 42 is <strong>Viola</strong>. Remember, those sounds are defined by the synthesizer that implements the GM 1 specification and might change from synth to synth.</p>
<p>The percussion keymap is a bit different. On MIDI Channel 10, each MIDI note number (pitch) corresponds to a specific drum sound. Our previous example can be read as follows:</p>
<ul>
<li><strong>36</strong>: Bass Drum 1</li>
<li><strong>38</strong>: Acoustic Snare</li>
<li><strong>44</strong>: Pedal Hi-Hat</li>
<li><strong>51</strong>: Ride Cymbal 1</li>
</ul>
<p>You can always refer to the full table later at <a href="https://www.midi.org/specifications-old/item/gm-level-1-sound-set">www.midi.org/specifications-old/item/gm-level-1-sound-set</a>.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Encoding percussion events as classes</h1>
                </header>
            
            <article>
                
<p>The last section explained percussion mapping in terms of MIDI mapping. But how do we encode the percussion for the Drums RNN model? To encode the MIDI mapping to a vector, we'll use what is called <strong>one</strong>-<strong>hot encoding</strong>, which basically maps every possible input events to classes then to a binary vector.</p>
<p>For that to happen, we need to reduce the number of drum classes first, from all of the possible drums in MIDI (46 different drums is way too much) to a more manageable 9 classes. You can see the mapping in the <kbd>DEFAULT_DRUM_TYPE_PITCHES</kbd> <span>property </span><span>of the </span><kbd>magenta.music.drums_encoder_decoder</kbd><span> </span><span>module</span><span>. We then bit flip in a vector at the index defined by summing two to the power of the class index for each class in the set.</span></p>
<p class="mce-root"/>
<p>For example, our set of pitches, <em>{51, 38}</em>, for the first step maps to classes <em>{8, 1}</em>. This value will bit flip index 258 in the vector, because <em>2<sup>8</sup> + 2<sup>1</sup> = 258</em>. The vector is of the size 2<sup>9</sup> for each step, plus some binary counters and flags we won't talk about here.</p>
<p>This diagram shows the encoding part of the first step of the primer example as described:</p>
<p class="CDPAlignCenter CDPAlign"><img src="assets/b159a218-91b0-47dc-96c8-872cfb84fa5c.png"/></p>
<p>In that specific encoding, some information is lost because there are less class then MIDI notes. This means that, for example, if both MIDI notes, 35 and 36, map to the same class index 0, then the difference between either 35 or 36 is lost. In that specific case, 36 is chosen arbitrarily (you can actually see that from the previous example in the section, <em>Priming the model with Led Zeppelin</em>, the MIDI note 35 is lost).</p>
<p>This encoding is used for training when converting from the dataset into the sequences, and during generation, if a primer is used to initialize the model. When using a primer, the primer is encoded to produce input for the model. The model state is then initialized with that input.</p>
<p>The reverse of that operation is also important for a generation: when the model makes a new generation, it needs to be decoded to find the sequence it represents.</p>
<p>There are different ways of encoding the events, and others are used in Magenta. This is the encoding for the "drum_kit" configuration for this model, which is <kbd>LookbackEventSequenceEncoderDecoder</kbd>, implementing the encoding of repeated events using binary counters. The encoding of the <kbd>one_drum</kbd> configuration is different and simpler; you can check it out in <kbd>OneHotEventSequenceEncoderDecoder</kbd>.</p>
<p class="mce-root"/>
<p>The one-hot encoding of the drum classes is implemented in the <kbd>MultiDrumOneHotEncoding</kbd><span> </span><span>class</span><span>, which is used in other models as well, such as the MusicVAE model we'll see in Chapter 4. When instantiated with no drum pitches, it will use the reduced drum encoding of 9 classes that we saw in this section, which is expressive enough to capture many instruments, while keeping the model at a manageable size.</span></p>
<p>We'll be seeing more on the subject of encoding in the following chapters.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Sending MIDI files to other applications</h1>
                </header>
            
            <article>
                
<p>While generating MIDI files and writing them on disk is nice, dynamically sending the MIDI notes to another piece of software would be more useful, so that our Python Magenta application could interact directly with other music software. We will dedicate a whole chapter to this topic since there is a lot to talk about.</p>
<p>If you want to know more about this topic right now, you can go see <a href="8018122a-b28e-44ff-8533-5061a0ad356b.xhtml">Chapter 9</a>, <em>Making Magenta Interact with Music Applications</em>, and come back here later.</p>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Summary</h1>
                </header>
            
            <article>
                
<p>In this chapter, we introduced an RNN and the role it plays in music generation, by showing that operating on a sequence and remembering the past are mandatory properties for music generation.</p>
<p>We also generated a MIDI file using the Drums RNN model on the command line. We've covered most of its parameters and learned how to configure the model's output. By looking at the generation algorithm, we explained how it worked and how the different flags can change its execution.</p>
<p>By using the Drums RNN model in Python, we've shown how we can build a versatile application. By doing that, we learned about the MIDI specification, how Magenta encodes <kbd>NoteSequence</kbd> using Protobuf, and how to encode a sequence as a one-hot vector. We've also introduced the idea of sending the generated MIDI to other applications, a topic we'll cover in <a href="8018122a-b28e-44ff-8533-5061a0ad356b.xhtml">Chapter 9</a>, <em>Making Magenta Interact with Music Applications</em>.</p>
<p>In the next chapter, we'll be using other models to generate melody. We'll also continue writing Python code by finishing our learning of RNNs.</p>
<p class="mce-root"/>
<p class="mce-root"/>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Questions</h1>
                </header>
            
            <article>
                
<ol>
<li>If you want to generate a musical score, what do you train your model to do?</li>
<li class="mce-root">What are the properties that are interesting in RNNs concerning music prediction?</li>
<li>Given an RNN hidden layer with the notation <em>h(t + 2)</em>, what two inputs is the hidden layer getting?</li>
<li>Given the following parameters for the generation, <kbd>--num_steps 32</kbd> and <kbd>--qpm 80</kbd>, how long will the generated MIDI be in seconds? How many bars will it be?</li>
<li>What happens if you increase <kbd>--branch_factor</kbd> and increase <kbd>--temperature</kbd> during the generation phase?</li>
<li>How many nodes will the beam search algorithm go through at the last iteration for a generation of 3 steps with the <kbd>--branch_factor 4</kbd> and <kbd>--beam_size 2</kbd> parameters?</li>
<li>What is the Protobuf Message class that is used in Magenta to represent a sequence of MIDI notes? (NoteSequence)</li>
<li>Using the one-hot encoding described in the encoding section, what is the encoded vector for a step playing the MIDI notes, <em>{36, 40, 42}</em>?</li>
<li>Using the same encoding, what are the decoded MIDI notes from an encoded vector with index 131 at 1?</li>
</ol>


            </article>

            
        </section>
    

        <section>

                            <header>
                    <h1 class="header-title">Further reading</h1>
                </header>
            
            <article>
                
<ul>
<li><strong>The Unreasonable Effectiveness of Recurrent Neural Networks</strong>: An excellent article on RNNs (<a href="https://karpathy.github.io/2015/05/21/rnn-effectiveness/">karpathy.github.io/2015/05/21/rnn-effectiveness/</a>)</li>
<li><span><strong>Understanding softmax and the negative log-likelihood</strong>: Complimentary information on log-likelihood (<a href="https://ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/">ljvmiranda921.github.io/notebook/2017/08/13/softmax-and-the-negative-log-likelihood/</a></span>)</li>
<li><strong><span>Finding</span> <span>Structure</span> <span>in</span> <span>Time</span></strong>: An <span>original paper (1990) on RNNs</span> <span>(</span><a href="https://crl.ucsd.edu/~elman/Papers/fsit.pdf">crl.ucsd.edu/~elman/Papers/fsit.pdf)</a></li>
<li><strong>Gradient-Based Learning Applied to Document Recognition</strong>: An <span>original paper (1998) on CNNs</span> (<a href="http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf">yann.lecun.com/exdb/publis/pdf/lecun-98.pdf</a>)</li>
<li><strong>The Neural Network Zoo</strong>: An amazing list of neural network architectures that you can refer to throughout this book (<a href="https://www.asimovinstitute.org/neural-network-zoo/">asimovinstitute.org/neural-network-zoo/</a>)</li>
</ul>


            </article>

            
        </section>
    </body></html>