["```py\n        from sklearn.datasets import load_iris\n        ```", "```py\n        from sklearn.model_selection import train_test_split\n        ```", "```py\n        from sklearn.preprocessing import StandardScaler\n        ```", "```py\n        from sklearn.linear_model import Perceptron\n        ```", "```py\n    # Load the Iris dataset\n    ```", "```py\n    X, y = load_iris(return_X_y=True)\n    ```", "```py\n    # Split the data\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        X, y, random_state=0)\n    ```", "```py\n    # Rescale the data\n    ```", "```py\n    scaler = StandardScaler()\n    ```", "```py\n    X_train = scaler.fit_transform(X_train)\n    ```", "```py\n    X_test = scaler.transform(X_test)\n    ```", "```py\n    perc = Perceptron()perc.fit(X_train, y_train)\n    ```", "```py\n    # Print the R2-score on train and test\n    ```", "```py\n    print('R2-score on train set:',\n    ```", "```py\n        perc.score(X_train, y_train))\n    ```", "```py\n    print('R2-score on test set:',\n    ```", "```py\n        perc.score(X_test, y_test))\n    ```", "```py\nR2-score on train set: 0.9285714285714286\nR2-score on test set: 0.8421052631578947\n```", "```py\n    print('weights:', perc.coef_)\n    ```", "```py\n    print('bias:', perc.intercept_)\n    ```", "```py\nweights: [[-0.49201984  2.77164495 -3.07208498 -2.51124259]\n  [ 0.41482008 -1.94508614  3.2852582  -2.60994774]\n  [-0.32320969  0.48524348  5.73376173  4.93525738]] bias: [-2\\. -3\\. -6.]\n```", "```py\nimport numpy as np\nclass LogicalGatePerceptron:\n    def __init__(self, weights: np.array, bias: float):\n        self.weights = weights\n        self.bias = bias\n    def forward(self, X: np.array) -> int:\n        return (np.dot(\n            X, self.weights) + self.bias > 0).astype(int)\n```", "```py\n# Define X and y\nX = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\ny = [0, 0, 0, 1]\n```", "```py\ngate = LogicalGatePerceptron(np.array([1, 1]), -1)\ny_pred = gate.forward(X)\nprint('Error:', (y - y_pred).sum())\n```", "```py\nError: 0\n```", "```py\npip install torch\n```", "```py\n    from sklearn.datasets import fetch_california_housing\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    from sklearn.preprocessing import StandardScaler\n    ```", "```py\n    from sklearn.metrics import r2_score\n    ```", "```py\n    import matplotlib.pyplot as plt\n    ```", "```py\n        import torch\n        ```", "```py\n        import torch.nn as nn\n        ```", "```py\n        import torch.nn.functional as F\n        ```", "```py\n        from torch.utils.data import Dataset, DataLoader\n        ```", "```py\n    X, y = fetch_california_housing(return_X_y=True)\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        X.astype(np.float32), y.astype(np.float32),\n    ```", "```py\n           test_size=0.2, random_state=0)\n    ```", "```py\n    scaler = StandardScaler()\n    ```", "```py\n    X_train = scaler.fit_transform(X_train)\n    ```", "```py\n    X_test = scaler.transform(X_test)\n    ```", "```py\nclass CaliforniaDataset(Dataset):\n    def __init__(self, X: np.array, y: np.array):\n        self.X = torch.from_numpy(X)\n        self.y = torch.from_numpy(y)\n    def __len__(self) -> int:\n        return len(self.X)\n    def __getitem__(self, idx: int) -> tuple[torch.Tensor]:\n        return self.X[idx], self.y[idx]\n```", "```py\n    # Instantiate datasets\n    ```", "```py\n    training_data = CaliforniaDataset(X_train, y_train)\n    ```", "```py\n    test_data = CaliforniaDataset(X_test, y_test)\n    ```", "```py\n    # Instantiate data loaders\n    ```", "```py\n    train_dataloader = DataLoader(training_data,\n    ```", "```py\n        batch_size=64, shuffle=True)\n    ```", "```py\n    test_dataloader = DataLoader(test_data, batch_size=64,\n    ```", "```py\n        shuffle=True)\n    ```", "```py\n        class Net(nn.Module):\n        ```", "```py\n            def __init__(self, input_shape: int,\n        ```", "```py\n                hidden_units: int = 24):\n        ```", "```py\n                    super(Net, self).__init__()\n        ```", "```py\n                    self.hidden_units = hidden_units\n        ```", "```py\n                    self.fc1 = nn.Linear(input_shape,\n        ```", "```py\n                        self.hidden_units)\n        ```", "```py\n                    self.fc2 = nn.Linear(self.hidden_units,\n        ```", "```py\n                        self.hidden_units)\n        ```", "```py\n                    self.output = nn.Linear(self.hidden_units,\n        ```", "```py\n                        1)\n        ```", "```py\n            def forward(self,\n        ```", "```py\n                x: torch.Tensor) -> torch.Tensor:\n        ```", "```py\n                    x = self.fc1(x)\n        ```", "```py\n                    x = F.relu(x)\n        ```", "```py\n                    x = self.fc2(x)\n        ```", "```py\n                    x = F.relu(x)\n        ```", "```py\n                    output = self.output(x)\n        ```", "```py\n                    return output\n        ```", "```py\n    # Instantiate the network\n    ```", "```py\n    net = Net(X_train.shape[1])\n    ```", "```py\n    # Generate one random sample of 8 features\n    ```", "```py\n    random_data = torch.rand((1, X_train.shape[1]))\n    ```", "```py\n    # Compute the forward\n    ```", "```py\n    propagationprint(net(random_data))\n    ```", "```py\ntensor([[-0.0003]], grad_fn=<AddmmBackward0>)\n```", "```py\n    criterion = nn.MSELoss()\n    ```", "```py\n    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n    ```", "```py\n    losses = []\n    ```", "```py\n    # Loop over the dataset multiple times\n    ```", "```py\n    for epoch in range(10):\n    ```", "```py\n        # Reset the loss for this epoch\n    ```", "```py\n        running_loss = 0.\n    ```", "```py\n        For I, data in enumerate(train_dataloader, 0):\n    ```", "```py\n            # Get the inputs per batch: data is a list of [inputs, labels]\n    ```", "```py\n            inputs, labels = data\n    ```", "```py\n            # Zero the parameter gradients\n    ```", "```py\n            optimizer.zero_grad()\n    ```", "```py\n            # Forward propagate + backward + optimize\n    ```", "```py\n            outputs = net(inputs)\n    ```", "```py\n            # Unsqueeze for dimension matching\n    ```", "```py\n            labels = labels.unsqueeze(1)\n    ```", "```py\n            # Compute the loss\n    ```", "```py\n            Loss = criterion(outputs, labels)\n    ```", "```py\n            # Backpropagate and update the weights\n    ```", "```py\n            loss.backward()\n    ```", "```py\n            optimizer.step()\n    ```", "```py\n            # Add this loss to the running loss\n    ```", "```py\n            running_loss += loss.item()\n    ```", "```py\n         # Compute the loss for this epoch and add it to the list\n    ```", "```py\n        epoch_loss = running_loss / len(\n    ```", "```py\n            train_dataloader)\n    ```", "```py\n        losses.append(epoch_loss)\n    ```", "```py\n        # Print the epoch and training loss\n    ```", "```py\n        print(f'[epoch {epoch + 1}] loss: {\n    ```", "```py\n            epoch_loss:.3f}')print('Finished Training')\n    ```", "```py\n    plt.plot(losses)\n    ```", "```py\n    plt.xlabel('epoch')\n    ```", "```py\n    plt.ylabel('loss (MSE)')plt.show()\n    ```", "```py\n    # Compute the predictions with the trained neural\n    ```", "```py\n    Network\n    ```", "```py\n    y_train_pred = net(torch.tensor((\n    ```", "```py\n        X_train))).detach().numpy()\n    ```", "```py\n    y_test_pred = net(torch.tensor((\n    ```", "```py\n        X_test))).detach().numpy()\n    ```", "```py\n    # Compute the R2-score\n    ```", "```py\n    print('R2-score on training set:',\n    ```", "```py\n        r2_score(y_train, y_train_pred))\n    ```", "```py\n    print('R2-score on test set:',\n    ```", "```py\n        r2_score(y_test, y_test_pred))\n    ```", "```py\nR2-score on training set: 0.7542622050620708 R2-score on test set: 0.7401526252651656\n```", "```py\nimport numpy as np\nimport matplotlib.pyplot as plt\nx = np.arange(-2, 2, 0.02)\nsigmoid = 1./(1+np.exp(-x))\ntanh = (np.exp(x)-np.exp(-x))/(np.exp(x)+np.exp(-x))\nrelu = np.max([np.zeros(len(x)), x], axis=0)\nplt.plot(x, sigmoid)\nplt.plot(x, tanh)\nplt.plot(x, relu)plt.grid()\nplt.xlabel('x')\nplt.ylabel('activation')\nplt.legend(['sigmoid', 'tanh', 'relu'])\nplt.show()\n```", "```py\n    from sklearn.datasets import load_breast_cancer\n    ```", "```py\n    from sklearn.model_selection import train_test_split\n    ```", "```py\n    from sklearn.preprocessing import StandardScaler\n    ```", "```py\n    from sklearn.metrics import accuracy_score\n    ```", "```py\n    import matplotlib.pyplot as plt\n    ```", "```py\n    import torchimport torch.nn as nn\n    ```", "```py\n    import torch.nn.functional as F\n    ```", "```py\n    from torch.utils.data import Dataset, DataLoader\n    ```", "```py\n    X, y = load_breast_cancer(return_X_y=True)\n    ```", "```py\n    X_train, X_test, y_train, y_test = train_test_split(\n    ```", "```py\n        X.astype(np.float32), y.astype(np.float32),\n    ```", "```py\n        test_size=0.2, random_state=0)\n    ```", "```py\n    class BreastCancerDataset(Dataset):\n    ```", "```py\n        def __init__(self, X: np.array, y: np.array,\n    ```", "```py\n            x_scaler: StandardScaler = None):\n    ```", "```py\n                if x_scaler is None:\n    ```", "```py\n                    self.x_scaler = StandardScaler()\n    ```", "```py\n                    X = self.x_scaler.fit_transform(X)\n    ```", "```py\n                else:\n    ```", "```py\n                    self.x_scaler = x_scaler\n    ```", "```py\n                    X = self.x_scaler.transform(X)\n    ```", "```py\n                self.X = torch.from_numpy(X)\n    ```", "```py\n                self.y = torch.from_numpy(y)\n    ```", "```py\n        def __len__(self) -> int:\n    ```", "```py\n            return len(self.X)\n    ```", "```py\n        def __getitem__(self, idx: int) -> tuple[torch.Tensor]:\n    ```", "```py\n            return self.X[idx], self.y[idx]\n    ```", "```py\n    training_data = BreastCancerDataset(X_train, y_train)\n    ```", "```py\n    test_data = BreastCancerDataset(X_test, y_test,\n    ```", "```py\n        training_data.x_scaler)\n    ```", "```py\n    train_dataloader = DataLoader(training_data,\n    ```", "```py\n        batch_size=64, shuffle=True)\n    ```", "```py\n    test_dataloader = DataLoader(test_data, batch_size=64,\n    ```", "```py\n        shuffle=True)\n    ```", "```py\n    class Net(nn.Module):\n    ```", "```py\n        def __init__(self, input_shape: int,\n    ```", "```py\n            hidden_units: int = 24):\n    ```", "```py\n                super(Net, self).__init__()\n    ```", "```py\n                    self.hidden_units = hidden_units\n    ```", "```py\n                    self.fc1 = nn.Linear(input_shape,\n    ```", "```py\n                        self.hidden_units)\n    ```", "```py\n                    self.fc2 = nn.Linear(\n    ```", "```py\n                        self.hidden_units,\n    ```", "```py\n                        self.hidden_units)\n    ```", "```py\n                    self.output = nn.Linear(\n    ```", "```py\n                        self.hidden_units, 1)\n    ```", "```py\n        def forward(self, x: torch.Tensor) -> torch.Tensor:\n    ```", "```py\n            x = self.fc1(x)\n    ```", "```py\n            x = F.relu(x)\n    ```", "```py\n            x = self.fc2(x)\n    ```", "```py\n            x = F.relu(x)\n    ```", "```py\n            output = torch.sigmoid(self.output(x))\n    ```", "```py\n            return output\n    ```", "```py\n    # Instantiate the network\n    ```", "```py\n    net = Net(X_train.shape[1])\n    ```", "```py\n    # Generate one random sample\n    ```", "```py\n    random_data = torch.rand((1, X_train.shape[1]))\n    ```", "```py\n    # Compute the forward propagation\n    ```", "```py\n    print(net(random_data))\n    ```", "```py\ntensor([[0.4487]], grad_fn=<SigmoidBackward0>)\n```", "```py\n    criterion = nn.BCELoss()\n    ```", "```py\n    optimizer = torch.optim.Adam(net.parameters(),\n    ```", "```py\n        lr=0.001)\n    ```", "```py\n        train_losses = []\n        ```", "```py\n        test_losses = []\n        ```", "```py\n        # Loop over the dataset 50 times\n        ```", "```py\n        for epoch in range(50):\n        ```", "```py\n            ## Train the model on the training set\n        ```", "```py\n            running_train_loss = 0.\n        ```", "```py\n            # Switch to train mode\n        ```", "```py\n            net.train()\n        ```", "```py\n            # Loop over the batches in train set\n        ```", "```py\n            for i, data in enumerate(train_dataloader, 0):\n        ```", "```py\n                # Get the inputs: data is a list of [inputs, labels]\n        ```", "```py\n                inputs, labels = data\n        ```", "```py\n                # Zero the parameter gradients\n        ```", "```py\n                optimizer.zero_grad()\n        ```", "```py\n                # Forward + backward + optimize\n        ```", "```py\n                outputs = net(inputs)\n        ```", "```py\n                loss = criterion(outputs, labels.unsqueeze(1))\n        ```", "```py\n                loss.backward()\n        ```", "```py\n                optimizer.step()\n        ```", "```py\n                # Add current loss to running loss\n        ```", "```py\n                running_train_loss += loss.item()\n        ```", "```py\n            # Once epoch is over, compute and store the epoch loss\n        ```", "```py\n            train_epoch_loss = running_train_loss / len(\n        ```", "```py\n                train_dataloader)\n        ```", "```py\n            train_losses.append(train_epoch_loss)\n        ```", "```py\n            ## Evaluate the model on the test set\n        ```", "```py\n            running_test_loss = 0.\n        ```", "```py\n            # Switch to eval model\n        ```", "```py\n            net.eval()\n        ```", "```py\n            with torch.no_grad():\n        ```", "```py\n                # Loop over the batches in test set\n        ```", "```py\n                for i, data in enumerate(test_dataloader, 0):\n        ```", "```py\n                    # Get the inputs\n        ```", "```py\n                    inputs, labels = data\n        ```", "```py\n                    # Compute forward propagation\n        ```", "```py\n                    outputs = net(inputs)\n        ```", "```py\n                    # Compute loss\n        ```", "```py\n                    loss = criterion(outputs,\n        ```", "```py\n                        labels.unsqueeze(1))\n        ```", "```py\n                    # Add to running loss\n        ```", "```py\n                    running_test_loss += loss.item()\n        ```", "```py\n                    # Compute and store the epoch loss\n        ```", "```py\n                    test_epoch_loss = running_test_loss / len(\n        ```", "```py\n                        test_dataloader)\n        ```", "```py\n                    test_losses.append(test_epoch_loss)\n        ```", "```py\n            # Print stats\n        ```", "```py\n            print(f'[epoch {epoch + 1}] Training loss: {\n        ```", "```py\n                train_epoch_loss:.3f} | Test loss: {\n        ```", "```py\n                    test_epoch_loss:.3f}')\n        ```", "```py\n            print('Finished Training')\n        ```", "```py\n    plt.plot(train_losses, label='train')\n    ```", "```py\n    plt.plot(test_losses, label='test')\n    ```", "```py\n    plt.xlabel('epoch')plt.ylabel('loss (BCE)')\n    ```", "```py\n    plt.legend()\n    ```", "```py\n    plt.show()\n    ```", "```py\n        # Compute the predictions with the trained neural network\n        ```", "```py\n        y_train_pred = net(torch.tensor((\n        ```", "```py\n            training_data.x_scaler.transform(\n        ```", "```py\n                X_train)))).detach().numpy() > 0.5\n        ```", "```py\n        y_test_pred = net(torch.tensor((\n        ```", "```py\n            training_data.x_scaler.transform(\n        ```", "```py\n                X_test)))).detach().numpy() > 0.5\n        ```", "```py\n        # Compute the accuracy score\n        ```", "```py\n        print('Accuracy on training set:', accuracy_score(\n        ```", "```py\n            y_train, y_train_pred))\n        ```", "```py\n        print('Accuracy on test set:', accuracy_score(y_test,\n        ```", "```py\n            y_test_pred))\n        ```", "```py\nAccuracy on training set: 0.9912087912087912 Accuracy on test set: 0.9649122807017544\n```", "```py\n    import torch\n    ```", "```py\n    import torch.nn as nn\n    ```", "```py\n    import torch.nn.functional as F\n    ```", "```py\n    from torch.utils.data import DataLoader\n    ```", "```py\n    from torchvision.datasets import MNIST\n    ```", "```py\n    import torchvision.transforms as transforms\n    ```", "```py\n    import matplotlib.pyplot as plt\n    ```", "```py\n        transform = transforms.Compose([transforms.ToTensor(),\n        ```", "```py\n            transforms.Normalize((0.1307), (0.3081)),\n        ```", "```py\n            transforms.Lambda(torch.flatten)])\n        ```", "```py\n    trainset = MNIST('./data', train=True, download=True,\n    ```", "```py\n        transform=transform)\n    ```", "```py\n    train_dataloader = DataLoader(trainset, batch_size=64,\n    ```", "```py\n        shuffle=True)\n    ```", "```py\n    testset = MNIST('./data', train=False, download=True,\n    ```", "```py\n        transform=transform)\n    ```", "```py\n    test_dataloader = DataLoader(testset, batch_size=64,\n    ```", "```py\n        shuffle=True)\n    ```", "```py\n    class Net(nn.Module):\n    ```", "```py\n        def __init__(self, input_shape: int,\n    ```", "```py\n            hidden_units: int = 24):\n    ```", "```py\n                super(Net, self).__init__()\n    ```", "```py\n                self.hidden_units = hidden_units\n    ```", "```py\n                self.fc1 = nn.Linear(input_shape,\n    ```", "```py\n                    self.hidden_units)\n    ```", "```py\n                self.fc2 = nn.Linear(\n    ```", "```py\n                    self.hidden_units,\n    ```", "```py\n                    self.hidden_units)\n    ```", "```py\n                self.output = nn.Linear(\n    ```", "```py\n                    self.hidden_units, 10)\n    ```", "```py\n        def forward(self,\n    ```", "```py\n            x: torch.Tensor) -> torch.Tensor:\n    ```", "```py\n                x = self.fc1(x)\n    ```", "```py\n                x = F.relu(x)\n    ```", "```py\n                x = self.fc2(x)\n    ```", "```py\n                x = F.relu(x)\n    ```", "```py\n                output = torch.softmax(self.output(x),\n    ```", "```py\n                    dim=1)\n    ```", "```py\n                return output\n    ```", "```py\n    # Instantiate the model\n    ```", "```py\n    net = Net(784)\n    ```", "```py\n    # Generate randomly one random 28x28 image as a 784 values tensor\n    ```", "```py\n    random_data = torch.rand((1, 784))\n    ```", "```py\n    result = net(random_data)\n    ```", "```py\n    print('Resulting output tensor:', result)\n    ```", "```py\n    print('Sum of the output tensor:', result.sum())\n    ```", "```py\nResulting output tensor: tensor([[0.0918, 0.0960, 0.0924, 0.0945, 0.0931, 0.0745, 0.1081, 0.1166, 0.1238,              0.1092]], grad_fn=<SoftmaxBackward0>) Sum of the output tensor: tensor(1.0000, grad_fn=<SumBackward0>)\n```", "```py\n    criterion = nn.CrossEntropyLoss()\n    ```", "```py\n    optimizer = torch.optim.Adam(net.parameters(),\n    ```", "```py\n        lr=0.001)\n    ```", "```py\n    def epoch_step(net, dataloader, training_set: bool):\n    ```", "```py\n        running_loss = 0.\n    ```", "```py\n        Correct = 0.\n    ```", "```py\n        For i, data in enumerate(dataloader, 0):\n    ```", "```py\n            # Get the inputs: data is a list of [inputs, labels]\n    ```", "```py\n            inputs, labels = data\n    ```", "```py\n            if training_set:\n    ```", "```py\n                # Zero the parameter gradients\n    ```", "```py\n                optimizer.zero_grad()\n    ```", "```py\n                # Forward + backward + optimize\n    ```", "```py\n                outputs = net(inputs)\n    ```", "```py\n                loss = criterion(outputs, labels)\n    ```", "```py\n                if training_set:\n    ```", "```py\n                    loss.backward()\n    ```", "```py\n                    optimizer.step()\n    ```", "```py\n                # Add correct predictions for this batch\n    ```", "```py\n                correct += (outputs.argmax(\n    ```", "```py\n                    dim=1) == labels).float().sum()\n    ```", "```py\n                # Compute loss for this batch\n    ```", "```py\n                running_loss += loss.item()\n    ```", "```py\n        return running_loss, correct\n    ```", "```py\n# Create empty lists to store the losses and accuracies\ntrain_losses = []\ntest_losses = []\ntrain_accuracy = []\ntest_accuracy = []\n# Loop over the dataset 20 times for 20 epochs\nfor epoch in range(20):\n    ## Train the model on the training set\n    net.train()\n    running_train_loss, correct = epoch_step(net,\n        dataloader=train_dataloader,training_set=True)\n    # Compute and store loss and accuracy for this epoch\n    train_epoch_loss = running_train_loss / len(\n        train_dataloader)\n    train_losses.append(train_epoch_loss)\n    train_epoch_accuracy = correct / len(trainset)\n     rain_accuracy.append(train_epoch_accuracy)\n    ## Evaluate the model on the test set\n    net.eval()\n    with torch.no_grad():\n        running_test_loss, correct = epoch_step(net,\n            dataloader=test_dataloader,training_set=False)\n        test_epoch_loss = running_test_loss / len(\n            test_dataloader)\n        test_losses.append(test_epoch_loss)\n        test_epoch_accuracy = correct / len(testset)\n        test_accuracy.append(test_epoch_accuracy)\n    # Print stats\n    print(f'[epoch {epoch + 1}] Training: loss={train_epoch_loss:.3f} accuracy={train_epoch_accuracy:.3f} |\\\n\\t Test: loss={test_epoch_loss:.3f} accuracy={test_epoch_accuracy:.3f}')\nprint('Finished Training')\n```", "```py\n    plt.plot(train_losses, label='train')\n    ```", "```py\n    plt.plot(test_losses, label='test')\n    ```", "```py\n    plt.xlabel('epoch')plt.ylabel('loss (CE)')\n    ```", "```py\n    plt.legend()plt.show()\n    ```", "```py\n    plt.plot(train_accuracy, label='train')\n    ```", "```py\n    plt.plot(test_accuracy, label='test')\n    ```", "```py\n    plt.xlabel('epoch')\n    ```", "```py\n    plt.ylabel('Accuracy')plt.legend()plt.show()\n    ```", "```py\n# Save the model's state dict\ntorch.save(net.state_dict(), 'path_to_model.pt')\n# Instantiate a new model\nnew_model = Net(784)\n# Load the model's weights\nnew_model.load_state_dict(torch.load('path_to_model.pt'))\n```", "```py\nplt.figure(figsize=(12, 8))\nfor i in range(6):\n    plt.subplot(3, 3, i+1)\n    # Compute the predicted number\n    pred = new_model(\n        testset[i][0].unsqueeze(0)).argmax(axis=1)\n    # Display the image and predicted number as title\n    plt.imshow(testset[i][0].detach().numpy().reshape(\n        28, 28), cmap='gray_r')\n    plt.title(f'Prediction: {pred.detach().numpy()}')\n    plt.axis('off')\n```", "```py\ndevice = torch.device(\n    \"cuda\" if torch.cuda.is_available() else \"cpu\") print(device)\n```", "```py\ntrain_losses = []\ntest_losses = []\ntrain_accuracy = []\ntest_accuracy = []\n# Move the model to the GPU\nnet = net.to(device)\nfor epoch in range(20):\n    running_train_loss = 0.\n    correct = 0.\n    net.train()\n    for i, data in enumerate(train_dataloader, 0):\n        inputs, labels = data\n        # Move the data to the device\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n    running_test_loss = 0.\n    correct = 0.\n    net.eval()\n    with torch.no_grad():\n        for i, data in enumerate(test_dataloader, 0):\n            inputs, labels = data\n            # Move the data to the device\n            inputs = inputs.to(device)\n            labels = labels.to(device)\nprint('Finished Training')\n```"]