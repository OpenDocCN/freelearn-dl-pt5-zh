- en: Data Preparation for Training
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: So far, we've used existing Magenta pre-trained models since they are quite
    powerful and easy to use. But training our own models is crucial since it allows
    us to generate music in a specific style or generate specific structures or instruments.
    Building and preparing a dataset is the first step before training our own model.
    To do that, we need to look at existing datasets and APIs that will help us to
    find meaningful data. Then, we need to build two datasets in MIDI for specific
    styles—dance and jazz. Finally, we will need to prepare the MIDI files for training
    using data transformations and pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following topics will be covered in this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: Looking at existing datasets
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a dance music dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a jazz dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing the data using pipelines
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we''ll use the following tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**A command line** or **Bash** to launch Magenta from the Terminal'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Python** and its libraries'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The Python **multiprocessing** module for multi-threaded data preparation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Matplotlib** to plot our data preparation results'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Magenta** to launch data pipeline conversion'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**MIDI**, **ABCNotation**, and **MusicXML** as data formats'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: External APIs such as **Last.fm**
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In Magenta, we'll make use of **data pipelines**. We will explain these in depth
    later in this chapter, but if you feel like you need more information, the pipeline
    README file in Magenta's source code ([github.com/tensorflow/magenta/tree/master/magenta/pipelines](https://github.com/tensorflow/magenta/tree/master/magenta/pipelines))
    is a good place to start. You can also take a look at Magenta's code, which is
    well documented. There's also additional content in the *Further reading* section.
  prefs: []
  type: TYPE_NORMAL
- en: The code for this chapter can be found in this book's GitHub repository, in
    the `Chapter06` folder, which is located at [github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter06](https://github.com/PacktPublishing/hands-on-music-generation-with-magenta/tree/master/Chapter06).
    For this chapter, you should use the `cd Chapter06` command before you start.
  prefs: []
  type: TYPE_NORMAL
- en: Check out the following video to see the Code in Action: [http://bit.ly/3aXWLmC](http://bit.ly/3aXWLmC)
  prefs: []
  type: TYPE_NORMAL
- en: Looking at existing datasets
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we'll be preparing some data for training. Note that this will
    be covered in more detail in [Chapter 7](6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml),
    *Training Magenta Models*. Preparing data and training models are two different
    activities that are done in tandem—first, we prepare the data, then train the
    models, and finally go back to preparing the data to improve our model's performance.
  prefs: []
  type: TYPE_NORMAL
- en: First, we'll start by looking at symbolic representations other than MIDI, such
    as MusicXML and ABCNotation, since Magenta also handles them, even if the datasets
    we'll be working with in this chapter will be in MIDI only. Then, we'll provide
    an overview of existing datasets, including datasets from the Magenta team that
    were used to train some models we've already covered. This overview is by no means
    exhaustive but can serve as a starting point when it comes to finding training
    data.
  prefs: []
  type: TYPE_NORMAL
- en: The main dataset we'll be focusing on is the **Lakh MIDI dataset** (**LMD**),
    a recent and well crafted MIDI dataset that will serve as a basis for most of
    our examples. You can also use other datasets; the code we are providing here
    can be easily adapted to other content.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at symbolic representations
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are three main symbolic representations: MIDI, MusicXML, and ABCNotation.
    We''ve already covered MIDI in detail, but we haven''t talked about MusicXML and
    ABCNotation yet. While we won''t be using these two in this chapter, it is nice
    to know they exist and that Magenta can handle them as well as MIDI files.'
  prefs: []
  type: TYPE_NORMAL
- en: '**MusicXML**, as its name suggests, is an XML-based musical representation
    format. It has the advantage of being text-based, meaning it doesn''t require
    an external library such as PrettyMIDI to be processed and is supported in many
    sheet music editors, such as MuseScore. You can find the MusicXML specification
    on its official website: [www.musicxml.com](https://www.musicxml.com/).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s an example of a MusicXML file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '**ABCNotation**, as its name suggests, is a text-based musical representation
    format based on the letter notation (A-G). The format is rather compact, with
    some header information concerning the whole file, followed by the content of
    the song using the letter notation. The notation is also well supported in sheet
    music software. Here''s an example of an ABCNotation file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: We will provide some ABCNotation content in the *Looking at other datasets*
    section of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: The tools Magenta provides for preparing datasets for training handle MIDI,
    MusicXML, and ABCNotation in a single command, which is really handy. The `convert_dir_to_note_sequences` command
    will parse the content depending on its type and return `NoteSequence` regardless.
  prefs: []
  type: TYPE_NORMAL
- en: We'll look at these tools in more detail in [Chapter 7](6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml),
    *Training Magenta Models*.
  prefs: []
  type: TYPE_NORMAL
- en: Building a dataset from the ground up
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using an existing dataset and trimming it down is the easiest way to start building
    and preparing a dataset for training since it is a rather fast method of getting
    enough data for training.
  prefs: []
  type: TYPE_NORMAL
- en: Another option is to build the dataset from scratch, either by creating new
    data for it or by incorporating data from various sources. While requiring more
    work, this method might give better results during training since the resulting
    data is carefully selected. This is a process you should follow if you are a musician
    and have your own MIDI files ready.
  prefs: []
  type: TYPE_NORMAL
- en: Using the LMD for MIDI and audio files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The LMD ([colinraffel.com/projects/lmd](https://colinraffel.com/projects/lmd/))
    is one of the most complete and easy-to-use MIDI datasets. If you don't have anything
    handy right now, we recommend using it. This chapter's code will use this dataset,
    but you can also follow the examples using another dataset since even if the information
    is different, most of the techniques we will use here can still be used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset has various distributions, but we''ll be looking at the following
    ones in particular:'
  prefs: []
  type: TYPE_NORMAL
- en: '**LMD-full**: This is the full dataset, which contains 176,581 MIDI files.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LMD-matched**: The dataset is partially matched to another dataset, the **million
    song dataset** (**MSD**), which contains 45,129 MIDI files. This subset is useful
    because the matched files contain metadata such as artist and title.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LMD-aligned**: The LMD-matched dataset is aligned with an audio MP3 preview.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Using the MSD for metadata information
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MSD ([millionsongdataset.com](http://millionsongdataset.com/)) is a large
    scale dataset that has over a million entries containing audio features and metadata
    information. We won't be using the MSD dataset directly; instead, we'll be using
    the matched content included in the LMD-matched dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Using the MAESTRO dataset for performance music
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **MIDI and Audio Edited for Synchronous TRacks and Organization** (**MAESTRO**)
    dataset ([magenta.tensorflow.org/datasets/maestro](https://magenta.tensorflow.org/datasets/maestro))
    is curated by the Magenta team and is based on live performances that have been
    recorded to both audio and MIDI, for over 200 hours of content. Since the recorded
    performances have been played by humans, the notes have expressive timing and
    dynamics (effects pedals are also represented).
  prefs: []
  type: TYPE_NORMAL
- en: The dataset's content comes from the International Piano-e-Competition ([piano-e-competition.com/](http://piano-e-competition.com/)),
    which mainly contains classical music. This dataset has multiple usages, such
    as automatic audio to symbolic representation transcription, which has been used
    to train the Magenta Onsets and Frames model. It has also been used to train the
    Performance RNN model we covered in [Chapter 3](48023567-4100-492a-a28e-53b18a63e01e.xhtml),
    *Generating Polyphonic Melodies*.
  prefs: []
  type: TYPE_NORMAL
- en: Similar to MAESTRO but with less content, you also have the MusicNet and MAPS
    datasets available. We won't be using the MAESTRO dataset for our examples, but
    it is an important dataset you might want to look at. For more information, take
    a look at the *Further reading* section at the end of this chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Groove MIDI Dataset for groovy drums
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The **Groove MIDI Dataset** ([www.tensorflow.org/datasets/catalog/groove](https://www.tensorflow.org/datasets/catalog/groove))
    is composed of 13.6 hours of aligned MIDI and (synthesized) audio of human-performed,
    tempo-aligned expressive drumming captured on an electronic drum. The dataset
    is also split into 2-bars and 4-bars MIDI segments and has been used to train
    the GrooVAE model, which we presented in [Chapter 4](838da33e-26a9-4701-bfd3-5014dfff4146.xhtml),
    *Latent Space Interpolation with MusicVAE*.
  prefs: []
  type: TYPE_NORMAL
- en: The GMD is an impressive dataset of recorded performances from professional
    drummers, who also improvised for the occasion, resulting in a diverse dataset.
    The performances are annotated with a genre, which can be used to filter and extract
    certain MIDI files.
  prefs: []
  type: TYPE_NORMAL
- en: While not quantized, the GMD can also be used to train quantized models, such
    as the drums RNN. The pipelines, which will be shown in the *Preparing the data
    using pipelines* section, can transform input data such as unquantized MIDI into
    quantized MIDI.
  prefs: []
  type: TYPE_NORMAL
- en: Using the Bach Doodle Dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Bach Doodle is a web application made by Google to celebrate the anniversary
    of the German composer and musician, Johann Sebastian Bach. Doodle allows the
    user to compose a 2-bars melody and ask the application to harmonize the input
    melody in Bach's style using Coconet and TensorFlow.js running in the browser.
  prefs: []
  type: TYPE_NORMAL
- en: 'The resulting Bach Doodle Dataset ([magenta.tensorflow.org/datasets/bach-doodle](https://magenta.tensorflow.org/datasets/bach-doodle))
    of harmonized composition is impressive: 21.6 million harmonizations for about
    6 years of user-entered music. It contains the user input sequence and the output
    sequence from the network in note sequence format, as well as some metadata, such
    as the country of the user and the number of times it was played.'
  prefs: []
  type: TYPE_NORMAL
- en: See the *Further reading* section for more information about data visualization
    regarding the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Using the NSynth dataset for audio content
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We covered the NSynth dataset ([www.tensorflow.org/datasets/catalog/nsynth](https://www.tensorflow.org/datasets/catalog/nsynth))
    in the previous chapter, *Audio Generation with NSynth and GANSynth*. We won't
    be covering audio training in this book, but this is a good dataset to use for
    training the GANSynth model.
  prefs: []
  type: TYPE_NORMAL
- en: Using APIs to enrich existing data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Using APIs is a good way of finding more information about MIDI tracks. We'll
    be showing an example of this in this chapter, where we'll query an API using
    the song artist and title to find genres and tags associated with the song.
  prefs: []
  type: TYPE_NORMAL
- en: There are multiple services you can use to find such information. We won't list
    all of them, but two good starting points are **Last.fm** ([www.last.fm/](https://www.last.fm/))
    and Spotify's **Echo Nest** ([static.echonest.com/enspex/](http://static.echonest.com/enspex/)).
    The **tagtraum** dataset ([www.tagtraum.com/msd_genre_datasets.html](http://www.tagtraum.com/msd_genre_datasets.html))
    is another good dataset for genre, which is based on MSD.
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we'll be using the **Last.fm** API to fetch song information,
    and we'll learn how to use its API in an upcoming section, *Building a jazz dataset*.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at other data sources
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Outside of curated datasets, there are tons of other sources for music files
    on the internet—mainly websites offering searchable databases. The downside of
    using such sources is that you'll have to download and verify each file by hand,
    which might be time-consuming but necessary if you want to build your own dataset
    from the ground up.
  prefs: []
  type: TYPE_NORMAL
- en: Websites such as **MidiWorld** ([www.midiworld.com](https://www.midiworld.com/))
    and **MuseScore** ([musescore.com](https://musescore.com/)) contain a lot of MIDI
    files, often classified by style, composer, and instrument. There are also posts
    on Reddit that list pretty big MIDI datasets of varying quality.
  prefs: []
  type: TYPE_NORMAL
- en: For formats other than MIDI, you have the ABCNotation website ([abcnotation.com](http://abcnotation.com)),
    which features over 600,000 files of mainly folk and traditional music, but with
    links to other sources.
  prefs: []
  type: TYPE_NORMAL
- en: Building a dance music dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have datasets available so that we can build our own dataset, we'll
    look at different ways of using the information contained in a MIDI file. This
    section will serve as an introduction to the different tools that can be used
    for dataset creation using only MIDI files. In this section, we'll use the **LMD-full**
    distribution.
  prefs: []
  type: TYPE_NORMAL
- en: In the next section, *Building a jazz dataset*, we will delve deeper into using
    external information.
  prefs: []
  type: TYPE_NORMAL
- en: Threading the execution to handle large datasets faster
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: When building datasets, we want our code to execute fast because of the amount
    of data we'll be handling. In Python, using threading and multiprocessing is one
    way to go. There are many ways of executing code in parallel in Python, but we'll
    be using the `multiprocessing` module because of its simple design, which is also
    similar to other popular techniques such as using the `joblib` module.
  prefs: []
  type: TYPE_NORMAL
- en: You can find this code in the `multiprocessing_utils.py` file, in the source
    code of this chapter. There are more comments and content in the source code,
    so check it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'For our examples, we''ll be using the following code to start four threads
    that will execute in parallel:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain this code block in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: You can modify the number of threads in `Pool(4)` to fit your hardware. One
    thread per physical CPU is often a proper value for good performance.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We instantiate `Manager`, which is needed to share resources inside threads,
    and `AtomicCounter`, which is available in the `multiprocessing_utils` module
    from this book's code, to share a counter between the threads.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Finally, we use the `pool.starmap` method to launch the `process` method on
    the `elements` list (we'll be defining this method soon). The `starmap` method
    calls the `process` method with two parameters, the first one being **one element**
    of the `elements` list, with the second being `counter`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `process` method will handle one element at the time and will increment
    the counter at each call.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `process` method should be able to return `None`, making it possible to
    filter out elements.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The thread's life cycle and the split of the elements list is handled by the
    `multiprocessing` module.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Extracting drum instruments from a MIDI file
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MIDI files can contain many instruments—multiple percussion instruments, pianos,
    guitars, and more. Extracting specific instruments and saving the result in a
    new MIDI file is a necessary step when building a dataset.
  prefs: []
  type: TYPE_NORMAL
- en: In this example, we'll be using `PrettyMIDI` to fetch the instruments information
    into a MIDI file, extract the drum instruments, merge them into a single drum
    instrument, and save the resulting instrument in a new MIDI file. Some MIDI files
    have multiple drum instruments to split the drum into multiple parts, such as
    bass drum, snare, and so on. For our example, we've chosen to merge them, but
    depending on what you are trying to do, you might want to keep them separate or
    keep only some parts.
  prefs: []
  type: TYPE_NORMAL
- en: You can find this code in the `chapter_06_example_00.py` file, in the source
    code of this chapter. There are more comments and content in the source code,
    so check it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `extract_drums` method takes `midi_path` and returns a single `PrettyMIDI`
    instance containing one merged drum instrument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: First, we use `copy.deepcopy` to copy the whole MIDI file content because we
    still want to keep the time signatures and tempo changes from the original file.
    That information is kept in the `PrettyMIDI` instance that's returned in the `copy.deepcopy` method
    from the `pm_drums` variable. Then, we filter the instruments using the `is_drum`
    property. If there are multiple drum instruments, we merge them together into
    a new `Instrument` by copying the notes.
  prefs: []
  type: TYPE_NORMAL
- en: Detecting specific musical structures
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we can extract specific instruments from the MIDI files, we can also
    find specific structures in the MIDI files to further refine our dataset. Dance
    and techno music, in most cases, have a bass drum on each beat, giving you that
    inescapable urge to dance. Let's see whether we can find that in our MIDI files.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `get_bass_drums_on_beat` method returns the proportion of bass drums that
    fall directly on the beat:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'The `get_beats` method from `PrettyMIDI` returns an array stating the start
    time of each beat. For example, on a 150 QPM file, we have the following array:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Then, we take only the bass drum pitches (35 or 36) in the given MIDI and make
    the assumption that we have exactly one instrument because our previous method,
    `extract_drums`, should have been called before. Then, we check whether, for each
    beat, a bass drum was played at that time and return the proportion as `float`.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing the beats of our MIDI files
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's put everything together to check our results.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll add some arguments to our program using the `argparse` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: Here, we've declared two command-line arguments, `--path_output_dir` and `--bass_drums_on_beat_threshold`,
    using the `argparse` module. The output directory is useful if we wish to save
    the extracted MIDI files in a separate folder, while the threshold is useful for
    filtering more or less of the extracted MIDI sequences.
  prefs: []
  type: TYPE_NORMAL
- en: Writing the process method
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have our arguments ready, we can write the processing method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is the `process` method that will be called by our multi-threaded code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: The `process` method will create the output directory if it doesn't already
    exist. Then, it will call the `extract_drums` method with the given MIDI path,
    and then call the `get_bass_drums_on_beat` method using the returned `PrettyMIDI`
    drum, which returns the `bass_drums_on_beat` as a proportion. Then, if the value
    is over the threshold, we save that MIDI file on disk; otherize, we exit the method.
  prefs: []
  type: TYPE_NORMAL
- en: The return values of the `process` method are important – by returning the `PrettyMIDI`
    file and the proportion of bass drums on beat, we'll be able to make statistics
    about our dataset to make informed decisions about its size and content. The `process`
    method can also return an empty (`None`) value or raise an exception, which will
    make the caller drop that MIDI file.
  prefs: []
  type: TYPE_NORMAL
- en: Calling the process method using threads
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have a processing method, we can use it to launch the execution.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s create an `app` method that calls the `process` method using threads:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: The `app` method will be called with a list of MIDI paths when the program launches.
    First, we clean up the output directory for the process method so that we can
    write in it. Then, we start four threads using `Pool(4)` (refer to the previous
    section, *Threading the execution to handle large datasets*, for more information).
    Finally, we calculate how many results were returned for information purposes.
  prefs: []
  type: TYPE_NORMAL
- en: Plotting the results using Matplotlib
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Using the returned results, we can find statistics about our dataset. As an
    example, let''s plot the drum length:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'We are using Matplotlib ([matplotlib.org/](https://matplotlib.org/)), a popular
    and easy to use plotting library for Python. This will result in the following
    output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dd14f85c-4db9-49f8-a1f6-94e65d9fd4df.png)'
  prefs: []
  type: TYPE_IMG
- en: Making plots of different statistics helps you visualize the content and size
    of your dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Processing a sample of the dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we have everything in place, we can call the `app` method using a subset
    of our dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll use a smaller sample size to make sure our code works properly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s explain the preceding code in more detail:'
  prefs: []
  type: TYPE_NORMAL
- en: We declare two new arguments, `--sample_size` and `--path_dataset_dir`. The
    first declares the size of the sample, while the second declares the location
    of the dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Then, we use `glob.glob` on the root folder of the dataset, which will return
    a list of paths, with one element per MIDI file.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Because this operation takes a while on big datasets, you can also cache the
    result on disk (using the `pickle` module, for example) if you execute it often.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We use `random.sample` to take a subset of the MIDI paths and use the resulting
    MIDI paths to call the `app` method.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can use one of the distributions of LMD to launch the following code. You
    need to download it (for example, the **LMD-full** distribution) and extract the
    ZIP file. See the *Using the Lakh MIDI Dataset* section for the download links.
  prefs: []
  type: TYPE_NORMAL
- en: 'To launch this code with a small sample size, in a Terminal, use the following
    command (by replacing `PATH_DATASET` with the root folder of the extracted dataset
    and `PATH_OUTPUT`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'The extracted MIDI file will be in `PATH_OUTPUT`, resulting in the following
    statistics:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: Here, we can see that approximately 10% of the MIDI file has a `--bass_drums_on_beat_threshold`
    over 0.75\. This returns 12,634 results on the whole LMD dataset, which is more
    than enough to train our model later. We'll look at training in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Building a jazz dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous section, we introduced the tools that are necessary for building
    a dataset based on information contained in the MIDI files from the full LMD dataset.
    In this section, we'll delve deeper into building a custom dataset by using external
    APIs such as the Last.fm API.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll use the **LMD-matched** distribution since it is (partially)
    matched with the MSD containing metadata information that will be useful for us,
    such as artist and title. That metadata can then be used in conjunction with Last.fm
    to get the song's genre. We'll also be extracting drum and piano instruments,
    just like we did in the previous section.
  prefs: []
  type: TYPE_NORMAL
- en: The LMD extraction tools
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we start, we''ll look at how to handle the LMD dataset. First, we need
    to download the following three elements from [colinraffel.com/projects/lmd/](https://colinraffel.com/projects/lmd/):'
  prefs: []
  type: TYPE_NORMAL
- en: '**LMD-matched**: A subset of LMD that is matched with MDS'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**LMD-matched metadata**: The H5 database containing the metadata information'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Match scores**: The dictionary of match scores'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Once extracted in the same folder, you should have the following elements:
    the `lmd_matched` directory, the `lmd_matched_h5` directory, and the `match_scores.json` file.'
  prefs: []
  type: TYPE_NORMAL
- en: You can find this code in the `lakh_utils.py` file, in the source code of this
    chapter. There are more comments and content in the source code, so check it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the `lakh_utils.py` file, you have the utilities to find metadata and MIDI
    file paths from a unique identifier, `MSD_ID`. Our starting point will be the
    `match_scores.json` file, a dictionary of matched files in the following format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: As the key, we have the `MSD_ID`, and as the value, we have a dictionary of
    matches, each with a score. From an `MSD_ID`, we can get the highest score match
    using the `get_matched_midi_md5` method. From that MD5, we'll be able to load
    the corresponding MIDI file using the `get_midi_path` method.
  prefs: []
  type: TYPE_NORMAL
- en: Fetching a song's genre using the Last.fm API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first part of our example uses the Last.fm API to fetch each song's genre.
    There are other APIs, such as Spotify's Echo Nest, that can be used to fetch such
    information. You can choose another service provider for this section if you feel
    like it.
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to create an account on [www.last.fm/api/](https://www.last.fm/api/).
    Since we won't be making any changes using the API, once you have an account,
    you only need to keep the **API key**.
  prefs: []
  type: TYPE_NORMAL
- en: You can find this section's code in the `chapter_06_example_01.py` file, in
    the source code of this chapter. There are more comments and content in the source
    code, so check it out.
  prefs: []
  type: TYPE_NORMAL
- en: Reading information from the MSD
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Before we call Last.fm to get the song''s genre, we need to find the artist
    and title of each song. Because LMD is matched with MSD, finding that information
    is easy. Follow these steps to do so:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s define a `process` method, as we did in the previous chapter,
    that can be called using threads, and that fetches the artist''s information from
    the H5 database:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: This code looks just like the code we wrote in the previous section. Here, we
    use the `tables` module to open the H5 database. Then, we use the `msd_id_to_h5`
    method from our `lakh_utils` module to get the path to the H5 file. Finally, we
    fetch the artist and title in the H5 database before returning the result in a
    dictionary.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can call the `process` method, just like we did in the previous chapter.
    Before doing that, we need to load the score matches dictionary, which contains
    all the matches between LMD and MSD:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: To do that, we need to use the `get_msd_score_matches` method, which loads the
    dictionary in memory. Then, we take a sample of the full dataset using our `app`
    method.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, to launch this code with a small sample size, in a Terminal, use the
    following command (by replacing `PATH_DATASET` and `PATH_MATCH_SCORES`):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'You should receive the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can plot the 25 most common artists, which should result in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/dc57cefb-9c5d-4b6d-8a67-f89485f40645.png)'
  prefs: []
  type: TYPE_IMG
- en: You could create a dataset based on one or multiple artists you like if you
    wanted to. You might end up with too few MIDI files for the training process,
    but it might be worth a shot.
  prefs: []
  type: TYPE_NORMAL
- en: Using top tags to find genres
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we know how to fetch information in the matched MSD database, we can
    call Last.fm to fetch the genre information for a track.
  prefs: []
  type: TYPE_NORMAL
- en: You can find this section's code in the `chapter_06_example_02.py` file, in
    the source code of this chapter. There are more comments and content in the source
    code, so check it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way to call the Last.fm API is to perform a simple `GET` request.
    We''ll do this in a `get_tags` method that takes the H5 database as a parameter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: This code makes a request to the `track.gettoptags` API endpoint, which returns
    an ordered list of genres for the track, ordered from most tag count to less tag
    count, where the tag count is calculated from the user's submissions. The correct
    classification of those tags varies greatly from one artist to the other.
  prefs: []
  type: TYPE_NORMAL
- en: You can find a lot of information on a track, artist, or release using APIs
    such s Last.fm or Echo Nest. Make sure you check out what information they provide
    when building your own dataset.
  prefs: []
  type: TYPE_NORMAL
- en: While a bit naive (we don't clean up the track name an artist name, or retry
    using another matching), most of the tracks (over 80%) are found on Last.fm, which
    is good enough for the purpose of our example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here''s a simple process method that we can use to call our `get_tags` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'This example is based on jazz music, but you can use other genres for this
    example if you wish. You can plot the most popular common genres in LMD using
    the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'This should produce a plot that looks similar to the one shown in the following
    diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/f219f960-0db1-49cd-987f-d1c916694e02.png)'
  prefs: []
  type: TYPE_IMG
- en: We'll be using the **jazz** genre, but you might want to combine multiple genres,
    such as **jazz** and **blues**, so that you have more content to work with or
    to create hybrid styles. We'll look at how much data you actually need for your
    models so that you can train them properly in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Finding instrument classes using MIDI
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'We''ll be training on two instruments for our example, but you can do something
    else if you feel like it:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Percussion**: We''ll be extracting drum tracks from the MIDI file to train
    a Drums RNN model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Piano**: We''ll be extracting piano tracks to train a Melody RNN model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Here, the first step is finding the instruments that we have in our dataset.
    In the `PrettyMIDI` module, the `Instrument` class contains a `program` property
    that can be used to find such information. As a reminder, you can find more information
    about the various programs in the General MIDI 1 Sound Set specification ([www.midi.org/specifications/item/gm-level-1-sound-set](https://www.midi.org/specifications/item/gm-level-1-sound-set)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Each program corresponds to an instrument, and each instrument is classified
    in an instrument class. We''ll be using this classification to find statistics
    about our dataset. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: You can find this section's code in the `chapter_06_example_04.py` file, in
    the source code of this chapter. There are more comments and content in the source
    code, so check it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let''s write the `get_instrument_classes` method for this purpose:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: First, we load a `PrettyMIDI` instance and then convert the `program` into its
    instrument class. Here, you can see that we are handling the drums separately
    since there is no `program` property for drums.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can write our `process` method as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'The most common instrument classes can be found using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'You should have similar results to what can be seen in the following diagram
    on the LMD:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/22ac43bf-acfe-4473-8ad8-67bafcd5da12.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that the most used instrument classes in the LMD are **Guitar**,
    **Drums**, **Ensemble**, and P**iano**. We'll be using the **Drums** and **Piano** classes
    in the upcoming sections, but you can use another class if you feel like it.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting jazz, drums, and piano tracks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Now that we are able to find the instrument tracks we want, filter the song
    by genre, and export the resulting MIDI files, we can put everything together
    to create two jazz datasets, one containing percussion and the other containing
    piano.
  prefs: []
  type: TYPE_NORMAL
- en: Extracting and merging jazz drums
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: We've already implemented most of the code for extracting jazz drums, namely
    the `get_tags` method and the `extract_drums` method.
  prefs: []
  type: TYPE_NORMAL
- en: You can find this section's code in the `chapter_06_example_07.py` file, in
    the source code of this chapter. There are more comments and content in the source
    code, so check it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `process` method should call the `get_tags` and `extract_drums` methods
    like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: Here, we are using the `ast` module to parse the `tags` argument. This is useful
    because it allows us to use the Python list syntax for the value of a flag, that
    is, `--tags="['jazz', 'blues']"`. Then, we can check if the tags coming from Last.fm
    match with one of the required tags and write the resulting MIDI drums file to
    disk if so.
  prefs: []
  type: TYPE_NORMAL
- en: 'The drum''s lengths and genre repartition can be seen in the following plots:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/58ac3d06-1bd5-4bd5-8757-67e39f5674d1.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we can see that we have around 2,000 MIDI files when combining both "**jazz**"
    and "**blues**".
  prefs: []
  type: TYPE_NORMAL
- en: Extracting and splitting jazz pianos
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The last method we need to write is the piano extraction method. The `extract_pianos`
    method is similar to the previous `extract_drums` method, but instead of merging
    the tracks together, it splits them into separate piano tracks, potentially returning
    multiple tracks for each song. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: You can find this section's code in the `chapter_06_example_08.py` file, in
    the source code of this chapter. There are more comments and content in the source
    code, so check it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we''ll write the `extract_pianos` method, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: We've already covered most of the code in this snippet. The difference here
    is that we are filtering the instrument on any of the piano programs, ranging
    from 0 to 8\. We're also returning multiple piano MIDI files.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can call our method using the following `process` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'This code was covered in the previous section on drums, except here, each piano
    file is written separately using an index. The piano''s lengths and genres can
    be plotted like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/a83ea8b1-eb95-4280-9dd2-1d7687090c27.png)'
  prefs: []
  type: TYPE_IMG
- en: Here, we've found just under 2,000 piano tracks for **jazz** and **blues**.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing the data using pipelines
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous sections, we looked at existing datasets and developed tools
    so that we can find and extract specific content. By doing so, we've effectively
    built a dataset we want to train our model on. But building the dataset isn't
    all – we also need to **prepare** it. By preparing, we mean the action of removing
    everything that isn't useful for training, cutting, and splitting tracks, and
    also automatically adding more content.
  prefs: []
  type: TYPE_NORMAL
- en: In this section, we'll be looking at some built-in utilities that we can use
    to transform the different data formats (MIDI, MusicXML, and ABCNotation) into
    a training-ready format. These utilities are called `pipelines` in Magenta, and
    are a sequence of operations that are executed on the input data.
  prefs: []
  type: TYPE_NORMAL
- en: An example of an operation that is already implemented in pipelines includes
    discarding melodies with too many pitches that are too long or too short. Another
    operation is transposing melodies, which consists of taking a melody and creating
    a new one by shifting the note's pitches up or down. This is a common practice
    in machine learning called **dataset augmentation**, which is useful if we wish
    to make the model train better by providing it with variations of the original
    data.
  prefs: []
  type: TYPE_NORMAL
- en: Let's take a look at what pipelines are and how they can be used. In this section,
    we'll be using the Melody RNN pipeline as an example, but each model has its own
    pipeline with its own specifics. For example, the Drums RNN pipeline does not
    transpose the drums sequences because it wouldn't make sense to do so.
  prefs: []
  type: TYPE_NORMAL
- en: Before we start talking about pipelines, we'll have a brief look at manually
    refining the dataset we built previously.
  prefs: []
  type: TYPE_NORMAL
- en: Refining the dataset manually
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'This might sound obvious, but we''ll stress it because it is also really important:
    verify the MIDI files you''ve extracted. The dataset''s content should correspond
    to what you were looking for in terms of music.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The easiest way of verifying a track''s content is by opening the content in
    MuseScore and listening to it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/43a92975-da8c-4b3f-9f04-132c13d44b96.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Let''s have a look a the things we can verify for each of these files:'
  prefs: []
  type: TYPE_NORMAL
- en: The first thing to verify is whether the **instruments** we've extracted are
    present in the resulting MIDI files, meaning that, for our jazz piano example,
    we should have only one instrument track and that it should be any of the eight
    piano programs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Another thing to verify is whether the **genre** of the tracks fits our requirements.
    Does the piano sound like jazz music? Does the music actually sound good to you?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Other problems to look out for include tracks that are **incomplete**, too short,
    too long, or full of silence. Some of those tracks will be filtered by the data
    preparation pipeline, but a manual pass on the data is also important.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If some of the tracks you like are filtered by the data preparation pipeline,
    for example, for being too short, you can manually fix this issue by copying and
    pasting parts of it to make it longer. You can also write a pipeline to automatically
    do that.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If some of the tracks don't fit your requirements, remove them from the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Once you've validated the dataset's content and removed all the unwanted tracks,
    you can also cut and clean the content of each file. The easiest way of doing
    that is by going into MuseScore, listening to the track, removing the parts you
    don't like, and exporting the file back to MIDI.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at the Melody RNN pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Once we've manually refined the dataset, we are ready to prepare it using a
    pipeline, resulting in data that has been prepared for training. As an example,
    we'll be looking at the Melody RNN pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: Launching the data preparation stage on our dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The first step when it comes to preparing the data is to call the `convert_dir_to_note_sequences`
    command, which is the same regardless of the model you will be using. This command
    will take a directory containing MIDI files, MusicXML files, or ABCNotation files
    as input and convert them into TensorFlow records of `NoteSequence`.
  prefs: []
  type: TYPE_NORMAL
- en: 'We recommend that you create another folder for your training data (separate
    from the dataset folder you created previously). Now, let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, change directory to the folder you''ve created and call the `convert_dir_to_note_sequences`
    command using the following command (replace `PATH_OUTPUT_DIR` with the directory
    you used in the previous section):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: This will output a bunch of "Converted MIDI" files and produce a `notesequences.tfrecord`. From
    now on, the data is in the same format, regardless of the symbolic representation
    we used when building the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can launch the pipeline on our data using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: First, we need to give `--config` as an argument. This is necessary because
    the encoder and decoder are defined in the configuration (see [Chapter 3](48023567-4100-492a-a28e-53b18a63e01e.xhtml),
    *Generating Polyphonic Melodies*, for a refresher on how encoding and decoding
    works).
  prefs: []
  type: TYPE_NORMAL
- en: We also pass the `--eval_ratio` argument, which will give the pipeline the number
    of elements in the training and evaluation sets. When executed, the pipeline will
    output statistics and warnings about the files it encounters.
  prefs: []
  type: TYPE_NORMAL
- en: 'The statistics are printed on the console for each increment of 500 files that
    are processed, but only the last part (after the **Completed.** output) is of
    interest to us. The following is the output of the 500 samples of the jazz piano
    dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'The statistics of interest here are as follows::'
  prefs: []
  type: TYPE_NORMAL
- en: '`Processed 500 inputs total. **Produced 122 outputs.**`: This gives us the
    input size or the number of provided MIDI files, as well as the number of resulting
    `SequenceExample` that will be used for training (122, counting both evaluation
    and training sets). This is the most important statistic.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DAGPipeline_MelodyExtractor_MODE_melody_lengths_in_bars`: This gives you the
    length of the resulting `SequenceExample` elements for each "MODE".'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `SequenceExample` encapsulates the data that will be fed to the network
    during training. Those statistics are useful because the quantity (as well as
    the quality) of the data is important for the model's training. If a model doesn't
    train properly on 122 outputs, we'll need to make sure we have more data for the
    next time we train.
  prefs: []
  type: TYPE_NORMAL
- en: In that sense, it is really important to look at the produced outputs, which
    tells us about the exact amount of data the network will receive. It doesn't matter
    whether we feed 100,000 MIDI files to the data preparation pipeline if a small
    amount of `SequenceExample` is produced because the input data isn't good. If
    a pipeline produces a small number of outputs for a big input, look at the statistics
    and find out which part of the processing step is removing the elements.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's have a look at how the pipeline is defined and executed for our example.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding a pipeline execution
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The statistics we provided in the previous section are a bit confusing because
    they are not shown in the order they are executed. Let''s have a proper look at
    how this is really executed to understand what''s going on:'
  prefs: []
  type: TYPE_NORMAL
- en: '`DagInput` initiates the pipeline''s execution, taking each `NoteSequence`
    of the TensorFlow records as input (500 elements).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomPartition` randomly splits the elements into training and evaluation
    sets given the ratio provided in the command (450 elements in the training set
    and 50 elements in the evaluation set).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TimeChangeSplitter` splits the elements at each time change (doesn''t output
    statistics).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`Quantizer` quantizes the note sequence on the closest step defined by the `steps_per_quarter`
    attribute in the configuration and discards elements with multiple tempos and
    time signatures (doesn''t output statistics).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`TranspositionPipeline` transposes the note sequence into multiple pitches,
    adding new elements in the process (2,387 elements generated by transposition
    for the training set).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MelodyExtractor` extracts the melodies from the `NoteSequence`, returning
    a `Melody` and removing elements if needed, such as polyphonic tracks and tracks
    that are too short or too long (1,466 elements are removed for the training set).
    This part also outputs the lengths of the melodies in bars:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The minimum and maximum length of the melody are defined by `min_bars` and `max_steps`,
    respectively. See the next section to learn how to change them.
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`ignore_polyphonic_notes`, which is set `True`, discards polyphonic tracks.'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '`EncoderPipeline` encodes`Melody` into `SequenceExample` using `KeyMelodyEncoderDecoder`
    defined for the attention configuration (doesn''t output statistics). The encoder
    pipeline receives the configuration passed as an argument; for example, `LookbackEventSequenceEncoderDecoder`
    for the `lookback_rnn` configuration.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`DagOutput` finishes the execution.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If you want to look at the implementation of the `Pipeline`, have a look at
    the `get_pipeline` method in the `melody_rnn_pipeline` module.
  prefs: []
  type: TYPE_NORMAL
- en: Writing your own pipeline
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: As you might have noticed from the code in the `get_pipeline` method, most of
    the configurations cannot be changed. However, we can write our own pipeline and
    call it directly.
  prefs: []
  type: TYPE_NORMAL
- en: You can find this section's code in the `melody_rnn_pipeline_example.py` file,
    in the source code of this chapter. There are more comments and content in the
    source code, so check it out.
  prefs: []
  type: TYPE_NORMAL
- en: 'For this example, we''ll take the existing Melody RNN pipeline, copy it, and
    change the transposition and sequence length. Let''s get started:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, copy the `get_pipeline` method and call it using the following Python
    code (replacing `INPUT_DIR` and `OUTPUT_DIR` with the proper values):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'You should see the same output that we received previously when we used the
    pipeline method. By taking a small sample (500 pieces of data) of the piano jazz
    dataset, we received the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s change some parameters to see how it works. In the following code,
    we''ve added some transpositions (the default transposition value is `(0,)`, which
    means no transposition shifts):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: By using the transpositions `(0,12)`, we're telling the transposition pipeline
    to create, for each existing sequence, a sequence 12 pitches higher, corresponding
    to a full octave shift up. Keep the rest of the code as is.
  prefs: []
  type: TYPE_NORMAL
- en: Transposition values should follow musical intervals expressed in semitones
    (a pitch value in MIDI). The simplest interval is the perfect interval that we
    are using, which corresponds to an octave, or 12 semitones or MIDI pitches. Other
    intervals can be used, such as the Major third, which is used in the Polyphony
    RNN pipeline, with a transposition range of `(-4, 5)`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, the output should look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: Notice how we now have approximately twice as much data to work with. Data augmentation
    is important for handling small datasets.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also change the minimum and maximum lengths of the sequences in the
    melody extractor, like so:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The preceding code will output a total of 92 outputs (instead of our previous
    230).
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also write our own pipeline class. For example, we could automatically
    cut sequences that are too long or duplicate sequences that are too short, instead
    of discarding them. For a note sequence pipeline, we need to extend the `NoteSequencePipeline`
    class and implement the `transform` method, as shown in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Take a look at the `sequences_lib` module in Magenta, which contains tons of
    utilities for handling note sequences. Each dataset needs to be prepared and the
    easiest way to prepare the data is by creating new pipelines.
  prefs: []
  type: TYPE_NORMAL
- en: Looking at MusicVAE data conversion
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The MusicVAE model doesn't use pipelines – actually, it doesn't even have a
    dataset creation script. Compared to our previous example with Melody RNN, it
    still uses similar transformations (such as data augmentation) and is more configurable
    since some of the transformations can be configured, instead of us needing to
    write a new pipeline.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s have a look at a simple MusicVAE configuration contained in the `configs`
    module of the `music_vae` module. Here, you can find the following `cat-mel_2bar_small`
    configuration:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'The following list further explains the code:'
  prefs: []
  type: TYPE_NORMAL
- en: By looking at the `NoteSequenceAugmenter` class, you can see that it takes note
    of sequence augmentation by using shifting (like in our custom pipeline) and stretching,
    another data augmentation technique.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It also limits the maximum length of the melody to `max_bars=100`, but remember
    that MusicVAE handles limited size samples because of its network type. In this
    example, each sample is sliced to a length of `slice_bars=2`.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The note sequence augmenter lets you decide a transposition range that it will
    randomly choose a value from.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Stretching isn't used for Melody RNN because most stretching ratios don't work
    for quantized sequences. Stretching can be used for Performance RNN, for example.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We won't be looking at creating a new configuration just now. See [Chapter 7](6f012812-5c24-44d4-b8cb-ddfd3ed78f5c.xhtml),
    *Training Magenta Models*, for more information on how to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we looked at how to build and prepare a dataset that will be
    used for training. First, we looked at existing datasets and explained how some
    are more suitable than others for a specific use case. We then looked at the LMD
    and the MSD, which are useful for their size and completeness, and datasets from
    the Magenta team, such as the MAESTRO dataset and the GMD. We also looked at external
    APIs such as Last.fm, which can be used to enrich existing datasets.
  prefs: []
  type: TYPE_NORMAL
- en: Then, we built a dance music dataset and used information contained in MIDI
    files to detect specific structures and instruments. We learned how to compute
    our results using multiprocessing and how to plot statistics about the resulting
    MIDI files.
  prefs: []
  type: TYPE_NORMAL
- en: After, we built a jazz dataset by extracting information from the LMD and using
    the Last.fm API to find the genre of each song. We also looked at how to find
    and extract different instrument tracks in the MIDI files.
  prefs: []
  type: TYPE_NORMAL
- en: Finally, we prepared the data for training. By using pipelines, we were able
    to process the files we extracted, remove the files that weren't of the proper
    length, quantize them, and use data augmentation techniques to create a proper
    dataset, ready for training. By doing this, we saw how different models have different
    pipelines, depending on their network type.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we'll use what we produced in this chapter to train some
    models on the datasets we've produced. You'll see that training is an empirical
    process that requires a lot of back and forth between preparing the data and training
    the model. During this process, you will likely come back to this chapter for
    more information.
  prefs: []
  type: TYPE_NORMAL
- en: Questions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: What are the advantages and disadvantages of the different symbolic representations?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a piece of code that will extract cello instruments from MIDI files.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: How many rock songs are present in LMD? How many match one of the "jazz", "blues",
    "country" tags?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Write a piece of code that will extend MIDI files that are too short for the
    Melody RNN pipeline.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Extract the jazz drums from GMD. Can we train a quantized model with this?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Why is data augmentation important?
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**The MAESTRO Dataset and Wave2Midi2Wave:** A Magenta team blog post on the
    MAESTRO dataset and its usage in the Wave2Midi2Wave method ([magenta.tensorflow.org/maestro-wave2midi2wave](https://magenta.tensorflow.org/maestro-wave2midi2wave))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Enabling Factorized Piano Music Modeling and Generation with the MAESTRO
    Dataset:** A paper (2019) about MAESTRO and Wave2Midi2Wave ([arxiv.org/abs/1810.12247](https://arxiv.org/abs/1810.12247))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Celebrating Johann Sebastian Bach:** The Bach Doodle, which gave us the Bach
    Doodle Dataset ([www.google.com/doodles/celebrating-johann-sebastian-bach](https://www.google.com/doodles/celebrating-johann-sebastian-bach))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Visualizing the Bach Doodle Dataset:** An amazing visualization of the Bach
    Doodle Dataset ([magenta.tensorflow.org/bach-doodle-viz](https://magenta.tensorflow.org/bach-doodle-viz))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**The Bach Doodle: Approachable music composition with machine learning at
    scale:** A paper (2019) about the Bach Doodle dataset ([arxiv.org/abs/1907.06637](https://arxiv.org/abs/1907.06637))'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
