- en: '*Chapter 5*: Text Classification and Regression Using AutoKeras'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we will focus on the use of AutoKeras to work with text (a
    sequence of words).
  prefs: []
  type: TYPE_NORMAL
- en: In the previous chapter, we saw that there was a specialized type of network
    suitable for image processing, called a **convolutional neural network** (**CNN**).
    In this chapter, we will see what **recurrent neural networks** (**RNNs**) are
    and how they work. An RNN is a type of neural network that is very suited to working
    with text.
  prefs: []
  type: TYPE_NORMAL
- en: We will also use a classifier and a regressor to solve text-based tasks. By
    the end of the chapter, you will have learned how to use AutoKeras to solve a
    wide variety of problems that are text-based, such as extracting emotions from
    tweets, detecting spam in emails, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we will cover the following topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Working with text data
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Understanding RNNs—what are these neural networks and how do they work?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One-dimensional CNNs (Conv1D)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Creating an email spam detector
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Predicting news popularity in social media
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'All coding examples in this book are available as Jupyter notebooks that can
    be downloaded from the following link: [https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras](https://github.com/PacktPublishing/Automated-Machine-Learning-with-AutoKeras).'
  prefs: []
  type: TYPE_NORMAL
- en: As code cells can be executed, each notebook can be self-installable, by adding
    a code snippet with the requirements you need. For this reason, at the beginning
    of each notebook there is a code cell for environmental setup, which installs
    AutoKeras and its dependencies.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, to run the coding examples, you only need a computer with Ubuntu Linux
    as the operating system and can install the Jupyter Notebook with the following
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Alternatively, you can also run these notebooks using Google Colaboratory, in
    which case you will only need a web browser. For further details, see the *AutoKeras
    with Google Colaboratory* section in [*Chapter 2*](B16953_02_Final_PG_ePub.xhtml#_idTextAnchor029),
    *Getting Started with AutoKeras*. Furthermore, in the *Installing AutoKeras* section,
    you will also find other installation options.
  prefs: []
  type: TYPE_NORMAL
- en: Working with text data
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: AutoKeras allows us to quickly and easily create high-performance models for
    solving text-based tasks.
  prefs: []
  type: TYPE_NORMAL
- en: 'Text is an excellent source of information to feed DL models, and there is
    a multitude of sources that are text-based, such as social media, chats, emails,
    articles, books, and countless tasks to automate based on text, such as the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Translation**: Convert source text in one language to text in another language.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Conversational bots**: Simulate human conversation using ML models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Sentiment analysis**: Classification of emotions by analyzing text data.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Spam classifiers**: Email classification using machine learning models.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Document summarizers**: Generate summaries of documents automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Text generators**: Generate text from scratch automatically.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: As with other types of data, AutoKeras will do all the preprocessing so that
    we can pass the text directly to our model, but before starting with the practical
    examples, let's take a look at what it does under the hood.
  prefs: []
  type: TYPE_NORMAL
- en: Tokenization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we already know, neural networks take vectors of numbers as input, so the
    text must be converted to numerical Tensors in a process called **vectorization**.
    Before that, however, we must cut the text into segments.
  prefs: []
  type: TYPE_NORMAL
- en: 'This text segmentation can be done with the help of different units, such as
    the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Word**: Divide the text by words.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Character**: Fragment the text into characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**N-gram**: Extract N-grams of words or characters. N-grams are overlapping
    groupings of multiple consecutive words or characters.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The units mentioned previously are called **tokens**, and the process of dividing
    the text into said tokens is called **tokenization**. This is a necessary step
    to convert the text to tensors in the vectorization process, which we explain
    next.
  prefs: []
  type: TYPE_NORMAL
- en: Vectorization
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Once the text is tokenized, vectorization is performed. This process transforms
    each word/character/N-gram into a vector.
  prefs: []
  type: TYPE_NORMAL
- en: 'All text vectorization processes consist of the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Applying some tokenization scheme
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Associating numeric vectors with the generated tokens
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: These vectors, packed into sequence tensors, feed into **deep neural networks**
    (**DNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: 'There are multiple ways to associate a token with a vector. Let''s see two
    of the most important ones, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`1` and all the remaining values of the vector are zeros.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Token embedding** is another form of token-vector association that is widely
    used and is more powerful than one-hot encoding. While the vectors obtained by
    one-hot encoding are binary (one input with value *1* and the rest of values *0*)
    and large (they must have the same length as the number of words in the vocabulary),
    the embeddings of words are low-dimensional floating-point vectors.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The word vectors obtained by one-hot encoding are static (the position in the
    array determines the word and these values never change), while the word-embedding
    vectors are dynamic (they are learned from the data), in such a way that their
    values are modified during learning, in the same way that they are with the weights
    of the neural network layers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'It is this dynamism that allows it to store more information in a much smaller
    size, as you can see in the following screenshot:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.1 – One-hot encoding versus embedding comparison](img/B16953_05_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.1 – One-hot encoding versus embedding comparison
  prefs: []
  type: TYPE_NORMAL
- en: Just as convolutional networks were the most appropriate choice for image-based
    tasks, when we talk about word processing, the most optimal type of network is
    an RNN. Let's see what this consists of in the following section.
  prefs: []
  type: TYPE_NORMAL
- en: Understanding RNNs
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'A common feature of all the neural networks seen so far is that they don''t
    have a memory. Networks formed by either fully connected layers or convolutional
    layers process each input independently so that it is isolated from the other
    layers. However, in RNNs, "the past" is taken into account, and this is done using
    its previous output as the state; so, an RNN layer will have two inputs, one is
    which is the standard input of the current vector, and the other being the output
    of the previous vector, as seen in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 – RNN loop unfolded](img/B16953_05_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.2 – RNN loop unfolded
  prefs: []
  type: TYPE_NORMAL
- en: 'The RNN implements this memory feature with an internal loop over the entire
    sequence of elements. Let''s explain it with some pseudocode, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: There are several types of RNN architectures with much more complex systems
    than the one presented here, but this is beyond the scope of the book. Understanding
    the concepts explained here is enough, since both the configuration and the choice
    of architecture to be used will be handled by AutoKeras.
  prefs: []
  type: TYPE_NORMAL
- en: One-dimensional CNNs (Conv1D)
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Another architecture to take into account when working with texts is one-dimensional
    CNNs (Conv1D). The principle on which they are based is similar to the 2D CNN
    that we saw in the previous chapter, [*Chapter 4*](B16953_04_Final_PG_ePub.xhtml#_idTextAnchor063),
    *Image Classification and Regression Using AutoKeras*. These neural networks manage
    to learn patterns in text through filters, in the same way as they did with images
    in the previous chapter.
  prefs: []
  type: TYPE_NORMAL
- en: 'An example of a one-dimensional CNN is shown in the following diagram:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.3 – 1D convolution over text sequences](img/B16953_05_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.3 – 1D convolution over text sequences
  prefs: []
  type: TYPE_NORMAL
- en: It is good to know that if the chronological order of the elements in the sequence
    is important for the prediction, the RNNs are much more effective, thus one-dimensional
    CNNs are often combined with the RNNs to create high-performance models. The exhaustive
    search performed by AutoKeras takes both into account to find the best model.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's put the learned concepts into practice with some practical examples.
  prefs: []
  type: TYPE_NORMAL
- en: Creating an email spam detector
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The model we are going to create will detect spam emails from an `emails` dataset.
    This is a little dataset of 5,572 emails, labeled with a `spam` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'The notebook with the complete source code can be found at the following link:'
  prefs: []
  type: TYPE_NORMAL
- en: '[https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb](https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now have a look at the relevant cells of the notebook in detail, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`pip` package manager:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`tensorflow`, `pandas`, `numpy`, and `autokeras` as needed dependencies for
    this project:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`emails` spam dataset from our GitHub repository, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We now prepare our dataset by renaming the relevant columns and removing unnecessary
    ones, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 5.4 – Notebook output of dataset preview](img/B16953_05_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.4 – Notebook output of dataset preview
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s now split the dataset into `train` and `test` datasets, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: We are ready to create the spam classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Creating the spam predictor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we will use the AutoKeras `TextClassifier` class to find the best classification
    model. Just for this little example, we set `max_trials` (the maximum number of
    different Keras models to try) to 2, and we do not set the `epochs` parameter
    but rather define an `EarlyStopping` callback of `2` epochs, such that the training
    process stops if the loss of validation does not improve in two consecutive epochs.
    The code is shown in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the training to search for the optimal classifier for the training
    dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.5 – Notebook output of text classifier training'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_05_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – Notebook output of text classifier training
  prefs: []
  type: TYPE_NORMAL
- en: The previous output shows that the accuracy with the training dataset is increasing.
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, we achieved a loss value of `0.080` in the validation set. It's
    a really good number just for one minute of training. We have limited the search
    to two architectures (`max_trials = 2`). Increasing this number would give us
    a more accurate model, although it would also take longer to finish.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'It''s time to evaluate the best model with the testing dataset. We can do this
    by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, `0.9849` as prediction accuracy in the test set is a really good
    final prediction score for the time invested.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, we can see a little summary of the architecture of the best generated
    model. We can do this by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Best model architecture summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_05_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – Best model architecture summary
  prefs: []
  type: TYPE_NORMAL
- en: As we can see here, AutoKeras has chosen a convolution model (Conv1D) to do
    the task. As we explained in [*Chapter 4*](B16953_04_Final_PG_ePub.xhtml#_idTextAnchor063),
    *Image Classification and Regression Using AutoKeras*, this kind of architecture
    works great when the order of the elements in the sequence is not important for
    the prediction, as in this case.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is a visual representation of the architecture:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.7 – Best model architecture visualization'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_05_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.7 – Best model architecture visualization
  prefs: []
  type: TYPE_NORMAL
- en: As you already know, generating the models and choosing the best one is a task
    done by AutoKeras automatically, but let's briefly explain these blocks.
  prefs: []
  type: TYPE_NORMAL
- en: Each block represents a layer, and the output of each is connected to the input
    of the next, except the first block (whose input is the text) and the last block
    (whose output is the predicted number). The blocks before the Conv1D block are
    all data-preprocessing blocks to vectorize the text-generating embeddings to feed
    this Conv1D block, as well as reduce the dimension of the filters through the
    max pooling layer. Notice that AutoKeras has also added several dropout blocks
    to reduce overfitting.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next section, we are going to resolve a text regression problem with
    a practical example: we are going to create a news popularity predictor.'
  prefs: []
  type: TYPE_NORMAL
- en: Predicting news popularity in social media
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this section, we will create a model that will find out the popularity score
    for an article on social media platforms, based on its text. For this, we will
    train the model with a *News Popularity* dataset collected between 2015 and 2016
    ([https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms](https://archive.ics.uci.edu/ml/datasets/News+Popularity+in+Multiple+Social+Media+Platforms)).
  prefs: []
  type: TYPE_NORMAL
- en: As we want to approximate a score (number of likes), we will use a text regressor
    for this task.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the next screenshot, you can see some samples taken from this dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.8 – A few samples from the News Popularity dataset'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_05_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.8 – A few samples from the News Popularity dataset
  prefs: []
  type: TYPE_NORMAL
- en: This notebook with the complete source code can be found at [https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb](https://colab.research.google.com/github/PacktPublishing/Automated-Machine-Learning-with-AutoKeras/blob/main/Chapter05/Chapter5_SpamDetector.ipynb).
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now explain the relevant code cells of the notebook in detail, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Getting the articles dataset**: Before training, we have to download the
    dataset that contains the text of each article, as well as the popularity score.
    Here is the code to do this:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`Title` and `Headline` text columns to feed our regression model.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now, we extract the popularity score of each article on LinkedIn, to be used
    as labels. We have decided to use only the LinkedIn scores to simplify the example.
    The code is shown in the following snippet:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Next, we will create the train and test datasets, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`train` and `test` set using the `sklearn` function, as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Once we have created our datasets we are ready to feed our model, but first,
    we have to create it.
  prefs: []
  type: TYPE_NORMAL
- en: Creating a text regressor
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Because we want to predict a popularity score from a set of text sentences,
    and this score is a scalar value, we are going to use AutoKeras `TextRegressor`.
    For this example, we set `max_trials` to `2`, and we do not set the `epochs` parameter
    but rather define an `EarlyStopping` callback of `2` epochs of patience, such
    that the training process stops if the validation loss does not decrease in two
    consecutive epochs. The code can be seen in the following snippet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'Let''s run the training to search for the optimal regressor for the training
    dataset, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.9 – Notebook output of the training of our news popularity predictor](img/B16953_05_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 5.9 – Notebook output of the training of our news popularity predictor
  prefs: []
  type: TYPE_NORMAL
- en: As we can see from the previous output, after a few minutes we have a model
    with `14726` as the best validation loss (`121` (square root of `14726`) in the
    final score, which is not a bad result for the time invested. Let's see how it's
    working with the test set.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Time to evaluate the best model with the testing dataset. We do this by running
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, `13944` is a really good prediction score for the time invested.
    If we run AutoKeras with more trials, we will get better results.
  prefs: []
  type: TYPE_NORMAL
- en: Visualizing the model
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, it''s time to take a look at what we have under the hood. We''ll run the
    following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.10 – Best model architecture summary'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B16953_05_10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.10 – Best model architecture summary
  prefs: []
  type: TYPE_NORMAL
- en: As in the previous classification example, AutoKeras has chosen a convolution
    model (Conv1D) to do the task. As we explained before, this is a less time-consuming
    architecture than RNN and is most suitable when the order of the elements in the
    sequence is not important for the prediction.
  prefs: []
  type: TYPE_NORMAL
- en: Improving the model performance
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: As we did in previous examples, if we need more precision in less time, we can
    fine-tune our model using an advanced AutoKeras feature that allows you to customize
    your search space.
  prefs: []
  type: TYPE_NORMAL
- en: By using `AutoModel` with `TextBlock` instead of `TextRegressor`, we can create
    high-level configurations, such as `block_type` for the type of neural network
    to look for; or, if your text source has a larger vocabulary (number of distinct
    words), you may need to create a custom pipeline in AutoKeras to increase the
    `max_tokens` parameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'See the following example for more details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous code block, we have done the following with the settings:'
  prefs: []
  type: TYPE_NORMAL
- en: The `EarlyStopping` block will stop the training if the validation loss doesn't
    decrease in two consecutive epochs.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The `max_token` parameter is set to `20000` because our text source has a larger
    vocabulary (number of distinct words).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With `TextBlock(block_type="ngram"`, we are telling AutoKeras to only scan models
    using N-gram embeddings.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: You can also not specify any of these arguments, in which case these different
    options would be tuned automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Evaluating the model with the test set
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'After training, it is time to measure the actual prediction of our model using
    the reserved test dataset. In this way, we can rule out that the good results
    obtained with the training set are due to overfitting. Run the following code
    to do this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output of the preceding code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: The performance is slightly better than in the model without fine-tuning, but
    training it for a longer time surely improves it.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how neural networks work with text data, what
    recurrent neural networks are and how they work.
  prefs: []
  type: TYPE_NORMAL
- en: We've also put the concept of neural network into practice, using the power
    of AutoKeras, by implementing a spam predictor and a news popularity regressor,
    in just a few lines of code.
  prefs: []
  type: TYPE_NORMAL
- en: Now that we have learned how to work with text, we are ready to move on to the
    next chapter, where you will learn how to work with structured data by implementing
    classification and regression models using AutoKeras.
  prefs: []
  type: TYPE_NORMAL
