["```py\nI searched the U Mich archives fairly thoroughly for 3D graphics packages,\nI always thought it to be a mirror of sumex-aim.stanford.edu... I was wrong.\nI'll look into GrafSys... it does sound interesting!\nThanks Cheinan.\nBobC\n```", "```py\n!pip install bertopic\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sentence_transformers import SentenceTransformer\nfrom bertopic import BERTopic\nfrom umap import UMAP\nfrom hdbscan import HDBSCAN\n# install data\ndocs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n```", "```py\n# Prepare embeddings\ndocs = fetch_20newsgroups(subset='all',  remove=('headers', 'footers', 'quotes'))['data']\n#The model is a Hugging Face transformer model\nembedding_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\ncorpus_embeddings = embedding_model.encode(docs, show_progress_bar = True)\nBatches: 100%|########################################################################| 589/589 [21:48<00:00,  2.22s/it]\n```", "```py\ncorpus_embeddings.view()\narray([[ 0.002078  ,  0.02345043,  0.02480883, ...,  0.00143592,\n         0.0151075 ,  0.05287581],\n       [ 0.05006033,  0.02698092, -0.00886482, ..., -0.00887168,\n        -0.06737082,  0.05656359],\n       [ 0.01640477,  0.08100049, -0.04953594, ..., -0.04184629,\n        -0.07800221, -0.03130952],\n       ...,\n       [-0.00509084,  0.01817271,  0.04388074, ...,  0.01331367,\n        -0.05997065, -0.05430664],\n       [ 0.03508159, -0.05842971, -0.03385153, ..., -0.02824297,\n        -0.05223113,  0.03760364],\n       [-0.06498063, -0.01133722,  0.03949645, ..., -0.03573753,\n         0.07217913,  0.02192113]], dtype=float32)\n```", "```py\nmodel = BERTopic().fit(docs, corpus_embeddings)\n```", "```py\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizer_model = CountVectorizer(stop_words = \"english\", max_df = .95, min_df = .01)\n# setting parameters for HDBSCAN (clustering) and UMAP (dimensionality reduction)\nhdbscan_model = HDBSCAN(min_cluster_size = 30, metric = 'euclidean', prediction_data = True)\numap_model = UMAP(n_neighbors = 15, n_components = 10, metric = 'cosine', low_memory = False)\n# Train BERTopic\nmodel = BERTopic(\n    vectorizer_model = vectorizer_model,\n    nr_topics = 'auto',\n    top_n_words = 10,\n    umap_model = umap_model,\n    hdbscan_model = hdbscan_model,\n    min_topic_size = 30,\n    calculate_probabilities = True).fit(docs, corpus_embeddings)\n```", "```py\nmodel.visualize_documents(docs, embeddings = corpus_embeddings)\n```", "```py\nsentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\nnew_docs = [\"I'm looking for a new graphics card\",\"when is the next nasa launch\"]\nembeddings = sentence_model.encode(new_docs)\ntopics, probs = model.transform(new_docs,embeddings)\nprint(topics)\n[-1, 3]\n```"]