- en: '*Chapter 5*: Running DL Pipelines in Different Environments'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*第5章*：在不同环境中运行 DL 管道'
- en: It is critical to have the flexibility of running a **deep learning** (**DL**)
    pipeline in different execution environments such as local or remote, on-premises,
    or in the cloud. This is because, during different stages of the DL development,
    there may be different constraints or preferences to either improve the velocity
    of the development or ensure security compliance. For example, it is desirable
    to do small-scale model experimentation in a local or laptop environment, while
    for a full hyperparameter tuning, we need to run the model on a cloud-hosted GPU
    cluster to get a quick turn-around time. Given the diverse execution environments
    in both hardware and software configurations, it used to be a challenge to achieve
    this kind of flexibility within a single framework. MLflow provides an easy-to-use
    framework to run DL pipelines at scale in different environments. We will learn
    how to do that in this chapter.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在不同执行环境中运行 **深度学习** (**DL**) 管道的灵活性至关重要，例如在本地、远程、企业内部或云中运行。这是因为在 DL 开发的不同阶段，可能会有不同的约束或偏好，目的是提高开发速度或确保安全合规性。例如，进行小规模模型实验时，最好在本地或笔记本环境中进行，而对于完整的超参数调优，我们需要在云托管的
    GPU 集群上运行模型，以实现快速的迭代时间。考虑到硬件和软件配置中的多样化执行环境，过去在单一框架内实现这种灵活性是一个挑战。MLflow 提供了一个易于使用的框架，能够在不同环境中按规模运行
    DL 管道。本章将教你如何实现这一点。
- en: 'In this chapter, we will first learn about the different DL pipeline execution
    scenarios and their execution environments. We will also learn how to run the
    different steps of the DL pipeline in different execution environments. Specifically,
    we will cover the following topics:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先了解不同的 DL 管道执行场景及其执行环境。我们还将学习如何在不同的执行环境中运行 DL 管道的不同步骤。具体而言，我们将涵盖以下主题：
- en: An overview of different execution scenarios and environments
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同执行场景和环境的概述
- en: Running locally with local code
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地运行本地代码
- en: Running remote code in GitHub locally
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 GitHub 中远程运行本地代码
- en: Running local code remotely in the cloud
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在云中远程运行本地代码
- en: Running remotely in the cloud with remote code in GitHub
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在云中远程运行，并使用 GitHub 中的远程代码
- en: By the end of this chapter, you will be comfortable setting up the DL pipelines
    to run either locally or remotely with different execution environments.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章结束时，你将能够熟练地设置 DL 管道，以便在不同执行环境中本地或远程运行。
- en: Technical requirements
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 技术要求
- en: 'The following technical requirements are needed for completing the learning
    in this chapter:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 完成本章学习所需的技术要求如下：
- en: 'The code in this chapter can be found at the following GitHub URL: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter05](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter05).'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本章的代码可以在以下 GitHub URL 找到：[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter05](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter05)。
- en: 'Installation of the Databricks **command-line interface** (**CLI**) tool to
    access the Databricks platform remote execution of DL pipelines: [https://github.com/databricks/databricks-cli](https://github.com/databricks/databricks-cli).'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装 Databricks **命令行界面** (**CLI**) 工具以访问 Databricks 平台并远程执行 DL 管道：[https://github.com/databricks/databricks-cli](https://github.com/databricks/databricks-cli)。
- en: Access to a Databricks instance (must be the Enterprise version, as the Community
    version does not support remote execution) for learning how to run DL pipelines
    remotely on a cluster in Databricks.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问 Databricks 实例（必须是企业版，因为社区版不支持远程执行），以学习如何在 Databricks 集群上远程运行 DL 管道。
- en: A full-fledged MLflow tracking server when running locally. This MLflow tracking
    server setup is the same as in previous chapters.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本地运行时使用完整的 MLflow 跟踪服务器。此 MLflow 跟踪服务器设置与前几章相同。
- en: An overview of different execution scenarios and environments
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 不同执行场景和环境的概述
- en: 'In our previous chapters, we mainly focused on learning how to track DL pipelines
    using MLflow''s tracking capabilities. Most of our execution environments are
    in a local environment, such as a local laptop or desktop environment. However,
    as we already know, the DL full life cycle consists of different stages where
    we may need to run the DL pipelines either entirely, partially, or as a single
    step in a different execution environment. Here are two typical examples:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们主要集中在学习如何使用 MLflow 的跟踪功能来跟踪深度学习管道。我们的执行环境大多数是在本地环境中，比如本地笔记本电脑或桌面环境。然而，正如我们所知，深度学习完整生命周期由多个阶段组成，其中我们可能需要在不同的执行环境中完全、部分地或单独运行深度学习管道。以下是两个典型的例子：
- en: When accessing data for model training purposes, it is not uncommon to require
    the data to reside in an enterprise-security and privacy-compliant environment,
    where both the computation and the storage cannot leave a compliant boundary.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在访问用于模型训练的数据时，通常需要数据处于符合企业安全性和隐私合规要求的环境中，在这种环境下，计算和存储不能超出合规边界。
- en: When training a DL model, it is usually desirable to use a remote GPU cluster
    to maximize the efficiency of model training, where a local laptop usually does
    not have the required hardware capability.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练深度学习（DL）模型时，通常希望使用远程GPU集群来最大化模型训练的效率，因为本地笔记本电脑通常不具备所需的硬件能力。
- en: Both cases require a carefully defined execution environment that might be needed
    in one or multiple stages of the DL lifecycle. Note that this is not just a requirement
    to be flexible when moving from the development stage to a production environment,
    where the execution hardware and software configuration could be understandably
    different. It is also a requirement to be able to switch running environments
    during development stages or in different production environments without making
    major changes to the DL pipelines.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种情况都需要仔细定义的执行环境，这种环境可能在深度学习生命周期的一个或多个阶段中有所需求。需要注意的是，这不仅仅是为了在从开发阶段过渡到生产环境时的灵活性要求，因生产环境中的执行硬件和软件配置通常会有所不同。这也是一个要求，即能够在开发阶段或不同的生产环境中切换运行环境，而不需要对深度学习管道进行重大修改。
- en: 'Here, we classify the different scenarios and execution environments into the
    following four scenarios, based on the different combinations of the location
    of the source code of DL pipelines and target execution environments, as shown
    in the following table:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们根据深度学习管道源代码和目标执行环境的不同组合，将不同的场景和执行环境分类为以下四种场景，如下表所示：
- en: '![Figure 5.1 – Four different scenarios of DL pipeline source codes and target
    execution environments'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: '![图 5.1 – 深度学习管道源代码和目标执行环境的四种不同场景](img/B18120_05_01.jpg)'
- en: '](img/B18120_05_01.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '](img/B18120_05_01.jpg)'
- en: Figure 5.1 – Four different scenarios of DL pipeline source codes and target
    execution environments
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 – 深度学习管道源代码和目标执行环境的四种不同场景
- en: '*Figure 5.1* describes how either in development or production environments,
    we could encounter the possibilities of using either local or remote code to run
    in a different execution environment. Let''s examine them one by one as follows:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*图 5.1* 描述了在开发或生产环境中，我们可能遇到的使用本地或远程代码在不同执行环境中运行的情况。我们将逐一检查这些情况，如下所示：'
- en: '**Local source code running in a local target environment**: This usually happens
    at the development stage, where modest computing power in a local environment
    is adequate to support quick prototyping or test runs for small changes in an
    existing pipeline. This is mostly the scenario we have been using in previous
    chapters for our MLflow experiments when learning how to track pipelines.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地源代码在本地目标环境中运行**：这通常发生在开发阶段，在本地环境中适度的计算能力足以支持快速原型开发或对现有管道的小变更进行测试运行。这也是我们在前几章中使用的场景，尤其是在学习如何跟踪管道时，使用了
    MLflow 的实验。'
- en: '**Local source code running in a remote target environment**: This usually
    happens at the development stage or re-training of an existing DL model, where
    a GPU or other types of hardware accelerators, such as **Tensor Processing Units**
    (**TPUs**) or **field-programmable gate arrays** (**FPGAs**), are needed to perform
    computational and data-intensive model training or debugging prior to merging
    the GitHub repository (using local code change first).'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**本地源代码在远程目标环境中运行**：这通常发生在开发阶段或重新训练现有深度学习模型时，在这种情况下，需要使用GPU或其他类型的硬件加速器，如**张量处理单元**（**TPU**）或**现场可编程门阵列**（**FPGA**），来执行计算密集型和数据密集型的模型训练或调试，以便在合并
    GitHub 仓库（首先使用本地代码更改）之前进行调试。'
- en: '**Remote source code running in a local target environment**: This usually
    happens when we don''t have any changes in the code but the data has changed,
    either during the development stage or the production stage. For example, during
    the DL development stage, we could change the data with newly augmented training
    data either through some data augmentation techniques (for example, using **AugLy**
    to augment existing training data: [https://github.com/facebookresearch/AugLy](https://github.com/facebookresearch/AugLy))
    or newly annotated training data. During the production deployment step, we often
    need to run a regression test to evaluate a to-be-deployed DL pipeline against
    a hold-out regression testing dataset, so that we don''t deploy a degraded model
    if the model performance accuracy metric does not meet the bar. In this case,
    the hold-out testing dataset is not usually big, so the execution can be done
    on the deployment server locally instead of launching to a remote cluster in a
    Databricks server.'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**远程源代码在本地目标环境中运行**：通常发生在我们代码没有更改，但数据发生变化时，无论是在开发阶段还是生产阶段。例如，在DL开发阶段，我们可能会通过某些数据增强技术（例如，使用**AugLy**来增强现有训练数据：[https://github.com/facebookresearch/AugLy](https://github.com/facebookresearch/AugLy)）或新的注释训练数据来改变数据。在生产部署步骤中，我们经常需要运行回归测试，以评估待部署的DL管道在保留的回归测试数据集上的表现，这样我们就不会在模型性能精度指标未达到标准时部署一个性能下降的模型。在这种情况下，保留的测试数据集通常不大，因此执行可以在本地部署服务器上完成，而不需要启动到Databricks服务器的远程集群。'
- en: '**Remote source code running in a remote target environment**: This can happen
    in the development stage or production stage, where we want to use a fixed version
    of the DL pipeline code from GitHub to run in a remote GPU cluster to do model
    training, hyperparameter tuning, or re-training. Such large-scale execution can
    be time-consuming, and a remote GPU cluster could be very useful.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**远程源代码在远程目标环境中运行**：这通常发生在开发阶段或生产阶段，当我们希望使用GitHub上的固定版本的DL管道代码，在远程GPU集群上进行模型训练、超参数调整或再训练时。此类大规模执行可能非常耗时，而远程GPU集群在这种情况下非常有用。'
- en: 'Given the four different scenarios, it would be desirable to have a framework
    to be able to run the same DL pipeline with minimal configuration changes under
    these conditions. Prior to the arrival of MLflow, it took quite a lot of engineering
    and manual efforts to support these scenarios. MLflow provides an MLproject framework
    that supports all these four scenarios through the following three configurable
    mechanisms:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 鉴于这四种不同的场景，最好能有一个框架，在这些条件下以最小的配置更改来运行相同的DL管道。在MLflow出现之前，支持这些场景需要大量的工程和手动工作。MLflow提供了一个MLproject框架，通过以下三种可配置机制来支持这四种场景：
- en: '**Entry points**: We can define one or multiple entry points to execute different
    steps of a DL pipeline. For example, the following is an example to define a main
    entry point:'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**入口点**：我们可以定义一个或多个入口点来执行深度学习（DL）管道的不同步骤。例如，以下是定义一个主入口点的示例：'
- en: '[PRE0]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'The entry point''s name is `main`, which, by default, will be used when executing
    an MLflow run without specifying an entry point for an MLproject. Under this `main`
    entry point, there is a list of parameters. We can define the parameter''s type
    and default value using a short syntax, as follows:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 入口点的名称是`main`，默认情况下，当执行MLflow运行时如果没有指定MLproject的入口点，则会使用该入口点。在这个`main`入口点下，有一组参数列表。我们可以使用简短的语法定义参数的类型和默认值，如下所示：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'We can also use a long syntax, as follows:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用长语法，如下所示：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, we define only one parameter, called `pipeline_steps`, using the short
    syntax format with a `str` type and a default value of `all`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们只定义了一个参数，叫做`pipeline_steps`，使用简短的语法格式，类型为`str`，默认值为`all`。
- en: '`yaml` configuration file or a Docker image to define the software and library
    dependencies that can be used by the MLproject''s entry points. Note that a single
    MLproject can either use a conda `yaml` file or a Docker image, but not both at
    the same time. Depending on the DL pipeline dependencies, sometimes using a conda
    .`yaml` file over a Docker image is preferred, since it is much more lightweight
    and easier to make changes without requiring additional Docker image storage locations
    and loading a large Docker image into memory in a resource-limited environment.
    However, a Docker image does sometimes have advantages if there are any Java packages
    (`.jar`) that are needed at runtime. If there are no such JAR dependencies, then
    it is preferred to have a conda .`yaml` file to specify the dependencies. Furthermore,
    as of MLflow version 1.22.0, running Docker-based projects on Databricks is not
    yet supported by the MLflow command line. If there are indeed any Java package
    dependencies, they can be installed using `yaml` configuration files to define
    execution environment dependencies in this book.'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`yaml` 配置文件或 Docker 镜像用来定义 MLproject 入口点可以使用的软件和库依赖。请注意，一个 MLproject 只能使用 conda
    `yaml` 文件或 Docker 镜像中的一个，而不能同时使用两者。根据深度学习管道的依赖，有时使用 conda .`yaml` 文件而非 Docker
    镜像更为合适，因为它更加轻量，且更容易修改，无需额外的 Docker 镜像存储位置或在资源有限的环境中加载大型 Docker 镜像。然而，如果在运行时需要
    Java 包（`.jar`），那么使用 Docker 镜像可能会有优势。如果没有这样的 JAR 依赖，那么更推荐使用 conda .`yaml` 文件来指定依赖。此外，MLflow
    版本 1.22.0 以后，在 Databricks 上运行基于 Docker 的项目尚未得到 MLflow 命令行支持。如果确实有 Java 包依赖，可以通过
    `yaml` 配置文件来定义执行环境依赖，本书中会有介绍。'
- en: '**Hardware dependencies**: We can use a cluster configuration JSON file to
    define the execution target backend environment, be it a GPU, CPU, or other types
    of clusters. This is only needed when the target backend execution environment
    is non-local, either in a Databricks server or a **Kubernetes** (**K8s**) cluster.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**硬件依赖**：我们可以使用集群配置 JSON 文件来定义执行目标后端环境，无论是 GPU、CPU 还是其他类型的集群。当目标后端执行环境非本地时，才需要这个配置，无论是在
    Databricks 服务器还是 **Kubernetes**（**K8s**）集群中。'
- en: Previously, we learned how to use MLproject to create a multiple-step DL pipeline
    running in a local environment in [*Chapter 4*](B18120_04_ePub.xhtml#_idTextAnchor050),
    *Tracking Code and Data Versioning*, for tracking purposes. Now, we are going
    to learn how to use MLproject for supporting the different running scenarios outlined
    previously.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们学习了如何使用 MLproject 创建一个多步骤的深度学习管道，在本地环境中运行，如 [*第 4 章*](B18120_04_ePub.xhtml#_idTextAnchor050)《跟踪代码和数据版本控制》，用于跟踪目的。现在，我们将学习如何使用
    MLproject 支持前面提到的不同运行场景。
- en: Running locally with local code
  id: totrans-40
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地运行带有本地代码的项目
- en: 'Let''s start with the first running scenario using the same **Natural Language
    Processing (NLP)** text sentiment classification example as the driving use case.
    You are advised to check out the following version of the source code from the
    GitHub location to follow along with the steps and learnings: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/26119e984e52dadd04b99e6f7e95f8dda8b59238/chapter05](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/26119e984e52dadd04b99e6f7e95f8dda8b59238/chapter05).
    Note that this requires a specific Git hash committed version, as shown in the
    URL path. That means we are asking you to check out a specific committed version,
    not the main branch.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一个运行场景开始，使用相同的 **自然语言处理（NLP）** 文本情感分类示例作为驱动案例。建议您从 GitHub 上获取以下版本的源代码，以便跟随步骤并学习：
    [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/26119e984e52dadd04b99e6f7e95f8dda8b59238/chapter05](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/26119e984e52dadd04b99e6f7e95f8dda8b59238/chapter05)。请注意，这需要一个特定的
    Git 哈希提交版本，如 URL 路径所示。这意味着我们要求您检查一个特定的提交版本，而不是主分支。
- en: 'Let''s start with the DL pipeline that downloads the review data to local storage
    as a first execution exercise. Once you check out this chapter''s code, you can
    type the following command line to execute the DL pipeline''s first step:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从下载评论数据到本地存储的深度学习管道开始，作为第一次执行练习。检查完本章的代码后，您可以输入以下命令行来执行深度学习管道的第一步：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If we don't specify an entry point, it defaults to `main`. In this case, this
    is our desired behavior since we want to run the `main` entry point to start the
    parent DL pipeline.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们没有指定入口点，它默认是 `main`。在这种情况下，这是我们期望的行为，因为我们希望运行 `main` 入口点来启动父级深度学习管道。
- en: 'The *dot* means the current local directory. This tells MLflow to use the code
    in the current directory as the source to execute the project. If this command
    line runs successfully, you should be able to see the first two lines of output
    in the console as follows, which also reveal where the target execution environment
    is:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '*点*表示当前本地目录。这告诉 MLflow 使用当前目录中的代码作为执行项目的源。如果此命令行运行成功，你应该能够在控制台中看到前两行输出，如下所示，同时也可以看到目标执行环境的位置：'
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Note that the second output line shows `mlflow.projects.backend.local`, which
    means the target running environment is local. You may wonder where we define
    the local execution environment in our initial command line. It turns out that
    by default, the value for the parameter called `--backend` (or `-b`) is `local`.
    So, if we spell out the default values, the `mlflow run` command line will look
    like the following:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，第二行输出显示了 `mlflow.projects.backend.local`，这意味着目标运行环境是本地的。你可能会好奇我们在初始命令行中在哪里定义了本地执行环境。事实证明，默认情况下，名为
    `--backend`（或 `-b`）的参数的值是 `local`。因此，如果我们列出默认值，`mlflow run` 命令行将如下所示：
- en: '[PRE5]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that we also need to specify `experiment-name` in the command line or through
    an environment variable named `MLFLOW_EXPERIMENT_NAME` to define the experiment
    in which this project will run. Alternatively, you can specify an `experiment-id`
    parameter, or an environment variable named `MLFLOW_EXPERIMENT_ID`, to define
    the experiment integer ID that already exists. You only need to define either
    the ID or the name of the environment, but not both. It is common to define a
    human-readable experiment name and then query the experiment ID for that experiment
    in other parts of the code so that they will not be out of sync.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，我们还需要在命令行中或通过名为 `MLFLOW_EXPERIMENT_NAME` 的环境变量指定 `experiment-name`，以定义此项目将运行的实验。或者，你可以指定一个
    `experiment-id` 参数，或者一个名为 `MLFLOW_EXPERIMENT_ID` 的环境变量，以定义已经存在的实验整数 ID。你只需要定义环境的
    ID 或名称之一，而不是两者。通常我们会定义一个人类可读的实验名称，然后在代码的其他部分查询该实验的 ID，以确保它们不会不同步。
- en: MLflow Experiment Name or ID for Running an MLproject
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 MLproject 的 MLflow 实验名称或 ID
- en: To run an MLproject either using the CLI or the `mlflow.run` Python API, if
    we don't specify `experiment-name` or `experiment-id` through either an environment
    variable or a parameter assignment, it will default to the `Default` MLflow experiment.
    This is not desirable, as we want to organize our experiments into clearly separated
    experiments. In addition, once an MLproject starts running, any child runs will
    not be able to switch to a different experiment name or ID. So, the best practice
    will be always to specify an experiment name or an ID before launching an MLflow
    project run.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 CLI 或 `mlflow.run` Python API 运行一个 MLproject，如果我们没有通过环境变量或参数赋值指定 `experiment-name`
    或 `experiment-id`，它将默认使用 `Default` MLflow 实验。这并不是我们想要的，因为我们希望将实验组织成明确分开的实验。此外，一旦
    MLproject 开始运行，任何子运行将无法切换到不同的实验名称或 ID。因此，最佳实践是始终在启动 MLflow 项目运行之前指定实验名称或 ID。
- en: 'Once you finish the run, you will see the output as in the following lines:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你完成运行，你将看到如下的输出：
- en: '[PRE6]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Note that this is a nested MLflow run since we first launch a `main` entry
    point that starts the whole pipeline (that''s why there is `mlflow.parentRunId`),
    and then under this pipeline, we run one or multiple steps. Here, the step we
    run is called `download_data`, which is another entry point defined in the MLproject,
    but is invoked using the `mlflow.run` Python API, as follows, in the `main.py`
    file:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这是一个嵌套的 MLflow 运行，因为我们首先启动一个 `main` 入口点来启动整个管道（这就是为什么有 `mlflow.parentRunId`），然后在这个管道下，我们运行一个或多个步骤。这里我们运行的步骤叫做
    `download_data`，这是在 MLproject 中定义的另一个入口点，但它是通过 `mlflow.run` Python API 调用的，如下所示，在
    `main.py` 文件中：
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Note that this also specifies which code source to use (`local`, since we specified
    a *dot*), and by default, a local execution environment. That''s why you should
    be able to see the following lines in the console output:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这还指定了要使用的代码源（`local`，因为我们指定了一个 *点*），并默认使用本地执行环境。这就是为什么你应该能够在控制台输出中看到以下内容的原因：
- en: '[PRE8]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'You should also see a few other details of the run parameters for this entry
    point. The last two lines of the command line output should look like the following:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你还应该能看到该入口点的运行参数的其他几个细节。命令行输出的最后两行应如下所示：
- en: '[PRE9]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: If you see this, you should feel proud that you have successfully run a pipeline
    with one step to completion.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你看到这个输出，你应该感到自豪，因为你已经成功运行了一个包含一个步骤的管道，并且已完成。
- en: While this is something we have done before without knowing some of the details,
    the next section will allow us to run remote code in a local environment, where
    you will see the increasing flexibility and power of MLproject.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们之前也做过类似的事情，虽然没有了解其中的一些细节，但接下来的部分将让我们能够在本地环境中运行远程代码，你将看到 MLproject 的灵活性和功能越来越强大。
- en: Running remote code in GitHub locally
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在本地运行 GitHub 上的远程代码
- en: 'Now, let''s see how we run remote code from a GitHub repository on a local
    execution environment. This allows us to precisely run a specific version that
    has been checked into the GitHub repository using the commit hash. Let''s use
    the same example as before by running a single `download_data` step of the DL
    pipeline that we have been using in this chapter. In the command line prompt,
    run the following command:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看如何在本地执行环境中运行 GitHub 仓库中的远程代码。这让我们能够准确地运行一个特定版本，该版本已被提交到 GitHub 仓库中并使用提交哈希标识。我们继续使用之前的例子，在本章节中运行
    DL 管道的单个 `download_data` 步骤。在命令行提示符中，运行以下命令：
- en: '[PRE10]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Notice the difference between this command line and the one in the previous
    section. Instead of a *dot* to refer to a local copy of the code, we are pointing
    to a remote GitHub repository ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow))
    and the folder name (`chapter05`) that contains the MLproject file we want to
    reference. The `#` symbol denotes the relative path to the root folder, according
    to MLflow''s convention (see details on the MLflow documentation at this website:
    [https://www.mlflow.org/docs/latest/projects.html#running-projects](https://www.mlflow.org/docs/latest/projects.html#running-projects)).
    We then define a version number by specifying the Git commit hash using the `-v`
    parameter. In this case, it is this version we have in the GitHub repository:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这条命令行和前一节中的命令行之间的区别。我们不再用一个 *点* 来表示本地代码的副本，而是指向一个远程的 GitHub 仓库（[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow)），并指定包含我们想要引用的
    MLproject 文件的文件夹名称（`chapter05`）。`#` 符号表示相对于根文件夹的路径，这是根据 MLflow 的约定来定义的（详情请参考 MLflow
    文档：[https://www.mlflow.org/docs/latest/projects.html#running-projects](https://www.mlflow.org/docs/latest/projects.html#running-projects)）。然后我们通过指定
    Git 提交哈希，使用 `-v` 参数来定义版本号。在这种情况下，它是我们在 GitHub 仓库中的这个版本：
- en: '[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/26119e984e52dadd04b99e6f7e95f8dda8b59238/chapter05](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/26119e984e52dadd04b99e6f7e95f8dda8b59238/chapter05)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/26119e984e52dadd04b99e6f7e95f8dda8b59238/chapter05](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/26119e984e52dadd04b99e6f7e95f8dda8b59238/chapter05)'
- en: Hidden Bug of Running an MLflow Project with GitHub's Main Branch
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 运行 GitHub 的主分支时，MLflow 项目的隐性 Bug
- en: 'When we omit the `-v` parameter in the MLflow run, MLflow will assume we want
    to use the default `main` branch of a GitHub project. However, MLflow''s source
    code has a hardcoded reference to the `main` branch of a GitHub project as `origin.refs.master`,
    requiring the existence of a `master` branch in the GitHub project. This does
    not work in newer GitHub projects such as this book''s project, since the default
    branch is called `main`, not `master` anymore, due to the recent changes introduced
    by GitHub (see details here: [https://github.com/github/renaming](https://github.com/github/renaming)).
    So, at the time of writing this book, in the MLflow version 1.22.0, there is no
    way to run a default `main` branch of a GitHub project. We need to specifically
    declare the Git commit hash version when running an MLflow project in the GitHub
    repository.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在 MLflow 运行中省略 `-v` 参数时，MLflow 会假设我们想使用 GitHub 项目的默认 `main` 分支。然而，MLflow
    的源代码中硬编码了对 GitHub 项目 `main` 分支的引用，作为 `origin.refs.master`，这要求 GitHub 项目中必须存在 `master`
    分支。这在新的 GitHub 项目中不起作用，例如本书中的项目，因为默认分支已经不再叫 `master`，而是叫做 `main`，这是由于 GitHub 最近的更改所导致的（详见：[https://github.com/github/renaming](https://github.com/github/renaming)）。因此，在写这本书时，MLflow
    版本 1.22.0 无法运行 GitHub 项目的默认 `main` 分支。我们需要在运行 GitHub 仓库中的 MLflow 项目时，明确声明 Git
    提交哈希版本。
- en: 'So, what happens when you use the code in a remote GitHub project repository
    when running an MLflow project? It becomes clear when you see the first line of
    the following console output:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，当你在运行 MLflow 项目时使用远程 GitHub 项目仓库中的代码会发生什么呢？当你看到以下控制台输出的第一行时，这个问题就会变得清晰。
- en: '[PRE11]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This means that MLflow, on behalf of the user, starts to clone the remote project
    to a local temporary folder called `/var/folders/51/whxjy4r92dx18788yp11ycyr0000gp/T/tmpdyzaa1ye`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着 MLflow 会代表用户开始将远程项目克隆到一个本地临时文件夹，路径为`/var/folders/51/whxjy4r92dx18788yp11ycyr0000gp/T/tmpdyzaa1ye`。
- en: If you navigate to this temporary folder, you will see that the entire project
    content from GitHub has been cloned to this folder, not just the folder containing
    the ML project you want to run.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你导航到这个临时文件夹，你会看到整个 GitHub 项目的内容都已经被克隆到这个文件夹，而不仅仅是包含你要运行的 ML 项目的文件夹。
- en: 'The rest of the console output is as we have seen when using the local code.
    Once you finish the run with the `download_data` step, you should be able to find
    the downloaded data in the temporary folder under `chapter05`, since we define
    the local destination folder as a `./data` relative path in the ML project file:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 其余的控制台输出就像我们在使用本地代码时看到的一样。完成 `download_data` 步骤的运行后，你应该能够在 `chapter05` 下的临时文件夹中找到下载的数据，因为我们在
    ML 项目文件中将本地目标文件夹定义为相对于路径 `./data`：
- en: '[PRE12]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: MLflow automatically converts this to an absolute path, and it becomes a relative
    path to the cloned project folder under `chapter05`, since that's where the MLproject
    file resides.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: MLflow 会自动将其转换为绝对路径，然后变成相对路径，指向 `chapter05` 下的克隆项目文件夹，因为 MLproject 文件就位于该文件夹中。
- en: This capability to reference a remote GitHub project and run it in a local environment,
    whether this local environment is your laptop or a virtual machine in the cloud,
    is powerful. This enables automation through **continuous integration and continuous
    deployment** (**CI/CD**) since this can be directly invoked in a command line,
    which can then be scripted into a CI/CD script. The tracking part is also precise,
    since we have the Git commit hash logged in the MLflow tracking server, which
    allows us to know exactly which version of the code was executed.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这种能够引用远程 GitHub 项目并在本地环境中运行的能力，无论这个本地环境是你的笔记本还是云端的虚拟机，都非常强大。它使得通过**持续集成和持续部署**（**CI/CD**）实现自动化成为可能，因为这一过程可以直接在命令行中调用，并且可以被编写成
    CI/CD 脚本。追踪部分也非常精准，因为我们有在 MLflow 跟踪服务器中记录的 Git 提交哈希，这使得我们能够准确知道执行的是哪个版本的代码。
- en: Note in both the scenarios we just covered, the execution environment is a local
    machine where the MLflow run command was issued. The MLflow project runs to completion
    *synchronously*, meaning it is a blocking call and it will run to completion and
    show you the progress in the console output in real time.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在我们刚才讨论的两种场景中，执行环境是发出 MLflow run 命令的本地机器。MLflow 项目会*同步*执行至完成，这意味着它是一个阻塞调用，运行过程中会实时显示控制台输出的进度。
- en: However, there are additional running scenarios we need to support. For example,
    sometimes the machine where we issue the MLflow project run command is not powerful
    enough to support the computation we need, such as training a DL model with many
    epochs. Another scenario could be if the data to be downloaded or accessed for
    training is multiple gigabytes and you don't want to download it to your local
    laptop for model development. This requires us to be able to run the code in a
    remote cluster. Let's look at how we can do that in the next section.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们需要支持一些额外的运行场景。例如，有时发出 MLflow 项目运行命令的机器不够强大，无法支持我们所需的计算任务，比如训练一个需要多个 epoch
    的深度学习模型。另一个场景可能是，训练所需的数据达到多个 GB，你不希望将其下载到本地笔记本进行模型开发。这要求我们能够在远程集群中运行代码。接下来我们将看看如何实现这一点。
- en: Running local code remotely in the cloud
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 在云端远程运行本地代码
- en: In previous chapters, we ran all our code in a local laptop environment, and
    limited our DL fine-tuning step to only three epochs due to the limited power
    of a laptop. This serves the purpose of getting the code running and testing quickly
    in a local environment but does not serve to build an actual high-performance
    DL model. We really need to run the fine-tuning step in a remote GPU cluster.
    Ideally, we should only change some configuration and still issue the MLflow run
    command line in a local laptop console, but the actual pipeline will be submitted
    to a remote cluster in the cloud. Let's see how we can do this for our DL pipeline.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们在本地笔记本环境中运行了所有代码，并且由于笔记本的计算能力有限，我们将深度学习微调步骤限制为仅三个 epoch。这能够实现代码的快速运行和本地环境的测试，但并不能真正构建一个高性能的深度学习模型。我们实际上需要在远程
    GPU 集群中运行微调步骤。理想情况下，我们应该只需更改一些配置，仍然在本地笔记本控制台中发出 MLflow run 命令，但实际的流水线将提交到云端的远程集群。接下来，我们将看看如何在我们的深度学习流水线中实现这一点。
- en: 'Let''s start with submitting code to run in a Databricks server. There are
    three prerequisites:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从向 Databricks 服务器提交代码开始。需要三个前提条件：
- en: '**An Enterprise Databricks server**: You need to have access to an Enterprise-licensed
    Databricks server or a free trial version of the Databricks server ([https://docs.databricks.com/getting-started/try-databricks.html#sign-up-for-a-databricks-free-trial](https://docs.databricks.com/getting-started/try-databricks.html#sign-up-for-a-databricks-free-trial))
    in the cloud. The Community version of Databricks does not support this remote
    execution.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**企业版 Databricks 服务器**：您需要访问一个企业许可的 Databricks 服务器或 Databricks 服务器的免费试用版（[https://docs.databricks.com/getting-started/try-databricks.html#sign-up-for-a-databricks-free-trial](https://docs.databricks.com/getting-started/try-databricks.html#sign-up-for-a-databricks-free-trial)）在云端。Databricks
    的社区版不支持此远程执行。'
- en: '**The Databricks CLI**: You need to set up the Databricks CLI where you issue
    the MLflow project run commands. To install it, simply run the following command:'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Databricks CLI**：您需要在运行 MLflow 项目命令的地方设置 Databricks CLI。要安装它，只需运行以下命令：'
- en: '[PRE13]'
  id: totrans-84
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: We also include this dependency in the `requirements.txt` file of `chapter05`
    when you check out the code for this chapter.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还在 `chapter05` 的 `requirements.txt` 文件中包括了这个依赖，当您获取本章代码时。
- en: '`.databrickscfg` file in your local home folder. You don''t need both, but
    if you do have both, the one defined using environment variables will take a higher
    precedence when being picked up by the Databricks command line. The approach of
    using environment variables and generating access tokens is described in the *Setting
    up MLflow to interact with a remote MLflow server* section of [*Chapter 1*](B18120_01_ePub.xhtml#_idTextAnchor015),
    *Deep Learning Life Cycle and MLOps Challenges*. Note these environment variables
    can be set up in the command line directly or can be put into your `.bash_profile`
    file if you are using a macOS or Linux machine.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`.databrickscfg` 文件位于您的本地主文件夹中。您不需要同时存在两者，但如果有两个，使用环境变量定义的文件会在 Databricks 命令行中优先被选取。使用环境变量和生成访问令牌的方法在[*第1章*](B18120_01_ePub.xhtml#_idTextAnchor015)，“深度学习生命周期与
    MLOps 挑战”中的 *设置 MLflow 与远程 MLflow 服务器交互* 部分有详细描述。请注意，这些环境变量可以直接在命令行中设置，也可以放入 `.bash_profile`
    文件中，如果您使用的是 macOS 或 Linux 机器。'
- en: 'Here, we describe how we can use the Databricks command-line tool to generate
    a `.databrickscfg` file:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里我们描述了如何使用 Databricks 命令行工具生成 `.databrickscfg` 文件：
- en: 'Run the following command to set up the token configuration:'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行以下命令来设置令牌配置：
- en: '[PRE14]'
  id: totrans-89
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Follow the prompt to fill in the remote Databricks host URL and the access
    token:'
  id: totrans-90
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照提示填写远程 Databricks 主机 URL 和访问令牌：
- en: '[PRE15]'
  id: totrans-91
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Now, if you check your local home folder, you should find a hidden file called
    `.databrickscfg`.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 现在，如果您检查本地主文件夹，应该会找到一个名为 `.databrickscfg` 的隐藏文件。
- en: 'If you open this file, you should be able to see something like the following:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您打开这个文件，应该能看到类似以下内容：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Note that the last line indicates the remote job submission and execution API
    version that the Databricks server is using.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，最后一行指示的是 Databricks 服务器正在使用的远程作业提交和执行 API 版本。
- en: 'Now that you have the access set up correctly, let''s see how we can run the
    DL pipeline remotely in the remote Databricks server using the following steps:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经正确设置了访问权限，让我们看看如何使用以下步骤在远程 Databricks 服务器上远程运行 DL 流水线：
- en: 'Since we are going to use the remote Databricks server, the local MLflow server
    we set up before no longer works. This means that we need to disable and comment
    out the following lines in the `main.py` file, which are only useful to the local
    MLflow server setup (check out the latest version of the code for `chapter05`
    from GitHub to follow the steps, at [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow.git](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow.git)):'
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 由于我们将使用远程 Databricks 服务器，因此之前设置的本地 MLflow 服务器不再有效。这意味着我们需要在 `main.py` 文件中禁用并注释掉以下几行，这些行仅对本地
    MLflow 服务器配置有用（从 GitHub 获取最新版本的 `chapter05` 代码以跟随步骤：[https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow.git](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow.git)）：
- en: '[PRE17]'
  id: totrans-98
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Instead, we should use the following environment variable that can be defined
    in a `.bash_profile` file or directly executed in the command line:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们应该使用以下环境变量，它可以在 `.bash_profile` 文件中定义，或者直接在命令行中执行：
- en: '[PRE18]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This will use the MLflow tracking server on the Databricks server. If you don't
    specify this, it will default to a localhost but will fail since there is no localhost
    version of MLflow on the remote Databricks server. So, make sure you have this
    set up correctly. Now, we are ready to run our local code remotely.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, run the following command line to submit the local code to the remote
    Databricks server to run. We will just start with the `download_data` step, as
    follows:'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You will see this time that the command line has two new parameters: `-b databricks`,
    which specifies the backend as a Databricks server, and `--backend-config cluster_spec.json`,
    which details the cluster specification. The content of this `cluster_spec.json`
    file is as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This `cluster_spec.json` file is typically located in the same folder in which
    the MLproject file is located and needs to be predefined so that the MLflow run
    command can pick it up. The example we give here only defines a minimal set of
    parameters needed to create a job cluster on Databricks using AWS's GPU virtual
    machine as a single node, but you can create a much richer cluster specification
    if necessary (see the following *Cluster Specification for Databricks* box for
    more details).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
- en: Cluster Specification for Databricks
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
- en: When submitting jobs to Databricks, it requires the creation of a new job cluster,
    which is different from an interactive cluster that you already have, where you
    can run an interactive job by attaching a notebook. A cluster specification is
    defined by minimally specifying the Databricks runtime version, which in our current
    example is `9.1.x-gpu-ml-scala2.12`, the number of worker nodes, and the node
    type ID, as shown in our example. It is recommended to use the `g4dn.xlarge`)
    for learning purposes. There are many other configurations that you can define
    in this cluster specification, including storage and access permission, and `init`
    scripts. The easiest way to generate a working cluster specification JSON file
    is to use the Databricks portal UI to create a new cluster, where you can select
    the Databricks runtime version, cluster node types, and other parameters ([https://docs.databricks.com/clusters/create.html](https://docs.databricks.com/clusters/create.html)).
    Then, you can get the JSON representation of the cluster by clicking on the JSON
    link on the top right of the **Create Cluster** UI page (see *Figure 5.2*).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.2 - An example of creating a cluster on Databricks'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_05_02.jpg)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.2 - An example of creating a cluster on Databricks
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
- en: 'Also notice that the `experiment-name` parameter in the preceding command no
    longer just takes an experiment name string but needs to include an absolute path
    in the Databricks workspace. This is different from the local MLflow tracking
    server. This convention must be followed to make this remote job submission work.
    Note that if you want to have several levels of subfolder structures, such as
    the following, then each subfolder must already exist in the Databricks server:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This means that the `rootPath`, `subfolder1,` and `subfolder2` folders must
    already exist. If not, the command line will fail since it cannot create the parent
    folder automatically on the Databricks server. That last string, `my_experiment_name`,
    can be automatically created if it does not already exist since that''s the actual
    experiment name that will host all the experiment runs. Note that, in this example,
    we are using the command-line parameter to specify the experiment name, but it
    is also possible to use the environment variable to specify it, as follows:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Once this command is executed, you will see a much shorter console output message
    this time compared with the previous run in a local environment. This is because
    when executing code this way, it runs *asynchronous*, which means the job is submitted
    to the remote Databricks server and immediately returns to the console without
    waiting. Let''s look at the first three lines of the output:'
  id: totrans-116
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  id: totrans-117
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The first line means that the experiment does not exist in the Databricks server,
    so it is being created. If you run this a second time, this will not show up.
    The second and third lines describe the process where MLflow packages the MLproject
    as a `.tar.gz` file and uploads it to the Databricks file server. Note that, unlike
    a GitHub project where it needs to check out the entire project from the repository,
    here, it only needs to package the `chapter05` folder since that's where our MLproject
    resides. This can be confirmed by looking at the job running logs in the Databricks
    cluster, which we will explain (where to get the job URL and how to look for the
    logs) in the next few paragraphs.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
- en: Synchronous and Asynchronous Running of MLproject
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
- en: 'The official MLflow run CLI does not support a parameter to specify the running
    of an MLflow project in asynchronous or synchronous mode. However, the MLflow
    run Python API does have a parameter called `synchronous`, which by default is
    set to be `True`. When using MLflow''s CLI to run an MLflow job using Databricks
    as the backend, the default behavior is asynchronous. Sometimes, synchronous behavior
    of the CLI run command is desirable during CI/CD automation when you need to make
    sure the MLflow run completes successfully before moving to the next step. This
    cannot be done with the official MLflow run CLI, but you can write a wrapper CLI
    Python function to call MLflow''s Python API with synchronous mode set to `True`
    and then use your own CLI Python command to run the MLflow job in synchronous
    mode. Also, note that `mlflow.run()` is the high-level fluent (object-oriented)
    API for the `mlflow.projects.run()` API. We use the `mlflow.run()` API extensively
    in this book for consistency. For details on the MLflow run Python API, see the
    official documentation page: [https://www.mlflow.org/docs/latest/python_api/mlflow.projects.html#mlflow.projects.run](https://www.mlflow.org/docs/latest/python_api/mlflow.projects.html#mlflow.projects.run).'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
- en: 'The next few lines of the output look similar to the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: These lines describe that the job has been submitted to the Databricks server
    and the job run ID and the job URL are shown in the last line (replace `???` with
    your actual Databricks URL to make this work for you). Notice that the MLflow
    run ID is `279456`, which is different from the ID you see in the job URL (`168339`).
    This is because the job URL is managed by the Databricks job management system
    and has a different way to generate and track each actual job.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
- en: Click the job URL link (`https://???.cloud.databricks.com#job/168339/run/1`)
    and check the status of this job, which will show the progress and standard output
    and error logs (see *Figure 5.3*). Usually, this page will take a few minutes
    to start showing the running progress since it needs to create a brand new cluster
    based on `cluster_spec.json` before it can start running the job.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.3 – MLflow run job status page with standard output'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_05_03.jpg)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.3 – MLflow run job status page with standard output
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.3* shows the job was successfully finished (`chapter05` folder was
    uploaded and extracted in the **Databricks File System** (**DBFS**). As mentioned
    previously, only the MLproject we want to run was packaged, uploaded, and extracted
    in the DBFS, not the entire project repository.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
- en: 'On the same job status page, you will also find the standard errors section,
    which shows the logs describing the pipeline step we wanted to run: `download_data`.
    These are not errors but just informational messages. All Python logs are aggregated
    here. See *Figure 5.4* for details:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.4 – MLflow job information logged on the job status page'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_05_04.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.4 – MLflow job information logged on the job status page
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.4* shows the log that''s very similar to what we see when we run
    in the local interactive environment, but now these runs were executed in the
    cluster we specified when we submitted the job. Note that the pipeline experiment
    ID is `427565` in *Figure 5.4*. You should be able to find the successfully completed
    MLflow DL pipeline runs in the integrated MLflow tracking server on the Databricks
    server, using the experiment ID `427565` in the following URL pattern:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
- en: '`https://[your databricks hostname]/#mlflow/experiments/427565`'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
- en: If you see the familiar tracking results as we have seen in previous chapters,
    give yourself a big hug since you just completed a major learning milestone in
    running local code in a remote Databricks cluster!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
- en: 'Furthermore, we can run multiple steps of the DL pipeline using this approach
    without changing any code in the individual step''s implementation. For example,
    if we want to run both the `download_data` and `fine_tuning_model` steps of the
    DL pipeline, we can issue the following command:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The output console will show the following short messages:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'You can then go to the job URL page shown in the last line of the console output
    and wait until it creates a new cluster and completes both steps. You should then
    be able to find both steps in the experiment folder logged in the MLflow tracking
    server, using the same experiment URL (since we use the same experiment name):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
- en: '`https://[your databricks hostname]/#mlflow/experiments/427565`'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know how to run local code in a remote Databricks cluster, we will
    learn how to run the code from a GitHub repository in a remote Databricks cluster.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
- en: Running remotely in the cloud with remote code in GitHub
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The most reliable way to reproduce a DL pipeline is to point to a specific version
    of the project code in GitHub and then run it in the cloud without invoking any
    local resources. This way, we know the exact version of the code as well as using
    the same running environment defined in the project. Let's see how this works
    with our DL pipeline.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
- en: 'As a prerequisite and a reminder, the following three environment variables
    need to be set up before you issue the MLflow run command to complete this section
    of the learning:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We already know how to set up these environment variables from the last section.
    There is potentially one more setup needed, which is to allow your Databricks
    server to access your GitHub repository if it is non-public (see the following
    *GitHub Token for Databricks to Access a Non-Public or Enterprise Project Repository*
    box).
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
- en: GitHub Token for Databricks to Access a Non-Public or Enterprise Project Repository
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
- en: 'To allow Databricks to access the project repository in GitHub, there is another
    token that''s needed. This can be generated by going to your personal GitHub page
    (https://github.com/settings/tokens) and then following the steps described on
    this page ([https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token](https://docs.github.com/en/authentication/keeping-your-account-and-data-secure/creating-a-personal-access-token)).
    You can then follow the instructions on the Databricks documentation website to
    set it up: [https://docs.databricks.com/repos.html#configure-your-git-integration-with-databricks](https://docs.databricks.com/repos.html#configure-your-git-integration-with-databricks).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s run the project using the specific version in the GitHub repository
    for the full pipeline on the remote Databricks cluster:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We will then see the output as brief as six lines. Let''s look at what the
    important information on each line shows and how this works:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
- en: 'The first line shows where the content of the project repository was downloaded
    to locally:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  id: totrans-154
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'If we go to the temporary directory shown in this message on the local machine
    where we execute this command, we see that the entire repository is already downloaded
    to this folder: `/var/folders/51/whxjy4r92dx18788yp11ycyr0000gp/T/tmpzcepn5h5`.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
- en: 'The next two lines show the project content was zipped and uploaded to a DBFS
    folder on the Databricks server:'
  id: totrans-156
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  id: totrans-157
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If we use the local command-line tool of Databricks, we can list this `.tar.gz`
    file as if it is a local file (but in fact, it is located remotely on the Databricks
    server):'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'You should see output similar to the following, which describes the attributes
    of the file (size, owner/group ID, and whether it is a file or directory):'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The next line shows that it starts to run the `main` entry point for this GitHub
    project:'
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Note the difference when we run the local code (it was a *dot* after the project,
    which means the current directory on the local system). Now, it lists the full
    path of the GitHub repository location.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
- en: 'The last two lines are like the previous section''s output, where it lists
    out the job URL:'
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'If we click the job URL in the last line of the console output, we will be
    able to see the following content on that website (*Figure 5.5*):'
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 5.5 – MLflow run job status page using the code from the GitHub repository'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_05_05.jpg)'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.5 – MLflow run job status page using the code from the GitHub repository
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.5* shows the end status of this job. Notice that the title of the
    page now says **MLflow Run for https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow#chapter05**,
    instead of **MLflow Run for .** as shown in the previous section when using local
    code to run.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
- en: 'The status of the job shows this was run successfully and you will also see
    that the results are logged in the experiment page as before, with all three steps
    finished. The model is also registered in the model registry as expected, in the
    Databricks server under the following URL:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
- en: '`https://[your_databricks_hostname]/#mlflow/models/dl_finetuned_model`'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
- en: 'In summary, the mechanism of how this approach works is shown in the following
    diagram (*Figure 5.6*):'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 5.6 – Summary view of running remote GitHub code in a remote Databricks
    cluster server'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_05_06.jpg)'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
- en: Figure 5.6 – Summary view of running remote GitHub code in a remote Databricks
    cluster server
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 5.6* shows that there are three different locations (a machine where
    we issue the MLflow run command, a remote Databricks server, and a remote GitHub
    project). When an MLflow run command is issued, the remote GitHub project source
    code is cloned to the machine where the MLflow run command was issued, and then
    uploaded to the remote Databricks server with a job submitted to execute the multiple
    steps of the DL pipeline. This is an asynchronous execution, and the status of
    the job needs to be monitored based on the job URL created.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
- en: Running an MLflow Project on Other Backends
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
- en: 'Right now, Databricks supports two types of remote running backend environments:
    Databricks and K8s. However, as of MLflow version 1.22.0 ([https://www.mlflow.org/docs/latest/projects.html#run-an-mlflow-project-on-kubernetes-experimental](https://www.mlflow.org/docs/latest/projects.html#run-an-mlflow-project-on-kubernetes-experimental)),
    running MLflow projects on K8s is still in experimental mode and is subject to
    change. If you are interested in learning more about this, refer to the reference
    in the *Further reading* section to explore an example provided. There are also
    other third-party provided backends (also called community plugins) such as `hadoop-yarn`
    ([https://github.com/criteo/mlflow-yarn](https://github.com/criteo/mlflow-yarn)).
    Due to the availability of Databricks in all major cloud providers and its maturity
    in supporting enterprise security-compliant production scenarios, this book currently
    focuses on learning about running MLflow projects remotely in a Databricks server.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  id: totrans-181
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we have learned how to run a DL pipeline in different execution
    environments (local or remote Databricks clusters) using either local source code
    or GitHub project repository code. This is critical not just for reproducibility
    and flexibility in executing a DL pipeline, but also provides much better productivity
    and future automation possibility using CI/CD tools. The power to run one or multiple
    steps of a DL pipeline in remote resource-rich environments gives us the speed
    to execute large-scale computation and data-intensive jobs that are typically
    seen in production-quality DL model training and fine-tuning. This allows us to
    do hyperparameter tuning or cross-validation of a DL model if necessary. We will
    start to learn how to run large-scale hyperparameter tuning in the next chapter
    as our natural next step.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MLflow run projects parameters (for both command line and Python API): [https://www.mlflow.org/docs/latest/projects.html#running-projects](https://www.mlflow.org/docs/latest/projects.html#running-projects)'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLflow run command line (CLI) documentation: [https://www.mlflow.org/docs/latest/cli.html#mlflow-run](https://www.mlflow.org/docs/latest/cli.html#mlflow-run)'
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLflow run projects on Databricks: [https://www.mlflow.org/docs/latest/projects.html#run-an-mlflow-project-on-databricks](https://www.mlflow.org/docs/latest/projects.html#run-an-mlflow-project-on-databricks)'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'An example of running an MLflow project on K8s: [https://github.com/SameeraGrandhi/mlflow-on-k8s/tree/master/examples/LogisticRegression](https://github.com/SameeraGrandhi/mlflow-on-k8s/tree/master/examples/LogisticRegression)'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Running MLflow projects on Azure: [https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-mlflow-projects](https://docs.microsoft.com/en-us/azure/machine-learning/how-to-train-mlflow-projects)'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
