<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer335">
<h1 class="chapter-number" id="_idParaDest-281"><a id="_idTextAnchor281"/>11</h1>
<h1 id="_idParaDest-282"><a id="_idTextAnchor282"/>Regularization in Computer Vision – Synthetic Image Generation</h1>
<p>This chapter will focus on the techniques and methods used to generate synthetic images for data augmentation. Having diverse data is often one of the most efficient ways to regularize computer vision models. Many approaches allow us to generate synthetic images; from simple tricks such as image flipping to new image creation using generative models. Several techniques will be explored in this chapter, including <span class="No-Break">the following:</span></p>
<ul>
<li>Image augmentation <span class="No-Break">with Albumentations</span></li>
<li>Creating synthetic images for object detection – training an object detection model with only <span class="No-Break">synthetic data</span></li>
<li>Real-time style transfer – training a model for real-time style transfer based on Stable Diffusion, a powerful <span class="No-Break">generative model</span></li>
</ul>
<h1 id="_idParaDest-283"><a id="_idTextAnchor283"/>Technical requirements</h1>
<p>In this chapter, we will train several deep learning models and generate images. We will need the <span class="No-Break">following libraries:</span></p>
<ul>
<li><span class="No-Break">NumPy</span></li>
<li><span class="No-Break">Matplotlib</span></li>
<li><span class="No-Break">Albumentations</span></li>
<li><span class="No-Break">Pytorch</span></li>
<li><span class="No-Break">torchvision</span></li>
<li><span class="No-Break">ultralytics</span></li>
</ul>
<h1 id="_idParaDest-284"><a id="_idTextAnchor284"/>Applying image augmentation with Albumentations</h1>
<p>More <a id="_idIndexMarker765"/>often <a id="_idIndexMarker766"/>than not, in <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>), data<a id="_idIndexMarker767"/> is crucial to getting better performances of models. Computer vision is no exception, and data augmentation with images can be easily taken to <span class="No-Break">another level.</span></p>
<p>Indeed, it is possible to easily augment an image, for example, by mirroring it, as shown in <span class="No-Break"><em class="italic">Figure 11</em></span><span class="No-Break"><em class="italic">.1</em></span><span class="No-Break">.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer314">
<img alt="Figure 11.1 – On the left, the original picture of my dog, and on the right, a mirrored picture of my dog" height="1012" src="image/B19629_11_01.jpg" width="1593"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.1 – On the left, the original picture of my dog, and on the right, a mirrored picture of my dog</p>
<p>However, beyond this, many more types of augmentation are possible and can be divided into two main categories: pixel-level and <span class="No-Break">spatial-level transformations.</span></p>
<p>Let's discuss both of these in the <span class="No-Break">following sections.</span></p>
<h2 id="_idParaDest-285"><a id="_idTextAnchor285"/>Spatial-level augmentation</h2>
<p>The mirroring is an <a id="_idIndexMarker768"/>example of spatial-level augmentation; however, much more than simple mirroring can be done. For example, see <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">Shifting</strong>: Shifting an image in a <span class="No-Break">certain direction</span></li>
<li><strong class="bold">Shearing</strong>: Add shearing to <span class="No-Break">an image</span></li>
<li><strong class="bold">Cropping</strong>: Cropping only part of <span class="No-Break">an image</span></li>
<li><strong class="bold">Rotating</strong>: Applying rotation to <span class="No-Break">an image</span></li>
<li><strong class="bold">Transposing</strong>: Transposing an image (in other words, applying both vertical and <span class="No-Break">horizontal flipping)</span></li>
<li><strong class="bold">Perspective</strong>: Applying a 4-point perspective to <span class="No-Break">an image</span></li>
</ul>
<p>As we can see, there are a lot of possibilities in spatial-level augmentation. <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.2</em> shows some examples of these possibilities on a given image and displays some of the possible<a id="_idIndexMarker769"/> artifacts: black<a id="_idIndexMarker770"/> borders on the shifted image and mirror padding on the <span class="No-Break">rotated image.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer315">
<img alt="Figure 11.2 – The original image (top left) and five different augmentations (note that some artifacts may appear, such as black borders)" height="1131" src="image/B19629_11_02.jpg" width="1400"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.2 – The original image (top left) and five different augmentations (note that some artifacts may appear, such as black borders)</p>
<h2 id="_idParaDest-286"><a id="_idTextAnchor286"/>Pixel-level augmentation</h2>
<p>Another type of <a id="_idIndexMarker771"/>augmentation is at the pixel level and can be as useful<a id="_idIndexMarker772"/> as spatial-level augmentation. A simple example could be to change the brightness and contrast level of an image so that a model can be more robust in various <span class="No-Break">lighting conditions.</span></p>
<p>A non-exhaustive list of pixel-level augmentation is <span class="No-Break">the following:</span></p>
<ul>
<li><strong class="bold">Brightness</strong>: Modify <span class="No-Break">the brightness</span></li>
<li><strong class="bold">Contrast</strong>: Modify <span class="No-Break">the contrast</span></li>
<li><strong class="bold">Blurring</strong>: Blur <span class="No-Break">the image</span></li>
<li><strong class="bold">HSV</strong>: Randomly modify the <strong class="bold">hue</strong>, <strong class="bold">saturation</strong>, and <strong class="bold">value</strong> of <span class="No-Break">the image</span></li>
<li><strong class="bold">Color conversion</strong>: Convert the image into black and white <span class="No-Break">or sepia</span></li>
<li><strong class="bold">Noise</strong>: Add noise to <span class="No-Break">the image</span></li>
<li>There are many possibilities, and this can make a huge difference in model robustness. A few examples of the results of these augmentations are shown in the <span class="No-Break">following figure:</span></li>
</ul>
<div>
<div class="IMG---Figure" id="_idContainer316">
<img alt="Figure 11.3 – An original image (top left) and several pixel-level augmentations" height="1131" src="image/B19629_11_03.jpg" width="1396"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.3 – An original image (top left) and several pixel-level augmentations</p>
<p>As we can see, using both pixel-level and spatial-level transformations, it is fairly easy to augment a single image into 5 or 10 images. Moreover, these augmentations can sometimes be composed together for more diversity. Of course, it does not replace a real, large, and <a id="_idIndexMarker773"/>diverse <a id="_idIndexMarker774"/>dataset, but image augmentation is usually cheaper than collecting data and allows us to get real boosts in <span class="No-Break">model performance.</span></p>
<h2 id="_idParaDest-287"><a id="_idTextAnchor287"/>Albumentations</h2>
<p>Of course, we do not <a id="_idIndexMarker775"/>have to reimplement all of these image augmentations manually. Several libraries for image augmentation exist, and Albumentations is arguably the most complete, free, and open source solution on the market. As we will see in this recipe, the Albumentations library allows powerful image augmentations <a id="_idIndexMarker776"/>with just a few lines <span class="No-Break">of code.</span></p>
<h2 id="_idParaDest-288"><a id="_idTextAnchor288"/>Getting started</h2>
<p>In this recipe, we<a id="_idIndexMarker777"/> will apply image augmentation to a simple challenge: classifying cats <span class="No-Break">and dogs.</span></p>
<p>We first need to download and prepare the dataset. The dataset, originally proposed by Microsoft, is made of 12,491 cat pictures and 12,470 dog pictures. It can be downloaded with the Kaggle API with the following <span class="No-Break">command-line operation:</span></p>
<pre class="source-code">
kaggle datasets download -d karakaggle/kaggle-cat-vs-dog-dataset --unzip</pre>
<p>This will download a folder <span class="No-Break">named </span><span class="No-Break"><strong class="source-inline">kagglecatsanddogs_3367a</strong></span><span class="No-Break">.</span></p>
<p>Unfortunately, the dataset is not yet split into train and test sets. The following code will split it into 80% train and 20% <span class="No-Break">test sets:</span></p>
<pre class="source-code">
from glob import glob
import os
cats_folder = 'kagglecatsanddogs_3367a/PetImages/Cat/'
dogs_folder = 'kagglecatsanddogs_3367a/PetImages/Dog/'
cats_paths = sorted(glob(cats_folder + '*.jpg'))
dogs_paths = sorted(glob(dogs_folder + '*.jpg'))
train_ratio = 0.8
os.mkdir(cats_folder + 'train')
os.mkdir(cats_folder + 'test')
os.mkdir(dogs_folder + 'train')
os.mkdir(dogs_folder + 'test')
for i in range(len(cats_paths)):
    if i &lt;= train_ratio * len(cats_paths):
        os.rename(cats_paths[i], cats_folder + 'train/' + cats_paths[i].split('/')[-1])
    else:
        os.rename(cats_paths[i], cats_folder + 'test/' + cats_paths[i].split('/')[-1])
for i in range(len(dogs_paths)):
    if i &lt;= train_ratio * len(dogs_paths):
        os.rename(dogs_paths[i], dogs_folder + 'train/' + dogs_paths[i].split('/')[-1])
    else:
        os.rename(dogs_paths[i], dogs_folder + 'test/' + dogs_paths[i].split('/')[-1])</pre>
<p>This will create <a id="_idIndexMarker778"/>subfolders of <strong class="source-inline">train</strong> and <strong class="source-inline">test</strong>, so that the <strong class="source-inline">kagglecatsanddogs_3367a</strong> folder tree now looks like <span class="No-Break">the following:</span></p>
<pre class="source-code">
kagglecatsanddogs_3367a
└── PetImages
    ├── Cat
    │   ├── train: 9993 images
    │   └── test: 2497 images
    └── Dog
        ├── train: 9976 images
        └── test: 2493 images</pre>
<p>We will now be able to efficiently train and evaluate a model against <span class="No-Break">this dataset.</span></p>
<p>The required libraries can be installed with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install matplotlib numpy torch torchvision albumentations</pre>
<h2 id="_idParaDest-289"><a id="_idTextAnchor289"/>How to do it…</h2>
<p>We will now train a MobileNet V3 network on the train dataset and evaluate it against the test dataset. Then, we will add image augmentation using Albumentations in order to improve the performance of <span class="No-Break">the model:</span></p>
<ol>
<li>First, import the <span class="No-Break">needed libraries:</span><ul><li><strong class="source-inline">matplotlib</strong> for display <span class="No-Break">and visualization</span></li><li><strong class="source-inline">numpy</strong> for <span class="No-Break">data manipulation</span></li><li><strong class="source-inline">Pillow</strong> for <span class="No-Break">image loading</span></li><li><strong class="source-inline">glob</strong> for <span class="No-Break">folder parsing</span></li><li><strong class="source-inline">torch</strong> and <strong class="source-inline">torchvision</strong> for the model training and related <span class="No-Break"><strong class="source-inline">util</strong></span><span class="No-Break"> instances</span></li></ul></li>
</ol>
<p>Here are<a id="_idIndexMarker779"/> the <span class="No-Break"><strong class="source-inline">import</strong></span><span class="No-Break"> statements:</span></p>
<pre class="source-code">
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from torchvision.models import mobilenet_v3_small
from glob import glob
from PIL import Image</pre>
<ol>
<li value="2">Next, we implement the <strong class="source-inline">DogsAndCats</strong> dataset class. It takes the <span class="No-Break">following arguments:</span><ul><li><strong class="source-inline">cats_folder</strong>: The path to the folder containing the <span class="No-Break">cat pictures</span></li><li><strong class="source-inline">dogs_folder</strong>: The path to the folder containing the <span class="No-Break">dog pictures</span></li><li><strong class="source-inline">transform</strong>: The transformation to apply to images (for example, resizing, converting into tensors, and <span class="No-Break">so on...)</span></li><li><strong class="source-inline">augment</strong>: To apply image augmentation, as we will do in the second part of <span class="No-Break">this recipe</span></li></ul></li>
</ol>
<p>Here is the code <a id="_idIndexMarker780"/>for <span class="No-Break">the implementation:</span></p>
<pre class="source-code">
class DogsAndCats(Dataset) :
    def __init__(self, cats_folder: str,
        dogs_folder: str, transform, augment = None):
            self.cats_path = sorted(glob(
                f'{cats_folder}/*.jpg'))
            self.dogs_path = sorted(glob(
                f'{dogs_folder}/*.jpg'))
            self.images_path = self.cats_path + self.dogs_path
            self.labels = [0.]*len(
            self.cats_path) + [1.]*len(self.dogs_path)
            self.transform = transform
            self.augment = augment
    def __len__(self):
        return len(self.images_path)
    def __getitem__(self, idx):
        image = Image.open(self.images_path[
            idx]).convert('RGB')
        if self.augment is not None:
            image = self.augment(
                image=np.array(image))["image"]
        return self.transform(image),
        torch.tensor(self.labels[idx],
            dtype=torch.float32)</pre>
<p>This class is rather simple: the constructor collects all the paths of the images and defines the labels accordingly. The getter simply loads an image, optionally applies image augmentation, and returns the image as <strong class="source-inline">tensor</strong> with its <span class="No-Break">associated label.</span></p>
<ol>
<li value="3">Then, we instantiate the transformation class. Here, we compose <span class="No-Break">three transformations:</span><ul><li><span class="No-Break">Tensor conversion</span></li><li>Resizing<a id="_idIndexMarker781"/> to 224x224 images since not all images are the <span class="No-Break">same size</span></li><li>Normalization of the <span class="No-Break">image input</span></li></ul></li>
</ol>
<p>Here is the code <span class="No-Break">for it:</span></p>
<pre class="source-code">
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Resize((224, 224), antialias=True),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5,
        0.5)),
])</pre>
<ol>
<li value="4">Then, we create a few useful variables, such as the batch size, device, and number <span class="No-Break">of epochs:</span><pre class="source-code">
batch_size = 64</pre><pre class="source-code">
device = torch.device(</pre><pre class="source-code">
    'cuda' if torch.cuda.is_available() else 'cpu')</pre><pre class="source-code">
epochs = 20</pre></li>
<li>Instantiate the datasets and data loaders. Reusing the train and test folders prepared earlier in the <em class="italic">Getting ready</em> subsection, we can now easily create our two loaders. They both use the <span class="No-Break">same transformation:</span><pre class="source-code">
trainset = DogsAndCats(</pre><pre class="source-code">
    'kagglecatsanddogs_3367a/PetImages/Cat/train/',</pre><pre class="source-code">
    'kagglecatsanddogs_3367a/PetImages/Dog/train/',</pre><pre class="source-code">
    transform=transform</pre><pre class="source-code">
)</pre><pre class="source-code">
train_dataloader = DataLoader(trainset,</pre><pre class="source-code">
    batch_size=batch_size, shuffle=True)</pre><pre class="source-code">
testset = DogsAndCats(</pre><pre class="source-code">
    'kagglecatsanddogs_3367a/PetImages/Cat/test/',</pre><pre class="source-code">
    'kagglecatsanddogs_3367a/PetImages/Dog/test/',</pre><pre class="source-code">
    transform=transform</pre><pre class="source-code">
)</pre><pre class="source-code">
test_dataloader = DataLoader(testset,</pre><pre class="source-code">
    batch_size=batch_size, shuffle=True)</pre></li>
<li>Now,  we display <a id="_idIndexMarker782"/>a few images along with their labels so that we get a glimpse at the dataset using the <span class="No-Break">following code:</span><pre class="source-code">
def display_images(dataloader, classes = ['cat', 'dog']):</pre><pre class="source-code">
    plt.figure(figsize=(14, 10))</pre><pre class="source-code">
    images, labels = next(iter(dataloader))</pre><pre class="source-code">
    for idx in range(8):</pre><pre class="source-code">
        plt.subplot(2, 4, idx+1)</pre><pre class="source-code">
        plt.imshow(images[idx].permute(</pre><pre class="source-code">
            1, 2, 0).numpy() * 0.5 + 0.5)</pre><pre class="source-code">
        plt.title(classes[int(labels[idx].item())])</pre><pre class="source-code">
        plt.axis('off')</pre><pre class="source-code">
display_images(train_dataloader)</pre></li>
</ol>
<p>Here are <a id="_idIndexMarker783"/><span class="No-Break">the images:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer317">
<img alt="Figure 11.4 – A sample of images from the dataset" height="1031" src="image/B19629_11_04.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.4 – A sample of images from the dataset</p>
<p>As we can see in the figure, this is a dataset made up of regular images of dogs and cats in various contexts, sometimes with humans in the <span class="No-Break">pictures too.</span></p>
<ol>
<li value="7">Now, we implement the <strong class="source-inline">Classifier</strong> class. We will reuse the existing <strong class="source-inline">mobilenet_v3_small</strong> implementation provided in <strong class="source-inline">pytorch</strong> and simply add an output layer with one unit and a sigmoid activation function, shown <span class="No-Break">as follows:</span><pre class="source-code">
class Classifier(nn.Module):</pre><pre class="source-code">
    def __init__(self):</pre><pre class="source-code">
        super(Classifier, self).__init__()</pre><pre class="source-code">
        self.mobilenet = mobilenet_v3_small()</pre><pre class="source-code">
        self.output_layer = nn.Linear(1000, 1)</pre><pre class="source-code">
    def forward(self, x):</pre><pre class="source-code">
        x = self.mobilenet(x)</pre><pre class="source-code">
        x = nn.Sigmoid()(self.output_layer(x))</pre><pre class="source-code">
        return x</pre></li>
<li>Next, we instantiate <span class="No-Break">the model:</span><pre class="source-code">
model = Classifier()</pre><pre class="source-code">
model = model.to(device)</pre></li>
<li>Then, instantiate<a id="_idIndexMarker784"/> the loss function as the binary cross-entropy loss, well suited to binary classification. Here we instantiate the <span class="No-Break">Adam optimizer:</span><pre class="source-code">
criterion = nn.BCELoss()</pre><pre class="source-code">
optimizer = torch.optim.Adam(model.parameters(),</pre><pre class="source-code">
    lr=0.001)</pre></li>
<li>Then, train the model for 20 epochs and store the outputs. To do so, we use the <strong class="source-inline">train_model</strong> function, which trains the input model for a given number of epochs and with a given dataset. It returns the loss and accuracy for the training and test set for each epoch. This function is available in the GitHub repository (<a href="https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_11/chapter_11.ipynb">https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_11/chapter_11.ipynb</a>), and is typical code for binary classification training, as we used in <span class="No-Break">previous chapters:</span><pre class="source-code">
train_losses, test_losses, train_accuracy, </pre><pre class="source-code">
test_accuracy = train_model(</pre><pre class="source-code">
    epochs, model, criterion, optimizer, device,</pre><pre class="source-code">
    train_dataloader, test_dataloader, trainset,</pre><pre class="source-code">
    testset</pre><pre class="source-code">
)</pre></li>
<li>Finally, display <a id="_idIndexMarker785"/>the loss and accuracy as a function of <span class="No-Break">the epoch:</span><pre class="source-code">
plt.figure(figsize=(10, 10))</pre><pre class="source-code">
plt.subplot(2, 1, 1)</pre><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.ylabel('BCE Loss')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.subplot(2, 1, 2)</pre><pre class="source-code">
plt.plot(train_accuracy, label='train')</pre><pre class="source-code">
plt.plot(test_accuracy, label='test')</pre><pre class="source-code">
plt.xlabel('Epoch')</pre><pre class="source-code">
plt.ylabel('Accuracy')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here are the plots <span class="No-Break">for it:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer318">
<img alt="Figure 11.5 – Binary cross-entropy loss (top) and accuracy (bottom) as a function of the epoch for both train and test sets with no augmentation (the loss and accuracy are suggesting overfitting while the test accuracy plateaus at around 88%)" height="803" src="image/B19629_11_05.jpg" width="838"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.5 – Binary cross-entropy loss (top) and accuracy (bottom) as a function of the epoch for both train and test sets with no augmentation (the loss and accuracy are suggesting overfitting while the test accuracy plateaus at around 88%)</p>
<p>As we can see, the test accuracy seems to reach a plateau after roughly 10 epochs, with a peak accuracy <a id="_idIndexMarker786"/>of around 88%. The train accuracy gets as high as 98%, suggesting strong overfitting on the <span class="No-Break">train set.</span></p>
<h3>Training with image augmentation</h3>
<p>Let’s now redo <a id="_idIndexMarker787"/>the same exercise with <span class="No-Break">image augmentation:</span></p>
<ol>
<li>First, we need to implement the desired image augmentation. Using the same pattern as with the transformations in <strong class="source-inline">pytorch</strong>, using Albumentations, we can instantiate a <strong class="source-inline">Compose</strong> class with a list of augmentations. In our case, we use the <span class="No-Break">following augmentations:</span><ul><li><strong class="source-inline">HorizontalFlip</strong>: This involves basic mirroring, occurring with a 50% probability, meaning 50% of the images will be <span class="No-Break">randomly mirrored</span></li><li><strong class="source-inline">Rotate</strong>: This will randomly rotate an image in the range of [-90, 90] degrees (this range can be modified) with a probability <span class="No-Break">of 50%</span></li><li><strong class="source-inline">RandomBrightnessContrast</strong>: This will randomly change the brightness and contrast of the image with a probability <span class="No-Break">of 20%</span></li></ul></li>
</ol>
<p>Here are <span class="No-Break">the instantiations:</span></p>
<pre class="source-code">
import albumentations as A
augment = A.Compose([
    A.HorizontalFlip(p=0.5),
    A.Rotate(p=0.5),
    A.RandomBrightnessContrast(p=0.2),
])</pre>
<ol>
<li value="2">Then, instantiate a new, augmented training set and training data loader. To do so, we simply have to provide our <strong class="source-inline">augment</strong> object as an argument of the <span class="No-Break"><strong class="source-inline">DogsAndCats</strong></span><span class="No-Break"> class:</span><pre class="source-code">
augmented_trainset = DogsAndCats(</pre><pre class="source-code">
    'kagglecatsanddogs_3367a/PetImages/Cat/train/',</pre><pre class="source-code">
    'kagglecatsanddogs_3367a/PetImages/Dog/train/',</pre><pre class="source-code">
    transform=transform,</pre><pre class="source-code">
    augment=augment,</pre><pre class="source-code">
)</pre><pre class="source-code">
augmented_train_dataloader = DataLoader(</pre><pre class="source-code">
    augmented_trainset, batch_size=batch_size,</pre><pre class="source-code">
    shuffle=True)</pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">We do not apply augmentation to the test set since we need to be able to compare the performances to the results without augmentation. Besides that, it would be useless to augment the test set, unless you are using Test Time Augmentation (see the <em class="italic">There’s more…</em> section for more <span class="No-Break">about it).</span></p>
<ol>
<li value="3">Then,  display a few images from this new, augmented dataset <span class="No-Break">as follows:</span><pre class="source-code">
display_images(augmented_train_dataloader)</pre></li>
</ol>
<p>Here are <span class="No-Break">the</span><span class="No-Break"><a id="_idIndexMarker788"/></span><span class="No-Break"> images:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer319">
<img alt="Figure 11.6 – Example of augmented images (some have been rotated, some have been mirrored and some have modified brightness and contrast)" height="1035" src="image/B19629_11_06.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.6 – Example of augmented images (some have been rotated, some have been mirrored and some have modified brightness and contrast)</p>
<p>As we can see, some images seem now rotated. Besides, some images are also mirrored and have a modified brightness and contrast, efficiently improving the diversity of <span class="No-Break">the dataset.</span></p>
<ol>
<li value="4">Then, we instantiate the model and <span class="No-Break">the optimizer:</span><pre class="source-code">
model = Classifier()</pre><pre class="source-code">
model = model.to(device)</pre><pre class="source-code">
optimizer = torch.optim.Adam(model.parameters(),</pre><pre class="source-code">
    lr=0.001)</pre></li>
<li>Next, train the <a id="_idIndexMarker789"/>model on this new training set while keeping the same test set and store the output losses <span class="No-Break">and metrics:</span><pre class="source-code">
train_losses, test_losses, train_accuracy, </pre><pre class="source-code">
test_accuracy = train_model(</pre><pre class="source-code">
    epochs, model, criterion, optimizer, device,</pre><pre class="source-code">
    augmented_train_dataloader, test_dataloader,</pre><pre class="source-code">
    trainset, testset</pre><pre class="source-code">
)</pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">In this recipe, we are doing augmentation online, meaning that every time we load a new batch of images, we randomly apply augmentation to these images; consequently, at each epoch, we may train from differently <span class="No-Break">augmented images.</span></p>
<p>Another approach is to augment data offline: we preprocess and augment the dataset, store the augmented images, and then only train the model on <span class="No-Break">this data.</span></p>
<p>Both approaches have pros and cons: offline augmentation allows us to augment images only once but requires more storage space, while online preprocessing may take more time to train but does not require any <span class="No-Break">extra storage.</span></p>
<ol>
<li value="6">Now finally, we plot the results: the loss and accuracy for both the training and test sets. Here<a id="_idIndexMarker790"/> is the code <span class="No-Break">for that:</span><pre class="source-code">
plt.figure(figsize=(10, 10))</pre><pre class="source-code">
plt.subplot(2, 1, 1)</pre><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.ylabel('BCE Loss')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.subplot(2, 1, 2)</pre><pre class="source-code">
plt.plot(train_accuracy, label='train')</pre><pre class="source-code">
plt.plot(test_accuracy, label='test')</pre><pre class="source-code">
plt.xlabel('epoch')</pre><pre class="source-code">
plt.ylabel('Accuracy')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here are <span class="No-Break">the plots:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer320">
<img alt="Figure 11.7 – Loss and accuracy for the augmented dataset" height="803" src="image/B19629_11_07.jpg" width="838"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.7 – Loss and accuracy for the augmented dataset</p>
<p>As you can see in the preceding figure, compared to the regular dataset, not only is the overfitting almost totally removed but the accuracy also climbs up to more than 91%, compared to <span class="No-Break">88% previously.</span></p>
<p>Thanks to this<a id="_idIndexMarker791"/> rather simple image augmentation, we could get the accuracy to climb from 88% to 91%, while reducing overfitting: the train set now has the same performances as the <span class="No-Break">test set.</span></p>
<h2 id="_idParaDest-290"><a id="_idTextAnchor290"/>There’s more…</h2>
<p>While we used augmentation for training, there is a method that takes advantage of image augmentation at test time to improve the performances of models. This is sometimes<a id="_idIndexMarker792"/> called <strong class="bold">Test </strong><span class="No-Break"><strong class="bold">Time Augmentation</strong></span><span class="No-Break">.</span></p>
<p>The idea is simple: compute model inference on several, augmented images, and compute the final prediction with a <span class="No-Break">majority vote.</span></p>
<p>Let’s take a simple example. Assuming we have an input image that must be classified with our trained dogs and cats model, we augment this input image with mirroring and with brightness and contrast, so that we have <span class="No-Break">three images:</span></p>
<ul>
<li><strong class="bold">Image 1</strong>: The <span class="No-Break">original image</span></li>
<li><strong class="bold">Image 2</strong>: The <span class="No-Break">mirrored image</span></li>
<li><strong class="bold">Image 3</strong>: The image with modified brightness <span class="No-Break">and contrast</span></li>
</ul>
<p>We will now compute model inference on those three images, getting the <span class="No-Break">following predictions:</span></p>
<ul>
<li><strong class="bold">Image 1 </strong><span class="No-Break"><strong class="bold">prediction</strong></span><span class="No-Break">: Cat</span></li>
<li><strong class="bold">Image 2 </strong><span class="No-Break"><strong class="bold">prediction</strong></span><span class="No-Break">: Cat</span></li>
<li><strong class="bold">Image 3 </strong><span class="No-Break"><strong class="bold">prediction</strong></span><span class="No-Break">: Dog</span></li>
</ul>
<p>We can now compute a majority vote by choosing the most represented predicted class, resulting in a cat <span class="No-Break">class prediction.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">In practice, we would most likely use a soft majority vote, averaging the predicted probabilities (either for binary or multiclass classification), but the concept remains <span class="No-Break">the same.</span></p>
<p>Test Time Augmentation<a id="_idIndexMarker793"/> is commonly used in competitions and can indeed improve the performance of the model for no added training cost. In a production environment though, where the inference cost is key, this method is <span class="No-Break">rarely used.</span></p>
<h2 id="_idParaDest-291"><a id="_idTextAnchor291"/>See also</h2>
<p>In this recipe, we used Albumentations for a simple classification task, but it can be used for much more than that: it allows us to perform image augmentation for object detection, instance segmentation, semantic segmentation, landmarks, and <span class="No-Break">so on.</span></p>
<p>To know more about how to use it fully, have a look at the well-written documentation, with many working<a id="_idIndexMarker794"/> examples <span class="No-Break">here: </span><a href=""><span class="No-Break">https://albumentations.ai/docs/</span></a><span class="No-Break">.</span></p>
<h1 id="_idParaDest-292"><a id="_idTextAnchor292"/>Creating synthetic images for object detection</h1>
<p>For some <a id="_idIndexMarker795"/>projects, you may have so little <a id="_idIndexMarker796"/>data that the only thing you can do is use this data in the test set. In some rare cases, it is possible to create a synthetic dataset to create a robust enough model and test it against the small, real <span class="No-Break">test set.</span></p>
<p>This is what we will do in this recipe: we have a small test set of pictures of QR codes, and we want to build an object detection model for the detection of QR codes. All we have as a train set is a set of generated QR codes and downloaded images collected on open image websites such <span class="No-Break">as </span><span class="No-Break">unsplash.com</span><span class="No-Break">.</span></p>
<h2 id="_idParaDest-293"><a id="_idTextAnchor293"/>Getting started</h2>
<p>Download and unzip the dataset from <a href="">https://www.kaggle.com/datasets/vincentv/qr-detection-yolo</a> with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
kaggle datasets download -d vincentv/qr-detection-yolo --unzip</pre>
<p>This dataset is made up of the following <span class="No-Break">folder architecture:</span></p>
<pre class="source-code">
QR-detection-yolo
├── train
│   ├── images: 9750 images
│   └── labels: 9750 text files
├── test
│   ├── images: 683 images
│   └── labels: 683 text files
└── background_images: 44 images</pre>
<p>It is made up of <span class="No-Break">three folders:</span></p>
<ul>
<li><strong class="bold">The train set</strong>: Only generated QR codes with <span class="No-Break">no context</span></li>
<li><strong class="bold">The test set</strong>: Pictures of QR codes in various contexts <span class="No-Break">and environments</span></li>
<li><strong class="bold">Background images</strong>: Random images of context such <span class="No-Break">as stores</span></li>
</ul>
<p>The goal is to use the data in the train set and the background images to generate realistic synthetic images to train a model on and only then to evaluate the model against the test set, made of <span class="No-Break">real images.</span></p>
<p>For this recipe, the needed libraries can be installed with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install albumentations opencv-python matplotlib numpy ultralytics.</pre>
<h2 id="_idParaDest-294"><a id="_idTextAnchor294"/>How to do it…</h2>
<p>Let’s divide this recipe into <span class="No-Break">three parts:</span></p>
<ol>
<li>First, we will explore the dataset and implement a few <span class="No-Break">helper functions.</span></li>
<li>The second<a id="_idIndexMarker797"/> part is about generating<a id="_idIndexMarker798"/> synthetic data using QR codes and <span class="No-Break">background images.</span></li>
<li>The last part is about training a YOLO model on the generated data and evaluating <span class="No-Break">this model.</span></li>
</ol>
<p>Let us understand each of these in the <span class="No-Break">following sections.</span></p>
<h3>Exploring the dataset</h3>
<p>Let’s start<a id="_idIndexMarker799"/> by creating a few helper functions and use them to display a few images of the train and <span class="No-Break">test sets:</span></p>
<ol>
<li>Import the <span class="No-Break">following libraries:</span><ul><li><strong class="source-inline">glob</strong> for <span class="No-Break">listing files</span></li><li><strong class="source-inline">os</strong> for making a directory in which to store the created <span class="No-Break">synthetic images</span></li><li><strong class="source-inline">albumentations</strong> for <span class="No-Break">data augmentation</span></li><li><strong class="source-inline">cv2</strong> for <span class="No-Break">image manipulation</span></li><li><strong class="source-inline">matplotlib</strong> <span class="No-Break">for display</span></li><li><strong class="source-inline">numpy</strong> for various <span class="No-Break">data manipulation</span></li><li><strong class="source-inline">YOLO</strong> for <span class="No-Break">the model</span></li></ul></li>
</ol>
<p>Here are <span class="No-Break">the imports:</span></p>
<pre class="source-code">
from glob import glob
import os
import albumentations as A
import cv2
import matplotlib.pyplot as plt
import numpy as np
from ultralytics import YOLO</pre>
<ol>
<li value="2">Let’s<a id="_idIndexMarker800"/> implement a <strong class="source-inline">read_labels</strong> helper function, which will read the text file with the YOLO labels and return them as <span class="No-Break">a list:</span><pre class="source-code">
def read_labels(labels_path):</pre><pre class="source-code">
    res = []</pre><pre class="source-code">
    with open(labels_path, 'r') as file:</pre><pre class="source-code">
        lines = file.readlines()</pre><pre class="source-code">
        for line in lines:</pre><pre class="source-code">
            cls,xc,yc,w,h = line.strip().split(' ')</pre><pre class="source-code">
            res.append([int(float(cls)), float(xc),</pre><pre class="source-code">
                float(yc), float(w), float(h)])</pre><pre class="source-code">
        file.close()</pre><pre class="source-code">
    return res</pre></li>
<li>Now let’s implement a <strong class="source-inline">plot_labels</strong> helper function, which will reuse the previous <strong class="source-inline">read_labels</strong> function, read a few images and corresponding labels, and <a id="_idIndexMarker801"/>display these images with the <span class="No-Break">bounding boxes:</span><pre class="source-code">
def plot_labels(images_folder, labels_folder,</pre><pre class="source-code">
    classes):</pre><pre class="source-code">
        images_path = sorted(glob(</pre><pre class="source-code">
            images_folder + '/*.jpg'))</pre><pre class="source-code">
     labels_path = sorted(glob(</pre><pre class="source-code">
            labels_folder + '/*.txt'))</pre><pre class="source-code">
    plt.figure(figsize=(10, 6))</pre><pre class="source-code">
    for i in range(8):</pre><pre class="source-code">
        idx = np.random.randint(len(images_path))</pre><pre class="source-code">
        image = plt.imread(images_path[idx])</pre><pre class="source-code">
        labels = read_labels(labels_path[idx])</pre><pre class="source-code">
        for cls, xc, yc, w, h in labels:</pre><pre class="source-code">
            xc = int(xc*image.shape[1])</pre><pre class="source-code">
            yc = int(yc*image.shape[0])</pre><pre class="source-code">
            w = int(w*image.shape[1])</pre><pre class="source-code">
            h = int(h*image.shape[0])</pre><pre class="source-code">
            cv2.rectangle(image,</pre><pre class="source-code">
                (xc - w//2, yc - h//2),</pre><pre class="source-code">
                (xc + w//2 ,yc + h//2), (255,0,0), 2)</pre><pre class="source-code">
            cv2.putText(image, f'{classes[int(cls)]}',</pre><pre class="source-code">
                (xc-w//2, yc - h//2 - 10),</pre><pre class="source-code">
                cv2.FONT_HERSHEY_SIMPLEX, 0.5,</pre><pre class="source-code">
                (1.,0.,0.), 1)</pre><pre class="source-code">
        plt.subplot(2, 4, i + 1)</pre><pre class="source-code">
        plt.imshow(image)</pre><pre class="source-code">
        plt.axis('off')</pre></li>
<li>Now, display a set of images from the train set and their bounding boxes with the <span class="No-Break">following code:</span><pre class="source-code">
plot_labels('QR-detection-yolo/train/images/',</pre><pre class="source-code">
    'QR-detection-yolo/train/labels/', 'QR Code')</pre></li>
</ol>
<p>Here are a few sample images in the form of <span class="No-Break">QR codes:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer321">
<img alt="Figure 11.8 – A few samples from the train set with the associated labels (this dataset is only made up of generated QR codes on a white background)" height="745" src="image/B19629_11_08.jpg" width="1378"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.8 – A few samples from the train set with the associated labels (this dataset is only made up of generated QR codes on a white background)</p>
<p>As <a id="_idIndexMarker802"/>explained, the train set is only made up of generated QR codes of various sizes on a white background with no <span class="No-Break">more context.</span></p>
<ol>
<li value="5">Let’s now display a few images from the test set with the <span class="No-Break">following code:</span><pre class="source-code">
plot_labels('QR-detection-yolo/test/images/',</pre><pre class="source-code">
    'QR-detection-yolo/test/labels/', 'QR Code')</pre></li>
</ol>
<p>Here are the <span class="No-Break">resulting images:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer322">
<img alt="Figure 11.9 – A few examples from the test set, made of real-world images of QR codes" height="814" src="image/B19629_11_09.jpg" width="1495"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.9 – A few examples from the test set, made of real-world images of QR codes</p>
<p>The test set <a id="_idIndexMarker803"/>contains more complex, real examples of QR codes, and is much <span class="No-Break">more challenging.</span></p>
<h3>Generating a synthetic dataset from background images</h3>
<p>In this part, we<a id="_idIndexMarker804"/> will now generate a<a id="_idIndexMarker805"/> dataset of realistic, synthetic data. To do so, we will use the images of the QR codes from the training set as well as a set of background images. Here are <span class="No-Break">the steps:</span></p>
<ol>
<li>Let’s now generate a synthetic dataset using <span class="No-Break">two ingredients:</span><ul><li>Real <span class="No-Break">background images</span></li><li>Numerically generated <span class="No-Break">QR codes</span></li></ul></li>
</ol>
<p>For that, we will use a rather long and complex function, <strong class="source-inline">generate_synthetic_background_image_with_tag</strong>, which does <span class="No-Break">the following:</span></p>
<ul>
<li>Picks a random background image in the <span class="No-Break">given folder</span></li>
<li>Picks a random QR code image in the <span class="No-Break">given folder</span></li>
<li>Augments the picked <span class="No-Break">QR code</span></li>
<li>Randomly inserts the augmented QR code into the <span class="No-Break">background image</span></li>
<li>Applies a little more augmentation to the newly <span class="No-Break">created image</span></li>
<li>Stores the generated image and the corresponding labels in <span class="No-Break">YOLO format</span></li>
</ul>
<p>The code that does this is available in the GitHub repository and is too long to be displayed here, so we will only display its signature and docstring here. However, you are strongly encouraged to have a look at it and to play with it. The code can <a id="_idIndexMarker806"/>be <a id="_idIndexMarker807"/><span class="No-Break">found here:</span></p>
<p><a href=""><span class="No-Break">https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_11/chapter_11.ipynb</span></a></p>
<pre class="source-code">
def generate_synthetic_background_image_with_tag(
    n_images_to_generate: int,
    output_path: str,
    raw_tags_folder: str,
    background_images_path: str,
    labels_path: str,
    background_proba: float = 0.8,
):
    """Generate images with random tag and synthetic background.
    Parameters
    ----------
    n_images_to_generate : int
        The number of images to generate.
    output_path : str
        The output directory path where to store the generated images.
        If the path does not exist, the directory is created.
    raw_tags_folder : str
        Path to the folder containing the raw QR codes.
    background_images_path : str
        Path to the folder containing the background images.
    labels_path : str
        Path to the folder containing the labels.
        Files must be in the same order as the ones in the raw_tags_folder.
    background_proba : float (optional, default=0.8)
        Probability to use a background image when generating a new sample.
    """</pre>
<p class="callout-heading">Note</p>
<p class="callout">This function does this generation as many times as we want and provides a few other features; feel free to have a close look at it and <span class="No-Break">update it.</span></p>
<ol>
<li value="2">We can now use<a id="_idIndexMarker808"/> this<a id="_idIndexMarker809"/> function to generate 3,000 images by calling the <strong class="source-inline">generate_synthetic_background_image_with_tag</strong> function (3,000 is a rather arbitrary choice; feel free to generate fewer images or more images). This may take a few minutes. The generated images and their associated labels will be stored in the <strong class="source-inline">QR-detection-yolo/generated_qr_code_images/</strong> folder, which will be created if it does <span class="No-Break">not exist:</span><pre class="source-code">
generate_synthetic_background_image_with_tag(</pre><pre class="source-code">
    n_images_to_generate=3000,</pre><pre class="source-code">
    output_path='QR-detection-yolo/generated_qr_code_images/',</pre><pre class="source-code">
    raw_tags_folder='QR-detection-yolo/train/images/',</pre><pre class="source-code">
    background_images_path='QR-detection-yolo/background_images/',</pre><pre class="source-code">
    labels_path='QR-detection-yolo/train/labels/'</pre><pre class="source-code">
)</pre></li>
</ol>
<p>Let’s have a look at<a id="_idIndexMarker810"/> a<a id="_idIndexMarker811"/> few examples of generated images with the <span class="No-Break">following code:</span></p>
<pre class="source-code">
plot_labels(
    'QR-detection-yolo/generated_qr_code_images/images/',
    'QR-detection-yolo/generated_qr_code_images/labels/',
    'QR Code'
)</pre>
<p>Here are <span class="No-Break">the images:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer323">
<img alt="Figure 11.10 – Examples of synthetically created images, made of background images and generated QR codes with various image augmentations" height="1178" src="image/B19629_11_10.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.10 – Examples of synthetically created images, made of background images and generated QR codes with various image augmentations</p>
<p>As we can see, some images are simple augmented QR codes with no background context, as is<a id="_idIndexMarker812"/> possible <a id="_idIndexMarker813"/>due to the generating function. This can be tweaked with the <span class="No-Break"><strong class="source-inline">background_proba</strong></span><span class="No-Break"> argument.</span></p>
<h3>Model training</h3>
<p>We can now start <a id="_idIndexMarker814"/>the model training part: we will train a YOLO model on the 3,000 images generated in the previous step and evaluate this model against the test set. Here are <span class="No-Break">the steps:</span></p>
<ol>
<li>First, instantiate a YOLO model with pre-trained weights <span class="No-Break">as follows:</span><pre class="source-code">
# Create a new YOLO model with pretrained weights</pre><pre class="source-code">
model = YOLO('yolov8n.pt')</pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">You may have <strong class="source-inline">FileNotFoundError</strong> because of an incorrect dataset path. A <strong class="source-inline">config</strong> file in <strong class="source-inline">~/.config/Ultralytics/settings.yaml</strong> has a previous path. A quick and harmless fix is to simply delete this file; a new one will then be <span class="No-Break">automatically generated.</span></p>
<ol>
<li value="2">Then, we need to create a <strong class="source-inline">.yaml</strong> file, <strong class="source-inline">data_qr_generated.yaml</strong>, with the <span class="No-Break">following content:</span><pre class="source-code">
train: ../../QR-detection-yolo/generated_qr_code_images/images</pre><pre class="source-code">
val: ../../QR-detection-yolo/test/images</pre><pre class="source-code">
nc: 1</pre><pre class="source-code">
names: ['QR_CODE']</pre></li>
<li>This <strong class="source-inline">.yaml</strong> file can <a id="_idIndexMarker815"/>be used to train the model on our dataset, on <strong class="source-inline">50 epochs</strong>. We also specify the initial learning rate to be 0.001 with <strong class="source-inline">lr0=0.001</strong> because the default learning rate (0.01) is rather large for fine-tuning a pre-trained model in <span class="No-Break">our case:</span><pre class="source-code">
# Train the model for 50 epochs</pre><pre class="source-code">
model.train(data='data_qr_generated.yaml', epochs=50,</pre><pre class="source-code">
    lr0=0.001, name='generated_qrcode')</pre></li>
</ol>
<p>Results should be stored in the created <span class="No-Break">folder, </span><span class="No-Break"><strong class="source-inline">runs/detect/generated_qrcode</strong></span><span class="No-Break">.</span></p>
<ol>
<li value="4">Before having a look at the results, let’s implement a <strong class="source-inline">plot_results_one_image</strong> helper function to display the output of the model, <span class="No-Break">as follows:</span><pre class="source-code">
def plot_results_random_images(test_images, model, classes=['QR_code']):</pre><pre class="source-code">
    images = glob(test_images + '/*.jpg')</pre><pre class="source-code">
    plt.figure(figsize=(14, 10))</pre><pre class="source-code">
    for i in range(8):</pre><pre class="source-code">
        idx = np.random.randint(len(images))</pre><pre class="source-code">
        result = model.predict(images[idx])</pre><pre class="source-code">
        image = result[0].orig_img.copy()</pre><pre class="source-code">
        raw_res = result[0].boxes.data</pre><pre class="source-code">
        for detection in raw_res:</pre><pre class="source-code">
            x1, y1, x2, y2, p,</pre><pre class="source-code">
                cls = detection.cpu().tolist()</pre><pre class="source-code">
            cv2.rectangle(image, (int(x1), int(y1)),</pre><pre class="source-code">
                (int(x2), int(y2)), (255,0,0), 2)</pre><pre class="source-code">
            cv2.putText(image, f'{classes[int(cls)]}',</pre><pre class="source-code">
                (int(x1), int(y1) - 10),</pre><pre class="source-code">
                    cv2.FONT_HERSHEY_SIMPLEX, 1,</pre><pre class="source-code">
                    (255,0,0), 2)</pre><pre class="source-code">
        plt.subplot(2, 4, i + 1)</pre><pre class="source-code">
        plt.axis('off')</pre><pre class="source-code">
        plt.imshow(image)</pre></li>
<li>We can then <a id="_idIndexMarker816"/>reload the best weights and compute the inference and display the results on an image from the <span class="No-Break">test set:</span><pre class="source-code">
# Load the best weights</pre><pre class="source-code">
model = YOLO(</pre><pre class="source-code">
    'runs/detect/generated_qrcode/weights/best.pt')</pre><pre class="source-code">
# Plot the results</pre><pre class="source-code">
Plot_results_random_images(</pre><pre class="source-code">
    'QR-detection-yolo/test/images/', model)</pre></li>
</ol>
<p>Here are <span class="No-Break">the results:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer324">
<img alt="Figure 11.11 – Examples of results of the YOLO model trained on synthetic data (even though the model is not perfect, is it capable of detecting QR codes in rather complex and various situations)" height="666" src="image/B19629_11_11.jpg" width="1095"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.11 – Examples of results of the YOLO model trained on synthetic data (even though the model is not perfect, is it capable of detecting QR codes in rather complex and various situations)</p>
<p>As we can see, the model is not working perfectly yet but still manages to get QR codes in<a id="_idIndexMarker817"/> several complex situations. However, in a few cases, such as with really small QR codes, bad-quality images, or highly deformed QR codes, the model does not seem to <span class="No-Break">perform well.</span></p>
<ol>
<li value="6">Finally, we can visualize the losses and other metrics generated by the <span class="No-Break">YOLO library:</span><pre class="source-code">
plt.figure(figsize=(10, 8))</pre><pre class="source-code">
plt.imshow(plt.imread(</pre><pre class="source-code">
    'runs/detect/generated_qrcode/results.png'))</pre></li>
</ol>
<p>Here are <span class="No-Break">the losses:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer325">
<img alt="Figure 11.12 – Metrics computed by the YOLO library" height="822" src="image/B19629_11_12.jpg" width="1645"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.12 – Metrics computed by the YOLO library</p>
<p>In agreement with the displayed results for a few images, the metrics are not perfect, with a mAP50<a id="_idIndexMarker818"/> around <span class="No-Break">75% only.</span></p>
<p>This could probably be improved by adding more well-chosen <span class="No-Break">image augmentation.</span></p>
<h2 id="_idParaDest-295"><a id="_idTextAnchor295"/>There’s more…</h2>
<p>There are more<a id="_idIndexMarker819"/> techniques for generating images with labels even if we don’t have any real data in the first place. In this recipe, we only used background images, generated QR codes, and augmentations, but it is possible to use generative models to generate even <span class="No-Break">more data.</span></p>
<p>Let’s see how to do this with DALL-E, a model proposed <span class="No-Break">by OpenAI:</span></p>
<ol>
<li>First, we can import the required libraries. The <strong class="source-inline">openai</strong> library can be installed with <strong class="source-inline">pip </strong><span class="No-Break"><strong class="source-inline">install openai</strong></span><span class="No-Break">:</span><pre class="source-code">
import openai</pre><pre class="source-code">
import urllib</pre><pre class="source-code">
from PIL import Image</pre><pre class="source-code">
import matplotlib.pyplot as plt</pre><pre class="source-code">
openai.api_key = 'xx-xxx'</pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">You need to create your own API key by creating your own account <span class="No-Break">on </span><a href=""><span class="No-Break">openai.com</span></a><span class="No-Break">.</span></p>
<ol>
<li value="2">Let’s now create a helper function that converts the bounding boxes and the image into <a id="_idIndexMarker820"/>a mask since we want to complete outside of the <span class="No-Break">bounding box:</span><pre class="source-code">
def get_mask_to_complete(image_path, label_path, output_filename, margin: int = 100):</pre><pre class="source-code">
    image = plt.imread(image_path)</pre><pre class="source-code">
    labels = read_labels(label_path)</pre><pre class="source-code">
    output_mask = np.zeros(image.shape[:2])</pre><pre class="source-code">
    for cls, xc, yc, w, h in labels:</pre><pre class="source-code">
        xc = int(xc*image.shape[1])</pre><pre class="source-code">
        yc = int(yc*image.shape[0])</pre><pre class="source-code">
        w = int(w*image.shape[1])</pre><pre class="source-code">
        h = int(h*image.shape[0])</pre><pre class="source-code">
        output_mask[yc-h//2-margin:yc+h//2+margin,</pre><pre class="source-code">
            xc-w//2-margin:xc+w//2+margin] = 255</pre><pre class="source-code">
    output_mask = np.concatenate([image,</pre><pre class="source-code">
        np.expand_dims(output_mask, -1)],</pre><pre class="source-code">
            axis=-1).astype(np.uint8)</pre><pre class="source-code">
    # Save the images</pre><pre class="source-code">
    output_mask_filename = output_filename.split('.')[0] + '_mask.png'</pre><pre class="source-code">
    plt.imsave(output_filename, image)</pre><pre class="source-code">
    plt.imsave(output_mask_filename, output_mask)</pre><pre class="source-code">
    return output_mask_filename</pre></li>
<li>We can now compute a mask and display the result side by side with the original image <span class="No-Break">as follows:</span><pre class="source-code">
output_image_filename = 'image_edit.png'</pre><pre class="source-code">
mask_filename = get_mask_to_complete(</pre><pre class="source-code">
    'QR-detection-yolo/generated_qr_code_images/images/synthetic_image_0.jpg',</pre><pre class="source-code">
    'QR-detection-yolo/generated_qr_code_images/labels/synthetic_image_0.txt',</pre><pre class="source-code">
    output_image_filename</pre><pre class="source-code">
)</pre><pre class="source-code">
# Display the masked image and the original image side by side</pre><pre class="source-code">
plt.figure(figsize=(12, 10))</pre><pre class="source-code">
plt.subplot(1, 2, 1)</pre><pre class="source-code">
plt.imshow(plt.imread(output_image_filename))</pre><pre class="source-code">
plt.subplot(1, 2, 2)</pre><pre class="source-code">
plt.imshow(plt.imread(mask_filename))</pre></li>
</ol>
<p>Here<a id="_idIndexMarker821"/> are <span class="No-Break">the results:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer326">
<img alt="Figure 11.13 – An original image on the left, and the associated masked image to be used for data generation on the right" height="282" src="image/B19629_11_13.jpg" width="985"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.13 – An original image on the left, and the associated masked image to be used for data generation on the right</p>
<p class="callout-heading">Note</p>
<p class="callout">We keep a margin for the masked image so that when calling DALL-E 2, it has a sense of the surroundings. If we provide only a QR code and white surroundings in the mask, the result may not be <span class="No-Break">good enough.</span></p>
<ol>
<li value="4">We can now query the OpenAI model DALL-E 2 to fill around this QR code and generate a new image using the <strong class="source-inline">create_edit</strong> method from the <strong class="source-inline">openai</strong> library. The function requires the following <span class="No-Break">few parameters:</span><ul><li>The input image (in PNG format, less than <span class="No-Break">4 MB)</span></li><li>The input mask (in PNG format and less than 4 <span class="No-Break">MB too)</span></li><li>A prompt describing what the expected output <span class="No-Break">image is</span></li><li>The number of images <span class="No-Break">to generate</span></li><li>The output size in pixels (either 256x256, 512x512, <span class="No-Break">or 1,024x1,024)</span></li></ul></li>
</ol>
<p>Let’s now<a id="_idIndexMarker822"/> query DALL-E on our image, and then display the original and the generated images side <span class="No-Break">by side:</span></p>
<pre class="source-code">
# Query openAI API to generate image
response = openai.Image.create_edit(
    image=open(output_image_filename, 'rb'),
    mask=open(mask_filename, 'rb'),
    prompt="A store in background",
    n=1,
    size="512x512"
)
# Download and display the generated image
plt.figure(figsize=(12, 10))
image_url = response['data'][0]['url']
plt.subplot(1, 2, 1)
plt.imshow(plt.imread(output_image_filename))
plt.subplot(1, 2, 2)
plt.imshow(np.array(Image.open(urllib.request.urlopen(
    image_url))))</pre>
<p>Here is how the <span class="No-Break">images appear:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer327">
<img alt="Figure 11.14 – The original image on the left, and the generated image using DALL-E 2 on the right" height="458" src="image/B19629_11_14.jpg" width="985"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.14 – The original image on the left, and the generated image using DALL-E 2 on the right</p>
<p>As we can see in <span class="No-Break"><em class="italic">Figure 11</em></span><em class="italic">.14</em>, using this technique allows us to create more realistic <a id="_idIndexMarker823"/>images that can easily be used for training. These created images can also be augmented <span class="No-Break">using Albumentations.</span></p>
<p class="callout-heading">Note</p>
<p class="callout">There are a few drawbacks though. The generated image is of size 512x512, meaning the bounding box coordinates have to be converted (this can be done using Albumentations) and the generated image is not always good and requires a <span class="No-Break">visual check.</span></p>
<ol>
<li value="5">We can also create variations of a given image using the <strong class="source-inline">create_variation</strong> function. This function is simpler to use and requires similar <span class="No-Break">input arguments:</span><ul><li>The input image (still a PNG image smaller than <span class="No-Break">4 MB)</span></li><li>The number of varied images <span class="No-Break">to generate</span></li><li>The output image size in pixels (again, either 256x256, 512x512, <span class="No-Break">or 1,024x1,024)</span></li></ul></li>
</ol>
<p>Here is the<a id="_idIndexMarker824"/> code <span class="No-Break">for this:</span></p>
<pre class="source-code">
# Query to create variation of a given image
response = openai.Image.create_variation(
    image=open(output_image_filename, "rb"),
    n=1,
    size="512x512"
)
# Download and display the generated image
plt.figure(figsize=(12, 10))
image_url = response['data'][0]['url']
plt.subplot(1, 2, 1)
plt.imshow(plt.imread(output_image_filename))
plt.subplot(1, 2, 2)
plt.imshow(np.array(Image.open(urllib.request.urlopen(
    image_url))))</pre>
<p>Here is <span class="No-Break">the output:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer328">
<img alt="Figure 11.15 – The original image (left) and the generated variation using DALL-E (right)" height="463" src="image/B19629_11_15.jpg" width="985"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.15 – The original image (left) and the generated variation using DALL-E (right)</p>
<p>The result presented in the preceding figure is pretty good: we can see a meeting room in the background and a QR code in the foreground, just like in the original image. However, this data would not be easy to use unless we labeled it manually since we have no certainty the QR code will be at the same location (even if we resize the bounding box<a id="_idIndexMarker825"/> coordinates). Still, using such models can be of great help for other use cases, such <span class="No-Break">as classification.</span></p>
<h2 id="_idParaDest-296"><a id="_idTextAnchor296"/>See also</h2>
<ul>
<li>The list of <a id="_idIndexMarker826"/>train parameters available with <span class="No-Break">YOLOv8: </span><a href=""><span class="No-Break">https://docs.ultralytics.com/modes/train/</span></a></li>
<li>The DALL-E API <a id="_idIndexMarker827"/><span class="No-Break">documentation: </span><a href=""><span class="No-Break">https://platform.openai.com/docs/guides/images/usage</span></a></li>
</ul>
<h1 id="_idParaDest-297"><a id="_idTextAnchor297"/>Implementing real-time style transfer</h1>
<p>In this recipe, we<a id="_idIndexMarker828"/> will build our own lightweight style transfer model based on the U-Net architecture. To do so, we will use a dataset generated using Stable Diffusion (see more next about what Stable Diffusion is). This can be seen as a kind of knowledge distillation: we will use the data generated by a large, teacher model (Stable Diffusion, which weighs several gigabytes) to train a small, student model (here, a U-Net++ of less than 30 MBs). This is a funny way to use generative models to create data, but the concepts developed here can be used in many other applications: some will be proposed in the <em class="italic">There’s more…</em> section, along with guidance on creating your own style transfer dataset using Stable Diffusion. But before that, let’s give some context about <span class="No-Break">style transfer.</span></p>
<p>Style transfer is a famous and fun use of deep learning, allowing us to change the style of a given image into another style. Many examples exist, such as Mona Lisa in Van Gogh’s Starry<a id="_idIndexMarker829"/> Night style, as represented in the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer329">
<img alt="Figure 11.16 – Mona Lisa in the Starry Night style" height="720" src="image/B19629_11_16.jpg" width="483"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.16 – Mona Lisa in the Starry Night style</p>
<p>Until recently, style transfer was mostly performed using <strong class="bold">Generative Adversarial Networks</strong> (<strong class="bold">GANs</strong>), which <a id="_idIndexMarker830"/>are quite hard to <span class="No-Break">train properly.</span></p>
<p>It is now simpler than ever to apply style transfer to images, using pre-trained models based on Stable Diffusion. Unfortunately, Stable Diffusion is a large and complex model that sometimes requires several seconds to generate a single image on a recent <span class="No-Break">graphics card.</span></p>
<p>In this recipe, we will train a U-Net-like model allowing for real-time transfer learning on any device. To accomplish this, we will employ a form of knowledge distillation. Specifically, we will train the U-net model using Stable Diffusion data and incorporate a VGG perceptual loss for <span class="No-Break">that purpose.</span></p>
<p class="callout-heading">Note</p>
<p class="callout"><strong class="bold">VGG</strong> stands <a id="_idIndexMarker831"/>for <strong class="bold">Visual Geometry Group</strong>, the name of the Oxford team who proposed this deep learning model architecture. It is a standard deep learning model in <span class="No-Break">Computer Vision.</span></p>
<p>Before moving on to the <a id="_idIndexMarker832"/>recipe, let’s have a look at two important concepts for <span class="No-Break">this recipe:</span></p>
<ul>
<li><span class="No-Break">Stable Diffusion</span></li>
<li><span class="No-Break">Perceptual loss</span></li>
</ul>
<h2 id="_idParaDest-298"><a id="_idTextAnchor298"/>Stable Diffusion</h2>
<p>Stable<a id="_idIndexMarker833"/> Diffusion<a id="_idIndexMarker834"/> is a complex and powerful model, allowing us to use image and text prompts to generate <span class="No-Break">new images.</span></p>
<div>
<div class="IMG---Figure" id="_idContainer330">
<img alt="Figure 11.17 – Architecture diagram of Stable Diffusion" height="851" src="image/B19629_11_17.jpg" width="1650"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.17 – Architecture diagram of Stable Diffusion</p>
<p>As we can see from the preceding figure, the way Stable Diffusion is trained can be summarized, with simplifications, <span class="No-Break">as follows:</span></p>
<ul>
<li>Diffusion is gradually applied <strong class="bold">T</strong> times to an input image <strong class="bold">Z</strong>: this is like adding random noise to the <span class="No-Break">input image</span></li>
<li>These diffused images are passed through a denoising <span class="No-Break">U-Net model</span></li>
<li>A condition is optionally added, such as a descriptive text or another image prompt, as <span class="No-Break">an embedding</span></li>
<li>The model is trained to output the <span class="No-Break">input image</span></li>
</ul>
<p>Once a model is well trained, it can be used for inference by skipping the first part <span class="No-Break">as follows:</span></p>
<ul>
<li>Given a seed, a random image is generated and given as input to the <span class="No-Break">denoising U-Net</span></li>
<li>An input prompt is added as condition: it can be text or an input image, <span class="No-Break">for example</span></li>
<li>An output image is then generated: this is the <span class="No-Break">final result</span></li>
</ul>
<p>Although this is a simplistic explanation of how it works, it allows us to get a general understanding <a id="_idIndexMarker835"/>of what it does and what are the<a id="_idIndexMarker836"/> expected inputs to generate a <span class="No-Break">new image.</span></p>
<h2 id="_idParaDest-299"><a id="_idTextAnchor299"/>Perceptual loss</h2>
<p>Perceptual loss has<a id="_idIndexMarker837"/> been proposed to train a model to<a id="_idIndexMarker838"/> learn about perceptual features in an image. It was developed specifically for style transfer and allows you to focus not only on the pixel-to-pixel content itself but also on the style of <span class="No-Break">the image.</span></p>
<p>It takes two images as input: the model prediction and the label image, and it is commonly based on a VGG neural network pre-trained on the ImageNet dataset or any similar generic <span class="No-Break">image dataset.</span></p>
<p>More specifically, for both images (for example, the model prediction and the label), the following computations <span class="No-Break">are made:</span></p>
<ul>
<li>The feedforward computation of the VGG model is applied to <span class="No-Break">each image</span></li>
<li>The outputs after each block of the VGG model are stored, allowing us to get more and more specific features with <span class="No-Break">deeper blocks</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">In a deep neural network, the first layers are commonly about learning generic features and shapes, while the deepest layers are about learning more specific features. The perceptual loss takes advantage of <span class="No-Break">this property.</span></p>
<p>Using these stored computations, perceptual loss is finally computed as the sum of the <span class="No-Break">following values:</span></p>
<ul>
<li>The differences (for example, L1 or L2 norm) between the computations for each block output: These can be represented as the feature reconstruction loss and will focus on <span class="No-Break">image features.</span></li>
<li>The differences between the Gram matrices of the computations for each block output: These can be represented as the style reconstruction loss and will focus on the <span class="No-Break">image’s style.</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">A Gram matrix of a given set of vectors is made by computing the dot product of each pair of vectors and then arranging the results into a matrix. It can be seen as a similarity or a correlation between the <span class="No-Break">given vectors.</span></p>
<p>At the end, minimizing<a id="_idIndexMarker839"/> this perceptual loss should <a id="_idIndexMarker840"/>allow us to apply a style from a given image to another, as we will see in <span class="No-Break">this recipe.</span></p>
<h2 id="_idParaDest-300"><a id="_idTextAnchor300"/>Getting started</h2>
<p>You can download the<a id="_idIndexMarker841"/> full dataset that I created for this recipe on Kaggle with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
kaggle datasets download -d vincentv/qr-detection-yolo --unzip</pre>
<p>You then have the following <span class="No-Break">folder architecture:</span></p>
<pre class="source-code">
anime-style-transfer
├── train
│   ├── images: 820 images
│   └── labels: 820 images
└── test
     ├── images: 93 images
     └── labels: 93 images</pre>
<p>This is a rather small dataset, but it should allow us to get good enough performances to show the potential of <span class="No-Break">this technique.</span></p>
<p>For more about how to create such a dataset using ControlNet yourself, have a look at the <em class="italic">There’s </em><span class="No-Break"><em class="italic">more…</em></span><span class="No-Break"> subsection.</span></p>
<p>The libraries needed for this recipe can be installed with the following <span class="No-Break">command line:</span></p>
<pre class="source-code">
pip install matplotlib numpy torch torchvision segmentation-models-pytorch albumentations tqdm</pre>
<h2 id="_idParaDest-301"><a id="_idTextAnchor301"/>How to do it…</h2>
<p>Here are the<a id="_idIndexMarker842"/> steps for <span class="No-Break">this recipe:</span></p>
<ol>
<li>Import the <span class="No-Break">required libraries:</span><ul><li><strong class="source-inline">matplotlib</strong> <span class="No-Break">for visualization</span></li><li><strong class="source-inline">numpy</strong> <span class="No-Break">for manipulation</span></li><li>Several <strong class="source-inline">torch</strong> and <span class="No-Break"><strong class="source-inline">torchvision</strong></span><span class="No-Break"> modules</span></li><li><strong class="source-inline">segmentation models pytorch</strong> for <span class="No-Break">the model</span></li><li><strong class="source-inline">albumentations</strong> for <span class="No-Break">image augmentation</span></li></ul></li>
</ol>
<p>Here is the code <span class="No-Break">for it:</span></p>
<pre class="source-code">
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.utils.data import DataLoader, Dataset
import torchvision.transforms as transforms
from torchvision.models import vgg16, VGG16_Weights
from glob import glob
import segmentation_models_pytorch as smp
from torch.optim.lr_scheduler import ExponentialLR
import albumentations as A
import tqdm</pre>
<ol>
<li value="2">Implement <strong class="source-inline">AnimeStyleDataset</strong>, allowing us to load the dataset. Note that we use the <strong class="source-inline">ReplayCompose</strong> tool from <strong class="source-inline">Albumentations</strong>, allowing us to apply the exact same<a id="_idIndexMarker843"/> image augmentation to the image and the <span class="No-Break">associated label:</span><pre class="source-code">
class AnimeStyleDataset(Dataset):</pre><pre class="source-code">
    def __init__(self, input_path: str,</pre><pre class="source-code">
        output_path: str, transform, augment = None):</pre><pre class="source-code">
            self.input_paths = sorted(glob(</pre><pre class="source-code">
                f'{input_path}/*.png'))</pre><pre class="source-code">
            self.output_paths = sorted(glob(</pre><pre class="source-code">
                f'{output_path}/*.png'))</pre><pre class="source-code">
            self.transform = transform</pre><pre class="source-code">
            self.augment = augment</pre><pre class="source-code">
    def __len__(self):</pre><pre class="source-code">
        return len(self.input_paths)</pre><pre class="source-code">
    def __getitem__(self, idx):</pre><pre class="source-code">
        input_img = plt.imread(self.input_paths[idx])</pre><pre class="source-code">
        output_img = plt.imread(</pre><pre class="source-code">
            self.output_paths[idx])</pre><pre class="source-code">
        if self.augment:</pre><pre class="source-code">
            augmented = self.augment(image=input_img)</pre><pre class="source-code">
            input_img = augmented['image']</pre><pre class="source-code">
            output_img = A.ReplayCompose.replay(</pre><pre class="source-code">
                augmented['replay'],</pre><pre class="source-code">
                image=output_img)['image']</pre><pre class="source-code">
        return self.transform(input_img),</pre><pre class="source-code">
            self.transform(output_img)</pre></li>
<li>Instantiate the <a id="_idIndexMarker844"/>augmentation, which is a composition of the <span class="No-Break">following transformations:</span><ul><li><span class="No-Break"><strong class="source-inline">Resize</strong></span></li><li>A horizontal flip with a probability <span class="No-Break">of 50%</span></li><li><strong class="source-inline">ShiftScaleRotate</strong>, allowing us to randomly add <span class="No-Break">geometrical variety</span></li><li><strong class="source-inline">RandomBrightnessContrast</strong>, allowing us to add variety to <span class="No-Break">the light</span></li><li><strong class="source-inline">RandomCropFromBorders</strong>, which will randomly crop the borders of <span class="No-Break">the images</span></li></ul></li>
</ol>
<p>Here is the code <span class="No-Break">for it:</span></p>
<pre class="source-code">
augment = A.ReplayCompose([
    A.Resize(512, 512),
    A.HorizontalFlip(p=0.5),
    A.ShiftScaleRotate(shift_limit=0.05,
        scale_limit=0.05, rotate_limit=15, p=0.5),
    A.RandomBrightnessContrast(p=0.5),
    A.RandomCropFromBorders(0.2, 0.2, 0.2, 0.2, p=0.5)
])</pre>
<ol>
<li value="4">Instantiate the transformation, allowing us to convert the torch tensors and rescale the pixel values. Also, define the batch size and the device <span class="No-Break">as follows:</span><pre class="source-code">
batch_size = 12</pre><pre class="source-code">
device = torch.device(</pre><pre class="source-code">
    'cuda' if torch.cuda.is_available() else 'cpu')</pre><pre class="source-code">
mean = (0.485, 0.456, 0.406)</pre><pre class="source-code">
std = (0.229, 0.224, 0.225)</pre><pre class="source-code">
transform = transforms.Compose([</pre><pre class="source-code">
    transforms.ToTensor(),</pre><pre class="source-code">
    transforms.Resize((512, 512), antialias=True),</pre><pre class="source-code">
    transforms.Normalize(mean, std),</pre><pre class="source-code">
])</pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">Here, we use the mean and standard deviation rescaling specific to the ImageNet dataset because the VGG perceptual loss (see ahead) is trained on this specific set of values. Also, the batch size may need to be adjusted depending on your hardware specifications, especially the memory of your graphics <span class="No-Break">processing unit.</span></p>
<ol>
<li value="5">Instantiate the <a id="_idIndexMarker845"/>datasets and data loaders, providing the train and test folders. Note that we apply augmentation to the train <span class="No-Break">set only:</span><pre class="source-code">
trainset = AnimeStyleDataset(</pre><pre class="source-code">
    'anime-style-transfer/train/images/',</pre><pre class="source-code">
    'anime-style-transfer/train/labels/',</pre><pre class="source-code">
    transform=transform,</pre><pre class="source-code">
    augment=augment,</pre><pre class="source-code">
)</pre><pre class="source-code">
train_dataloader = DataLoader(trainset,</pre><pre class="source-code">
<strong class="source-inline">    </strong>batch_size=batch_size, shuffle=True)</pre><pre class="source-code">
testset = AnimeStyleDataset(</pre><pre class="source-code">
    'anime-style-transfer/test/images/',</pre><pre class="source-code">
    'anime-style-transfer/test/labels/',</pre><pre class="source-code">
    transform=transform,</pre><pre class="source-code">
)</pre><pre class="source-code">
test_dataloader = DataLoader(testset,</pre><pre class="source-code">
    batch_size=batch_size, shuffle=True)</pre></li>
<li>Display a few<a id="_idIndexMarker846"/> images along with their labels so that we have a glimpse at the dataset. For that, we first need a helper <strong class="source-inline">unnormalize</strong> function to rescale the images’ values to the range [<span class="No-Break">0, 1]:</span><pre class="source-code">
def unnormalize(x, mean, std):</pre><pre class="source-code">
    x = np.asarray(x, dtype=np.float32)</pre><pre class="source-code">
    for dim in range(3):</pre><pre class="source-code">
        x[:, :, dim] = (x[:, :, dim] * std[dim]) + mean[dim]</pre><pre class="source-code">
    return x</pre><pre class="source-code">
plt.figure(figsize=(12, 6))</pre><pre class="source-code">
images, labels = next(iter(train_dataloader))</pre><pre class="source-code">
for idx in range(4):</pre><pre class="source-code">
    plt.subplot(2, 4, idx*2+1)</pre><pre class="source-code">
    plt.imshow(unnormalize(images[idx].permute(</pre><pre class="source-code">
        1, 2, 0).numpy(), mean, std))</pre><pre class="source-code">
    plt.axis('off')</pre><pre class="source-code">
    plt.subplot(2, 4, idx*2+2)</pre><pre class="source-code">
    plt.imshow(unnormalize(labels[idx].permute(</pre><pre class="source-code">
        1, 2, 0).numpy(), mean, std))</pre><pre class="source-code">
    plt.axis('off')</pre></li>
</ol>
<p>Here are<a id="_idIndexMarker847"/> <span class="No-Break">the results:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer331">
<img alt="Figure 11.18 – A set of four images with their associated anime labels" height="279" src="image/B19629_11_18.jpg" width="560"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.18 – A set of four images with their associated anime labels</p>
<p>As we can see in the preceding figure, the dataset is made up of images of faces, and the labels are the equivalent pictures with some drawing and anime style applied. These images were generated using Stable Diffusion and ControlNet; see how to do that yourself in the <em class="italic">There’s </em><span class="No-Break"><em class="italic">more…</em></span><span class="No-Break"> section.</span></p>
<ol>
<li value="7">Now we instantiate the model class. Here, we reuse the existing <strong class="source-inline">mobilenetv3_large_100</strong> implementation provided in the SMP library with U-Net++ architecture. We specify the input and output channels to be <strong class="source-inline">3</strong>, using the <strong class="source-inline">in_channels</strong> and <strong class="source-inline">n_classes</strong> parameters respectively. We also reuse <strong class="source-inline">imagenet</strong> weights for the encoder. Here is <span class="No-Break">the code:</span><pre class="source-code">
model = smp.UnetPlusPlus(</pre><pre class="source-code">
    encoder_name='timm-mobilenetv3_large_100',</pre><pre class="source-code">
    encoder_weights='imagenet',</pre><pre class="source-code">
    in_channels=3,</pre><pre class="source-code">
    classes=3,</pre><pre class="source-code">
    )</pre><pre class="source-code">
model = model.to(device)</pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">For more about the SMP library, refer to the<em class="italic"> Semantic segmentation using transfer learning</em> recipe of <a href="B19629_10.xhtml#_idTextAnchor255"><span class="No-Break"><em class="italic">Chapter 10</em></span></a><span class="No-Break">.</span></p>
<ol>
<li value="8">Now, we<a id="_idIndexMarker848"/> implement the VGG perceptual loss <span class="No-Break">as follows:</span><pre class="source-code">
class VGGPerceptualLoss(torch.nn.Module):</pre><pre class="source-code">
    def __init__(self):</pre><pre class="source-code">
        super(VGGPerceptualLoss, self).__init__()</pre><pre class="source-code">
        blocks = []</pre><pre class="source-code">
        blocks.append(vgg16(weights=VGG16_Weights.DEFAULT).features[:4].eval())</pre><pre class="source-code">
        blocks.append(vgg16(weights=VGG16_Weights.DEFAULT).features[4:9].eval())</pre><pre class="source-code">
        blocks.append(vgg16(weights=VGG16_Weights.DEFAULT).features[9:16].eval())</pre><pre class="source-code">
        blocks.append(vgg16(weights=VGG16_Weights.DEFAULT).features[16:23].eval())</pre><pre class="source-code">
        for block in blocks:</pre><pre class="source-code">
            block = block.to(device)</pre><pre class="source-code">
            for param in block.parameters():</pre><pre class="source-code">
                param.requires_grad = False</pre><pre class="source-code">
            self.blocks = torch.nn.ModuleList(blocks)</pre><pre class="source-code">
            self.transform = torch.nn.functional.interpolate</pre><pre class="source-code">
    def forward(self, input, target):</pre><pre class="source-code">
        input = self.transform(input, mode='bilinear',</pre><pre class="source-code">
            size=(224, 224), align_corners=False)</pre><pre class="source-code">
            target = self.transform(target,</pre><pre class="source-code">
                mode='bilinear', size=(224, 224),</pre><pre class="source-code">
                align_corners=False)</pre><pre class="source-code">
            loss = 0.0</pre><pre class="source-code">
            x = input</pre><pre class="source-code">
            y = target</pre><pre class="source-code">
            for i, block in enumerate(self.blocks):</pre><pre class="source-code">
                x = block(x)</pre><pre class="source-code">
                y = block(y)</pre><pre class="source-code">
                loss += torch.nn.functional.l1_loss(</pre><pre class="source-code">
                    x, y)</pre><pre class="source-code">
                act_x = x.reshape(x.shape[0],</pre><pre class="source-code">
                    x.shape[1], -1)</pre><pre class="source-code">
                act_y = y.reshape(y.shape[0],</pre><pre class="source-code">
                    y.shape[1], -1)</pre><pre class="source-code">
                gram_x = act_x @ act_x.permute(</pre><pre class="source-code">
                    0, 2, 1)</pre><pre class="source-code">
                gram_y = act_y @ act_y.permute(</pre><pre class="source-code">
                    0, 2, 1)</pre><pre class="source-code">
                loss += torch.nn.functional.l1_loss(</pre><pre class="source-code">
                    gram_x, gram_y)</pre><pre class="source-code">
        return loss</pre></li>
</ol>
<p>In this implementation, we have <span class="No-Break">two methods:</span></p>
<ul>
<li>The <strong class="source-inline">init</strong> function, defining all the blocks and setting them <span class="No-Break">as non-trainable</span></li>
<li>The <strong class="source-inline">forward</strong> function, resizing the image to 224x224 (the original VGG input shape) and computing the loss for <span class="No-Break">each block</span></li>
</ul>
<ol>
<li value="9">Next, we define<a id="_idIndexMarker849"/> the optimizer, an exponential learning rate scheduler, and the VGG loss, as well as the weights of the style and <span class="No-Break">content loss:</span><pre class="source-code">
optimizer = torch.optim.Adam(model.parameters(),</pre><pre class="source-code">
    lr=0.001)</pre><pre class="source-code">
scheduler = ExponentialLR(optimizer, gamma=0.995)</pre><pre class="source-code">
vgg_loss = VGGPerceptualLoss()</pre><pre class="source-code">
content_loss_weight=1.</pre><pre class="source-code">
style_loss_weight=5e-4</pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">The style loss (that is, the VGG perceptual loss) is by default much larger than the content loss (that is, the L1 loss). So, here we counterbalance that by applying a low weight to the <span class="No-Break">style loss.</span></p>
<ol>
<li value="10">Train the model over 50 epochs and store the losses for the train and test sets. To do so, let’s use the <strong class="source-inline">train_style_transfer</strong> function available in the GitHub <a id="_idIndexMarker850"/><span class="No-Break">repository (</span><a href="https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_11/chapter_11.ipynb"><span class="No-Break">https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_11/chapter_11.ipynb</span></a><span class="No-Break">):</span><pre class="source-code">
train_losses, test_losses = train_style_transfer(</pre><pre class="source-code">
    model,</pre><pre class="source-code">
    train_dataloader,</pre><pre class="source-code">
    test_dataloader,</pre><pre class="source-code">
    vgg_loss,</pre><pre class="source-code">
    content_loss_weight,</pre><pre class="source-code">
    style_loss_weight,</pre><pre class="source-code">
    device,</pre><pre class="source-code">
    epochs=50,</pre><pre class="source-code">
)</pre></li>
</ol>
<p class="callout-heading">Note</p>
<p class="callout">This function is a typical training loop as we implemented many times already. The only difference is the loss computation, which is computed <span class="No-Break">as follows:</span></p>
<p class="callout"><strong class="source-inline">style_loss = </strong><span class="No-Break"><strong class="source-inline">vgg_loss(outputs, labels)</strong></span></p>
<p class="callout"><strong class="source-inline">content_loss = </strong><span class="No-Break"><strong class="source-inline">torch.nn.functional.l1_loss(outputs, labels)</strong></span></p>
<p class="callout"><strong class="source-inline">loss = style_loss_weight*style_loss + </strong><span class="No-Break"><strong class="source-inline">content_loss_weight*content_loss</strong></span></p>
<ol>
<li value="11">Plot the loss as a function of the epoch for the train and <span class="No-Break">test sets:</span><pre class="source-code">
plt.plot(train_losses, label='train')</pre><pre class="source-code">
plt.plot(test_losses, label='test')</pre><pre class="source-code">
plt.ylabel('Loss')</pre><pre class="source-code">
plt.xlabel('Epoch')</pre><pre class="source-code">
plt.legend()</pre><pre class="source-code">
plt.show()</pre></li>
</ol>
<p>Here are <span class="No-Break">the results:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer332">
<img alt="Figure 11.19 – Train and test losses as a function of the epoch for the style transfer network" height="413" src="image/B19629_11_19.jpg" width="529"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.19 – Train and test losses as a function of the epoch for the style transfer network</p>
<p>As we can see in <a id="_idIndexMarker851"/>the preceding figure, the model is learning but tends to overfit slightly as the test loss does not decrease significantly anymore after about <span class="No-Break">40 epochs.</span></p>
<ol>
<li value="12">Finally, test the trained model on a bunch of images in the test set and display <span class="No-Break">the results:</span><pre class="source-code">
images, labels = next(iter(test_dataloader))</pre><pre class="source-code">
with torch.no_grad():</pre><pre class="source-code">
    outputs = model(images.to(device)).cpu()</pre><pre class="source-code">
plt.figure(figsize=(12, 6))</pre><pre class="source-code">
for idx in range(4):</pre><pre class="source-code">
    plt.subplot(2, 4, idx*2+1)</pre><pre class="source-code">
    plt.imshow(unnormalize(images[idx].permute(</pre><pre class="source-code">
        1, 2, 0).numpy(), mean, std))</pre><pre class="source-code">
    plt.axis('off')</pre><pre class="source-code">
    plt.subplot(2, 4, idx*2+2)</pre><pre class="source-code">
    plt.imshow(unnormalize(outputs[idx].permute(</pre><pre class="source-code">
        1, 2, 0).numpy(), mean, std).clip(0, 1))</pre><pre class="source-code">
    plt.axis('off')</pre></li>
</ol>
<p>Here are the <a id="_idIndexMarker852"/><span class="No-Break">results displayed:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer333">
<img alt="Figure 11.20 – A few images and their predicted transferred style" height="279" src="image/B19629_11_20.jpg" width="560"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.20 – A few images and their predicted transferred style</p>
<p>As we can see in the preceding figure, the model manages to transfer some of the style, by giving a smooth skin and sometimes coloring the hair. It is not perfect though, as the output images seem to be <span class="No-Break">too bright.</span></p>
<p>It is likely that by fine-tuning the loss weights and other hyperparameters, it would be possible to<a id="_idIndexMarker853"/> achieve <span class="No-Break">better results.</span></p>
<h2 id="_idParaDest-302"><a id="_idTextAnchor302"/>There’s more…</h2>
<p>While this recipe <a id="_idIndexMarker854"/>showed how to use generative models such as Stable Diffusion to create new data in a fun way, it is possible to use it in many other applications. Let’s see here how to use Stable Diffusion to create your own style transfer dataset, as well as a few other <span class="No-Break">possible applications.</span></p>
<p>As mentioned earlier, Stable Diffusion allows us to create realistic and creative images based on input prompts. Unfortunately, on its own, it cannot effectively apply a style to a given image without compromising the original image’s details (for example, the face shape, etc.). To do so, we can use another model based on Stable <span class="No-Break">Diffusion: ControlNet.</span></p>
<p>ControlNet works like Stable Diffusion: it takes input prompts and generates output images. However, unlike Stable Diffusion, ControlNet will take control information as input, allowing us to specifically generate data based on a control image: this is exactly what was done to create the dataset of this recipe, efficiently adding a drawing style to faces while keeping the overall <span class="No-Break">facial features.</span></p>
<p>The control information can take many forms, such as <span class="No-Break">the following:</span></p>
<ul>
<li>Image contours with Canny edges or Hough lines, allowing us to perform realistic and limitless image augmentation for <span class="No-Break">image classification</span></li>
<li>Depth estimation, allowing us to efficiently generate <span class="No-Break">background images</span></li>
<li>Semantic segmentation, allowing image augmentation for semantic <span class="No-Break">segmentation tasks</span></li>
<li>Pose estimation, generating more images with people in a given pose, which can be useful for object detection, semantic segmentation, <span class="No-Break">and more</span></li>
<li>Much more, such as scribbles and <span class="No-Break">normal maps</span></li>
</ul>
<p class="callout-heading">Note</p>
<p class="callout">The Canny edge detector and Hough line transform are typical image processing algorithms, allowing us to detect edges and straight lines in <span class="No-Break">images, respectively.</span></p>
<p>As a concrete example, in the following figure, using an input image and the computed Canny edges as input, as well as a text prompt such as <em class="italic">A realistic cute shiba inu in a fluffy basket</em>, ControlNet allows us to generate a new image really close to the first one. Refer to the <span class="No-Break">following figure:</span></p>
<div>
<div class="IMG---Figure" id="_idContainer334">
<img alt="Figure 11.21 – On the left, the input image; at the center, the computed Canny edges; and on the right, the generated image with ControlNet and the prompt “A realistic cute shiba inu in a fluffy basket”" height="580" src="image/B19629_11_21.jpg" width="1347"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 11.21 – On the left, the input image; at the center, the computed Canny edges; and on the right, the generated image with ControlNet and the prompt “A realistic cute shiba inu in a fluffy basket”</p>
<p>There are several<a id="_idIndexMarker855"/> ways to install and use ControlNet, but the official repository can be installed with the <span class="No-Break">following commands:</span></p>
<pre class="source-code">
git clone git@github.com:lllyasviel/ControlNet.git
cd ControlNet
conda env create -f environment.yaml
conda activate control</pre>
<p>From there, you have to download the models specifically for your needs, available on HuggingFace. For example, you can download the Canny model with the <span class="No-Break">following command:</span></p>
<pre class="source-code">
wget https://huggingface.co/lllyasviel/ControlNet/resolve/main/models/control_sd15_canny.pth -P models/</pre>
<p class="callout-heading">Note</p>
<p class="callout">The downloaded file is more than 6 GB, so this might <span class="No-Break">take time.</span></p>
<p>Finally, you can launch ControlNet UI with the following command line, <strong class="source-inline">python gradio_canny2image.py</strong>, and then follow the instructions by going to the created <span class="No-Break">localhost, </span><span class="No-Break"><strong class="source-inline">http://0.0.0.0:7860</strong></span><span class="No-Break">.</span></p>
<p>Using ControlNet and Stable Diffusion, given a powerful enough computer, you can now generate almost limitless new images, allowing you to train really robust and well-regularized <a id="_idIndexMarker856"/>models for <span class="No-Break">computer vision.</span></p>
<h2 id="_idParaDest-303"><a id="_idTextAnchor303"/>See also</h2>
<ul>
<li>Paper on <a id="_idIndexMarker857"/>the U-Net++ <span class="No-Break">architecture: </span><a href=""><span class="No-Break">https://arxiv.org/abs/1807.10165</span></a></li>
<li>More about<a id="_idIndexMarker858"/> neural style <span class="No-Break">transfer: </span><a href=""><span class="No-Break">https://en.wikipedia.org/wiki/Neural_style_transfer</span></a></li>
<li>The Wikipedia <a id="_idIndexMarker859"/>page about Stable <span class="No-Break">Diffusion: </span><a href=""><span class="No-Break">https://en.wikipedia.org/wiki/Stable_Diffusion</span></a></li>
<li>The paper on<a id="_idIndexMarker860"/> perceptual <span class="No-Break">loss: </span><a href=""><span class="No-Break">https://arxiv.org/pdf/1603.08155.pdf</span></a></li>
<li>This recipe was inspired <span class="No-Break">by </span><a href=""><span class="No-Break">https://medium.com/@JMangia/optimize-a-face-to-cartoon-style-transfer-model-trained-quickly-on-small-style-dataset-and-50594126e792</span></a></li>
<li>The official <a id="_idIndexMarker861"/>ControlNet <span class="No-Break">repository: </span><a href=""><span class="No-Break">https://github.com/lllyasviel/ControlNet</span></a></li>
<li>An advanced way of using ControlNet, allowing us to compose several <span class="No-Break">models: </span><a href=""><span class="No-Break">https://github.com/Mikubill/sd-webui-controlnet</span></a></li>
<li>The Wikipedia page<a id="_idIndexMarker862"/> about the Canny edge <span class="No-Break">detector: </span><a href="https://en.wikipedia.org/wiki/Canny_edge_detector"><span class="No-Break">https://en.wikipedia.org/wiki/Canny_edge_detector</span></a></li>
</ul>
</div>
</div></body></html>