<html><head></head><body>
		<div>
			<div id="_idContainer118" class="Content">
			</div>
		</div>
		<div id="_idContainer119" class="Content">
			<h1 id="_idParaDest-109">4. <a id="_idTextAnchor133"/>Deep Learning for Text – Embeddings</h1>
		</div>
		<div id="_idContainer135" class="Content">
			<p class="callout-heading">Overview</p>
			<p class="callout">In this chapter, we will begin our foray into <strong class="bold">Natural<a id="_idTextAnchor134"/> Language Processing</strong> for text. We will start by using the <strong class="bold">Natural Language Toolkit</strong> to perform text preprocessing on raw text data, where we will tokenize the raw text and remove punctuations and stop words. As we progress through this chapter, we will implement classical approaches to text representation, such as one-hot encoding and the <strong class="bold">TF-lDF</strong> approach. This chapter demonstrates the power of word embeddings and explains the popular deep learning-based approaches for embeddings. We will use the <strong class="bold">Skip-gram</strong> and <strong class="bold">Continuous Bag of Words</strong> algorithms to generate our own word embeddings. We will explore the properties of the embeddings, the different parameters of the algorithms, and generate vectors for phrases. By the end of this chapter, you will be able to handle text data and start using word embeddings by using pre-trained models, as well as your own embeddings.</p>
			<h1 id="_idParaDest-110"><a id="_idTextAnchor135"/>Introduction</h1>
			<p>How does Siri know exactly what to do when you ask her to "<em class="italic">play a mellow song from the 80s</em>"? How does Google find the most relevant results for even your ill-formed search queries in a fraction of a second? How does your translation app translate text from German to English almost instantly? How does your email client protect you and automatically identify all those malicious spam/phishing emails? The answer to all these questions, and what powers many more amazing applications, is using <strong class="bold">Natural Language Processing</strong> (<strong class="bold">NLP</strong>).</p>
			<p>So far, we've dealt with structured, numeric data – images that were also numeric matrices. In this chapter, we'll begin our discussion by talking about handling text data and unlock the skills needed to harness this goldmine of unstructured information. We will discuss a key idea in this chapter – representation, particularly using embeddings. We will discuss the considerations and implement the approaches for representation. We will begin with the simplest approaches and end with word embeddings – an amazingly powerful approach for representing text data. Word embeddings will help you get state-of-the-art results in NLP tasks when coupled with deep learning approaches.</p>
			<p>NLP is a field concerned with helping machines make sense of natural (human) language. As shown in the following figure, NLP resides at the intersection of linguistics, computer science, and artificial intelligence:</p>
			<div>
				<div id="_idContainer120" class="IMG---Figure">
					<img src="image/B15385_04_01.jpg" alt="Figure 4.1: Where NLP fits&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.1: Where NLP fits</p>
			<p>It is a vast field – think of all the places language (spoken and written) is used. NLP enables and powers the kind of applications listed in the preceding figure, including the following:</p>
			<ul>
				<li>Classification of documents into categories (text classification)</li>
				<li>Translation between languages, say, German to English (sequence-to-sequence learning)</li>
				<li>Automatically classifying the sentiment of a tweet or a movie review (sentiment analysis)</li>
				<li>Chatbots that reply to your query instantly, 24/7</li>
			</ul>
			<p>Before we go any further, we need to acknowledge and appreciate that NLP isn't easy. Consider the following sentence: "<em class="italic">The boy saw a man with a telescope.</em>"</p>
			<p>Who had the telescope? Did the boy use a telescope to see the man through it? Or was the man carrying a telescope with him? There is an ambiguity that we can't resolve with this sentence alone. Maybe some more context will help us figure this out.</p>
			<p>Let's consider this sentence, then: "<em class="italic">Rahim convinced Mohan to buy a television for himself.</em>" Who was the TV bought for – Rahim or Mohan? This is another case of ambiguity that we may be able to resolve with more context, but again, it may be very difficult for a machine/program.</p>
			<p>Let's consider another example: "<em class="italic">Rahim has quit skydiving.</em>" This sentence implies that Rahim did a fair amount of skydiving. There is a presupposition in this sentence, which is hard for a machine to infer.</p>
			<p>Language is a complex system that uses symbols (words/terms) and combines them in many ways to communicate ideas. Making sense of language is not always very easy, and there are many reasons for this. Ambiguity is by far the biggest reason: words can have different meanings in different contexts. Add to that subtext, different perspectives, and so on. We can never be sure if the same words are understood the same way by different people. A poem can be interpreted in many ways by those who read it, where each reader brings their unique perspective and understanding of the world and employs them to make sense of the poem in their own way.</p>
			<h1 id="_idParaDest-111"><a id="_idTextAnchor136"/>Deep Learning for Natural Language Processing</h1>
			<p>The emergence of deep learning has had a strong positive impact on many fields, and NLP is no exception. By now, you can appreciate that deep learning approaches have given us accuracies like never before, and this has helped us improve in many areas. There are several tasks in NLP that have gained tremendously from deep learning approaches. Applications that use sentiment prediction, machine translation, and chatbots previously required a lot of manual intervention. With deep learning and NLP, these tasks are completely automated and bring with them impressive performance. The simple, high-level view shown in <em class="italic">Figure 4.2</em> shows how deep learning can be used for processing natural language. Deep learning provides us with not only great representations of natural language that machines can understand but also very powerful modeling approaches well suited for tasks in NLP.</p>
			<div>
				<div id="_idContainer121" class="IMG---Figure">
					<img src="image/B15385_04_02.jpg" alt="Figure 4.2: Deep learning for NLP&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.2: Deep learning for NLP</p>
			<p>That being said, we need to be cautious to avoid underestimating the difficulty of getting machines to perform tasks involving human language and the field of NLP. Deep learning hasn't solved all of the challenges in NLP, but it has indeed caused a paradigm shift in the way several tasks in NLP are approached and has helped advance some applications in this field, making otherwise difficult tasks accessible and easy for anyone and everyone. We will perform some of these in <em class="italic">Chapter 5</em>, <em class="italic">Deep Learning for Sequences</em>.</p>
			<p>One such key task is text data representation – which is, in simple terms, converting raw text into something a model would understand. Word embeddings constitute a deep learning-based approach that has changed the game and gives a very powerful representation of text. We'll discuss embeddings in detail and create our own embeddings later in this chapter. First, let's get our hands dirty by working with some text and performing some very important data preparation.</p>
			<h2 id="_idParaDest-112"><a id="_idTextAnchor137"/>Getting Started with Text Data Handling</h2>
			<p>Let's get some test data into Python to begin. First, we'll create some toy data of our own and get familiar with the tools. Then, we'll use Lewis Carrol's classic work, "<em class="italic">Alice's Adventures in Wonderland</em>", which is available through Project Gutenberg (<a href="http://gutenberg.org">gutenberg.org</a>). Conveniently enough, we have this easily accessible through the <strong class="bold">Natural Language ToolKit</strong> (<strong class="bold">NLTK</strong>), a great library for performing NLP from scratch.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The code implementations for this chapter can be found at <a href="https://packt.live/3gEgkSP">https://packt.live/3gEgkSP</a>. All the code in this chapter must be run in a single Jupyter Notebook.</p>
			<p>NLTK should come with the Anaconda distribution. If not, you can install NLTK by using the following command in the command line:</p>
			<p class="source-code">pip install nltk</p>
			<p>This should work on Windows. For macOS and Linux, you can use the following command:</p>
			<p class="source-code">$ sudo pip install -U nltk</p>
			<p>Our dummy data can be created using the following command (we're using Jupyter Notebooks here; feel free to use any interface):</p>
			<p class="source-code">raw_txt = """Welcome to the world of Deep Learning for NLP! \</p>
			<p class="source-code">             We're in this together, and we'll learn together. \</p>
			<p class="source-code">             NLP is amazing, \</p>
			<p class="source-code">             and Deep Learning makes it even more fun. \</p>
			<p class="source-code">             Let's learn!"""</p>
			<p>We have the text in <strong class="source-inline">raw_txt</strong>, which is a string variable, so now, we're ready to start processing it.</p>
			<h2 id="_idParaDest-113"><a id="_idTextAnchor138"/>Text Preprocessing</h2>
			<p>Text preprocessing refers to the process of getting the text data ready for your primary analysis/model. Regardless of your end goal – which could be sentiment analysis, classification, clustering, or any of the many others – you need to get your raw text data cleaned up and ready for analysis. This is the first part of any application involving NLP.</p>
			<p>What do we mean by <strong class="bold">clean up</strong>, and when is the text data ready? We know that the text data we encounter in our day-to-day lives can be very messy (think about social media, product reviews, service reviews, and so on) and has various imperfections. Depending on the task at hand and the kind of data you're dealing with, the imperfections you care about will vary, and <strong class="bold">cleaning up</strong> can mean very different things. As an example, in some applications, preprocessing could just mean "dividing the sentences into individual terms." The steps you take here can and will have an impact on the final outcome of your analysis. Let's discuss this in more detail.</p>
			<h3 id="_idParaDest-114"><a id="_idTextAnchor139"/>Tokenization</h3>
			<p>The first step in preprocessing is inevitably <strong class="bold">tokenization</strong> – splitting the raw input text sequence into <strong class="bold">tokens</strong>. In simple terms, it is breaking the raw text into constituent elements that you want to work on. This token can be a paragraph, sentence, word, or even a character. If you want to separate a paragraph into sentences, then you would tokenize the paragraph into sentences. If you want to separate the words in a sentence, then you would tokenize the sentence into words.</p>
			<p>For our raw text, first, we want to separate the sentences. To do so, we have multiple options in Python – here, we'll use the tokenize API in NLTK.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">We'll be using Jupyter Notebooks throughout this book, which is something that we recommend. However, feel free to use any IDE you wish.</p>
			<p>Before we can use the API, we have to <strong class="source-inline">import nltk</strong> and download the <strong class="source-inline">punkt</strong> sentence tokenizer. Then, we need to import the <strong class="source-inline">tokenize</strong> library. All this can be done using the following commands:</p>
			<p class="source-code">import nltk</p>
			<p class="source-code">nltk.download('punkt')</p>
			<p class="source-code">from nltk import tokenize</p>
			<p>The tokenize API has utilities to extract different levels of tokens (sentences, words, or characters) for different types of data (a very handy tweet tokenizer, too). We'll use the <strong class="source-inline">sent_tokenize()</strong> method here. The <strong class="source-inline">sent_tokenize()</strong> method breaks input text into constituent sentences. Let's see it in action:</p>
			<p class="source-code">tokenize.sent_tokenize(raw_txt)</p>
			<p>This should give us the following individual sentences:</p>
			<p class="source-code">['Welcome to the world of Deep Learning for NLP!',</p>
			<p class="source-code"> "We're in this together, and we'll learn together.",</p>
			<p class="source-code"> 'NLP is amazing, and Deep Learning makes it even more fun.',</p>
			<p class="source-code"> "Let's learn!"]</p>
			<p>Looking at the output, it seems like <strong class="source-inline">sent_tokenize()</strong> is doing a pretty good job. It has correctly identified the sentence boundaries and given us the four sentences, as expected. Let's assign the result to a variable for ease of handling and let's check the data type of the result and its constituents:</p>
			<p class="source-code">txt_sents = tokenize.sent_tokenize(raw_txt)</p>
			<p class="source-code">type(txt_sents), len(txt_sents)</p>
			<p>The following is the output of the preceding code:</p>
			<p class="source-code">(list, 4)</p>
			<p>As we can see, it's a list with four elements, where each element contains the sentence as a string.</p>
			<p>We can try breaking sentences into individual words using the <strong class="source-inline">word_tokenize()</strong> method. This method breaks a given sentence into its constituent words. It uses smart rules to figure out word boundaries. Let's use list comprehension (comprehensions in Python are a concise approach to constructing new sequences) for a bit for convenience:</p>
			<p class="source-code">txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]</p>
			<p class="source-code">type(txt_words), type(txt_words[0])</p>
			<p>The preceding command gives us the following output:</p>
			<p class="source-code">(list, list)</p>
			<p>The output is as expected – the elements of the resulting list are lists themselves, containing the words that form the sentence. Let's also print out the first two elements of the result:</p>
			<p class="source-code">print(txt_words[:2])</p>
			<p>The output would be as follows:</p>
			<p class="source-code">[['Welcome', 'to', 'the', 'world', 'of', </p>
			<p class="source-code">  'Deep', 'Learning', 'for', 'NLP', '!'], </p>
			<p class="source-code">  ['We', "'re", 'in', 'this', 'together', </p>
			<p class="source-code">  ',', 'and', 'we', "'ll", 'learn', 'together', '.']]</p>
			<p>The sentences have been broken into individual words. We can also see that contractions like "we'll" have been broken into constituents, that is, "we" and "'ll". All punctuation (commas, periods, exclamation marks, and so on) are separate tokens. This is very convenient for us if we wish to remove them, which we will do later.</p>
			<h3 id="_idParaDest-115"><a id="_idTextAnchor140"/>Normalizing Case</h3>
			<p>Another common step is to normalize case – we usually don't want "car", "CAR", "Car", and "caR" to be treated as separate entities. To do so, we typically convert all text into lowercase (we could also convert it into uppercase if we wanted).</p>
			<p>All strings in Python have a <strong class="source-inline">lower()</strong> method to them, so converting a string variable (<strong class="source-inline">strvar</strong>) into lowercase is as simple as <strong class="source-inline">strvar.lower()</strong>. </p>
			<p class="callout-heading">Note</p>
			<p class="callout">We could have used this right in the beginning, before tokenization, and it would have been as simple as <strong class="source-inline">raw_txt = raw_txt.lower()</strong>.</p>
			<p>We will normalize the case of our data using the <strong class="source-inline">lower()</strong> method after tokenizing into individual sentences. We'll accomplish this with the following commands:</p>
			<p class="source-code">txt_sents  = [sent.lower() for sent in txt_sents]</p>
			<p class="source-code">txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]</p>
			<p>Let's print out a couple of sentences to see what the result looks like:</p>
			<p class="source-code">print(txt_words[:2])</p>
			<p>The output will be as follows:</p>
			<p class="source-code">[['welcome', 'to', 'the', 'world', 'of', </p>
			<p class="source-code">  'deep', 'learning', 'for', 'nlp', '!'], </p>
			<p class="source-code"> ['we', "'re", 'in', 'this', 'together', </p>
			<p class="source-code">  ',', 'and', 'we', "'ll", 'learn', 'together', '.']]</p>
			<p>We can see that the output has all the terms in lowercase this time. We've taken the raw text, broken it into sentences, normalized the case, and then broken that down into words. Now, we have all the tokens that we need, but we still seem to have a lot of punctuation marks as tokens that we need to get rid of. Let's go ahead and perform more "cleanup".</p>
			<h3 id="_idParaDest-116"><a id="_idTextAnchor141"/>Removing Punctuation</h3>
			<p>We can see that the data currently has all punctuation as separate tokens. Again, bear in mind that there could be tasks where punctuations could be important. As an example, when performing sentiment analysis, that is, predicting if the sentiment in the text is positive or negative, an exclamation can add value. For our task, let's remove these since we're only interested in representing the terms of language. To do so, we need to have a list of all the punctuation marks we want to remove. Luckily, we have such a list in the string base library in Python, which we can simply import and assign to a list variable:</p>
			<p class="source-code">from string import punctuation</p>
			<p class="source-code">list_punct = list(punctuation)</p>
			<p class="source-code">print(list_punct)</p>
			<p>You should get the following output:</p>
			<p class="source-code">['!', '"', '#', '$', '%', '&amp;', "'", '(', ')', '*', '+', ',', </p>
			<p class="source-code"> '-', '.', '/', ':', ';', '&lt;', '=', '&gt;', '?', '@', </p>
			<p class="source-code"> '[', '\\', ']', '^', '_', '`', '{', '|', '}', '~']</p>
			<p>All the usual punctuation marks are available here. If there are any additional punctuation marks you want to remove, you can simply add them to the <strong class="source-inline">list_punct</strong> variable.</p>
			<p>We can define a function to remove punctuation from a given list of tokens. This function will expect a list of tokens, from which it will drop the tokens that are available in the <strong class="source-inline">list_punct</strong> variable:</p>
			<p class="source-code">def drop_punct(input_tokens):</p>
			<p class="source-code">    return [token for token in input_tokens \</p>
			<p class="source-code">            if token not in list_punct]</p>
			<p>We can test this out on some dummy tokens using the following command:</p>
			<p class="source-code">drop_punct(["let",".","us",".","go","!"])</p>
			<p>We get the following result:</p>
			<p class="source-code">['let', 'us', 'go']</p>
			<p>The function works as intended. Now, we need to pass the <strong class="source-inline">txt_words</strong> variable we modified in the previous section to the <strong class="source-inline">drop_punct</strong> function we just created. We will store our result in a new variable called <strong class="source-inline">txt_words_nopunct</strong>:</p>
			<p class="source-code">txt_words_nopunct = [drop_punct(sent) for sent in txt_words]</p>
			<p class="source-code">print(txt_words_nopunct)</p>
			<p>We will get the following output:</p>
			<p class="source-code">[['welcome', 'to', 'the', 'world', 'of', </p>
			<p class="source-code">  'deep', 'learning', 'for', 'nlp'], </p>
			<p class="source-code"> ['we', "'re", 'in', 'this', 'together', 'and', </p>
			<p class="source-code">  'we', "'ll", 'learn', 'together'], </p>
			<p class="source-code"> ['nlp', 'is', 'amazing', 'and', </p>
			<p class="source-code">  'deep', 'learning', 'makes', 'it', 'even', 'more', 'fun'], </p>
			<p class="source-code"> ['let', "'s", 'learn']]</p>
			<p>As you can see from the preceding output, the function we created has removed all the punctuation marks from our raw text. Now, the data looks much cleaner without the punctuation, but we still need to get rid of non-informative terms. We'll discuss that in the next section.</p>
			<h3 id="_idParaDest-117"><a id="_idTextAnchor142"/>Removing Stop Words</h3>
			<p>In day-to-day language, we have a lot of terms that don't add a lot of information/value*. These are typically referred to as "stop words". We can think of these as belonging to two broad categories:</p>
			<ol>
				<li><strong class="bold">General/functional</strong>: These are filler words in the language that don't provide a lot of information but help stitch together other informative words to form meaningful sentences, such as "the", "an", "of", and so on.</li>
				<li><strong class="bold">Contextual</strong>: These aren't general functional terms, but given the context, don't add a lot of value. If you're working with reviews of a mobile phone, where all reviews are talking about the phone, the term "phone" itself may not add a lot of information.<p class="callout-heading">Note</p><p class="callout">*The notion of "value" changes with each task. Functional words such as "the" and "and" may not be important for, say, automatic document categorization into subjects, but can be very important for other applications, such as part-of-speech tagging (identifying verbs, adjectives, nouns, pronouns, and so on).</p></li>
			</ol>
			<p>Functional stop words are conveniently built into NLTK. We just need to import them and then we can store them in a variable. Once stored, they can be accessed just like any Python list. Let's import them and see how many of these words we have:</p>
			<p class="source-code">import nltk</p>
			<p class="source-code">nltk.download("stopwords")</p>
			<p class="source-code">from nltk.corpus import stopwords</p>
			<p class="source-code">list_stop = stopwords.words("english")</p>
			<p class="source-code">len(list_stop)</p>
			<p>We will see the following output:</p>
			<p class="source-code">179</p>
			<p>We can see that we have 179 built-in stop words. Let's also print some of them:</p>
			<p class="source-code">print(list_stop[:50])</p>
			<p>The output will be as follows:</p>
			<p class="source-code">['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', </p>
			<p class="source-code"> 'you', "you're", "you've", "you'll", "you'd", 'your', </p>
			<p class="source-code"> 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', </p>
			<p class="source-code"> 'himself', 'she', "she's", 'her', 'hers', 'herself', </p>
			<p class="source-code"> 'it', "it's", 'its', 'itself', 'they', 'them', </p>
			<p class="source-code"> 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', </p>
			<p class="source-code"> 'this', 'that', "that'll", 'these', 'those', 'am', 'is', 'are', </p>
			<p class="source-code"> 'was', 'were', 'be']</p>
			<p>We can see that most of these terms are very commonly used "filler" terms that have a "functional" role in the language, and don't add a lot of information.</p>
			<p>Now, removing stop words can be done the same way we removed punctuation.</p>
			<h2 id="_idParaDest-118"><a id="_idTextAnchor143"/>Exercise 4.01: Tokenizing, Case Normalization, Punctuation, and Stop Word Removal</h2>
			<p>In this exercise, we will remove stop words from the data, and also apply everything we have learned so far. We'll start by performing tokenization (sentences and words); then, we'll perform case normalization, followed by punctuation and stop word removal.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before commencing this exercise, ensure that you are using a Jupyter Notebook where you have downloaded both the <strong class="source-inline">punkt</strong> sentence tokenizer and the <strong class="source-inline">stopwords</strong> corpus, as demonstrated in the <em class="italic">Text Preprocessing</em> section. </p>
			<p>We'll keep the code concise this time. We'll be defining and manipulating the <strong class="source-inline">raw_txt</strong> variable. Let's get started:</p>
			<ol>
				<li value="1">Run the following commands to import <strong class="source-inline">nltk</strong> and the <strong class="source-inline">tokenize</strong> module from it:<p class="source-code">import nltk</p><p class="source-code">from nltk import tokenize</p></li>
				<li>Define the <strong class="source-inline">raw_txt</strong> variable so that it contains the text "<strong class="source-inline">Welcome to the world of deep learning for NLP! We're in this together, and we'll learn together. NLP is amazing, and deep learning makes it even more fun. Let's learn!</strong>":<p class="source-code">raw_txt = """Welcome to the world of deep learning for NLP! \</p><p class="source-code">             We're in this together, and we'll learn together. \</p><p class="source-code">             NLP is amazing, \</p><p class="source-code">             and deep learning makes it even more fun. \</p><p class="source-code">             Let's learn!"""</p></li>
				<li>Use the <strong class="source-inline">sent_tokenize()</strong> method to separate the raw text into individual sentences and store the result in a variable. Use the <strong class="source-inline">lower()</strong> method to convert the string into lowercase before tokenizing:<p class="source-code">txt_sents = tokenize.sent_tokenize(raw_txt.lower())</p><p class="callout-heading">Note</p><p class="callout">The <strong class="source-inline">txt_sents</strong> variable we've just created will be used later on in the chapter as well.</p></li>
				<li>Using list comprehension, apply the <strong class="source-inline">word_tokenize()</strong> method to separate each sentence into its constituent words:<p class="source-code">txt_words = [tokenize.word_tokenize(sent) for sent in txt_sents]</p></li>
				<li>Import <strong class="source-inline">punctuation</strong> from the <strong class="source-inline">string</strong> module and convert it into a list:<p class="source-code">from string import punctuation</p><p class="source-code">stop_punct = list(punctuation)</p></li>
				<li>Import the built-in stop words for English from NLTK and save them in a variable:<p class="source-code">from nltk.corpus import stopwords</p><p class="source-code">stop_nltk = stopwords.words("english")</p></li>
				<li>Create a combined list that contains the punctuations as well as the NLTK stop words. Note that we can remove them together in one go:<p class="source-code">stop_final = stop_punct + stop_nltk</p></li>
				<li>Define a function that will remove stop words and punctuation from the input sentence, provided as a collection of tokens:<p class="source-code">def drop_stop(input_tokens):</p><p class="source-code">    return [token for token in input_tokens \</p><p class="source-code">            if token not in stop_final]</p></li>
				<li>Remove redundant tokens by applying the function to the tokenized sentences and store the result in a variable:<p class="source-code">txt_words_nostop = [drop_stop(sent) for sent in txt_words]</p></li>
				<li>Print the first cleaned-up sentence from the data:<p class="source-code">print(txt_words_nostop[0])</p><p>With the stop words removed, the result will look like this:</p><p class="source-code">['welcome', 'world', 'deep', 'learning', 'nlp']</p><p class="callout-heading">Note</p><p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2VVNEgf">https://packt.live/2VVNEgf</a>.</p><p class="callout">You can also run this example online at <a href="https://packt.live/38Gr54r">https://packt.live/38Gr54r</a>. You must execute the entire Notebook in order to get the desired result.</p></li>
			</ol>
			<p>In this exercise, we performed all the cleanup steps we've learned about so far. This time around, we combined certain steps and made the code more concise. These are some very common steps that we should apply when dealing with text data. You could try to further optimize and modularize by defining a function that returns the result after all the processing steps. We encourage you to try it out.</p>
			<p>So far, the steps in the cleanup process were steps that got rid of tokens that weren't very useful in our assessment. But there are a few more things we could do to make our data even better – we can try using our understanding of the language to combine tokens, identify tokens that have practically the same meaning, and remove further redundancy. A couple of popular approaches are stemming and lemmatization.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The variables we have created in this exercise will be used in later sections of the chapter as well. Ensure that you're completing this exercise first before moving to the upcoming exercises and activities. </p>
			<h2 id="_idParaDest-119"><a id="_idTextAnchor144"/>Stemming and Lemmatization</h2>
			<p>"Eat", "eats", "eating", "ate" – aren't they all just variations of the same word, all referring to the same action? In most text and spoken language, in general, we have multiple forms of the same word. Typically, we don't want these to be considered as separate tokens. A search engine would need to return similar results if the query is "red shoes" or "red shoe"– it would be a terrible search experience otherwise. We acknowledge that such cases are very common and that we need a strategy to handle such cases. But what should we do with the variants of a word? A reasonable approach is to map them all to a common token so that they are all treated the same.</p>
			<p>Stemming is a rule-based approach to achieve normalization by reducing a word to its "stem". The stem is the root of the word before any affixes (an element added to make a variant) are added. This approach is rather simple – chop off the suffix to get the stem. A popular algorithm is the <strong class="bold">Porter stemming</strong> algorithm, which applies a series of such rules:</p>
			<div>
				<div id="_idContainer122" class="IMG---Figure">
					<img src="image/B15385_04_03.jpg" alt="Figure 4.3: Examples of the Porter stemming algorithm's rule-based approach&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.3: Examples of the Porter stemming algorithm's rule-based approach</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The full set of Porter stemming algorithm rules can be found at <a href="http://snowball.tartarus.org/algorithms/porter/stemmer.html">http://snowball.tartarus.org/algorithms/porter/stemmer.html</a>.</p>
			<p>Let's look at the Porter stemming algorithm in action. Let's import the <strong class="source-inline">PorterStemmer</strong> function from the <strong class="source-inline">'stem'</strong> module in NLTK and create an instance of it:</p>
			<p class="source-code">from nltk.stem import PorterStemmer</p>
			<p class="source-code">stemmer_p = PorterStemmer()</p>
			<p>Note that the stemmer works on individual tokens, not sentences as a whole. Let's see how the stemmer stems the word "<strong class="source-inline">driving</strong>":</p>
			<p class="source-code">print(stemmer_p.stem("driving"))</p>
			<p>The output will be as follows:</p>
			<p class="source-code">drive</p>
			<p>Let's see how we can apply this to a whole sentence. Note that we will have to tokenize the sentence:</p>
			<p class="source-code">txt = "I mustered all my drive, drove to the driving school!"</p>
			<p>The following code is used for tokenizing the sentence and applying the stemmer to each term:</p>
			<p class="source-code">tokens = tokenize.word_tokenize(txt)</p>
			<p class="source-code">print([stemmer_p.stem(word) for word in tokens])</p>
			<p>The output is as follows:</p>
			<p class="source-code">['I', 'muster', 'all', 'my', 'drive', ',', 'drove', 'to', </p>
			<p class="source-code"> 'the', 'drive', 'school', '!']</p>
			<p>We can see that the stemmer has correctly reduced "mustered" to "muster" and "driving" to "drive", while "drove" is untouched. Also, note that the result of a stemmer need not be a valid English word.</p>
			<p>Lemmatization is a more sophisticated approach that refers to a dictionary and finds a valid root form (the lemma) of the word. Lemmatization works best when the part of speech of the word is also provided – it considers the role the term is playing and returns the appropriate form. The output from a lemmatization step is always a valid English word. However, lemmatization is computationally very expensive, and for it to work well, it needs the part-of-speech tag, which typically isn't available in the data. Let's have a brief look at it. First, let's import <strong class="source-inline">WordNetLemmatizer</strong> from <strong class="source-inline">nltk.stem</strong> and instantiate it:</p>
			<p class="source-code">nltk.download('wordnet')</p>
			<p class="source-code">from nltk.stem import WordNetLemmatizer</p>
			<p class="source-code">lemmatizer = WordNetLemmatizer()</p>
			<p>Let's apply the <strong class="source-inline">lemmatizer</strong> on the term <strong class="source-inline">ponies</strong>:</p>
			<p class="source-code">lemmatizer.lemmatize("ponies")</p>
			<p>The following is the output:</p>
			<p class="source-code">'pony'</p>
			<p>For our discussions, stemming is sufficient. The result from stemming may not always be a valid word. For example, <strong class="source-inline">poni</strong> is the stem for <strong class="source-inline">ponies</strong> but isn't a valid English word. Also, there may be some inaccuracies, but for the objective of mapping to a common word, this crude method works just fine.</p>
			<h2 id="_idParaDest-120"><a id="_idTextAnchor145"/>Exercise 4.02: Stemming Our Data</h2>
			<p>In this exercise, we will continue with data preprocessing. We removed the stop words and punctuation in the previous exercise. Now, we will use the Porter stemming algorithm to stem the tokens. Since we'll be using the <strong class="source-inline">txt_words_nostop</strong> variable we created previously, let's continue with the same Jupyter Notebook we created in <em class="italic">Exercise 4.01</em>, <em class="italic">Tokenizing, Case Normalization, Punctuation, and Stop Word Removal</em>. The variable, at this point, will contain the following text:</p>
			<p class="source-code">[['welcome', 'world', 'deep', 'learning', 'nlp'],</p>
			<p class="source-code"> ["'re", 'together', "'ll", 'learn', 'together'],</p>
			<p class="source-code"> ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'],</p>
			<p class="source-code"> ['let', "'s", 'learn']]</p>
			<p>The following are the steps to complete this exercise:</p>
			<ol>
				<li value="1">Import <strong class="source-inline">PorterStemmer</strong> from NLTK using the following command:<p class="source-code">from nltk.stem import PorterStemmer</p></li>
				<li>Instantiate the stemmer:<p class="source-code">stemmer_p = PorterStemmer()</p></li>
				<li>Apply the stemmer to the first sentence in <strong class="source-inline">txt_words_nostop</strong>:<p class="source-code">print([stemmer_p.stem(token) for token in txt_words_nostop[0]])</p><p>When we print the result, we get the following output:</p><p class="source-code">['<strong class="bold">welcom</strong>', 'world', 'deep', '<strong class="bold">learn</strong>', 'nlp']</p><p>We can see that <strong class="source-inline">welcome</strong> has been changed to <strong class="source-inline">welcom</strong> and <strong class="source-inline">learning</strong> to <strong class="source-inline">learn</strong>. This is consistent with the rules of the Porter stemming algorithm.</p></li>
				<li>Apply the stemmer to all the sentences in the data. You could use loops, or a nested list comprehension:<p class="source-code">txt_words_stem = [[stemmer_p.stem(token) for token in sent] \</p><p class="source-code">                   for sent in txt_words_nostop]</p></li>
				<li>Print the output using the following command:<p class="source-code">txt_words_stem</p><p>The output will be as follows:</p><p class="source-code">[['welcom', 'world', 'deep', 'learn', 'nlp'],</p><p class="source-code"> ["'re", 'togeth', "'ll", 'learn', 'togeth'],</p><p class="source-code"> ['nlp', 'amaz', 'deep', 'learn', 'make', 'even', 'fun'],</p><p class="source-code"> ['let', "'s", 'learn']]</p></li>
			</ol>
			<p>It looks like plenty of modifications have been made by the stemmer. Many of the words aren't valid anymore but are still recognizable, and that's okay.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2VVNEgf">https://packt.live/2VVNEgf</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38Gr54r">https://packt.live/38Gr54r</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>In this exercise, we used the Porter stemming algorithm to stem the terms of our tokenized data. Stemming works on individual terms, so it needs to be applied after tokenizing into terms. Stemming reduced some terms to their base form, which weren't necessarily valid English words.</p>
			<h3 id="_idParaDest-121"><a id="_idTextAnchor146"/>Beyond Stemming and Lemmatization</h3>
			<p>Beyond stemming and lemmatization, there are many specific approaches to handle word variations. We have techniques such as phonetic hashing to identify spelling variations of a word induced by pronunciations. Then, there is spelling correction to identify and rectify errors in spelling. Another potential step is abbreviation handling so that <em class="italic">television</em> and <em class="italic">TV</em> are treated the same. The result from these steps can be further augmented by performing domain-specific term handling. You get the drift… there are a lot of steps possible, and, depending on your data and the criticality of your application, you may include some of these in your processing.</p>
			<p>In general, though, the steps we performed together are largely sufficient – case normalization, tokenization, stop word, and punctuation removal, followed by stemming/lemmatization. These are some common steps that most NLP applications include.</p>
			<h3 id="_idParaDest-122"><a id="_idTextAnchor147"/>Downloading Text Corpora Using NLTK</h3>
			<p>So far, we've performed these steps on dummy data that we created. Now, it's time to try out our newly acquired skills on a larger and more authentic text. First, let's acquire that text – Lewis Carrol's classic work, "<em class="italic">Alice's Adventures in Wonderland</em>", which is available through Project Gutenberg and accessible through NLTK.</p>
			<p>You may need to download the <strong class="source-inline">'gutenberg'</strong> corpus through NLTK. First, import NLTK using the following command:</p>
			<p class="source-code">import nltk</p>
			<p>Then, use the <strong class="source-inline">nltk.download()</strong> command to open up an app, that is, the <strong class="bold">NLTK Downloader</strong> interface (shown in the following screenshot):</p>
			<p class="source-code">nltk.download()</p>
			<p>We can see that the app has multiple tabs. Click the <strong class="bold">Corpora</strong> tab:</p>
			<p> </p>
			<div>
				<div id="_idContainer123" class="IMG---Figure">
					<img src="image/B15385_04_04.jpg" alt="Figure 4.4: NLTK Downloader&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.4: NLTK Downloader</p>
			<p>In the <strong class="source-inline">Corpora</strong> tab, scroll down until you reach <strong class="source-inline">gutenberg</strong>. If the status is <strong class="source-inline">not installed</strong>, go ahead and click the <strong class="source-inline">Download</strong> button in the lower-left corner. That should install the <strong class="source-inline">gutenberg</strong> corpus:</p>
			<p> </p>
			<div>
				<div id="_idContainer124" class="IMG---Figure">
					<img src="image/B15385_04_05.jpg" alt="Figure 4.5: NLTK Downloader's Corpora tab&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.5: NLTK Downloader's Corpora tab</p>
			<p>Close the interface. Now, you can access some classic texts right from NLTK. We'll read in the text and store it in a variable:</p>
			<p class="source-code">alice_raw = nltk.corpus.gutenberg.raw('carroll-alice.txt')</p>
			<p>The text is stored in <strong class="source-inline">alice_raw</strong>, which is one big character string. Let's have a look at the first few characters of this string:</p>
			<p class="source-code">alice_raw[:800]</p>
			<p>The output will be as follows:</p>
			<p class="source-code">"[Alice's Adventures in Wonderland by Lewis Carroll 1865]</p>
			<p class="source-code">  \n\nCHAPTER I. Down the Rabbit-Hole\n\nAlice was beginning</p>
			<p class="source-code">  to get very tired of sitting by her sister on the\nbank, </p>
			<p class="source-code">  and of having nothing to do: once or twice she had peeped </p>
			<p class="source-code">  into the\nbook her sister was reading, but it had no pictures </p>
			<p class="source-code">  or conversations in\nit, 'and what is the use of a book,' </p>
			<p class="source-code">  thought Alice 'without pictures or\nconversation?'</p>
			<p class="source-code">  \n\nSo she was considering in her own mind </p>
			<p class="source-code">  (as well as she could, for the\nhot day made her feel </p>
			<p class="source-code">  very sleepy and stupid), whether the pleasure\nof making </p>
			<p class="source-code">  a daisy-chain would be worth the trouble of getting up </p>
			<p class="source-code">  and\npicking the daisies, when suddenly a White Rabbit </p>
			<p class="source-code">  with pink eyes ran\nclose by her.\n\nThere was nothing </p>
			<p class="source-code">  so VERY remarkable in that; nor did Alice think </p>
			<p class="source-code">  it so\nVERY much out of the way to hear the Rabbit"</p>
			<p>We can see the raw text in the output, which contains the usual imperfections that we expect – varying case, stop words, punctuation, and so on.</p>
			<p>We're ready. Let's test out our skills through an activity.</p>
			<h2 id="_idParaDest-123">Activity 4.01: Text <a id="_idTextAnchor148"/>Preprocessing of the 'Alice in Wonderland' Text</h2>
			<p>In this activity, you will apply all the preprocessing steps you've learned about so far to a much larger, real text. We'll work with the text for Alice in Wonderland that we stored in the <strong class="source-inline">alice_raw</strong> variable:</p>
			<p class="source-code">alice_raw[:800]</p>
			<p>The text currently looks like this:</p>
			<p class="source-code">"[Alice's Adventures in Wonderland by Lewis Carroll 1865]</p>
			<p class="source-code">  \n\nCHAPTER I. Down the Rabbit-Hole\n\nAlice was beginning </p>
			<p class="source-code">  to get very tired of sitting by her sister on the\nbank, </p>
			<p class="source-code">  and of having nothing to do: once or twice she had peeped </p>
			<p class="source-code">  into the\nbook her sister was reading, but it had no pictures </p>
			<p class="source-code">  or conversations in\nit, 'and what is the use of a book,' </p>
			<p class="source-code">  thought Alice 'without pictures or\nconversation?</p>
			<p class="source-code">  '\n\nSo she was considering in her own mind </p>
			<p class="source-code">  (as well as she could, for the\nhot day made her feel </p>
			<p class="source-code">  very sleepy and stupid), whether the pleasure\nof making </p>
			<p class="source-code">  a daisy-chain would be worth the trouble of getting up </p>
			<p class="source-code">  and\npicking the daisies, when suddenly a White Rabbit </p>
			<p class="source-code">  with pink eyes ran\nclose by her.\n\nThere was nothing </p>
			<p class="source-code">  so VERY remarkable in that; nor did Alice think </p>
			<p class="source-code">  it so\nVERY much out of the way to hear the Rabbit"</p>
			<p>By the end of this activity, you will have cleaned and tokenized the data, removed a lot of imperfections, removed stop words and punctuation, and have applied stemming on the data.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Before beginning this activity, make sure you have the <strong class="source-inline">gutenberg</strong> corpus installed and the <strong class="source-inline">alice_raw</strong> variable created, as shown in the previous section titled <em class="italic">Downloading Text Corpora Using NLTK</em>.</p>
			<p>The following are the steps you need to perform:</p>
			<ol>
				<li value="1">Continuing in the same Jupyter Notebook, use the raw text in the <strong class="source-inline">'alice_raw'</strong> variable. Change the raw text to lowercase.</li>
				<li>Tokenize the sentences.</li>
				<li>Import punctuation from the <strong class="source-inline">string</strong> module and the stop words from NLTK.</li>
				<li>Create a variable holding the contextual stop words, that is, <strong class="source-inline">--</strong> and <strong class="source-inline">said</strong>.</li>
				<li>Create a master list for stop words to remove that contain terms from punctuation, NLTK stop words and contextual stop words.</li>
				<li>Define a function to drop these tokens from any input sentence (tokenized).</li>
				<li>Use the <strong class="source-inline">PorterStemmer</strong> algorithm from NLTK to perform stemming on the result.</li>
				<li>Print out the first five sentences from the result.<p class="callout-heading">Note</p><p class="callout">The detailed steps for this activity, along with the solutions and additional commentary, are presented on page 405.</p></li>
			</ol>
			<p>The expected output looks like this:</p>
			<p class="source-code">[['alic', "'s", 'adventur', 'wonderland', 'lewi', 'carrol', </p>
			<p class="source-code">  '1865', 'chapter', 'i.', 'rabbit-hol', 'alic', 'begin', </p>
			<p class="source-code">  'get', 'tire', 'sit', 'sister', 'bank', 'noth', 'twice', </p>
			<p class="source-code">  'peep', 'book', 'sister', 'read', 'pictur', 'convers', </p>
			<p class="source-code">  "'and", 'use', 'book', 'thought', 'alic', "'without", </p>
			<p class="source-code">  'pictur', 'convers'], </p>
			<p class="source-code"> ['consid', 'mind', 'well', 'could', 'hot', 'day', 'made', </p>
			<p class="source-code">  'feel', 'sleepi', 'stupid', 'whether', 'pleasur', 'make', </p>
			<p class="source-code">  'daisy-chain', 'would', 'worth', 'troubl', 'get', 'pick', </p>
			<p class="source-code">  'daisi', 'suddenli', 'white', 'rabbit', </p>
			<p class="source-code">  'pink', 'eye', 'ran', 'close'], </p>
			<p class="source-code"> ['noth', 'remark', 'alic', 'think', 'much', 'way', 'hear', </p>
			<p class="source-code">  'rabbit', 'say', "'oh", 'dear'], </p>
			<p class="source-code"> ['oh', 'dear'], </p>
			<p class="source-code"> ['shall', 'late']]</p>
			<p>Let's take a look at what we have achieved so far and what lies ahead.</p>
			<p>So far, we've learned how to perform text preprocessing – the process of getting the text data ready for our primary analysis/model. We started with raw text data that has, potentially, many imperfections. We learned how to handle many of these imperfections and are now at a juncture where we are comfortable with handling text data and getting it ready for further analysis. This is an important first part in any NLP application. So, we took raw text data and got clean data in return. What's next?</p>
			<p>The next section is a very important one since it has a very strong bearing on the quality of your analysis. It is known as representation. Let's discuss it.</p>
			<h2 id="_idParaDest-124"><a id="_idTextAnchor149"/>Text Representation Considerations</h2>
			<p>We have processed our raw input data into cleaned text. Now, we need to transform this cleaned text into something a predictive model understands. But what does a predictive model understand? Does it understand the different words? Does it read a word as we do? Can it work with the text that we supply to it?</p>
			<p>By now, you understand that models work on numbers. The input to a model is a stream of numbers. It doesn't understand images, but it can work with matrices and numbers representing those images. For handling images, the key idea is to convert them into numbers and generate features out of them. The idea is the same for text: we need to convert the text into numbers, which will act as features for the model.</p>
			<p><strong class="bold">Representation</strong> is all about converting the text into numbers/features that the model understands. Doesn't sound like there is much to it, right? If you think that, then here's something for you to consider: input features are very important for any modeling exercise, and representation is the process of creating those features. It has a very significant effect on the outcome of your model and is a process that you should pay a great deal of attention to.</p>
			<p>How do you go about text representation, then? What's the "best" way to represent text, if there is such a thing at all? Let's discuss a few approaches.</p>
			<h1 id="_idParaDest-125"><a id="_idTextAnchor150"/>Classical Approaches to Text Representation</h1>
			<p>Text representation approaches have evolved significantly over the years, and the advent of neural networks and deep neural networks has made a significant impact on the way we now represent text (more on that later). We have come a long way indeed: from handcrafting features to marking if a certain word is present in the text, to creating powerful representations such as word embeddings. While there are a lot of approaches, some more suitable for the task than the others, we will discuss a few major classical approaches and work with all of them in Python.</p>
			<h2 id="_idParaDest-126"><a id="_idTextAnchor151"/>One-Hot Encoding</h2>
			<p>One-hot encoding is, perhaps, one of the most intuitive approaches toward text representation. A one-hot encoded feature for a word is a binary indicator of the term being present in the text. It's a simple approach that is easy to interpret – the presence or absence of a word. To understand this better, let's consider our sample text before stemming, and let's see how one-hot encoding works for a particular term of interest, say, <strong class="source-inline">nlp</strong>.</p>
			<p>Let's see what the text currently looks like using the following command:</p>
			<p class="source-code">txt_words_nostop</p>
			<p>We can see that the text looks like this: </p>
			<p class="source-code">[['welcome', 'world', 'deep', 'learning', '<strong class="bold">nlp</strong>'],</p>
			<p class="source-code"> ["'re", 'together', "'ll", 'learn', 'together'],</p>
			<p class="source-code"> ['<strong class="bold">nlp</strong>', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'],</p>
			<p class="source-code"> ['let', "'s", 'learn']]</p>
			<p>Our word of interest is <strong class="source-inline">nlp</strong>. Here's what the one-hot encoded feature for it would look like:</p>
			<div>
				<div id="_idContainer125" class="IMG---Figure">
					<img src="image/B15385_04_06.jpg" alt="Figure 4.6: One-hot encoded feature for 'nlp'&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.6: One-hot encoded feature for 'nlp'</p>
			<p>We can see that the feature is <strong class="source-inline">1</strong>, but only for the sentences where the term <strong class="source-inline">nlp</strong> is present and is <strong class="source-inline">0</strong> otherwise. We can make such indicator variables for each word that we're interested in. So, if we're interested in three terms, we make three such features:</p>
			<div>
				<div id="_idContainer126" class="IMG---Figure">
					<img src="image/B15385_04_07.jpg" alt="Figure 4.7: One-hot encoded features for 'nlp', 'deep', and 'learn'&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.7: One-hot encoded features for 'nlp', 'deep', and 'learn'</p>
			<p>Let's recreate this using Python in an exercise.</p>
			<h2 id="_idParaDest-127"><a id="_idTextAnchor152"/>Exercise 4.03: Creating One-Hot Encoding for Our Data</h2>
			<p>In this exercise, we will replicate the preceding example. The target terms are <strong class="source-inline">nlp</strong>, <strong class="source-inline">deep</strong>, and <strong class="source-inline">learn</strong>. We will create a one-hot encoded feature for these terms using our own function and store the result in a <strong class="source-inline">numpy</strong> array.</p>
			<p>Again, we'll be using the <strong class="source-inline">txt_words_nostop</strong> variable we created in <em class="italic">Exercise 4.01</em>, <em class="italic">Tokenizing, Case Normalization, Punctuation, and Stop Word Removal</em>. So, you will need to continue this exercise in the same Jupyter Notebook. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Print out the <strong class="source-inline">txt_words_nostop</strong> variable to see what we're working with:<p class="source-code">print(txt_words_nostop)</p><p>The output will be as follows:</p><p class="source-code">[['welcome', 'world', 'deep', 'learning', 'nlp'], </p><p class="source-code"> ["'re", 'together', "'ll", 'learn', 'together'], </p><p class="source-code"> ['nlp', 'amazing', 'deep', 'learning', 'makes', 'even', 'fun'], </p><p class="source-code"> ['let', "'s", 'learn']]</p></li>
				<li>Define a list with the target terms, that is, <strong class="source-inline">"nlp", "deep", "learn"</strong>:<p class="source-code">target_terms = ["nlp","deep","learn"]</p></li>
				<li>Define a function that takes in a single tokenized sentence and returns a <strong class="source-inline">0</strong> or <strong class="source-inline">1</strong> for each target term, depending on its presence in the text. Note that the length of the output is fixed at <strong class="source-inline">3</strong>:<p class="source-code">def get_onehot(sent):</p><p class="source-code">    return [1 if term in sent else 0 for term in target_terms]</p><p>We're iterating over the target terms and checking if they're available in the input sentence.</p></li>
				<li>Apply the function to each sentence in our text and store the result in a variable:<p class="source-code">one_hot_mat = [get_onehot(sent) for sent in txt_words_nostop]</p></li>
				<li>Import <strong class="source-inline">numpy</strong>, create a <strong class="source-inline">numpy</strong> <strong class="source-inline">array</strong> from the result, and print it:<p class="source-code">import numpy as np</p><p class="source-code">np.array(one_hot_mat)</p><p>The array's output is as follows:</p><p class="source-code">array([[1, 1, 0],</p><p class="source-code">       [0, 0, 1],</p><p class="source-code">       [1, 1, 0],</p><p class="source-code">       [0, 0, 1]])</p></li>
			</ol>
			<p>We can see that the output contains four rows, one for each sentence. Each of the columns in the array contains the one-hot encoding for a target term. The values for "learn" are 0, 1, 0, 1, which is consistent with our expectations.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2VVNEgf">https://packt.live/2VVNEgf</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38Gr54r">https://packt.live/38Gr54r</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>In this exercise, we saw how we can generate features from text using one-hot encoding. The example used a list of target terms. This may work when you have a very specific objective in mind where we know exactly which terms are useful. Indeed, this was the method that was heavily employed until a few years ago, where people handcrafted features from text. In many situations, this is not feasible – since we don't know exactly which terms are important, we use one-hot encoding for a large number of terms (5,000, 10,000, or even more).</p>
			<p>The other aspect is whether the presence/absence of the term enough for most situations. Do we not want to include more information? Maybe the frequency of the term instead of just its presence, or maybe even some other smarter measure? Let's see how this works.</p>
			<h2 id="_idParaDest-128"><a id="_idTextAnchor153"/>Term Frequencies</h2>
			<p>We discussed that one-hot encoding merely indicates the presence or absence of a term. A reasonable argument here is that the frequency of terms is also important. It may be that a term that's present more times in a document is more important for the document. Maybe representing the term by its frequency is a better approach than simply the indicator. The frequency approach is straightforward – for each term, count the number of times it appears in a particular text. If a term is absent from the document/text, it gets a 0. We do this for all the terms in our vocabulary. Therefore, we have as many features as the number of words in our vocabulary (something we can choose; this can be thought of as a hyperparameter). We should note that after the preprocessing steps, the "<em class="italic">terms</em>" that we're working with are tokens that may not be valid words in the language:</p>
			<p class="callout-heading">Note:</p>
			<p class="callout">The <em class="italic">vocabulary</em> is the superset of all the terms that we'll use in the final model. Vocabulary size refers to the number of unique terms in the vocabulary. You could have 20,000 unique terms in the raw text but choose to work with the most frequent 10,000 terms; this would be the effective vocabulary size.</p>
			<p>Consider the following image; if we had <em class="italic">N</em> documents and had <em class="italic">V</em> (<strong class="source-inline">t1, t2, t3</strong> … <strong class="source-inline">t</strong><span class="subscript">V</span>) words in our working vocabulary, the representation for the data would be a matrix of dimensions <em class="italic">N × V</em>.</p>
			<div>
				<div id="_idContainer127" class="IMG---Figure">
					<img src="image/B15385_04_08.jpg" alt="Figure 4.8: Document-term matrix&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.8: Document-term matrix</p>
			<p>This matrix is our <strong class="bold">Document-Term Matrix</strong> (<strong class="bold">DTM</strong>) – where each row represents a document, and each column represents a term. The values in the cells can represent some measure (count, or any other measure). We'll work with term frequencies in this section.</p>
			<p>We could create our own function again, but we have a very handy utility called <strong class="source-inline">'CountVectorizer'</strong> for this in <strong class="source-inline">scikit-learn</strong> that we'll use instead. Let's familiarize ourselves with it, beginning by importing the utility:</p>
			<p class="source-code">from sklearn.feature_extraction.text import CountVectorizer</p>
			<p>The vectorizer can work with raw text, as well as tokenized data (as in our case). To work on the raw text, we would use the following code, where we will create a DTM with term frequencies from our raw text (<strong class="source-inline">txt_sents</strong>).</p>
			<p>Before we begin, let's take a quick look at the contents of this variable: </p>
			<p class="source-code">txt_sents</p>
			<p>The output should be as follows:</p>
			<p class="source-code">['welcome to the world of deep learning for nlp!',</p>
			<p class="source-code"> "we're in this together, and we'll learn together.",</p>
			<p class="source-code"> 'nlp is amazing, and deep learning makes it even more fun.',</p>
			<p class="source-code"> "let's learn!"]</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If the contents of the <strong class="source-inline">txt_sents</strong> variable have been overwritten while working on <em class="italic">Activity 4.01,</em> <em class="italic">Text Preprocessing of the 'Alice in Wonderland' Text</em>, you can revisit <em class="italic">Step 3</em> of <em class="italic">Exercise 4.01,</em> <em class="italic">Tokenizing, Case Normalization, Punctuation, and Stop Word Removal</em> and redefine the variable so that its contents match the preceding output.</p>
			<p>Now, let's instantiate the vectorizer. Note that we need to provide the vocabulary size. This picks the top <em class="italic">n</em> terms from the data for creating the matrix:</p>
			<p class="source-code">vectorizer = CountVectorizer(max_features = 5)</p>
			<p>We chose five terms here; the result will contain five columns in the matrix. Let's train (<strong class="source-inline">'fit'</strong>) the vectorizer on the data:</p>
			<p class="source-code">vectorizer.fit(txt_sents)</p>
			<p>The vectorizer has now learned a vocabulary – the top five terms – and has created an index for each term in the vocabulary. Let's have a look at the vocabulary:</p>
			<p class="source-code">vectorizer.vocabulary_</p>
			<p>The preceding attribute gives us the following output:</p>
			<p class="source-code">{'deep': 1, 'we': 4, 'together': 3, 'and': 0, 'learn': 2}</p>
			<p>We can see which terms have been picked (the top five).</p>
			<p>Now, let's apply the vectorizer to the data to create the DTM. A minor detail: the result from a vectorizer is a sparse matrix. To view it, we'll convert it into an array:</p>
			<p class="source-code">txt_dtm = vectorizer.fit_transform(txt_sents)</p>
			<p class="source-code">txt_dtm.toarray()</p>
			<p>Have a look at the output:</p>
			<p class="source-code">array([[0, 1, 0, 0, 0],</p>
			<p class="source-code">       [1, 0, 1, 2, 2],</p>
			<p class="source-code">       [1, 1, 0, 0, 0],</p>
			<p class="source-code">       [0, 0, 1, 0, 0]], dtype=int64)</p>
			<p>The second document (the second row) has a frequency of <strong class="source-inline">2</strong> for the last two terms. What are those terms? Well, indices 3 and 4 are the terms <strong class="source-inline">'together'</strong> and <strong class="source-inline">'we'</strong>, respectively. Let's print out the original text to see if the output is as expected:</p>
			<p class="source-code">txt_sents</p>
			<p>The output will be as follows:</p>
			<p class="source-code">['welcome to the world of deep learning for nlp!',</p>
			<p class="source-code"> "we're in this together, and we'll learn together.",</p>
			<p class="source-code"> 'nlp is amazing, and deep learning makes it even more fun.',</p>
			<p class="source-code"> "let's learn!"]</p>
			<p>This is just as we expected, and it looks like the count vectorizer works just fine.</p>
			<p>Notice that the vectorizer tokenizes the sentence as well. If you don't want that and want to use preprocessed tokens instead (<strong class="source-inline">txt_words_stem</strong>), you simply need to pass a dummy tokenizer and preprocessor to <strong class="source-inline">CountVectorizer</strong>. Let's see how that works. First, we create a function that does nothing and simply returns the tokenized sentence/document:</p>
			<p class="source-code">def do_nothing(doc):</p>
			<p class="source-code">    return doc</p>
			<p>Now, we'll instantiate the vectorizer to use this function as the preprocessor and tokenizer:</p>
			<p class="source-code">vectorizer = CountVectorizer(max_features=5,</p>
			<p class="source-code">                             preprocessor=do_nothing,</p>
			<p class="source-code">                             tokenizer=do_nothing)</p>
			<p>Here, we're fitting and transforming the data in one step using the <strong class="source-inline">fit_transform()</strong> method from the tokenizer, and then we view the result. The method identifies the unique terms as the <em class="italic">vocabulary</em> when fitting on the data, then counts and returns the occurrence of each term for each document when transforming. Let's see it in action:</p>
			<p class="source-code">txt_dtm = vectorizer.fit_transform(txt_words_stem)</p>
			<p class="source-code">txt_dtm.toarray()</p>
			<p>The output array will be as follows:</p>
			<p class="source-code">array([[0, 1, 1, 1, 0],</p>
			<p class="source-code">       [1, 0, 1, 0, 2],</p>
			<p class="source-code">       [0, 1, 1, 1, 0],</p>
			<p class="source-code">       [0, 0, 1, 0, 0]], dtype=int64)</p>
			<p>We can see that the output is different from that of the previous result. Is this difference expected? To understand, let's look at the vocabulary of the vectorizer:</p>
			<p class="source-code">vectorizer.vocabulary_</p>
			<p>The output will be as follows:</p>
			<p class="source-code">{'deep': 1, 'learn': 2, 'nlp': 3, 'togeth': 4, "'ll": 0}</p>
			<p>We're working with preprocessed data, remember? We have already removed stop words and stemmed. Let's try printing out the input data just to be sure:</p>
			<p class="source-code">txt_words_stem</p>
			<p>The output will be as follows:</p>
			<p class="source-code">[['welcom', 'world', 'deep', 'learn', 'nlp'],</p>
			<p class="source-code"> ["'re", 'togeth', "'ll", 'learn', 'togeth'],</p>
			<p class="source-code"> ['nlp', 'amaz', 'deep', 'learn', 'make', 'even', 'fun'],</p>
			<p class="source-code"> ['let', "'s", 'learn']]</p>
			<p>We can see that the DTM is working according to the new vocabulary and the frequencies that were obtained after preprocessing.</p>
			<p>So, this was the second approach of generating features from text data, that is, using the frequencies of the terms. In the next section, we will look at another very popular method.</p>
			<h2 id="_idParaDest-129"><a id="_idTextAnchor154"/>The TF-IDF Method</h2>
			<p>Does the high frequency of a term in a document mean that the word is very important for the document? Not really. What if that term is very common in all the documents? A common assumption that's employed in text data handling is that if a term is present in all documents, it may not be very differentiating or important for this particular document at hand. Seems like a reasonable assumption. Once more, let's consider the example of the term "<em class="italic">mobile</em>" when we're working with mobile phone reviews. The term is likely to be present in a very high proportion of reviews. But if your task is identifying the sentiment in the reviews, the term may not add a lot of information.</p>
			<p>We can bump up the importance of terms that are present in the document but rare in the entire data and decrease the importance of terms that are present in most of the documents.</p>
			<p>The TF-IDF method, which stands for <em class="italic">Term Frequency – Inverse Document Frequency</em>, defines <strong class="bold">Inverse Document Frequency</strong> (<strong class="bold">IDF</strong>) as follows:</p>
			<div>
				<div id="_idContainer128" class="IMG---Figure">
					<img src="image/B15385_04_09.jpg" alt="Figure 4.9: Equation for TF-IDF&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.9: Equation for TF-IDF</p>
			<p><em class="italic">n</em> is the total number of documents, while <em class="italic">df(t)</em> is the number of documents where the term <em class="italic">t</em> occurs. This is used as a factor to adjust the term frequency. You can see that it works just as we want it to – it increases the importance for rare terms, and decreases it for common terms. Note that there are variations to this formula, but we'll stick with what <strong class="source-inline">scikit-learn</strong> uses. Like <strong class="source-inline">CountVectorizer</strong>, the TF-IDF vectorizer tokenizes the sentence and learns the vocabulary, but instead of returning the counts for a term in a document, it returns the adjusted (IDF-multiplied) counts.</p>
			<p>Now, let's apply this interesting new approach to our data.</p>
			<h2 id="_idParaDest-130"><a id="_idTextAnchor155"/>Exercise 4.04: Document-Term Matrix with TF-IDF</h2>
			<p>In this exercise, we'll implement the third approach to feature generation from text – TF-IDF. We will use scikit-learn's <strong class="source-inline">TfidfVectorizer</strong> utility and create the DTM for our raw text data. Since we're using the <strong class="source-inline">txt_sents</strong> variable we created earlier in this chapter, we'll need to use the same Jupyter Notebook. The text contained in the variable currently looks like this:</p>
			<p class="source-code">['welcome to the world of deep learning for nlp!',</p>
			<p class="source-code"> "we're in this together, and we'll learn together.",</p>
			<p class="source-code"> 'nlp is amazing, and deep learning makes it even more fun.',</p>
			<p class="source-code"> "let's learn!"]</p>
			<p class="callout-heading">Note</p>
			<p class="callout">If the contents of the <strong class="source-inline">txt_sents</strong> variable have been overwritten while working on <em class="italic">Activity 4.01</em>, <em class="italic">Text Preprocessing of the 'Alice in Wonderland' Text</em>, you can revisit <em class="italic">Step 3</em> of <em class="italic">Exercise 4.01</em>,<em class="italic"> Tokenizing, Case Normalization, Punctuation, and Stop Word Removal</em> and redefine the variable so that its contents match the preceding output. </p>
			<p>The following are the steps to perform:</p>
			<ol>
				<li value="1">Import the <strong class="source-inline">TfidfVectorizer</strong> utility from <strong class="source-inline">scikit learn</strong>:<p class="source-code">from sklearn.feature_extraction.text import TfidfVectorizer</p></li>
				<li>Instantiate the <strong class="source-inline">vectorizer</strong> with a vocabulary size of <strong class="source-inline">5</strong>:<p class="source-code">vectorizer_tfidf = TfidfVectorizer(max_features=5)</p></li>
				<li>Fit the <strong class="source-inline">vectorizer</strong> on the raw data of <strong class="source-inline">txt_sents</strong>:<p class="source-code">vectorizer_tfidf.fit(txt_sents)</p></li>
				<li>Print out the vocabulary learned by the <strong class="source-inline">vectorizer</strong>:<p class="source-code">vectorizer_tfidf.vocabulary_</p><p>The trained vocabulary will look as follows:</p><p class="source-code">{'deep': 1, 'we': 4, 'together': 3, 'and': 0, 'learn': 2}</p><p>Notice that the vocabulary is the same as that of the count vectorizer. This is expected. We're not changing the vocabulary; we're adjusting its importance for the documents.</p></li>
				<li>Transform the data using the trained vectorizer:<p class="source-code">txt_tfidf = vectorizer_tfidf.transform(txt_sents)</p></li>
				<li>Print out the resulting DTM:<p class="source-code">txt_tfidf.toarray()</p><p>The output will be as follows:</p><p class="source-code">array([[0.        , 1.        , </p><p class="source-code">        0.        , 0.        , 0.        ],</p><p class="source-code">       [0.25932364, 0.        , 0.25932364, </p><p class="source-code">        0.65783832, 0.65783832],</p><p class="source-code">       [0.70710678, 0.70710678, 0.        , </p><p class="source-code">        0.        , 0.        ],</p><p class="source-code">       [0.        , 0.        , 1.        , </p><p class="source-code">        0.        , 0.        ]])</p><p>We can clearly see that the output values are different from the frequencies and that the values less than 1 indicate that many values have been lowered after multiplication with IDF.</p></li>
				<li>We also need to see the IDF for each of the terms in the vocabulary to check if the factor is indeed working as we expect it to. Print out the IDF values for the terms using the <strong class="source-inline">idf_</strong> attribute:<p class="source-code">vectorizer_tfidf.idf_</p><p>The output will be as follows:</p><p class="source-code">array([1.51082562, 1.51082562, 1.51082562, </p><p class="source-code">       1.91629073, 1.91629073])</p></li>
			</ol>
			<p>The terms <strong class="source-inline">'and'</strong>, <strong class="source-inline">'deep'</strong>, and <strong class="source-inline">'learn'</strong> have a lower IDF, while the terms <strong class="source-inline">'together'</strong> and <strong class="source-inline">'we'</strong> have a higher IDF. This is just as we expect it to be – the terms <strong class="source-inline">'together'</strong> and <strong class="source-inline">'we'</strong> appear only in one document, while the others appear in two. So, the TF-IDF scheme is indeed giving more importance to rarer words.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2VVNEgf">https://packt.live/2VVNEgf</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38Gr54r">https://packt.live/38Gr54r</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>In this exercise, we saw how we can represent text using the TF-IDF approach. We also saw how the approach downweighs more frequent terms by noticing that the IDF values were lower for higher-frequency terms. We ended up with a DTM containing the TF-IDF values for the terms.</p>
			<h2 id="_idParaDest-131"><a id="_idTextAnchor156"/>Summarizing the Classical Approaches</h2>
			<p>We've just looked at three approaches to the classical way of text representation. We began with one-hot encoding, where the feature for a term was simply marking its presence in a document. The count/frequency-based approach attempted to add the importance of the term by using its frequency in a document. The TF-IDF approach attempted to use a "normalized" importance value of the term, factoring in how common the term is across the documents.</p>
			<p>All three approaches that we've discussed so far fall under the "<em class="italic">Bag of Words</em>" approach to representation. So, why are they called "<em class="italic">Bag of Words</em>"? For a couple of reasons. The first reason is that they don't retain the order of the tokens – once in the bag, the position of the terms/tokens doesn't matter. The second reason is that this approach retains features for individual terms. So, for each document, you have, in a way, a "mixed bag of tokens", or a "<em class="italic">bag of words</em>", for simplicity.</p>
			<p>The result from all three approaches had a dimensionality of <em class="italic">N × V</em>, where <em class="italic">N</em> is the number of documents and <em class="italic">V</em> is the vocabulary size. Note that all three representations are very sparse – a typical sentence is very short (maybe 20 words), but the vocabulary size is typically in the thousands, resulting in most of the cells of the DTM being 0. This doesn't seem ideal. Well, there's this and a few more shortcomings of such representations, which we'll see shortly, that have led to the success of deep learning-based methods for representation. Let's discuss these ideas next.</p>
			<h1 id="_idParaDest-132"><a id="_idTextAnchor157"/>Distributed Representation for Text</h1>
			<p>Why are word embeddings so popular? Why are we claiming they are amazingly powerful? What makes them so special? To understand and appreciate word embeddings, we need to acknowledge the shortcomings of the representations so far.</p>
			<p>The terms "<em class="italic">footpath</em>" and "<em class="italic">sidewalk</em>" are synonyms. Do you think the approaches we've discussed so far will be able to capture this information? Well, you could manually go in and replace "<em class="italic">sidewalk</em>" with "<em class="italic">footpath</em>" so that both have the same token eventually, but can you do this for all possible synonyms in the language?</p>
			<p>The terms "<em class="italic">hot</em>" and "<em class="italic">cold</em>" are antonyms. Do the previous Bag-of-Words representations capture this? What about "<em class="italic">dog</em>" being a type of "<em class="italic">animal</em>"? "<em class="italic">Cockpit</em>" being a part of a "<em class="italic">plane</em>"? Differentiating between a dog's bark and a tree's bark? Can you handle all these cases manually?</p>
			<p>All the preceding are examples of "<strong class="bold">semantic associations</strong>" between the terms – simply put, their meanings are linked in some way or another. Bag-of-words representations can't capture these. This is where the notion of distributional semantics comes in. The key idea of distributional semantics is that terms with similar distributions have similar meanings.</p>
			<p>A quick, fun quiz for you: Guess the meaning of the term <em class="italic">furbaby</em> from the following text:</p>
			<p>"<em class="italic">I adopted a young Persian furbaby a month back. Like all furbabys, it loves to scratch its back and hates water, but unlike other furbabys, it miserably fails at catching a mouse.</em>"</p>
			<p>You may have guessed it right: <em class="italic">furbaby</em> is referring to a cat. This was easy, wasn't it?</p>
			<p>But how did you do that? Nowhere has the term cat been used. You looked at the context (the terms surrounding it) for "<em class="italic">furbaby</em>" and, based on your understanding of language and the world, you figured that these terms are generally associated with cats. You intuitively used this notion: words with similar meaning appear in similar contexts. If "<em class="italic">furbaby</em>" and "<em class="italic">cat</em>" appeared in similar contexts, their meaning must be similar.</p>
			<p>"<em class="italic">You shall know a word by the company it keeps.</em>"</p>
			<p>This famous, and now overused, quote by John Firth captures this idea very well. It's overused for all the right reasons. Let's see how this notion is employed in word embeddings.</p>
			<h2 id="_idParaDest-133"><a id="_idTextAnchor158"/>Word Embeddings and Word Vectors</h2>
			<p>Word embeddings are representations of each term as a vector with low dimensionality. The one-hot encoded representation for a term was also a vector, but with dimensionality in the several thousands. Word embeddings/word vectors have much lower dimensionality and result from distributional semantics-based approaches – essentially, the representation captures the notion that words with similar meanings appear in similar contexts.</p>
			<p>Word vectors attempt to capture the meanings of terms. This idea makes them very powerful, assuming, of course, they have been created correctly. With word vectors, vector operations such as adding/subtracting vectors and dot products are possible and have some very interesting meanings. There is also this great property that items with similar meanings are spatially closer. All of this leads to some amazing results.</p>
			<p>A very interesting result is that word vectors can perform well on analogy tasks. Analogy tasks are defined as tasks of the format – "<em class="italic">a</em> is to <em class="italic">b</em> as <em class="italic">x</em> is to ?" – that is, find an entity that has the same relation to <em class="italic">x</em> as <em class="italic">b</em> has to <em class="italic">a</em>. As an example, if you ask "man is to uncle as a woman is to ?", the result would be "<strong class="source-inline">aunt</strong>" (more on this later). You can also find out semantic regularities between terms – the relationships between terms and sets of terms. Let's look at the following figure, which is based on word vectors/embeddings, to understand this better:</p>
			<div>
				<div id="_idContainer129" class="IMG---Figure">
					<img src="image/B15385_04_10.jpg" alt="Figure 4.10: Semantic relationships between terms&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.10: Semantic relationships between terms</p>
			<p>The preceding figure shows some examples. The vectors can have high dimensionality (up to 300 or even more), so dimensionality reduction to two dimensions is performed to visualize it. The dotted connection between the two terms represents the relation between the terms. The direction of this connection is the important bit. On the left panel, we can see that the segment connecting <strong class="source-inline">slow</strong> and <strong class="source-inline">slower</strong> is parallel to the segment connecting <strong class="source-inline">short</strong> and <strong class="source-inline">shorter</strong>. What does this mean? This means that the word embeddings learned that the relationship between <strong class="source-inline">short</strong> and <strong class="source-inline">shorter</strong> is the same as the relationship between <strong class="source-inline">slow</strong> and <strong class="source-inline">slower</strong>. Likewise, the embeddings learned that the relationship between <strong class="source-inline">clearer</strong> and <strong class="source-inline">clearest</strong> is the same as that between <strong class="source-inline">darker</strong> and <strong class="source-inline">darkest</strong>. Pretty neat, right?</p>
			<p>Similarly, the right-hand side of <em class="italic">Figure 4.10</em> shows that the embeddings learned that the relationship between <strong class="source-inline">sir</strong> and <strong class="source-inline">madam</strong> is the same as that between <strong class="source-inline">king</strong> and <strong class="source-inline">queen</strong>. Embeddings have also captured other kinds of semantic associations between terms, which we discussed in the previous section. Isn't that amazing?</p>
			<p>This would not be possible with the approaches we discussed earlier. Word embeddings truly are working around the "meaning" of terms. We hope you can already appreciate the utility and power of word vectors. If you're not convinced yet, we'll soon be working with them and will see this for ourselves.</p>
			<p>To generate word embeddings, there are several algorithms we can use. We will discuss two major approaches and apprise you of some other popular approaches. We will see how the distributional semantics approach is leveraged to derive these word embeddings.</p>
			<h3 id="_idParaDest-134"><a id="_idTextAnchor159"/>word2vec</h3>
			<p>Back in school, to test if we understood the meaning of certain terms, our language teachers used a very popular technique: "<em class="italic">fill in the blanks</em>". Based on the words around it, we needed to identify the word that would best fill that blank. If you understood the meaning well, you would do well. Think about this – isn't this distributional semantics?</p>
			<p>In the <em class="italic">'furbaby'</em> example, you could predict the term <strong class="source-inline">'cat'</strong> because you understood the contexts and terms it occurs with. The exercise was effectively a "fill-in-the-blank" exercise. You could fill the blank only because you understood the meaning of 'cat'.</p>
			<p>If you can predict a term given some context around it, you understand the meaning of the term.</p>
			<p>This simple idea is exactly the formulation behind the <strong class="source-inline">word2vec</strong> algorithm. The <strong class="source-inline">word2vec</strong> algorithm/process is a prediction exercise, a massive "fill-in-the-blank" exercise, in a way. In short, this is what the algorithm does:</p>
			<p>Given the contextual words, predict the missing target word.</p>
			<p>That's all there is to it. The <strong class="source-inline">word2vec</strong> algorithm predicts the target word given the context. Let's understand how these are defined.</p>
			<p>Consider the sentence, "<em class="italic">The Persian cat eats fish and hates bathing</em>." We define context as some fixed number of the terms to the left and right of the target word, which is in the center. For our example, let <strong class="source-inline">'cat'</strong> be the target word, and let's take two words on either side of the target as our context:</p>
			<p> </p>
			<div>
				<div id="_idContainer130" class="IMG---Figure">
					<img src="image/B15385_04_11.jpg" alt="Figure 4.11: &quot;cat&quot; as the target term&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.11: "cat" as the target term</p>
			<p>The five terms together form a <strong class="source-inline">'window'</strong>, which has the target term at the center and the context terms around it. In this example, since we are considering two terms on either side, the window size is 2 (more on these parameters later). The window is a sliding one and moves over the terms in the sentence. The next window would have <strong class="source-inline">'eats'</strong> at the center, with <strong class="source-inline">'cat'</strong> now becoming part of the context: </p>
			<div>
				<div id="_idContainer131" class="IMG---Figure">
					<img src="image/B15385_04_12.jpg" alt="Figure 4.12: Windows for the target term&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.12: Windows for the target term</p>
			<p><strong class="source-inline">C1</strong>, <strong class="source-inline">C2</strong>, <strong class="source-inline">C3</strong>, and <strong class="source-inline">C4</strong> denote the contexts for each window. In <strong class="source-inline">C3</strong>, "fish" is the target word, which is predicted using the terms "cat", "eats", "and", and "hates". The formulation is clear, but how does the model learn the representations? Let's discuss that next:</p>
			<p> </p>
			<div>
				<div id="_idContainer132" class="IMG---Figure">
					<img src="image/B15385_04_13.jpg" alt="Figure 4.13: The CBOW architecture with an example&#13; &#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.13: The CBOW architecture with an example</p>
			<p>The model shown in the preceding figure uses a neural network with a single hidden layer. The output layer is for the target term and is one-hot encoded with <em class="italic">V</em> outputs, one for each term – the predicted term, which is <strong class="source-inline">'cat'</strong>, is the term that gets <strong class="source-inline">'hot'</strong> in the output, of course. The input layer for the context terms is also size <em class="italic">V</em>, but fires for all the terms in the context. The hidden layer is of dimensionality <em class="italic">V x D</em> (where <em class="italic">D</em> is the dimension of the vectors). This hidden layer is where these magical representations of the terms are learned. Note that there is just one input layer, as the weights matrix <em class="italic">W</em> suggests.</p>
			<p>While the network trains, predicting the target word better with each epoch, the parameters of the hidden layer are also getting updates. These parameters are effectively D-length vectors for each term. This D-length vector for a term is our word embedding for that term. After the iterations complete, we would have learned our word embeddings for all the terms in the vocabulary. Pretty neat, isn't it?</p>
			<p>The approach we just discussed is the CBOW approach to training word vectors. The context is a simple bag of words (as we discussed in the previous section on classical approaches; order doesn't matter, remember), hence the name. There is another popular approach, the Skip-gram approach, which inverts the approach of the CBOW method – it predicts the context words based on the center word. This approach may seem a little less intuitive initially but works well. We'll discuss the differences between the results from CBOW and Skip-gram later in this chapter:</p>
			<p> </p>
			<div>
				<div id="_idContainer133" class="IMG---Figure">
					<img src="image/B15385_04_14.jpg" alt="Figure 4.14: The Skip-gram architecture&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.14: The Skip-gram architecture</p>
			<p>Let's see the CBOW approach in action in Python. We'll create our own word embeddings and assess if we can indeed get the amazing results we have been claiming so far.</p>
			<h2 id="_idParaDest-135"><a id="_idTextAnchor160"/>Training Our Own Word Embeddings</h2>
			<p>There are many implementations of the <strong class="source-inline">word2vec</strong> algorithm available in different packages. We will use the implementation in <strong class="bold">Gensim</strong>, which is a great package for many NLP tasks. The implementation of word2vec in Gensim is close to the original paper by <em class="italic">Mikolov et al.</em> in 2013 (<a href="https://arxiv.org/pdf/1301.3781.pdf">https://arxiv.org/pdf/1301.3781.pdf</a>). Gensim also supports other algorithms for word embeddings; more on this later.</p>
			<p>If you don't have Gensim installed, you can install it by typing the following command into a Jupyter Notebook:</p>
			<p class="source-code">!pip install gensim</p>
			<p>The dataset we'll use is the <strong class="source-inline">text8</strong> corpus (<a href="http://mattmahoney.net/dc/textdata.html">http://mattmahoney.net/dc/textdata.html</a>), which is the first billion characters from Wikipedia. It should, therefore, cover data from a variety of topics, not specific to one domain. Conveniently, Gensim has a utility (the <strong class="source-inline">downloader</strong> API) to read in the data. Let's read in the data after importing the <strong class="source-inline">downloader</strong> utility from Gensim:</p>
			<p class="source-code">import gensim.downloader as api</p>
			<p class="source-code">dataset = api.load("text8")</p>
			<p>This step downloads the <strong class="source-inline">text8</strong> data and can take a while, depending on your internet connectivity. Alternatively, the data is available here (<a href="https://packt.live/3gKXU2D">https://packt.live/3gKXU2D</a>) to be downloaded and read using the <strong class="source-inline">Text8Corpus</strong> utility in Gensim, as shown in the following code:</p>
			<p class="source-code">from gensim.models import word2vec</p>
			<p class="source-code">dataset = word2vec.Text8Corpus("text8")</p>
			<p>The <strong class="source-inline">text8</strong> data is now available as an iterable, which can simply be passed to the <strong class="source-inline">word2vec</strong> algorithm.</p>
			<p>Before we train the embeddings, to make the results reproducible, let's set the seed as <strong class="source-inline">1</strong> for random number generation using NumPy:</p>
			<p class="source-code">np.random.seed(1)</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Although we have set the seed, there are more causes for variation of results. Some of this is because of an internal hash seed that the Python version on your system may use. Using multiple cores can also cause the results to vary. In any case, while the values you see may be different, and there could be some changes in the order of the results, the output you see should largely agree with ours. Note that this applies to all the practical elements pertaining to word vectors in this chapter.</p>
			<p>Now, let's train our first word embedding by using the <strong class="source-inline">word2Vec</strong> method:</p>
			<p class="source-code">model = word2vec.Word2Vec(dataset)</p>
			<p>This may take a minute or two, or less, depending on your system. Once complete, we will have our trained word vectors in the model and have access to multiple handy utilities to work with these word vectors. Let's access the word vector/embedding for a term:</p>
			<p class="source-code">print(model.wv["animal"])</p>
			<p>The output will be as follows:</p>
			<div>
				<div id="_idContainer134" class="IMG---Figure">
					<img src="image/B15385_04_15.jpg" alt="Figure 4.15: The embedding for &quot;animal&quot;&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 4.15: The embedding for "animal"</p>
			<p>You have a series of numbers – the vector for the term. Let's find the length of the vector:</p>
			<p class="source-code">len(model.wv["animal"])</p>
			<p>The length of the vector is as follows:</p>
			<p class="source-code">100</p>
			<p>The representation for each term is now a vector of length 100 (the length is a hyperparameter we can change; we used the default setting to get started). The vector for any term can be accessed as we did previously. Among the other handy utilities is the <strong class="source-inline">most_similar()</strong> method, which helps us find the terms that are the most similar to a target term. Let's see it in action:</p>
			<p class="source-code">model.wv.most_similar("animal")</p>
			<p>The output will be as follows:</p>
			<p class="source-code">[('insect', 0.7598186135292053),</p>
			<p class="source-code"> ('animals', 0.729228138923645),</p>
			<p class="source-code"> ('aquatic', 0.6679497957229614),</p>
			<p class="source-code"> ('insects', 0.6522265672683716),</p>
			<p class="source-code"> ('organism', 0.6486647725105286),</p>
			<p class="source-code"> ('mammal', 0.6478426456451416),</p>
			<p class="source-code"> ('eating', 0.6435647010803223),</p>
			<p class="source-code"> ('ants', 0.6415578722953796),</p>
			<p class="source-code"> ('humans', 0.6414449214935303),</p>
			<p class="source-code"> ('feces', 0.6313734650611877)]</p>
			<p>The output is a list of tuples, with each tuple containing the term and its similarity score with the term "animal".</p>
			<p>We can see <strong class="source-inline">insect</strong>, <strong class="source-inline">animals</strong>, <strong class="source-inline">insects</strong>, and <strong class="source-inline">mammal</strong> in the top-most similar terms to "animal". This seems like a very good result, right? But how is the similarity being calculated? Words are being represented by vectors, and the vectors are trying to capture meaning – the similarity between terms is the similarity between their corresponding vectors. The <strong class="source-inline">most_similar()</strong> method uses <strong class="bold">cosine similarity</strong> between the vectors and returns the terms with the highest values. The value corresponding to each term in the result is the cosine similarity with the target word's vector.</p>
			<p>Cosine similarity measures are suitable here as we expect terms that are similar in meaning to be spatially together. Cosine similarity is the cosine of the angle between the vectors. Terms with similar meaning and representation will have an angle closer to 0 and a similarity score closer to 1, whereas terms with completely unrelated meanings will have an angle closer to 90, and a cosine similarity closer to 0. Let's see what the model has learned as top terms related to "happiness":</p>
			<p class="source-code">model.wv.most_similar("happiness")</p>
			<p>The most similar items turn out to be the following (the most similar ones are at the top):</p>
			<p class="source-code">[('humanity', 0.7819231748580933),</p>
			<p class="source-code"> ('perfection', 0.7699881792068481),</p>
			<p class="source-code"> ('pleasure', 0.7422512769699097),</p>
			<p class="source-code"> ('righteousness', 0.7402842044830322),</p>
			<p class="source-code"> ('desires', 0.7374188899993896),</p>
			<p class="source-code"> ('dignity', 0.7189303040504456),</p>
			<p class="source-code"> ('goodness', 0.7103697657585144),</p>
			<p class="source-code"> ('fear', 0.7047020196914673),</p>
			<p class="source-code"> ('mankind', 0.7046756744384766),</p>
			<p class="source-code"> ('salvation', 0.6990150213241577)]</p>
			<p>Humanity, mankind, goodness, righteousness, and compassion -- we have some life lessons here. It seems to have learned what many people seemingly can't figure out in their entire lifetime. Remember, it is just a series of matrix multiplications.</p>
			<h2 id="_idParaDest-136"><a id="_idTextAnchor161"/>Semantic Regularities in Word Embeddings</h2>
			<p>We mentioned earlier that these representations capture regularities in language and are good at solving simple analogy tasks. The offsets between vector embeddings seem to capture the analogical relationship between words. So, for example, <em class="italic">"king" - "man" + "woman"</em> is expected to result in "<em class="italic">queen</em>". Let's see if the model that we trained on the <strong class="source-inline">text8</strong> corpus also understands some regularities.</p>
			<p>We'll use the <strong class="source-inline">most_similar()</strong> method here, which allows us to add and subtract vectors from each other. We'll provide <strong class="source-inline">'king'</strong> and <strong class="source-inline">'woman'</strong> as vectors to add to each other, use <strong class="source-inline">'man'</strong> to subtract from the result, and then check out the five terms that are the most similar to the resulting vector:</p>
			<p class="source-code">model.wv.most_similar(positive=['woman', 'king'], \</p>
			<p class="source-code">                      negative=['man'], topn=5)</p>
			<p>The output will be as follows:</p>
			<p class="source-code">[('queen', 0.6803990602493286),</p>
			<p class="source-code"> ('empress', 0.6331825852394104),</p>
			<p class="source-code"> ('princess', 0.6145625114440918),</p>
			<p class="source-code"> ('throne', 0.6131302714347839),</p>
			<p class="source-code"> ('emperor', 0.6064509153366089)]</p>
			<p>The top result is <strong class="source-inline">'queen'</strong>. Looks like the model is capturing these regularities. Let's try out another example. "Man" is to "uncle" as "woman" is to ? Or in an arithmetic form, what is the vector closest to <em class="italic">uncle - man + woman = ?</em></p>
			<p class="source-code">model.wv.most_similar(positive=['uncle', 'woman'], \</p>
			<p class="source-code">                      negative=['man'], topn=5)</p>
			<p>The following is the output of the preceding code:</p>
			<p class="source-code">[('aunt', 0.8145735263824463),</p>
			<p class="source-code"> ('grandmother', 0.8067640066146851),</p>
			<p class="source-code"> ('niece', 0.7993890643119812),</p>
			<p class="source-code"> ('wife', 0.7965766787528992),</p>
			<p class="source-code"> ('widow', 0.7914236187934875)]</p>
			<p>This seems to be working great. Notice that all the top five results are for the feminine gender. So, we took <strong class="source-inline">uncle</strong>, removed the masculine elements, added feminine elements, and now we have some really good results.</p>
			<p>Let's look at some other examples of vector arithmetic. We can take vectors for two different terms and average them to arrive at vectors for a phrase as well. Let's try it for ourselves.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Taking the average of individual vectors is just one of the many ways of arriving at phrase vectors. Variations range from weighted averages to more complex mathematical functions.</p>
			<h2 id="_idParaDest-137"><a id="_idTextAnchor162"/>Exercise 4.05: Vectors for Phrases</h2>
			<p>In this exercise, we will begin to create vectors for two different phrases, <strong class="source-inline">get happy</strong> and <strong class="source-inline">make merry</strong>, by taking the average of the individual vectors. We will find a similarity between the representations for the phrases. You will need to continue this exercise in the same Jupyter Notebook we have been using throughout this chapter. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Extract the vector for the term "<em class="italic">get</em>" and store it in a variable:<p class="source-code">v1 = model.wv['get']</p></li>
				<li>Extract the vector for the term "<em class="italic">happy</em>" and store it in a variable:<p class="source-code">v2 = model.wv['happy']</p></li>
				<li>Create a vector as the element-wise average of the two vectors, <strong class="source-inline">(v1 + v2)/2</strong>. This is our vector for the entire phrase "get happy":<p class="source-code">res1 = (v1+v2)/2</p></li>
				<li>Similarly, extract vectors for the terms "<em class="italic">make</em>" and "<em class="italic">merry</em>":<p class="source-code">v1 = model.wv['make']</p><p class="source-code">v2 = model.wv['merry']</p></li>
				<li>Create a vector for the phrase by averaging the individual vectors:<p class="source-code">res2 = (v1+v2)/2</p></li>
				<li>Using the <strong class="source-inline">cosine_similarities()</strong> method in the model, find the cosine similarity between the two:<p class="source-code">model.wv.cosine_similarities(res1, [res2])</p><p>The cosine similarity comes out as follows:</p><p class="source-code">array([0.5798107], dtype=float32)</p></li>
			</ol>
			<p>The result is a cosine similarity of about <strong class="source-inline">0.58</strong>, which is positive and much higher than <strong class="source-inline">0</strong>. This means that the model thinks the phrases "get happy" and "make merry" are similar in meaning. Not bad, right? Instead of a simple average, we could use weighted averages, or come up with more sophisticated methods of combining individual vectors.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2VVNEgf">https://packt.live/2VVNEgf</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38Gr54r">https://packt.live/38Gr54r</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>In this exercise, we saw how we could use vector arithmetic to represent phrases, instead of individual terms, and we saw that meaning is still captured. This brings us to a very important lesson – <em class="italic">vector arithmetic on word embeddings has meaning</em>.</p>
			<p>These vector arithmetic operations work on the meaning of terms, resulting in some very interesting results.</p>
			<p>We hope you now appreciate the power of word embeddings. We realize that these results come from just some matrix multiplication and take a minute to train on our dataset. Word embeddings are almost magical, and it is pleasantly surprising how such a simple prediction formulation results in such a powerful representation.</p>
			<p>When we created the word vectors previously, we didn't pay much attention to the controls/parameters. There are many, but only some have a significant impact on the quality of the representations. We will now come to understand the different parameters of the <strong class="source-inline">word2vec</strong> algorithm and see the effect of changing these for ourselves.</p>
			<h3 id="_idParaDest-138"><a id="_idTextAnchor163"/>Effect of Parameters  – "size" of the Vector</h3>
			<p>The <strong class="source-inline">size</strong> parameter of the <strong class="source-inline">word2vec</strong> algorithm is the length of the vector for each term. By default, as we saw earlier, this is 100. We will try reducing this parameter and assess the differences, if any, in the results. Let's retrain the word embeddings, with <strong class="source-inline">size</strong> as 30 this time:</p>
			<p class="source-code">model = word2vec.Word2Vec(dataset, size=30)</p>
			<p>Now, let's check the analogy task from earlier, that is, <strong class="source-inline">king - man + woman</strong>:</p>
			<p class="source-code">model.wv.most_similar(positive=['woman', 'king'], \</p>
			<p class="source-code">                      negative=['man'], topn=5)</p>
			<p>This should give us the following output:</p>
			<p class="source-code">[('emperor', 0.8314059972763062),</p>
			<p class="source-code"> ('empress', 0.8250986933708191),</p>
			<p class="source-code"> ('son', 0.8157491683959961),</p>
			<p class="source-code"> ('prince', 0.8060941696166992),</p>
			<p class="source-code"> ('archbishop', 0.8003251552581787)]</p>
			<p>We can see that <strong class="source-inline">queen</strong> isn't present in the top five results. It looks like by using a very low dimensionality, we aren't capturing enough information in the representation for a term.</p>
			<h3 id="_idParaDest-139"><a id="_idTextAnchor164"/>Effect of Parameters – "window size"</h3>
			<p>The <strong class="source-inline">window size</strong> parameter defines the context; concretely, the window size is the number of terms to the left and to the right of the target term while building the context. The effect of this parameter is not very obvious. The general observation is that when you use a higher window size (say, 20), the top similar terms seem to be terms that are used along with the target term, not necessarily having a similar meaning. On the other hand, reducing the window size (to, say, 2), returns the top terms that are very similar in meaning, and are synonyms in many cases.</p>
			<h2 id="_idParaDest-140"><a id="_idTextAnchor165"/>Skip-gram versus CBOW</h2>
			<p>Choosing between Skip-gram and CBOW as the learning algorithm is exercised by setting <strong class="source-inline">sg = 1</strong> for Skip-gram (the default is <strong class="source-inline">sg = 0</strong>, that is, CBOW). Recall that the Skip-gram approach predicts the context words based on the central target word. This flips the formulation of CBOW, where the context words are used to predict the target word. But how do we choose between the two? What are the benefits of one over the other? To see for ourselves, let's train embeddings using Skip-gram and compare some results with what we had for CBOW. To begin, let's take a particular example for CBOW. First, we'll recreate the CBOW word vectors with the default vector size by not specifying the size parameter. Oeuvre is a term for the body of work of an artist/performer. We'll see the most similar terms for the uncommon term, <strong class="source-inline">oeuvre</strong>:</p>
			<p class="source-code">model = word2vec.Word2Vec(dataset)</p>
			<p class="source-code">model.wv.most_similar("oeuvre", topn=5)</p>
			<p>The following terms come out as the most similar terms:</p>
			<p class="source-code">[('baglione', 0.7203884124755859),</p>
			<p class="source-code"> ('chateaubriand', 0.7119786143302917),</p>
			<p class="source-code"> ('kurosawa', 0.6956337690353394),</p>
			<p class="source-code"> ('swinburne', 0.6926312446594238),</p>
			<p class="source-code"> ('poetess', 0.6910216808319092)]</p>
			<p>We can see that most results are the names of artists (<strong class="source-inline">swinburne</strong>, <strong class="source-inline">kurosawa</strong>, and <strong class="source-inline">baglione</strong>) or food dishes (chateaubriand). None of the top five results are close in meaning to the target term. Now, let's retrain our vectors using the Skip-gram method and see the result on the same task:</p>
			<p class="source-code">model_sg = word2vec.Word2Vec(dataset, sg=1)</p>
			<p class="source-code">model_sg.wv.most_similar("oeuvre", topn=5)</p>
			<p>This gives us the following output:</p>
			<p class="source-code">[('masterful', 0.8347533345222473),</p>
			<p class="source-code"> ('orchestration', 0.8149941563606262),</p>
			<p class="source-code"> ('mussorgsky', 0.8116796016693115),</p>
			<p class="source-code"> ('showcasing', 0.8080146312713623),</p>
			<p class="source-code"> ('lithographs', 0.805435299873352)]</p>
			<p>We can see that the top terms are much closer in meaning (<strong class="source-inline">masterful</strong>, <strong class="source-inline">orchestration</strong>, <strong class="source-inline">showcasing</strong>). So, the Skip-gram method seems to work better for rare words.</p>
			<p>Why is this so? The CBOW method smooths over a lot of the distributional statistics by effectively averaging overall context words (remember, all the context terms together go as an input), while Skip-gram does not. When you have a small dataset, the smoothing that's done by CBOW is desirable. If you have a small/moderately sized dataset, and if you are concerned about the representation of rare terms, then Skip-gram is a good option.</p>
			<h3 id="_idParaDest-141"><a id="_idTextAnchor166"/>Effect of Training Data</h3>
			<p>A very important decision while training your word vectors is the underlying data. The patterns and similarities will be learned from the data you supply to the algorithm, and we expect the model to learn differently from data from different domains, different kinds of settings, and so on. To appreciate this, we load different corpora from different contexts and see how the embeddings vary.</p>
			<p>The Brown corpus is a collection of general text, collected from 15 different topics to make it general (from politics to religion, books to music, and many other themes). It contains 500 text samples and about 1 million words. The "movie" corpus contains movie-review data from IMDb. Both of these are available in NLTK.</p>
			<h2 id="_idParaDest-142"><a id="_idTextAnchor167"/>Exercise 4.06: Training Word Vectors on Different Datasets</h2>
			<p>In this exercise, we will train our own word vectors on the Brown corpus and the IMDb movie reviews corpus. We will assess the differences in the representations learned and the effect of the underlying training data. Follow these steps to complete this exercise:</p>
			<ol>
				<li value="1">Import the Brown and IMDb movie reviews corpus from NLTK:<p class="source-code">nltk.download('brown')</p><p class="source-code">nltk.download('movie_reviews')</p><p class="source-code">from nltk.corpus import brown, movie_reviews</p></li>
				<li>The corpora have a convenient method, <strong class="source-inline">sent()</strong>, to extract the individual sentences and words (tokenized sentences, which can be directly passed to the <strong class="source-inline">word2vec</strong> algorithm). Since both the corpora are rather small, use the Skip-gram method to create the embeddings:<p class="source-code">model_brown = word2vec.Word2Vec(brown.sents(), sg=1)</p><p class="source-code">model_movie = word2vec.Word2Vec(movie_reviews.sents(), sg=1)</p><p>We now have two embeddings that have been learned on different contexts for the same term. Let's see the most similar terms for <strong class="source-inline">money</strong> from the model on the Brown corpus.</p></li>
				<li>Print out the <em class="italic">top five terms </em>most similar to <strong class="source-inline">money</strong> from the model that were learned on the Brown corpus:<p class="source-code">model_brown.wv.most_similar('money', topn=5)</p><p>The following is the output of the preceding code:</p><p class="source-code">[('job', 0.8477444648742676),</p><p class="source-code"> ('care', 0.8424298763275146),</p><p class="source-code"> ('friendship', 0.8394286632537842),</p><p class="source-code"> ('risk', 0.8268661499023438),</p><p class="source-code"> ('permission', 0.8243911862373352)]</p><p>We can see that the top term is <strong class="source-inline">'job'</strong>; fair enough. Let's see what the model learned regarding movie reviews.</p></li>
				<li>Print out the top five terms most similar to <strong class="source-inline">money</strong> from the model that learned from the movie corpus:<p class="source-code">model_movie.wv.most_similar('money', topn=5)</p><p>The following are the top terms:</p><p class="source-code">[('cash', 0.7299771904945374),</p><p class="source-code"> ('ransom', 0.7130625247955322),</p><p class="source-code"> ('record', 0.7028014063835144),</p><p class="source-code"> ('risk', 0.6977001428604126),</p><p class="source-code"> ('paid', 0.6940697431564331)]</p></li>
			</ol>
			<p>The top terms are <strong class="source-inline">cash</strong> and <strong class="source-inline">ransom</strong>. Considering the language being used in movies, and thus in movie reviews, this isn't very surprising.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">To access the source code for this specific section, please refer to <a href="https://packt.live/2VVNEgf">https://packt.live/2VVNEgf</a>.</p>
			<p class="callout">You can also run this example online at <a href="https://packt.live/38Gr54r">https://packt.live/38Gr54r</a>. You must execute the entire Notebook in order to get the desired result.</p>
			<p>In this exercise, we created word vectors using different datasets and saw that the representations for the same terms and the associations that were learned are very affected by the underlying data. So, choose your data wisely.</p>
			<h2 id="_idParaDest-143"><a id="_idTextAnchor168"/>Using Pre-Trained Word Vectors</h2>
			<p>So far, we've trained our own word embeddings using the small datasets we had access to. The folks at the Stanford NLP group have trained word embeddings on 6 billion tokens with 400,000 terms in the vocabulary. Individually, we will not have the resources to handle this scale. Fortunately, the Stanford NLP group has been benevolent enough to make these trained embeddings available to the general public so that people like us can benefit from their work. The trained embeddings are available on the GloVe page (<a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a>).</p>
			<p>A quick note on GloVe: the method that's used for training is slightly different. The objective is modified to make the similar terms occur closer in space, in a little more explicit fashion. You can read about the details on the project page for GloVe (<a href="https://nlp.stanford.edu/projects/glove/">https://nlp.stanford.edu/projects/glove/</a>), which also has a link to the original paper proposing it. The end result, however, is very similar in performance to word2vec.</p>
			<p>We'll download the <strong class="source-inline">glove.6B.zip</strong> file from the GloVe project page. The file contains 50D, 100D, 200D, and 300D vectors. We'll work with the 100D vectors here. Please unzip the file and make sure you have the text files in your working directory. The trained vectors are available as a text file, and the format is slightly different. We'll use the <strong class="source-inline">glove2word2vec</strong> utility that's available in Gensim to convert into a format that Gensim can easily load:</p>
			<p class="source-code">from gensim.scripts.glove2word2vec import glove2word2vec</p>
			<p class="source-code">glove_input_file = 'glove.6B.100d.txt'</p>
			<p class="source-code">word2vec_output_file = 'glove.6B.100d.w2vformat.txt'</p>
			<p class="source-code">glove2word2vec(glove_input_file, word2vec_output_file)</p>
			<p>We specified the input and the output file and ran the <strong class="source-inline">glove2word2vec</strong> utility. As the name suggests, the utility takes in word vectors in GloVe format and converts them into <strong class="source-inline">word2vec</strong> format. After this, the <strong class="source-inline">word2vec</strong> models can understand these embeddings easily. Now, let's load the <strong class="source-inline">keyed</strong> word vectors from the text file (reformatted):</p>
			<p class="source-code">from gensim.models.keyedvectors import KeyedVectors</p>
			<p class="source-code">glove_model = KeyedVectors.load_word2vec_format\</p>
			<p class="source-code">              ("glove.6B.100d.w2vformat.txt", binary=False)</p>
			<p>With this done, we have the GloVe embeddings in the model, along with all the handy utilities we had for the embeddings model from word2vec. Let's check out the top terms similar to <strong class="source-inline">"money"</strong>:</p>
			<p class="source-code">glove_model.most_similar("money", topn=5)</p>
			<p>The output is as follows:</p>
			<p class="source-code">[('funds', 0.8508071899414062),</p>
			<p class="source-code"> ('cash', 0.848483681678772),</p>
			<p class="source-code"> ('fund', 0.7594833374023438),</p>
			<p class="source-code"> ('paying', 0.7415367364883423),</p>
			<p class="source-code"> ('pay', 0.740767240524292)]</p>
			<p>For closure, let's also check how this model performs on the king and queen tasks:</p>
			<p class="source-code">glove_model.most_similar(positive=['woman', 'king'], \</p>
			<p class="source-code">                         negative=['man'], topn=5)</p>
			<p>The following is the output of the preceding code:</p>
			<p class="source-code">[('queen', 0.7698541283607483),</p>
			<p class="source-code"> ('monarch', 0.6843380928039551),</p>
			<p class="source-code"> ('throne', 0.6755737066268921),</p>
			<p class="source-code"> ('daughter', 0.6594556570053101),</p>
			<p class="source-code"> ('princess', 0.6520533561706543)]</p>
			<p>Now that we have these embeddings in a model, we can work with them the same way we worked with the embeddings we created previously and can benefit from the larger dataset and vocabulary and the processing power used by the contributing organization.</p>
			<h2 id="_idParaDest-144"><a id="_idTextAnchor169"/>Bias in Embeddings – A Word of Caution</h2>
			<p>When discussing regularities and analogies, we saw the following example:</p>
			<p><em class="italic">king – man + woman = queen</em></p>
			<p>It's great that the embeddings are capturing these regularities by learning from the text data. Let's try something similar to a profession. Let's see the term closest to <em class="italic">doctor – man + woman</em>:</p>
			<p class="source-code">model.wv.most_similar(positive=['woman', 'doctor'], \</p>
			<p class="source-code">                      negative=['man'], topn=5)</p>
			<p>The output regarding the top five results will be as follows:</p>
			<p class="source-code">[('nurse', 0.6464251279830933),</p>
			<p class="source-code"> ('child', 0.5847542881965637),</p>
			<p class="source-code"> ('teacher', 0.569127082824707),</p>
			<p class="source-code"> ('detective', 0.5451491475105286),</p>
			<p class="source-code"> ('boyfriend', 0.5403486490249634)]</p>
			<p>That's not the kind of result we want. Doctors are males, while females are nurses? Let's try another example. This time, let's try what the model thinks regarding females as corresponding to "smart" for "males":</p>
			<p class="source-code">model.wv.most_similar(positive=['woman', 'smart'], \</p>
			<p class="source-code">                      negative=['man'], topn=5)</p>
			<p>We get the following top five results:</p>
			<p class="source-code">[('cute', 0.6156168580055237),</p>
			<p class="source-code"> ('dumb', 0.6035820245742798),</p>
			<p class="source-code"> ('crazy', 0.5834532976150513),</p>
			<p class="source-code"> ('pet', 0.582811713218689),</p>
			<p class="source-code"> ('fancy', 0.5697714686393738)]</p>
			<p>We can see that the top terms are <strong class="source-inline">'cute'</strong>, <strong class="source-inline">'dumb'</strong>, and <strong class="source-inline">'crazy'</strong>. That's not good at all.</p>
			<p>What's happening here? Is this seemingly great representation approach sexist? Is the word2vec algorithm sexist? There definitely is bias in the resulting word vectors, but think about where the bias is coming from. It's the underlying data that uses <strong class="source-inline">'nurse'</strong> for females in contexts where <strong class="source-inline">'doctor'</strong> is used for males. It is, therefore, the underlying text that contains the bias, not the algorithm.</p>
			<p>This topic has recently gained significant attention, and there is ongoing research around ways to assess and get rid of biases from the learned embeddings, but a good approach is to avoid biases in the data to begin with. If you trained word embeddings on YouTube comments, don't be surprised if they contain all kinds of extreme biases. You're better off avoiding text data that you suspect to have biases.</p>
			<h2 id="_idParaDest-145"><a id="_idTextAnchor170"/>Other Notable Approaches to Word Embeddings</h2>
			<p>We worked with the word2vec approach primarily, and we briefly looked at the GloVe approach. While these are the most popular approaches, there are a few other approaches worth mentioning:</p>
			<p><strong class="bold">FastText</strong>: Created by <strong class="bold">Facebook's AI Research</strong> (<strong class="bold">FAIR</strong>) lab, it uses subword information to enrich the word embeddings. You can read more about it on the official page (<a href="https://research.fb.com/downloads/fasttext/">https://research.fb.com/downloads/fasttext/</a>).</p>
			<p><strong class="bold">WordRank</strong>: Treats the embeddings problem as a word-ranking problem. Its performance is similar to word2vec in several tasks. You can read more about this at <a href="https://arxiv.org/abs/1506.02761">https://arxiv.org/abs/1506.02761</a>.</p>
			<p>Other than these, some popular libraries now have pre-trained embeddings available (SpaCy is a good example). The choices are aplenty. We can't do a detailed treatment of these choices here, but please do explore the options.</p>
			<p>We've discussed a lot of ideas around representation in this chapter. Now, let's implement these ideas with the help of an activity.</p>
			<h2 id="_idParaDest-146"><a id="_idTextAnchor171"/>Activity 4.02: Text Representation for Alice in Wonderland</h2>
			<p>In the previous activity, we tokenized and performed basic preprocessing of the text. In this activity, we will advance this process by using representation approaches for the text. You will create your own embeddings from the data and see the kind of relations we have. You will also utilize pre-trained embeddings to represent the data in the text.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">Note that you'll need to have completed <em class="italic">Activity 4.01</em>, <em class="italic">Text Preprocessing of the 'Alice in Wonderland' Text</em>, to proceed with this activity. In that activity, we performed stop word removal on the text.</p>
			<p>You need to perform the following steps:</p>
			<p>We'll continue using the same Jupyter Notebook that we used for <em class="italic">Activity 4.01</em>, <em class="italic">Text Preprocessing of the 'Alice in Wonderland' Text</em>. We'll work on the result of the stop word removal step we got in that activity (let's say it is stored in a variable called <strong class="source-inline">alice_words_nostop</strong>). Print the first three sentences from the result.</p>
			<ol>
				<li value="1">Import <strong class="source-inline">word2vec</strong> from Gensim and train your word embeddings with default parameters.</li>
				<li>Find the terms most similar to <strong class="source-inline">rabbit</strong>.</li>
				<li>Using a window size 2, retrain the word vectors.</li>
				<li>Find the terms most similar to <strong class="source-inline">rabbit</strong>.</li>
				<li>Retrain the word vectors using the Skip-gram method with a window size of <strong class="source-inline">5</strong>.</li>
				<li>Find the terms most similar to <strong class="source-inline">rabbit</strong>.</li>
				<li>Find the representation for the phrase <strong class="source-inline">white rabbit</strong> by averaging the vectors for <strong class="source-inline">white</strong> and <strong class="source-inline">rabbit</strong>.</li>
				<li>Find the representation for <strong class="source-inline">mad hatter</strong> by averaging the vectors for <strong class="source-inline">mad</strong> and <strong class="source-inline">hatter</strong>.</li>
				<li>Find the cosine similarity between these two phrases.</li>
				<li>Load pre-trained GloVe embeddings of size 100D.</li>
				<li>Find representations for <strong class="source-inline">white rabbit</strong> and <strong class="source-inline">mad hatter</strong>.</li>
				<li>Find the cosine similarity between the two phrases. Has the cosine similarity changed?</li>
			</ol>
			<p>As a result of this activity, we will have our own word vectors that have been trained on "Alice's Adventures in Wonderland" and have representation for the terms available in the text.</p>
			<p class="callout-heading">Note</p>
			<p class="callout">The detailed steps for this activity, along with the solutions and additional commentary, are presented on page 407.</p>
			<h1 id="_idParaDest-147"><a id="_idTextAnchor172"/>Summary</h1>
			<p>In this chapter, we began by discussing the peculiarities of text data and how ambiguity makes NLP difficult. We discussed that there are two key ideas in working with text – preprocessing and representation. We discussed the many tasks involved in preprocessing, that is, getting your data cleaned up and ready for analysis. We saw various approaches to removing imperfections from the data.</p>
			<p>Representation was the next big aspect – we understood the considerations in representing text and converting text into numbers. We looked at various approaches, beginning with classical approaches, which included one-hot encoding, the count-based approach, and the TF-IDF method.</p>
			<p>Word embeddings are a whole new approach to representing text that leverage ideas from distributional semantics – terms that appear in similar contexts have similar meanings. The word2vec algorithm smartly exploits this idea by formulating a prediction problem: predict a target word given the context. It uses a neural network for the prediction and, in the process, learns vector representations for the terms.</p>
			<p>We saw that these representations are amazing as they seem to capture meaning, and simple arithmetic operations gave some very interesting and meaningful results. You can even create representations for phrases or even sentences/documents using word vectors. This sets the stage for later when we use word embeddings in more sophisticated deep learning architectures for NLP.</p>
			<p>In the next chapter, we'll continue our exploration of sequences by applying deep learning approaches such as recurrent neural networks and one-dimensional convolutions to them. </p>
		</div>
		<div>
			<div id="_idContainer136" class="Content">
			</div>
		</div>
		<div>
			<div id="_idContainer137" class="Content">
			</div>
		</div>
	</body></html>