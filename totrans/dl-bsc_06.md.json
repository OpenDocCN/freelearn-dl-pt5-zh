["```py\nclass MulLayer:\n   def __init__ (self): \n      self.x = None\n      self.y = None\n    def forward(self, x, y):\n       self.x = x\n       self.y =y\n       out = x * y\n       return out\n    def backward(self, dout):\n       dx = dout * self.y # Reverse x and y\n       dy = dout * self.x\n       return dx, dy\n```", "```py\napple = 100\napple_num = 2\ntax = 1.1\n# layer\nmul_apple_layer = MulLayer()\nmul_tax_layer = MulLayer()\n# forward\napple_price = mul_apple_layer.forward(apple, apple_num)\nprice = mul_tax_layer.forward(apple_price, tax)\nprint(price) # 220\n```", "```py\n# backward\ndprice = 1\ndapple_price, dtax = mul_tax_layer.backward(dprice)\ndapple, dapple_num = mul_apple_layer.backward(dapple_price)\nprint(dapple, dapple_num, dtax) # 2.2 110 200\n```", "```py\nclass AddLayer:\n   def __init__ (self):\n      pass\n   def forward(self, x, y):\n      out = x + y\n      return out\n   def backward(self, dout): \n      dx = dout * 1\n      dy = dout * 1\n      return dx, dy\n```", "```py\napple = 100\napple_num = 2\norange = 150\norange_num = 3\ntax = 1.1\n# layer\nmul_apple_layer = MulLayer()\nmul_orange_layer = MulLayer()\nadd_apple_orange_layer = AddLayer()\nmul_tax_layer = MulLayer()\n# forward\napple_price = mul_apple_layer.forward(apple, apple_num) #(1)\norange_price = mul_orange_layer.forward(orange, orange_num) #(2)\nall_price = add_apple_orange_layer.forward(apple_price, orange_price) #(3)\nprice = mul_tax_layer.forward(all_price, tax) #(4)\n# backward\ndprice = 1\ndall_price, dtax = mul_tax_layer.backward(dprice) #(4)\ndapple_price, dorange_price = add_apple_orange_layer.backward(dall_price) #(3)\ndorange, dorange_num = mul_orange_layer.backward(dorange_price) #(2)\ndapple, dapple_num = mul_apple_layer.backward(dapple_price) #(1)\nprint(price) # 715\nprint(dapple_num, dapple, dorange, dorange_num, dtax) # 110 2.2 3.3 165 650\n```", "```py\nclass Relu:\n   def __init__ (self): \n      self.mask = None\n   def forward(self, x):\n      self.mask = (x <= 0)\n      out = x.copy()\n      out[self.mask] = 0\n      return out\n   def backward(self, dout):\n      dout[self.mask] = 0\n      dx = dout\n      return dx\n```", "```py\n>>> x = np.array( [[1.0, -0.5], [-2.0, 3.0]] )\n>>> print(x)\n[[  1\\.    -0.5]\n[-2\\.    3\\. ]]\n>>> mask = (x <= 0)\n>>> print(mask)\n[[False True]\n[ True False]] \n```", "```py\nclass Sigmoid:\n   def __init__ (self): \n      self.out = None\n   def forward(self, x):\n      out = 1 / (1 + np.exp(-x)) \n      self.out = out\n   return out\ndef backward(self, dout):\n   dx = dout * (1.0 - self.out) * self.out\n   return dx\n```", "```py\n>>> X = np.random.rand(2) # Input values\n>>> W = np.random.rand(2,3) # Weights\n>>> B = np.random.rand(3) # Biases\n>>>\n>>> X.shape # (2,)\n>>> W.shape # (2, 3)\n>>> B.shape # (3,)\n>>>\n>>> Y = np.dot(X, W) + B\n```", "```py\n>>> X_dot_W = np.array([[0, 0, 0], [10, 10, 10]])\n>>> B = np.array([1, 2, 3])\n>>>\n>>> X_dot_W\narray([[  0,    0,    0],\n    [ 10,  10, 10]])\n>>> X_dot_W + B\narray([[  1,    2,    3],\n    [11,  12, 13]])\n```", "```py\n>>> dY = np.array([[1, 2, 3,], [4, 5, 6]])\n>>> dY\narray([[1,  2,  3],\n[4,  5,  6]])\n>>>\n>>> dB = np.sum(dY, axis=0)\n>>> dB\narray([5,  7,  9])\n```", "```py\nclass Affine:\n    def __init__ (self, W, b): \n       self.W = W\n       self.b = b\n       self.x = None\n       self.dW = None\n       self.db = None\n    def forward(self, x): \n       self.x = x\n       out = np.dot(x, self.W) + self.b\n       return out\n    def backward(self, dout):\n        dx = np.dot(dout, self.W.T)\n       self.dW = np.dot(self.x.T, dout)\n       self.db = np.sum(dout, axis=0)\n       return dx\n```", "```py\nclass SoftmaxWithLoss:\n    def __init__ (self):\n        self.loss = None # Loss\n        self.y = None\t# Output of softmax\n        self.t = None\t# Label data (one-hot vector)\n    def forward(self, x, t):\n        self.t = t\n        self.y = softmax(x)\n        self.loss = cross_entropy_error(self.y, self.t)\n        return self.loss\n    def backward(self, dout=1): \n        batch_size = self.t.shape[0]\n        dx = (self.y - self.t) / batch_size\n        return dx\n```", "```py\nimport sys, os\nsys.path.append(os.pardir)\nimport numpy as np\nfrom common.layers import *\nfrom common.gradient import numerical_gradient\nfrom collections import OrderedDict\nclass TwoLayerNet:\n    def __init__ (self, input_size, hidden_size, output_size,\n            weight_init_std=0.01):\n    # Initialize weights\n    self.params = {}\n    self.params['W1'] = weight_init_std * \\\n                    np.random.randn(input_size, hidden_size)\n    self.params['b1'] = np.zeros(hidden_size)\n    self.params['W2'] = weight_init_std * \\\n                    np.random.randn(hidden_size, output_size)\n    self.params['b2'] = p.zeros(output_size)\n    # Create layers\nself.layers = OrderedDict( )\n    self.layers['Affine1'] = \\\n        Affine(self.params['W1'], self.params['b1'])\nself.layers['Relu1'] = Relu( ) \n    self.layers['Affine2'] = \\\n    Affine(self.params['W2'], self.params['b2'])\n    self.lastLayer = SoftmaxWithLoss( )\n    def predict(self, x):\n        for layer in self.layers.values( ):\nx = layer.forward(x)\n        return x\n    # x: input data, t: label data\n    def loss(self, x, t):\n        y = self.predict(x)\n        return self.lastLayer.forward(y, t)\n    def accuracy (self, x, t): \n        y = self.predict(x)\n        y = np.argmax(y, axis=1)\n        if t.ndim != 1 : t = np.argmax(t, axis=1)\n        accuracy = np.sum(y == t) / float(x.shape[0])\n        return accuracy\n    # x: input data, t: teacher data\n    def numerical_gradient(self, x, t):\n        loss_W = lambda W: self.loss(x, t)\n        grads = {}\n        grads['W1'] = numerical_gradient(loss_W, self.params['W1'])\n        grads['b1'] = numerical_gradient(loss_W, self.params['b1'])\n        grads['W2'] = numerical_gradient(loss_W, self.params['W2'])\n        grads['b2'] = numerical_gradient(loss_W, self.params['b2'])\n        return grads\n    def gradient(self, x, t): \n        # forward\n        self.loss(x, t)\n        # backward\n        dout = 1\n        dout = self.lastLayer.backward(dout)\n        layers = list(self.layers.values( ))\n        layers.reverse( )\nfor layer in layers:\n            dout = layer.backward(dout)\n        # Settings\n        grads = {}\n        grads['W1'] = self.layers['Affine1'].dW\n        grads['b1'] = self.layers['Affine1'].db\n        grads['W2'] = self.layers['Affine2'].dW\n        grads['b2'] = self.layers['Affine2'].db\n        return grads\n```", "```py\nimport sys, os\nsys.path.append(os.pardir)\nimport numpy as np\nfrom dataset.mnist import load_mnist\nfrom two_layer_net import TwoLayerNet\n# Load data\n(x_train, t_train), (x_test, t_test) = \\\n   load_mnist(normalize=True, one_hot_label=True)\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\nx_batch = x_train[:3]\nt_batch = t_train[:3]\ngrad_numerical = network.numerical_gradient(x_batch, t_batch) \ngrad_backprop = network.gradient(x_batch, t_batch)\n# Calculate the average of the absolute errors of individual weights\nfor key in grad_numerical.keys( ):\n   diff = np.average( np.abs(grad_backprop[key] - grad_numerical[key]) )\n   print(key + \":\" + str(diff))\n```", "```py\nb1:9.70418809871e-13\nW2:8.41139039497e-13\nb2:1.1945999745e-10\nW1:2.2232446644e-13\n```", "```py\nimport sys, os\nsys.path.append(os.pardir)\nimport numpy as np\nfrom dataset.mnist import load_mnist\nfrom two_layer_net import TwoLayerNet\n# Load data\n(x_train, t_train), (x_test, t_test) = \\\n    load_mnist(normalize=True, one_hot_label=True)\nnetwork = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)\niters_num = 10000\ntrain_size = x_train.shape[0]\nbatch_size = 100\nlearning_rate = 0.1\ntrain_loss_list = [ ]\ntrain_acc_list = [ ]\ntest_acc_list = [ ]\niter_per_epoch = max(train_size / batch_size, 1)\nfor i in range(iters_num):\n   batch_mask = np.random.choice(train_size, batch_size)\n   x_batch = x_train[batch_mask]\n   t_batch = t_train[batch_mask]\n   # Use backpropagation to obtain a gradient\n   grad = network.gradient(x_batch, t_batch)\n   # Update\n   for key in ('W1', 'b1', 'W2', 'b2'): \n      network.params[key] -= learning_rate * grad[key]\n   loss = network.loss(x_batch, t_batch)\n   train_loss_list.append(loss)\nif i % iter_per_epoch == 0:\n   train_acc = network.accuracy(x_train, t_train)\n   test_acc = network.accuracy(x_test, t_test)\n   train_acc_list.append(train_acc)\n   test_acc_list.append(test_acc)\n   print(train_acc, test_acc)\n```"]