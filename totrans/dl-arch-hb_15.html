<html><head></head><body>
<div id="_idContainer129">
<h1 class="chapter-number" id="_idParaDest-208"><a id="_idTextAnchor217"/><span class="koboSpan" id="kobo.1.1">15</span></h1>
<h1 id="_idParaDest-209"><a id="_idTextAnchor218"/><span class="koboSpan" id="kobo.2.1">Deploying Deep Learning Models to Production</span></h1>
<p><span class="koboSpan" id="kobo.3.1">In the previous chapters, we delved into the intricacies of data </span><a id="_idIndexMarker1077"/><span class="koboSpan" id="kobo.4.1">preparation, </span><strong class="bold"><span class="koboSpan" id="kobo.5.1">deep learning</span></strong><span class="koboSpan" id="kobo.6.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.7.1">DL</span></strong><span class="koboSpan" id="kobo.8.1">) model development, and how to deliver insightful outcomes from our DL models. </span><span class="koboSpan" id="kobo.8.2">Through meticulous data analysis, feature engineering, model optimization, and model analysis, we have learned the techniques to ensure our DL models can perform well and as desired. </span><span class="koboSpan" id="kobo.8.3">As we transition into the next phase of our journey, the focus now shifts toward deploying these DL models in </span><span class="No-Break"><span class="koboSpan" id="kobo.9.1">production environments.</span></span></p>
<p><span class="koboSpan" id="kobo.10.1">Reaching the stage of deploying a DL model to production is a significant accomplishment, considering that most models don’t make it that far. </span><span class="koboSpan" id="kobo.10.2">If your project has reached this milestone, it signifies that you have successfully satisfied stakeholders, presented valuable insights, and performed thorough value and metric analysis. </span><span class="koboSpan" id="kobo.10.3">Congratulations, as you are now one step closer to joining the small percentage of successful projects amidst countless attempts. </span><span class="koboSpan" id="kobo.10.4">It’s worth noting that, according to a 2022 Gartner survey highlighted by VentureBeat, which was executed online from October to December 2021 with 699 respondents from organizations in the US, Germany, and the UK, only around half (54%) of AI models make it into production. </span><span class="koboSpan" id="kobo.10.5">Furthermore, the 2023 State of AI Infrastructure Survey, published by Run AI, an AI resource management solutions provider, reported that in over 88% of the companies surveyed, less than half of the AI models reached the production stage. </span><span class="koboSpan" id="kobo.10.6">This involved 450 industry professionals across the US and Western Europe. </span><span class="koboSpan" id="kobo.10.7">These two surveys emphasize the challenges faced in this process and the significance of reaching </span><span class="No-Break"><span class="koboSpan" id="kobo.11.1">this stage.</span></span></p>
<p><span class="koboSpan" id="kobo.12.1">The ultimate goal here is to make these DL models accessible to end users, in an intuitive way, enabling them to harness the full potential of DL in real-world applications. </span><span class="koboSpan" id="kobo.12.2">In this chapter, we will explore the various strategies, tools, and best practices to seamlessly integrate our DL models into production systems, ensuring scalability, reliability, and ease of use for a diverse range </span><span class="No-Break"><span class="koboSpan" id="kobo.13.1">of users.</span></span></p>
<p><span class="koboSpan" id="kobo.14.1">Specifically, we will be going through the </span><span class="No-Break"><span class="koboSpan" id="kobo.15.1">following topics:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.16.1">Exploring the crucial components for DL </span><span class="No-Break"><span class="koboSpan" id="kobo.17.1">model deployment</span></span></li>
<li><span class="koboSpan" id="kobo.18.1">Identifying key DL model </span><span class="No-Break"><span class="koboSpan" id="kobo.19.1">deployment requirements</span></span></li>
<li><span class="koboSpan" id="kobo.20.1">Choosing the right DL model </span><span class="No-Break"><span class="koboSpan" id="kobo.21.1">deployment options</span></span></li>
<li><span class="koboSpan" id="kobo.22.1">Exploring deployment decisions based on practical </span><span class="No-Break"><span class="koboSpan" id="kobo.23.1">use cases</span></span></li>
<li><span class="koboSpan" id="kobo.24.1">Discovering general recommendations for </span><span class="No-Break"><span class="koboSpan" id="kobo.25.1">DL deployment</span></span></li>
<li><span class="koboSpan" id="kobo.26.1">Deploying a language model with ONNX, TensorRT, and NVIDIA </span><span class="No-Break"><span class="koboSpan" id="kobo.27.1">Triton Server</span></span></li>
</ul>
<h1 id="_idParaDest-210"><a id="_idTextAnchor219"/><span class="koboSpan" id="kobo.28.1">Technical requirements</span></h1>
<p><span class="koboSpan" id="kobo.29.1">We will have a practical topic in the last section of this chapter. </span><span class="koboSpan" id="kobo.29.2">This tutorial requires you to have a Linux machine with an NVIDIA GPU device ideally in Ubuntu with Python 3.10 and the </span><strong class="source-inline"><span class="koboSpan" id="kobo.30.1">nvidia-docker</span></strong><span class="koboSpan" id="kobo.31.1"> tool installed. </span><span class="koboSpan" id="kobo.31.2">Additionally, we will require the following Python libraries to </span><span class="No-Break"><span class="koboSpan" id="kobo.32.1">be installed:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.33.1">numpy</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.34.1">transformers==4.21.3</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.35.1">nvidia-tensorrt==8.4.1.5</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.36.1">torch==1.12.0</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.37.1">transformers-deploy</span></strong></span></li>
<li><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.38.1">tritonclient</span></strong></span></li>
</ul>
<p><span class="koboSpan" id="kobo.39.1">The code files are available on </span><span class="No-Break"><span class="koboSpan" id="kobo.40.1">GitHub: </span></span><a href="https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_15"><span class="No-Break"><span class="koboSpan" id="kobo.41.1">https://github.com/PacktPublishing/The-Deep-Learning-Architect-Handbook/tree/main/CHAPTER_15</span></span></a><span class="No-Break"><span class="koboSpan" id="kobo.42.1">.</span></span></p>
<h1 id="_idParaDest-211"><a id="_idTextAnchor220"/><span class="koboSpan" id="kobo.43.1">Exploring the crucial components for DL model deployment</span></h1>
<p><span class="koboSpan" id="kobo.44.1">So, what does it take to deploy</span><a id="_idIndexMarker1078"/><span class="koboSpan" id="kobo.45.1"> a DL model? </span><span class="koboSpan" id="kobo.45.2">It starts with having a holistic view of each required component and defining clear requirements that guide decision-making for every aspect. </span><span class="koboSpan" id="kobo.45.3">This approach ensures alignment with the business goals and requirements, maximizing the chances of a successful deployment. </span><span class="koboSpan" id="kobo.45.4">With careful planning, diligent execution, and a focus on meeting the needs of the business, you can increase the likelihood of successfully deploying your DL model and unlocking its value for users. </span><span class="koboSpan" id="kobo.45.5">We will start by discovering components that are required to deploy a </span><span class="No-Break"><span class="koboSpan" id="kobo.46.1">DL model.</span></span></p>
<p><span class="koboSpan" id="kobo.47.1">Deploying a DL model to production involves more than just the trained model itself. </span><span class="koboSpan" id="kobo.47.2">It requires seamless collaboration among various components, working together to enable users to effectively extract value from the model’s predictions. </span><span class="koboSpan" id="kobo.47.3">These components are </span><span class="No-Break"><span class="koboSpan" id="kobo.48.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.49.1">Architectural choices</span></strong><span class="koboSpan" id="kobo.50.1">: The overall design and structure of the deployment system. </span><span class="koboSpan" id="kobo.50.2">Should the model be implemented as a separate service, microservice, or directly part of an existing service? </span><span class="koboSpan" id="kobo.50.3">Should the model be hosted on the cloud or on-premises? </span><span class="koboSpan" id="kobo.50.4">Another aspect to consider is whether to use container orchestration platforms, such as Kubernetes, Docker Swarm, or Apache Mesos, to manage and scale deployments of deep learning models in </span><span class="No-Break"><span class="koboSpan" id="kobo.51.1">containerized applications.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.52.1">These platforms provide flexible deployment across multiple machines, cloud providers, or on-premises infrastructure, and can be used in conjunction with other tools and services for efficient management of containerized applications </span><span class="No-Break"><span class="koboSpan" id="kobo.53.1">and microservices.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.54.1">Hardware/physical infrastructure choices</span></strong><span class="koboSpan" id="kobo.55.1">: This involves the decision of which physical computing device you want to use and the choice of each of the components that make up the computing device. </span><span class="koboSpan" id="kobo.55.2">Should the model be run on a CPU, GPU, TPU, or an </span><strong class="bold"><span class="koboSpan" id="kobo.56.1">Artificial Neural Engine</span></strong><span class="koboSpan" id="kobo.57.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.58.1">ANE</span></strong><span class="koboSpan" id="kobo.59.1">) in </span><a id="_idIndexMarker1079"/><span class="No-Break"><span class="koboSpan" id="kobo.60.1">an iPhone?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.61.1">Model packaging methods and frameworks</span></strong><span class="koboSpan" id="kobo.62.1">: This is a component that involves serializing the model’s architecture, weights, and configuration into a file or container format, allowing for easy distribution, deployment, and usage across various environments. </span><span class="koboSpan" id="kobo.62.2">Usually, the DL framework will provide out-of-the-box support for model packaging. </span><span class="koboSpan" id="kobo.62.3">Do you have architectural choices and hardware infrastructure choices or preferences that require the model to be packaged in a </span><span class="No-Break"><span class="koboSpan" id="kobo.63.1">specific way?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.64.1">Model safety, trust, and reliability component</span></strong><span class="koboSpan" id="kobo.65.1">: This encompasses the measures taken to ensure that the deployed model is secure, trustworthy, and reliable in making accurate predictions. </span><span class="koboSpan" id="kobo.65.2">It involves implementing guardrails to prevent misuse or unintended behavior, ensuring model consistency, monitoring model performance, and providing prediction explanations to help users understand and trust the model’s output. </span><span class="koboSpan" id="kobo.65.3">Ensuring data privacy and compliance with relevant regulations is also</span><a id="_idIndexMarker1080"/><span class="koboSpan" id="kobo.66.1"> a critical aspect of this component. </span><span class="koboSpan" id="kobo.66.2">Are there any specific safety, trust, or reliability requirements that must be met for the deployment of your </span><span class="No-Break"><span class="koboSpan" id="kobo.67.1">DL model?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.68.1">Security and authentication methods</span></strong><span class="koboSpan" id="kobo.69.1">: These involve protecting your DL model and its associated infrastructure, as well as controlling access to the model by implementing suitable authentication, authorization, and encryption mechanisms. </span><span class="koboSpan" id="kobo.69.2">This ensures that only authorized users can access and interact with the model, preventing unauthorized access, data breaches, and potential misuse of the model. </span><span class="koboSpan" id="kobo.69.3">What are the necessary security and authentication requirements for your DL model deployment, and how will they be integrated into </span><span class="No-Break"><span class="koboSpan" id="kobo.70.1">your system?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.71.1">Communication protocols</span></strong><span class="koboSpan" id="kobo.72.1">: These define the rules and formats for exchanging data between the deployed model and other components or users in the system. </span><span class="koboSpan" id="kobo.72.2">It involves selecting appropriate protocols based on the requirements, such as latency, reliability, and data formats. </span><span class="koboSpan" id="kobo.72.3">Examples of communication protocols are HTTP, RESTful APIs, gRPC, server-sent events, and WebSockets. </span><span class="koboSpan" id="kobo.72.4">What communication protocols best suit your DL model deployment, and how will they be implemented to enable seamless interaction between the model and </span><span class="No-Break"><span class="koboSpan" id="kobo.73.1">its users?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.74.1">User interfaces</span></strong><span class="koboSpan" id="kobo.75.1">: These are the visual components and interaction methods that allow users or downstream systems to access, interact with, and obtain predictions from the deployed DL model. </span><span class="koboSpan" id="kobo.75.2">User interfaces can be web-based, mobile, desktop applications, APIs, or even voice-activated systems, depending on the use case and target audience. </span><span class="koboSpan" id="kobo.75.3">Designing user-friendly and intuitive interfaces is essential to ensure that users can easily understand and make the most of the model’s predictions. </span><span class="koboSpan" id="kobo.75.4">What type of user interface is best suited for your DL model deployment, and how will it be designed to provide an optimal user experience while effectively delivering the model’s capabilities? </span><span class="koboSpan" id="kobo.75.5">Here are some examples of user interface design challenges specific to </span><span class="No-Break"><span class="koboSpan" id="kobo.76.1">DL models:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.77.1">Visualizing complex data</span></strong><span class="koboSpan" id="kobo.78.1">: DL models often work with multi-dimensional data, which can be challenging to display in a user-friendly manner. </span><span class="koboSpan" id="kobo.78.2">Designers may need to devise innovative ways to visualize and represent such data, making it accessible and understandable </span><span class="No-Break"><span class="koboSpan" id="kobo.79.1">for users.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.80.1">Handling real-time data</span></strong><span class="koboSpan" id="kobo.81.1">: In scenarios where DL models process and analyze real-time data, the user interface must efficiently manage data streaming and updates, ensuring that users receive timely and accurate information without </span><span class="No-Break"><span class="koboSpan" id="kobo.82.1">being overwhelmed.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.83.1">Facilitating model interactions</span></strong><span class="koboSpan" id="kobo.84.1">: Users may need to interact with the DL model to adjust parameters, provide feedback, or request additional information. </span><span class="koboSpan" id="kobo.84.2">Designing intuitive UI elements for these interactions is crucial to ensure users can effectively engage with </span><span class="No-Break"><span class="koboSpan" id="kobo.85.1">the model.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.86.1">Interpreting model output</span></strong><span class="koboSpan" id="kobo.87.1">: DL models can produce complex and nuanced output, which may be challenging for users to understand and act upon. </span><span class="koboSpan" id="kobo.87.2">Designers must find ways to present model predictions in a clear and actionable manner while also providing contextual information to help users interpret </span><span class="No-Break"><span class="koboSpan" id="kobo.88.1">the results.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.89.1">Managing uncertainty</span></strong><span class="koboSpan" id="kobo.90.1">: DL models may produce predictions with varying degrees of confidence or uncertainty. </span><span class="koboSpan" id="kobo.90.2">Designers </span><a id="_idIndexMarker1081"/><span class="koboSpan" id="kobo.91.1">should consider how to communicate this uncertainty to users, ensuring that they are aware of the limitations and potential risks associated with the </span><span class="No-Break"><span class="koboSpan" id="kobo.92.1">model’s output.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.93.1">Accessibility and inclusivity</span></strong><span class="koboSpan" id="kobo.94.1">: User interfaces for DL models should cater to a diverse range of users, including those with different abilities, languages, and cultural backgrounds. </span><span class="koboSpan" id="kobo.94.2">Designers must ensure that their interfaces are accessible and inclusive, taking into account various user needs </span><span class="No-Break"><span class="koboSpan" id="kobo.95.1">and preferences.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.96.1">Monitoring and logging components</span></strong><span class="koboSpan" id="kobo.97.1">: These tools allow you to track the performance, usage, and health of your DL model in real time. </span><span class="koboSpan" id="kobo.97.2">By collecting and analyzing relevant metrics, logs, and alerts, this component helps identify potential issues, optimize the model’s performance, and ensure a stable deployment environment. </span><span class="koboSpan" id="kobo.97.3">How will you implement monitoring and logging to track your DL model’s health and performance, and what metrics will be crucial to measure </span><span class="No-Break"><span class="koboSpan" id="kobo.98.1">its success?</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.99.1">Continuous integration/continuous deployment</span></strong><span class="koboSpan" id="kobo.100.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.101.1">CI/CD</span></strong><span class="koboSpan" id="kobo.102.1">): This </span><a id="_idIndexMarker1082"/><span class="koboSpan" id="kobo.103.1">process involves the automated building, testing, and deployment of your DL model whenever changes are made to its code, data, or infrastructure. </span><span class="koboSpan" id="kobo.103.2">CI/CD streamlines the development life cycle, enabling faster iterations and improvements while ensuring that the deployed model remains up-to-date and reliable. </span><span class="koboSpan" id="kobo.103.3">What CI/CD practices and tools will you adopt to maintain a seamless deployment pipeline for your </span><span class="No-Break"><span class="koboSpan" id="kobo.104.1">DL model?</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.105.1">With numerous options available for each of these components, it’s essential to have a strategy to decide which ones to use. </span><span class="koboSpan" id="kobo.105.2">The first logical step in this process is to define the specific requirements that will guide decision-making for </span><a id="_idIndexMarker1083"/><span class="koboSpan" id="kobo.106.1">each component. </span><span class="koboSpan" id="kobo.106.2">In the next section, we will discuss how to establish these requirements, ensuring that your choices align with your </span><span class="No-Break"><span class="koboSpan" id="kobo.107.1">business goals.</span></span></p>
<h1 id="_idParaDest-212"><a id="_idTextAnchor221"/><span class="koboSpan" id="kobo.108.1">Identifying key DL model deployment requirements</span></h1>
<p><span class="koboSpan" id="kobo.109.1">To determine the most suitable </span><a id="_idIndexMarker1084"/><span class="koboSpan" id="kobo.110.1">deployment strategy from a variety of options, it is essential to identify and define seven key requirements. </span><span class="koboSpan" id="kobo.110.2">These are latency and availability, cost, scalability, model hardware, data privacy, safety, and trust and reliability requirements. </span><span class="koboSpan" id="kobo.110.3">Let’s dive into each of these requirements </span><span class="No-Break"><span class="koboSpan" id="kobo.111.1">in detail:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.112.1">Latency and availability requirements</span></strong><span class="koboSpan" id="kobo.113.1">: These are two closely connected components and should be defined together. </span><span class="koboSpan" id="kobo.113.2">Availability requirements refer to the desired level of uptime and accessibility of the model’s prediction. </span><span class="koboSpan" id="kobo.113.3">Latency requirements refer to the maximum acceptable delay or response time that the models must meet to provide timely predictions or results. </span><span class="koboSpan" id="kobo.113.4">A deployment with a low availability requirement usually can tolerate high latency predictions, and vice versa. </span><span class="koboSpan" id="kobo.113.5">One reason is that a low-latency capable infrastructure can’t ensure low latency if it is not available when model predictions are requested. </span><span class="koboSpan" id="kobo.113.6">However, there are edge cases that can require complete availability and low latency only for a short period but can be unavailable for the rest of the time, which is considered low availability but with low latency requirements. </span><span class="koboSpan" id="kobo.113.7">Here are a few best practices when determining latency and </span><span class="No-Break"><span class="koboSpan" id="kobo.114.1">availability requirements:</span></span><ul><li><span class="koboSpan" id="kobo.115.1">Consider the expectations and needs of the end users or applications utilizing the DL model. </span><span class="koboSpan" id="kobo.115.2">Consult with stakeholders to understand the desired response times and availability levels </span><span class="No-Break"><span class="koboSpan" id="kobo.116.1">they expect.</span></span></li><li><span class="koboSpan" id="kobo.117.1">Assess the impact of latency and availability on the overall system or business process. </span><span class="koboSpan" id="kobo.117.2">Identify critical points where delays can significantly affect user experience or business operations. </span><span class="koboSpan" id="kobo.117.3">Is waiting for a minimum of 1 hour for predictions going to provide the value the </span><span class="No-Break"><span class="koboSpan" id="kobo.118.1">business wanted?</span></span></li><li><span class="koboSpan" id="kobo.119.1">Identify time windows or periods where availability is particularly crucial. </span><span class="koboSpan" id="kobo.119.2">Determine if the DL model needs to be available 24/7 or if there are specific hours or events when high availability </span><span class="No-Break"><span class="koboSpan" id="kobo.120.1">is essential.</span></span></li><li><span class="koboSpan" id="kobo.121.1">Set both the ideal and maximum latency and availability thresholds. </span><span class="koboSpan" id="kobo.121.2">The maximum is usually where a significant value can still be obtained, and an ideal condition would just slightly increase </span><span class="No-Break"><span class="koboSpan" id="kobo.122.1">that value.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.123.1">Cost requirements</span></strong><span class="koboSpan" id="kobo.124.1">: Budget constraints are a critical consideration in any business, and it is essential to determine the maximum cost you are willing to allocate for deploying a machine learning model based on the expected value it will bring. </span><span class="koboSpan" id="kobo.124.2">To ensure that the expenses do not exceed what the organization is willing to invest, it is advisable to conduct a cost-benefit analysis. </span><span class="koboSpan" id="kobo.124.3">This analysis will involve evaluating the cost implications of various components within the deployment infrastructure, including achieving higher levels of latency and availability. </span><span class="koboSpan" id="kobo.124.4">By carefully balancing the desired requirements against the associated infrastructure costs and operational complexities, you can make informed decisions that align with the overall financial goals of your organization while still leveraging the benefits of </span><span class="No-Break"><span class="koboSpan" id="kobo.125.1">machine learning.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.126.1">Scalability requirements</span></strong><span class="koboSpan" id="kobo.127.1">: Scalability is the ability of a deployment infrastructure to handle an increase or decrease in workload demands without compromising performance or quality. </span><span class="koboSpan" id="kobo.127.2">It is essential to determine the scalability requirements of your deep learning model, as this will impact the choice of deployment strategy and infrastructure. </span><span class="koboSpan" id="kobo.127.3">Do you expect the model usage to grow over time? </span><span class="koboSpan" id="kobo.127.4">How fast do you expect it to grow? </span><span class="koboSpan" id="kobo.127.5">Do you need to scale horizontally (adding more instances of the model) or vertically (increasing the resources of existing instances)? </span><span class="koboSpan" id="kobo.127.6">Having an expectation regarding the utilization growth rate will allow you to choose appropriate components and decisions in operationalizing </span><span class="No-Break"><span class="koboSpan" id="kobo.128.1">your model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.129.1">Model hardware requirements</span></strong><span class="koboSpan" id="kobo.130.1">: The choice of hardware for deploying a DL model is crucial as it can significantly impact the performance, latency, and cost of the overall deployment. </span><span class="koboSpan" id="kobo.130.2">To properly identify hardware requirements, consider </span><span class="No-Break"><span class="koboSpan" id="kobo.131.1">the following:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.132.1">Compatibility</span></strong><span class="koboSpan" id="kobo.133.1">: Ensure the chosen hardware is compatible with the frameworks and libraries used to develop the DL model. </span><span class="koboSpan" id="kobo.133.2">This </span><a id="_idIndexMarker1085"/><span class="koboSpan" id="kobo.134.1">includes checking if the hardware can support specific functions, such as GPU acceleration, that may be essential for </span><span class="No-Break"><span class="koboSpan" id="kobo.135.1">model performance.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.136.1">Processing power</span></strong><span class="koboSpan" id="kobo.137.1">: Evaluate the processing power required to efficiently run the model, including the number of cores, memory, and storage. </span><span class="koboSpan" id="kobo.137.2">Consider how the model’s complexity and size may impact </span><span class="No-Break"><span class="koboSpan" id="kobo.138.1">hardware requirements.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.139.1">Power consumption and heat dissipation</span></strong><span class="koboSpan" id="kobo.140.1">: The power consumption and heat dissipation of the chosen hardware can affect the overall operational cost and the environmental footprint of the deployment. </span><span class="koboSpan" id="kobo.140.2">Choose hardware that balances performance with </span><span class="No-Break"><span class="koboSpan" id="kobo.141.1">energy efficiency.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.142.1">Future-proofing</span></strong><span class="koboSpan" id="kobo.143.1">: Consider the expected lifespan of the hardware and its ability to accommodate future updates or improvements to the model. </span><span class="koboSpan" id="kobo.143.2">Opt for hardware that can easily be upgraded or replaced </span><span class="No-Break"><span class="koboSpan" id="kobo.144.1">if necessary.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.145.1">Integration</span></strong><span class="koboSpan" id="kobo.146.1">: Ensure the hardware can be seamlessly integrated with the rest of the deployment infrastructure and any other relevant systems </span><span class="No-Break"><span class="koboSpan" id="kobo.147.1">or components.</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.148.1">By thoroughly assessing model hardware requirements, you can make informed decisions that ensure optimal performance while minimizing costs and potential bottlenecks in your DL </span><span class="No-Break"><span class="koboSpan" id="kobo.149.1">model deployment.</span></span></p></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.150.1">Data privacy requirements</span></strong><span class="koboSpan" id="kobo.151.1">: Ensuring the privacy and security of data used in the DL model and predictions by it is crucial as it can impact the trust and compliance of the deployment. </span><span class="koboSpan" id="kobo.151.2">To identify and address data privacy requirements, consider </span><span class="No-Break"><span class="koboSpan" id="kobo.152.1">the following:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.153.1">Regulatory compliance</span></strong><span class="koboSpan" id="kobo.154.1">: Understand the data protection regulations and industry standards applicable to your organization, such as GDPR, HIPAA, or CCPA. </span><span class="koboSpan" id="kobo.154.2">Ensure that the deployment strategy and infrastructure comply with </span><span class="No-Break"><span class="koboSpan" id="kobo.155.1">these regulations.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.156.1">Data storage and processing locations</span></strong><span class="koboSpan" id="kobo.157.1">: Assess where the data will be stored and processed during the deployment. </span><span class="koboSpan" id="kobo.157.2">Determine if any data residency requirements or restrictions exist, such as the need to store data in a specific </span><span class="No-Break"><span class="koboSpan" id="kobo.158.1">geographic region.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.159.1">Data access controls</span></strong><span class="koboSpan" id="kobo.160.1">: All DL applications should have the requirement to implement appropriate access controls to ensure that only authorized users or systems can access the data. </span><span class="koboSpan" id="kobo.160.2">This includes implementing authentication, authorization, and </span><span class="No-Break"><span class="koboSpan" id="kobo.161.1">encryption mechanisms.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.162.1">Data retention and deletion policies</span></strong><span class="koboSpan" id="kobo.163.1">: Check if there are legal and regulatory requirements for data retention and deletion. </span><span class="koboSpan" id="kobo.163.2">Ensure that the deployment infrastructure supports these policies and allows for secure data disposal </span><span class="No-Break"><span class="koboSpan" id="kobo.164.1">if necessary.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.165.1">Data monitoring and auditing</span></strong><span class="koboSpan" id="kobo.166.1">: Check if there is a need to implement monitoring and auditing mechanisms to track data usage and access throughout </span><span class="No-Break"><span class="koboSpan" id="kobo.167.1">the deployment.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.168.1">Data breach response plan</span></strong><span class="koboSpan" id="kobo.169.1">: Such a plan should include roles and responsibilities, communication channels, and remediation actions. </span><span class="koboSpan" id="kobo.169.2">Check if there is a need to develop a data breach response plan that outlines the steps to be taken when there’s a data breach or a </span><span class="No-Break"><span class="koboSpan" id="kobo.170.1">security incident.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.171.1">Safety requirements</span></strong><span class="koboSpan" id="kobo.172.1">: Reflect on the </span><a id="_idIndexMarker1086"/><span class="koboSpan" id="kobo.173.1">potential legal and ethical boundaries that the model must comply with in the specific region you want to deploy </span><span class="No-Break"><span class="koboSpan" id="kobo.174.1">your model.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.175.1">Trust and reliability requirements</span></strong><span class="koboSpan" id="kobo.176.1">: Trust and reliability for machine learning models refer to the confidence in a model’s consistent performance, accuracy, and adherence to ethical and regulatory standards during its deployment and operation. </span><span class="koboSpan" id="kobo.176.2">Consider these questions when </span><span class="No-Break"><span class="koboSpan" id="kobo.177.1">determining requirements:</span></span><ul><li><span class="koboSpan" id="kobo.178.1">How frequently will the model be updated </span><span class="No-Break"><span class="koboSpan" id="kobo.179.1">or modified?</span></span></li><li><span class="koboSpan" id="kobo.180.1">Is tracking multiple model </span><span class="No-Break"><span class="koboSpan" id="kobo.181.1">versions necessary?</span></span></li><li><span class="koboSpan" id="kobo.182.1">Will the model face concept or data drift in its </span><span class="No-Break"><span class="koboSpan" id="kobo.183.1">operating environment?</span></span></li><li><span class="koboSpan" id="kobo.184.1">How important is efficient error detection </span><span class="No-Break"><span class="koboSpan" id="kobo.185.1">and resolution?</span></span></li><li><span class="koboSpan" id="kobo.186.1">How often will the model receive updates or </span><span class="No-Break"><span class="koboSpan" id="kobo.187.1">new features?</span></span></li><li><span class="koboSpan" id="kobo.188.1">Is adapting to user feedback or changing </span><span class="No-Break"><span class="koboSpan" id="kobo.189.1">requirements essential?</span></span></li><li><span class="koboSpan" id="kobo.190.1">Are there opportunities to leverage advances in DL to improve </span><span class="No-Break"><span class="koboSpan" id="kobo.191.1">the model?</span></span></li><li><span class="koboSpan" id="kobo.192.1">Is maintaining a stable and secure production environment </span><span class="No-Break"><span class="koboSpan" id="kobo.193.1">a priority?</span></span></li><li><span class="koboSpan" id="kobo.194.1">How critical is the model’s performance to its users or </span><span class="No-Break"><span class="koboSpan" id="kobo.195.1">business functions?</span></span></li><li><span class="koboSpan" id="kobo.196.1">Are there strict SLAs or regulatory requirements related </span><span class="No-Break"><span class="koboSpan" id="kobo.197.1">to performance?</span></span></li><li><span class="koboSpan" id="kobo.198.1">Is consistent performance across different environments and </span><span class="No-Break"><span class="koboSpan" id="kobo.199.1">configurations important?</span></span></li><li><span class="koboSpan" id="kobo.200.1">Do the model’s predictions have significant consequences, making consistency essential for user trust </span><span class="No-Break"><span class="koboSpan" id="kobo.201.1">and success?</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.202.1">Some of these requirements are best determined early on in the planning stage. </span><span class="koboSpan" id="kobo.202.2">For instance, defining latency requirements from the outset allows you to select an appropriate model that ensures runtime duration falls within the</span><a id="_idIndexMarker1087"/><span class="koboSpan" id="kobo.203.1"> specified latency constraints. </span><span class="koboSpan" id="kobo.203.2">Having explored the types of requirements that need to be defined and the approximate methods for defining them, we are now prepared to discuss choosing the right </span><span class="No-Break"><span class="koboSpan" id="kobo.204.1">deployment options.</span></span></p>
<h1 id="_idParaDest-213"><a id="_idTextAnchor222"/><span class="koboSpan" id="kobo.205.1">Choosing the right DL model deployment options</span></h1>
<p><span class="koboSpan" id="kobo.206.1">Selecting the right deployment </span><a id="_idIndexMarker1088"/><span class="koboSpan" id="kobo.207.1">options for your DL model is a crucial step in ensuring optimal performance, scalability, and cost-effectiveness. </span><span class="koboSpan" id="kobo.207.2">To assist you in making an informed decision, we will explore recommended options based on different requirements. </span><span class="koboSpan" id="kobo.207.3">These recommendations encompass various aspects, such as hardware and physical infrastructure, monitoring and logging components, and deployment strategies. </span><span class="koboSpan" id="kobo.207.4">By carefully evaluating your model’s characteristics, resource constraints, and desired outcomes, you should be able to identify the most suitable deployment solution that aligns with your objectives while maximizing efficiency and return on investment through this guide. </span><span class="koboSpan" id="kobo.207.5">The tangible deployment components we will explore here are architectural decisions, computing hardware, model packaging and frameworks, communication protocols, and user interfaces. </span><span class="koboSpan" id="kobo.207.6">Let’s dive into each component one by one, starting with </span><span class="No-Break"><span class="koboSpan" id="kobo.208.1">architectural choices.</span></span></p>
<h2 id="_idParaDest-214"><a id="_idTextAnchor223"/><span class="koboSpan" id="kobo.209.1">Architectural choices</span></h2>
<p><span class="koboSpan" id="kobo.210.1">Architectural choices for</span><a id="_idIndexMarker1089"/><span class="koboSpan" id="kobo.211.1"> a machine learning service involve designing the infrastructure, data pipelines, and deployment methods for efficient and reliable operations. </span><span class="koboSpan" id="kobo.211.2">We will start with service </span><span class="No-Break"><span class="koboSpan" id="kobo.212.1">placement considerations:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.213.1">Microservice</span></strong><span class="koboSpan" id="kobo.214.1">: </span><strong class="bold"><span class="koboSpan" id="kobo.215.1">Deploy the Deep Learning</span></strong><span class="koboSpan" id="kobo.216.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.217.1">DL</span></strong><span class="koboSpan" id="kobo.218.1">) model as a small, loosely coupled, and independently deployable service with its own APIs. </span><span class="koboSpan" id="kobo.218.2">A microservice is a software architecture design pattern where an application is structured as a collection of small, loosely coupled, and independently deployable services. </span><span class="koboSpan" id="kobo.218.3">Each microservice is responsible for a specific functionality or domain within the application and communicates with other microservices through well-defined </span><strong class="bold"><span class="koboSpan" id="kobo.219.1">Application Programming Interfaces</span></strong><span class="koboSpan" id="kobo.220.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.221.1">APIs</span></strong><span class="koboSpan" id="kobo.222.1">). </span><span class="koboSpan" id="kobo.222.2">So, when</span><a id="_idIndexMarker1090"/><span class="koboSpan" id="kobo.223.1"> deploying as a microservice, a prerequisite is that other components are also implemented as a microservice. </span><span class="koboSpan" id="kobo.223.2">Its advantages are </span><span class="No-Break"><span class="koboSpan" id="kobo.224.1">as follows:</span></span><ul><li><span class="No-Break"><span class="koboSpan" id="kobo.225.1">Better scalability</span></span></li><li><span class="koboSpan" id="kobo.226.1">Easier updates </span><span class="No-Break"><span class="koboSpan" id="kobo.227.1">and maintenance</span></span></li><li><span class="No-Break"><span class="koboSpan" id="kobo.228.1">Higher resilience</span></span></li><li><span class="koboSpan" id="kobo.229.1">Flexibility in </span><span class="No-Break"><span class="koboSpan" id="kobo.230.1">technology choices</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.231.1">Choose this microservice in the </span><span class="No-Break"><span class="koboSpan" id="kobo.232.1">following circumstances:</span></span></p><ul><li><span class="koboSpan" id="kobo.233.1">When model usage is expected </span><span class="No-Break"><span class="koboSpan" id="kobo.234.1">to grow</span></span></li><li><span class="koboSpan" id="kobo.235.1">When frequent updates </span><span class="No-Break"><span class="koboSpan" id="kobo.236.1">are needed</span></span></li><li><span class="koboSpan" id="kobo.237.1">When integration with various external systems </span><span class="No-Break"><span class="koboSpan" id="kobo.238.1">is required</span></span></li><li><span class="koboSpan" id="kobo.239.1">When high resilience </span><span class="No-Break"><span class="koboSpan" id="kobo.240.1">is crucial</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.241.1">Standalone service:</span></strong><span class="koboSpan" id="kobo.242.1"> Deploy the DL model as a separate, independent service, that is not a microservice. </span><span class="koboSpan" id="kobo.242.2">Consider a movie recommendation application - a microservice approach would be to create a </span><strong class="bold"><span class="koboSpan" id="kobo.243.1">Review Analysis Service</span></strong><span class="koboSpan" id="kobo.244.1"> microservice that processes movie reviews using a DL model. </span><span class="koboSpan" id="kobo.244.2">It has its own API, data storage, and deployment pipeline, operating independently from other services in the application. </span><span class="koboSpan" id="kobo.244.3">For a separate service approach in the same application, a </span><strong class="bold"><span class="koboSpan" id="kobo.245.1">Movie Recommendation Service</span></strong><span class="koboSpan" id="kobo.246.1"> combines user preference management, movie review analysis (using the DL model), and recommendation generation. </span><span class="koboSpan" id="kobo.246.2">It's more monolithic, combining related functionalities, with</span><a id="_idIndexMarker1091"/><span class="koboSpan" id="kobo.247.1"> its own API but no separate microservice for review analysis. </span><span class="koboSpan" id="kobo.247.2">Its advantages are </span><span class="No-Break"><span class="koboSpan" id="kobo.248.1">as follows:</span></span><ul><li><span class="koboSpan" id="kobo.249.1">Easier management and administration </span></li><li><span class="koboSpan" id="kobo.250.1">Better suited for complex applications </span></li><li><span class="koboSpan" id="kobo.251.1">Consolidated resources and data access </span></li><li><span class="koboSpan" id="kobo.252.1">Simplified communication between components </span></li><li><span class="koboSpan" id="kobo.253.1">More predictable performance </span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.254.1">Choose this standalone service in the following circumstances: </span></p><ul><li><span class="koboSpan" id="kobo.255.1">When the application has a limited number of services </span></li><li><span class="koboSpan" id="kobo.256.1">When the model is complex and requires a more monolithic approach </span></li><li><span class="koboSpan" id="kobo.257.1">When the scope of the model does not change frequently </span></li><li><span class="koboSpan" id="kobo.258.1">When a balance between resilience and complexity </span><span class="No-Break"><span class="koboSpan" id="kobo.259.1">is preferred</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.260.1">Part of the existing service</span></strong><span class="koboSpan" id="kobo.261.1">: Integrate the DL model into an existing service of an application or system. </span><span class="koboSpan" id="kobo.261.2">The advantages are </span><span class="No-Break"><span class="koboSpan" id="kobo.262.1">as follows:</span></span><ul><li><span class="No-Break"><span class="koboSpan" id="kobo.263.1">Less complexity</span></span></li><li><span class="No-Break"><span class="koboSpan" id="kobo.264.1">Improved performance</span></span></li><li><span class="koboSpan" id="kobo.265.1">Easier </span><span class="No-Break"><span class="koboSpan" id="kobo.266.1">data synchronization</span></span></li><li><span class="koboSpan" id="kobo.267.1">Potential </span><span class="No-Break"><span class="koboSpan" id="kobo.268.1">cost savings</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.269.1">Choose to integrate with an existing service in the </span><span class="No-Break"><span class="koboSpan" id="kobo.270.1">following circumstances:</span></span></p><ul><li><span class="koboSpan" id="kobo.271.1">When model usage growth </span><span class="No-Break"><span class="koboSpan" id="kobo.272.1">is limited</span></span></li><li><span class="koboSpan" id="kobo.273.1">When infrequent updates or modifications </span><span class="No-Break"><span class="koboSpan" id="kobo.274.1">are needed</span></span></li><li><span class="koboSpan" id="kobo.275.1">When you have limited integration with </span><span class="No-Break"><span class="koboSpan" id="kobo.276.1">external systems</span></span></li><li><span class="koboSpan" id="kobo.277.1">When high resilience is </span><span class="No-Break"><span class="koboSpan" id="kobo.278.1">not crucial</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.279.1">Decide between microservice or integrating with an existing service by considering scalability, update frequency, integration requirements, and resilience. </span><span class="koboSpan" id="kobo.279.2">Align these factors with your specific requirements to make the best decision for your DL model deployment. </span><span class="koboSpan" id="kobo.279.3">Next, we will go through recommendations for choosing the physical </span><span class="No-Break"><span class="koboSpan" id="kobo.280.1">deployment environment:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.281.1">Cloud</span></strong><span class="koboSpan" id="kobo.282.1">: Cloud deployments </span><a id="_idIndexMarker1092"/><span class="koboSpan" id="kobo.283.1">are suitable when you require high availability and can tolerate moderate latency. </span><span class="koboSpan" id="kobo.283.2">They minimize upfront costs and offer flexible pay-as-you-go pricing models. </span><span class="koboSpan" id="kobo.283.3">Cloud-based infrastructure provides virtually unlimited resources, allows for rapid auto-scaling, and typically offers high uptime guarantees and managed services. </span><span class="koboSpan" id="kobo.283.4">However, you need to carefully evaluate the cloud providers’ security offerings and ensure compatibility with your DL framework and libraries. </span><span class="koboSpan" id="kobo.283.5">A few companies that offer GPU are AWS, GCP, Microsoft Azure, and </span><span class="No-Break"><span class="koboSpan" id="kobo.284.1">IBM Cloud.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.285.1">Server on-premises</span></strong><span class="koboSpan" id="kobo.286.1">: Server on-premises deployments give you more control over your hardware and network resources, making them ideal for low latency and high availability within a specific geographical region. </span><span class="koboSpan" id="kobo.286.2">They require an upfront investment in terms of hardware and maintenance but can provide long-term cost savings, especially if you have high and consistent resource demands. </span><span class="koboSpan" id="kobo.286.3">On-premises deployments also offer more control over security measures and data privacy but require more effort in maintaining and updating security measures. </span><span class="koboSpan" id="kobo.286.4">Ensure compatibility with your DL framework </span><span class="No-Break"><span class="koboSpan" id="kobo.287.1">and libraries.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.288.1">Edge on-premises</span></strong><span class="koboSpan" id="kobo.289.1">: Also known as edge computing, this approach processes data close to the source, offering extremely low latency and improved security and data privacy. </span><span class="koboSpan" id="kobo.289.2">Edge deployments are suitable when data processing and storage need to happen close to the source, and they can reduce data transfer costs. </span><span class="koboSpan" id="kobo.289.3">However, managing security across multiple edge devices and ensuring compatibility with your DL framework and libraries can be challenging. </span><span class="koboSpan" id="kobo.289.4">Edge deployments offer scalability in terms of distributing processing across multiple edge devices but may require more management and </span><span class="No-Break"><span class="koboSpan" id="kobo.290.1">maintenance efforts.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.291.1">Next, we will dive into</span><a id="_idIndexMarker1093"/><span class="koboSpan" id="kobo.292.1"> container orchestration platforms, which have a significant impact on how applications and services are designed, deployed, and managed within a system. </span><span class="koboSpan" id="kobo.292.2">A container is a lightweight, standalone, and executable software package that includes everything needed to run a piece of software, including the code, runtime, system tools, libraries, and settings. </span><span class="koboSpan" id="kobo.292.3">Containers are isolated from each other and from the host system, allowing them to run consistently across different computing environments. </span><span class="koboSpan" id="kobo.292.4">There are two main types of container technologies: Docker containers and Linux containers (LXC). </span></p>
<p><span class="koboSpan" id="kobo.293.1">Container orchestration platforms help manage and scale deployments of deep learning models in containerized applications, utilizing technologies such as Docker containers or LXC. </span><span class="koboSpan" id="kobo.293.2">These platforms provide flexible deployment across multiple machines, cloud providers, or on-premises infrastructure. </span><span class="koboSpan" id="kobo.293.3">They can be used in conjunction with other tools and services, enabling efficient management of containerized applications and microservices. </span><span class="koboSpan" id="kobo.293.4">Some popular container orchestration platforms to choose among are: </span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.294.1">Kubernetes (open source):</span></strong><span class="koboSpan" id="kobo.295.1"> Kubernetes is an open source container orchestration platform that automates the deployment, scaling, and management of containerized applications, including deep learning models. </span><span class="koboSpan" id="kobo.295.2">It works with various container technologies, including Docker and LXC. </span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.296.1">Docker Swarm (open source): </span></strong><span class="koboSpan" id="kobo.297.1">Docker Swarm is a native clustering and scheduling tool for Docker containers. </span><span class="koboSpan" id="kobo.297.2">It is tightly integrated with the Docker ecosystem, providing a simple way to deploy and manage containerized applications. </span><span class="koboSpan" id="kobo.297.3">While not as feature-rich as Kubernetes, Docker Swarm is known for its ease of use and faster setup. </span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.298.1">Apache Mesos (open source): </span></strong><span class="koboSpan" id="kobo.299.1">Apache Mesos is a distributed systems kernel that abstracts CPU, memory, and storage resources away from machines, enabling fault-tolerant and elastic distributed systems. </span><span class="koboSpan" id="kobo.299.2">It can be used in conjunction with other frameworks such as Marathon or DC/OS to provide container orchestration capabilities for deploying and managing deep learning models. </span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.300.1">Amazon Elastic Kubernetes Service (EKS) and Amazon Elastic Container Service (ECS) (paid-for services):</span></strong><span class="koboSpan" id="kobo.301.1"> These are </span><a id="_idIndexMarker1094"/><span class="koboSpan" id="kobo.302.1">managed</span><a id="_idIndexMarker1095"/><span class="koboSpan" id="kobo.303.1"> container orchestration services provided by AWS. </span><span class="koboSpan" id="kobo.303.2">EKS is a managed Kubernetes service, while ECS is a proprietary container orchestration platform from AWS. </span><span class="koboSpan" id="kobo.303.3">Both services simplify the deployment, scaling, and management of containerized applications on AWS infrastructure. </span></li>
</ul>
<p><span class="koboSpan" id="kobo.304.1">Choose a container</span><a id="_idIndexMarker1096"/><span class="koboSpan" id="kobo.305.1"> orchestration platform that best suits your deep learning deployment requirements, such as flexibility, scalability, compatibility with your preferred container technology, cloud provider, and integration with other tools and services.</span></p>
<p><span class="koboSpan" id="kobo.306.1">Next, we will dive into architectural trade-offs between real-time and </span><span class="No-Break"><span class="koboSpan" id="kobo.307.1">batch predictions:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.308.1">Real-time predictions</span></strong><span class="koboSpan" id="kobo.309.1">: It’s recommended </span><a id="_idIndexMarker1097"/><span class="koboSpan" id="kobo.310.1">to have the model always loaded in memory to reduce latency and respond quickly to requests. </span><span class="koboSpan" id="kobo.310.2">This setup is suitable for applications where immediate response is critical, such as autonomous vehicles, live chatbots, or fraud detection systems. </span><span class="koboSpan" id="kobo.310.3">Here are some recommendations when using </span><span class="No-Break"><span class="koboSpan" id="kobo.311.1">this option:</span></span><ul><li><span class="koboSpan" id="kobo.312.1">Use a dedicated server or cloud instance with enough memory and processing power to handle the model and </span><span class="No-Break"><span class="koboSpan" id="kobo.313.1">concurrent requests</span></span></li><li><span class="koboSpan" id="kobo.314.1">Optimize the model for inference by using techniques such as quantization, pruning, or </span><span class="No-Break"><span class="koboSpan" id="kobo.315.1">model distillation</span></span></li><li><span class="koboSpan" id="kobo.316.1">Implement a load balancer if necessary to distribute incoming requests across multiple instances of </span><span class="No-Break"><span class="koboSpan" id="kobo.317.1">the model</span></span></li><li><span class="koboSpan" id="kobo.318.1">Monitor resource usage and performance to ensure the system meets real-time requirements and scales </span><span class="No-Break"><span class="koboSpan" id="kobo.319.1">as needed</span></span></li><li><span class="koboSpan" id="kobo.320.1">Have a queue system to ensure workers are not overloaded or implement autoscaling to handle </span><span class="No-Break"><span class="koboSpan" id="kobo.321.1">overload cases</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.322.1">On-demand batch predictions</span></strong><span class="koboSpan" id="kobo.323.1">: Batch predictions are suitable for scenarios where real-time responses are not crucial, and predictions can be processed in groups. </span><span class="koboSpan" id="kobo.323.2">This setup requires extra time to spin up worker infrastructure, initialize the model, and load trained model weights. </span><span class="koboSpan" id="kobo.323.3">Here are some recommendations when using </span><span class="No-Break"><span class="koboSpan" id="kobo.324.1">this option:</span></span><ul><li><span class="koboSpan" id="kobo.325.1">Use a queue system such as RabbitMQ or Amazon SQS to manage incoming </span><span class="No-Break"><span class="koboSpan" id="kobo.326.1">prediction requests</span></span></li><li><span class="koboSpan" id="kobo.327.1">Set up a batch processing system that initializes the model and loads weights when </span><span class="No-Break"><span class="koboSpan" id="kobo.328.1">processing starts</span></span></li><li><span class="koboSpan" id="kobo.329.1">Optimize the batch size to balance processing time and </span><span class="No-Break"><span class="koboSpan" id="kobo.330.1">resource usage</span></span></li><li><span class="koboSpan" id="kobo.331.1">Implement</span><a id="_idIndexMarker1098"/><span class="koboSpan" id="kobo.332.1"> auto-scaling to handle variable workloads and ensure efficient use </span><span class="No-Break"><span class="koboSpan" id="kobo.333.1">of resources</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.334.1">Next, we will explore computer hardware choices </span><span class="No-Break"><span class="koboSpan" id="kobo.335.1">and recommendations.</span></span></p>
<h2 id="_idParaDest-215"><a id="_idTextAnchor224"/><span class="koboSpan" id="kobo.336.1">Computing hardware choices</span></h2>
<p><span class="koboSpan" id="kobo.337.1">Selecting hardware to</span><a id="_idIndexMarker1099"/><span class="koboSpan" id="kobo.338.1"> carry out model computations is all about trading off cost, availability, and runtime. </span><span class="koboSpan" id="kobo.338.2">Let’s explore the different options, along with recommendations on when to opt for </span><span class="No-Break"><span class="koboSpan" id="kobo.339.1">each option:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.340.1">CPU</span></strong><span class="koboSpan" id="kobo.341.1">: CPUs are a versatile </span><a id="_idIndexMarker1100"/><span class="koboSpan" id="kobo.342.1">and cost-effective option for deploying DL models. </span><span class="koboSpan" id="kobo.342.2">They are compatible with most frameworks and libraries and provide decent performance for less complex models. </span><span class="koboSpan" id="kobo.342.3">CPUs are a good choice when cost constraints are a priority, and you don’t require the high processing power that GPUs or </span><span class="No-Break"><span class="koboSpan" id="kobo.343.1">TPUs offer.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.344.1">GPU</span></strong><span class="koboSpan" id="kobo.345.1">: GPUs provide</span><a id="_idIndexMarker1101"/><span class="koboSpan" id="kobo.346.1"> faster processing and better parallelization, significantly reducing latency and improving performance. </span><span class="koboSpan" id="kobo.346.2">They are ideal for complex models that demand high processing power. </span><span class="koboSpan" id="kobo.346.3">GPUs are an excellent choice when you require low latency and high availability, but they come with higher costs compared </span><span class="No-Break"><span class="koboSpan" id="kobo.347.1">to CPUs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.348.1">TPU</span></strong><span class="koboSpan" id="kobo.349.1">: TPUs are</span><a id="_idIndexMarker1102"/><span class="koboSpan" id="kobo.350.1"> specialized hardware designed for machine learning tasks, offering high performance and efficient processing. </span><span class="koboSpan" id="kobo.350.2">They are particularly suitable for large models or computationally intensive tasks. </span><span class="koboSpan" id="kobo.350.3">TPUs are a great option when you need exceptional processing power and low latency but be aware of the potential higher costs and that it is only available in GCP and usable only </span><span class="No-Break"><span class="koboSpan" id="kobo.351.1">in TensorFlow!</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.352.1">Artificial Neural Engines</span></strong><span class="koboSpan" id="kobo.353.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.354.1">ANEs</span></strong><span class="koboSpan" id="kobo.355.1">): ANEs are </span><a id="_idIndexMarker1103"/><span class="koboSpan" id="kobo.356.1">specialized AI accelerators found in devices such as iPhones. </span><span class="koboSpan" id="kobo.356.2">They provide efficient processing for DL tasks on edge devices, offering low latency and energy-efficient performance. </span><span class="koboSpan" id="kobo.356.3">ANEs are a good choice when your application requires user interface requirements on an iPhone, which is an edge device. </span><span class="koboSpan" id="kobo.356.4">Note that it is only compatible with the CoreML framework and that the ONNX weights format is needed to convert weights easily </span><span class="No-Break"><span class="koboSpan" id="kobo.357.1">to CoreML.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.358.1">FPGA</span></strong><span class="koboSpan" id="kobo.359.1">: FPGAs</span><a id="_idIndexMarker1104"/><span class="koboSpan" id="kobo.360.1"> are highly customizable (its hardware circuitry can be programmed!) and energy-efficient hardware and are suitable for deploying DL models that require low latency and adaptability. </span><span class="koboSpan" id="kobo.360.2">The con here is the need to have deep expertise in the FPGA programming language and circuit development to successfully allow inference with a trained neural network efficiently. </span><span class="koboSpan" id="kobo.360.3">This is an </span><a id="_idIndexMarker1105"/><span class="koboSpan" id="kobo.361.1">out-of-bounds device for </span><span class="No-Break"><span class="koboSpan" id="kobo.362.1">most teams.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.363.1">Next, we will explore model packaging and framework choices </span><span class="No-Break"><span class="koboSpan" id="kobo.364.1">and recommendations.</span></span></p>
<h2 id="_idParaDest-216"><a id="_idTextAnchor225"/><span class="koboSpan" id="kobo.365.1">Model packaging and frameworks</span></h2>
<p><span class="koboSpan" id="kobo.366.1">This controls how DL models </span><a id="_idIndexMarker1106"/><span class="koboSpan" id="kobo.367.1">are executed and where recommendations can depend on the compute hardware used, as well as portability and runtime requirements. </span><span class="koboSpan" id="kobo.367.2">Here are some popular examples, along with recommendations on when to </span><span class="No-Break"><span class="koboSpan" id="kobo.368.1">use them:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.369.1">Original framework packaging</span></strong><span class="koboSpan" id="kobo.370.1">: You can take advantage of specific optimizations and features provided by the framework, potentially improving performance. </span><span class="koboSpan" id="kobo.370.2">However, certain cases may require compatibility with specific hardware options, such as using a TPU, which is only supported by the TensorFlow framework, so if you have a TPU and you stick with PyTorch, you will not be able to use </span><span class="No-Break"><span class="koboSpan" id="kobo.371.1">the TPU.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.372.1">Open Neural Network Exchange</span></strong><span class="koboSpan" id="kobo.373.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.374.1">ONNX</span></strong><span class="koboSpan" id="kobo.375.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.376.1">framework</span></strong><span class="koboSpan" id="kobo.377.1">: ONNX provides an open standard for representing DL models, allowing </span><a id="_idIndexMarker1107"/><span class="koboSpan" id="kobo.378.1">you to convert your model to different frameworks and run it on various hardware platforms. </span><span class="koboSpan" id="kobo.378.2">Using ONNX can increase the flexibility and portability of your model, enabling you to choose from a wider range of hardware and infrastructure options. </span><span class="koboSpan" id="kobo.378.3">Moreover, it allows you to leverage optimizations and features provided by different DL frameworks. </span><span class="koboSpan" id="kobo.378.4">A convenient and general solution to address packaging issues is to convert your model into the ONNX format, which can then be easily converted into other formats as needed. </span><span class="koboSpan" id="kobo.378.5">This approach streamlines the process and ensures smooth integration with various hardware and framework options, such as leveraging ANE in an iPhone to accelerate your deep learning model within </span><span class="No-Break"><span class="koboSpan" id="kobo.379.1">an app.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.380.1">ONNX Runtime</span></strong><span class="koboSpan" id="kobo.381.1">: This is an inference accelerator that’s designed to accelerate DL model inference in any hardware by leveraging compute and memory optimizations. </span><span class="koboSpan" id="kobo.381.2">It is faster to run a model in ONNX Runtime than to run it in their native DL framework, such as TensorFlow </span><span class="No-Break"><span class="koboSpan" id="kobo.382.1">or PyTorch.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.383.1">TensorRT</span></strong><span class="koboSpan" id="kobo.384.1">: This is a high-performance DL inference optimizer and runtime/compiler library from NVIDIA that’s designed to accelerate DL model inference on NVIDIA GPUs. </span><span class="koboSpan" id="kobo.384.2">It supports TensorFlow and ONNX and provides easy ways to convert model weights so that they’re compatible with its framework, offering fast and efficient model deployment. </span><span class="koboSpan" id="kobo.384.3">TensorRT allows faster model inference speed in GPUs by collectively tuning the model at a lower level, leveraging different GPU internal hardware capabilities to maximize the model efficiency during inference. </span><span class="koboSpan" id="kobo.384.4">As ONNX weights are compatible with TensorRT, a typical path to convert PyTorch model weights into a TensorRT-compatible weight format is to convert PyTorch model weights into ONNX weights. </span><span class="koboSpan" id="kobo.384.5">On an NVIDIA GPU, TensorRT is known to be faster than </span><span class="No-Break"><span class="koboSpan" id="kobo.385.1">ONNX Runtime.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.386.1">Open Visual Inference &amp; Neural Network Optimization</span></strong><span class="koboSpan" id="kobo.387.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.388.1">OpenVINO</span></strong><span class="koboSpan" id="kobo.389.1">): This is a toolkit from Intel that accelerates DL model</span><a id="_idIndexMarker1108"/><span class="koboSpan" id="kobo.390.1"> inference across Intel hardware, including CPUs, GPUs, and FPGAs. </span><span class="koboSpan" id="kobo.390.2">It supports TensorFlow, ONNX, and other frameworks, offering optimized model deployment in </span><span class="No-Break"><span class="koboSpan" id="kobo.391.1">diverse </span></span><span class="No-Break"><a id="_idIndexMarker1109"/></span><span class="No-Break"><span class="koboSpan" id="kobo.392.1">environments.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.393.1">Next, we will explore communication protocol choices </span><span class="No-Break"><span class="koboSpan" id="kobo.394.1">and recommendations.</span></span></p>
<h2 id="_idParaDest-217"><a id="_idTextAnchor226"/><span class="koboSpan" id="kobo.395.1">Communication protocols to use</span></h2>
<p><span class="koboSpan" id="kobo.396.1">The protocol you should </span><a id="_idIndexMarker1110"/><span class="koboSpan" id="kobo.397.1">use can depend on the runtime requirements, network load requirements, user interface chosen, mode of deployment, and compute requirements. </span><span class="koboSpan" id="kobo.397.2">Here are some examples, along with </span><span class="No-Break"><span class="koboSpan" id="kobo.398.1">their recommendations:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.399.1">MQTT</span></strong><span class="koboSpan" id="kobo.400.1">: Use MQTT when you need</span><a id="_idIndexMarker1111"/><span class="koboSpan" id="kobo.401.1"> a lightweight, low-latency protocol for devices with limited resources, such as IoT devices, and real-time communication and status updates are essential for your application. </span><span class="koboSpan" id="kobo.401.2">Power consumption and heat dissipation are </span><span class="No-Break"><span class="koboSpan" id="kobo.402.1">important factors.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.403.1">HTTP or REST API</span></strong><span class="koboSpan" id="kobo.404.1">: Choose this </span><a id="_idIndexMarker1112"/><span class="koboSpan" id="kobo.405.1">when you require a well-supported and easy-to-implement protocol for web services and data exchange, your application follows a request-response communication pattern, and finally where compliance with data protection regulations and data privacy </span><span class="No-Break"><span class="koboSpan" id="kobo.406.1">is crucial.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.407.1">gRPC</span></strong><span class="koboSpan" id="kobo.408.1">: Opt for gRPC when </span><a id="_idIndexMarker1113"/><span class="koboSpan" id="kobo.409.1">you need a high-performance, low-latency protocol for large-scale distributed systems or microservices, bidirectional streaming, and support for multiple programming languages </span><span class="No-Break"><span class="koboSpan" id="kobo.410.1">are essential.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.411.1">Server-Sent Events</span></strong><span class="koboSpan" id="kobo.412.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.413.1">SSE</span></strong><span class="koboSpan" id="kobo.414.1">)</span><strong class="bold"><span class="koboSpan" id="kobo.415.1"> or WebSockets</span></strong><span class="koboSpan" id="kobo.416.1">: Use them </span><a id="_idIndexMarker1114"/><span class="koboSpan" id="kobo.417.1">when real-time notifications or live updates are critical for your web</span><a id="_idIndexMarker1115"/><span class="koboSpan" id="kobo.418.1"> application. </span><span class="koboSpan" id="kobo.418.2">If you require unidirectional communication between server and client, use SSE. </span><span class="koboSpan" id="kobo.418.3">If you require bidirectional communication between server and client, use WebSockets. </span><span class="koboSpan" id="kobo.418.4">A notable domain that requires these communication protocols is live collaborative tools with machine learning. </span><span class="koboSpan" id="kobo.418.5">Here are </span><span class="No-Break"><span class="koboSpan" id="kobo.419.1">some examples:</span></span><ul><li><span class="koboSpan" id="kobo.420.1">Grammarly </span><span class="No-Break"><span class="koboSpan" id="kobo.421.1">uses Websockets</span></span></li><li><span class="koboSpan" id="kobo.422.1">ChatGPT </span><span class="No-Break"><span class="koboSpan" id="kobo.423.1">uses SSE</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.424.1">Next, we will explore user interface choices </span><span class="No-Break"><span class="koboSpan" id="kobo.425.1">and recommendations.</span></span></p>
<h2 id="_idParaDest-218"><a id="_idTextAnchor227"/><span class="koboSpan" id="kobo.426.1">User interfaces</span></h2>
<p><span class="koboSpan" id="kobo.427.1">When designing user</span><a id="_idIndexMarker1116"/><span class="koboSpan" id="kobo.428.1"> interfaces for machine learning applications, it is essential to consider factors such as user experience, accessibility, responsiveness, and adaptability. </span><span class="koboSpan" id="kobo.428.2">Here are some recommendations for </span><span class="No-Break"><span class="koboSpan" id="kobo.429.1">user interfaces:</span></span></p>
<ul>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.430.1">Web applications</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.431.1">:</span></span><ul><li><span class="koboSpan" id="kobo.432.1">They are suitable </span><a id="_idIndexMarker1117"/><span class="koboSpan" id="kobo.433.1">for cross-platform access as users can access the application through a </span><span class="No-Break"><span class="koboSpan" id="kobo.434.1">web browser</span></span></li><li><span class="koboSpan" id="kobo.435.1">Use popular web development frameworks such as React, Angular, or Vue.js to build responsive and interactive </span><span class="No-Break"><span class="koboSpan" id="kobo.436.1">user interfaces</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.437.1">Example use case</span></strong><span class="koboSpan" id="kobo.438.1">: A sentiment analysis tool that allows users to input text and receive sentiment scores by interacting with a machine learning model through a </span><span class="No-Break"><span class="koboSpan" id="kobo.439.1">web-based interface</span></span></li></ul><p class="list-inset"><span class="koboSpan" id="kobo.440.1">For web applications, you need to also choose a web framework wisely according to the benefits it provides, along with the latency trade-offs. </span><span class="koboSpan" id="kobo.440.2">Refer to https://www.techempower.com/benchmarks/?utm_source=pocket_mylist#section=data-r20&amp;hw=ph&amp;test=db for an estimate of the latency you will get for different web frameworks for a single web </span><span class="No-Break"><span class="koboSpan" id="kobo.441.1">API query.</span></span></p></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.442.1">Mobile applications</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.443.1">:</span></span><ul><li><span class="koboSpan" id="kobo.444.1">Ideal for on-the-go </span><a id="_idIndexMarker1118"/><span class="koboSpan" id="kobo.445.1">access to machine learning features through smartphones </span><span class="No-Break"><span class="koboSpan" id="kobo.446.1">and tablets</span></span></li><li><span class="koboSpan" id="kobo.447.1">Develop native apps for iOS and Android platforms using Swift or Kotlin, or use cross-platform frameworks such as React Native </span><span class="No-Break"><span class="koboSpan" id="kobo.448.1">or Flutter</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.449.1">Example use case</span></strong><span class="koboSpan" id="kobo.450.1">: A mobile app that uses a machine learning model for image recognition to identify plants or animals </span><a id="_idIndexMarker1119"/><span class="koboSpan" id="kobo.451.1">by analyzing </span><span class="No-Break"><span class="koboSpan" id="kobo.452.1">user-captured photos</span></span></li></ul></li>
<li><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.453.1">Desktop applications</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.454.1">:</span></span><ul><li><span class="koboSpan" id="kobo.455.1">Suitable for users </span><a id="_idIndexMarker1120"/><span class="koboSpan" id="kobo.456.1">who require a dedicated, platform-specific application with </span><span class="No-Break"><span class="koboSpan" id="kobo.457.1">offline functionality</span></span></li><li><span class="koboSpan" id="kobo.458.1">Use technologies such as Electron or Qt for cross-platform desktop applications or platform-specific languages such as C# for Windows or Swift </span><span class="No-Break"><span class="koboSpan" id="kobo.459.1">for macOS</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.460.1">Example use case</span></strong><span class="koboSpan" id="kobo.461.1">: A video editing software with built-in machine learning-powered features such as object tracking, automatic color grading, or </span><span class="No-Break"><span class="koboSpan" id="kobo.462.1">scene detection</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.463.1">Voice User </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.464.1">Interfaces</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.465.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.466.1">VUI</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.467.1">):</span></span><ul><li><span class="koboSpan" id="kobo.468.1">Ideal for hands-free</span><a id="_idIndexMarker1121"/><span class="koboSpan" id="kobo.469.1"> interaction with machine learning-powered services through </span><span class="No-Break"><span class="koboSpan" id="kobo.470.1">voice commands</span></span></li><li><span class="koboSpan" id="kobo.471.1">Integrate with popular voice assistant platforms such as Amazon Alexa, Google Assistant, or </span><span class="No-Break"><span class="koboSpan" id="kobo.472.1">Apple Siri</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.473.1">Example use case</span></strong><span class="koboSpan" id="kobo.474.1">: A voice-activated home automation system that uses natural language processing to control smart devices based on </span><span class="No-Break"><span class="koboSpan" id="kobo.475.1">user commands</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.476.1">Conversational </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.477.1">UI (chatbots)</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.478.1">:</span></span><ul><li><span class="koboSpan" id="kobo.479.1">Suitable for engaging users </span><a id="_idIndexMarker1122"/><span class="koboSpan" id="kobo.480.1">more naturally and interactively through text or </span><span class="No-Break"><span class="koboSpan" id="kobo.481.1">voice conversations</span></span></li><li><span class="koboSpan" id="kobo.482.1">Use chatbot development platforms such as Dialogflow, Rasa, or Microsoft </span><span class="No-Break"><span class="koboSpan" id="kobo.483.1">Bot Framework</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.484.1">Example use case</span></strong><span class="koboSpan" id="kobo.485.1">: A customer support chatbot that uses machine learning-powered natural language understanding to answer user queries and </span><span class="No-Break"><span class="koboSpan" id="kobo.486.1">provide assistance</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.487.1">Augmented reality</span></strong><span class="koboSpan" id="kobo.488.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.489.1">AR</span></strong><span class="koboSpan" id="kobo.490.1">) </span><strong class="bold"><span class="koboSpan" id="kobo.491.1">and virtual </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.492.1">reality</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.493.1"> (</span></span><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.494.1">VR</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.495.1">):</span></span><ul><li><span class="koboSpan" id="kobo.496.1">Ideal for immersive</span><a id="_idIndexMarker1123"/><span class="koboSpan" id="kobo.497.1"> and interactive</span><a id="_idIndexMarker1124"/><span class="koboSpan" id="kobo.498.1"> experiences that combine the real and </span><span class="No-Break"><span class="koboSpan" id="kobo.499.1">digital worlds</span></span></li><li><span class="koboSpan" id="kobo.500.1">Use AR/VR development platforms such as Unity or Unreal Engine and integrate machine learning models for object recognition, motion tracking, or </span><span class="No-Break"><span class="koboSpan" id="kobo.501.1">scene understanding</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.502.1">Example use case</span></strong><span class="koboSpan" id="kobo.503.1">: A virtual training simulator that uses deep learning models to analyze and assess user performance in real time. </span><span class="koboSpan" id="kobo.503.2">In this AR/VR application, users can practice various skills, such as medical procedures, mechanical repairs, or emergency response scenarios. </span><span class="koboSpan" id="kobo.503.3">The deep learning model evaluates the user’s actions through visual input, provides instant feedback, and offers personalized guidance for improvement, enhancing the learning experience and accelerating </span><span class="No-Break"><span class="koboSpan" id="kobo.504.1">skill development.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.505.1">API-based user interface</span></strong><span class="koboSpan" id="kobo.506.1">: An API-based user interface provides a flexible and scalable way to integrate your machine learning model with various applications, platforms, and services. </span><span class="koboSpan" id="kobo.506.2">This approach allows developers to build custom user interfaces or incorporate machine learning-powered features into existing applications, expanding the reach and impact of your model. </span><span class="koboSpan" id="kobo.506.3">This is suitable for enabling other applications, systems, or services to access and interact with your machine learning model programmatically. </span><span class="koboSpan" id="kobo.506.4">Two recommendations for this approach are </span><span class="No-Break"><span class="koboSpan" id="kobo.507.1">as follows:</span></span><ul><li><span class="koboSpan" id="kobo.508.1">Use REST, gRPC, SSE, Websockets, or MQTT to create well-structured and documented APIs that expose the machine learning model’s functionality to </span><span class="No-Break"><span class="koboSpan" id="kobo.509.1">external clients</span></span></li><li><span class="koboSpan" id="kobo.510.1">Implement authentication and authorization mechanisms (for example, API keys and OAuth) to ensure secure access to </span><span class="No-Break"><span class="koboSpan" id="kobo.511.1">the API</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.512.1">Example use case</span></strong><span class="koboSpan" id="kobo.513.1">: A sentiment analysis API that allows developers to integrate machine learning-powered sentiment analysis into their applications by sending text data and receiving sentiment scores through </span><span class="No-Break"><span class="koboSpan" id="kobo.514.1">API calls</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.515.1">Choosing the right </span><a id="_idIndexMarker1125"/><span class="koboSpan" id="kobo.516.1">deployment options for your DL model involves carefully evaluating architectural choices, hardware options, communication protocols, and user interfaces that best align with your specific requirements and objectives. </span><span class="koboSpan" id="kobo.516.2">By considering factors such as scalability, update frequency, integration needs, and resilience, you can select the most suitable deployment solution that maximizes efficiency </span><span class="No-Break"><span class="koboSpan" id="kobo.517.1">and ROI.</span></span></p>
<p><span class="koboSpan" id="kobo.518.1">Next, let’s discuss some</span><a id="_idIndexMarker1126"/><span class="koboSpan" id="kobo.519.1"> practical examples for deciding on components when deploying DL models </span><span class="No-Break"><span class="koboSpan" id="kobo.520.1">in production.</span></span></p>
<h1 id="_idParaDest-219"><a id="_idTextAnchor228"/><span class="koboSpan" id="kobo.521.1">Exploring deployment decisions based on practical use cases</span></h1>
<p><span class="koboSpan" id="kobo.522.1">In this section, we will </span><a id="_idIndexMarker1127"/><span class="koboSpan" id="kobo.523.1">explore practical deployment decisions for DL models in production, focusing on two distinct use cases: a sentiment analysis application for an e-commerce company and a face detection and recognition system for security cameras. </span><span class="koboSpan" id="kobo.523.2">By examining these real-world scenarios, we will gain valuable insights into establishing robust deployment strategies tailored to specific needs </span><span class="No-Break"><span class="koboSpan" id="kobo.524.1">and objectives.</span></span></p>
<h2 id="_idParaDest-220"><a id="_idTextAnchor229"/><span class="koboSpan" id="kobo.525.1">Exploring deployment decisions for a sentiment analysis application</span></h2>
<p><span class="koboSpan" id="kobo.526.1">Suppose you are developing</span><a id="_idIndexMarker1128"/><span class="koboSpan" id="kobo.527.1"> a</span><a id="_idIndexMarker1129"/><span class="koboSpan" id="kobo.528.1"> sentiment analysis application to be used by an e-commerce company to analyze customer reviews in real-time. </span><span class="koboSpan" id="kobo.528.2">The system needs to process a large number of reviews every day, and low latency is essential to provide immediate insights for the company. </span><span class="koboSpan" id="kobo.528.3">In this case, your choices could be </span><span class="No-Break"><span class="koboSpan" id="kobo.529.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.530.1">Architectural choice</span></strong><span class="koboSpan" id="kobo.531.1">: As an independent service, as it would allow better scalability and easier updates to handle the growing number </span><span class="No-Break"><span class="koboSpan" id="kobo.532.1">of requests.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.533.1">Hardware/infrastructure choice</span></strong><span class="koboSpan" id="kobo.534.1">: GPU on a cloud service, as it provides better parallelization and processing power for a large number of </span><span class="No-Break"><span class="koboSpan" id="kobo.535.1">simultaneous requests.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.536.1">Model packaging and framework</span></strong><span class="koboSpan" id="kobo.537.1">: ONNX and TensorRT, as they offer efficient model deployment and </span><span class="No-Break"><span class="koboSpan" id="kobo.538.1">inference acceleration.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.539.1">Safety, trust, and reliability</span></strong><span class="koboSpan" id="kobo.540.1">: Implement monitoring for data drift and model performance, regularly retrain the model on updated data, and ensure compliance with data privacy regulations. </span><span class="koboSpan" id="kobo.540.2">For example, anonymize user information and avoid storing </span><strong class="bold"><span class="koboSpan" id="kobo.541.1">personally identifiable information</span></strong><span class="koboSpan" id="kobo.542.1"> (</span><strong class="bold"><span class="koboSpan" id="kobo.543.1">PII</span></strong><span class="koboSpan" id="kobo.544.1">) in the </span><a id="_idIndexMarker1130"/><span class="koboSpan" id="kobo.545.1">analysis as it can infringe upon data protection regulations, such as the GDPR in the European Union or the CCPA in the United States, depending on the country the application is intended to be </span><span class="No-Break"><span class="koboSpan" id="kobo.546.1">deployed in.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.547.1">Communication protocol</span></strong><span class="koboSpan" id="kobo.548.1">: RESTful APIs or gRPC, as they are well-suited for web services and can handle a large number of requests with </span><span class="No-Break"><span class="koboSpan" id="kobo.549.1">low latency.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.550.1">User interface</span></strong><span class="koboSpan" id="kobo.551.1">: A web-based dashboard where the company’s staff can monitor the sentiment analysis results in </span><span class="No-Break"><span class="koboSpan" id="kobo.552.1">real time.</span></span></li>
</ul>
<h2 id="_idParaDest-221"><a id="_idTextAnchor230"/><span class="koboSpan" id="kobo.553.1">Exploring deployment decisions for a face detection and recognition system for security cameras</span></h2>
<p><span class="koboSpan" id="kobo.554.1">Suppose you are building </span><a id="_idIndexMarker1131"/><span class="koboSpan" id="kobo.555.1">an object </span><a id="_idIndexMarker1132"/><span class="koboSpan" id="kobo.556.1">detection system for security cameras that need to detect intruders in real time. </span><span class="koboSpan" id="kobo.556.2">In this case, your choices could be </span><span class="No-Break"><span class="koboSpan" id="kobo.557.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.558.1">Architectural choice</span></strong><span class="koboSpan" id="kobo.559.1">: Edge on-premises, as it provides low latency and improved security by processing data close to the source. </span><span class="koboSpan" id="kobo.559.2">This choice also reduces the time needed for data to travel through the network, as no video streaming to some cloud server </span><span class="No-Break"><span class="koboSpan" id="kobo.560.1">is needed.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.561.1">Hardware/infrastructure choice</span></strong><span class="koboSpan" id="kobo.562.1">: GPU or TPU on the edge device, depending on the compatibility with the DL framework and the </span><span class="No-Break"><span class="koboSpan" id="kobo.563.1">model’s complexity.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.564.1">Model packaging and framework</span></strong><span class="koboSpan" id="kobo.565.1">: ONNX and TensorRT, as they offer efficient model deployment and </span><span class="No-Break"><span class="koboSpan" id="kobo.566.1">inference acceleration.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.567.1">Safety, trust, and reliability</span></strong><span class="koboSpan" id="kobo.568.1">: Implement monitoring for model performance and ensure compliance with local regulations related to video surveillance such as data privacy, retention policies, and consent requirements, to maintain ethical and legal standards in video analytics. </span><span class="koboSpan" id="kobo.568.2">For example, the facial images shouldn’t be stored, only the extracted facial features, as this can infringe personal data protection-related regulations, depending on the country it is intended to be </span><span class="No-Break"><span class="koboSpan" id="kobo.569.1">deployed in.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.570.1">Communication protocol</span></strong><span class="koboSpan" id="kobo.571.1">: MQTT or WebSockets, as they provide low-latency communication between edge devices and the central </span><span class="No-Break"><span class="koboSpan" id="kobo.572.1">monitoring system.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.573.1">User interface</span></strong><span class="koboSpan" id="kobo.574.1">: A desktop application that displays real-time video feeds with object detection overlays for security personnel </span><span class="No-Break"><span class="koboSpan" id="kobo.575.1">to monitor.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.576.1">By considering the specific requirements of each use case, you can make informed decisions on the components required for</span><a id="_idIndexMarker1133"/><span class="koboSpan" id="kobo.577.1"> deploying DL models in production. </span><span class="koboSpan" id="kobo.577.2">Now, let’s move on to some general recommendations for successful DL </span><span class="No-Break"><span class="koboSpan" id="kobo.578.1">model deployment.</span></span></p>
<h1 id="_idParaDest-222"><a id="_idTextAnchor231"/><span class="koboSpan" id="kobo.579.1">Discovering general recommendations for DL deployment</span></h1>
<p><span class="koboSpan" id="kobo.580.1">Here, we will discover DL</span><a id="_idIndexMarker1134"/><span class="koboSpan" id="kobo.581.1"> deployment recommendations related to three verticals, namely model safety, trust, and reliability assurance, model latency optimization, and tools that help abstract model deployment-related decisions and ease the model deployment process. </span><span class="koboSpan" id="kobo.581.2">We will dive into the three verticals one </span><span class="No-Break"><span class="koboSpan" id="kobo.582.1">by one.</span></span></p>
<h2 id="_idParaDest-223"><a id="_idTextAnchor232"/><span class="koboSpan" id="kobo.583.1">Model safety, trust, and reliability assurance</span></h2>
<p><span class="koboSpan" id="kobo.584.1">Ensuring model </span><a id="_idIndexMarker1135"/><span class="koboSpan" id="kobo.585.1">safety, trust, and reliability is a crucial aspect of deploying DL systems. </span><span class="koboSpan" id="kobo.585.2">In this section, we will explore various recommendations and best practices to help you establish a robust framework for maintaining the integrity of your models. </span><span class="koboSpan" id="kobo.585.3">This includes compliance with regulations, implementing guardrails, prediction consistency, comprehensive testing, staging and production deployment strategies, usability tests, retraining and updating deployed models, human-in-the-loop decision-making, and model governance. </span><span class="koboSpan" id="kobo.585.4">By adopting these measures, you can effectively mitigate risks, enhance performance, and foster user trust in your </span><span class="No-Break"><span class="koboSpan" id="kobo.586.1">DL deployment.</span></span></p>
<h3><span class="koboSpan" id="kobo.587.1">Comply with regulations and implement guardrails</span></h3>
<p><span class="koboSpan" id="kobo.588.1">Regulatory compliance and guardrails are essential components of responsible deep learning deployment, ensuring that your model adheres to relevant laws, industry standards, and ethical guidelines. </span><span class="koboSpan" id="kobo.588.2">Implementing a robust compliance framework not only mitigates legal and reputational risks but also fosters trust among </span><a id="_idIndexMarker1136"/><span class="koboSpan" id="kobo.589.1">users and stakeholders. </span><span class="koboSpan" id="kobo.589.2">It’s a very broad topic, so here are a few examples to </span><span class="No-Break"><span class="koboSpan" id="kobo.590.1">learn from:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.591.1">Content moderation for social media platforms</span></strong><span class="koboSpan" id="kobo.592.1">: Compliance with community guidelines and regional laws can be achieved by implementing AI-powered filters for detecting and flagging inappropriate content, setting up a human review process for ambiguous cases, and providing users with a transparent mechanism to </span><span class="No-Break"><span class="koboSpan" id="kobo.593.1">appeal decisions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.594.1">AI-powered recruitment tools</span></strong><span class="koboSpan" id="kobo.595.1">: Compliance with anti-discrimination laws can involve steps such as monitoring bias and fairness metric performance and ensuring that any automated decisions are transparent and explainable to both employers </span><span class="No-Break"><span class="koboSpan" id="kobo.596.1">and applicants.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.597.1">Facial recognition systems</span></strong><span class="koboSpan" id="kobo.598.1">: Compliance with privacy and ethical guidelines can be achieved through steps such as obtaining explicit consent from individuals before collecting and processing their biometric data, implementing robust data security measures, and ensuring transparency about the system’s capabilities </span><span class="No-Break"><span class="koboSpan" id="kobo.599.1">and limitations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.600.1">DL-based video surveillance systems, such as people detection</span></strong><span class="koboSpan" id="kobo.601.1">: Compliance with privacy and ethical guidelines can be achieved through measures such as setting up clear signage to inform the public about the presence of surveillance cameras, restricting data access to authorized personnel, and adhering to data retention and deletion policies as per </span><span class="No-Break"><span class="koboSpan" id="kobo.602.1">local regulations.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.603.1">Recommendation systems (YouTube, Netflix, and Tiktok)</span></strong><span class="koboSpan" id="kobo.604.1">: Ensuring compliance with data protection regulations can involve steps such as implementing privacy-preserving data processing techniques, providing users with the ability to opt out of personalized recommendations, and being transparent about data collection and </span><span class="No-Break"><span class="koboSpan" id="kobo.605.1">usage policies.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.606.1">Generative AI</span></strong><span class="koboSpan" id="kobo.607.1">: Compliance can be achieved by using content filtering mechanisms to prevent harmful content generation, which includes hate speech, explicit material, and content that encourages criminal activities, or prevent dangerous recommendations about </span><span class="No-Break"><span class="koboSpan" id="kobo.608.1">medical issues.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.609.1">As we continue to explore model safety, trust, and reliability assurance, let’s examine the vital aspect of ensuring prediction consistency in </span><span class="No-Break"><span class="koboSpan" id="kobo.610.1">DL deployment.</span></span></p>
<h3><span class="koboSpan" id="kobo.611.1">Ensure prediction consistency</span></h3>
<p><span class="koboSpan" id="kobo.612.1">Prediction consistency is all about a model’s ability to generate the same predictions when faced with the same input data, no matter the hardware, pre/post serialization and loading, infrastructure, or whether it’s a single row or a random batch. </span><span class="koboSpan" id="kobo.612.2">Inconsistent predictions can lead to mismatched expectations of a model’s accuracy and overall performance. </span><span class="koboSpan" id="kobo.612.3">To maintain consistency across various factors, it’s essential to track and replicate the environmental dependencies involved in training, evaluation, and inference. </span><span class="koboSpan" id="kobo.612.4">Tools such as Docker can help create isolated environments with specific dependencies, ensuring a seamless experience and eliminating potential issues. </span><span class="koboSpan" id="kobo.612.5">Additionally, consider making automated tests to objectively prevent any inconsistency from going through, essentially working as </span><span class="No-Break"><span class="koboSpan" id="kobo.613.1">a guardrail.</span></span></p>
<p><span class="koboSpan" id="kobo.614.1">Moving forward, we will discuss the significance of comprehensive testing in maintaining a reliable </span><span class="No-Break"><span class="koboSpan" id="kobo.615.1">DL deployment.</span></span></p>
<h3><span class="koboSpan" id="kobo.616.1">Testing</span></h3>
<p><span class="koboSpan" id="kobo.617.1">Other than prediction consistency tests, generally, comprehensive testing will ensure that your DL model and system perform as expected at all times and meet user requirements. </span><span class="koboSpan" id="kobo.617.2">DL systems are essentially software systems and require similar things to ensure a successful deployment. </span><span class="koboSpan" id="kobo.617.3">The test components are </span><span class="No-Break"><span class="koboSpan" id="kobo.618.1">as follows:</span></span><a id="_idIndexMarker1137"/></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.619.1">Unit, integration, and functional tests</span></strong><span class="koboSpan" id="kobo.620.1">: Unit, integration, and functional testing are essential for ensuring the reliability, maintainability, and overall quality of software components. </span><span class="koboSpan" id="kobo.620.2">Here’s why they </span><span class="No-Break"><span class="koboSpan" id="kobo.621.1">are important:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.622.1">Unit testing</span></strong><span class="koboSpan" id="kobo.623.1">: This focuses on individual components or functions, verifying their correctness and isolating potential issues early in development. </span><span class="koboSpan" id="kobo.623.2">This helps catch bugs before they propagate, reduces debugging time, and improves </span><span class="No-Break"><span class="koboSpan" id="kobo.624.1">code maintainability.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.625.1">Integration testing</span></strong><span class="koboSpan" id="kobo.626.1">: This validates the interactions between different components, ensuring they work together as intended. </span><span class="koboSpan" id="kobo.626.2">This helps identify interface issues, data flow problems, and inconsistencies that can arise when combining components, ensuring a </span><span class="No-Break"><span class="koboSpan" id="kobo.627.1">smooth integration.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.628.1">Functional testing</span></strong><span class="koboSpan" id="kobo.629.1">: This assesses the software’s ability to fulfill its intended purpose and meet user requirements. </span><span class="koboSpan" id="kobo.629.2">Testing end-to-end functionality ensures that the software operates correctly in real-world scenarios and delivers a positive </span><span class="No-Break"><span class="koboSpan" id="kobo.630.1">user experience.</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.631.1">Failover and recovery testing</span></strong><span class="koboSpan" id="kobo.632.1">: Verify the model’s ability to recover from failures, such as hardware or software crashes, and maintain high availability in the face of </span><span class="No-Break"><span class="koboSpan" id="kobo.633.1">unexpected disruptions.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.634.1">Load stress testing</span></strong><span class="koboSpan" id="kobo.635.1">: Evaluate the model’s performance under various load conditions to identify bottlenecks and ensure it can handle the expected user traffic. </span><span class="koboSpan" id="kobo.635.2">These tests can also help you catch errors such as GPU memory overflow, CPU overload, or </span><span class="No-Break"><span class="koboSpan" id="kobo.636.1">insufficient storage.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.637.1">Broad and diverse testing</span></strong><span class="koboSpan" id="kobo.638.1">: The model may not be able to handle unexpected input data, edge cases, or system failures gracefully, causing crashes or undesired behavior. </span><span class="koboSpan" id="kobo.638.2">Thinking up all the possible ways the system will be used can help you catch issues with </span><span class="No-Break"><span class="koboSpan" id="kobo.639.1">the system.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.640.1">Adopting the staging and production deployment steps</span></strong><span class="koboSpan" id="kobo.641.1">: Embracing a staging and production strategy in DL production deployment is highly beneficial for ensuring model reliability and performance. </span><span class="koboSpan" id="kobo.641.2">This approach involves setting up separate environments for testing (staging) and final deployment (production), allowing you to validate your model’s behavior and identify potential issues before the model goes live. </span><span class="koboSpan" id="kobo.641.3">By adopting this strategy, you can minimize the risks associated with deploying untested models, streamline the process of identifying and resolving issues, and enhance the overall reliability of your DL solutions. </span><span class="koboSpan" id="kobo.641.4">Ensure the pipeline can continuously be in production for 24 hours </span><span class="No-Break"><span class="koboSpan" id="kobo.642.1">without failure.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.643.1">Usability tests</span></strong><span class="koboSpan" id="kobo.644.1">: Usability tests focus on ensuring that software applications deliver an effective, efficient, and satisfying user experience. </span><span class="koboSpan" id="kobo.644.2">Both automated and manual tests are useful and complementary to </span><span class="No-Break"><span class="koboSpan" id="kobo.645.1">each other:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.646.1">Manual usability testing</span></strong><span class="koboSpan" id="kobo.647.1">: This involves real users interacting with the software to identify potential usability issues, understand user behavior, and gather qualitative feedback. </span><span class="koboSpan" id="kobo.647.2">Manual testing helps uncover problems that may not be detectable through automated testing, such as confusing navigation, unclear instructions, or subjective preferences. </span><span class="koboSpan" id="kobo.647.3">This human-centric approach provides valuable insights into how users perceive the software and identifies areas </span><span class="No-Break"><span class="koboSpan" id="kobo.648.1">for improvement.</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.649.1">Automated usability testing</span></strong><span class="koboSpan" id="kobo.650.1">: This complements manual testing by using tools and scripts to simulate user interactions, validate user interface elements, and check for accessibility and responsiveness. </span><span class="koboSpan" id="kobo.650.2">Automated testing offers several advantages, including increased efficiency, speed, and coverage, as well as the ability to consistently test across multiple devices, platforms, and browsers. </span><span class="koboSpan" id="kobo.650.3">This helps with identifying usability issues that may not be apparent during manual testing, ensuring a consistent and high-quality </span><span class="No-Break"><span class="koboSpan" id="kobo.651.1">user experience.</span></span></li></ul></li>
</ul>
<p><span class="koboSpan" id="kobo.652.1">Next, let’s consider the</span><a id="_idIndexMarker1138"/><span class="koboSpan" id="kobo.653.1"> importance of retraining and updating the deployed model to ensure its continued effectiveness </span><span class="No-Break"><span class="koboSpan" id="kobo.654.1">and relevance.</span></span></p>
<h3><span class="koboSpan" id="kobo.655.1">Retraining and updating the deployed model</span></h3>
<p><span class="koboSpan" id="kobo.656.1">A retraining and updating strategy is crucial for maintaining the effectiveness of your DL model as it addresses the potential need for regular updates in response to changing data patterns. </span><span class="koboSpan" id="kobo.656.2">By periodically retraining your model on fresh, relevant data, you can ensure it stays current and continues to deliver accurate predictions. </span><span class="koboSpan" id="kobo.656.3">This not only helps maintain the model’s performance but also keeps it in tune with evolving trends and user requirements. </span><span class="koboSpan" id="kobo.656.4">In </span><a href="B18187_16.xhtml#_idTextAnchor238"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.657.1">Chapter 16</span></em></span></a><span class="koboSpan" id="kobo.658.1">, </span><em class="italic"><span class="koboSpan" id="kobo.659.1">Governing Deep Learning Models</span></em><span class="koboSpan" id="kobo.660.1">, we will delve deeper into the importance of retraining and updating, exploring its benefits and best practices to help you successfully implement this strategy in your DL </span><span class="No-Break"><span class="koboSpan" id="kobo.661.1">deployment practically.</span></span></p>
<p><span class="koboSpan" id="kobo.662.1">To further enhance our DL deployment, we will explore the benefits of adopting a human-in-the-loop </span><span class="No-Break"><span class="koboSpan" id="kobo.663.1">decision-making flow.</span></span></p>
<h3><span class="koboSpan" id="kobo.664.1">Adopting a human-in-the-loop decision-making flow</span></h3>
<p><span class="koboSpan" id="kobo.665.1">Incorporating human-in-the-loop scenarios into your DL deployment can greatly enhance model performance and reliability, either as a permanent solution or by triggering alerts when certain conditions are met. </span><span class="koboSpan" id="kobo.665.2">By involving human experts in the decision-making process, you can bridge the gap between the model’s predictions and real-world complexities, allowing for more accurate and nuanced decisions. </span><span class="koboSpan" id="kobo.665.3">This collaborative approach enables continuous improvement by leveraging human expertise to validate, correct, and fine-tune the model’s output. </span><span class="koboSpan" id="kobo.665.4">Additionally, human-in-the-loop systems foster trust and accountability as users can be confident that complex or high-stakes decisions are not made solely by algorithms but are also supported by human judgment </span><span class="No-Break"><span class="koboSpan" id="kobo.666.1">and oversight.</span></span></p>
<p><span class="koboSpan" id="kobo.667.1">Lastly, we will delve into the crucial role of model governance in overseeing and managing the overall DL </span><span class="No-Break"><span class="koboSpan" id="kobo.668.1">deployment process.</span></span></p>
<h3><span class="koboSpan" id="kobo.669.1">Model governance</span></h3>
<p><span class="koboSpan" id="kobo.670.1">Monitoring and governance play a pivotal role in ensuring the ongoing effectiveness and reliability of your deep learning deployment. </span><span class="koboSpan" id="kobo.670.2">By tracking various aspects of your model, such as data drift and concept drift monitoring, you can identify and address issues that may affect its performance over time. </span><span class="koboSpan" id="kobo.670.3">Data drift monitoring helps detect changes in the underlying data distribution, while concept drift monitoring focuses on shifts in the relationships between input features and target variables. </span><span class="koboSpan" id="kobo.670.4">Establishing a robust monitoring and governance framework enables you to proactively manage your model’s performance</span><a id="_idIndexMarker1139"/><span class="koboSpan" id="kobo.671.1"> and maintain its accuracy in the face of evolving trends and conditions. </span><span class="koboSpan" id="kobo.671.2">In </span><a href="B18187_16.xhtml#_idTextAnchor238"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.672.1">Chapter 16</span></em></span></a><span class="koboSpan" id="kobo.673.1">, </span><em class="italic"><span class="koboSpan" id="kobo.674.1">Governing Deep Learning Models</span></em><span class="koboSpan" id="kobo.675.1">, we will explore these aspects in greater detail, along with other critical components of model monitoring and governance, to help you develop a comprehensive strategy for maintaining your DL </span><span class="No-Break"><span class="koboSpan" id="kobo.676.1">deployment’s effectiveness.</span></span></p>
<p><span class="koboSpan" id="kobo.677.1">Next, we will explore the recommendations for model </span><span class="No-Break"><span class="koboSpan" id="kobo.678.1">latency optimization.</span></span></p>
<h2 id="_idParaDest-224"><a id="_idTextAnchor233"/><span class="koboSpan" id="kobo.679.1">Optimizing model latency</span></h2>
<p><span class="koboSpan" id="kobo.680.1">Assuming you have </span><a id="_idIndexMarker1140"/><span class="koboSpan" id="kobo.681.1">chosen an ideal architecture, trained a model, extracted insights, selected the inference model compiler/acceleration framework, and selected the target hardware infrastructure and architecture for hosting your models, there are additional steps you can take to improve model latency at this stage. </span><span class="koboSpan" id="kobo.681.2">The following techniques can </span><span class="No-Break"><span class="koboSpan" id="kobo.682.1">be employed:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.683.1">Model pruning</span></strong><span class="koboSpan" id="kobo.684.1">: Remove unnecessary neurons or weights in the neural network without affecting the overall performance significantly. </span><span class="koboSpan" id="kobo.684.2">Pruning techniques include weight pruning, neuron pruning, and filter pruning. </span><span class="koboSpan" id="kobo.684.3">This can reduce model size and computational requirements, resulting in faster </span><span class="No-Break"><span class="koboSpan" id="kobo.685.1">inference times.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.686.1">Model quantization</span></strong><span class="koboSpan" id="kobo.687.1">: Reduce the precision of model parameters (for example, weights and biases) from 32-bit floating-point numbers to lower bit-width representations such as 16-bit or 8-bit integers. </span><span class="koboSpan" id="kobo.687.2">Quantization can accelerate model inference without significant loss in accuracy, especially when deploying DL models on hardware with limited </span><span class="No-Break"><span class="koboSpan" id="kobo.688.1">computational resources.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.689.1">Model distillation</span></strong><span class="koboSpan" id="kobo.690.1">: Train a smaller, faster “student” model to mimic the behavior of a larger, slower “teacher” model. </span><span class="koboSpan" id="kobo.690.2">The student model learns from the teacher model’s outputs, achieving comparable performance with reduced complexity and faster inference times. </span><span class="koboSpan" id="kobo.690.3">This method was demonstrated in </span><a href="B18187_13.xhtml#_idTextAnchor196"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.691.1">Chapter 13</span></em></span></a><span class="koboSpan" id="kobo.692.1">, </span><em class="italic"><span class="koboSpan" id="kobo.693.1">Exploring Bias </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.694.1">and Fairness</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.695.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.696.1">Model parallelism</span></strong><span class="koboSpan" id="kobo.697.1">: In model parallelism, different parts of a neural network are distributed across multiple devices or processors, allowing concurrent computation on different portions of the model. </span><span class="koboSpan" id="kobo.697.2">For huge models that cannot fit entirely within the memory of a single GPU, this method is an essential step. </span><span class="koboSpan" id="kobo.697.3">For models that have highly parallel operations, latency can be reduced significantly. </span><span class="koboSpan" id="kobo.697.4">Model parallelism can be achieved at various parallelism levels, such as layer-level, pipeline-level, or tensor-slicing </span><span class="No-Break"><span class="koboSpan" id="kobo.698.1">level parallelism.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.699.1">Batch inference</span></strong><span class="koboSpan" id="kobo.700.1">: Process multiple input samples simultaneously through batch processing, enabling the model to </span><a id="_idIndexMarker1141"/><span class="koboSpan" id="kobo.701.1">make better use of the underlying hardware, leading to faster overall </span><span class="No-Break"><span class="koboSpan" id="kobo.702.1">inference times.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.703.1">Next, we will explore tools that </span><span class="No-Break"><span class="koboSpan" id="kobo.704.1">abstract deployment.</span></span></p>
<h2 id="_idParaDest-225"><a id="_idTextAnchor234"/><span class="koboSpan" id="kobo.705.1">Tools that abstract deployment</span></h2>
<p><span class="koboSpan" id="kobo.706.1">There are numerous tools </span><a id="_idIndexMarker1142"/><span class="koboSpan" id="kobo.707.1">and platforms available that help abstract the model deployment process, making it easier and more efficient to deploy machine learning models in various environments. </span><span class="koboSpan" id="kobo.707.2">Here’s an overview of some popular tools and platforms, including both open source and </span><span class="No-Break"><span class="koboSpan" id="kobo.708.1">paid-for tools:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.709.1">TensorFlow serving (open source tool)</span></strong><span class="koboSpan" id="kobo.710.1">: A flexible, high-performance serving system for deploying TensorFlow models in a production environment that provides out-of-the-box support for model versioning, REST and gRPC APIs, and efficient model serving on GPUs </span><span class="No-Break"><span class="koboSpan" id="kobo.711.1">and CPUs.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.712.1">TorchServe (open source tool)</span></strong><span class="koboSpan" id="kobo.713.1">: The PyTorch equivalent of </span><span class="No-Break"><span class="koboSpan" id="kobo.714.1">TensorFlow Serving.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.715.1">TensorFlow Extended (TFX) (open source tool)</span></strong><span class="koboSpan" id="kobo.716.1">: An end-to-end platform for deploying, managing, and </span><a id="_idIndexMarker1143"/><span class="koboSpan" id="kobo.717.1">maintaining machine learning pipelines in production. </span><span class="koboSpan" id="kobo.717.2">TFX integrates with TensorFlow, TensorFlow Serving, and other tools to provide a seamless </span><span class="No-Break"><span class="koboSpan" id="kobo.718.1">deployment experience.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.719.1">MLflow (open source tool)</span></strong><span class="koboSpan" id="kobo.720.1">: An open source platform that streamlines the end-to-end machine learning life cycle, including experimentation, reproducibility, deployment, and monitoring. </span><span class="koboSpan" id="kobo.720.2">It supports multiple languages and machine learning libraries, making it a versatile choice for </span><span class="No-Break"><span class="koboSpan" id="kobo.721.1">diverse projects.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.722.1">Kubeflow (open source tool)</span></strong><span class="koboSpan" id="kobo.723.1">: A Kubernetes-integrated solution that’s designed to facilitate the creation, coordination, deployment, and execution of adaptable and transportable machine learning tasks. </span><span class="koboSpan" id="kobo.723.2">It simplifies the deployment process by providing a consistent and unified environment across different cloud providers and </span><span class="No-Break"><span class="koboSpan" id="kobo.724.1">on-premises infrastructure.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.725.1">Streamlit (open source tool)</span></strong><span class="koboSpan" id="kobo.726.1">: A Python library that enables developers to quickly build and deploy custom web applications for machine learning and data science projects. </span><span class="koboSpan" id="kobo.726.2">Streamlit simplifies the process of creating interactive web apps with minimal coding, making it easier to share and deploy models through </span><span class="No-Break"><span class="koboSpan" id="kobo.727.1">web apps.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.728.1">NVIDIA Triton (open source tool)</span></strong><span class="koboSpan" id="kobo.729.1">: An open source tool that can be used to deploy DL models. </span><span class="koboSpan" id="kobo.729.2">It natively supports many frameworks, most notably TensorRT, Pytorch, ONNX Runtime, OpenVINO, and a general Python backend that allows you to wrap and run any DL framework and Python code. </span><span class="koboSpan" id="kobo.729.3">It provides predictions through HTTP REST APIs and the gRPC protocol. </span><span class="koboSpan" id="kobo.729.4">It also natively provides the Prometheus-compatible and standard time series performance metric logs, which can be subsequently used for model monitoring in the Grafana dashboard. </span><span class="koboSpan" id="kobo.729.5">It also allows us to configure custom metrics in its C API. </span><span class="koboSpan" id="kobo.729.6">Most relevantly, it eases multiple GPU utilization and GPU memory assignments. </span><span class="koboSpan" id="kobo.729.7">We will be exploring this tool practically in the </span><span class="No-Break"><span class="koboSpan" id="kobo.730.1">next section.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.731.1">Azure ML deployment (paid-for tool)</span></strong><span class="koboSpan" id="kobo.732.1">: Microsoft Azure’s machine learning service that simplifies model deployment in the cloud. </span><span class="koboSpan" id="kobo.732.2">It provides tools for managing, monitoring, and scaling deployed models, and supports popular frameworks such as TensorFlow </span><span class="No-Break"><span class="koboSpan" id="kobo.733.1">and PyTorch.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.734.1">DataRobot (paid-for tool)</span></strong><span class="koboSpan" id="kobo.735.1">: DataRobot is an automated machine learning platform that simplifies the process of building, deploying, and maintaining machine learning models. </span><span class="koboSpan" id="kobo.735.2">It provides a wide range of tools and features, including customization, model versioning, monitoring, and collaboration. </span><span class="koboSpan" id="kobo.735.3">We will be exploring the usage of this platform in </span><a href="B18187_18.xhtml#_idTextAnchor265"><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.736.1">Chapter 18</span></em></span></a><span class="koboSpan" id="kobo.737.1">, </span><em class="italic"><span class="koboSpan" id="kobo.738.1">Exploring the DataRobot </span></em><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.739.1">AI Platform</span></em></span><span class="No-Break"><span class="koboSpan" id="kobo.740.1">.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.741.1">Google Vertex AI (paid-for tool)</span></strong><span class="koboSpan" id="kobo.742.1">: A managed machine learning platform from Google Cloud that streamlines the end-to-end machine learning workflow, including model training, deployment, and management. </span><span class="koboSpan" id="kobo.742.2">It integrates with TensorFlow, PyTorch, and other </span><span class="No-Break"><span class="koboSpan" id="kobo.743.1">popular frameworks.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.744.1">Amazon SageMaker (paid-for tool)</span></strong><span class="koboSpan" id="kobo.745.1">: A fully managed machine learning service from AWS that allows developers to build, train, and deploy machine learning models quickly and easily. </span><span class="koboSpan" id="kobo.745.2">It supports multiple frameworks and provides tools for model versioning, monitoring, </span><span class="No-Break"><span class="koboSpan" id="kobo.746.1">and scaling.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.747.1">These tools and platforms  </span><a id="_idIndexMarker1144"/><span class="koboSpan" id="kobo.748.1">help simplify and streamline the model deployment process, enabling developers to efficiently deploy their machine learning models in </span><span class="No-Break"><span class="koboSpan" id="kobo.749.1">various environments.</span></span></p>
<p><span class="koboSpan" id="kobo.750.1">Successful DL deployment requires addressing key aspects such as safety, trust, reliability, and latency optimization while leveraging tools and platforms that simplify the process. </span><span class="koboSpan" id="kobo.750.2">By adhering to these recommendations and utilizing appropriate tools, developers can effectively deploy and manage their DL models in various environments, ensuring consistent and </span><span class="No-Break"><span class="koboSpan" id="kobo.751.1">reliable performance.</span></span></p>
<p><span class="koboSpan" id="kobo.752.1">It should be apparent by now that many trade-offs and criteria need to be evaluated and considered before you can make a DL deployment system component choice. </span><span class="koboSpan" id="kobo.752.2">However, if you don’t have access to a paid tool, have a DL model, and have access to a GPU machine that has enough RAM to host your model, three tools are </span><a id="_idIndexMarker1145"/><span class="koboSpan" id="kobo.753.1">a no-brainer to choose from. </span><span class="koboSpan" id="kobo.753.2">In the next section, we will dive into a topic that both reveals those three tools and practically </span><span class="No-Break"><span class="koboSpan" id="kobo.754.1">uses them.</span></span></p>
<h1 id="_idParaDest-226"><a id="_idTextAnchor235"/><span class="koboSpan" id="kobo.755.1">Deploying a language model with ONNX, TensorRT, and NVIDIA Triton Server</span></h1>
<p><span class="koboSpan" id="kobo.756.1">The three tools are </span><a id="_idIndexMarker1146"/><span class="koboSpan" id="kobo.757.1">ONNX, TensorRT, and NVIDIA</span><a id="_idIndexMarker1147"/><span class="koboSpan" id="kobo.758.1"> Triton Server. </span><span class="koboSpan" id="kobo.758.2">ONNX and TensorRT are meant to perform GPU-based </span><a id="_idIndexMarker1148"/><span class="koboSpan" id="kobo.759.1">inference</span><a id="_idIndexMarker1149"/><span class="koboSpan" id="kobo.760.1"> acceleration, while NVIDIA Triton </span><a id="_idIndexMarker1150"/><span class="koboSpan" id="kobo.761.1">Server is meant to host HTTP or GRPC APIs. </span><span class="koboSpan" id="kobo.761.2">We will explore these</span><a id="_idIndexMarker1151"/><span class="koboSpan" id="kobo.762.1"> three tools practically in this section. </span><span class="koboSpan" id="kobo.762.2">TensorRT is known to perform the best model optimization toward the GPU to speed up inference, while NVIDIA Triton Server is a battle-tested tool for hosting DP models that have compatibility with TensorRT natively. </span><span class="koboSpan" id="kobo.762.3">ONNX, on the other hand, is an intermediate framework in the setup, which we will use primarily to host the weight formats that are directly supported </span><span class="No-Break"><span class="koboSpan" id="kobo.763.1">by TensorRT.</span></span></p>
<p><span class="koboSpan" id="kobo.764.1">In this practical tutorial, we will be deploying a Hugging Face-sourced language model that can be supported on most NVIDIA GPU devices. </span><span class="koboSpan" id="kobo.764.2">We will be converting our PyTorch-based language model from Hugging Face into ONNX weights, which will allow TensorRT to load the Hugging Face language model. </span><span class="koboSpan" id="kobo.764.3">Then, we will create the code and configuration required by the NVIDIA Triton Server framework to host the language model. </span><span class="koboSpan" id="kobo.764.4">NVIDIA Triton Server supports two ways of deploying a model, which is to deploy the DL model with its pre-processing and post-processing methods as a single pipeline all embedded into a Python class, and to deploy the DL model by logically separating the pipeline into separate components. </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.765.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.766.1">.1</span></em><span class="koboSpan" id="kobo.767.1"> depicts both approaches with a pipeline that requires </span><span class="No-Break"><span class="koboSpan" id="kobo.768.1">two models:</span></span></p>
<div>
<div class="IMG---Figure" id="_idContainer128">
<span class="koboSpan" id="kobo.769.1"><img alt="Figure 15.1 – Two approaches for configuring the model deployment with two models in the pipeline" src="image/B18187_15_01.jpg"/></span>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US"><span class="koboSpan" id="kobo.770.1">Figure 15.1 – Two approaches for configuring the model deployment with two models in the pipeline</span></p>
<p><span class="koboSpan" id="kobo.771.1">An intuitive and</span><a id="_idIndexMarker1152"/><span class="koboSpan" id="kobo.772.1"> straightforward</span><a id="_idIndexMarker1153"/><span class="koboSpan" id="kobo.773.1"> way to deploy your DL model pipeline</span><a id="_idIndexMarker1154"/><span class="koboSpan" id="kobo.774.1"> is to go with the first</span><a id="_idIndexMarker1155"/><span class="koboSpan" id="kobo.775.1"> approach. </span><span class="koboSpan" id="kobo.775.2">However, the second</span><a id="_idIndexMarker1156"/><span class="koboSpan" id="kobo.776.1"> approach, which involves</span><a id="_idIndexMarker1157"/><span class="koboSpan" id="kobo.777.1"> breaking down and separating each component into its configuration, provides </span><span class="No-Break"><span class="koboSpan" id="kobo.778.1">multiple benefits:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.779.1">Modularity and reusability</span></strong><span class="koboSpan" id="kobo.780.1">: The modularity aspect allows the individual parts to be reused across different pipelines or projects. </span><span class="koboSpan" id="kobo.780.2">Additionally, it allows easier component swapping while maintaining other components in </span><span class="No-Break"><span class="koboSpan" id="kobo.781.1">the pipeline.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.782.1">Scalability and flexibility</span></strong><span class="koboSpan" id="kobo.783.1">: This allows you to deploy different components to different GPUs and assign different instances of each component running at one time. </span><span class="koboSpan" id="kobo.783.2">Additionally, this method allows CPU-bound methods to not get tied to </span><span class="No-Break"><span class="koboSpan" id="kobo.784.1">a GPU.</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.785.1">Parallelism and latency reduction</span></strong><span class="koboSpan" id="kobo.786.1">: Native parallelism can be enabled through parallel branches instead of you needing to implement it in </span><span class="No-Break"><span class="koboSpan" id="kobo.787.1">Python code.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.788.1">Consider the following two approaches as ways to organize a factory </span><span class="No-Break"><span class="koboSpan" id="kobo.789.1">assembly line:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.790.1">In the first approach, the entire </span><a id="_idIndexMarker1158"/><span class="koboSpan" id="kobo.791.1">assembly process is combined into a single pipeline. </span><span class="koboSpan" id="kobo.791.2">This means that all components are processed and assembled sequentially in one integrated process. </span><span class="koboSpan" id="kobo.791.3">This can be easier to set up and manage, but it may not be as flexible or scalable as the </span><span class="No-Break"><span class="koboSpan" id="kobo.792.1">second approach.</span></span></li>
<li><span class="koboSpan" id="kobo.793.1">In the second approach, the assembly process is broken down into separate, modular components that can be individually managed and optimized. </span><span class="koboSpan" id="kobo.793.2">This allows for greater flexibility and scalability as each component can be fine-tuned or replaced without it affecting the entire pipeline. </span><span class="koboSpan" id="kobo.793.3">Additionally, this approach enables parallel processing, where</span><a id="_idIndexMarker1159"/><span class="koboSpan" id="kobo.794.1"> multiple components</span><a id="_idIndexMarker1160"/><span class="koboSpan" id="kobo.795.1"> can be processed</span><a id="_idIndexMarker1161"/><span class="koboSpan" id="kobo.796.1"> simultaneously, potentially reducing</span><a id="_idIndexMarker1162"/><span class="koboSpan" id="kobo.797.1"> overall latency</span><a id="_idIndexMarker1163"/><span class="koboSpan" id="kobo.798.1"> and </span><span class="No-Break"><span class="koboSpan" id="kobo.799.1">improving</span></span><span class="No-Break"><a id="_idIndexMarker1164"/></span><span class="No-Break"><span class="koboSpan" id="kobo.800.1"> efficiency.</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.801.1">To make things simple, we will be demonstrating the first </span><span class="No-Break"><span class="koboSpan" id="kobo.802.1">approach here.</span></span></p>
<h2 id="_idParaDest-227"><a id="_idTextAnchor236"/><span class="koboSpan" id="kobo.803.1">Practically deploying a DL model with the single pipeline approach</span></h2>
<p><span class="koboSpan" id="kobo.804.1">In this tutorial, we will explore</span><a id="_idIndexMarker1165"/><span class="koboSpan" id="kobo.805.1"> the process of deploying</span><a id="_idIndexMarker1166"/><span class="koboSpan" id="kobo.806.1"> a DL model using ONNX, TensorRT, and NVIDIA Triton Server. </span><span class="koboSpan" id="kobo.806.2">While deploying the model using NVIDIA Triton Server, you may encounter issues related to model loading, configuration, or inference. </span><span class="koboSpan" id="kobo.806.3">Here are some </span><span class="No-Break"><span class="koboSpan" id="kobo.807.1">troubleshooting tips:</span></span></p>
<ul>
<li><span class="koboSpan" id="kobo.808.1">Verify that the model files, configuration files, and other required files are in the correct locations and have the proper </span><span class="No-Break"><span class="koboSpan" id="kobo.809.1">file permissions</span></span></li>
<li><span class="koboSpan" id="kobo.810.1">Ensure that the model configuration file (</span><strong class="source-inline"><span class="koboSpan" id="kobo.811.1">config.pbtxt</span></strong><span class="koboSpan" id="kobo.812.1">) has the correct settings, such as input and output tensor names, data types, </span><span class="No-Break"><span class="koboSpan" id="kobo.813.1">and dimensions</span></span></li>
<li><span class="koboSpan" id="kobo.814.1">Check the NVIDIA Triton Server logs for any error messages or warnings that could provide insights into </span><span class="No-Break"><span class="koboSpan" id="kobo.815.1">the issue</span></span></li>
<li><span class="koboSpan" id="kobo.816.1">Make sure that the necessary dependencies, such as the DL framework, ONNX, and TensorRT, are installed and compatible with your system </span><span class="No-Break"><span class="koboSpan" id="kobo.817.1">and hardware</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.818.1">Let’s start the practical</span><a id="_idIndexMarker1167"/><span class="koboSpan" id="kobo.819.1"> tutorial in a </span><span class="No-Break"><span class="koboSpan" id="kobo.820.1">step-by-step</span></span><span class="No-Break"><a id="_idIndexMarker1168"/></span><span class="No-Break"><span class="koboSpan" id="kobo.821.1"> manner:</span></span></p>
<ol>
<li><span class="koboSpan" id="kobo.822.1">First, we need to install the </span><strong class="source-inline"><span class="koboSpan" id="kobo.823.1">transformer-deploy</span></strong><span class="koboSpan" id="kobo.824.1"> repository by running the </span><span class="No-Break"><span class="koboSpan" id="kobo.825.1">following code:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.826.1">
pip install git+ https://github.com/ELS-RD/transformer-deploy/tree/6b88e24ade6ce199e825adc0477b28a07f51f17d</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.827.1">Alternatively, we can use the </span><span class="No-Break"><span class="koboSpan" id="kobo.828.1">following command:</span></span></p><pre class="source-code"><strong class="bold"><span class="koboSpan" id="kobo.829.1">git clone https://github.com/ELS-RD/transformer-deploy</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.830.1">git checkout 6b88e24ade6ce199e825adc0477b28a07f51f17d</span></strong>
<strong class="bold"><span class="koboSpan" id="kobo.831.1">python setup.py install</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.832.1">This will download the helper code required to convert any Hugging Face model into ONNX format, load the model into the TensorRT framework with its optimizations, perform inference with the TensorRT framework, and perform speed benchmarks using the base framework, ONNX Runtime, </span><span class="No-Break"><span class="koboSpan" id="kobo.833.1">and TensorRT.</span></span></p></li> <li><span class="koboSpan" id="kobo.834.1">Next, we must convert the chosen Hugging Face text generation model into ONNX graph format. </span><strong class="source-inline"><span class="koboSpan" id="kobo.835.1">transformer-deploy</span></strong><span class="koboSpan" id="kobo.836.1"> uses the tracing mode from PyTorch to convert a PyTorch model into ONNX. </span><span class="koboSpan" id="kobo.836.2">This involves sending some example PyTorch tensor data to the PyTorch helper tool, which will then trace the data shapes in the model through a forward pass to form the graph in ONNX. </span><span class="koboSpan" id="kobo.836.3">The following code shows a snippet of this tool as a </span><strong class="source-inline"><span class="koboSpan" id="kobo.837.1">pytorch</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.838.1">library method:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.839.1">
with torch.no_grad():
      torch.onnx.export(
           model_pytorch,
           args=tuple(inputs_pytorch.values()),
           f=output_path,
           opset_version=13,
           do_constant_folding=True,
           input_names=input_names,
           output_names=output_names,
           dynamic_axes=dynamic_axis,
           training=TrainingMode.EVAL,
      )</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.840.1">This encompasses numerous</span><a id="_idIndexMarker1169"/><span class="koboSpan" id="kobo.841.1"> parameters, and it’s worth</span><a id="_idIndexMarker1170"/><span class="koboSpan" id="kobo.842.1"> taking a closer look at what each </span><span class="No-Break"><span class="koboSpan" id="kobo.843.1">variable signifies:</span></span></p><ul><li><strong class="source-inline"><span class="koboSpan" id="kobo.844.1">model_pytorch</span></strong><span class="koboSpan" id="kobo.845.1"> is the Hugging Face LLM PyTorch </span><span class="No-Break"><span class="koboSpan" id="kobo.846.1">model instance.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.847.1">inputs_pytorch</span></strong><span class="koboSpan" id="kobo.848.1"> is the example </span><span class="No-Break"><span class="koboSpan" id="kobo.849.1">input data.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.850.1">output_path</span></strong><span class="koboSpan" id="kobo.851.1"> specifies the output path to save the </span><span class="No-Break"><span class="koboSpan" id="kobo.852.1">model to.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.853.1">opset_version</span></strong><span class="koboSpan" id="kobo.854.1"> specified the ONNX version to use, where a higher version usually adds </span><span class="No-Break"><span class="koboSpan" id="kobo.855.1">more optimizations.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.856.1">input_names</span></strong><span class="koboSpan" id="kobo.857.1"> are the PyTorch model forward method input </span><span class="No-Break"><span class="koboSpan" id="kobo.858.1">argument names.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.859.1">output_names</span></strong><span class="koboSpan" id="kobo.860.1"> specifies the output names of the model </span><span class="No-Break"><span class="koboSpan" id="kobo.861.1">you want.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.862.1">dynamic_axes</span></strong><span class="koboSpan" id="kobo.863.1"> declares which axis is for batch and which axis contains the input </span><span class="No-Break"><span class="koboSpan" id="kobo.864.1">sequence data.</span></span></li><li><strong class="source-inline"><span class="koboSpan" id="kobo.865.1">training</span></strong><span class="koboSpan" id="kobo.866.1"> defines whether it will be used for training or evaluation. </span><span class="koboSpan" id="kobo.866.2">It should always be set to evaluation mode for inference </span><span class="No-Break"><span class="koboSpan" id="kobo.867.1">conversion purposes.</span></span></li></ul></li> <li><span class="koboSpan" id="kobo.868.1">After conversion, we need</span><a id="_idIndexMarker1171"/><span class="koboSpan" id="kobo.869.1"> to load the model into TensorRT, perform a graph optimization step and serialization</span><a id="_idIndexMarker1172"/><span class="koboSpan" id="kobo.870.1"> at the same time to produce a TensorRT ICudaEngine, deserialize the same engine, and save it to disk as a local file. </span><span class="koboSpan" id="kobo.870.2">The optimization steps are </span><span class="No-Break"><span class="koboSpan" id="kobo.871.1">as follows:</span></span><ol><li class="Alphabets"><strong class="bold"><span class="koboSpan" id="kobo.872.1">Kernel auto-tuning</span></strong><span class="koboSpan" id="kobo.873.1">: TensorRT evaluates and tests multiple implementations for each operation and selects the most efficient kernel that works best on the </span><span class="No-Break"><span class="koboSpan" id="kobo.874.1">specific hardware.</span></span></li><li class="Alphabets"><strong class="bold"><span class="koboSpan" id="kobo.875.1">Graph simplification</span></strong><span class="koboSpan" id="kobo.876.1">: TensorRT may simplify the computational graph by fusing operations or combining them using optimized kernels, reducing redundant computations, and </span><span class="No-Break"><span class="koboSpan" id="kobo.877.1">improving performance.</span></span></li><li class="Alphabets"><strong class="bold"><span class="koboSpan" id="kobo.878.1">Mixed precision</span></strong><span class="koboSpan" id="kobo.879.1">: TensorRT can use mixed quantization, running different layers of the graph at various precisions. </span><span class="koboSpan" id="kobo.879.2">This maximizes performance by utilizing lower precision for some parts without significant loss </span><span class="No-Break"><span class="koboSpan" id="kobo.880.1">of accuracy.</span></span></li><li class="Alphabets"><strong class="bold"><span class="koboSpan" id="kobo.881.1">Precision calibration</span></strong><span class="koboSpan" id="kobo.882.1">: TensorRT can calibrate the model’s numerical precision to find the minimal precision required to maintain acceptable accuracy, thus reducing memory and </span><span class="No-Break"><span class="koboSpan" id="kobo.883.1">computation needs.</span></span></li><li class="Alphabets"><strong class="bold"><span class="koboSpan" id="kobo.884.1">Constant folding</span></strong><span class="koboSpan" id="kobo.885.1">: TensorRT can fold constant tensors into the computation graph (a process that precomputes information that is fixed and only needs to be computed once), optimizing the execution and reducing </span><span class="No-Break"><span class="koboSpan" id="kobo.886.1">runtime overhead.</span></span></li><li class="Alphabets"><strong class="bold"><span class="koboSpan" id="kobo.887.1">Dynamic Tensor memory</span></strong><span class="koboSpan" id="kobo.888.1">: TensorRT may optimize tensor memory usage by dynamically allocating and reusing memory when needed, reducing memory fragmentation and </span><span class="No-Break"><span class="koboSpan" id="kobo.889.1">improving performance.</span></span></li></ol><p class="list-inset"><span class="koboSpan" id="kobo.890.1">The following code shows</span><a id="_idIndexMarker1173"/><span class="koboSpan" id="kobo.891.1"> a snippet </span><a id="_idIndexMarker1174"/><span class="koboSpan" id="kobo.892.1">of this process. </span><span class="koboSpan" id="kobo.892.2">This is what happens when we use the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.893.1">transformer-deploy</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.894.1"> tool:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.895.1">
trt_engine = builder.build_serialized_network(network_def, config)
engine: ICudaEngine = runtime.deserialize_cuda_engine(trt_engine)
with open(engine_file_path, "wb") as f:
     f.write(engine.serialize())</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.896.1">Here, </span><strong class="source-inline"><span class="koboSpan" id="kobo.897.1">network_def</span></strong><span class="koboSpan" id="kobo.898.1"> contains the ONNX graph definition of the model, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.899.1">config</span></strong><span class="koboSpan" id="kobo.900.1"> specifies all the optimization strategies where the TensorRT default will </span><span class="No-Break"><span class="koboSpan" id="kobo.901.1">be used.</span></span></p></li> <li><span class="koboSpan" id="kobo.902.1">Now, we will execute the command that will convert the chosen Hugging Face text generation model into a TensorRT </span><span class="No-Break"><span class="koboSpan" id="kobo.903.1">serialized engine:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.904.1">convert_model -m roneneldan/TinyStories-3M --backend tensorrt onnx --task text-generation --seq-len 128 128 128 --batch-size 1 1 1 --auth-token True --atol 5.0</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.905.1">This command also performs runtime latency benchmarks using PyTorch, ONNX Runtime, and TensorRT so that we can get a first-hand feeling of the difference in runtime performance. </span><span class="koboSpan" id="kobo.905.2">The Hugging Face model we’ve chosen is a relatively small model with 3 million parameters called </span><strong class="source-inline"><span class="koboSpan" id="kobo.906.1">roneneldan</span></strong><span class="koboSpan" id="kobo.907.1">/</span><strong class="source-inline"><span class="koboSpan" id="kobo.908.1">TinyStories-3M</span></strong><span class="koboSpan" id="kobo.909.1">. </span><span class="koboSpan" id="kobo.909.2">Additionally, one aspect that we need to take care of when using any graph optimizations and conversions to another framework is to make sure it maintains a satisfactory level of accuracy and doesn’t degrade too much from the model when it is run in the original base framework. </span><span class="koboSpan" id="kobo.909.3">A natural way to do this is to check the validation performance where the model is trained. </span><span class="koboSpan" id="kobo.909.4">In this case, we don’t know what dataset the Hugging Face model is trained from, so a workaround can be to take any relevant text generation dataset and validate on both the base framework and target framework setup. </span><span class="koboSpan" id="kobo.909.5">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.910.1">transformer-deploy</span></strong><span class="koboSpan" id="kobo.911.1"> tool performs a simple predicted values deviation check to make sure the predicted values that are generated using the target framework don’t deviate too far from the values generated using the base framework. </span><span class="koboSpan" id="kobo.911.2">The </span><strong class="source-inline"><span class="koboSpan" id="kobo.912.1">atol</span></strong><span class="koboSpan" id="kobo.913.1"> parameter controls the leniency of this </span><span class="No-Break"><span class="koboSpan" id="kobo.914.1">deviation check.</span></span></p></li> <li><span class="koboSpan" id="kobo.915.1">The result of running the command in </span><em class="italic"><span class="koboSpan" id="kobo.916.1">step 4</span></em><span class="koboSpan" id="kobo.917.1"> is </span><span class="No-Break"><span class="koboSpan" id="kobo.918.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.919.1">
[Pytorch (FP32)] mean=6.05ms, sd=0.21ms, min=5.91ms, max=8.12ms, median=6.01ms, 95p=6.27ms, 99p=7.16ms
[Pytorch (FP16)] mean=7.76ms, sd=0.25ms, min=7.52ms, max=10.16ms, median=7.72ms, 95p=7.98ms, 99p=9.20ms
[TensorRT (FP16)] mean=0.61ms, sd=0.06ms, min=0.56ms, max=0.86ms, median=0.57ms, 95p=0.70ms, 99p=0.71ms
[ONNX Runtime (FP32)] mean=1.89ms, sd=0.06ms, min=1.81ms, max=2.48ms, median=1.88ms, 95p=1.93ms, 99p=2.21ms
[ONNX Runtime (FP16)] mean=2.07ms, sd=0.09ms, min=1.99ms, max=3.11ms, median=2.06ms, 95p=2.11ms, 99p=2.62ms
Each inference engine output is within 5.0 tolerance compared to Pytorch output</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.920.1">The results here show</span><a id="_idIndexMarker1175"/><span class="koboSpan" id="kobo.921.1"> the latency benchmark</span><a id="_idIndexMarker1176"/><span class="koboSpan" id="kobo.922.1"> stats on the different frameworks and settings. </span><span class="koboSpan" id="kobo.922.2">PyTorch is the slowest, followed by ONNX Runtime, and finally, TensorRT is the fastest! </span><span class="koboSpan" id="kobo.922.3">The command also saved the TensorRT model ending under the </span><span class="No-Break"><span class="koboSpan" id="kobo.923.1">following path:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.924.1">triton_models/model.plan</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.925.1">Here, we will move the model into new folders at the </span><span class="No-Break"><span class="koboSpan" id="kobo.926.1">following path:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.927.1">models/transformer_tensorrt_text_generation/1/model.plan</span></pre></li> <li><span class="koboSpan" id="kobo.928.1">Now, it is time to create</span><a id="_idIndexMarker1177"/><span class="koboSpan" id="kobo.929.1"> the code and configurations</span><a id="_idIndexMarker1178"/><span class="koboSpan" id="kobo.930.1"> required to host this TensorRT model in NVIDIA Triton Server. </span><span class="koboSpan" id="kobo.930.2">Following approach 1 from </span><span class="No-Break"><em class="italic"><span class="koboSpan" id="kobo.931.1">Figure 15</span></em></span><em class="italic"><span class="koboSpan" id="kobo.932.1">.1</span></em><span class="koboSpan" id="kobo.933.1">, we need to define a Python class to initialize and perform inference using the Hugging Face tokenizer and the TensorRT engine. </span><span class="koboSpan" id="kobo.933.2">Let’s start by importing the necessary Python libraries into this deployment Python </span><span class="No-Break"><span class="koboSpan" id="kobo.934.1">code file:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.935.1">
from typing import List
import numpy as np
import tensorrt as trt
import torch
import triton_python_backend_utils as pb_utils
from transformers import AutoTokenizer, TensorType
from transformer_deploy.backends.trt_utils import load_engine</span></pre></li> <li><span class="koboSpan" id="kobo.936.1">Next, we must specify the model and path where we stored the serialized </span><span class="No-Break"><span class="koboSpan" id="kobo.937.1">TensorRT model:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.938.1">
model = "roneneldan/TinyStories-3M"
tensorrt_path = "/models/transformer_tensorrt_text_generation/1/model.plan"</span></pre></li> <li><span class="koboSpan" id="kobo.939.1">Now, we must define the </span><strong class="source-inline"><span class="koboSpan" id="kobo.940.1">TritonPythonModel</span></strong><span class="koboSpan" id="kobo.941.1"> class interface, starting with the </span><span class="No-Break"><span class="koboSpan" id="kobo.942.1">initialization method:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.943.1">
class TritonPythonModel:
    def initialize(self, args):
        self.tokenizer = AutoTokenizer.from_pretrained(model)
        self.model_input_names = self.tokenizer.model_input_names
        trt_logger = trt.Logger(trt.Logger.VERBOSE)
        runtime = trt.Runtime(trt_logger)
        self.model = load_engine(
            runtime=runtime, engine_file_path=tensorrt_path
        )</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.944.1">The class name must be the same as the initialization method name – that is, </span><strong class="source-inline"><span class="koboSpan" id="kobo.945.1">initialize</span></strong><span class="koboSpan" id="kobo.946.1">. </span><span class="koboSpan" id="kobo.946.2">This code loads the pre-trained</span><a id="_idIndexMarker1179"/><span class="koboSpan" id="kobo.947.1"> tokenizer from the Hugging Face library methods and downloads</span><a id="_idIndexMarker1180"/><span class="koboSpan" id="kobo.948.1"> the tokenizer from the internet. </span><span class="koboSpan" id="kobo.948.2">Note that for production deployment of a model, it is advised to have a managed instance of the tokenizer weights or any model weights somewhere to ensure a reliable deployment process. </span><span class="koboSpan" id="kobo.948.3">Additionally, the code loads the serialized </span><span class="No-Break"><span class="koboSpan" id="kobo.949.1">TensorRT engine.</span></span></p></li> <li><span class="koboSpan" id="kobo.950.1">Next, we need to define the actual inference part of the tokenizer and model, </span><span class="No-Break"><span class="koboSpan" id="kobo.951.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.952.1">
def execute(self, requests):
        responses = []
        for request in requests:
            query = [t.decode("UTF-8") for t in pb_utils.get_input_tensor_by_name(request, "TEXT").as_numpy().tolist()]
            tokens = self.tokenizer(
                text=query, return_tensors=TensorType.PYTORCH, return_attention_mask=False
            )
            input_ids = tokens.input_ids.type(dtype=torch.int32)
            input_ids = input_ids.to("cuda")
            output_seq: torch.Tensor = self.model({"input_ids": input_ids})['output'].cpu().argmax(2)
            decoded_texts: List[str] = [self.tokenizer.decode(seq, skip_special_tokens=True) for seq in output_seq]
            tensor_output = [pb_utils.Tensor("OUTPUT_TEXT", np.array(t, dtype=object)) for t in decoded_texts]
            responses.append(pb_utils.InferenceResponse(tensor_output))
        return responses</span></pre></li> <li><span class="koboSpan" id="kobo.953.1">This code should live</span><a id="_idIndexMarker1181"/><span class="koboSpan" id="kobo.954.1"> under the Python</span><a id="_idIndexMarker1182"/><span class="koboSpan" id="kobo.955.1"> code </span><strong class="source-inline"><span class="koboSpan" id="kobo.956.1">model.py</span></strong><span class="koboSpan" id="kobo.957.1"> file under the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.958.1">models/transformer_tensorrt_text_generation/1/model.py</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.959.1"> path.</span></span><p class="list-inset"><span class="koboSpan" id="kobo.960.1">The folder named </span><strong class="source-inline"><span class="koboSpan" id="kobo.961.1">1</span></strong><span class="koboSpan" id="kobo.962.1"> is to symbolize the version of the </span><strong class="source-inline"><span class="koboSpan" id="kobo.963.1">transformer_tensorrt_text_generation</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.964.1">model name.</span></span></p></li>
<li><span class="koboSpan" id="kobo.965.1">The final file we need is a configuration file that specifies the name of the model, the max batch size of the model, the backend type of the model (in this case, Python), the input type, the name and dimensions of the model, the output type, the name and dimensions of the model, the number of instances of this pipeline, and finally whether to use GPU or CPU. </span><span class="koboSpan" id="kobo.965.2">The file needs to be named </span><strong class="source-inline"><span class="koboSpan" id="kobo.966.1">config.pbtxt</span></strong><span class="koboSpan" id="kobo.967.1">. </span><span class="koboSpan" id="kobo.967.2">The content of this file for our usage is </span><span class="No-Break"><span class="koboSpan" id="kobo.968.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.969.1">
name: "transformer_tensorrt_text_generation"
max_batch_size: 0
backend: "python"
input [
{
      name: "TEXT"
      data_type: TYPE_STRING
      dims: [ -1 ]
}
]
output [
{
      name: "OUTPUT_TEXT"
      data_type: TYPE_STRING
      dims: [ -1 ]
}
]
instance_group [
{
     count: 1
     kind: KIND_GPU
}
]</span></pre><p class="list-inset"><span class="koboSpan" id="kobo.970.1">This should be stored under the following </span><span class="No-Break"><span class="koboSpan" id="kobo.971.1">file path:</span></span></p><pre class="source-code"><span class="koboSpan" id="kobo.972.1">models/transformer_tensorrt_text_generation/config.pbtxt</span></pre></li> <li><span class="koboSpan" id="kobo.973.1">Now, we have all the code</span><a id="_idIndexMarker1183"/><span class="koboSpan" id="kobo.974.1"> and configuration needed to run</span><a id="_idIndexMarker1184"/><span class="koboSpan" id="kobo.975.1"> NVIDIA Triton Server and deploy our language model, which is an easy-to-use </span><strong class="source-inline"><span class="koboSpan" id="kobo.976.1">nvidia-docker</span></strong><span class="koboSpan" id="kobo.977.1">-based deployment with a publicly available and downloadable image. </span><span class="koboSpan" id="kobo.977.2">The language model can be deployed on NVIDIA Triton Server by executing the </span><span class="No-Break"><span class="koboSpan" id="kobo.978.1">following command:</span></span><pre class="source-code">
<strong class="bold"><span class="koboSpan" id="kobo.979.1">sudo docker run --gpus=all -it --shm-size=256m --rm -p8000:8000 -p8001:8001 -p8002:8002 -v ${PWD}/ models:/models nvcr.io/nvidia/tritonserver:23.05-py3</span></strong></pre><p class="list-inset"><span class="koboSpan" id="kobo.980.1">After entering this environment, run the following commands to install the libraries and start NVIDIA </span><span class="No-Break"><span class="koboSpan" id="kobo.981.1">Triton Server:</span></span></p><pre class="source-code"><strong class="bold"><span class="koboSpan" id="kobo.982.1">pip install transformers==4.21.3 nvidia-tensorrt==8.4.1.5 git+https://github.com/ELS-RD/transformer-deploy torch==1.12.0  -f  &amp;&amp; tritonserver --model-repository=/models</span></strong></pre></li> <li><span class="koboSpan" id="kobo.983.1">Now that the model has been deployed, we need some client-side Python code to feed text into the language model and obtain a generated text. </span><span class="koboSpan" id="kobo.983.2">We will put all the client code in a single code file called </span><strong class="source-inline"><span class="koboSpan" id="kobo.984.1">triton_client.py</span></strong><span class="koboSpan" id="kobo.985.1">, which will define the code that’s needed to query the hosted model to obtain a generated text and print it out on the command line. </span><span class="koboSpan" id="kobo.985.2">The first step is to import the necessary libraries, which in this case will only be the HTTP client from the </span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.986.1">tritonclient</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.987.1"> library:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.988.1">
import tritonclient.http as httpclient</span></pre></li> <li><span class="koboSpan" id="kobo.989.1">We also need to specify the model name defined in the </span><strong class="source-inline"><span class="koboSpan" id="kobo.990.1">config.pbtxt</span></strong><span class="koboSpan" id="kobo.991.1"> file from </span><em class="italic"><span class="koboSpan" id="kobo.992.1">step 12</span></em><span class="koboSpan" id="kobo.993.1">, along with the model version, </span><span class="No-Break"><span class="koboSpan" id="kobo.994.1">as follows:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.995.1">
MODEL_NAME = "transformer_tensorrt_text_generation"
MODEL_VERSION = "1"</span></pre></li> <li><span class="koboSpan" id="kobo.996.1">Now, we will define the client</span><a id="_idIndexMarker1185"/><span class="koboSpan" id="kobo.997.1"> using the </span><strong class="source-inline"><span class="koboSpan" id="kobo.998.1">httpclient</span></strong><span class="koboSpan" id="kobo.999.1"> helper tool, define the input data, configure the output</span><a id="_idIndexMarker1186"/><span class="koboSpan" id="kobo.1000.1"> data so that it’s obtained according to the </span><strong class="source-inline"><span class="koboSpan" id="kobo.1001.1">config.pbtxt</span></strong><span class="koboSpan" id="kobo.1002.1"> specified output name, and print the </span><span class="No-Break"><span class="koboSpan" id="kobo.1003.1">generated text:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1004.1">
def main():
    client = httpclient.InferenceServerClient(url="localhost:8000"
    input_text = np.array(["Tell me a joke."], dtype=object)
    input_tensors = [
        httpclient.InferInput("TEXT", (1,), datatype="BYTES")
    ]
    input_tensors[0].set_data_from_numpy(input_text)
    outputs = [
       httpclient.InferRequestedOutput("OUTPUT_TEXT")
    ]
    query_response = client.infer(
        model_name=MODEL_NAME,
        model_version=MODEL_VERSION,
        inputs=input_tensors,
        outputs=outputs
      )
      output_text = query_response.as_numpy("OUTPUT_TEXT")
      print(output_text)
    if __name__ == '__main__':
        main()</span></pre></li> <li><span class="koboSpan" id="kobo.1005.1">Running </span><strong class="source-inline"><span class="koboSpan" id="kobo.1006.1">python triton_client.py</span></strong><span class="koboSpan" id="kobo.1007.1"> in the command line will return the </span><span class="No-Break"><span class="koboSpan" id="kobo.1008.1">following response:</span></span><pre class="source-code"><span class="koboSpan" id="kobo.1009.1">
b' you are big!"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n'</span></pre></li> </ol>
<p><span class="koboSpan" id="kobo.1010.1">And with that, we are done with the </span><span class="No-Break"><span class="koboSpan" id="kobo.1011.1">practical tutorial!</span></span></p>
<p><span class="koboSpan" id="kobo.1012.1">This topic serves to show the minimal workflow needed to deploy a language model with acceleration with NVIDIA Triton Server, which is not too different from an audio DL model or a computer vision DL model. </span><span class="koboSpan" id="kobo.1012.2">Try the workflow presented here using other language models and try to play around with all the settings! </span><span class="koboSpan" id="kobo.1012.3">Note that there can be some issues with either the conversion or the optimization stage due to highly custom layers from new language models, so you will either need to work on fixing it in the base libraries themselves or raise it to the respective teams and wait for it to </span><span class="No-Break"><span class="koboSpan" id="kobo.1013.1">be upgraded.</span></span></p>
<p><span class="koboSpan" id="kobo.1014.1">When deploying specifically with language</span><a id="_idIndexMarker1187"/><span class="koboSpan" id="kobo.1015.1"> models, there are a few more tools</span><a id="_idIndexMarker1188"/><span class="koboSpan" id="kobo.1016.1"> that can be used for deployment that might be worth considering due to their high-level abstraction of the Hugging Face models and out-of-the-box official support of selected LLMs. </span><span class="koboSpan" id="kobo.1016.2">These are </span><span class="No-Break"><span class="koboSpan" id="kobo.1017.1">as follows:</span></span></p>
<ul>
<li><strong class="bold"><span class="koboSpan" id="kobo.1018.1">VLLM</span></strong><span class="koboSpan" id="kobo.1019.1"> (</span><a href="https://github.com/vllm-project/vllm"><span class="koboSpan" id="kobo.1020.1">https://github.com/vllm-project/vllm</span></a><span class="koboSpan" id="kobo.1021.1">): This boasts fast inference speeds for the following</span><a id="_idIndexMarker1189"/><span class="koboSpan" id="kobo.1022.1"> models: Baichuan-7B (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1023.1">baichuan-inc/Baichuan-7B</span></strong><span class="koboSpan" id="kobo.1024.1">), BLOOM (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1025.1">bigscience/bloom</span></strong><span class="koboSpan" id="kobo.1026.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1027.1">bigscience/bloomz</span></strong><span class="koboSpan" id="kobo.1028.1">), GPT-2 (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1029.1">gpt2</span></strong><span class="koboSpan" id="kobo.1030.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1031.1">gpt2-xl</span></strong><span class="koboSpan" id="kobo.1032.1">), GPT BigCode (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1033.1">bigcode/starcoder</span></strong><span class="koboSpan" id="kobo.1034.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1035.1">bigcode/gpt_bigcode-santacoder</span></strong><span class="koboSpan" id="kobo.1036.1">), GPT-J (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1037.1">EleutherAI/gpt-j-6b</span></strong><span class="koboSpan" id="kobo.1038.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1039.1">nomic-ai/gpt4all-j</span></strong><span class="koboSpan" id="kobo.1040.1">), GPT-NeoX (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1041.1">EleutherAI/gpt-neox-20b</span></strong><span class="koboSpan" id="kobo.1042.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1043.1">databricks/dolly-v2-12b</span></strong><span class="koboSpan" id="kobo.1044.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1045.1">stabilityai/stablelm-tuned-alpha-7b</span></strong><span class="koboSpan" id="kobo.1046.1">), LLaMA and LLaMA-2 (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1047.1">meta-llama/Llama-2-70b-hf</span></strong><span class="koboSpan" id="kobo.1048.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1049.1">lmsys/vicuna-13b-v1.3</span></strong><span class="koboSpan" id="kobo.1050.1">, </span><strong class="source-inline"><span class="koboSpan" id="kobo.1051.1">young-geng/koala</span></strong><span class="koboSpan" id="kobo.1052.1">, and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1053.1">openlm-research/open_llama_13b</span></strong><span class="koboSpan" id="kobo.1054.1">), MPT (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1055.1">mosaicml/mpt-7b</span></strong><span class="koboSpan" id="kobo.1056.1"> and </span><strong class="source-inline"><span class="koboSpan" id="kobo.1057.1">mosaicml/mpt-30b</span></strong><span class="koboSpan" id="kobo.1058.1">), and OPT (</span><strong class="source-inline"><span class="koboSpan" id="kobo.1059.1">facebook/opt-66b</span></strong> <span class="No-Break"><span class="koboSpan" id="kobo.1060.1">and </span></span><span class="No-Break"><strong class="source-inline"><span class="koboSpan" id="kobo.1061.1">facebook/opt-iml-max-30b</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1062.1">)</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1063.1">CTranslate2</span></strong><span class="koboSpan" id="kobo.1064.1"> (https://github.com/OpenNMT/CTranslate2): This boasts efficient inference</span><a id="_idIndexMarker1190"/><span class="koboSpan" id="kobo.1065.1"> with support of the </span><span class="No-Break"><span class="koboSpan" id="kobo.1066.1">following models:</span></span><ul><li><strong class="bold"><span class="koboSpan" id="kobo.1067.1">Encoder-decoder models</span></strong><span class="koboSpan" id="kobo.1068.1">: Transformer base/big, M2M-100, NLLB, BART, mBART, Pegasus, T5, </span><span class="No-Break"><span class="koboSpan" id="kobo.1069.1">and Whisper</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1070.1">Decoder-only models</span></strong><span class="koboSpan" id="kobo.1071.1">: GPT-2, GPT-J, GPT-NeoX, OPT, BLOOM, MPT, Llama, CodeGen, GPTBigCode, </span><span class="No-Break"><span class="koboSpan" id="kobo.1072.1">and Falcon</span></span></li><li><strong class="bold"><span class="koboSpan" id="kobo.1073.1">Encoder-only </span></strong><span class="No-Break"><strong class="bold"><span class="koboSpan" id="kobo.1074.1">models</span></strong></span><span class="No-Break"><span class="koboSpan" id="kobo.1075.1">: BERT</span></span></li></ul></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1076.1">Text-generation-interface</span></strong><span class="koboSpan" id="kobo.1077.1"> (https://github.com/huggingface/text-generation-inference): This is not as efficient without an accelerator</span><a id="_idIndexMarker1191"/><span class="koboSpan" id="kobo.1078.1"> but it provides manually performed offline optimizations for the following models: BLOOM, FLAN-T5, Galactica, GPT-Neox, Llama, OPT, SantaCoder, Starcoder, Falcon 7B, Falcon 40B, MPT, and </span><span class="No-Break"><span class="koboSpan" id="kobo.1079.1">Llama V2</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1080.1">OpenLLM</span></strong><span class="koboSpan" id="kobo.1081.1"> (</span><a href="https://github.com/bentoml/OpenLLM"><span class="koboSpan" id="kobo.1082.1">https://github.com/bentoml/OpenLLM</span></a><span class="koboSpan" id="kobo.1083.1">): This boasts integration with Langchain</span><a id="_idIndexMarker1192"/><span class="koboSpan" id="kobo.1084.1"> and Hugging Face agents but without using </span><span class="No-Break"><span class="koboSpan" id="kobo.1085.1">acceleration/compiler libraries</span></span></li>
<li><strong class="bold"><span class="koboSpan" id="kobo.1086.1">Mlc-llm</span></strong><span class="koboSpan" id="kobo.1087.1"> (</span><a href="https://github.com/mlc-ai/mlc-llm"><span class="koboSpan" id="kobo.1088.1">https://github.com/mlc-ai/mlc-llm</span></a><span class="koboSpan" id="kobo.1089.1">): This boasts support on a variety of devices, such</span><a id="_idIndexMarker1193"/><span class="koboSpan" id="kobo.1090.1"> as </span><span class="No-Break"><span class="koboSpan" id="kobo.1091.1">mobile phones</span></span></li>
</ul>
<p><span class="koboSpan" id="kobo.1092.1">As a final point, the practical deployment example presented here wouldn’t be as effective without us following the recommendations and guidelines presented in the previous topic, so be sure to follow through with every one </span><span class="No-Break"><span class="koboSpan" id="kobo.1093.1">of them!</span></span></p>
<h1 id="_idParaDest-228"><a id="_idTextAnchor237"/><span class="koboSpan" id="kobo.1094.1">Summary</span></h1>
<p><span class="koboSpan" id="kobo.1095.1">In this chapter, we explored the various aspects of deploying DL models in production environments, focusing on key components, requirements, and strategies. </span><span class="koboSpan" id="kobo.1095.2">We discussed architectural choices, hardware infrastructure, model packaging, safety, trust, reliability, security, authentication, communication protocols, user interfaces, monitoring, and logging components, along with continuous integration </span><span class="No-Break"><span class="koboSpan" id="kobo.1096.1">and deployment.</span></span></p>
<p><span class="koboSpan" id="kobo.1097.1">This chapter also provided a step-by-step guide for choosing the right deployment options based on specific needs, such as latency, availability, scalability, cost, model hardware, data privacy, and safety requirements. </span><span class="koboSpan" id="kobo.1097.2">We also explored general recommendations for ensuring model safety, trust, and reliability, optimizing model latency, and utilizing tools that simplify the </span><span class="No-Break"><span class="koboSpan" id="kobo.1098.1">deployment process.</span></span></p>
<p><span class="koboSpan" id="kobo.1099.1">A practical tutorial on deploying a language model with ONNX, TensorRT, and NVIDIA Triton Server was presented, showcasing a minimal workflow needed for accelerated deployment using NVIDIA </span><span class="No-Break"><span class="koboSpan" id="kobo.1100.1">Triton Server.</span></span></p>
<p><span class="koboSpan" id="kobo.1101.1">By understanding and implementing the strategies and best practices presented in this chapter, you can successfully deploy DL models in production with the most sensible choice for each component required and unlock their full potential. </span><span class="koboSpan" id="kobo.1101.2">To build on this success path, we need to make sure we don’t forget about our model after we deploy it and always consider monitoring our </span><span class="No-Break"><span class="koboSpan" id="kobo.1102.1">deployed model.</span></span></p>
<p><span class="koboSpan" id="kobo.1103.1">In the next chapter, we will dive into the many aspects of monitoring that we need to consider to ensure the continued success of our machine learning </span><span class="No-Break"><span class="koboSpan" id="kobo.1104.1">use case.</span></span></p>
</div>
</body></html>