- en: Setting Up R for Deep Learning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this book, we will primarily use the following libraries for deep learning:
    **H2O**, **MXNet**, and **Keras**. We will also use the **Restricted Boltzmann
    Machine** (**RBM**) package specifically for RBMs and **deep belief networks**
    (**DBNs**). In addition, we will conclude the book by using the `ReinforcementLearning`
    package.'
  prefs: []
  type: TYPE_NORMAL
- en: In this chapter, we will install all of the previously listed packages. Each
    package can be used to train deep learning models in R. However, each has its
    particular strengths and weaknesses. We will explore the underlying architecture
    for each of these packages, which will help us to understand how they execute
    code. The packages have been created to allow R programmers to perform deep learning, with
    the exception of `RBM` and `ReinforcementLearning`, which are not written natively
    in R. This does have important implications for us to consider, starting with
    ensuring that we have all the necessary dependencies in order to install the packages.
  prefs: []
  type: TYPE_NORMAL
- en: 'This chapter will cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Installing the packages
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Preparing a sample dataset
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring Keras
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring H2O
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning and RBM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Deep learning library comparison
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: You can find the code files used in this chapter at [https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R](https://github.com/PacktPublishing/Hands-on-Deep-Learning-with-R).
  prefs: []
  type: TYPE_NORMAL
- en: Installing the packages
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Some packages can be directly installed from CRAN or GitHub, while `H2O` and
    `MXNet` are a bit more complex. We will start with the packages that are the most
    straightforward to install and then move on to those that are more complex.
  prefs: []
  type: TYPE_NORMAL
- en: Installing ReinforcementLearning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'You can install `ReinforcementLearning` by using `install.packages`, since
    there is a CRAN release for this package, with the following line of code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Installing RBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `RBM` package is only available on GitHub and is not available on CRAN,
    so the installation here is slightly different. First, you will need to install
    the `devtools` package if you do not already have it installed on your system.
    Next, use the `install_github()` function from the `devtools` package in place
    of `install.packages` in order to install the `RBM` package, as in the following
    code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: Installing Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Installing Keras is similar to the way that we installed `RBM`, with one subtle,
    but important, difference. After running `install_github()` function to download
    and install the package, you run `install_keras()` to complete the installation. From
    the Keras documentation, calling `install_keras()` function is not required if
    you would prefer to install Keras yourself.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you choose this route, the R package will find the version that you have
    installed. For this book, we will use `install_keras()` to complete installation,
    as in the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'If you would prefer to install the GPU version, then just make one change when
    calling the function, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: Running `install_keras()` will install Keras and TensorFlow in a virtual environment
    by default, except on a Windows machine where—at the time of writing—this is not
    supported, in which case a `conda` environment will be used and Anaconda will
    need to be installed on the Windows machine beforehand. By default, the CPU version
    of TensorFlow and the latest release of Keras will be installed; an optional argument
    can be added in order to install the GPU version, as in the preceding code. For
    this book, we will accept the default values and run `install_keras`.
  prefs: []
  type: TYPE_NORMAL
- en: If you have a machine with multiple versions of Python, you may discover some
    issues. If the Python instance that you wish to use is not declared, then R will
    attempt to find Python by first looking in common locations such as `usr/bin`
    and `usr/local/bin`.
  prefs: []
  type: TYPE_NORMAL
- en: 'When using Keras, you will likely want to point to the Python instance in your
    TensorFlow virtual environment. By default, the virtual environment will be called
    `r-tensorflow`. You can tell R the version of Python you would like to use by
    using the `use_python()` function from the `reticulate` package. Within the function,
    simply note the path to the instance of Python in the virtual environment. On
    my machine, this looks as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: This should look similar on your machine.
  prefs: []
  type: TYPE_NORMAL
- en: Once R has the path to the right instance of Python, the code that we will introduce
    later in this chapter should work. However, if you are not referencing the correct
    version of Python, you will encounter errors and the code will not run.
  prefs: []
  type: TYPE_NORMAL
- en: Installing H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For H2O, we will use the installation instructions from the H2O website. Using
    this approach, we will first search for any previous installations of H2O and
    remove those. Next, `RCurl` and `jsonlite` are installed and, after this, H2O
    is installed from the AWS S3 bucket containing the latest release. This is accomplished
    by simply making a change to the location of the repository when obtaining the
    package files, which, by default, is a CRAN server. We install H2O by running
    the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: Installing MXNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'There are multiple ways to install MXNet. The following code is the easiest
    installation instruction for setting up the CPU version of MXNet:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'For the GPU support, use the following installation code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: OpenCV and OpenBLAS are required to use MXNet. If you need to install these,
    you can do so through one of the following options.
  prefs: []
  type: TYPE_NORMAL
- en: 'For macOS X, Homebrew can be used to install these libraries:'
  prefs: []
  type: TYPE_NORMAL
- en: If Homebrew is not already installed, installation instructions can be found
    at [https://brew.sh/](https://brew.sh/).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'With Homebrew available, open a Terminal window and install the libraries using
    the following commands:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, as shown here, create a symbolic link to ensure that the latest version
    of OpenBLAS is being used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'For Windows, the process is slightly more involved and so the details will
    not be noted in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: To install OpenCV, follow the instructions found at [https://docs.opencv.org/3.4.3/d3/d52/tutorial_windows_install.html](https://docs.opencv.org/3.4.3/d3/d52/tutorial_windows_install.html).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To install OpenBLAS, follow the instructions found at [https://github.com/xianyi/OpenBLAS/wiki/How-to-use-OpenBLAS-in-Microsoft-Visual-Studio](https://github.com/xianyi/OpenBLAS/wiki/How-to-use-OpenBLAS-in-Microsoft-Visual-Studio).
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'With OpenCV and OpenBLAS installed, the preceding lines should work to download
    and install the MXNet package. However, if you run into errors while trying to
    load the library, then it may be necessary to build the MXNet package and then
    create the R package. The instructions to complete this are very clear and detailed,
    but they are too long to include in this book:'
  prefs: []
  type: TYPE_NORMAL
- en: '**For macOS X**: [https://mxnet.incubator.apache.org/versions/master/install/osx_setup.html](https://mxnet.incubator.apache.org/versions/master/install/osx_setup.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**For Windows**: [https://mxnet.incubator.apache.org/versions/master/install/windows_setup.html](https://mxnet.incubator.apache.org/versions/master/install/windows_setup.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If there are still issues when downloading and installing the package after
    following the steps needed to build the MXNet library and the R bindings, this
    could be due to a number of possible reasons, many of which have been documented.
    Unfortunately, trying to work through all the possible installation scenarios
    and issues is outside the scope of this book. However, to use MXNet for learning
    purposes, it is possible to use a kernel from the Kaggle website, where MXNet
    is available.
  prefs: []
  type: TYPE_NORMAL
- en: Preparing a sample dataset
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For Keras, H2O, and MXNet, we will use the adult census dataset, which uses
    U.S. Census data to predict whether someone makes more or less than USD50,000
    a year. We will perform the data preparation for the Keras and MXNet examples
    here, so we are not repeating the same code in both examples:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following code, we will load the data and label the two datasets to
    prepare for combining them:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: As a result of running the preceding code, we will now have our libraries loaded
    and ready to use. We also have the `train` and `test` data loaded, which can now
    be seen in the `Environment` pane.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, we will combine the datasets so we can make some changes to all the data
    at the same time. We will use the `complete.cases` function to remove rows with
    `NA` for the sake of simplicity for these examples. We will also remove white
    space around the character terms so that terms such as `Male` and `Male ` are
    both treated as the same term. Let''s have a look at the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now perform some extra pre-processing steps on the `train` dataset.
    First, we use the `filter()` function to extract the `train` data from the combined
    data frame labeled `all`. After that, we will extract the `target` column as a
    vector and then remove the `target` and `label` columns. We isolate the `train`
    data and `train` target variables using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'We will now separate the columns with numeric values and those with character
    values so that we can encode the character value column to prepare for a completely
    numeric matrix. We separate the numeric and character columns using the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will use the `dummyVars()` function from `caret` to pivot the character
    values within a column into separate columns and we will indicate whether a character
    string is present in the row by assigning `1` to the row. If the character string
    is not present, then the column will contain `0` for that row. We perform this
    one-hot encoding step by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'After transforming the data, we will now bind the two datasets back together
    with the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'We will then repeat the same steps for the `test` dataset by running the following
    code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'When we created our target vector, it converted the factor values to `1` and
    `2`. However, we would like this to be `1` and `0`, so we will subtract `1` from
    the vector, as in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The last step is to clean up one column from the `train` dataset because it
    is not present in the `test` dataset. We remove this particular column by running
    the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have loaded and prepared this dataset, we can use it in the next
    step to demonstrate some preliminary examples using all the packages that we have
    installed. At this point, our goal is to take a look at the syntax and ensure
    that the code can run and that the libraries are installed correctly. In later
    chapters, we will go into more depth about each package.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring Keras
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Keras was created and is maintained by Francois Chollet. Keras lays claim to
    being designed for humans, so common use cases are simple to execute and the syntax
    is clear and comprehensible. Keras is made to work with a number of lower-level
    deep learning languages and, in this book, Keras will be the interface that we
    use to utilize a number of popular deep learning backends, including TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Available functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Keras offers support for a broad array of deep learning methods, including the
    following:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Recurrent neural networks** (**RNNs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Long short-term memory** (**LSTM**) networks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Convolutional neural networks** (**CNNs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Multilayer perceptrons** (**MLPs**)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Variable autoencoders**'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This is not an exhaustive list and further support is available for additional
    methods. However, these are what will be covered in the later chapters of this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: A Keras example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will train a multilayer perceptron on the adult census
    dataset that we just prepared. This example is included to introduce the syntax
    of the package and to show that a basic exercise can be completed without an excessive
    amount of code:'
  prefs: []
  type: TYPE_NORMAL
- en: If you have multiple versions of Python installed on your system, then this
    can become an issue. Use the `reticulate` package and the `use_python()` function
    to define the path to the Python instance that you would like to use; for example,
    `use_python(usr/local/bin/python3)`. You can also use `RETICULATE_PYTHON` within
    your `.Rprofile` file to set the path to the Python instance that R should use.
  prefs: []
  type: TYPE_NORMAL
- en: 'To begin, we will load the `tensorflow` and `keras` libraries, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will convert our datasets into matrices, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can create a sequential model, which will move through each layer in
    succession. We will have one layer and then we will compile the results. We define
    our model by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'In the previous step, we defined our model and now, in the following code,
    we will fit this model to our training dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, we can evaluate our model by comparing our model results to the `test`
    target values. We evaluate model performance by running the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: This is the general syntax for `keras`. As we have shown, it is compatible with
    piping and has a syntax that will be familiar to R programmers. Next, we will
    look at an example using the MXNet package.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring MXNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: MXNet is a deep learning library designed by the Apache Software Foundation.
    It supports both imperative and symbolic programming. It is designed for speed
    by serializing functions with dependencies while running functions with no dependencies
    in parallel. It provides compatibility with CPU and GPU processors.
  prefs: []
  type: TYPE_NORMAL
- en: Available functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'MXNet provides the means to run a very extensive list of deep learning methods,
    including the following:'
  prefs: []
  type: TYPE_NORMAL
- en: CNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RNN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: GAN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: LSTM
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Autoencoders
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: RBM/DBN
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Reinforcement learning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Getting started with MXNet
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For MXNet, we will use the same prepared adult census dataset. We will also
    use a multilayer perceptron as our model. Fitting a model using MXNet will be
    quite familiar to you if you are familiar with fitting a model using a number
    of other common machine learning packages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we will load the MXNet package with the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we will define our multilayer perceptron. A `seed` value is set for reproducibility
    purposes. Afterward, the training data is converted to a data matrix and passed
    as an argument to the model, along with the training target values, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, we will make our predictions by applying the model to a data matrix version
    of the `test` data, as in the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then use a confusion matrix to evaluate performance, with the adjusted
    target class on the *y*-axis and the predicted results on the *x*-axis, as shown:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: The syntax for MXNet should look familiar to those with experience with doing
    machine learning coding in R. The function to train the model takes in descriptive
    and target data, as well as captures values for a number of options, just like
    using RandomForest or XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: The options are slightly different and we will cover how to best assign values
    to these arguments in a later chapter. However, the syntax is quite similar to
    the syntax used for other machine learning libraries in R. Next, we will write
    the code to train a minimal model using H2O.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring H2O
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'H2O has been around longer than Keras and MXNet and is still used widely. It
    makes use of Java and MapReduce in-memory compression to handle big datasets.
    H2O is used for many machine learning tasks and also supports deep learning. In
    particular, H2O provides native support for feedforward artificial neural networks
    (multilayer perceptrons). H2O performs automatic data preparation and missing
    value handling. Loading data requires the use of a special data type: `H2OFrame`.'
  prefs: []
  type: TYPE_NORMAL
- en: Available functions
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: H2O only natively supports feedforward neural networks. Compared with the other
    main packages for deep learning, this creates an obvious limitation for this library.
    However, this is a very common deep learning implementation. In addition, H2O
    allows for large objects to be stored outside memory in the H2O cluster. For these
    reasons, H2O is still a valuable library to know about when studying deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: An H2O example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For this example, we will again use the adult census dataset to predict income.
    As with our Keras example, this will be kept extremely minimal and we will cover
    just enough to illustrate the syntax for working with H2O, as well as the design
    nuances that differ from other packages:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first major difference when working with H2O is that we must explicitly
    initialize our H2O session, which will generate a Java Virtual Machine instance
    and connect it with R. This is accomplished with the following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'Loading data to use with H2O requires converting the data to `H2OFrame`. `H2OFrame`
    is very similar to data frames, with the major distinction having to do with where
    the object is stored. While data frames are held in memory, `H2OFrame` is stored
    on the H2O cluster. This feature can be an advantage with very large datasets.
    In the following example, we will convert the data into the proper format using
    a two-step process. First, we load the data by reading `csv` in the usual way.
    Second, we will convert the data frames to `H2OFrame`. We convert our data into
    the proper format using the following code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'For this example, we will perform some imputation as the sole pre-processing
    step. In this step, we will replace all missing values and we will use `mean`
    for numeric data and `mode` for factor data. In H2O, setting `column = 0` will
    apply the function to the entire frame. Of note is that the function is called
    on the data; however, it is not necessary to assign the results to a new object
    as the imputations will be directly reflected in the data passed through as an
    argument to the function. It is also worth highlighting that in H2O, we can pass
    a vector to the method argument and it will be used for every variable in this
    case by first checking whether the first method can be used and, if not, moving
    on to the second method. Pre-processing this data is accomplished by running the
    following lines of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'In addition, in this step, we will define the `dependent` and `independent`
    variables. The `dependent` variable is held in the `target` column, while all
    the remaining columns contain the `independent` variables, which will be used
    for predicting the `target` variable during this task:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'With all of the preparation steps complete, we can now create a minimal model.
    The H2O `deeplearning` function will create a feedforward artificial neural network.
    In this example, just the minimum required to run the model will be included.
    However, this function can accept 80 to 90 arguments and we will cover many of
    these in the later chapters. In the following code, we provide a name for our
    model, identify the training data, set a seed for reproducibility through replicating
    pseudo-random numbers involved in the model, define the `dependent` and `independent`
    variables, and note the number of times the model should be run and how the data
    should be cut for each round:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'After running the model, the performance can be evaluated on the out-of-fold
    samples using the following line of code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, when our model is complete, the cluster must be explicitly shut down
    just as it was initialized. The following function will close the current `h2o` instance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'We can observe the following in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: The syntax for H2O varies quite a bit from other machine learning libraries.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: First, we need to initiate the Java Virtual Machine and we need to store our
    data in special data containers with this package.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In addition, we can see that imputation happens by running the function on a
    data object without assigning the changes back to an object.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We can see that we also need to include all the independent variable column
    names, which is slightly different from other models.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: All of this is to say that H2O may feel a little unfamiliar as you use it. It
    is also limited in terms of the algorithms available. However, the ability to
    work with larger datasets is a definite advantage to this package.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Now that we have looked at the comprehensive deep learning packages, we will
    focus on packages written with R that perform a specific modeling task or a limited
    set of tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring ReinforcementLearning and RBM
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The `ReinforcementLearning` and `RBM` packages differ from the libraries already
    covered in two important ways: first, they are specialized packages that have
    functions for only one specific deep learning task instead of attempting to support
    myriad deep learning options, and second, they are completely written in R and
    have no additional language dependencies. This can be an advantage as the complexity
    of the previous libraries means that the packages can break when changes happen
    outside the package. The support pages for these libraries are full of examples
    of installation FAQs and troubleshooting instructions, as well as some cases where
    a given package may suddenly stop working or become deprecated. In these cases,
    we encourage you to continue searching CRAN and other sites as the R community
    is well known for its dynamic, evolving, and robust support and development.'
  prefs: []
  type: TYPE_NORMAL
- en: Reinforcement learning example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this example, we will create a sample environment for reinforcement learning.
    The concept of reinforcement learning will be explored in more detail in a later
    chapter. For this example, we will generate a series of states and actions, along
    with the reward for taking those actions, that is, whether taking the action led
    to the desired result or a negative consequence. Afterward, we will define how
    our agent should respond or learn from actions. Once all of this has been defined,
    we will run the program and the agent will navigate through the environment to
    learn to solve the task. We will define and run a minimal reinforcement learning
    example by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: 'We can see the following in this example:'
  prefs: []
  type: TYPE_NORMAL
- en: The syntax is very familiar and similar to many other R packages that we might
    use. In addition, we can see that we can complete a simple reinforcement learning
    task using a minimal amount of code.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In the GitHub repository for the package, all of the functions are written in
    R, which provides the convenience to explore the possible reasons for issues if
    they arrive. This also alleviates any concerns over dependencies on additional
    languages present in the more complex packages noted previously.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: An RBM example
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Here is a simple example using the `RBM` package. RBMs can be created using
    the `MXNet` library. However, we include this package in this book to note when
    it makes the most sense to train an `RBM` model using `MXNet` and when a standalone
    implementation of the algorithm may be a better fit.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the following example, we assign the `train` Fashion MNIST dataset to an
    object, create an RBM model on this data, and then make a prediction using the
    modeled results. A detailed exploration of how the RBM algorithm achieves this
    outcome and suggested applications are included in a future chapter. We will see
    how simply we can train this model and use it for prediction with a familiar syntax
    by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'As with the `ReinforcementLearning` package, the following applies:'
  prefs: []
  type: TYPE_NORMAL
- en: RBM is written entirely in R, so exploring the code in the repository is an
    excellent way to better understand how this particular technique works.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Also, as noted before, if you only need to train a model using RBM, then using
    a standalone package can be a great way to avoid having to load too many unnecessary
    functions, as is the case when using a library such as MXNet.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Both comprehensive packages and standalone packages have their place in a deep
    learning workflow, so this book will highlight the advantages and disadvantages
    of each of them.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Comparing the deep learning libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'When comparing the three comprehensive machine learning libraries highlighted
    in this chapter (Keras, H2O, and MXNet), there are three primary differences:
    external language dependencies, functions, and syntax (ease of use and cognitive
    load). We will now cover each of these main differences in turn.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first major difference between the three packages is the external language
    dependencies for each. As mentioned earlier, none of these packages are written
    in R. What this means is that you will need additional languages installed on
    your machine in order for these packages to work. It also means that you cannot
    easily look at the source documentation to see how a particular function works
    or why you are receiving a certain error (unless you know one of the languages,
    of course). The packages are written using the following languages: Keras in Python,
    H2O in Java, and MXNet in C#.'
  prefs: []
  type: TYPE_NORMAL
- en: The next major difference has to do with the types of models that can be implemented
    from each package. You can use all three packages to train a feedforward model,
    such as a multilayer perceptron, where all hidden layers are fully connected layers.
    Keras and MXNet allow you to train deep learning models that include different
    types of hidden layers, as well as feedback loops between layers. These include
    RNNs, LSTMs, and CNNs. MXNet offers support for additional algorithms, including
    GANs, RBMs/DBNs, and reinforcement learning.
  prefs: []
  type: TYPE_NORMAL
- en: The last major difference has to do with the syntax for the model. Some use
    a very familiar syntax, which, in turn, makes them easier to learn and use. There
    is less for you to remember when the code resembles other R code that you use
    for other purposes. To this end, Keras has a very familiar syntax. It is modular,
    which means every function performs a discrete step within the overall model and
    all the functions can be piped together.
  prefs: []
  type: TYPE_NORMAL
- en: This closely resembles the way that `tidyverse` functions are chained together
    for data preparation. MXNet follows a syntax similar to other machine learning
    packages, where a dataset and target variable vector are passed to the function
    to train the model, along with numerous additional arguments that control how
    the model is created. The syntax for H2O deviates the furthest from common R programming
    conventions. It requires a cluster to be initialized ahead of any modeling. Data
    must also be stored in specific data objects and some functions operate on a data
    object just by calling a function on that object without assigning the results
    to a new object, as in typical R programming.
  prefs: []
  type: TYPE_NORMAL
- en: Aside from these differences, Keras also provides a means for using TensorFlow,
    while H2O allows larger objects to be stored out of memory. As noted, MXNet has
    the most robust offering of deep learning algorithms. As we can see, each package
    has its advantages and, throughout this book, we will explore them in depth and
    note the most suitable use cases and applications along the way.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Having completed this chapter, you should now have all of the libraries that
    will be used in this book installed. In addition, you should be familiar with
    the syntax for each of them, and you should have seen a preliminary example of
    how to train a model using each one. We also explored some of the differences
    between the deep learning libraries, noting their strengths as well as their limitations.
    The three main packages (Keras, MXNet, and H2O) are widely used for deep learning
    in industry and academia, and an understanding of these will enable you to tackle
    a number of deep learning problems. We are now ready to explore them all in more
    depth. However, before we do, we will review neural networks—the building block
    for all deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: In the following chapter, you will learn about artificial neural networks, which
    comprise the base building block for all deep learning. We will not use these
    deep learning libraries yet in the next chapter; however, the basics of how neural
    networks are coded will be critical as we move forward. Everything we cover in
    the next chapter will carry forward and be useful as we code examples of deep
    learning models. All deep learning cases are variations of the basic neural network,
    which we will learn how to create in the next chapter.
  prefs: []
  type: TYPE_NORMAL
