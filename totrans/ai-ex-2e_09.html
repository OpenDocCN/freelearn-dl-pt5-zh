<html><head></head><body>
  <div id="_idContainer142">
    <h1 class="chapterNumber">9</h1>
    <h1 id="_idParaDest-158" class="chapterTitle">Abstract Image Classification with Convolutional Neural Networks (CNNs)</h1>
    <p class="normal">The invention<a id="_idIndexMarker410"/> of <strong class="bold">convolutional neural networks</strong> (<strong class="bold">CNNs</strong>) applied to vision represents by far one of the most innovative achievements in the history of applied mathematics. With their multiple layers (visible and hidden), CNNs have brought artificial intelligence from machine learning to deep learning.</p>
    <p class="normal">In <em class="italics">Chapter 8</em>, <em class="italics">Solving the XOR Problem with a Feedforward Neural Network</em>, we saw that <em class="italics">f</em>(<em class="italics">x</em>, <em class="italics">w</em>) is the building block of any neural network. A function <em class="italics">f</em> will transform an input <em class="italics">x</em> with weights <em class="italics">w</em> to produce an output. This output can be used as such or fed into another layer. In this chapter, we will generalize this principle and introduce several layers. At the same time, we will use datasets with images. We will have a dataset for training and a dataset for validation to confirm that our model works.</p>
    <p class="normal">A CNN relies on two basic tools of linear algebra: kernels and functions, applying them to convolutions as described in this chapter. These tools have been used in mathematics for decades.</p>
    <p class="normal">However, it took the incredible imagination of Yann LeCun, Yoshua Bengio, and others—who built a mathematical model of several layers—to solve real-life problems with CNNs.</p>
    <p class="normal">This chapter describes the marvels of CNNs, one of the pillars of <strong class="bold">artificial neural networks</strong> (<strong class="bold">ANNs</strong>). A CNN will <a id="_idIndexMarker411"/>be built from scratch, trained, and saved. The classification model described will detect production failures on a food-processing production line. Image detection will go beyond object recognition and produce abstract results in the form of concepts.</p>
    <p class="normal">A Python TensorFlow 2 program will be built layer by layer and trained. Additional sample programs will illustrate key functions.</p>
    <p class="normal">The following topics will be covered in this chapter:</p>
    <ul>
      <li class="list">The differences between 1D, 2D, and 3D CNNs</li>
      <li class="list">Adding layers to a convolutional neural network</li>
      <li class="list">Kernels and filters</li>
      <li class="list">Shaping images</li>
      <li class="list">The ReLU activation function</li>
      <li class="list">Kernel initialization</li>
      <li class="list">Pooling</li>
      <li class="list">Flattening</li>
      <li class="list">Dense layers</li>
      <li class="list">Compiling the model</li>
      <li class="list">The cross-entropy loss function</li>
      <li class="list">The Adam optimizer</li>
      <li class="list">Training the model</li>
      <li class="list">Saving the model</li>
      <li class="list">Visualizing the PNG of a model</li>
    </ul>
    <p class="normal">We'll begin by introducing CNNs and defining what they are.</p>
    <h1 id="_idParaDest-159" class="title">Introducing CNNs</h1>
    <p class="normal">This section <a id="_idIndexMarker412"/>describes the basic components of a CNN. <code class="Code-In-Text--PACKT-">CNN_SRATEGY_MODEL.py</code> will illustrate the basic CNN components used to build a model for abstract image detection. For machines, as for humans, concepts are the building blocks of cognition. CNNs constitute one of the pillars of deep learning (multiple layers and neurons).</p>
    <p class="normal">In this chapter, TensorFlow 2 with Python will be running using Keras libraries that are now part of TensorFlow. If you do not have Python or do not wish to follow the programming exercises, the chapter is self-contained, with graphs and explanations.</p>
    <h2 id="_idParaDest-160" class="title">Defining a CNN</h2>
    <p class="normal">A convolutional <a id="_idIndexMarker413"/>neural network processes information, such as an image, for example, and makes sense out of it.</p>
    <p class="normal">For example, imagine you have to represent the sun with an ordinary pencil and a piece of paper. It is a sunny day, and the sun is shining very brightly—too brightly. You put on a special pair of very dense sunglasses. Now you can look at the sun for a few seconds. You have just applied a color reduction filter, one of the first operations of a convolutional network.</p>
    <p class="normal">Then, you try to draw the sun. You draw a circle and put some gray in the middle. You have just applied an edge filter. Finally, you go over the circle several times to make it easy to recognize, progressively reducing what you saw into a representation of it. Now, with the circle, some gray in the middle, and a few lines of rays around it, anybody can see you drew the sun. You smile; you did it! You took a color image of the sun and made a mathematical representation of it as a circle, which would probably look something like this:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_01.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.1: Mathematical representation of a circle</p>
    <p class="normal">You just went through the basic processes of a convolutional network.</p>
    <p class="normal">The word <strong class="bold">convolutional</strong> means that <a id="_idIndexMarker414"/>you transformed the sun you were looking at into a drawing, area by area. But, you did not look at the whole sky at once. You made many eye movements to capture the sun, area by area, and you did the same when drawing. If you made a mathematical representation of the way you transformed each area from your vision to your paper abstraction, it would be a kernel. You can see that the convolutional operation converts an object into a more abstract representation. This is not limited to images but can apply to any type of data (words, sounds and video) we want to draw patterns from.</p>
    <p class="normal">With that concept in mind, the following graph shows the successive mathematical steps to follow in this chapter's model for a machine to process an image just as you did. A convolutional network is a succession of steps that will transform what you see into a classification status.</p>
    <p class="normal">In the graph, each box<a id="_idIndexMarker415"/> represents a layer. Each layer has an input that comes from the previous layer. Each layer will then transform the input and then produce an output that will become the input of the next layer. At each layer, the key features that are necessary to classify the images will be isolated.</p>
    <p class="normal">In your example, it would serve to find out whether your drawing represents the sun or not. This falls under a binary classification model (yes or no, or 1 or 0).</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_02.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.2: Architecture of a CNN</p>
    <p class="normal">Notice that the size of the <a id="_idIndexMarker416"/>outputs diminishes progressively until the outputs reach 1, the binary classification status that will return (1 or 0). These successive steps, or layers, represent what you did when you went from observing the sun to drawing it. In the end, if we draw poorly and nobody recognizes the sun, it means that we'll have to go back to step 1 and change some parameters (weights in this case). That way, we train to represent the sun better until somebody says, "Yes, it is a sun!" That is probability = 1. Another person may say that it is not a sun (probability = 0). In that case, more training would be required.</p>
    <p class="normal">If you carry out this experiment of drawing the sun, you will notice that, as a human, you transform one area at a time with your eye and pencil. You repeat the way you do it in each area. The mathematical repetition you perform is your <strong class="bold">kernel</strong>. Using a kernel per area is the fastest way to draw. For us humans, in fact, it is the only way we can draw. A CNN is based on this process.</p>
    <p class="normal">In this section, we <a id="_idIndexMarker417"/>looked at some key aspects of a CNN model, using the analogy of representing the sun as a drawing. This is just one way to start a convolutional neural network, and there are hundreds of different ways to do so. However, once you understand one model, you will have the understanding necessary to implement other variations.</p>
    <p class="normal">In the following section, we'll see how to initialize and build our own CNN.</p>
    <h2 id="_idParaDest-161" class="title">Initializing the CNN</h2>
    <p class="normal"><code class="Code-In-Text--PACKT-">CNN_SRATEGY_MODEL.py</code> builds the CNN using TensorFlow 2. TensorFlow 2 has made tremendous improvements in<a id="_idIndexMarker418"/> terms of development. The Keras datasets, layers, and models are now part of the TensorFlow instance:</p>
    <pre class="programlisting"><code class="hljs elm"><span class="hljs-keyword">import</span> tensorflow <span class="hljs-keyword">as</span> tf
<span class="hljs-title">from</span> tensorflow.keras <span class="hljs-keyword">import</span> datasets, layers, models
</code></pre>
    <p class="normal">The CNN only requires two lines of headers to build the layers! In TensorFlow 2, for each layer, we simply have to call <code class="Code-In-Text--PACKT-">layers.&lt;add your layer here&gt;</code> and that's it!</p>
    <p class="normal">The model used is a Keras <code class="Code-In-Text--PACKT-">sequential()</code> called from the TensorFlow <code class="Code-In-Text--PACKT-">from tensorflow.keras</code> instance:</p>
    <pre class="programlisting"><code class="hljs ini"><span class="hljs-attr">classifier</span> = models.Sequential()
</code></pre>
    <p class="normal">And that's it. We have just started to build our own CNN in just a few lines of code. TensorFlow 2 has simplified the whole process of creating a CNN, making it an easy, intuitive process, as we will see throughout this chapter.</p>
    <p class="normal">Let's begin to build upon the foundations of our CNN in the following section and add a convolutional layer.</p>
    <h2 id="_idParaDest-162" class="title">Adding a 2D convolution layer</h2>
    <p class="normal">In this chapter, we will be <a id="_idIndexMarker419"/>using a two-dimensional model as our example. Two-dimensional relationships can be real-life <a id="_idIndexMarker420"/>images and also many other objects, as described in this chapter. This chapter describes a two-dimensional network, although others exist:</p>
    <ul>
      <li class="list">A one-dimensional CNN mostly describes a temporal mode, for example, a sequence of sounds (phonemes = parts of words), words, numbers, and any other type of sequence.</li>
      <li class="list">A volumetric module is a 3D convolution, such as recognizing a cube or a video. For example, for a self-driving car, it is critical to recognize the difference between a 2D picture of a person in an advertisement near a road and a real 3D image of a pedestrian that is starting to cross the same road!</li>
    </ul>
    <p class="normal">In this chapter, a spatial 2D convolution module will be applied to images of different kinds. The main program, <code class="Code-In-Text--PACKT-">CNN_STRATEGY_MODEL.py</code>, will describe how to build and save a model.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">classifier.add</code> will add a layer to the model. The name <strong class="bold">classifier</strong> does not represent a function but simply the<a id="_idIndexMarker421"/> arbitrary name that was given to this model in this particular program. The model will end up with <em class="italics">n</em> layers. Look at the following line of code:</p>
    <pre class="programlisting"><code class="hljs angelscript">classifier.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>),input_shape = (<span class="hljs-number">64</span>, <span class="hljs-number">64</span>, <span class="hljs-number">3</span>), activation = <span class="hljs-string">'relu'</span>))
</code></pre>
    <p class="normal">This line of code contains a lot of information: the filters (applied with kernels), the input shape, and an activation function. The function contains many more options. Once you understand these in-depth, you can implement other options one by one, as you deem necessary, for each project you have to work on.</p>
    <h3 id="_idParaDest-163" class="title">Kernel</h3>
    <p class="normal">Just to <a id="_idIndexMarker422"/>get started, intuitively, let's take another everyday model. This<a id="_idIndexMarker423"/> model is a bit more mathematical and closer to a CNN's kernel representation. Imagine a floor of very small square tiles in an office building. You would like each floor tile to be converted from dirty to clean, for example.</p>
    <p class="normal">You can imagine a cleaning machine capable of converting 3×3 small tiles (pixels) one at a time from dirty to clean. You would laugh if you saw somebody come with one enormous cleaning machine to clean all of the 32×32 tiles (pixels) at the same time. You know it would be very bulky, slow, and difficult to use, intuitively. On top of that, you would need one big machine per surface size! Not only is a kernel an efficient way to filter, but a kernel convolution is also a time-saving resource process. The small cleaning machine is <a id="_idIndexMarker424"/>the kernel (dirty-to-clean filter), which will save you time performing the convolution (going over all of the tiles to clean a 3×3 area), transforming the floor from dirty to clean.</p>
    <p class="normal">In this case, 32 different filters have been added with 3×3 sized kernels:</p>
    <pre class="programlisting"><code class="hljs angelscript">classifier.add(layers.Conv2D(<span class="hljs-number">32</span>, (<span class="hljs-number">3</span>, <span class="hljs-number">3</span>)...
</code></pre>
    <p class="normal">The use of kernels as filters is the core of a convolutional network. <code class="Code-In-Text--PACKT-">(32, (3,3))</code> means <code class="Code-In-Text--PACKT-">(number of filters, (size of kernels))</code>.</p>
    <h4 class="title">An intuitive approach</h4>
    <p class="normal">To understand a <a id="_idIndexMarker425"/>kernel intuitively, keep the sun and cleaning tiles examples in mind. In this section, a photograph of a cat will show how kernels work.</p>
    <p class="normal">In a model analyzing cats, the initial photograph would look like this:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_03.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.3: Cat photograph for model analysis</p>
    <p class="normal">On the first run of this layer, even with no training, an untrained kernel would transform the photograph:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_04.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.4: Cat photograph transformation</p>
    <p class="normal">The first layer has already begun isolating the features of the cat. The edges have begun to appear: the cat's body, ears, nose, and eyes. In itself, this first filter (one of 32) with a size 3×3 kernel—in the first layer and with no training—already produces effective results. The size of a <a id="_idIndexMarker426"/>kernel can vary according to your needs. A 3×3 kernel will require a larger number of weights than a 1×1 kernel, for example. A 1×1 kernel will have only one weight, which restricts the size of the features to represent. The rule is that the smaller the kernel, the fewer weights we have to find. It will also perform a feature reduction. When the size of the kernel increases, the number of weights and features to find increases as well as the number of features represented.</p>
    <p class="normal">Each subsequent layer will make the features stand out much better, with smaller and smaller matrices and vectors, until the program obtains a clear mathematical representation.</p>
    <p class="normal">Now that we have an intuitive view of how a filter works, let's explore a developer's approach.</p>
    <h4 class="title">The developers' approach</h4>
    <p class="normal">Developers like to <a id="_idIndexMarker427"/>see the result first to decide how to approach a problem.</p>
    <p class="normal">Let's take a quick, tangible shortcut to understand kernels through <code class="Code-In-Text--PACKT-">Edge_detection_Kernel.py</code> with an edge detection kernel:</p>
    <pre class="programlisting"><code class="hljs angelscript">#I.An edge detection kernel
kernel_edge_detection = np.<span class="hljs-built_in">array</span>([[<span class="hljs-number">0.</span>,<span class="hljs-number">1.</span>,<span class="hljs-number">0.</span>],
[<span class="hljs-number">1.</span>,<span class="hljs-number">-4.</span>,<span class="hljs-number">1.</span>],
[<span class="hljs-number">0.</span>,<span class="hljs-number">1.</span>,<span class="hljs-number">0.</span>]])
</code></pre>
    <p class="normal">The kernel is a 3×3 matrix, like the cat example. But the values are preset, and not trained with weights. There is no learning here; only a matrix needs to be applied. The major difference with a CNN is that it will learn how to optimize kernels itself through weights and biases.</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">img.bmp</code> is loaded, and the 3×3 matrix is applied to the pixels of the loaded image, area by area:</p>
    <pre class="programlisting"><code class="hljs ini"><span class="hljs-comment">#II.Load image and convolution</span>
<span class="hljs-attr">image</span>=mpimg.imread(<span class="hljs-string">'img.bmp'</span>)[:,:,<span class="hljs-number">0</span>]
<span class="hljs-attr">shape</span> = image.shape
</code></pre>
    <p class="normal">The image before the <a id="_idIndexMarker428"/>convolution applying the kernel is the letter <strong class="bold">A</strong> (letter recognition):</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_05.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.5: The letter "A"</p>
    <p class="normal">Now the convolution transforms the image, as shown in the following code:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment">#III.Convolution</span>
image_after_kernel = filter.convolve(image,kernel_edge_detection,<span class="hljs-attribute">mode</span>=<span class="hljs-string">'constant'</span>, <span class="hljs-attribute">cval</span>=0)
</code></pre>
    <p class="normal">The edges of A now appear clearly in white, as shown in the following graph:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_06.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.6: The white edges of A are visible</p>
    <p class="normal">The original image on top displayed a very thick A. The preceding graph displays a thin, identifiable A feature through thin edges that a neural network can classify within a few mathematical operations. The first layers of a convolutional network train to find the right weights to<a id="_idIndexMarker429"/> generate the right kernel automatically.</p>
    <p class="normal">Now that we have an intuitive and practical developer's view of a filter, let's add some mathematics to our approach.</p>
    <h4 class="title">A mathematical approach</h4>
    <p class="normal">The initial<a id="_idIndexMarker430"/> image has a set of values you can display, as follows:</p>
    <pre class="programlisting"><code class="hljs processing">#II.Load <span class="hljs-built_in">image</span>
<span class="hljs-built_in">image</span>=mpimg.imread(<span class="hljs-string">'img.bmp'</span>)[:,:,<span class="hljs-number">0</span>]
<span class="hljs-built_in">shape</span> = <span class="hljs-built_in">image</span>.<span class="hljs-built_in">shape</span>
<span class="hljs-built_in">print</span>(<span class="hljs-string">"image shape"</span>,<span class="hljs-built_in">shape</span>)
</code></pre>
    <p class="normal">The code will print a numerical output of the image, as follows:</p>
    <pre class="programlisting"><code class="hljs angelscript">image shape (<span class="hljs-number">100</span>, <span class="hljs-number">100</span>)
image before convolution
[[<span class="hljs-number">255</span> <span class="hljs-number">255</span> <span class="hljs-number">255</span> ..., <span class="hljs-number">255</span> <span class="hljs-number">255</span> <span class="hljs-number">255</span>]
 [<span class="hljs-number">255</span> <span class="hljs-number">255</span> <span class="hljs-number">255</span> ..., <span class="hljs-number">255</span> <span class="hljs-number">255</span> <span class="hljs-number">255</span>]
 [<span class="hljs-number">255</span> <span class="hljs-number">255</span> <span class="hljs-number">255</span> ..., <span class="hljs-number">255</span> <span class="hljs-number">255</span> <span class="hljs-number">255</span>]
 ...,
</code></pre>
    <p class="normal">The convolution filter is applied using <code class="Code-In-Text--PACKT-">filter.convolve</code>, a mathematical function, to transform the image and filter it.</p>
    <p class="normal">The convolution filter function uses several variables:</p>
    <ul>
      <li class="list">The spatial index for the 3×3 kernel to apply; in this case, it must know how to access the data. This is performed through a spatial index, <em class="italics">j</em>, which manages data in grids. Databases also use spatial indexes to access data. The axes of those grids determine the density of a spatial index. Kernels and the image are convolved using <em class="italics">j</em> over <em class="italics">W</em>, the weights kernel.</li>
      <li class="list"><em class="italics">W</em> is the <a id="_idIndexMarker431"/>weights kernel.</li>
      <li class="list"><em class="italics">I</em> is the input image.</li>
      <li class="list"><em class="italics">k</em> is the coordinate of the center of <em class="italics">W</em>. The default value is 0 in this case.</li>
    </ul>
    <p class="normal">These variables then enter the <code class="Code-In-Text--PACKT-">filter.convolve</code> function as represented by the following equation:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_001.png" alt=""/></figure>
    <p class="normal">A CNN relies on kernels. Take all the time you need to explore convolutions through the three dimensions required to master AI: an intuitive approach, development testing, and mathematical representation.</p>
    <p class="normal">Now that we have a mathematical idea on how a convolutional filter works, let's determine the shape and the activation function to the convolutional layer.</p>
    <h3 id="_idParaDest-164" class="title">Shape</h3>
    <p class="normal"><code class="Code-In-Text--PACKT-">input_shape</code> defines the <a id="_idIndexMarker432"/>size of the image, which is 64×64 pixels (height×width), as shown here:</p>
    <pre class="programlisting"><code class="hljs routeros">classifier.<span class="hljs-builtin-name">add</span>(<span class="hljs-built_in">..</span>.input_shape = (64, 64, 3)<span class="hljs-built_in">..</span>.)
</code></pre>
    <p class="normal"><code class="Code-In-Text--PACKT-">3</code> indicates the number of channels. In this case, <code class="Code-In-Text--PACKT-">3</code> indicates the three parameters of an RGB color. Each channel can have a given value of 0 to 255.</p>
    <h3 id="_idParaDest-165" class="title">ReLU</h3>
    <p class="normal">Activation functions provide<a id="_idIndexMarker433"/> useful ways to influence the transformation of weighted data calculations. Their output will change the course of classification, a prediction, or whatever goal the network was built for. This model applies a <strong class="bold">rectified linear unit</strong> (<strong class="bold">ReLU</strong>), as shown in the following <a id="_idIndexMarker434"/>code:</p>
    <pre class="programlisting"><code class="hljs routeros">classifier.<span class="hljs-builtin-name">add</span>(<span class="hljs-built_in">..</span>., activation = <span class="hljs-string">'relu'</span>))
</code></pre>
    <p class="normal">ReLU activation functions apply variations of the following function to an input value:</p>
    <p class="center"><em class="italics">f</em>(<em class="italics">x</em>) = max{0, <em class="italics">x</em>}</p>
    <p class="normal">The function returns 0 for negative values; it returns positive values as <em class="italics">x</em>; it returns 0 for 0 values. Half of the domain of the function will return zeros. This means that when you provide positive values, the derivative will always be 1. ReLU avoids the squashing effect of the logistic sigmoid function, for example. However, the decision to use one activation function rather than another will depend on the goal of each ANN model.</p>
    <p class="normal">In mathematical terms, a <strong class="bold">rectified linear unit (ReLU)</strong> function will take all the negative values and apply 0 to them. And all the positive values remain unchanged.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">ReLU.py</code> program provides some functions, including a NumPy function, to test how ReLU works.</p>
    <p class="normal">You can enter test values or use the ones in the source code:</p>
    <pre class="programlisting"><code class="hljs routeros">import numpy as np
<span class="hljs-attribute">nx</span>=-3
<span class="hljs-attribute">px</span>=5
</code></pre>
    <p class="normal"><code class="Code-In-Text--PACKT-">nx</code> expects a negative value, and <code class="Code-In-Text--PACKT-">px</code> expects a positive value for testing purposes for the <code class="Code-In-Text--PACKT-">relu(x)</code> and <code class="Code-In-Text--PACKT-">lrelu(x)</code> functions. Use the <code class="Code-In-Text--PACKT-">f(x)</code> function if you wish to include zeros in your testing session.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">relu(x)</code> function will calculate the ReLU value:</p>
    <pre class="programlisting"><code class="hljs ruby"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">relu</span><span class="hljs-params">(x)</span></span>:
    <span class="hljs-keyword">if</span>(x&lt;=<span class="hljs-number">0</span>)<span class="hljs-symbol">:ReLU=</span><span class="hljs-number">0</span>
    <span class="hljs-keyword">if</span>(x&gt;<span class="hljs-number">0</span>)<span class="hljs-symbol">:ReLU=x</span>
    <span class="hljs-keyword">return</span> ReLU
</code></pre>
    <p class="normal">In this case, the program will return the following result:</p>
    <pre class="programlisting"><code class="hljs angelscript">negative x= <span class="hljs-number">-3</span> positive x= <span class="hljs-number">5</span>
ReLU nx= <span class="hljs-number">0</span>
ReLU px= <span class="hljs-number">5</span>
</code></pre>
    <p class="normal">The result of a negative value becomes 0, and a positive value remains unchanged. The derivative or slope is thus always 1, which is practical in many cases and provides good visibility when debugging a CNN or any other ANN.</p>
    <p class="normal">The NumPy function, defined as follows, will provide the same results:</p>
    <pre class="programlisting"><code class="hljs ruby"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">f</span><span class="hljs-params">(x)</span></span>:
    vfx=np.maximum(<span class="hljs-number">0</span>.<span class="hljs-number">1</span>,x)
    <span class="hljs-keyword">return</span> vfx
</code></pre>
    <p class="normal">Through trial and error, ANN research has come up with several variations of ReLU.</p>
    <p class="normal">One important example occurs <a id="_idIndexMarker435"/>when many input values are negative. ReLU will constantly produce zeros, making gradient descent difficult, if not impossible.</p>
    <p class="normal">A clever solution was found using a leaky ReLU. A leaky ReLU does not return 0 for a negative value but a small value you can choose, 0.1 instead of 0, for example. See the following equation:</p>
    <p class="center"><em class="italics">f</em>(<em class="italics">x</em>) = max{0.1, <em class="italics">x</em>}</p>
    <p class="normal">The leaky ReLU fixes the problem of "dying" neurons. Suppose you have a layer that keeps returning negative values when activating neurons. The ReLU activation will always return 0 in this case. That means that these neurons are "dead." They will never be activated. To avoid these "dying" neurons, a leaky ReLU provides the small positive value seen previously (0.1) that makes sure that a neuron does not "die."</p>
    <p class="normal">Now gradient descent will work fine. In the sample code, the function is implemented as follows:</p>
    <pre class="programlisting"><code class="hljs ruby"><span class="hljs-function"><span class="hljs-keyword">def</span> <span class="hljs-title">lrelu</span><span class="hljs-params">(x)</span></span>:
    <span class="hljs-keyword">if</span>(x&lt;<span class="hljs-number">0</span>)<span class="hljs-symbol">:lReLU=</span><span class="hljs-number">0</span>.<span class="hljs-number">01</span>
    <span class="hljs-keyword">if</span>(x&gt;<span class="hljs-number">0</span>)<span class="hljs-symbol">:lReLU=x</span>
    <span class="hljs-keyword">return</span> lReLU
</code></pre>
    <p class="normal">Although many other variations of ReLU exist, with this in mind, you have an idea of what it does.</p>
    <p class="normal">Enter some values of your own, and the program will display the results, as shown here:</p>
    <pre class="programlisting"><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"negative x="</span>,nx,<span class="hljs-string">"positive x="</span>,px)</span></span>
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"ReLU nx="</span>,relu(nx)</span></span>)
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"ReLU px="</span>,relu(px)</span></span>)
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"Leaky ReLU nx="</span>,lrelu(nx)</span></span>)
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"f(nx) ReLu="</span>,f(nx)</span></span>)
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"f(px) ReLu="</span>,f(px)</span></span>)
<span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"f(0):"</span>,f(<span class="hljs-number">0</span>)</span></span>)
</code></pre>
    <p class="normal">The results will display the ReLU results as follows:</p>
    <pre class="programlisting"><code class="hljs angelscript">negative x= <span class="hljs-number">-3</span> positive x= <span class="hljs-number">5</span>
ReLU nx= <span class="hljs-number">0</span>
ReLU px= <span class="hljs-number">5</span>
Leaky ReLU nx= <span class="hljs-number">0.01</span>
</code></pre>
    <p class="normal">We have processed a <a id="_idIndexMarker436"/>large representation of the input image. We now need to reduce the size of our representation to obtain a better, more abstract representation. By pooling some of the pixels we will also reduce the calculations of the subsequent layers.</p>
    <h2 id="_idParaDest-166" class="title">Pooling</h2>
    <p class="normal">A CNN<a id="_idIndexMarker437"/> contains hidden<a id="_idIndexMarker438"/> layers. The input is visible. Then as the layers work to transform the data, "hidden" work goes on. The output layer is visible again. Let's continue to explore the "hidden" layers! Pooling reduces the size of an input representation, in this case, an image. Max pooling consists of applying a max pooling window to a layer of the image:</p>
    <pre class="programlisting"><code class="hljs angelscript">classifier.add(layers.MaxPooling2D(pool_size = (<span class="hljs-number">2</span>, <span class="hljs-number">2</span>)))
</code></pre>
    <p class="normal">This <code class="Code-In-Text--PACKT-">pool_size</code> 2×2 window will first find the maximum value of the 2×2 matrix at the top left of the image matrix. This first maximum value is 4. It is thus the first value of the pooling window on the right.</p>
    <p class="normal">Then, the max pooling window hops over 2 squares and finds that 5 is the highest value. 5 is written in the max pooling window. The hop action is called a <strong class="bold">stride</strong>. A stride<a id="_idIndexMarker439"/> value of 2 will avoid overlapping, although some CNN models have strides that overlap. It all depends on your goal. Look at the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_07.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.7: Pooling example</p>
    <p class="normal">The output size has now gone from a 62×62×32 (number of filters) to a 31×31×32, as shown in the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_08.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.8: Output size changes (pooling)</p>
    <p class="normal">Other pooling methods exist, such as average pooling, which uses the average of the pooling window and not <a id="_idIndexMarker440"/>the maximum value. This depends on the model and shows the hard work that needs to be put in to train a model.</p>
    <h2 id="_idParaDest-167" class="title">Next convolution and pooling layer</h2>
    <p class="normal">The next two layers of the<a id="_idIndexMarker441"/> CNN repeat the same method <a id="_idIndexMarker442"/>as the first two described previously, and it is implemented as follows in the source code:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment"># Step 3 Adding a second convolutional layer and pooling layer</span>
<span class="hljs-builtin-name">print</span>(<span class="hljs-string">"Step 3a Convolution"</span>)
classifier.<span class="hljs-builtin-name">add</span>(layers.Conv2D(32, (3, 3), activation = <span class="hljs-string">'relu'</span>))
<span class="hljs-builtin-name">print</span>(<span class="hljs-string">"Step 3b Pooling"</span>)
classifier.<span class="hljs-builtin-name">add</span>(layers.MaxPooling2D(pool_size = (2, 2)))
</code></pre>
    <p class="normal">These two layers have drastically downsized the input to 14×14×32, as shown in this diagram:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_09.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.9: Convolution and pooling layer</p>
    <p class="normal">It is possible to insert a padding layer on a CNN. As we shrink our image layer by layer, the filters in a convolutional network will impact the center pixels more than the outer pixels. Suppose you start drawing on a piece of paper. You tend to fill the center of the paper and avoid the edges. The edges of the piece of paper contain less information. If you decide to apply <a id="_idIndexMarker443"/>padding to the edges, the<a id="_idIndexMarker444"/> image will be more complete. In a neural network, padding has the same function. It makes sure the edges are taken into account by adding values. Padding can be implemented before or after pooling, for example. We will implement an example of padding in <em class="italics">Chapter 13</em>, <em class="italics">Visualizing Networks with TensorFlow 2.x and TensorBoard</em>.</p>
    <p class="normal">The next layer can apply flattening to the output of the pooling of this section, as we'll see in the next section.</p>
    <h2 id="_idParaDest-168" class="title">Flattening</h2>
    <p class="normal">The flattening layer<a id="_idIndexMarker445"/> takes the output of the max pooling layer and transforms the vector of size <em class="italics">x</em> * <em class="italics">y</em> * <em class="italics">z</em> into a flattened vector, as shown in the following code:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-comment"># Step 4 – Flattening</span>
<span class="hljs-builtin-name">print</span>(<span class="hljs-string">"Step 4 Flattening"</span>)
classifier.<span class="hljs-builtin-name">add</span>(layers.Flatten())
</code></pre>
    <p class="normal">In this case, the layer vector will be 14 × 14 × 32 = 6,272, as shown in the following diagram:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_10.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.10: Flattening layer</p>
    <p class="normal">This operation <a id="_idIndexMarker446"/>creates a standard layer with 6,272 very practical connections for the dense operations that follow. After flattening has been carried out, a fully connected dense network can be implemented.</p>
    <h2 id="_idParaDest-169" class="title">Dense layers</h2>
    <p class="normal">Dense layers are<a id="_idIndexMarker447"/> fully connected. Full connections are possible through the size reductions calculated so far, as shown before.</p>
    <p class="normal">The successive layers in this sequential model have brought the size of the image down enough to use dense layers to finish the job. <code class="Code-In-Text--PACKT-">dense_1</code> comes first, as shown here:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_11.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.11: Dense layer</p>
    <p class="normal">The flattening layer produced a 14×14×32 size 6,272 layer with a weight for each input. If it had not gone through the previous layers, the flattening would have produced a much larger layer, slowing feature extractions down. The result would produce nothing effective.</p>
    <p class="normal">With the main features extracted by the filters through successive layers and size reduction, the dense operations will lead directly to a prediction using ReLU on the dense operation and then the logistic sigmoid function to produce the final result:</p>
    <pre class="programlisting"><code class="hljs reasonml">print(<span class="hljs-string">"Step 5 Dense"</span>)
classifier.add(layers.<span class="hljs-constructor">Dense(<span class="hljs-params">units</span> = 128, <span class="hljs-params">activation</span> = '<span class="hljs-params">relu</span>')</span>)
classifier.add(layers.<span class="hljs-constructor">Dense(<span class="hljs-params">units</span> = 1, <span class="hljs-params">activation</span> = '<span class="hljs-params">sigmoid</span>')</span>)
</code></pre>
    <p class="normal">Now that we have the dense layer, let's explore the dense layer's activation functions.</p>
    <h3 id="_idParaDest-170" class="title">Dense activation functions</h3>
    <p class="normal">The ReLU activation <a id="_idIndexMarker448"/>function can be applied to a dense layer as in other layers.</p>
    <p class="normal">The domain of the ReLU activation function is applied to the result of the first dense operation. The ReLU activation function will output the initial input for values &gt;=0 and will output 0 for values &lt;0:</p>
    <p class="center"><em class="italics">f</em>(input_value) = max{0, input_value)</p>
    <p class="normal">The logistic activation function is applied to the second dense operation, as described in <em class="italics">Chapter 2</em>, <em class="italics">Building a Reward Matrix – Designing Your Datasets</em>.</p>
    <p class="normal">It will produce a value between 0 and 1:</p>
    <p class="normal"><em class="italics">LS</em>(<em class="italics">x</em>)={0,1}</p>
    <p class="normal">We have now built the last dense layer after the <em class="italics">LS</em> activation function.</p>
    <p class="normal">The last dense layer is of size 1 and will classify the initial input—an image in this case:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_12.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.12: Dense layer 2</p>
    <p class="normal">The layers of the model have now been added. Training can begin.</p>
    <h1 id="_idParaDest-171" class="title">Training a CNN model</h1>
    <p class="normal">Training a<a id="_idIndexMarker449"/> CNN model involves four phases: compiling the model, loading the training data, loading the test data, and running the model through epochs of loss evaluation and parameter-updating cycles.</p>
    <p class="normal">In this section, the choice of theme for the training dataset will be an example from the food-processing industry. The idea here is not only to recognize an object but to form a concept. We will explore concept learning neural networks further in <em class="italics">Chapter 10</em>, <em class="italics">Conceptual Representation Learning</em>. For the moment, let's train our model.</p>
    <h2 id="_idParaDest-172" class="title">The goal</h2>
    <p class="normal">The primary goal of this model consists of detecting production efficiency flaws on a food-processing conveyor belt. The use of CIFAR-10 (images) and MNIST (a handwritten digit database) proves useful to understand and train some models. However, in this example, the goal is not to recognize objects but a concept.</p>
    <p class="normal">The following image shows a<a id="_idIndexMarker450"/> section of the conveyor belt that contains an acceptable level of products, in this case, portions of chocolate cake:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_13.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.13: Portions of chocolate cake example</p>
    <p class="normal">The images can represent anything from chocolate cakes, cars on a road or any other type of object. The main point is to detect when there are "gaps" or "holes" in the rows of objects. To train the CNN, I used images containing objects I sometimes drew just to train the system to "see" gaps.</p>
    <p class="normal">However, sometimes production slows down, and the output goes down to an alert level, as shown in the following photograph:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_14.png" alt=""/></figure>
    <p class="packt_figref">Figure 9.14: Portions of chocolate cake example</p>
    <p class="normal">The alert-level image shows a gap that will slow down the packaging section of the factory dramatically. There are three lines of objects in the preceding image. On line one, you can see five little objects (here pieces of cake), on line two, only three. On line three, you can only see two objects. There are thus objects missing on lines two and three. This constitutes a gap in this case that is a real problem on production lines. The level of the number of <a id="_idIndexMarker451"/>acceptable objects in a frame is a parameter.</p>
    <p class="normal">Now that we have our goal, let's begin compiling the model.</p>
    <h2 id="_idParaDest-173" class="title">Compiling the model</h2>
    <p class="normal">Compiling a<a id="_idIndexMarker452"/> TensorFlow 2 model requires a minimum of two options: a loss function and an optimizer. You evaluate how much you are losing and then optimize your parameters, just as in real life. A metric option has been added to measure the performance of the model. With a metric, you can analyze your losses and optimize your situation, as shown in the following code:</p>
    <pre class="programlisting"><code class="hljs gradle">classifier.<span class="hljs-keyword">compile</span>(optimizer = <span class="hljs-string">'adam'</span>, loss = <span class="hljs-string">'binary_crossentropy'</span>, metrics = [<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <p class="normal">Let's take a look at some specific aspects of model compiling, starting with the loss function.</p>
    <h3 id="_idParaDest-174" class="title">The loss function</h3>
    <p class="normal">The loss function<a id="_idIndexMarker453"/> provides information on how far the state of the model <em class="italics">y</em><sub>1</sub> (weights and biases) is from its target state <em class="italics">y</em>.</p>
    <p class="normal">A description of the quadratic loss function precedes that of the binary cross-entropy functions applied to the case study model in this chapter.</p>
    <h4 class="title">The quadratic loss function</h4>
    <p class="normal">Let's refresh the concept <a id="_idIndexMarker454"/>of gradient descent. Imagine you are on a <a id="_idIndexMarker455"/>hill and want to walk down that hill. Your goal is to get to <em class="italics">y</em>, the bottom of the hill. Presently, you are at location <em class="italics">a</em>. Google Maps shows you that you still have to go a certain distance:</p>
    <p class="center"><em class="italics">y</em> – <em class="italics">a</em></p>
    <p class="normal">That formula is great for the moment. But now suppose you are almost at the bottom of the hill, and the person walking in front of you has dropped a coin. You have to slow down now, and Google Maps is not helping much because at this zoom level. </p>
    <p class="normal">You must then zoom into smaller distances with a quadratic objective (or cost) function:</p>
    <p class="center"><em class="italics">O</em> = (<em class="italics">y</em> – <em class="italics">a</em>)<sup>2</sup></p>
    <p class="normal">To make it more comfortable to analyze, <em class="italics">O</em> is divided by 2, producing a standard quadratic cost function:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_002.png" alt=""/></figure>
    <p class="normal"><em class="italics">y</em> is the goal. <em class="italics">a</em> is the result of the operation of applying the weights, biases, and finally, the activation functions.</p>
    <p class="normal">With the derivatives <a id="_idIndexMarker456"/>of the results, the weights and biases can <a id="_idIndexMarker457"/>be updated. In our hill example, if you move one meter (<em class="italics">y</em>) per step (<em class="italics">x</em>), that is much more than moving 0.5 meters (<em class="italics">y</em>) per step. Depending on your position on the hill, you can see that you cannot apply a constant learning rate (conceptually, the length of your step); you adapt it just like Adam, the optimizer, does.</p>
    <h4 class="title">Binary cross-entropy</h4>
    <p class="normal">Cross-entropy comes<a id="_idIndexMarker458"/> in handy when the learning slows down. In the hill example, it slowed down at the bottom. But, remember, a path can lead you sideways, meaning you are momentarily stuck at a given height. Cross-entropy solves that by being able to function well with very small values (steps on the hill).</p>
    <p class="normal">Suppose you have the following structure:</p>
    <ul>
      <li class="list">Inputs = {<em class="italics">x</em><sub>1</sub>, <em class="italics">x</em><sub>2</sub>, …, <em class="italics">x</em><sub style="font-style: italic;">n</sub>}</li>
      <li class="list">Weights = {<em class="italics">w</em><sub>1</sub>, <em class="italics">w</em><sub>2</sub>, …, <em class="italics">w</em><sub style="font-style: italic;">n</sub>}</li>
      <li class="list">A bias (or sometimes more) is <em class="italics">b</em></li>
      <li class="list">An activation function (ReLU, logistic sigmoid, or other)</li>
    </ul>
    <p class="normal">Before the activation, <em class="italics">z</em> represents the sum of the classical operations:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_003.png" alt=""/></figure>
    <p class="normal">Now the activation function is applied to <em class="italics">z</em> to obtain the present output of the model.</p>
    <p class="center"><em class="italics">y</em><sub>1</sub> = <em class="italics">act</em>(<em class="italics">z</em>)</p>
    <p class="normal">With this in mind, the cross-entropy loss formula can be explained:</p>
    <figure class="mediaobject"><img src="../Images/B15438_09_004.png" alt=""/></figure>
    <p class="normal">In this function:</p>
    <ul>
      <li class="list"><em class="italics">n</em> is the total number of items of the input training, with multiclass data. The choice of the logarithm base (2, <em class="italics">e</em>, 10) will produce different effects.</li>
      <li class="list"><em class="italics">y</em> is the output goal.</li>
      <li class="list"><em class="italics">y</em><sub>1</sub> is the present value, as described previously.</li>
    </ul>
    <p class="normal">This loss function <a id="_idIndexMarker459"/>is always positive; the values have a minus sign in front of them, and the function starts with a minus. The output produces small numbers that tend to zero as the system progresses.</p>
    <p class="normal">The loss function uses this basic concept with more mathematical inputs to update the parameters.</p>
    <p class="normal">A binary cross-entropy loss function is a binomial function that will produce a probability output of 0 or 1 and not a value between 0 and 1 as in standard cross-entropy. In the binomial classification model, the output will be 0 or 1.</p>
    <p class="normal">In this case, the sum <img src="../Images/B15438_09_005.png" alt=""/> is not necessary when <em class="italics">M</em> (number of classes) = 2. The binary cross-entropy loss function is then as follows:</p>
    <p class="center">Loss = –<em class="italics">y</em> log <em class="italics">y</em><sub>1</sub> + (1 – <em class="italics">y</em>) log (1 – <em class="italics">y</em><sub>1</sub>)</p>
    <p class="normal">The whole concept of this loss function method is for the CNN network to provide information for the optimizer to adapt the weights accordingly and automatically.</p>
    <h3 id="_idParaDest-175" class="title">The Adam optimizer</h3>
    <p class="normal">In the hill example, you<a id="_idIndexMarker460"/> first walked with big strides down the hill using momentum (larger strides because you are going in the right direction). Then, you had to take smaller steps to find the object. You adapted your estimation of your moment to your need; hence, the <a id="_idIndexMarker461"/>name <strong class="bold">adaptive moment estimation</strong> (<strong class="bold">Adam</strong>).</p>
    <p class="normal">Adam constantly compares mean past gradients to present gradients. In the hill example, it compares how fast you were going.</p>
    <p class="normal">The Adam optimizer represents an alternative to the classical gradient descent method. Adam goes further by applying its optimizer to random (stochastic) mini-batches of the dataset. This approach is an efficient version of stochastic gradient descent.</p>
    <p class="normal">Then, with even <a id="_idIndexMarker462"/>more inventiveness, Adam adds <strong class="bold">root-mean-square deviation</strong> (<strong class="bold">RMSprop</strong>) to the <a id="_idIndexMarker463"/>process by applying per-parameter learning weights. It analyzes how fast the means of the weights are changing (such as the gradients in our hill slope example) and adapts the learning weights.</p>
    <h3 id="_idParaDest-176" class="title">Metrics</h3>
    <p class="normal">Metrics<a id="_idIndexMarker464"/> are there to measure the performance of your model during the training process. The metric function behaves like a loss function. However, it is not used to train the model.</p>
    <p class="normal">In this case, the <code class="Code-In-Text--PACKT-">accuracy</code> parameter was this:</p>
    <pre class="programlisting"><code class="hljs ini"><span class="hljs-attr">...metrics</span> = [<span class="hljs-string">'accuracy'</span>])
</code></pre>
    <p class="normal">Here, a value that descends toward 0 shows whether the training is on the right track and moves up to 1 when the training requires Adam function optimizing to set the training on track again.</p>
    <p class="normal">With this, we have compiled our model. We can now consider our training dataset.</p>
    <h2 id="_idParaDest-177" class="title">The training dataset</h2>
    <p class="normal">The training dataset is<a id="_idIndexMarker465"/> available on GitHub. The dataset contains the image shown previously for the food-processing conveyor belt example. I created a training dataset with a repetition of a few images that I used to illustrate the architecture of a CNN simply. In a real-life project, it will take careful designing with a trial-and-error approach to create a proper dataset that represents all of the cases that the CNN will face.</p>
    <p class="normal">The class <code class="Code-In-Text--PACKT-">A</code> directory contains the acceptable level images of a production line that is producing acceptable levels of products. The class <code class="Code-In-Text--PACKT-">B</code> directory contains the alert-level images of a production line that is producing unacceptable levels of products.</p>
    <p class="normal">The number of images in the dataset is limited because of the following:</p>
    <ul>
      <li class="list">For experimental training purposes, the images produced good results</li>
      <li class="list">The training-testing phase runs faster to study the program</li>
    </ul>
    <p class="normal">The goal of the model is to detect the alert levels, an abstract conceptual application of a CNN.</p>
    <h3 id="_idParaDest-178" class="title">Data augmentation</h3>
    <p class="normal">Data augmentation<a id="_idIndexMarker466"/> increases the size of the dataset by generating distorted versions of the images provided.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">ImageDataGenerator</code> function generates batches of all images found in tensor formats. It will perform data augmentation by distorting the images (shear range, for example). Data augmentation is a fast way to use the images you have and create more virtual images through distortions:</p>
    <pre class="programlisting"><code class="hljs nix"><span class="hljs-attr">train_datagen</span> =
tf.compat.v2.keras.preprocessing.image.ImageDataGenerator(
<span class="hljs-attr">rescale</span> = <span class="hljs-number">1</span>./<span class="hljs-number">255</span>,<span class="hljs-attr">shear_range</span> = <span class="hljs-number">0.2</span>,<span class="hljs-attr">zoom_range</span> = <span class="hljs-number">0.2</span>,
<span class="hljs-attr">horizontal_flip</span> = True)
</code></pre>
    <p class="normal">The code description is as follows:</p>
    <ul>
      <li class="list"><code class="Code-In-Text--PACKT-">rescale</code> will rescale the input image if not <code class="Code-In-Text--PACKT-">0</code> (or <code class="Code-In-Text--PACKT-">None</code>). In this case, the data is multiplied by 1/255 before applying any other operation.</li>
      <li class="list"><code class="Code-In-Text--PACKT-">shear_range</code> will displace each value in the same direction, determined in this case by the <code class="Code-In-Text--PACKT-">0.2</code>. It will slightly distort the image at one point, giving some more virtual images to train.</li>
      <li class="list"><code class="Code-In-Text--PACKT-">zoom_range</code> is the value of zoom.</li>
      <li class="list"><code class="Code-In-Text--PACKT-">horizontal_flip</code> is set to <code class="Code-In-Text--PACKT-">True</code>. This is a Boolean that randomly flips inputs horizontally.</li>
    </ul>
    <p class="normal"><code class="Code-In-Text--PACKT-">ImageDataGenerator</code> provides many more options for real-time data augmentation, such as rotation range, height shift, and others.</p>
    <h3 id="_idParaDest-179" class="title">Loading the data</h3>
    <p class="normal">Loading the <a id="_idIndexMarker467"/>data goes through the <code class="Code-In-Text--PACKT-">train_datagen</code> preprocessing image function (described previously) and is implemented in the following code:</p>
    <pre class="programlisting"><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"Step 7b training set"</span>)</span></span>
training_set = train_datagen.flow_from_directory(directory+<span class="hljs-string">'training_set'</span>,
target_size = (<span class="hljs-number">64</span>, <span class="hljs-number">64</span>),
batch_size = batchs,
class_mode = <span class="hljs-string">'binary'</span>)
</code></pre>
    <p class="normal">The flow in this program uses the following options:</p>
    <ul>
      <li class="list"><code class="Code-In-Text--PACKT-">flow_from_directory</code> sets the directory + <code class="Code-In-Text--PACKT-">'training_set'</code> to the path where the two binary classes to train are stored.</li>
      <li class="list"><code class="Code-In-Text--PACKT-">target_size</code> will be resized to that dimension. In this case, it is 64×64.</li>
      <li class="list"><code class="Code-In-Text--PACKT-">batch_size</code> is the size of the batches of data. The default value is 32, and it's set to <code class="Code-In-Text--PACKT-">10</code> in this case.</li>
      <li class="list"><code class="Code-In-Text--PACKT-">class_mode</code> determines the label arrays returned: <code class="Code-In-Text--PACKT-">None</code> or <code class="Code-In-Text--PACKT-">'categorical'</code> will be 2D <a id="_idIndexMarker468"/>one-hot encoded labels. In this case, <code class="Code-In-Text--PACKT-">'binary'</code> returns 1D binary labels.</li>
    </ul>
    <p class="normal">Having looked at the training dataset, let's move on to the testing dataset.</p>
    <h2 id="_idParaDest-180" class="title">The testing dataset</h2>
    <p class="normal">The testing dataset <a id="_idIndexMarker469"/>flow follows the same structure as the training dataset flow described previously. However, for testing purposes, the task can be made easier or more difficult, depending on the choice of the model. To make the task more difficult, add images with defects or noise. This will force the system to train more and the project team to do more hard work to fine-tune the model. I chose to use a small dataset to illustrate the architecture of a CNN. In a real-life project choosing the right data that contains all of the cases that the CNN will face takes time and a trial-and-error approach.</p>
    <p class="normal">Data augmentation provides an efficient way of producing distorted images without adding images to the dataset. Both methods, among many others, can be applied at the same time when necessary.</p>
    <h3 id="_idParaDest-181" class="title">Data augmentation on the testing dataset</h3>
    <p class="normal">In this model, the<a id="_idIndexMarker470"/> data only goes through rescaling. Many other <a id="_idIndexMarker471"/>options could be added to complicate the training task to avoid overfitting, for example, or simply because the dataset is small:</p>
    <pre class="programlisting"><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"Step 8a test"</span>)</span></span>
test_datagen = tf<span class="hljs-selector-class">.compat</span><span class="hljs-selector-class">.v2</span><span class="hljs-selector-class">.keras</span><span class="hljs-selector-class">.preprocessing</span><span class="hljs-selector-class">.image</span>.ImageDataGenerator(rescale = <span class="hljs-number">1</span>./<span class="hljs-number">255</span>)
</code></pre>
    <p class="normal">Building datasets is one of the most difficult tasks in an artificial intelligence project. Data augmentation can be a solution if the results are efficient. If not, other techniques must be used. One technique is to gather very large datasets when that is possible and then use data augmentation just to distort the data a bit for training purposes.</p>
    <h3 id="_idParaDest-182" class="title">Loading the data</h3>
    <p class="normal">Loading the testing data<a id="_idIndexMarker472"/> remains limited to what is necessary for this model. Other options can fine-tune the task at hand:</p>
    <pre class="programlisting"><code class="hljs stylus"><span class="hljs-function"><span class="hljs-title">print</span><span class="hljs-params">(<span class="hljs-string">"Step 8b testing set"</span>)</span></span>
test_set = test_datagen.flow_from_directory(directory+<span class="hljs-string">'test_set'</span>,
target_size = (<span class="hljs-number">64</span>, <span class="hljs-number">64</span>),
batch_size = batchs,
class_mode = <span class="hljs-string">'binary'</span>)
</code></pre>
    <p class="normal">Never underestimate dataset fine-tuning. Sometimes, this phase can last weeks before finding the right dataset and arguments.</p>
    <p class="normal">Once the data is loaded, the CNN classifier is ready to be trained. Let's now see how this is done.</p>
    <h2 id="_idParaDest-183" class="title">Training with the classifier</h2>
    <p class="normal">The classifier<a id="_idIndexMarker473"/> has been built and can be run:</p>
    <pre class="programlisting"><code class="hljs routeros"><span class="hljs-builtin-name">print</span>(<span class="hljs-string">"Step 9 training"</span>)
<span class="hljs-builtin-name">print</span>(<span class="hljs-string">"Classifier"</span>,classifier.fit_generator(training_set,
steps_per_epoch = estep,
epochs = ep,
validation_data = test_set,
validation_steps = vs,<span class="hljs-attribute">verbose</span>=2))
</code></pre>
    <p class="normal">You will notice that, in this chapter, we have a directory for the training data and a separate directory for the test data. In <em class="italics">Chapter 5</em>, <em class="italics">How to Use Decision Trees to Enhance K-Means Clustering</em>, we split the datasets into training subsets and testing subsets. This can be applied to a CNN's dataset as well. This is a decision you will need to make, depending on the situation.</p>
    <p class="normal">For example, sometimes, the test set will be more difficult than the training set, which justifies a separate directory. In other cases, splitting the data can be the most efficient method.</p>
    <p class="normal">The <code class="Code-In-Text--PACKT-">fit_generator</code> function, which fits the model generated batch by batch, contains the main hyperparameters to run the training session through the following arguments in this model. The hyperparameter settings determine the behavior of the training algorithm:</p>
    <ul>
      <li class="list"><code class="Code-In-Text--PACKT-">training_set</code> is the training set flow described previously.</li>
      <li class="list"><code class="Code-In-Text--PACKT-">steps_per_epoch</code> is the total number of steps (batches of samples) to yield from the generator. The variable used in the following code is <code class="Code-In-Text--PACKT-">estep</code>.</li>
      <li class="list"><code class="Code-In-Text--PACKT-">epochs</code> is the variable of the total number of iterations made on the data input. The variable used is <code class="Code-In-Text--PACKT-">ep</code> in the preceding code.</li>
      <li class="list"><code class="Code-In-Text--PACKT-">validation_data=test_set</code> is the testing data flow.</li>
      <li class="list"><code class="Code-In-Text--PACKT-">validation_steps=vs</code> is used with the generator and defines the number of batches of samples to test as defined by <code class="Code-In-Text--PACKT-">vs</code> in the following code at the beginning of the program:
        <pre class="programlisting"><code class="hljs ini"><span class="hljs-attr">estep</span>=<span class="hljs-number">100</span> <span class="hljs-comment">#10000</span>
<span class="hljs-attr">vs</span>=<span class="hljs-number">1000</span> <span class="hljs-comment">#8000-&gt;100</span>
<span class="hljs-attr">ep</span>=<span class="hljs-number">3</span> <span class="hljs-comment">#25-&gt;2</span>
</code></pre>
      </li>
    </ul>
    <p class="normal">While the<a id="_idIndexMarker474"/> training runs, measurements are displayed: loss, accuracy, epochs, information on the structure of the layers, and the steps calculated by the algorithm.</p>
    <p class="normal">Here is an example of the loss and accuracy data displayed:</p>
    <pre class="programlisting"><code class="hljs angelscript">Epoch <span class="hljs-number">1</span>/<span class="hljs-number">2</span>
 - <span class="hljs-number">23</span>s - loss: <span class="hljs-number">0.1437</span> - acc: <span class="hljs-number">0.9400</span> - val_loss: <span class="hljs-number">0.4083</span> - val_acc: <span class="hljs-number">0.5000</span>
Epoch <span class="hljs-number">2</span>/<span class="hljs-number">2</span>
 - <span class="hljs-number">21</span>s - loss: <span class="hljs-number">1.9443e-06</span> - acc: <span class="hljs-number">1.0000</span> - val_loss: <span class="hljs-number">0.3464</span> - val_acc: <span class="hljs-number">0.5500</span>
</code></pre>
    <p class="normal">Now that we have built and trained the model, we need to save it. Saving the model will avoid having to train the model each time we wish to use it.</p>
    <h2 id="_idParaDest-184" class="title">Saving the model</h2>
    <p class="normal">By saving the<a id="_idIndexMarker475"/> model, we will not have to train it again every time to use it. We will only go back to training when it's required to fine-tune it.</p>
    <p class="normal">TensorFlow 2 provides a method to save the structure of the model and the weights in a single line of code and a single serialized file:</p>
    <p class="normal"><code class="Code-In-Text--PACKT-">model3.h5</code> saved in the following code, contains serialized data with the model structure and weights. It contains the parameters and options of each layer. This information is very useful to fine-tune the model:</p>
    <pre class="programlisting"><code class="hljs maxima"><span class="hljs-built_in">print</span>(<span class="hljs-string">"Step 10: Saving the model"</span>)
classifier.<span class="hljs-built_in">save</span>(<span class="hljs-built_in">directory</span>+<span class="hljs-string">"model/model3.h5"</span>)
</code></pre>
    <p class="normal">The model has been built, trained, and saved.</p>
    <h3 id="_idParaDest-185" class="title">Next steps</h3>
    <p class="normal">The model has been built and trained. In <em class="italics">Chapter 10</em>, <em class="italics">Conceptual Representation Learning</em>, we will explore how to load and run it with no training.</p>
    <h1 id="_idParaDest-186" class="title">Summary</h1>
    <p class="normal">Building and training a CNN will only succeed with hard work, choosing the model, the right datasets, and hyperparameters. The model must contain convolutions, pooling, flattening, dense layers, activation functions, and optimizing parameters (weights and biases) to form solid building blocks to train and use a model.</p>
    <p class="normal">Training a CNN to solve a real-life problem can help sell AI to a manager or a sales prospect. In this case, using the model to help a food-processing factory solve a conveyor belt productivity problem takes AI a step further into everyday corporate life.</p>
    <p class="normal">A CNN that recognizes abstract concepts within an image takes deep learning one step closer to powerful machine thinking. A machine that can detect objects in an image and extract concepts from the results represents the true final level of AI.</p>
    <p class="normal">Once the training is over, saving the model provides a practical way to use it by loading it and applying it to new images to classify them. This chapter concluded after we had trained and saved the model.</p>
    <p class="normal"><em class="italics">Chapter 10</em>, <em class="italics">Conceptual Representation Learning</em>, will dive deeper into how to design symbolic neural networks.</p>
    <h1 id="_idParaDest-187" class="title">Questions</h1>
    <ol>
      <li class="list">A CNN can only process images. (Yes | No)</li>
      <li class="list">A kernel is a preset matrix used for convolutions. (Yes | No)</li>
      <li class="list">Does pooling have a pooling matrix, or is it random?</li>
      <li class="list">The size of the dataset always has to be large. (Yes | No)</li>
      <li class="list">Finding a dataset is not a problem with all the available image banks on the web. (Yes | No)</li>
      <li class="list">Once a CNN is built, training it does not take much time. (Yes | No)</li>
      <li class="list">A trained CNN model applies to only one type of image. (Yes | No)</li>
      <li class="list">A quadratic loss function is not very efficient compared to a cross-entropy function. (Yes | No)</li>
      <li class="list">The performance of a deep learning CNN does not present a real issue with modern CPUs and GPUs. (Yes | No)</li>
    </ol>
    <h1 id="_idParaDest-188" class="title">Further reading and references</h1>
    <ul>
      <li class="list">TensorFlow 2: <a href="https://www.tensorflow.org/beta"><span class="url">https://www.tensorflow.org/beta</span></a></li>
    </ul>
  </div>
</body></html>