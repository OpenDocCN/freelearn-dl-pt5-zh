<html><head></head><body>
<div id="sbo-rt-content"><div id="_idContainer031">
<h1 class="chapter-number" id="_idParaDest-28"><a id="_idTextAnchor027"/>2</h1>
<h1 id="_idParaDest-29"><a id="_idTextAnchor028"/>Introducing Foundations of Vector Representation</h1>
<p><strong class="bold">Vectors</strong> and <strong class="bold">vector representation</strong> are at<a id="_idIndexMarker103"/> the very core of neural search since the quality of <a id="_idIndexMarker104"/>vectors determines the quality of search results. In this chapter, you will learn about the concept of <strong class="bold">vectors</strong> within <strong class="bold">machine learning</strong> (<strong class="bold">ML</strong>). You will see common search algorithms using vector representation as well as their weaknesses and strengths.</p>
<p>We’re going to cover the following main topics in this chapter:</p>
<ul>
<li>Introducing vectors in ML</li>
<li>Measuring the similarity between two vectors</li>
<li>Local and distributed representations</li>
</ul>
<p>By the end of this chapter, you will have a solid understanding of how every type of data can be represented in vectors and why this concept is at the very core of neural search.</p>
<h1 id="_idParaDest-30"><a id="_idTextAnchor029"/>Technical requirements</h1>
<p>This chapter has the following technical requirements:</p>
<ul>
<li>A laptop with a minimum of 4 GB RAM (8 GB or more is preferred)</li>
<li>Python installed with version 3.7, 3.8, or 3.9 on a Unix-like operating system, such as macOS or Ubuntu</li>
</ul>
<p>The code for this chapter can be found at <a href="https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter02">https://github.com/PacktPublishing/Neural-Search-From-Prototype-to-Production-with-Jina/tree/main/src/Chapter02</a>.</p>
<h1 id="_idParaDest-31"><a id="_idTextAnchor030"/>Introducing vectors in ML</h1>
<p>Text is an <a id="_idIndexMarker105"/>important means of recording human knowledge. As of June 2021, the number of web pages indexed by mainstream search engines such as Google and Bing has reached 2.4 billion, and the majority of information is stored as text. How to store this textual information, and even how to efficiently retrieve the required information from the repository, has become a major issue in information retrieval. The first step in solving these problems lies in representing text in a format that is <em class="italic">comprehensible</em> to computers.</p>
<p>As network-based information has become increasingly diverse, in addition to text, web pages contain a large amount of multimedia information, such as pictures, music, and video files. These files are more diverse than text in terms of form and content and satisfy users’ needs from different perspectives. How to represent and retrieve these types of information, as well as how to pinpoint the multimodal information needed by users from the vast mass of data available on the internet is also an important factor to be considered in the design of search engines. To achieve this, we need to represent each document as its vector representation.</p>
<p>A <em class="italic">vector</em> is an<a id="_idIndexMarker106"/> object that has both a magnitude and a direction, as you may remember learning in school. If we can represent our data using vector representation, then we’re able to use the angle to measure the similarity of two pieces of information. To be more concrete, we can say the following:</p>
<ul>
<li>Two pieces of information are represented as vectors</li>
<li>Both vectors start from the origin [<em class="italic">0, 0</em>] (assuming two dimensions)</li>
<li>Two vectors form an angle</li>
</ul>
<p><em class="italic">Figure 2.1</em> illustrates the<a id="_idIndexMarker107"/> relationship between two vectors with respect to their angle:</p>
<div>
<div class="IMG---Figure" id="_idContainer013">
<img alt="Figure 2.1 – An example of vector representation " height="683" src="image/Figure_2.1_B17488.jpg" width="748"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.1 – An example of vector representation</p>
<p><strong class="bold">vec1</strong> and <strong class="bold">vec2</strong> have the <a id="_idIndexMarker108"/>same direction but different lengths. <strong class="bold">vec2</strong> and <strong class="bold">vec3</strong> have the same lengths but point in opposite directions. If the angle is 0 degrees, the two vectors are identical. If the vector is 180 degrees, the two vectors are completely opposite. We can measure the similarity between two vectors by the angle: the<a id="_idIndexMarker109"/> smaller the angle, the closer the vectors are. This method is also called <strong class="bold">cosine similarity</strong>. </p>
<p>In reality, cosine similarity is <a id="_idIndexMarker110"/>one of the most commonly used similarity measurements to determine the similarity between two vectors, but not the only one. We’ll dive into it in more detail, as well as other similarity metrics, in the <em class="italic">Measuring the similarity between two vectors</em> section. Before that, you might be wondering how we can encode our raw information, such as text or audio, into a vector of numeric values. In this section, we’re going to do that. </p>
<p>We’ll dive into the details of cosine similarity using <em class="italic">Python</em> and the <em class="italic">NumPy library</em>. As well as that, we will introduce other similarity metrics and briefly cover local and distributed vector representation in the following subsections.</p>
<h2 id="_idParaDest-32"><a id="_idTextAnchor031"/>Using vectors to represent data</h2>
<p>Let’s start <a id="_idIndexMarker111"/>with the most <a id="_idIndexMarker112"/>common scenario: <strong class="bold">representing text information</strong>. </p>
<p>First of all, let’s define <a id="_idIndexMarker113"/>the concept of a <strong class="bold">feature vector</strong>. Let’s say we want to build a search system for Wikipedia (in English). As of July 2022, English Wikipedia has over 6.5 million articles containing over 4 billion words (180,000 unique words). We can call these unique words the Vocabulary of Wikipedia. </p>
<p>Each of the articles in this Wikipedia collection should be encoded into a series of numerical values; this is referred to as a feature vector. To this end, we can encode 6.5 million articles into 6.5 million indexed feature vectors, then use a similarity metric, such as cosine similarity, to measure the similarity between the encoded query feature vector and the indexed 6.5 million feature vectors.</p>
<p>The encoding process involves finding an optimal function to transform the original data into its vector representation. How can we achieve this goal? </p>
<p>Again, we start with the simplest method: using a <strong class="bold">bit vector</strong>. A bit vector means all the values inside the vector <a id="_idIndexMarker114"/>will be either 0 or 1, depending on the occurrence of the word. Let’s say we loop over all unique words in the Vocabulary; if the word occurs in this particular document, <em class="italic">d</em>, then we set the value of the location of this unique word to be 1, otherwise 0.</p>
<p>Let’s refresh what we introduced in <a href="B17488_01.xhtml#_idTextAnchor014"><em class="italic">Chapter 1</em></a>, <em class="italic">Neural Networks for Neural Search</em>, in the <em class="italic">How does the traditional search system work?</em> section, imagining we have two documents:</p>
<ul>
<li><strong class="source-inline">doc1</strong> = <em class="italic">Jina is a neural search framework</em></li>
<li><strong class="source-inline">doc2</strong> = <em class="italic">Jina is built with cutting edge technology called deep learning</em></li>
</ul>
<ol>
<li>If we merge <a id="_idIndexMarker115"/>these two documents, we have a <a id="_idIndexMarker116"/>Vocabulary (of unique words), as<a id="_idIndexMarker117"/> follows:<p class="source-code"><strong class="bold">vocab = 'Jina is a neural search framework built with cutting age technology called deep learning'</strong></p></li>
<li>Imagining the preceding variable, <strong class="source-inline">vocab</strong>, is our Vocabulary, after preprocessing (tokenizing and stemming), we get a list of tokens, as follows:<p class="source-code"><strong class="bold">vocab = ['a', 'age', 'built', 'call', 'cut', 'deep', 'framework', 'is', 'jina', 'learn', 'neural', 'search', 'technolog', 'with']</strong></p></li>
</ol>
<p>Note that the aforementioned Vocabulary has been sorted alphabetically. </p>
<ol>
<li value="3">To <a id="_idIndexMarker118"/>encode <strong class="source-inline">doc1</strong> into a <a id="_idIndexMarker119"/>vector representation, we loop through <a id="_idIndexMarker120"/>all the words inside the <strong class="bold">Vocabulary</strong>, check <a id="_idIndexMarker121"/>the occurrence of the word within <strong class="source-inline">doc1</strong>, and create the bit vector:<p class="source-code">import nltk</p><p class="source-code">doc1 = 'Jina is a neural search framework'</p><p class="source-code">doc2 = 'Jina is built with cutting age technology called deep learning'</p><p class="source-code">def tokenize_and_stem(doc1, doc2):</p><p class="source-code">    tokens = nltk.word_tokenize(doc1 + doc2)</p><p class="source-code">    stemmer = nltk.stem.porter.PorterStemmer()</p><p class="source-code">    stemmed_tokens = [stemmer.stem(token) for token in </p><p class="source-code">                     tokens]</p><p class="source-code">    return sorted(stemmed_tokens)</p><p class="source-code">def encode(vocab, doc):</p><p class="source-code">    encoded = [0] * len(vocab)</p><p class="source-code">    for idx, token in enumerate(vocab):</p><p class="source-code">        if token in doc:</p><p class="source-code">            encoded[idx] = 1  # token present in doc</p><p class="source-code">    return encoded</p><p class="source-code">if __name__ == '__main__':</p><p class="source-code">    tokens = tokenize_and_stem(doc1, doc2)</p><p class="source-code">    encoded_doc1 = encode(vocab=tokens, doc=doc1)</p><p class="source-code">    print(encoded_doc1)</p></li>
</ol>
<p>The<a id="_idIndexMarker122"/> preceding code block<a id="_idIndexMarker123"/> encodes <strong class="source-inline">doc1</strong> into a bit vector. In the <strong class="source-inline">encode</strong> function, we firstly created a Python list filled with 0s; the length of the list is identical to the size of Vocabulary. Then, we loop over the Vocabulary to check the occurrence of the word inside the document to encode. If present, we set the value of the encoded vector as <strong class="source-inline">1</strong>. In the end, we get this:</p>
<p class="source-code"><strong class="bold">&gt;&gt;&gt; [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0]</strong></p>
<p>In this way, we’ve successfully encoded a document into its bit vector representation.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">You might have noticed that in the preceding example, the output of the bit vector contains a lot of 0s values. In a real-world scenario, as the size of the Vocabulary gets much larger, and the dimensionality of the vector gets very high, there is a high chance that most of the dimensions in the encoded documents are filled with 0s, which is extremely inefficient to store and retrieve. This is also <a id="_idIndexMarker124"/>called a <strong class="bold">sparse vector</strong>. Some Python libraries, such as SciPy, have strong sparse vector support. Some deep learning libraries, such as TensorFlow and PyTorch, have built-in sparse tensor support. Meanwhile, Jina primitive data types support SciPy, TensorFlow, and PyTorch sparse representations.</p>
<p>So far, we have learned that a vector is an object that has both a magnitude and a direction. We also managed to create the simplest form of vector representation of two text documents using a bit vector. Now, it would be very interesting to know how similar these two documents are. Let us learn more about this in the next section. </p>
<h1 id="_idParaDest-33"><a id="_idTextAnchor032"/>Measuring similarity between two vectors</h1>
<p>Measuring similarity between<a id="_idIndexMarker125"/> two vectors is important in a neural search system. Once all of the documents have been indexed into their vector representation, given a user query, we carry out the same encoding process to the query. In the end, we compare the encoded query vector against all the encoded document vectors to find out what the most similar documents are.</p>
<p>We can continue our example from the previous section, trying to measure the similarity between <strong class="source-inline">doc1</strong> and <strong class="source-inline">doc2</strong>. First of all, we need to run the script two times to encode both <strong class="source-inline">doc1</strong> and <strong class="source-inline">doc2</strong>:</p>
<pre class="source-code">doc1 = 'Jina is a neural search framework'
doc2 = 'Jina is built with cutting age technology called deep learning'</pre>
<p>Then, we can produce a vector representation for both of them:</p>
<pre class="source-code">encoded_doc1 = [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0]
encoded_doc2 = [1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1]</pre>
<p>Since the dimension of the encoded result is always identical to the size of Vocabulary, the problem has been converted to how to measure the similarity between two vector representations: <strong class="source-inline">encoded_doc1</strong> and <strong class="source-inline">encoded_doc2</strong>.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">The aforementioned vector representation of <strong class="source-inline">encoded_doc1</strong> and <strong class="source-inline">encoded_doc2</strong> has a depth of 15. It is easy for us to visualize 1D data as a point, 2D data as a line, or 3D data, but not for high-dimensional data. Practically, we might perform dimensionality reduction to reduce high-dimensional vectors to 3D or 2D in order to plot them. The most common technique is<a id="_idIndexMarker126"/> called <strong class="bold">t-sne</strong>.</p>
<p>Imaging two encoded vector representations can be plotted in a 2D vector space. We can visualize <strong class="source-inline">encoded_doc1</strong> and <strong class="source-inline">encoded_doc2</strong> as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer014">
<img alt="Figure 2.2 – Cosine similarity " height="661" src="image/Figure_2.2_B17488.jpg" width="720"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.2 – Cosine similarity</p>
<p>Then, we can<a id="_idIndexMarker127"/> measure <a id="_idIndexMarker128"/>the similarity between <strong class="source-inline">encoded_doc1</strong> and <strong class="source-inline">encoded_doc2</strong> using their angles, specifically, the cosine similarity. The law of cosine tells us that:</p>
<div>
<div class="IMG---Figure" id="_idContainer015">
<img alt="" height="98" src="image/Formula_2.1_B17488.jpg" width="617"/>
</div>
</div>
<p>Let’s say <em class="italic">p</em> is represented as [x1, y1] and <em class="italic">q</em> is represented as [x2, y2]; then, the aforementioned formula can be rewritten as:</p>
<div>
<div class="IMG---Figure" id="_idContainer016">
<img alt="" height="126" src="image/Formula_2.2_B17488.jpg" width="806"/>
</div>
</div>
<p>Since cosine similarity also <a id="_idIndexMarker129"/>works for high-dimensional data, the aforementioned formula <a id="_idTextAnchor033"/>can be again rewritten as:</p>
<div>
<div class="IMG---Figure" id="_idContainer017">
<img alt="" height="156" src="image/Formula_2.3_B17488.jpg" width="756"/>
</div>
</div>
<p>Based on the formula, we can compute the<a id="_idIndexMarker130"/> cosine similarity between <strong class="source-inline">encoded_doc1</strong> and <strong class="source-inline">encoded_doc2</strong>, as follows:</p>
<pre class="source-code">import math
def compute_cosine_sim(encoded_doc1, encoded_doc2):
    numerator = sum([i * j for i, j in zip(encoded_doc1, 
                encoded_doc2)])
    denominator_1 = math.sqrt(sum([i * i for i in 
                    encoded_doc1]))
    denominator_2 = math.sqrt(sum([i * i for i in 
                    encoded_doc2]))
    return numerator/(denominator_1 * denominator_2)</pre>
<p>If we print out the result of the similarity between <strong class="source-inline">encoded_doc1</strong> and <strong class="source-inline">encoded_doc2</strong>, we get the following:</p>
<p class="source-code">&gt;&gt;&gt; 0.40451991747794525</p>
<p>Here, we get the <a id="_idIndexMarker131"/>cosine similarity between two <a id="_idIndexMarker132"/>encoded vectors, roughly equal to <em class="italic">0.405</em>. In a search system, when the user submits a query, we will encode the query into its vector representation. We have encoded all the documents (that we want to search) into their vector representations individually offline. In this way, we can compute the similarity score of the query vector against all document vectors to produce the final ranking list.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">The preceding code illustrates how you can compute the cosine similarity. The code is not optimized. In reality, you should always use NumPy to perform vectorized operations over vectors (NumPy arrays) to achieve higher performance.</p>
<h2 id="_idParaDest-34"><a id="_idTextAnchor034"/>Metrics beyond cosine similarity</h2>
<p>Through cosine similarity is the most commonly used similarity/distance metric, there are some other commonly used metrics <a id="_idIndexMarker133"/>as well. We will cover another two commonly <a id="_idIndexMarker134"/>used distance functions in this section, namely, <strong class="bold">Euclidean distance</strong> and <strong class="bold">Manhattan distance</strong>.</p>
<p class="callout-heading">Important Note</p>
<p class="callout">Similarity metrics measure how alike two documents are. On the other hand, distance metrics measure the dissimilarity between two documents. In the search scenario, you always want to get the top k matches against your query. So, if you are using similarity metrics, always get the first k items from the ranked list. On the other hand, while using distance metrics, always get the last k items from the ranked list or reverse the ranked list and get the first k items.</p>
<p>Unlike cosine similarity, which takes the angle of two vectors as its similarity measure, the Euclidean distance takes the length of the line segment between two data points. For instance, consider two 2D docs in the following figure:</p>
<div>
<div class="IMG---Figure" id="_idContainer018">
<img alt="Figure 2.3 – Euclidean distance " height="610" src="image/Figure_2.3_B17488.jpg" width="688"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.3 – Euclidean distance</p>
<p>As you can see in <em class="italic">Figure 2.3</em>, previously, we<a id="_idIndexMarker135"/> used the angle between <strong class="source-inline">vec1</strong> and <strong class="source-inline">vec2</strong> to compute their cosine similarity. For Euclidean distance, we compute it in a different way. Both <strong class="source-inline">vec1</strong> and <strong class="source-inline">vec2</strong> have a starting point of 0 and the <strong class="source-inline">p</strong> and <strong class="source-inline">q</strong> endpoints, respectively. Now, the distance between these two vectors becomes:</p>
<div>
<div class="IMG---Figure" id="_idContainer019">
<img alt="" height="61" src="image/Formula_2.4_B17488.jpg" width="814"/>
</div>
</div>
<p>Another distance <a id="_idIndexMarker136"/>metric is called <strong class="bold">Manhattan distance</strong> (or <strong class="bold">city-block distance</strong>). It is the<a id="_idIndexMarker137"/> distance between two points measured along axes at right angles. In a plane with <strong class="source-inline">p</strong> at (p1, p2) and <strong class="source-inline">q</strong> at (q1, q2), the distance between these two vectors becomes:</p>
<div>
<div class="IMG---Figure" id="_idContainer020">
<img alt="" height="48" src="image/Formula_2.5_B17488.jpg" width="666"/>
</div>
</div>
<p>As can be seen in <em class="italic">Figure 2.4</em>, the hyperplane has been split into small blocks. Each block has a width of 1 and a height of 1. The distance between <strong class="source-inline">p</strong> and <strong class="source-inline">q</strong> becomes 4:</p>
<div>
<div class="IMG---Figure" id="_idContainer021">
<img alt="Figure 2.4 – Manhattan distance " height="656" src="image/Figure_2.4_B17488.jpg" width="732"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.4 – Manhattan distance</p>
<p>There are many other distance metrics as well, such as<a id="_idIndexMarker138"/> the <strong class="bold">Hamming distance</strong> and <strong class="bold">angular distance</strong>, but <a id="_idIndexMarker139"/>we won’t cover each of them given the fact that cosine and Euclidean are the most commonly used similarity metrics. This, in turn, leads to an interesting question: which distance/similarity metric should I use to make vector similarity computation more effective? The answer is <em class="italic">it depends</em>.</p>
<p>First of all, it depends on your task and your data. But, in general, when performing text retrieval and related tasks, cosine similarity will be your first choice. It has been widely adopted for applications such as measuring similarity between two pieces of encoded text documents. </p>
<p>The deep learning model might also impact your similarity/distance metric choice. For instance, if you applied metric learning techniques to fine-tune your ML model to optimize certain similarity metrics, then you might stick with the same similarity metric that you optimized. To be more specific, note the following: </p>
<ul>
<li>You can apply <em class="italic">Siamese neural networks</em> to optimize pairs of inputs (query and document) based on the Euclidean distance and get a new model</li>
<li>When extracting features with the model, it’s better to use the <em class="italic">Euclidean distance</em> as the similarity measure</li>
<li>If your vectors have extremely high dimensions, it might be a good idea to switch from the Euclidean distance to the <em class="italic">Manhattan distance</em> since it delivers more robust results</li>
</ul>
<p class="callout-heading">Important Note</p>
<p class="callout">In application, different ANN libraries might use different distance metrics as default configuration. For instance, Annoy encourages users to use the angular distance to compute vector distances. It is a variation of the Euclidean distance. More about ANN will be introduced in <a href="B17488_03.xhtml#_idTextAnchor044"><em class="italic">Chapter 3</em></a>, <em class="italic">System Design and Engineering Challenges</em>.</p>
<p>There are multiple ways to encode data into vector representations. Generally speaking, this can be classified into two forms: <strong class="bold">local representation</strong> and <strong class="bold">distributed representation</strong>. The aforementioned way of encoding data into vector representation can be classified into local representation since it treats each unique word as one dimension. </p>
<p>In the next section, we’ll introduce the most important local representation and distributed representation algorithms. </p>
<h1 id="_idParaDest-35"><a id="_idTextAnchor035"/>Local and distributed representations</h1>
<p>In this section, we’ll dive into <strong class="bold">local representations</strong> and <strong class="bold">distributed representations</strong>. We will go through the characteristics of two different representations and list the most widely used local and global representations to encode different modalities of data.</p>
<h2 id="_idParaDest-36"><a id="_idTextAnchor036"/>Local vector representation</h2>
<p>As a classic method of<a id="_idIndexMarker140"/> text representation, <strong class="bold">local representation</strong> only makes <a id="_idIndexMarker141"/>use of the <strong class="bold">disjointed dimensions</strong> in the vector for a certain word when it is represented as a vector. Disjointed dimension means that each dimensionality of the vector represents a single token.</p>
<p>When only one dimension is used, it is<a id="_idIndexMarker142"/> called <strong class="bold">one-hot representation</strong>. <em class="italic">One-hot</em> means that the word is represented as a long vector, and the dimension of the vector is the total number of words to be represented. Most dimensions are 0, while only one dimension has a value of 1. Different words with a dimension of 1 are not used. If this method of representation is stored sparsely, that is, assigning a digital ID to each word based on the dimension of 1, it will be concise. </p>
<p>One-hot also means that no additional learning process is required under the assumption that all words are independent of each other. This maintains the orthogonality between vectors representing words and therefore has a strong discriminative ability. With maximum entropy, a support vector machine, conditional random field, and other ML algorithms, the one-hot representation has great effects on multiple aspects, such as text classification, text clustering, and part-of-speech tagging. For an application scenario of ad hoc retrieval where keyword matching plays a leading role, the bag-of-words model based on one-hot representation is still the mainstream choice. </p>
<p>However, one-hot representation<a id="_idIndexMarker143"/> ignores the semantic relationships between words. In addition, when representing a <a id="_idIndexMarker144"/>Vocabulary, <strong class="bold">V</strong>, that contains <strong class="bold">N</strong> words, the one-hot representation needs to construct a vector of dimension <strong class="bold">N</strong>. This leads to the problems of parameter explosion and data sparseness. </p>
<p>Another type of local representation is referred to <a id="_idIndexMarker145"/>as <strong class="bold">bag-of-words</strong>, or <em class="italic">bit vector representation</em>, which we introduced earlier in the chapter.</p>
<p>As a method of vector representation, the bag-of-words model regards the text as a collection of words, only documenting whether the words appear in the text or not but ignoring the word order and grammar in a body of text. Based on the one-hot representation of words, bag-of-words represents the text as a vector composed of 0s and 1s, and offers great support for bit operations. This method can conduct regular query processing in retrieval scenarios. Because it also maintains the orthogonality between words, it still works well for tasks such as <a id="_idIndexMarker146"/>text classification. Now, we will build a bit vector representation using a <em class="italic">Python ML framework</em> called <strong class="bold">scikit-learn</strong>:</p>
<pre class="source-code">from sklearn.feature_extraction.text import CountVectorizer
corpus = [
    'Jina is a neural search framework for neural search',
    'Jina is built with cutting edge technology called deep 
     learning',
]
vectorizer = CountVectorizer(binary=True)
X = vectorizer.fit_transform(corpus)
print(X.toarray())</pre>
<p>The output looks like this:</p>
<p class="source-code">&gt;&gt;&gt; array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0],</p>
<p class="source-code">           [1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1]])</p>
<p>Based on the bag-of-words (bit vector) model, the bag-of-words representation algorithm<a id="_idIndexMarker147"/> takes into<a id="_idIndexMarker148"/> account the frequency of words appearing in a body of text. Therefore, the bag-of-words encoded feature values corresponding to different words are no longer 0 or 1, but the frequency of such words appears in the body of text. Generally speaking, the more frequently a word appears in the text, the more important the word is to the text. To get the representation, you can simply put <strong class="source-inline">binary=False</strong> in the preceding implementation:</p>
<pre class="source-code">from sklearn.feature_extraction.text import CountVectorizer
corpus = [
    'Jina is a neural search framework for neural search',
    'Jina is built with cutting edge technology called deep 
     learning',
]
vectorizer = CountVectorizer(binary=False)
X = vectorizer.fit_transform(corpus)
print(X.toarray())</pre>
<p>As you can discover<a id="_idIndexMarker149"/> from the following output, the term frequency has been taken into consideration. For example, since the <strong class="source-inline">neural</strong> token occurred two times, the value of the encoded result has increased by <strong class="source-inline">1:</strong></p>
<p class="source-code">&gt;&gt;&gt; array([[0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 2, 2, 0, 0],</p>
<p class="source-code">          [1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1]])</p>
<p>Last but not least, we have one of the most-used local <a id="_idIndexMarker150"/>representations, called <strong class="bold">term frequency-inverse document frequency</strong> (<strong class="bold">tf-idf</strong>) <strong class="bold">representation</strong>.</p>
<p>tf-idf is a common representation method for information retrieval and data mining. The TF-IDF value of word <em class="italic">i</em> in text j is as follows:</p>
<div>
<div class="IMG---Figure" id="_idContainer022">
<img alt="" height="118" src="image/Formula_2.6_B17488.jpg" width="1014"/>
</div>
</div>
<p>Here, <em class="italic">ni, j</em> denotes the frequency of word <em class="italic">i</em> appearing in the text <em class="italic">j</em>; <em class="italic">|d_j |</em> denotes the total number of words in the text; <em class="italic">|D|</em> indicates the number of tokens in the corpus, and <img alt="" height="33" src="image/Formula_2.7_B17488.png" width="164"/> represents the number of documents containing the word <em class="italic">i</em>. By factoring in the frequency of words appearing in the text, the TF-IDF algorithm further considers the universal importance of the word in the entire body of text by calculating the IDF of the word. That is, the more frequently a word appears in the text, the less frequently it appears in other parts of the body of text. This shows that the more important the word is for the current text, the higher weight it will be given. The scikit-learn implementation of this algorithm is as follows:</p>
<pre class="source-code">from sklearn.feature_extraction.text import TfidfVectorizer
corpus = [
    'Jina is a neural search framework for neural search',
    'Jina is built with cutting edge technology called deep 
     learning',
]
vectorizer = TfidfVectorizer()
X = vectorizer.fit_transform(corpus)
print(X.toarray())</pre>
<p>The Tf-Idf weighted encoding result looks like this:</p>
<p class="source-code">&gt;&gt;&gt; array([[0., 0., 0., 0., 0., 0.30134034, 0.30134034, 0.21440614, 0.21440614, 0.,0.60268068, 0.60268068, 0., 0.       ],</p>
<p class="source-code">          [0.33310232, 0.33310232, 0.33310232, 0.33310232, 0.33310232, 0., 0. , 0.23700504, 0.23700504, 0.33310232, 0., 0., 0.33310232, 0.33310232]])</p>
<p>Up until now, we have introduced local<a id="_idIndexMarker151"/> vector representation. In the next section, we will dive deep into a distributed vector representation, why we need it, and the commonly used algorithms.</p>
<h2 id="_idParaDest-37"><a id="_idTextAnchor037"/>Distributed vector representation</h2>
<p>Although the local <a id="_idIndexMarker152"/>representation of texts has advantages in tasks such as text classification and data recall, it has the problem of data sparseness.</p>
<p>To be more specific, if a corpus has 100,000 distinct tokens, the dimensionality of the vector will become 100,000. Suppose we have a document that contains 200 tokens. In order to represent this document, only 200 entries of the vector out of 100,000 are non-zero. All other dimensions still get a 0 value since the tokens of the Vocabulary did not occur in the document. </p>
<p>This has posed great challenges to data storage and retrieval. Accordingly, a natural idea is to obtain a low-dimensional dense vector of the text, which is called a <strong class="bold">distributed representation</strong> of the text. </p>
<p>In this section, the distributed representation of single modalities, such as text, images, and audio, is first described; then, the distributed representation method of multimodal joint learning is presented. We’ll also selectively introduce several important representation learning algorithms based on the modality of the data, that is, text, image, audio, and cross-modal representation learning. Let’s first look at text-based algorithms.</p>
<p>In the following table, we have<a id="_idIndexMarker153"/> listed some selected models to encode different modalities of data:</p>
<table class="No-Table-Style" id="table001-1">
<colgroup>
<col/>
<col/>
<col/>
<col/>
</colgroup>
<tbody>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="bold">Model</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Modality</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Domain</strong></p>
</td>
<td class="No-Table-Style">
<p><strong class="bold">Application</strong></p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">BERT</strong></p>
</td>
<td class="No-Table-Style">
<p>Text</p>
</td>
<td class="No-Table-Style">
<p>Dense retrieval</p>
</td>
<td class="No-Table-Style">
<p>Text-to-text search, question answering</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">VGGNet</strong></p>
</td>
<td class="No-Table-Style">
<p>Image</p>
</td>
<td class="No-Table-Style">
<p>Content-based image retrieval</p>
</td>
<td class="No-Table-Style">
<p>Image-to-image search</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">ResNet</strong></p>
</td>
<td class="No-Table-Style">
<p>Image</p>
</td>
<td class="No-Table-Style">
<p>Content-based image retrieval</p>
</td>
<td class="No-Table-Style">
<p>Image-to-image search</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">Wave2Vec</strong></p>
</td>
<td class="No-Table-Style">
<p>Acoustic</p>
</td>
<td class="No-Table-Style">
<p>Content-based audio retrieval</p>
</td>
<td class="No-Table-Style">
<p>Audio-to-audio search</p>
</td>
</tr>
<tr class="No-Table-Style">
<td class="No-Table-Style">
<p><strong class="source-inline">CLIP</strong></p>
</td>
<td class="No-Table-Style">
<p>Text and image</p>
</td>
<td class="No-Table-Style">
<p>Cross-modal retrieval</p>
</td>
<td class="No-Table-Style">
<p>Text-to-image search</p>
</td>
</tr>
</tbody>
</table>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Table 1.1 – Selected models that can be served as encoders for different modality of inputs</p>
<h3>Text-based algorithms</h3>
<p>Because text <a id="_idIndexMarker154"/>carries important information, the distributed representation of texts serves as a major function of search engines and has been extensively studied in academic works and the industry. Given the fact that we have a huge amount of unlabeled text data (such as Wikipedia), when it comes to text-based algorithms, we normally employ unsupervised pretraining on a large corpus.</p>
<p>Based on the belief<a id="_idIndexMarker155"/> that similar <a id="_idIndexMarker156"/>words have a similar context, Mikolov et al. proposed the <em class="italic">word2vec</em> algorithm, which includes two simple neural network models for learning: the <strong class="bold">Continuous-Bag-of-Words</strong> (<strong class="bold">CBOW</strong>) and <strong class="bold">skip-gram</strong> (<strong class="bold">SG</strong>) models. </p>
<p>Specifically, the CBOW model is<a id="_idIndexMarker157"/> used to derive the representation of a word, <img alt="" height="31" src="image/WT.png" width="46"/>, using its surrounding words, such as two words before and two words after. For example, given a sentence in a Wikipedia document, we randomly mask out one token inside this sentence. We try to predict the masked token by its surrounding tokens:</p>
<pre class="source-code">doc1 = 'Jina is a neural [MASK] framework'</pre>
<p>In the preceding document, we masked the token search and tried to predict the vector representation of the masked token, <em class="italic">u</em>, by summing up the representation of surrounding tokens, <img alt="" height="31" src="image/Formula_2.8_B17488.png" width="33"/>, and conducting the dot product between <em class="italic">u</em> and <img alt="" height="33" src="image/Formula_2.8_B174881.png" width="32"/>. At training time, we’ll select a token, <em class="italic">y</em>, to maximize the dot product:</p>
<div>
<div class="IMG---Figure" id="_idContainer027">
<img alt="" height="121" src="image/Formula_2.10_B17488.jpg" width="370"/>
</div>
</div>
<p class="callout-heading">Important Note</p>
<p class="callout">It should be noted that before training, we will randomly initialize the vector values.</p>
<p>On the other hand, SG tries to predict the vector representations of the surrounding tokens from the current token. The difference between CBOW and SG is illustrated in <em class="italic">Figure 2.5</em>: </p>
<div>
<div class="IMG---Figure" id="_idContainer028">
<img alt="Figure 2.5 – CBOW and SG (source: Efficient estimation of word representations in vector space) " height="801" src="image/Figure_2.5_B17488.jpg" width="1451"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure 2.5 – CBOW and SG (source: Efficient estimation of word representations in vector space)</p>
<p>Both models are <a id="_idIndexMarker158"/>used to learn the word representation by maximizing the log-likelihood of the objective function on the entire corpus. To alleviate the burden of numerous calculations caused by the softmax <a id="_idIndexMarker159"/>function at the output layer, Mikolov et al. created two optimization methods, namely, <strong class="bold">hierarchical softmax</strong> and <strong class="bold">negative sampling</strong>. Conventional deep neural networks predict each next word as a <a id="_idIndexMarker160"/>classification task. This network must have many output classes as unique tokens. For example, when predicting the next word in the English Wikipedia, the number of classes is over 160,000. This is extremely inefficient. Hierarchical softmax and negative sampling replace the flat softmax layer with a hierarchical layer that has the words as leaves and convert the multiclass classification problem into a binary classification problem by classifying whether two tokens are a true pair (semantically similar) or a false pair (independent tokens). This greatly improves the prediction speed of word embeddings.</p>
<p>After pretraining, we can give a token to this word2vec model and get a so-called word embedding. This word embedding is represented by a vector. Some pretrained <strong class="source-inline">word2vec</strong> vectors are represented as 300D word vectors. The dimensionality is much smaller than the sparse vector space model we introduced before. So, we also refer to these vectors as dense vectors.</p>
<p><a id="_idTextAnchor038"/>In algorithms such as <em class="italic">word2vec</em> and <em class="italic">GloVe</em>, the <a id="_idIndexMarker161"/>representation vector of a word generally remains<a id="_idIndexMarker162"/> unchanged after training and can be applied to downstream applications, such as named entity recognition.</p>
<p>However, the semantics <a id="_idIndexMarker163"/>of the same word in different contexts may vary or even have significantly different meanings. In 2019, Google announced <strong class="bold">Bidirectional Encoder Representations from Transformers</strong> (<strong class="bold">BERT</strong>), a transformer-based neural network for natural language processing. BERT uses a transformer network to represent the text and obtains the contextual information of the text through a masked language model. In addition, BERT also employs <strong class="bold">next sentence prediction</strong> (<strong class="bold">NSP</strong>) to <a id="_idIndexMarker164"/>strengthen the textual representation of relationships and has achieved good results for many textual representation tasks.</p>
<p>Similar to word2vec, BERT has been pretrained on the Wikipedia dataset and some other datasets, such as BookCorpus. They form a Vocabulary of above 3 billion tokens. BERT has also been trained in different languages, such as English and German, as well as on multilingual datasets.</p>
<p>BERT can <a id="_idIndexMarker165"/>be trained on a large amount of corpus without any annotations through the pretrain and fine-tune paradigm. During prediction, the text to be predicted is put in a well-trained network again to obtain a dynamic vector representation containing contextual information. During training, BERT replaces the words in the original text according to a certain ratio and uses the training model to make correct predictions. BERT will also add some special characters, such as <strong class="source-inline">[CLS]</strong> and <strong class="source-inline">[SEP]</strong>, to help the model correctly determine whether the two input sentences are continuous. Again, we have <strong class="source-inline">doc1</strong> and <strong class="source-inline">doc2</strong>, as follows; <strong class="source-inline">doc2</strong> is the next sentence of <strong class="source-inline">doc1</strong>:</p>
<ul>
<li><strong class="source-inline">doc1</strong> = <em class="italic">Jina is a neural search framework</em></li>
<li><strong class="source-inline">doc2</strong> = <em class="italic">Jina is built with cutting edge technology called deep learning</em></li>
</ul>
<p>During pretraining, we consider two documents as two sentences, and represent documents as follows:</p>
<pre class="source-code">doc = '[CLS] Jina is a neural [MASK] framework [SEP] Jina is built with cutting edge technology called deep learning'.</pre>
<p>After the <a id="_idIndexMarker166"/>text is input, BERT’s input consists of three types of vectors, that is, <strong class="bold">token embedding</strong>, <strong class="bold">segment embedding</strong>, and <strong class="bold">positional embedding</strong>. The<a id="_idIndexMarker167"/> three representations are<a id="_idIndexMarker168"/> summed and<a id="_idIndexMarker169"/> then input into the subsequent transformer network. In the meantime, some tokens are being replaced by the <strong class="source-inline">[MASK]</strong> token. According to the author of the BERT paper (<em class="italic">BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding</em>), around 15% of tokens are masked out (Jacob et al.).</p>
<div>
<div class="IMG---Figure" id="_idContainer029">
<img alt="Figure. 2.6 – BERT input representation. Each input embedding is the sum of three embeddings " height="339" src="image/Figure_2.6_B17488.jpg" width="1143"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure. 2.6 – BERT input representation. Each input embedding is the sum of three embeddings</p>
<p>At the pretraining time, since we use NSP as the training objective, around 50% of the second sentences are the “true” next sentence, while another 50% of the sentences are randomly selected from the corpus, which means they’re not the sentence that follows on from the first sentence. This helps us provide positive pairs and negative pairs to improve model pretraining. The objective function of BERT is to correctly predict the masked token as well as whether the next sentence is the correct one.</p>
<p>As was mentioned before, after <a id="_idIndexMarker170"/>pretraining BERT, we can fine-tune the model for specific tasks. The author of the BERT paper fine-tuned a pretrained model on different downstream tasks, such as question answering and language understanding, and it achieved state-of-the-art performance on 11 downstream datasets.</p>
<h3>Vision-based algorithms</h3>
<p>With the<a id="_idIndexMarker171"/> rapid development of the internet, information carriers on the internet are increasingly diversified and images provide a variety of visual features. Many researchers expect to encode images as vectors for representation. The most widely used model architecture<a id="_idIndexMarker172"/> for imagery analysis is called <strong class="bold">convolutional neural network</strong> (<strong class="bold">CNN</strong>).</p>
<p>A CNN receives an<a id="_idIndexMarker173"/> image of shape (<strong class="source-inline">Height</strong>, <strong class="source-inline">Width</strong>, <strong class="source-inline">Num_Channels</strong>) as input (normally, it’s a three-channel RGB image or a one-channel grayscale image). The image will be passing through one of multiple convolutional layers. This takes a kernel (or filter) and slides through the input, and the image becomes an abstracted activation map. </p>
<p>After one of multiple convolutional operations, the output of the activation map will be sent through a pooling layer. The pooling layer takes a small cluster of neurons in the feature map and applies max or mean operations in this cluster. This is referred to as max pooling and mean pooling. The pooling layer can significantly reduce the dimensionality of the feature map into a more compact representation.</p>
<p>Normally, a combination of one of multiple convolutional layers and one pooling layer is named a convolutional block. For example, three convolutional layers plus one pooling layer make a convolutional block. At the end of the convolutional block, we normally apply a flatten operation to get the vector representation of the image data.</p>
<p>In the following screenshot, we demonstrate a beautifully designed CNN model named VGG16. As can be seen, it <a id="_idIndexMarker174"/>consists of five convolutional blocks, each one containing two or three convolutional layers and a max pooling layer. At the end of these blocks, the activation map is flattened a<a id="_idTextAnchor039"/>s a feature vector:</p>
<div>
<div class="IMG---Figure" id="_idContainer030">
<img alt="Figure. 2.7 – VGG16 consists of five convolutional blocks and produces classification results with a softmax classification head " height="438" src="image/Figure_2.7_B17488.jpg" width="1541"/>
</div>
</div>
<p class="IMG---Caption" lang="en-US" xml:lang="en-US">Figure. 2.7 – VGG16 consists of five convolutional blocks and produces classification results with a softmax classification head</p>
<p>It is worth <a id="_idIndexMarker175"/>mentioning that VGG16 <a id="_idIndexMarker176"/>is designed for ImageNet classification. So, after the activation map is flattened as a feature vector, it is connected to two fully connected layers (dense layers) and a softmax classification head. </p>
<p>In practice, we will remove the softmax classification head to turn this classification model into an embedding model. Given an input image, this embedding model produces a flattened feature map rather than the classified classes of the objects in the image. Besides, ResNet is a more complicated but frequently used vision feature extractor compared <a id="_idTextAnchor040"/>with VGGNet.</p>
<p>Apart from text and images, audio search is an important search application, for instance, to identify music from a short clip or search for music with a similar style. In the next section, we will list several deep learning models in this direction. </p>
<h3>Acoustic-based algorithms</h3>
<p>Given a sequence of acoustic<a id="_idIndexMarker177"/> inputs, deep learning-powered algorithms have a huge impact on the acoustic domain. For instance, they have been widely used for text-to-speech tasks. Given a piece of music as query, finding similar (or the same) music is commonly used for music applications.</p>
<p>One of the<a id="_idIndexMarker178"/> latest state-of-the-art algorithms trained on audio data is called <strong class="bold">wave2vec 2.0</strong>. Similar to BERT, wave2vec is trained in an unsupervised fashion. Taking a piece of audio data, during pretraining, wave2vec masks out parts of the audio inputs and tries to learn what has been masked out.</p>
<p>The major difference between wave2vec and BERT is that audio is a continuous signal with no clear segmentation into tokens. Wave2vec considers each 25 ms-long audio as a basic unit and feeds each 25 ms basic unit into a CNN model to learn a unit-level feature representation. Then, part of the input is masked out and fed into a BERT-like transformer model to predict the masked output. The training objective is to minimize the contrastive loss between the original audio and predicted audio.</p>
<p>It is worth <a id="_idIndexMarker179"/>mentioning that contrastive (self-supervised) pretraining is also widely used in the representation learning of text or images. For example, given an image as input, we can augment the image content a little bit to produce two views of the same image: even though these two views look different, we know they come from the same image.</p>
<p>This self-supervised contrastive learning has been widely used for representation learning: to learn a good feature vector given any kind of input<a id="_idTextAnchor041"/>. When applying the model to a specific domain, it is still recommended to give some labeled data to fine-tune the model with some extra labels.</p>
<h3>Algorithms beyond text, visual, and acoustic</h3>
<p>In real life, many kinds of information carriers exist. In addition to text, images, and speech, videos, actions, and even proteins contain a wealth of information. Therefore, many attempts have been made to obtain vector representations. Researchers at DeepMind have developed<a id="_idIndexMarker180"/> the <em class="italic">AlphaFold</em> and <em class="italic">AlphaFold2</em> algorithms. Based on traditional features, such as those of an <a id="_idIndexMarker181"/>amino acid sequence, AlphaFold algorithms can be used to obtain protein expression vectors and calculate its 3D structure in space, which greatly improves experiment efficiency in the field of protein analysis.</p>
<p>Moreover, in 2021, GitHub launched Copilot<a id="_idIndexMarker182"/> to help programmers with the automatic completion of code. Prior to this, OpenAI <a id="_idIndexMarker183"/>developed the <em class="italic">Codex</em> model, which was able to convert natural language into code. Based on Codex’s model architecture, GitHub uses their open source TB-level code base to train the model on a large scale and completes the Copilot model to help programmers write new code. Copilot also supports the generation and completion of multiple programming languages, such as Python, JavaScript, and Go. In the search field, if we want to perform a code search or evaluate the similarity of two pieces of code, the Codex model <a id="_idIndexMarker184"/>can be employed to encode the source code into a vector representation.</p>
<p>The aforesaid operations mostly focus on separate encodings of text, images, or audio, so the encoded vector space may vary significantly. To map the information of different modalities to the same vector space, OpenAI researchers proposed<a id="_idIndexMarker185"/> the CLIP model, which can effectively map an image to text. Specifically, CLIP includes an image encoder and a text encoder. After inputting an image and multiple texts, CLIP encodes them at the same time and hopes to find the text most suitable for each image. By training CLIP on a large-scale dataset, CLIP can acquire an excellent representation of images and text and map them in the same vector space.</p>
<h1 id="_idParaDest-38"><a id="_idTextAnchor042"/>Summary</h1>
<p>This chapter described the method of vector representation, which is a major step in the operation of search engines. </p>
<p>First, we introduced the importance of vector representation and how to use it, and then addressed local and distributed vector representation algorithms. In terms of distributed vector representation, the commonly used representation algorithms for text, images, and audio were covered, and common representation methods for other modalities and multimodality were summarized. Hence, we found that the dense vector representation method often entails relatively rich contextual information when compared with sparse vectors. </p>
<p>When building a scalable neural search system, it is important to create an encoder that can encode raw documents into high-quality embeddings. This encoding process needs to be performed fast to reduce the indexing time. At search time, it is critical to apply the same encoding process and find the top-ranked documents in a reasonable amount of time. In the next chapter, we’ll utilize the ideas in this chapter and build a mental map on how to create a scalable neural search system.</p>
<h1 id="_idParaDest-39"><a id="_idTextAnchor043"/>Further reading</h1>
<ul>
<li>Devlin, Jacob, et al. “Bert: Pre-training of deep bidirectional transformers for language understanding.” <em class="italic">arXiv preprint arXiv:1810.04805</em> (2018).</li>
<li>Simonyan, Karen and Andrew Zisserman. “Very deep convolutional networks for large-scale image recognition.” <em class="italic">arXiv preprint arXiv:1409.1556</em> (2014).</li>
<li>He, Kaiming et al. “Deep residual learning for image recognition.” <em class="italic">Proceedings of the IEEE conference on computer vision and pattern recognition</em>. 2016.</li>
<li>Schneider, Steffen, et al. “wav2vec: Unsupervised pre-training for speech recognition.” <em class="italic">arXiv preprint arXiv:1904.05862</em> (2019).</li>
<li>Radford, Alec et al. “Learning transferable visual models from natural language supervision.” <em class="italic">International Conference on Machine Learning</em>. PMLR, 2021.</li>
</ul>
</div>
</div>
</body></html>