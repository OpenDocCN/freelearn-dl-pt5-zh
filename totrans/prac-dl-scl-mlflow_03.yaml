- en: '*Chapter 2*: Getting Started with MLflow for Deep Learning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: One of the key capabilities of MLflow is to enable **Machine Learning** (**ML**)
    experiment management. This is critical because data science requires reproducibility
    and traceability so that a **Deep Learning** (**DL**) model can be easily reproduced
    with the same data, code, and execution environment. This chapter will help us
    get started with how to implement DL experiment management quickly. We will learn
    about MLflow experiment management concepts and capabilities, set up an MLflow
    development environment, and complete our first DL experiment using MLflow. By
    the end of this chapter, we will have a working MLflow tracking server showing
    our first DL experiment results.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Setting up MLflow
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Implementing our first MLflow logging-enabled DL experiment
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Exploring MLflow's components and usage patterns
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To complete the experiment in this chapter, we will need the following tools,
    libraries, and GitHub repositories installed or checked out on our computer:'
  prefs: []
  type: TYPE_NORMAL
- en: 'VS Code: The version we use in this book is August 2021 (that is, version 1.60).
    We use VS Code for our local code development environment. This is the recommended
    way for local developments. Please refer to [https://code.visualstudio.com/updates/v1_60](https://code.visualstudio.com/updates/v1_60).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'MLflow: Version 1.20.2\. In this chapter, in the *Setting up MLflow* section,
    we will walk through how to set up MLflow locally or remotely. Please refer to
    [https://github.com/mlflow/mlflow/releases/tag/v1.20.2](https://github.com/mlflow/mlflow/releases/tag/v1.20.2).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Miniconda: Version 4.10.3\. Please refer to [https://docs.conda.io/en/latest/miniconda.html](https://docs.conda.io/en/latest/miniconda.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'PyTorch `lightning-flash`: 0.5.0\. Please refer to [https://github.com/PyTorchLightning/lightning-flash/releases/tag/0.5.0](https://github.com/PyTorchLightning/lightning-flash/releases/tag/0.5.0).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The GitHub URL for the code in this chapter: You can find this at [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter02](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/tree/main/chapter02).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Setting up MLflow
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: '**MLflow** is an open source tool that is primarily written in Python. It has
    over 10,000 stars tagged in its GitHub source repository ([https://github.com/mlflow/mlflow](https://github.com/mlflow/mlflow)).
    The benefits of using MLflow are numerous, but we can illustrate one benefit with
    the following scenario: Let''s say you are starting a new ML project, trying to
    evaluate different algorithms and model parameters. Within a few days, you run
    hundreds of experiments with lots of code changes using different ML/DL libraries
    and get different models with different parameters and accuracies. You need to
    compare which model works better and also allow your team members to reproduce
    the results for model review purposes. Do you prepare a spreadsheet and write
    down the model name, parameters, accuracies, and location of the models? How can
    someone else rerun your code or use your trained model with a different set of
    evaluation datasets? This can quickly become unmanageable when you have lots of
    iterations for different projects. MLflow can help you to track your experiments,
    compare your model runs and allow others to reproduce your results easily, reuse
    your trained models for review purposes, and even deploy your model to production
    with ease.'
  prefs: []
  type: TYPE_NORMAL
- en: Sound exciting? Well, let's set up MLflow so that we can explore its components
    and patterns. MLflow allows both a local setup and a cloud-based setup. We will
    walk through both of these setup scenarios in the following sections.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up MLflow locally using miniconda
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: First, let's set up MLflow in a local development environment. This allows quick
    prototyping and helps you to get familiar with the basic functionality of the
    MLflow tool. Additionally, it allows you to interact with a remote MLflow cloud
    server when required. Follow these instructions to set up MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming you already have a virtual conda environment created from [*Chapter
    1*](B18120_01_ePub.xhtml#_idTextAnchor015), *Deep Learning Life Cycle and MLOps
    Challenges*, you are ready to install MLflow in the same virtual environment:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'The preceding command will install the latest version of MLflow. If you want
    to install a specific version of MLflow, you can use the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see, I have installed MLflow version 1.20.2\. By default, MLflow
    will use the local filesystem to store all of the experiment artifacts (for example,
    a serialized model) and metadata (parameters, metrics, and more). If a relational
    database is needed as MLflow''s backend storage, additional installation and configuration
    are required. For now, let''s use the filesystem for storage. You can verify your
    MLflow installation locally by typing the following into the command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, it will show the installed MLflow version, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'This confirms that we have installed version 1.20.2 of MLflow on our local
    development environment. Additionally, you can launch the MLflow UI locally to
    see the MLflow tracking server UI, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Following this, you will see that the UI web server is running:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1 – Starting the MLflow UI in a local environment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_001.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.1 – Starting the MLflow UI in a local environment
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2.1* shows the local MLflow UI website: `http://127.0.0.1:5000/`. If
    you click on this URL, you will see the following MLflow UI showing up in your
    browser window. Since this is a brand new MLflow installation, there is only one
    **Default** experiment with no runs under it yet (please refer to *Figure 2.2*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2 – The MLflow Default Experiments UI web page'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_002.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.2 – The MLflow Default Experiments UI web page
  prefs: []
  type: TYPE_NORMAL
- en: Seeing the default MLflow UI page up and running concludes the successful setup
    of MLflow locally with a local working MLflow tracking server.
  prefs: []
  type: TYPE_NORMAL
- en: Setting up MLflow to interact with a remote MLflow server
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In a corporate production environment, MLflow is usually hosted on a cloud
    server, which could be self-hosted or one of the Databricks'' managed services
    in one of the cloud providers (such as AWS, Azure, or Google Cloud). In those
    cases, there is a requirement to set up your local development environment so
    that you can run your ML/DL experiment locally but interact with the MLflow server
    remotely. Next, we will describe how to do this using environment variables with
    the help of the following three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a bash shell command-line environment, define three new environment variables
    if you are using a Databricks-managed MLflow tracking server. The first environment
    variable is `MLFLOW_TRACKING_URI`, and the assigned value is `databricks`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The second environment variable is `DATABRICKS_HOST`. If your Databricks managed
    website looks like `https://dbc-*.cloud.databricks.com/`, then that's the value
    of the `DATABRICKS_HOST` variable (replace `*` with your actual website string).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'The third environment variable is `DATABRICKS_TOKEN`. Navigate to your Databricks-managed
    website at `https://dbc-*.cloud.databricks.com/#setting/account`, click on **Access
    Tokens**, and then click on **Generate New Token**. You will see a pop-up window
    with a **Comment** field (which can be used to record why this token will be used)
    and expiration date, as shown in *Figure 2.3*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.3 – Generating a Databricks access token'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_003.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.3 – Generating a Databricks access token
  prefs: []
  type: TYPE_NORMAL
- en: 'Click on the `DATABRICKS_TOKEN` environment variable as the value:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4 – Copying the generated token that will be used for the environment
    variable'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_004.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.4 – Copying the generated token that will be used for the environment
    variable
  prefs: []
  type: TYPE_NORMAL
- en: Once you have these three environment variables set up, you will be able to
    interact with the Databricks-managed MLflow server in the future. Note that the
    access token has an expiration date for security reasons, which can be revoked
    at any time by the administrator, so make sure you have the environment variable
    updated accordingly when the token is refreshed.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, we have learned how to set up MLflow locally to interact with a
    local MLflow tracking server or a remote MLflow tracking server. This will allow
    us to implement our first MLflow tracking-enabled DL model in the next section
    so that we can explore MLflow concepts and components in a hands-on way.
  prefs: []
  type: TYPE_NORMAL
- en: Implementing our first DL experiment with MLflow autologging
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let''s use the DL sentiment classifier we built in [*Chapter 1*](B18120_01_ePub.xhtml#_idTextAnchor015),
    *Deep Learning Life Cycle and MLOps Challenges*, and add MLflow autologging to
    it to explore MLflow''s tracking capabilities:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need to import the MLflow module:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will provide MLflow **Application Programming Interfaces** (**APIs**) for
    logging and loading models.
  prefs: []
  type: TYPE_NORMAL
- en: 'Just before we run the training code, we need to set up an active experiment
    using `mlflow.set_experiment` for the current running code:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This sets an experiment named `dl_model_chapter02` to be the current active
    experiment. If this experiment does not exist in your current tracking server,
    it will be created automatically.
  prefs: []
  type: TYPE_NORMAL
- en: Environment Variable
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that you might need to set the tracking server URI using the `MLFLOW_TRACKING_URI`
    environment variable before you run your first experiment. If you are using a
    hosted Databricks server, implement the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '`export MLFLOW_TRACKING_URI=databricks`'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are using a local server, then set this environment variable to empty
    or the default localhost at port number `5000` as follows (note that this is our
    current section''s scenario and assumes you are using a local server):'
  prefs: []
  type: TYPE_NORMAL
- en: '`export MLFLOW_TRACKING_URI= http://127.0.0.1:5000`'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, add one line of code to enable autologging in MLflow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will allow the default parameters, metrics, and model to be automatically
    logged to the MLflow tracking server.
  prefs: []
  type: TYPE_NORMAL
- en: Autologging in MLflow
  prefs: []
  type: TYPE_NORMAL
- en: Autologging in MLflow is still in experiment mode (as of version 1.20.2) and
    might change in the future. Here, we use it to explore the MLflow components since
    it only requires one line of code to automatically log everything of interest.
    In the upcoming chapters, we will learn about and implement additional ways to
    perform tracking and logging in MLflow. Also, note that currently, autologging
    in MLflow for PyTorch (as of version 1.20.2) only works for the PyTorch Lightning
    framework, not any arbitrary PyTorch code.
  prefs: []
  type: TYPE_NORMAL
- en: 'Use the Python context manager `with` statement to start the experiment run
    by calling `mlflow.start_run`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Notice that all lines of code underneath the `with` block are the regular DL
    model fine-tuning and testing steps. We only enable automatic MLflow logging so
    that we can observe the metadata that is being tracked/logged by the MLflow tracking
    server.
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, you can run the entire code of `first_dl_with_mlflow.py` (the full code
    can be viewed in this chapter''s GitHub at [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter02/first_dl_with_mlflow.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter02/first_dl_with_mlflow.py))
    using the following command line:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'On a non-GPU macOS laptop, the entire run takes less than 10 minutes. You should
    have an output on your screen, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5 – DL model training/testing with MLflow autologging enabled'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_005.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.5 – DL model training/testing with MLflow autologging enabled
  prefs: []
  type: TYPE_NORMAL
- en: 'If you are running this for the first time, you will see that the experiment
    with the name of `dl_model_chapter02` does not exist. Instead, MLflow automatically
    creates this experiment for you:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6 – MLflow automatically creates a new environment if it does not
    exist'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_006.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.6 – MLflow automatically creates a new environment if it does not exist
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, we can open the MLflow UI locally to see what has been logged in the local
    tracking server by navigating to `http://127.0.0.1:5000/`. Here, you will see
    that a new experiment (`dl_model_chapter02`) with a new run (`chapter02`) has
    been logged:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 2.7 – The MLflow tracking server UI shows a new experiment with a
    new run'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_007.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.7 – The MLflow tracking server UI shows a new experiment with a new
    run
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, click on the hyperlink of the **Start Time** column in *Figure 2.7*. You
    will see the details of the logged metadata of the run:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8 – The MLflow run UI shows the metadata details about the experiment
    run'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_008.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.8 – The MLflow run UI shows the metadata details about the experiment
    run
  prefs: []
  type: TYPE_NORMAL
- en: If you can view this screen in your own local environment, then congratulations!
    You just completed the implementation of MLflow tracking for our first DL model!
    In the next section, we will explore central concepts and components in MLflow
    using our working example.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring MLflow's components and usage patterns
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Let's use the working example implemented in the previous section to explore
    the following central concepts, components, and usages in MLflow. These include
    experiments, runs, metadata about experiments, artifacts for experiments, models,
    and code.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring experiments and runs in MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '**Experiment** is a first-class entity in the MLflow APIs. This makes sense
    as data scientists and ML engineers need to run lots of experiments in order to
    build a working model that meets the requirements. However, the idea of an experiment
    goes beyond just the model development stage and extends to the entire life cycle
    of the ML/DL development and deployment. So, this means that when we do retraining
    or training for a production version of the model, we need to treat them as *production-quality*
    experiments. This unified view of experiments builds a bridge between the offline
    and online production environments. Each experiment consists of many runs where
    you can either change the model parameters, input data, or even model type for
    each run. So, an experiment is an umbrella entity containing a series of runs.
    The following diagram (*Figure 2.9*) illustrates that a data scientist could carry
    out both offline experiments and online production experiments across multiple
    stages of the life cycle of ML/DL models:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.9 – Experiments across the offline and online production life cycles
    of ML/DL models'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_009.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.9 – Experiments across the offline and online production life cycles
    of ML/DL models
  prefs: []
  type: TYPE_NORMAL
- en: As you can see from *Figure 2.9*, during the model development stage, a data
    scientist could run multiple runs of the same experiment or multiple experiments
    depending on the project scenarios. If it is a small ML project, having all runs
    under one single offline experiment could be enough. If it is a complex ML project,
    it is reasonable to design different experiments and conduct runs under each experiment.
    A good reference for designing ML experiments can be found at [https://machinelearningmastery.com/controlled-experiments-in-machine-learning/](https://machinelearningmastery.com/controlled-experiments-in-machine-learning/).
    Then, during the model production phase, it is desirable to set up production-quality
    experiments, as we need to perform model improvement and continuous deployment
    with model retraining. A production experiment will provide a gated accuracy check
    to prevent regression of the new model. Often, this is achieved by running automatic
    model evaluation and validation against a hold-out test dataset to check whether
    a new model still meets the release bar in terms of accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s explore the MLflow experiments in a hands-on way. Run the following
    MLflow command line to interact with the tracking server:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'If your `MLFLOW_TRACKING_URI` environment variable points to a remote tracking
    server, then it will list all the experiments that you have read access to. If
    you want to see what''s in the local tracking server, you could set `MLFLOW_TRACKING_URI`
    to nothing (that is, empty), as follows (note that you don''t need to do this
    if you have never had this environment variable in your local user profile; however,
    doing this will make sure you use a local tracking server):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Prior to your first implementation of the DL model with MLflow autologging
    enabled, the output of listing all your experiments should look similar to *Figure
    2.10* (note that this also depends on where you run the command line; the following
    output assumes you run the command in your local folder where you can check the
    code for [*Chapter 2*](B18120_02_ePub.xhtml#_idTextAnchor027) on GitHub):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.10 – The default MLflow experiment list in a local environment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_010.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.10 – The default MLflow experiment list in a local environment
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2.10* lists the three columns of the experiment property: `mlruns`
    folder underneath the directory where you execute the MLflow commands). The `mlruns`
    folder is used by a filesystem-based MLflow tracking server to store all the metadata
    of experiment runs and artifacts.'
  prefs: []
  type: TYPE_NORMAL
- en: The Command-Line Interface (CLI) versus REST APIs versus Programming Language-Specific
    APIs
  prefs: []
  type: TYPE_NORMAL
- en: MLflow provides three different types of tools and APIs to interact with the
    tracking server. Here, the CLI is used so that we can explore the MLflow components.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, let''s explore a specific MLflow experiment, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, create a new experiment using the MLflow CLI, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The preceding command creates a new experiment named `dl_model_chapter02`.
    If you have already run the first DL model with MLflow autologging in the previous
    section, the preceding command will cause an error message, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'This is to be expected, and you have done nothing wrong. Now if you list all
    the experiments in the local tracking server, it should include the newly created
    experiment, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.11 – The new MLflow experiments list after creating a new experiment'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_011.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.11 – The new MLflow experiments list after creating a new experiment
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s examine the relationship between experiments and runs. If you look
    carefully at the URL of the run page (*Figure 2.8*), you will see something similar
    to the following:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '`http://127.0.0.1:5000/#/experiments/1/runs/2f2ec6e72a5042f891abe0d3a533eec7`'
  prefs: []
  type: TYPE_NORMAL
- en: As you might have gathered, the integer after the `experiments` path is the
    experiment ID. Then, after the experiment ID, there is a `runs` path, followed
    by a GUID-like random string, which is the run ID. So, now we understand how the
    runs are organized under the experiment with a globally unique ID (called a run
    ID).
  prefs: []
  type: TYPE_NORMAL
- en: 'Knowing a run''s globally unique ID is very useful. This is because we can
    retrieve the run''s logged data using `run_id`. If you use the `mlflow runs describe
    --run-id <run_id>` command line, you can get the list of metadata that this run
    has logged. For the experiment we just ran, the following shows the full command
    with the run ID:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: 'The output snippets of this command line are as follows (*Figure 2.12*):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12 – The MLflow command line describes the run in the JSON data
    format'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_012.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.12 – The MLflow command line describes the run in the JSON data format
  prefs: []
  type: TYPE_NORMAL
- en: Note that *Figure 2.12* presents all the metadata about this run in JSON format.
    This metadata includes parameters used by the model training; metrics for measuring
    the accuracy of the model in training, validation, and testing; and more. The
    same data is also presented in the MLflow UI in *Figure 2.8*. Note that the powerful
    MLflow CLI will allow very convenient exploration of the MLflow logged metadata
    and artifacts as well as enabling shell script-based automation, as we will explore
    in the upcoming chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring MLflow models and their usages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Now, let''s explore how the DL model artifacts are logged in the MLflow tracking
    server. On the same run page, as shown in *Figure 2.8*, if you scroll down toward
    the bottom, you will see the Artifacts section (*Figure 2.13*). This lists all
    the metadata regarding the model and the serialized model itself:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13 – The model artifacts logged by MLflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_013.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.13 – The model artifacts logged by MLflow
  prefs: []
  type: TYPE_NORMAL
- en: The MLflow Tracking Server's Backend Store and Artifact Store
  prefs: []
  type: TYPE_NORMAL
- en: 'An MLflow tracking server has two types of storage: first, a backend store,
    which stores experiments and runs metadata along with params, metrics, and tags
    for runs; and second, an artifact store, which stores larger files such as serialized
    models, text files, or even generated plots for visualizing model results. For
    the purpose of simplicity, in this chapter, we are using a local filesystem for
    both the backend store and the artifact store. However, some of the more advanced
    features such as model registry are not available in a filesystem-based artifact
    store. In later chapters, we will learn how to use a model registry.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s look at the list of artifacts, one by one:'
  prefs: []
  type: TYPE_NORMAL
- en: '`model_summary.txt`: At the `root` folder level, this file looks similar to
    the following output if you click on it. It describes the model metrics and the
    layers of the DL model (please refer to *Figure 2.14*):'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.14 – The model summary file logged by MLflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_014.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.14 – The model summary file logged by MLflow
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2.14* provides a quick overview of what the DL model looks like in
    terms of the number and type of neural network layers, the number and size of
    the parameters, and the type of metrics used in training and validation. This
    is very helpful when the DL model architecture is needed to be shared and communicated
    among team members or stakeholders.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `model` folder: This folder contains a subfolder, called `data`, and three
    files called `MLmodel`, `conda.yaml`, and `requirements.txt`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`MLmodel`: This file describes the flavor of the model that MLflow supports.
    `MLmodel` file (*Figure 2.15*):'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.15 – Content of the MLmodel file for our first DL model run with
    MLflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_015.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.15 – Content of the MLmodel file for our first DL model run with MLflow
  prefs: []
  type: TYPE_NORMAL
- en: '*Figure 2.15* illustrates that this is a PyTorch flavor model with `run_id`
    that we have just run.'
  prefs: []
  type: TYPE_NORMAL
- en: '`conda.yaml`: This is a conda environment definition file used by the model
    to describe our dependencies. *Figure 2.16* lists the content logged by MLflow
    in the run we just completed:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![Figure 2.16 – The content of the conda.yaml file logged by MLflow'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_02_016.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 2.16 – The content of the conda.yaml file logged by MLflow
  prefs: []
  type: TYPE_NORMAL
- en: '`requirements.txt`: This is a Python `pip`-specific dependency definition file.
    It is just like the `pip` section in the `conda.yaml` file, as shown in *Figure
    2.16*.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`data`: This is a folder that contains the actual serialized model, called
    `model.pth`, and a description file, called `pickle_module_info.txt`, whose content
    for our first DL experiment is as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This means the model is serialized using a PyTorch-compatible pickle serialization
    method provided by MLflow. This allows MLflow to load the model back to memory
    later if needed.
  prefs: []
  type: TYPE_NORMAL
- en: Model Registry versus Model Logging
  prefs: []
  type: TYPE_NORMAL
- en: The MLflow model registry requires a relational database such as MySQL as the
    artifact store, not just a plain filesystem. Therefore, in this chapter, we will
    not explore it yet. Note that a model registry is different from model logging
    in that, for each run, you want to log model metadata and artifacts. However,
    only for certain runs that meet your production requirements, you may want to
    register them in the model registry for production deployment and version control.
    In later chapters, we will learn how to register models.
  prefs: []
  type: TYPE_NORMAL
- en: By now, you should have a good understanding of the list of files and metadata
    about the model and the serialized model (along with the `.pth` file extension
    in our experiment, which refers to a PyTorch serialized model) logged in the MLflow
    artifact store. In the upcoming chapters, we will learn more about how the MLflow
    model flavor works and how to use the logged model for model registry and deployment.
    MLflow model flavors are model frameworks such as PyTorch, TensorFlow, and scikit-learn,
    which are supported by MLflow. Interested readers can find more details about
    the current built-in model flavors supported by MLflow from the official MLflow
    documentation site at [https://www.mlflow.org/docs/latest/models.html#built-in-model-flavors](https://www.mlflow.org/docs/latest/models.html#built-in-model-flavors).
  prefs: []
  type: TYPE_NORMAL
- en: Exploring MLflow code tracking and its usages
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'When exploring the metadata of the run, we can also discover how the code is
    being tracked. As shown in the MLflow UI and the command-line output in JSON,
    the code is tracked in three ways: a filename, a Git commit hash, and a source
    type. You can execute the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'You should be able to find the following segments of JSON key-value pairs in
    the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'Based on this `ad6c7338a416ff4c2848d726b092057457c22408` Git commit hash, we
    can go on to find the exact copy of the Python code we used: [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/blob/ad6c7338a416ff4c2848d726b092057457c22408/chapter02/first_dl_with_mlflow.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLflow/blob/ad6c7338a416ff4c2848d726b092057457c22408/chapter02/first_dl_with_mlflow.py).'
  prefs: []
  type: TYPE_NORMAL
- en: Note that, here, the source type is `LOCAL`. This means that we execute the
    MLflow-enabled source code from a local copy of the code. In later chapters, we
    will learn about other source types.
  prefs: []
  type: TYPE_NORMAL
- en: LOCAL versus Remote GitHub Code
  prefs: []
  type: TYPE_NORMAL
- en: 'If the source is a local copy of the code, there is a caveat regarding the
    Git commit hash that you see in the MLflow tracking server. If you make code changes
    locally but forget to commit them and then immediately start an MLflow experiment
    tracking run, MLflow will only log the most recent Git commit hash. We can solve
    this problem in one of two ways:'
  prefs: []
  type: TYPE_NORMAL
- en: 1\. Commit our code changes before running the MLflow experiment.
  prefs: []
  type: TYPE_NORMAL
- en: 2\. Use remote GitHub code to run the experiment.
  prefs: []
  type: TYPE_NORMAL
- en: Since the first method is not easily enforceable, the second method is preferred.
    Using remote GitHub code to run a DL experiment is an advanced topic that we will
    explore in later chapters.
  prefs: []
  type: TYPE_NORMAL
- en: So far, we have learned about the MLflow tracking server, experiments, and runs.
    Additionally, we have logged metadata about runs such as parameters and metrics,
    examined code tracking, and explored model logging. These tracking and logging
    capabilities ensure that we have a solid ML experiment management system, not
    only for model development but also for model deployment in the future, as we
    need to track which runs produce the model for production. *Reproducibility* and
    *provenance-tracking* are the hallmarks of what MLflow provides. In addition to
    this, MLflow provides other components such as **MLproject** for standardized
    ML project code organization, a model registry for model versioning control, model
    deployment capabilities, and model explainability tools. All of these MLflow components
    cover the whole life cycle of ML/DL development, deployment, and production, which
    we will examine in more depth in future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we learned how to set up MLflow to work with either a local
    MLflow tracking server or a remote MLflow tracking server. Then, we implemented
    our first DL model with MLflow autologging enabled. This allowed us to explore
    MLflow in a hands-on way to understand a few central concepts and foundational
    components such as experiments, runs, metadata about experiments and runs, code
    tracking, model logging, and model flavor. The knowledge and first-round experiences
    gained in this chapter will help us to learn more in-depth MLflow tracking APIs
    in the next chapter.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To further your knowledge, you can consult the following resources and documentation:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The MLflow *Command-Line Interface* documentation: [https://www.mlflow.org/docs/latest/cli.html](https://www.mlflow.org/docs/latest/cli.html)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The MLflow PyTorch autologging documentation: [https://www.mlflow.org/docs/latest/tracking.html#pytorch-experimental](https://www.mlflow.org/docs/latest/tracking.html#pytorch-experimental)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The MLflow PyTorch model flavor documentation: [https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html#module-mlflow.pytorch](https://www.mlflow.org/docs/latest/python_api/mlflow.pytorch.html#module-mlflow.pytorch)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*MLflow and PyTorch — Where Cutting Edge AI meets MLOps*: [https://medium.com/pytorch/mlflow-and-pytorch-where-cutting-edge-ai-meets-mlops-1985cf8aa789](https://medium.com/pytorch/mlflow-and-pytorch-where-cutting-edge-ai-meets-mlops-1985cf8aa789)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*Controlled Experiments in Machine Learning*: [https://machinelearningmastery.com/controlled-experiments-in-machine-learning/](https://machinelearningmastery.com/controlled-experiments-in-machine-learning/)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
