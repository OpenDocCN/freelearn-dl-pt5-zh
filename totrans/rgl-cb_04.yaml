- en: '4'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Regularization with Tree-Based Models
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Tree-based models using ensemble learning such as Random Forest or Gradient
    Boosting are often seen as easy-to-use, state-of-the-art models for regular machine
    learning tasks.
  prefs: []
  type: TYPE_NORMAL
- en: Many Kaggle competitions have been won with such models, as they can be quite
    robust and efficient at finding complex patterns in data. Knowing how to regularize
    and fine-tune them is key to having the very best performance.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we’ll look at the following recipes:'
  prefs: []
  type: TYPE_NORMAL
- en: Building a classification tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building regression trees
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularizing a decision tree
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a Random Forest algorithm
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization of Random Forest
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training a boosting model with XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization with XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, you will train and fine-tune several decision tree-based models,
    as well as visualize a tree. The following libraries will be required for this
    chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: NumPy
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Matplotlib
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Scikit-learn
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Graphviz
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: XGBoost
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: pickle
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Building a classification tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Decision trees are a separate class of models in machine learning. Although
    a decision tree alone can be considered a weak learner, combined with the power
    of ensemble learning such as bagging or boosting, decision trees get great performances.
    Before digging into ensemble learning models and how to regularize them, in this
    recipe, we will review how decision trees work and how to use them on a classification
    task on the iris dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'To give an intuition of the power of decision trees, let’s consider a use case.
    We would like to know whether to sell ice creams on the beach based on two input
    features: sun and temperature.'
  prefs: []
  type: TYPE_NORMAL
- en: We have the data in *Figure 4**.1* and would like to train a model on it.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – A circle if we should sell ice creams as a function of sun and
    temperature and a cross if we shouldn’t](img/B19629_04_01.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.1 – A circle if we should sell ice creams as a function of sun and
    temperature and a cross if we shouldn’t
  prefs: []
  type: TYPE_NORMAL
- en: For a human, this seems quite easy. For a linear model though, not so much.
    If we try to use logistic regression on this data, it will end up drawing a decision
    line such as the left in *Figure 4**.2*. Even with features that are raised to
    a higher power level, the logistic regression would struggle and propose something
    such as the decision line on the right in *Figure 4**.2*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Potential result of a linear model at classifying this dataset:
    on the left with raw features, on the right with higher power features](img/B19629_04_02.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.2 – Potential result of a linear model at classifying this dataset:
    on the left with raw features, on the right with higher power features'
  prefs: []
  type: TYPE_NORMAL
- en: 'In a word, this data is not linearly separable. But it can be divided into
    two separate linearly separable problems:'
  prefs: []
  type: TYPE_NORMAL
- en: Is the weather sunny?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Is the temperature warm?
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we fulfill those two conditions, then we should sell ice cream. This can
    be summarized as the tree in *Figure 4**.3*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – A decision tree correctly classifying all data points](img/B19629_04_03.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.3 – A decision tree correctly classifying all data points
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s cover a bit of vocabulary here:'
  prefs: []
  type: TYPE_NORMAL
- en: 'We have **Warm**, which is the first decision node and the root node with two
    branches: **Yes** and **No**.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have another decision node in **Sunny**. A decision node is any node containing
    two (sometimes more) branches.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: We have three leaves. A leaf does not have any branches and contains a final
    prediction.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Just as for binary trees in computer science, the depth of the tree is the number
    of edges between the root node and the lowest leaf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we go back to our dataset, the decision line would now look like the one
    in *Figure 4**.4* with not one but two lines combined, providing an effective
    solution:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.4 – Result of a decision tree at classifying this dataset](img/B19629_04_04.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.4 – Result of a decision tree at classifying this dataset
  prefs: []
  type: TYPE_NORMAL
- en: From now on, any new data will fall into one of those leaves, allowing it to
    be classified correctly.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is the power of decision trees: they can compute complex, nonlinear rules,
    allowing them great flexibility. A decision tree is trained using a **greedy algorithm**,
    meaning it only tries to optimize one step at a time.'
  prefs: []
  type: TYPE_NORMAL
- en: More specifically, it means the decision tree is not optimized globally, but
    one node at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'This can be seen as a recursive algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: Take all samples in a node.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Find a threshold in a feature that minimizes the disorder of the splits. In
    other words, find the feature and threshold giving the best class separation.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Split this into two new nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go back to *step 1* until your node is pure (meaning that only one class remains)
    or any other condition, and thus a leaf.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: But how do we actually choose the splits so that they are optimal? Of course,
    we use a loss function, which uses disorder measurement. Let’s dig into those
    two topics before wrapping it up.
  prefs: []
  type: TYPE_NORMAL
- en: Disorder measurement
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: For a classification tree to be effective, it must have as little disorder as
    possible in its leaves. Indeed, in the previous example, we assumed all leaves
    are pure. They contain samples from only one class. In reality, leaves may be
    impure and contain samples from several classes.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: If after training a tree a leaf remains impure, we would use the majority class
    of that leaf for classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, the idea is to minimize impurity, but how do we measure it? There are two
    ways: entropy and Gini impurity. Let’s have a look at both.'
  prefs: []
  type: TYPE_NORMAL
- en: Entropy
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Entropy is a general word that is used in many contexts, such as physics and
    computer science. The entropy **E** we use here can be defined with the following
    equation, where pi is the proportion of subclass ![](img/Formula_04_001.png) in
    a sample:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_002.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s consider a concrete example as depicted in *Figure 4**.5*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – A node with 10 samples of two classes: red and blue](img/B19629_04_05.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 4.5 – A node with 10 samples of two classes: red and blue'
  prefs: []
  type: TYPE_NORMAL
- en: 'In this example, the entropy would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_003.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Indeed, we have the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_004.png)=3/10, since we have three blue samples out of 10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_04_005.png)=7/10, since we have seven blue samples out of 10'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'If we look at extreme cases, we understand entropy is well suited to compute
    disorder:'
  prefs: []
  type: TYPE_NORMAL
- en: if ![](img/Formula_04_006.png)=0, then ![](img/Formula_04_007.png)=1 and E =
    0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if ![](img/Formula_04_008.png)pblue = ![](img/Formula_04_009.png) = 0.5, then
    E = 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'So, we understand that the entropy reaches a maximum value of one when the
    node contains perfectly mixed samples, and the entropy goes to zero when a node
    contains only one class. This is summarized by the curve in *Figure 4**.6*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Entropy as a function of p for two classes](img/B19629_04_06.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.6 – Entropy as a function of p for two classes
  prefs: []
  type: TYPE_NORMAL
- en: Gini impurity
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Gini impurity is another way to measure disorder. The formula of Gini impurity
    G is quite simple:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_010.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Again, ![](img/Formula_04_011.png) is the proportion of class in the node.
  prefs: []
  type: TYPE_NORMAL
- en: 'Applied to the example node in *Figure 4**.5*, the computation of the Gini
    impurity would lead to the following result:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_012.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'The result is quite different from entropy, but let’s check that the properties
    remain the same with extreme values:'
  prefs: []
  type: TYPE_NORMAL
- en: if ![](img/Formula_04_013.png), then ![](img/Formula_04_014.png) and G = 0
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: if ![](img/Formula_04_015.png) = 0.5, then G = 0.5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Indeed, the Gini impurity reaches a maximum value of 0.5 when the disorder is
    maximum and is equal to 0 when the node is pure.
  prefs: []
  type: TYPE_NORMAL
- en: Entropy or Gini?
  prefs: []
  type: TYPE_NORMAL
- en: That said, what should we use? Well, this can be seen as a hyperparameter, and
    scikit-learn’s implementation allows one to choose between entropy and Gini.
  prefs: []
  type: TYPE_NORMAL
- en: In practice, the results are often the same for both. But Gini impurity is faster
    to compute (entropy involves more expensive log computations), so it is usually
    the first choice.
  prefs: []
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We have a disorder measurement, but what is the loss we should minimize? The
    ultimate goal is to make splits that minimize the disorder, one node at a time.
  prefs: []
  type: TYPE_NORMAL
- en: 'Considering a decision node always has two children, we can define them as
    left and right nodes. Then, the loss for this node can be written like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_016.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Let’s break down the formula:'
  prefs: []
  type: TYPE_NORMAL
- en: m, ![](img/Formula_04_017.png), and ![](img/Formula_04_018.png) are the number
    of samples in each node respectively
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '![](img/Formula_04_019.png) and ![](img/Formula_04_020.png) are the Gini impurities
    of the left and right nodes'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Of course, this can be computed with entropy instead of Gini impurity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Assuming we choose a split decision, we then have a parent node and two children
    nodes defined by the split in *Figure 4**.7*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – One parent node and two children nodes, with their respective
    Gini impurities](img/B19629_04_07.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.7 – One parent node and two children nodes, with their respective Gini
    impurities
  prefs: []
  type: TYPE_NORMAL
- en: 'In this case, the loss **L** would be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_021.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Using this loss computation, we are now able to minimize the impurity (thus
    maximizing the purity of a node).
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: What we defined here as the loss is only the loss at a node level. Indeed, as
    stated earlier, the decision tree is trained using a greedy approach, not a gradient
    descent.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Finally, before going into the practical details of this recipe, we need to
    have the following libraries installed: scikit-learn, `graphviz`, and `matplotlib`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'They can be installed with the following command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before actually training a decision tree, let’s quickly go through all the
    steps to train a decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: We have a node containing samples of N classes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We iterate through all the features and all the possible values of a feature.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: For each feature value, we compute the Gini impurity and the loss.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: We keep the feature value with the lowest loss and split the node into two children
    nodes.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go back to *step 1* with both nodes until a node is pure (or the stop condition
    is fulfilled).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Using this approach, the decision tree will eventually find the right set of
    decisions to successfully separate any classes. Then, two cases are possible for
    each leaf:'
  prefs: []
  type: TYPE_NORMAL
- en: If the leaf is pure, predict this class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: If the leaf is impure, predict the most represented class
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: One way to test all the possible feature values is to use all the existing values
    in the dataset. Another is to use a linear split over the range of existing values
    in the dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now train a decision tree on the iris dataset with scikit-learn:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, we need the required imports: `matplotlib` for data visualization (not
    necessary otherwise), `load_iris` for loading the dataset, `train_test_split`
    for splitting the data into training and test sets, and the `DecisionTreeClassifier`
    decision tree implementation from scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE2]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE3]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE4]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can now load the data using `load_iris`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE6]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We split the dataset into training and test sets with `train_test_split`, keeping
    the default parameters and only specifying the random state for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE8]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE9]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this step, we display a two-dimensional projection of the data. This is
    just for pedagogical purposes but is not mandatory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE11]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE12]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – The three iris classes as a function of the sepal width and
    sepal length (plot produced  by the code)](img/B19629_04_08.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.8 – The three iris classes as a function of the sepal width and sepal
    length (plot produced by the code)
  prefs: []
  type: TYPE_NORMAL
- en: 'Instantiate the `DecisionTreeClassifier` model. We use the default parameters
    here:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model on the training set. We did not prepare the data with any preprocessing
    here because we have only quantitative features, and decision trees are not sensitive
    to scale, unlike linear models. But it would not hurt either to rescale the quantitative
    features:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, we evaluate the accuracy of the model on both the training and test
    sets, using the `score()` method of the classification tree:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This prints the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: We are achieving satisfactory results, even if we are clearly facing overfitting
    with 100% accuracy on the train set.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Unlike linear models, there are no weights associated with each feature since
    a tree is made up of splits.
  prefs: []
  type: TYPE_NORMAL
- en: 'For visualization purposes, we can display the tree thanks to the `graphviz`
    library. This is mostly for pedagogical use or interest but is not necessarily
    useful otherwise:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the tree for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – Tree visualization produced by the graphviz library](img/B19629_04_09.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.9 – Tree visualization produced by the graphviz library
  prefs: []
  type: TYPE_NORMAL
- en: From this tree visualization, we can see that the 37 samples of the setosa class
    are fully classified right away at the first decision node (considering the data
    visualization, this should not be a surprise). The samples of classes virginica
    and versicolor seem to be much more intertwined in the provided features, thus
    the tree requires many more decision nodes to fully discriminate them.
  prefs: []
  type: TYPE_NORMAL
- en: 'Unlike linear models, we do not have weights associated with each feature.
    But we can have a piece of somehow equivalent information, called feature importance,
    available with the `.``feature_importances` attribute:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot for it:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Feature importance as a function of the feature name (histogram
    produced by the code)](img/B19629_04_10.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.10 – Feature importance as a function of the feature name (histogram
    produced by the code)
  prefs: []
  type: TYPE_NORMAL
- en: This feature importance is relative (meaning the sum of all feature importance
    is equal to 1) and is computed based on the number of samples classified thanks
    to this feature.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Feature importance is computed based on the amount of reduction of the metric
    used for splitting (for example, Gini impurity or entropy). If one single feature
    allows to make all the splits, then this feature will have an importance of 1.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The sci-kit learning documentation on classification trees as available at
    the following URL: [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Building regression trees
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Before digging into the regularization of decision trees in general, let’s have
    a recipe for regression trees. Indeed, all the explanations in the previous recipe
    were assuming we have a classification task. Let’s explain how to apply it to
    a regression task and apply it to the California housing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'For regression trees, only a few steps need to be modified compared to classification
    trees: the inference and the loss computation. Besides that, the overall principle
    is the same.'
  prefs: []
  type: TYPE_NORMAL
- en: The inference
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In order to make an inference, we can no longer use the most represented class
    in a leaf (or in the case of pure leaf, the only class). So, we use the average
    of the labels in each node.
  prefs: []
  type: TYPE_NORMAL
- en: In the example proposed in *Figure 4**.11*, assuming this is a leaf, we would
    have an inference value that is the average of those 10 values equal to 14 in
    this case.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – The example of 10 samples with associated values](img/B19629_04_11.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.11 – The example of 10 samples with associated values
  prefs: []
  type: TYPE_NORMAL
- en: The loss
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Instead of using a disorder measurement to compute the loss, in regression
    trees, the mean squared error is used. So, the loss is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_022.jpg)'
  prefs: []
  type: TYPE_IMG
- en: 'Assume again a given split leading to the ![](img/Formula_04_023.png) samples
    in the left node and the ![](img/Formula_04_024.png) samples in the right node.
    The **MSE** for each split is computed using the average of the labels into that
    node:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.12 – Example of a node split on a regression task](img/B19629_04_12.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.12 – Example of a node split on a regression task
  prefs: []
  type: TYPE_NORMAL
- en: 'If we take the example of the proposed split in *Figure 4**.12*, we have all
    we need to compute the L loss:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_025.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Based on those two slight changes, we can train a regression tree with the same
    recursive, greedy algorithm as with classification trees.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Before getting practical, all we need for this recipe is to have the scikit-learn
    library installed. If not yet installed, just type the following command line
    in your terminal:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'We will train a regression tree using the `DecisionTreeRegressor` class from
    scikit-learn on the California housing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, the required imports: the `fetch_california_housing` function to load
    the California housing dataset, the `train_test_split` function to split data,
    and the `DecisionTreeRegressor` class with the regression tree implementation:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE29]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE30]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset using the `fetch_california_housing` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and test sets using the `train_test_split` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the `DecisionTreeRegressor` object. We just keep the default parameters
    here, but they can be customized at this point:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the regression tree on the training set using the `.fit()` method of
    the `DecisionTreeRegressor` class. Note that we do not apply any specification
    preprocessing to the data because we have only quantitative features, and decision
    trees are not sensitive to feature scale issues:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the R2-score of the regression tree on both the training and test
    sets using the built-in `.score()` method of the model class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This would show something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: As we can see, we face here a strong overfitting, having a perfect R2-score
    on the training set, while having a much worse (but still decent overall) R2-score
    on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Look at the official `DecisionTreeRegressor` documentation more information:
    [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.xhtml#sklearn-tree-decisiontreeregressor](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeRegressor.xhtml#sklearn-tree-decisiontreeregressor).'
  prefs: []
  type: TYPE_NORMAL
- en: Regularizing a decision tree
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this recipe, we will look at the means to regularize decision trees. We will
    review and comment on a couple of methods for reference and provide a few more
    to be explored.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Obviously, we cannot use L1 or L2 regularization as we did with linear models.
    Since we have no weights for the features and no overall loss such as the mean
    squared error or the binary cross entropy, it is not possible to apply this method
    here.
  prefs: []
  type: TYPE_NORMAL
- en: But we do have other ways to regularize, such as the max depth of the tree,
    the minimum number of samples per leaf, the minimum number of samples per split,
    the max number of features, or the minimum impurity decrease. In this recipe,
    we will look at those.
  prefs: []
  type: TYPE_NORMAL
- en: 'To do that, we only need the following libraries: scikit-learn, `matplotlib`
    and `NumPy`. Also, since we will provide some visualization to give some idea
    of regularization, we will use the following `plot_decision_function` function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: This function will allow us to visualize the decision function of our decision
    tree and get a better understanding of what is overfitting and regularization
    when it comes to a classification tree.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: We will give a recipe to regularize a decision tree based on the maximum depth,
    and then explore a few others in the *There’s* *more* section.
  prefs: []
  type: TYPE_NORMAL
- en: The maximum depth is quite often one of the first hyperparameters to fine-tune
    when trying to regularize. Indeed, as we have seen earlier, decision trees can
    learn complex data patterns using more decision nodes. If not stopped, the decision
    trees may tend to overfit the data with too many consecutive decision nodes.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will now train a classification tree with a limited maximum depth on the
    iris dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Make the required imports:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: The `load_iris` function to load the dataset
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: The `train_test_split` function to split the data into training and test sets
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The `DecisionTreeClassifier` class:'
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE44]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE45]'
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset using the `load_iris` function. In order to be able to fully
    visualize the effects of regularization, we also keep only two features out of
    four, so that we can display them on a plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE47]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data into training and test sets using the `train_test_split` function.
    We only specify the random state for reproducibility and let the other parameters
    be by default:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate a decision tree object, limiting the maximum depth to five with
    the `max_depth=5` parameter. We also set the random state to `0` for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Fit the classification tree on the training set using the `.fit()` method.
    As mentioned earlier, since the features are all quantitative and decision trees
    are not sensitive to the features scale, there is no need to apply rescaling:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the model accuracy, using the `.score()` method of the `DecisionTreeClassifier`
    class:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This would print the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: How it works…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In order to have a better understanding of how it works, let’s look at the
    two dimensions of the iris dataset we retained. We will use the `plot_decision_function()`
    function defined in *Getting ready* to plot the decision function of a decision
    tree with no regularization (that is, default hyperparameters):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the plot:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.13 – Decision function of the model as a function of the sepal width
    and sepal length with a very complex and questionable decision function (plot
    produced by the code)](img/B19629_04_13.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.13 – Decision function of the model as a function of the sepal width
    and sepal length with a very complex and questionable decision function (plot
    produced by the code)
  prefs: []
  type: TYPE_NORMAL
- en: From this plot, we can deduce we are typically facing overfitting. Indeed, the
    boundaries are really specific, sometimes trying to make a complex pattern for
    only one sample, instead of focusing on the higher-level pattern.
  prefs: []
  type: TYPE_NORMAL
- en: 'Indeed, if we look at the accuracy score for both the training and test set,
    we have the following results:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: While the accuracy is about 94% on the training set, it is only about 63% on
    the test set. There is overfitting, and regularization may be helpful.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The accuracy is far lower than in the first recipe because we use only two features
    for visualization and pedagogical purposes. The reasoning remains true if we keep
    the four features, though.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s now add regularization by limiting the maximum depth of the decision
    tree as we did in this recipe:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: If the maximum depth of the tree is `min_samples_split` samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'It means that by default, the trees are expanded with no limit on the depth.
    The limit is then perhaps set by other factors and may go very deep. If we fix
    that by limiting the depth to `5`, let’s see the impact on the decision function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.14 – Decision function with maximum depth regularization (plot produced
    by the code)](img/B19629_04_14.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.14 – Decision function with maximum depth regularization (plot produced
    by the code)
  prefs: []
  type: TYPE_NORMAL
- en: 'By limiting the max depth to `5`, we get a less specific decision function,
    even if there seems to be some overfitting remaining. If we have a look again
    at the accuracy score, we can see that it actually helped slightly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: 'This would provide the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Indeed, the accuracy score on the test set climbed from 63% to 66%, while the
    accuracy on the training set decreased from 95% to 87%. This is typically what
    we can expect from regularization: this added bias (and thus decreased training
    set performances) and decreased the variance (and thus allowed us to generalize
    better).'
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The maximum depth hyperparameter is really convenient because it’s easy to understand
    and fine-tune. But, there are many other hyperparameters that can help regularize
    decision trees. Let’s review some of them here. We will focus on the minimum sample
    hyperparameters and then propose a few other hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: Minimum samples
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Other hyperparameters allowing us to regularize are the ones controlling the
    minimum number of samples per leaf and the minimum number of samples per split.
  prefs: []
  type: TYPE_NORMAL
- en: The idea is rather straightforward and intuitive but more subtle than the maximum
    depth. In the decision tree we visualized earlier in this chapter, we could see
    that the first splits classify substantial amounts of samples. The first split
    successfully classified 37 samples as setosa while keeping 75 in the other split.
    On the other end of the decision tree, the lowest nodes are sometimes splitting
    over only three or four samples.
  prefs: []
  type: TYPE_NORMAL
- en: Is splitting over only three samples significant? What if out of these three
    samples, there is an outlier? Generally, it does not sound like a promising idea
    to create a rule for only three samples if the end goal is to have a robust, well-generalizing
    model.
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two different but somewhat related hyperparameters that allow us to
    deal with that:'
  prefs: []
  type: TYPE_NORMAL
- en: '`min_samples_split`: The minimum samples required to split an internal node.
    If a float is provided, then it uses a fraction of the total number of samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_samples_leaf`: The minimum samples required to be considered a leaf. If
    a float is provided, then it uses a fraction of the total number of samples.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: While `min_samples_split` is acting at the decision node level, `min_samples_leaf`
    is acting only at the leaf level.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s see if that allows us to avoid overfitting in specific regions in our
    case. We set the minimum number of samples per split to 15 (while keeping all
    other parameters to default values). This is expected to regularize, since we
    know from the decision tree visualization some splits were for less than five
    samples:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Here is the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.15 – Decision function with minimum samples per split regularization
    (plot produced by the code)](img/B19629_04_15.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.15 – Decision function with minimum samples per split regularization
    (plot produced by the code)
  prefs: []
  type: TYPE_NORMAL
- en: The resulting decision function is a bit different than when regularizing with
    the maximum depth and seems to be indeed more regularized than without constraint
    on this hyperparameter.
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also look at the accuracy score to confirm the regularization was successful:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'We’ll get the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: Compared to default hyperparameters, the accuracy score on the test set climbed
    from 63% to 74%, while the accuracy on the training set decreased from 95% to
    86%. Compared to the maximum depth hyperparameter, we added slightly more regularization,
    and got slightly better results on the test set.
  prefs: []
  type: TYPE_NORMAL
- en: In general, the hyperparameters on the number of samples (either per leaf or
    split) may allow a finer regularization than the maximum depth. Indeed, the max
    depth hyperparameter is setting a common hard limit to the whole decision tree.
    But it may happen that two nodes at the same depth level do not carry the same
    number of samples. One node may have hundreds of samples (and then a splitting
    is probably relevant), while another node may have just a few samples.
  prefs: []
  type: TYPE_NORMAL
- en: 'The criterion for a minimum number of samples on its side is more subtle: no
    matter the depth in the tree, if a node does not have enough samples, then we
    decide it is not worth splitting.'
  prefs: []
  type: TYPE_NORMAL
- en: Other hyperparameters
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Other hyperparameters can be used to regularize. We will not go through all
    the details for each of them, but rather list them and explain them briefly:'
  prefs: []
  type: TYPE_NORMAL
- en: '`max_features`: By default, the decision tree is finding the best split among
    all features. You can choose to add randomness by setting another maximum number
    of features to use at each split. May add regularization by adding noise.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_leaf_nodes`: Set a straight limit on the number of leaves in the tree.
    Somewhat like the max depth hyperparameter, it will regularize by limiting the
    number of splits, giving priority to nodes having the highest impurity reduction.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_impurity_decrease`: This will split a node only if the impurity decrease
    is above the given threshold. This allows us to regularize by selecting highly
    impacting node splits only.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Although we did not mention regression trees, the behavior and principles are
    analogous, and the same hyperparameters can be fine-tuned with the same behavior.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The scikit-learn documentation is pretty explicit about all the hyperparameters
    and their potential impact: [https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.xhtml).'
  prefs: []
  type: TYPE_NORMAL
- en: Training the Random Forest algorithm
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: The Random Forest algorithm is an ensemble learning model, meaning it uses an
    ensemble of decision trees, hence *forest* in its name.
  prefs: []
  type: TYPE_NORMAL
- en: In this recipe, we will explain how it works and then train a Random Forest
    model on the California housing dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Ensemble learning is based somehow on the idea of collective intelligence. Let’s
    do a thought experiment to understand the power of collective intelligence.
  prefs: []
  type: TYPE_NORMAL
- en: Let’s assume we have a bot that randomly answers correctly to any binary question
    51% of the time. This would be considered inefficient and unreliable.
  prefs: []
  type: TYPE_NORMAL
- en: But now, let’s also assume we are using not only one but an army of those randomly
    answering bots and use the majority vote as the final answer. If we have 1,000
    of those bots, the majority vote will provide the right answer 75% of the time.
    If we have 10,000 bots, the majority vote will provide the right answer 97% of
    the time. This would turn a low-performing system into a remarkably high-performing
    system.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'A strong assumption was made for this example: each bot must be independent
    of the others. Otherwise, this example does not hold true. Indeed, the extreme
    counter-example would be that all bots are answering the same answer to any question,
    in which case, no matter how many bots you use, the accuracy remains at 51%.'
  prefs: []
  type: TYPE_NORMAL
- en: This is the idea of collective intelligence, which relates somehow to human
    society. Most of the time, collective knowledge outperforms individual knowledge.
  prefs: []
  type: TYPE_NORMAL
- en: 'This is also the idea behind ensemble models: an ensemble of weak learners
    can become a powerful model. To do that with Random Forest, we need to define
    two key aspects:'
  prefs: []
  type: TYPE_NORMAL
- en: How to compute the majority vote
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: How to ensure the independence of each decision tree in our model with randomness
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Majority vote
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'To properly explain majority vote, let’s imagine we have an ensemble of three
    decision trees trained on a binary classification task. On a given sample, the
    predictions are the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '| **Tree** | **Predicted probability of** **class 1** | **Class predictions**
    |'
  prefs: []
  type: TYPE_TB
- en: '| Tree 1 | 0.05 | 0 |'
  prefs: []
  type: TYPE_TB
- en: '| Tree 2 | 0.6 | 1 |'
  prefs: []
  type: TYPE_TB
- en: '| Tree 3 | 0.55 | 1 |'
  prefs: []
  type: TYPE_TB
- en: Table 4.1 – Predictions
  prefs: []
  type: TYPE_NORMAL
- en: 'We have two pieces of information for each decision tree:'
  prefs: []
  type: TYPE_NORMAL
- en: The predicted probability of class 1
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The predicted class (usually computed as class 1 if probability > 0.5, class
    0 otherwise)
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The `DecisionTreeClassifier.predict_proba()` method allows us to get the prediction
    probability. It is computed by using the proportion of the given class in the
    prediction leaf.
  prefs: []
  type: TYPE_NORMAL
- en: 'We could come up with many ways to compute the majority vote on such data,
    but let’s explore two, hard vote and soft vote:'
  prefs: []
  type: TYPE_NORMAL
- en: A hard vote is the most intuitive one. This is the simple majority vote of the
    predicted classes. In our case, class 1 is predicted two times out of three. In
    this case, the hard majority vote is class 1.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: A soft vote uses the average probability and then applies a threshold. In our
    case, the average probability is 0.4, which is below the threshold of 0.5\. In
    that case, the soft majority vote is class 0.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: It is particularly interesting to note that, even if two out of three trees
    predicted class 1, the only tree that was really confident (having a high probability)
    was the tree predicting class 0.
  prefs: []
  type: TYPE_NORMAL
- en: 'A real-life example would be, when facing a question:'
  prefs: []
  type: TYPE_NORMAL
- en: Two friends give answer A, but are unsure
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: One friend gives answer B but is highly confident
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'What would you do in such a case? The odds are you would listen to that highly
    confident friend. This is exactly what a soft majority vote is about: giving more
    power to highly confident trees. Most of the time, the soft vote outperforms the
    hard vote. Fortunately, Random Forest implementation in scikit-learn is based
    on the soft vote.'
  prefs: []
  type: TYPE_NORMAL
- en: Bagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Bagging is a key concept in Random Forest to ensure the independence of decision
    trees and is made of bootstrapping and aggregating. Let’s see how those two steps
    are working together to get the best out of ensembling decision trees.
  prefs: []
  type: TYPE_NORMAL
- en: Bootstrapping is random sampling with replacement. In simple terms, if we apply
    bootstrapping to samples with replacement, it means we will randomly pick samples
    in the dataset with replacement. What *with replacement* means is that, once a
    sample has been picked, it is not removed from the dataset and may be picked again.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s assume we have an initial dataset of 10 samples, either blue or red.
    If we use bootstrapping to select 10 samples in this initial dataset, we may have
    some samples missing, and some samples appearing several times. If we do that
    three independent times, we may have three slightly different datasets, such as
    in *Figure 4**.16*. We call those newly created datasets subsamples:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.16 – An example of bootstrapping an initial dataset three times
    and selecting 10 samples for replacement (the three created subsamples are slightly
    different)](img/B19629_04_16.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.16 – An example of bootstrapping an initial dataset three times and
    selecting 10 samples for replacement (the three created subsamples are slightly
    different)
  prefs: []
  type: TYPE_NORMAL
- en: 'Since those subsamples are slightly different, in Random Forest, a decision
    tree is trained on each of those and ends up with hopefully independent models.
    The next step is the aggregating of those models’ results, through a soft majority
    vote. Once those two steps (bootstrapping and aggregating) are combined, this
    is what we call bagging. *Figure 4**.17* summarizes those two steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.17 – Bootstrapping on the samples and then aggregating the results
    to end with an ensemble model](img/B19629_04_17.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.17 – Bootstrapping on the samples and then aggregating the results
    to end with an ensemble model
  prefs: []
  type: TYPE_NORMAL
- en: 'As we have seen, the *random* in Random Forest comes from the bootstrapping
    of the samples, meaning we randomly select a subsample of the original dataset
    for each trained decision tree. But in reality, other levels of randomness have
    been omitted for pedagogical reasons. Without going into all the details, there
    are three levels of randomness in a Random Forest algorithm:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Bootstrapping of the samples**: Samples are selected with replacement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Bootstrapping of the features**: Features are selected with replacement'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Feature selection of the best split of a node**: By default, in scikit-learn,
    all features are used, thus there is no randomness at this level'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Now that we have a solid enough understanding of how Random Forest works, let’s
    train a Random Forest algorithm on a regression task. To do so, we only need scikit-learn
    to be installed. If this has not already been done, just install it with the following
    command line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'As with other machine learning models in scikit-learn, training a Random Forest
    algorithm is quite easy. There are two main classes:'
  prefs: []
  type: TYPE_NORMAL
- en: '`RandomForestRegressor` for regression tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`RandomForestClassifier` for classification tasks'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Here, we will use the `RandomForestRegressor` on the California housing dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: 'First, let’s make the required imports: `fetch_california_housing` to load
    the data, `train_test_split` for splitting the dataset, and `RandomForestRegressor`
    for the model itself:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset using `fetch_california_housing`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data with `train_test_split`. Here, we just use the default parameters
    and set the random state to 0 for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the `RandomForestRegressor` model. We just keep the default parameters
    of the class here for simplicity; we only specify the random state:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model on the training set with the `.fit()` method. This can take
    a few seconds to compute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the R2-score on both the training and test set using the `.``score()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Our output would be as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: We have an R2-score on the training set of 97%, while on the test set, it is
    only 79%. This means we are facing overfitting, and we will see in the next recipe
    how to add regularization to such models.
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The documentation of this class in scikit-learn: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.xhtml#sklearn-ensemble-randomforestregressor](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.xhtml#sklearn-ensemble-randomforestregressor)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Likewise, there is the documentation of the Random Forest classifier for classification
    tasks: [https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.xhtml)'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization of Random Forest
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A Random Forest algorithm shares many hyperparameters with decision trees since
    a Random Forest is made up of trees. But a few more hyperparameters do exist,
    so in this recipe, we will present them and show how to use them to improve results
    on the California housing dataset regression.
  prefs: []
  type: TYPE_NORMAL
- en: Getting started
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Random Forests are known to be quite prone to overfitting. Even if it’s not
    a formal proof, in the previous recipe, we were indeed facing quite strong overfitting.
    But Random Forests, like decision trees, have many hyperparameters allowing us
    to try to reduce overfitting. As for a decision tree, we can use the following
    hyperparameters:'
  prefs: []
  type: TYPE_NORMAL
- en: Maximum depth
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimum samples per leaf
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Minimum samples per split
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_features`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_leaf_nodes`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`min_impurity_decrease`'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'But some other hyperparameters can be fine-tuned too:'
  prefs: []
  type: TYPE_NORMAL
- en: '`n_estimators`: This is the number of decision trees trained in the random
    forest.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`max_samples`: The number of samples to draw from the given dataset to train
    each decision tree. A lower value would add regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technically speaking, for this recipe, it is assumed that scikit-learn is installed.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this recipe, we will try to add regularization by limiting the max number
    of features to the log of the total number of features. If you are reusing the
    same environment as for the previous recipe, you can jump directly to *step 4*:'
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, let’s make the required imports: `fetch_california_housing` to load
    the data, `train_test_split` for splitting the dataset, and `RandomForestRegressor`
    for the model itself:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE86]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the dataset using `fetch_california_housing`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Split the data with `train_test_split`. Here, we just use the default parameters,
    meaning we have a 75%25% split and set the random state to `0` for reproducibility:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the `RandomForestRegressor` model. This time, we specify `max_features=''log2''`
    so that for each split, only a random subset (of size `log2(n)`, assuming *n*
    features) of all the features is used:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model on the training set with the `.fit()` method. This may take
    a few seconds to compute:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Evaluate the R2-score on both the training and test set using the `.``score()`
    method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This would return the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: Compared to the previous recipe with default hyperparameters, it improved the
    R2-score on the test set from 79% to 81%, while not significantly changing the
    score on the training set.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: In this case, as in many others in machine learning, it might be tricky (or
    sometimes impossible) to have the performances on the train and test set meeting
    halfway, meaning that even if the R2-score is 97% on training and 79% on the test
    set, there is absolutely no guarantee you can improve the R2-score on the test
    set. Sometimes, even the best hyperparameters are not the right key to improve
    performance.
  prefs: []
  type: TYPE_NORMAL
- en: In a word, all the regularization rules for decision trees can be applied to
    Random Forests, and a few more are available. As usual, an effective way to find
    the right set of hyperparameters is through hyperparameter optimization. Random
    Forest takes somewhat longer to train than a simple decision tree, so it may take
    some time.
  prefs: []
  type: TYPE_NORMAL
- en: Training a boosting model with XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Let’s now see another application of decision trees: boosting. While bagging
    (used in Random Forest models) is training several trees in parallel, boosting
    is about training trees sequentially. In this recipe, we will have a quick review
    of what is boosting, and then train a boosting model with XGBoost, a widely used
    boosting library.'
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Let’s have a look at introducing limits of bagging, then see how boosting may
    address some of those limits and how. Finally, let’s train a model on the already
    prepared Titanic dataset with XGBoost.
  prefs: []
  type: TYPE_NORMAL
- en: Limits of bagging
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Let’s assume we have a binary classification task, and we trained Random Forest
    in three decision trees on two features. Bagging is expected to perform well if
    anywhere in the feature space, at least two out of three decision trees are right,
    as in *Figure 4**.18*.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.18 – The absence of overlap in dashed circle areas highlights decision
    tree errors, demonstrating Random Forest’s strong performance](img/B19629_04_18.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.18 – The absence of overlap in dashed circle areas highlights decision
    tree errors, demonstrating Random Forest’s strong performance
  prefs: []
  type: TYPE_NORMAL
- en: In *Figure 4**.18*, we observe that the areas inside the dashed circles are
    where a decision tree is wrong. Since they don’t overlap, at least two out of
    three decision trees are right everywhere. Thus, Random Forest is performing well.
  prefs: []
  type: TYPE_NORMAL
- en: Unfortunately, always having two out of three decision trees right is a strong
    assumption. What happens if only one or fewer decision tree is right in the feature
    space? As represented in *Figure 4**.19*, the Random Forest algorithm starts performing
    poorly.
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.19 – When one or fewer out of three decision trees is right, Random
    Forest is performing poorly](img/B19629_04_19.jpg)'
  prefs: []
  type: TYPE_IMG
- en: Figure 4.19 – When one or fewer out of three decision trees is right, Random
    Forest is performing poorly
  prefs: []
  type: TYPE_NORMAL
- en: Let’s see how boosting can fix this issue by sequentially training decision
    trees to each try to fix the errors of the previous one.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: Those examples are simplified since Random Forest can be using soft vote, and
    thus predict a class that only a minority of trees predicted. But the principle
    remains true overall.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient boosting principles
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Gradient boosting has several implementations with a some differences: XGBoost,
    CatBoost, and LightGBM all have pros and cons, and the details of each are beyond
    the scope of this book. Rather, we will explain some general principles of the
    gradient boosting algorithm, enough to give a high-level understanding of the
    model.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The algorithm training can be summarized with the following steps:'
  prefs: []
  type: TYPE_NORMAL
- en: Compute an average guess on the ![](img/Formula_04_026.png) training set.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the pseudo-residuals of each sample toward the last guessed prediction,
    ![](img/Formula_04_027.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Train a decision tree on the pseudo-residuals as labels, allowing to have predictions
    ![](img/Formula_04_028.png).
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Compute the weight ![](img/Formula_04_029.png) of that decision tree based on
    its performance.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Update the guess with the learning rate 𝜂, 𝛾i and these predicted pseudo-residuals:
    ![](img/Formula_04_030.png)o.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Go back to *step 2*. with the updated ![](img/Formula_04_031.png), iterate until
    reaching the maximum number of decision trees or another criterion.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'In the end, the final prediction will be the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '![](img/Formula_04_032.jpg)'
  prefs: []
  type: TYPE_IMG
- en: The 𝜂 learning rate is a hyperparameter, typically 0.001.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: All boosting implementations are a bit different. For example, not all boosting
    implementations have weights 𝛾 associated with their trees. But since this is
    the case for XGBoost that we will use here, it is worth mentioning it for a better
    understanding.
  prefs: []
  type: TYPE_NORMAL
- en: In the end, having enough decision trees allows a model to perform well enough
    in most cases, hopefully. Unlike Random Forest, boosting models tend to avoid
    pitfalls such as having most number of wrong decision trees at the same place,
    since each decision tree is trying to fix remaining errors.
  prefs: []
  type: TYPE_NORMAL
- en: Also, boosting models tend to be more robust and generalized than Random Forest
    models, making them really powerful in many applications.
  prefs: []
  type: TYPE_NORMAL
- en: 'Finally, for this recipe, we will need the following libraries to be installed:
    `pickle` and `xgboost`. They can be installed using `pip` with the following command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'We will also reuse a prepared Titanic dataset from a previous recipe to avoid
    spending too much time on the data preparation. This data can be downloaded at
    [https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_02/prepared_titanic.pkl](https://github.com/PacktPublishing/The-Regularization-Cookbook/blob/main/chapter_02/prepared_titanic.pkl)
    and should be added locally before doing the recipe with the following command
    line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'XGBoost is a very popular implementation of gradient boosting. It can be used
    with the same pattern as models in scikit-learn, using the following methods:'
  prefs: []
  type: TYPE_NORMAL
- en: '`fit(X, y)` to train the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`predict(X)` to compute predictions'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`score(X, y)` to evaluate the model'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Let’s use it on the Titanic dataset with the default parameters downloaded
    locally:'
  prefs: []
  type: TYPE_NORMAL
- en: 'The first step is the required imports. Here, we need pickle to read the data
    from the binary format and the `XGBoost` classification model class `XGBClassifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We load the already prepared data using pickle. Note that we already get a
    split dataset because it was actually saved this way:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the boosting model. We specify `use_label_encoder=False` because
    our qualitative features are actually already encoded with one hot encoding and
    because this feature is about to be deprecated:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model on the training set using the `.fit()` method, exactly like
    we would do for a scikit-learn model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Compute the accuracy of the model on both training and test sets, using the
    `.score()` method. Again, this is the same as in scikit-learn:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We’ll get the following now:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs: []
  type: TYPE_PRE
- en: 'We notice overfitting: a 97% accuracy rate on the training set but only 81%
    on the test set. But in the end, the results of the test set are quite good, since
    it is quite hard to have much higher than 85% accuracy on the Titanic dataset.'
  prefs: []
  type: TYPE_NORMAL
- en: See also
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The XGBoost documentation quality is arguably not as good as scikit-learn’s,
    but it is still useful: [https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBClassifier](https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBClassifier).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'There is also the regression counterpart class XGBRegressor and its documentation:
    [https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBRegressor](https://xgboost.readthedocs.io/en/stable/python/python_api.xhtml#xgboost.XGBRegressor).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Regularization with XGBoost
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: After a recipe introducing boosting and the use of XGBoost for classification,
    let’s now have a look at how to regularize such models. We will be using the same
    Titanic dataset and try to improve test accuracy.
  prefs: []
  type: TYPE_NORMAL
- en: Getting ready
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Just like Random Forest, an XGBoost model is made of decision trees. Consequently,
    it has some hyperparameters such as the maximum depth of trees (`max_depth`) or
    the number of trees (`n_estimators`) that can allow to regularize in the same
    way. It also has several other hyperparameters related to the decision trees that
    can be fine-tuned:'
  prefs: []
  type: TYPE_NORMAL
- en: '`subsample`: The number of samples to randomly draw for training, equivalent
    to `max_sample` for scikit-learn’s decision trees. A smaller value may add regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`colsample_bytree`: The number of features to randomly draw (equivalent to
    scikit-learn’s `max_features`) for each tree. A smaller value may add regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`colsample_bylevel`: The number of features to randomly draw at the tree level.
    A smaller value may add regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`colsample_bynode`: The number of features to randomly draw at the node level.
    A smaller value may add regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Finally, more hyperparameters that are not shared with decision trees or Random
    Forest can allow fine-tuning the XGBoost models:'
  prefs: []
  type: TYPE_NORMAL
- en: '`learning_rate`: The learning rate. A smaller learning rate may train even
    closer to the training set. Thus, a larger learning rate may regularize, although
    it may also degrade the performances'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reg_alpha`: The strength of the L1 regularization. A higher value implies
    more regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`reg_lambda`: The strength of the L2 regularizations. A higher value implies
    more regularization.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Unlike other tree-based models seen so far, XGBoost allows L1 and L2 regularization
    too. Indeed, since each tree has an associated weight of 𝛾i, it is possible to
    add L1 or L2 regularization on those parameters, just the way it is done for linear
    models.
  prefs: []
  type: TYPE_NORMAL
- en: Those are the main hyperparameters to fine-tune to optimize and properly regularize
    an XGBoost whenever needed. Although it’s a powerful, robust, and efficient model,
    it can be hard to fine-tune optimally, since there is quite a large number of
    hyperparameters.
  prefs: []
  type: TYPE_NORMAL
- en: More practically, in this recipe, we will only add L1 regularization. To do
    so, all we need is to have XGBoost installed, as well as the Titanic-prepared
    data, as for the previous recipe.
  prefs: []
  type: TYPE_NORMAL
- en: How to do it…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this recipe, let’s add L1 regularization with the parameter `reg_alpha` in
    order to add bias and hopefully reduce the variance of the model. We will reuse
    the `XGBClassifier` model on the prepared Titanic data, as we did for the previous
    recipe. If your environment is still the same as the previous recipe, you can
    jump to *step 3*.
  prefs: []
  type: TYPE_NORMAL
- en: 'As usual, we start with the required imports: `pickle` to read the data from
    the binary format and the `XGBoost` classification model class `XGBClassifier`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Then, we load the already prepared data using `pickle`. It assumes the `prepared_titanic.pkl`
    file is locally downloaded:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Instantiate the boosting model. Besides specifying `use_label_encoder=False`,
    we now specify `reg_alpha=1` to add `L1` regularization:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Train the model on the training set using the `.``fit()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Finally, compute the accuracy of the model on both training and test sets,
    using the `.``score()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE119]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE120]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '[PRE121]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'This would print the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE122]'
  prefs: []
  type: TYPE_PRE
- en: Compared to the previous recipe with default hyperparameters, adding L1 penalization
    allowed us to improve the results. The accuracy scores are now about 84% on the
    test set, and they lowered to 94% on the training set, effectively adding regularization.
  prefs: []
  type: TYPE_NORMAL
- en: There’s more…
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Finding the best hyperparameter set with XGBoost can be tricky as there are
    many hyperparameters. Of course, using hyperparameter optimization techniques
    is required to gain some previous time.
  prefs: []
  type: TYPE_NORMAL
- en: For regression tasks, as mentioned earlier, the `XGBoost` library has an `XGBRegressor`
    class that makes them possible, with the same hyperparameters having the same
    effects on regularization.
  prefs: []
  type: TYPE_NORMAL
