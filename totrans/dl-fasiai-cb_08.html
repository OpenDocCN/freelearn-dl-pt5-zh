<html><head></head><body>
		<div id="_idContainer295">
			<h1 id="_idParaDest-190"><em class="italic"><a id="_idTextAnchor205"/>Chapter 8</em>: Extended fastai and Deployment Features</h1>
			<p>So far in this book, you have learned how to ingest and explore datasets using fastai, how to train fastai models with tabular, text, and image datasets, and how to deploy fastai models. Throughout the book so far, the emphasis has been on covering as much of the functionality of fastai as possible using the highest-level fastai API. In particular, we have emphasized using <strong class="source-inline">dataloaders</strong> objects as the basis for defining the datasets used to train the model. Up to this point in the book, we have taken the <em class="italic">happy path</em> whenever possible. To demonstrate how to accomplish tasks using fastai, we have chosen the most straightforward way possible. </p>
			<p>In this chapter, we are going to take some steps off the <em class="italic">happy path</em> to explore additional features of fastai. You will learn how to track what is happening with your model more closely, how to control the training process, and generally how to take advantage of more of the capabilities that fastai has to offer. We are also going to cover some more advanced topics related to model deployment. </p>
			<p>Here are the recipes that will be covered in this chapter:</p>
			<ul>
				<li>Getting more details about models trained with tabular data</li>
				<li>Getting more details about image classification models</li>
				<li>Training a model with augmented data</li>
				<li>Using callbacks to get the most out of your training cycle</li>
				<li>Making your model deployments available to others</li>
				<li>Displaying thumbnails in your image classification model deployment</li>
				<li>Test your knowledge</li>
			</ul>
			<h1 id="_idParaDest-191"><a id="_idTextAnchor206"/>Technical requirements</h1>
			<p>In this chapter, you will be using both your cloud environment and your local environment for model deployment:</p>
			<ul>
				<li>Ensure that you have completed the setup sections from <a href="B16216_01_Final_VK_ePub.xhtml#_idTextAnchor019"><em class="italic">Chapter 1</em></a>, <em class="italic">Getting Started with fastai</em>, and have a working Gradient instance or Colab setup. </li>
				<li>Ensure that you have completed the steps described in the <em class="italic">Setting up fastai on your local system</em> recipe in <a href="B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 7</em></a><em class="italic">, Deployment and Model Maintenance</em>, to set up fastai on your local system.</li>
			</ul>
			<p>Ensure that you have cloned the repo for the book from <a href="https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook">https://github.com/PacktPublishing/Deep-Learning-with-fastai-Cookbook</a> and have access to the <strong class="source-inline">ch8</strong> folder. This folder contains the code samples described in this chapter.</p>
			<h1 id="_idParaDest-192"><a id="_idTextAnchor207"/>Getting more details about models trained with tabular data</h1>
			<p>In<a id="_idIndexMarker613"/> the <em class="italic">Training a model in fastai with a curated tabular dataset</em> recipe<a id="_idIndexMarker614"/> of <a href="B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083"><em class="italic">Chapter 3</em></a><em class="italic">, Training Models with Tabular Data</em>, you trained a fastai model on a tabular dataset and used accuracy as the metric. In this recipe, you will learn how to get additional metrics <a id="_idIndexMarker615"/>for<a id="_idIndexMarker616"/> this model: <strong class="bold">precision</strong> and <strong class="bold">recall</strong>. Precision is the ratio of true positives divided by true positives plus false positives. Recall is the ratio of true positives divided by true positives plus false negatives. </p>
			<p>These are useful metrics. For example, the model we are training in this recipe is predicting whether an individual's income is over 50,000. If it is critical to avoid false positives – that is, predicting an income over 50,000 when the individual has an income less than that – then we want precision to be as high as possible. This recipe will show you how to add these useful metrics to the training process for a fastai model.</p>
			<h2 id="_idParaDest-193"><a id="_idTextAnchor208"/>Getting ready</h2>
			<p>Confirm that you can open the <strong class="source-inline">training_with_tabular_datasets_metrics.ipynb</strong> notebook in the <strong class="source-inline">ch8</strong> directory of your repo.</p>
			<h2 id="_idParaDest-194"><a id="_idTextAnchor209"/>How to do it…</h2>
			<p>In this recipe, you will be running through the <strong class="source-inline">training_with_tabular_datasets_metrics.ipynb</strong> notebook. Once you have the notebook open in your fastai environment, complete the following steps:</p>
			<ol>
				<li>Run <a id="_idIndexMarker617"/>the cells in the notebook up to<a id="_idIndexMarker618"/> the <strong class="source-inline">Define and train model</strong> cell to import the required libraries, set up your notebook, and prepare the dataset.</li>
				<li>Run the following cell to define and train the model:<p class="source-code">recall_instance = Recall()</p><p class="source-code">precision_instance = Precision()</p><p class="source-code">learn = tabular_learner(dls,layers=[200,100], metrics=[accuracy,recall_instance,precision_instance])</p><p class="source-code">learn.fit_one_cycle(3)</p><p>Here are the key items in this cell:</p><p>a) <strong class="source-inline">recall_instance = Recall()</strong> – defines a recall metric object. Note that you will get an error if you put <strong class="source-inline">Recall</strong> directly in the metrics list for the model. Instead, you need to define a recall metric object, such as <strong class="source-inline">recall_instance</strong>, and include that object in the metrics list. See the fastai documentation (<a href="https://docs.fast.ai/metrics.html#Recall">https://docs.fast.ai/metrics.html#Recall</a>) for more details on this metric. </p><p>b) <strong class="source-inline">precision_instance = Precision()</strong> – defines a precision metric object. You will get an error if you put <strong class="source-inline">Precision</strong> directly in the metrics list, so you need to define the <strong class="source-inline">precision_instance</strong> object first and then include that object in the metrics list for the model.</p><p>c) <strong class="source-inline">metrics=[accuracy,recall_instance,precision_instance]</strong> – specifies that the model will be trained with accuracy, recall, and precision as metrics.</p><p>The output of this cell, as shown in <em class="italic">Figure 8.1</em>, includes accuracy as well as recall <a id="_idIndexMarker619"/>and <a id="_idIndexMarker620"/>precision for each epoch of the training run:</p></li>
			</ol>
			<div>
				<div id="_idContainer251" class="IMG---Figure">
					<img src="image/B16216_8_01.jpg" alt="Figure 8.1 – Training output including recall and precision&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.1 – Training output including recall and precision</p>
			<p>Congratulations! You have trained a model with tabular data and generated recall and precision metrics for the training process.</p>
			<h2 id="_idParaDest-195"><a id="_idTextAnchor210"/>How it works…</h2>
			<p>You may have asked yourself how I knew that you could not include <strong class="source-inline">Recall</strong> and <strong class="source-inline">Precision</strong> directly in the metrics list for the model and that you needed to define objects first and then include those objects in the metrics list. The simple answer is that it was trial and error. More specifically, when I attempted to include <strong class="source-inline">Recall</strong> and <strong class="source-inline">Precision</strong> directly in the metrics list, I got the following error:</p>
			<p class="source-code">TypeError: unsupported operand type(s) for *: 'AccumMetric' and 'int'</p>
			<p>When I searched <a id="_idIndexMarker621"/>for this error, I came across this post in the fastai forum: <a href="https://forums.fast.ai/t/problem-with-f1scoremulti-metric/63721">https://forums.fast.ai/t/problem-with-f1scoremulti-metric/63721</a>. The post explained the reason for the error and that to get around it I needed to define the <strong class="source-inline">Recall</strong> and <strong class="source-inline">Precision</strong> objects first and then include them in the metrics list. </p>
			<p>This experience is an example of both a weakness and a strength of fastai. The weakness is that the documentation for <strong class="source-inline">Precision</strong> and <strong class="source-inline">Recall</strong> is missing an essential detail – you cannot use them directly in the metrics list. The strength is that the fastai forum provides<a id="_idIndexMarker622"/> clear <a id="_idIndexMarker623"/>and accurate resolutions for issues like this and demonstrates the strength of the fastai community. </p>
			<h1 id="_idParaDest-196"><a id="_idTextAnchor211"/>Getting more details about image classification models</h1>
			<p>In<a id="_idIndexMarker624"/> the <em class="italic">Training a classification model with a simple curated vision dataset</em> recipe of <a href="B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 6</em></a><em class="italic">, Training Models with Visual Data</em>, you trained an image classification model using the <strong class="source-inline">CIFAR</strong> curated dataset. The code to train and exercise the model was straightforward because we took advantage of the highest-level structures in fastai. In this recipe, we will revisit this image classification model and explore techniques in fastai to get additional information about the model and its performance, including the following:</p>
			<ul>
				<li>Examining the <strong class="bold">pipeline</strong> that fastai generates to prepare the data</li>
				<li>Getting a chart of the training and validation loss during the training process</li>
				<li>Displaying the images where the model performs worst</li>
				<li>Displaying<a id="_idIndexMarker625"/> the <strong class="bold">confusion matrix</strong> to get a snapshot of where the model is not doing well</li>
				<li>Applying the model to the test set and examining the model's performance on the test set</li>
			</ul>
			<p>In this recipe, we are going to expand the recipe where we trained the <strong class="source-inline">CIFAR</strong> curated dataset. By taking advantage of the additional features of fastai, we will be able to understand our model better.</p>
			<h2 id="_idParaDest-197"><a id="_idTextAnchor212"/>Getting ready</h2>
			<p>Confirm that you c<a id="_idTextAnchor213"/>an open the <strong class="source-inline">training_with_image_datasets_datablock.ipynb</strong> notebook in the <strong class="source-inline">ch8</strong> directory of your repo.</p>
			<h2 id="_idParaDest-198"><a id="_idTextAnchor214"/>How to do it…</h2>
			<p>In this section, you will be running through the <strong class="source-inline">training_with_image_datasets_datablock.ipynb</strong> notebook. Once you have the notebook open in your fastai environment, complete the following steps:</p>
			<ol>
				<li value="1">Update the following cell to ensure that <strong class="source-inline">model_path</strong> points to a writeable directory in your Gradient or Colab instance: <p class="source-code">model_path = '/notebooks/temp'</p></li>
				<li>Run the<a id="_idIndexMarker626"/> cells in the notebook up to the <strong class="source-inline">Define a DataBlock</strong> cell to import the required libraries, set up your notebook, and ingest the <strong class="source-inline">CIFAR</strong> dataset.</li>
				<li>Run the following cell to define a <strong class="source-inline">DataBlock</strong> object. By defining a <strong class="source-inline">DataBlock</strong> object explicitly, we will be able to do additional actions that we couldn't do directly on a <strong class="source-inline">dataloaders</strong> object, such as getting a summary of the pipeline:<p class="source-code">db<a id="_idTextAnchor215"/> = DataBlock(blocks = (ImageBlock, CategoryBlock),</p><p class="source-code">                 get_items=get_image_files,</p><p class="source-code">                 splitter=RandomSplitter(seed=42),</p><p class="source-code">                 get_y=parent_label)</p><p>Here are the key items in this cell:</p><p>a) <strong class="source-inline">blocks = (ImageBlock, CategoryBlock)</strong> – specifies that the input to the model is images (<strong class="source-inline">ImageBlock</strong>) and the target is a categorization of the input images (<strong class="source-inline">CategoryBlock</strong>).</p><p>b) <strong class="source-inline">get_items=get_image_files</strong> – specifies that the <strong class="source-inline">get_image_files</strong> function is called to get the input to the <strong class="source-inline">DataBlock</strong> object.</p><p>c) <strong class="source-inline">splitter=RandomSplitter(seed=42)</strong> – specifies how the validation set is defined from the training set. By default, 20% of the training set is randomly selected to make up the validation set. By specifying a value for <strong class="source-inline">seed</strong>, this call to <strong class="source-inline">RandomSplitter</strong> produces consistent results across multiple runs. See<a id="_idIndexMarker627"/> the documentation for <strong class="source-inline">RandomSplitter</strong> (<a href="https://docs.fast.ai/data.transforms.html#RandomSplitter">https://docs.fast.ai/data.transforms.html#RandomSplitter</a>) for more details.</p><p>d) <strong class="source-inline">get_y=parent_label</strong> – specifies that the labels for the images (that is, the <a id="_idIndexMarker628"/>categories to which the images belong) are defined by the directories where the images are located in the input dataset. For example, on Gradient, cat images in the training set are found in the <strong class="source-inline">/storage/data/cifar10/train/cat</strong> directory.</p></li>
				<li>Run the following cell to define a <strong class="source-inline">dataloaders</strong> object using the <strong class="source-inline">DataBlock</strong> object <strong class="source-inline">db</strong> that you created in the previous cell:<p class="source-code">dls = db.dataloaders(path/'train',bs=32)</p><p>Here are the key items in this cell:</p><p>a) <strong class="source-inline">db.dataloaders</strong> – specifies that the <strong class="source-inline">dataloaders</strong> object is created using the <strong class="source-inline">DataBlock</strong> object <strong class="source-inline">db</strong></p><p>b) <strong class="source-inline">path/'train'</strong> – specifies that the input to this model is only the training subset of the <strong class="source-inline">CIFAR</strong> dataset.</p></li>
				<li>Run the following cell to get a summary of the pipeline:<p class="source-code">db.summary(path/"train")</p><p>Let's look at the key parts of the output of this cell. First, the output shows details about the input dataset, including the source directory, the size of the whole dataset, and the size of the training and validation sets, as shown in the following screenshot: </p><div id="_idContainer252" class="IMG---Figure"><img src="image/B16216_8_02.jpg" alt=""/></div><p class="figure-caption">Figure 8.2 – Summary description of the input dataset</p><p>Next, the output shows the pipeline that fastai applies to a single input sample, including<a id="_idIndexMarker629"/> the source directory of the sample, the image object that is created for the sample, and the label (category) for the sample, as shown in <em class="italic">Figure 8.3</em>:</p><div id="_idContainer253" class="IMG---Figure"><img src="image/B16216_8_03.jpg" alt="Figure 8.3 – Summary description of the pipeline for one image file&#13;&#10;"/></div><p class="figure-caption">Figure 8.3 – Summary description of the pipeline for one image file</p><p>Next, the output shows the pipeline that fastai applies to build a single batch, that is, converting the image objects that are output from the sample pipeline into tensors. As shown in <em class="italic">Figure 8.4</em>, the 32 x 32-pixel image objects are converted to 3 x 32 x 32 tensors, where the first dimension contains color information about the image:</p><div id="_idContainer254" class="IMG---Figure"><img src="image/B16216_8_04.jpg" alt="Figure 8.4 – Summary description of the pipeline applied to a single batch&#13;&#10;"/></div><p class="figure-caption">Figure 8.4 – Summary description of the pipeline applied to a single batch</p><p>Finally, the <a id="_idIndexMarker630"/>output shows the transformations applied to the batches as a whole, as shown in <em class="italic">Figure 8.5</em>:</p><div id="_idContainer255" class="IMG---Figure"><img src="image/B16216_8_05.jpg" alt="Figure 8.5 – Summary description of the pipeline applied to all batches&#13;&#10;"/></div><p class="figure-caption">Figure 8.5 – Summary description of the pipeline applied to all batches</p></li>
				<li>Run the following cell to define a <strong class="source-inline">DataBlock</strong> object for the test set:<p class="source-code">db_test = DataBlock(blocks = (ImageBlock, CategoryBlock),</p><p class="source-code">                 get_items=get_image_files,</p><p class="source-code">                 splitter=RandomSplitter(valid_pct=0.99,seed=42),</p><p class="source-code">                 get_y=parent_label)</p><p>Note that unlike the <strong class="source-inline">DataBlock</strong> object for the training set, we define <strong class="source-inline">db_test</strong> with an explicit value for <strong class="source-inline">valid_pct</strong>. We set this value to 99% because we won't be doing any training of the model when we apply the test set to it, so there is no need to hold back any of the test set for training. We don't set <strong class="source-inline">valid_pct</strong> to <strong class="source-inline">1.0</strong> because that value will produce an error when you apply a summary to <strong class="source-inline">db_test</strong>. </p></li>
				<li>Run the <a id="_idIndexMarker631"/>cells in the notebook up to the <strong class="source-inline">Define and train the model</strong> cell to examine the dataset.</li>
				<li>Run the following cell to define the model with a <strong class="source-inline">cnn_learner</strong> object. Note that because you defined a <strong class="source-inline">dataloaders</strong> object from the <strong class="source-inline">DataBlock</strong> object you get the best of both worlds: the additional features (such as a summary) available only with a <strong class="source-inline">DataBlock</strong> object along with the familiar code pattern for <strong class="source-inline">dataloaders</strong> objects that you have used for most of the recipes in this book:<p class="source-code">learn = cnn_learner(dls, resnet18, </p><p class="source-code">                    loss_func=LabelSmoothingCrossEntropy(), </p><p class="source-code">                    metrics=accuracy)</p></li>
				<li>Run the following cell to train the model:<p class="source-code">learn.fine_tune(2,cbs=ShowGraphCallback())</p><p>Note the <strong class="source-inline">cbs=ShowGraphCallback()</strong> parameter. With this parameter, the output of the training process includes a graph of training and validation loss, as shown in <em class="italic">Figure 8.6</em>:</p><div id="_idContainer256" class="IMG---Figure"><img src="image/B16216_8_06.jpg" alt="Figure 8.6 – Training and validation loss graph&#13;&#10;"/></div><p class="figure-caption">Figure 8.6 – Training and validation loss graph</p><p>This<a id="_idIndexMarker632"/> graph contains the same data as the table of training results that you get by default from the training process, as shown in <em class="italic">Figure 8.7</em>:</p><div id="_idContainer257" class="IMG---Figure"><img src="image/B16216_8_07.jpg" alt="Figure 8.7 – Training and validation loss table&#13;&#10;"/></div><p class="figure-caption">Figure 8.7 – Training and validation loss table</p></li>
				<li>Run the following cell to save the trained model. We update the path for the model temporarily to a directory that is writeable in Gradient so that we can save the model:<p class="source-code">save_path = learn.path</p><p class="source-code">learn.path = Path(model_path)</p><p class="source-code">learn.save('cifar_save_'+modifier)</p><p class="source-code">learn.path = save_path</p><p>Here are the key items in this cell:</p><p>a) <strong class="source-inline">save_path = learn.path</strong> – specifies that the current path for the model<a id="_idIndexMarker633"/> is saved to <strong class="source-inline">save_path</strong>.</p><p>b) <strong class="source-inline">learn.path = Path(model_path)</strong> – specifies that the path for the model is set to a writeable directory.</p><p>c) <strong class="source-inline">learn.save('cifar_save_'+modifier)</strong> – saves the model. We will load the saved model later to exercise the model with the test set.</p><p>d) <strong class="source-inline">learn.path = save_path</strong> – resets the path for the model to its original value.</p></li>
				<li>Run the following cell to confirm the performance of the trained model in terms of accuracy:<p class="source-code">learn.validate()</p><p>The second value in the output should match the accuracy you saw in the final epoch of the training, as shown in <em class="italic">Figure 8.8</em>:</p><div id="_idContainer258" class="IMG---Figure"><img src="image/B16216_8_08.jpg" alt="Figure 8.8 – Output of validate&#13;&#10;"/></div><p class="figure-caption">Figure 8.8 – Output of validate</p></li>
				<li>Run the cells up to the <strong class="source-inline">Examine the top loss examples and confusion matrix</strong> cell.</li>
				<li>Run the following cell to see the samples where the model has the biggest loss:<p class="source-code">interp = ClassificationInterpretation.from_learner(learn)</p><p class="source-code">interp.plot_top_losses(9, figsize=(15,11))</p><p>Here are the key items in this cell:</p><p>a) <strong class="source-inline">interp = ClassificationInterpretation.from_learner(learn)</strong> – specifies<a id="_idIndexMarker634"/> that <strong class="source-inline">interp</strong> is an interpretation object for the <strong class="source-inline">learn</strong> model</p><p>b) <strong class="source-inline">interp.plot_top_losses(9, figsize=(15,11))</strong> – specifies that the nine images with the highest losses should be displayed</p><p>The output shows examples of the images where the model has the biggest loss along with the predicted contents of the image and the actual contents of the image. You can think of these as the images where the model did the worst job predicting what's in the images. <em class="italic">Figure 8.9</em> shows a subset of the output. For example, for the first displayed image, the model predicted the image contained a bird while the image is actually labeled as a cat:</p><div id="_idContainer259" class="IMG---Figure"><img src="image/B16216_8_09.jpg" alt="Figure 8.9 – Examples of images with the most loss&#13;&#10;"/></div><p class="figure-caption">Figure 8.9 – Examples of images with the most loss</p></li>
				<li>Run the following cell to generate a confusion matrix for the trained model:<p class="source-code">interp.plot_confusion_matrix()</p><p>The output of this cell is a confusion matrix that summarizes the performance of the trained model, as shown in <em class="italic">Figure 8.10</em>. A confusion matrix is an <em class="italic">N</em> x <em class="italic">N</em> matrix, where <em class="italic">N</em> is the number of target classes. It compares the actual target class values (the vertical axis) with the predicted values (the horizontal axis). The diagonal of the matrix shows the cases where the model made the correct prediction, while all the entries off the diagonal are cases where the model made an<a id="_idIndexMarker635"/> incorrect prediction. For example, in the confusion matrix shown in <em class="italic">Figure 8.10</em>, in 138 instances the model predicted that an image of a dog was a cat, and in 166 instances it predicted that an image of a cat was a dog:</p><div id="_idContainer260" class="IMG---Figure"><img src="image/B16216_8_010.jpg" alt="Figure 8.10 – Confusion matrix for the trained model&#13;&#10;"/></div><p class="figure-caption">Figure 8.10 – Confusion matrix for the trained model</p></li>
				<li>Now that<a id="_idIndexMarker636"/> you have examined the performance of the model with the training set, let's examine how the model does with the test set. To do this, we will define a new <strong class="source-inline">dataloaders</strong> object using the test set, define a model with this <strong class="source-inline">dataloaders</strong> object, load the saved weights from the trained model, and then do the same steps to evaluate the model performance that we did with the model trained on the training set. To begin, run the following cell to create a new <strong class="source-inline">dataloaders</strong> object <strong class="source-inline">dls_test</strong> that is defined with the test dataset:<p class="source-code">dls_test = db_test.dataloaders(path/'test',bs=32)</p></li>
				<li>Run the following cell to define a new model object, <strong class="source-inline">learn_test</strong>, that is based on the <strong class="source-inline">dataloaders</strong> object you created in the previous step. Note that the model definition is identical to the model you defined for the training set in <em class="italic">Step 8</em> except that it uses the <strong class="source-inline">dataloaders</strong> object <strong class="source-inline">dls_test</strong> that was defined with the test dataset in the previous step:<p class="source-code">learn_test = cnn_learner(dls_test, resnet18, </p><p class="source-code">                    loss_func=LabelSmoothingCrossEntropy(), </p><p class="source-code">                    metrics=accuracy)</p></li>
				<li>Run the<a id="_idIndexMarker637"/> following cell to load the saved weights from the model trained with the<a id="_idTextAnchor216"/> training set:<p class="source-code">learn_test.path = Path(model_path)</p><p class="source-code">learn_test.load('cifar_save_'+modifier)</p><p>Here are the key items in this cell:</p><p>a) <strong class="source-inline">learn_test.path = Path(model_path)</strong> – specifies that the path for the <strong class="source-inline">learn_test</strong> model is changed to the directory where the model weights were saved in <em class="italic">Step 10</em></p><p>b) <strong class="source-inline">learn_test.load('cifar_save_'+modifier')</strong> – specifies that the <strong class="source-inline">learn_test</strong> model gets loaded with the weights from the model trained with the training set</p><p>Now we are all set to exercise the model with the test set. </p></li>
				<li>Run the following cell to see the overall accuracy of the model on the test set:<p class="source-code">learn_test.validate()</p><p>The second value in the output is the accuracy of the model on the test set, as shown in <em class="italic">Figure 8.11</em>:</p><div id="_idContainer261" class="IMG---Figure"><img src="image/B16216_8_011.jpg" alt="Figure 8.11 – Output of validate on the test set&#13;&#10;"/></div><p class="figure-caption">Figure 8.11 – Output of validate on the test set</p></li>
				<li>Run the following cell to see the images in the test set where the model has the biggest loss:<p class="source-code">interp_test = ClassificationInterpretation.from_learner(learn_test)</p><p class="source-code">interp_test.plot_top_losses(9, figsize=(15,11))</p><p>The output shows examples of the images in the test set where there was the biggest loss along with the predicted contents of the image and the actual contents of<a id="_idIndexMarker638"/> the image in the test set. <em class="italic">Figure 8.12</em> shows a subset of the output:</p><div id="_idContainer262" class="IMG---Figure"><img src="image/B16216_8_012.jpg" alt="Figure 8.12 – Sample images from the test set where the model performed worst&#13;&#10;"/></div><p class="figure-caption">Figure 8.12 – Sample images from the test set where the model performed worst</p></li>
				<li>Run the following cell to get the confusion matrix for the model applied to the test set:<p class="source-code">interp_test.plot_confusion_matrix()</p><p>The output of this cell is a confusion matrix that summarizes the performance of the model on the test set, as shown in <em class="italic">Figure 8.13</em>. Note that the numbers in this confusion matrix are smaller than the numbers in the confusion matrix for the model <a id="_idIndexMarker639"/>applied to the training set: </p></li>
			</ol>
			<div>
				<div id="_idContainer263" class="IMG---Figure">
					<img src="image/B16216_8_013.jpg" alt="Figure 8.13 – Confusion matrix for the model applied to the test set&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.13 – Confusion matrix for the model applied to the test set</p>
			<p>Congratulations! You have worked through an image classification model to see the benefit of additional information that fastai can provide. You have also learned how to apply the model to the entire test set and examine the model's performance on the test set.</p>
			<h2 id="_idParaDest-199"><a id="_idTextAnchor217"/>How it works…</h2>
			<p>It's instructive to compare the model we created in the recipe in this section with the model we created in the <em class="italic">Training a classification model with a simple curated vision dataset</em> recipe of <a href="B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 6</em></a><em class="italic">, Training Models with Visual Data</em>. </p>
			<p>Here is the definition of the <strong class="source-inline">dataloaders</strong> object from the <a href="B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 6</em></a><em class="italic">, Training Models with Visual Data</em>, recipe:</p>
			<p class="source-code">dls = ImageDataLoaders.from_folder(path, train='train', valid='test')</p>
			<p>And here is the definition of the <strong class="source-inline">dataloaders</strong> object from this recipe. Unlike the previous <strong class="source-inline">dataloaders</strong> definition, this definition makes use of the <strong class="source-inline">DataBlock</strong> object <strong class="source-inline">db</strong>:</p>
			<p class="source-code">dls = db.dataloaders(path/'train',bs=32)</p>
			<p>Here is the<a id="_idIndexMarker640"/> definition of the <strong class="source-inline">DataBlock</strong> object <strong class="source-inline">db</strong>:</p>
			<p class="source-code">db = DataBlock(blocks = (ImageBlock, CategoryBlock),</p>
			<p class="source-code">                 get_items=get_image_files,</p>
			<p class="source-code">                 splitter=RandomSplitter(seed=42),</p>
			<p class="source-code">                 get_y=parent_label)</p>
			<p>What is the benefit of defining the <strong class="source-inline">dataloaders</strong> object using the <strong class="source-inline">DataBlock</strong> object? </p>
			<p>First, by starting with a <strong class="source-inline">DataBlock</strong> object, you have more control over the details of how the dataset is set up. You can explicitly define the function that defines the input dataset (the function assigned to <strong class="source-inline">get_items</strong>) as well as the function that defines the label (the function assigned to <strong class="source-inline">get_y</strong>). You may recall that we took advantage of this flexibility in the <em class="italic">Training a multi-image classification model with a curated vision dataset</em> recipe of <a href="B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 6</em></a><em class="italic">, Training Models with Visual Data</em>. In that recipe, we needed to ensure that the input dataset excluded images that had no annotation. By using a <strong class="source-inline">DataBlock</strong> object in that recipe, we were able to define a custom function to assign to <strong class="source-inline">get_items</strong> that excluded images with no annotation.</p>
			<p>Second, we can take advantage of some additional functions in fastai if we have a <strong class="source-inline">DataBlock</strong> object. In this recipe, we were able to apply the <strong class="source-inline">summary()</strong> function to the <strong class="source-inline">DataBlock</strong> object to see the pipeline that fastai applied to the input dataset. The <strong class="source-inline">summary()</strong> function cannot be applied to a <strong class="source-inline">dataloaders</strong> object, so we would have missed out on the additional details about the data pipeline if we had not defined a <strong class="source-inline">DataBlock</strong> object. </p>
			<p>If a <strong class="source-inline">DataBlock</strong> object is so useful, why didn't we use one in the <em class="italic">Training a classification model with a simple curated vision dataset</em> recipe of <a href="B16216_06_Final_VK_ePub.xhtml#_idTextAnchor152"><em class="italic">Chapter 6</em></a><em class="italic">, Training Models with Visual Data</em>? We only used a <strong class="source-inline">dataloaders</strong> object in that recipe (instead of starting with a <strong class="source-inline">DataBlock</strong> object) because that recipe was relatively simple – we didn't need the additional flexibility of a <strong class="source-inline">DataBlock</strong> object. Throughout this book, we have stuck with the highest-level APIs for fastai whenever we could, including in that recipe. Simplicity is a key benefit of fastai, so if it's possible to go with the <a id="_idIndexMarker641"/>highest-level APIs (including using <strong class="source-inline">dataloaders</strong> directly), it makes sense to keep it simple and stick with the highest-level APIs.</p>
			<h1 id="_idParaDest-200"><a id="_idTextAnchor218"/>Training a model with augmented data</h1>
			<p>In the<a id="_idIndexMarker642"/> previous recipe, you learned about some <a id="_idIndexMarker643"/>additional facilities that fastai provides to keep track of your model and you learned how to apply the test set to the model trained on the training set. In this recipe, you will learn how to combine these techniques with another technique that fastai makes it easy to incorporate in your model training: <strong class="bold">data augmentation</strong>. With<a id="_idIndexMarker644"/> data augmentation, you can expand your training dataset to include variations on the original training samples and, potentially, improve the performance of the trained model. </p>
			<p><em class="italic">Figure 8.14</em> shows some results of augmentation applied to an image from the <strong class="source-inline">CIFAR</strong> dataset:</p>
			<div>
				<div id="_idContainer264" class="IMG---Figure">
					<img src="image/B16216_8_014.jpg" alt="Figure 8.14 – Augmentation applied to an image&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.14 – Augmentation applied to an image</p>
			<p>You can see in these examples that the augmentations applied to the image include flipping the image on the vertical axis, rotating the image, and adjusting the brightness of the image. </p>
			<p>As in the previous recipe, in this recipe, we are going to work with an image classification model trained on the <strong class="source-inline">CIFAR</strong> curated dataset, but in this recipe, we will experiment with augmenting the images in the dataset.</p>
			<h2 id="_idParaDest-201"><a id="_idTextAnchor219"/>Getting ready</h2>
			<p>Confirm that you can open the <strong class="source-inline">training_with_image_datasets_datablock_augmented.ipynb</strong> notebook<a id="_idIndexMarker645"/> in<a id="_idIndexMarker646"/> the <strong class="source-inline">ch8</strong> directory of your repo.</p>
			<h2 id="_idParaDest-202"><a id="_idTextAnchor220"/>How to do it…</h2>
			<p>In this recipe, you will be running through the <strong class="source-inline">training_with_image_datasets_datablock_augmented.ipynb</strong> notebook. Here is a high-level summary of what you will do in this recipe:</p>
			<ol>
				<li value="1">Train the model with non-augmented data.</li>
				<li>Train the model with augmented data.</li>
				<li>Exercise the model trained with non-augmented data with the test set.</li>
				<li>Exercise the model trained with augmented data with the test set. </li>
			</ol>
			<p>Once you have the notebook open in your fastai environment, complete the following steps:</p>
			<ol>
				<li value="1">Update the following cell to ensure that <strong class="source-inline">model_path</strong> points to a writeable directory in your Gradient or Colab instance: <p class="source-code">model_path = '/notebooks/temp'</p></li>
				<li>Run the cells in the notebook up to the <strong class="source-inline">Try augmenting the training set</strong> cell to import the required libraries, set up your notebook, and train a model on the non-augmented <strong class="source-inline">CIFAR</strong> dataset.</li>
				<li>Run the following cell to create a new <strong class="source-inline">DataBlock</strong> object <strong class="source-inline">db2</strong> that incorporates augmentation and a <strong class="source-inline">dataloaders</strong> object <strong class="source-inline">dls2</strong> that is defined with this new <strong class="source-inline">DataBlock</strong> object:<p class="source-code">db2 = db.new(batch_tfms=aug_transforms())</p><p class="source-code">dls2 = db2.dataloaders(path/'train',bs=32)</p><p>Here are the key items in this cell:</p><p>a) <strong class="source-inline">db2 = db.new(batch_tfms=aug_transforms())</strong> – defines a new <strong class="source-inline">DataBlock</strong> object <strong class="source-inline">db2</strong> that incorporates the default augmentation defined by <strong class="source-inline">aug_transforms()</strong>. See <a id="_idIndexMarker647"/>the fastai<a id="_idIndexMarker648"/> documentation for details on <strong class="source-inline">aug_transforms()</strong>: <a href="https://docs.fast.ai/vision.augment.html#aug_transforms">https://docs.fast.ai/vision.augment.html#aug_transforms</a>.</p><p>b) <strong class="source-inline">dls2 = db2.dataloaders(path/'train',bs=32)</strong> – defines a new <strong class="source-inline">dataloaders</strong> object <strong class="source-inline">dls2</strong> based on the <strong class="source-inline">DataBlock</strong> object <strong class="source-inline">db2</strong>. Now, <strong class="source-inline">dls2</strong> is defined with the training subset of the input dataset.</p></li>
				<li>Run the following cell to get a summary of the pipeline:<p class="source-code">db2.summary(path/"train")</p><p>The output of this cell gives us details about the pipeline that is applied to the dataset. First, the output shows details about the input dataset, including the source directory, the size of the whole dataset, and the size of the training and validation sets, as shown in <em class="italic">Figure 8.15</em>:</p><div id="_idContainer265" class="IMG---Figure"><img src="image/B16216_8_015.jpg" alt="Figure 8.15 – Summary description of the input dataset&#13;&#10;"/></div><p class="figure-caption">Figure 8.15 – Summary description of the input dataset</p><p>Next, the output shows the pipeline that fastai applies to a single input sample, including the source directory of the sample, the image object that is created for the sample, and the label (category) for the sample, as shown in <em class="italic">Figure 8.16</em>:</p><div id="_idContainer266" class="IMG---Figure"><img src="image/B16216_8_016.jpg" alt="Figure 8.16 – Summary description of the pipeline for one image file&#13;&#10;"/></div><p class="figure-caption">Figure 8.16 – Summary description of the pipeline for one image file</p><p>Next, the<a id="_idIndexMarker649"/> output shows the pipeline <a id="_idIndexMarker650"/>that fastai applies to build a single batch, that is, converting the image objects that are output from the sample pipeline into tensors. As shown in <em class="italic">Figure 8.17</em>, the 32 x 32-pixel image objects are converted to 3 x 32 x 32 tensors, where the first dimension contains color information about the image:</p><div id="_idContainer267" class="IMG---Figure"><img src="image/B16216_8_017.jpg" alt="Figure 8.17 – Summary description of the pipeline applied to a single batch&#13;&#10;"/></div><p class="figure-caption">Figure 8.17 – Summary description of the pipeline applied to a single batch</p><p>These first three portions of the output are identical to the same portions of the output of <strong class="source-inline">summary()</strong> from the <em class="italic">Getting more details about image classification models</em> recipe. The final portion, however, is different and describes the transformations that are applied to images in the augmentation process, as shown in <em class="italic">Figure 8.18</em>:</p><div id="_idContainer268" class="IMG---Figure"><img src="image/B16216_8_018.jpg" alt="Figure 8.18 – Description of the pipeline (including augmentation transformations) &#13;&#10;"/></div><p class="figure-caption">Figure 8.18 – Description of the pipeline (including augmentation transformations) applied to all batches</p></li>
				<li>Run <a id="_idIndexMarker651"/>the<a id="_idIndexMarker652"/> following cell to see examples of the augmentation transformations being applied to an image in the dataset:<p class="source-code">dls2.train.show_batch(unique=True, max_n=8, nrows=2)</p><p>The <strong class="source-inline">unique=True</strong> argument specifies that we want to see examples of augmentations applied to a single image. </p><p>An example of the output of this cell is shown in <em class="italic">Figure 8.19</em>: note the variations in the augmented images, including being flipped on the vertical axis, having varying brightness, and having varying degrees of vertical space taken up by the truck:</p><div id="_idContainer269" class="IMG---Figure"><img src="image/B16216_8_019.jpg" alt="Figure 8.19 – Augmented versions of an image&#13;&#10;"/></div><p class="figure-caption">Figure 8.19 – Augmented versions of an image</p></li>
				<li>Run<a id="_idIndexMarker653"/> the<a id="_idIndexMarker654"/> following cell to define a model to be trained with the augmented dataset:<p class="source-code">learn2 = cnn_learner(dls2, resnet18, </p><p class="source-code">                    loss_func=LabelSmoothingCrossEntropy(), </p><p class="source-code">                    metrics=accuracy)</p></li>
				<li>Run the following cell to train the model with the augmented dataset:<p class="source-code">learn2.fine_tune(2)</p><p>The output of this cell shows the training loss, validation loss, and validation accuracy, as shown in <em class="italic">Figure 8.20</em>:</p><div id="_idContainer270" class="IMG---Figure"><img src="image/B16216_8_020.jpg" alt="Figure 8.20 – Output of training the model with the augmented dataset&#13;&#10;"/></div><p class="figure-caption">Figure 8.20 – Output of training the model with the augmented dataset</p></li>
				<li>Run the following cell to save the model that was trained on the augmented dataset:<p class="source-code">save_path = learn2.path</p><p class="source-code">learn2.path = Path(model_path)</p><p class="source-code">learn2.save('cifar_augmented_save_'+modifier)</p><p class="source-code">learn2.path = save_path</p></li>
				<li>Run <a id="_idIndexMarker655"/>the <a id="_idIndexMarker656"/>cells up to the <strong class="source-inline">Examine the performance of the model trained on the augmented dataset on the test set</strong> cell to get an idea of the performance of the model trained on the non-augmented dataset making predictions on the test dataset.</li>
				<li>Run the following cell to define a <strong class="source-inline">dataloaders</strong> object <strong class="source-inline">dls_test</strong> associated with the test set:<p class="source-code">dls_test = db_test.dataloaders(path/'test',bs=32)</p></li>
				<li>Run the following cell to define the <strong class="source-inline">learn_augment_test</strong> model to be exercised on the test dataset:<p class="source-code">learn_augment_test = cnn_learner(dls_test, resnet18, </p><p class="source-code">                    loss_func=LabelSmoothingCrossEntropy(), </p><p class="source-code">                    metrics=accuracy)</p></li>
				<li>Run the following cell to load the <strong class="source-inline">learn_augment_test</strong> model with the weights saved from training the model with the augmented dataset:<p class="source-code">learn_augment_test.path = Path(model_path)</p><p class="source-code">learn_augment_test.load('cifar_augmented_save_'+modifier)</p><p>Now we have a <strong class="source-inline">learn_augment_test</strong> learner object that has the weights from<a id="_idIndexMarker657"/> training <a id="_idIndexMarker658"/>with the augmented dataset and is associated with the test dataset.</p></li>
				<li>Run the following cell to get the overall accuracy of the <strong class="source-inline">learn_augment_test</strong> model exercised on the test set:<p class="source-code">learn_augment_test.validate()</p><p>The output of this cell shows the accuracy of the model on the test set, as shown in <em class="italic">Figure 8.21</em>:</p><div id="_idContainer271" class="IMG---Figure"><img src="image/B16216_8_021.jpg" alt="Figure 8.21 – Accuracy of the model trained on the augmented dataset applied to the test set&#13;&#10;"/></div><p class="figure-caption">Figure 8.21 – Accuracy of the model trained on the augmented dataset applied to the test set</p></li>
				<li>Run the following cell to get examples of the images from the test dataset where the model trained on the augmented dataset has the biggest loss:<p class="source-code">interp_augment_test = ClassificationInterpretation.from_learner(learn_augment_test)</p><p class="source-code">interp_augment_test.plot_top_losses(9, figsize=(15,11))</p><p>The output of this cell shows the images from the test set where the model trained on the augmented dataset has the biggest loss. <em class="italic">Figure 8.22</em> shows examples<a id="_idIndexMarker659"/> from <a id="_idIndexMarker660"/> this output:</p><div id="_idContainer272" class="IMG---Figure"><img src="image/B16216_8_022.jpg" alt="Figure 8.22 – Examples of images where the model trained on &#13;&#10;the augmented dataset has the biggest losses&#13;&#10;"/></div><p class="figure-caption">Figure 8.22 – Examples of images where the model trained on the augmented dataset has the biggest losses</p></li>
				<li>Run the following cell to see the confusion matrix for the model trained on the augmented dataset applied to the test set:<p class="source-code">interp_augment_test.plot_confusion_matrix()</p><p>The output of this cell is the confusion matrix shown in <em class="italic">Figure 8.23</em>:</p></li>
			</ol>
			<div>
				<div id="_idContainer273" class="IMG---Figure">
					<img src="image/B16216_8_023.jpg" alt="Figure 8.23 – Confusion matrix for the model trained on the augmented dataset applied to the test set&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.23 – Confusion matrix for the model trained on the augmented dataset applied to the test set</p>
			<p>Congratulations! You<a id="_idIndexMarker661"/> have trained a fastai image<a id="_idIndexMarker662"/> classification model using an augmented dataset and exercised the trained model on the test set.</p>
			<h2 id="_idParaDest-203"><a id="_idTextAnchor221"/>How it works…</h2>
			<p>In this recipe, we went through the process of training an image classification model on an augmented dataset. The notebook we worked through in this recipe also included training an image classification on a non-augmented dataset. Let's compare the performance between the models to see whether it was worthwhile to use the augmented dataset.</p>
			<p>If we compare the training results between the two models, as shown in <em class="italic">Figure 8.24</em>, the performance of the model trained on the non-augmented dataset seems to be better:</p>
			<div>
				<div id="_idContainer274" class="IMG---Figure">
					<img src="image/B16216_8_024.jpg" alt="Figure 8.24 – Comparison of training results when training with and without augmented data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.24 – Comparison of training results when training with and without augmented data</p>
			<p>Now let's <a id="_idIndexMarker663"/>compare the performance of the two models<a id="_idIndexMarker664"/> on the test dataset. <em class="italic">Figure 8.25</em> shows the output of <strong class="source-inline">validate()</strong> for the model trained on the non-augmented and augmented datasets, applied to the test set. Again, the model trained on the non-augmented dataset seems to be better:</p>
			<div>
				<div id="_idContainer275" class="IMG---Figure">
					<img src="image/B16216_8_025.jpg" alt="Figure 8.25 – Comparison of validate() results when training with and without augmented data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.25 – Comparison of validate() results when training with and without augmented data</p>
			<p><em class="italic">Figure 8.26</em> shows the confusion matrixes for the model trained on the non-augmented and augmented datasets, applied to the test set:</p>
			<div>
				<div id="_idContainer276" class="IMG---Figure">
					<img src="image/B16216_8_026.jpg" alt="Figure 8.26 – Comparison of confusion matrix when training with and without augmented data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.26 – Comparison of confusion matrix when training with and without augmented data</p>
			<p>Overall, the<a id="_idIndexMarker665"/> model trained with the augmented <a id="_idIndexMarker666"/>dataset does not seem to have better performance on the test set than the model trained with the non-augmented data. This may be disappointing, but it's OK. </p>
			<p>The purpose of this recipe was to demonstrate to you how to take advantage of the augmented data facilities in fastai, not to do a thorough analysis to get the best possible results with augmented data. Not every application will benefit from augmentation, and we only tried the default augmentation approach. fastai offers a wide range of augmentation options for image models, as described in the fastai documentation: <a href="https://docs.fast.ai/vision.augment.html">https://docs.fast.ai/vision.augment.html</a>. It is possible that this particular dataset had characteristics (such as relatively low resolution) that made augmentation less effective. It's also possible that the set of augmentations included in the default fastai augmentation approach were not ideal for this dataset, and a custom set of augmentations could have produced better results.</p>
			<p>How does <strong class="source-inline">aug_transforms()</strong>, the function applied in this recipe, augment the images in the training set? We can get a clue by comparing the pipeline summary for the model trained with the non-augmented training set with the model trained with augmented data. <em class="italic">Figure 8.27</em> shows the final section of the output of <strong class="source-inline">summary()</strong> for the non-augmented <strong class="source-inline">DataBlock</strong> object:</p>
			<div>
				<div id="_idContainer277" class="IMG---Figure">
					<img src="image/B16216_8_027.jpg" alt="Figure 8.27 – Output of summary for the non-augmented DataBlock object&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.27 – Output of summary for the non-augmented DataBlock object</p>
			<p><em class="italic">Figure 8.28</em> shows <a id="_idIndexMarker667"/>the final section of the output<a id="_idIndexMarker668"/> of <strong class="source-inline">summary()</strong> for the augmented <strong class="source-inline">DataBlock</strong> object:</p>
			<div>
				<div id="_idContainer278" class="IMG---Figure">
					<img src="image/B16216_8_028.jpg" alt="Figure 8.28 – Output of summary for the non-augmented DataBlock object&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.28 – Output of summary for the non-augmented DataBlock object</p>
			<p>By comparing the section of the summary output for these two models, you can understand what transformations are being applied on the augmented dataset, as shown in <em class="italic">Figure 8.29</em>:</p>
			<div>
				<div id="_idContainer279" class="IMG---Figure">
					<img src="image/B16216_8_029.jpg" alt="Figure 8.29 – Transformations applied in aug_transforms()&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.29 – Transformations applied in aug_transforms()</p>
			<p>You have now examined some of the differences between training an image classification<a id="_idIndexMarker669"/> model with an augmented dataset and<a id="_idIndexMarker670"/> training with a non-augmented dataset.</p>
			<h1 id="_idParaDest-204"><a id="_idTextAnchor222"/>Using callbacks to get the most out of your training cycle</h1>
			<p>So far in <a id="_idIndexMarker671"/>this book, we have kicked off the training process for a fastai model by applying <strong class="source-inline">fit_one_cycle</strong> or <strong class="source-inline">fine_tune</strong> to the learner object and have then just let the training run through the specified number of epochs. For many of the models we have trained in this book, this approach has been good enough, particularly for models where each epoch takes a long time and we only train for one or two epochs. But what about models where we want to train the model for 10 or more epochs? If we simply let the training process go to the end, we face the problem shown in the training results shown in <em class="italic">Figure 8.30</em>. In <em class="italic">Figure 8.30</em>, we see the result of training a tabular model for 10 epochs with <strong class="source-inline">metric</strong> set to <strong class="source-inline">accuracy</strong>:</p>
			<div>
				<div id="_idContainer280" class="IMG---Figure">
					<img src="image/B16216_8_030.jpg" alt="Figure 8.30 – Results from a 10-epoch run training a model with tabular data&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.30 – Results from a 10-epoch run training a model with tabular data</p>
			<p>The goal of this training process is to get a model with the best accuracy. With this goal in mind, there are two problems with the results shown in <em class="italic">Figure 8.30</em>:</p>
			<ol>
				<li value="1">The best accuracy is in epoch 5, but the model we get at the end of the training process has the accuracy from the last epoch. The output of <strong class="source-inline">validate()</strong> for this model, shown in <em class="italic">Figure 8.31</em>, shows that the accuracy for the model is not the best<a id="_idIndexMarker672"/> accuracy from the training run:<div id="_idContainer281" class="IMG---Figure"><img src="image/B16216_8_031.jpg" alt="Figure 8.31 – Output of validate() for the model trained with the 10-epoch run&#13;&#10;"/></div><p class="figure-caption">Figure 8.31 – Output of validate() for the model trained with the 10-epoch run</p><p>Note that if the accuracy was still steadily improving up to the end of the training run, the model would likely not be overfitting yet, though you would want to validate this by exercising the trained model with the test set.</p></li>
				<li>The training run keeps going after the accuracy is no longer improving, so training capacity is being wasted. For a run like this one, where every epoch completes in less than a minute, it isn't really a problem. However, consider the impact of wasted training cycles if each epoch took an hour and you were running on a paid Gradient instance. In that case, you could end up wasting many dollars on training cycles that didn't improve the performance of the model.</li>
			</ol>
			<p>Luckily, fastai includes a solution to <a id="_idIndexMarker673"/>both of these problems: <strong class="bold">callbacks</strong>. You can use callbacks to control the training process and automatically take actions during training. In this recipe, you will learn how to specify callbacks in fastai that stop the training process when the model isn't getting any better and save the best model from the training run. You will revisit the model you trained in the <em class="italic">Training a model in fastai with a curated tabular dataset</em> recipe of <a href="B16216_03_Final_VK_ePub.xhtml#_idTextAnchor083"><em class="italic">Chapter 3</em></a><em class="italic">, Training Models with Tabular Data</em>, but this time you will control the training process for the model using fastai callbacks.</p>
			<h2 id="_idParaDest-205"><a id="_idTextAnchor223"/>Getting ready</h2>
			<p>Confirm that you can open the <strong class="source-inline">training_with_tabular_datasets_callbacks.ipynb</strong> notebook in the <strong class="source-inline">ch8</strong> directory of your repo.</p>
			<h2 id="_idParaDest-206"><a id="_idTextAnchor224"/>How to do it…</h2>
			<p>In this<a id="_idIndexMarker674"/> recipe, you will be running through the <strong class="source-inline">training_with_tabular_datasets_callbacks.ipynb</strong> notebook. In this recipe, you will train three variations of the model: </p>
			<ul>
				<li>Training with no callbacks</li>
				<li>Training with a single callback to end the training process when it has stopped improving</li>
				<li>Training with two callbacks: one to end the training process when it has stopped improving, and one to save the best model</li>
			</ul>
			<p>Once you have the notebook open in your fastai environment, complete the following steps:</p>
			<ol>
				<li value="1">Update the following cell to ensure that <strong class="source-inline">model_path</strong> points to a writeable directory in your Gradient or Colab instance: <p class="source-code">model_path = '/notebooks/temp'</p></li>
				<li>Run the cells in the notebook up to the <strong class="source-inline">Define and train the model with no callbacks</strong> cell to import the required libraries, set up your notebook, and define a <strong class="source-inline">dataloaders</strong> object for the <strong class="source-inline">ADULT_SAMPLE</strong> dataset. <p>By calling <strong class="source-inline">set_seed()</strong> this way and defining a <strong class="source-inline">dataloaders</strong> object before each training run, we can get consistent training results across multiple training runs. You will see the significance of having consistent training results when we compare the results for training with and without callbacks.</p></li>
				<li>Run the following cell to define and train a model with no callbacks:<p class="source-code">%%time</p><p class="source-code">set_seed(dls,x=42)</p><p class="source-code">learn = tabular_learner(dls,layers=[200,100], metrics=accuracy)</p><p class="source-code">learn.fit_one_cycle(10)</p><p>The call<a id="_idIndexMarker675"/> to <strong class="source-inline">set_seed()</strong> specifies that the random seeds related to the model are set to <strong class="source-inline">42</strong> (an arbitrary value) and that results are reproducible, which is exactly what we want.</p><p>The output of this cell shows the training loss, validation loss, and accuracy by epoch, as well as the overall time taken for the training run, as shown in <em class="italic">Figure 8.32</em>. Note that the training run goes for all 10 epochs, even though the accuracy stops improving after epoch 2: </p><div id="_idContainer282" class="IMG---Figure"><img src="image/B16216_8_032.jpg" alt="Figure 8.32 – Results of training a model with no callbacks&#13;&#10;"/></div><p class="figure-caption">Figure 8.32 – Results of training a model with no callbacks</p></li>
				<li>Run the <a id="_idIndexMarker676"/>following cell to get the accuracy for the trained model:<p class="source-code">learn.validate()</p><p>The output of this cell is shown in <em class="italic">Figure 8.33</em>. The accuracy for the model is the accuracy achieved in epoch 9, the final epoch of the training run:</p><div id="_idContainer283" class="IMG---Figure"><img src="image/B16216_8_033.jpg" alt="Figure 8.33 – Output of validate() for a model trained with no callbacks&#13;&#10;"/></div><p class="figure-caption">Figure 8.33 – Output of validate() for a model trained with no callbacks</p></li>
				<li>Run the following cell to define and train a model with one callback – early stopping when the model's accuracy no longer improves:<p class="source-code">%%time</p><p class="source-code">set_seed(dls,x=42)</p><p class="source-code">learn_es = tabular_learner(dls,layers=[200,100], metrics=accuracy)</p><p class="source-code">learn_es.fit_one_cycle(10,cbs=EarlyStoppingCallback(monitor='accuracy', min_delta=0.01, patience=3))</p><p>The <strong class="source-inline">fit</strong> statement for this model includes the definition of an <strong class="source-inline">EarlyStoppingCallback</strong> callback. Here are the arguments used to define the callback:</p><p>a) <strong class="source-inline">monitor='accuracy'</strong> – specifies that the callback is tracking the value of the <a id="_idIndexMarker677"/>accuracy metric. When the accuracy metric stops improving for the specified period, the callback will be triggered.</p><p>b) <strong class="source-inline">min_delta=0.01</strong> – specifies that the callback pays attention to changes in accuracy between epochs that are at least 0.01. That is, if the change in accuracy between epochs is less than 0.01, the callback ignores the change.</p><p>c) <strong class="source-inline">patience=3</strong> – specifies that the callback is triggered when the accuracy metric stops improving for 3 epochs.</p><p>The output of this cell shows the training loss, validation loss, and accuracy by epoch, as well as the overall time taken for the training run, as shown in <em class="italic">Figure 8.34</em>:</p><div id="_idContainer284" class="IMG---Figure"><img src="image/B16216_8_034.jpg" alt="Figure 8.34 – Results of training a model with an early stopping callback&#13;&#10;"/></div><p class="figure-caption">Figure 8.34 – Results of training a model with an early stopping callback</p><p>Note that now the training stops after epoch 5. You can see that the metric being tracked by the callback, <strong class="source-inline">accuracy</strong>, stops improving after epoch 2. Once 3 more epochs have passed (the value set for the <strong class="source-inline">patience</strong> parameter for the callback), the training process stops early, at epoch 5, even though the <strong class="source-inline">fit_one_cycle()</strong> call specifies a run of 10 epochs. So, with an early stopping callback, we save 4 epochs and about 25% of the training time (51 seconds with the early stopping callback versus 68 seconds with no callbacks).</p></li>
				<li>Run the <a id="_idIndexMarker678"/>following cell to get the accuracy for the trained model:<p class="source-code">learn_es.validate()</p><p>The output of this cell is shown in <em class="italic">Figure 8.35</em>. The accuracy for the model is the accuracy achieved in epoch 5, the final epoch of the training run:</p><div id="_idContainer285" class="IMG---Figure"><img src="image/B16216_8_035.jpg" alt="Figure 8.35 – Output of validate() for a model trained with an early stopping callback&#13;&#10;"/></div><p class="figure-caption">Figure 8.35 – Output of validate() for a model trained with an early stopping callback</p></li>
				<li>With an early stopping callback, we reduce the training time compared to a model with no callbacks, but the accuracy of the trained model is lower than the best accuracy achieved during the training run. Let's train another model that includes a callback to save the best model. This callback will ensure that the accuracy of the trained model is the best result we get from the training run. To do this, start by running the following cell to define and train a model with two callbacks – early stopping when the model's accuracy no longer improves, and saving the best model:<p class="source-code">%%time</p><p class="source-code">set_seed(dls,x=42)</p><p class="source-code">learn_es_sm = tabular_learner(dls,layers=[200,100], metrics=accuracy)</p><p class="source-code">keep_path = learn_es_sm.path</p><p class="source-code"># set the model path to a writeable directory. If you don't do this, the code will produce an error on Gradient</p><p class="source-code">#learn_es_sm.path = Path('/notebooks/temp/models')</p><p class="source-code">learn_es_sm.path = Path(model_path)</p><p class="source-code">learn_es_sm.fit_one_cycle(10,cbs=[EarlyStoppingCallback(monitor='accuracy', min_delta=0.01, patience=3),SaveModelCallback(monitor='accuracy', min_delta=0.01)])</p><p class="source-code"># reset the model path</p><p class="source-code">learn_es_sm.path = keep_path</p><p>In addition<a id="_idIndexMarker679"/> to the definition of an <strong class="source-inline">EarlyStoppingCallback</strong> callback like the one specified in <em class="italic">Step 5</em>, the <strong class="source-inline">fit</strong> statement for this model also includes a <strong class="source-inline">SaveModelCallback</strong> callback. Here the arguments used to define this callback are as follows:</p><p>a) <strong class="source-inline">monitor='accuracy'</strong> – specifies that the <strong class="source-inline">SaveModelCallback</strong> callback is tracking the value of the accuracy metric. The model will be saved for epochs where the accuracy hits a new high-water mark.</p><p>b) <strong class="source-inline">min_delta=0.01</strong> – specifies that the callback pays attention to changes in accuracy between epochs that are at least 0.01.</p><p>Note that this cell also includes statements to adjust the model path to a directory that is writeable in Gradient. If you don't change the model's path to a writeable directory, you will get an error in Gradient when you run this cell. Also, note that you may see the <strong class="bold">Saved filed doesn't contain an optimizer state</strong> warning message when you run this cell – you don't need to worry about this message for the purposes of this recipe.</p><p>The output of this cell shows the training loss, validation loss, and accuracy by epoch, as well as the overall time taken for the training run, as shown in <em class="italic">Figure 8.36</em>. Note<a id="_idIndexMarker680"/> that the training stops after epoch 5 thanks to the early stopping callback: </p><div id="_idContainer286" class="IMG---Figure"><img src="image/B16216_8_036.jpg" alt="Figure 8.36 – Results of training a model with an early stopping callback and a model saving callback&#13;&#10;"/></div><p class="figure-caption">Figure 8.36 – Results of training a model with an early stopping callback and a model saving callback</p></li>
				<li>Run the following cell to get the accuracy for the trained model:<p class="source-code">learn_es.validate()</p><p>The output of this cell is shown in <em class="italic">Figure 8.37</em>. The accuracy for the model is the accuracy achieved in epoch 2, the best accuracy during the training run:</p></li>
			</ol>
			<div>
				<div id="_idContainer287" class="IMG---Figure">
					<img src="image/B16216_8_037.jpg" alt="Figure 8.37 – Output of validate() for a model trained with early stopping and model saving callbacks&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.37 – Output of validate() for a model trained with early stopping and model saving callbacks</p>
			<p>With both callbacks, we avoid running epochs that won't improve the performance of the model and we end up with a trained model with the best performance out of the epochs of the training run.</p>
			<p>Congratulations! You have successfully applied callbacks in fastai to optimize the training process so that you get the most out of the training cycle for your models.</p>
			<h2 id="_idParaDest-207"><a id="_idTextAnchor225"/>How it works…</h2>
			<p>This recipe demonstrated how you can use fastai callbacks to control the training process so you get the best results from the system capacity and time that you apply to training. There<a id="_idIndexMarker681"/> are a few additional details about fastai callbacks that are worth reviewing. In this section of the recipe, we will dig a bit deeper into how callbacks are used in fastai.</p>
			<h3>The set_seed() function is not the default function for fastai</h3>
			<p>In order to <a id="_idIndexMarker682"/>get clear comparisons between the performance of the model with and without callbacks, we needed to control random differences between training runs. That is, we want to train the model multiple times and get consistent loss and accuracy for epochs between training runs. For example, if the accuracy reported in epoch 1 is <strong class="source-inline">0.826271</strong> for the first training run, we want to get exactly the same accuracy in epoch 1 for each subsequent training run. By ensuring consistent performance between training runs, we can do <em class="italic">apples-to-apples</em> comparisons between runs, focusing on the impact of the callbacks rather than random differences between runs.</p>
			<p>In this recipe, we used the <strong class="source-inline">set_seed()</strong> function to control random differences between training <a id="_idIndexMarker683"/>runs. You may have noticed that while fastai documentation includes a <strong class="source-inline">set_seed()</strong> function (<a href="https://docs.fast.ai/torch_core.html#set_seed">https://docs.fast.ai/torch_core.html#set_seed</a>), we don't use that function in the recipe. Instead, we use the following function, which is adapted from code shared in this forum discussion – <a href="https://github.com/fastai/fastai/issues/2832">https://github.com/fastai/fastai/issues/2832</a>:</p>
			<p class="source-code">def set_seed(dls,x=42): </p>
			<p class="source-code">    random.seed(x)</p>
			<p class="source-code">    dls.rng.seed(x) </p>
			<p class="source-code">    np.random.seed(x)</p>
			<p class="source-code">    torch.manual_seed(x)</p>
			<p class="source-code">    torch.backends.cudnn.deterministic = True</p>
			<p class="source-code">    torch.backends.cudnn.benchmark = False</p>
			<p class="source-code">    if torch.cuda.is_available():</p>
			<p class="source-code">        torch.cuda.manual_seed_all(x)</p>
			<p>Why use this custom <strong class="source-inline">set_seed()</strong> function instead of the default <strong class="source-inline">set_seed()</strong> function documented by fastai? The simple reason is that the default <strong class="source-inline">set_seed()</strong> function did not<a id="_idIndexMarker684"/> work as expected – I didn't get consistent training results with it. With the <strong class="source-inline">set_seed()</strong> function listed previously, on the other hand, I was able to get consistent training results. </p>
			<h3>The Callbacks section of summary() doesn't include the callbacks defined in the recipe</h3>
			<p>You may notice that the end of the <strong class="source-inline">training_with_tabular_datasets_callbacks.ipynb</strong> notebook includes calls to <a id="_idIndexMarker685"/>the <strong class="source-inline">summary()</strong> function for the learner objects for the three models that you trained in the recipe. You might expect the <strong class="source-inline">Callbacks</strong> section of the output of the <strong class="source-inline">summary()</strong> function for the two models that include callbacks, <strong class="source-inline">learn_es</strong> and <strong class="source-inline">learn_es_sm</strong>, would list the early stopping and model saving callbacks, but that is not the case. <em class="italic">Figure 8.38</em> shows the <strong class="source-inline">Callbacks</strong> section for the two models that have explicitly defined callbacks, and this section is identical to the <strong class="source-inline">Callbacks</strong> section for the model with no callbacks:</p>
			<div>
				<div id="_idContainer288" class="IMG---Figure">
					<img src="image/B16216_8_038.jpg" alt="Figure 8.38 – Callbacks section of summary() output&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.38 – Callbacks section of summary() output</p>
			<p>Why doesn't the <strong class="source-inline">Callbacks</strong> section of <strong class="source-inline">summary()</strong> output include the callbacks defined with the model? The <strong class="source-inline">summary()</strong> function just lists the callbacks defined by fastai itself, not <a id="_idIndexMarker686"/>the callbacks that you define.</p>
			<h3>Is there anything else you can do with callbacks in fastai?</h3>
			<p>In this recipe, we<a id="_idIndexMarker687"/> used callbacks to ensure that the training cycle was as efficient as possible, but that's just scratching the surface of what you can do with callbacks in fastai. The fastai framework supports a broad range of callbacks to control aspects of the training process. In fact, in this book you have come across several kinds of fastai callbacks:</p>
			<ul>
				<li><strong class="bold">Tracking callbacks</strong> – the<a id="_idIndexMarker688"/> early stopping and model saving callbacks that we used in this recipe are examples of tracking callbacks. This category of callbacks is documented here: <a href="https://docs.fast.ai/callback.tracker.html">https://docs.fast.ai/callback.tracker.html</a>. </li>
				<li><strong class="bold">Progress and logging callbacks</strong> – you saw an example of a callback in this category<a id="_idIndexMarker689"/> in the <em class="italic">Getting more details about image classification models</em> recipe, where you used a <strong class="source-inline">ShowGraphCallback</strong> callback to display a graph of training and validation loss. Progress and logging callbacks are documented here: <a href="https://docs.fast.ai/callback.progress.html">https://docs.fast.ai/callback.progress.html</a>.</li>
				<li><strong class="bold">Callbacks for mixed-precision training</strong> – in the <em class="italic">Training a deep learning language model with a curated text dataset</em> recipe of <a href="B16216_04_Final_VK_ePub.xhtml#_idTextAnchor109"><em class="italic">Chapter 4</em></a><em class="italic">, Training Models with Text Data</em>, you<a id="_idIndexMarker690"/> used <strong class="source-inline">to_fp16()</strong> to specify mixed-precision training for the language model you trained in that section. Callbacks for mixed-precision training are documented here: <a href="https://docs.fast.ai/callback.fp16.html">https://docs.fast.ai/callback.fp16.html</a>.</li>
			</ul>
			<p>The callbacks that you have used so far by working through the recipes in this book show you some of the power and flexibility that you can get by using callbacks with fastai.</p>
			<h1 id="_idParaDest-208"><a id="_idTextAnchor226"/>Making your model deployments available to others</h1>
			<p>In <a href="B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 7</em></a><em class="italic">, Deployment and Model Maintenance</em>, you deployed a couple of fastai models. To<a id="_idIndexMarker691"/> get a prediction, you pointed your browser to <strong class="source-inline">localhost:5000</strong> and that opened up <strong class="source-inline">home.html</strong> where you set your scoring parameters, requested a prediction, and then got a prediction back in <strong class="source-inline">show-prediction.html</strong>. All this happened on your local system. Through the web deployments done in <a href="B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 7</em></a><em class="italic">, Deployment and Model Maintenance</em>, you can only get to the deployment on your local system because <strong class="source-inline">localhost</strong> is only accessible on your local system. What if you wanted to share these deployments with friends to allow them to try out your models on their own computers? </p>
			<p>The simplest way to do this is using a tool called <strong class="bold">ngrok</strong> that lets you share <strong class="source-inline">localhost</strong> on your computer with people working on other computers. In this recipe, we will go through steps that show you how to use ngrok to make your deployments available to others.</p>
			<h2 id="_idParaDest-209"><a id="_idTextAnchor227"/>Getting ready</h2>
			<p>Follow the<a id="_idIndexMarker692"/> instructions at <a href="https://dashboard.ngrok.com/get-started">https://dashboard.ngrok.com/get-started</a> to set up a free ngrok account and to set up ngrok on your local system. Note the directory where you install ngrok – you will need it in the recipe.</p>
			<h2 id="_idParaDest-210"><a id="_idTextAnchor228"/>How to do it…</h2>
			<p>With the help of ngrok, you can get a URL you can share with your friends so they can try out your fastai model deployments. This recipe will show you how to share your deployment of the tabular model.</p>
			<p>To share the deployment you created in the <em class="italic">Deploying a fastai model trained on a tabular dataset</em> recipe of <a href="B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 7</em></a><em class="italic">, Deployment and Model Maintenance</em>, go through the following steps: </p>
			<ol>
				<li value="1">Make the directory where you installed ngrok your current directory.</li>
				<li>Enter the following command on the command line/terminal:<p class="source-code"><strong class="bold">.\ngrok http 5000</strong></p><p>This command produces output as shown in <em class="italic">Figure 8.39</em>. Copy the <strong class="source-inline">https</strong> <strong class="source-inline">Forwarding</strong> URL:</p><div id="_idContainer289" class="IMG---Figure"><img src="image/B16216_8_039.jpg" alt="Figure 8.39 – Output of ngrok&#13;&#10;"/></div><p class="figure-caption">Figure 8.39 – Output of ngrok</p></li>
				<li>Start the<a id="_idIndexMarker693"/> deployment of the tabular model by making <strong class="source-inline">deploy_tabular</strong> your current directory and entering the following command:<p class="source-code"><strong class="bold">python web_flask_deploy.py</strong></p></li>
				<li>On a computer other than your local system, point the browser at the <strong class="source-inline">https</strong> <strong class="source-inline">Forwarding</strong> URL that you copied in <em class="italic">Step 2</em>. You should see <strong class="source-inline">home.html</strong>, as shown in<em class="italic"> Figure 8.40</em>:<div id="_idContainer290" class="IMG---Figure"><img src="image/B16216_8_040.jpg" alt="Figure 8.40 – home.html&#13;&#10;"/></div><p class="figure-caption">Figure 8.40 – home.html</p></li>
				<li>In <strong class="source-inline">home.html</strong>, select <strong class="screen-inline">Get prediction</strong> and <a id="_idIndexMarker694"/>confirm that you see a prediction in <strong class="source-inline">show-prediction.html</strong>, as shown in <em class="italic">Figure 8.41</em>:</li>
			</ol>
			<div>
				<div id="_idContainer291" class="IMG---Figure">
					<img src="image/B16216_8_041.jpg" alt="Figure 8.41 – show-prediction.html&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.41 – show-prediction.html</p>
			<p>Congratulations! You have successfully shared one of your fastai model deployments so that it is<a id="_idIndexMarker695"/> available to users on other systems with whom you share the ngrok forwarding URL.</p>
			<h2 id="_idParaDest-211"><a id="_idTextAnchor229"/>How it works…</h2>
			<p>When you run ngrok, you create a secure tunnel to <strong class="source-inline">localhost</strong> on your local system. When you share the forwarding URL that is returned by ngrok, the people who receive the URL can point their browser to that URL to see what is being served at <strong class="source-inline">localhost</strong> on your local system. </p>
			<p>You specify the port that the ngrok tunnel points to when you start ngrok. For example, when you entered the following command in this recipe, you specified that the ngrok forwarding URL points to <strong class="source-inline">localhost:5000</strong>: </p>
			<p class="source-code">.\ngrok http 5000</p>
			<p>Now you have some background on how ngrok makes it easy for you to share your model<a id="_idTextAnchor230"/> deployment with users on other systems. </p>
			<h1 id="_idParaDest-212"><a id="_idTextAnchor231"/>Displaying thumbnails in your image classification model deployment</h1>
			<p>When<a id="_idIndexMarker696"/> you created <a id="_idIndexMarker697"/>a deployment for the image classification model in the <em class="italic">Deploying a fastai model trained on an image dataset</em> recipe of <a href="B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 7</em></a><em class="italic">, Deployment and Model Maintenance</em>, there was something useful missing: a thumbnail of the image that you selected in <strong class="source-inline">home.html</strong>. In this recipe, you will see how to update <strong class="source-inline">home.html</strong> to display a thumbnail of the image that the user selects.</p>
			<h2 id="_idParaDest-213"><a id="_idTextAnchor232"/>Getting ready</h2>
			<p>Ensure that you have followed the steps in the <em class="italic">Deploying a fastai model trained on an image dataset</em> recipe of <a href="B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 7</em></a><em class="italic">, Deployment and Model Maintenance</em>, to do the deployment of the image classification model. </p>
			<h2 id="_idParaDest-214"><a id="_idTextAnchor233"/>How to do it…</h2>
			<p>In this recipe, you will be making updates to <strong class="source-inline">home.html</strong> in your image classification model <a id="_idIndexMarker698"/>deployment <a id="_idIndexMarker699"/>so that a thumbnail of the image you selected gets displayed. </p>
			<p>To make these updates, go through the following steps: </p>
			<ol>
				<li value="1">Make the directory for the image classification deployment, <strong class="source-inline">deploy_image</strong>, your current directory.</li>
				<li>Make the <strong class="source-inline">templates</strong> subdirectory your current directory by running the following command:<p class="source-code"><strong class="bold">cd templates</strong></p></li>
				<li>Open <strong class="source-inline">home.html</strong> in an editor. I like to use Notepad++ (see <a href="https://notepad-plus-plus.org/">https://notepad-plus-plus.org/</a>), but you can use the editor of your choice.</li>
				<li>Update the control for the file dialog to specify an <strong class="source-inline">onchange</strong> action of calling the <strong class="source-inline">getFile()</strong> JavaScript function. After the update, the control for the file dialog should look like this:<p class="source-code">  &lt;input type="file" </p><p class="source-code">       id="image_field" name="image_field"</p><p class="source-code">       accept="image/png, image/jpeg"</p><p class="source-code">     onchange="getFile();"&gt;</p></li>
				<li>Define <a id="_idIndexMarker700"/>a new <a id="_idIndexMarker701"/>JavaScript function in <strong class="source-inline">home.html</strong> called <strong class="source-inline">getFile()</strong>:<p class="source-code">  &lt;script&gt;</p><p class="source-code">  function getFile() {</p><p class="source-code">    file_list = document.getElementById("image_field").files;</p><p class="source-code">    img_f = document.createElement("img");</p><p class="source-code">    img_f.setAttribute("id","displayImage");</p><p class="source-code">    img_f.setAttribute("style","width:50px");</p><p class="source-code">    img_f.setAttribute("alt","image to display here");</p><p class="source-code">    document.body.appendChild(img_f);</p><p class="source-code">    document.getElementById("displayImage").src = \</p><p class="source-code">URL.createObjectURL(file_list[0]);</p><p class="source-code">  }</p><p class="source-code">  &lt;/script&gt;</p><p>Here are the key items in this function definition:</p><p>a) <strong class="source-inline">file_list = document.getElementById("image_field").files;</strong> – specifies that <strong class="source-inline">file_list</strong> contains the list of files associated with the <strong class="source-inline">image_field</strong> file selector</p><p>b) <strong class="source-inline">img_f = document.createElement("img");</strong> – defines a new image element called <strong class="source-inline">img_f</strong> on the page</p><p>c) <strong class="source-inline">img_f.setAttribute("id","displayImage");</strong> – assigns the <strong class="source-inline">displayImage</strong> ID to the new image element</p><p>d) <strong class="source-inline">document.body.appendChild(img_f);</strong> – adds the new image element to the bottom of the page</p><p>e) <strong class="source-inline">document.getElementById("displayImage").src = URL.createObjectURL(file_list[0]);</strong> – specifies that the file selected in the file dialog is displayed in the new image element</p></li>
				<li>Save<a id="_idIndexMarker702"/> your<a id="_idIndexMarker703"/> changes to <strong class="source-inline">home.html</strong> and make <strong class="source-inline">deploy_image</strong> your current directory by running the following command:<p class="source-code"><strong class="bold">cd ..</strong></p></li>
				<li>Start the Flask server by running the following command:<p class="source-code"><strong class="bold">python web_flask_deploy_image_model.py</strong></p></li>
				<li>Open <strong class="source-inline">localhost:5000</strong> in your browser to display <strong class="source-inline">home.html</strong>.</li>
				<li>Select the <strong class="bold">Choose File</strong> button and then select an image file from the <strong class="source-inline">test_images</strong> directory. If everything works, you should see a thumbnail of the image you selected at the bottom of the page, as shown in <em class="italic">Figure 8.42</em>:</li>
			</ol>
			<div>
				<div id="_idContainer292" class="IMG---Figure">
					<img src="image/B16216_8_042.jpg" alt="Figure 8.42 – home.html with a thumbnail of the selected image displayed&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.42 – home.html with a thumbnail of the selected image displayed</p>
			<p>Congratulations! You have updated the deployment for the image classification model so that you see a thumbnail when you select an image.</p>
			<h2 id="_idParaDest-215"><a id="_idTextAnchor234"/>How it works…</h2>
			<p>In this recipe, you have seen a small example of dynamically creating an element in an HTML page. Unlike all the other elements in <strong class="source-inline">home.html</strong>, the image element where we show the thumbnail is not there when <strong class="source-inline">home.html</strong> is first loaded. The image element only gets created when an image file has been selected and the <strong class="source-inline">getFile()</strong> function gets called. Why didn't we just have the image element hardcoded to be there when the file <a id="_idIndexMarker704"/>is<a id="_idIndexMarker705"/> first loaded, like the other controls?</p>
			<p>If we had hardcoded the image element, then when you loaded <strong class="source-inline">home.html</strong>, before an image file had been selected, you would see an ugly missing-image graphic where the thumbnail should be, as shown in <em class="italic">Figure 8.43</em>: </p>
			<div>
				<div id="_idContainer293" class="IMG---Figure">
					<img src="image/B16216_8_043.jpg" alt="Figure 8.43 – home.html with a hardcoded image element&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.43 – home.html with a hardcoded image element</p>
			<p>By dynamically creating the image element only after an image had been selected, we can avoid the messy missing-image graphic.</p>
			<p>You may remember that in the <em class="italic">Maintaining your fastai model</em> recipe of <a href="B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 7</em></a><em class="italic">, Deployment and Model Maintenance</em>, I mentioned that in a professional deployment you would not have to manually adjust the controls in <strong class="source-inline">home.html</strong> when the dataset schema got new values for categorical columns or brand-new columns. You could use the technique described in this recipe, dynamically creating controls, for most of the controls in <strong class="source-inline">home.html</strong> to make the deployment easier to adapt to schema changes.</p>
			<h1 id="_idParaDest-216"><a id="_idTextAnchor235"/>Test your knowledge</h1>
			<p>In this chapter, we have reviewed a broad range of topics, from taking full advantage of the information that fastai provides about models to making your web deployments available to users outside of your local system. In this section, you will get the opportunity to exercise some of the concepts you learned about in this chapter.</p>
			<h2 id="_idParaDest-217"><a id="_idTextAnchor236"/>Explore the value of repeatable results</h2>
			<p>In the <em class="italic">Using callbacks to get the most out of your training cycle</em> recipe, you made a call to<a id="_idIndexMarker706"/> the <strong class="source-inline">set_seed()</strong> function prior to training each of the models. In that recipe, I stated that these calls were necessary to ensure repeatable results for multiple training cycles. Test out this assertion yourself by following these steps:</p>
			<ol>
				<li value="1">First, make a copy of the <strong class="source-inline">training_with_tabular_datasets_callbacks.ipynb</strong> notebook.</li>
				<li>Update your new notebook by commenting out the first call to <strong class="source-inline">set_seed()</strong> and rerun the whole notebook. What differences do you see in the output of <strong class="source-inline">fit_one_cycle()</strong> between the model with no callbacks and the model with an early stopping callback? What about differences in the output of <strong class="source-inline">fit_one_cycle()</strong> between the model with an early stopping callback and the model with both an early stopping and a model saving callback?</li>
				<li>Update your new notebook again by commenting out the second call to <strong class="source-inline">set_seed()</strong> and rerun the whole notebook. Now what differences do you see in the output of <strong class="source-inline">fit_one_cycle()</strong> between the model with no callbacks and the model with an early stopping callback? What about differences in the output of <strong class="source-inline">fit_one_cycle()</strong> between the model with an early stopping callback and the model with both an early stopping and a model saving callback?</li>
				<li>Finally, update your notebook again by commenting out the final call to <strong class="source-inline">set_seed()</strong> and rerun the whole notebook again. Compare the results of the models again. Has anything changed?</li>
			</ol>
			<p>Congratulations! Hopefully, by following these steps, you have been able to prove to yourself the value of repeatable results. When you are comparing different variations of a model and you want to ensure you are getting an <em class="italic">apples-to-apples</em> comparison between variations, it can be very valuable to use the facilities in fastai to control the random<a id="_idIndexMarker707"/> initialization of aspects of the model so that you are guaranteed consistent results from multiple training runs.</p>
			<h2 id="_idParaDest-218"><a id="_idTextAnchor237"/>Displaying multiple thumbnails in your image classification model deployment</h2>
			<p>In <a id="_idIndexMarker708"/>the <em class="italic">Displaying thumbnails in your image classification model deployment</em> recipe, you learned <a id="_idIndexMarker709"/>how to enhance the image classification deployment from the <em class="italic">Deploying a fastai model trained on an image dataset</em> recipe of <a href="B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 7</em></a><em class="italic">, Deployment and Model Maintenance</em>, by showing a thumbnail of the image selected for classification. You may recall that in the <em class="italic">Test your knowledge</em> section of <a href="B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 7</em></a><em class="italic">, Deployment and Model Maintenance</em>, you went through the exercise of enhancing the image classification model deployment by allowing the user to select multiple images for classification. What if you want to combine these two enhancements by allowing the user to select multiple images for classification and showing thumbnails of each selected image? Go through the following steps to explore how you would do this:</p>
			<ol>
				<li value="1">Ensure you have completed the exercise in the <em class="italic">Test your knowledge</em> section of <a href="B16216_07_Final_VK_ePub.xhtml#_idTextAnchor178"><em class="italic">Chapter 7</em></a><em class="italic">, Deployment and Model Maintenance</em>, to create a deployment of the image classification model that allows the user to select multiple images to be classified. You will be updating the code you completed in that section so that it shows thumbnails of the selected images.</li>
				<li>Make a copy, called <strong class="source-inline">deploy_image_multi_test</strong>, of the <strong class="source-inline">deploy_image_test</strong> directory where you created the multi-image classification deployment. To do this, make the directory that contains <strong class="source-inline">deploy_image_test</strong> your current directory and run the following command:<p class="source-code"><strong class="bold">cp -r deploy_image_test deploy_image_multi_test</strong></p></li>
				<li>Make <strong class="source-inline">deploy_image_multi_test/templates</strong> your current directory. You will be making updates to the <strong class="source-inline">home.html</strong> file in this directory.</li>
				<li>In <strong class="source-inline">home.html</strong>, update the control for the file dialog to specify an <strong class="source-inline">onchange</strong> action <a id="_idIndexMarker710"/>of <a id="_idIndexMarker711"/>calling the <strong class="source-inline">getFile()</strong> JavaScript function. After the update, the control for the file dialog should look like this:<p class="source-code">&lt;input type="file" multiple</p><p class="source-code">       id="image_field" name="image_field"</p><p class="source-code">       accept="image/png, image/jpeg"</p><p class="source-code">     onchange="getFile();"&gt;</p></li>
				<li>Define a new JavaScript function in <strong class="source-inline">home.html</strong> called <strong class="source-inline">getFile()</strong>. This function will be a generalization of the <strong class="source-inline">getFile()</strong> function you defined in the <em class="italic">Displaying thumbnails in your image classification model deployment</em> recipe:<p class="source-code">function getFile() {</p><p class="source-code">  img_f = [];</p><p class="source-code">  var i = 0;</p><p class="source-code">  var di_string = "displayImage"</p><p class="source-code">  file_list = \</p><p class="source-code">document.getElementById("image_field").files;</p><p class="source-code">  for (file_item of file_list) {</p><p class="source-code">    img_f[i] = document.createElement("img");</p><p class="source-code">    var di_1 = di_string.concat(i)</p><p class="source-code">    img_f[i].setAttribute("id",di_1);</p><p class="source-code">    img_f[i].setAttribute("style","width:50px");</p><p class="source-code">    img_f[i].setAttribute("alt","image to display here");</p><p class="source-code">    document.body.appendChild(img_f[i]);</p><p class="source-code">    document.getElementById(di_1).src =\</p><p class="source-code">URL.createObjectURL(file_item);</p><p class="source-code">    i =  i+1</p><p class="source-code">  }</p><p class="source-code">  }</p><p>Here <a id="_idIndexMarker712"/>are<a id="_idIndexMarker713"/> the key items in this function definition:</p><p>a) <strong class="source-inline">file_list = document.getElementById("image_field").files;</strong> – specifies that <strong class="source-inline">file_list</strong> contains the list of files associated with the <strong class="source-inline">image_field</strong> file selector.</p><p>b) <strong class="source-inline">var di_string = "displayImage"</strong> – defines the <strong class="source-inline">di_string</strong> string that will be used as the prefix of the IDs of the image elements that will be added to the page.</p><p>c) <strong class="source-inline">for (file_item of file_list)</strong> – specifies that the <strong class="source-inline">for</strong> loop iterates through the items in <strong class="source-inline">file_list</strong>. For each item in <strong class="source-inline">file_list</strong>, an image element will be created to display the image associated with the item. </p><p>d) <strong class="source-inline">img_f[i] = document.createElement("img");</strong> – defines a new <strong class="source-inline">img_f[i]</strong>image element on the page.</p><p>e) <strong class="source-inline">var di_1 = di_string.concat(i)</strong> – defines a <strong class="source-inline">di_1</strong> string using the <strong class="source-inline">dl_string</strong> prefix and the <strong class="source-inline">i</strong> index. For example, the first time through the loop, the value of <strong class="source-inline">di_1</strong> will be <strong class="source-inline">displayImage1</strong>.</p><p>f) <strong class="source-inline">img_f[i].setAttribute("id",di_1);</strong> – assigns the <strong class="source-inline">di_1</strong> ID to the <strong class="source-inline">img_f[i]</strong> image element.</p><p>g) <strong class="source-inline">document.body.appendChild(img_f[i]);</strong> – adds the <strong class="source-inline">img_f[i]</strong> image element to the bottom of the page.</p><p>h) <strong class="source-inline">document.getElementById(di_1).src = URL.createObjectURL(file_item);</strong> – specifies that the image file associated with <strong class="source-inline">file_item</strong> is displayed in the <strong class="source-inline">img_f[i]</strong> image element. </p><p>With<a id="_idIndexMarker714"/> these<a id="_idIndexMarker715"/> changes, thumbnails for the files that the user selects in the file dialog will be displayed at the bottom of <strong class="source-inline">home.html</strong>.</p></li>
				<li>Now test whether everything works. Save your changes to <strong class="source-inline">home.html</strong>, make <strong class="source-inline">deploy_image_multi_test</strong> your current directory, and start the Flask server by running the following command:<p class="source-code"><strong class="bold">python web_flask_deploy_image_model.py</strong></p></li>
				<li>Open <strong class="source-inline">localhost:5000</strong> in your browser to display <strong class="source-inline">home.html</strong>.</li>
				<li>Select the <strong class="bold">Choose Files</strong> button and then select image files from the <strong class="source-inline">test_images</strong> directory. If everything works, you should see thumbnails of each of the images you selected at the bottom of the page, as shown in <em class="italic">Figure 8.44</em>:</li>
			</ol>
			<div>
				<div id="_idContainer294" class="IMG---Figure">
					<img src="image/B16216_8_044.jpg" alt="Figure 8.44 – home.html showing thumbnails for multiple selected images&#13;&#10;"/>
				</div>
			</div>
			<p class="figure-caption">Figure 8.44 – home.html showing thumbnails for multiple selected images</p>
			<p>Congratulations! You have combined two enhancements to the image classification deployment<a id="_idIndexMarker716"/> to <a id="_idIndexMarker717"/>allow your users to select multiple images for the model to classify and see thumbnails for the images they selected.</p>
			<h1 id="_idParaDest-219"><a id="_idTextAnchor238"/>Conclusion and additional resources on fastai</h1>
			<p>In this book, you <a id="_idIndexMarker718"/>have explored a broad range of the capabilities of the fastai framework. By adapting the recipes in this book, you should be able to apply fastai to create deep learning models to make predictions on a wide variety of datasets. You will also be able to deploy your models in simple web applications. </p>
			<p>There are many more capabilities in fastai beyond those covered in this book. Here are some additional fastai resources that you can use to learn more about the platform:</p>
			<ul>
				<li>To dig deeper into fastai, you can check out the online documentation for the framework (https://docs.fast.ai/). </li>
				<li>If you want a comprehensive guide to fastai, I highly recommend the outstanding book from Jeremy Howard (the originator of fastai) and Sylvain Gugger <em class="italic">Deep Learning for Coders with Fastai and PyTorch</em> (<a href="https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527">https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527</a>). </li>
				<li>Jeremy Howard's YouTube channel (<a href="https://www.youtube.com/user/howardjeremyp">https://www.youtube.com/user/howardjeremyp</a>) is another excellent source of information about fastai, including videos of Howard's deep learning course built on fastai, <em class="italic">Practical Deep Learning for Coders</em> (<a href="https://course.fast.ai/">https://course.fast.ai/</a>). </li>
				<li>When you <a id="_idIndexMarker719"/>are ready to go even deeper, Zachary Mueller's <em class="italic">Walk with fastai</em> site (<a href="https://walkwithfastai.com/">https://walkwithfastai.com/</a>) is a fantastic resource that consolidates many insights from the fastai forum (<a href="https://forums.fast.ai/">https://forums.fast.ai/</a>) along with Mueller's own encyclopedic understanding of the platform.</li>
			</ul>
			<p>Thank you for taking the time to read this book and following the recipes in it. I hope that you have found this book useful, and I encourage you to apply what you have learned to do great things by using fastai to harness the power of deep learning.</p>
		</div>
	</body></html>