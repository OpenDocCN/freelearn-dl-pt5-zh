- en: 2\. Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Overview
  prefs: []
  type: TYPE_NORMAL
- en: This chapter starts with an introduction to biological neurons; we see how an
    artificial neural network is inspired by biological neural networks. We will examine
    the structure and inner workings of a simple single-layer neuron called a perceptron
    and learn how to implement it in TensorFlow. We will move on to building multilayer
    neural networks to solve more complex multiclass classification tasks and discuss
    the practical considerations of designing a neural network. As we build deep neural
    networks, we will move on to Keras to build modular and easy-to-customize neural
    network models in Python. By the end of this chapter, you'll be adept at building
    neural networks to solve complex problems.
  prefs: []
  type: TYPE_NORMAL
- en: Introduction
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In the previous chapter, we learned how to implement basic mathematical concepts
    such as quadratic equations, linear algebra, and matrix multiplication in TensorFlow.
    Now that we have learned the basics, let's dive into **Artificial Neural Networks**
    (**ANNs**), which are central to artificial intelligence and deep learning.
  prefs: []
  type: TYPE_NORMAL
- en: 'Deep learning is a subset of machine learning. In supervised learning, we often
    use traditional machine learning techniques, such as support vector machines or
    tree-based models, where features are explicitly engineered by humans. However,
    in deep learning, the model explores and identifies the important features of
    a labeled dataset without human intervention. ANNs, inspired by biological neurons,
    have a layered representation, which helps them learn labels incrementally—from
    the minute details to the complex ones. Consider the example of image recognition:
    in a given image, an ANN would just as easily identify basic details such as light
    and dark areas as it would identify more complex structures such as shapes. Though
    neural network techniques are tremendously successful at tasks such as identifying
    objects in images, how they do so is a black box, as the features are learned
    implicitly. Deep learning techniques have turned out to be powerful at tackling
    very complex problems, such as speech/image recognition, and hence are used across
    industry in building self-driving cars, Google Now, and many more applications.'
  prefs: []
  type: TYPE_NORMAL
- en: Now that we know the importance of deep learning techniques, we will take a
    pragmatic step-by-step approach to understanding a mix of theory and practical
    considerations in building deep-learning-based solutions. We will start with the
    smallest component of a neural network, which is an artificial neuron, also referred
    to as a perceptron, and incrementally increase the complexity to explore **Multi-Layer
    Perceptrons** (**MLPs**) and advanced models such as **Recurrent Neural Networks**
    (**RNNs**) and **Convolutional Neural Networks** (**CNNs**).
  prefs: []
  type: TYPE_NORMAL
- en: Neural Networks and the Structure of Perceptrons
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: A neuron is a basic building block of the human nervous system, which relays
    electric signals across the body. The human brain consists of billions of interconnected
    biological neurons, and they are constantly communicating with each other by sending
    minute electrical binary signals by turning themselves on or off. The general
    meaning of a neural network is a network of interconnected neurons. In the current
    context, we are referring to ANNs, which are actually modeled on a biological
    neural network. The term artificial intelligence is derived from the fact that
    natural intelligence exists in the human brain (or any brain for that matter),
    and we humans are trying to simulate this natural intelligence artificially. Though
    ANNs are inspired by biological neurons, some of the advanced neural network architectures,
    such as CNNs and RNNs, do not actually mimic the behavior of a biological neuron.
    However, for ease of understanding, we will begin by drawing an analogy between
    the biological neuron and an artificial neuron (perceptron).
  prefs: []
  type: TYPE_NORMAL
- en: 'A simplified version of a biological neuron is represented in *Figure 2.1*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.1: Biological neuron'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_01.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.1: Biological neuron'
  prefs: []
  type: TYPE_NORMAL
- en: 'This is a highly simplified representation. There are three main components:'
  prefs: []
  type: TYPE_NORMAL
- en: The dendrites, which receive the input signals
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The cell body, where the signal is processed in some form
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: The tail-like axon, through which the neuron transfers the signal out to the
    next neuron
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'A perceptron can also be represented in a similar way, although it is not a
    physical entity but a mathematical model. *Figure 2.2* shows a high-level representation
    of an artificial neuron:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.2: Representation of an artificial neuron'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_02.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.2: Representation of an artificial neuron'
  prefs: []
  type: TYPE_NORMAL
- en: 'In an artificial neuron, as in a biological one, there is an input signal.
    The central node conflates all the signals and fires the output signal if it is
    above a certain threshold. A more detailed representation of a perceptron is shown
    in *Figure 2.3*. Each component of this perceptron is explained in the sections
    that follow:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.3: Representation of a perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_03.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.3: Representation of a perceptron'
  prefs: []
  type: TYPE_NORMAL
- en: 'A perceptron has the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Input layer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Weights
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Net input function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Activation function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Let's look at these components and their TensorFlow implementations in detail
    by considering an `OR` table dataset.
  prefs: []
  type: TYPE_NORMAL
- en: Input Layer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Each example of input data is fed through the input layer. Referring to the
    representation shown in *Figure 2.3*, depending on the size of the input example,
    the number of nodes will vary from *x*1 to *x*m. The input data can be structured
    data (such as a CSV file) or unstructured data, such as an image. These inputs,
    *x*1 to *x*m, are called features (`m` refers to the number of features). Let's
    illustrate this with an example.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say the data is in the form of a table as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.4: Sample input and output data – OR table'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_04.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.4: Sample input and output data – OR table'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here, the inputs to the neuron are the columns *x*1 and *x*2, which correspond
    to one row. At this point, it may be difficult to comprehend, but for now, accept
    it that the data is fed one row at a time in an iterative manner during training.
    We will represent the input data and the true labels (output `y`) with the TensorFlow
    `Variable` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: Weights
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Weights are associated with each neuron, and the input features dictate how
    much influence each of the input features should have in computing the next node.
    Each neuron will be connected to all the input features. In the example, since
    there were two inputs (*x*1 and *x*2) and the input layer is connected to one
    neuron, there will be two weights associated with it: *w*1 and *w*2\. A weight
    is a real number; it can be positive or negative and is mathematically represented
    as `Variable` class as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'Weights would be of the following dimension: *number of input features × output
    size*.'
  prefs: []
  type: TYPE_NORMAL
- en: Bias
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: In *Figure 2.3*, bias is represented by *b*, which is called additive bias.
    Every neuron has one bias. When *x* is zero, that is, no information is coming
    from the independent variables, then the output should be biased to just *b*.
    Like the weights, the bias also a real number, and the network has to learn the
    bias value to get the correct predictions.
  prefs: []
  type: TYPE_NORMAL
- en: 'In TensorFlow, bias is the same size as the output size and can be represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Net Input Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The net input function, also commonly referred to as the input function, can
    be described as the sum of the products of the inputs and their corresponding
    weights plus the bias. Mathematically, it is represented as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.5: Net input function in mathematical form'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_05.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.5: Net input function in mathematical form'
  prefs: []
  type: TYPE_NORMAL
- en: 'Here:'
  prefs: []
  type: TYPE_NORMAL
- en: '*x*i: input data—*x*1 to *x*m'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*w*i: weights—*w*1 to *w*m'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '*b*: additive bias'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'As you can see, this formula involves inputs and their associated weights and
    biases. This can be written in vectorized form, and we can use matrix multiplication,
    which we learned about in *Chapter 1*, *Building Blocks of Deep Learning*. We
    will see this when we start the code demo. Since all the variables are numbers,
    the result of the net input function is just a number, a real number. The net
    input function can be easily implemented using the TensorFlow `matmul` functionality
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: '`W` stands for weight, `X` stands for input, and `B` stands for bias.'
  prefs: []
  type: TYPE_NORMAL
- en: Activation Function (G)
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: The output of the net input function (`z`) is fed as input to the activation
    function. The activation function squashes the output of the net input function
    (`z`) into a new output range depending on the choice of activation function.
    There are a variety of activation functions, such as sigmoid (logistic), ReLU,
    and tanh. Each activation function has its own pros and cons. We will take a deep
    dive into activation functions later in the chapter. For now, we will start with
    a sigmoid activation function, also known as a logistic function. With the sigmoid
    activation function, the linear output `z` is squashed into a new output range
    of (0,1). The activation function provides non-linearity between layers, which
    gives neural networks the ability to approximate any continuous function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical equation of the sigmoid function is as follows, where *G(z)*
    is the sigmoid function and the right-hand equation details the derivative with
    respect to *z*:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.6: Mathematical form of the sigmoid function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_06.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.6: Mathematical form of the sigmoid function'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see in *Figure 2.7*, the sigmoid function is a more or less S-shaped
    curve with values between 0 and 1, no matter what the input is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.7: Sigmoid curve'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_07.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.7: Sigmoid curve'
  prefs: []
  type: TYPE_NORMAL
- en: And if we set a threshold (say `0.5`), we can convert this into a binary output.
    Any output greater than or equal to `.5` is considered `1`, and any value less
    than `.5` is considered `0`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activation functions such as sigmoid are provided out of the box in TensorFlow.
    A sigmoid function can be implemented in TensorFlow as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: Now that we have seen the structure of a perceptron and its code representation
    in TensorFlow, let's put all the components together to make a perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptrons in TensorFlow
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'In TensorFlow, a perceptron can be implemented just by defining a simple function,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'At a very high level, we can see that the input data passes through the net
    input function. The output of the net input function is passed to the activation
    function, which, in turn, gives us the predicted output. Now, let''s look at each
    line of the code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: The output of the net input function is stored in `z`. Let's see how we got
    that result by breaking it down further into two parts, that is, the matrix multiplication
    part contained in `tf.matmul` and the addition contained in `tf.add`.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s say we''re storing the result of the matrix multiplication of `X` and
    `W` in a variable called `m`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, let''s consider how we got that result. For example, let''s say `X` is
    a row matrix, like [ X1 X2 ], and `W` is a column matrix, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.8: Column matrix'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_08.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.8: Column matrix'
  prefs: []
  type: TYPE_NORMAL
- en: 'Recall from the previous chapter that `tf.matmul` will perform matrix multiplication.
    So, the result is this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'And then, we add the output, `m`, to the bias, `B`, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that what we do in the preceding step is the same as the mere addition
    of the two variables `m` and `b`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'Hence, the final output is:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`z` would be the output of the net input function.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s consider the next line:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: As we learned earlier, `tf.sigmoid` is a readily available implementation of
    the sigmoid function. The net input function's output (`z`) computed in the previous
    line is fed as input to the sigmoid function. The result of the sigmoid function
    is the output of the perceptron, which is in the range of 0 to 1\. During training,
    which will be explained later in the chapter, we will feed the data in batches
    to this function, which will calculate the predicted values.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.01: Perceptron Implementation'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement the perceptron in TensorFlow for an `OR`
    table. Let''s set the input data in TensorFlow and freeze the design parameters
    of perceptron:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s import the necessary package, which, in our case, is `tensorflow`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Set the input data and labels of the `OR` table data in TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'As you can see in the output, we will have a 4 × 2 matrix of input data:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We will set the actual labels in TensorFlow and use the `reshape()` function
    to reshape the `y` vector into a 4 × 1 matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is a 4 × 1 matrix, as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now let's design parameters of a perceptron.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Number of neurons (units) = 1*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '*Number of features (inputs) = 2 (number of examples × number of features)*'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The activation function will be the sigmoid function, since we are doing binary classification:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the preceding code, `X.shape[1]` will equal `2` (since the indices start
    with zero, `1` refers to the second index, which is `2`).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Define the connections weight matrix in TensorFlow:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The weight matrix would essentially be a columnar matrix as shown in the following
    figure. It will have the following dimension: *number of features (columns) ×
    output size*:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.9: A columnar matrix'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_09.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now create the variable for the bias:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There is only one bias per neuron, so in this case, the bias is just one number
    in the form of a single-element array. However, if we had a layer of 10 neurons,
    then it would be an array of 10 numbers—1 for each neuron.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'This will result in a 0-row matrix with a single element like this:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now that we have the weights and bias, the next step is to perform the computation
    to get the net input function, feed it to the activation function, and then get
    the final output. Let''s define a function called `perceptron` to get the output:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be a 4 × 1 array that contains the predictions by our perceptron:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As we can see, the predictions are not quite accurate. We will learn how to
    improve the results in the sections that follow.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3feF7MO](https://packt.live/3feF7MO).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2CkMiEE](https://packt.live/2CkMiEE).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we implemented a perceptron, which is a mathematical implementation
    of a single artificial neuron. Keep in mind that it is just the implementation
    of the model; we have not done any training. In the next section, we will see
    how to train the perceptron.
  prefs: []
  type: TYPE_NORMAL
- en: Training a Perceptron
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'To train a perceptron, we need the following components:'
  prefs: []
  type: TYPE_NORMAL
- en: Data representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Layers
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Neural network representation
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Loss function
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Optimizer
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Training loop
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'In the previous section, we covered most of the preceding components: the `perceptron()`,
    which uses a linear layer and a sigmoid layer to perform predictions. What we
    did in the previous section using input data and initial weights and biases is
    called **forward propagation**. The actual neural network training involves two
    stages: forward propagation and backward propagation. We will explore them in
    detail in the next few steps. Let''s look at the training process at a higher
    level:'
  prefs: []
  type: TYPE_NORMAL
- en: A training iteration where the neural network goes through all the training
    examples is called an Epoch. This is one of the hyperparameters to be tweaked
    in order to train a neural network.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In each pass, a neural network does forward propagation, where data travels
    from the input to the output. As seen in *Exercise 2.01*, *Perceptron Implementation*,
    inputs are fed to the perceptron. Input data passes through the net input function
    and the activation function to produce the predicted output. The predicted output
    is compared with the labels or the ground truth, and the error or loss is calculated.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In order to make a neural network learn, learning being the adjustment of weights
    and biases in order to make correct predictions, there needs to be a **loss function**,
    which will calculate the error between an actual label and the predicted label.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: To minimize the error in the neural network, the training loop needs an **optimizer**,
    which will minimize the loss on the basis of a loss function.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Once the error is calculated, the neural network then sees which nodes of the
    network contributed to the error and by how much. This is essential in order to
    make the predictions better in the next epoch. This way of propagating the error
    backward is called **backward propagation** (backpropagation). Backpropagation
    uses the chain rule from calculus to propagate the error (the error gradient)
    in reverse order until it reaches the input layer. As it propagates the error
    back through the network, it uses gradient descent to make fine adjustments to
    the weights and biases in the network by utilizing the error gradient calculated before.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: This cycle continues until the loss is minimized.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let''s implement the theory we have discussed in TensorFlow. Revisit the code
    in *Exercise 2.01*, *Perceptron Implementation,* where the perceptron we created
    just did one forward pass. We got the following predictions, and we saw that our
    perceptron had not learned anything:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: In order to make our perceptron learn, we need additional components, such as
    a training loop, a loss function, and an optimizer. Let's see how to implement
    these components in TensorFlow.
  prefs: []
  type: TYPE_NORMAL
- en: Perceptron Training Process in TensorFlow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the next exercise, when we train our model, we will use a **Stochastic Gradient
    Descent** (**SGD**) optimizer to minimize the loss. There are a few more advanced
    optimizers available and provided by TensorFlow out of the box. We will look at
    the pros and cons of each of them in later sections. The following code will instantiate
    a stochastic gradient descent optimizer using TensorFlow:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: The `perceptron` function takes care of the forward propagation. For the backpropagation
    of the error, we have used an optimizer. `Tf.optimizers.SGD` creates an instance
    of an optimizer. SGD will update the parameters of the networks—weights and biases—on
    each example from the input data. We will discuss the functioning of the gradient
    descent optimizer in greater detail later in this chapter. We will also discuss
    the significance of the `0.01` parameter, which is known as the learning rate.
    The learning rate is the magnitude by which SGD takes a step in order to reach
    the global optimum of the loss function. The learning rate is another hyperparameter
    that needs to be tweaked in order to train a neural network.
  prefs: []
  type: TYPE_NORMAL
- en: 'The following code can be used to define the epochs, training loop, and loss
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: Inside the training loop, the loss is calculated using the loss function, which
    is defined as a lambda function.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `tf.nn.sigmoid_cross_entropy_with_logits` function calculates the loss
    value of each observation. It takes two parameters: `Labels = y` and `logit =
    perceptron(x)`.'
  prefs: []
  type: TYPE_NORMAL
- en: '`perceptron(X)` returns the predicted value, which is the result of the forward
    propagation of the input, `x`. This is compared with the corresponding label value
    stored in `y`. The mean value is calculated using `Tf.reduce_mean`, and the magnitude
    is taken. The sign is ignored using the `abs` function. `Optimizer.minimize` takes
    the loss value and adjusts the weights and bias as a part of the backward propagation
    of the error.'
  prefs: []
  type: TYPE_NORMAL
- en: The forward propagation is executed again with the new values of weights and
    bias. And this forward and backward process continues for the number of iterations
    we define.
  prefs: []
  type: TYPE_NORMAL
- en: During the backpropagation, the weights and biases are updated only if the loss
    is less than the previous cycle. Otherwise, the weights and biases remain unchanged.
    In this way, the optimizer ensures that even though it loops through the required
    number of iterations, it only stores the values of `w` and `b` for which the loss
    is minimal.
  prefs: []
  type: TYPE_NORMAL
- en: We have set the number of epochs for the training to 1,000 iterations. There
    is no rule of thumb for setting the number of epochs since the number of epochs
    is a hyperparameter. But how do we know when training has taken place successfully?
  prefs: []
  type: TYPE_NORMAL
- en: 'When we can see that the values of weights and biases have changed, we can
    conclude the training has taken place. Let''s say we used a training loop for
    the `OR` data we saw in *Exercise 2.01*, *Perceptron Implementation*, we would
    see weights somewhat equal to the following:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'And the bias would be something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'When the network has learned, that is, the weights and biases have been updated,
    we can see whether it is making accurate predictions using `accuracy_score` from
    the `scikit-learn` package. We can use it to measure the accuracy of the predictions
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: Here, `accuracy_score` takes two parameters—the label values (`y`) and the predicted
    values (`ypred`)—and measures the accuracy. Let's say the result is `1.0`. This
    means the perceptron is 100% accurate.
  prefs: []
  type: TYPE_NORMAL
- en: In the next exercise, we will train our perceptron to perform a binary classification.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.02: Perceptron as a Binary Classifier'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous section, we learned how to train a perceptron. In this exercise,
    we will train our perceptron to approximate a slightly more complicated function.
    We will be using randomly generated external data with two classes: class `0`
    and class `1`. Our trained perceptron should be able to classify the random numbers
    based on their class:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The data is in a CSV file called `data.csv`. You can download the file from
    GitHub by visiting [https://packt.live/2BVtxIf](https://packt.live/2BVtxIf).
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Apart from `tensorflow`, we will need `pandas` to read the data from the CSV
    file, `confusion_matrix` and `accuracy_score` to measure the accuracy of our perceptron
    after the training, and `matplotlib` to visualize the data.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Read the data from the `data.csv` file. It should be in the same path as the
    Jupyter Notebook file in which you are running this exercise''s code. Otherwise,
    you will have to change the path in the code before executing it:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Examine the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.10: Contents of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_10.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.10: Contents of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: As you can see, the data has three columns. `x1` and `x2` are the features,
    and the `label` column contains the labels `0` or `1` for each observation. The
    best way to see this kind of data is through a scatter plot.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the data by plotting it using `matplotlib`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.11: Scatter plot of external data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_11.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.11: Scatter plot of external data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: This shows the two distinct classes of the data shown by the two different shapes.
    Data with the label `0` is represented by a star, while data with the label `1`
    is represented by a triangle.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Prepare the data. This step is not unique to neural networks; you must have
    seen it in regular machine learning as well. Before submitting the data to a model
    for training, you split it into features and labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`x_input` contains the features, `x1` and `x2`. The values at the end convert
    it into matrix format, which is what is expected as input when the tensors are
    created. `y_label` contains the labels in matrix format.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create TensorFlow variables for features and labels and typecast them to `float`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The rest of the code is for the training of the perceptron, which we saw in
    *Exercise 2.01*, *Perceptron Implementation*:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `#` symbol in the code snippet above denotes a code comment. Comments are
    added into code to help explain specific bits of logic.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Display the values of `weight` and `bias` to show that the perceptron has been trained:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Pass the input data to check whether the perceptron classifies it correctly:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Round off the output to convert it into binary format:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Measure the accuracy using the `accuracy_score` method, as we did in the previous
    exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The perceptron gives 100% accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The confusion matrix helps to get the performance measurement of a model. We
    will plot the confusion matrix using the `scikit-learn` package.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: All the numbers are along the diagonal, that is, 12 values corresponding to
    class 0 and 9 values corresponding to class 1 are properly classified by our trained
    perceptron (which has achieved 100% accuracy).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3gJ73bY](https://packt.live/3gJ73bY).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2DhelFw](https://packt.live/2DhelFw).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we trained our perceptron into a binary classifier, and it
    has done pretty well. In the next exercise, we will see how to create a multiclass
    classifier.
  prefs: []
  type: TYPE_NORMAL
- en: Multiclass Classifier
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: A classifier that can handle two classes is known as a **binary classifier**,
    like the one we saw in the preceding exercise. A classifier that can handle more
    than two classes is known as a **multiclass classifier**. We cannot build a multiclass
    classifier with a single neuron. Now we move from one neuron to one layer of multiple
    neurons, which is required for multiclass classifiers.
  prefs: []
  type: TYPE_NORMAL
- en: A single layer of multiple neurons can be trained to be a multiclass classifier.
    Some of the key points are detailed here. You need as many neurons as the number
    of classes; that is, for a 3-class classifier, you need 3 neurons; for a 10-class
    classifier you need 10 neurons, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: As we saw in binary classification, we used sigmoid (logistic layer) to get
    predictions in the range of 0 to 1\. In multiclass classification, we use a special
    type of activation function called the **Softmax** activation function to get
    probabilities across each class that sums to 1\. With the sigmoid function in
    a multiclass setting, the probabilities do not necessarily add up to 1, so Softmax
    is preferred.
  prefs: []
  type: TYPE_NORMAL
- en: Before we implement the multiclass classifier, let's explore the Softmax activation function.
  prefs: []
  type: TYPE_NORMAL
- en: The Softmax Activation Function
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'The Softmax function is also known as the **normalized exponential function**.
    As the word **normalized** suggests, the Softmax function normalizes the input
    into a probability distribution that sums to 1\. Mathematically, it is represented
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.12: Mathematical form of the Softmax function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.12: Mathematical form of the Softmax function'
  prefs: []
  type: TYPE_NORMAL
- en: To understand what Softmax does, let's use TensorFlow's built-in `softmax` function
    and see the output.
  prefs: []
  type: TYPE_NORMAL
- en: 'So, for the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'The output will be:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'As you can see in the output, the `values` input is mapped to a probability
    distribution that sums to 1\. Note that `7` (the highest value in the original
    input values) received the highest weight, `0.824637055`. This is what the Softmax
    function is mainly used for: to focus on the largest values and suppress values
    that are below the maximum value. Also, if we sum the output, it adds up to ~
    1.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Illustrating the example in more detail, let''s say we want to build a multiclass
    classifier with 3 classes. We will need 3 neurons connected to a Softmax activation
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.13: Softmax activation function used in a multiclass classification
    setting'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_13.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.13: Softmax activation function used in a multiclass classification
    setting'
  prefs: []
  type: TYPE_NORMAL
- en: 'As seen in *Figure 2.13*, `x`1, `x`2, and `x`3 are the input features, which
    go through the net input function of each of the three neurons, which have the
    weights and biases (`W`i, j and `b`i) associated with it. Lastly, the output of
    the neuron is fed to the common Softmax activation function instead of the individual
    sigmoid functions. The Softmax activation function spits out the probabilities
    of the 3 classes: `P1`, `P2`, and `P3`. The sum of these three probabilities will
    add to 1 because of the Softmax layer.'
  prefs: []
  type: TYPE_NORMAL
- en: 'As we saw in the previous section, Softmax highlights the maximum value and
    suppresses the rest of the values. Suppose a neural network is trained to classify
    the input into three classes, and for a given set of inputs, the output is class
    2; then it would say that `P2` has the highest value since it is passed through
    a Softmax layer. As you can see in the following figure, `P2` has the highest
    value, which means the prediction is correct:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.14: Probability P2 is the highest'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_14.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.14: Probability P2 is the highest'
  prefs: []
  type: TYPE_NORMAL
- en: 'An associated concept is one-hot encoding. As we have three different classes,
    `class1`, `class2`, and `class3`, we need to encode the class labels into a format
    that we can work with more easily; so, after applying one-hot encoding, we would
    see the following output:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.15: One-hot encoded data for three classes'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_15.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.15: One-hot encoded data for three classes'
  prefs: []
  type: TYPE_NORMAL
- en: 'This makes the results quick and easy to interpret. In this case, the output
    that has the highest value is set to 1, and all others are set to 0\. The one-hot
    encoded output of the preceding example would be like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 2.16: One-hot encoded output probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_16.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.16: One-hot encoded output probabilities'
  prefs: []
  type: TYPE_NORMAL
- en: The labels of the training data also need to be one-hot encoded. And if they
    have a different format, they need to be converted into one-hot-encoded format
    before training the model. Let's do an exercise on multiclass classification with
    one-hot encoding.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.03: Multiclass Classification Using a Perceptron'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'To perform multiclass classification, we will be using the Iris dataset ([https://archive.ics.uci.edu/ml/datasets/Iris](https://archive.ics.uci.edu/ml/datasets/Iris)),
    which has 3 classes of 50 instances each, where each class refers to a type of
    Iris. We will have a single layer of three neurons using the Softmax activation
    function:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'You can download the dataset from GitHub using this link: [https://packt.live/3ekiBBf](https://packt.live/3ekiBBf).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: You must be familiar with all of these imports as they were used in the previous
    exercise, except for `get_dummies`. This function converts a given label data
    into the corresponding one-hot-encoded format.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load the `iris.csv` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s examine the first five rows of the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.17: Contents of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_17.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.17: Contents of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the data by using a scatter plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting plot will be as follows. The *x* axis denotes the sepal length
    and the *y* axis denotes the sepal width. The shapes in the plot represent the
    three species of Iris, setosa (star), versicolor (triangle), and virginica (circle):'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.18: Iris data scatter plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_18.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.18: Iris data scatter plot'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: There are three classes, as can be seen in the visualization, denoted by different shapes.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Separate the features and the labels:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`values` will transform the features into matrix format.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Prepare the data by doing one-hot encoding on the classes:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`get_dummies(y)` will convert the labels into one-hot-encoded format.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create a variable to load the features and typecast it to `float32`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Implement the `perceptron` layer with three neurons:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The code looks very similar to the single perceptron implementation. Only the
    `Number_of_units` parameter is set to `3`. Therefore, the weight matrix will be
    4 x 3 and the bias matrix will be 1 x 3.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The other change is in the activation function:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`Output=tf.nn.softmax(x)`'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: We are using `softmax` instead of `sigmoid`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Create an instance of the `optimizer`. We will be using the `Adam` optimizer.
    At this point, you can think of `Adam` as an improved version of gradient descent
    that converges faster. We will cover it in detail later in the chapter:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Define the training function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Again, the code looks very similar to the single-neuron implementation except
    for the loss function. Instead of `sigmoid_cross_entropy_with_logits`, we use
    `softmax_cross_entropy_with_logits`.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Run the training for `1000` iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Print the values of the weights to see if they have changed. This is also an
    indication that our perceptron is learning:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output shows the learned weights of our perceptron:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'To test the accuracy, we feed the features to predict the output and then calculate
    the accuracy using `accuracy_score`, like in the previous exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: It has given 98% accuracy, which is pretty good.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2Dhes3U](https://packt.live/2Dhes3U).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3iJJKkm](https://packt.live/3iJJKkm).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we performed multiclass classification using our perceptron.
    Let's do a more complex and interesting case study of the handwritten digit recognition
    dataset in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: MNIST Case Study
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Now that we have seen how to train a single neuron and a single layer of neurons,
    let's take a look at more realistic data. MNIST is a famous case study. In the
    next exercise, we will create a 10-class classifier to classify the MNIST dataset.
    However, before that, you should get a good understanding of the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: '**Modified National Institute of Standards and Technology** (**MNIST**) refers
    to the modified dataset that the team led by Yann LeCun worked with at NIST. This
    project was aimed at handwritten digit recognition using neural networks.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to understand the dataset before we get into writing the code. The
    MNIST dataset is integrated into the TensorFlow library. It consists of 70,000
    handwritten images of the digits 0 to 9:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.19: Handwritten digits'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_19.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.19: Handwritten digits'
  prefs: []
  type: TYPE_NORMAL
- en: When we say images, you might think these are JPEG files, but they are not.
    They are actually stored in the form of pixel values. As far as the computer is
    concerned, an image is a bunch of numbers. These numbers are pixel values ranging
    from 0 to 255\. The dimension of each of these images is 28 x 28\. The images
    are stored in the form of a 28 x 28 matrix, each cell containing real numbers
    ranging from 0 to 255\. These are grayscale images (commonly known as black and
    white). 0 indicates white and 1 indicates complete black, and values in between
    indicate a certain shade of gray. The MNIST dataset is split into 60,000 training
    images and 10,000 test images.
  prefs: []
  type: TYPE_NORMAL
- en: Each image has a label associated with it ranging from 0 to 9\. In the next
    exercise, let's build a 10-class classifier to classify the handwritten MNIST
    images.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.04: Classifying Handwritten Digits'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will build a single-layer 10-class classifier consisting
    of 10 neurons with the Softmax activation function. It will have an input layer
    of 784 pixels:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries and packages just like we did in the earlier
    exercise:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create an instance of the MNIST dataset:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the MNIST dataset''s `train` and `test` data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Normalize the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Flatten the 2-dimensional images into row matrices. So, a 28 × 28 pixel gets
    flattened to `784` using the `reshape` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a `Variable` with the features and typecast it to `float32`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a one-hot encoding of the labels and transform it into a matrix:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create the single-layer neural network with `10` neurons and train it for `1000` iterations:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Prepare the test data to measure the accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Run the predictions by passing the test data through the network:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Calculate the accuracy:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The predicted accuracy is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/3efd7Yh](https://packt.live/3efd7Yh).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2Oc83ZW](https://packt.live/2Oc83ZW).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we saw how to create a single-layer multi-neuron neural network
    and train it as a multiclass classifier.
  prefs: []
  type: TYPE_NORMAL
- en: The next step is to build a multilayer neural network. However, before we do
    that, we must learn about the Keras API, since we use Keras to build dense neural
    networks.
  prefs: []
  type: TYPE_NORMAL
- en: Keras as a High-Level API
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In TensorFlow 1.0, there were several APIs, such as Estimator, Contrib, and
    layers. In TensorFlow 2.0, Keras is very tightly integrated with TensorFlow, and
    it provides a high-level API that is user-friendly, modular, composable, and easy
    to extend in order to build and train deep learning models. This also makes developing
    code for neural networks much easier. Let's see how it works.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.05: Binary Classification Using Keras'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will implement a very simple binary classifier with a
    single neuron using the Keras API. We will use the same `data.csv` file that we
    used in *Exercise 2.02*, *Perceptron as a Binary Classifier*:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be downloaded from GitHub by accessing the following GitHub
    link: https://packt.live/2BVtxIf.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In the code, `Sequential` is the type of Keras model that we will be using because
    it is very easy to add layers to it. `Dense` is the type of layer that will be
    added. These are the regular neural network layers as opposed to the convolutional
    layers or pooling layers that will be used later on.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Import the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inspect the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The following will be the output:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.20: Contents of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_20.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.20: Contents of the DataFrame'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the data using a scatter plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting plot is as follows, with the *x* axis denoting `x1` values and
    the y-axis denoting `x2` values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.21: Scatter plot of the data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_21.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.21: Scatter plot of the data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Prepare the data by separating the features and labels and setting the `tf` variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Create a neural network model consisting of a single layer with a neuron and
    a sigmoid activation function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The parameters in `mymodel.add(Dense())` are as follows: `units` is the number
    of neurons in the layer; `input_dim` is the number of features, which in this
    case is `2`; and `activation` is `sigmoid`.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Once the model is created, we use the `compile` method to pass the additional
    parameters that are needed for training, such as the type of the optimizer, the
    loss function, and so on:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: In this case, we are using the `adam` optimizer, which is an enhanced version
    of the gradient descent optimizer, and the loss function is `binary_crossentropy`,
    since this is a binary classifier.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: The `metrics` parameter is almost always set to `['accuracy']`, which is used
    to display information such as the number of epochs, the training loss, the training
    accuracy, the test loss, and the test accuracy during the training process.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The model is now ready to be trained. However, it is a good idea to check the
    configuration of the model by using the `summary` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.22: Summary of the sequential model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_22.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.22: Summary of the sequential model'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train the model by calling the `fit()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'It takes the features and labels as the data parameters along with the number
    of epochs, which in this case is `1000`. The model will start training and will
    continuously provide the status as shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.23: Model training logs using Keras'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_23.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.23: Model training logs using Keras'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'We will evaluate our model using Keras''s `evaluate` functionality:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: As you can see, our Keras model is able to train well, as our accuracy is 100%.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZVV1VY](https://packt.live/2ZVV1VY).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/38CzhTc](https://packt.live/38CzhTc).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we have learned how to build a perceptron using Keras. As
    you have seen, Keras makes the code more modular and more readable, and the parameters
    easier to tweak. In the next section, we will see how to build a multilayer or
    deep neural network using Keras.
  prefs: []
  type: TYPE_NORMAL
- en: Multilayer Neural Network or Deep Neural Network
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In the previous example, we developed a single-layer neural network, often
    referred to as a shallow neural network. A diagram of this follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.24: Shallow neural network'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_24.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.24: Shallow neural network'
  prefs: []
  type: TYPE_NORMAL
- en: 'One layer of neurons is not sufficient to solve more complex problems, such
    as face recognition or object detection. You need to stack up multiple layers.
    This is often referred to as creating a deep neural network. A diagram of this
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.25: Deep neural network'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_25.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.25: Deep neural network'
  prefs: []
  type: TYPE_NORMAL
- en: Before we jump into the code, let's try to understand how this works. Input
    data is fed to the neurons in the first layer. It must be noted that every input
    is fed to every neuron in the first layer, and every neuron has one output. The
    output from each neuron in the first layer is fed to every neuron in the second
    layer. The output of each neuron in the second layer is fed to every neuron in
    the third layer, and so on.
  prefs: []
  type: TYPE_NORMAL
- en: 'That is why this kind of network is also referred to as a dense neural network
    or a fully connected neural network. There are other types of neural networks
    with different workings, such as CNNs, but that is something we will discuss in
    the next chapter. There is no set rule about the number of neurons in each layer.
    This is usually determined by trial and error in a process known as hyperparameter
    tuning (which we''ll learn about later in the chapter). However, when it comes
    to the number of neurons in the last layers, there are some restrictions. The
    configuration of the last layer is determined as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.26: Last layer configuration'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_26.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.26: Last layer configuration'
  prefs: []
  type: TYPE_NORMAL
- en: ReLU Activation Function
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: One last thing to do before we implement the code for deep neural networks is
    learn about the ReLU activation function. This is one of the most popular activation
    functions used in multilayer neural networks.
  prefs: []
  type: TYPE_NORMAL
- en: '**ReLU** is a shortened form of **Rectified Linear Unit**. The output of the
    ReLU function is always a non-negative value that is greater than or equal to
    0:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.27: ReLU activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_27.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.27: ReLU activation function'
  prefs: []
  type: TYPE_NORMAL
- en: 'The mathematical expression for ReLU is:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.28: ReLU activation function'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_28.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.28: ReLU activation function'
  prefs: []
  type: TYPE_NORMAL
- en: ReLU converges much more quickly than the sigmoid activation function, and therefore
    it is by far the most widely used activation function. ReLU is used in almost
    every deep neural network. It is used in all the layers except the last layer,
    where either sigmoid or Softmax is used.
  prefs: []
  type: TYPE_NORMAL
- en: 'The ReLU activation function is provided by TensorFlow out of the box. To see
    how it is implemented, let''s give some sample input values to a ReLU function
    and see the output:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: As you can see, all the positive values are retained, and the negative values
    are suppressed to zero. Let's use this ReLU activation function in the next exercise
    to do a multilayer binary classification task.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.06: Multilayer Binary Classifier'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In this exercise, we will implement a multilayer binary classifier using the
    `data.csv` file that we used in *Exercise 2.02*, *Perceptron as a Binary Classifier*.
  prefs: []
  type: TYPE_NORMAL
- en: 'We will build a binary classifier with a deep neural network of the following
    configuration. There will be an input layer with 2 nodes and 2 hidden layers,
    the first with 50 neurons and the second with 20 neurons, and lastly a single
    neuron to do the final prediction belonging to any binary class:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'The dataset can be downloaded from GitHub using the following link: https://packt.live/2BVtxIf
    .'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries and packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Import and inspect the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.29: The first five rows of the data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_29.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.29: The first five rows of the data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Visualize the data using a scatter plot:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The resulting output is as follows, with the *x* axis showing `x1` values and
    the *y* axis showing `x2` values:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.30: Scatter plot for given data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_30.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.30: Scatter plot for given data'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Prepare the data by separating the features and labels and setting the `tf` variables:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Build the `Sequential` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Here are a couple of points to consider. We provide the input details for the
    first layer, then use the ReLU activation function for all the intermediate layers,
    as discussed earlier. Furthermore, the last layer has only one neuron with a sigmoid
    activation function for binary classifiers.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Provide the training parameters using the `compile` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Inspect the `model` configuration using the `summary` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.31: Deep neural network model summary using Keras'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_31.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.31: Deep neural network model summary using Keras'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the model summary, we can see that there are a total of `1191` parameters—weights
    and biases—to learn across the hidden layers to the output layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train the model by calling the `fit()` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Notice that, in this case, the model reaches 100% accuracy within `50` epochs,
    unlike the single-layer model, which needed about 1,000 epochs:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.32: Multilayer model train logs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_32.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.32: Multilayer model train logs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Let''s evaluate the model''s performance:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Our model has now been trained and demonstrates 100% accuracy.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2ZUkM94](https://packt.live/2ZUkM94).
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/3iKsD1W](https://packt.live/3iKsD1W).
    You must execute the entire Notebook in order to get the desired result.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In this exercise, we learned how to build a multilayer neural network using
    Keras. This is a binary classifier. In the next exercise, we will build a deep
    neural network for a multiclass classifier with the MNIST dataset.
  prefs: []
  type: TYPE_NORMAL
- en: 'Exercise 2.07: Deep Neural Network on MNIST Using Keras'
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'In this exercise, we will perform a multiclass classification by implementing
    a deep neural network (multi-layer) for the MNIST dataset where our input layer
    comprises 28 × 28 pixel images flattened to 784 input nodes followed by 2 hidden
    layers, the first with 50 neurons and the second with 20 neurons. Lastly, there
    will be a Softmax layer consisting of 10 neurons since we are classifying the
    handwritten digits into 10 classes:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Import the required libraries and packages:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Load the MNIST data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`train_features` has the training images in the form of 28 x 28 pixel values.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '`train_labels` has the training labels. Similarly, `test_features` has the
    test images in the form of 28 x 28 pixel values. `test_labels` has the test labels.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Normalize the data:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The pixel values of the images range from 0-255\. We need to normalize the values
    by dividing them by 255 so that the range goes from 0 to 1.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Build the `sequential` model:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: There are couple of points to note. The first layer in this case is not actually
    a layer of neurons but a `Flatten` function. This flattens the 28 x 28 image into
    a single array of `784`, which is fed to the first hidden layer of `50` neurons.
    The last layer has `10` neurons corresponding to the 10 classes with a `softmax`
    activation function.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Provide training parameters using the `compile` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Note
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'The loss function used here is different from the binary classifier. For a
    multiclass classifier, the following loss functions are used: `sparse_categorical_crossentropy`,
    which is used when the labels are not one-hot encoded, as in this case; and, `categorical_crossentropy`,
    which is used when the labels are one-hot encoded.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Inspect the model configuration using the `summary` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.33: Deep neural network summary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_33.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.33: Deep neural network summary'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: In the model summary, we can see that there are a total of 40,480 parameters—weights
    and biases—to learn across the hidden layers to the output layer.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Train the model by calling the `fit` method:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.34: Deep neural network training logs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_34.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.34: Deep neural network training logs'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Test the model by calling the `evaluate()` function:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Now that the model is trained and tested, in the next few steps, we will run
    the prediction with some images selected randomly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Load a random image from a test dataset. Let''s locate the 200th image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s see the shape of the image using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output is:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: We can see that the shape of the image is 28 x 28\. However, the model expects
    3-dimensional input. We need to reshape the image accordingly.
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Use the following code to reshape the image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Let''s call the `predict()` method of the model and store the output in a variable
    called `result`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`result` has the output in the form of 10 probability values, as shown here:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The position of the highest value will be the prediction. Let''s use the `argmax`
    function we learned about in the previous chapter to find out the prediction:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In this case, it is `3`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'In order to check whether the prediction is correct, we check the label of
    the corresponding image:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE115]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Again, the value is `3`:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '[PRE116]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'We can also visualize the image using `pyplot`:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE117]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The output will be as follows:'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '![Figure 2.35: Test image visualized'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: '](img/B15385_02_35.jpg)'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: 'Figure 2.35: Test image visualized'
  prefs: []
  type: TYPE_NORMAL
- en: And this shows that the prediction is correct.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: To access the source code for this specific section, please refer to [https://packt.live/2O5KRgd](https://packt.live/2O5KRgd).
  prefs: []
  type: TYPE_NORMAL
- en: You can also run this example online at [https://packt.live/2O8JHR0](https://packt.live/2O8JHR0).
    You must execute the entire Notebook in order to get the desired result.
  prefs: []
  type: TYPE_NORMAL
- en: In this exercise, we created a multilayer multiclass neural network model using
    Keras to classify the MNIST data. With the model we built, we were able to correctly
    predict a random handwritten digit.
  prefs: []
  type: TYPE_NORMAL
- en: Exploring the Optimizers and Hyperparameters of Neural Networks
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: Training a neural network to get good predictions requires tweaking a lot of
    hyperparameters such as optimizers, activation functions, the number of hidden
    layers, the number of neurons in each layer, the number of epochs, and the learning
    rate. Let's go through each of them one by one and discuss them in detail.
  prefs: []
  type: TYPE_NORMAL
- en: Gradient Descent Optimizers
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In an earlier section titled *Perceptron Training Process in TensorFlow*, we
    briefly touched upon the gradient descent optimizer without going into the details
    of how it works. This is a good time to explore the gradient descent optimizer
    in a little more detail. We will provide an intuitive explanation without going
    into the mathematical details.
  prefs: []
  type: TYPE_NORMAL
- en: 'The gradient descent optimizer''s function is to minimize the loss or error.
    To understand how gradient descent works, you can think of this analogy: imagine
    a person at the top of a hill who wants to reach the bottom. At the beginning
    of the training, the loss is large, like the height of the hill''s peak. The functioning
    of the optimizer is akin to the person descending the hill to the valley at the
    bottom, or rather, the lowest point of the hill, and not climbing up the hill
    that is on the other side of the valley.'
  prefs: []
  type: TYPE_NORMAL
- en: Remember the learning rate parameter that we used while creating the optimizer?
    That can be compared to the size of the steps the person takes to climb down the
    hill. If these steps are large, it is fine at the beginning since the person can
    climb down faster, but once they near the bottom, if the steps are too large,
    the person crosses over to the other side of the valley. Then, in order to climb
    back down to the bottom of the valley, the person will try to move back but will
    move over to the other side again. This results in going back and forth without
    reaching the bottom of the valley.
  prefs: []
  type: TYPE_NORMAL
- en: On the other hand, if the person takes very small steps (a very small learning
    rate), they will take forever to reach the bottom of the valley; in other words,
    the model will take forever to converge. So, finding a learning rate that is neither
    too small nor too big is very important. However, unfortunately, there is no rule
    of thumb to find out in advance what the right value should be—we have to find
    it by trial and error.
  prefs: []
  type: TYPE_NORMAL
- en: 'There are two main types of gradient-based optimizers: batch and stochastic
    gradient descent. Before we jump into them, let''s recall that one epoch means
    a training iteration where the neural network goes through all the training examples:'
  prefs: []
  type: TYPE_NORMAL
- en: In an epoch, when we reduce the loss across all the training examples, it is
    called **batch gradient descent**. This is also known as **full batch gradient
    descent**. To put it simply, after going through a full batch, we take a step
    to adjust the weights and biases of the network to reduce the loss and improve
    the predictions. There is a similar form of it called mini-batch gradient descent,
    where we take steps, that is, we adjust weights and biases, after going through
    a subset of the full dataset.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: In contrast to batch gradient descent, when we take a step at one example per
    iteration, we have **stochastic gradient descent** (**SGD**). The word *stochastic*
    tells us there is randomness involved here, which, in this case, is the batch
    that is randomly selected.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Though SGD works relatively well, there are advanced optimizers that can speed
    up the training process. They include SGD with momentum, Adagrad, and Adam.
  prefs: []
  type: TYPE_NORMAL
- en: The Vanishing Gradient Problem
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In the *Training a Perceptron* section, we learned about the forward and backward
    propagation of neural networks. When a neural network performs forward propagation,
    the error gradient is calculated with respect to the true label, and backpropagation
    is performed to see which parameters (the weights and biases) of the neural network
    have contributed to the error and the extent to which they have done so. The error
    gradient is propagated from the output layer to the input layer to calculate gradients
    with respect to each parameter, and in the last step, the gradient descent step
    is performed to adjust the weights and biases according to the calculated gradient.
    As the error gradient is propagated backward, the gradients calculated at each
    parameter become smaller and smaller as it advances to the lower (initial) layers.
    This decrease in the gradients means that the changes to the weights and biases
    become smaller and smaller. Hence, our neural network struggles to find the global
    minimum and does not give good results. This is called the vanishing gradient
    problem. The problem happens with the use of the sigmoid (logistic) function as
    an activation function, and hence we use the ReLU activation function to train
    deep neural network models to avoid gradient complications and improve the results.
  prefs: []
  type: TYPE_NORMAL
- en: Hyperparameter Tuning
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Like any other model training process in machine learning, it is possible to
    perform hyperparameter tuning to improve the performance of the neural network
    model. One of the parameters is the learning rate. The other parameters are as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Number of epochs**: Increasing the number of epochs generally increases the
    accuracy and lowers the loss'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of layers**: Increasing the number of layers increases the accuracy,
    as we saw in the exercises with MNIST'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Number of neurons per layer**: This also increases the accuracy'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: And once again, there is no way to know in advance what the right number of
    layers or the right number of neurons per layer is. This has to be figured out
    by trial and error. It has to be noted that the larger the number of layers and
    the larger the number of neurons per layer, the greater the computational power
    required. Therefore, we start with the smallest possible numbers and slowly increase
    the number of layers and neurons.
  prefs: []
  type: TYPE_NORMAL
- en: Overfitting and Dropout
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Neural networks with complex architectures and too many parameters tend to
    fit on all the data points, including noisy labels, leading to the problem of
    overfitting and neural networks that are not able to generalize well on unseen
    datasets. To tackle this issue, there is a technique called **dropout**:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 2.36: Dropout illustrated'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B15385_02_36.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: 'Figure 2.36: Dropout illustrated'
  prefs: []
  type: TYPE_NORMAL
- en: In this technique, a certain number of neurons are deactivated randomly during
    the training process. The number of neurons to be deactivated is provided as a
    parameter in the form of a percentage. For example, `Dropout = .2` means 20% of
    the neurons in that layer will be randomly deactivated during the training process.
    The same neurons are not deactivated more than once, but a different set of neurons
    is deactivated in each epoch. During testing, however, all the neurons are activated.
  prefs: []
  type: TYPE_NORMAL
- en: 'Here is an example of how we can add `Dropout` to a neural network model using Keras:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE118]'
  prefs: []
  type: TYPE_PRE
- en: In this case, a dropout of 20% is added to `Hidden Layer2`. It is not necessary
    for the dropout to be added to all layers. As a data scientist, you can experiment
    and decide what the `dropout` value should be and how many layers need it.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: 'A more detailed explanation of dropout can be found in the paper by Nitish
    Srivastava et al. available here: [http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf).'
  prefs: []
  type: TYPE_NORMAL
- en: As we have come to the end of this chapter, let's test what we have learned
    so far with the following activity.
  prefs: []
  type: TYPE_NORMAL
- en: 'Activity 2.01: Build a Multilayer Neural Network to Classify Sonar Signals'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this activity, we will use the Sonar dataset ([https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks)](https://archive.ics.uci.edu/ml/datasets/Connectionist+Bench+(Sonar,+Mines+vs.+Rocks))),
    which has patterns obtained by bouncing sonar signals off a metal cylinder at
    various angles and under various conditions. You will build a neural network-based
    classifier to classify between sonar signals bounced off a metal cylinder (the
    Mine class), and those bounced off a roughly cylindrical rock (the Rock class).
    We recommend using the Keras API to make your code more readable and modular,
    which will allow you to experiment with different parameters easily:'
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: You can download the sonar dataset from this link [https://packt.live/31Xtm9M](https://packt.live/31Xtm9M).
  prefs: []
  type: TYPE_NORMAL
- en: The first step is to understand the data so that you can figure out whether
    this is a binary classification problem or a multiclass classification problem.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Once you understand the data and the type of classification that needs to be
    done, the next step is network configuration: the number of neurons, the number
    of hidden layers, which activation function to use, and so on.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Recall the network configuration steps that we''ve covered so far. Let''s just
    reiterate a crucial point, the activation function part: for the output (the last)
    layer, we use sigmoid to do binary classification and Softmax to do multiclass classification.'
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
- en: Open the `sonar.csv` file to explore the dataset and see what the target variables
    are.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Separate the input features and the target variables.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Preprocess the data to make it neural network-compatible. Hint: one-hot encoding.'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Define a neural network using Keras and compile it with the right loss function.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Print out a model summary to verify the network parameters and considerations.
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: You are expected to get an accuracy value above 95% by designing a proper multilayer
    neural network using these steps.
  prefs: []
  type: TYPE_NORMAL
- en: Note
  prefs: []
  type: TYPE_NORMAL
- en: The detailed steps for this activity, along with the solutions and additional
    commentary, are presented on page 390.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: In this chapter, we started off by looking at biological neurons and then moved
    on to artificial neurons. We saw how neural networks work and took a practical
    approach to building single-layer and multilayer neural networks to solve supervised
    learning tasks. We looked at how a perceptron works, which is a single unit of
    a neural network, all the way to a deep neural network capable of performing multiclass
    classification. We saw how Keras makes it very easy to create deep neural networks
    with a minimal amount of code. Lastly, we looked at practical considerations to
    take into account when building a successful neural network, which involved important
    concepts such as gradient descent optimizers, overfitting, and dropout.
  prefs: []
  type: TYPE_NORMAL
- en: In the next chapter, we will go to the next level and build a more complicated
    neural network called a CNN, which is widely used in image recognition.
  prefs: []
  type: TYPE_NORMAL
