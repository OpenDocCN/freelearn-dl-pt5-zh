- en: '*Chapter 4*: Tracking Code and Data Versioning'
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: DL models are not just models – they are intimately tied to the code that trains
    and tests the model and the data that's used for training and testing. If we don't
    track the code and data that's used for the model, it is impossible to reproduce
    the model or improve it. Furthermore, there have been recent industry-wide awakenings
    and paradigm shifts toward a **data-centric AI** ([https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=5cbacdc574f5](https://www.forbes.com/sites/gilpress/2021/06/16/andrew-ng-launches-a-campaign-for-data-centric-ai/?sh=5cbacdc574f5)),
    where the importance of data is being lifted to a first-class artifact in building
    ML and, especially, DL models. Due to this, in this chapter, we will learn how
    to track code and data versioning using MLflow. We will learn about the different
    ways we can track code and pipeline versioning and how to use Delta Lake for data
    versioning. By the end of this chapter, you will be able to understand and implement
    tracking techniques for both code and data with MLflow.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this chapter, we''re going to cover the following main topics:'
  prefs: []
  type: TYPE_NORMAL
- en: Tracking notebook and pipeline versioning
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking locally, privately built Python libraries
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking data versioning in Delta Lake
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Technical requirements
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'The following are the technical requirements for this chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: 'VS Code with the Jupyter Notebook extension: [https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks](https://github.com/microsoft/vscode-jupyter/wiki/Setting-Up-Run-by-Line-and-Debugging-for-Notebooks).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'The code for this chapter, which can be found in this book''s GitHub repository:
    [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter04](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter04).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Access to a Databricks instance so that you can learn how to use Delta Lake
    to enable versioned data access.
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Tracking notebook and pipeline versioning
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Data scientists usually start by experimenting with Python notebooks offline,
    where interactive execution is a key benefit. Python notebooks have come a long
    way since the days of `.ipynb`. You may not be able to see the exact Git hash
    in the MLflow tracking server for each run using a Jupyter notebook either. There
    are a lot of interesting debates on whether or when a Jupyter notebook should
    be used, especially in a production environment (see a discussion here: [https://medium.com/mlops-community/jupyter-notebooks-in-production-4e0d38803251](https://medium.com/mlops-community/jupyter-notebooks-in-production-4e0d38803251)).
    There are multiple reasons why we shouldn''t use Jupyter notebooks in a production
    environment, especially when we need reproducibility in an end-to-end pipeline
    fashion, where unit testing, proper code versioning, and dependency management
    could be difficult with a lot of notebooks. There are some early innovations around
    scheduling, parameterizing, and executing Jupyter notebooks in a workflow fashion
    using the open source tool **papermill** by Netflix ([https://papermill.readthedocs.io/en/latest/index.html](https://papermill.readthedocs.io/en/latest/index.html)).
    However, a recent innovation by Databricks and VS Code makes notebooks much easier
    to be version controlled and integrated with MLflow. Let''s look at the notebook
    characteristics that were introduced by these two tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '**Interactive execution**: Both Databricks''s notebooks and VS Code''s notebooks
    can run the same way as traditional Jupyter notebooks, in a cell-by-cell execution
    mode. By doing this, you can immediately see the output of the results.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`.py` file extension. This allows all the regular Python code linting (code
    format and style checking) to be applied to a notebook.'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '**Special symbols for rendering code cells and Mark down cells**: Both Databricks
    and VS Code leverage some special symbols to render Python files as interactive
    notebooks. In Databricks, the special symbols to delineate code into different
    executable cells are as follows:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'The code below the special `COMMAND` line will be rendered as an executable
    cell in the Databricks web UI portal, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.1 – Databricks executable cell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_Figure_4.1.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.1 – Databricks executable cell
  prefs: []
  type: TYPE_NORMAL
- en: To execute the code in this cell, you can just click **Run Cell** via the top-right
    drop-down menu.
  prefs: []
  type: TYPE_NORMAL
- en: 'To add a large chunk of text to describe and comment on the code in Databricks
    (also known as Markdown cells), you can use the `# MAGIC` symbol at the beginning
    of the line, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'This is then rendered in the Databricks notebook as a Markdown comment cell,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.2 – Databricks Markdown text cell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_Figure_4.2.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.2 – Databricks Markdown text cell
  prefs: []
  type: TYPE_NORMAL
- en: 'In VS Code, a slightly different set of symbols is used for these two types
    of cells. For a code cell, the `# %%` symbols are used at the beginning of the
    cell block:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'This is then rendered in VS Code''s editor, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.3 – VSCode code cell'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_Figure_4.3.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.3 – VS Code code cell
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, there is a **Run Cell** button before the block of code that
    you can click to run the code block interactively. If you click the **Run Cell**
    button, the code block will start executing in the side panel of the editor window,
    as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![ Figure 4.4 – Running code interactively in VSCode'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_Figure_4.4.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.4 – Running code interactively in VS Code
  prefs: []
  type: TYPE_NORMAL
- en: 'To add a Markdown cell that contains comments, add the following to the beginning
    of the line, as well as the necessary symbols:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: This will ensure that the text is not an executable code block in VS Code.
  prefs: []
  type: TYPE_NORMAL
- en: Given the advantages of Databricks and VS Code notebooks, we suggest using either
    for version tracking. We can use GitHub to track the versioning of either type
    of notebook since they use a regular Python file format.
  prefs: []
  type: TYPE_NORMAL
- en: Two Ways to Use Databricks Notebook Version Control
  prefs: []
  type: TYPE_NORMAL
- en: 'For a managed Databricks instance, a notebook version can be tracked in two
    ways: by looking at the revision history on the side panel of the notebook on
    the Databricks web UI, or by linking to a remote GitHub repository. Detailed descriptions
    are available in the Databricks notebook documentation: [https://docs.databricks.com/notebooks/notebooks-use.html#version-control](https://docs.databricks.com/notebooks/notebooks-use.html#version-control).'
  prefs: []
  type: TYPE_NORMAL
- en: While the Databricks web portal provides excellent support for notebook version
    control and integration with MLflow experimentation tracking (see this chapter's
    callout boxes on *Two Ways to Use Databricks Notebook Version Control* and *Two
    Types of MLflow Experiments in Databricks Notebooks*), there is one major drawback
    of writing code in the Databricks notebook web UI. This is because the web UI
    is not a typical **integrated development environment** (**IDE**) compared to
    VS Code, where code style and formatting tools such as **flake8** ([https://flake8.pycqa.org/en/latest/](https://flake8.pycqa.org/en/latest/))
    and autopep8 ([https://pypi.org/project/autopep8/](https://pypi.org/project/autopep8/))
    can easily be enforced. This can have a major impact on code quality and maintainability.
    Thus, it is highly recommended that you use VS Code to author notebook code (either
    a Databricks notebook or a VS Code notebook).
  prefs: []
  type: TYPE_NORMAL
- en: Two Types of MLflow Experiments in Databricks Notebooks
  prefs: []
  type: TYPE_NORMAL
- en: 'For a managed Databricks web portal instance, there are two types of MLflow
    experiments you can perform: workspace and notebook experiments. A workspace experiment
    is mainly for a shared experiment folder that is not tied to a single notebook.
    Remote code execution can write to a workspace experiment folder if needed. On
    the other hand, a notebook scope experiment is tied to a specific notebook and
    can be found directly on one of the top-right menu items called **Experiment**
    in the notebook page on the Databricks web portal. For more details, please look
    at the Databricks documentation website: [https://docs.databricks.com/applications/mlflow/tracking.html#experiments](https://docs.databricks.com/applications/mlflow/tracking.html#experiments).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using this chapter''s VS Code notebook, `fine_tuning.py`, which can be found
    in this chapter''s GitHub repository ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/notebooks/fine_tuning.py](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/notebooks/fine_tuning.py)),
    you will be able to run it interactively in the VS Code editor and log the experiment
    in the MLflow Docker server that we set up in [*Chapter 3*](B18120_03_ePub.xhtml#_idTextAnchor040)*,
    Tracking Models, Parameters, and Metrics*. As a reminder, note that to run this
    notebook in VS Code successfully, you will need to set up your virtual environment,
    called `dl_model`, as described in the `README.md` file in this chapter''s GitHub
    repository. It consists of the following three steps:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'If you run this notebook cell-by-cell from beginning to end, your experiment
    page will look as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.5 – Logged experiment page after running a VSCode notebook interactively'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_Figure_4.5.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.5 – Logged experiment page after running a VS Code notebook interactively
  prefs: []
  type: TYPE_NORMAL
- en: You may immediately notice a problem in the preceding screenshot – `fine_tuning.py`
    file. This is because VS Code notebooks are not natively integrated into the MLflow
    tracking server for source file tracking; it can only show the **ipykernel** ([https://pypi.org/project/ipykernel/](https://pypi.org/project/ipykernel/))
    that VS Code uses to execute a VS Code notebook ([https://github.com/microsoft/vscode-jupyter](https://github.com/microsoft/vscode-jupyter)).
    Unfortunately, this is a limitation that, at the time of writing, cannot be addressed
    by running VS Code notebooks *interactively* for experiment code tracking. Databricks
    notebooks running inside a hosted Databricks web UI have no such problem as they
    have native integration with the MLflow tracking server that's bundled in the
    Databricks web portal.
  prefs: []
  type: TYPE_NORMAL
- en: 'However, since the VS Code notebooks are just Python code, we can run the notebooks
    in the command line *non-interactively*, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'This will log the actual source code''s filename and the Git commit hash in
    the MLflow experiment page without any issues, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.6 – Logged experiment page after running a VSCode notebook in the
    command line'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_Figure_4.6.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.6 – Logged experiment page after running a VS Code notebook in the
    command line
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows the correct source filename (`fine_tuning.py`)
    and the correct git commit hash (**661ffeda5ae53cff3623f2fcc8227d822e877e2d**).
    This workaround does not require us to change the notebook's code and could be
    very useful if our initial interactive notebook debugging is done and we want
    to get a complete run of the notebook, along with proper code version tracking
    in the MLflow tracking server. Note that all the other parameters, metrics, and
    models are tracked properly, regardless of whether we run the notebook interactively.
  prefs: []
  type: TYPE_NORMAL
- en: Pipeline tracking
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'Having discussed notebook code tracking (version and filename), let''s turn
    to the topic of pipeline tracking. Before we discuss pipeline tracking, however,
    we will discuss the definition of a pipeline in the ML/DL life cycle. Conceptually,
    a pipeline is a multi-step data processing and task workflow. However, the implementation
    of such a data/task workflow can be quite different. A pipeline can be defined
    as a first-class Python API in some ML packages. The two most well-known pipeline
    APIs are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '`sklearn.pipeline.Pipeline` ([https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html)):
    This is widely used for building tightly integrated multi-step pipelines for classical
    machine learning or data **extract, transform, and load** (**ETL**) pipelines
    using **pandas DataFrames** (https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '`pyspark.ml.Pipeline` ([https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html](https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.Pipeline.html)):
    This is a PySpark version for building simple and tightly integrated multi-step
    pipelines for machine learning or data ETL pipelines using **Spark DataFrames**
    (https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.DataFrame.html).'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: However, when we're building a DL model pipeline, we need to use multiple different
    Python packages at different steps of the pipeline, so a one-size-fits-all approach
    using a single pipeline API doesn't usually work. In addition, neither of the
    aforementioned pipeline APIs have native support for the current popular DL packages,
    such as **Huggingface** or **PyTorch-Lightning**, which require additional integration
    work. Although some open source DL pipeline APIs exist such as **Neuraxle** ([https://github.com/Neuraxio/Neuraxle](https://github.com/Neuraxio/Neuraxle)),
    which tries to provide a sklearn-like pipeline interface and framework, it is
    not widely used. Furthermore, using these API-based pipelines means that you'll
    be locked in when you need to add more steps to the pipeline, which could reduce
    your flexibility to extend or evolve a DL pipeline when new requirements arise.
  prefs: []
  type: TYPE_NORMAL
- en: 'In this book, we will take a different approach to define and build a DL pipeline
    that''s based on MLflow''s `fine_tuning.py`, into a multiple-step pipeline. This
    pipeline can be visualized as a three-step flow diagram, as shown here:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.7 – A three-step DL pipeline'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_Figure_4.7.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.7 – A three-step DL pipeline
  prefs: []
  type: TYPE_NORMAL
- en: 'This three-step flow is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: Download the data to a local execution environment
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Fine-tune the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: Register the model
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'These modular steps may seem to be overkill for our current example, but the
    power of having a distinctive functional step is evident when more complexities
    are involved, or when changes are needed at each step. Each step can be modified
    without them affecting the other steps if we define the parameters that need to
    be passed between them. Each step is a standalone Python file that can be executed
    independently with a set of input parameters. There will be a main pipeline Python
    file that can run the whole pipeline or a sub-section of the pipeline''s steps.
    In the `MLproject` file, which is a standard YAML file without the file extension,
    we can define four entry points (`main`, `download_data`, `fine_tuning_model`,
    and `register_model`), their required input parameters, their types and default
    values, and the command line to execute each entry point. In our example, these
    entry points will be provided in a Python command-line execution command. However,
    you can invoke any kind of execution, such as a batch shell script, if needed
    for any particular steps. For example, the following lines in the `MLproject`
    file for this chapter ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/MLproject](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/MLproject))
    describe the name of the project, the `conda` environment definition filename,
    and the main entry point:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: Here, the name of the project is `dl_model_chapter04`. `conda_env` refers to
    a local conda environment's YAML definition file, `conda.yaml`, which is located
    in the same directory as the `MLproject` file. The `entry_points` section lists
    the first entry point, called `main`. In the `parameters` section, there is one
    parameter called `pipeline_steps`, which allows the user to define a comma-separated
    list of DL pipeline steps to execute. This parameter is of the `str` type and
    its default value is `all`, which means that all the pipeline steps will run.
    Lastly, the `command` section lists how to execute this step in the command line.
  prefs: []
  type: TYPE_NORMAL
- en: 'The rest of the `MLproject` file defines the other three pipeline step entry
    points by following the same syntactic convention as the main entry point. For
    example, the following lines in the same `MLproject` file define the entry point
    of `download_data`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The `download_data` section, similar to the main entry point, also defines the
    list of parameters, types, and default values, as well as the command line to
    execute this step. We can define the rest of the steps in the same manner as we
    did in the `MLproject` file that we just checked out from this book's GitHub repository.
    For more details, take a look at the full content of that `MLproject` file.
  prefs: []
  type: TYPE_NORMAL
- en: 'After defining the `MLproject` file, it becomes clear that we have defined
    a multi-step pipeline in a declarative way. This is like a specification for the
    pipeline that says each step''s name, what input parameters it expects, and how
    to execute them. Now, the next step is to implement the Python function to execute
    each step of the pipeline. So, let''s look at the core implementation of the main
    entry point''s Python function, which is called `main.py`. The following lines
    of code (not the entire Python code in `main.py`) illustrate the core component
    of implementing the entire pipeline with just one step in the pipeline (`download_data`):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: This main function snippet contains a `run_pipeline` function, which will be
    run when the `main.py` file is executed in the command line. There is a parameter
    called `steps`, which will be passed to this function when it's provided. In this
    example, we are using the `click` Python package ([https://click.palletsprojects.com/en/8.0.x/](https://click.palletsprojects.com/en/8.0.x/))
    to parse command-line arguments. The `run_pipeline` function starts an MLflow
    experiment run by calling `mlflow.start_run` and passing two parameters (`run_name`
    and `nested`). We have used `run_name` before – it's the descriptive phrase for
    this run. However, the `nested` parameter is new, which means that this is a parent
    experiment run. This parent experiment run contains some child experiment runs
    that will be hierarchically tracked in MLflow. Each parent run can contain one
    or more child runs. In the example code, this contains one step of the pipeline
    run, called `download_data`, which is invoked by calling `mlflow.run`. This is
    the key MLflow function to invoke an MLproject's entry point programmatically.
    Once `download_data` has been invoked and the run has finished, the parent run
    will also finish, thus concluding the pipeline's run.
  prefs: []
  type: TYPE_NORMAL
- en: Two Ways to Execute an MLproject's Entry Point
  prefs: []
  type: TYPE_NORMAL
- en: There are two ways to execute an MLproject's entry point. First, you can use
    MLflow's Python API, known as `mlflow.run` ([https://www.mlflow.org/docs/latest/python_api/mlflow.projects.html#mlflow.projects.run](https://www.mlflow.org/docs/latest/python_api/mlflow.projects.html#mlflow.projects.run)).
    Alternatively, you can use the MLflow's command-line interface tool, called `mlflow
    run`, which can be called in a command-line shell environment to execute any entry
    point directly ([https://www.mlflow.org/docs/latest/cli.html#mlflow-run](https://www.mlflow.org/docs/latest/cli.html#mlflow-run)).
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, let''s learn how to implement each step in the pipeline generically. For
    each pipeline step, we put the Python files in a `pipeline` folder. In this example,
    we have three files: `download_data.py`, `fine_tuning_model.py`, and `register_model.py`.
    Thus, the relevant files for successfully building an MLflow supported pipeline
    project are as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'For the implementation of each pipeline step, we can use the following Python
    function templates. A placeholder section is reserved for implementing the actual
    pipeline step logic:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: This template allows us to standardize the way we implement the pipeline step
    task. The main idea here is that for each pipeline step task, it needs to start
    with `mlflow.start_run` to launch an MLflow experiment run. Once we've implemented
    specific execution logic in the function, we need to log some parameters using
    `mlflow.log_parameter`, or some artifacts in the artifact store using `mlflow.log_artifacts`,
    that can be passed to and used by the next step of the pipeline. This is called
    `mlflow.set_tag`.
  prefs: []
  type: TYPE_NORMAL
- en: 'For example, in the `download_data.py` step, the core implementation is as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'In this `download_data.py` implementation, the task is to download the data
    for model building from a remote URL to a local folder (`download_data(download_url,
    local_folder)`). Once we''ve done this, we will log a few parameters, such as
    `download_url` and `local_folder`. We can also log the newly downloaded data into
    the MLflow artifact store using `mlflow.log_artifacts`. For this example, this
    may not seem necessary since we only want to execute the next step in a local
    development environment. However, for a more realistic scenario in a distributed
    execution environment where each step could be run in different execution environments,
    this is very desirable since we only need to pass the artifact URL path to the
    next step of the pipeline to use; we don''t need to know how and where the previous
    step was executed. In this example, when the `mlflow.log_artifacts(local_folder,
    artifact_path="data")` statement is called, the downloaded data folder is uploaded
    to the MLflow artifact store. However, we will not use this artifact path for
    the downstream pipeline step in this chapter. We will explore how we use this
    kind of artifact store to pass artifacts to the next step in the pipeline later
    in this book. Here, we will use the log parameters to pass the downloaded data
    path to the next step of the pipeline (`mlflow.log_param("local_folder", local_folder)`).
    So, let''s look at how we can do that by extending `main.py` so that it includes
    the next step, which is the `fine_tuning_model` entry point, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'We use `mlflow.tracking.MlflowClient().get_run` to get the `download_run` MLflow
    run object and then use `download_run.data.params` to get `file_path_uri` (in
    this case, it is just a local folder path). This is then passed to the next `mlflow.run`,
    which is `fine_tuning_run`, as a key-value parameter (`parameters={"data_path":
    file_path_uri` ). This way, the `fine_tuning_run` pipeline step can use this parameter
    to prefix its data source path. This is a very simplified scenario to illustrate
    how we can pass data from one step to the next. Using the `mlflow.tracking.MlflowClient()`
    API, which is provided by MLflow ([https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html](https://www.mlflow.org/docs/latest/python_api/mlflow.tracking.html)),
    makes accessing a run''s information (parameters, metrics, and artifacts) straightforward.'
  prefs: []
  type: TYPE_NORMAL
- en: 'We can also extend the `main.py` file with the third step of the pipeline by
    adding the `register_model` step. This time, we need the logged model URI to register
    a trained model, which depends on `run_id` of the `fine_tuning_model` step. So,
    in the `fine_tuning_model` step, we need to get the `run_id` property of `fine_tuning_model`
    run and then pass it through the input parameter for the `register_model` run,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, the `register_model` step can use `fine_tuning_run_id` to locate the logged
    model. The core implementation of the `register_model` step is as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: This will register a fine-tuned model at the URI defined by the `logged_model`
    variable to an MLflow model registry.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you have followed these steps, then you should have a working pipeline that
    can be tracked by MLflow from end to end. As a reminder, a prerequisite is to
    have the local full-fledged MLflow server set up, as shown in [*Chapter 3*](B18120_03_ePub.xhtml#_idTextAnchor040)*,
    Tracking Models, Parameters, and Metrics*. You should have set up the virtual
    environment, `dl_model`, in the previous section. To test this pipeline, check
    out this chapter''s GitHub repository at [https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter04](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/tree/main/chapter04)
    and run the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: 'This will run the entire three-step pipeline and log the pipeline''s `run_id`
    (which is the parent run) and each step''s run as the child runs in the MLflow
    tracking server. The last few lines of the console screen''s output will display
    something as follows when it has finished running (you will see lots of outputs
    on the screen when you run the pipeline):'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.8 – Console output of running the pipeline with MLflow run_ids'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_Figure_4.8.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.8 – Console output of running the pipeline with MLflow run_ids
  prefs: []
  type: TYPE_NORMAL
- en: 'This shows the pipeline''s `run_id`, which is `f8f21fdf8fff4fd6a400eeb403b776c8`;
    the last step is the `run_id` property of `fine_tuning_model`, which is `5ba38e059695485396e709b809e9bb8d`.
    If we go to the MLflow tracking server''s UI web page by clicking on `http://localhost`,
    we should be able to see the following nested experiment runs in the `dl_model_chapter04`
    experiment folder, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.9 – A pipeline being run with nested three-step child runs in the
    MLflow tracking server'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_Figure_4.9.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.9 – A pipeline being run with nested three-step child runs in the MLflow
    tracking server
  prefs: []
  type: TYPE_NORMAL
- en: 'The preceding screenshot shows the pipeline run, along with the source `main.py`
    file and the nested run of the three steps of the pipeline. Each step has a corresponding
    entry point name defined in `MLproject` with a GitHub commit hash code version
    of `register_model` run page, you will see the following information:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.10 – Entry point register_model''s run page on the MLflow tracking
    server'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_Figure_4.10.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.10 – Entry point register_model's run page on the MLflow tracking server
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows not only some of the familiar information we
    have seen already, but also some new information such as `file:///`), the GitHub
    hash code version, the entry point `(-e register_model`), the execution environment,
    which is a local dev environment (`-b local`), and the expected parameters for
    the `register_model` function (`-P`). We will learn how to use MLflow's `MLproject`
    to run commands to execute tasks remotely later in this book. Here, we just need
    to understand that the source code is referred to through the entry point (`register_model`),
    not the filename itself, since the reference is declared as an entry point in
    the `MLproject` file.
  prefs: []
  type: TYPE_NORMAL
- en: If you saw the output shown in *Figure 4.9* and *Figure 4.10* in your MLflow
    tracking server, then it's time to celebrate – you have successfully executed
    a multi-step DL pipeline using MLflow!
  prefs: []
  type: TYPE_NORMAL
- en: In summary, to track a multi-step DL pipeline in MLflow, we can use `MLproject`
    to define entry points for each pipeline step and a main pipeline entry point.
    In the main pipeline function, we implement methods so that data can be passed
    between pipeline steps. Each pipeline step then uses the data that's been shared,
    as well as other input parameters, to execute a specific task. Both the main pipeline-level
    function and each step of the pipeline are tracked using the MLflow tracking server,
    which produces a parent `run_id` to track the main pipeline run and multiple MLflow
    nested runs to track each pipeline's step. We introduced a template for each pipeline
    step to implement this task in a standard way. We also explored the powerful pipeline
    chaining that's done through MLflow's `run` parameter and artifact store to learn
    how to pass data between pipeline steps.
  prefs: []
  type: TYPE_NORMAL
- en: Now that you know how to track notebooks and pipelines, let's learn how to track
    Python libraries.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking locally, privately built Python libraries
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'Now, let''s turn our attention to tracking locally, privately built Python
    libraries. For publicly released Python libraries, we can explicitly specify their
    released version, which is published in PyPI, in a requirements file or a `conda.yaml`
    file. For example, this chapter''s `conda.yaml` file ([https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/conda.yaml](https://github.com/PacktPublishing/Practical-Deep-Learning-at-Scale-with-MLFlow/blob/main/chapter04/conda.yaml))
    defines the Python version and provides a reference to a requirements file, as
    follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'The Python version is defined as `3.8.10` and is being enforced. This `conda.yaml`
    file also refers to a `requirements.txt` file, which contains the following versioned
    Python packages as a `requirements.txt` file, which is located in the same directory
    as the `conda.yaml` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'As we can see, all these packages are being tracked explicitly using their
    published PyPI ([https://pypi.org/](https://pypi.org/)) version number. When you
    run the MLflow `MLproject`, MLflow will use the `conda.yaml` file and the referenced
    `requirements.txt` file to create a conda virtual environment dynamically. This
    ensures that the execution environment is reproducible and that all the DL model
    pipelines can be run successfully. You may have noticed that such a virtual environment
    was created for you the first time you ran the previous section''s MLflow pipeline
    project. You can do this again by running the following command:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: 'This will produce a list of conda virtual environments in your current machine.
    You should be able to find a virtual environment starting with a `mlflow-` prefix,
    followed by a long string of alphanumerical characters, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'This is the virtual environment that''s created by MLflow dynamically, which
    follows the dependencies that are specified in `conda.yaml` and `requirements.txt`.
    Subsequently, when you log the fine-tuned model, `conda.yaml` and `requirements.txt`
    will be automatically logged in the MLflow artifact store, as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '![Figure 4.11 – Python packages are being logged and tracked in the MLflow
    artifact store'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_Figure_4.11.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.11 – Python packages are being logged and tracked in the MLflow artifact
    store
  prefs: []
  type: TYPE_NORMAL
- en: As we can see, the `conda.yaml` file was automatically expanded to include the
    content of `requirements.txt`, as well as other dependencies that conda decides
    to include.
  prefs: []
  type: TYPE_NORMAL
- en: 'For privately built Python packages, which means the Python packages that are
    not published to PyPI for public consumption and references, the recommended way
    to include such a Python package is by using `git+ssh`. Let''s assume that you
    have a privately built project called `cool-dl-utils`, that the organization you
    work for is called `cool_org`, and that your project''s repository has been set
    up in GitHub. If you want to include this project''s Python package in the requirements
    file, you need to make sure that you add your public key to your GitHub settings.
    If you want to learn how to generate a public key and load it into GitHub, take
    a look at GitHub''s guide at [https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account](https://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account).
    In the `requirements.txt` file, you can add the following line, which will reference
    a specific GitHub hash (`81218891bbf5a447103884a368a75ffe65b17a44`) and the Python
    `.egg` package that was built from this private repository (you can also reference
    a `.whl` package if you wish):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'If you have a numerically released version in your privately built package,
    you can also directly reference the release number in the `requirements.txt` file,
    as follows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: Here the release number of `cool-dl-utils` is `2.11.4`. This allows MLflow to
    pull this privately built package into the virtual environment to execute `MLproject`.
    In this chapter, we don't need to reference any privately built Python packages,
    but it is worth noting that MLflow can leverage the `git+ssh` approach to do that.
  prefs: []
  type: TYPE_NORMAL
- en: Now, let's learn how to track data versioning.
  prefs: []
  type: TYPE_NORMAL
- en: Tracking data versioning in Delta Lake
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this section, we''ll learn how data is tracked in MLflow. Historically,
    data management and versioning are usually considered as being different from
    machine learning and data science. However, the advent of data-centric AI is playing
    an increasingly important role, particularly in DL. Therefore, it is critical
    to know what and how data is being used to improve the DL model. In the first
    data-centric AI competition, which was organized by Andrew Ng in the summer of
    2021, the requirements to become a winner were not about changing and tuning a
    model, but rather improving the dataset of a fixed model ([https://https-deeplearning-ai.github.io/data-centric-comp/](https://https-deeplearning-ai.github.io/data-centric-comp/)).
    Here is a quote from the competition''s web page:'
  prefs: []
  type: TYPE_NORMAL
- en: '"The Data-Centric AI Competition inverts the traditional format and asks you
    to improve a dataset, given a fixed model. We will provide you with a dataset
    to improve by applying data-centric techniques such as fixing incorrect labels,
    adding examples that represent edge cases, applying data augmentation, and so
    on."'
  prefs: []
  type: TYPE_NORMAL
- en: This paradigm shift highlights the importance of data in deep learning, especially
    supervised deep learning, where labeled data is important. An implied underlying
    assumption is that different data will produce different model metrics, even if
    the same model architecture and parameters are used. This requires us to diligently
    track the data versioning process so that we know which version of the data is
    being used to produce the winning model.
  prefs: []
  type: TYPE_NORMAL
- en: There are several emerging frameworks for tracking data versioning in the ML/DL
    life cycle. One of the early pioneers in this domain is **DVC** ([http://dvc.org](http://dvc.org)).
    It uses a set of GitHub-like commands to pull/push data as if they are code. It
    allows the data to be stored remotely in S3, or Google Drive, among many other
    popular stores. However, the data that's stored in the remote store becomes hashed
    and isn't human-readable. This becomes a locked-in problem since the only way
    to access the data is through the DVC tool and configuration. In addition, it
    is hard to track how the data and its schema have been changed. While it is possible
    to integrate MLflow with DVC, its usability and flexibility are not as desirable
    as we want. Thus, we will not deep dive into this approach in this book. If you
    are interested in this, we suggest that you utilize the *Versioning data and models
    in ML projects using DVC and AWS* reference at the end of this chapter to find
    more details about using DVC.
  prefs: []
  type: TYPE_NORMAL
- en: The recently open sourced and open format-based **Delta Lake** ([https://delta.io/](https://delta.io/))
    is a practical solution for integrated data management and version control in
    a DL/ML project, especially since MLflow can directly support such integration.
    This is also the foundational data management layer, called **Lakehouse** ([https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html](https://databricks.com/blog/2020/01/30/what-is-a-data-lakehouse.html)),
    which unifies both data warehouse and streaming data into one data foundation
    layer. It supports both schema change tracking and data versioning, which is ideal
    for a DL/ML data use scenario. Delta tables are based on the open standard file
    format called **Parquet** ([https://parquet.apache.org/](https://parquet.apache.org/)),
    which is widely supported for large-scale data storage.
  prefs: []
  type: TYPE_NORMAL
- en: Delta Table in Databricks
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this section assumes that you have access to a Databricks service,
    which allows you to experiment with the Delta Lake format in the **Databricks
    File System** (**DBFS**). You can get a trial account for the community version
    by going to the Databricks portal: [https://community.cloud.databricks.com/login.html](https://community.cloud.databricks.com/login.html).'
  prefs: []
  type: TYPE_NORMAL
- en: 'Note that this section requires you to use **PySpark** to manipulate the data
    through both reading/writing data from/into storage such as S3\. Delta Lake has
    a capability called **Time Travel** that can automatically version the data. By
    passing a parameter such as a timestamp or a version number, you can read any
    historical data for that particular version or timestamp. This makes reproducing
    and tracking the experiments much easier as the only temporal metadata about the
    data is the version number or timestamp of the data. There are two ways to query
    the Delta table:'
  prefs: []
  type: TYPE_NORMAL
- en: '`timestampAsOf`: This lets you read the Delta table, as well as read a version
    that has a specific timestamp. The following code shows how the data can be read
    using `timestampAsOf`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: '`versionAsOf`: This defines the numerical value of the Delta table''s version.
    You also have the option to read a version that has a specific version, starting
    with version 0\. The following PySpark code reads the data with the `versionAsOf`
    option defined as version `52`:'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: '[PRE108]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: Having this kind of timestamped or versioned access is a key advantage to tracking
    any file version using a Delta table. So, let's look at a concrete example of
    this in MLflow so that we can track the IMDb dataset we have been using.
  prefs: []
  type: TYPE_NORMAL
- en: An example of tracking data using MLflow
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'For the IMDb datasets we have been using to fine-tune the sentiment classification
    model, we will upload these CSV files into Databricks'' data store or any S3 bucket
    that you can access from your Databricks portal. Once you''ve done that, follow
    these steps to create a Delta table that supports versioned and timestamped data
    access:'
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the following CSV files into a DataFrame (assuming that you uploaded the
    `train.csv` file into the `FileStore/imdb/` folder in Databricks):'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE109]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Write the `imdb_train_df` DataFrame in DBFS as a Delta table, as follows:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE110]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Read the `training.delta` file back into memory using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE111]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: 'Now, look at the history of the Delta table via the Databricks UI. You click
    on the **History** tab once you''ve read the Delta table from storage into memory:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '![Figure 4.12 – The train_delta table''s history with a version and a timestamp
    column'
  prefs: []
  type: TYPE_NORMAL
- en: '](img/B18120_Figure_4.12.jpg)'
  prefs: []
  type: TYPE_NORMAL
- en: Figure 4.12 – The train_delta table's history with a version and a timestamp
    column
  prefs: []
  type: TYPE_NORMAL
- en: The preceding screenshot shows that the version is **0** and that the timestamp
    is **2021-11-22**. This is the value that we can use to access the versionized
    data when passing the version number or timestamp to a Spark DataFrame reader.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the versioned `imdb/train_delta` file using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE112]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will read version `0` of the `train.delta` file. If we had other versions
    of this file, we could pass a different version number.
  prefs: []
  type: TYPE_NORMAL
- en: 'Read the timestamped `imdb/train_delta` file using the following command:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE113]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: This will read the timestamped data. At the time of writing, this is the only
    timestamp we have, which is fine. If we had more timestamped data, we could pass
    a different version to it.
  prefs: []
  type: TYPE_NORMAL
- en: 'Now, if we need to log this data version in the MLflow tracking experiment
    run, we can just log the path of the data, the version number, and/or the timestamp
    using `mlflow.log_parameter()`. This will log these as part of the experiment''s
    parameter key-value list:'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '[PRE114]'
  prefs:
  - PREF_IND
  type: TYPE_PRE
- en: The only requirement for using a Delta table is that the data needs to be stored
    in a form of storage that supports Delta tables, such as Lakehouse, which is supported
    by Databricks. This is of great value for enterprise ML/DL scenarios since we
    can track data versioning alongside code and model versioning.
  prefs: []
  type: TYPE_NORMAL
- en: In summary, Delta Lake provides a simple yet powerful way to version data. MLflow
    can easily log these version numbers and timestamps as part of the experiment's
    parameter lists to track the data, as well as all the other parameters, metrics,
    artifacts, code, and models consistently.
  prefs: []
  type: TYPE_NORMAL
- en: Summary
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'In this chapter, we took a deep dive into how we can track code and data versions
    in an MLflow experiment run. We started by reviewing the different types of notebooks:
    Jupyter notebooks, Databricks notebooks, and VS Code notebooks. We compared them
    and recommended that VS Code should be used to author a notebook due to its IDE
    support, as well as its Python styling, autocompletion, and many more rich features.'
  prefs: []
  type: TYPE_NORMAL
- en: Then, after reviewing the limitations of existing ML pipeline API frameworks,
    we discussed how to create a multi-step DL pipeline using MLflow's `run_id`, and
    then use a child `run_id` for each pipeline step. The flexibility to do pipeline
    chaining and tracking by passing parameters or artifact store locations to the
    next step can be done using `mlflow.run()` and `mlflow.tracking.MlflowClient()`.
    We successfully ran the end-to-end three-step pipeline using the MLflow nested
    run tracking capability. This will also open doors for us to extend the use of
    MLproject for running different steps in a distributed way in future chapters.
  prefs: []
  type: TYPE_NORMAL
- en: We also learned how to track privately built Python packages using the `git+ssh`
    approach. We then used the Delta Lake approach to gain versioned and timestamped
    access to data. This allows data to be tracked in two ways using a version number
    or a timestamp. MLflow can then log these version numbers or timestamps as a parameter
    during the MLflow experiment run. Since we are entering the data-centric AI era,
    being able to track data versioning is critical for reproducibility and time travel.
  prefs: []
  type: TYPE_NORMAL
- en: With that, we've finished learning how to comprehensively track code, data,
    and models using MLflow. In the next chapter, we will learn how to scale out our
    DL experiment in a distributed way.
  prefs: []
  type: TYPE_NORMAL
- en: Further reading
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 'For more information about the topics that were covered in this chapter, take
    a look at the following resources:'
  prefs: []
  type: TYPE_NORMAL
- en: 'MLflow notebook experiment tracking in Databricks: [https://docs.databricks.com/applications/mlflow/tracking.html#create-notebook-experiment](https://docs.databricks.com/applications/mlflow/tracking.html#create-notebook-experiment)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Building Multistep Workflows*: [https://www.mlflow.org/docs/latest/projects.html#building-multistep-workflows](https://www.mlflow.org/docs/latest/projects.html#building-multistep-workflows)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*End-to-end ML pipelines with MLflow projects*: [https://dzlab.github.io/ml/2020/08/09/mlflow-pipelines/](https://dzlab.github.io/ml/2020/08/09/mlflow-pipelines/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: 'Installing a privately built Python package: [https://medium.com/@ffreitasalves/pip-installing-a-package-from-a-private-repository-b57b19436f3e](mailto:https://medium.com/@ffreitasalves/pip-installing-a-package-from-a-private-repository-b57b19436f3e)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Versioning data and models in ML projects using DVC and AWS*: [https://medium.com/analytics-vidhya/versioning-data-and-models-in-ml-projects-using-dvc-and-aws-s3-286e664a7209](https://medium.com/analytics-vidhya/versioning-data-and-models-in-ml-projects-using-dvc-and-aws-s3-286e664a7209)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Introducing Delta Time Travel for Large Scale Data Lakes*: [https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html](https://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*How We Won the First Data-Centric AI Competition: Synaptic-AnN*: [https://www.deeplearning.ai/data-centric-ai-competition-synaptic-ann/](https://www.deeplearning.ai/data-centric-ai-competition-synaptic-ann/)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*Reproduce Anything: Machine Learning Meets Data Lakehouse*: [https://databricks.com/blog/2021/04/26/reproduce-anything-machine-learning-meets-data-lakehouse.html](https://databricks.com/blog/2021/04/26/reproduce-anything-machine-learning-meets-data-lakehouse.html)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
- en: '*DATABRICKS COMMUNITY EDITION: A BEGINNER''S GUIDE*: [https://www.topcoder.com/thrive/articles/databricks-community-edition-a-beginners-guide](https://www.topcoder.com/thrive/articles/databricks-community-edition-a-beginners-guide)'
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
